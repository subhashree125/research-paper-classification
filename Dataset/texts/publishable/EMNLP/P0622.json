{
  "Abstract": "In this paper, we aim to generate text classifica-tion data given arbitrary class definitions (i.e.,user instruction), so one can train a text classi-fier without any human annotation or raw cor-pus. Recent advances in large language models(LLMs) lead to pioneer attempts to individu-ally generate texts for each class via prompting.In this paper, we propose Incubator, the firstframework that can handle complicated andeven mutually dependent classes (e.g., TEDTalk given by Educator and Other). Specif-ically, our Incubator is a fine-tuned LLM thattakes the instruction of all class definitions asinput, and in each inference, it can jointly gen-erate one sample for every class. First, we tuneIncubator on the instruction-to-data mappingsthat we obtained from classification datasetsand descriptions on Hugging Face together within-context augmentation by GPT-4. To empha-size the uniformity and diversity in generations,we refine Incubator by fine-tuning with the clus-ter centers of semantic textual embeddings ofthe generated samples. We compare Incuba-tor on various classification tasks with strongbaselines such as direct LLM-based inferenceand training data generation by prompt engi-neering. Experiments show Incubator is able to(1) outperform previous methods on traditionalbenchmarks, (2) take label interdependency anduser preference into consideration, and (3) en-able logical text mining by incubating multipleclassifiers.",
  "Introduction": "Text classification is one of the most fundamen-tal natural language processing (NLP) tasks andplays a vital role in many NLP systems (Han andKamber, 2000). Traditional supervised text clas-sification fine-tunes models on expensive humanannotation (Zhang et al., 2015), limiting its usagefor lower-source domains. Zero-shot text classifi-cation reduces manual effort by building classifiers",
  ": A comparison of Incubator with differentmethods for zero-shot text classification": "with minimal inputs, such as label names (Wanget al., 2021; Zhang et al., 2023b; Wang et al.,2023a). These zero-shot methods are typicallybased on mining pseudo-training data from massiveraw texts with precise filtering algorithms, whichunfortunately limits their application to simple la-bels. For more complex labels, their distributionsare extremely scarce in raw texts and filtering al-gorithms struggle to recall these examples whilemaintaining their precision.",
  "Large language models (LLMs) (Touvron et al.,": "2023a,b; OpenAI, 2023), such as GPT-3 (Brownet al., 2020), have been recently introduced to ad-dress this problem with their proficient capabilityto capture the nuance in complex labels. Specifi-cally, people prompt LLMs to generate data basedon each label, and then fine-tune small classifiersas the final production (Ye et al., 2022a,b). Existing LLM-based zero-shot text classifica-tion methods, while feasible, face two major chal-lenges, (1) class definitions can go beyond a sim-ple label name, such as TED Talk given by Ed-ucator and (2) class definitions can depend oneach other. For example, the class Other is only",
  ": An overview of our Incubator framework": "meaningful when seeing other classes; As shownin , the class Optimistic shall not con-tain Love when Love itself presents as a class.Therefore, the scope of the class with the sametextual definition can vary as other classes change. We argue that the LLMs need further instruction-tuning (Ouyang et al., 2022), particularly for classi-fication data generation. Specifically, we leveragepublic classification datasets with descriptions forthis tuning. This allows the user to control the LLMto generate useful training data for small modelsbased on (1) label interdependency and (2) userpreferences described in the instructions. Conse-quently, the LLM-based zero-shot text classifica-tion can be formalized as model incubation thatUser requires a model by an instruction, the LLM(Incubator) then generates useful training data toincubate such a classifier. In this paper, we first collect pairs of datasetdescriptions and training data samples on Hug-gingface (Wolf et al., 2019a), each formalized asa dictionary with each label as a key and a sampleas the value. These data are beneficial for Incu-bator to learn label interdependency as the exam-ples from different classes are presented jointly.Then the data descriptions are manually convertedto user instructions, which establishes a mappingfrom the user instruction to training data. These instructions are augmented by a relatively strongLLM (e.g., GPT-4) using in-context learning (ICL)(Dong et al., 2023b) and used to instruction-tunean open-source LLM (e.g., LLaMA-2-7b-hf) as ourIncubator. Note that we can leverage GPT-4 withICL as Incubator too. We recommend open-sourceLLMs as Incubator because of open parameters,inference efficiency, and further fine-tuning. To alleviate the known negative impact of databias on text classification (Dixon et al., 2018; Liet al., 2021b; Jin et al., 2022) and bias in contentsgenerated by LLMs (Gallegos et al., 2023; Fanget al., 2023), we propose a novel self-diversificationtechnique to increase the data uniformity and diver-sity, utilizing the text representations from a textembedder (Wang et al., 2022). Specifically, weinstruct the Incubator many times (e.g., 1024), andthen use a clustering algorithm (e.g., K-means) toget the sample nearest to each cluster center whichare semantically different from one another. Thesesamples are incorporated in the same batch to fur-ther instruct-tune Incubator to increase the datauniformity and diversity. We conduct experiments to test the classifier in-cubation ability of our Incubator on various tasksto test its basic incubation ability, label interdepen-dency awareness, and user instruction followingability. These tasks involve incubating classifiers for (1) traditional benchmarks, (2) classificationtasks with Other as a label, and (3) classificationtasks with user customization for personal prefer-ence. We include strong baselines such as directlyinstructing the LLM to classify texts and promptingLLMs to generate data for each label separately.Experiment results verify our Incubator to beable to (1) incubate strong text classifiers that out-perform the baselines, (2) consider the label inter-dependency and follow the user preference in theinstruction, (3) incubate multiple text classifiersand use logical conjunctions to realize advancedtext mining systems.Our contributions in this paper are three-fold.",
  "Zero-shot Text Classification": "Traditional zero-shot text classification methodsare based on text mining in massive raw texts withlabel names for surface form matching (Wang et al.,2021; Wang and Shang, 2022; Zhang et al., 2023b;Wang et al., 2023a) or semantic matching (Han-jie et al., 2022; Aggarwal et al., 2023; Zhao et al.,2023). A related setup allows incorporating someseed words for each class to strengthen the textmining precision (Wang et al., 2023b; Dong et al.,2023a). With the emergence of LLMs, many pio-neer studies on LLM-based zero-shot text classifi-cation propose to prompt LLMs with label namesto synthesize texts falling in target classes. Thesetexts are used to fine-tune small classifiers on thosegenerated results (Ye et al., 2022a,b). However,these methods are substantially label-wise text gen-eration, which fails to consider the whole classifi-cation task, involving label interdependency and",
  "Instruction-tuning": "Following instructions (Zhang et al., 2023a) is afundamental capability for Large Language Mod-els (LLMs), crucial for understanding and actingupon user commands, thus enhancing their appealto user-specific applications. InstructGPT (Ouyanget al., 2022) represents an initial exploration intoLLMs tailored to follow instructions, revealingtheir capacity to perform tasks as directed by users.ChatGPT (OpenAI, 2023), with its superior ca-pability to follow instructions, bolstered by rein-forcement learning with human feedback (RLHF),has enjoyed considerable acclaim both within andbeyond the language research community. Fur-thermore, publicly available LLMs designed forinstruction-following, such as LLaMA (Touvronet al., 2023a,b; Meta, 2024), offer a rich foundationfor investigating the ability of LLMs to execute in-structions. Instruction-tuning not only contributesto the success of LLMs in text-to-text tasks (Zhanget al., 2023a), but is also able to customize imagegeneration (Chae et al., 2023) and text embeddings(Peng et al., 2024). Our work follows this trendto instruction-tune LLMs as Incubator, which cus-tomize classifiers according to user instructions.",
  "Model Incubation": "The area closest to model incubation is symbolicdistillation (West et al., 2022; Li et al., 2023),which distills a teacher model into a differenttype of student model. Those student models canfunction very differently from the initial languagemodeling teacher, such as commonsense reason-ing (West et al., 2022) and information extraction(Zhou et al., 2023). Another relevant domain istraining data generation including augmentation.Besides classification data generation (Ye et al.,2022a,b; Peng et al., 2023), there also exists gen-eration pipelines for question answering (Do et al.,2023; Gou et al., 2023) and natural language gen-eration (Xu et al., 2021). Model incubation differsfrom previous works as it takes user instruction asthe input, which allows a more user-oriented modelcustomization for personal usage.",
  "offers an overview of our Incubator frame-work, including two stages, (1) Instruction-tuning": "and (2) Self-diversification.The instruction-tuning stage utilizes the existing resources on theHuggingface platform to learn an LLM as Incu-bator to generate training data based on user in-structions. The self-diversification stage furtherimproves the uniformity and diversity in Incuba-tor generation with an auxiliary text embedder andclustering. We now elaborate on the details of thesetwo stages.",
  "Instruction-tuning for Incubator": "Instruction-to-data DatasetWe select 25 textclassification datasets on the Huggingface plat-form2 to build the initial instruction-to-data datasetfor instruction-tuning, such as financial news, coun-terfactual reviews, and toxic conversations. Foreach dataset, we extract its description and sam-ple a few (we select 10) samples per class from it,which are formalized as Python dictionaries. Thekeys in the dictionary are labels and each labelcorresponds to one text data as the value. Conse-quently, we get 250 instruction-to-data samples asthe initial dataset. We present a specific case insidethe dataset in the Appendix B. ICL-based AugmentationDirectly instruction-tuning the LLM on the initial dataset will likelyintroduce overfitting and bias to the Incubator dueto the limited number of instructions (Song et al.,2023). Thus, we address these issues by data aug-mentation (Ye et al., 2024) and use ICL (Donget al., 2023b) by GPT-4 (OpenAI, 2023) as theimplementation (Ho et al., 2023). We show thespecific prompt for in-context learning in of Appendix C. We have two in-context examplesthat map instructions to training data as Pythondictionaries, which are randomly sampled in eachquery. Finally, we augment the instruction-to-datadataset to 12K samples. This dataset is then usedto fine-tune the LLM as the Incubator.",
  "Self-diversification for Incubator": "Dataset uniformity and diversity are essential totext classification (Dixon et al., 2018) while thecontents from LLMs are generally biased, espe-cially when sampling from a single instruction(Gallegos et al., 2023; Fang et al., 2023). Thus,we propose a novel self-diversification technique toimprove the generation quality from our Incubator.The main idea is to instruction-tune the LLM on",
  "The source datasets are shown in Appendix E": "the same instruction with several semantically dif-ferent data samples. We refer to a pre-trained textembedder, specifically E5 (Wang et al., 2022), tocalculate the semantic similarity (Chandrasekaranand Mago, 2022). In our implementation, we reusethe instructions in the instruction-tuning dataset.For each instruction, we generate many (We select1024) training data3 and encode the data into thelatent embedding space. As the data are formalizedas Python dictionaries, we concatenate the embed-dings of the values (texts) corresponding to a fixedorder of keys.",
  "i=1E(d[li])": "where E(), d, li refer to the encoder, the data(dictionary) and the i-th label. represents theconcatenation operation and n represents the totallabel number. After all data are encoded, we run aK-means (We select K = 8) clustering algorithmon the embeddings and find the K samples withembeddings that are closest to the cluster centers.These samples, together with the instruction, estab-lish a one-to-many mapping that maps instructionto very semantically diverse data samples. We in-corporate these data in a batch of K and furtherinstruction-tune the LLM on it. Intuitively, this pro-cedure will increase the appearance probability ofdata with unique semantics to benefit the incubatedclassifier.",
  "Towards a comprehensive evaluation of our Incuba-tor, we organize the evaluation into three scenarios": "(1) Traditional BenchmarksWe include 8 tradi-tional text classification benchmarks, such as sen-timent analysis classification (1) SST-2 (Socheret al., 2013), (2) SST-5 (Socher et al., 2013), and (3)Emotion (Saravia et al., 2018), topic classification(4) AG News (Zhang et al., 2015), news locationclassification (5) NYT-LOC (Mozzherina, 2013),question type classification (6) TREC (Li and Roth,",
  "), intent classification (7) SNIPS (Couckeet al., 2018), and (8) sentiment classification to-wards a particular public figure Hillary (Barbieriet al., 2020)": "(2) Label OtherWe also test the ability of In-cubator to handle stronger label interdependency bydatasets with Other. We convert several datasetsby grouping minor categories based on the propor-tion as a single Other label, with details men-tioned in the Appendix D. These datasets includeunbalanced datasets: Emotion, NYT-LOC, andMassive (FitzGerald et al., 2022). These reviseddatasets will be also released for reproducibility (3) Complicated Class DefinitionsTo furthershowcase the usefulness of Incubator, we comewith several complicated instructions for Incubatorto incubate text classifiers that will be later usedto mine the desired texts from massive raw doc-uments, such as TED Talk Summary4, SteamGame Description5, and Text Message6.Note that all the datasets in our evaluations areexcluded from the instruction-tuning data of Incu-bator.",
  "Implementation Details": "We implement Incubator by fine-tuning the pa-rameters of LLaMA-2 (LLaMA-2-7b-hf) (Touvronet al., 2023b) on our constructed instruction-tuningdataset with AdamW optimizer (Loshchilov andHutter, 2019) and cosine annealing learning ratescheduler (Loshchilov and Hutter, 2017).Thespecific hyperparameters for the optimization areshown in in Appendix A.For all experiments, our Incubator is queried togenerate 1024 data dictionaries, each with one sam-",
  "Compared Methods": "One can directly prompt the LLM, LLaMA-2(LLaMA-2-7b-hf), which is the same as the LLMused in Incubator, with all the labels and the inputtext in the prompt and ask it to categorize the textinto one of the labels (Sun et al., 2023). We namethis method as Prompting.We first include some traditional zero-shot textclassifications for reference: Debiased Seed (Dong et al., 2023a): This is astate-of-the-art text mining method for zero-shottext classification. The method precisely assignspseudo-labels to texts by seed word (the sameas label name (Wang et al., 2023c)) matchingwith label cleaning (Mekala et al., 2022). Thesemined texts are then used to fine-tune a classifier. SemSup-XC++(Aggarwaletal.,2023):This method uses semantic similarity (Chan-drasekaran and Mago, 2022) between texts andlabel descriptions to assign labels with the high-est description similarity to texts. The originalSemSup-XC mines class descriptions and trainsa text embedding by contrastive learning (Gaoet al., 2021).We upgrade SemSup-XC to astronger SemSup-XC++ for LLMs and theadvancement in text embedding. We generatethe class descriptions by a state-of-the-artLLM, GPT-4 (OpenAI, 2023), and produce theembeddings by a strong text embedder (Wanget al., 2022).For the main comparison, we include strongbaselines that generate training data without re- quiring massive raw texts as follows. ZeroGen++ (Ye et al., 2022a): This methodprompts LLMs (LLaMA-2-7b-hf) to generatetexts based on label descriptions in generationinstructions. Different from our Incubator, Zero-Gen handles each label separately, such as Gen-erate a negative movie review. Towards a faircomparison with our method, we formalize ourinstruction-tuning dataset as the template used inZeroGen to further fine-tune the model. The base-line upgraded by further fine-tuning is namedZeroGen++. ProGen++ (Ye et al., 2022b): This method fur-ther develops ZeroGen++ by an iterative ICL-based augmentation. With the classifier obtainedfrom ZeroGen++, ProGen++ selects the mosthelpful data with an influence function (Koh andLiang, 2017) that measures the change in themodels loss on the test data point. The mostinfluential data points are selected as in-contextexamples to prompt the LLM to generate morehelpful data to strengthen the classifier.Incubator w/ GPT-4: This is a variant of ourIncubator that prompts GPT-4 with in-context ex-amples from the Huggingface platform and theinstruction to sample the training data. We presentthis not as a baseline but to showcase that the Incu-bator idea also applies to propriety LLMs.All data generation baselines generate the sameamount of data (1024 per class) towards a fair com-parison. The reported results are the average of 5runs, except for SemSup-XC++, which does nothave randomness in the method.",
  "Traditional Benchmark Results": "The experiment results on traditional benchmarksare shown in . The comparison betweenZeroGen and ProGen baselines verifies our Incu-bator has a significant advantage over those la-bel interdependency-agnostic methods, which in-dicates the advantage of Incubator to consider thefull label set in the instruction.Moreover, the self-diversification procedure isshown to highly contribute to the performance of In-cubator, which boosts the performances on 5 out of8 datasets and achieves comparable performanceson others. Thus, self-diversification is verified tobe a reliable and beneficial technique to strengthenthe Incubator.In comparison with data generation methods, thetext mining and semantic similarity-based baselinesshow significant limitations on some datasets. For",
  ": Results on datasets with the Other class": "instance, Debiased Seed shows a significantly weakperformance on Emotion and TREC (question clas-sification) as the seed words are hard to propose forthese classes. SemSup-XC++ also shows a limita-tion when texts are in a special domain for semanticsimilarity calculation (e.g., questions in TREC).We also present the performances of direct in-ference based on LLaMA-2-7b-hf, which is gener-ally outperformed by the small LMs fine-tuned ondatasets generated by LLMs. This result is con-sistent with the discovery that LLMs are bettergenerators than discriminators (Dai et al., 2023).However, this requires the LLM generator to beaware of all labels to avoid the ignorance of labelinterdependency. Otherwise, the generator mightunderperform direct prompting LLM as shown inthe comparison between ZeroGen++ (ProGen++)with direct prompting.Finally, we evaluate the ICL-based Incubatorwith GPT-4 as the backbone model. With a signifi-cantly larger amount of parameters, Incubator withGPT-4 outperforms the one based on LLaMA-2.This indicates larger backbone models can furtherscale up the performance of our Incubator. Also,tunable models can benefit from self-diversificationto narrow the gap between the close-source GPT-4, which can also be improved once it becomesopen-source for fine-tuning.",
  "Label Other Results": "We present the experiment results on datasets withmiscellaneous (label Other) in .Theawareness of the miscellaneous category is im-portant for classification (Li et al., 2021a), espe-cially when limited labels are known in a largecorpus. For ZeroGen or ProGen, we use the labelname Other than ... (other labels) to promptfor generation. We can observe a significantlylarger gap between the Incubator and the labelinterdependency-agnostic methods, which showsthe advantage of Incubator on datasets with mis-cellaneous. Furthermore, the self-diversificationshows a more prominent improvement in perfor-",
  "Complicated Class Definition Results": "We further showcase how Incubator can be appliedto satisfy personal demands, such as mining itemspreferred by an individual. For each raw corpus,we propose four attributes a user might be inter-ested in, such as About AI for TED Talks. Foreach attribute, we create an instruction to build atext classifier with two labels: the target attributeand the miscellaneous label Other. We use theincubated classifier to score each raw text and se-lect the texts with the top scores. For evaluation,we ask GPT-4 and humans whether the mined textssatisfy the demand with Precision@100 as the met-ric. The human evaluation for each result is doneby 3 professional human annotators and keeps themajority decision.The text mining performance is presented in Ta-ble 3. Incubator incubates strong text miners withgenerally high precision on all setups. Remark-ably, we achieve nearly or exactly 100% precisionon several targets. Moreover, our miners are vali-dated to be able to handle different text domains,enabling a broad application of our Incubator.In , we further compare the incubationperformance on complicated classes between Incu-bator and baselines. The presented result is con-sistent with previous ones, which further verifiesthe benefit of Incubator for personalized classifierincubation.",
  "We further showcase how to utilize Incubator tosatisfy more complicated user demands. We in-": "crease the label complexity by adding logical con-junctions into labels, that are and (), or (),and not (). The logical conjunctions representa finer-grained demand from the user. For instance,one may want to search for texts that are Positiveand about food, as Positive About food.To realize such finer-grained text mining, weutilize the maneuverability of Incubator to incubatemultiple text miners and combine their scores withlogical probabilistic calculations as follows, P(LA LB) = P(LA)P(LB) P(LA LB) = P(LA) + P(LB) P(LA LB) P(LA) = 1 P(LA)where LA, LB are two labels used as the targets forthe incubation. Here we view the labels as indepen-dent for simplification. We use the Text Messagecorpus for text mining. For evaluation, we keepthe previous scenario unchanged. We compare twotypes of incubation scenarios, Direct Incubation Incubator only incubates onetext miner with the full label name, such as Pos-itive and about food. Conjunctive Incubation first decomposes thelabel name into multiple ones with correspond-ing conjunctions, like decomposing Positiveand about food into Positive About food.Then the score is calculated based on logicalprobabilistic calculations.The experiment results are presented in .Conjunctive incubation generally outperforms di-rect incubation, which shows the benefit of thisstrategy. As conjunctive incubation also showsstrong capability on three logical variables, thisshows how Incubator can be customized to morecomplex settings.",
  "Case Studies on Generated Training Data": "To more concretely demonstrate the intermediateprocesses in the incubation, we launch a study onthe generated texts from the Incubator for classifierincubation. We demonstrate the generated trainingdata for data mining in the text message corpus in. For each column, there is a piece of textgenerated with the target value and the other one inthe same Python dictionary with the miscellaneous",
  "TargetGenerated data with target labelGenerated data with misc label": "Hey! I love the new update. Its awesome!Just checking in on the progress of the project.PositiveWow, you got the tickets for our dream holiday!Ive booked the flights for next week.I absolutely love the new design of the app.Im having trouble logging into my account. Can you send me the report by end of today?What did you do during the weekend?RequestCould you please bring me a coffee?How was your day?Can you pass me the salt?Hey, did you catch the game last night? The pizza at Marios is the best in town!I have an important meeting at 10am tomorrow.About foodIm craving for a burger and fries!I might go for a run later.I just tried that new sushi place. Totally worth it!Hey, what time does the movie start? We need to finalize the report by tomorrow.Hey, do you want to catch a movie tonight?Work-relatedThe meeting is scheduled at 3 PM tomorrow.Do you want to catch up for dinner tonight?The project deadline has been extended.Hey! What are you up to this weekend?",
  ": The performance of incubated retrievers with logical conjunctions": "label Other.The most straightforward observation is the gen-erated data correctly follows the label, which guar-antees the foundational precision of the incubatedclassifiers. Also, the generated texts incorporatea wide range of syntactic structures and semanticcontents for the training data diversity. For themiscellaneous label, we can observe the Incubatorto cover various potential negative labels. For in-stance, the miscellaneous category for About foodincludes labels such as About meeting, Aboutsports, About movie, which broadens the nega-tive set understood by the incubated classifier.Finally, we can view some attribute correlationsbetween the data in the same generated Pythondictionary. In the Positive example, the threesamples have the same topic Project, Travel,and App. With these data different in the targetattribute but same in other attributes, the incubatedclassifier can better focus on the target attribute andeliminate spurious correlations.",
  "Incubation Dataset Size": "We first adjust the number of data generated fromIncubator to investigate how the incubated classifierwill be affected. We conduct experiments on TRECand SNIPS datasets with incubation data size from4 to 1024. The results are illustrated in .From the shown scaling-up trend, there is a clearthreshold (64) on the dataset size, after which the Number of data per class",
  "Instruction Robustness": "We then check the robustness of Incubator to in-structions by testing with different but semanticallyequal instructions. We rephrase each instructionfor TREC and SNIPS into 10 different versions andthen run the incubation pipeline for evaluation. Therobustness evaluation is presented in . Wecan observe the lexical and syntactical attributes,which are changed in the rephrasing, have limited",
  "Efficiency Analysis": "We analyze the time efficiency of the Incubator toexplore its efficiency in deployment. For datasetgeneration, we run the LLaMA model with the ac-celeration by the vllm package (Kwon et al., 2023).For the small classifier incubation, we fine-tune themodel with the trainer in the transformers pack-age (Wolf et al., 2019b). We evaluate the time fordataset generation and classifier incubation (fine-tuning). The time is obtained by averaging theresults in experiments on the 8 traditional bench-marks, which is illustrated in . All experi-ments are run on a single A100 device.For dataset generation, the average time is67.53s. The generation times for all benchmarksare distributed around this average since vllm hasa fixed max length limitation for decoding. Forclassifier incubation, the time is almost linearly de-pendent on the number of labels, which shows anaverage of 15.16s time cost per class.Thus, the time efficiency of our Incubator is fea-sible to incubate personal classifiers. Also, themain time cost happens in classifier incubationrather than calling the LLM for dataset generation,especially when the label number is large.",
  "Conclusion and Future Work": "In summary, this paper proposes a new frameworkfor model incubation by querying an instruction-tuned LLM. Our model, Incubator, is pre-trainedon Huggingface resources and ICL-based augmen-tation. The Incubator is further strengthened bya novel self-diversification technique. We showthat Incubator can incubate strong classifiers fortraditional benchmarks and customized text min-ing, following instructions. We also include com-prehensive analysis to explore the properties of theIncubator for deeper insight and better application.",
  "While Incubator shows strong performance in pro-ducing reliable and customized classifiers, it hassome limitations that can be further improved infuture works": "Instruction Effort:Current Incubator requiresthe user to include all label names in the instruction,which adds effort for the user to create instructions,especially when the label number is large or theuser is unclear about the label names. A combina-tion with existing work (Wang et al., 2023a) mightbe a direction to reduce user efforts further. LLM Knowledge Dependence:As an LLM-only methods, the Incubator is only able to generatetext within its knowledge scope. For emerging la-bels, theFuture work will concentrate on two tracks. 1)Improve the incubation quality: We can incor-porate existing or new methods to improve datageneration quality like higher diversity and hardernegative samples. 2) Broaden the scope of incu-bated models: The incubated model can be morethan classifiers, such as question responder andtext summarizer. These models might require morecomplicated instruction understanding and othertechniques for model enhancement. Incubator stillhas to rely on delicate explanations or in-contextexamples to handle them. Pranjal Aggarwal, Ameet Deshpande, and Karthik R.Narasimhan. 2023. Semsup-xc: Semantic supervi-sion for zero and few-shot extreme classification.In International Conference on Machine Learning,ICML 2023, 23-29 July 2023, Honolulu, Hawaii,USA, volume 202 of Proceedings of Machine Learn-ing Research, pages 228247. PMLR. Francesco Barbieri, Jos Camacho-Collados, Luis Es-pinosa Anke, and Leonardo Neves. 2020. Tweeteval:Unified benchmark and comparative evaluation fortweet classification. In Findings of the Associationfor Computational Linguistics: EMNLP 2020, OnlineEvent, 16-20 November 2020, volume EMNLP 2020of Findings of ACL, pages 16441650. Associationfor Computational Linguistics. Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish,Alec Radford, Ilya Sutskever, and Dario Amodei.2020. Language models are few-shot learners. In Ad-vances in Neural Information Processing Systems 33:Annual Conference on Neural Information Process-ing Systems 2020, NeurIPS 2020, December 6-12,2020, virtual.",
  "Dhivya Chandrasekaran and Vijay Mago. 2022. Evolu-tion of semantic similarity - A survey. ACM Comput.Surv., 54(2):41:141:37": "Alice Coucke, Alaa Saade, Adrien Ball, ThodoreBluche, Alexandre Caulier, David Leroy, ClmentDoumouro, Thibault Gisselbrecht, Francesco Calt-agirone, Thibaut Lavril, Mal Primet, and JosephDureau. 2018. Snips voice platform: an embeddedspoken language understanding system for private-by-design voice interfaces. CoRR, abs/1805.10190. Haixing Dai, Zhengliang Liu, Wenxiong Liao, XiaokeHuang, Yihan Cao, Zihao Wu, Lin Zhao, ShaochenXu, Wei Liu, Ninghao Liu, Sheng Li, Dajiang Zhu,Hongmin Cai, Lichao Sun, Quanzheng Li, DinggangShen, Tianming Liu, and Xiang Li. 2023.Aug-gpt: Leveraging chatgpt for text data augmentation.Preprint, arXiv:2302.13007. Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain,and Lucy Vasserman. 2018. Measuring and mitigat-ing unintended bias in text classification. In Pro-ceedings of the 2018 AAAI/ACM Conference on AI,Ethics, and Society, AIES 2018, New Orleans, LA,USA, February 02-03, 2018, pages 6773. ACM. Xuan Long Do, Bowei Zou, Shafiq R. Joty, Anh TranTai, Liangming Pan, Nancy F. Chen, and Ai Ti Aw.2023.Modeling what-to-ask and how-to-ask foranswer-unaware conversational question generation.In Proceedings of the 61st Annual Meeting of theAssociation for Computational Linguistics (Volume 1:Long Papers), ACL 2023, Toronto, Canada, July 9-14,2023, pages 1078510803. Association for Computa-tional Linguistics.",
  "Xiao Fang, Shangkun Che, Minjia Mao, HongzheZhang, Ming Zhao, and Xiaohang Zhao. 2023.Bias of ai-generated content: An examination ofnews produced by large language models. CoRR,abs/2309.09825": "Jack FitzGerald, Christopher Hench, Charith Peris,Scott Mackie, Kay Rottmann, Ana Sanchez, AaronNash, Liam Urbach, Vishesh Kakarala, RichaSingh, Swetha Ranganath, Laurie Crist, MishaBritan, Wouter Leeuwis, Gokhan Tur, and PremNatarajan. 2022.Massive: A 1m-example mul-tilingual natural language understanding datasetwith 51 typologically-diverse languages. Preprint,arXiv:2204.08582. Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow,Md. Mehrab Tanjim, Sungchul Kim, Franck Dernon-court, Tong Yu, Ruiyi Zhang, and Nesreen K. Ahmed.2023. Bias and fairness in large language models: Asurvey. CoRR, abs/2309.00770.",
  "Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021": "Simcse: Simple contrastive learning of sentence em-beddings. In Proceedings of the 2021 Conference onEmpirical Methods in Natural Language Processing,EMNLP 2021, Virtual Event / Punta Cana, Domini-can Republic, 7-11 November, 2021, pages 68946910. Association for Computational Linguistics. Qi Gou, Zehua Xia, Bowen Yu, Haiyang Yu, Fei Huang,Yongbin Li, and Cam-Tu Nguyen. 2023. Diversifyquestion generation with retrieval-augmented styletransfer. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Process-ing, EMNLP 2023, Singapore, December 6-10, 2023,pages 16771690. Association for ComputationalLinguistics.",
  "Machine Learning, ICML 2017, Sydney, NSW, Aus-tralia, 6-11 August 2017, volume 70 of Proceedingsof Machine Learning Research, pages 18851894.PMLR": "Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, YingSheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonza-lez, Hao Zhang, and Ion Stoica. 2023. Efficient mem-ory management for large language model servingwith pagedattention. In Proceedings of the 29th Sym-posium on Operating Systems Principles, SOSP 2023,Koblenz, Germany, October 23-26, 2023, pages 611626. ACM. Liunian Harold Li, Jack Hessel, Youngjae Yu, XiangRen, Kai-Wei Chang, and Yejin Choi. 2023. Sym-bolic chain-of-thought distillation: Small models canalso \"think\" step-by-step. In Proceedings of the 61stAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), ACL 2023,Toronto, Canada, July 9-14, 2023, pages 26652679.Association for Computational Linguistics. Peiran Li, Fang Guo, and Jingbo Shang. 2021a. \"misc\"-aware weakly supervised aspect classification. InProceedings of the 2021 SIAM International Con-ference on Data Mining, SDM 2021, Virtual Event,April 29 - May 1, 2021, pages 468476. SIAM.",
  "Xin Li and Dan Roth. 2002. Learning question clas-sifiers. In COLING 2002: The 19th InternationalConference on Computational Linguistics": "Zichao Li, Dheeraj Mekala, Chengyu Dong, and JingboShang. 2021b. Bfclass: A backdoor-free text classi-fication framework. In Findings of the Associationfor Computational Linguistics: EMNLP 2021, Vir-tual Event / Punta Cana, Dominican Republic, 16-20 November, 2021, pages 444453. Association forComputational Linguistics. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A robustly optimized BERT pretrainingapproach. CoRR, abs/1907.11692. Ilya Loshchilov and Frank Hutter. 2017.SGDR:stochastic gradient descent with warm restarts. In 5thInternational Conference on Learning Representa-tions, ICLR 2017, Toulon, France, April 24-26, 2017,Conference Track Proceedings. OpenReview.net. Ilya Loshchilov and Frank Hutter. 2019. Decoupledweight decay regularization. In 7th InternationalConference on Learning Representations, ICLR 2019,New Orleans, LA, USA, May 6-9, 2019. OpenRe-view.net. Dheeraj Mekala, Chengyu Dong, and Jingbo Shang.2022. LOPS: learning order inspired pseudo-labelselection for weakly supervised text classification.In Findings of the Association for ComputationalLinguistics: EMNLP 2022, Abu Dhabi, United ArabEmirates, December 7-11, 2022, pages 48944908.Association for Computational Linguistics.",
  "OpenAI. 2023.GPT-4 technical report.CoRR,abs/2303.08774": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll L. Wainwright, Pamela Mishkin, ChongZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,John Schulman, Jacob Hilton, Fraser Kelton, LukeMiller, Maddie Simens, Amanda Askell, Peter Welin-der, Paul F. Christiano, Jan Leike, and Ryan Lowe.2022. Training language models to follow instruc-tions with human feedback. In Advances in NeuralInformation Processing Systems 35: Annual Confer-ence on Neural Information Processing Systems 2022,NeurIPS 2022, New Orleans, LA, USA, November 28- December 9, 2022.",
  "Generating efficient training data via llm-based at-tribute manipulation. CoRR, abs/2307.07099": "Letian Peng, Yuwei Zhang, Zilong Wang, Jayanth Srini-vasa, Gaowen Liu, Zihan Wang, and Jingbo Shang.2024. Answer is all you need: Instruction-followingtext embedding via answering the question. CoRR,abs/2402.09642. Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang,Junlin Wu, and Yi-Shin Chen. 2018. CARER: Con-textualized affect representations for emotion recog-nition. In Proceedings of the 2018 Conference onEmpirical Methods in Natural Language Processing,pages 36873697, Brussels, Belgium. Associationfor Computational Linguistics. Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Y. Ng,and Christopher Potts. 2013. Recursive deep mod-els for semantic compositionality over a sentimenttreebank. In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Processing,EMNLP 2013, 18-21 October 2013, Grand HyattSeattle, Seattle, Washington, USA, A meeting of SIG-DAT, a Special Interest Group of the ACL, pages16311642. ACL. Yisheng Song, Ting Wang, Puyu Cai, Subrota K. Mon-dal, and Jyoti Prakash Sahoo. 2023. A comprehen-sive survey of few-shot learning: Evolution, applica-tions, challenges, and opportunities. ACM Comput.Surv., 55(13s):271:1271:40. Xiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, ShangweiGuo, Tianwei Zhang, and Guoyin Wang. 2023. Textclassification via large language models. In Find-ings of the Association for Computational Linguis-tics: EMNLP 2023, Singapore, December 6-10, 2023,",
  "pages 89909005. Association for ComputationalLinguistics": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Aurlien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. 2023a. Llama: Openand efficient foundation language models. CoRR,abs/2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurlien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023b. Llama 2: Open foundation andfine-tuned chat models. CoRR, abs/2307.09288.",
  "Zihan Wang, Dheeraj Mekala, and Jingbo Shang. 2021": "X-class: Text classification with extremely weak su-pervision. In Proceedings of the 2021 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, NAACL-HLT 2021, Online, June 6-11,2021, pages 30433053. Association for Computa-tional Linguistics. Zihan Wang, Tianle Wang, Dheeraj Mekala, and JingboShang. 2023b. A benchmark on extremely weakly su-pervised text classification: Reconcile seed matchingand prompting approaches. In Findings of the As-sociation for Computational Linguistics: ACL 2023,Toronto, Canada, July 9-14, 2023, pages 39443962.Association for Computational Linguistics. Zihan Wang, Tianle Wang, Dheeraj Mekala, and JingboShang. 2023c. A benchmark on extremely weakly su-pervised text classification: Reconcile seed matchingand prompting approaches. In Findings of the As-sociation for Computational Linguistics: ACL 2023,Toronto, Canada, July 9-14, 2023, pages 39443962.Association for Computational Linguistics. Zilong Wang and Jingbo Shang. 2022. Towards few-shot entity recognition in document images: A label-aware sequence-to-sequence framework. In Find-ings of the Association for Computational Linguis-tics: ACL 2022, pages 41744186. Peter West, Chandra Bhagavatula, Jack Hessel, Jena D.Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,Sean Welleck, and Yejin Choi. 2022.Symbolicknowledge distillation: from general language mod-els to commonsense models. In Proceedings of the2022 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, NAACL 2022, Seattle,WA, United States, July 10-15, 2022, pages 46024625. Association for Computational Linguistics. Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz,and Jamie Brew. 2019a. Huggingfaces transformers:State-of-the-art natural language processing. CoRR,abs/1910.03771. Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz,and Jamie Brew. 2019b. Huggingfaces transformers:State-of-the-art natural language processing. CoRR,abs/1910.03771. Xinnuo Xu, Guoyin Wang, Young-Bum Kim, andSungjin Lee. 2021. Augnlg: Few-shot natural lan-guage generation using self-trained data augmenta-tion. In Proceedings of the 59th Annual Meeting ofthe Association for Computational Linguistics andthe 11th International Joint Conference on NaturalLanguage Processing, ACL/IJCNLP 2021, (Volume 1:Long Papers), Virtual Event, August 1-6, 2021, pages11831195. Association for Computational Linguis-tics. Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiang-tao Feng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.2022a. Zerogen: Efficient zero-shot learning viadataset generation. In Proceedings of the 2022 Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP 2022, Abu Dhabi, United ArabEmirates, December 7-11, 2022, pages 1165311669.Association for Computational Linguistics. Jiacheng Ye, Jiahui Gao, Zhiyong Wu, Jiangtao Feng,Tao Yu, and Lingpeng Kong. 2022b. Progen: Pro-gressive zero-shot dataset generation via in-contextfeedback. In Findings of the Association for Com-putational Linguistics: EMNLP 2022, Abu Dhabi,United Arab Emirates, December 7-11, 2022, pages",
  "Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015": "Character-level convolutional networks for text clas-sification. In Advances in Neural Information Pro-cessing Systems 28: Annual Conference on Neural In-formation Processing Systems 2015, December 7-12,2015, Montreal, Quebec, Canada, pages 649657. Yu Zhang, Bowen Jin, Xiusi Chen, Yanzhen Shen, YunyiZhang, Yu Meng, and Jiawei Han. 2023b. Weaklysupervised multi-label classification of full-text sci-entific papers.In Proceedings of the 29th ACMSIGKDD Conference on Knowledge Discovery andData Mining, KDD 2023, Long Beach, CA, USA,August 6-10, 2023, pages 34583469. ACM. Xuandong Zhao, Siqi Ouyang, Zhiguo Yu, Ming Wu,and Lei Li. 2023. Pre-trained language models canbe fully zero-shot learners. In Proceedings of the 61stAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), ACL 2023,Toronto, Canada, July 9-14, 2023, pages 1559015606. Association for Computational Linguistics.",
  "Dataset: app_reviews": "Description: It is a large dataset of Android applications belonging to 23 different apps categories, which provides an overview of the types of feedback users report on the apps and documents the evolution of the related code metrics. The dataset contains about 395 applications of the F-Droid repository, including around 600 versions, 280,000 user reviews (extracted with specific text mining approaches)"
}