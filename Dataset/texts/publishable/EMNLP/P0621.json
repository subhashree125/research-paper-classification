{
  "Abstract": "For extremely weak-supervised text classifica-tion, pioneer research generates pseudo labelsby mining texts similar to the class names fromthe raw corpus, which may end up with verylimited or even no samples for the minorityclasses. Recent works have started to generatethe relevant texts by prompting LLMs usingthe class names or definitions; however, thereis a high risk that LLMs cannot generate in-distribution (i.e., similar to the corpus wherethe text classifier will be applied) data, lead-ing to ungeneralizable classifiers. In this paper,we combine the advantages of these two ap-proaches and propose to bridge the gap via anovel framework, text grafting, which aims toobtain clean and near-distribution weak super-vision for minority classes. Specifically, wefirst use LLM-based logits to mine maskedtemplates from the raw corpus, which have ahigh potential for data synthesis into the targetminority class. Then, the templates are filledby state-of-the-art LLMs to synthesize near-distribution texts falling into minority classes.Text grafting shows significant improvementover direct mining or synthesis on minorityclasses. We also use analysis and case stud-ies to comprehend the property of text grafting.",
  "Introduction": "Recent research has made rapid progress on ex-tremely weak-supervised text classification (XWS-TC) (Wang et al., 2023), limiting the supervision toa brief natural-language description without any an-notated samples. For example, text mining-basedXWS-TC (Meng et al., 2020; Wang et al., 2021;Shen et al., 2021; Mekala et al., 2022; Zhao et al.,2023; Dong et al., 2023a) takes only class namesor seed words from humans and discovers potentialin-class texts following designated heuristics.Minority classes are arguably the most chal-lenging part of XWS-TC. The class distribution",
  ": High-level comparison among three discussedXWS-TC frameworks": "in real-world datasets is often a long-tailed distribu-tion (Zhang et al., 2023), with a non-trivial numberof minority classes. These minority classes have avery small number of documents in the raw corpus,therefore, it is difficult to locate the right docu-ments by mining-based methods, leading to noisypseudo-labels. Under extreme circumstances, themining-based methods may end up with no samplefor minority classes.A potential way to address this issue is datasynthesis-based XWS-TC (Ye et al., 2022a,b; Pengand Shang, 2024), which hopes to generate in-class texts by prompting large language models(LLM) (Brown et al., 2020; OpenAI, 2023; Tou-vron et al., 2023a,b; Meta, 2024; Mesnard et al.,2024; OpenAI, 2024) with class names or defini- tions. However, such synthesized texts may followa distribution different from the corpus where thetext classifier will be later applied (Mitchell et al.,2023), which makes the learned text classifier out-of-distribution, leading to poor performance.This paper combines the advantages of mining-based and synthesis-based frameworks to proposea new framework, text grafting, which aims to ob-tain clean and near-distribution weak supervisionfor minority classes. As specified in , textgrafting incorporates three stages: (1) PotentialText Mining gathers raw texts with beneficial com-ponents to synthesize in-class texts for the targetminority class. (2) Template Creation forms tem-plates by masking the components that do not con-tribute to the in-class text synthesis. (3) TemplateFilling synthesizes in-class texts by filling in theblanks. systematically compares the weaksupervision obtained by different frameworks.To identify the words not contributing to the clas-sification, we borrow the marginalization idea fromLLM reasoning (Holtzman et al., 2021). We getthe probability logit of each word in the raw textby instructing LLMs (relatively small, specificallyGemma (Mesnard et al., 2024)) to generate with orwithout the in-class as a requirement. The differ-ence between the two logits represents the potentialof each word to appear in the grafted text. As onlywords with high potential will be left, we use theaverage potential of top-K% words to representthe text potential score. The bottom-(100 K)%words will be masked to form the template fordata synthesis. We rank the templates by their po-tential scores and select top-T% templates for thelast template-filling stage. Finally, these selectedtemplates are filled by prompting a state-of-the-artLLM, GPT-4o (OpenAI, 2024).We compare the three mentioned frameworks onvarious raw corpora to classify different minorityclasses. The experiment results show text graft-ing can outperform state-of-the-art text mining anddataset synthesis methods. The ablation study ver-ifies that all stages and the intermediate templatecontribute to the success of our proposed text graft-ing. The mask-and-filling scenario also shows itsadvantage over simple in-context generation, sinceit forces the LLM to incorporate components fromthe raw texts. We also involve an extreme situa-tion where the target class does not appear in theraw corpus completely. Remarkably, text graftingshows its robustness to this extreme situation, indi-cating its applicability does not require the target class to appear in the raw corpus. This enablestext grafting to work on a very small corpus whichboosts efficiency.Furthermore, we analyze and discuss the prop-erty of text grafting. We apply principal componentanalysis to visualize that the drafted texts are in-deed near in-distribution. We also find the graftedtexts are near-distribution enough that we do notneed to synthesize negative samples as in tradi-tional data synthesis, which reduces the cost. Wealso conduct a comprehensive hyperparameter anal-ysis of our method. Interestingly, we found thatThe mask ratio is searched to be better set to a highvalue like 0.75 and the mined template number canbe as small as 200. These case studies explore theadvantages of text grafting in distribution approxi-mation and its failure when the raw texts are nearthe distribution of LLM generation.We summarize our contributions as follows,",
  "Related Works": "Extremely Weak-Supervised Text Classification(XWS-TC) needs only minimal human guidance tolabel the text, such as a few rules by human expertsthat match the text to the labels (Wang et al., 2023).Mainstream XWS-TC methods can be divided intotwo categories: Text Mining and Data Synthesis. Text Miningis a fundamentak task (Han andKamber, 2000) for natural language processing.In XWS-TC, the text miner follows high-levelrules from humans to annotate raw texts, whichare used to train the text classifier. A mainstreamrule is whether a seed word appears in the rawtext (Mekala and Shang, 2020; Meng et al., 2020;Wang et al., 2021), categorized as seed methods.Another mining way is to prompt language modelsfor logits that reflect the probability of texts falling",
  "The datasets and models used in the experiments are re-leased in github.com/KomeijiForce/TextGrafting": "in classes (Brown et al., 2020), which can be cali-brated by several techniques (Holtzman et al., 2021;Zhao et al., 2021; Han et al., 2023). The strong per-formance of existing text mining methods is highlydependent on the precision of the class-indicativerules (Dong et al., 2023a), which is hard to main-tain for minority classes. Data Synthesis(He et al., 2022) addresses theprecision degradation in text mining by directlyprompting LLMs with the label names to generatein-class texts (Ye et al., 2022a; Peng and Shang,2024). With the powerful generative ability ofLLMs, the synthesized texts are generally clean(in-class) for training strong classifiers. However,synthesized texts hold LLM-specific patterns, dis-covered by LLM-generated text detectors (Mitchellet al., 2023; Wu et al., 2023). This pattern is hard tobe eliminated even with in-context learning (Koikeet al., 2024). Thus, synthesized texts are generallyout-of-domain and consequently fine-tune a weakerclassifier on the test set. Minority Classeswidely appear in classifica-tion datasets as a result of long-tailed distribu-tion (Zhang et al., 2023; Henning et al., 2023). Forminority classes with supervised annotations, tech-niques like re-sampling (Shen et al., 2016; Pouyan-far et al., 2018; Tepper et al., 2020) and data aug-mentation (Wei and Zou, 2019; Juuti et al., 2020;Tian et al., 2021; Chen et al., 2021). However,these methods are applied to unbalanced annota-tions, which are unavailable under XWS. Counterfactual Augmentationrefers to generat-ing annotated data out of the dataset or raw corpus.Different from regular augmentation, counterfac-tual augmentation changes the reference, e.g., la-bel flipping (Zhou et al., 2022; Peng et al., 2023).Counterfactual augmentation is also applied fortext-to-text tasks like translation (Liu et al., 2021)or summarization (Rajagopal et al., 2022). Coun-terfactual augmentation shares the same require-ment for known reference as regular augmentation.This paper explores a counterfactual augmentationmethod for unannotated raw text under XWS.",
  "f(X) that discerns a text falling in c or not. Wedenote the j-th word in the i-th text of the rawcorpus as x(i,j)": "Text Mininggathers in-class texts with high-level rules g(X) that can precisely assign X totarget class c. Example rules include whether Xcontains words indicating c (seed words) (Donget al., 2023a) or X has top confidence to be in cby prompting LLMs (Brown et al., 2020) amongD. The mined D(TM) = {X(i)|g(X(i))}i=1:|D| iscombined with some randomly sampled negativetexts (due to the scarcity of c) to train f().However, text miners fail in minority classes dueto their low proportion in the raw corpus. By run-ning a state-of-the-art text mining method (Donget al., 2023a) on AG-News (Zhang et al., 2015)with class name proportion modified by sampling,we observe the mining precision drops sharply withthe decrease of proportion, presented in .Another concern is the class might be too minorthat even no ground truth can be mined from theraw corpus, limiting the precision to 0% no matterhow intuitive the mining rule is. Data Synthesisdoes not annotate raw texts forclassifier fine-tuning but directly prompts LLMs togenerate in-class texts (X LLM(Ic)), where Icis an instruction to write a text in class c. With thestrong capability of state-of-the-art LLMs (OpenAI,2024; Meta, 2024), the generated X are highlyconfident to fall in class. Another advantage ofdata synthesis is the ability of LLMs to generatenegative samples (Ye et al., 2022a; Peng and Shang,2024). However, synthesized texts consist of pat-terns different from other sources (Mitchell et al.,2023), which indicates classifiers f() fine-tuned bysynthesized texts are out-of-domain, consequentlyweaker in the classification task.",
  "Overview of Text Grafting": "As depicted in , our text grafting is a hybridmethod that combines the strengths of text miningand data synthesis. The core observation is thatout-of-class texts can contain useful componentsfor writing in-class texts. The text mining stageof text grafting aims to discover these potentialcomponents and formalize them as templates. Inthe data synthesis stage, the templates are filled byLLMs to produce in-class texts. With componentsfrom both raw texts and synthesis, the grafted textsare both in-class and near-distribution, which aresupposed to fine-tune a better classifier than onlytext mining or data synthesis.",
  "Implementation": "In detail, the text mining stage includes PotentialText Mining and Template Creation, while inthe data synthesis stage we conduct Template Fill-ing. The text mining stage requires relatively smallopen-source LLMs with higher efficiency and ac-cessible logits. Template Filling can utilize state-of-the-art LLMs even with API accessibility. Potential Text Miningdiscovers texts with po-tential components to appear in the grafted texts.We evaluate the potential of each word x(i,j) inthe raw text X(i) with regularized logits promptedfrom LLMs following the regularization idea inDC-PMI (Holtzman et al., 2021). The potentialp(i,j) for x(i,j) is defined as the difference be-tween the probability logit of x(i,j) prompted byan instruction with the class name (Ic) and an in-struction for regularization (Ir). The difference canalso be viewed as the probability of x(i,j) raised byincorporating the class name c into the instruction.",
  "p(i,j) = log PLLM(x(i,j)|Ic) log PLLM(x(i,j)|Ir) (1)": "The words with top-K% p among the wordsin text Xi will remain in the template. Thus, theaverage of their p represents the potential (Pi)of the template created based on Xi. As we aremining potential templates rather than directly in-class texts, the mining rate K% can be much largerthan text mining.",
  ": The prompts used in text grafting. In prompts,<label> refers to the label names like Surprised while<style> represents the distribution like Tweet": "Template Creationsimply masks the words withbottom-(100 K)% potential p by blank tokens_ and uses the top-K% as template part. TextXi is thus converted to template Ti, which is pre-pared for LLMs to fill in during the data synthesisstage. As the example in , the componentswith the top potential to be in a grafted Surprisedremain in the template such as believe, whenluck, feel. These components support the datasynthesis to better write an in-class text while keep-ing the style in distribution with the writing struc-ture from the raw corpus. Template Fillingprompts an LLM to fill in theblanks in T, which produces a grafted text thatgenerally falls in the target class c. Referring tothe example in , the LLM well utilizes thewriting structure in the template and fills in theblanks to produce the in-class text. As the templatekeeps the writing structure of the raw corpus, thegrafted text is quite similar to the original one butflipped into the target minority class.Specific prompts in these stages are shown in Ta-ble 2, where the label and distribution informationis filled to support the text grafting.",
  "Evaluation": "DatasetsWe take several minority classes frompopular text classification datasets to evaluate theperformance of different XWS-TC methods on mi-nority classes.We include 1) TweetEval (Bar-bieri et al., 2020) and Emotion (Saravia et al.,2018), which contain minority emotion classesOptimism (8.9%) and Surprised (3.6%); 2) 20News (Lang, 1995), which contains minority newstopic Religion (3.3%) and Politics (4.1%);3) BigPatent (Sharma et al., 2019), which con-tains minority patent class Mechanical Engineer-ing (7.0%). The raw corpus is down-sampled to10, 000 samples to improve experiment efficiencyand save budget costs. We use the F1 score as themetric for evaluation.",
  "Data SynthesisText Mining": ": The overview of text grafting with the minority class Surprised in the Emotion dataset as an example.Text grafting includes two stages: 1) Text (Template) Mining: Create scored templates and select the ones with thetop scores. 2) Data Synthesis: Prompt the LLM to fill in the templates to synthesize in-class texts.",
  "BaselinesWe include various text mining anddata synthesis methods as the baselines for compar-ison to illustrate the advantage of our text grafting.Text mining methods include,": "Prompting Confidence (Brown et al., 2020),which is a prompting method that directly queriesan LLM whether the text falls in the target mi-nority class, and uses the probability logit of an-swering yes for ranking. Considering the classminority, the mining rate is set to 1%. Debiased Seed Word (Dong et al., 2023a),which is the current state-of-the-art XWS-TCmethod. This method uses a seed word (the sameas the label name) to match the target minorityclass and then drops the seed word from the con-text to eliminate spurious correlation. Then thetexts are filtered by text selection (Mekala et al.,2022) to produce the final mined texts.Data synthesis methods include,",
  "All text synthesis methods synthesize 1000 textsas positive (in the target minority class) or negativesamples (out of the target minority class, 2000 intotal)": "The LLM used for text mining is a popular andadvanced open-source LLM, Gemma (Mesnardet al., 2024) (gemma-1.1-7b-it) with accessiblepossibility logits. The LLM used for data synthe-sis is the state-of-the-art LLM, GPT-4o (OpenAI,2024). Grafting HyperparametersThe mining rates ofour text grafter are set to 25% (K%) for potentialcomponents in templates and 10% (N%) for poten-tial templates. Thus, the synthesized data numberis less than 1000, not more than the data numberfrom pure data synthesis. Fine-tuning HyperparametersWe fine-tune aRoBERTa-Large (Liu et al., 2019) as the classifierwith the AdamW (Loshchilov and Hutter, 2019)as the optimizer whose learning rate is initializedto 1 105. The classifier is fine-tuned by 10epochs with batch size 8 and 20% training data aresplit for validation to select the best-performingcheckpoint. All the experiment results are achievedby an average of 5 runs. The two stages in textgrafting apply the same LLM as text mining anddata synthesis.",
  "Main Result": "The main results from our experiments are pre-sented in . The comparison inside textmining methods shows the advantage of the seedmethod over the prompt method, consistent withthe findings of Wang et al.. The comparison amongtext synthesis methods reflects the importance ofknowledge about the distribution of the corpus, asin-context generation outperforms other baselineswith raw texts as an example for synthesis. Finally,text grafting outperforms all the baselines, whichverifies the benefit of text grafting to produce in-class and near-distribution texts.However, there is still a significant gap betweenthe performance of supervised classification andXWS-TC even with text grafting. This indicatesthe grafted texts still have differences with the rawcorpus distribution for further improvement.",
  ": The visualization of text distributions fromdifferent methods": "comparison focuses on the necessity of text miningand data grafting in the pipelines of text grafting.Without Mining removes the template score-basedsorting and lets the LLM fill in randomly selectedtemplates, which significantly underperforms theinitial grafting. Without Synthesis does not createtemplates for data synthesis, but directly uses thep averaged over all words to mine texts for fine-tuning, equal to DC-PMI (Holtzman et al., 2021).The result is similar to the Prompting Confidencemethod, which shows the limitation of text miningfor minority classes. Then we emphasize the ne-cessity of intermediate templates. With RandomMasking randomly masks the mined texts instead Optimism Mechanical SurprisedReligionPoliticsAverage5 F1 Score Data Synthesis (w/ Negative Synthesis, 2000 LLM DS Calls)Data Synthesis (w/o Negative Synthesis, 1000 LLM DS Calls)Text Grafting (w/ Negative Synthesis, 2000 LLM DS Calls)Text Grafting (w/o Negative Synthesis, 1000 LLM DS Calls)",
  ": The analysis on the necessity of negative datasynthesis": "of following the word-level potential p, whichalso results in a performance drop. With MaskFilling In-Context Generation takes the minedtexts as the in-context examples, which result ina similar performance as the one without mining,indicating the importance of template creation andfilling. Based on these ablation results, our graftingframework is shown to be essential for achievingoptimal performance by effectively combining datasynthesis, text mining, and templates.",
  "Further Analysis": "Q1: How does Text Grafting Benefit End-to-End XWS-TC? shows how text graft-ing can be integrated into end-to-end XWS-TCpipelines for different languages. We include theEnglish Emotion dataset with Surprised andLove as the minority classes and the ChineseTNEWS dataset (Xu et al., 2020) with a minor-ity class Stock. For the minority classes, textsare synthesized by grafting while other classesapply the traditional debiased seed word method.The result shows text grafting improves end-to-endXWS-TC on different languages, which verifiesthe cross-lingual benefit of integrating text graftinginto XWS-TC pipelines to handle minority classes. Q2: What if the class proportion is 0%?In theZero-Occur part of , we also include the dis-cussed extreme situation when the raw corpus doesnot contain any text falling in the target minorityclass. A dramatic drop appears in the performanceof text mining as there is no ground truth that anyminer can get. The data synthesis and text graft-ing methods are robust to this change as they donot require the existence of ground truth examples.Thus, text grafting is verified to be applicable toraw corpus without the target minority class. Thus,text grafting can be based on a small subset of thecorpus which might not contain the target minority 0.5000.6250.7500.8751.000",
  "class to boost efficiency": "Q3: How are grafted texts near-distribution?In , we apply semantic text embeddings(Gao et al., 2021) to represent the texts mined orsynthesized by different methods. These embed-dings are then reduced to 2-dimension by principalcomponent analysis (F.R.S., 1901) for visualiza-tion. We use the Optimism class of the TweetE-val benchmark and compare the most competitivemethods (Debiased Seed Word, Incubator, TextGrafting) of different frameworks. We can observethat text mining only discovers a limited proportionof in-class texts. The synthesized texts fall into avery different domain from the raw corpus, whichfine-tunes an out-of-domain classifier with limitedgeneralizability. In contrast, the grafted texts aremuch more near-distribution, contributing to theperformance of the fine-tuned classifier. Q4: Is Negative Data Synthesis Necessary?Fordata synthesis-based methods, the synthesis of neg-ative data is an essential stage in the pipeline, whichdoubles the calls for LLM to synthesize texts. Intext grafting, we efficiently use the raw texts as thenegative examples. Thus, we explore the necessityof negative synthesis by evaluating the performanceof data synthesis (In-Context Generation) and textgrafting with or without negative data synthesiswith the results presented in .Based on the results, we observe negative datasynthesis is very necessary to pure data synthesis asthe performance drops dramatically by removing",
  ": A case study on the strength and possible failure of text grafting": "this stage. In contrast, text grafting without neg-ative data synthesis works even better, indicatingthat our text grafting can work more efficiently byreducing the effort to call LLM at double times.We attribute this efficiency to the near-distributionproperty of the grafted texts, which makes the dis-crimination between them and the original rawtexts no longer degrade to the classifying of textsources (Mitchell et al., 2023). Q5: What mask ratio to choose?In ,we analyze the mask ratio used in text graft-ing.Within the considered set of mask ratios,{0.5, 0.625, 0.75, 0.875, 1.0}, the best-performingratio is 0.75 among different datasets, the same asthe setup in our experiments. We can also observea trend of performance decrease when the maskratio becomes away from 0.75. This indicates atoo-high masking ratio will make the synthesizedtext deviate from the domain of raw corpus (100%leads to in-context generation). On the other hand,a too-low mask ratio will limit the synthesizer togenerate in-class texts, which might cause moresevere performance drops. Q6: How many templates to mine?In ,we further analyze the necessary number of tem-plates to train a strong classifier, which can guidethe efficient application of text grafting. The resultof the surprised class shows about 200 samplescan reach the best performance, which results inabout $0.2 budget for each class (OpenAI, 2024).We also present how the efficiency of text min-ing (Debiased Seed Word) and data synthesis (In-Context Generation) is affected by sample numbers.Text mining cannot fine-tune a well-performingclassifier due to severe noise in minority class min-ing. Data synthesis shows a similar scaling trendas text grafting but generally underperforms textgrafting.",
  "In , we depict workflows of text grafting incomparison with in-context generation to illustratethe strength of grafting and possible failure": "Strengthof text grafting is the ability of state-of-the-art LLMs to fill in hard templates as shownin the first case. While the template is not easy tobe grafted into the target Politics class, the LLMcomes up with the methodology to synthesize sucha text. The text is also more similar in writing styleto the original text than the in-context generation,which depicts the benefit from text grafting. Failureof text grafting can happen when the cor-pus does not have a writing style very far fromthe way that LLMs can imitate. As shown in thesecond case, the LLM can synthesize the animalquestion without the intermediate template on theTREC corpus (Li and Roth, 2002), which reducesthe necessity of text grafting. The XWS-TC of theminority class Animal on this corpus also showsa similar performance between data synthesis (F1Score = 53.88) and text grafting (F1 Score = 53.46),which again emphasizes near-distribution to bean essential motivation to use text grafting.",
  "Conclusion and Future Work": "We introduced text grafting, a technique to gener-ate in-distribution texts for minority classes usingLLMs. By mining high-potential masked templatesfrom the raw corpus and filling them with state-of-the-art LLMs, we achieve significant improvementsin classifier performance on minority classes. Ouranalysis and case studies demonstrate the effective-ness of text grafting in enhancing text synthesis forminority classes. Future work will concentrate onimproving the precision of template mining andthe extension of text grafting to other tasks likeinformation extraction.",
  "Limitation": "Despite the presented strengths in the paper, thereare still several limitations in the text graftingpipeline. As a hybrid method, text grafting requiresa large raw corpus more than data synthesis andLLM calls more than text mining. Other limitationsof text grafting also succeed from text mining anddata synthesis, such as the dependency on LLMability (for mining and synthesis). Thus, the appli-cation scope for text grafting depends on how LLMcomprehends the class name semantics. The per-formance of different classes might also be biasedto the LLM ability in different classes. Francesco Barbieri, Jos Camacho-Collados, Luis Es-pinosa Anke, and Leonardo Neves. 2020. Tweeteval:Unified benchmark and comparative evaluation fortweet classification. In Findings of the Associationfor Computational Linguistics: EMNLP 2020, OnlineEvent, 16-20 November 2020, volume EMNLP 2020of Findings of ACL, pages 16441650. Associationfor Computational Linguistics.",
  "Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind": "Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish,Alec Radford, Ilya Sutskever, and Dario Amodei.2020. Language models are few-shot learners. In Ad-vances in Neural Information Processing Systems 33:Annual Conference on Neural Information Process-ing Systems 2020, NeurIPS 2020, December 6-12,2020, virtual. Junya Chen, Zidi Xiu, Benjamin Goldstein, RicardoHenao, Lawrence Carin, and Chenyang Tao. 2021.Supercharging imbalanced data learning with energy-based contrastive representation transfer.In Ad-vances in Neural Information Processing Systems 34:Annual Conference on Neural Information Process-ing Systems 2021, NeurIPS 2021, December 6-14,2021, virtual, pages 2122921243.",
  "Xuanli He, Islam Nassar, Jamie Kiros, Gholamreza Haf-fari, and Mohammad Norouzi. 2022. Generate, an-notate, and learn: NLP with synthetic text. Trans.Assoc. Comput. Linguistics, 10:826842": "Sophie Henning, William Beluch, Alexander Fraser, andAnnemarie Friedrich. 2023. A survey of methods foraddressing class imbalance in deep-learning basednatural language processing. In Proceedings of the17th Conference of the European Chapter of the As-sociation for Computational Linguistics, EACL 2023,Dubrovnik, Croatia, May 2-6, 2023, pages 523540.Association for Computational Linguistics. Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi,and Luke Zettlemoyer. 2021. Surface form competi-tion: Why the highest probability answer isnt alwaysright. In Proceedings of the 2021 Conference onEmpirical Methods in Natural Language Processing,EMNLP 2021, Virtual Event / Punta Cana, Domini-can Republic, 7-11 November, 2021, pages 70387051. Association for Computational Linguistics. Mika Juuti, Tommi Grndahl, Adrian Flanagan, andN. Asokan. 2020. A little goes a long way: Improv-ing toxic language classification despite data scarcity.In Findings of the Association for Computational Lin-guistics: EMNLP 2020, Online Event, 16-20 Novem-ber 2020, volume EMNLP 2020 of Findings of ACL,pages 29913009. Association for ComputationalLinguistics. Ryuto Koike, Masahiro Kaneko, and Naoaki Okazaki.2024.OUTFOX: llm-generated essay detectionthrough in-context learning with adversarially gen-erated examples. In Thirty-Eighth AAAI Conferenceon Artificial Intelligence, AAAI 2024, Thirty-SixthConference on Innovative Applications of ArtificialIntelligence, IAAI 2024, Fourteenth Symposium onEducational Advances in Artificial Intelligence, EAAI2014, February 20-27, 2024, Vancouver, Canada,pages 2125821266. AAAI Press.",
  "Xin Li and Dan Roth. 2002. Learning question clas-sifiers. In COLING 2002: The 19th InternationalConference on Computational Linguistics": "Qi Liu, Matt J. Kusner, and Phil Blunsom. 2021. Coun-terfactual data augmentation for neural machine trans-lation. In Proceedings of the 2021 Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies, NAACL-HLT 2021, Online, June 6-11, 2021,pages 187197. Association for Computational Lin-guistics. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A robustly optimized BERT pretrainingapproach. CoRR, abs/1907.11692.",
  "New Orleans, LA, USA, May 6-9, 2019. OpenRe-view.net": "Dheeraj Mekala, Chengyu Dong, and Jingbo Shang.2022. LOPS: learning order inspired pseudo-labelselection for weakly supervised text classification.In Findings of the Association for ComputationalLinguistics: EMNLP 2022, Abu Dhabi, United ArabEmirates, December 7-11, 2022, pages 48944908.Association for Computational Linguistics. Dheeraj Mekala and Jingbo Shang. 2020. Contextu-alized weak supervision for text classification. InProceedings of the 58th Annual Meeting of the As-sociation for Computational Linguistics, ACL 2020,Online, July 5-10, 2020, pages 323333. Associationfor Computational Linguistics. Yu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong,Heng Ji, Chao Zhang, and Jiawei Han. 2020. Textclassification using label names only: A languagemodel self-training approach. In Proceedings of the2020 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP 2020, Online, Novem-ber 16-20, 2020, pages 90069017. Association forComputational Linguistics. Thomas Mesnard, Cassidy Hardin, Robert Dadashi,Surya Bhupatiraju, Shreya Pathak, Laurent Sifre,Morgane Rivire, Mihir Sanjay Kale, Juliette Love,Pouya Tafti, Lonard Hussenot, Aakanksha Chowdh-ery, Adam Roberts, Aditya Barua, Alex Botev, AlexCastro-Ros, Ambrose Slone, Amlie Hliou, AndreaTacchetti, Anna Bulanova, Antonia Paterson, BethTsai, Bobak Shahriari, Charline Le Lan, Christo-pher A. Choquette-Choo, Clment Crepy, Daniel Cer,Daphne Ippolito, David Reid, Elena Buchatskaya,Eric Ni, Eric Noland, Geng Yan, George Tucker,George-Christian Muraru, Grigory Rozhdestvenskiy,Henryk Michalewski, Ian Tenney, Ivan Grishchenko,Jacob Austin, James Keeling, Jane Labanowski,Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan,Jeremy Chen, Johan Ferret, Justin Chiu, and et al.2024. Gemma: Open models based on gemini re-search and technology. CoRR, abs/2403.08295.",
  "Generating efficient training data via llm-based at-tribute manipulation. CoRR, abs/2307.07099": "Samira Pouyanfar, Yudong Tao, Anup Mohan, HaimanTian, Ahmed S. Kaseb, Kent Gauen, Ryan Dailey,Sarah Aghajanzadeh, Yung-Hsiang Lu, Shu-ChingChen, and Mei-Ling Shyu. 2018. Dynamic samplingin convolutional neural networks for imbalanced dataclassification. In IEEE 1st Conference on MultimediaInformation Processing and Retrieval, MIPR 2018,Miami, FL, USA, April 10-12, 2018, pages 112117.IEEE. Dheeraj Rajagopal, Siamak Shakeri, Ccero Nogueirados Santos, Eduard H. Hovy, and Chung-ChingChang. 2022.Counterfactual data augmentationimproves factuality of abstractive summarization.CoRR, abs/2205.12416. Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang,Junlin Wu, and Yi-Shin Chen. 2018. CARER: Con-textualized affect representations for emotion recog-nition. In Proceedings of the 2018 Conference onEmpirical Methods in Natural Language Processing,pages 36873697, Brussels, Belgium. Associationfor Computational Linguistics. Eva Sharma, Chen Li, and Lu Wang. 2019.BIG-PATENT: A large-scale dataset for abstractive andcoherent summarization. In Proceedings of the 57thConference of the Association for Computational Lin-guistics, ACL 2019, Florence, Italy, July 28- August2, 2019, Volume 1: Long Papers, pages 22042213.Association for Computational Linguistics. Jiaming Shen, Wenda Qiu, Yu Meng, Jingbo Shang,Xiang Ren, and Jiawei Han. 2021. Taxoclass: Hi-erarchical multi-label text classification using onlyclass names. In Proceedings of the 2021 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, NAACL-HLT 2021, Online, June 6-11,2021, pages 42394249. Association for Computa-tional Linguistics.",
  "Li Shen, Zhouchen Lin, and Qingming Huang. 2016": "Relay backpropagation for effective learning of deepconvolutional neural networks. In Computer Vision -ECCV 2016 - 14th European Conference, Amsterdam,The Netherlands, October 11-14, 2016, Proceedings,Part VII, volume 9911 of Lecture Notes in ComputerScience, pages 467482. Springer. Naama Tepper, Esther Goldbraich, Naama Zwerdling,George Kour, Ateret Anaby-Tavor, and Boaz Carmeli.2020. Balancing via generation for multi-class textclassification improvement. In Findings of the As-sociation for Computational Linguistics: EMNLP2020, Online Event, 16-20 November 2020, volumeEMNLP 2020 of Findings of ACL, pages 14401452.Association for Computational Linguistics.",
  "Jiachen Tian, Shizhan Chen, Xiaowang Zhang, ZhiyongFeng, Deyi Xiong, Shaojuan Wu, and Chunliu Dou": "2021. Re-embedding difficult samples via mutual in-formation constrained semantically oversampling forimbalanced text classification. In Proceedings of the2021 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP 2021, Virtual Event/ Punta Cana, Dominican Republic, 7-11 November,2021, pages 31483161. Association for Computa-tional Linguistics. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Aurlien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. 2023a. Llama: Openand efficient foundation language models. CoRR,abs/2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurlien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023b. Llama 2: Open foundation andfine-tuned chat models. CoRR, abs/2307.09288.",
  "Zihan Wang, Dheeraj Mekala, and Jingbo Shang. 2021": "X-class: Text classification with extremely weak su-pervision. In Proceedings of the 2021 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, NAACL-HLT 2021, Online, June 6-11,2021, pages 30433053. Association for Computa-tional Linguistics. Zihan Wang, Tianle Wang, Dheeraj Mekala, and JingboShang. 2023. A benchmark on extremely weakly su-pervised text classification: Reconcile seed matchingand prompting approaches. In Findings of the As-sociation for Computational Linguistics: ACL 2023,Toronto, Canada, July 9-14, 2023, pages 39443962.Association for Computational Linguistics. Jason W. Wei and Kai Zou. 2019. EDA: easy dataaugmentation techniques for boosting performanceon text classification tasks. In Proceedings of the2019 Conference on Empirical Methods in Natu-ral Language Processing and the 9th InternationalJoint Conference on Natural Language Processing,",
  "Junchao Wu, Shu Yang, Runzhe Zhan, Yulin Yuan,Derek F. Wong, and Lidia S. Chao. 2023. A surveyon llm-generated text detection: Necessity, methods,and future directions. CoRR, abs/2310.14724": "Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao,Yudong Li, Yechen Xu, Kai Sun, Dian Yu, CongYu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi,Yiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang,Weijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian,Yiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao,Qipeng Zhao, Cong Yue, Xinrui Zhang, ZhengliangYang, Kyle Richardson, and Zhenzhong Lan. 2020.CLUE: A chinese language understanding evaluationbenchmark. In Proceedings of the 28th InternationalConference on Computational Linguistics, COLING2020, Barcelona, Spain (Online), December 8-13,2020, pages 47624772. International Committee onComputational Linguistics. Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiang-tao Feng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.2022a. Zerogen: Efficient zero-shot learning viadataset generation. In Proceedings of the 2022 Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP 2022, Abu Dhabi, United ArabEmirates, December 7-11, 2022, pages 1165311669.Association for Computational Linguistics. Jiacheng Ye, Jiahui Gao, Zhiyong Wu, Jiangtao Feng,Tao Yu, and Lingpeng Kong. 2022b. Progen: Pro-gressive zero-shot dataset generation via in-contextfeedback. In Findings of the Association for Com-putational Linguistics: EMNLP 2022, Abu Dhabi,United Arab Emirates, December 7-11, 2022, pages36713683. Association for Computational Linguis-tics."
}