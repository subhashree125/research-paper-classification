{
  "Abstract": "Cross-domain Named Entity Recognition (CD-NER) is crucial for Knowledge Graph (KG)construction and natural language processing(NLP), enabling learning from source to targetdomains with limited data. Previous studiesoften rely on manually collected entity-relevantsentences from the web or attempt to bridge thegap between tokens and entity labels across do-mains. These approaches are time-consumingand inefficient, as these data are often weaklycorrelated with the target task and require exten-sive pre-training. To address these issues, wepropose automatically generating task-orientedknowledge (GTOK) using large language mod-els (LLMs), focusing on the reasoning processof entity extraction. Then, we employ task-oriented pre-training (TOPT) to facilitate do-main adaptation. Additionally, current cross-domain NER methods often lack explicit ex-planations for their effectiveness. Therefore,we introduce the concept of information den-sity to better evaluate the models effectivenessbefore performing entity recognition. We con-duct systematic experiments and analyses todemonstrate the effectiveness of our proposedapproach and the validity of using informationdensity for model evaluation",
  "Introduction": "Cross-domain Named Entity Recognition (CD-NER) involves identifying and classifying namedentities (e.g., people, organizations, locations) intext from different domains. Traditional NER sys-tems (Ju et al., 2021; Chen et al., 2023a), typi-cally trained on domain-specific data, often per-form poorly on text from other domains (Jin et al.,2023; Chen et al., 2024b). While, CDNER ad-",
  "Target Text: To allow for multiple entities ,a separate Hinge loss is computed foreach capsule.Entity and Type: (Hinge loss, metrics)": "The hinge loss is used for \"maximum-margin\" classification, most notably for support vector machines (SVMs).The term max(0, 1 - y , f(x)) is the hinge loss used by support vector machines; the quadratically smoothed hinge loss is a generalization of mathL. The Hinge loss is a measure of the difference between the predicted output of a capsule and the actual output. By computing a separate Hinge loss for each capsule, the model can learn to distinguish between different entities and improve its accuracy.",
  "[Hinge loss] in GTOK Corpus (Ours)": ": DAPT Corpus based on retrieval denotes themanual collected knowledge related to target domainentity from web (Liu et al., 2021). While, our GTOKCorpus based on generation is automatically generatedfrom a fundamental large language model (LLM), whichis strongly related to the target domain entity and therecognition process. dresses this by developing approaches and modelsthat generalize across domains.Previous CDNER studies mainly adopt twoparadigms: 1) Capturing domain differences (Jiaet al., 2019; Liu et al., 2020b; Jia and Zhang, 2020),such as linking tokens to domain-specific entitytypes to enhance generalization (Hu et al., 2022b).2) Relying on external knowledge (Zheng et al.,2022; Chen et al., 2023b), like manually collectingentity descriptions from a few labeled samples inthe target domain and using continuous pre-trainingon this knowledge to facilitate entity recognition(DAPT Corpus (Liu et al., 2021)).Despite their success, these methods have limita-tions: 1) Manual Collection: Collecting large-scale external knowledge is time-consuming and labor-intensive. Automating this process could save con-siderable time. 2) Relevance: Much of the collectedentity knowledge is only relevant to the entity butnot closely related to the CDNER task. For exam-ple, shows that sentences about \"HingeLoss\" in the DAPT Corpus are mere definitions,irrelevant to the NER task, which requires iden-tifying all possible entity spans and types in thetext. The automatically extracted logical reasoningprocesses of NER, as shown in the GTOK Corpus,could more effectively help models generalize. 3)Validation Strategies: Current works mostly usepost-analysis methods like NER performance com-parison implicitly to validate their approaches. Em-ploying quantitative pre-analysis methods, such asestimating the impact of external knowledge explic-itly before the NER task, would mark significantprogress. To tackle these issues, we propose a novel gen-erative framework with NER task-oriented pre-training on generated knowledge, namely TOPT.Our framework comprises generating task-orientedknowledge, task-oriented pre-training with maskedspan modeling, fine-tuning the NER model, and in-ferring on the target domain. Inspired by the strongemergence and reasoning capabilities of large lan-guage models (LLMs, 7B level), we first use anLLM to generate a small-scale task-oriented knowl-edge corpus (GTOK Corpus), illustrating the entityrecognition reasoning flow, as in . Next, weemploy masked span language modeling (MSLM)to pre-train the NER model on the GTOK Cor-pus, guiding the model to understand the entityrecognition task. We then fine-tune the model withlabeled samples from both source and target do-mains. Finally, the fine-tuned model infers entityspans and labels in the target test set. Note thatinformation density is introduced to evaluate themodel potential ability with external knowledge toperform CDNER. In summary, our contributionsare: We utilize LLMs to automatically generatetask-oriented knowledge corpora, facilitating theNER models understanding of entity recognitionlogic. This is the first automated generative frame-work of NER task-oriented knowledge using LLMs,requiring minimal data, easy collection, and fastpre-training compared to traditional DAPT-basedstudies.",
  "Related Work": "Cross-domain NER (CDNER). Previous CDNERworks rely on auxiliary tasks (Liu et al., 2020a;Dou et al., 2023; Fang et al., 2023) or proposenovel model architectures for multi-task and few-shot learning (Wang et al., 2020; Hu et al., 2022b;Hou et al., 2020). However, these methods oftenrequire extensive manual acquisition of externalcorpora, specific settings for entity categories, andlarge labeled datasets, leading to inefficient trans-fer ability (Kim et al., 2015; Liu et al., 2020a; Leeet al., 2018). Our approach differs by using largelanguage models (LLMs) to auto-generate task-oriented knowledge, rather than entity-specific in-formation, saving time and resources. We also re-formulate CDNER as a text-to-text generation prob-lem with instructive learning, enabling the modelto learn entity identification and label classificationmore effectively.Large Language Models (LLMs). LLMs haveshown potential across various NLP tasks (Ope-nAI and et al., 2024). Direct fine-tuning of LLMs,even with parameter-efficient methods (Houlsbyet al., 2019; Li and Liang, 2021; Hu et al., 2022a),is costly and time-consuming (Yang et al., 2024).However, LLMs can be applied to downstreamtasks without fine-tuning, such as generating high-quality corpora for text classification (Li et al.,2023) and expanding multilingual datasets for com-monsense reasoning (Whitehouse et al., 2023). Un-like above studies, we use LLMs to generate task-oriented knowledge, focusing on logical reasoningpaths for CDNER in the target domain. Moreover,we utilize these corpora to pre-train the NER model,which is then fine-tuned with labeled data fromsource and target domains to bridge the domaingap.Uniform Information Density (UID). UIDtheory explains efficient human communica-tion. Jaeger and Levy (2006) and Zhan and Levy(2019) discuss UID in human speech, while Collins(2014) shows UID can predict natural syntactic al-",
  "Methodology": "In this section, we first present the detailed modulesof our TOPT: task-oriented knowledge generation,masked span modeling for pre-training, text-to-textgeneration for CDNER. Then, we introduce howto employ the UID to explain why our approachwith generative task-oriented knowledge (GTOK)outperforms SOTA with other manual large-scalecorpus.Problem Definition. Given a n-token sentencex =< x1, , xn > and k-type entity set =<t1, , tk >, the object of NER task is to extractall entities ei E from x and assign one of thetypes in to each entity, where ei = (xstart:end, t)denotes the i-th entity of x and t refers to thetype of the entity. xstart:end refers to a continuesword span < xstart, , xend > in x, where startand end refers to the entity boundary indexes re-spectively. Given dataset D of the source domainand dataset T of the target domain, the object ofthe cross-domain NER task is to acquire target-related knowledge from D to enhance models per-formance on T . To be accordant with real-world ap-plications, D is supposed to contain a single sourceas well as a combined multiple sources.",
  "To further amplify domain-adaptation and enhancethe task relevance of the pre-training strategy, we": "construct a generated task-oriented knowledge cor-pus (GTOK Corpus) by applying large languagemodels (LLMs) since LLMs are trained on mani-fold corpora that are supposed to involve domainsof NER tasks.Moreover, directly fine-tuningLLMs seems consuming too much time and toomany resources, which is not a good idea for down-stream tasks.Specifically, an intuitive instruction as below isconstructed to guide the LLM model to explainwhy the given text span should be recognized asan entity to generate task-oriented corpus. For sen-tence x of domain d and entities ei E of x, theLLM model is instructed:",
  "t=1p(yi|X, y0, y1, . . . , yi1)(1)": "where yi A = {a0, a1, , aN1}, which is afinite alphabet.Consequently, we can obtain several sentencesof an entity extraction flow by reasoning in the rawtextual context < x >, such as the bottom partin . Then, with respect to all entities inraw textual context < x >, we employ the frozenLLM M to get an entity explanation cluster of each< x >. Formally,",
  "Masked Span Language ModelingPre-training": "Masked language modeling(MLM) is a commonapproach for training models in a self-supervisedsetting. Meanwhile, inspired by the better learningability of span masking (Liu et al., 2021), we usespan-level MLM (Masked Span Language Model-ing, MSLM) to amplify domain adaptation basedabove obtained GTOK corpus K. As shown in Fig-ure 2, for a given sentence x =< x1, , xn >,stochastic text span <xi, xi+1, , xj> ismasked by so called sentinel token to distinct fromordinary stochastic token masks [mask]. We abideby the mask setting of BERT(Devlin et al., 2019)and apply Bernoulli distribution to create matrixM of masked vector L:",
  "Text-to-text Generation for CDNER": "To reduce the variance between different domains,we reformulate the NER task as a text-to-text gener-ation problem with the instructor of a target domain.Specifically, the inputs are divided into 3 parts: INSTRUCTION: asks the model to work asan annotator to label the entities. OPTIONS: contains all domain specific entityin . SENTENCE: the input sentence x.To be specific, the model takes the reformulatedinput (I, o, x) and generates the output y that con-tains the entities:",
  "y = LM(I, o, x)(6)": "where denotes the trained parameters of themodel LM. The output sequence y is convertedinto a natural language which is consistent withthe input x and reformulated to the template as(xstart:end, t). gives an example of thegeneral workflow.The model is supposed to be more effective ingenerating a sequence of entities with options con-taining domain-specific entities. Hence there isno need to modify the structure of the model fortransferring to a new domain. Despite transfer-ring from only a single domain, a naive idea toenhance the models performance is transferringfrom multiple domains. Given domains D =<d1, , d > and their corresponding parame-ters =< 1, , >, the combined multiplesource parameter is:",
  "To explain the difference between DAPT andGTOK corpus as well as why GTOK corpus dobetter, we introduce the uniform information den-sity (UID) (Jaeger and Levy, 2006; Meister et al.,2021) hypothesis:": "Hypothesis 3.1 UID predicts that communicativeefficiency is maximized when informationagainquantified as per-unit surprisalis distributed asuniformly as possible throughout a signal.In other words, UID-based features enable ob-servable distinctions in the surprisal patterns oftexts, which helps in understanding why GTOKCorpus facilitates the model performing better thanDAPT Corpus (Venkatraman et al., 2023). Follow-ing this claim, we further assume:Hypothesis 3.2 Communication efficiency can becorrelated with the learning efficiency of the lan-guage model, which means the model could learnbetter on unlabeled corpora with more uniformlydistributed information(quantified by UID).To this end, we first theoretically present therationality. In Shannons information theory, lan-guage can be regarded as a communication sys-tem and each linguistic unit of the language car-ries some information. The amount of informa-tion can be quantified with surprisal (degree ofsurprise) (Tribus, 1961).Suppose a linguisticsignal: u = u1, , un, where ui is the i-th linguistic unit, the surprisal s() is defined as:s(ui) = logP(ui|u<i). That is, the smaller theprobability of occurrence of a linguistic unit, themore information it contains. We can assume thatthe cognitive load of the entire linguistic signal uderives from the sum of each linguistic unit in it:s(u) = s(ui).To simplify the calculations, we leverage Bi-Gram language model for approximate UID:",
  ": The statistics of tokens for each domain inDAPT and GTOK corpus (M: million, K: kilo-)": "where P(ui1, ui) denotes the joint probability ofui1, ui appearing at the same time with ui exactlyafter ui1, and P(ui|ui1) denotes the conditionalprobability of ui appearing behind ui1.Based on the above rationale, we can concludethat if information density of one corpus for pre-training distributes more uniformly than that ofanother corpus, the former corpus involves more ef-fective information for subsequent NER task (Jainet al., 2018; Clark et al., 2023). Then, we em-pirically present the rationality of our hypothesisthrough corresponding results as .4, alsoincluding the calculation of information entropy indifferent corpus for domain adaptation.",
  "Datasets": "The experiments are conducted on two publicdatasets, including CrossNER (Liu et al., 2021)and CoNLL2003 (Tjong Kim Sang and De Meul-der, 2003) following previous studies (Hu et al.,2022b; Chen et al., 2023b):1) CoNLL2003 has been widely used to evaluateNER models and contains four entity categories:PERSON (PER), LOCATION (LOC), ORGANI-ZATION (ORG), and Miscellaneous (MISC). Weutilize the CoNLL2003 dataset as the source do-main for its extensive knowledge. 2) The Cross-NER dataset involves five separate domains of Ar-tificial Intelligence, Literature, Music, Politics, andNatural Science, where each domain contains morevariance entity categories than CoNLL2003. Weabide by the original splits of train, validation, andtest sets. More detailed information and statisticsabout these datasets can be found in Appendix C.Note that we use the previous DAPT and ourGTOK as the external pre-training corpus for CD-NER. The statistics summary can refer to .",
  ": The statistics of generated GTOK corpus. Avg.Sen. denotes the average explanation sentences of araw text. Fail Rate denotes the rate of LLM failing toexplain an entity": "domain are strictly invisible in black boxes). TheLLM is asked to explain why the entity could be la-beled in the given sentence, however not all entitiescan be covered for the limitation of the knowledgethat LLM contains (generated texts with/withoutexplanations are marked as positive/negative textsrespectively). We remove all negative texts by key-word detection (e.g. \"not accurate\") and positivetexts are cleaned by using regular expressions to ex-clude non-task-relevant sentences (e.g. \"Thank youfor ...\"). Ultimately, the remaining explanationsare constructed as the GTOK corpus. We measureseveral statistics of GTOK corpus and the resultsare listed in .The GTOK corpus produced as described aboveis leveraged to further pre-train the model Flan-T5-base (Chung et al., 2024) by MSLM pre-training.The unlabeled corpus is masked by sentinel tokensand fed into the model, where each sentence (con-tains n tokens) will be duplicated to make a 10 nmatrix and the matrix is masked by the mask matrixM defined in .2. After several epochs oftraining, we will end up with the TOPT-model.",
  "in LLMs, which results are obtained by directlyinstructing it (1800B parameters) with the sameprompt in .2) CP-NER (Chen et al.,": "2023b) introduces collaborative domain-prefix tun-ing based T5 as well, which is the SOTA model. 3)LANER (Hu et al., 2022b) proposes a novel au-toregressive framework by label-aware(relevanceof label and token). 4) LightNER (Chen et al.,2022) proposes a tuning structure for low-resourceNER by pluggable prompting. 5) LST (Zhenget al., 2022) reformulates the NER task as the graph-matching problem that the label relevance is rep-resented as graphs. 6) DAPTN (Liu et al., 2021)leverages retrieval-based unlabeled corpus to adaptthe model to the target domain, which is the firsttime to emphasize the importance of focusing onbuilding a knowledge base only in the target do-main. 7) MCCL (Jia and Zhang, 2020) proposes amulti-cell compositional LSTM structure and eachentity type is modeled by a separate cell state.",
  "better: ) and UID variance (lower correspondingto better: ). Through the main experiments, wemainly answer the following questions:": "(1) Is it necessary to design our TOPT? Ta-ble 2 and 4 display the performance comparisonof existing recent and representative studies forCDNER with single source and multi-source, re-spectively. From these tables, we can observe that1) As the SOTA in LLMs family with 1800B pa-rameters, GPT-4 performs very well in many gen-eration and reasoning tasks, however, it exhibits theworst performance in NER. This may be becausethe training objective of GPT-4 focus on generativetasks, which predict the next word based on con-text, rather than optimizing specifically for NERtasks even though it utilized various very large-scale corpora for training. 2) Among all baselines,CP-NER is obviously superior to previous otherapproaches. This is mainly because it employs aprefix-based pre-training method between sourceand target domains, as well as the simple setting toonly detect the start position of an entity span. 3)It is worth noting an interesting phenomenon thatprevious studies have only improved by 1%-2%each time in terms of average results in the single-source scenario, which is very limited. However,our TOPT directly improves by about 5% regard-ing single-source and 8% regarding multi-source,compared to the SOTA CP-NER. The reason maybe two-folds. Firstly, we have discovered exter-nal knowledge related to the task by LLMs ratherthan entity-related only. Secondly, the NER taskhas been transformed into a text-to-text genera-tion problem based on our pre-trained TOPT model,",
  "which is consistent with the previous pre-trainingobjective": "(2) Does the GTOK corpus work? We con-duct an ablation study to evaluate the model pre-trained by DAPT (w/ DAPT) or without GTOK(w/o GTOK) corpus. From and 4, wecan find that the model pre-trained by GTOK cor-pus performs better than those not pre-trained onGTOK or pre-trained by DAPT corpus. The resulthighlights the significant role of our GTOK cor-pus in TOPT framework. Besides, according tothe statistics of GTOK and DAPT in , withquantifying corpus scale by word token amounts,DAPT corpus contains almost a thousand timestokens than GTOK corpus (81740K to 65.6K perdomain on average respectively), which representspre-training with DAPT corpus will consume muchmore time and hardware devices. Conversely, ourGTOK corpus is more efficient and economical forpre-training. (3) How does UID explain the reason that ourTOPT outperforms all baselines? We obtain theUID results of DAPT and GTOK corpus by themethod described in .4. showsthe UID distributions of each domain, where they axis denotes the UID value of a sentence andthe x axis denotes the length of a sentence. Asdemonstrated in this figure and the variance of UIDvalues in , our GTOK corpus has a more uni-formly distributed UID than the DAPT corpus, thatis the y-values of these points are relatively close.Hence, the GTOK corpus carries more informationand can train the text-to-text model better, whichis consistent with our Hypothesis 3.2. Note that",
  ": Performance of our model pre-trained byGTOK corpora which are generated by various LLMs": "although the corpus we generate contains rich infor-mation, it needs to be combined with our designedpre-training and generative fine-tuning. They havethe same generative objectives. Therefore, directlyusing previous methods with BERT pre-trainingand sequence labelling cannot fully leverage theadvantages of the above corpus, which is indeedthe case in our preliminary experiments listed inAppendix E.",
  "Analysis and Discussion": "To better verify the effectiveness of our TOPTframework, we conduct further analyses on trans-ferring single source CoNLL2023 to the AI andMusic domains, respectively. This is not lackingin generality since two single-source transfers alsodemonstrate the same rationale as other alterna-tives.Effect of GTOK Generated from DifferentLLMs. We evaluate the impact of different LLMsapplied to generate GTOK corpus.We adoptVicuna-7b (Chiang et al., 2023) as another GTOKcorpus generator to construct v-GTOK and con-tinue model pre-training as well as fine-tuning un-der the same setting of Llama. As shown in , the models pre-trained on GTOK and v-GTOKhave similar performance on domain AI and Music.This indicates that our framework is not sensitiveto different LLMs for CDNER.Effect of GTOK with Mixed Source DomainData. To further verify the importance of GTOK inthe target domain rather than the source, we gener-ate task-oriented knowledge on training sets fromboth the source domain and the target domain. Asdisplayed in , Unmixed represents GTOKonly from the target, and 50 denotes GTOK alsofrom 50 samples of the source besides all target",
  ": The prediction result of a testing case in AIdomain": "samples. The meanings of 100 and 200 are sim-ilar. From this table, we can see that the use oftask-oriented knowledge from the source domainreduces performance. This is mainly because itincreases the importance of the source domain andthus causes the domain adaptation to lose balance.Case Study. From , we can find thatthere is the reasoning path for the recognition ofentity \"ROUGE\" in our GTOK Corpus, which pro-vides a similar context with the testing sample andpresents obvious entity extraction clues (\"metric,measure, and evaluate\") for CDNER. Therefore,our TOPT can predict the exact entity and its type.While, CP-NER only resorts to its unified prefixand task-irrelevant external knowledge, thus identi-fying the wrong entity label as \"algorithm\". Morecases are given in the Appendix E.",
  "Conclusion": "We propose a novel approach for cross-domainNER tasks, namely TOPT. We first apply LLMs toautomatically generate a task-oriented knowledgecorpus and pre-train the model on the generatedcorpus to enhance domain-adaptation and NERtask sensitivity, thus, improving the models per-formance on cross-domain NER. Employing thesecomprehensive experiments, our approach achieves a better performance than previous SOTA cross-domain NER approaches. Besides, we reformulatethe NER task as \"text-to-text\" generation, whichavoids unique settings for separated domains andmakes real-world applications easier. Moreover,we introduce uniform information density theoryto analyze the effectiveness of our approach andexplain why the generated corpus is better.In the future, we will attempt to mine more task-oriented knowledge for CDNER, and investigatemore domain to verify our approach. Moreover, weplan to apply our task-oriented pre-training strate-gies into other areas to motivate their further devel-opment in NLP.",
  "Limitations": "Although our approach has achieved impressiveresults on cross-domain NER, there is still a lim-itation. The GTOK corpus is the most significantpart of TOPT, while the GTOK corpus is stronglycorrelated to the LLMs knowledge and genera-tive ability. The LLMs are not omnipotent in alldomains (especially specialized domains, e.g. Bio-Medical NER), which means the LLMs might failto generate a corpus for some domains due to a lackof knowledge. Thus, when applying our approachin specialized domains, the LLM may need to bereplaced by LLMs fine-tuned for specific domains. M. Aylett and A. Turk. 2004. The smooth signal re-dundancy hypothesis: a functional explanation forrelationships between redundancy, prosodic promi-nence, and duration in spontaneous speech. LangSpeech, 47(Pt 1):3156.",
  "Shuhao Chen, Yulong Zhang, Weisen Jiang, JiangangLu, and Yu Zhang. 2024a. Vllavo: Mitigating visualgap through llms": "Xiang Chen, Lei Li, Shumin Deng, Chuanqi Tan,Changliang Xu, Fei Huang, Luo Si, Huajun Chen,and Ningyu Zhang. 2022. LightNER: A lightweighttuning paradigm for low-resource NER via plug-gable prompting. In Proceedings of the 29th Inter-national Conference on Computational Linguistics,pages 23742387, Gyeongju, Republic of Korea. In-ternational Committee on Computational Linguistics. Xiang Chen, Lei Li, Shuofei Qiao, Ningyu Zhang,Chuanqi Tan, Yong Jiang, Fei Huang, and HuajunChen. 2023b. One model for all domains: Collab-orative domain-prefix tuning for cross-domain ner.In Proceedings of the Thirty-Second InternationalJoint Conference on Artificial Intelligence, IJCAI-23,pages 50305038. International Joint Conferences onArtificial Intelligence Organization. Main Track. Xiang Chen, Lei Li, Yuqi Zhu, Shumin Deng, ChuanqiTan, Fei Huang, Luo Si, Ningyu Zhang, and Hua-jun Chen. 2024b.Sequence labeling as non-autoregressive dual-query set generation. IEEE ACMTrans. Audio Speech Lang. Process., 32:15461558. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E. Gonzalez, IonStoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgptquality. Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, Al-bert Webson, Shixiang Shane Gu, Zhuyun Dai,Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh-ery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,Dasha Valter, Sharan Narang, Gaurav Mishra, AdamsYu, Vincent Zhao, Yanping Huang, Andrew Dai,Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-cob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,and Jason Wei. 2024. Scaling instruction-finetunedlanguage models. Journal of Machine Learning Re-search, 25(70):153. Thomas Hikaru Clark, Clara Meister, Tiago Pimentel,Michael Hahn, Ryan Cotterell, Richard Futrell, andRoger Levy. 2023. A Cross-Linguistic Pressure forUniform Information Density in Word Order. Trans-actions of the Association for Computational Linguis-tics, 11:10481065.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training of": "deep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics. Chenxiao Dou, Xianghui Sun, Yaoshu Wang, YunjieJi, Baochang Ma, and Xiangang Li. 2023. Domain-adapted dependency parsing for cross-domain namedentity recognition.In Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligenceand Thirty-Fifth Conference on Innovative Applica-tions of Artificial Intelligence and Thirteenth Sympo-sium on Educational Advances in Artificial Intelli-gence, AAAI23/IAAI23/EAAI23. AAAI Press. Jinyuan Fang, Xiaobin Wang, Zaiqiao Meng, PengjunXie, Fei Huang, and Yong Jiang. 2023. MANNER:A variational memory-augmented model for cross do-main few-shot named entity recognition. In Proceed-ings of the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 42614276, Toronto, Canada. Association forComputational Linguistics. Dmitriy Genzel and Eugene Charniak. 2002. Entropyrate constancy in text. In Proceedings of the 40th An-nual Meeting of the Association for ComputationalLinguistics, pages 199206, Philadelphia, Pennsylva-nia, USA. Association for Computational Linguistics. Yutai Hou, Wanxiang Che, Yongkui Lai, Zhihan Zhou,Yijia Liu, Han Liu, and Ting Liu. 2020. Few-shotslot tagging with collapsed dependency transfer andlabel-enhanced task-adaptive projection network. InProceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 13811393, Online. Association for Computational Linguis-tics. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,Bruna Morrone, Quentin De Laroussilhe, AndreaGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.Parameter-efficient transfer learning for NLP.InProceedings of the 36th International Conferenceon Machine Learning, volume 97 of Proceedingsof Machine Learning Research, pages 27902799.PMLR. Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and WeizhuChen. 2022a. LoRA: Low-rank adaptation of largelanguage models. In International Conference onLearning Representations. Jinpeng Hu, He Zhao, Dan Guo, Xiang Wan, and Tsung-Hui Chang. 2022b. A label-aware autoregressiveframework for cross-domain NER. In Findings of theAssociation for Computational Linguistics: NAACL2022, pages 22222232, Seattle, United States. Asso-ciation for Computational Linguistics.",
  "Advances in Neural Information Processing Systems,volume 19. MIT Press": "Ayush Jain, Vishal Singh, Sidharth Ranjan, Rajakrish-nan Rajkumar, and Sumeet Agarwal. 2018. UniformInformation Density effects on syntactic choice inHindi. In Proceedings of the Workshop on Linguis-tic Complexity and Natural Language Processing,pages 3848, Santa Fe, New-Mexico. Association forComputational Linguistics. Chen Jia, Liang Xiao, and Yue Zhang. 2019. Cross-domain NER using cross-domain language modeling.In Proceedings of the 57th Conference of the Associ-ation for Computational Linguistics, ACL 2019, Flo-rence, Italy, July 28- August 2, 2019, Volume 1: LongPapers, pages 24642474. Association for Computa-tional Linguistics. Chen Jia and Yue Zhang. 2020. Multi-cell composi-tional LSTM for NER domain adaptation. In Pro-ceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 59065917, Online. Association for Computational Lin-guistics. Zhuoran Jin, Pengfei Cao, Zhitao He, Yubo Chen, KangLiu, and Jun Zhao. 2023. Alignment precedes fu-sion: Open-vocabulary named entity recognition ascontext-type semantic matching. In Findings of theAssociation for Computational Linguistics: EMNLP2023, Singapore, December 6-10, 2023, pages 1461614637. Association for Computational Linguistics. Xincheng Ju, Dong Zhang, Rong Xiao, Junhui Li,Shoushan Li, Min Zhang, and Guodong Zhou. 2021.Joint multi-modal aspect-sentiment analysis with aux-iliary cross-modal relation detection. In Proceedingsof EMNLP 2021, pages 43954405. Association forComputational Linguistics. Young-Bum Kim, Karl Stratos, Ruhi Sarikaya, and Min-woo Jeong. 2015. New transfer learning techniquesfor disparate label sets. In Proceedings of the 53rdAnnual Meeting of the Association for ComputationalLinguistics and the 7th International Joint Confer-ence on Natural Language Processing (Volume 1:Long Papers), pages 473482, Beijing, China. Asso-ciation for Computational Linguistics. Ji Young Lee, Franck Dernoncourt, and Peter Szolovits.2018. Transfer learning for named-entity recognitionwith neural networks. In Proceedings of the EleventhInternational Conference on Language Resourcesand Evaluation (LREC 2018), Miyazaki, Japan. Eu-ropean Language Resources Association (ELRA). Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:Optimizing continuous prompts for generation. InProceedings of the 59th Annual Meeting of the Asso-ciation for Computational Linguistics and the 11thInternational Joint Conference on Natural LanguageProcessing (Volume 1: Long Papers), pages 45824597, Online. Association for Computational Lin-guistics. Zhuoyan Li, Hangxiao Zhu, Zhuoran Lu, and MingYin. 2023. Synthetic data generation with large lan-guage models for text classification: Potential andlimitations. In Proceedings of the 2023 Conferenceon Empirical Methods in Natural Language Process-ing, pages 1044310461, Singapore. Association forComputational Linguistics. Zihan Liu, Genta Indra Winata, and Pascale Fung.2020a. Zero-resource cross-domain named entityrecognition. In Proceedings of the 5th Workshop onRepresentation Learning for NLP, pages 16, Online.Association for Computational Linguistics. Zihan Liu, Genta Indra Winata, Peng Xu, and PascaleFung. 2020b.Coach: A coarse-to-fine approachfor cross-domain slot filling. In Proceedings of the58th Annual Meeting of the Association for Compu-tational Linguistics, ACL 2020, Online, July 5-10,2020, pages 1925. Association for ComputationalLinguistics. Zihan Liu, Yan Xu, Tiezheng Yu, Wenliang Dai, ZiweiJi, Samuel Cahyawijaya, Andrea Madotto, and Pas-cale Fung. 2021. Crossner: Evaluating cross-domainnamed entity recognition. Proceedings of the AAAIConference on Artificial Intelligence, 35(15):1345213460. Clara Meister, Ryan Cotterell, and Tim Vieira. 2020. Ifbeam search is the answer, what was the question?In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 21732185, Online. Association for Computa-tional Linguistics. Clara Meister, Tiago Pimentel, Patrick Haller, LenaJger, Ryan Cotterell, and Roger Levy. 2021. Revisit-ing the Uniform Information Density hypothesis. InProceedings of the 2021 Conference on EmpiricalMethods in Natural Language Processing, pages 963980, Online and Punta Cana, Dominican Republic.Association for Computational Linguistics.",
  "Saranya Venkatraman, Adaku Uchendu, and Dong-won Lee. 2023. Gpt-who: An information density-based machine-generated text detector.CoRR,abs/2310.06202": "Jing Wang, Mayank Kulkarni, and Daniel Preotiuc-Pietro. 2020. Multi-domain named entity recognitionwith genre-aware and agnostic inference. In Proceed-ings of the 58th Annual Meeting of the Associationfor Computational Linguistics, pages 84768488, On-line. Association for Computational Linguistics. Chenxi Whitehouse, Monojit Choudhury, and AlhamAji. 2023. LLM-powered data augmentation for en-hanced cross-lingual performance. In Proceedings ofthe 2023 Conference on Empirical Methods in Natu-ral Language Processing, pages 671686, Singapore.Association for Computational Linguistics. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale,Rami Al-Rfou, Aditya Siddhant, Aditya Barua, andColin Raffel. 2021. mT5: A massively multilingualpre-trained text-to-text transformer. In Proceedingsof the 2021 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 483498, On-line. Association for Computational Linguistics. Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiao-tian Han, Qizhang Feng, Haoming Jiang, ShaochenZhong, Bing Yin, and Xia Hu. 2024. Harnessing thepower of llms in practice: A survey on chatgpt andbeyond. ACM Trans. Knowl. Discov. Data, 18(6).",
  "texts, which help in understanding why GTOK Cor-pus facilitates the model performing better thanDAPT Corpus (Venkatraman et al., 2023). Followthis claim, we further assumes:": "Hypothesis B.2 Communication efficiency can becorrelated with the learning efficiency of languagemodel, which means the model could learn betteron unlabeled corpora that have more uniformlydistributed information(quantified by UID). To this end, we first theoretically present therationality. In Shannon information theory, lan-guage can be regarded as a communication sys-tem and each linguistic unit of the language car-ries several information. The amount of informa-tion can be quantified with surprisal (degree ofsurprise, (Tribus, 1961)). Suppose a linguistic sig-nal:u =< u1, , un >",
  "i=1P(ui1, ui)logP(ui|ui1)": "where P(ui1, ui) denotes the joint probability ofui1, ui appearing at the same time with ui exactlyafter ui1, and P(ui|ui1) denotes the conditionalprobability of ui appearing behind ui1.Based on the above rationale, we can concludethat if information density of one corpus for pre-training distributes more uniformly than that ofanother corpus, the former corpus involves more ef-fective information for subsequent NER task (Jainet al., 2018; Clark et al., 2023). Then, we em-pirically present the rationality of our hypothesisthrough corresponding results as .4, alsoincluding the calculation of information entropy indifferent corpus for domain adaptation.",
  ": Statistics of CoNLL2003 and CrossNER": "Literature:award, book, country, event,literary-genre, location, magazine, misc, organi-sation, person, poem, writer.Music: album, award, band, country, event, lo-cation, misc, musical-artist, musical-instrument,music-genre, organisation, person, song.Politics: country, election, event, location, misc,organisation, person, political-party, politician.Science: academic-journal, astronomical-object,award, chemical-compound, chemical-element,country, discipline, enzyme, event, location, misc,organisation, person, protein, scientist, theory, uni-versity.For previous external manual collected knowl-edge for CDNER, the domain-adaptive pre-trainingcorpus (DAPT corpus) (Liu et al., 2021) is consid-ered as the most representative and achieve SOTA.It was collected and gathered from Wikipediawhile it only has weak task correlation. Specifi-cally, as shown in , although sentences ofDAPT corpus contain domain-related entities, largeamount of them practically have no correlation tothe NER task.",
  "DBaselines and Settings": "We conduct the following baselines for a thoroughcomparison: GTP-4: The results of GPT-4 are obtainedby directly instructing the GPT-4 model (1800Bparameters) of OpenAI with the same prompt in. CP-NER (Chen et al., 2023b): This methodintroduces collaborative domain-prefix tuning tobetter transfer knowledge in cross-domain NERtasks, based on T5 as well. It is the SOTA model. LANER (Hu et al., 2022b): This approach pro-poses a novel autoregressive framework by label-aware(relevance of label and token) to better trans-fer label information. LightNER (Chen et al., 2022): This method",
  "ESupplement Details": "Additional details of preliminary results, UID plotsand case studies are listed below.Preliminary Results. The preliminary results(micro F1 score) with our pre-training and tuningparadigm by BERT-based backbone and sequencelabelling on two single-domain generalization arelisted in . Due to the poor performance ofsequence labelling on BERT, we employ text-to-text generation based on T5.UID plots. The UID results listed below areobtained by the method described in .4. (a) shows the UID distributions of GTOKcorpus generated by Llama and Vicuna, and (b) shows the UID distributions of mixed corpus. shows the distribution of information en-tropy for the corpus in the above two experiments,respectively.Case studies. shows the additional pre-dicting results of testing cases in AI, Literature,and Music. In domain AI, there is a clear reason-ing path for entity \"Prolog\" in our GTOK corpus,which provides a similar context with (\"program-ming language\"). Similarly, in domain Music, the",
  "FOther Results": "To compare our approach with LLMs, we directlyfine-tune Llama-2-7B (Touvron et al., 2023) withPEFT method (here we leverage QLoRA (Dettmerset al., 2023)) on single and multiple transfer set-tings. Specifically, QLoRA quantizes the LLM to4 bits and freezes the parameters. The rank param-eter r of Low-Rank Adapter layer is 64 and thescale parameter is 16. The results are listed in. Moreover, our approach is much fasterthan fine-tuning LLM at both train and inferencestrategy. At train strategy, the average time con-sumption per epoch of our approach is 9.35minwhile Llama-2-7B is 59.82min. At inference strat-egy, the average time consumption per sentence ofour approach is 0.71s while Llama-2-7B is 6.54s.",
  "G.1Cross-domain NER": "Cross-domain NER is proposed to transfer knowl-edge from \"rich\" domain to \"poor\" domain to boostthe models performance on target domains thatonly have few labeled corpora in real-world appli-cations (Kim et al., 2015; Liu et al., 2020a; Leeet al., 2018). Previous works have introduced sev-eral approaches to handle cross-domain NER tasksuch as adding auxiliaries (Liu et al., 2020a; Douet al., 2023; Fang et al., 2023) or proposing novelmodel architecture (Wang et al., 2020; Hu et al.,2022b; Hou et al., 2020) for multi-task learning andfew-shot learning. However, these methods requirespecific settings for entity categories as well as avast labeled training set, which makes the transfernot that efficient. Our approach reformulates thecross-domain NER task as a text-to-text generationproblem with domain-specific instruction to betterlearn from the source domains, hence the modelcould learn how to identify an entity and classifythe entity.",
  ": Performance comparison of fine-tuned Llama-2-7B and our approaches": "carry almost all NLP tasks (OpenAI and et al.,2024).Same as PLMs (Xue et al., 2021), theLLMs can be fine-tuned for downstream tasks,while even with parameter-efficient fine-tuningmethod(PEFT, (Houlsby et al., 2019; Li and Liang,2021; Hu et al., 2022a)), fine-tuning a LLM fordownstream tasks is still expensive and time-consuming (Yang et al., 2024). However, we candirectly apply LLMs in downstream tasks with-out fine-tuning them. Li et al. (2023) explores thepossibility of generating high-quality corpora withLLMs instead of collecting manually in text clas-sification tasks. Whitehouse et al. (2023) appliesLLMs to expand existing multilingual common-sense reasoning datasets and the model trainedon the augmented datasets achieves higher preci-sion. Chen et al. (2024a) leverages visual-LLM togenerate descriptions of plots to mitigate gaps be-tween different domains. Inspired by the aboveresearch, we also apply LLMs to generate domain-adaptation corpora to mitigate the gap between",
  "G.3Uniform Information Density": "Information density has been applied to analyze hu-man sentences (Genzel and Charniak, 2002; Aylettand Turk, 2004). Based on the information den-sity, uniform information density (UID) theory isproposed to explain how humans can communicateefficiently. Jaeger and Levy (2006) and Zhan andLevy (2019) introduce the relationship betweenUID and how humans talk while Collins (2014)introduces the UID could predict which syntacticalternations humans sounded more natural. Meis-ter et al. (2020) argues the beam search used indecode-models is related to the UID of model out-puts. Meister et al. (2021) introduces the relation-ship between UID and reading time, which quanti-fies the communication efficiency of the sentence.Based on this research, we adopt the UID theoryfor corpus analysis."
}