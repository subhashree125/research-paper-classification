{
  "Abstract": "Despite impressive performance on languagemodelling and complex reasoning tasks, LargeLanguage Models (LLMs) fall short on thesame tasks in uncommon settings or with dis-tribution shifts, exhibiting a lack of general-isation ability. By contrast, systems such ascausal models, that learn abstract variables andcausal relationships, can demonstrate increasedrobustness against changes in the distribution.One reason for this success is the existenceand use of Independent Causal Mechanisms(ICMs) representing high-level concepts thatonly sparsely interact. In this work, we ap-ply two concepts from causality to learn ICMswithin LLMs. We develop a new LLM archi-tecture composed of multiple sparsely interact-ing language modelling modules. We showthat such causal constraints can improve out-of-distribution performance on abstract and causalreasoning tasks. We also investigate the levelof independence and domain specialisation andshow that LLMs rely on pre-trained partiallydomain-invariant mechanisms resilient to fine-tuning.",
  "Introduction": "The latest generation of Large Language Models(LLMs) with over several billion parameters hasdemonstrated impressive performance on an exten-sive range of in-context language and reasoningtasks (Bubeck et al., 2023; Brown et al., 2020; Weiet al., 2022b,a; Touvron et al., 2023a) and an evengreater range when fine-tuned for a specific task(Touvron et al., 2023b; Hu et al., 2022). However,these observations do not hold for tasks that fall out-side the training data distribution, sometimes evenwhen the task is only slightly perturbed. In partic-ular, standard LLMs perform poorly on complexreasoning tasks, such as abstract, causal, or logicalreasoning (Wu et al., 2023; Gendron et al., 2023a;Zecevic et al., 2023; Jin et al., 2023; Liu et al.,2023; Bao et al., 2023). Gendron et al. (2023a); Jin et al. (2023); Wu et al. (2023) showed that fine-tuning LLMs can increase their in-distribution per-formance, but the improvement does not transfer todifferent distributions, highlighting that LLMs donot generalise as we might expect a person to whenapplied to domains requiring complex reasoning.Several hypotheses have been proposed to explainthis flaw, such as the lack of abstract or symbolicrepresentations within the latent space of LLMs(Wu et al., 2023; Gendron et al., 2023a; Goyal andBengio, 2020). These claims are supported by thebrittleness that LLMs can exhibit; when changingthe wording of a question, the performance of anLLM can vary drastically (Wei et al., 2022b; Jinet al., 2023). This observation hints that LLMsmay rely on domain-specific information or spu-rious correlations in the training data that do notgeneralise to other distributions.Causal models rely on the concept that causalmechanisms invariant under changes in environ-ment exist. The Independent Causal Mechanismsprinciple further states that the causal genera-tive process of a systems variables is composed ofautonomous modules that do not inform or influ-ence each other. \" (Peters et al., 2017; Schlkopfet al., 2021). These principles are applied in di-verse ways in the field of causality, either in thestructure of the model, which may be built in amodular fashion to respect causal relationships, asin Structural Causal Models (Pearl, 2009), or in thedistribution of the data, which may be rendered in-dependent and identically distributed (i.i.d) from anunbalanced distribution by division into subgroups(Austin, 2011; Gendron et al., 2023c). Integrat-ing these methods into the architecture of a LargeLanguage Model could increase its robustness andout-of-distribution (o.o.d) generalisation.We investigate this idea in this work: we aim tobetter understand how LLMs reason in and out-of-distribution and whether they can behave asmodels of Independent Causal Mechanisms un- der certain constraints and with fine-tuning. Tothis end, we propose an LLM architecture inte-grating the concept of mechanisms as independent,self-contained LLM modules. This model is sum-marised in . We aim to answer the follow-ing questions: (i) Can LLMs be used as self-routersfor specialised mechanisms, and does it improvetheir performance? (ii) Can LLMs capture domain-invariant abstractions with information-based reg-ularisation? (iii) How useful is domain-specificknowledge on reasoning tasks? and (iv) Can ourproposed architecture approximate IndependentCausal Mechanisms? Our contributions can besummarised as follows:",
  "Related Work": "LLM Mixtures-of-ExpertsModular architec-tures divide the computations of a network intosub-networks. The Switch Transformer (Feduset al., 2022b) separates the feed-forward layers ofthe transformer model (Vaswani et al., 2017) intomultiple expert modules. This strategy allows train-ing larger models at a lower cost, but the expertmodules are not guaranteed to specialise in specificdomains. Multiple sparse architectures have fol-lowed but mainly focus on optimising the trainingof LLMs for reduced resources and not inducingdomain specialisation (Fedus et al., 2022a). Oneexception is the work of Gururangan et al. (2022),which conditions the activation of an expert mod-ule on the input domain. However, only the feed-forward layers are used as experts and the domainsare assumed to be known during training. Clarket al. (2022) investigate the performance of variousrouting strategies for LLMs and show that the gainfrom using specialised modules is high for small models but decreases as the model size increases.Introduced recently, Mixtral-of-Experts is a mod-ular LLM using the same routing principle as theSwitch Transformer. It outperforms dense LLMs ofsimilar size on reading comprehension, common-sense knowledge and reasoning tasks (Jiang et al.,2024). However, the authors observe that the rout-ing process does not lead to domain-specialisedmodules. The assignment of experts is not basedon domain information. Our work differs in thatit is not directed at optimising LLM training butat inducing functional modularity and studying itseffects on generalisation for reasoning tasks. Modular Neural NetworksOther classes ofmodular neural models are designed to learn spe-cialised sub-networks for specific domains. Recur-rent Independent Mechanisms (Goyal et al., 2021)attempt to learn models of independent mecha-nisms with an LSTM architecture (Hochreiter andSchmidhuber, 1997) to model the dynamics ofphysical objects. Mittal et al. (2022) investigaterouting mechanisms for Mixture-of-Experts mod-els. They find that specialisation can yield betterresults as the number of tasks increases. How-ever, the learned routing strategies do not capturedomain specialisation. In particular, approachesbased on backpropagation to the task loss oftencollapse to a single module. Mittal et al. (2022)sexperiments are restricted to small models and syn-thetic binary classification and regression tasks; westudy a novel routing method via vector quantisa-tion and perform our experiments on architecturesover 1B parameters on reasoning tasks. Causal ModelsCausal models aim to answerqueries requiring knowledge of the causal relation-ships linking the data (Bareinboim et al., 2022).Schlkopf et al. (2021); Goyal and Bengio (2020)argue that for artificial systems to achieve robustand o.o.d reasoning, they must reason in termsof causes and effects and not only correlations,which current LLMs cannot do yet (Bareinboimet al., 2022; Zecevic et al., 2023). Structural CausalModels (SCMs) are graphical models representingcausal relationships as mapping functions from par-ent nodes to their child nodes in a Directed AcyclicGraph (Pearl, 2009). If fully specified, an SCM canrepresent the complete inner workings of a system.However, building an SCM requires access to high-level causal variables, which is not the case in manydeep learning tasks that take low-level observationsas inputs (Schlkopf et al., 2021). The do-calculus,",
  "Decoder": ": Proposed Independent Causal Language Models (ICLM) architecture for language-modelling tasks. Theinput text (on the left, in blue) is fed to multiple pretrained LLM modules (in red). A router uses clustering on inputtext embeddings (in purple) to activate a domain-specific module for this input. The domain-invariant module isalways activated. The latent representations generated by the activated modules are combined using an aggregationscheme (in orange) and converted into a probability distribution for the next word (on the right, in blue). Anadditional loss (in green) minimises the Mutual Information between the domain-invariant and the domain-specificrepresentations. The router ensures that the domain-specific modules only gain in-domain knowledge while theMutual Information loss regularises the domain-invariant module towards learning abstract representations. defined by Pearl (1995, 2009), is used to identifythe causal effect of a variable on another with thehelp of the do operator: do() represents an inter-vention, i.e. the forced attribution of a value to avariable. If P(Y |do(X)) = P(Y ), then X hasno causal effect on Y (they may still be correlatedif they share common ancestors). Another classof causal models relies on determining the flowof information in a system (Shannon, 1948; Paluet al., 2001; Schreiber, 2000). However, these con-cepts have yet to be applied to language models.In the domain of transformers, the Causal Trans-former (Melnychuk et al., 2022) and Causal Atten-tion (Yang et al., 2021) introduce cross-attentionmechanisms to reduce biases from the training dis-tribution.",
  "Causal Information Routing for LLMs": "We now describe our proposed modular archi-tecture: Independent Causal Language Models(ICLM), where each module is an LLM fine-tunedfor a specific specialisation or generalisation objec-tive. We aim to build a system that can adapt tochanging distributions and capture better abstrac-tions. Our architecture is separated into N + 2LLM modules connected by three main compo-nents. The LLM modules are composed of a routerthat generates embeddings of the inputs, a domain-invariant module trained to learn abstractions and N domain-specific modules trained to specialiseon a single task or domain. The other componentsare the routing strategy, Mutual Information lossand the aggregation scheme. The routing strat-egy uses the embeddings from the LLM router toredirect the inputs to a specific module. Specif-ically, routing is performed in an unsupervisedfashion: each embedding is projected into a clus-tered space. The centroid of each cluster is as-sociated with a domain-specific module to whichit assigns a binary activation weight for a giveninput. If the input belongs to a cluster, the cor-responding module is activated. By contrast, thedomain-invariant module processes all inputs. TheMutual Information loss induces abstraction withinthe domain-invariant module; minimising this lossreduces shared information between the domain-specific and domain-invariant modules. I.e. it isintended to cause the domain-invariant module togain domain-invariant knowledge and the domain-specific modules domain-specific knowledge. Fi-nally, the aggregation scheme combines the outputof the activated domain-specific module and thedomain-invariant module to produce the final out-put. shows an overview of our method.We describe the routing strategy in .1, theMutual Information minimisation in .2and the aggregation scheme in .3. discusses how the architecture reflects Indepen-",
  "Routing Strategy": "The routing strategy redirects the input tokens toa domain-specific module. This step divides theinference into independent modules to increase thespecialisation of each module and reduce spuriousdistribution biases. In particular, the distributionmay be imbalanced: a data class may dominatethe training distribution and spuriously drive thegradients in a dense model. The routing moduleis used to balance out the distribution. The inputsbelonging to the dominant class are restricted to asingle module and cannot adversarially affect theother modules. In parallel, data points far from thisdata class are trained on a specialised module.We use a pre-trained LLM (the router) with nofinal language modelling layer to build an inputembedding space. The embeddings serve as inputsto the unsupervised routing strategy. In the strat-egy, all modules receive the inputs, but the outputsof non-activated modules are blocked. I.e. theiroutputs are associated with a weight of zero (acti-vated modules have a weight equal to one). Thisactivation process by weighting allows us to studymore complex (non-binary) weighting schemes,discussed in Appendix C. This unsupervised learn-ing method grants more flexibility than the matrixmultiplication used in sparse transformers, as anyclustering algorithm can be used. In particular, incontinual learning settings, one could imagine us-ing a varying number of clusters (Ester et al., 1996)and dynamically allocating new modules as datais being fed to the router. In our work, we restrictourselves to simple clustering methods as we findthat they are sufficiently fine-grained for our tasks.We perform clustering at the input level, i.e. eachpoint in the clustering space represents a completeinput context. Vector QuantisationWe use the vector quan-tisation procedure introduced for the VQ-VAE(van den Oord et al., 2017) as a clustering method.N vectors hc are arbitrarily initialised in the em-bedding space, acting as cluster centroids. Theattribution of an input to a cluster is determined bymeasuring the shortest Euclidean distance betweenthem. The router generates an embedding for eachtoken in the input so we measure the distance be-tween a centroid and each token and sum them toobtain the total distance. The location of the cen-troids is iteratively updated to move closer to the",
  "input embeddings using vector quantisation. Thecorresponding routing loss is defined as follows:": "LR = MSE(sg(hc), hr) + MSE(hc, sg(hr))(1)with hr one token embedding and hc the co-ordinates of the selected centroid, sg is thestop_gradients operation, and is a hyperparame-ter. This method has been very successful in trans-posing high-level concepts from a continuous toa discrete space (Bao et al., 2022; Ramesh et al.,2021) and in building disentangled or interpretablesemantic spaces (Gendron et al., 2023b; Yu et al.,2023). This approach is simple and assumes clus-ters with non-overlapping convex hulls. We con-sider more complex strategies in Appendix C.",
  "Mutual Information Minimisation": "The second aim of the architecture is to induce ab-straction and domain-invariance in LLMs. To thisend, we introduce a regularisation process basedon information theory. We minimise the MutualInformation (I) (Shannon, 1948; Kreer, 1957) be-tween the domain-specific and domain-invariantmodules. Specifically, we minimise the informa-tion between the last hidden states of the modules.The idea is to drive the domain-specific modules togain knowledge specific to their distribution only,while the domain-invariant module gains knowl-edge common to all distributions and discards thedomain-specific information that could be detri-mental to generalisation. The Mutual Informationbetween two random processes corresponds to thedependence between the two processes, i.e. theamount of information gained on the first processby observing the second one. The Mutual Informa-tion between two random variables HI H andHS H is given by:",
  "I(HI, HS) = KL(PHI,HS||PHI PHS)(2)": "where KL is the Kullback-Leibler divergence (Kull-back and Leibler, 1951), HI is the random variablerepresenting the last hidden state of the domain-invariant module and HS is its counterpart in onedomain-specific module. The hidden states are in-terpreted as logits distributed in a feature spaceH. PHI,HS is their joint distribution, and PHI andPHS are their marginals. They are later decodedusing a final linear layer into the space of possiblenext words corresponding to the vocabulary of the",
  "n[1,N]I(HI, HSn)(3)": "The probabilities PHI(h) and PHS(h) cannotbe directly computed for any given hidden stateh H.We can only access the probabilitiesPHI(h|c) and PHS(h|c) for a given input con-text c C.The marginalisation on C is in-tractable because of the exponential input space:|C| = V L, with V the vocabulary size of theLLM and L the maximum length of the input se-quence (typically V L = (32.103)4096). We canapproximate it by sampling C at the batch level B:P(h) = cC P(h|c) P(c) 1|B|cB P(h|c)with |B| |C|. We do the same with the jointdistribution PHI,HS.",
  "Aggregation of Outputs": "Before aggregating the domain-invariant anddomain-specific modules, we perform a sharedbatch normalisation (Ioffe and Szegedy, 2015) be-tween their last hidden states. For a batch of size|B|, one domain-specific active module and onedomain-invariant module, batch normalisation isoperated on 2 |B| samples. Batch normalisa-tion ensures that the module outputs have the samemean and variance. We then use a standard lan-guage modelling head that converts the hiddenstates into a probability distribution for the nexttoken. The language modelling head is a fully con-nected layer that takes the concatenated hiddenstates as inputs and outputs a probability distribu-tion in the vocabulary of the language model. Thisis a simple aggregation method with great expres-sivity due to the shared final dense layer. However,this layer can be subject to biases, e.g. if prioritis-ing information from one module at the expenseof the others. We study other aggregation schemes,less expressive but more resilient to this issue, inAppendix D.",
  "+ 1": ": Simplified temporal causal graph G duringtraining before adding Mutual Information minimisa-tion.C is the input context.HR, HI, HSn, HSare the latent states of the router, domain-invariant,domain-specific and activated domain-specific (afterrouter weighting) modules. For simplicity, we onlyshow the state HSn of the activated domain-specificmodule n. Y and Ytrue are the output and true distribu-tions. WR, WSn and WI are the trainable parametersof the modules. LY = Lo + Linv + Ldom andLR are the output and router losses. Black edges showthe forward pass at step . Blue dashed edges show thebackward pass at step . Red dotted edges illustrate thecausal links between the forward and backward passes. Linv and Ldom are cross-entropy losses betweenthe output logits of the invariant module and thoseof the activated domain-specific module. LR is thevector quantisation loss obtained from the routingstrategy (Eq. 1). LI is the Mutual Information loss(Eq. 3). We consider three separate self-supervisedlosses Lo, Linv and Ldom to induce the modulesto match the target distributions individually andprevent collapse to a single useful module. , , and are constant hyperparameters.",
  "Theoretical Perspective": "In this section, we provide theoretical evidence onhow our model approximates Independent CausalMechanisms and under what assumptions. Inde-pendent Causal Mechanisms consist of autonomousmodules that work independently. In our case, alldomain-specific modules are trained for specifictasks/distributions. The domain-invariant moduleis trained only to use domain-invariant knowledge.The router module is tasked to split the input dis-tribution into N more balanced distributions. Weaim to verify that the modules are not causally re-lated. More formally, we aim to study under whatconditions the following holds:",
  "P(HI|do(HSn)) = P(HI) n [1, N](8)": "HR, HI and HSn n [1, N] are the respectiverepresentations generated by the router, domain-invariant and N domain-specific modules.Equations 5 and 6 are verified. The proof is pro-vided in Appendix B; the main idea is that the useof a separate loss function for training the routerprevents the other modules from causally actingon the router, either in the forward or backwardpasses. It can be verified in . However, ifan invariant module is part of the model, Equations7 and 8 do not hold. The domain-specific modulesdo not directly influence each other because therouting mechanism allows a single module to gothrough the forward and backward passes. Nev-ertheless, a causal path can be drawn through thedomain-invariant module as it is always activated.For example, assuming a model with two domain-specific modules, S0 and S1, activated one afterthe other, a path exists and can be represented ina simplified version as HS1... LY... HI...LY+1... HS2. Again, details of the proof aregiven in Appendix B. As HI and HSn are causallyrelated, we need to reduce the dependency betweenthe two quantities using a regularisation term. Min-imising the Mutual Information between HI andHSn amounts to reducing the mutual dependencebetween the variables. I(HI, HSn) = 0 if and onlyif HI and HSn are independent. If verified, theloss LY can be divided into two independent com-ponents and Equations 7 and 8 hold. We verifyexperimentally in Appendix E.1 that the MutualInformation is close to zero after 50 trainingsteps.",
  "Experimental Setup": "By default, we use N = 2 domain-specific mod-ules and one domain-invariant module, as thedatasets we use contain two subdomains each. Wealso perform experiments with an ablated modelthat does not have a domain-invariant module. Inaddition, we study the individual performance ofthe domain-invariant and domain-specific modules. We use a pretrained LLaMA2-7B (Touvron et al.,2023b) for all our modules. We use Low-Rank Ap-proximation of LLMs (LoRA) (Hu et al., 2022) tofine-tune the modules on their respective tasks. Allmodels are fine-tuned for 3 epochs with AdamW(Loshchilov and Hutter, 2019) and a batch size of16. Loss hyperparameters are = 0.1, = 0.1, = 0.1, = 0.01, = 0.25. It is worth not-ing that the number of parameters used is onlymarginally higher than that of the base LLaMA2,as only low-memory LoRA adapter weights arelearned during training.",
  "Datasets": "We perform experiments on the text-based ACREand RAVEN datasets (Zhang et al., 2021a, 2019;Gendron et al., 2023a)1. ACRE and RAVEN areadapted from Visual Question Answering datasetsto be used by language models. The visual ACRE(Zhang et al., 2021a) is an abstract causal reasoningdataset where the model must deduce the causalmechanisms from a small set of image examples.The visual RAVEN (Zhang et al., 2019) is an ab-stract reasoning dataset where the model must com-plete a sequence of Raven Progressive Matrices(Raven, 1938). The text ACRE and RAVEN con-tain symbolic and natural language descriptions ofthe images and instructions for solving the task.They require knowledge of the underlying causalmechanisms to be solved and have o.o.d sets to chal-lenge this ability in the tested systems. More detailsabout the datasets are provided in Appendix A. Out-of-Distribution RegimesEach dataset hastwo o.o.d regimes. In ACRE, the compositionalitysplit changes the composition of the context exam-ples: combinations of figure shapes and coloursunseen in the training set are proposed; the sys-tematicity split alters the distribution of the contextexample activations: the context contains more pos-itive examples than in the training set. In RAVEN,the four split contains four figures instead of one;the in-center split describes two figures with onecontaining the other instead of being placed next toeach other.",
  "We use the data provided at": "most challenging o.o.d sets. Moreover, the perfor-mance of the individual domain-invariant moduleshighlights that the modules have learned more gen-eralisable knowledge than with standard training.The domain-specific modules compete with thebaselines trained on the corresponding specific do-main, showing that the router accurately distributesthe inputs to the right modules. The modules evenoutperform the oracle router on RAVEN in almostall settings. We investigate a potential reason forthis phenomenon in .2.",
  "Continual Learning": "We investigate the capacity of our model to be usedin continual learning settings. Continual learningconsists of training a model with continuous datastreams or sets evolving over time, where the modelacquires and accumulates knowledge incrementally.The main challenge lies in the catastrophic forget-ting of the previous knowledge when gaining newinformation (Wang et al., 2023). We study a simpleusecase where we want our model to learn one newtask after training on a previous task. We choosethe scenario ACRE RAV EN as RAVEN ismore challenging, particularly the o.o.d sets. Theresults are shown in . The domain-invariantmodule can use general information extracted fromACRE to improve its performance on RAVEN,even outperforming the baseline trained on RAVENonly. The domain-specific modules can also par-tially mitigate the catastrophic forgetting problemobserved in LLaMA2. Their weights are not acti-vated by the router on RAVEN inputs, thus not up-dated, and their performance on ACRE is preserved.However, the aggregation process is affected, lead-ing to reduced performance on ACRE Text.",
  "Evolution of Module Independence": "We study the independence of the module hid-den states during training () and infer-ence (). we focus on two complemen-tary measures: Mutual Information and the Pear-son Correlation Coefficient that measures linearcorrelation between variables. The latter is lim-ited to linear dependence but is more easily inter-pretable. The shared Mutual Information as well asthe Pearson Correlation Coefficient between mod-ules are effectively reduced by the regularisationscheme during fine-tuning. However, the modulehidden states remain correlated, in particular at inference time. Further investigation in AppendixE.5 further shows that this correlation is maintainedacross most layers, which indicates the presenceof a general domain-invariant mechanism sharedby all modules and composing the basis of theirreasoning abilities. Its influence is reduced viathe fine-tuning procedure that develops domain-specific knowledge but it remains the main mecha-nism used, particularly at test time.",
  "ICLMACRE* ()0.6530.9500.6630.9310.6340.901------ICLMRAV EN* ()------1.0000.9800.7030.7030.5150.228": "ACRERAV ENICLM* (ours)0.0890.9010.1190.9310.0500.8711.0000.9900.7720.7720.8330.248ICLM-Invariant* (ours)0.2870.3960.2770.4160.2380.4551.0000.9700.6730.7230.7230.238LLaMA2-Finetuned-Sequential*0.0790.3760.1490.3860.0890.4260.9800.7720.6340.5540.5840.069 : Accuracy when the model is trained sequentially on ACRE then RAVEN i.i.d training sets. ICLM can usethe information from ACRE to improve its performance on RAVEN, outperforming the baseline trained on RAVENonly, while preserving more knowledge from the previous task than the base LLaMA2-7B finetuned sequentially. Inparticular, the accuracy of ACRE-Symbolic sets is almost untouched. Both datasets have a clear division between textand symbolic embeddings. However, the o.o.d setsare not well separated in ACRE while they are inRAVEN. This division can explain the similarityin the results between the ACRE i.i.d and o.o.dsets, as shown in . Moreover, as the distri-butions are very similar, the impact of the routerand the need for abstraction are reduced. On theother hand, there is a clear separation between thei.i.d and o.o.d RAVEN embeddings, explaining thedifferences in behaviours from the models acrossthe sets. Adding more modules could allow takingmore advantage of this separation, with each mod-ule specialising to a subdomain closer to one of theo.o.d embeddings.",
  "Conclusion": "Performing strong out-of-distribution reasoning isa challenging task, and despite their impressiveperformance on a wide range of problems, LLMshave not demonstrated this ability yet. Combiningthis popular model with causal models could helpbridge this gap. This work presents a modular ar-chitecture yielding LLMs to behave as IndependentCausal Mechanisms. We show theoretically that theproposed model generates causally-independentmodules. We perform experiments on abstract and",
  "(d) Vector quantisation router": ": 2D projection of the hidden states of LLaMA2on ACRE and RAVEN i.i.d and o.o.d sets. Groundtruth samples are labelled as in (text/symbolici.i.d/o.o.d sets). Text and symbolic inputs are alwaysclustered separately. i.i.d and o.o.d sets are clusteredtogether in ACRE and separated in RAVEN. The routerfollows the text and symbolic division. causal reasoning tasks in o.o.d and continual learn-ing settings and show that these principles increasestrong reasoning and generalisation. We furthershow that the proposed modules specialise to theirdomain with fine-tuning but still partially rely on ashared domain-invariant mechanism, highlightinga limitation for representing ICMs with LLMs.",
  "Limitations": "The model proposed in this paper is constrainedto work in a modular manner. All modules aresparsely connected at the level of the language mod-elling head. This single connection offers a usefulinitial framework to study the ICMs within thecontext of LLMs and can represent a wide rangeof problems (requiring the composition of severalindependent reasoning processes) but it can onlyrepresent causal DAGs with a single layer depth,potentially hindering the expressivity of more com-plex mechanism interactions. Generating complexcausal computation graphs tailored to the task athand may improve performance, but this problemis out of the scope of this paper.We also focus our investigation on the indepen-dence and accuracy of the modules and do notattempt to directly represent the true causal mech-anisms of the tasks as they are unknown. More-over, we conduct experiments on reasoning tasks toverify if inducing high-level modularity can yieldincreases in performance and generalisation. Weaim not to outperform the state-of-the-art on theproblems but to study whether the proposed mech-anisms can yield such increases.In addition, training and fine-tuning Large Lan-guage Models has a high computational cost. Dueto this high cost, we perform a single fine-tuningrun per task and conduct experiments on this model.The training cost for our model is only slightlyhigher than for a base LLaMA2 because a quan-tized version of the base model can be used. How-ever, the cost increases during inference becauseeach module has to be associated with a fullyloaded LLaMA2.Our current implementationloads all modules in parallel with the aim to studythe interactions between them (see andAppendix D), but this choice is memory intensiveand does not permit us to directly build ICLM witha significantly larger number of modules. To scaleICLM, a production implementation could use asingle base model and load only the required mod-ules sequentially. Such implementation would havethe same theoretical properties at a negligible mem-ory cost.",
  "Peter C. Austin. 2011. An introduction to propensityscore methods for reducing the effects of confound-ing in observational studies. Multivariate BehavioralResearch, 46(3):399424. PMID: 21818162": "Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei.2022. Beit: BERT pre-training of image transform-ers. In The Tenth International Conference on Learn-ing Representations, ICLR 2022, Virtual Event, April25-29, 2022. OpenReview.net. Qiming Bao, Gal Gendron, Alex Yuxuan Peng, WanjunZhong, Neset Tan, Yang Chen, Michael Witbrock,and Jiamou Liu. 2023. A systematic evaluation oflarge language models on out-of-distribution logicalreasoning tasks. CoRR, abs/2310.09430. Elias Bareinboim, Juan D. Correa, Duligur Ibeling, andThomas Icard. 2022. On pearls hierarchy and thefoundations of causal inference. In Hector Geffner,Rina Dechter, and Joseph Y. Halpern, editors, Prob-abilistic and Causal Inference: The Works of JudeaPearl, volume 36 of ACM Books, pages 507556.ACM.",
  "Ingwer Borg and Patrick JF Groenen. 2005. Modernmultidimensional scaling: Theory and applications.Springer Science & Business Media": "Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish,Alec Radford, Ilya Sutskever, and Dario Amodei.2020. Language models are few-shot learners. InProceedings of the 34th International Conference onNeural Information Processing Systems, NIPS20,Red Hook, NY, USA. Curran Associates Inc. Sbastien Bubeck, Varun Chandrasekaran, Ronen El-dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Pe-ter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro,and Yi Zhang. 2023. Sparks of artificial general in-telligence: Early experiments with gpt-4.",
  "Franois Chollet. 2019. On the measure of intelligence.CoRR, abs/1911.01547": "Aidan Clark, Diego de Las Casas, Aurelia Guy, ArthurMensch, Michela Paganini, Jordan Hoffmann, Bog-dan Damoc, Blake A. Hechtman, Trevor Cai, Se-bastian Borgeaud, George van den Driessche, ElizaRutherford, Tom Hennigan, Matthew J. Johnson,Albin Cassirer, Chris Jones, Elena Buchatskaya,David Budden, Laurent Sifre, Simon Osindero, OriolVinyals, MarcAurelio Ranzato, Jack W. Rae, ErichElsen, Koray Kavukcuoglu, and Karen Simonyan.2022. Unified scaling laws for routed language mod-els. In International Conference on Machine Learn-ing, ICML 2022, 17-23 July 2022, Baltimore, Mary-land, USA, volume 162 of Proceedings of MachineLearning Research, pages 40574086. PMLR.",
  "Anirudh Goyal and Yoshua Bengio. 2020. Inductivebiases for deep learning of higher-level cognition.CoRR, abs/2011.15091": "Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Sha-gun Sodhani, Sergey Levine, Yoshua Bengio, andBernhard Schlkopf. 2021. Recurrent independentmechanisms. In 9th International Conference onLearning Representations, ICLR 2021, Virtual Event,Austria, May 3-7, 2021. OpenReview.net. Suchin Gururangan, Mike Lewis, Ari Holtzman,Noah A. Smith, and Luke Zettlemoyer. 2022. Demixlayers: Disentangling domains for modular languagemodeling. In Proceedings of the 2022 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, NAACL 2022, Seattle, WA, United States,July 10-15, 2022, pages 55575576. Association forComputational Linguistics.",
  "Sepp Hochreiter and Jrgen Schmidhuber. 1997. Longshort-term memory. Neural Comput., 9(8):17351780": "Edward J. Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. 2022. Lora: Low-rank adaptation oflarge language models. In The Tenth InternationalConference on Learning Representations, ICLR 2022,Virtual Event, April 25-29, 2022. OpenReview.net. Sergey Ioffe and Christian Szegedy. 2015. Batch nor-malization: Accelerating deep network training byreducing internal covariate shift.In Proceedingsof the 32nd International Conference on MachineLearning, ICML 2015, Lille, France, 6-11 July 2015,",
  "volume 37 of JMLR Workshop and Conference Pro-ceedings, pages 448456. JMLR.org": "Albert Q. Jiang, Alexandre Sablayrolles, AntoineRoux, Arthur Mensch, Blanche Savary, Chris Bam-ford, Devendra Singh Chaplot, Diego de Las Casas,Emma Bou Hanna, Florian Bressand, GiannaLengyel,Guillaume Bour,Guillaume Lample,Llio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,Sophia Yang, Szymon Antoniak, Teven Le Scao,Thophile Gervet, Thibaut Lavril, Thomas Wang,Timothe Lacroix, and William El Sayed. 2024. Mix-tral of experts. CoRR, abs/2401.04088. Zhijing Jin, Jiarui Liu, Zhiheng Lyu, Spencer Poff,Mrinmaya Sachan, Rada Mihalcea, Mona T. Diab,and Bernhard Schlkopf. 2023. Can large languagemodels infer causation from correlation?CoRR,abs/2306.05836.",
  "Stuart Lloyd. 1982. Least squares quantization in pcm.IEEE transactions on information theory, 28(2):129137": "Ilya Loshchilov and Frank Hutter. 2019. Decoupledweight decay regularization. In 7th InternationalConference on Learning Representations, ICLR 2019,New Orleans, LA, USA, May 6-9, 2019. OpenRe-view.net. Valentyn Melnychuk, Dennis Frauen, and Stefan Feuer-riegel. 2022. Causal transformer for estimating coun-terfactual outcomes. In International Conference onMachine Learning, ICML 2022, 17-23 July 2022, Bal-timore, Maryland, USA, volume 162 of Proceedingsof Machine Learning Research, pages 1529315329.PMLR. Sarthak Mittal, Yoshua Bengio, and Guillaume Lajoie.2022. Is a modular architecture enough?In Ad-vances in Neural Information Processing Systems 35:Annual Conference on Neural Information Process-ing Systems 2022, NeurIPS 2022, New Orleans, LA,USA, November 28 - December 9, 2022.",
  "Claude Elwood Shannon. 1948. A mathematical theoryof communication. The Bell system technical journal,27(3):379423": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Aurlien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. 2023a. Llama: Openand efficient foundation language models. CoRR,abs/2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurlien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023b. Llama 2: Open foundation andfine-tuned chat models. CoRR, abs/2307.09288. Aron van den Oord, Oriol Vinyals, and KorayKavukcuoglu. 2017. Neural discrete representationlearning. In Advances in Neural Information Pro-cessing Systems 30: Annual Conference on NeuralInformation Processing Systems 2017, December 4-9,2017, Long Beach, CA, USA, pages 63066315. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N. Gomez, LukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. In Advances in Neural Information Pro-cessing Systems 30: Annual Conference on NeuralInformation Processing Systems 2017, December 4-9,2017, Long Beach, CA, USA, pages 59986008.",
  "Jason Wei, Maarten Paul Bosma, Vincent Zhao, KelvinGuu, Adams Wei Yu, Brian Lester, Nan Du, An-drew Mingbo Dai, and Quoc V. Le. 2022a. Finetunedlanguage models are zero-shot learners": "Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,and Denny Zhou. 2022b. Chain-of-thought prompt-ing elicits reasoning in large language models. InAdvances in Neural Information Processing Systems,volume 35, pages 2482424837. Curran Associates,Inc. Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyrek,Boyuan Chen, Bailin Wang, Najoung Kim, JacobAndreas, and Yoon Kim. 2023. Reasoning or recit-ing? exploring the capabilities and limitations of lan-guage models through counterfactual tasks. CoRR,abs/2307.02477. Xu Yang, Hanwang Zhang, Guojun Qi, and Jianfei Cai.2021. Causal attention for vision-language tasks.In IEEE Conference on Computer Vision and Pat-tern Recognition, CVPR 2021, virtual, June 19-25,2021, pages 98479857. Computer Vision Founda-tion / IEEE. Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar,Wolfgang Macherey, Yanping Huang, David A. Ross,Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, KevinMurphy, Alexander G. Hauptmann, and Lu Jiang.2023.SPAE: semantic pyramid autoencoder formultimodal generation with frozen llms.CoRR,abs/2306.17842.",
  "Matej Zecevic, Moritz Willig, Devendra Singh Dhami,and Kristian Kersting. 2023. Causal parrots: Largelanguage models may talk causality but are not causal.CoRR, abs/2308.13067": "Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, andSong-Chun Zhu. 2019. RAVEN: A dataset for re-lational and analogical visual reasoning. In IEEEConference on Computer Vision and Pattern Recogni-tion, CVPR 2019, Long Beach, CA, USA, June 16-20,2019, pages 53175327. Computer Vision Founda-tion / IEEE. Chi Zhang, Baoxiong Jia, Mark Edmonds, Song-ChunZhu, and Yixin Zhu. 2021a. ACRE: abstract causalreasoning beyond covariation. In IEEE Conferenceon Computer Vision and Pattern Recognition, CVPR2021, virtual, June 19-25, 2021, pages 1064310653.Computer Vision Foundation / IEEE.",
  "A.1ACRE": "ACRE is an abstract causal reasoning dataset where the model must deduce the causal mechanisms from asmall set of image examples. Each sample in the dataset contains six images representing objects anda light, activated or not. The goal of the task is to determine from the images the objects causing theactivation of the light and determine the state of the light in four test cases: activated, deactivated, orundetermined if the activation causes cannot be retrieved. Figures 6, 7 and 8 provide examples from thedatasets.",
  "Example Cases": "A cyan cylinder in rubber is visible. The light is on.A gray cube in rubber is visible. The light is off.A cyan cylinder in rubber is visible. A gray cube in rubber is visible. The light is on.A blue cube in metal is visible. The light is off.A gray cylinder in rubber is visible. A gray cube in metal is visible. The light is off.A red sphere in metal is visible. A yellow cube in rubber is visible. The light is on.",
  "A.2RAVEN": "RAVEN is an abstract reasoning dataset where the model must complete a sequence of Raven ProgressiveMatrices (Raven, 1938). Each sample in the dataset contains eight Raven Progressive Matrices (Raven,1938). The goal of the task is to determine the matrix that completes the sequence from a set of eightpropositions. Figures 9 and 10 provide examples from the datasets. The RAVEN dataset contains multiplesplits, with different categories of matrices: with a single figure, with four figures, with nine figures, withtwo figures side by side, with two figures up and down, with one figure inside another, an with four figuresinside another one. In our experiments, we use the set with a single figure for training and i.i.d testing andthe sets with four and nine figures for o.o.d testing. shows two examples from the set with fourfigures and from the set with four figures inside one.",
  "Pre-Prompt": "Find the pattern number 9 that completes the sequence. Pick the letter in front of the correct pattern that logicallyfollows in the sequence from the answer set. Patterns in the sequence are preceded by a number from 1 to 8. Patterns inthe answer set are preceded by a letter from A to H. Only return the letter in front of the correct pattern.",
  "A. [(A, J, E, B,)]B. [(F, J, E, B,)]C. [(D, A, E, B,)]D. [(D, B, E, B,)]E. [(D, J, E, B,)]F. [(D, E, E, B,)]G. [(D, G, E, B,)]H. [(D, C, E, B,)]": "1. On an image, a large lime square rotated at 180 degrees.2. On an image, a medium lime square rotated at 180 degrees.3. On an image, a huge lime square rotated at 180 degrees.4. On an image, a huge yellow circle rotated at 0 degrees.5. On an image, a large yellow circle rotated at 0 degrees.6. On an image, a medium yellow circle rotated at 0 degrees.7. On an image, a medium white hexagon rotated at -90 degrees.8. On an image, a huge white hexagon rotated at -90 degrees. A. On an image, a tiny white hexagon rotated at -90 degrees.B. On an image, a giant white hexagon rotated at -90 degrees.C. On an image, a large red hexagon rotated at -90 degrees.D. On an image, a large orange hexagon rotated at -90 degrees.E. On an image, a large white hexagon rotated at -90 degrees.F. On an image, a large green hexagon rotated at -90 degrees.G. On an image, a large blue hexagon rotated at -90 degrees.H. On an image, a large yellow hexagon rotated at -90 degrees.",
  "The answer is E": ": Example of RAVEN task, from (Gendron et al., 2023a). In the test case, the target answer is indicated initalics. The text in blue shows the text for the symbolic dataset. The text in green shows the text for the naturallanguage dataset. The text in gray is the same for both datasets.",
  "(c) Temporal causal graph with Informa-tion Minimisation loss added": ": Causal graphs with and without domain-invariant module and Mutual Information minimisation loss.C is the input context. HR, HI, HSn, HS are the latent states of the router, domain-invariant, domain-specificand activated domain-specific (after router weighting) modules. For simplicity, we only show the state HSn of theactivated domain-specific module n. Y and Ytrue are the output and true distributions. WR, WSn and WI are thetrainable parameters of the modules. LY = Lo + Linv + Ldom and LR are the output and router losses.Black edges show the forward pass at step . Blue dashed edges show the backward pass at step . Red dottededges illustrate the causal links between the forward and backward passes. For simplicity, we only show the step forthe loss variables as they appear twice. All other variables are at step .",
  "P(HI|do(HSn)) = P(HI) n [1, N](12)": "HR, HI and HSn n [1, N] are the respective representations generated by the router, domain-invariant and N domain-specific modules.The rules of do-calculus, defined in Pearl (1995), allow one to reduce interventional queries (with thedo() operator) to observational queries. We will only use the deletion of actions rule. A simplified rule isshown in Equation 13:",
  "if (Y X)GX(13)": "GX represents the causal graph G with the incoming edges of X removed.Let us first address the causal relationships of the router. Equations 10 and 9 can be verified using thesimplified causal graph in . They are a direct application of rule 13. When removing the parentsof HSn or HI, HR is d-separated (Pearl, 1988) from them: the backward path through C is blocked andthe forward path through WR is not connected to HSn or HI. This is due to the use of a separate lossfunction for training the router when using the vector quantisation routing strategy. One could notice thatwe do not represent the sum of losses of Equation 4. We omit it in the simplified graph. Its impact on thebackward pass is incidental since each element can be optimised independently.Let us now address the causal relationships of one activated domain-specific module n with itscounterparts (Equation 11). Again, under graph GHSn, HSn, the backward path through C betweenis blocked. In addition, we make the assumption that only the module n is activated and is connected",
  "and HSn HR Y n [1, N] \\ {n}, with AHR B equivalent to having A X HR and": "X B. A HR B removes the second link. Therefore, during the backward pass, there is only one linkLY WSn HSn and no path to the other domain-specific modules n. A last type of path can exist;here is an example: assuming a model with two domain-specific modules, S0 and S1, activated one after the other, the following path exists: HS1HR Y LY WIC HI Y+1 LY+1 WS2C HS2.There is a causal path forward path from HS1 to HS2. The path does not exist if there is no invariantmodule, and Equation 11 holds.If an invariant module is part of the model, Equations 11 and 12 do not hold because of the path above:HI and HSn are not independent in the causal graph GHSn. Independence is achieved by minimising theMutual Information between HI and HSn, as discussed in the main paper.",
  "K-Means ClusteringThis strategy computes the clusters using K-Means (Lloyds algorithm) (Lloyd,": "1982). We first learn the cluster centroids on the training set separately. Then, we fine-tune the othermodules. Therefore, the clustering mechanism is independent of the gradient descent during fine-tuning,and the quality of the clusters with respect to the data distribution depends solely on the robustness ofthe clustering method. For efficiency reasons, we do not directly perform the clustering on the hiddenstates of the router module. Before clustering an input embedding, we project it to a more dense spacewith fewer dimensions (typically 64). We apply Multidimensional Scaling with the SMACOF algorithm(Borg and Groenen, 2005). The algorithm requires us to provide a base of the input space to performthe projection. We span the space using a random set of vectors from the training space. Because thedistribution is skewed, we do not have a warranty to build a base. To remain computation-efficient, wesample 8 M vectors with M the dimensionality of the reduced space. Euclidean Distance WeightingThis strategy differs from the other ones as it does not use vectorquantisation. Instead, we compute the Euclidean distance between the embeddings and the centroidcoordinates (randomly initialised) and use softmin to convert the distances into continuous weightsbetween zero and one. The lower the distance between the embedding and a centroid, the higher theweight the corresponding domain-specific module will have on the output. Consequently, with this method,all domain-specific modules are always activated. This operation is differentiable and is the closest tothe routing process of Mixture-of-Expert models like the Switch Transformer (Fedus et al., 2022b). Thismethod does not follow the causal structure discussed in . Instead, it uses the output loss toupdate the centroid coordinates.The results obtained with these two routing strategies are provided in Appendix E.3.",
  "DAdditional Aggregation Schemes": "In this section, we describe two additional aggregation schemes between the domain-invariant and domain-specific modules. Instead of using a shared language modelling head, we propose to use a separate headfor each module and combine their outputs at the end by a weighted sum. This method tackles the issueof prioritised modules (e.g. one module being overused at the expense of the others). However, theinformation from the modules is not linearly combined but added separately, reducing expressivity.We want to bound the output of each model such that it influences the final prediction by a pre-determined factor (given by the router output for the domain-specific modules and provided as a hyperpa-rameter for the domain-invariant module). Each module outputs unbounded logits. The lack of bounds prevents them from directly multiplying the logits by their weighting factor and summing them together.Indeed, one module could overcome the weighting by increasing the magnitude of its logits. We considertwo combination schemes: in the logit space and in the probability space. Combination in the logit spaceThe aggregation scheme in the logit space is very similar to the oneperformed in the latent space in the main paper. We first perform a shared batch normalisation (Ioffe andSzegedy, 2015) between the modules to overcome the unbounded issue in the logit space. For a batchof size |B|, one domain-specific active module and one domain-invariant module, batch normalisation isoperated on 2|B| samples. We attribute a weight wI to the domain-invariant module as a hyperparameterand wSn = rn (1 wI) to the domain-specific module, with rn the weight given by the router. Afternormalisation, We multiply each logit value by its corresponding weight and sum them together. Combination in the probability spaceEach module outputs unbounded logits, so we first convert eachoutput into normalised probabilities (that sum to one). We then perform the weighting in each probabilityspace before converting them back to logits (shown in Equation 14). Finally, the outputs from all modulesare summed together (shown in Equation 15). The final probabilities are shown in Equation 16.",
  "P(Y |c) = (l(Y |c))(16)": "c is the input context. P(Y |c) is the final output distribution between all words Y , obtained usingsoftmax normalisation on the output logits l(Y |c). The output logits are obtained by summing theweighted logits of the domain-invariant module lI(Y |c) and all domain-specific modules lSn(Y |c).Equation 14 shows the weighting process for all modules (a {I, S1, . . . , SN}). The weight wI is ahyperparameter set prior to training. The weights wSn n [1, N] combine the weight wI with the routerweights rn: wSn = rn (1wI). Ba is a normalisation term that ensures the conversion function betweenprobabilities and logits is invertible.The results obtained with these two aggregation schemes are provided in Appendix E.4.",
  "E.1Evolution of the Mutual Information Across Training": "To ensure the independence between the domain-specific and domain-invariant modules, we minimisethe mutual Information between them. shows the evolution of Mutual Information duringtraining. We observe that it quickly decreases to reach below 0, 0001. shows the same loss forthe variants using aggregation in the logit and probability spaces. Unlike for the main model, we observesmall spikes in the loss after 100 training steps. The aggregation scheme that uses a shared languagemodelling head (our default) seems more stable during training.",
  "E.2Routing Alignment and Visualisation": "We study the module attribution performed by the router more deeply. shows the alignmentbetween the two domain-specific modules. We first observe that the division is mainly syntactic: eachmodule specialises towards one type of input format, either text or symbolic. It aligns perfectly with thedataset.Figures 14, 15, 16 and 17 show visualisations of the clusters in a 2D space. Figures 14 and 15 showthe i.i.d and o.o.d sets of ACRE. Figures 16 and 17 show the i.i.d and o.o.d sets of RAVEN. As in themain paper, the projection is made using Multidimensional Scaling (MDS) (Borg and Groenen, 2005).For illustration purposes, we observe the clusters formed by the K-Means method for N = 4 modules.We also observe the clusters formed from the penultimate hidden states of the router. As discussed above",
  "n = 00.01.00.01.00.01.0n = 11.00.01.00.01.00.0": "and in the main paper, there is a clear division between text and symbolic embeddings, but the o.o.d setsare not well separated in ACRE while they are in RAVEN. This division (and absence of division) is alsopresent in the previous hidden states, although the separation is less obvious: all embeddings tend to alignto a single axis.We want to study the routers behaviour further when faced with a diverse set of input data. To this end,we feed six different datasets to the model: the i.i.d text and symbolic sets of ACRE and RAVEN, PVR(Zhang et al., 2021b) and ARC (Chollet, 2019) datasets. The visualisations are in . Overall, thedatasets are well separated but have different shapes. While some form dense amalgamates, others spreadin the latent space. The observations from ACRE and RAVEN suggest that the distance in the embeddingspace between a module cluster and an input can be an indicator of the modules performance on the input.The o.o.d sets of ACRE are merged in the latent space, and the model maintains accuracy across the sets.In parallel, the o.o.d sets of RAVEN are separated by clear boundaries, and the accuracy drops as thedistance with the i.i.d set increases. Experiments on a larger scale are needed to validate or invalidate thehypothesis and discriminate the true causes responsible for this behaviour from spurious correlations.",
  "E.3Variations of the Routing Strategy": "We perform additional experiments on ACRE and RAVEN datasets using the routing strategies introducedin Appendix C: K-Means and weighting. Tables 4 and 5 show the results.The alternative routing strategies achieve similar and sometimes superior performance than the baseICLM model. As observed in the previous section, the router creates well-defined clusters that theK-Means and Euclidean distance vector quantisation strategies tend to follow. No explicit differentiationof the routing process can be observed from the visualisations. The difference in performance maylie in the optimisation process. K-Means does not backpropagate information to the router; weightingbackpropagates from the output loss, and vector quantisation backpropagates from a secondary loss.",
  "(d) K-Means (l = 2)": ": Clusters formed from the hidden states of LLaMA2 on the training sets of ACRE, ARC, PVR andRAVEN. The visualisations contain the last two levels (l) of hidden layers. The ground truth shows the true splitsbetween each dataset. The learned clusters use 4 centroids. : Accuracy on the ACRE i.i.d and o.o.d test sets. Datasets are represented in columns, and models in rows.ICLM is trained on text and symbolic i.i.d training sets. Models with a indicate that the results are introduced inthis paper. The best model is shown in bold.",
  "E.5Module Correlation Across Hidden States": "To further investigate the level of Independence of the LLM modules, we measure the Pearson CorrelationCoefficient between all hidden states of the domain-invariant and domain-specific modules during infer-ence on ACRE and RAVEN. Figures 19 and 20 show the results. The intra-module correlations (Figures19a, 19e, 19h, 19i, 20a, 20e, 20h and 20i) show that hidden states from close layers in the model are highlycorrelated. Furthermore, correlation blocks are visible, i.e. sequences of layers that demonstrate a highlevel of correlation between them and a low level of correlation with the layers not in the sequence. Weobserve five correlation blocks well-defined on ACRE and with fuzzy-edges on RAVEN.",
  "(j) Dom 0-Dom 1": ": Measures of the Pearson Correlation Coefficient between module hidden states during inference onRAVEN dataset. Rows and columns represent layers 0 to 33 of a LLaMA2 module. Router refers to the routingmodule, Inv refers to the domain-invariant module, and Dom i refers to the domain-specific module i. The captionindicates the modules used for each row-column pair.",
  "(u) Dom 2-Dom 3": ": Measures of the Pearson Correlation Coefficient between module hidden states of the transfer learningmodel during inference on RAVEN dataset. Rows and columns represent layers 0 to 33 of a LLaMA2 module.Router refers to the routing module, Inv refers to the domain-invariant module, and Dom i refers to the i domain-specific module. The caption indicates the modules used for each row-column pair."
}