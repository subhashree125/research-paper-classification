{
  "Abstract": "Multimodal Large Language Models (MLLMs)demonstrate remarkable performance across awide range of domains, with increasing em-phasis on enhancing their zero-shot generaliza-tion capabilities for unseen tasks across vari-ous modalities. Instruction tuning has emergedas an effective strategy for achieving zero-shot generalization by finetuning pretrainedmodels on diverse multimodal tasks. As thescale of MLLMs continues to grow, parameter-efficient finetuning becomes increasingly criti-cal. However, most existing parameter-efficientapproaches focus only on single modalitiesand often overlook the multimodal character-istics during finetuning. In this work, we in-troduce a novel Multimodal Prompt Tuning(M2PT) approach for efficient instruction tun-ing of MLLMs. M2PT effectively integratesvisual and textual prompts into the vision en-coder and language processor respectively dur-ing finetuning, facilitating the extraction andalignment of features across modalities. Em-pirical results on various multimodal evalua-tion datasets demonstrate the superior perfor-mance of our approach compared to severalstate-of-the-art baselines. A comprehensive setof ablation studies validates the effectivenessof our prompt design and the efficiency of ourapproach.",
  "Introduction": "Human cognition intricately integrates various sen-sory modalities to perceive, interpret, and engagewith the environment, fostering a comprehensiveunderstanding of the surrounding world (Liu et al.,2023c; Dumas et al., 2009; Chen et al., 2024; Jinet al., 2024a,c). The development of MultimodalLarge Language Models (MLLMs) (Alayrac et al.,2022; Yin et al., 2023; Han et al., 2024a,c; Jinet al., 2024b,d) marks a significant advancement inemulating this capability, playing a pivotal role in",
  "tively. These modality specific prompts play acrucial role in effectively guiding the modelsfine-tuning process, enabling fast and accuratemultimodal model adaptation": "We design the cross-modality interaction be-tween the prompts from different modalitiesduring the instruction tuning. By doing so,M2PT leverages the strengths of each modal-ity, resulting in comprehensive and coherentlearning results. This synergy empowers themodel to perform complex tasks that requirethe integration from multimodal perspectives. We conduct comprehensive experiments onvarious multimodal tasks in the zero-shot set-ting, demonstrating the effectiveness of theproposed approach over several state-of-the-art parameter efficient finetuning methods.",
  "Related Work": "Multimodal Large Language Models.MLLMs(Dai et al., 2023; Driess et al., 2023; Liu et al.,2023c; Yao et al., 2023; Sun et al., 2024a) integratemultimodal information (e.g., audio, image, video),extending beyond the textual semantic informationprocessed by conventional Large Language Models(LLMs). A general structure of MLLMs includesthree main components (Li et al., 2024a): a pre-trained modality encoder to encode multimodalitydata, a pre-trained LLM to reason encoded mul-timodal data and perform generation tasks, and aconnection layer to project modality informationinto tokens. During the standard full finetuningprocess (Liu et al., 2023c), a substantial amount ofweights within all intermediate layers and the pre-trained LLM are continuously updated. Conceptu-ally different, our approach can elegantly fine-tunethe model with adjusting a minimum of weights. Instruction Tuning.To enhance the zero-shotand in-context learning (Brown et al., 2020; Liet al., 2024b) capabilities of large language mod-els (LLMs) (Zhao et al., 2023), researchers haveexplored instruction tuning (Ouyang et al., 2022;Zhang et al., 2023), a technique that enablespre-trained LLMs to be more adaptable for intri-cate multimodal tasks. Specifically, instruction-tuning is a process that refines LLMs by finetuningthem on meticulously curated instruction-followingdatasets encapsulating user intent and desired out-puts (Ouyang et al., 2022). With the rapid advance-ment of multimodal models, instruction tuning has emerged not only as a state-of-the-art approach fornatural language processing (NLP) tasks but alsonaturally extend to vision-related tasks such as im-age captioning (He et al., 2020; Cornia et al., 2020),image classification (Radford et al., 2021), visualquestion answering (VQA) (Antol et al., 2015). Parameter-Efficient Finetuning.With the dras-tic growth in the scale of AI models, especiallyMLLMs and LLMs, Parameter-efficient Finetun-ing (PEFT) (Hu et al., 2022; He et al., 2023; Jieet al., 2023; Yan et al., 2023) have shown its ca-pability to efficiently adapt pre-trained models todiverse downstream tasks without updating signifi-cant amount of model parameters. Generally, cur-rent PEFT strategies can be categorized into partialtuning (Chen et al., 2021; Jia et al., 2021), extramodule such as Low-Rank Adaptation (LoRA) (Jieet al., 2023) and prompt tuning (Jia et al., 2022; Juet al., 2022b; Dong et al., 2023a). However, partialtuning faces limitations, including unsatisfactoryperformance relative to full finetuning (Jia et al.,2022; Chen et al., 2021; Jia et al., 2021). Extramodule exhibits limited adaptability when consid-ering various model backbones. In contrast, prompttuning (Lester et al., 2021; Ma et al., 2022; He et al.,2022a; Liu et al., 2023d; Qiu et al., 2020) providesa general and straightforward solution with power-ful performance gains. It introduces a set of learn-able parameters to the input sequence of backbonemodels, updating only these parameters during fine-tuning. Despite its simplicity, the applicability ofprompt tuning within the multimodal paradigmremains largely unexplored. Unlike approachessuch as MaPLe (Khattak et al., 2023), CoOp (Zhouet al., 2022b), CoCoOp (Zhou et al., 2022a) andMoPE (Wu et al., 2024b), which focus on craftingCLIP-based prompts for classification tasks, ourwork targets enhancing the capabilities of MLLMsin zero-shot instruction-following scenarios, result-ing in a fundamentally distinct method design. Fur-thermore, PromptFuse (Liang et al., 2022) onlyintroduces learnable prompts into textual modal-ity, neglecting the synergy of multimodality. Ourapproach offers flexibility in prompt design, allow-ing prompts to be independently tailored for eachmodality and inserted at various layers. This flex-ibility significantly enhances the MLLMs adapt-ability while reducing the number of parameters,improving performance across various datasets.",
  "Preliminaries": "Multimodal Large Language Modelsintegratevisual and language processing capabilities, lever-aging the power of LLMs to enhance the compre-hension of multimodal information. A prevalentworkflow of MLLMs begins with the utilization ofpre-trained vision encoders fv (e.g., LLaVA (Liuet al., 2023c) and its variants (Li et al., 2022; Sunet al., 2023)), encoding visual input Xim and ex-tracting output Ov = fv(Xim) through remarkablevision understanding ability. Subsequently, the vi-sion output is further projected into language space,aligning with the textual embedding and enablingthe model to understand and respond to instructionseffectively. Ultimately, the integrated LLM fllmassimilates Ov and text embedding Ot. Harnessingthe extensive knowledge of LLM, integrating multi-modal inputs to generate coherent and contextuallyrelevant language response y, represented as:",
  "y = fllm(Ov, Ot).(1)": "Prompt Tuning is a form of PEFT approach,demonstrating exceptional efficacy within single-modality settings under both visual (Han et al.,2023) and textual (Wang et al., 2023a) domains.It entails learnable continuous soft prompts intothe input space while concurrently preserving themajority of parameters within the backbone frozen.Specifically, given a K layer transformer-basedmodel f, soft prompts P k combined with the inputof k-th layer to obtain the output Ok as:",
  "Multimodal Prompt Tuning": "In this section, we formally introduce M2PT, anovel multimodal prompt tuning approach for theeffective and efficient finetuning of MLLMs. Theoverall model architecture is depicted in .Fundamentally, our model necessitates the trainingof only three targeted components, while keepingthe backbone parameters from both visual encoderand LLM frozen. Specifically, these componentsinclude Visual Prompt (Pv), which is the learn-able parameter (i.e., soft prompt) integrated into",
  "Block": ": Overview of our M2PT approach. Here, visual prompts are embedded into each layer of the VisualEncoder, and textual prompts are embedded into each layer of the LLM. These prompts facilitate the extraction andalignment of features across modalities (e.g., vision, language). The cross-modality interaction between visual andtextual features is enhanced through layered integration, ultimately improving the models capability in zero-shotinstruction learning tasks (see 4). the visual encoder; Textual Prompt (Pt) is in-corporated into the LLM in order to capture thenuanced semantic relationships across modalities; Cross-modality Interaction, where parametersare learned to enhance the alignment between fea-tures extracted by the vision encoder and textualrepresentations. In summary, prompts from thesedistinct modalities (i.e., -) facilitate the modelsacquisition of knowledge from multimodal fine-tuning datasets, capturing the distinctive charac-teristics inherent in each modality and fosteringcross-modal interaction.Visual Prompt.We denote the set of visualprompts as Pv = {P 1v , P iv, , P Nv }, where P ivindicates the set of visual prompts in the i-th layerof the visual encoder, consistent with previous prac-tice for prompt tuning (Jia et al., 2022; Han et al.,2023). Each prompt is a dv-dimensional vectorwith the same dimensions as the original visiontokens. Prompts in each layer are placed at theleftmost positions within the sequence to interactwith other vision tokens. Formally, we have:",
  "Oiv = fiv(P iv, Oi1),(3)": "where i {2, 3, . . . , N}, and Oiv is the i-th layervision embedding.Textual Prompt. Visual prompts serve as an ef-fective tool for capturing semantic content withinthe visual inputs, as well as gaining the ability tointeract with the text modality through the mappingfrom visual domain. Nevertheless, the optimiza-tion of visual elements does not directly affect the inherent representation of LLMs in text modality.Naturally, to further enhance the text modalitysprocessing ability, we introduce the textual promptsto capture text patterns and influence the inner rep-resentation within the LLM. Specifically, textualprompts are denoted as Pt = {P 1t , P jt , , P Mt },where j indicates the prompt inject position of thej-th layer in a total M layers LLM. Each promptis a dt-dimensional vector with the same dimen-sionality as the original text tokens. Formally, weincorporate textual prompts into the LLM as:",
  "(4)": "where j {2, 3, . . . , M}, y is the textual outputof the MLLM, Ot is the textual embedding, andfhead is the task-specific head in order to decodethe embeddings into texts.Cross-modality Interaction. To achieve align-ment between visual and textual modality, we intro-duce a tunable interaction layer fin, which is specif-ically designed to align the output ONv producedby the visual encoder and the textual embedding,through a linear projection:",
  "Ov = fin(ONv ).(5)": "Here Ov represents the aligned vision embed-ding. This transformation ensures that the visualencoders output is effectively mapped onto a com-mon textual representation space. The projectedvisual tokens then interact with the textual tokens : Zero-shot Multimodal Evaluation on all multimodal datasets. The MMAvg represents the averagescore on the right seven tasks. LLaVAAlign is the stage-one LLaVA without end-to-end finetuning, and LLaVAF Tindicates the fully fine-tuned LLaVA. All the fine-tuned processes are using the same Vision-Flan dataset. M2PTa/bmeans textual and visual prompt lengths a and b, respectively. The best performance is in bold.",
  "through all LLM layers, facilitating the integra-tion of visual and textual information. Our elegantdesign of M2PT enjoys a few appealing character-istics:": "Cross-modal Integration. Our M2PT model em-ploys a unified prompt tuning paradigm. This ap-proach not only captures the distinctive attributesof each modality but also facilitates the fluid ex-change of cross-modal information, enhancingthe models capability to comprehend and gener-ate multimodal data effectively. Optimized Parameter Utilization. M2PT demon-strates superior parameter efficiency by focusingonly on the training of a minimal set of parame-ters while keeping the majority of the models pa-rameters frozen, allowing a significant reductionin the number of parameters required (0.09%).Despite this reduction, M2PT maintains superiorperformance on multimodal tasks (see )with a balance between computational efficiencyand overall effectiveness in zero-shot setting.",
  "Implementation Details": "We employ LLaVA (Liu et al., 2023c) with CLIP-L (Radford et al., 2021) (i.e., 24 transformer blocks)as the visual encoder and Vicuna-7B-v1.3 (Zhenget al., 2023) (i.e., 32 transformer blocks) as thebase LLM for all variants. For the cross-modalityinteraction, we use a linear layer to map the embed-ding dimension from dv to dt to ensure the align-ment between the two modalities. For the promptinitialization, we employ Xavier (Glorot and Ben-gio, 2010) initialization on both visual and textualprompt to ensure stable modal information deliveryfrom these prompts at the early stages of training,thereby facilitating rapid convergence of the model.More implementation details are provided in 4.1and Appendix S2.",
  "Experiment Setup": "Datasets.For training, we conduct instructiontuning on Vision-Flan (Xu et al., 2024b), which isa human-annotated multimodal instruction tuningdataset with 191 diverse tasks. To reduce compu-tational costs, we follow common practice (Shenet al., 2024) and employ a scaled-down versioncontaining up to 1, 000 instances per task, result-ing in a total of 191, 105 instances.For zero-shot evaluation, we examine our approach on thecomprehensive multimodal evaluation benchmarkMME (Fu et al., 2023), measuring both percep-tion and cognition abilities across 14 subtasks.We further evaluate the models capabilities us-ing 7 multimodal datasets. Specifically, for Op-tical Character Recognition, we utilize the Text-VQA (Singh et al., 2019), and for reasoning, weemploy the Visual Spatial Reasoning (VSR) (Liuet al., 2023a). Following (Zhai et al., 2023; Shenet al., 2024), the perception capability is testedon CIFAR-10/100 (Krizhevsky et al., 2009) andMNIST (Deng, 2012). SNLI-VE (Xie et al., 2019)evaluates Visual Entailment capabilities, while thePOPE (Li et al., 2023) dataset examines the ten-dency towards object hallucination. More detailsare provided in the Appendix S1.Training Details. Following previous works (Hanet al., 2023; Shen et al., 2024; Jia et al., 2022),we conduct grid search to match the best tuninghyperparameters, learning rate (i.e., [1e3, 9e4,7e4, 4e4, 1e4, 5e5]), textual prompt length(i.e., ) and visual prompt length(i.e., ).For all models, thelearning rate is scheduled following a cosine de-cay policy, the warm up ratio is set at 0.03 andtrained for 3 epochs except in training epoch ex-periment. We follow the same batch size settingin (Shen et al., 2024) as 128 for instruction tun-",
  "(b) Visual Encoder attention activation": ": Comprehensive visualization of attentionactivation maps. This figure presents a detailed exami-nation of the activation patterns within the last layer ofLLM and Visual Encoder, respectively. As seen, the vi-sion prompts and textual prompts have noticeably highactivation levels during inference (i.e., and representtextual prompts activation signal and visual promptsactivation signal, respectively). ing. Further details are provided in the AppendixS2.M2PT is implemented in Pytorch (Paszkeet al., 2019). Experiments are conducted on 8NVIDIA A100 GPUs. Our code is available at Metrics. The MME incorporates bothPerception and Cognition metrics1. For other mul-timodal datasets, we use Vicuna-13B-v1.5 (Zhenget al., 2023) to assess the accuracy of each predic-tion compared to the groundtruth. Further detailsare provided in the Appendix S3.",
  "Main Result": "In , our main result exhibits a comprehen-sive zero-shot evaluation of M2PT with severalbaselines on eight multimodal datasets. Specif-ically, we consider four state-of-the-art PEFTapproaches, including LoRA (Hu et al., 2022),APrompt (Wang et al., 2023a), PTUM (Yang et al.,2023a) and VPT (Han et al., 2024b). Full fine-tuned LLaVA (i.e., LLaVAFT (Liu et al., 2023c))serves as an upper-bound of multimodal evaluation.We report the performance of M2PT under twodifferent settings, with M2PT10/10 and M2PT10/20.There are several key observations from these results.First, M2PT achieves the best perfor-mance among all PEFT approaches in most cases,demonstrating the effective design of visual and tex-tual prompts. For example, on MME task, M2PTdemonstrates a significant improvement of 6.90%and 7.51% compared to two strong prompt tuningmethods, Aprompt and VPT, respectively. Thishighlights the limitation of existing prompt tuningapproaches that primarily focus on single modality,failing to capture the cross-modality interactions.In contrast, the interaction layer together with themultimodal LLM employed by our approach suc-cessfully bridges this gap, resulting in enhancedperformance. Second, the performance of M2PTreaches 94.75% of the full finetuning performancewhile considering only 0.09% of the total modelparameters, demonstrating the parameter efficiencyof M2PT. Moreover, we observe that M2PT outper-forms the full finetuning LLaVA on VSR, MNISTand POPE tasks, showing its strong capability inzero/few shot learning. This is consistent with theobservations in several previous works (Han et al.,2023; Yang et al., 2023b). Third, it can be seenthat M2PT does not perform very well on the visualentailment task, SNLI-VE. Our hypothesis is thatlogical relationships or causal scenario understand-ing is critical in this task, which might not be fullycaptured by prompt tuning based approaches.",
  "Analysis and Discussion": "Attention Activation Pattern Analysis.Follow-ing common practice (Sun et al., 2024b), we extractand discuss the activation maps from the attentionblock of MLLMs, and investigate the influence ofvisual and textual prompts in . We randomlyselect two samples from the MME dataset and vi-sualize their corresponding activation maps of theattention block in the last layers of both visual en-coder ((a)) and LLM ((b)).To analyze the impact of textual and visualprompts to the frozen components, we categorizethem, according to LLaVas model structure, intotextual prompts, system text tokens, visual prompts,image tokens and instructions. Several findings canbe observed. First, in the LLM attention activationmap, we observe that the token regions correspond-ing to textual prompts exhibit elevated activationlevels, indicating their significant role in shapingthe models responses. The activation levels of vi-sual prompts within the LLM, while comparativelylower, remain notable relative to most other regions. MMECIFAR-10POPE MME Score w/o interaction layerw/o visual promptw/o textual promptMPT 70.0 72.5 75.0 77.5 80.0 82.5 85.0 87.5 90.0 CIFAR-10 & POPE Score 80.380.1 82.9 80.3 76.5 79.6 89.3 81.3",
  ": Impact of Different Components": "This suggests a secondary yet substantial role inmultimodal inference. Second, in the activationmaps from Visual Encoder, the activation levels as-sociated with visual prompts are noticeably higherthan those of most other tokens, underscoring thecritical role of visual prompts in processing visualinformation during tuning. These observations sup-port that visual prompts effectively interact with thetextual prompts, enhancing the alignment betweenmodalities and thereby improving the models per-formance on the zero-shot instruction learning. Ourcomponent analysis study below further strength-ens this claim quantitatively. Impact of Different Components.To investi-gate the impact of different components in M2PTacross various datasets (i.e., visual prompts, textualprompts, and interaction layer), we conducted com-ponent ablation experiments in by removingeach component at a time from M2PT10/20. Theresults demonstrate that the model performancedrops when any of the trainable prompts is re-moved, which aligns with our expectations. More-over, the model performance also decreases with-out the trainable interaction layer, indicating theimportance of this layer. Furthermore, we observethat the importance of different components variesacross the tasks. For example, textual prompts playthe most important role in CIFAR-10 and POPE,while visual prompts lift the performance most onMME. Once again, it is worth noting that combin-ing the multimodal prompts with the interactionlayer in M2PT leads to the best performance. Impact of Prompt Length.In M2PT, promptlength is the only hyperparameter that needs to betuned. To further analyze the impact of differentprompt lengths on model performance, we conducta comprehensive study on the lengths of visualand textual prompts on the Vision-Flan dataset tobetter understand their properties. Following com-mon practice (Han et al., 2023; Jia et al., 2022;Han et al., 2024b), we use the grid search to span",
  "Textual": "81.6880.9281.3681.24 81.2881.2981.2681.24 81.8681.8181.1381.17 80.6282.2681.6081.39 POPE MME 51.0 51.5 52.0 52.5 53.0 53.5 54.0 54.5 55.0 55.5 VSR CIFAR-10 80.50 80.75 81.00 81.25 81.50 81.75 82.00 82.25 82.50 POPE : Performance of Different Prompt Length.Each cell in the map corresponds to the score of a modelwith a textual prompt length (row) and a visual promptlength. A darker hue indicates a higher score, whereasa lighter hue signifies a lower score.: Prompt Location Experiment (M2PT10/20).",
  "(d) Latter Half1249.3983.7279.34(e) All1503.9889.2981.26": "a range from 5 to 40 on both visual and textualprompt lengths, reported in . We stop at aprompt length of 40 because performance satura-tion is observed around this point. Thus, extendingthe prompt length further would result in increasedparameter usage without significant performancegains (i.e., M2PT shows a slight decrease in perfor-mance on MME. We argue that this may be due tooverparameterization (Han et al., 2023; Jia et al.,2022)). When visual prompt length extends from5 to 20, and textual prompt length extends from5 to 10, noticeable performance gains can be ob-served. It can be seen that there is no universaloptimal prompt length that consistently achievesthe best performance across different tasks. Forinstance, the optimal performance of the model onMME is achieved with a configuration of 20 visualprompts and 10 textual prompts, while 10 visualprompts and 40 textual prompts achieve the high-est performance on POPE. We hypothesize thatdifferent tasks exhibit distinct data distributions,with difficult tasks potentially requiring longerprompts to effectively capture the underlying pat-terns. Nonetheless, we observed that the perfor-mance of M2PT remains relatively stable within acertain range of prompt lengths. Impact of Prompt Location.Following (Jiaet al., 2022), studies the insertion ofprompts at different layers. We design five dis-tinct settings in which prompts are integrated intoboth the Vision Encoder and LLM but at differ-ent locations. Specifically, we introduce promptsinto: (a) the first layer; (b) every odd-number layer(i.e., [1, 3, . . . , 23] N, [1, 3, . . . , 31] M); (c)the first half of the layers (i.e., N, M); (d) the latter half of the layers (i.e., N, M); and (e) all layers.Each variant reports the best prompt length combi-nation selected with MME evaluation. Generally,M2PTs performance is positively correlated withprompt depth. Yet the accuracy drops when insert-ing prompts from top to bottom, suggesting thatprompts at earlier layers matter more than those atlatter layers, which is consistent with the observa-tions in (Jia et al., 2022; Wang et al., 2023a). MME Score Data Volume % (Vision-Flan) MMECIFAR-10POPE CIFAR-10 & POPE Score",
  ": Training Epoch Experiment (M2PT10/20)": "Effect of Data Volume and Training Epoch.In, we randomly sample data from Vision-Flanat different scales (i.e., 25%, 50%, 75%, 100%) toevaluate the performance of the model with limiteddata. The results demonstrate that M2PT main-tains excellent performance despite significant datareduction, highlighting efficiency and robustnessregarding data quantity. This property indicatesthat M2PT exhibits substantial tolerance to datascale, suggesting a promising future for real-worldapplications with constrained data.In , we analyze the M2PTs performanceon different training epochs. This experiment is",
  ": Failure cases study on CIFAR-10": "conducted using the optimal hyperparameter com-bination of 10 textual and 20 visual prompts. Itcan be seen that the model performance generallyimproves with more training epochs, confirmingthat M2PT can achieve remarkable performanceafter sufficient training and demonstrate robustnessin adapting to different training epochs.Case Study.In , we present cases whereM2PT demonstrates success on the MME andCIFAR-10, while other approaches fail (i.e., VPT,PTUM). For example, on MME, while M2PT cor-rectly identifies a chair in the image, both VPTand PTUM fail to capture this concept and respondwith the wrong answer. On CIFAR-10, VPT andPTUM both misidentify cat as a different cate-gory. We posit that the insufficiency of VPT maystem from its inadequate understanding of logi-cal relationships or causal scenarios. PTUM em-ploys single-modality tuning exclusively, renderingit inadequate for managing complex multi-modalbehavior and capturing interactions between modal-ities. In sharp contrast, M2PT takes an aspect ofmultimodal, enhancing both visual modality com-prehension and textual modality causal inference.We further conduct failure case studies for M2PTon CIFAR-10 dataset. The results in indi-cate that the predominant misclassification withinthe CIFAR-10 (i.e., 44% or 443 examples of allsuch misclassification) is the mis-identification ofships as boats. This high error rate could po-tentially be attributed to the limited resolution of",
  "Conclusion": "We introduce M2PT, a novel framework in mul-timodal prompt tuning for zero-shot instructionlearning. Our framework offers several advantages:i) it introduces visual and textual prompts elegantlyinto vision encoder and LLM, respectively, en-abling fast and accurate multimodal adaptation;ii) it synergizes modalities by cross-modality inter-action, enjoying coherent integration from multi-modal perspectives; and iii) it significantly reducesthe number of trainable parameters compared toconventional finetuning methods, while maintain-ing robust performance across zero-shot tasks. Asa whole, we conclude that the outcomes elucidatedin this paper impart essential understandings andnecessitate further exploration within this realm.",
  "Limitations": "For potential limitations, M2PT requires two hy-perparameters on prompt length searching (i.e., Vi-sual Prompt, Textual Prompt). Though in practice,we find both lengths vary into a relatively narrowrange (see 5), and are sufficient enough to out-perform all current methods, there is still possibleintegration (He et al., 2022b) of a local search-ing network to generate optimal combinations oflengths. Another potential limitation is that M2PT,akin to other PEFT approaches (Han et al., 2023;Jia et al., 2022), lacks ad-hoc explainability (Biehlet al., 2016; Wang et al., 2023b). While in 4, wedemonstrates that activation maps from MLLMsare significantly influenced by visual and textualprompts, further research is necessary to elucidatethe underlying nature of these prompts.",
  "This research was supported by the National Sci-ence Foundation under Grant No. 2242243": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,Antoine Miech, Iain Barr, Yana Hasson, KarelLenc, Arthur Mensch, Katherine Millican, MalcolmReynolds, Roman Ring, Eliza Rutherford, SerkanCabi, Tengda Han, Zhitao Gong, Sina Samangooei,Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, SahandSharifzadeh, Mikolaj Binkowski, Ricardo Barreira,Oriol Vinyals, Andrew Zisserman, and Karn Si-monyan. 2022. Flamingo: a visual language modelfor few-shot learning. In Advances in Neural In-formation Processing Systems 35: Annual Confer-ence on Neural Information Processing Systems 2022,NeurIPS 2022, New Orleans, LA, USA, November 28- December 9, 2022. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-garet Mitchell, Dhruv Batra, C. Lawrence Zitnick,and Devi Parikh. 2015. VQA: visual question an-swering. In 2015 IEEE International Conferenceon Computer Vision, ICCV 2015, Santiago, Chile,December 7-13, 2015, pages 24252433. IEEE Com-puter Society. Alejandro Barredo Arrieta, Natalia Daz Rodrguez,Javier Del Ser, Adrien Bennetot, Siham Tabik, Al-berto Barbado, Salvador Garca, Sergio Gil-Lopez,Daniel Molina, Richard Benjamins, Raja Chatila, andFrancisco Herrera. 2020. Explainable artificial intel-ligence (XAI): concepts, taxonomies, opportunitiesand challenges toward responsible AI. Inf. Fusion,58:82115.",
  "Michael Biehl, Barbara Hammer, and Thomas Villmann.2016. Prototype-based models in machine learning.Wiley Interdisciplinary Reviews: Cognitive Science,7(2):92111": "Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish,Alec Radford, Ilya Sutskever, and Dario Amodei.2020. Language models are few-shot learners. In Ad-vances in Neural Information Processing Systems 33:Annual Conference on Neural Information Process-ing Systems 2020, NeurIPS 2020, December 6-12,2020, virtual. Mingkai Chen, Taowen Wang, James Chenhao Liang,Chuan Liu, Chunshu Wu, Qifan Wang, Ying NianWu, Michael Huang, Chuang Ren, Ang Li, TongGeng, and Dongfang Liu. 2024. Inertial confinementfusion forecasting via llms. CoRR, abs/2407.11098. Xinlei Chen, Saining Xie, and Kaiming He. 2021. Anempirical study of training self-supervised visiontransformers. In 2021 IEEE/CVF International Con-ference on Computer Vision, ICCV 2021, Montreal,QC, Canada, October 10-17, 2021, pages 96209629.IEEE.",
  "Language Processing, EMNLP 2023, Singapore, De-cember 6-10, 2023, pages 1017310187. Associationfor Computational Linguistics": "Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi,and Rita Cucchiara. 2020. Meshed-memory trans-former for image captioning. In 2020 IEEE/CVFConference on Computer Vision and Pattern Recog-nition, CVPR 2020, Seattle, WA, USA, June 13-19,2020, pages 1057510584. Computer Vision Founda-tion / IEEE. Wenliang Dai, Junnan Li, Dongxu Li, AnthonyMeng Huat Tiong, Junqi Zhao, Weisheng Wang,Boyang Li, Pascale Fung, and Steven C. H. Hoi.2023. Instructblip: Towards general-purpose vision-language models with instruction tuning.In Ad-vances in Neural Information Processing Systems36: Annual Conference on Neural Information Pro-cessing Systems 2023, NeurIPS 2023, New Orleans,LA, USA, December 10 - 16, 2023.",
  "Shaohua Dong, Yunhe Feng, Qing Yang, Yan Huang,Dongfang Liu, and Heng Fan. 2023a. Efficient multi-modal semantic segmentation via dual-prompt learn-ing. CoRR, abs/2312.00360": "Wei Dong, Dawei Yan, Zhijun Lin, and Peng Wang.2023b. Efficient adaptation of large vision trans-former via adapter re-composing. In Advances inNeural Information Processing Systems 36: AnnualConference on Neural Information Processing Sys-tems 2023, NeurIPS 2023, New Orleans, LA, USA,December 10 - 16, 2023. AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,Dirk Weissenborn,Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, MatthiasMinderer, Georg Heigold, Sylvain Gelly, JakobUszkoreit, and Neil Houlsby. 2021.An imageis worth 16x16 words:Transformers for imagerecognition at scale. In 9th International Conferenceon Learning Representations, ICLR 2021, VirtualEvent, Austria, May 3-7, 2021. OpenReview.net. Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, CoreyLynch, Aakanksha Chowdhery, Brian Ichter, AyzaanWahid, Jonathan Tompson, Quan Vuong, TianheYu, Wenlong Huang, Yevgen Chebotar, Pierre Ser-manet, Daniel Duckworth, Sergey Levine, VincentVanhoucke, Karol Hausman, Marc Toussaint, KlausGreff, Andy Zeng, Igor Mordatch, and Pete Florence.2023. Palm-e: An embodied multimodal languagemodel.In International Conference on MachineLearning, ICML 2023, 23-29 July 2023, Honolulu,Hawaii, USA, volume 202 of Proceedings of MachineLearning Research, pages 84698488. PMLR.",
  "Jrg Kohlas, editors, Human Machine Interaction,Research Results of the MMI Program, volume 5440of Lecture Notes in Computer Science, pages 326.Springer": "Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jin-rui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Ron-grong Ji. 2023. MME: A comprehensive evaluationbenchmark for multimodal large language models.CoRR, abs/2306.13394. Xavier Glorot and Yoshua Bengio. 2010. Understand-ing the difficulty of training deep feedforward neuralnetworks. In Proceedings of the Thirteenth Inter-national Conference on Artificial Intelligence andStatistics, AISTATS 2010, Chia Laguna Resort, Sar-dinia, Italy, May 13-15, 2010, volume 9 of JMLRProceedings, pages 249256. JMLR.org. Cheng Han, James Chenhao Liang, Qifan Wang, Ma-jid Rabbani, Sohail A. Dianat, Raghuveer Rao,Ying Nian Wu, and Dongfang Liu. 2024a. Imagetranslation as diffusion visual programmers. In TheTwelfth International Conference on Learning Rep-resentations, ICLR 2024, Vienna, Austria, May 7-11,2024. OpenReview.net. Cheng Han, Qifan Wang, Yiming Cui, Zhiwen Cao,Wenguan Wang, Siyuan Qi, and Dongfang Liu. 2023.E2vpt: An effective and efficient approach for visualprompt tuning. In IEEE/CVF International Confer-ence on Computer Vision, ICCV 2023, Paris, France,October 1-6, 2023, pages 1744517456. IEEE.",
  "Cheng Han, Qifan Wang, Yiming Cui, Wenguan Wang,Lifu Huang, Siyuan Qi, and Dongfang Liu. 2024b.Facing the elephant in the room: Visual prompt tun-ing or full finetuning? CoRR, abs/2401.12902": "Cheng Han, Qifan Wang, Sohail A. Dianat, Majid Rab-bani, Raghuveer M. Rao, Yi Fang, Qiang Guan, LifuHuang, and Dongfang Liu. 2024c. AMD: automaticmulti-step distillation of large-scale vision models.CoRR, abs/2407.04208. SenHe,WentongLiao,HamedR.Tavakoli,Michael Ying Yang, Bodo Rosenhahn, and NicolasPugeault. 2020. Image captioning through imagetransformer. In Computer Vision - ACCV 2020 - 15thAsian Conference on Computer Vision, Kyoto, Japan,November 30 - December 4, 2020, Revised SelectedPapers, Part IV, volume 12625 of Lecture Notes inComputer Science, pages 153169. Springer. Xuehai He, Chunyuan Li, Pengchuan Zhang, JianweiYang, and Xin Eric Wang. 2023. Parameter-efficientmodel adaptation for vision transformers. In Thirty-Seventh AAAI Conference on Artificial Intelligence,AAAI 2023, Thirty-Fifth Conference on InnovativeApplications of Artificial Intelligence, IAAI 2023,Thirteenth Symposium on Educational Advances inArtificial Intelligence, EAAI 2023, Washington, DC,USA, February 7-14, 2023, pages 817825. AAAIPress. Yun He, Huaixiu Steven Zheng, Yi Tay, Jai PrakashGupta, Yu Du, Vamsi Aribandi, Zhe Zhao, YaGuangLi, Zhao Chen, Donald Metzler, Heng-Tze Cheng,and Ed H. Chi. 2022a. Hyperprompt: Prompt-basedtask-conditioning of transformers. In InternationalConference on Machine Learning, ICML 2022, 17-23July 2022, Baltimore, Maryland, USA, volume 162 ofProceedings of Machine Learning Research, pages86788690. PMLR. Yun He, Huaixiu Steven Zheng, Yi Tay, Jai PrakashGupta, Yu Du, Vamsi Aribandi, Zhe Zhao, YaGuangLi, Zhao Chen, Donald Metzler, Heng-Tze Cheng,and Ed H. Chi. 2022b. Hyperprompt: Prompt-basedtask-conditioning of transformers. In InternationalConference on Machine Learning, ICML 2022, 17-23July 2022, Baltimore, Maryland, USA, volume 162 ofProceedings of Machine Learning Research, pages86788690. PMLR. Edward J. Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. 2022. Lora: Low-rank adaptation oflarge language models. In The Tenth InternationalConference on Learning Representations, ICLR 2022,Virtual Event, April 25-29, 2022. OpenReview.net.",
  "Shibo Jie, Haoqing Wang, and Zhi-Hong Deng. 2023": "Revisiting the parameter efficiency of adaptersfrom the perspective of precision redundancy. InIEEE/CVF International Conference on ComputerVision, ICCV 2023, Paris, France, October 1-6, 2023,pages 1717117180. IEEE. Mingyu Jin, Haochen Xue, Zhenting Wang, BomingKang, Ruosong Ye, Kaixiong Zhou, Mengnan Du,and Yongfeng Zhang. 2024a. ProLLM: Protein chain-of-thoughts enhanced LLM for protein-protein inter-action prediction. In First Conference on LanguageModeling. Mingyu Jin, Qinkai Yu, Jingyuan Huang, QingchengZeng, Zhenting Wang, Wenyue Hua, Haiyan Zhao,Kai Mei, Yanda Meng, Kaize Ding, Fan Yang,Mengnan Du, and Yongfeng Zhang. 2024b.Ex-ploring concept depth: How large language mod-els acquire knowledge at different layers?CoRR,abs/2404.07066.",
  "and Yanda Meng. 2024c.Health-llm: Personal-ized retrieval-augmented disease prediction system.CoRR, abs/2402.00746": "Mingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao,Wenyue Hua, Yanda Meng, Yongfeng Zhang, andMengnan Du. 2024d. The impact of reasoning steplength on large language models. In Findings ofthe Association for Computational Linguistics, ACL2024, Bangkok, Thailand and virtual meeting, Au-gust 11-16, 2024, pages 18301842. Association forComputational Linguistics. Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, andWeidi Xie. 2022a. Prompting visual-language mod-els for efficient video understanding. In ComputerVision - ECCV 2022 - 17th European Conference, TelAviv, Israel, October 23-27, 2022, Proceedings, PartXXXV, volume 13695 of Lecture Notes in ComputerScience, pages 105124. Springer. Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, andWeidi Xie. 2022b. Prompting visual-language mod-els for efficient video understanding. In ComputerVision - ECCV 2022 - 17th European Conference, TelAviv, Israel, October 23-27, 2022, Proceedings, PartXXXV, volume 13695 of Lecture Notes in ComputerScience, pages 105124. Springer. Muhammad Uzair Khattak, Hanoona Abdul Rasheed,Muhammad Maaz, Salman H. Khan, and Fahad Shah-baz Khan. 2023. Maple: Multi-modal prompt learn-ing. In IEEE/CVF Conference on Computer Visionand Pattern Recognition, CVPR 2023, Vancouver,BC, Canada, June 17-24, 2023, pages 1911319122.IEEE.",
  "Brian Lester, Rami Al-Rfou, and Noah Constant. 2021": "The power of scale for parameter-efficient prompttuning. In Proceedings of the 2021 Conference onEmpirical Methods in Natural Language Processing,EMNLP 2021, Virtual Event / Punta Cana, Domini-can Republic, 7-11 November, 2021, pages 30453059. Association for Computational Linguistics. Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang,Linjie Li, Lijuan Wang, and Jianfeng Gao. 2024a.Multimodal foundation models: From specialists togeneral-purpose assistants. Found. Trends Comput.Graph. Vis., 16(1-2):1214.",
  "Hanwang Zhang, and Yueting Zhuang. 2024b. Fine-tuning multimodal llms to follow zero-shot demon-strative instructions. Preprint, arXiv:2308.04152": "Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H.Hoi. 2022. BLIP: bootstrapping language-image pre-training for unified vision-language understandingand generation. In International Conference on Ma-chine Learning, ICML 2022, 17-23 July 2022, Balti-more, Maryland, USA, volume 162 of Proceedingsof Machine Learning Research, pages 1288812900.PMLR. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang,Wayne Xin Zhao, and Ji-Rong Wen. 2023. Eval-uating object hallucination in large vision-languagemodels. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Process-ing, EMNLP 2023, Singapore, December 6-10, 2023,pages 292305. Association for Computational Lin-guistics.",
  "Haotian Liu, Chunyuan Li, Yuheng Li, and Yong JaeLee. 2023b. Improved baselines with visual instruc-tion tuning. CoRR, abs/2310.03744": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong JaeLee. 2023c. Visual instruction tuning. In Advancesin Neural Information Processing Systems 36: An-nual Conference on Neural Information ProcessingSystems 2023, NeurIPS 2023, New Orleans, LA, USA,December 10 - 16, 2023. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,Hiroaki Hayashi, and Graham Neubig. 2023d. Pre-train, prompt, and predict: A systematic survey ofprompting methods in natural language processing.ACM Comput. Surv., 55(9):195:1195:35.",
  "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll L. Wainwright, Pamela Mishkin, Chong": "Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray,John Schulman, Jacob Hilton, Fraser Kelton, LukeMiller, Maddie Simens, Amanda Askell, Peter Welin-der, Paul F. Christiano, Jan Leike, and Ryan Lowe.2022. Training language models to follow instruc-tions with human feedback. In Advances in NeuralInformation Processing Systems 35: Annual Confer-ence on Neural Information Processing Systems 2022,NeurIPS 2022, New Orleans, LA, USA, November 28- December 9, 2022. Adam Paszke, Sam Gross, Francisco Massa, AdamLerer, James Bradbury, Gregory Chanan, TrevorKilleen, Zeming Lin, Natalia Gimelshein, LucaAntiga, Alban Desmaison, Andreas Kpf, Edward Z.Yang, Zachary DeVito, Martin Raison, Alykhan Te-jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,Junjie Bai, and Soumith Chintala. 2019. Pytorch: Animperative style, high-performance deep learning li-brary. In Advances in Neural Information ProcessingSystems 32: Annual Conference on Neural Informa-tion Processing Systems 2019, NeurIPS 2019, De-cember 8-14, 2019, Vancouver, BC, Canada, pages80248035.",
  "Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,Ning Dai, and Xuanjing Huang. 2020. Pre-trainedmodels for natural language processing: A survey.CoRR, abs/2003.08271": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,Gretchen Krueger, and Ilya Sutskever. 2021. Learn-ing transferable visual models from natural languagesupervision. In Proceedings of the 38th InternationalConference on Machine Learning, ICML 2021, 18-24July 2021, Virtual Event, volume 139 of Proceedingsof Machine Learning Research, pages 87488763.PMLR.",
  "Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, andYue Cao. 2023. EVA-CLIP: improved training tech-niques for CLIP at scale. CoRR, abs/2303.15389": "Qifan Wang, Yuning Mao, Jingang Wang, HanchaoYu, Shaoliang Nie, Sinong Wang, Fuli Feng, LifuHuang, Xiaojun Quan, Zenglin Xu, and DongfangLiu. 2023a. Aprompt: Attention prompt tuning forefficient adaptation of pre-trained language models.In Proceedings of the 2023 Conference on EmpiricalMethods in Natural Language Processing, EMNLP2023, Singapore, December 6-10, 2023, pages 91479160. Association for Computational Linguistics. Wenguan Wang, Cheng Han, Tianfei Zhou, and Dong-fang Liu. 2023b. Visual recognition with deep near-est centroids. In The Eleventh International Con-ference on Learning Representations, ICLR 2023,Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, RuoxiSun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, GuolongSu, Vincent Perot, Jennifer G. Dy, and Tomas Pfister.2022. Dualprompt: Complementary prompting forrehearsal-free continual learning. In Computer Vision- ECCV 2022 - 17th European Conference, Tel Aviv,Israel, October 23-27, 2022, Proceedings, Part XXVI,volume 13686 of Lecture Notes in Computer Science,pages 631648. Springer. Qiong Wu, Weihao Ye, Yiyi Zhou, Xiaoshuai Sun, andRongrong Ji. 2024a.Not all attention is needed:Parameter and computation efficient transfer learn-ing for multi-modal large language models. CoRR,abs/2403.15226. Zichen Wu, Hsiu-Yuan Huang, Fanyi Qu, and YunfangWu. 2024b. Mixture-of-prompt-experts for multi-modal semantic understanding. In Proceedings ofthe 2024 Joint International Conference on Computa-tional Linguistics, Language Resources and Evalua-tion, LREC/COLING 2024, 20-25 May, 2024, Torino,Italy, pages 1138111393. ELRA and ICCL.",
  "Zhiyang Xu, Ying Shen, and Lifu Huang. 2023. Multiin-struct: Improving multi-modal zero-shot learning via": "instruction tuning. In Proceedings of the 61st AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), ACL 2023, Toronto,Canada, July 9-14, 2023, pages 1144511465. Asso-ciation for Computational Linguistics. Liqi Yan, Cheng Han, Zenglin Xu, Dongfang Liu, andQifan Wang. 2023. Prompt learns prompt: Exploringknowledge-aware generative prompt collaborationfor video captioning. In Proceedings of the Thirty-Second International Joint Conference on ArtificialIntelligence, IJCAI 2023, 19th-25th August 2023,Macao, SAR, China, pages 16221630. ijcai.org. Hao Yang, Junyang Lin, An Yang, Peng Wang, andChang Zhou. 2023a. Prompt tuning for unified mul-timodal pretrained models. In Findings of the As-sociation for Computational Linguistics: ACL 2023,Toronto, Canada, July 9-14, 2023, pages 402416.Association for Computational Linguistics. Li Yang, Qifan Wang, Jingang Wang, Xiaojun Quan,Fuli Feng, Yu Chen, Madian Khabsa, Sinong Wang,Zenglin Xu, and Dongfang Liu. 2023b. Mixpave:Mix-prompt tuning for few-shot product attributevalue extraction.In Findings of the Associationfor Computational Linguistics: ACL 2023, Toronto,Canada, July 9-14, 2023, pages 99789991. Associa-tion for Computational Linguistics.",
  "Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, QingQu, Yong Jae Lee, and Yi Ma. 2023. Investigating thecatastrophic forgetting in multimodal large languagemodels. CoRR, abs/2309.10313": "Chong Zhang, Xinyi Liu, Mingyu Jin, ZhongmouZhang, Lingyao Li, Zhenting Wang, Wenyue Hua,Dong Shu, Suiyuan Zhu, Xiaobo Jin, Sujian Li,Mengnan Du, and Yongfeng Zhang. 2024. When AImeets finance (stockagent): Large language model-based stock trading in simulated real-world environ-ments. CoRR, abs/2407.18957. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tian-wei Zhang, Fei Wu, and Guoyin Wang. 2023. In-struction tuning for large language models: A survey.CoRR, abs/2308.10792. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,Xiaolei Wang, Yupeng Hou, Yingqian Min, Be-ichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,Chen Yang, Yushuo Chen, Zhipeng Chen, JinhaoJiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang",
  "Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.2023. A survey of large language models. CoRR,abs/2303.18223": "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,Joseph E. Gonzalez, and Ion Stoica. 2023. Judgingllm-as-a-judge with mt-bench and chatbot arena. InAdvances in Neural Information Processing Systems36: Annual Conference on Neural Information Pro-cessing Systems 2023, NeurIPS 2023, New Orleans,LA, USA, December 10 - 16, 2023.",
  "S1Data Statistics": "Table S1 shows details of 9 multimodal datasetsfor our finetuning and evaluation. Vision-Flan (Xuet al., 2024b) includes 191 different multimodaltasks which is ideal for our finetuning process.Each multimodal tasks contains up to 1,000 in-stances, resulting in a total of 191,105 instances.MME (Yin et al., 2023) serves as our comprehen-sive multimodal evaluation benchmark, measuringboth perception and cognition capabilities across14 subtasks. We further leverage 7 multimodaldatasets for our evaluation. Specifically, for Op-tical Character Recognition, we utilize the Text-VQA (Singh et al., 2019), and for reasoning, we em-ploy the Visual Spatial Reasoning (VSR) (Liu et al.,2023a). Following (Zhai et al., 2023; Shen et al.,2024), the perception capability is tested on CIFAR-10/100 (Krizhevsky et al., 2009) and MNIST (Deng,2012). SNLI-VE (Xie et al., 2019) evaluates VisualEntailment capabilities, while the POPE (Li et al.,2023) dataset examines the tendency towards ob-ject hallucination. The MME metric is the sum ofaccuracy values across all subtasks, while for theother 7 multimodal evaluation datasets, the metricused is just accuracy.",
  "Vision-Flan191KDiverseMME2374DiverseText-VQA5000OCRVSR1222ReasoningSNLI-VE17901EntailmentCIFAR-1010000PerceptionCIFAR-10010000PerceptionMNIST10000PerceptionPOPE9000Object Hallucination": "employ LLaVA with Vicuna-7B-v1.3 as the baseLLM and CLIP-L as the vision encoder for all vari-ants. The finetuning process for M2PT10/20 takesapproximately 9 hours on 4 A100 GPUs (40G),with a batch size of 8 per GPU and a gradient ac-cumulation step of 4 (128 in total), with the samedata preprocess and normalize method as LLaVA.Additional configurations of M2PT are shown inTable S2. For LoRA, we directly import the bestresults from (Shen et al., 2024). For the prompttuning baselines, APrompt (Wang et al., 2023a) andPTUM (Yang et al., 2023a) add textual/attentionprompts into the LLM, while VPT (Han et al.,2024b) only appends visual prompts to the visionencoder. We use the optimal settings in the originalpapers to train their models, with grid search on thebest learning rate. For the other configuration, weadopt LLaVAs (Liu et al., 2023c) default settingsas provided in its codebase.",
  "S3Evaluation Metrics": "For evluation, we utilize MME (Yin et al., 2023)and the other 7 multimodal datasets (see S1). ForMME, we employ the official evaluation tool (Yinet al., 2023) of MME, including the Perceptionand Cognition metrics. For the other 7 multimodaldatasets, following (Shen et al., 2024), we employthe same prompt template to guide Vicuna-13B-v1.5 (Zheng et al., 2023) in evaluating the accuracy of each prediction, considering the specified taskinstructions and the groundtruth target output. Alltasks are classification tasks and we calculated thefinal score of each multimodal dataset based on thepercentage of vicuna 13b answering Yes.",
  "S4More Case Study": "To further investigate the models performance anddelineate instances of its suboptimal functioning,we conduct an in-depth visual assessment of asample cohort drawn from eight distinct Zero-Shotdatasets, as illustrated in Fig. S1. This visualizationfacilitates a comprehensive understanding of themodels efficacy across a diverse array of tasks anddata, while concurrently revealing potential con-straints inherent to the model and the underlyingcauses of its occasional shortcomings. From thelisted failure cases, we summarize 2 failure pat-terns, which are: (a) Small objects perception fail-ure in Text-VQA, CIFAR-10, MNIST. Small targetsin images pose a challenge to perception, impactingthe quality of VQA. Further research is essential toenhance accuracy in diverse contexts. (b) Semanticsimilar failure in CIFAR-10, MNIST, POPE, theinability to distinguish between semantically sim-ilar objects results in MLLMs generating wronganswers. Developing methods that can effectivelydifferentiate between similar objects is essential forreal-world applications. This involves enhancingthe models capacity to learn fine-grained featuresand contextual information, thereby improving itsoverall accuracy and robustness.",
  "S5Discussion with Previous Works": "This section provides discussion that connectsM2PT with previous methods. If we remove thetextual prompts and the interaction layer, our modelarchitecture degenerates to the visual prompt tun-ing approaches (Jia et al., 2022; Han et al., 2024b,2023). If we completely freeze the vision encoderby only introducing the textual prompts, our modelis similar to those traditional prompt tuning meth-ods (Lester et al., 2021; Ma et al., 2022; Yang et al.,2023a) in LLM. Moreover, if we further incorpo-rate the attention prompts in the LLM, our model isclose to the APrompt approach (Wang et al., 2023a;Han et al., 2023). Nevertheless, very limited workfocuses on the efficient tuning of multimodal largelanguage models.",
  "S6Prompt Initialization": "Table S3 reports the performance of M2PT withrespect to x widely adopted initialization methods:Xavier (Glorot and Bengio, 2010) and random onMME. The results show that Xavier generally pro-vides more stable and preferable performances. Inconclusion, M2PT shows robustness on differentinitialization methods and is able to achieve com-parable performance with full finetuning.",
  "new challenges and unveils some intriguing ques-tions. For instance, incorporating an advanced net-work into M2PT to search the optimal combina-tions of prompt lengths (i.e., Visual Prompt, Tex-": "tual Prompt) might significantly reduce the searchspace of lengths and lead to further performancegains. Another essential future direction is the de-sign and analysis of network interpretability (Arri-eta et al., 2020; Laugel et al., 2019; Rudin, 2019)and ad-hoc explainability (Biehl et al., 2016; Wanget al., 2023b), which limits current adoption ofM2PT in decision-critical, real-world applications.Overall, we believe the results presented in thispaper warrant further exploration."
}