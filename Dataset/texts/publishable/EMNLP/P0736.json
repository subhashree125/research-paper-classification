{
  "Abstract": "In this paper, we explore the utility of Trans-lationese as synthetic data created using ma-chine translation for pre-training language mod-els (LMs) for low-resource languages (LRLs).Our simple methodology consists of translat-ing large amounts of web-crawled monolingualdocuments (clean) into the LRLs, followed byfiltering the translated documents using tinyLMs trained on small but clean LRL data. Tak-ing the case of Indian languages, we pre-trainLMs from scratch with 28M and 85M parame-ters, and then fine-tune them for 5 downstreamnatural language understanding (NLU) and 4generative (NLG) tasks. We observe that pre-training on filtered synthetic data leads to rela-tive performance drops of only 0.87% for NLUand 2.35% for NLG, compared to pre-trainingon clean data, and this gap further diminishesupon the inclusion of a small amount of cleandata. We also study the impact of synthetic datafiltering and the choice of source language forsynthetic data generation. Furthermore, evalu-ating continually pre-trained larger models likeGemma-2B and Llama-3-8B in few-shot set-tings, we observe that using synthetic data iscompetitive with using clean data. Our findingssuggest that synthetic data shows promise forbridging the pre-training gap between Englishand LRLs.",
  "Introduction": "Large language models (LLMs) (Brown et al.,2020; Luccioni et al., 2023; Almazrouei et al.,2023; Lin et al., 2022) have been able to per-form very well on downstream tasks like MMLU(Hendrycks et al., 2021), Big-Bench (Srivastavaet al., 2022), etc, and have even started to reachhuman potential in many of these tasks. But thisperformance has very largely been credited to theirscale and the vast amount of data that they havebeen fed. Most of these language models (LMs)perform well in languages like English where abun-dant data is available (Kudugunta et al., 2023), but",
  "EnglishHindiGujarati": ": Comparing NLU performance in English,Hindi, and Gujarati shows that filtering synthetic dataand adding 10% clean data improves models, approach-ing the performance of those trained only on clean webdata. a vast majority of languages dont have comparabledata as compared to English. As a consequence,many LLMs, both monolingual and multilingual,involving these languages still show poor perfor-mance for various downstream tasks. For exam-ple, the largest open source multilingual modelBLOOM (Luccioni et al., 2023) covers 46 naturallanguages spanning 9 language families, but the top5 languages comprise 74.14% of the data. Even formodels like mT5 (Xue et al., 2021), the top 10 of107 languages account for more than 75.48% of thetraining data. Despite the benefits of multilingual-ism (Dabre et al., 2020), this data skew still meansthat low-resource languages will underperform. Fortunately synthetic data is an option and pre-vious works such as, but not limited to, back-translation (Sennrich et al., 2016a), sequence dis-tillation (Kim and Rush, 2016), also known as for-ward translation, etc. have shown that syntheticdata obtained using machine translation (MT) cansupplement resource scarcity and can significantlyenhance model performance (Popovic et al., 2020; Gala et al., 2023). However, to the best of ourknowledge, there has been no work on showingthe effectiveness of synthetic data for pre-trainingLMs. Furthermore, the quality of synthetic data isalso important, which many works take for granted.While round-trip-translation (Moon et al., 2020) orreferenceless neural quality estimation (QE) (Reiet al., 2021) are viable, they either involve twice thecompute or a reasonably large model not availablefor most languages, and this might not be optimalto determine the quality of synthetic documentsefficiently. We thus consider TinyLMs (Eldan andLi, 2023) as an efficient alternative, which havebeen shown to model documents by their fluentparagraph generation capabilities.In this paper, we focus on Indic languages suchas Hindi, Gujarati, and Marathi, and present a com-prehensive study of the utility of synthetic mono-lingual data, also called translationese (Gellerstam,1986), obtained using machine translation (MT) forpre-training LMs. We propose a simple frame-work that involves training tiny language mod-els, henceforth TinyLMs, on original web-crawleddata (clean) and then using them to filter syntheticdata. We then compare LMs of different scales pre-trained on clean and synthetic data followed by fine-tuning on natural language understanding (NLP)and generation (NLG) downstream tasks, wherewe observe that, while unfiltered synthetic databased LMs are inferior compared to LMs trainedon clean data, filtering leads to performance com-parable to the latter. We further show that tuningthese synthetic data LMs on small clean data leadsto further improvements. We also show that thesetrends hold when continually pre-training LLMssuch as Gemma-2B and Llama-3-8B.Our contributions are:a. A simple framework involving high-quality MTmodels and TinyLMs trained on clean web-crawleddata to mass-produce and filter synthetic data forLM training.b. Demonstrating the efficacy of language models(up to Llama-3-8B) trained on filtered syntheticdata across a range of NLU and NLG tasks for lowresource Indic languages.c. A new document-level monolingual corpora (In-dicMonoDoc) consisting of 39.5B tokens worth ofmonolingual clean document-level data spanning22 scheduled languages and English1.",
  "This paper focuses on creating, filtering, and utiliz-ing synthetic data to train TinyLMs": "Monolingual Data: Previous efforts to collectmonolingual corpora for Indic languages includethe EMILLE/CIIL corpus (McEnery et al., 2000),HindMonoCorp (Bojar et al., 2014), Leipzig cor-pus (Goldhahn et al., 2012), IndicCorpv1 (Kak-wani et al., 2020a), and IndicCorpv2 (Doddapaneniet al., 2023). While IndicCorpv2 is large, it issentence-level and suitable for NLU models butnot for longer contexts needed by language mod-els. We extend these corpora and demonstrate theeffectiveness of using synthetic data. Synthetic Data Generation and Quality Esti-mation: Synthetic data aids NLP tasks like backtranslation for machine translation (Sennrich et al.,2016a; Edunov et al., 2018; Marie et al., 2020;Bogoychev and Sennrich, 2019; Ni et al., 2022)and native language identification (Goldin et al.,2018).However, using synthetic data for pre-training LMs is less explored due to hallucination(Maynez et al., 2020) and ungrounded text (Thorneet al., 2018). Evaluation methods like RTT BLEUscores are computationally intensive, while oth-ers like BARTScore (Yuan et al., 2021), T5Score(Qin et al., 2023), MQM, and COMET (Rei et al.,2020) require large-scale models or human annota-tions, limiting scalability. Approaches like KenLM(Heafield, 2011) have been used to filter monolin-gual corpora based on perplexity. Transfer Learning and Cross-Lingual Fine-Tuning:Approaches like translate-train,asdescribed by Conneau et al. (2018), involvefine-tuning a multilingual PLM using machine-translated training data and evaluating in the targetlanguage. Oh et al. (2022) combined translate-trainand translate-test for improved cross-lingual fine-tuning. In contrast, our work focuses on pretraininglanguage models and exploring how synthetic textimpacts pretraining and various downstream NLUand NLG tasks. TinyLMs: Small LMs, even with 10M parameters,produce fluent and consistent text (Eldan and Li,2023). Challenges like BabyLM (Warstadt et al.,2023) focus on improving LMs within a fixed databudget. We take motivation from this and leverageTinyLMs for efficient filtering of synthetic docu-ments. : Overview of our approach to pre-train language models using translationese data. We leverage richmonolingual corpora in the src language and scarce corpora in the tgt language. Our method involves employing apre-trained machine translation model to translate src to tgt. We then filter, using perplexity, the resulting text usinga TinyLM trained solely on clean tgt monolingual data. The filtered synthetic data can be used to further pretrainlarger language models.",
  "Methodology": "In this section, we describe our framework forleveraging synthetic data for LM training. Thisprocess consists of collecting monolingual (clean)data from the web for low-resource languages,training TinyLMs with it, translating clean datafrom a high resource language such as English intolow-resource languages, using the aforementionedTinyLMs to filter synthetic data, and then usingthis filtered data to train LMs for downstream tasks.Our framework is illustrated in .",
  "Collecting Clean Monolingual Corpora": "Following Doddapaneni et al. (2023); Rae et al.(2022); Team et al. (2022), for all languages of in-terest, we a. obtain a list of URLs to be crawled viaword-level n-grams passed to a search engine, b.after URL deduplication, we crawl all applicablewebpages, c. automatically and manually (OrtizSurez et al., 2019; Abadji et al., 2022) filter outunwanted text like HTML tags and emoticons, d.use language detection-based (LID) filtering usingcld32 and IndicLID-FTN model (Madhani et al.,2023a) to discard languages not of interest, e. per-form document filtering to drop documents contain-ing offensive text using toxic words list providedby Team et al. (2022), f. merge all the filtered cor-pus with Wikipedia, OSCAR (Ortiz Surez et al.,2019) and some dumps of mC4 (Xue et al., 2021)and finally, g. perform deduplication at paragraphlevel using Murmurhash algorithm3 with a 128-bitunsigned hash for each monolingual split of thecorpora.We crawl data for English, with Indic context,and 22 Indic languages. As a result, we end up withIndicMonoDoc, with 27.5 billion tokens worth ofIndic language documents and 12 billion tokensof English documents for a total of 39.5 billion to-kens of clean monolingual data. This is larger thanthe corpora released by Doddapaneni et al. (2023),surpassing it by almost 2 times. We use IndicMon- oDoc for all experiments with clean data. gives an overview of the comparison of IndicMon-oDoc. Note that, creation of IndicMonoDoc isimportant since IndicCorpV2 is a sentence-levelcorpus, and training LMs need a document-levelcorpus. It is important to note that we paid specialattention to the low-resource languages. In thispaper we only use data corresponding to English,Hindi, Gujarati and Marathi. We report additionaldetails of IndicMonoDoc in Appendix E.",
  "Generating Translationese (Synthetic)": "We utilize state-of-the-art MT models like Indic-Trans2 (Gala et al., 2023) to generate translationesedata. Using beam search with a beam value of 5,we translate English tokens from the clean corpusto the target languages. Due to token limits in MTmodels, we split documents using the Moses Sen-tence Splitter4 for sentence-level translations, thenmerge them back into documents. We use the 1BEn-Indic version5 of IndicTrans2 to translate 5BEnglish tokens worth of documents from IndicMon-oDoc into translationese data for Hindi, Marathiand Gujarati.",
  "Tiny Language Models (TinyLMs)": "TinyLMs are small language models inspired byEldan and Li (2023). We use the Transformer ar-chitecture (Vaswani et al., 2017) and train themwith clean monolingual documents. RoPE embed-dings (Su et al., 2023) are used instead of learnedpositional encodings for handling long documents.Following Chinchilla scaling laws (Hoffmann et al.,2022), we use compute-optimal word tokens. Al-though it is plausible to train a TinyLM on unfil-tered translationese data to filter itself, our prelim-inary experiments revealed that they favor poor-quality data and hence we avoid this route.",
  "ilog p (wi | w<i)": "where the negative log-likelihood measures the er-ror of the models predictions. While calculat-ing perplexity over a sequence of tokens, W w1, w2, . . . , wN we skip the first s tokens wheres = 10, e = 1024 and calculate loss until only thefirst e tokens of the document. We find setting eto larger values can lead to higher variance in thedocument scores due to the size of the TinyLM.Following initial analyses, we choose s and e suchthat we remove the high uncertainty of the languageat the start of an unseen document and avoid pe-nalizing longer documents due to the fragility ofthe extrapolation ability of TinyLM6. Note that it isimportant to choose e such that the language modelgives a uniform estimate of perplexity over an al-ready seen sequence of tokens ws, ws+1, . . . , we.For our experiments, we use the TinyLMs to scoreall synthetically generated translationese data andcalculate a document score using the above method.Following, Laurenon et al. (2022) we do subsam-pling by thresholding document perplexity scoresexcept Laurenon et al. (2022) did it using Ken-LM(Heafield, 2011) and we do it using our TinyLM.We keep the threshold value such that we includeenough documents to reach the computed optimaltoken count for pretraining experiments.",
  "Experiments": "This section outlines the training procedures anddatasets for the models described in . Wepre-train decoder only LMs and fine-tune all mod-els from scratch in monolingual and bilingual set-tings using the causal language modeling (CLM)objective for NLG tasks and a linear classificationhead for classification tasks. We specify the datasetsamples used for pretraining and fine-tuning, andanalyze the effects of synthetic corpora on pretrain-ing.",
  "Pretraining Data Settings": "We refer to translated text or translationese as syn-thetic or syn and original or web-crawled data asclean throughout our experiments. For the pre-training of all base models, we use the followingnaming convention to denote our training splits foreach model:XX-clean: This is a clean subset sampled randomlyfrom IndicMonoDoc where XX represents the lan-guage English (EN), Hindi (HI) or Gujarati (GU).",
  "During experiments we saw that these TinyLMs can onlygo up to a certain context length before deteriorating in quality": "syn-XX_yy-unfiltered: Denotes synthetic mono-lingual documents in XX language generated byusing yy as a source during translation.syn-XX_yy-filtered: Filtered synthetic data.+10%: Refers to extended pretraining on a cleanedsubset of IndicMonoDoc with an additional 10%tokens compared to regular training.BI-XX-YY Prefix:Denotes bilingual modelstrained using an equal mixture of monolingual cor-pora in XX and YY languages. We append an _synprefix to either XX or YY if a synthetic versionof that language is employed in training, and a-parallel/nonparallel tag to denote whether a paral-lel version of XX and YY are used or not.Note, for each split we only use the number oftokens that are required to reach the point of op-timality (Hoffmann et al., 2022) by the languagemodel.",
  "Implementation and Training": "Tokenizer: We use a common byte-pair-encoding(BPE) (Sennrich et al., 2016b) tokenizer using Sen-tencepiece7 for all experiments. We train a sharedvocabulary of 56k subwords between three lan-guages, English, Hindi, and Gujarati by using 5Million randomly sampled sentences per languageand upsampling for Gujarati.TinyLMs: We use Pytorch Lightning8 for our im-plementations and train TinyLMs as described in.3 for filtering. We use hidden sizes of 768and have two variants, one with 4 layers (mini) andone with 12 layers (base; same as GPT2-base) with28M and 85M non-embedding parameters respec-tively. The mini models are trained on clean datawith sequence lengths of 40969 (mini-4k) for filter-ing synthetic documents as described in .4.On the other hand, for our main pre-training anddownstream fine-tuning experiments, we train miniand base models with sequence lengths of 1024(mini-1k and base-1k). Following Hoffmann et al.(2022) we use 2.4 billion word tokens per languageto compute optimal training of base models. SinceGujarati has only 900M tokens in our dataset, when-ever Gujarati is involved as the target, we train onlythe mini-1k model. For models involving Englishand Hindi, we train both mini and base models.Additional details are in Appendix B.",
  "Downstream Tasks and Evaluation": "We finetune the mini-1k and base-1k models forclassification, regression, and generation tasks. Hy-perparameter tuning is performed using the vali-dation set for models trained only with clean data,and this process is repeated for different data splits.More details on hyperparameters and evaluationcan be found in Appendix B. Primary scores arereported on IndicGLUE (Kakwani et al., 2020a)and IndicXNLI (iXNLI) (Aggarwal et al., 2022)for Hindi and Gujarati, and the GLUE benchmarkvalidation set (Wang et al., 2018) for English. Wealso experiment with other generation tasks likeCNN-Dailymail (Nallapati et al., 2016), Dailog-Sum (Chen et al., 2021), XL-Sum (Hasan et al.,2021), IndicNLG (Kumar et al., 2022), FLoRes-200 (Team et al., 2022), IN22-Conv & IN22-Gen(Gala et al., 2023) and use standard evaluation met-rics suitable for each task like accuracy, f1-score,Rouge-L (Lin, 2004) and chrF++ (Popovic, 2017).Further details about each of the evaluation datasetscan be found in Appendix B.1.",
  "Main Results": "In this section, we present results for Hindi, Gu-jarati, and English language models trained onclean data, as well as synthetic data generated fromtranslations. We demonstrate the impact of filter-ing and adding additional clean data for extendedpretraining of LMs trained solely on synthetic text.Additionally, we observe the effect of using theclean source text along with its translations (syn-thetic parallel documents) on downstream tasks.We follow the naming convention for different datasplits as specified in . We provide detailsfor the pretraining of each model in Appendix B.We provide additional results in Appendix A.Filtered Synthetic Data is Competitive withWeb Scraped Data: The results in and2 indicate that syn-HI_en-unfiltered, syn-GU_en-unfiltered, and syn-EN_hi-unfiltered exhibit lowerdownstream performance compared to their fil-tered counterparts: syn-HI_en-filtered, syn-GU_en-filtered, and syn-EN_hi-filtered, respectively. It isevident that filtering the synthetic documents usingTinyLMs significantly improves the performanceof both NLU and NLG tasks. We also observe",
  "syn-GU_en-filtered+10%68.0492.4180.2317.6213.1615.0015.26": ": Results for Hindi and Gujarati: NLU/NLG tasks on base-1k (Hindi) and mini-1k (Gujarati) models ondifferent clean and synthetic splits. Test accuracy for NLU tasks; Rouge-L F1 scores for NLG tasks. Bold valuesrepresent the best amongst synthetic splits. that for tasks like CoLA (Warstadt et al., 2019),language models trained solely on synthetic datalag behind when compared to other tasks as seen in of Appendix A. This suggests that syntheticcorpora may lack certain important elements neces-sary for language models to perform competitivelyin linguistic acceptability tasks, as opposed to LMstrained on clean, non-synthetic corpora. Resultsfor base-1k for English are presented in in Appendix A because we focus our attention onIndic languages. Fine-tuning on small amounts of Web ScrapedData boosts performance: Even after filtering,we observe that language models trained solely onsynthetic text slightly underperform LMs trainedon clean text. To address this issue, we conduct ex-tended pretraining of LMs using clean data sourcedfrom IndicMonoDoc. The objective is to determineif this additional training improves performance.We only incorporate an additional 10% of cleandata compared to the LMs previous training data.We see these results across all three languages, andfor Hindi and Gujarati, we see that by incorporat-ing even a small amount of clean data, we observean increase in performance on downstream tasks,bringing the LM at par or closer to the performanceof a clean LM. We see an improvement in LMstrained using unfiltered synthetic corpora as wellbut we believe that filtering leads to the removal ofnoisy documents and thus better performance. Weobserve improved performance in language mod-els (LMs) trained with unfiltered synthetic corpora, but filtering out noisy documents enhances perfor-mance further. Our ablation study ( in Ap-pendix A) investigates whether adding 10% moresynthetic data contributes to this improvement orif the data type is key. While performance gainscould stem from statistical variances, the consis-tency across nearly all downstream tasks suggestsotherwise.Impact of source language for synthetic datageneration: Choosing the right source languagefor synthetic corpora is crucial, as it influences thecharacteristics of the generated translationese text.We evaluate this using Hindi and Gujarati cleandocuments from IndicMonoDoc, translating theminto English. We use the 1B Indic-En version10 to translate 5B Hindi tokens and 900M11 Gujaratitokens into English. In , we see that thesynthetic text generated from Hindi achieves per-formance at par with the EN-clean model, whilethe synthetic text from Gujarati significantly lagsbehind. This is likely because Hindi is more maca-ronic than Gujarati, i.e., a lot of Hindi text from theweb consists of Hinglish, resulting in better trans-lationese text due to increased overlap betweenlanguages. This can also be due to the weakertranslations generated by the MT model. The per-formance gap is notable in tasks like STS bench-mark, NLI (qnli and mnli), and CoLA, suggesting",
  "Further Exploration": "Analysis of Synthetic Data: shows theperplexity mean and variance scores for TinyLMacross token positions in the documents.Thisshows that on unseen documents, TinyLM showshigher variance on English documents generatedby translating Gujarati documents from IndicMon-oDoc as compared to English clean and Englishsynthetic generated from Hindi. This also gives rea-son for the deterioration in results in dueto Gujarati documents. shows the distribu-tion of lengths of filtered documents by TinyLMsshowing that they do not add any bias for shorterdocuments during filtering.Impact of model size: Following and 3, wesee that even after scaling down we see consistent improvements for filtering and adding additionaldata, which empirically shows that indeed usingsynthetic text after filtering is a viable option forpretraining LMs of varying sizes. In wesee that after filtering and extended pretraining,synthetic text outperforms LMs trained on cleandocuments from the web in Hindi. This is alsosupported by our experiments on finetuning Llama-3-8B in .3.",
  ": Performance of English models on NLG tasks.All the results reported here are on base-1k and useRouge-L F1 scores": "Impact on NLG: Without extended pretraining,language models trained on synthetic text performas well as those trained on clean documents, sug-gesting that for NLG tasks, synthetic data sufficesfor pretraining, eliminating the need for clean data.This trend is evident across Hindi, Gujarati, andEnglish NLG results (Tables 1 and 4). As their per-formance matches models trained on clean data, werefrain from extended pretraining for NLG tasks,focusing primarily on abstractive summarizationfor evaluating generation capabilities.",
  "Scaling to Llama-3-8B": "To show the effect of using translationese on largermodels, we select Llama-3-8B12 and Gemma-2B(Team et al., 2024) and perform continual pretrain-ing over clean and synthetic data to improve abilityover the low resource target language. We takeMarathi as a replacement for Hindi for scaling ex-periments since data for Hindi is abundantly avail-able and existing models already have a good lan-guage understanding of Hindi making it harder tocompare the effects of utilizing clean vs syntheticdata. For a fair comparison, we limit each datasplit to 344M tokens for Gujarati and 465M tokensfor Marathi and follow a similar procedure as de-scribed in .4 to generate and filter data forMarathi. We perform extended training for a singleepoch using LoRA (Hu et al., 2021) finetuning onWq, Wv projection matrices using =16 and r=8.We keep the learning rate at 3e5 with a weightdecay of 0.01 and an effective batch size of 58k.Perplexity: We report average sentence level per-plexity on sentences from IN22-Conv and IN22-Gen (Gala et al., 2023) in . We see that fil-tered synthetic data for Gujarati outperforms cleandata, but for Marathi, it does not. This means thatfiltering improves performance at scale but relieson the quality of translationese in the target lan-guage. We report perplexities on individual testsets in Appendix A.",
  "synthetic-unfiltered92.81315.69710.9412.816synthetic-filtered104.14814.62210.1502.236": ": Average perplexity () of models trained onTranslationese vs. Clean data on IN22-Conv and IN22-Gen test sets shows improvement with large-scale mod-els. Bold represents best among synthetic data splits. Few Shot Prompting: We evaluate our contin-ually pre-trained models using few-shot prompt-ing on IndicSentiment classification (Doddapaneniet al., 2023) as the NLU task and EnIndic ma-chine translation on IN22-Gen and FloRes-200 asthe NLG task. Prompts used are shown in Ap-pendix B.5 with examples are randomly sampledfrom the validation set for FloRes and other ex-amples from the IN22-Gen test set, ensuring noexample is repeated in the prompts. We use a",
  ": Average accuracy with standard deviation (su-perscript) over 5 runs on 10-shot IndicSentiment classi-fication task. Bold represents the best among syntheticdata experiments": "beam width of 5 with early stopping enabled. Re-sults are shown in Tables 6 and 7. We see thatfiltering improves performance on MT when com-pared to synthetic splits, but IndicSentiment, showsonly marginal improvements. Nonetheless, modelstrained on filtered data show lower perplexity andbetter performance in few-shot settings, indicatingtheir promise. We leave the exploration of trainingmultilingual LLMs on large-scale translationesedata for future research.",
  "Conclusion": "In this paper, we performed a first of its kind studyshowing the promise of using translationese datafor training language models for low-resource lan-guages. Our simple pipeline involves the transla-tion of high-resource source language documentsat scale, followed by perplexity based filtering us-ing small and efficient language models trainedon clean target low-resource language data. Wethen showed on a variety of downstream naturallanguage understanding and generative tasks thatboth small and large language models pre-trainedon clean synthetic data are comparable to thosetrained on clean data. While we observed that thesource language for synthetic data generation mat-ters, it is clear that synthetic data can help bridgethe resource scarcity faced by a vast majority oflanguages for language modeling. Future work willfocus on better and faster synthetic data generationand filtering mechanisms.",
  "Acknowledgements": "We extend our thanks to the Department of Com-puter Science and Engineering at the Indian Insti-tute of Technology (IIT) Bombay for providingaccess to GPU servers, and to the Centre for De-velopment of Advanced Computing (C-DAC) forgranting us access to the Param Siddhi Supercom-puter. These computational resources were essen-tial for the successful completion of this work.",
  "We consider the following limitations of our work": "We show that synthetic data also helps forlarger models like Llama-3-8B but for evenlarger models above 100B parameters, effectsof translationese may be different. However,synthetic data generated from translations cansurely help fill knowledge gaps. Due to the extensive size of the test sets forIndicNLG tasks (Question Generation, Wik-iBio generation, Headline Generation, andSentence Summarization), we couldnt exper-iment with them in their entirety. However,since we already use 4000 examples per lan-guage, we anticipate that the overall trendsremain unchanged. We report GLUE validation set results for allmodels due to the large scale of our experi-ments, following existing practices. Our goalis to demonstrate synthetic data utility, not toachieve state-of-the-art results. Our framework heavily relies on the transla-tion models performance. Despite this de-pendency, we are confident that our approachwill significantly enhance the performance ofmid-resource languages, especially where thetranslation model is already proficient.",
  "Ethical Considerations": "As a part of this paper, we release monolingual andsynthetic data. While we have taken care to removeany toxic content, accidental occurrences may existand thus we exercise caution when using our datafor training language models as they may producetoxic outputs. Given that we have shown the utilityof synthetic data for training LMs, it should bepossible to mass produce synthetic toxic data invarious languages leading to LMs that can generate multilingual toxic content. However, this opens upresearch opportunities on how to detect and filtertoxic content from synthetically created data.We release the code and models with an MITLicense13. The dataset is released under a CC-0License14.",
  "Julien Abadji, Pedro Ortiz Suarez, Laurent Romary, andBenot Sagot. 2022. Towards a Cleaner Document-Oriented Multilingual Crawled Corpus.arXiv e-prints, page arXiv:2201.06642": "Divyanshu Aggarwal,Vivek Gupta,and AnoopKunchukuttan. 2022. IndicXNLI: Evaluating multi-lingual inference for Indian languages. In Proceed-ings of the 2022 Conference on Empirical Methods inNatural Language Processing, pages 1099411006,Abu Dhabi, United Arab Emirates. Association forComputational Linguistics. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-shamsi, Alessandro Cappelli, Ruxandra Cojocaru,Mrouane Debbah, tienne Goffinet, Daniel Hesslow,Julien Launay, Quentin Malartic, Daniele Mazzotta,Badreddine Noune, Baptiste Pannier, and GuilhermePenedo. 2023. The falcon series of open languagemodels.",
  "Nikolay Bogoychev and Rico Sennrich. 2019. Domain,translationese and noise in synthetic data for neuralmachine translation. CoRR, abs/1911.03362": "Ondrej Bojar, Vojtech Diatka, Pavel Rychl`y, PavelStrank, Vt Suchomel, Ales Tamchyna, and DanielZeman. 2014. Hindencorp-hindi-english and hindi-only corpus for machine translation. In LREC, pages35503555. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020.Language models are few-shot learners.In Ad-vances in Neural Information Processing Systems,volume 33, pages 18771901. Curran Associates,Inc.",
  "Raj Dabre, Chenhui Chu, and Anoop Kunchukuttan.2020. A survey of multilingual neural machine trans-lation. ACM Comput. Surv., 53(5)": "Daniel Deutsch and Dan Roth. 2020. SacreROUGE: Anopen-source library for using and developing sum-marization evaluation metrics. In Proceedings ofSecond Workshop for NLP Open Source Software(NLP-OSS), pages 120125, Online. Association forComputational Linguistics. Sumanth Doddapaneni, Rahul Aralikatte, GowthamRamesh, Shreya Goyal, Mitesh M Khapra, AnoopKunchukuttan, and Pratyush Kumar. 2023. Towardsleaving no indic language behind: Building mono-lingual corpora, benchmark and models for indiclanguages. In Proceedings of the 61st Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 1240212426. Sergey Edunov, Myle Ott, Michael Auli, and DavidGrangier. 2018. Understanding back-translation atscale. In Proceedings of the 2018 Conference onEmpirical Methods in Natural Language Processing,pages 489500, Brussels, Belgium. Association forComputational Linguistics.",
  "Yvette Graham, Barry Haddow, and Philipp Koehn.2019. Translationese in machine translation eval-uation. CoRR, abs/1906.09833": "Tahmid Hasan, Abhik Bhattacharjee, Md. Saiful Is-lam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang,M. Sohel Rahman, and Rifat Shahriyar. 2021. XL-sum: Large-scale multilingual abstractive summariza-tion for 44 languages. In Findings of the Associationfor Computational Linguistics: ACL-IJCNLP 2021,pages 46934703, Online. Association for Computa-tional Linguistics. Kenneth Heafield. 2011. KenLM: Faster and smallerlanguage model queries. In Proceedings of the SixthWorkshop on Statistical Machine Translation, pages187197, Edinburgh, Scotland. Association for Com-putational Linguistics. Dan Hendrycks, Collin Burns, Steven Basart, AndyZou, Mantas Mazeika, Dawn Song, and Jacob Stein-hardt. 2021. Measuring massive multitask languageunderstanding. In 9th International Conference onLearning Representations, ICLR 2021, Virtual Event,Austria, May 3-7, 2021. OpenReview.net. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,Elena Buchatskaya, Trevor Cai, Eliza Rutherford,Diego de Las Casas, Lisa Anne Hendricks, JohannesWelbl, Aidan Clark, Tom Hennigan, Eric Noland,Katie Millican, George van den Driessche, BogdanDamoc, Aurelia Guy, Simon Osindero, Karen Si-monyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,and Laurent Sifre. 2022. Training compute-optimallarge language models.",
  "Edward J. Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, and WeizhuChen. 2021. Lora: Low-rank adaptation of largelanguage models. CoRR, abs/2106.09685": "Divyanshu Kakwani, Anoop Kunchukuttan, SatishGolla, NC Gokul, Avik Bhattacharyya, Mitesh MKhapra, and Pratyush Kumar. 2020a. Indicnlpsuite:Monolingual corpora, evaluation benchmarks andpre-trained multilingual language models for indianlanguages. In Findings of the Association for Com-putational Linguistics: EMNLP 2020, pages 49484961. Divyanshu Kakwani, Anoop Kunchukuttan, SatishGolla, Gokul N.C., Avik Bhattacharyya, Mitesh M.Khapra, and Pratyush Kumar. 2020b. IndicNLPSuite:Monolingual corpora, evaluation benchmarks andpre-trained multilingual language models for Indianlanguages. In Findings of the Association for Com-putational Linguistics: EMNLP 2020, pages 49484961, Online. Association for Computational Lin-guistics. Yoon Kim and Alexander M. Rush. 2016. Sequence-level knowledge distillation. In Proceedings of the2016 Conference on Empirical Methods in Natu-ral Language Processing, pages 13171327, Austin,Texas. Association for Computational Linguistics. Diederik P. Kingma and Jimmy Ba. 2015. Adam: Amethod for stochastic optimization.In 3rd Inter-national Conference on Learning Representations,ICLR 2015, San Diego, CA, USA, May 7-9, 2015,Conference Track Proceedings. Sneha Kudugunta, Isaac Caswell, Biao Zhang, XavierGarcia, Christopher A Choquette-Choo, KatherineLee, Derrick Xin, Aditya Kusupati, Romi Stella,Ankur Bapna, et al. 2023. Madlad-400: A multilin-gual and document-level large audited dataset. arXivpreprint arXiv:2309.04662. Aman Kumar, Himani Shrotriya, Prachi Sahu, AmoghMishra, Raj Dabre, Ratish Puduppully, AnoopKunchukuttan, Mitesh M. Khapra, and Pratyush Ku-mar. 2022.IndicNLG benchmark: Multilingualdatasets for diverse NLG tasks in Indic languages.In Proceedings of the 2022 Conference on Empiri-cal Methods in Natural Language Processing, pages53635394, Abu Dhabi, United Arab Emirates. As-sociation for Computational Linguistics. Hugo Laurenon, Lucile Saulnier, Thomas Wang,Christopher Akiki, Albert Villanova del Moral, TevenLe Scao, Leandro Von Werra, Chenghao Mou, Ed-uardo Gonzlez Ponferrada, Huu Nguyen, et al. 2022.The bigscience roots corpus: A 1.6 tb composite mul-tilingual dataset. Advances in Neural InformationProcessing Systems, 35:3180931826.",
  "Chin-Yew Lin. 2004. ROUGE: A package for auto-matic evaluation of summaries. In Text Summariza-tion Branches Out, pages 7481, Barcelona, Spain.Association for Computational Linguistics": "Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, TianluWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-man Goyal, Shruti Bhosale, Jingfei Du, RamakanthPasunuru, Sam Shleifer, Punit Singh Koura, VishravChaudhary, Brian OHoro, Jeff Wang, Luke Zettle-moyer, Zornitsa Kozareva, Mona Diab, Veselin Stoy-anov, and Xian Li. 2022. Few-shot learning withmultilingual generative language models. In Proceed-ings of the 2022 Conference on Empirical Methodsin Natural Language Processing, pages 90199052,Abu Dhabi, United Arab Emirates. Association forComputational Linguistics. Ilya Loshchilov and Frank Hutter. 2019. Decoupledweight decay regularization. In 7th InternationalConference on Learning Representations, ICLR 2019,New Orleans, LA, USA, May 6-9, 2019. OpenRe-view.net.",
  "Yash Madhani,Mitesh M. Khapra,and AnoopKunchukuttan. 2023a. Bhasha-abhijnaanam: Native-script and romanized language identification for 22indic languages": "Yash Madhani, Sushane Parthan, Priyanka Bedekar,Gokul Nc, Ruchi Khapra, Anoop Kunchukuttan,Pratyush Kumar, and Mitesh Khapra. 2023b. Aksha-rantar: Open Indic-language transliteration datasetsand models for the next billion users. In Findingsof the Association for Computational Linguistics:EMNLP 2023, pages 4057, Singapore. Associationfor Computational Linguistics. Benjamin Marie, Raphael Rubino, and Atsushi Fujita.2020. Tagged back-translation revisited: Why doesit really work? In Proceedings of the 58th AnnualMeeting of the Association for Computational Lin-guistics, pages 59905997, Online. Association forComputational Linguistics. Joshua Maynez, Shashi Narayan, Bernd Bohnet, andRyan McDonald. 2020. On faithfulness and factu-ality in abstractive summarization. In Proceedingsof the 58th Annual Meeting of the Association forComputational Linguistics, pages 19061919, On-line. Association for Computational Linguistics. Anthony McEnery, Paul Baker, Robert Gaizauskas, andHamish Cunningham. 2000. Emille: Building a cor-pus of south asian languages. In Proceedings of theInternational Conference on Machine Translationand Multilingual Applications in the new Millennium:MT 2000. Jihyung Moon, Hyunchang Cho, and Eunjeong L. Park.2020. Revisiting round-trip translation for qualityestimation. In Proceedings of the 22nd Annual Con-ference of the European Association for MachineTranslation, pages 91104, Lisboa, Portugal. Euro-pean Association for Machine Translation.",
  "Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, BingXiang, et al. 2016. Abstractive text summarizationusing sequence-to-sequence rnns and beyond. arXivpreprint arXiv:1602.06023": "Jingwei Ni, Zhijing Jin, Markus Freitag, MrinmayaSachan, and Bernhard Schlkopf. 2022. Original ortranslated? a causal analysis of the impact of trans-lationese on machine translation performance. InProceedings of the 2022 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 53035320, Seattle, United States. Associationfor Computational Linguistics.",
  "Jaehoon Oh, Jongwoo Ko, and Se-Young Yun. 2022": "Synergy with translation artifacts for training andinference in multilingual tasks. In Proceedings ofthe 2022 Conference on Empirical Methods in Nat-ural Language Processing, pages 67476754, AbuDhabi, United Arab Emirates. Association for Com-putational Linguistics. Pedro Javier Ortiz Surez, Benot Sagot, and LaurentRomary. 2019. Asynchronous pipelines for process-ing huge corpora on medium to low resource infras-tructures. Proceedings of the Workshop on Chal-lenges in the Management of Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019, pages 9 16,Mannheim. Leibniz-Institut fr Deutsche Sprache. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalu-ation of machine translation. In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics, pages 311318, Philadelphia,Pennsylvania, USA. Association for ComputationalLinguistics. Maja Popovic. 2017. chrF++: words helping charac-ter n-grams. In Proceedings of the Second Confer-ence on Machine Translation, pages 612618, Copen-hagen, Denmark. Association for Computational Lin-guistics. Maja Popovic, Alberto Poncelas, Marija Brkic, andAndy Way. 2020. Neural machine translation fortranslating into Croatian and Serbian. In Proceedingsof the 7th Workshop on NLP for Similar Languages,Varieties and Dialects, pages 102113, Barcelona,Spain (Online). International Committee on Compu-tational Linguistics (ICCL). Yiwei Qin, Weizhe Yuan, Graham Neubig, and PengfeiLiu. 2023. T5Score: Discriminative fine-tuning ofgenerative evaluation metrics. In Findings of theAssociation for Computational Linguistics: EMNLP2023, pages 1518515202, Singapore. Associationfor Computational Linguistics.",
  "Ella Rabinovich and Shuly Wintner. 2015. Unsuper-vised identification of translationese. Transactions ofthe Association for Computational Linguistics, 3:419432": "Jack W. Rae, Sebastian Borgeaud, Trevor Cai, KatieMillican, Jordan Hoffmann, Francis Song, JohnAslanides, Sarah Henderson, Roman Ring, Susan-nah Young, Eliza Rutherford, Tom Hennigan, Ja-cob Menick, Albin Cassirer, Richard Powell, Georgevan den Driessche, Lisa Anne Hendricks, Mari-beth Rauh, Po-Sen Huang, Amelia Glaese, Jo-hannes Welbl, Sumanth Dathathri, Saffron Huang,Jonathan Uesato, John Mellor, Irina Higgins, Anto-nia Creswell, Nat McAleese, Amy Wu, Erich Elsen,Siddhant Jayakumar, Elena Buchatskaya, David Bud-den, Esme Sutherland, Karen Simonyan, Michela Pa-ganini, Laurent Sifre, Lena Martens, Xiang LorraineLi, Adhiguna Kuncoro, Aida Nematzadeh, ElenaGribovskaya, Domenic Donato, Angeliki Lazaridou,Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-poukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-tiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,Daniel Toyama, Cyprien de Masson dAutume, YujiaLi, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,Aidan Clark, Diego de Las Casas, Aurelia Guy,Chris Jones, James Bradbury, Matthew Johnson,Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, LauraRimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Ko-ray Kavukcuoglu, and Geoffrey Irving. 2022. Scalinglanguage models: Methods, analysis & insights fromtraining gopher. Gowtham Ramesh, Sumanth Doddapaneni, AravinthBheemaraj, Mayank Jobanputra, Raghavan AK,Ajitesh Sharma, Sujit Sahoo, Harshita Diddee, Ma-halakshmi J, Divyanshu Kakwani, Navneet Kumar,Aswin Pradeep, Srihari Nagaraj, Kumar Deepak,Vivek Raghavan, Anoop Kunchukuttan, Pratyush Ku-mar, and Mitesh Shantadevi Khapra. 2022. Samanan-tar: The largest publicly available parallel corporacollection for 11 indic languages. Transactions of theAssociation for Computational Linguistics, 10:145162. Ricardo Rei, Ana C Farinha, Chrysoula Zerva, Daanvan Stigt, Craig Stewart, Pedro Ramos, TaisiyaGlushkova, Andr F. T. Martins, and Alon Lavie.2021. Are references really needed? unbabel-IST2021 submission for the metrics shared task. In Pro-ceedings of the Sixth Conference on Machine Trans-lation, pages 10301040, Online. Association forComputational Linguistics. Ricardo Rei, Craig Stewart, Ana C Farinha, and AlonLavie. 2020. COMET: A neural framework for MTevaluation. In Proceedings of the 2020 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP), pages 26852702, Online. Associationfor Computational Linguistics. Abigail See, Peter J. Liu, and Christopher D. Manning.2017. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 10731083, Vancouver, Canada. Association for Computa-tional Linguistics. Rico Sennrich, Barry Haddow, and Alexandra Birch.2016a. Improving neural machine translation modelswith monolingual data. In Proceedings of the 54thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 8696,Berlin, Germany. Association for Computational Lin-guistics. Rico Sennrich, Barry Haddow, and Alexandra Birch.2016b. Neural machine translation of rare wordswith subword units. In Proceedings of the 54th An-nual Meeting of the Association for ComputationalLinguistics, ACL 2016, August 7-12, 2016, Berlin,Germany, Volume 1: Long Papers. The Associationfor Computer Linguistics. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,Abu Awal Md Shoeb, Abubakar Abid, AdamFisch, Adam R. Brown, Adam Santoro, AdityaGupta, Adri Garriga-Alonso, Agnieszka Kluska,Aitor Lewkowycz, Akshat Agarwal, Alethea Power,Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Par-rish, Allen Nie, Aman Hussain, Amanda Askell,Amanda Dsouza, Ameet Rahane, Anantharaman S.Iyer, Anders Andreassen, Andrea Santilli, AndreasStuhlmller, Andrew M. Dai, Andrew La, Andrew K.Lampinen, Andy Zou, Angela Jiang, Angelica Chen,Anh Vuong, Animesh Gupta, Anna Gottardi, Anto-nio Norelli, Anu Venkatesh, Arash Gholamidavoodi,Arfa Tabassum, Arul Menezes, Arun Kirubarajan,Asher Mullokandov, Ashish Sabharwal, Austin Her-rick, Avia Efrat, Aykut Erdem, Ayla Karakas, andet al. 2022. Beyond the imitation game: Quantifyingand extrapolating the capabilities of language models.CoRR, abs/2206.04615.",
  "Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,Bo Wen, and Yunfeng Liu. 2023. Roformer: En-hanced transformer with rotary position embedding": "Gemma Team, Thomas Mesnard, Cassidy Hardin,Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,Laurent Sifre, Morgane Rivire, Mihir SanjayKale, Juliette Love, Pouya Tafti, Lonard Hussenot,Pier Giuseppe Sessa, Aakanksha Chowdhery, AdamRoberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amlie Hliou, Andrea Tac-chetti, Anna Bulanova, Antonia Paterson, BethTsai, Bobak Shahriari, Charline Le Lan, Christo-pher A. Choquette-Choo, Clment Crepy, Daniel Cer,Daphne Ippolito, David Reid, Elena Buchatskaya,Eric Ni, Eric Noland, Geng Yan, George Tucker,George-Christian Muraru, Grigory Rozhdestvenskiy,Henryk Michalewski, Ian Tenney, Ivan Grishchenko,Jacob Austin, James Keeling, Jane Labanowski,Jean-Baptiste Lespiau, Jeff Stanway, Jenny Bren-nan, Jeremy Chen, Johan Ferret, Justin Chiu, JustinMao-Jones, Katherine Lee, Kathy Yu, Katie Milli-can, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon,Machel Reid, Maciej Mikua, Mateo Wirth, MichaelSharman, Nikolai Chinaev, Nithum Thain, OlivierBachem, Oscar Chang, Oscar Wahltinez, Paige Bai-ley, Paul Michel, Petko Yotov, Rahma Chaabouni,Ramona Comanescu, Reena Jana, Rohan Anil, RossMcIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith,Sebastian Borgeaud, Sertan Girgin, Sholto Douglas,Shree Pandya, Siamak Shakeri, Soham De, Ted Kli-menko, Tom Hennigan, Vlad Feinberg, WojciechStokowiec, Yu hui Chen, Zafarali Ahmed, ZhitaoGong, Tris Warkentin, Ludovic Peran, Minh Giang,Clment Farabet, Oriol Vinyals, Jeff Dean, KorayKavukcuoglu, Demis Hassabis, Zoubin Ghahramani,Douglas Eck, Joelle Barral, Fernando Pereira, EliCollins, Armand Joulin, Noah Fiedel, Evan Senter,Alek Andreev, and Kathleen Kenealy. 2024. Gemma:Open models based on gemini research and technol-ogy. NLLB Team, Marta R. Costa-juss, James Cross, Onurelebi, Maha Elbayad, Kenneth Heafield, Kevin Hef-fernan, Elahe Kalbassi, Janice Lam, Daniel Licht,Jean Maillard, Anna Sun, Skyler Wang, GuillaumeWenzek, Al Youngblood, Bapi Akula, Loic Bar-rault, Gabriel Mejia Gonzalez, Prangthip Hansanti,John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, ChauTran, Pierre Andrews, Necip Fazil Ayan, ShrutiBhosale, Sergey Edunov, Angela Fan, CynthiaGao, Vedanuj Goswami, Francisco Guzmn, PhilippKoehn, Alexandre Mourachko, Christophe Ropers,Safiyyah Saleem, Holger Schwenk, and Jeff Wang.2022.No language left behind: Scaling human-centered machine translation. James Thorne, Andreas Vlachos, Oana Cocarascu,Christos Christodoulopoulos, and Arpit Mittal. 2018.The fact extraction and VERification (FEVER)shared task. In Proceedings of the First Workshop onFact Extraction and VERification (FEVER), pages 19, Brussels, Belgium. Association for ComputationalLinguistics. Antonio Toral, Sheila Castilho, Ke Hu, and Andy Way.2018. Attaining the unattainable? reassessing claimsof human parity in neural machine translation. In Pro-ceedings of the Third Conference on Machine Trans-lation: Research Papers, pages 113123, Brussels,Belgium. Association for Computational Linguistics. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processingsystems, 30. Alex Wang, Amanpreet Singh, Julian Michael, FelixHill, Omer Levy, and Samuel R Bowman. 2018.Glue: A multi-task benchmark and analysis platformfor natural language understanding. arXiv preprintarXiv:1804.07461. Alex Warstadt, Aaron Mueller, Leshem Choshen, EthanWilcox, Chengxu Zhuang, Juan Ciro, Rafael Mos-quera, Bhargavi Paranjabe, Adina Williams, TalLinzen, and Ryan Cotterell. 2023. Findings of theBabyLM challenge: Sample-efficient pretraining ondevelopmentally plausible corpora. In Proceedingsof the BabyLM Challenge at the 27th Conference onComputational Natural Language Learning, pages134, Singapore. Association for Computational Lin-guistics.",
  "AAdditional results": "We report additional results in this section. Ta-bles 14, 15 show the chrF++ and BLEU scoresacross three translation evaluation benchmarks.This shows that using parallel synthetic data doesnot deteriorate the performance of the languagemodel. Similar results are shown in forIndicNLG tasks where performance on Hindi gen-eration tasks are only affected by a small marginand coupled with results in showing thatscores are not affected by using Hindi syntheticparallel data.Using synthetic for one language doesnt impactperformance in another: For many multilinguallanguage models, data imbalance causes a gap inperformance across languages. But what if we cancombine synthetic data along with clean data fortraining multilingual models? would the syntheticpart deteriorate the performance of the multilingualmodel? To experiment with this, we train bilin-gual base-1k models over different combinations ofclean and synthetic corpora for English and Hindiand evaluate their performance on GLUE (Wanget al., 2018), and report performance on IndicNLG,and Machine translation in Appendix A. Follow-ing , we see that using Hindi synthetic datadoes not affect its performance compared to BI-EN-HI-clean model which is solely trained on cleancorpora. This implies that it is possible to trainmultilingual models where some languages aretrained only over a clean subset and others onsynthetic without deteriorating performance acrosslanguages. We further see that using parallel datadoes not have much impact on multilingual models.Impact on Machine Translation: (MT) We fo-cus on MT separately as a special case of NLG.We hypothesized that using parallel synthetic docu-ments for bilingual models would improve transla-tion performance by enhancing alignment betweenlanguages. However, our evaluation fails this hy-pothesis. Results indicate that using nonparallelsynthetic documents yields similar translation per-formance across language directions and bench-marks compared to parallel synthetic documents.This might be because there is no explicit align-",
  "B.1Evaluation Datasets": "For evaluation, we utilize a diverse set of datasetscovering four languages: English, Hindi, Gujarati,and Marathi.For Hindi and Gujarati, we relyon the IndicGLUE benchmark15 (Kakwani et al.,2020b), which provides a range of tasks for natu-ral language understanding (NLU), including nat-ural language inference (IndicXNLI/iXNLI), arti-cle genre classification (bbc-a, iNLTK), discoursemode classification (MIDAS), and sentiment anal-ysis (iitp-mr, iitp-pr). For natural language gen-eration (NLG), we employ the IndicNLG bench-mark16 (Kumar et al., 2022), which includes taskslike headline generation, sentence summarization,question generation, and Wikipedia biography gen-eration.The IndicGLUE dataset is semi-automaticallycurated using website metadata and Wikipedia arti-cles, while IndicNLG is derived from Wikipedia ar-ticles and news websites for summarization, alongwith parallel corpora and pivot-based translationfor paraphrasing tasks. Additionally, we incorpo-rate the test sets from IN22 (Gala et al., 2023) andFlores-200 (Team et al., 2022) to evaluate perfor-mance on machine translation tasks.We use the well-known GLUE benchmark,which includes nine NLU tasks in English such asnatural language inference (NLI), semantic similar-ity, text classification, and linguistic acceptability.For English summarization tasks, we rely on XL-Sum (Hasan et al., 2021), DialogSum (Chen et al.,2021), and CNN/DailyMail (See et al., 2017).",
  "B.3Extended pretraining": "For the mini-1k models, we randomly sample 100Mtokens from the clean subset of IndicMonoDoc forthe target language, and for the base-1k model,we sample 200M for extended pretraining. Weuse the same hyperparameters as training and per-form extended pretraining for 2 epochs over thisnewly sampled clean data. For scaling experiments,we utilize TorchTune17 for fine-tuning Llama-3-8Band Gemma-2B models. For a fair comparison, welimit each data split to 344M tokens for Gujaratiand 465M tokens for Marathi and follow a similarprocedure as described in .4 to generateand filter data for Marathi. We perform extendedtraining for a single epoch using LoRA (Hu et al.,2021) finetuning on Wq, Wv projection matricesusing =16 and r=8. We keep the learning rate at3e5 with a weight decay of 0.01 and an effectivebatch size of 58k. We use the AdamW optimizer(Loshchilov and Hutter, 2019) with 1000 warmupsteps and a cosine learning rate scheduler.",
  "B.4Fine-tuning": "For GLUE tasks we use the dev split on the cleanpart and do hyperparameter tuning to achieve thebest scores, and then we use the same hyperparam-eters for all synthetic experiments. For IndicGLUEwe follow a similar setting for the val split to findgood hyperparameters and report results on the testsplit like Kakwani et al. (2020a). For all classifi-cation and regression tasks, we use a single linearlayer and use an appropriate activation functionfor classification and regression respectively. Weuse an Adam optimizer (Kingma and Ba, 2015)with a learning rate of 1e5 and a batch size of48. For NLG tasks we do extended pretrainingusing a separator token in between the input andoutput sequence with an effective batch size of 768examples and only calculate loss for the output se-quence. We use an AdamW optimizer (Loshchilovand Hutter, 2019) with learning rate = 6e4, weightdecay = 1e1, 1 = 0.9, 2 = 0.95 and = 1e5.For translation, we randomly sample 1M parallelsentence for each language pair from the samanan-tar corpus (Ramesh et al., 2022) and evaluate onFloRes (Team et al., 2022), IN22-Conv and IN22-Gen (Gala et al., 2023). We list the batch size andnumber of epochs of each task in .",
  "B.5Prompting": "We use random sampling from validation setswhenever available and utilize other examples fromthe test set otherwise. We take 5 random samplesfor each evaluation of NLG tasks and 10 randomsamples for each evaluation of NLU tasks. We listdown the prompt used below for Marathi evalua-tions, we use similar prompts for Gujarati as well.",
  ": Hyperparameters used for finetuning tasks": "coefficient. We report chrF++ scores19 and BLEUscores20 (Papineni et al., 2002) using the sacre-BLEU21 implementation and Rouge-L f1 scoresusing the sacreRouge (Deutsch and Roth, 2020)implementation by the xl-sum repository22.We report English scores for NLU on the valida-tion split of the GLUE benchmark and test splitsfor XL-Sum, CNN Dailymail, and Dialogsum NLGbenchmarks. For Hindi and Gujarati, we use thetest split of IndicGLUE and IndicXNLI.For classification and regression tasks, we usethe models finetuned according to hyperparame-ters mentioned in Appendix B.4 to keep fair com-parison for all models and mention results on thefinal epoch. For generations on IndicNLG and En-glish NLG tasks, we use beam search with a beamwidth of 5, length penalty of 1.0, n_gram repetitionpenalty of 4 n_grams with sampling set to false andearly stopping set to true. We also set a maximumgeneration length to 64 tokens. For the translationtask, we follow a beam search with a beam width of5, maximum new tokens to 256 and early stoppingto true.",
  "C.1Creating synthetic data": "Translationese is a term used to describe peculiar-ities in the text translated into a specific language,differentiating it from content originally written inthat language (Gellerstam, 1986). Translated textsinto the target language (via humans or machine-generated) often show distinctive features that dif-ferentiate them from their original counterparts inthe target language. These disparities arise fromeither the influence of the translation process itselfon the final product or the inherent fingerprintsof the source language subtly present in the tar-get language rendition (Rabinovich and Wintner,2015). This is a common phenomenon in transla-tion models where the target language translationsoften show characteristics of the source languageand add bias to the evaluation of downstream tasks(Toral et al., 2018; Zhang and Toral, 2019; Grahamet al., 2019). So far a lot of work on synthetic trans-lated data has been done for using back translations(Sennrich et al., 2016a; Edunov et al., 2018) for im-proving Machine translation performance (Marieet al., 2020; Bogoychev and Sennrich, 2019; Niet al., 2022) or for classification tasks like nativelanguage identification (Goldin et al., 2018), etc.Tranlationese data has been used for many tasksbut we explore the efficacy of using translationesedata for pretraining of language models. We col-lect monolingual corpora in the source languageas mentioned in .1 and utilize a powerfuloff-the-shelf translation model IndicTrans2 (Galaet al., 2023) to generate translationese data. SinceIndicTrans2 can only handle a max sentence lengthof 256 BPE tokens, we split the documents usingMoses Sentence Splitter23 to perform translationsinto the target language at the sentence level andthen merge again to form documents. We also re-pair translations that exceed in length 256 BPEtokens using the TinyLM trained on clean corporaas mentioned in to complete the sen-tence translation, we encounter only 0.002% ofsuch cases. We use this corpus for the syntheticand clean+synthetic part of our experiments.",
  "Following , we use these TinyLMs to filterthe generated synthetic translationese corpora fromIndicTrans2. We do this by using perplexity as a": ": Violin plot displaying the distribution oflengths of clean and filtered English documents on dif-ferent data splits: en-clean (English web documents),syn-en_hi (synthetic English documents translated fromHindi), and syn-en_gu (synthetic English documentstranslated from Gujarati). measure of document quality score. For languagemodels, perplexity quantifies how well a modelpredicts a sequence of tokens. A lower perplexityindicates better predictive performance. While cal-culating perplexity over a sequence of tokens W w1, w2, . . . , wN we skip the first s tokens wheres = 10, e = 1024 and calculate loss until only thefirst e tokens of the document. We find setting eto larger values can lead to higher variance in thedocument scores due to the size of the TinyLM.After initial analysis, we choose s and e such thatwe remove the high uncertainty of the language atthe start of an unseen document and avoid penal-izing longer documents due to the fragility of theextrapolation ability of TinyLM24. Note that it isimportant to choose e such that the language modelgives a uniform estimate of perplexity over an al-ready seen sequence of tokens ws, ws+1, . . . , we.For our experiments, we use the TinyLMs to scoreall synthetically generated translationese data andcalculate a document score using the above method.Following Laurenon et al. (2022), we do subsam-pling by thresholding document perplexity scoresexcept Laurenon et al. (2022) did it using Ken-LM(Heafield, 2011) and we do it using our TinyLM.We keep the threshold value such that we includeenough documents to reach the computed optimaltoken count for pretraining experiments.",
  "DQualitative Analysis": "Since translation errors occur frequently, leadingto biased, ungrammatical, or erroneous translationsthat can have drastic consequences on training, itis important to mitigate or remove such errors intranslationese corpora. The most common machinetranslation errors include mistranslations due toambiguous words, incorrect handling of expres-sions, syntax and grammar errors, and issues withpreserving context across longer sentences. Manyapproaches have been proposed to address theseerrors, but most are computationally expensive, es-pecially when translationese data is used for pre-training. Instead, we ask whether a language modelcan identify such issues. To investigate this, weexamine which types of English sentences werefiltered.In many cases, we found that the filtered doc-uments included errors like code-mixing and rep-etitions, often generated by the diverging outputof the translation model. This is expected sincesuch phenomena are rarely seen in natural writtenlanguage, and the model assigning high entropysuggests an unlikely sequence outcome. Althoughthe model regarded some erroneous instances asfalse positives, many such cases were successfullyavoided. We also noticed that, due to the smallsize of the model, many perfectly good documentswere filtered out because of the models inabilityto evaluate them. This issue was observed less fre-quently in larger models used for evaluation. Asseen in , the filtering model fails to under-stand complex words and named entities, which itregarded as unlikely due to its preference for sim- pler terms and more likely entities. Other commonelements that were filtered included numbers, dates,and abbreviations. While this can lead to the loss ofvaluable information, as many good documents arediscarded, we empirically observe the benefits ofsuch filtering. These errors could likely be reducedby using a larger filtering model that can better ap-proximate the source language but we leave thisanalysis for future work.",
  "EIndicMonoDoc": "In this section, we describe the process of creat-ing the IndicMonoDoc corpus which is the largestdocument-level corpora for Indic languages consist-ing of 39.5 billion tokens spanning 23 languages.IndicMonoDoc comprises 27.5B Indic tokens and12B tokens of English tokens. showslanguage language-wise deduplicated size of theIndicMonoDoc corpus and shows a com-parative 100% stacked bar plot with IndicCorpv2which is a sentence level corpora.",
  "E.1Crawling": "To extract URLs from the web we sample wordlevel n-grams; n=2,...,6 from a sample monolingualcorpora to create a list of keyword searches. Wethen randomly merge k; k=1,..,4 keywords to forma query. Using these queries we perform automaticweb searches to collect a large repository of URLs.We merge this list with a manual list of sources toperform URL-level deduplication. We crawl thesewebpages leaving out some of them25. We leaveout webpages that consist of a considerable amountof English content using a simple script recognitionregex. We perform this scrapping majorly for thebottom 14 low-resource languages. We also addscript-level recognition using Unicode characters26",
  "E.2Post processing": "A lot of crawled content consists of unwanted textlike HTML tags, emoticons, and text in another lan-guage. We use manual filtering pipelines inspiredby OSCAR (Ortiz Surez et al., 2019), (Abadjiet al., 2022) to remove such content. We addition-ally use a language detection-based (LID) filteringusing cld327 and IndicLID-FTN model (Madhaniet al., 2023a) to discard languages not of inter-est. Following Doddapaneni et al. (2023) we per-form document filtering to remove offensive textfrom the corpora using a list of offensive words andphrases extended from work by Team et al. (2022)which consists of offensive words in 209 languages.We also use a Romanized version of this list usingthe transliteration tool by Madhani et al. (2023b) toperform toxic document filtering in 17 languages.Following Kakwani et al. (2020a) & Doddapaneniet al. (2023) we merge all the filtered corpus withWikipedia, OSCAR (Ortiz Surez et al., 2019) andsome dumps of mC4 (Xue et al., 2021). Finally,we perform deduplication at paragraph level usingMurmurhash algorithm28 with a 128-bit unsignedhash for each monolingual split of the corpora. Af-ter all post-processing steps, the language wise sizeof the corpora is mentioned in . A majorchunk of the corpus is comprised of English, Hindi,and Bengali which make up 72.15% of the corpora."
}