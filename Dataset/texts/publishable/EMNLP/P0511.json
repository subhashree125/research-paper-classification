{
  "Abstract": "Sentiment classification (SC) often suffersfrom low-resource challenges such as domain-specific contexts, imbalanced label distribu-tions, and few-shot scenarios. The potential ofthe diffusion language model (LM) for textualdata augmentation (DA) remains unexplored,moreover, textual DA methods struggle to bal-ance the diversity and consistency of new sam-ples. Most DA methods either perform log-ical modifications or rephrase less importanttokens in the original sequence with the lan-guage model. In the context of SC, strongemotional tokens could act critically on thesentiment of the whole sequence. Therefore,contrary to rephrasing less important context,we propose DiffusionCLS to leverage a dif-fusion LM to capture in-domain knowledgeand generate pseudo samples by reconstruct-ing strong label-related tokens. This approachensures a balance between consistency and di-versity, avoiding the introduction of noise andaugmenting crucial features of datasets. Dif-fusionCLS also comprises a Noise-ResistantTraining objective to help the model generalize.Experiments demonstrate the effectiveness ofour method in various low-resource scenariosincluding domain-specific and domain-generalproblems. Ablation studies confirm the effec-tiveness of our frameworks modules, and visu-alization studies highlight optimal deploymentconditions, reinforcing our conclusions.",
  "Introduction": "Sentiment classification is a crucial application oftext classification (TC) in Natural Language Pro-cessing (NLP) and can play a crucial role in multi-ple areas. However, NLP applications in domain-specific scenarios, such as disasters and pandemics,often meet with low-resource conditions, especiallydomain-specific problems, imbalance data distribu-tion, and data deficiency (Sedinkina et al., 2022;",
  "Corresponding author": "Lakshmi and Velmurugan, 2023; Nabil et al., 2023;Gatto and Preum, 2023). Recently, the birth ofpre-trained language models (PLMs) and large lan-guage models (LLMs) have advanced the NLP field,giving birth to numerous downstream models basedon them. On the one hand, these PLMs take themodels to a new height of performance, on the otherhand, since these models are highly data-hungry,they struggle to perform satisfactorily on most tasksunder noisy, data-sparse and low-resource condi-tions (Patwa et al., 2024; Chen et al., 2023b; Wanget al., 2024; Yu et al., 2023).",
  "It was really frustrating": ": Examples of CTR methods. Most CTR meth-ods rephrase minor tokens while DiffusionCLS recon-structs strong label-related tokens. Cor. and Gen. de-notes the corrupted sequence and generated sequencerespectively. To address these challenges, one effective ap-proach is data augmentation (DA), which enrichesthe diversity of the dataset without explicitly col-lecting new data (Feng et al., 2021). Classic rule-based DA methods employ logical modifications toobtain pseudo samples, such as EDA (Wei and Zou,2019), and AEDA (Karimi et al., 2021). Model-based DA methods develop rapidly as the trans-former architecture dominates the NLP field, mostof these methods execute DA through corrupt-then-reconstruct (CTR), as examples shown in . Namely, masked language model (MLM) (Wu et al., 2019; Kumar et al., 2020), and GENIUS(Guo et al., 2022) which applies BART as the sam-ple generator. Also, Anaby-Tavor et al. (2020)proposed LAMBADA, which finetunes GPT-2 andgenerates pseudo samples with label prompts.However, these methods often struggle withdomain-specific tasks and uneven label distribu-tions. Some methods generate samples solely rely-ing on pre-trained knowledge, like GENIUS. Theother though finetuned on the downstream dataset,these methods generate samples only conditionedon the label itself, such as LAMBADA, leading tostrong label inconsistency, especially in data-sparsesettings. Also, most CTR methods focus on re-placing minor tokens in sequences but keeping thecrucial tokens stationary to generate high-qualitypseudo samples.In contrast, we corrupt the most label-relatedtokens first and reconstruct the whole sentence con-ditioned on the context and label prompt, as shownin , to diversify the key label-related tokensrather than less important contexts. This approachnot only augments sample diversity but also up-holds consistency through selective masking. In-spired by DiffusionBERT (He et al., 2023), whichis designed to recover the most informative tokensfrom those with less informatics, we propose Diffu-sionCLS. Additionally, building upon the findingsof Guo et al. (2022), we further introduce consis-tency and diversity as crucial elements for qualityof samples. High-quality pseudo samples mustalign with their labels and domain contexts, mini-mizing noise introduction. Integrating these sam-ples enhances dataset diversity, thereby positivelyimpacting the model performance.DiffusionCLS initially finetunes PLM with a dif-fusion objective, functioning as a sample genera-tor, followed by training the TC model in a noise-resistant manner. By fine-tuning the diffusion LM,we can then input original samples with their cru-cial tokens corrupted and use the label as a gen-eration prompt to get new samples. This methoddiversifies the original dataset by replacing stronglabel-related tokens and also steers the model to-wards producing high-quality pseudo samples thatcomply with the diversity-consistency rule. Also,experimental codes have been released on GitHub1.The major contributions of this paper can besummarized as follows:",
  "Low-Resource Text Classification": "Motivated by the observation that data is oftenscarce in specific domains or emergent applica-tion scenarios, low-resource TC (Chen et al., 2018)has recently attracted considerable attention. Low-resource TC involves effectively categorizing textin scenarios where data is scarce or limited. Goudjilet al. (2018) and Tan et al. (2019) have explored sev-eral methods for low-resource TC, which mainlyinvolve traditional machine learning techniques toincrease data quantity and diversity.Recently, since the studies by Lan et al. (2019)and Sun et al. (2020) demonstrated the impressiveperformance of PLMs across various NLP tasks,a significant amount of work has leaned towardsusing PLMs to address low-resource TC problems(Wen and Fang, 2023; Ogueji et al., 2021; Liu et al.,2019; Devlin et al., 2018). However, PLMs re-quires amounts of annotated samples for finetuning,data-sparce significantly impacts models perfor-mances and DA could mitigate such problems.",
  ": Overview of the proposed method. DiffusionCLS comprises four core components: Label-Aware NoiseSchedule, Label-Aware Prompting, Conditional Sample Generation, and Noise-Resistant Training": "domain inconsistency. Moreover, these methodsfocus only on a specific original input, resulting inlimited diversity.Another type of data augmentation method in-cludes representation augmentation approaches.These methods generate pseudo-representation vec-tors by interpolating or perturbing the represen-tations of original samples. For instance, Zhanget al. (2017) proposed the groundbreaking tech-nique known as mixup, and Chen et al. (2023a)recently proposed AWD, an advanced approach intextual DA.Recent advancements in generative modelshave led to research on GPT-based paraphrasingdata augmentation methods, such as LAMBADA(Anaby-Tavor et al., 2020), which fine-tuned GPT-2model to generate new samples. However, LAM-BADA generates new samples based solely on spe-cific labels, neglecting information from the origi-nal samples. Another research direction involvesnot fine-tuning PLMs but combining the languagemodeling capability of pretrained models with thegenerative diversity of diffusion models (He et al.,2023), which significantly improves the capabilityof the generative encoder, i.e., MLM.Since diffusion LMs can generate new sequencesfrom masked original sequences, which matchesthe goal of retaining key information and rephras-ing secondary information in generative data aug-mentation. Therefore, on top of diffusion LM, wepropose DiffusionCLS, simultaneously consider-ing label and domain consistency and generatingpseudo samples by partially paraphrasing stronglabel-related tokens. Extensive experiments verifythe effectiveness of our method and hopefully beextended to numerous NLP tasks.",
  "Methodology": "Sentiment classification models often overfit andlack generalization due to sample deficiency. Toaddress this, we propose DiffusionCLS, consist-ing of Label-Aware Noise Schedule, Label-AwarePrompting, Conditional Sample Generation, andNoise-Resistant Training. A diffusion LM-basedsample generator is integrated to generate new sam-ples from the original dataset, enhancing TC modelperformance. illustrates DiffusionCLS. The diffusionLM-based sample generator generates new sam-ples for data augmentation, while the TC model istrained for the specific task. Label-Aware Prompt-ing and Label-Aware Noise Schedule are crucialfor training the sample generator, and ConditionalSample Generation and Noise-Resistant Trainingcontribute to the training of the TC model.",
  "Sample Generator": "To generate usable samples for further TC modeltraining, there are two crucial rules of success tosatisfy, diversity and consistency. Therefore, weexpect the generated samples to be as diverse aspossible with consistency to the TC label and origi-nal domain simultaneously. However, higher diver-sity also leads to a higher difficulty in maintainingconsistency.As He et al. (2023) excavated the potential ofcombining diffusion models with LMs for sequencegeneration, we built the sample generator from thediscrete diffusion model scratch. Precisely, we de-sign the Label-Aware Noise Schedule for the diffu-sion LM, which helps the model to generate diverseand consistent samples. Additionally, we integrate Label-Aware Prompting into the training regime,enabling the model to grasp label-specific knowl-edge, subsequently serving as the guiding conditionfor sample generation. These two modules help thegenerator to surpass the diversity-consistency chal-lenge and excel in performance. 3.1.1Label-Aware Noise ScheduleA proper algorithm of noise schedule could guidethe diffusion LM to capture more accurate seman-tic relations. Moreover, the effectiveness of time-agnostic decoding has been demonstrated, indicat-ing that incorporating implicit time information inthe noise schedule process is effective (Ho et al.,2020; Nichol and Dhariwal, 2021; He et al., 2023).Since the generated samples are also expected tostay consistent with the TC label and the originaldomain, we proposed Label-Aware Noise Sched-ule.The Label-Aware Noise Schedule begins by in-tegrating a proxy model that has been fine-tunedfor the TC task. This proxy model allows for thedetermination of the importance of each token inthe TC process, quantified through attention scoresbetween the [CLS] token and other tokens, whichare derived from the last layer of proxy model andcalculated as follows.",
  "h=1shi ,(1)": "where shi represents the i-th token attention score inthe h-th attention head, and wi denotes the weightmeasuring the importance of the i-th token.Motivated by He et al. (2023)s DiffusionBERT,we incorporates absorbing state in the LM noiseschedule. In our method, during the masking transi-tion procedure, each token in the sequence remainsunchanged or transitions to [MASK] with a certainprobability. The transition probability of token i atstep t can be denoted as:",
  ": The probability of a token remaining un-masked, with set to 0.5": "lower weight first, then recover the tokens that arestrongly related to the classification task later.The probability of a token being masked is tiedto its attention score relative to the [CLS] token,reflecting its contribution to the TC objective. Fig-ure 2 shows that masking probabilities depend onthe tokens label-related information. Label-AwareNoise Scheduling guides the model to recover themost label-related key tokens from those less cru-cial to the classification task.",
  ": Label-Aware Prompting, each masked se-quence is concatenated with their corresponding label": "To address this challenge, we propose Label-Aware Prompting, a method that offers supplemen-tary conditional information during both trainingand inference phrases. This additional informationaids the model in generating samples that upholdlabel consistency.As illustrated, following the maskingof samples in the noise schedule process, the la-bels of these samples are concatenated with their",
  "Text Classification Model": "In this work, we adopt encoder-based PLM as ourbackbone model and finetuned them for the TCtask. Though diffusion LM is strong enough tomaintain consistency and diversity at the same time,the introduction of pseudo samples unavoidablyintroduced noise data to the training of the TCmodel. To mitigate such a problem, we design acontrastive learning-based noise-resistant trainingmethod, further improving the scalability of theproposed DiffusionCLS.",
  "Reflective Conditional SampleGeneration": "We implement label prompting as a prior for thesample generator, akin to Label-Aware Prompting.Additionally, we introduce a novel reflective condi-tional sample generation module within the trainingloop of the TC model. This module dynamicallygenerates masked sequences for the sample gen-erator, integrating insights from label annotationsand attention scores derived from the TC modelsimultaneously, calculating weights for each tokenwith Eq.1.However, generating pseudo samples from vary-ing degrees of masking will result in various de-grees of context replacement flexibility, thus im-pacting the consistency and diversity of pseudosamples. Essentially, providing a proper amount ofconditional information will lead to plausible sam-ples. Thus, we perform multiple experiments tosearch for the best condition, which will be furtherdiscussed in .5.",
  "Noise-Resistant Training": "The introduction of pseudo samples unavoidablyintroduced noise data to the training of the TCmodel. To mitigate such a problem, we design acontrastive learning-based noise-resistant trainingmethod, further improving the scalability of theproposed DiffusionCLS. demonstrates the Noise-resistant Train-ing. Specifically, besides including supervisionsignals from labels of original and generated sam-ples, we also guide the model to enlarge the gapbetween samples with different labels.Consider a dataset comprising m distinct cat-egories C = {c1, c2, ..., cm}, we can obtain ksamples from the original training set, and thecorresponding subscript list is I = {1, 2, ..., k 1, k}.Essentially, a batch of sentences S ={s1, s2, ..., sk1, sk}, their corresponding label se-quence L = [l1, l2, ..., lk1, lk] with li C, andnegative set for each sample Ni = {j I|lj = li}.From this, we derive semantic representationsH = {h1, h2, ..., hk1, hk} from the TC model.Furthermore, employing a sample generator yieldsB new samples for each original sample si, denotedas Gi = {gsi0 , gsi1 , ..., gsiB1, gsiB}, where gsi0 = si.",
  "Push": ": Noise-resistant contrastive learning. Crosspoints are generated samples while round dots denoteoriginal samples. Train-with-noise objective aiming atenlarging the gap between original samples with differ-ent labels. Contrastive Loss. To avoid expanding the im-pact of noise samples, we calculate contrastive lossfrom only the original samples. With the aim toenlarge the gap between samples from differentcategories, the contrastive loss can be calculatedas:",
  "Datasets and Baselines": "To measure the efficiency of the propose Dif-fusionCLS, we utilize both domain-specific anddomain-general datasets comprising samples inChinese, English, Arabic, French, and Span-ish. Namely, domain-specific SMP2020-EWECT2,India-COVID-X3, SenWave (Yang et al., 2020),and domain-general SST-2 (Maas et al., 2011). Ad-ditionally, to compare with the most cutting-edgelow-resource TC methods, we utilize SST-2 datasetto evaluate our method in the few-shot setting.Dataset statistic and descriptions are demonstratedin Appendix A.To thoroughly explore and validate the capabili-ties of DiffusionCLS, we compare our method witha range of data augmentation techniques, from clas-sic approaches to the latest advancements for low-resource TC. Specifically, we take Resample, BackTranslation (Shleifer, 2019), Easy Data Augmenta-tion (EDA) (Wei and Zou, 2019), SFT GPT-2 refer-enced to LAMBADA (Anaby-Tavor et al., 2020),AEDA (Karimi et al., 2021), and GENIUS (Guoet al., 2022) as our baselines. Also, we compareour method in the few-shot setting with a couple ofcutting-edge methods, namely, SSMBA (Ng et al.,2020), ALP (Kim et al., 2022), and SE (Zhenget al., 2023). More details of our baselines aredemonstrated in Appendix B.",
  "Experiment Setup": "We set up two experimental modes, entire datamode and partial data mode, to reveal the effective-ness of our method in different scenarios. Sincesevere imbalanced distribution challenges existed,we take macro-F1 and accuracy as our major evalu-ation metrics.Also, we conduct 5-shot and 10-shot experi-ments on SST-2 to investigate the performance ofDiffusionCLS in extreme low-resource conditions.For evaluation, we use accuracy as the metric andreport the average results over three random seedsto minimize the effects of stochasticity.Additionally, we setup comparisons betweenvariant augmentation policies, namely, generatenew samples until the dataset distribution is bal-anced, and generate n pseudo samples for eachsample (n-samples-each), which denoted as B/Dand G/E in , and n=4 in our experiments.",
  "Results and Analysis": "The results of entire-data-setting experiments ondatasets SMP2020-EWECT and India-COVID-Xare mainly demonstrated in , which we com-pare DiffusionCLS with other strong DA baselines.For experiments with partial-data and few-shot set-tings, results are majorly showed in and.Results under Entire Data Mode. As shownin , in general, the proposed DiffusionCLSoutperforms most DA methods on domain-specificdatasets SMP2020-EWECT and India-COVID-X,especially under G/E policy. Notably, the Diffu-sionCLS positively impacts the TC model acrossall policies and datasets, which most baselines fail.Our method excels in dealing with the chal-lenge of uneven datasets. Under severe unevendistribution and domain-specific scenarios, i.e., thedataset SMP2020-EWECT, most DA baselines failto impact the classification model positively ex-cept DiffusionCLS, which achieves the best perfor-mance. Also, our method achieves competitive per-formance under data-sparse and domain-specificscenarios, i.e., in the dataset India-COVID-X, mostDA methods bring improvement to the classifica-tion model, and our DiffusionCLS ranked second.Rule-based DA methods such as EDA, ratherlack diversity bringing overfit problems or solelyrelying on out-domain knowledge therefore break-ing consistency and impacting the task model neg-atively. For model-based methods, though mostmethods significantly increase the diversity of thegenerated samples, they rather generate samplessolely depending on pretraining knowledge and in-context-learning techniques or generate samplesonly conditioned on the label itself, posing a chal-lenge of maintaining consistency.Results under Partial Data Mode and Few-shot Settings. As shown in and inAppendix C, the proposed DiffusionCLS methodconsistently improves the classification model. No-tably, DiffusionCLS matches the PLM baselineperformance on the Arabic SenWave dataset usingonly 50% of the data samples.We also compare DiffusionCLS with the mostcutting-edge few-shot methods on SST-2 datasetunder 5-shot and 10-shot setting, the results areshown in . Though our method fails tosurpass all few-shot baselines, it still achieves com-",
  "+ DiffusionCLS (ours)G/E67.98%80.23%2.11%1.06%74.65%74.41%3.66%3.78%": ": Experiment results on SMP2020-EWECT and India-COVID-X datasets, with N/A indicating no augmenta-tion, B/D for balancing pseudo samples, and G/E for the n-samples-each policy. We adopt bert-base as the EnglishPLM and wwm-roberta as the Chinese PLM. + denotes the model is trained with the corresponding augmentationmethod. Acc and F represent performance variance between training with augmentation and without.",
  "Ablation Study": "To validate the effectiveness of modules in the pro-posed DiffusionCLS, we conduct ablation stud-ies to study the impacts of each module. presents the results of the ablation experiments.In each row of the experiment results, one of themodules in DiffusionCLS has been removed fordiscussion, except D.A., which removes all mod-ules related to the generator and only applies noise-resistance training. Overall, all modules in the proposed Diffusion-CLS works positively to the TC model, comparedwith the pure PLM model, the application of Dif-fusionCLS leads to 2.11% and 3.66% rises in F1values on dataset SMP2020-EWECT and India-COVID-X respectively.The results of ablation studies further validatethat the Label-Aware Prompting effectively im-proves the quality of pseudo samples. Also, theNoise-Resistant Training reduces the impact ofnoise pseudo samples.",
  "Discussions and Visualizations": "Generating pseudo samples from more masked to-kens provides more flexibility for generation andtends to result in more diverse samples, however, itwill enlarge the possibility of breaking the consis-tency since less information is provided.To analyze the optimal amount of masks forgenerating new pseudo samples, we conduct ex-periments on the India-COVID-X dataset. Duringconditional sample generation, we gather maskedsequences from 32 noise-adding steps, group theminto sets of eight, and evaluate how varying mask-ing levels impact the models performance.As shown in , our observations indi-cate a unimodal trend. The models performanceimproves with increased masking, peaks at the 4thgroup, and then declines with further masking. Thisreflects the diversity-consistency trade-off, moremasked tokens create more diverse samples, butoverly diverse samples may be inconsistent withoriginal labels or domain.To explore the relationship between generatedpseudo samples and original samples, we con-duct 2D t-SNE visualization. shows that",
  "Conclusion": "In this work, we propose DiffusionCLS, a novel ap-proach tackling SC challenges under low-resourceconditions, especially in domain-specific and un-even distribution scenarios. Utilizing a diffusionLM, DiffusionCLS captures in-domain knowledgeto generate high-quality pseudo samples maintain-ing both diversity and consistency. This methodsurpasses various kinds of data augmentation tech-niques. Our experiments demonstrate that Diffu-sionCLS significantly enhances SC performanceacross various domain-specific and multilingualdatasets. Ablation and visualization studies furthervalidate our approach, emphasizing the importanceof balancing diversity and consistency in pseudosamples. DiffusionCLS presents a robust solutionfor data augmentation in low-resource NLP applica-tions, paving a promising path for future research.",
  "This work was supported by the National SocialScience Fund of China (No. 22BTQ045)": "Ateret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich,Amir Kantor, George Kour, Segev Shlomov, NaamaTepper, and Naama Zwerdling. 2020. Do not haveenough data? deep learning to the rescue! In Pro-ceedings of the AAAI conference on artificial intelli-gence, volume 34, pages 73837390. Junfan Chen, Richong Zhang, Zheyan Luo, ChunmingHu, and Yongyi Mao. 2023a. Adversarial word dilu-tion as text data augmentation in low-resource regime.In Proceedings of the AAAI Conference on ArtificialIntelligence, volume 37, pages 1262612634. Xilun Chen, Yu Sun, Ben Athiwaratkun, Claire Cardie,and Kilian Weinberger. 2018. Adversarial deep aver-aging networks for cross-lingual sentiment classifica-tion. Transactions of the Association for Computa-tional Linguistics, 6:557570. Zhuowei Chen, Yujia Tian, Lianxi Wang, and ShengyiJiang. 2023b. A distantly-supervised relation extrac-tion method based on selective gate and noise cor-rection. In China National Conference on ChineseComputational Linguistics, pages 159174. Springer.",
  "learning method using svm for text classification.International Journal of Automation and Computing,15:290298": "Biyang Guo, Yeyun Gong, Yelong Shen, Songqiao Han,Hailiang Huang, Nan Duan, and Weizhu Chen. 2022.Genius: Sketch-based language model pre-trainingvia extreme and selective masking for text generationand augmentation. arXiv preprint arXiv:2211.10330. Zhengfu He, Tianxiang Sun, Qiong Tang, KuanningWang, Xuanjing Huang, and Xipeng Qiu. 2023. Dif-fusionBERT: Improving generative masked languagemodels with diffusion models. In Proceedings of the61st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages45214534, Toronto, Canada. Association for Com-putational Linguistics.",
  "Akbar Karimi, Leonardo Rossi, and Andrea Prati. 2021": "AEDA: An easier data augmentation technique fortext classification. In Findings of the Associationfor Computational Linguistics: EMNLP 2021, pages27482754, Punta Cana, Dominican Republic. Asso-ciation for Computational Linguistics. Hazel H Kim, Daecheol Woo, Seong Joon Oh, Jeong-Won Cha, and Yo-Sub Han. 2022. Alp: Data aug-mentation using lexicalized pcfgs for few-shot textclassification. In Proceedings of the aaai conferenceon artificial intelligence, volume 36, pages 1089410902. Varun Kumar, Ashutosh Choudhary, and Eunah Cho.2020. Data augmentation using pre-trained trans-former models. In Proceedings of the 2nd Workshopon Life-long Learning for Spoken Language Systems,pages 1826.",
  "S Deepa Lakshmi and T Velmurugan. 2023. Classi-fication of disaster tweets using natural languageprocessing pipeline. Acta Scientific COMPUTERSCIENCES Volume, 5(3)": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman,Kevin Gimpel, Piyush Sharma, and Radu Soricut.2019. Albert: A lite bert for self-supervised learn-ing of language representations.arXiv preprintarXiv:1909.11942. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A robustly optimized bert pretraining ap-proach. arXiv preprint arXiv:1907.11692. Andrew L. Maas, Raymond E. Daly, Peter T. Pham,Dan Huang, Andrew Y. Ng, and Christopher Potts.2011. Learning word vectors for sentiment analysis.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: Human",
  "Language Technologies, pages 142150, Portland,Oregon, USA. Association for Computational Lin-guistics": "Alvi Ahmmed Nabil, Dola Das, Md Shahidul Salim,Shamsul Arifeen, and HM Abdul Fattah. 2023.Bangla emergency post classification on social mediausing transformer based bert models. In 2023 6thInternational Conference on Electrical Informationand Communication Technology (EICT), pages 16.IEEE. Nathan Ng, Kyunghyun Cho, and Marzyeh Ghassemi.2020. SSMBA: Self-supervised manifold based dataaugmentation for improving out-of-domain robust-ness. In Proceedings of the 2020 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 12681283, Online. Association forComputational Linguistics.",
  "Alexander Quinn Nichol and Prafulla Dhariwal. 2021.Improved denoising diffusion probabilistic models.In International conference on machine learning,pages 81628171. PMLR": "Kelechi Ogueji, Yuxin Zhu, and Jimmy Lin. 2021.Small data? no problem! exploring the viabilityof pretrained multilingual language models for low-resourced languages. In Proceedings of the 1st Work-shop on Multilingual Representation Learning, pages116126. Parth Patwa, Simone Filice, Zhiyu Chen, GiuseppeCastellucci, Oleg Rokhlenko, and Shervin Malmasi.2024. Enhancing low-resource LLMs classificationwith PEFT and synthetic data. In Proceedings ofthe 2024 Joint International Conference on Compu-tational Linguistics, Language Resources and Eval-uation (LREC-COLING 2024), pages 60176023,Torino, Italia. ELRA and ICCL.",
  "Sam Shleifer. 2019. Low resource text classificationwith ulmfit and backtranslation.arXiv preprintarXiv:1903.09244": "Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, HaoTian, Hua Wu, and Haifeng Wang. 2020. Ernie 2.0: Acontinual pre-training framework for language under-standing. In Proceedings of the AAAI conference onartificial intelligence, volume 34, pages 89688975. Ming Tan, Yang Yu, Haoyu Wang, Dakuo Wang, SaloniPotdar, Shiyu Chang, and Mo Yu. 2019. Out-of-domain detection for low-resource text classificationtasks. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processingand the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), pages35663572. Lianxi Wang, Yujia Tian, and Zhuowei Chen. 2024. En-hancing Hindi feature representation through fusionof dual-script word embeddings. In Proceedings ofthe 2024 Joint International Conference on Compu-tational Linguistics, Language Resources and Eval-uation (LREC-COLING 2024), pages 59665976,Torino, Italia. ELRA and ICCL. Jason Wei and Kai Zou. 2019. EDA: Easy data augmen-tation techniques for boosting performance on textclassification tasks. In Proceedings of the 2019 Con-ference on Empirical Methods in Natural LanguageProcessing and the 9th International Joint Confer-ence on Natural Language Processing (EMNLP-IJCNLP), pages 63826388, Hong Kong, China. As-sociation for Computational Linguistics. Zhihao Wen and Yuan Fang. 2023. Augmenting low-resource text classification with graph-grounded pre-training and prompting. In Proceedings of the 46thInternational ACM SIGIR Conference on Researchand Development in Information Retrieval, pages506516. Xing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han,and Songlin Hu. 2019.Conditional bert contex-tual augmentation. In Computational ScienceICCS2019: 19th International Conference, Faro, Portugal,June 1214, 2019, Proceedings, Part IV 19, pages8495. Springer. Qiang Yang, Hind Alamro, Somayah Albaradei, AdilSalhi, Xiaoting Lv, Changsheng Ma, Manal Alshehri,Inji Jaber, Faroug Tifratene, Wei Wang, TakashiGojobori, Carlos M. Duarte, Xin Gao, and Xian-gliang Zhang. 2020. Senwave: Monitoring the globalsentiments under the covid-19 pandemic. Preprint,arXiv:2006.10842. Jianfei Yu, Qiankun Zhao, and Rui Xia. 2023. Cross-domain data augmentation with domain-adaptive lan-guage modeling for aspect-based sentiment analysis.In Proceedings of the 61st Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 14561470, Toronto, Canada.Association for Computational Linguistics.",
  "SMP2020-EWECT6. This Chinese datasetincludes 8,606 pandemic-related posts, cate-gorized into neutral, happy, angry, sad, fear,and surprise, with highly imbalanced labeldistribution": "India-COVID-X7. This dataset containscleaned English tweets from India X platformon topics such as coronavirus, COVID-19, andlockdown. The tweets have been labeled intofour sentiment categories with relatively bal-anced label distribution. SenWave(Yang et al., 2020). This dataset in-cludes about 5,000 English tweets and approx-imately 3,000 Arabic tweets in the specificdomain of the pandemic and lockdown, whichare annotated with sentiment labels. English-translated French and Spanish annotated sam-ples are also included. We extract all singlelabel samples for experiments."
}