{
  "Abstract": "The era of Large Language Models (LLMs)raises new demands for automatic evaluationmetrics, which should be adaptable to variousapplication scenarios while maintaining lowcost and effectiveness. Traditional metrics forautomatic text evaluation are often tailored tospecific scenarios, while LLM-based evalua-tion metrics are costly, requiring fine-tuningor rely heavily on the generation capabilitiesof LLMs. Besides, previous LLM-based met-rics ignore the fact that, within the space ofLLM representations, there exist direction vec-tors that indicate the estimation of text quality.To this end, we introduce RepEval, a metricthat leverages the projection of LLM represen-tations for evaluation. Through simple promptmodifications, RepEval can easily transitionto various tasks, requiring only minimal sam-ple pairs for direction vector construction. Re-sults on fourteen datasets across two evaluationtasks demonstrate the high effectiveness of ourmethod, which exhibits a higher correlationwith human judgments than previous methods,even in complex evaluation scenarios involvingpair-wise selection under nuanced aspects. Ourwork underscores the richness of informationregarding text quality embedded within LLMrepresentations, offering insights for the devel-opment of new metrics.1",
  "Introduction": "Text evaluation is widely applied in the era of LLM,such as detecting harmful responses (Sun et al.,2023; Kim et al., 2024), identifying high-qualitydata for model training (Meta, 2024; Cai et al.,2024) and constructing preference data for modelalignment (Nvidia, 2024; Bai et al., 2022; Lee et al.,2023). Such requirements pose significant chal-lenges to automatic text evaluation metrics, as met-rics must be adaptive to diverse evaluation tasks",
  "* Luoyi Fu is the corresponding author.1The project is publicly available for research purpose": "and achieve high-quality assessment while main-taining a low cost. However, traditional automaticevaluation metrics, such as BLEU (Papineni et al.,2002) and COMET (Rei et al., 2020), are usuallydesigned for specific tasks or criteria, making themdifficult to transfer to new application scenarios.Also, their requirement for references and otherinputs makes them infeasible in various evaluationcontexts. LLM-based metrics offer a possible solu-tion (Gao et al., 2023; Chiang and Lee), but suchmetrics may also encounter certain limitations. Onthe one hand, they rely heavily on the generationability of LLM to adhere to predefined formats,which typically require more model parameters orfine-tuning, resulting in higher costs for inferenceand deployment. On the other hand, their assess-ment is frequently unsatisfactory, which does notalign well with human judgments and exhibits un-stable performance (Shen et al., 2023).Fortunately, though language models may strug-gle to generate appropriate responses, their repre-sentations contain rich information related to cor-rect answers, which could be extracted with neuralnetwork or other models (Zou et al., 2023). Imag-ine, when people are assessing a piece of text, theymay have a clear sense of its quality yet strug-gle to quantify their impressions with a precisescore. This implies that during evaluation, we canreduce the reliance on the generation capabilitiesof LLMs and instead focus on the meaningful infor-mation contained in their representations. By doingso, we can utilize models with fewer parameters,thereby avoiding excessive computational resourceconsumption while achieving better performance.The remaining questions are: Do representationsof LLM really encapsulate information relevant totext quality? How can we effectively extract andapply this information to evaluation tasks?In this study, we introduce RepEval, a metricutilizing the projection of LLM representation forcustom evaluation. We explored the performance of RepEval in two scenarios: absolute evaluationand pair-wise evaluation. In absolute evaluation,which requires evaluation metrics to output scoresas assessment, our intuition is that representationsof high-quality and low-quality text exhibit distinctdistributions. We validate that, in vector space,their projection in a specific direction characterizesthe degree of variation in textual properties. In pair-wise evaluation, metrics need to select the betterone out of the two inputs. To solve this problem,we construct a projection vector that measures theprobability of whether the preceding sentence isbetter than the latter.For absolute evaluation, experiments on threecriteria with ten datasets show that our methodhas better correlations with human judgments thanprevious metrics, which is flexible and easy to ex-tend to other applications. As to pair-wise evalua-tion, experiments on four tasks with custom criteriademonstrate that our method remains highly feasi-ble in complex application scenarios, achieving ex-cellent classification accuracy. Through visualiza-tion, we further demonstrate that a well-designedprompt can transfer the representation to differentpositions within the semantic space, thus facilitat-ing evaluations based on diverse criteria. We alsodemonstrate that using PCA can produce nearlyoptimal projection vectors, and we explore the opti-mization strategy of RepEval, offering a reasonablescheme for representation creation.In summary, the key contributions of this workare:",
  "Reference-based metrics": "Reference-based metrics measure the similarity be-tween the hypothesis and one or multiple refer-ences, and a hypothesis more similar to the refer-ence is considered to be better (Gehrmann et al.,2023). These metrics can be further classified intotwo types: n-gram-based and embedding-based.Popular n-gram-based metrics include BLEU (Pa-pineni et al., 2002), ROUGE (Lin, 2004) and ME-TEOR (Banerjee and Lavie, 2005). Embedding-based metrics include BERTScore (Zhang et al.,2019) and MoverScore (Zhao et al., 2019). How-ever, the requirement of human-written referenceslimits their applications, as the creation of refer-ences is always a serious problem.",
  "Reference-free metrics": "Reference-free metrics instead require the sourceto generate the hypothesis in the Natural LanguageGeneration(NLG) process. Their advantage liesin the independence of human-written references,which costs expensive manual preparation. Polularreference-free metrics include BARTScore (Yuanet al., 2021), UniEval (Zhong et al., 2022) andGPTScore (Fu et al., 2023). Compared to reference-based metrics, they often exhibit better perfor-mance and adaptability (Sheng et al., 2024). How-ever, these metrics are mostly designed for specificapplication scenarios and criteria, making it chal-lenging to effectively apply them to new tasks.",
  "LLM-based metrics": "In recent years, there is a new trend to utilize LLMin text evaluation. Relying on the powerful ca-pabilities of LLM, these studies use few-shot orzero-shot methods to directly generate the assess-ment results (Gao et al., 2023; Chiang and Lee).To enhance model performance, some studies havetrained models specifically for evaluation throughfine-tuning (Kim et al., 2024). However, LLMswith better generation capability usually containmore parameters, which is costly for evaluation,while the outputs are often unsatisfactory (Shenet al., 2023). The method of fine-tuning is alsotime-consuming and expensive as well.",
  ": Pipeline of collecting representations with decoder-only LLM and constructing project direction": "function. In the same scenarios, an answer writtenby human experts can be viewed as a referenceref.A common evaluation scenario is absolute eval-uation, where an automatic metric function f isapplied to evaluate a single hyp based on the spe-cific criterion and output the evaluation result inthe form of a score. This process can be describedas Equation 1. We should note that src and refare not necessary for all metrics. Also, for somemetrics, the evaluation scores are irrelevant to thecriterion.",
  "score = f(criterion, hyp, src, ref)(1)": "Another scenario is pair-wise evaluation. Eachtime in the evaluation, a pair of hyp is provided,and metrics are required to choose the better onefrom two hyps based on specific criteria. Datasetsin this scenario are all collected from complicatedtasks, which have custom evaluation criteria fordifferent samples. This scenario requires the modelto clearly understand the evaluation criteria andaccurately discern the quality difference betweenhyp pairs.",
  "[h1, h2, . . . , hN])(2)": "where si is the metric score of the i-th sample in acertain dataset, hi is the relative human judgment,and is the correlation function. In this study, weuse Spearman Correlation (Spearman, 1987).In pair-wise evaluation scenarios, we use theaccuracy of detecting better hyp as the meta-evaluation method, as shown in Equation 3",
  "Representations of LLM": "In this study, representation refers to the hiddenstates of LLM with specific input texts. LLMs uti-lized in this study are in decoder-only architecture,typically comprising n decoder layers and a lan-guage modeling head with the hidden size of d. Asshown in , specifically, given a text inputwith s tokens, denote the output of the ith layeras hi, where i [0, n 1], and hi Rsd. Wefurther denote the hidden states of kth token onlayer i as hik.Suppose we choose the last kth token on theith layer as the representation rep, we then haverep = hik.",
  "Is the following Hyp <criterion_description>?Hyp: <hyp>Src: <src>The sentence is": "Here, <hyp> is filled by hyp to be evaluated,<src> is optional and only used in consistencyevaluation, while <criterion_description> is dif-ferent for each criterion. Please refer to the Ap-pendix C.5 for more information. We also add acontrol group without the prompt template, usingonly hyp as inputs.For pair-wise tasks, we need to compare the qual-ity of two different hypA and hypB. Datasets re-lated to the pair-wise evaluation are collected fromcomplicated tasks, adopting different score criteriafor each sample, such as harmlessness, honesty, etc.Follows Kim et al. (2024), here, <instruction> isthe description of the task description, <response1> and <response 2> could be filled by hypAand hypB, and <score criterion> is the evalua-tion requirement. More details could be found inthe Appendix C.5",
  "Project Direction": "In the previous steps, we converted both evaluationtasks into binary classification problems by con-structing proper prompts and obtained the relevantreps. Next, we need to figure out a specific projec-tion direction d, where the projection of rep on drepresents the probability of the answer is Yes.We utilize Principal Component Analysis (PCA)to accomplish this task. In absolute evaluation,assume we have K high-quality texts, i.e. theyreceive high scores from human evaluators, andwe denote their representations as rep+. Simi-larly, we collect K low-quality texts and their rep-resentations, denoted as rep. For each pair of(rep+, rep), their difference is given by rep =rep+rep or rep = reprep+. In pair-wiseevaluation, consider K pairs of texts, where onesentence (A) is better than the other (B). Accord-ing to the process described in section 4.1, sinceA is better than B, we denote the representation ofrepAB as rep+ and repBA as rep. Here, repindicates the probability that A is better than B.\"As shown in , reps represents thechange in the likelihood of the answer being Yesinstead of No in each sample, while their prin-cipal components should capture the overall vari-ations. Therefore, with rep as inputs, assumingthat we collect k main component vectors withPCA, as well as their importance score. Mark theith vector and its importance as di and wi. we canobtain the final d following Equation 4:",
  "COH Newsroom0.4440.3920.2730.3730.2740.2070.4210.5950.6230.4580.221SummEval0.5340.5160.4180.2630.3470.2470.2620.4120.4080.5920.333": ": Absolute Evaluation Results. Each row represents the Spearmans correlations of a metric with humanjudgments on absolute evaluation datasets. The bold scores represent the top two highest correlation results foreach task on each criterion. Coherence, consistency, and fluency are written in abbreviations COH, CON, and FLUrespectively. PCA(n) represents n samples are used in training. Hyp-only can not be used for consistency evaluation.",
  "Selection of layer and token": "As shown in , when constructing represen-tations, there are many layers and tokens to choosefrom, and the optimal layer may depend on specifictasks and input. As we only utilize decoder-onlyLLM, which predicts the next token from left toright, the reps of the last few tokens contain thesemantic information of the entire preceding textand are selected for application.After projection vectors are collected, we test theperformance of different tokens combined with dif-ferent layers, and select the target token and layer",
  "Datasets": "For absolute evaluation, we focus on three evalu-ation criteria: fluency, consistency and coherence,which are widely applied in NLG tasks. We utilizedatasets from four tasks: Asset (Alva-Manchegoet al., 2020) for simplification, SummEval (Fabbriet al., 2021) and Newsroom (Grusky et al., 2018)for summarization, WebNLG (Shimorina et al.,2019), SFRES, and SFHOT (Wen et al., 2015) fordata-to-text, and USR-Persona and USR-Topic fordialogue (Mehri and Eskenazi, 2020).For pair-wise evaluation, according to Kim et al. (2024), we utilize datasets HHH Alignment (Askellet al., 2021), MT Bench Human Judgment, Auto-JEval (Li et al., 2023), and Preference Bench (Kimet al., 2024). All samples in these datasets con-tain a pair of hyps, instructions to generate thehyp, human judgments and relevant criteria. Forboth scenarios, all texts in datasets are written inEnglish.",
  "Training Dataset": "We utilize Asset and GCDC for absolute evalua-tion. Asset belongs to the simplification task, whileGCDC is a real-world text dataset specifically cre-ated for coherence evaluation, both unrelated toother datasets in this work. Please refer to theAppendix for how we select positive samples andnegative samples to construct rep+ and rep. Since the criteria and application scenarios ofpair-wise datasets differ greatly from each other,they can be regarded as unrelated external data.Therefore, for the evaluation of MT Bench HumanJudgment, Auto-J Eval, and Preference Bench, weutilize HHH Alignment to construct a training set.For the evaluation of HHH Alignment, we utilizethe MT Bench as training data.",
  "Absolute Evaluation": "Following the description in previous sections, thecorrelations between human judgments and scoresgenerated by each metric are presented in .We observe that RepEval outperforms existingmetrics on almost all datasets, even surpassing theperformance of GPT-4. With just five text pairs,the PCA method surpasses all baseline metrics onhalf of the datasets, and with 20 pairs, it achievesa top-two performance on seven datasets, simi-lar to the results obtained by SVM. Consideringthat the training of SVM requires much more sam-ples to achieve similarly good results, PCA sig-nificantly reduces the manual cost of constructingsamples while maintaining relatively good perfor-mance. The Hyp-only experiments outcome indi-cates that even without the addition of a prompttemplate, the embeddings in LLM contain informa-tion related to evaluation criteria such as fluencyand coherence. Another notable point is that RepE-vals performance is evidently better than directlyprompting Mistral-7b for evaluation, indicating thateven when LLM struggles to generate a satisfyingresponse, their representations can still convey valu-able information.In summary, the projection of reps can effi-ciently extract information related to the text qual-ity on the desired evaluation criterion of hyp witha few samples. Therefore, in most cases, theres noneed to employ more complex models like SVM.Additionally, RepEval only requires hyp as input,whereas traditional metrics depend on src or ref. -1 -2 -3 -4",
  "Pair-wise Evaluation": "The accuracy of each method in pair-wise evalua-tion is presented in . We can observe thatdespite the varying generation tasks and evalua-tion criteria for each sample, RepEval still achieveshigh accuracy in selecting the better hyp. Com-pared to the generation results of vanilla Mistral-7b, the improvement of RepEval in pair-wise eval-uation further validates that, failing to generate agood response does not mean that LLM doesntknow the answers, as reps already contain clear di-rections pointing towards the correct classificationwithin the semantic space. Moreover, RepEval onlyadopts general LLM that has not been fine-tunedon evaluation tasks. Compared to PROMETHEUS,which is a text evaluation LLM fine-tuned with mil-lions of data, our method saves the expensive costof training, while maintaining relatively good orbetter performance. At the same time, by usingonly a 7b model, RepEval is still comparable to oreven surpasses LLMs like GPT-4. The above experimental results demonstrate thatwhen there is no need to explain the judgment re-sults, RepEval is highly competitive and can accu-rately make pair-wise selections. By only usinga general LLM for inference, RepEval eliminatesthe high costs associated with pre-training. Addi-tionally, since the optimal layers often reside in themiddle layers, it reduces both inference time andcomputational costs by not requiring the inferenceof all parameters.",
  "How prompt influence reps?": "The design of the prompt is an important step whenapplying RepEval for evaluation. Especially inabsolute evaluation, when we need to evaluate dif-ferent aspects of the same sample, we need to usedifferent prompt templates to obtain the correspond-ing rep. However, what role do these promptsplay? Do they truly distinguish between applica-tion scenarios? The previous experiments did notprovide an answer to this question.In this section, we utilize t-SNE for the visualiza- HHH Alignment MT Bench Auto-J",
  "Correlation": "Absolute Evaluation (FLU) PCA : Random Test Results Box plots represent meta-evaluation results corresponding to random vectors v,while the scatter points in the figure represent the results corresponding to direction vector d obtained through PCA.For pair-wise evaluation, the y-axis starts at 0.5, which is the expected accuracy of random guessing. tion of reps. We choose SummEval and Newsroomfor this experiment, as they include evaluation re-sults for two criteria: fluency and coherence. Wecollected reps obtained from the two prompt tem-plates and visualized their distribution using t-SNE,which is shown in .It can be seen that representations collected fromdifferent prompt templates exhibit different distri-butions and can be clearly separated from eachother. This indicates that the prompts successfullytransfer hyp to different positions within the seman-tic space, enabling the construction of the corre-sponding project direction in the transformed spaceand providing relevant assessments of the targetcriterion.",
  "Selection of Token and Layer": "To better utilize RepEval, in this section, we ex-plore the performance of RepEval with differentlayers and token selections. Limited by space, wetake fluency on absolute evaluation as an exampleand select four datasets from four tasks. All exper-iments follow the settings described in .The results are in .The results show that, surprisingly, the last tokenis not always the best one. Moreover, the correla-tion scores increase sharply in the middle layersand achieve the best result. A possible explanationcould be that reps collected from middle layerscontain more information relevant to the currentcontext. Comparatively, reps from the last layersare more useful to the next token prediction.This provides us with the following suggestionsfor improving RepEval. Firstly, we can opt for thetoken in the last second or third position, insteadof the last one token. Secondly, choose embed-dings from the second half of the layers. The layer",
  "A Good Projection or Not?": "Previous experiments show that PCA works effec-tively in identifying a suitable projection vector,surpassing other non-linear methods such as SVM.However, it remains uncertain whether PCA identi-fies the best projection. To address this question,we conduct the following experiments.We randomly generated 2000 vectors v with thesame shape as the vector d obtained by PCA in.2. We then collect scores using the pro-cess outlined in .2.1, replacing d with vThe selection of token and layer positions followedthe settings of PCA outlined in . Thedistribution of meta-evaluation results is shown in.We observe that d obtained through PCA is arelatively optimal result. Compared to randomvectors, it achieves nearly the highest correlationscores in absolute evaluation, as well as the highestaccuracy scores in pair-wise evaluation. This indi-cates that if reps contains related task informationand that there exist projection vectors d character-izing the direction of variation in text quality, PCAcan efficiently help researchers find the target d,and be applied for evaluation.",
  "Conclusion": "We introduce RepEval, an evaluation metric utiliz-ing the projection of LLM representations to obtainevaluation results, which exhibits a stronger correla-tion with human judgments in absolute evaluation,as well as higher accuracy in pair-wise selectionthan previous methods. RepEval is flexible and is easy to transfer to other evaluation scenarios, re-quiring only a few sample pairs for training, whileavoiding the usage of LLMs with a large number ofparameters such as GPT-4. We also provide sugges-tions on the proper application of RepEval, such asthe selection of tokens and layers. Our work pro-vides insights into the development of new metrics.",
  "Limitations": "In this study, the language is restricted to English.Further research is necessary to validate the iden-tified performance across a broader spectrum oftasks and languages.The analysis in this study is primarily drivenby experimental data, and we acknowledge the ab-sence of a more comprehensive mathematical ex-planation of the underlying mechanisms of RepE-val. Additionally, our evaluation relies solely oncorrelation and accuracy as measurement methods.A more detailed analysis is left for future work.",
  "ThisworkwassupportedbyNSFChina(No.61960206002,62020106005, 42050105,62061146002), Shanghai Pilot Program for BasicResearch - Shanghai Jiao Tong University": "Fernando Alva-Manchego, Louis Martin, Antoine Bor-des, Carolina Scarton, Benot Sagot, and Lucia Spe-cia. 2020. ASSET: A dataset for tuning and evalua-tion of sentence simplification models with multiplerewriting transformations. In Proceedings of the 58thAnnual Meeting of the Association for ComputationalLinguistics, pages 46684679, Online. Associationfor Computational Linguistics. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,Deep Ganguli, Tom Henighan, Andy Jones, NicholasJoseph, Ben Mann, Nova DasSarma, et al. 2021. Ageneral language assistant as a laboratory for align-ment. arXiv preprint arXiv:2112.00861. Yuntao Bai,Saurav Kadavath,Sandipan Kundu,Amanda Askell, Jackson Kernion, Andy Jones,Anna Chen,Anna Goldie,Azalia Mirhoseini,Cameron McKinnon, et al. 2022.Constitutionalai: Harmlessness from ai feedback. arXiv preprintarXiv:2212.08073. Satanjeev Banerjee and Alon Lavie. 2005. METEOR:An automatic metric for MT evaluation with im-proved correlation with human judgments. In Pro-ceedings of the ACL Workshop on Intrinsic and Ex-trinsic Evaluation Measures for Machine Transla-tion and/or Summarization, pages 6572, Ann Arbor,",
  "Pengcheng He, Xiaodong Liu, Jianfeng Gao, andWeizhu Chen. 2021. Deberta: Decoding-enhancedbert with disentangled attention. In InternationalConference on Learning Representations": "Karl Moritz Hermann, Tomas Kocisky, Edward Grefen-stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,and Phil Blunsom. 2015. Teaching machines to readand comprehend. Advances in neural informationprocessing systems, 28. Or Honovich, Roee Aharoni, Jonathan Herzig, HagaiTaitelbaum, Doron Kukliansy, Vered Cohen, ThomasScialom, Idan Szpektor, Avinatan Hassidim, andYossi Matias. 2022. TRUE: Re-evaluating factualconsistency evaluation. In Proceedings of the 2022Conference of the North American Chapter of the",
  "Association for Computational Linguistics: HumanLanguage Technologies, pages 39053920, Seattle,United States. Association for Computational Lin-guistics": "Katharina Kann, Sascha Rothe, and Katja Filippova.2018. Sentence-level fluency evaluation: Referenceshelp, but can be spared!In Proceedings of the22nd Conference on Computational Natural Lan-guage Learning, pages 313323. Seungone Kim,Juyoung Suk,Shayne Longpre,Bill Yuchen Lin, Jamin Shin, Sean Welleck, GrahamNeubig, Moontae Lee, Kyungjae Lee, and MinjoonSeo. 2024. Prometheus 2: An open source languagemodel specialized in evaluating other language mod-els. arXiv preprint arXiv:2405.01535. Alice Lai and Joel Tetreault. 2018. Discourse coherencein the wild: A dataset, evaluation and methods. InProceedings of the 19th Annual SIGdial Meeting onDiscourse and Dialogue, pages 214223, Melbourne,Australia. Association for Computational Linguistics. Harrison Lee, Samrat Phatale, Hassan Mansoor, KellieLu, Thomas Mesnard, Colton Bishop, Victor Car-bune, and Abhinav Rastogi. 2023. Rlaif: Scalingreinforcement learning from human feedback with aifeedback. arXiv preprint arXiv:2309.00267. Mike Lewis, Yinhan Liu, Naman Goyal, MarjanGhazvininejad, Abdelrahman Mohamed, Omer Levy,Veselin Stoyanov, and Luke Zettlemoyer. 2020. Bart:Denoising sequence-to-sequence pre-training for nat-ural language generation, translation, and comprehen-sion. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics, pages78717880.",
  "Chin-Yew Lin. 2004. Rouge: A package for automaticevaluation of summaries.In Text summarizationbranches out, pages 7481": "Franois Mairesse, Milica Gaic, Filip Jurccek, SimonKeizer, Blaise Thomson, Kai Yu, and Steve Young.2010. Phrase-based statistical language generationusing graphical models and active learning. In Pro-ceedings of the 48th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 15521561, Uppsala, Sweden. Association for Computa-tional Linguistics. Shikib Mehri and Maxine Eskenazi. 2020. USR: Anunsupervised and reference free evaluation metricfor dialog generation. In Proceedings of the 58thAnnual Meeting of the Association for ComputationalLinguistics, pages 681707, Online. Association forComputational Linguistics.",
  "Thomas Scialom and Felix Hill. 2021. Beametrics: Abenchmark for language generation evaluation evalu-ation. ArXiv, abs/2110.09147": "Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, YangYou, and Lidong Bing. 2023. Large language mod-els are not yet human-level evaluators for abstrac-tive summarization. In Findings of the Associationfor Computational Linguistics: EMNLP 2023, pages42154233, Singapore. Association for Computa-tional Linguistics. Shuqian Sheng, Yi Xu, Luoyi Fu, Jiaxin Ding, Lei Zhou,Xinbing Wang, and Chenghu Zhou. 2024. Is refer-ence necessary in the evaluation of NLG systems?when and where? In Proceedings of the 2024 Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies (Volume 1: Long Papers), pages85808596, Mexico City, Mexico. Association forComputational Linguistics.",
  "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-berger, and Yoav Artzi. 2019. Bertscore: Evaluatingtext generation with bert. In International Confer-ence on Learning Representations": "Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-tian M. Meyer, and Steffen Eger. 2019. MoverScore:Text generation evaluating with contextualized em-beddings and earth mover distance. In Proceedingsof the 2019 Conference on Empirical Methods inNatural Language Processing and the 9th Interna-tional Joint Conference on Natural Language Pro-cessing (EMNLP-IJCNLP), pages 563578, HongKong, China. Association for Computational Lin-guistics. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024.Judging llm-as-a-judge with mt-bench and chatbotarena. Advances in Neural Information ProcessingSystems, 36. Ming Zhong, Yang Liu, Da Yin, Yuning Mao, YizhuJiao, Pengfei Liu, Chenguang Zhu, Heng Ji, andJiawei Han. 2022.Towards a unified multi-dimensional evaluator for text generation. In Pro-ceedings of the 2022 Conference on Empirical Meth-ods in Natural Language Processing, pages 20232038, Abu Dhabi, United Arab Emirates. Associationfor Computational Linguistics. Andy Zou, Long Phan, Sarah Chen, James Campbell,Phillip Guo, Richard Ren, Alexander Pan, XuwangYin, Mantas Mazeika, Ann-Kathrin Dombrowski,et al. 2023.Representation engineering: A top-down approach to ai transparency. arXiv preprintarXiv:2310.01405.",
  "AExperiment Settings": "When evaluating fluency and consistency, we con-struct the training dataset using Asset. For coher-ence evaluation, we utilize GCDC. During the train-ing of the PCA model, the number of training pairsis set to 5 and 20. Additionally, we employ theSVM model for comparison with the PCA method,using 100 pairs for SVM training. As SVM needsmore training data, during construction, we ensurethe distinctiveness of each pair, though some pairsmay contain the same good or bad text. No re-peated data is contained in the training set of PCA.We collected representations with Mistral-7b fol-lowing the process described in .1. Weemploy the Sklearn implementation of PCA andSVM. For SVM, the kernel is set as Radial BasisFunction (RBF), gamma = 1/d, and the regular-ization parameter C = 1. We utilized Mistral-7bto generate representations using a single NVIDIAGeForce RTX 3090. The training of PCA and SVMmodels was performed on a CPU. More experimentdetails can be found in Appendix C.",
  "C.1Datasets": "C.1.1Absolute EvaluationASSETASSET is a dataset created for the tun-ing and evaluation of sentence simplification mod-els (Alva-Manchego et al., 2020). In this research,we use the human rating corpus, which contains100 pairs of original sentences and system simpli-fication as well as the human evaluation resultsfor the system output. For each pair, the rating is",
  "done by 15 crowd-sourced workers from 3 aspects:fluency, adequacy, and simplicity": "BAGELBAGEL features annotations on data-to-text tasks gathered from a dialogue system, withhuman annotations covering informativeness andnaturalness, according to Mairesse et al. (2010).In this context, informativeness is compared withthe gold standard, differing from our defined usage.However, for our purposes, we solely utilize thejudgment results related to naturalness. GCDCGCDC is created with real-world texts,which is designed for the development of discoursecoherence algorithms (Lai and Tetreault, 2018).Each sample in GCDC contains three evaluationscores of coherence on a 3-point scale from 1 (lowcoherence) to 3 (high coherence). NEWSROOMNEWSROOM gathers 60 articlesalong with summarization outcomes from 7 models,featuring human-written summaries as references,as documented by Grusky et al. (2018). The evalu-ation encompasses coherence, fluency, relevance,and informativeness. QAGSQAGS encompasses reference texts andannotation results focused on consistency in thecontext of the summarization task, as outlined byWang et al. (2020). The approach involves collect-ing three annotations for each sentence in a gener-ated summary, utilizing a majority vote strategy todetermine a consistency score. The final score isobtained by calculating the mean value across allsentences. SFHOT and SFRESSFHOT and SFRES deliverevaluation results for the data-to-text task, incor-porating annotations of naturalness and informa-tiveness, as detailed by Wen et al. (2015). In thiscontext, informativeness gauges the consistent de-gree between sources and hypotheses. This datasetis utilized for analyzing consistency, while natural-ness serves as a proxy for fluency. SummEvalSummEval offers a compilation ofsummarization outcomes produced by languagemodels, as detailed by Fabbri et al. (2021). Thesemodels undergo training on the CNN/DailyMaildatasets, as described by Hermann et al. (2015),along with their corresponding reference texts.Each generated summary in the dataset includesscore results from both expert annotators andcrowd-workers, covering four dimensions: coher-ence, consistency, fluency, and informativeness. USRThe USR dataset offers evaluation resultsfor the dialogue task across five aspects: fluency,coherence, engagingness, groundedness, and un-derstandability. In alignment with the rephrasingstrategy outlined by Zhong et al. (2022), the origi-nal aspects \"maintains context\" and \"natural\" are re-named as \"coherence\" and \"fluency,\" respectively. WebNLGWebNLG includes human evaluationresults from the 2017 WebNLG Challenge, whichfocuses on the data-to-text task, as described byShimorina et al. (2019). The candidate text under-goes evaluation based on three aspects: fluency,grammar, and semantics. In this context, fluencyassesses whether a text is smooth and natural, andthe fluency score is employed for experimentationpurposes.Features contained in each absolute evaluationdataset are listed in . With the exception ofGCDC, all datasets include src.",
  "C.1.2Pair-wise Evaluation": "HHH AlignmentHHH Alignment contains theevaluation result based on four criteria: helpful-ness, harmlessness, honesty, and other, as well asthe relevant 221 response pairs judged by humanevaluators (Askell et al., 2021). MT BenchMT-bench consists of a series ofopen-ended questions that evaluate a chatbotsmulti-turn conversational and instruction-followingability, which collect 3,360 response pairs basedon 80 prompts, as well as judgment from humanevaluators (Zheng et al., 2024).",
  "C.3SVM": "We also add experiments with the Support VectorMachine (SVM) for comparison. With representa-tion rep as inputs, the SVM method involves train-ing a binary classifier on good-bad text pairs, andwe use the probability of a text belonging to goodtext as the score result. To be specific, considera specific text, denote the predicted probability ofbeing good text as p1, the predicted probability ofbeing bad text as p0, and the score satisfies",
  "C.5.2Pair-wise Evaluation": "Refer to the prompt design of Kim et al. (2024), weuse the following prompt template for all pair-wiseevaluation. Here, for pairs from different datasets,the score rubric should also be chanted to the re-lated one. ###Task Description: An instruction (might in-clude an Input inside it), a response to evaluate,and a scoring rubric representing evaluationcriteria are given.Choose a better response between ResponseA and Response B. You should refer to thescoring rubric.###Instruction: You are a fair judge assistantassigned to deliver insightful feedback thatcompares individual performances, highlight-ing how each stands relative to others withinthe same cohort.###Response A: <hyp_1>###Response B: <hyp_2>###Score Rubric: <score_rubric>###Ans: \"\"\"",
  "C.6.2Absolute Evaluation of Coherence": "Score the following text with respect to co-herence with one to five stars, where one starmeans \"incoherence\" and five stars means \"per-fect coherence\". Note that coherence measuresthe quality of all sentences collectively, to thefit together and sound naturally. Consider thequality of the sentences as a whole and justoutput an overall score and no more other.Summary: <hyp>Stars:",
  "C.6.3Absolute Evaluation of Consistency": "Score the following summarization given thecorresponding article with respect to consis-tency with one to five stars, where one starmeans \"inconsistency\" and five stars means\"perfect consistency\". Note that consistencymeasures whether the facts in the summary areconsistent with the facts in the original article.Consider whether the summary reproduces allfacts accurately and does not make up untrueinformation.Article: <src>Summary: <hyp>Stars:"
}