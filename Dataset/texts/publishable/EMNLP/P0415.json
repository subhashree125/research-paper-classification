{
  "This paper contains prompts and modeloutputs that are offensive in nature": "Existingdebiasingtechniquesaretypi-cally training-based or require access to themodels internals and output distributions, sothey are inaccessible to end-users lookingto adapt LLM outputs for their particularneeds.In this study, we examine whetherstructured prompting techniques can offeropportunities for fair text generation.Weevaluate a comprehensive end-user-focusediterative framework of debiasing that appliesSystem 2 thinking processes for prompts toinduce logical, reflective, and critical textgeneration, with single, multi-step, instruction,and role-based variants.By systematicallyevaluating many LLMs across many datasetsand different prompting strategies, we showthat the more complex System 2-based Implica-tive Prompts significantly improve over othertechniques demonstrating lower mean biasin the outputs with competitive performanceon the downstream tasks.Our work offersresearch directions for the design and thepotentialofend-user-focusedevaluativeframeworks for LLM use.",
  "Introduction": "Large Language Models (LLMs) are known to per-petuate the societal biases present in their trainingcorpora (Vig et al., 2020; Gallegos et al., 2023;Li et al., 2023a). These biases occur due to un-vetted data sources or unbalanced representationsof social groups within this data and can havefar-reaching consequences by affecting decision-making processes, perpetuating stereotypes, andexacerbating existing inequalities (Sun et al., 2024;Thakur, 2023). To this end, numerous techniqueshave been developed for bias mitigation in LLMs",
  "*These authors contributed equallyWork done while at Adobe MDSR Labs": "such as re-training model representations (Lianget al., 2021; Webster et al., 2020), fine-tuning mod-els with augmented data (Zmigrod et al., 2019), oradjusting the models output logits and their decod-ing strategies (Schick et al., 2021; Banerjee et al.,2023). However, due to security, privacy and com-mercial reasons, many state-of-the-art LLMs areclosed API-only models that do not provide accessto the models internals, training data or the flexibil-ity to modify the LLMs decoding strategies. Thisimplies that users cannot employ any of the afore-mentioned debiasing techniques for such LLMsand are dependent on the model providers. Further,we believe that there can be instances where userspossess the models or prefer using the open-sourceLLMs. However, even then curating fair data (Zmi-grod et al., 2019) that is sufficient in scale and qual-ity to re-train the LLMs is prohibitively expensiveand out of reach for many. Moreover, given thatmodern day LLMs are very carefully tuned duringthe pre-training to demonstrate efficacy across mul-titude of tasks, any modification to their weights ordecoding strategies may lead to intractable adverseeffects on other downstream tasks except fairness.To this, we ask the following question - How canwe address the problem of biases in LLMs withouthaving access to the model or its output probabili-ties?\" Hence, we focus on the end users freedomto prompt the LLMs and debias according to theirrequirements.Contributions. We develop and evaluate an end-user-focused iterative framework for debiasinglanguage models. Inspired by human decision-making (Kahneman, 2011), we have organized theexisting prompting methods and introduced newones along three broad categories (Prefix Prompt-ing, Self-Refinement, and Implication Prompting)and following two dimensions (single v/s k-stepprompting, and instruction v/s role-prompting). Wereport an evaluation of many state-of-the-art LLMswith various prompting techniques exemplifying these categories and complexities and evaluate theoutputs on several benchmarks. Our frameworksdemonstrate debiasing performance equal to exist-ing white-box methods without any decrease in per-formance on downstream tasks. To the best of ourknowledge, this paper represents the first in-depthexploration of this direction, and we anticipate thatour framework paves the way for future research inprompt-based debiasing of LLMs.",
  "Related Work": "Due to the vast nature of LLM training cor-pora (Wang and Komatsuzaki, 2021; Team, 2023;Jiang et al., 2023; Touvron et al., 2023), it is in-feasible to vet them for potentially biased or harm-ful text data. Given the resource-intensive natureof retraining approaches, recent work focuses onpost-hoc debiasing techniques. Liang et al. (2020)introduced Sent-Debias, demonstrating the capabil-ity to debias sentences by eliminating the projec-tion of bias subspace from sentence representations.Additionally, SelfDebias (Schick et al., 2021) andCAFIE (Banerjee et al., 2023) utilize output proba-bilities to generate fairer outcomes through biasedprompts and counterfactuals, respectively. Unlikethe proposed prompting frameworks, these meth-ods require retraining, access to model parameters,and modification of decoding strategies. Prompt-ing and Bias Mitigation. The most common wayto prompt a model is to simply provide it with aninstruction and allow it to complete the text. An-other popular way to prompt LLMs is by usingroles and personas (Kong et al., 2023) to emulatehuman-like interactions for better zero-shot perfor-mance. Alternatively, Few-Shot prompting (Brownet al., 2020b) allows the models to adapt to tasks byinferring from examples provided directly withinthe input, improving flexibility. However, theseapproaches are not well suited for reasoning tasks.This led to works that provide LLMs with natu-ral language chains-of-thought (Wei et al., 2022;Kojima et al., 2022), which provides intermediatereasoning steps to the LLMs and improves theirperformance across arithmetic and reasoning ques-tions. Drawing parallels to how humans improvetheir outputs through reflection, (Madaan et al.,2023) use LLMs to generate outputs, provide feed-back and then self-refine. Although well-studiedotherwise, we argue that limited research has beendedicated to examining fairness through the afore-mentioned prompting techniques. Ma et al. (2023) propose a prompt-search frame-work for predictive fairness requiring significantcomputational resources to find the best promptmaking it impractical in a generic setting. In con-trast, Borchers et al. (2022) explore keyword-basedprompt engineering to address gender bias in jobadvertisements. Yet, this body of work is discon-nected from the work applying reasoning-basedprompts for better output generation.In summary, we note that while intricate prompt-ing strategies are being developed for a wide rangeof tasks, they are not specifically studied for fairtext generation. While some studies exist (Borcherset al., 2022; Si et al., 2023), they are restricted to ba-sic prompting approaches such as keyword-basedor simple prefixes. Thus, no prior work formallystudies the detailed adaptation of existing state-of-the-art prompting frameworks for fairness or the op-timal ways to prompt LLMs for bias removal. Mostfindings suggest no significant improvement inbias reduction through prompting (Borchers et al.,2022), yet Brown et al. (2020a) demonstrate thatrefined natural language instructions can, in fact,effectively steer GPT-3 in mitigating gender bias.While encouraging, this approach lacks a compre-hensive analysis of different prompting strategies(e.g., iterative, multi-prompt, feedback-based re-finement), their impact on different biases (e.g., re-ligion, race, sexual orientation), and their varianceacross different recent LLMs (e.g., MPT, Llama-2, Mistral). Hence, this gap motivates our currentwork that comprehensively studies these dimen-sions and proposes effective prompting techniquesfor bias removal.",
  "Prompting Framework": "In this section, we describe the prompting strate-gies we use to mitigate biases or stereotypes inlanguage model outputs. Our approach is inspiredby the heuristics of decision-making discussedby Kahneman (2011). Many decisions are made in-tuitively and exemplify System 1 decision-makingas they are automatic, unconscious, and directresponses to stimuli. However, humans can beprompted to second-guess their instincts throughslow, effortful, and logical thinking, known asSystem 2 decision-making, and exemplified mostsimply through Prefix Prompting.If logicallyretracing ones steps does not work, we cannudge people to be fair by providing them withevidence of the known risks (e.g., biased outputs). This inspires our second category of System2 decision-making under risk (Kahneman andTversky, 2013).Finally, humans can also becompelled to correct their reasoning by providingexplicit reasoning or feedback on why their outputsare biased, denoted as critical reflection in System2 decision-making (Kahneman, 2011).",
  ": Comparison of the prompting strategies. Theshown examples are chosen from the Regard dataset.Long sentences are abbreviated (...) for presentation.k=1 indicates a single refinement step": "Drawing upon this human analogy, in our work,we chose three broad categories of prompting ap-proaches based on the specificity of the feedbackprovided to the LLM. The simplest prompts in-volve direct requests, which exemplify our firstcategory, Prefix Prompting, in which we instructthe model to not be biased. Our next category ofprompts applying System 2 decision-making underrisk invokes Self-Refinement wherein LLMs re-fer to their self-generated biased texts. We invokea multi-step process that provides the LLM withits self-generated biased outputs and urges it tobe fair during the subsequent generations. Finally,prompts for System 2 decision-making with criti-cal reflection is exemplified through ImplicationPrompting which encourages the LLM towardsfair generation by providing them with reasoning. Once again, we invoke a multi-step process to en-courage the LLM towards fair generation by pro-viding a reasoning of why an output is biased. Theapproaches are exemplified in which usesexamples from the Regard dataset to demonstratethe difference in the outputs vs. the base (row 1)when an instruction vs. a role prompt is provided(rows 2 vs. 3), and in a single vs. a multi-stepprompting process is followed (row 3 vs. row 4).Finally, row 5 suggests how implication promptingcompares to the other methods.Notations. Let M be a pre-trained language modelwith token vocabulary V . Given a prompt C repre-sented as a sequence of tokens (x1 . . . xN), M gen-erates a probability distribution Po : V .Subsequently, employing a decoding strategy, Pois used to stochastically sample the next tokens(xN+1, xN+1 ...) and generate the output text S.Thus, we have M(C, V ) = S.",
  "Prefix Prompting (PP)": "In this approach, we prepend a prefix to the in-put user prompt, and instruct the model to pro-duce unbiased outputs. More specifically, given Cbe an input user prompt that introduces biases inthe language model output, we prepend it with adebiasing prefix Idebias to obtain the final promptCdebias = concat(Idebias, C). Following (Borcherset al., 2022) and (Kong et al., 2023), our exper-iments employ two types of prefixes, describednext.",
  "Self Refinement (SR)": "We observed that the outputs generated with a sim-ple single-step prefix prompting still possessed asubstantial bias. Thus, we extend it to a k-stepapproach where our objective is to obtain more re-fined outputs by asking the model to refer to itsprevious step outputs for carrying out subsequentgenerations.Next, we describe this strategy for k=1, and pro-vide its generic formulation in Algorithm 1.Step I Given the input user prompt C, we first perform the Prefix Prompting strategy to obtain out-put S0, which is the reference text for the LLMssubsequent generation.Step II We then combine S0 with another debi-asing prefix ISR to obtain the final prompt CSR =concat(S0, ISR). With this prompt, we query thelanguage model and generate an output S1. Simi-lar to Sec 3.1, we employ two variants of ISR, asdescribed next. Instruction SR Prompt:Here is a text you generated: [OUTPUT]Now refer to this text and generate some text that isunbiased and does not discriminate against people on thebasis of their gender, race, religion, or any other sensitiveattribute. [USER PROMPT] Role SR Prompt:Here is a text you generated: [OUTPUT]Nowrefertothistextconsideringthatyou are an unbiased person who does not discrimi-nate against people on the basis of their gender, race,religion, or any other sensitive attribute. [USER PROMPT]",
  "Implication Prompting (IP)": "Self-Refinement (SR) can be challenging for theLLM since it entails simply referring to its ownbiased outputs for generating fair text. Thus, wehypothesize that a better way would be to providethe LLM with a reasoning or implication alongsideits generated text explaining why it is biased. Giventhat we cannot hard-code implications correspond-ing to every possible model output, we proposeto generate such implications on the fly using thesame LLM itself. More concretely, we divide thisframework into three steps (see Algorithm 2), de-scribed next.Step I The first input to the LLM is simply theuser prompt C that generates an output S whichmight be biased.Step II The next input to the LLM is CImpl, aprompt made using the initial output S concate-nated with an instruction IImpl. This instruction asks the model to generate the Implication SImpl ofthe output S. The goal of this step is to make themodel identify any biases or stereotypes present inthe initial output. Following (Borchers et al., 2022;Kong et al., 2023; Brown et al., 2020b), we employthree popular reasoning instructions IImpl for ourexperiments.",
  "Zero-Shot CoT-Based CImpl:Here is a text: [OUTPUT]Thinking step by step, generate the implication of thegiven text:": "Fewshot-Based CImpl:For each text, generate the implication which shows thestereotype present in the text.[Text 1]: We shouldnt lower our standards just to hiremore women.[Implication 1]: This text implies that woman are lessqualified than men...[Text 4]: [OUTPUT][Implication 4]: Step III The final prompt CIP combines themodels initial biased output S, the generated im-plication SImpl, and an instruction IIP. We nowquery the language model with CIP to obtain thefinal debiased output. Final Prompt CIP :You are an unbiased person who does not discriminateagainst people on the basis of their gender, race, religion,or any other sensitive attribute.Here is a sentence you generated: [OUTPUT]Here is the implication that indicates the bias present inthe text: [IMPLICATION]Now, considering the implication and referring to theoriginal sentence, generate an unbiased text that doesnot discriminate against people on the basis of their gen-der, race, religion, or any other sensitive attribute: [USERPROMPT]",
  "Models and Metrics": "In this section, we discuss the language models andthe metrics used in our experiments. More specifi-cally, we evaluate four state-of-the-art LLMs overfour standard metrics serving as vital indicators ofthe models adherence to fairness, and inclusivity.Models.Our study includes four LLMs hav-ing diverse architectures and training methodolo-gies: GPT-J (6B) (Wang and Komatsuzaki, 2021),Mistral-v0.1 (7B) (Jiang et al., 2023), Llama-2(7B) (Touvron et al., 2023), and MPT-Instruct (7B)(Team, 2023). We download the weights and imple-mentations of these models from the Huggingfacelibrary and perform our experiments on one 32GBNVIDIA V100 GPU.StereoSet. (Nadeem et al., 2020). This datasetmeasures bias across gender, race, religion, andprofession attributes. Each sentence in this datasetfeatures a template with a blank, which can be filledwith a stereotypical, anti-stereotypical, or unrelatedword. For example, the template could be Theboy is [BLANK] at school. For this sentence, thestereotypical word is rowdy, the anti-stereotypicalword is calm, and the unrelated word is mirror.For our experiments, we use a subset of the dataset(53%) consisting of sentence clusters where the[BLANK] word is at the end of the template. This is because some of our approaches require an inter-mediate output that cannot be reasonably producedfor sentences with the blank in the middle due tocausal language modeling. We have confirmedthat using this subset does not impact performancesince the base models results on this subset arevery similar to the results on the entire dataset. Weevaluate model performance using three metrics:Stereotype Score (SS), Language Modeling score(LM), and Idealized Context Association Test score(ICAT). The SS score reflects the fraction of timesthe stereotypical sentence has a higher probabilitythan the anti-stereotypical sentence, with an idealscore of 50%. The LM score measures the propor-tion of times the unrelated sentence has the lowestprobability of generation, having an ideal score of100%. ICAT score combines SS and LM scores,representing the tradeoff between bias reductionand language modeling ability, with an ideal scoreof 100%.Regard. (Sheng et al., 2019). Sentiment classifiershave long been used as bias estimators; however,(Sheng et al., 2019) argues that sentiments are notoften correlated to the human judgment of bias. Forinstance, in the sentence XYZ worked as a pimpfor 15 years, even though the sentiment is neu-tral, the presence of the word pimp still surfacesa negative connotation towards the demographic",
  "+ Few-shot IP0.080.050.080.07+ Few-shot IP0.060.120.250.14": ": Regard scores for Gender, Race, and Orientation. Numbers in bold represent the best results for the model,and underlined numbers represent the best results for a prompting category. * denotes a p-value less than 0.05 onsingle-tailed t-testing. XYZ. Addressing this discrepancy, the concept ofregard estimates the bias by leveraging the socialperception of a demographic, which is measuredby considering characteristics like occupations andrespect towards a demographic.More specifically, (Sheng et al., 2019) capturesbiases across three attributes using pairs of de-mographics: Gender (female and male), Race(Black and White), and Sexual Orientation (Gayand Straight).They begin by constructing 10prompt templates per demographic (say \"Male\")and generate 10 sentences per template. Then, byusing a classifier1, they compute regard per outputof a demographic to obtain an overall regard scorefor a demographic:",
  "RGender = SFemale SMale(2)": "The ideal regard score is 0, while a negative numberindicates stereotypical bias and a positive numberrepresents anti-stereotypical bias.Toxicity (Gehman et al., 2020). In this metric, weassess the models performance beyond bias andevaluate its toxicity mitigation capabilities using the RealToxicityPrompts dataset. By employing afine-tuned hate speech detection model2, we com-pute the probability of model completions beingtoxic across 1000 randomly sampled prompts. Foreach prompting approach, we report the mean toxic-ity score, and the percent change in toxicity relativeto the base models toxicity score. The lower meantoxicity signals effective toxicity mitigation, and amore negative change indicates better performance.",
  "Results and Discussion": "Our findings suggests that prompts applying Sys-tem 2 decision-making directives improve languagemodels ability to anticipate and reduce biases inits generated text. We expect that while gener-ated text leverages statistical correlations found inthe training data, creating more structured promptsaround mitigating bias enhances the models abil-ity to search through its latent space for patternsthat might align with a correct answer. Ratherthan offering evidence of logical deducation or LMcognition, what our results imply is that System 2prompts offer a reliable heuristic for a stochasticsearch of relevant potential solution paths.In this section, we refer to our quantitative evalua-tions (Tables 2, 3, 4) to discuss the insights obtainedfrom each of them.Role-based Prefix Prompting debiases betterthan Instruction-based. Notably, the persona/-",
  "+ Instruction IP0.036-1.51%+ Instruction IP0.0443.02%+ Zero-Shot CoT IP0.0371.22%+ Zero-Shot CoT IP0.038-16.63%+ Few-shot IP0.0383.92%+ Few-shot IP0.0461.12%": ": Mean toxicity and the percentage change in toxicity compared to the base LM. Numbers in bold representthe best results for the model, and underlined numbers represent the best results for a given prompting strategysuch as Self-Refinement (SR) or Implication Prompting (IP). * denotes a p-value less than 0.05 on single-tailedt-testing. role prefix outperforms the standard instructionprefix on all three metrics. On StereoSet (), Role prefix has, on average across all models,a 2.14% lower SS score and a 5.08% higher ICATscore compared to instruction prefix. In the caseof Regard (see ), the Role prefixs averageperformance exceeds that of the instruction prefixby nearly 39.47% across all models. Furthermore, reveals that outputs generated using theRole prefix are 4.34% less toxic than those pro-duced with the instruction prefix. We substantiatemore about these findings in .Combining prefixes with the previously gener-ated output of LLMs improves debiasing. For2/3 benchmarks, we find that Self-Refinement issignificantly better than Prefix Prompting. Specif-ically, Self-Refinement with k=1 has, on average,an SS score 6.85% lower than the prefix prompt-ing approach, and a 11.65% higher ICAT score.This performance improvement is nearly 21.64%on the regard metric. On toxicity, however, SRwith k=1 shows a slight increase in average toxi-city compared to prefix prompting (1.11%). Fur-ther, we found that even though single iterationSelf-Refinement frameworks show a significant im-provement in performance over prefix prompting,performing two or more iterations of this frame-work often does not yield a competitive or anyincrease. SR with k=2 provides a mere 0.23% av-erage improvement in SS score over SR with k=1. Similarly, the ICAT score improves by only 0.42%and we notice no improvement in the Regard met-ric. We report this behavior for more values of k >2 in .Implication Prompting achieves the overall fairoutputs. For all the benchmarks, we consistentlyfind that Implication Prompting outperforms theother two frameworks. By averaging across IP vari-ants and models, we find that it has a 4.05% lowerSS score and a 6.80% higher ICAT score on Stere-oSet compared to all other methods. Similarly, itshows an average improvement of 26.85% on Re-gard and a 6.98% decrease in average toxicity ofoutputs. Thus, we conclude that providing reason-ing about why an output is biased indeed has apositive impact on fair text generation.Tradeoff between Bias and Language Model-ing Ability. Prior research has noted a decreasein language modeling ability that accompanies areduction in output bias. However, there is no con-sistent trend demonstrating this in our experiments.While GPTJ and MPT Instruct show a decreasein the LM Score on StereoSet as the SS Score im-proves, Mistral and Llama-2 exhibit the LM scoreof multi-step approaches to outperform the basemodel. By averaging across the models, we ob-serve that prefix prompting approaches possess a0.61% increase in LM score over the base model,self-refinement methods show a 0.46% drop in LMscore, and implication prompting reports a 0.09% decrease over the base model. In Appendix B, weperform evaluation on more downstream tasks suchas TruthfulQA (Lin et al., 2022), BoolQ (Clarket al., 2019) and note competitive performances ofprompting frameworks compared to the baselines.",
  ": Varying the choices of instruction and roleprefixes on StereoSet, Regard, and Toxicity. Scores areaveraged across all 4 LLMs": "Choice of Role and Instruction prefixes. In ad-dition to the role and instruction prefixes givenin .1, we now experiment with four dif-ferent choices of each prefix to further establishour findings. We create these prefix variations byrephrasing the existing ones or using synonymouswords. More details on these prefixes are includedin the Appendix. From , we observe thatthe role prefixes consistently perform better thanthe instruction ones, having a 1.7% higher ICATscore, and a 4.5% lower toxicity score.Increasing Self Refinement (SR) steps - k. In, we note that the performance of self-refinement with k=2 is only marginally differentfrom that of k=1. To understand this further, weexperiment with variations in the number of iter-ations (k) of refinement and report our results inFigures 1a, 1b, 1c. We see a similar trend for k=3,4and note that each of their performances lie withincomparable ranges of k=1. Thus, we conclude thatSR with k=1 is sufficient to reap benefits over PP.Varying the models for Implication generation.In .3, we discuss the use of the samemodel architecture to generate the underlying im-plication of a models output. However, we now ablate this choice by selecting models that are ac-cordingly smaller and larger than the input model.Specifically for this experiment, we choose GPTJ(6B), MPT (7B), and Mistral (7B) as the input mod-els and debias them by generating implicationsfrom TinyLLama (1.1B) (Zhang et al., 2024) andLlama-2 (13B). The results in Figures 1d, 1e, 1f areaveraged across the three models and demonstratethat despite slight variations, the performances ofimplications generated by both TinyLlama andLlama-2 lie in close range of the implications gen-erated by Mistral itself. This observation furtherestablishes the efficacy of reasoning-based meth-ods, while highlighting that low-latency modelscan be used for implication generation.",
  "Conclusion": "This study addresses the challenge of mitigatingbiases of LLMs under common settings that limitdirect access to their internal mechanics. Leverag-ing the principles of System 2 thinking, we eval-uate three prompt-based strategies designed forequitable text generation: Prefix Prompting, Self-Refinement, and Implication Prompting. Our evalu-ation, spanning a variety of metrics and models, re-veals the distinct advantages of these methods. No-tably, Implication Prompting emerges as the mosteffective technique, as it directly communicates therationale for avoiding biases to the LLM, followedby Self-Refinement and Prefix Prompting in termsof efficacy. This hierarchy highlights how sophis-ticated prompts, particularly those that engage themodel in deeper reasoning, can provide a strate-gic edge in mitigating biases more effectively thansimpler approaches. Our findings pave the way forfuture explorations into prompt-based debiasing ofLLMs, offering a foundational step towards morenuanced and effective bias mitigation strategies.",
  "Limitations and Future Work": "The metaphor of thinking fast and slow proved auseful guiding framework for our prompting strate-gies; yet, LLMs, at the current state of the art,are not thinking machines; generated text repro-duces textual patterns that are associated with theprompts in the representations learned from thetraining data (Bender et al., 2021). We cautionagainst making conclusions around LLM reason-ing based on our results.Our work suffers from limitations common to otherdebiasing studies, including the potential oversim-",
  "(f) Toxicity": ": Fig. (a), (b), and (c) show performance upon varying number of refinement steps on ICAT, Regard andToxicity. Fig. (d), (e), (f) show performance upon varying the size of the implication generation model. plification of complex social biases into promptsthat may not capture the full scope of biases inlanguage models. Additionally, the reliance onprompt-based techniques assumes model responsesto prompts are consistent, which may not holdacross different LLMs or when models are updated.We have tried to control for these errors by repeat-edly prompting models when such errors couldhave occurred and reporting means instead of ab-solute errors. We have also reported p-correctedt-tests to demonstrate that our results are not anartifact of the sample selected. Furthermore, theSystem 2 framework of promoting will only workif the models latent space contains relevant infor-mation about the task that can benefit from a moredirected search. Therefore, the framework may notgeneralize to different tasks, depending on whetherthe information needed is included in the languagemodels training data.Our work was hindered by the constraints on ourcomputational resources, as we were unable to ex-periment with larger models such as 70B variantsof Llama-2 (Touvron et al., 2023) and Mixtureof Experts models such as Mixtral (45B) (Jianget al., 2024). Further, due to space and time con-straints, many other advanced prompting methodssuch as Tree-of-Thought (Yao et al., 2023), Self-Consistency (Wang et al., 2023), and DirectionalStimulus Prompting (Li et al., 2023b) were not ex-plored.Yet, our framework is generalizable in that it offersinsights into their expected relative performancebased on whether or not they are prompted with pre- fixing, self-refinement, implicative prompts, andrepeated refinements. In future work, we plan todesign more sophisticated debiasing problems thatcan challenge and improve the generalizability ofend-user-focused frameworks such as ours.",
  "This work is supported by the Ministry of Educa-tion, Singapore under its MOE AcRF TIER3 Grant(MOE-MOET32022-0001) and the MOE Tier 1programme (WBS A-8000231-01-00)": "Pragyan Banerjee, Abhinav Java, Surgan Jandial, SimraShahid, Shaz Furniturewala, Balaji Krishnamurthy,and Sumit Bhatia. 2023. All should be equal in theeyes of language models: Counterfactually aware fairtext generation. Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On thedangers of stochastic parrots: Can language modelsbe too big?. In Proceedings of the 2021 ACM confer-ence on fairness, accountability, and transparency,pages 610623. Conrad Borchers, Dalia Gala, Benjamin Gilburt, EduardOravkin, Wilfried Bounsi, Yuki M Asano, and Han-nah Kirk. 2022. Looking for a handsome carpenter!debiasing GPT-3 job advertisements. In Proceedingsof the 4th Workshop on Gender Bias in Natural Lan-guage Processing (GeBNLP), pages 212224, Seattle,Washington. Association for Computational Linguis-tics.",
  "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind": "Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020a.Language models are few-shot learners.In Ad-vances in Neural Information Processing Systems,volume 33, pages 18771901. Curran Associates,Inc. Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish,Alec Radford, Ilya Sutskever, and Dario Amodei.2020b. Language models are few-shot learners.",
  "Samuel Gehman, Suchin Gururangan, Maarten Sap,Yejin Choi, and Noah A Smith. 2020. Realtoxici-typrompts: Evaluating neural toxic degeneration inlanguage models. arXiv preprint arXiv:2009.11462": "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, Llio Renard Lavaud,Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,Thibaut Lavril, Thomas Wang, Timothe Lacroix,and William El Sayed. 2023. Mistral 7b. Albert Q. Jiang, Alexandre Sablayrolles, AntoineRoux, Arthur Mensch, Blanche Savary, ChrisBamford, Devendra Singh Chaplot, Diego de lasCasas, Emma Bou Hanna, Florian Bressand, Gi-anna Lengyel, Guillaume Bour, Guillaume Lam-ple, Llio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,Sophia Yang, Szymon Antoniak, Teven Le Scao,Thophile Gervet, Thibaut Lavril, Thomas Wang,Timothe Lacroix, and William El Sayed. 2024. Mix-tral of experts.",
  "Truthfulqa: Measuring how models mimic humanfalsehoods": "Huan Ma, Changqing Zhang, Yatao Bian, Lemao Liu,Zhirui Zhang, Peilin Zhao, Shu Zhang, Huazhu Fu,Qinghua Hu, and Bingzhe Wu. 2023.Fairness-guided few-shot prompting for large language mod-els. arXiv preprint arXiv:2303.13217. Aman Madaan, Niket Tandon, Prakhar Gupta, SkylerHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,Sean Welleck,Bodhisattwa Prasad Majumder,Shashank Gupta, Amir Yazdanbakhsh, and PeterClark. 2023. Self-refine: Iterative refinement withself-feedback.",
  "Chenglei Si, Zhe Gan, Zhengyuan Yang, ShuohangWang, Jianfeng Wang, Jordan Boyd-Graber, and Li-juan Wang. 2023. Prompting gpt-3 to be reliable": "Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qi-hui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu,Yixuan Zhang, Xiner Li, Zhengliang Liu, Yixin Liu,Yijue Wang, Zhikun Zhang, Bhavya Kailkhura, Caim-ing Xiong, Chaowei Xiao, Chunyuan Li, Eric Xing,Furong Huang, Hao Liu, Heng Ji, Hongyi Wang,Huan Zhang, Huaxiu Yao, Manolis Kellis, MarinkaZitnik, Meng Jiang, Mohit Bansal, James Zou, JianPei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao,Jiliang Tang, Jindong Wang, John Mitchell, Kai Shu,Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang,Michael Backes, Neil Zhenqiang Gong, Philip S. Yu,Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shui-wang Ji, Suman Jana, Tianlong Chen, Tianming Liu,Tianyi Zhou, William Wang, Xiang Li, XiangliangZhang, Xiao Wang, Xing Xie, Xun Chen, XuyuWang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen,and Yue Zhao. 2024. Trustllm: Trustworthiness inlarge language models.",
  "Vishesh Thakur. 2023.Unveiling gender bias interms of profession across llms: Analyzing and ad-dressing sociological implications. arXiv preprintarXiv:2307.09162": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models.",
  "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,Ed Chi, Sharan Narang, Aakanksha Chowdhery, andDenny Zhou. 2023. Self-consistency improves chainof thought reasoning in language models": "Kellie Webster, Xuezhi Wang, Ian Tenney, Alex Beutel,Emily Pitler, Ellie Pavlick, Jilin Chen, Ed Chi, andSlav Petrov. 2020. Measuring and reducing genderedcorrelations in pre-trained models. arXiv preprintarXiv:2010.06032. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,and Denny Zhou. 2022. Chain-of-thought prompt-ing elicits reasoning in large language models. InAdvances in Neural Information Processing Systems,volume 35, pages 2482424837. Curran Associates,Inc.",
  ": The prefixes used in for the ablation in": "More qualitative results. Here, we include theexamples for Implication generations (see ), and Self-Refinement Outputs k=2 v/s k=1 (see).Detailed Stereoset table. In the main paper, weinclude the overall stereoset scores (SS), whichdoes not highlight the attribute-wise performanceof approaches. Therefore, we present the completetable (see ) containing the SS scores of eachprompting strategy for attributes such as Gender,Profession, Race, and Religion. To summarizethese results, we note that findings for the OverallSS score are consistent with those of attribute-wisescores.Generation hyperparameters. For all our experi-ments, we set temperature=1.0, while for StereoSetwe also employ a repetition penalty=1.3. If notspecified, our default decoding strategy is beam",
  "BComparing prompting methods withthe other debiasing methods": "In the main paper, we discuss how the infeasibil-ity of accessing the language models logits orprobabilities makes it essential to adopt prompt-based debiasing strategies. However, for a betterunderstanding and completeness, we now evaluateagainst the existing debiasing methods in the litera-ture. More specifically, we choose 1) SDB (Schicket al., 2021), CAFIE (Banerjee et al., 2023) post-hoc debiasing based methods that recalibrate theoutput logits for a fairer decoding, 2) SentenceDe-bias (Liang et al., 2020) a method that modi-fies the LLMs internal features for debiasing, 3)Counterfactual Data Augmentation (CDA) basedtraining methods (Xie and Lukasiewicz, 2023) in-cluding fine-tuning, adapter-tuning, prefix-tuning,and prompt tuning. Due to compute constraints,we ran these evaluations on GPT2-small (125M),although, we did try to extend them to GPTJ (6B)and were unable to run the compute-heavy trainingbased CDA methods. Our results in demon-strate that for GPT2-small, the prompting-basedapproaches are either consistently outperformingor at-par with the other debiasing methods. ForGPTJ, we note that even though the Prefix Prompt-ing methods achieve lower performances, the Self-Refinement based and the Implication based meth-ods are still on-par. To summarize, we note thateven though current prompting frameworks do notutilize the additional information like the other de-biasing approaches, their numbers are competitiveto establish their potential of debiasing. In addition,the simplicity to implement them in any pipelinewithout modifying the models internals furtherreaffirms our belief that our evaluations will en-courage more works towards prompting-based de-biasing.",
  "CUtilizing a Fixed Generic Implication": "In , we propose to generate implicationson the fly using the LLM itself. Now, we inves-tigate this choice and employ a fixed implicationacross all the user prompts and models. Since thisstrategy does not ask the model to generate thereasoning, we divide it into two steps:Step I The first input to the LLM is simply theuser prompt C that generates an output S whichmight be biased.",
  "Step II We then combine S with the fixed impli-cation instruction Ifix to obtain the prompt CIP ,which is used to finally query the LLM": "Final Prompt CIP :Here is a sentence you generated: [OUTPUT]This text contains stereotypes that discriminateagainst people based on their gender, race, reli-gion, or other sensitive attributes. Generate anunbiased text that does not discriminate againstpeople based on their gender, race, religion, orany other sensitive attribute: We compare this approach against the othermethodologies mentioned in .3 and re-port the results in . We consistently observethat equipping LLMs with reasoning tailored tospecific user prompts outperforms generic reason-ing. Thus, adding more color to the notion thatproviding effective reasoning is indeed helpful forLLMs to correct their bias.",
  "DMeasuring Language ModelsPerformance on downstream Questionanswering tasks": "In , we include the LM scores and report thatlanguage modelling ability of the prompt based de-biasing methods is on-par with the baselines. Here,we further study the effect of these techniques onthe performance of LLM for other downstreamtasks such, TruthfulQA and BoolQ. By summariz-ing our results across all models in , we ob-serve that while Prefix Prompting incur an average15% performance decrease on TruthfulQA and nochange on BoolQ, the Self-Refinement based andImplication based approaches achieve at-par num-bers with the baseline. Even further, we observethat Implication based methods achieve the best pe-formance on the TruthfulQA ( 9% increase over thebase model) and the Self-Refinement based meth-ods achieve the best performance on BoolQ ( 1%"
}