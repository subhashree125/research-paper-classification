{
  "Abstract": "Large language models (LLMs) can handlemultilingual and cross-lingual text withina single input;however, previous worksleveraging multilingualism in LLMs primarilyfocus on using English as the pivot languagetoenhancelanguageunderstandingandreasoning. Given that multiple languages are acompensation for the losses caused by a singlelanguages limitations, its a natural next stepto enrich the models learning context throughthe integration of the original input with itsmultiple translations. In this paper, we startby revealing that LLMs learn from ParallelMultilingual Input (PMI). Our comprehensiveevaluation shows that PMI enhances themodels comprehension of the input, achievingsuperiorperformancethanconventionalin-context learning (ICL). Furthermore, toexplore how multilingual processing affectsprediction, we examine the activated neuronsin LLMs.Surprisingly, involving morelanguages in the input activates fewer neurons,leading to more focused and effective neuralactivation patterns. This neural reaction coinci-dently mirrors the neuroscience insight aboutsynaptic pruning, highlighting a similaritybetween artificial and biological brains. Ourparallel multilingual data and code could befound at",
  "Introduction": "Many of the recent large language models (LLMs)are multilingual. Unlike language-specific NLPsystems, such as machine translation systems spe-cialized to a given language pair, these modelsare generally trained on large-scale multilingualdatasets, using a unified vocabulary. Because ofthis training approach, it is possible to learn a uni-versal representation of texts across different lan-",
  ": Comparing the effectiveness of our PMIversus direct and pivot translation on the Qwen-14Bmodel and the FLORES-200 dataset. We also providethe results of ChatGPT in": "guages. Therefore, the resulting models can be di-rectly applied to a variety of multilingual and cross-lingual tasks. For example, most commercializedLLMs can respond to user queries in different lan-guages, without needing to specify what languagesare used. More recently, the multilingual capabili-ties of these models have been shown to help cross-lingual in-context learning (ICL). By providingsimple prompts involving cross-lingual thinkingand reasoning, LLMs can understand and generatetext in languages that were less represented in thetraining data (Qin et al., 2023; Huang et al., 2023;Zhang et al., 2023; Nguyen et al., 2023).Despite the apparent usefulness of multilingual-ism in LLMs, previous work has primarily focusedon using English as the pivot language in languageunderstanding and reasoning. It is a natural nextstep to incorporate more languages and investigatehow these languages are simultaneously processedin LLMs. In this paper, we explore methods thatmake use of parallel multilingual input (PMI) inICL and explain how neurons are activated in thisprocessing. There are two major findings.",
  "contexts from all these languages to make pre-dictions. On the FLORES-200 machine trans-lation benchmark, it achieves improvementsof 11.3 BLEU points and 1.52 COMET pointsover the baseline": "Somewhat surprisingly, as more languages areinvolved in the input, fewer neurons are acti-vated in the LLMs, facilitating more targetedand effective neuron activation patterns. Thisresult links multilingual representation learn-ing to synaptic pruning in neuroscience (Hut-tenlocher et al., 1979; Huttenlocher, 1990): asa brain develops, some neural connections arestrengthened, while others are deemed redun-dant and eliminated, making the transmissionof neural signals more efficient. More specifically, we find that in addition tothe performance improvements from incorporatingmore languages, LLMs can gain advantages fromextensive languages even involving ones that do notsurpass baseline performances. With the help ofhigh-quality machine translation, we efficiently ac-quire abundant parallel input, enabling us to applythis method to various tasks. Experimental resultsacross 8 datasets, 7 languages, and 10 LLMs fur-ther demonstrate the effectiveness and applicabilityof PMI. Since previous neuron activation statistics areprimarily designed for the vanilla transformermodel (Zhang et al., 2022; Li et al., 2023), wehave extended these methods to analyze more ad-vanced LLM architectures. When LLMs receivePMI, we observe simultaneous performance im-provements and neuron inhibition. In addition,PMI selectively activates only a small portion of themost commonly used neurons while inhibiting therest. Further analysis reveals that few-shot learn-ing produces a similar effect on neuron activation,and integrating it with PMI enhances this neuralreaction. These findings are consistently sustainedacross different models and tasks. We introduce our PMI and evaluate it with hu-man translation in .1. Subsequently, wecomprehensively analyze the performance gainsbrought by PMI in .2 and explain its effec-tiveness from a view of neuron activation in . Moreover, we apply PMI to various tasks underreal scenario setups in .",
  "Y=argmax P(yt|f(X))(1)": "where Y denotes the target output of the task andyt denotes the token generated at moment t. PMIextends beyond the conventional ICL approach offeeding LLMs solely with inputs in one language.Instead, it encompasses providing input in multiplelanguages, translated by professional human trans-lators or sophisticated machine translation (MT)systems. The PMI can be shown as:",
  "Y=argmax P(yt|f(M, X))(2)": "where M = {m1, m2, ..., mk} is a parallel lan-guage set containing k translations of the input.The template f() we used is neutral for both theinput X and its translations M, making LLMs can-not distinguish them. shows the differencebetween the conventional ICL and our PMI whentranslating De En.Three aspects should be considered when con-structing a PMI prompt, including the choice oflanguages, the choice of translators, and the dis-play order of languages. As shown in AppendixD.1, our preliminary experiments suggest that: (1)choosing the language that LLMs understand betteris crucial; (2) higher translation quality can lead tolarger improvements; (3) it is preferable to placelanguages better understood at head and tail of theinput sequence.",
  "PMI-5De + Ru + Ro + Uk + It + Es42.487.342.386.9": ": Experiments of PMI, direct and pivot transla-tion on the FLORES-200. We provide k parallel lan-guages denoted as PMI-k. Pivot row reports the bestperformance among all pivot translations in the first lineand the performance of Russian in the second line. Experimental Settings.We conducted trans-lation experiments on the FLORES-200 whichallowed us to probe the upper bound of theperformance by constructing PMI using human-translated parallel sentences.Direct and pivottranslation were our baselines.We utilizedtwo powerful multilingual LLMs,includingChatGPT (gpt-3.5-turbo-0613) and Qwen-14B(Qwen-14B-Chat) (Bai et al., 2023) 1. ChatGPTwas prompted with one-shot for baseline andPMI prompts. While Qwen-14B exhibited con-fusion when processing PMI prompts, so we madesome instruction training data of PMI and baselineprompts, and employed the LoRA technique (Huet al., 2022) to fine-tune Qwen-14B. More detailscan be found in Appendix E. The translation per-formance was evaluated in terms of SacreBLEU(Post, 2018) and COMET-22 (wmt22-comet-da)(Rei et al., 2022). Results and Analyses. delineates theperformance of direct translation (Direct), pivottranslation (Pivot), and PMI in three translationdirections. We see, first of all, PMI achieves thebest result among all the baselines especially whenmore parallel languages are used. Despite that theCOMET score of some baselines reaches as high as 1We also tried Bloomz (Muennighoff et al., 2023), how-ever, compared to the performance on WMT, it showed devianthigh performance on FLORES-200 indicating a data leakage,which is also reported by Zhu et al. (2023). 90, PMI still beats both direct and pivot translationwith significant improvements. Furthermore, wefind that PMI even benefits from parallel languageswhich perform worse than direct translation. Forexample, integrating Russian into PMI achieves bet-ter performance than the baseline. Besides, whenEnglish becomes the original input, PMI leads toa small performance increase. We attribute this tothe fact that LLMs have shown great success inunderstanding English input, leaving little room forimprovement.",
  "Multiple Languages or InformationSources?": "Due to the parallel languages being translated bynumerous human experts in the above experiments,one may argue that the improvement of PMI resultsfrom multiple information sources rather than lan-guages. Specifically, multiple information sourcescan bring different perspectives of the original in-put, and translating inputs derived from humanexperts is like doing ensemble learning based onvarious strong translation systems. To separatelyquantify the effects of multiple languages and infor-mation sources, we decompose the PMI based onthe human translations (PMIGT ) into three prompt-ing strategies:",
  "Mono-source and monolingual: The origi-nal input is paraphrased into different versionswithout changing the semantics. We denotethis prompt as PMIPA": "Multi-source but monolingual: The humantranslation texts used in PMI are translatedinto the language of the original input by onetranslator. This prompt integrates differentinformation sources but expresses in one lan-guage, e.g., we provide De + De (Ru) + De(Fr) + De (Uk) + De (It) + De (Es) to LLMswhere the language in parentheses representsthe human translation text. We call it PMIMS. Multilingual but mono-source: The originalinput is translated into different parallel lan-guages by one translator. The source of thisprompt is only the original input whereas theexpression holds a multilingual form, like De+ Ru (De) + Fr (De) + Uk (De) + It (De) + Es(De), which is represented by PMIML. Wealso illustrate these prompts in .",
  "inhibit": ": The impact of ReLU-like activation functions on neurons during the forward process of transformermodels. Figure (a) shows that activation function () like ReLU and some of its variants, when encounteringnegative inputs, saturate to zero and weaken the values multiplied by their outputs. Figure (b) details the equivalencebetween artificial neurons and the linear-transform matrix of MLPs. Figure (c) illustrates that ReLU-like activationfunctions inhibit neurons in Wup and some weights of Wdown when the input is negative.",
  "PMIML45.40.589.70.140.11.186.80.2PMIGT52.990.945.988.1": ": The ablation study of the mono-source andmonolingual (PMIP A), multi-source but monolingual(PMIMS), multilingual but mono-source (PMIML),multi-source and multilingual (PMIGT ) prompts on theFLORES-200. The best results are in bold among allthe prompts except for PMIGT . Experimental Settings.With access to Qwen-14B, ChatGPT and GPT-4 (gpt-4-0613), we con-ducted experiments on two translation directionsof FLORES-200. The translation system used byboth PMIMS and PMIML prompt was the NLLB-54B model (Costa-juss et al., 2022). We derivedthe paraphrased sentences by requesting ChatGPT.Notably, Qwen-14B used in this experiment is dif-ferent from the one in the previous experiment, aswe have to fine-tune Qwen-14B with extra trainingdata based on the PMIMS prompt for fairness.",
  "Results and Analyses.From , we can seethat both PMIMS and PMIML prompt achieve im-provement most of the time, while none of them": "can reach the same performance as the PMIGTprompt. In addition, the PMIML prompt far outper-forms the PMIPA prompt, which demonstrates thatmultilingual input helps LLMs again. Also, we seethat despite the similar baseline performance, GPT-4 always outperforms ChatGPT significantly whenbeing armed with PMI, suggesting that strongerLLMs benefit more from the PMI.",
  "PMI Can Help: From a View of NeuronActivation": "Although LLMs benefit from PMI, there is stillno idea about how this mechanism works. Con-sidering that knowledge is memorized in differentneurons in transformer models (Dai et al., 2022),hence a straightforward hypothesis is that givingthe input in multiple languages may increase thenumber of activated neurons in the inference pro-cess. To quantify how many neurons in transformermodels are activated during inference, some workspropose to make statistics of the nonzero values inthe intermediate output of multi-layer perceptrons(MLPs) after a ReLU activation function (Zhanget al., 2022; Li et al., 2023). This is based on theidea that, in matrix multiplication, zero can be omit-ted; therefore, neurons that output zero are consid-ered inhibited while others are activated. Next, wewill explain this statistical method.",
  "Activation Proportion (%)": ": The COMET score and the activation proportion of Qwen-14B armed with different prompts on FLORES-200. Notably, whether a method inhibits or activates neurons depends on its activation proportion being belowor above the baseline level. Thus, a point on the curves suggests inhibition if it falls below the first point, andactivation if it exceeds the first point. and indicates the model used in .1 and 2.2, respectively. #Times Being Activated DirectPMI-1PMI-3PMI-5 : The distribution of the top 1% of activatedneurons in Qwen-14B on FLORES-200 De En. Thehorizontal axis represents different neurons arranged indescending order based on the number of times they areactivated. where X and Y stand for input and output, respec-tively. Wup and Wdown represent different MLPlayers containing artificial neurons. The vanillatransformer uses ReLU as the activation function(Vaswani et al., 2017), i.e., max(x, 0). In Fig-ure 3 (b) and (c), ReLU outputs zero value meanstwo aspects: the neuron in Wup is inhibited andstripped from the whole neural network; the weightin Wdown that accepts the zero value is inhibited. Counting activated neurons in MLPs with ReLUvariants.Despite the success of ReLU, recentworks find that making a ReLU-like non-linearityto output negative values can increase trainingspeed (Clevert et al., 2016; Hendrycks and Gimpel,2016). Hence, as shown in , these variantsof ReLU become popular among LLMs. We drawReLU, GELU and SiLU in (a). We see de-spite both GELU and SiLU performing as smoothReLU, they remain the basic character, i.e., saturat-ing to zero at negative input values and protectingpositive input values. In other words, these ReLUvariants significantly reduce the absolute value ofany negative input to a level that is close to or equalto zero. As a result, some neurons and weightsare inhibited as before. This motivates us to make statistics of activated neurons in MLPs with ReLUvariants by counting the output values of the acti-vation function that are greater than zero.Other works combine GELU and SiLU with thegated linear units (Shazeer, 2020) like this:",
  "shows performances and the proportionof activated neurons2 on Qwen-14B models. Fromthe results, we get the following observations:": "Activated neurons are far fewer than inhib-ited ones.Despite performing dense computa-tions, only a small number of neurons around 27%are activated in Qwen-14B during the inferencestage, which is similar to the sparse activation phe-nomenon observed by Li et al. (2023). Besides, thedifferences in the proportion of activated neuronsare small in numerical terms, we attribute this tothe finding that few parameters are in charge oflinguistic competence in LLMs (Zhao et al., 2023). 2Note that the proportion mentioned is derived by aver-aging the percentages of activated neurons for each tokengenerated by an LLM across the dataset. We discuss thisimplementation in detail in Appendix B. More languages, more inhibited neurons, moreperformance gain.As shown in (a) and(b), if we add more parallel languages in PMI, thenthe proportion of activated neurons becomes smallmeanwhile LLM yields better translations, indi-cating a consistent correlation between inhibitingneurons and performance improvements.",
  "Multilingual input inhibits neurons whereasmonolingual input activates neurons.Figure": "4 (c) and (d) show the proportion of activated neu-rons caused by monolingual and multilingual input.We see that, compared to direct translation, thoughmonolingual and multilingual input can achieve bet-ter performance, their influence on neurons is theopposite, i.e., monolingual input activates neuronswhereas multilingual input inhibits neurons. More-over, PMIGT inhibits more neurons than PMIMLand PMIMS activates more neurons than PMIPA. PMI simulates a one-off synaptic pruning.During the maturation of biological brains, synap-tic pruning is a necessary process that removes lesscommonly used neural connections, thus makingfrequently-used neural pathways more powerfuland efficient (Huttenlocher et al., 1979; Hutten-locher, 1990). In other words, the brain benefitsfrom little and precise neuron activation. We showthat PMI simulates the synaptic pruning during theinference from two aspects: (1) as demonstratedabove, PMI inhibits neurons; (2) PMI promotesmore precise neuron activation. recordsthe activation state of the most commonly usedneurons. It shows that compared to the baselineprompt, PMI promotes the activation of the top1% of neurons commonly used. Meanwhile, otherneurons rarely used are activated fewer times toachieve an overall effect of inhibition, as shownin . This indicates that more targeted andeffective neuron activation patternswhere someimportant neurons are activated more while othersless oftencould be facilitated by PMI. Synapticpruning occurs during the maturation of the brain,while PMI enhances models specifically at theirinference stages, not during training. Therefore,we propose that PMI simulates a one-off synapticpruning, exerting a short-term effect on models.",
  "Tasks and Evaluation": "We totally evaluated PMI on six tasks. (1) Ma-chine Translation: We conducted experiments onfive high-resource directions of WMT22 and onelow-resource direction of WMT21. (2) NatureLanguage Inference: We chose RTE (Wang et al.,2019) and three languages in XNLI (Conneau et al.,2018). The metric was accuracy. (3) ReadingComprehension: We did evaluation on this longsequence task using BoolQ3 (Clark et al., 2019).Our metric was accuracy. (4) Text Simplification:We used Wiki-auto (Jiang et al., 2020), and SARI4 (Alva-Manchego et al., 2020) was chosen as themetric. (5) Abstractive Summarization: For thisparagraph-level task, we mainly reported the perfor-mance on two languages in XLSum (Hasan et al.,2021). The metric was F1-Rouge5 (Lin, 2004). (6)Mathematical Reasoning: We conducted exper-iments on GSM8K (Cobbe et al., 2021). We alsoapply the chain-of-thought (CoT) technique (Weiet al., 2022) to explore whether PMI could enhancethe reasoning capabilities of large language models(LLMs). The metric was accuracy. To streamlinecomputation, we reconstructed our test set by ran-domly selecting 1000 samples from BoolQ, Wiki-auto, and XLSum, along with 3000 samples fromXNLI, leaving other tasks unchanged.",
  "Models": "The experiment was conducted on 8 instruction-tuned open source multilingual LLMs whoseparameters range from 7B to 176B, includingLLaMA3-8B (AI@Meta, 2024), Bloomz-176B(Muennighoff et al., 2023), Qwen-7B, -14B, -72B(Bai et al., 2023), ALMA-13B (Xu et al., 2023), Yi-34B (01-ai, 2023) and mT0-13B (Scao et al., 2022).We also evaluated the effectiveness of PMI on twocommercial ones, involving ChatGPT and GPT-4. All of them are pre-trained with multilingualcorpus except for ALMA-13B which is speciallyfine-tuned for the MT task based on LLaMA2-13B(Touvron et al., 2023). Other details about mod-els, training, and decoding setups can be found inAppendix E.",
  "report the results of one-shot on ChatGPT whilezero-shot on others for the best performance": "Pivot Promptindicates that the original inputis translated into a parallel language, and LLMsare fed with the translation to accomplish the task.To ensure high-quality translations and the repro-ducibility of our study, we utilized the publicly andeasily accessible GPT-4 for translating the WMTand GSM8K datasets. For other datasets, we em-ployed ChatGPT. We display the maximum scoresof pivot prompts, see Appendix F for full results.",
  "Results and Analyses": "PMI effectively pushes the boundaries acrossvarious tasks and languages. suggeststhat PMI achieves superior results across 6 trans-lation directions including high-resource and low-resource source languages. Additionally, Tables 4and 5 show PMIs competitive edge against base-lines in various tasks, irrespective of text length.Furthermore, in , we can see that PMI out-performs few-shot learning on the translation task, especially in terms of the COMET score.We also evaluate the effectiveness of PMI onmathematical reasoning tasks and CoT scenarios. suggests that PMI can further boost thesuperior reasoning performance of GPT models,with accuracy nearly reaching 96% on the GSM8Kbenchmark. Beyond the noted improvements in thecommonly used 5-shot and 8-shot scenarios, wealso observed significant performance gains withPMI in 0-shot settings for GPT-4. We attribute thisto PMI aiding LLMs in gaining a more compre-hensive understanding of the tasks in scarce shotsscenarios.",
  "Weak model augments strong model": "shows that when we utilize parallel multilingualtranslations from GPT-4 to augment a strongerLLM like GPT-4o, the performance of GPT-4o+PMI surpasses two exceptional baselines, in-cluding GPT-4 and GPT-4o. It underscores thenecessity of using PMI instead of relying solely ona remarkable MT system. Also, this demonstratesthat PMI still yields better performance when the",
  "parallel translations come from a weak model, fur-ther validating its effectiveness and practicality": "Automatic translation triggers learning fromPMI.Since the lack of high-quality human trans-lation, all the translations used in experiments comefrom GPT-4 or ChatGPT. We see, on the one hand,PMI powered by MT outperforms pivot prompts.Even though some pivot prompts have inferior per-formance than the direct prompt, integrating theselanguages into PMI still boosts the comprehensionof LLMs. On the other hand, shows thatPMI armed with MT achieves improvements byinhibiting neurons and promoting more precise ac-tivation. These results demonstrate the consistentlearning behavior triggered by translations fromhuman experts and MT systems. Few-shot learning performs similarly as PMI. and suggest that few-shot learningalso inhibits neurons and facilitates more preciseactivation, and combining few-shot learning andPMI further enhances this neuron reaction. Superiority of PMI remains when English is theoriginal or parallel language.Despite the sub-tle improvements on FLORES-200 En De in.1, results of RTE, BoolQ, and WMT De Fr show that PMI not only achieves prime per-formance on English-source inputs but also outper-forms all pivot prompts when we choose Englishas one of the parallel languages.We discuss the fine-tuning demands of PMI in",
  "Related Work": "Multi-way Neural Machine Translation.Multi-way input is a successful method to enhance mul-tilingual neural machine translation (MNMT) sys-tems by providing the source language and its trans-lations in different languages (Och and Ney, 2001).In the inference stage, most works rely on high-quality translations from human experts (Zoph andKnight, 2016; Firat et al., 2016; Nishimura et al.,2018; Choi et al., 2018). However, this groundtruth multilingual data is scarce in reality, limitingthe application of multi-way input. Different frommulti-way MNMT, we find that LLMs benefit fromPMI even when parallel multilingual input is de-rived from automatic MT systems, enabling us toapply PMI on a wide range of tasks. Statistics of Activated Neurons in TransformerModels.Recently, statistics of activated neuronsin transformer models by counting nonzero valuesin the output of ReLU is introduced by Zhang et al.(2022). Moreover, Li et al. (2023) show that thesparse activation of neurons is ubiquitous. In this",
  "work, we extend the statistical method to advancedtransformer architectures. We hope this effort canhelp deepen our insights into the learning mecha-nism behind LLMs": "Cross-lingual In-context Learning.Severalworks have investigated cross-lingual prompts(Wang et al., 2023; Shi et al., 2023; Mu et al., 2023).One line of research requests LLMs to address theinput problem in multiple languages orderly, thenemphasizes self-consistency by aligning results ofthese languages to improve performance on reason-ing tasks (Qin et al., 2023). To augment LLMsperformance with multilingual input, other worksencourage LLMs to rephrase the input in Englishand then perform step-by-step analysis, indeed turn-ing English into a pivot language (Huang et al.,2023; Zhang et al., 2023; Nguyen et al., 2023). Ourwork, in contrast, explores the behavior of LLMsthat learns from parallel input in multiple languagessimultaneously, revealing a new ICL capability.",
  "Limitations": "In fact, during the inference, LLMs will inevitablyrefer to the semantics of the translation in PMIto understand the input comprehensively. As a re-sult, though our extensive experiments have demon-strated that LLMs can benefit from PMI, the qualityof translation will influence the final performance.On the other hand, we do not discuss the effect ofcross-language such as code-switch multilingualprompts because it deviates from the intention ofPMI, i.e., providing parallel input. However, it isstill a potential direction and we leave it for futurework.",
  "Acknowledgements": "This work was supported in part by the NationalScience Foundation of China (No.62276056), theNatural Science Foundation of Liaoning Provinceof China (2022-KF-16-01), the Fundamental Re-search Funds for the Central Universities (Nos.N2216016 and N2316002), the Yunnan Fundamen-tal Research Projects (No. 202401BC070021), andthe Program of Introducing Talents of Discipline toUniversities, Plan 111 (No.B16009). The authorswould like to thank Yunhe Gao, Chi Hu, Erfeng He,and anonymous reviewers for their advices.",
  "AI@Meta. 2024. Llama 3 model card. Github": "Fernando Alva-Manchego, Louis Martin, Antoine Bor-des, Carolina Scarton, Benot Sagot, and Lucia Spe-cia. 2020. ASSET: A dataset for tuning and evalu-ation of sentence simplification models with multi-ple rewriting transformations. In Proceedings of the58th Annual Meeting of the Association for Compu-tational Linguistics, ACL 2020, Online, July 5-10,2020, pages 46684679. Association for Computa-tional Linguistics. Duarte Alves, Nuno Miguel Guerreiro, Joo Alves,Jos Pombal, Ricardo Rei, Jos Guilherme Camargode Souza, Pierre Colombo, and Andr Martins. 2023.Steering large language models for machine trans-lation with finetuning and in-context learning. In",
  "Findings of the Association for Computational Lin-guistics: EMNLP 2023, Singapore, December 6-10,2023, pages 1112711148. Association for Computa-tional Linguistics": "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, FeiHuang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,Xuancheng Ren, Chuanqi Tan, Sinan Tan, JianhongTu, Peng Wang, Shijie Wang, Wei Wang, ShengguangWu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, JianYang, Shusheng Yang, Yang Yao, Bowen Yu, HongyiYuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang,Yichang Zhang, Zhenru Zhang, Chang Zhou, Jin-gren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023.Qwen technical report. CoRR, abs/2309.16609. Gyu-Hyeon Choi, Jong-Hun Shin, and Young Kil Kim.2018.Improving a multi-source neural machinetranslation model with corpus extension for low-resource languages. In Proceedings of the EleventhInternational Conference on Language Resourcesand Evaluation, LREC 2018, Miyazaki, Japan, May7-12, 2018. European Language Resources Associa-tion (ELRA). Christopher Clark, Kenton Lee, Ming-Wei Chang,Tom Kwiatkowski, Michael Collins, and KristinaToutanova. 2019. Boolq: Exploring the surprisingdifficulty of natural yes/no questions. In Proceedingsof the 2019 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, NAACL-HLT 2019,Minneapolis, MN, USA, June 2-7, 2019, Volume 1(Long and Short Papers), pages 29242936. Associa-tion for Computational Linguistics. Djork-Arn Clevert, Thomas Unterthiner, and SeppHochreiter. 2016. Fast and accurate deep networklearning by exponential linear units (elus). In 4th In-ternational Conference on Learning Representations,ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,Conference Track Proceedings. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, ReiichiroNakano, Christopher Hesse, and John Schulman.2021. Training verifiers to solve math word prob-lems. CoRR, abs/2110.14168. Alexis Conneau, Ruty Rinott, Guillaume Lample, Ad-ina Williams, Samuel R. Bowman, Holger Schwenk,and Veselin Stoyanov. 2018. XNLI: evaluating cross-lingual sentence representations. In Proceedings ofthe 2018 Conference on Empirical Methods in Natu-ral Language Processing, Brussels, Belgium, Octo-ber 31 - November 4, 2018, pages 24752485. Asso-ciation for Computational Linguistics.",
  "Marta R. Costa-juss, James Cross, Onur elebi,Maha Elbayad, Kenneth Heafield, Kevin Heffer-nan, Elahe Kalbassi, Janice Lam, Daniel Licht,": "Jean Maillard, Anna Sun, Skyler Wang, GuillaumeWenzek, Al Youngblood, Bapi Akula, Loc Bar-rault, Gabriel Mejia Gonzalez, Prangthip Hansanti,John Hoffman, Semarley Jarrett, Kaushik RamSadagopan, Dirk Rowe, Shannon Spruit, ChauTran, Pierre Andrews, Necip Fazil Ayan, ShrutiBhosale, Sergey Edunov, Angela Fan, CynthiaGao, Vedanuj Goswami, Francisco Guzmn, PhilippKoehn, Alexandre Mourachko, Christophe Rop-ers, Safiyyah Saleem, Holger Schwenk, and JeffWang. 2022.No language left behind:Scal-ing human-centered machine translation.CoRR,abs/2207.04672. Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, BaobaoChang, and Furu Wei. 2022. Knowledge neuronsin pretrained transformers. In Proceedings of the60th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), ACL2022, Dublin, Ireland, May 22-27, 2022, pages 84938502. Association for Computational Linguistics. Orhan Firat, Baskaran Sankaran, Yaser Al-Onaizan,Fatos T. Yarman-Vural, and Kyunghyun Cho. 2016.Zero-resource translation with multi-lingual neuralmachine translation.In Proceedings of the 2016Conference on Empirical Methods in Natural Lan-guage Processing, EMNLP 2016, Austin, Texas, USA,November 1-4, 2016, pages 268277. The Associa-tion for Computational Linguistics. Tahmid Hasan, Abhik Bhattacharjee, Md. Saiful Is-lam, Kazi Samin Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M. Sohel Rahman, and Rifat Shahri-yar. 2021.Xl-sum: Large-scale multilingual ab-stractive summarization for 44 languages. In Find-ings of the Association for Computational Linguis-tics: ACL/IJCNLP 2021, Online Event, August 1-6,2021, volume ACL/IJCNLP 2021 of Findings of ACL,pages 46934703. Association for ComputationalLinguistics.",
  "hiyouga. 2023. Llama factory": "Edward J. Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. 2022. Lora: Low-rank adaptation oflarge language models. In The Tenth InternationalConference on Learning Representations, ICLR 2022,Virtual Event, April 25-29, 2022. OpenReview.net. Haoyang Huang, Tianyi Tang, Dongdong Zhang, XinZhao, Ting Song, Yan Xia, and Furu Wei. 2023.Not all languages are created equal in llms: Improv-ing multilingual capability by cross-lingual-thoughtprompting. In Findings of the Association for Com-putational Linguistics: EMNLP 2023, Singapore, De-cember 6-10, 2023, pages 1236512394. Associationfor Computational Linguistics.",
  "Peter R Huttenlocher et al. 1979. Synaptic density inhuman frontal cortex-developmental changes and ef-fects of aging. Brain Res, 163(2):195205": "Chao Jiang, Mounica Maddela, Wuwei Lan, YangZhong, and Wei Xu. 2020. Neural CRF model forsentence alignment in text simplification. In Proceed-ings of the 58th Annual Meeting of the Associationfor Computational Linguistics, ACL 2020, Online,July 5-10, 2020, pages 79437960. Association forComputational Linguistics. Zonglin Li, Chong You, Srinadh Bhojanapalli, DaliangLi, Ankit Singh Rawat, Sashank J. Reddi, Ke Ye,Felix Chern, Felix X. Yu, Ruiqi Guo, and SanjivKumar. 2023. The lazy neuron phenomenon: Onemergence of activation sparsity in transformers. InThe Eleventh International Conference on LearningRepresentations, ICLR 2023, Kigali, Rwanda, May1-5, 2023. OpenReview.net.",
  "Chin-Yew Lin. 2004. ROUGE: A package for auto-matic evaluation of summaries. In Text Summariza-tion Branches Out, pages 7481, Barcelona, Spain.Association for Computational Linguistics": "Yongyu Mu, Abudurexiti Reheman, Zhiquan Cao,Yuchun Fan, Bei Li, Yinqiao Li, Tong Xiao, Chun-liang Zhang, and Jingbo Zhu. 2023. Augmentinglarge language model translators via translation mem-ories. In Findings of the Association for Compu-tational Linguistics: ACL 2023, Toronto, Canada,July 9-14, 2023, pages 1028710299. Association forComputational Linguistics. Niklas Muennighoff, Thomas Wang, Lintang Sutawika,Adam Roberts, Stella Biderman, Teven Le Scao,M. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai-ley Schoelkopf, Xiangru Tang, Dragomir Radev,Alham Fikri Aji, Khalid Almubarak, Samuel Al-banie, Zaid Alyafeai, Albert Webson, Edward Raff,and Colin Raffel. 2023.Crosslingual generaliza-tion through multitask finetuning. In Proceedingsof the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),ACL 2023, Toronto, Canada, July 9-14, 2023, pages1599116111. Association for Computational Lin-guistics. Xuan-Phi Nguyen, Sharifah Mahani Aljunied, ShafiqJoty, and Lidong Bing. 2023. Democratizing llmsfor low-resource languages by leveraging their en-glish dominant abilities with linguistically-diverseprompts. CoRR, abs/2306.11372. Yuta Nishimura, Katsuhito Sudoh, Graham Neubig, andSatoshi Nakamura. 2018. Multi-source neural ma-chine translation with data augmentation. In Pro-ceedings of the 15th International Conference onSpoken Language Translation, IWSLT 2018, Bruges,Belgium, October 29-30, 2018, pages 4853. Interna-tional Conference on Spoken Language Translation.",
  "Franz Josef Och and Hermann Ney. 2001. Statisticalmulti-source translation. In Proceedings of MachineTranslation Summit VIII, MTSummit 2001, Santiagode Compostela, Spain, September 18-22, 2001": "Matt Post. 2018. A call for clarity in reporting BLEUscores. In Proceedings of the Third Conference onMachine Translation: Research Papers, WMT 2018,Belgium, Brussels, October 31 - November 1, 2018,pages 186191. Association for Computational Lin-guistics. Libo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang,and Wanxiang Che. 2023.Cross-lingual prompt-ing: Improving zero-shot chain-of-thought reasoningacross languages. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP 2023, Singapore, December 6-10, 2023, pages 26952709. Association for Compu-tational Linguistics. Ricardo Rei, Jos G. C. de Souza, Duarte M. Alves,Chrysoula Zerva, Ana C. Farinha, Taisiya Glushkova,Alon Lavie, Lusa Coheur, and Andr F. T. Martins.2022.COMET-22: unbabel-ist 2022 submissionfor the metrics shared task. In Proceedings of theSeventh Conference on Machine Translation, WMT2022, Abu Dhabi, United Arab Emirates (Hybrid),December 7-8, 2022, pages 578585. Association forComputational Linguistics. Teven Le Scao, Angela Fan, Christopher Akiki, El-lie Pavlick, Suzana Ilic, Daniel Hesslow, RomanCastagn, Alexandra Sasha Luccioni, Franois Yvon,Matthias Gall, Jonathan Tow, Alexander M. Rush,Stella Biderman, Albert Webson, Pawan Sasanka Am-manamanchi, Thomas Wang, Benot Sagot, NiklasMuennighoff, Albert Villanova del Moral, OlatunjiRuwase, Rachel Bawden, Stas Bekman, AngelinaMcMillan-Major, Iz Beltagy, Huu Nguyen, LucileSaulnier, Samson Tan, Pedro Ortiz Suarez, Vic-tor Sanh, Hugo Laurenon, Yacine Jernite, JulienLaunay, Margaret Mitchell, Colin Raffel, AaronGokaslan, Adi Simhi, Aitor Soroa, Alham FikriAji, Amit Alfassy, Anna Rogers, Ariel KreisbergNitzav, Canwen Xu, Chenghao Mou, Chris Emezue,Christopher Klamm, Colin Leong, Daniel van Strien,David Ifeoluwa Adelani, and et al. 2022. BLOOM:A 176b-parameter open-access multilingual languagemodel. CoRR, abs/2211.05100.",
  "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, Nikolay": "Bashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurlien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N. Gomez, LukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. In Advances in Neural Information Pro-cessing Systems 30: Annual Conference on NeuralInformation Processing Systems 2017, December 4-9,2017, Long Beach, CA, USA, pages 59986008. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-preet Singh, Julian Michael, Felix Hill, Omer Levy,and Samuel R. Bowman. 2019. Superglue: A stickierbenchmark for general-purpose language understand-ing systems.In Advances in Neural InformationProcessing Systems 32: Annual Conference on Neu-ral Information Processing Systems 2019, NeurIPS2019, December 8-14, 2019, Vancouver, BC, Canada,pages 32613275.",
  "Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Has-san Awadalla. 2023. A paradigm shift in machinetranslation: Boosting translation performance oflarge language models. CoRR, abs/2309.11674": "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale,Rami Al-Rfou, Aditya Siddhant, Aditya Barua, andColin Raffel. 2021. mt5: A massively multilingualpre-trained text-to-text transformer. In Proceedingsof the 2021 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, NAACL-HLT 2021,Online, June 6-11, 2021, pages 483498. Associationfor Computational Linguistics. (a) #Times Being Activated PMI4shot+PMI5shot+PMI Direct4shot5shot (b) #Times Being Activated PMI4shot+PMI5shot+PMI Direct4shot5shot :Distribution of all activated neurons inBloomz-176B on RTE. The horizontal axis of the figure(a) represents different neurons arranged in descendingorder of the number of times being activated, and thehorizontal axis of the figure (b) stands for the numberof transformer layers. Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li,Maosong Sun, and Jie Zhou. 2022. Moefication:Transformer feed-forward layers are mixtures of ex-perts. In Findings of the Association for Computa-tional Linguistics: ACL 2022, Dublin, Ireland, May22-27, 2022, pages 877890. Association for Com-putational Linguistics.",
  "Jun Zhao, Zhihao Zhang, Yide Ma, Qi Zhang, Tao Gui,Luhui Gao, and Xuanjing Huang. 2023. UnveilingA core linguistic region in large language models.CoRR, abs/2310.14928": "Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu,Lingpeng Kong, Jiajun Chen, Lei Li, and ShujianHuang. 2023. Multilingual machine translation withlarge language models: Empirical results and analy-sis. CoRR, abs/2304.04675. Barret Zoph and Kevin Knight. 2016. Multi-sourceneural translation. In NAACL HLT 2016, The 2016Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, San Diego California, USA,June 12-17, 2016, pages 3034. The Association forComputational Linguistics.",
  "Translate to German": ": An illustration of different strategies for con-structing parallel inputs in .2. Taking De Entranslation as an example, PMIGT consists of multilin-gual human translations from several experts; PMIP Ais made up of monolingual sentences paraphrased fromthe original German input; PMIMS is composed of Ger-man translations where their source language texts arefrom different experts; and PMIML includes multilin-gual translations of the original German input derivedfrom a single translator. the original input of tasks in our prompts. All ofthe prompts are listed in . In this table,the content that is italicized and highlighted in grayindicates variable elements, which should be re-placed according to the specific task requirements.",
  "BMore Details About Statistical Methodof Activated Neurons": "Implementation of Counting Activated Neurons.During the inference stage, each time LLMs calcu-late the representation of a token including inputand output, the intermediate result of MLPs standsfor an activation state of neurons. It is essential tonote that we only make statistics of activated neu-rons based on the intermediate result correspond-ing to the output tokens. This implementation isbased on two concerns: (1) only the activation stateof neurons corresponding to the output tokens di-rectly contributes to the model-generated results.(2) since different prompting strategies differ inthe length of input significantly, if the statistics aremade based on both input and output tokens, thenthe results will be disturbed by the factor of lengthbut not the actual impact of prompts, resulting inmisdirected conclusions.",
  "CSupplementary Results About NeuronActivation": "In (a), we can see that: (1) in the inter-val from 0 to 200000, the curves of PMI, few-shotlearning and their combination are above that ofbaseline (i.e., Direct), indicating that they activatetop 200,000 commonly used neurons; (2) beyondthe 200,000 mark, these curves are below the curveof baseline, demonstrating that these prompts per-form inhibiting other less used neurons. Further-more, in (b), we can see that the inhib-ited neurons concentrate in the back two-thirds ofmodel layers. Figures 10 and 7 report the distribu-tion of the top 1% of activated neurons in Bloomz-176B where PMI shows a clear impact of activationon most commonly used neurons.To visualize the activation happening in eachneuron, in , we draw heat maps of Qwen-14B and Bloomz-176B when using the PMI-5 totranslate De En in the FLORES-200 and WMTdataset, respectively. It suggests that the neurons of",
  "(b) Bloomz-176B": ": The heat maps of activated neurons in MLPs of Qwen-14B and Bloomz-176B when using the PMI-5to translate De En in the FLORES-200 and WMT dataset, respectively. The horizontal axis represents thedimension of the middle outputs in MLPs (i.e., each neuron). The vertical axis represents the number of layers inthe model. And each element in the map stands for the number of times of was activated during the inference stage. #Times Being Activated DirectPMI-1PMI-3PMI-5",
  ": The distribution of the top 1% of activatedneurons in Bloomz-176B on WMT22 De En. Thehorizontal axis represents different neurons arranged indescending order of the number of times being activated": "Qwen-14B are more active while those of Bloomz-176B seem lazy and are activated fewer times. Fur-thermore, in each model, there are significant dif-ferences in the number of times being activatedamong different layers.In , we also make statistics of activatedneurons in Bloomz-176B and Qwen-14B duringthe inference on the WMT dataset. shows the results of few-shot learning,which suggests that it also inhibits neurons andmore neurons are inhibited after the LLM is fine-tuned.",
  "D.1Preliminary Experiments of ConstructingPMI": "Choose the parallel language that LLMs can un-derstand.We test the impact of selecting parallellanguages on the PMI-1 translating De En ofthe FLORES-200, where Zh, Fr, Uk, and It areselected as the parallel languages. By comparingthe results of translating them to English, we exam-ine the models understanding of these languages.In , experimental results show that PMI-1 achieves better performance when the score ofpivot translation is high and returns worse resultswhen the score of pivot translation is low. This",
  "suggests that choosing parallel languages that themodel comprehends better can bring more benefitsfor PMI": "Provide the highest quality translations as far asyou can.Here, we utilize some translation sys-tems with different performances to construct theparallel input of PMI in various qualities, includingNLLB-1.3B, NLLB-54B, Qwen-14B, ChatGPT,and GPT-4. Experiments are conducted on bothQwen-14B and ChatGPT. In , translationsystems are arranged in the ascending order of theirtranslation performance according to the curve, andthe results show that higher quality of translationscan result in larger improvements. Place better understood language at the headand tail of the input sequence.We test the per-formance of PMI prompts with identical paralleltexts but in different language order, and conductexperiments on De En and Zh En of theFLORES-200 using Qwen-14B. Results in show that an LLM yields superior results whenGerman is placed at the beginning and Spanish isplaced at the end. Considering German and Span-ish achieve higher score than other languages, we",
  "D.3Effectiveness of PMI on more modernLLMs": "As LLMs develop further, we anticipate that moreand more LLMs will benefit from PMI in the future.Here, we make experiments on Qwen1.5-14B, asuccessor of Qwen-14B. The latter is fine-tunedwith PMI prompts in our paper, while the former isthe original official version. From , we cansee that Qwen1.5-14B responds to PMI prompts",
  "D.4Self-augmentation": "In , we report the experimental results ofprompting Qwen-14B with PMI while the parallelsentence pairs are translated by Qwen-14B itself.Although the improvements resulting from PMI arenot as large as those reported in , PMI stilloutperforms baselines, especially at the COMETscore. This further demonstrates the applicabilityof PMI. We attribute the diminished performancegains to the lower quality of translations producedby Qwen-14B compared to those from GPT-4.",
  "D.5Inference Speed": "Since the inference speed of LLMs inevitably slowsdown as the input sequence lengthens, we alsofocus on the trade-off between performance andinference speed when increasing the number ofparallel languages in the PMI. Here, we conductexperiments on the FLORES-200 De En andQwen-14B model. indicates that for everyadditional parallel language integrated into the PMIinput, there is an approximate 30% increase of time cost, along with a 5% improvement of performance.Notably, when the number of parallel languagesreaches three, the improvement can reach up to24.34%. Despite the increased inference cost, it isreasonable and acceptable considering the substan-tial performance gain.",
  "E.3Training Setups": "Limited by parameters and training data, it mightbe a challenge for every LLM to understand PMIprompts inherently. To address this, we conductedtraining data and fine-tuned the models whichseemed confused when facing the PMI prompt.Specifically, we leveraged LLaMA-Factory6 (hiy-ouga, 2023) and the LoRA technology to train mod-els, where we set the LoRA-rank to 8, LoRA-alphato 32 and dropout to 0.1. Since the different amountof trainable parameters in the LoRA module, weapplied different training strategies to ensure thatevery model can adequately understand prompts of",
  "Direct": "You will be presented with a long text. Your task is to summarizethis text in 1-2 sentences in source-language , capturing the mostimportant and core content. The summary should distill the essence ofthe article concisely and accurately. Please provide a single summaryfor the text without any explanation. Here is the text:source-textYour summary:",
  "Youwillbepresentedwiththesamesentenceinfourdifferentlanguages:source-language ,parallel-language1 ,": "parallel-language2 , and parallel-language3 .These sentencesconvey the exact same meaning. Your task is to simplify the sen-tence into source-language to make it easier to understand, whilemaintaining its core meaning and factual content. It is important tonote that since all sentences have the same meaning, you only needto provide one simplified source-language version. Please gener-",
  "You will be provided with a set of sentence pairs that are se-mantically identical but presented in four different languages:src-language ,parallel-language1 ,parallel-language2 ,and": "parallel-language3 . Each pair consists of a premise and a hypothe-sis. Despite the language differences, the meaning of these sentencesis the same across all languages. Your task is to analyze these sen-tence pairs and determine the relationship between the premise andthe hypothesis. There are two possible relationships: entailmentand not_entailment. entailment means the first sentence logicallyimplies the second one. not_entailment means the first sentencelogically conflicts with the second one. Please provide a single pre-diction for the relationship based on these sentence pairs, without anyexplanation. Here are the sentence pairs:src-language :",
  "PMI": "You are provided with a set of parallel mathematical problems inmultiple languages. Each problem presents the same mathematicalquestion, but expressed in different languages. Your task is to com-prehend the problem in any of these languages, reason through theproblem in English, and finally, generate a solution in English.Question in English: source-sentenceQuestion in parallel-language : parallel-sentence"
}