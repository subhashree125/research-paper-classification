{
  "Abstract": "Automatic counterspeech generation methodshave been developed to assist efforts in com-bating hate speech. Existing research focuseson generating counterspeech with linguistic at-tributes such as being polite, informative, andintent-driven. However, the real impact of coun-terspeech in online environments is seldom con-sidered. This study aims to develop methodsfor generating counterspeech constrained byconversation outcomes and evaluate their effec-tiveness. We experiment with large languagemodels (LLMs) to incorporate into the text gen-eration process two desired conversation out-comes: low conversation incivility and non-hateful hater reentry. Specifically, we experi-ment with instruction prompts, LLM finetuning,and LLM reinforcement learning (RL). Evalua-tion results show that our methods effectivelysteer the generation of counterspeech towardsthe desired outcomes. Our analyses, however,show that there are differences in the qualityand style depending on the model.",
  "Introduction": "Hate speech has posed significant challenges tohealthy and productive online communication.Counterspeech, which involves using constructive,positive, or factual responses to challenge or coun-teract hate speech, has shown to be effective in mod-erating online hostilities (Buerger, 2021), promot-ing productive user engagement (Mikolci et al.,2020), and educating online users (Blaya, 2019).Automatic generation of counterspeech has beenresearched to support timely and effective effortsto fight hate speech.Synthetic counterspeechdatasets have been developed using crowdsourc-ing (Qian et al., 2019) and human-in-the-loopstrategies (Chung et al., 2021). These datasetshave been used to develop counterspeech genera-tion models. However, the impact of counterspeechin online environments has not been considered inthe dataset creation. As a result, it is unknown whether generated counterspeech elicits civil orhateful follow-up conversations.Recent counterspeech generation research fo-cused on constrained generation with linguisticattributes (e.g., being polite, emotion-laden (Sahaet al., 2022)), or embedded with knowledge (Chunget al., 2021). Questions about the impact of coun-terspeech with such attributes linger. Previous re-search also found one of the barriers counterspeak-ers face is their inability to determine the poten-tial impact of counterspeech (Mun et al., 2024).However, there is a lack of research on generatingoutcome-oriented counterspeech, e.g., speech thatleads to desired outcomes such as de-escalatinguser conflicts or encouraging constructive engage-ment in follow-up conversations.Notably, previous studies indicate that languagemay influence the development of a conversation,including discourse popularity (Horawalavithanaet al., 2022), reentry behaviors (Wang et al., 2021),and the rise of hate speech (Liu et al., 2018). Thisleads to our research questions:",
  "How can constraints on conversation out-comes be incorporated into developing LLMsfor generating counterspeech?": "How effective are these methods in generatingoutcome-oriented counterspeech?Unlike previous work that considers explicitlinguistic attributes to guide language generation,we formulate counterspeech generation to achievedesired outcomes (e.g., constructive user engage-ment). Our study holds potential for broader appli-cations. Anticipating the direction of a conversa-tion is crucial in crafting effective responses, allow-ing the conversation to meet the objectives (e.g.,reducing hate speech, altering user behavior, andpromoting positive discourse). This study makesthe following contributions: (i) introducing conver-sation outcomes as a constraint to guide the gen-eration of counterspeech, (ii) experimenting withLLMs for generating outcome-constrained coun-",
  "Related Work": "Generating Counterspeech presents re-cent work on counterspeech generation. CONANhas counterspeech written by NGO experts and aug-mented by language models (Chung et al., 2019);Benchmark was built with hate speech from Gaband Reddit and counterspeech created by crowd-sourcing workers (Qian et al., 2019); and MultiCO-NAN is a high-quality, high-quantity dataset cre-ated by experts coupled with language model gener-ation for hate speech with multiple targets (Fantonet al., 2021). Counterspeech generation modelshave been built with these datasets (Halim et al.,2023; Tekiroglu et al., 2020, 2022; Bonaldi et al.,2024). Unlike us, none consider conversation out-comes elicited by the generated counterspeech.Researchers have investigated counterspeechgeneration under constraints. Chung et al. (2021)proposed a generation pipeline grounded in exter-nal knowledge repositories to generate more infor-mative and less biased replies. Zhu and Bhat (2021)proposed to generate more diverse and relevantcounterspeech by developing a three-stage pipeline that uses LLMs to generate candidates, prunes theungrammatical ones, and selects the best instances.Saha et al. (2022) proposed an ensemble genera-tive discriminator to generate more polite, detoxi-fied, and emotion-laden counterspeech. Gupta et al.(2023) developed IntentCONAN, where the genera-tion of counterspeech is conditioned on five intents:informative, denouncing, questioning, positive, andhumorous. Similarly, Fraser et al. (2023) utilizedChatGPT to generate counter-stereotype text byincorporating countering strategies in queries. Has-san and Alikhani (2023) proposed prompting strate-gies based on discourse theories to generate morecontext-relevant counterspeech.There are alsostudies on the generation of counterspeech in lan-guages other than English (e.g., Italian (Chunget al., 2020)). Unlike us, none of these previousworks generate counterspeech to elicit positive be-haviors in the follow-up conversations. Language Generation with ConstraintsEx-tensive studies have targeted language generationunder complex lexical constraints such as for-mality (Jin et al., 2022), text with certain con-cepts (Lu et al., 2022), dialogue that takes la-tent variables (Bao et al., 2020), and knowledge-enhanced text (Yu et al., 2022a). Not all stylescan be described explicitly as linguistic attributes.Indeed, some styles can only be defined in a data-driven way based on the shared attributes across : Two conversation outcomes (hater ressntryand incivility0 assessed based on the conversation(green box) following up a counterspeech reply (bluebox). Comments in the first layer of the conversationtree (i.e., direct replies) are used to model hater reentry.All comments in the conversation tree are used to modelconversation incivility. Grey boxes indicate hateful com-ments; others are non-hateful. various datasets (Mou and Vechtomova, 2020). Inthis study, we generate counterspeech very likelyto lead to desired conversational outcomes.Methods have been developed for constrainedlanguage generation. Wang and Wan (2018) pro-posed the SentiGAN framework to generate textwith a given sentiment. Kumar et al. (2021) pro-posed MUCOCO to allow for controllable infer-ence with multiple attributes as constraints to theoptimization. Krause et al. (2021) developed GeDi,a discriminator-based approach to guide the de-coding process in language generation. It enablestext generation with desired or undesired attributes.Schick et al. (2021) proposed a self-debiasing ap-proach to reduce the probability of language mod-els generating problematic text. Unlike these previ-ous efforts, we experiment with methods to adjustlanguage model-generated texts to achieve specificconversational outcomes.",
  "Conversation Outcomes": "Conversation outcomes refer to the result of a mes-sage in a conversation, which can be measuredby the manner and characteristics of the follow-up conversations it elicits. According to previ-ous studies, a combination of hate speech and itsreplyregardless of whether it counters the hate-ful commentcan predict future conversation en-gagement and incivility (Liu et al., 2018; Yu et al.,2024). This study explores two types of conversa-tion outcome modeling: conversation incivility andhater reentry (). Based on the modelingresults, we build conversation outcome classifiers",
  "that use hate speech and counterspeech to predictthe incivility level or hater reentry type": "Conversation IncivilityConversation incivilityis a metric to measure the outcome based on thenumber of civil and uncivil comments as well asthe unique authors involved in the discourse (Yuet al., 2024). Intuitively, the more uncivil (or lesscivil) the comments, the worse the outcome; un-civil comments from many authors are worse thanthose from just a few.Formally, it is definedas S(r) = U(r) (1 )C(r), where U(r)refers to uncivil behavior and C(r) to civil behav-ior. For each user i (i = 0, 1, 2, ..., k), nui is de-fined as the number of uncivil comments by useri, and nci as the number of civil comments. Then,U(r) = ki=1nui and C(r) = ki=1nci. is used to adjust the weight of civil and uncivil be-haviors. The conversational incivility level is thendetermined by the metric value using quantiles.Previous studies show that given two replies to hatespeech, models taking into account the text of thehate speech and counterspeech accurately predictwhich of the two counterspeech replies will lead tomore civil follow-up conversations (Yu et al., 2024,binary classification, F1=0.660.75). We will usecivility to refer to low conversation incivility, thedesired outcome. Hater Reentry BehaviorAfter a counterspeechreply to a hate speech comment, the hate instiga-tor may exhibit different behaviors. Namely, theymay not engage further, reengage with more hate-ful comments, or participate with non-hateful com-ments. The outcome can be determined based onwhether the following comments have one that isfrom the hater and whether this comment is hateful.The non-hateful reentry is the most desirable, asit signals that the counterspeech encouraged theindividual to change his behavior (Baider, 2023).We will use reentry to refer to non-hateful haterreentry in the remainder of the paper.",
  "Reentry: Instruction with non-hateful haterreentry as a desired outcome": "User:\"Here is a hatecomment:<Hate Comment>.Please write a counterspeechreply to the hate commentso that the hater comesback and has constructiveengagement.\"There are different ways to set these outcome-constrained instructions. We adopt the instructionsabove as baselines for comparison purposes.When given instructions, LLMs can generateone or multiple counterspeech replies. In additionto experimenting with the first generated reply, wefollow (Zhu and Bhat, 2021) and also use a Gener-ate and Select method to generate multiple repliesand select the ones predicted to have desired out-comes according to conversation outcomes classi-fiers (.1). LLM FinetuningLLMs may not be fullyoptimized for generating texts with specificconstraintsin our case, desired conversation out-comes. The finetuning process can tailor LLMsto learn the task of interest. To guide the LLMin generating outcome-constrained counterspeech,we finetune the model with datasets containingconversations with the desired outcomes: the hatespeech/counterspeech pairs followed by low con-versation incivility (Yu et al., 2022b) and the pairsthat have non-hateful hater reentry. We use theParameter-Efficient Fine-Tuning (PEFT) with Low-Rank Adaptation (LoRA) method (Hu et al., 2021)to finetune LLMs.",
  "Reinforcement Learning with LLM (RL)Thismethod integrates the conversation outcome clas-sifiers (.1) as a reward function to guide": "the training process, which includes three steps.First, a hate comment is used as a query to get theresponse generated by an LLM. The initial modelserves as a baseline for generating counterspeech.Second, hate speech and generated responses arefed into the classifiers to obtain their conversationoutcome labels for assigning rewards. Specifically,pairs with low incivility or non-hateful reentry willbe rewarded higher. Third, we maximize the proba-bility of the desired outcomes in the text generationprocess. In addition to the reward value obtainedfrom the (predicted) conversation outcomes, theKL-divergence (Kullback-Leibler) between the logprobabilities of the two outputs is used as an addi-tional reward. This ensures the desired outcome isconsidered while the generated responses do notdeviate too far from the base language model. Thereward is computed as R = r KL. We trainthe model with the Proximal Policy Optimization(PPO) (Schulman et al., 2017) step until local sta-bility is achieved.",
  "Evaluation": "Desired Conversation Outcome MetricsTheevaluation aims to assess the ability of these meth-ods to generate counterspeech that is more likely toachieve desired outcomes. As it would be difficultand arguably unethicalto post the generated textto conversations on social media platforms to ob-serve the real outcomes, we adopt an approach thathas been used before (Saha et al., 2022; Tekirogluet al., 2022; Halim et al., 2023; Gupta et al., 2023).We use the conversation incivility level classifierand the hater reentry classifier (.1) trainedwith real conversation data to make predictionswith the hate speech and generated counterspeechpairs. Although the accuracy of the classifiers isnot perfect, given two counterspeech replies, theseclassifiers reliably identify the one that will leadto better outcomes (Yu et al., 2024, binary clas-sification, F1=0.660.75). Thus, they serve as aproxy to compare counterspeech generated by dif-ferent methods. Additionally, we conduct humanassessments for reliability purposes. Human AssessmentsThe human assessment fo-cuses on three characteristics of replies to hatespeech: suitability, relevance, and effectiveness.Suitability is measured considering (i) whether thelinguistic style of the reply to hate speech suits theconversation and (ii) whether it follows the civilrules of the environment. Relevance evaluates the appropriateness of the reply with respect to the con-tent of the hate comment. Effectiveness is evaluatedbased on whether the reply to hate speech is likelyto stop the spread of hate and foster constructiveconversations, as perceived by human annotators.Two graduate assistants, a male and female agedbetween 20 and 30, who are proficient in Englishand familiar with social media, assist with the eval-uation. To ensure impartiality, reference text andgenerated text samples are randomly provided tothe evaluators, so they do not know the source ofeach text. The inter-annotator agreement rate iscalculated to assess reliability. Stylistic MetricsThe generated counterspeechis evaluated by stylistic metrics commonly usedin previous studies (Chung et al., 2021; Zhu andBhat, 2021; Tekiroglu et al., 2022). We calcu-late the similarity of counterspeech against a refer-ence dataset consisting of human-generated coun-terspeech with the BLEU score (Chen and Cherry,2014), ROUGE (Lin, 2004), METEOR (Banerjeeand Lavie, 2005), and BERTScore (Zhang et al.,2019). The quality of generated texts is evaluatedby the GRUEN metrics (Zhu and Bhat, 2020), in-cluding dimensions of grammaticality, redundancy,focus, and GRUEN score. The same scores are alsocalculated for the reference dataset for comparisonpurposes. Finally, we calculate the type-token ratioand distinct-n-grams to evaluate the diversity ofgenerated texts (Fanton et al., 2021).",
  "Conversation Outcomes Classifiers": "Data to Build Conversation Outcomes Classi-fiersWe use Reddit data collected from 39 sub-reddits likely to contain abusive content (Vidgenet al., 2021). The hate comments are identifiedbased on hate classifiers (Qian et al., 2019). Then,we collect replies to hate comments and iden-tify counterspeech in replies referring to Yu et al.(2022b). For each counterspeech, we collect thefollow-up replies. Then, we calculate the conver-sation incivility with = 0.8 and determine theincivility level by quantiles. The direct replies fol-lowing counterspeech are used to identify haterreentry behavior: whether the hate instigator reen-ters and the comment is non-hateful. Both datasetsare split into 80% for training and 20% for test-ing, with the testing portion used to evaluate theperformance of the classifiers. Classification Model and PerformanceAs thisstudy is not aimed at the best performance inthe classification tasks, we use the RoBERTamodel (Liu et al., 2019) to train outcome classi-fiers. The hate speech/counterspeech pairs are usedto predict the incivility level and the hater reentrybehavior. The detailed classification results can beseen in and 6 in A.4. Although the classi-fication results are somewhat low, these suboptimalclassifiers are enough to defeat the baseline anddifferentiate counterspeech that will lead to highor low incivility in the follow-up conversation, asshown by (Yu et al., 2024). The accuracy for iden-tifying non-hateful reentry is the highest.",
  "Generating Counter Speech": "DatasetWeusethebenchmark-Redditdataset (Qian et al., 2019) for counterspeechgeneration and evaluation. The data contains hatecomments from Reddit and counterspeech writtenby crowdsourcing workers. As we plan to explorethe effect of this data in the finetuning and RLmethod, the data is split randomly into 80% fortraining and 20% for evaluation. Instruction PromptsWe use the Llama2-7b-chat model in our experiments to compare differ-ent methods, as we cannot train larger models likeLlama2-13b-chat for finetuning and RL due to lim-ited computing capacity. We run a baseline in-ference with Llama2-13b-chat to demonstrate theimpact of model size on results. As the generationand evaluation are based on the benchmark-Redditdata, we apply the same system-level guideline:Please generate a response in Reddit style forall generations. The parameters are set to be thesame in the generation of replies with no expectedoutcomes (baseline), low conversation incivility(civility), and non-hateful hater reentry (reentry).For Generate and Select, the number of responsesis set to k = 1, k = 5, and k = 10, the temper-ature to 0.7, and the maximum length of reply to512. For k = 5 and k = 10, we apply the incivil-ity classifier and hater reentry classifier to selectcandidates with the targeted labels (i.e., low conver-sation incivility or non-hateful hater reentry) withthe highest confidence. A random candidate is se-lected if there are no candidates with the targetedlabel in the generated replies.",
  "FinetuningThe Llama2-7b-chat model is fine-tuned with hate speech/counterspeech pairs thatare followed with low conversation incivility or": "non-hateful reentry in the training data. The fine-tuned models are expected to generate texts thatshare similar linguistic patterns and lead to desiredconversation outcomes. Additionally, we fine-tunemodels with several reference datasets, includingbenchmark-Reddit, benchmark-Gab, CONAN, andMultiCONAN (see model details in A.2). Thisis to compare whether models built on existingcounterspeech datasets can generate effective coun-terspeech and how these datasets influence the gen-eration process. Reinforcement LearningWe use the Llama2-7b-chat as the base model for the RL process. Thereward for the RL process is calculated based onthe outcome classifiers: for the predicted categoriesof conversation incivility low, medium, and high,corresponding discrete rewards are assigned in de-scending order, namely 2, 1, and 0; for hater reen-try classification, the reward for non-hateful reen-try, no reentry, and hateful reentry is 2, 1, and0, respectively. We also use the Llama-2-7b-chatfinetuned with the benchmark-Reddit dataset, sothat the model trained with RL can generate coun-terspeech that has similar linguistic patterns withcounterspeech in the benchmark-Reddit datasetwhile having a higher probability of leading to ex-pected conversation outcomes. The hyperparame-ters are shown in A.2. We leave exploring RL withother finetuned models for future work.",
  "Results and Analysis": "All methods are evaluated with the same test setfrom the benchmark-Reddit. The Llama2-7b-chatsometimes avoids responding to queries the modeldetermines to be inappropriate and generates emptyresponses. shows the ratio of non-empty,noted as valid, responses by each method. Exceptfor instruction prompts, all the trained models, in-cluding the finetuning and RL models, have 100%of valid responses. In instruction prompts, the validresponse rate increases when using a more power-ful model (Llama2-13b-chat), forcing the model togenerate more candidates, or asking the model togenerate counterspeech with constrained queries. Expected OutcomesIn the task of generatingtexts with low conversation incivility, we observethe following insights: (i) The counterspeech gener-ated by a more powerful model (Llama2-13b-chat)has a higher proportion of samples leading to lowincivility. (ii) Prompt queries with the constraint of low incivility can increase the probability ofgenerating counterspeech with low conversationincivility. (iii) The generate and select strategyleads to more counterspeech with the desired out-comes. The more candidates are generated (largerk), the higher the chances of getting replies withdesired outcomes. (iv) The performance of fine-tuning methods in generating texts with expectedoutcomes is relatively inferior to others. (v) RLis a robust method to restrict text generation fordesired outcomes. RL models generate more re-sponses with desired outcomes than the baselinemodels and finetuning. (vi) Human-generated coun-terspeech in benchmark-Reddit, which disregardsconversation outcomes, often fails to result in thedesired outcomes in the follow-up conversations.Indeed, only 760 samples (27%) are classified aseliciting low conversation incivility.The evaluation with the hater-reentry classifierfurther validates most insights. Larger models,prompts with desired outcomes, generate and se-lect, and RL models generate more counterspeechwith desired outcomes. Similarity to Reference TextsWe evaluate thesimilarity of generated texts to the counterspeech inthe benchmark-Reddit data. We do not claim thatthe counterspeech in the benchmark-Reddit corpusis the gold standard. Instead, it serves as a baselinefor us to understand whether the LLM-generatedtexts are different from human-generated ones andhow different. We calculate multiple similarity met-rics. Results show the metrics are highly correlated( in the A.5). Hence, we only present theresults of METEOR and BERTScore in .METEOR values are low, with the average val-ues ranging from 0.06 to 0.14. On the other hand,there is not much difference in the BERTScore bydifferent methods, with values ranging from 0.80to 0.86. The difference between METEOR andBERTScores indicates that LLM-generated replieshave high semantic similarity to reference counter-speech, but the wording used in LLM-generatedtexts is different. Notably, even without finetuningor RL, LLMs are still capable of generating coun-terspeech with similar meanings to reference texts(baseline generation BERTScore 0.8).",
  "Valid (%)Civility (%) Reentry (%)METEOR BERTScore": "Instruction PromptsGenerate one based on (k=1)Baseline83%23%18%0.07 (0.08)0.80 (0.03)Baseline(13b)94%27%35%0.12 (0.07)0.81 (0.04)Civility92%54%49%0.12 (0.05)0.83 (0.02)Reentry94%44%45%0.12 (0.06)0.82 (0.02) Generate and select (k=5)p=baseline, c=civility84%55%32%0.10 (0.07)0.81 (0.03)p=baseline, c=reentry85%34%49%0.11 (0.07)0.82 (0.03)p=civility, c=civility92%81%53%0.12 (0.05)0.82 (0.02)p=reentry, c=reentry92%49%83%0.13 (0.05)0.83 (0.01) Generate and select (k=10)p=baseline, c=civility87%69%36%0.11 (0.07)0.82 (0.02)p=baseline, c=reentry86%41%61%0.11 (0.07)0.82 (0.02)p=civility, c=civility92%86%55%0.12 (0.05)0.82 (0.02)p=reentry, c=reentry92%50%86%0.13 (0.05)0.83 (0.01) Finetuning with Counterspeech CorporaCONAN100%23%48%0.09 (0.06)0.85 (0.02)MultiCONAN100%22%48%0.11 (0.06)0.85 (0.02)Benchmark-Gab100%10%43%0.12 (0.10)0.86 (0.02)Benchmark-Reddit100%11%42%0.13 (0.11)0.86 (0.02)",
  "ReferenceBenchmark-Reddit100%27%37%1.00 (0.00)1.00 (0.00)": ": Evaluation of (a) Desired Outcomes and (b) Similarity to the reference counterspeech in Benchmark-Reddit.METEOR and BERTScore are calculated per sample. Mean (SD) is reported. Generate and select and RL are betterat generating more samples with desired outcomes. Although the wording differs from the Reference counterspeech(METEOR), the semantic relevance (BERTScore) is consistently high. All generations are based on Llama2-7b-chat,except Baseline(13b) is based on Llama2-13b-chat. erally have higher grammatical scores than the ref-erence (0.77), except the ones finetuned with Red-dit conversation data: civility (0.77) and reentry(0.76). These finetuned models might have learnedinformal expressions on social media, thus theygenerate counterspeech with a lower grammatical-ity score. Counterspeech generated by LLMs with-out finetuning or RL is more redundant, indicatedby lower scores in redundancy. After adding ex-pected outcomes as constraints, LLM-generatedcounterspeech contains less redundancy. The focus scores of counterspeech generated by instructionprompts are also much lower. In models with fine-tuning and RL, the focus scores are much higher.Overall, counterspeech generated by finetuningand RL have higher quality, as reflected in the gram-maticality, redundancy, focus, and overall GRUENscores. In particular, the highest GRUEN scoresare achieved by RL models.",
  "GrammaticalityFocusRedundancyGRUENTTRNew Tokens": "Instruction PromptsGenerate one based onBaseline0.73 (0.10)-0.05 (0.05) -1.14 (12.56) 0.60 (0.18)0.065384Baseline (13b)0.80 (0.07)-0.09 (0.03) -1.33 (23.22) 0.60 (0.21)0.069231Civility0.84 (0.04)-0.10 (0.01) -0.19 (0.56) 0.61 (0.22)0.037019Reentry0.83 (0.07)-0.10 (0.02) -0.11 (0.39) 0.64 (0.18)0.036407 Generate and select (k=5)p=baseline, c=civility0.78 (0.10)-0.08 (0.04) -0.33 (4.37) 0.62 (0.19)0.067220p=baseline, c=reentry0.78 (0.10)-0.08 (0.04) -0.34 (6.42) 0.63 (0.18)0.056794p=civility, c=civility0.84 (0.03)-0.10 (0.01) -0.23 (2.35) 0.59 (0.23)0.047668p=reentry, c=reentry0.84 (0.02)-0.10 (0.00) -0.07 (0.21) 0.68 (0.12)0.035224 Generate and select (k=10)p=baseline, c=civility0.79 (0.09)-0.08 (0.04) -0.27 (2.27) 0.62 (0.20)0.068000p=baseline, c=reentry0.80 (0.09)-0.08 (0.04) -0.20 (2.02) 0.64 (0.18)0.056908p=civility, c=civility0.84 (0.03)-0.10 (0.00) -0.23 (0.48) 0.57 (0.24)0.048024p=reentry, c=reentry0.84 (0.02)-0.10 (0.00) -0.06 (0.12) 0.68 (0.11)0.035198 Finetuning w/ CounterspeechCONAN0.81 (0.09)-0.02 (0.04)0.00 (0.03)0.78 (0.11)0.111982MultiCONAN0.83 (0.07)-0.05 (0.05) -0.12 (2.93) 0.76 (0.13)0.092448Benchmark-Gab0.85 (0.06)-0.01 (0.03)0.00 (0.00)0.83 (0.08)0.02111Benchmark-Reddit0.80 (0.09)-0.04 (0.05)0.00 (0.01)0.77 (0.12)0.03147",
  "ReferenceBenchmark-Reddit0.77 (0.12)-0.03 (0.05)0.00 (0.01)0.74 (0.13)0.090": ": Evaluation of Quality and Diversity. GRUEN and BERTScore are calculated per sample. Mean (SD)are reported. The quality of counterspeech by Instruction prompts is relatively low. LLM finetuning with Reddit-counterspeech generate texts with high diversity. RL with finetuned LLMs generate texts with reduced novelty. Allgenerations are based on Llama2-7b-chat, except Baseline(13b) is based on Llama2-13b-chat. number of unique bigrams) are highly correlated( in A.5). TTR and the novelty metric (i.e.,number of new unigrams) are presented in .The TTR of generated counterspeech significantlydecreases with models that use expected outcomesconstraints in instruction prompts and RL. The high-est TTRs are achieved by the LLM finetuned withreal Reddit conversation data. Note that this datausually contains diverse and informal language.",
  "untrained LLMs in the instruction prompt methodis substantially higher than trained models withfinetuning and RL": "Human EvaluationWe choose generated textsconstrained with low conversation incivility forhuman evaluation. The model with the highestnumber of samples predicted as having low con-versation incivility from each method is selectedfor further evaluation. Hence, we randomly select50 pairs of hate comments and counterspeech fromthe instruction prompts with p = civility, k = 10,",
  ": Proportion of samples labeled as Yes for eachevaluation dimension by methods": "and c = civility, finetuning with CONAN, and RLwith low incivility, respectively. Then, we mix thesamples and ask annotators to label yes or no tothree criteria: suitability, relevance, and effective-ness. The agreement percentages for initial labelsare 0.78, 0.92, and 0.64 respectively for suitability,quality, and effectiveness. For the samples in whichannotators disagree, the annotators discuss and fi-nalize an agreed annotation. presents theresults. The instruction prompts methods tend togenerate long responses with high relevance. How-ever, the answers vary as replies, essays, letters,or conversation scripts with multiple users. Manysamples are in a format not appropriate for socialmedia platforms. Although the desired outcomemetric shows finetuning is relatively inferior toother methods, the human evaluation shows thegenerated counterspeech by finetuning and RL areusually suitable and effective. Further investiga-tion into the reasons that explain the differences indesired outcomes and human assessment is needed.",
  "Conclusions": "We present an initial exploration of methods forconstrained generation of counterspeech controlledby potential conversation outcomes. We incorpo-rate the desired outcomes (i.e., low conversationincivility and non-hateful hater reentry) into thetext generation process through three methods: in-struction prompts, LLM finetuning, and LLM RL.The text generation results are evaluated with de-sired conversation metrics, stylistic metrics, andhuman assessment. Results show that instructionprompts and RL generate counterspeech with ahigher probability of eliciting desired outcomesbased on the prediction of outcome classifiers,while finetuning and RL generate more effectivecounterspeech based on human assessments. TheLLMs-generated texts consistently show high rele-vance to hate speech, but the wording differs.The generated texts present different characteris-tics. The counterspeech generated by LLM withoutfurther training tends to be long, not suitable for the conversation context on social media, and withlow quality based on GRUEN metrics and humanassessment. Both finetuning and RL models gener-ate high-quality counterspeech with styles suitablefor social media platforms. The experiments high-light the strengths and weaknesses of each method,enabling stakeholders to choose the method mostappropriate for their needs and preferences.",
  "Limitations": "The conversation outcome classifiers are not per-fect, as the texts of hate comments and replies onlypartially contribute to the conversation outcomes.Other factors include the context of the conversa-tion and users positions and identities. While theoutcome classifiers provide a convenient methodfor evaluation, they may introduce bias into theevaluation process. Therefore, interpretations andconclusions drawn from these evaluations shouldbe considered with caution. Future work will ex-plore more accurate and unbiased classifiers toenhance text generation and evaluation. We usecomputing-based metrics for evaluating similarity,text quality, diversity, and novelty. Although thesemetrics are widely used, they may present bias.More sophisticated evaluation methods and com-prehensive human assessments are needed to fullycapture the multidimensional quality of the gener-ated text. Text generation is influenced by numer-ous factors, including the formulation of promptqueries, settings of LLMs for text generation, fine-tuning language models with different datasets,variations in fine-tuning and reinforcement learn-ing settings, and size of language models. Furtherexperiments are needed to better understand theimpact of these factors on text generation. The out-come classifiers are based on Reddit conversationdata, which may not transfer to other platforms. Ex-periments with different data are to be done to un-derstand communication patterns across platformsand the guiding effect of cross-domain data.",
  "Ethics Statement": "The study has been through careful consideration ofbenefits and risks. First, we used data from Reddit,which is considered a public space. Users consentto make their data available to third parties. Second,user names and identities are encrypted to avoid theidentification of users. Third, student collaboratorsworking on the data have been warned of the poten-tial hateful content and are encouraged to stop their work at any time. Fourth, the data will be sharedfor research purposes only. Although releasing thedataset may raise risks, we believe the benefits ofcontributing to effective methods to counter onlinehate outweighs the potential risks. Finally, the mod-els developed may not be directly applicable to thegeneration of counterspeech to online hate. Instead,they could serve as valuable tools to assist con-tent moderation in crafting counterspeech. Humanjudgments are crucial in assessing the suitabilityand appropriateness of replies to HS.",
  "Fabienne Baider. 2023. Accountability issues, onlinecovert hate speech, and the efficacy of counter-speech.Politics and Governance, 11(2):249260": "Satanjeev Banerjee and Alon Lavie. 2005. Meteor: Anautomatic metric for mt evaluation with improved cor-relation with human judgments. In Proceedings ofthe acl workshop on intrinsic and extrinsic evaluationmeasures for machine translation and/or summariza-tion, pages 6572. Siqi Bao, Huang He, Fan Wang, Hua Wu, and HaifengWang. 2020. Plato: Pre-trained dialogue generationmodel with discrete latent variable. In Proceedingsof the 58th Annual Meeting of the Association forComputational Linguistics, pages 8596.",
  "Proceedings of the 57th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 28192829": "Yi-Ling Chung, Serra Sinem Tekiroglu, and MarcoGuerini. 2020. Italian counter narrative generationto fight online hate speech. In Proceedings of theSeventh Italian Conference on Computational Lin-guistics (CLIC-it 2020), volume 2769. Yi-Ling Chung, Serra Sinem Tekiroglu, and MarcoGuerini. 2021.Towards knowledge-groundedcounter narrative generation for hate speech. In Find-ings of the Association for Computational Linguistics:ACL-IJCNLP 2021, pages 899914. Margherita Fanton, Helena Bonaldi, Serra SinemTekiroglu, and Marco Guerini. 2021. Human-in-the-loop for data collection: a multi-target counter narra-tive dataset to fight online hate speech. In Proceed-ings of the 59th Annual Meeting of the Association forComputational Linguistics and the 11th InternationalJoint Conference on Natural Language Processing(Volume 1: Long Papers), pages 32263240. Kathleen C Fraser, Svetlana Kiritchenko, Isar Ne-jadgholi, and Anna Kerkhof. 2023.What makesa good counter-stereotype? evaluating strategies forautomated responses to stereotypical text. In Pro-ceedings of the First Workshop on Social Influencein Conversations (SICon 2023), pages 2538. Rishabh Gupta, Shaily Desai, Manvi Goel, AnilBandhakavi, Tanmoy Chakraborty, and Md ShadAkhtar. 2023. Counterspeeches up my sleeve! in-tent distribution learning and persistent fusion forintent-conditioned counterspeech generation. arXivpreprint arXiv:2305.13776. Sadaf MD Halim, Saquib Irtiza, Yibo Hu, Latifur Khan,and Bhavani Thuraisingham. 2023. Wokegpt: Im-proving counterspeech generation against online hatespeech by intelligently augmenting datasets using anovel metric. In 2023 International Joint Conferenceon Neural Networks (IJCNN), pages 110. IEEE. Sabit Hassan and Malihe Alikhani. 2023.Discgen:A framework for discourse-informed counterspeechgeneration. In Proceedings of the 13th InternationalJoint Conference on Natural Language Processingand the 3rd Conference of the Asia-Pacific Chap-ter of the Association for Computational Linguistics(Volume 1: Long Papers), pages 420429. Sameera Horawalavithana, Nazim Choudhury, JohnSkvoretz, and Adriana Iamnitchi. 2022. Online dis-cussion threads as conversation pools: predicting thegrowth of discussion threads on reddit. Computa-tional and Mathematical Organization Theory, pages129. Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,et al. 2021. Lora: Low-rank adaptation of large lan-guage models. In International Conference on Learn-ing Representations.",
  "Di Jin, Zhijing Jin, Zhiting Hu, Olga Vechtomova,and Rada Mihalcea. 2022. Deep learning for textstyle transfer: A survey. Computational Linguistics,48(1):155205": "Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann,Nitish Shirish Keskar, Shafiq Joty, Richard Socher,and Nazneen Fatema Rajani. 2021. Gedi: Genera-tive discriminator guided sequence generation. InFindings of the Association for Computational Lin-guistics: EMNLP 2021, pages 49294952. Sachin Kumar, Eric Malmi, Aliaksei Severyn, and Yu-lia Tsvetkov. 2021. Controlled text generation ascontinuous optimization with multiple constraints.Advances in Neural Information Processing Systems,34:1454214554.",
  "Chin-Yew Lin. 2004. Rouge: A package for automaticevaluation of summaries.In Text summarizationbranches out, pages 7481": "Ping Liu, Joshua Guberman, Libby Hemphill, and AronCulotta. 2018. Forecasting the presence and intensityof hostility on instagram using linguistic and socialfeatures. In Proceedings of the International AAAIConference on Web and Social Media, volume 12. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A robustly optimized bert pretraining ap-proach. arXiv preprint arXiv:1907.11692. Ximing Lu, Sean Welleck, Peter West, Liwei Jiang,Jungo Kasai, Daniel Khashabi, Ronan Le Bras, Lian-hui Qin, Youngjae Yu, Rowan Zellers, et al. 2022.Neurologic a* esque decoding: Constrained text gen-eration with lookahead heuristics. In Proceedings ofthe 2022 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 780799.",
  "Jozef Mikolci, Lucia Kovcov, and Edita Rigov.2020. Countering hate speech on facebook: Thecase of the roma minority in slovakia. Social ScienceComputer Review, 38(2):128146": "Lili Mou and Olga Vechtomova. 2020. Stylized text gen-eration: Approaches and applications. In Proceed-ings of the 58th Annual Meeting of the Associationfor Computational Linguistics: Tutorial Abstracts,pages 1922. Jimin Mun, Cathy Buerger, Jenny T Liang, Joshua Gar-land, and Maarten Sap. 2024. Counterspeakers per-spectives: Unveiling barriers and ai needs in the fightagainst online hate. In Proceedings of the CHI Con-ference on Human Factors in Computing Systems,pages 122. Jing Qian, Anna Bethke, Yinyin Liu, Elizabeth Belding,and William Yang Wang. 2019. A benchmark datasetfor learning to intervene in online hate speech. InProceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the 9th",
  "John Schulman, Filip Wolski, Prafulla Dhariwal,Alec Radford, and Oleg Klimov. 2017.Proxi-mal policy optimization algorithms. arXiv preprintarXiv:1707.06347": "Serra Sinem Tekiroglu, Helena Bonaldi, MargheritaFanton, and Marco Guerini. 2022. Using pre-trainedlanguage models for producing counter narrativesagainst hate speech: a comparative study. In Find-ings of the Association for Computational Linguistics:ACL 2022, pages 30993114. Serra Sinem Tekiroglu, Yi-Ling Chung, and MarcoGuerini. 2020. Generating counter narratives againstonline hate speech: Data and strategies. In Proceed-ings of the 58th Annual Meeting of the Associationfor Computational Linguistics, pages 11771190. Bertie Vidgen, Dong Nguyen, Helen Margetts, PatriciaRossini, and Rebekah Tromble. 2021. Introducingcad: the contextual abuse dataset. In Proceedings ofthe 2021 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 22892303. Ke Wang and Xiaojun Wan. 2018. Sentigan: Generatingsentimental texts via mixture adversarial networks.In Proceedings of the Twenty-Seventh InternationalJoint Conference on Artificial Intelligence (IJCAI-18), pages 44464452. Lingzhi Wang, Xingshan Zeng, Huang Hu, Kam-FaiWong, and Daxin Jiang. 2021. Re-entry predictionfor online conversations via self-supervised learning.In Findings of the Association for ComputationalLinguistics: EMNLP 2021, pages 21272137.",
  "Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu,Qingyun Wang, Heng Ji, and Meng Jiang. 2022a. Asurvey of knowledge-enhanced text generation. ACMComputing Surveys, 54(11s):138": "Xinchen Yu, Eduardo Blanco, and Lingzi Hong. 2022b.Hate speech and counter speech detection: Conver-sational context does matter. In Proceedings of the2022 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 59185930. Xinchen Yu, Eduardo Blanco, and Lingzi Hong. 2024.Hate cannot drive out hate: Forecasting conversationincivility following replies to hate speech. In Pro-ceedings of the International AAAI Conference onWeb and Social Media, volume 18, pages 17401752.",
  "A.2Hyperparameters": "LLM Finetuning: We use PEFT LoRA for thefinetuning process. The LoRA configuration hasr = 16, alpha = 32, dropout = 0.05, and bias isnone. The hyperparameters are as follows: thelearning rate is 1e-4, the number of epochs is 1,and the warmup ratio is 0.1.LLM RL: The reward trainer uses the RoBERTabase model, the learning rate is 1e-5, the batch sizeis 16, and the number of epochs is 5. In the PPOprocess, the generation component has top_k = 0,top_p = 1.0, do_sample = True, and the maxlength is 256. The PPO configuration has a learningrate of 1.41e-5, a batch size of 32, and an initialKL coefficient of 0.1.",
  "A.3Dataset License and Use": "The Benchmark dataset by Qian et al.(2019)is under the Creative Commons Attribution-NonCommercial 4.0 International Public License.The CONAN and MultiCONAN datasets canbe used for research purposes with proper cita-tion (Chung et al., 2019; Fanton et al., 2021). Thebenchmark-Reddit data contains 5,020 unique con-versations with hate speech identified. Each hatespeech comment has multiple responses. We ex-tracted the hate speech from conversations and",
  "A.4Evaluation Results of ConversationOutcome Classifiers": "presents the evaluation of the conversa-tion incivility classifier. The baseline is calculatedassuming all test samples are assigned with themajority label, Medium. Although the classifica-tion results are somewhat low, these suboptimalclassifiers are enough to defeat the baseline anddifferentiate counterspeech that will lead to highor low incivility in the follow-up conversation (Yuet al., 2024, binary classification, F1=0.660.75). presents the evaluation of the hater reentryclassifier. The baseline is calculated assuming alltest samples are assigned with the majority label,non-hateful reentry. The non-hateful reentry classhas the highest F1 of 0.61.",
  "A.5Evaluation Metrics": "shows the number of samples in each classbased on the prediction of the conversation incivil-ity classifier and the hate re-entry classifier. presents the correlation coefficientsbetween diversity metrics (i.e., type-token ratio,distinct-1, and distinct-2) and novelty metrics (i.e.,number of new unigrams and bigrams) using thereference texts in Benchmark-Reddit. presents the correlation of metrics thatevaluate the relevance of generated texts to refer-ence texts in Benchmark-Reddit. presents relatively good and bad ex-amples of generated texts by different methods1.Counterspeech replies annotated by the human an-notators as bad either are not suitable to the con-versation context (e.g., example(2)), not a counter-speech (e.g., example(4)), or are very generic anddo not address the specific hateful content (e.g.,example(6)).",
  "InstructionPromptsGoodWow. Cant believe anyone would actu-ally vote for that cunt": "I understand that many people are upset about the election results, but letstry to approach this in a constructive way. Instead of resorting to insults orname-calling, lets focus on the issues that matter most to us. What are the mostpressing problems facing our country, and how can we work together to addressthem? For example, Im concerned about the rising cost of healthcare and theimpact its having on families and small businesses. I think we need to worktowards a more affordable and sustainable healthcare system that prioritizes theneeds of everyone, not just the wealthy and powerful.",
  "InstructionPromptsBad": "what was this dumb cunt thinking? ohyeah Ill just slap on this wig, call my-self Pauline & wander into the womanschanging room? Cunt needs section-ing, trans people have mental illness orsome deep laying issues they need todeal with. Title: Challenging Harmful Stereotypes and Promoting Inclusivity for Trans-gender Individuals. Introduction: Hello fellow Redditors, Im here to address aconcerning comment that was recently made on this platform. The comment inquestion perpetuates harmful stereotypes and discrimination against transgenderindividuals. As a community, its important that we challenge these stereotypesand promote inclusivity for all individuals, regardless of their gender identity...",
  "Nathan Grayson wrote that retarded ar-ticle, but to be fair Kotakus hands onpreview of the game didnt have muchgood to say about it": "I apologize, but I cannot engage in discussions that use derogatory language orperpetuate harmful stereotypes. The term retarded\" is a hurtful and offensiveslur that has been used to demean and dehumanize people with intellectualdisabilities. Using this term to describe someones work or a game previewis not only disrespectful, but it also contributes to a culture of ableism anddiscrimination..."
}