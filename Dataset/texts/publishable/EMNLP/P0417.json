{
  "Abstract": "Large language models (LLMs) encapsulatea vast amount of factual information withintheir pre-trained weights, as evidenced by theirability to answer diverse questions across dif-ferent domains. However, this knowledge isinherently limited, relying heavily on the char-acteristics of the training data. Consequently,using external datasets to incorporate new in-formation or refine the capabilities of LLMson previously seen information poses a sig-nificant challenge.In this study, we com-pare two common approaches: unsupervisedfine-tuning and retrieval-augmented generation(RAG). We evaluate both approaches on a vari-ety of knowledge-intensive tasks across differ-ent topics. Our findings reveal that while unsu-pervised fine-tuning offers some improvement,RAG consistently outperforms it, both for ex-isting knowledge encountered during trainingand entirely new knowledge. Moreover, wefind that LLMs struggle to learn new factualinformation through unsupervised fine-tuning,and that exposing them to numerous variationsof the same fact during training could alleviatethis problem.",
  "Introduction": "Large language models (LLMs) are able to cap-ture vast amounts of factual information (Petroniet al., 2019; Cohen et al., 2023; Hu et al., 2023).LLMs exhibit a remarkable level of knowledge invarious domains due to their massive pre-trainingdatasets. However, there are two significant limita-tions to this knowledge. First, it is static and doesnot update with time. Second, it is non-specificand thus may lack nuanced expertise in particulardomains. While these are two different problems,they are deeply related since their solution is thesame: enhancing the models knowledge.Recently, the idea of adapting LLMs to partic-ular domains and updating their knowledge has",
  "*Equal contribution": "become increasingly common (Yu et al., 2022).Various models have been suggested to improvefactual knowledge and capabilities in diverse fieldssuch as healthcare (Singhal et al., 2023a,b; Wuet al., 2023a), finance (Wu et al., 2023b; Yang et al.,2023), and law (Huang et al., 2023; Nguyen, 2023).In this work, we focus on the evaluation of amodels knowledge and its ability to memorize,understand, and retrieve factual data. We aim to un-derstand the concept of knowledge injection (Wanget al., 2020; Chen et al., 2022; Liu et al., 2020;Lauscher et al., 2020). Given some knowledgebase in the form of a text corpus, what is the bestway to teach a pre-trained model this knowledge?One way to add knowledge to a pre-trainedmodel is through fine-tuning. With fine-tuning,we continue the models training process and adaptit using task-specific data. By exposing the modelto a specific knowledge base, we expect the modelweights to adapt accordingly. This process is meantto optimize the model for targeted applications, en-hancing its performance and contextual relevancein specialized domains.Another method to enhance a models knowl-edge base is through the use of in-context learning(ICL) (Chen et al., 2021; Radford et al., 2019; Minet al., 2021; Lampinen et al., 2022). The main ideabehind ICL is to improve the performance of pre-trained LLMs on new tasks by modifying the inputquery to the model without directly changing theweights of the model. One form of ICL is retrievalaugmented generation (RAG) (Lewis et al., 2020;Neelakantan et al., 2022). RAG uses informationretrieval techniques to enable LLMs to obtain rel-evant information from a knowledge source andincorporate it into generated text.This study aims to evaluate the knowledge injec-tion capabilities of LLMs through a comparison offine-tuning and RAG. To illustrate the rationale, letus use an analogy. Consider three college studentstaking a test on a specific topic. All had access to class materials but didnt know the topic before-hand. The first student had the textbook only duringthe test, the second had pre-test access and studied,and the third lost access upon the test announce-ment. Who would probably perform better?",
  "Background": "To assess knowledge injection, we must first under-stand what knowledge means for LLMs.Knowledge and Language ModelsDefiningknowledge is a complex philosophical task far be-yond the scope of this research. However, we canexamine what factual knowledge means in the con-text of language models. If a model knows a fact,it can accurately and consistently answer questionsabout it. Furthermore, it can reliably distinguishbetween true and false statements related to thisfact. We can then extend this definition to a wholeknowledge base, not just a single fact.Mathematically, let Q = {qn}Nn=1 be a set ofN multiple choice factual questions, where eachquestion has L possible answers and exactly onecorrect answer. Let A = {(a1n, . . . , aLn)}Nn=1 bethe corresponding set of possible answers, and C ={cn}Nn=1 be the correct answers.Let M be a language model. We denote byM(qn) {a1n, . . . , aLn} the predicted answer ofthe model to the n-th question. We define theknowledge score L of M in relation to Q to bethe standard accuracy score:",
  "L.(2)": "In simpler terms, the model can consistently givecorrect answers, outperforming a simple randomguessing baseline. Naturally, if the knowledgescore LM,Q is higher for one model compared toanother, then we assert that the former is moreknowledgeable with regards to Q compared to thelatter.Previously Seen KnowledgeOne importantdistinction to make is between knowledge thatthe model has been exposed to before during pre-training as opposed to entirely new facts. Con-sidering the size of modern LLM training sets,they cover a vast amount of information available through web-sourced text. As a result, even inniche domains, the goal of knowledge injectionis not necessarily to teach the model entirely newfacts but rather to \"refresh\" its memory by inducinga bias toward a particular domain.Knowledge and ReasoningWe emphasizethat this knowledge evaluation framework forLLMs is imperfect. Importantly, it doesnt ad-dress other quality metrics influencing a modelsresponse. Creating a purely knowledge-intensivedataset without involving some level of reasoningis challenging. Consequently, a model with ro-bust reasoning abilities might excel on unfamiliarknowledge-intensive tasks by making \"educatedguesses\" in a multiple-choice exam. Therefore, anyevaluation of knowledge in LLMs should considerthis, with results seen as part of a broader range ofbenchmarks for reasoning (Sakaguchi et al., 2021),reading comprehension (Dua et al., 2019), andgeneral language abilities (Srivastava et al., 2022).However, this evaluation framework still stronglyemphasizes factual information above all else.Causes for Factual ErrorsThere are manypossible reasons for the failure of models to answerfactual questions accurately. In (Wang et al., 2023),Wang et al. introduce a taxonomy of five mainmodel-level causes: Domain knowledge deficit: A language modelmay lack comprehensive expertise in a specificdomain to which it has not been exposed. Forexample, a model trained exclusively on textswritten by William Shakespeare would performpoorly when asked about the works of MarkTwain. Outdated Information: LLMs invariably havea cutoff date determined by their trainingdataset. Consequently, any events, discoveries,or changes occurring after the last training up-date will not be within the models knowledgewithout access to external sources. Immemorization: Sometimes, a model is ex-posed to knowledge during its training processbut does not retain it. This is especially true forrare facts that appear in the training dataset onlyscarcely (Kandpal et al., 2023).",
  "(Kirkpatrick et al., 2017; Goodfellow et al., 2013;Chen et al., 2020; Luo et al., 2023), where mod-els lose some of the knowledge they had prior tothe fine-tuning process": "Reasoning Failure: In certain instances, a lan-guage model might possess relevant knowledgeabout a fact but fail to utilize it properly. This isparticularly evident in complex multi-step reason-ing tasks (Tan et al., 2023) or when posed withdifferent questions about the same fact, resultingin disparate outcomes (Berglund et al., 2023). We observe that most of these issues arise duringthe pre-training phase, with catastrophic forgettingbeing the notable exception. Hence, many LLMswill suffer from factual errors of this kind regard-less of any post-training process.",
  "Injecting Knowledge to LanguageModels": "Following the background given in , itis clear that general pre-training is insufficient formany knowledge-intensive tasks. To solve this,an additional post-processing step is essential toaugment the knowledge of a pre-trained model.This step is often reffered to as knowledge injection(Wang et al., 2020; Chen et al., 2022; Liu et al.,2020; Lauscher et al., 2020).In this section, we examine two widely usedframeworks for knowledge injection: fine-tuning",
  "Problem formulation": "In Equations (1) and (2), we presented a formu-lation for knowledge in language models throughthe lens of question-answering (Q&A). We now ex-tend this formulation to the problem of knowledgeinjection using the same terminology.Given a set of factual questions, there existssome text corpus containing information that isrelevant to these questions. The central assumptionof knowledge injection is that given full access tothis corpus, it could serve as an auxiliary knowl-edge base and improve the models performanceon this set of questions.Mathematically, let M be a pre-trained model,and let Q be a set of factual questions, as before.Now, assume we have a relevant auxiliary knowl-edge base BQ. Our objective is to discover a trans-formation, denoted as F, that, when applied, wouldenhance the knowledge about Q:",
  "Fine-Tuning": "Fine-tuning is the process of adjusting a pre-trainedmodel on a specific, often narrower, dataset or taskto enhance its performance in that particular do-main. Here, it is vital to distinguish between dif-ferent types of fine-tuning. FT techniques are com-monly classified into supervised, unsupervised, andreinforcement learning (RL) based methods. Weproceed by briefly reviewing these methods andtheir relation to the problem of knowledge injec-tion.Supervised Fine-TuningSupervised fine-tuning (SFT) requires sets of labeled input-outputpairs. One of the most common SFT methodsis instruction tuning (Wang et al., 2022; Mishraet al., 2021; Ouyang et al., 2022; Taori et al., 2023),which has emerged as one of the most powerfulmethods to improve model performance. With in-struction tuning, the input is a natural languagetask description, and the output is an example ofthe desired behavior. Many current state-of-the-artLLMs have gone through instruction tuning aftertheir pre-training phase.Instruction tuning has been shown to be veryeffective at improving the overall quality of themodel, with a particular emphasis on its zero-shotand reasoning capabilities. However, despite theseadvantages, instruction tuning does not necessarilyteach the model new knowledge (Ouyang et al.,2022; Chung et al., 2022; Mitra et al., 2023; Chiaet al., 2023; Zhou et al., 2023). As such, instruc-tion tuning alone is not a viable solution to theknowledge injection problem.Reinforcement LearningAnother form ofFT relies on RL or RL-inspired optimization strate-gies to better align the model after its pre-trainingphase. A few prominent examples are reinforce-ment learning from human feedback (RLHF) (Ope-nAI, 2023; Touvron et al., 2023), direct preferenceoptimization (DPO) (Rafailov et al., 2023), andproximal policy optimization (PPO) (Schulmanet al., 2017; Tunstall et al., 2023).These techniques have been shown to be veryuseful, especially when used in conjunction with in-struction tuning. However, similarly to instructiontuning, these methods focus on the overall qualityof the response and its expected behavior and notnecessarily on its breadth of knowledge.Unsupervised Fine-TuningThe final FTstrategy we discuss is unsupervised, meaning thereare no available labels for the model to learn from.",
  "One common unsupervised FT technique is oftenreferred to as continual pre-training or unstruc-tured FT": "In this method, the FT process is viewed as adirect continuation of the pre-training phase. Westart with a saved checkpoint of the original LLMand train it in a causal auto-regressive manner, i.e.,predicting the next token. One major difference incomparison to actual pre-training is the learningrate. Usually, one would need a much lower learn-ing rate when continuing the pre-training of themodel to avoid catastrophic forgetting (Kirkpatricket al., 2017). It is well known that LLMs store vast amountsof knowledge during their pre-training phase (Zhouet al., 2023).So, it makes sense to continuethis process in order to inject knowledge into themodel. Hence, we use the unsupervised FT ap-proach throughout this work and evaluate its effi-cacy in enhancing the models capacity for learningnew information.",
  "Retrieval Augmented Generation": "Retrieval augmented generation (RAG) (Lewiset al., 2020) is a technique that expands LLMs ca-pabilities, especially in knowledge-intensive tasks,by using external knowledge sources. While theoriginal formulation involved additional trainingper task, it has since been demonstrated (Neelakan-tan et al., 2022) that a pre-trained embedding modelcan achieve improved performance with no addi-tional training involved. The idea is that given an auxiliary knowledgebase and an input query, we use the RAG architec-ture to find documents within the knowledge basethat resemble the input query. These documents arethen added to the input query, thus giving the modelfurther context about the subject of the query. In practice, implementing the suggested archi-tecture is quite straightforward: Given an auxiliaryknowledge base BQ and a pre-trained embeddingmodel Me, we create a dense vector representation(embedding) per document b BQ and store thesein a vector store. Upon receiving a new query q, weuse its embedding, Me(q), to retrieve qs top-Kclosest neighbors, bq = {bk}K1 , according to dot-product ranking. We then update q to be q = bqq,where denotes string concatenation. Finally, wereturn M(q) as the models output.",
  "Task Selection and Rationale": "MMLU BenchmarkTo properly evaluate thecapabilities of LLMs on knowledge-intensive tasks,we selected four distinct tasks from the MassivelyMultilingual Language Understanding Evaluation(MMLU) benchmark (Hendrycks et al., 2021) inthe topics of anatomy, astronomy, college biology,college chemistry and prehistory. The chosen tasks were selected based on their emphasis on factualknowledge and the minimal reliance on reasoning.As a heuristic, we opted for tasks where the ques-tions are short and involve no context. In practicewe selected four STEM subjects as well as one hu-manities subject, to ensure the evaluation is not lim-ited to certain fields. Note that prehistory involvesquestions spanning all non-modern history. Thisapproach aims to enable us to test LLM proficiencyin comprehending and manipulating information in isolation from its reasoning processes.Current Events TaskTo further isolateLLMs abilities to learn new knowledge, we cre-ated a task comprising multiple-choice questionsabout current events. This task includes multiple-choice questions about events that occurred afterthe cutoff of the various models training data.Specifically, we focused on \"current events\" fromthe USA, in the time span of August-November2023, that are included in the relevant Wikipediaindexes1. This method enables us to mostly guaran-tee that the models have not been exposed to thesefacts, thus allowing us to directly test knowledgeinjection capabilities.",
  "Data Collection and Preprocessing": "To effectively evaluate the LLMs performance onthese knowledge-intensive tasks, a comprehensiveauxiliary dataset was collected by scraping relevantarticles per topic from Wikipedia. The rationale be-hind selecting Wikipedia as the primary source ofknowledge is its broad coverage of relevant topicsand its reliability as a repository of crowd-verifiedknowledge. All articles pertinent to the tasks wereretrieved via the official Wikipedia API2 by identi-fying the relevant central page per topic.Subsequently, a rigorous cleaning process wasutilized to transform the data from raw subsec-tions to clean chunks. This step was done withthe \"wikiextractor\" tool (Attardi, 2015). The divi-sion into small, clean (e.g., remove HTML, URLs,etc.) chunks was aimed at enhancing the evalu-ation of the LLMs understanding across variousknowledge domains and aiding the LLMs in thefine-tuning process.",
  "Current Events Task Creation": "AftercollectingtherelevantchunksfromWikipedia, we created a new multiple-choicedataset with the help of GPT-4 (OpenAI, 2023).First, we removed any small chunks. For eachremaining chunk in the corpus, GPT-4 was in-structed to create four highly specific, high-qualitymultiple-choice questions with only one correctanswer. By specific, we mean that the questioncan be answered without knowledge of whichcontext the question refers to and with minimalambiguity. Next, GPT-4 was asked to select thetwo most specific of the four. This was followed",
  "Paraphrases Generation": "After creating the dataset, we utilized GPT-4 to gen-erate augmentations of the dataset. We instructedGPT-4 to provide paraphrased versions of the inputdata that fully retain the information while beingreworded. Each paraphrasing iteration was donewith a different seed to ensure variety.We selected 240 chunks at random for each taskand created two paraphrases per chunk. These wereset aside to be used as validation sets for hyperpa-rameter tuning. For the current events dataset, wecreated ten paraphrases for each chunk used in thefine-tuning process described in .",
  "Experiments and Results": "Experimental FrameworkWe used the popularLM-Evaluation-Harness (Gao et al., 2021) reposi-tory to evaluate the performance of LLMs on the se-lected knowledge-intensive tasks. LM-Evaluation-Harness is a robust benchmarking tool that cur-rently serves as the industry standard for modelevaluation and is the basis of the HuggingFaceleaderboard3. Leveraging this platform ensureda standardized evaluation framework and allowedconsistent comparison across models, methods, anddatasets. More importantly, by using the industrystandard for evaluation, we could avoid any dif-ferences stemming from prompt engineering andformatting issues and replicate the reported base-line results for each model.Model SelectionWe chose three models forinference evaluation: Llama2-7B (Touvron et al.,2023), Mistral-7B (Jiang et al., 2023), and Orca2-7B (Mitra et al., 2023). The choice of these mod-els was meant to represent the most popular open-source base models and an instruction-tuned modelacross various baseline capabilities. Additionally,we selected bge-large-en (Xiao et al., 2023) as theembedding model for the RAG component andused FAISS (Johnson et al., 2019) as its vector-store. This embedding model is currently the SOTAof open-source embedding models, according tothe HuggingFace MTEB leaderboard4.Configuration VariationsOur evaluation in-cluded multiple configurations, with a grid-search",
  ": The relative accuracy gain (as explainedin Equation (5)) for each knowledge-injection method,averaged (columnwise) across all experiments in Ta-ble 1": "over them, to allow for more comprehensive bench-marking.Firstly, we compared the baseline and fine-tunedmodels and their performance with the RAG com-ponent. Secondly, we explored the optimal numberof text chunks to add to the context in RAG. Specif-ically, different values of K {0, . . . , 5} wereemployed to analyze the impact on model perfor-mance. Finally, we explored 5-shot performancevs. 0-shot.Training SetupWe trained all of the mod-els using the unsupervised training procedure de-scribed in .2. For each dataset, we dividedthe auxiliary knowledge base into equal chunks ofsize 256 by concatenating or splitting the originalchunks based on their length. We also added twospecial tokens, <BOS> and <EOS>, to demar-cate the original chunks beginnings and ends topreserve the documents structure.The models were trained using learning ratesbetween 1106 and 5105, which were foundthrough a hyperparameter search. All models weretrained on 4 NVIDIA A-100 GPUs for a maximumof 5 epochs and a batch size of 64.Evaluation methodAll evaluations weredone by appending each of the multiple-choiceoptions to the question, followed by passing theconcatenation through the model to get a log prob-ability score per option. The highest score wasinterpreted as the models choice and used for ac-curacy calculation. More formally, this means thatin Equation (1) we say that M(qn) = cn if:",
  "(LM,Q LM,Q)/LM,Q,(5)": "where M is the base model and M is theknowledge-injected model, is shown in .In all cases, RAG performed significantly bettercompared to the base models. Furthermore, usingRAG with the base model as the generator wasconsistently better than only fine-tuning. In somecases, using the fine-tuned model instead of thebase model as the generator in the RAG pipelineimproved results even further. However, this isnot consistent and thus demonstrates the inherentinstability of fine-tuning. Additionally, we foundthat the 5-shot approach boosts the results by asmall margin in most cases, with a similar trendbeing observed in all of the different approaches.Current Events ResultsThe evaluation onthe current events task is shown in . RAGproves particularly effective due to the one-to-onecorrespondence between the questions and the aux-iliary dataset (see .3). Fine-tuning is notcompetitive with RAG. However, fine-tuning withmultiple paraphrases still provides a significant im-provement over the baseline. We note that com-bining RAG with fine-tuning shows inferior perfor-mance compared to RAG alone.It is worth noting that although the questions arebased on information the models were not exposedto during training, the results of the base modelssurpass 1 L = 0.25. This can partially be explainedby the models using reasoning and/or pre-existingknowledge when answering questions that are notindependent of the past information. Some exam-ples of this can be found in Appendix D.Fine-Tuning vs. RAG: In the results of both theMMLU and current events tasks, a significant ad-vantage for RAG over fine-tuning is evident. Whilefine-tuning improved results compared to the basemodel in most cases, it was not competitive withthe RAG approach.Several factors might contribute to this behav-ior. Firstly, RAG not only adds knowledge to amodel but also incorporates context relevant to thequestion, a feature lacking in fine-tuning. Addi-tionally, fine-tuning may impact other capabilities of the model due to a degree of catastrophic for-getting. Finally, its plausible that unsupervisedfine-tuned models might benefit from further align-ment through supervised or RL-based fine-tuning,as evidenced by the vastly improved performanceof Orca2 over the base Llama2.",
  "The Importance of Repetition": "Unlike the other tasks, where the model has beenexposed to aspects related to the topic during pre-training, current events includes new information.In this case, standard regular fine-tuning not onlydid not improve the performance of Llama2 butalso significantly degraded it. To improve the fine-tuning results, we explored augmentation of thedata using paraphrases.Data AugmentationData augmentation is awell-established method for enhancing the perfor-mance of language models and has been surveyedextensively (Shorten et al., 2021). Using generativemodels for augmentations has also been used suc-cessfully to improve classification models in thepast (Sharma et al., 2022). An example of dataaugmentation using paraphrasing can be found inAppendix C.Monotonic ImprovementThis approach re-sulted in notable improvements in our results, show-casing a direct correlation between the number ofparaphrases utilized and the models accuracy. Ourexperimentation revealed a compelling trend. Forall models tested, the accuracy was a monotonicallyincreasing function of the number of paraphrasesused (visualized in Appendix A, ). Thisobservation strongly suggests the positive impactof paraphrase augmentation, yielding informationrepetition, on the models ability to comprehendand generalize new knowledge from limited data.Learning New InformationIn Appendix A,, we can see an interesting phenomenonobserved throughout our experiments. After eachepoch, i.e., completing another iteration over theentire dataset, the training loss drops significantly.This is consistent with what is known about LLMsmemorizing the data during training and overfit-ting (Tirumala et al., 2022).Our hypothesis is as follows:",
  "This is well known for LLM pre-training (Kand-pal et al., 2023), and we see in this case that this": "holds for fine-tuning as well. The rationale for thishypothesis is that mere memorization of sentencesdoes not entail knowledge of their content, as wasalready shown in (Berglund et al., 2023). By pro-viding the information in numerous forms (like thedata augmentation process we used), the variousrelationships in the data (e.g., a = b, b= c)stand a higher chance of appearing naturally. Webelieve this can potentially both increase LM,Qin general, as well as ameliorate Berglund et al.sReversal Curse. While promising, this result stillwarrants further research.",
  "Conclusion and Future Work": "Large language models possess vast amounts ofknowledge on various topics. In this work, wetested their capability to adapt to new knowledge:both specialized and completely unseen. This isamong the first studies to compare two prominentapproaches in this domain, namely fine-tuning andretrieval augmented generation. While fine-tuningcan be useful for many use-cases, we found thatRAG is a more reliable choice for knowledge injec-tion.Some aspects of this work still warrant further re-search. For example, we focused on unsupervisedtraining as our primary fine-tuning method, as op-posed to instruction-tuning or RL-based methods.Researching combinations of various techniques,with diverse auxiliary knowledge bases, may yieldimproved results. This approach, combined withour hypothesis from , could further en-hance our understanding of knowledge injectionvia FT.While we believe that this work further enhancesour understanding of knowledge in LLMs, there isa lot more work to be done in this field. Specifically,more research is required regarding the questionof knowledge representation in LLMs, especiallyfrom a theoretical perspective.Finally, further efforts are needed to measureknowledge in LLMs. While we employed an em-pirical approach as described in Equation (2), it isimportant to explore other definitions and perspec-tives on knowledge as well, and extend upon thiswork.",
  "As in all machine learning applications, the choiceof hyperparameters significantly impacts the re-sults. We therefore strongly recommend optimiz-": "ing all relevant hyperparameters for specific cases.We have supported our claims by running the ex-periments on three different models. However, gen-eralization to other LLMs should be tested thor-oughly. For example, GPT-4 achieves near perfectaccuracy for some MMLU tasks (Nori et al., 2023),and thus further improvement is not applicable.Finally, while we chose various topics for theknowledge bases, all of our sources came fromWikipedia. Other datasets may yield different re-sults, and must be evaluated carefully.",
  "RoiCohen,MorGeva,JonathanBerant,andAmir Globerson. 2023.Crawling the internalknowledge-base of language models. arXiv preprintarXiv:2301.12810": "Dheeru Dua, Yizhong Wang, Pradeep Dasigi, GabrielStanovsky, Sameer Singh, and Matt Gardner. 2019.Drop:A reading comprehension benchmark re-quiring discrete reasoning over paragraphs. arXivpreprint arXiv:1903.00161. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black,Anthony DiPofi, Charles Foster, Laurence Golding,Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,Jason Phang, Laria Reynolds, Eric Tang, Anish Thite,Ben Wang, Kevin Wang, and Andy Zou. 2021. Aframework for few-shot language model evaluation.",
  "Jeff Johnson, Matthijs Douze, and Herv Jgou. 2019.Billion-scale similarity search with GPUs.IEEETransactions on Big Data, 7(3):535547": "Nikhil Kandpal, Haikang Deng, Adam Roberts, EricWallace, and Colin Raffel. 2023. Large languagemodels struggle to learn long-tail knowledge. In In-ternational Conference on Machine Learning, pages1569615707. PMLR. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,Joel Veness, Guillaume Desjardins, Andrei A Rusu,Kieran Milan, John Quan, Tiago Ramalho, Ag-nieszka Grabska-Barwinska, et al. 2017.Over-coming catastrophic forgetting in neural networks.Proceedings of the national academy of sciences,114(13):35213526. Andrew K Lampinen, Ishita Dasgupta, Stephanie CYChan, Kory Matthewson, Michael Henry Tessler,Antonia Creswell, James L McClelland, Jane XWang, and Felix Hill. 2022. Can language modelslearn from explanations in context? arXiv preprintarXiv:2204.02329. Anne Lauscher, Olga Majewska, Leonardo FR Ribeiro,Iryna Gurevych, Nikolai Rozanov, and GoranGlava. 2020.Common sense or world knowl-edge?investigating adapter-based knowledge in-jection into pretrained transformers. arXiv preprintarXiv:2005.11787. Patrick Lewis, Ethan Perez, Aleksandra Piktus, FabioPetroni, Vladimir Karpukhin, Naman Goyal, Hein-rich Kttler, Mike Lewis, Wen-tau Yih, Tim Rock-tschel, et al. 2020. Retrieval-augmented generationfor knowledge-intensive nlp tasks. Advances in Neu-ral Information Processing Systems, 33:94599474. Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju,Haotang Deng, and Ping Wang. 2020. K-bert: En-abling language representation with knowledge graph.In Proceedings of the AAAI Conference on ArtificialIntelligence, volume 34, pages 29012908. Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, JieZhou, and Yue Zhang. 2023. An empirical studyof catastrophic forgetting in large language mod-els during continual fine-tuning.arXiv preprintarXiv:2308.08747.",
  "Swaroop Mishra, Daniel Khashabi, Chitta Baral, andHannaneh Hajishirzi. 2021. Cross-task generaliza-tion via natural language crowdsourcing instructions.arXiv preprint arXiv:2104.08773": "Arindam Mitra, Luciano Del Corro, Shweti Mahajan,Andres Codas, Clarisse Simoes, Sahaj Agrawal, XuxiChen, Anastasia Razdaibiedina, Erik Jones, KritiAggarwal, et al. 2023.Orca 2: Teaching smalllanguage models how to reason.arXiv preprintarXiv:2311.11045. Arvind Neelakantan, Tao Xu, Raul Puri, Alec Rad-ford, Jesse Michael Han, Jerry Tworek, Qiming Yuan,Nikolas A. Tezak, Jong Wook Kim, Chris Hallacy,Johannes Heidecke, Pranav Shyam, Boris Power,Tyna Eloundou Nekoul, Girish Sastry, GretchenKrueger, David P. Schnurr, Felipe Petroski Such,Kenny Sai-Kin Hsu, Madeleine Thompson, TabarakKhan, Toki Sherbakov, Joanne Jang, Peter Welinder,and Lilian Weng. 2022. Text and code embeddingsby contrastive pre-training. ArXiv, abs/2201.10005.",
  "Connor Shorten, Taghi M. Khoshgoftaar, and BorkoFurht. 2021. Text data augmentation for deep learn-ing. Journal of Big Data, 8": "Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mah-davi, Jason Wei, Hyung Won Chung, Nathan Scales,Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl,et al. 2023a. Large language models encode clinicalknowledge. Nature, 620(7972):172180. Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,Ellery Wulczyn, Le Hou, Kevin Clark, StephenPfohl, Heather Cole-Lewis, Darlene Neal, et al.2023b. Towards expert-level medical question an-swering with large language models. arXiv preprintarXiv:2305.09617. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,Adam R Brown, Adam Santoro, Aditya Gupta,Adri Garriga-Alonso, et al. 2022.Beyond theimitation game: Quantifying and extrapolating thecapabilities of language models.arXiv preprintarXiv:2206.04615. Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu,Yongrui Chen, and Guilin Qi. 2023. Can chatgptreplace traditional kbqa models? an in-depth analysisof the question answering performance of the gpt llmfamily. In International Semantic Web Conference,pages 348367. Springer. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, YannDubois, Xuechen Li, Carlos Guestrin, Percy Liang,and Tatsunori B Hashimoto. 2023.Alpaca: Astrong, replicable instruction-following model. Stan-ford Center for Research on Foundation Models. stanford. edu/2023/03/13/alpaca. html,3(6):7.",
  "Kushal Tirumala, Aram H. Markosyan, Luke Zettle-moyer, and Armen Aghajanyan. 2022. Memorizationwithout overfitting: Analyzing the training dynamicsof large language models. ArXiv, abs/2205.10770": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Lewis Tunstall, Edward Beeching, Nathan Lambert,Nazneen Rajani, Kashif Rasul, Younes Belkada,Shengyi Huang, Leandro von Werra, ClmentineFourrier, Nathan Habib, et al. 2023. Zephyr: Di-rect distillation of lm alignment.arXiv preprintarXiv:2310.16944. Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xian-gru Tang, Tianhang Zhang, Cheng Jiayang, YunzhiYao, Wenyang Gao, Xuming Hu, Zehan Qi, et al.2023. Survey on factuality in large language models:Knowledge, retrieval and domain-specificity. arXivpreprint arXiv:2310.07521. Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,Xuanjing Huang, Guihong Cao, Daxin Jiang, MingZhou, et al. 2020. K-adapter: Infusing knowledgeinto pre-trained models with adapters. arXiv preprintarXiv:2002.01808. Yizhong Wang, Swaroop Mishra, Pegah Alipoor-molabashi,Yeganeh Kordi,Amirreza Mirzaei,Anjana Arunkumar, Arjun Ashok, Arut SelvanDhanasekaran, Atharva Naik, David Stap, et al. 2022.Super-naturalinstructions: Generalization via declar-ative instructions on 1600+ nlp tasks. arXiv preprintarXiv:2204.07705.",
  "BRAG Ablation Study": "As mentioned in , we compared variousvalues of K {0, . . . , 5}, shown in .Wewere unable to find an optimal value of K permodel, per 0/5-shot, or per task. In fact, otherthan Anatomy that worked well with K = 2 con-sistently, there seems to be no patterns that aid inpredicting the performance per K, unlike the re-sults presented in (Lewis et al., 2020) for othersetups. Moreover, the gap between the best andworst performing Ks can be large.",
  "The next example was taken from the follow-ing Wikipedia page: \"2023 Indianapolis mayoralelection\"": "Paraphrase I The mayoral election inIndianapolis took place on November 7,2023, with preliminary elections occur-ring on May 2. The sitting mayor, Demo-crat Joe Hogsett, successfully ran for athird term. Both Hogsett and his Repub-lican opponent, Jefferson Shreve, movedon to the main election. Paraphrase II On November 7, 2023,citizens of Indianapolis cast their votesto elect their Mayor, following primaryelections on May 2. Joe Hogsett, theDemocrat already in office, won his bidfor a third term. Hogsett and the Repub-lican candidate, Jefferson Shreve, werethe two contenders in the final electoralround."
}