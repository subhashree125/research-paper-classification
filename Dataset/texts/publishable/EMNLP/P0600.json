{
  "Abstract": "Can human reading comprehension be assessedfrom eye movements in reading? In this work,we address this longstanding question usinglarge-scale eyetracking data. We focus on acardinal and largely unaddressed variant of thisquestion: predicting reading comprehension ofa single participant for a single question fromtheir eye movements over a single paragraph.We tackle this task using a battery of recentmodels from the literature, and three new multi-modal language models. We evaluate the mod-els in two different reading regimes: ordinaryreading and information seeking, and exam-ine their generalization to new textual items,new participants, and the combination of both.The evaluations suggest that the task is highlychallenging, and highlight the importance ofbenchmarking against a strong text-only base-line. While in some cases eye movements pro-vide improvements over such a baseline, theytend to be small. This could be due to limi-tations of current modelling approaches, limi-tations of the data, or because eye movementbehavior does not sufficiently pertain to fine-grained aspects of reading comprehension pro-cesses. Our study provides an infrastructure formaking further progress on this question.1",
  "Introduction": "Reading comprehension is an indispensable skillfor successful participation in modern society. Con-sequently, many efforts and resources are investedin the development of reading comprehension as-sessments by educational institutions and commer-cial companies. The standard, and to date the onlypractical way to assess reading comprehension isthrough behavioral tasks, most commonly readingcomprehension questions. However, despite itsclear value and ubiquitous use, this approach is ex-tremely time-consuming and costly, which severely",
  "Code is available at": "limits the volume and public availability of readingcomprehension tests. Further, this testing method-ology relies on offline behavioral signals the endresponses to a few select reading comprehensionquestions, and has no ability to trace the rich onlinereading comprehension processes as they unfoldover time.An alternative vision for assessing reading com-prehension has been emerging in psycholinguis-tics and the psychology of reading. It posits thatreading comprehension may be decoded in real-time directly from eye movements in reading. Thisvision is rooted in literature that suggests a tightcorrespondence between eye movements and realtime language comprehension (Just and Carpenter,1980; Rayner, 1998; Rayner et al., 2016, amongothers). With the rise of modern machine learningand NLP, multiple studies over the past decade at-tempted to use eye movement data to predict read-ing comprehension (Copeland et al., 2014; Ahnet al., 2020; Reich et al., 2022; Mzire et al.,2023b, among others). This line of work suggeststhat in some cases various aspects of reading com-prehension can be predicted from eye movementswith above-chance performance. However, despitethe advances so far, predictive modeling of readingcomprehension from gaze is still in its infancy.A number of factors have been hinderingprogress in this area.One is the paucity andsmall size of reading comprehension data pairedwith eye movements. Second, the task of readingcomprehension prediction has thus far been pre-dominantly formulated as prediction of aggregatedscores across multiple questions rather than pre-diction of comprehension at the resolution of anindividual question. Further, reading comprehen-sion has been primarily studied when the reader hasno specific goals with respect to the text beyondgeneral comprehension, a regime that we refer toas ordinary reading. Many other reading regimescommon in daily life, such as explicit information seeking, remain largely unaddressed. Finally, de-spite the dramatic progress in machine learningand NLP in recent years, effective joint modelingof text and eye movements remains a nascent andchallenging domain of investigation.In this work, we take a step forward in advancingthe state-of-the-art in eye movement-based predic-tion of reading comprehension by combining newmodels, new data, and systematic evaluations. Ourprimary contributions are the following: Task:we introduce the challenging andlargely unaddressed task of predicting thereading comprehension of a single reader withrespect to a single reading comprehensionquestion over one passage. This task is en-abled by OneStop Eye Movements (Malmaudet al., 2020), the largest eyetracking for read-ing comprehension dataset to date with 486multiple-choice questions and 19,440 ques-tion responses from 360 participants. Modeling:we develop three new mod-els that combine text and eye movementsbased on the transformer encoder architec-ture:RoBERTa-QEye, MAG-QEye, andPostFusion-QEye. These models address bothtest format-agnostic and multiple-choice spe-cific variants of the task.",
  "Related Work": "Our study contributes to an existing body of workon the prediction of reading comprehension fromeye movements in reading. To address various as-pects of this task, prior studies used a wide rangeof models, including linear models (Mzire et al.,2023b,a), kernel methods (Makowski et al., 2019),feed-forward networks (e.g. Copeland et al., 2014),CNNs (Ahn et al., 2020) and RNNs (e.g. Ahn et al., 2020; Reich et al., 2022). These were typicallyapplied to the prediction of aggregated comprehen-sion scores over multiple items. In this work, weevaluate multiple models from prior work on thesingle-item reading comprehension task.While transformer models (Vaswani et al., 2017),have been used for joint modeling of eye move-ments and text (e.g. Deng et al., 2023; Yang andHollenstein, 2023), they have not been applied tothe problem of reading comprehension predictionfrom eye movements. In this work we introducethree new transformer models which draw on multi-modal transformers, in particular MAG (Rahmanet al., 2020) which integrated text, speech and vi-sion for sentiment analysis, and language visionmodels such as VisualBERT (Li et al., 2019) (seeZhu et al. (2023); Xu et al. (2023) for reviews).Most prior studies on reading comprehensionprediction from eye movements relied solely on eyemovement features (Copeland et al., 2014; South-well et al., 2020; Ahn et al., 2020; Mzire et al.,2023b,a), while a few combined eye movementswith properties of the underlying text (Martnez-Gmez and Aizawa, 2014; Makowski et al., 2019;Reich et al., 2022). In the current work, we takethe latter, under-explored approach. The impor-tance of combining eye movements with attributesof the text is motivated by a large literature in thepsychology of reading which points to systematiceffects of linguistic properties of the text on readingtimes (Rayner, 1998; Rayner et al., 2004; Klieglet al., 2004; Demberg and Keller, 2008; Smith andLevy, 2013, among others), in particular in thecontext of reading comprehension (Just and Car-penter, 1980) and linguistic proficiency (Berzaket al., 2018; Berzak and Levy, 2023).While highly informative, existing work is criti-cally limited by small data, especially with respectto the number of available questions and partici-pants. For example, Copeland et al. (2014) have 9text pages, 18 questions and 39 participants. SB-SAT (Ahn et al., 2020), the only publicly availableeyetracking dataset for reading comprehension, has22 text pages, 20 questions, and 95 participants.The small size of previously used datasets severelylimits the potential of NLP and machine learningapproaches for reading comprehension prediction.At the same time, the reading comprehension com-ponent of broad coverage eyetracking datasets suchas MECO (Siegelman et al., 2022) and CELER(Berzak et al., 2022) comprises only simple com-prehension questions that serve as attention checks, and as such are not well suited for studying read-ing comprehension. OneStop, used here, has alarge number of items, participants and questions,enabling meaningfully addressing item-level pre-diction of comprehension.Prior work varies in experimental designs. Inseveral studies, multiple questions are presented af-ter reading a multi-screen text without the ability toreturn to the text (Makowski et al., 2019; Ahn et al.,2020; Reich et al., 2022). This design is advanta-geous in the separation of text reading and questionanswering, but can lead to loose relations betweeneye movements and question-answering behaviordue to memory limitations. In other studies, suchas Copeland et al. (2014), participants can switchback and forth between the text and the questions.This creates a complex mix of ordinary reading andinformation seeking components which are difficultto disentangle. In OneStop, a single question ap-pears immediately after reading a single text page,setting a middle ground between the two primaryexisting approaches for question presentation, andalleviating their main disadvantages. At the sametime, it includes a question preview manipulationwhich allows to systematically compare readingcomprehension in ordinary reading and questionguided information seeking.An additional limitation of prior work is thescope and nature of the evaluations. With the ex-ception of Copeland et al. (2014), both trainingand evaluation were previously carried out overaggregated responses across multiple questions,and in some cases also across multiple texts. Theseapproaches, which focus on measuring overall com-prehension, do not enable testing direct links be-tween eye movements and understanding specificaspects of the text. In several studies (Martnez-Gmez and Aizawa, 2014; Makowski et al., 2019;Ahn et al., 2020; Reich et al., 2022), an additionalstep was taken, binning comprehension scores intotwo binary categories, high versus low comprehen-sion, thus further simplifying the task.A second important evaluation limitation in priorwork is evaluations in which eyetracking data forboth the test participants and items is used inthe training set. To our knowledge, except forMakowski et al. (2019), no work has evaluatedreading comprehension prediction when neitherthe participant nor the item appears in the trainingdata. This evaluation regime is needed to fully char-acterize model generalization ability. Importantly,even in less challenging regimes and with aggre- gated scores and binning, model performance inprior work is typically only modestly higher thanchance level. More stringent evaluations withoutbinning comprehension scores (Martnez-Gmezand Aizawa, 2014), or with held-out participantsand/or items (Makowski et al., 2019; Reich et al.,2022) tend to exhibit chance level performance.These results suggest that generalization in readingcomprehension prediction is highly challenging.",
  "Eyetracking Data": "We use OneStop, an extended version of the datasetcollected by Malmaud et al. (2020) over the tex-tual materials of OneStopQA (Berzak et al., 2020).OneStop is the largest English L1 eyetracking forreading corpus to date. The data was collectedusing an Eyelink 1000+ eyetracker at a samplingrate of 1000Hz. In this dataset, 360 adult nativeEnglish participants read newswire articles fromthe Guardian, and answer a multiple-choice read-ing comprehension question about each paragraph.The dataset includes 30 articles divided into 162paragraphs. The average paragraph length is 109words. Each paragraph has 3 possible questions,corresponding to a total of 486 questions.",
  "AnswerCategoryDegree of ComprehensionGatheringHunting": "ACorrectFull comprehension7,890 (81.2)8,450 (86.9)BIncorrectIdentified question-relevant information1000 (10.3)744(7.7)CIncorrectSome degree of attention to the text568(5.8)374(3.8)DIncorrectNo evidence for comprehension260(2.7)152(1.6) : Summary of the STARC annotation frameworkfor answer types AD, their corresponding degree ofcomprehension, and number of trials in which each an-swer type was chosen in OneStop. Values in parenthesesare percentages by reading regime. The articles are divided into three 10-articlebatches, where each participant is assigned to onebatch. In each trial of the experiment, participantsread a paragraph and then proceed to answer oneof the three possible questions on a new screen,without the ability to return to the paragraph. 180participants are in an ordinary reading (Gathering)regime where they do not see the question prior toreading the paragraph. The remaining 180 partici-pants are in an information seeking regime (Hunt-ing) where they are presented with the question(but not the answers) before reading the paragraph.The total number of trials is 19,440, split equallyacross the two reading regimes. This correspondsto 40 responses per question, 20 for each regimeparagraph combination. The total number of word",
  "Segmentation": ": Left: an example of an eye movement trajectory over a paragraph, where red circles represent fixations,and blue arrows represent saccades. Right: a schematic depiction of word-level feature extraction, resulting in avector Ewi: an eye movements and linguistic word properties feature representation for each word. tokens over which eyetracking data was collectedin OneStop is 3,827,216.The underlying textual materials and readingcomprehension questions follow the STARC an-notation framework (Berzak et al., 2020), whereanswer A is the correct answer, answer B is a mis-comprehension of the information required to an-swer correctly, C refers to another part of the textthat does not provide the answer to the questionand D has no textual support. These answer typescorrespond to an ordering of the answers by degreeof comprehension. presents a summary ofthe framework along with answer choice statisticsin the OneStop eyetracking data.",
  "Correct versus Incorrect Comprehension": "The primary task we address is item-level pre-diction of whether a participant will respond cor-rectly to a single question about a paragraph fromthe participants eye movements over the para-graph. For each paragraph p and a correspondingquestion qp, the possible answers are Ansqp ={aqp1 , aqp2 , aqp3 , aqp4 }. Note that the correct answerA and the three distractors {B, C, D} are randomlymapped per trial to a1 through a4. The set of p, qp,and optionally Ansqp, defines a textual item W.Given a participant S tested on item W, wherethe participants eye movements over the para-graph are EyespS, the complete trial informationis TrialWS := {W, EyesSp }. We make W optionalto allow for models that use only eye movementswithout the text.The prediction problem can then be formulatedas a binary classification task, we predict whetherthe participant will answer the question correctly.Formally, given a classifier h:",
  "h : TrialWS {0, 1}(1)": "where 1 indicates a correct answer (A) and 0 indi-cates an incorrect answer (B/C/D).Note that this task formulation abstracts awayfrom the multiple-choice format. This allows as-sessing comprehension without depending on theformat of the subsequent assessment task (e.g. an-swer choice, answer production), nor its detailssuch as the number of answer choices and theirspecific content in the multiple-choice format. Thecombination of these task characteristics enablesapplying prior models from the literature, all ofwhich predict a binary outcome without taking intoaccount the answers, and some of which use onlyeye movements without the text.",
  "Models": "We introduce three new models, RoBERTa-QEye,MAG-QEye and PostFusion-QEye, all of whichcombine text and eye movements information, andrely on the transformer language model encoder.Specifically, we use the RoBERTaLARGE model(Liu et al., 2019). Each of these models uses adifferent strategy for combining text with eye move-ments. RoBERTa-QEye augments the textual in-put with additional eye movement features. MAG-QEye uses eye movement information to modifycontextualized word representations at intermedi-ate layers of the language model. PostFusion-QEyeprocesses text and eye movements separately andthen combines them via cross-attention mecha-nisms. We further adjust a number of prior models",
  "(c) PostFusion-QEye": ": Model architectures. (a) RoBERTa-QEye treats eye movements as additional input features. (b) MAG-QEye uses eye movement information to modify contextualized word representations. (c) PostFusion-QEyeprocesses text and eye movements separately and combines them via cross-attention mechanisms. Model input:EyesP represents the participants eye movements over the paragraph p, qp is a question and [Ansqp] are optionalanswer choices which are provided only in the multiple choice version of the task. from the literature for the single-item reading com-prehension prediction task.Eye Movement Feature Representations Theeyetracking record is commonly represented as ascanpath consisting of fixations (periods in whichthe gaze position is stable) and saccades (rapid tran-sitions between fixations). The examined modelsrepresent this information in three different ways,in increasing level of granularity:",
  "Fixations: Accounting for each fixation andits preceding and following saccade": "Our new models focus on the word and fixationlevel approaches, using a variety of eye movementmeasures from the psycholinguistic literature. Asreading times are known to be affected by linguisticword properties such as predictability, frequency,and length (Rayner et al., 2004; Kliegl et al., 2004;Rayner et al., 2011), which are not directly encodedin word embeddings, we further add such proper-ties to the eye movement representations to allowthe models to learn eye movements-word propertyinteractions. The strength of such interactions hasbeen shown to be indicative of the readers linguis-tic proficiency (Berzak et al., 2018; Berzak andLevy, 2023), which is directly related to reading comprehension. The eye movement and linguisticword property features used in all the models arelisted in Appendix A. Note that two different fea-ture sets are used for representing eye movementsat the word and fixation levels. presentsan example of an eye movement trajectory overa paragraph and a schematic visualization of theword-level feature extraction approach.",
  "RoBERTa-QEye": "RoBERTa-QEye incorporates eye movements asadditional input sequences to RoBERTa by pro-jecting them to the word embedding space. Anoverview of the architecture is presented in Fig-ure 2a. The model is implemented in two vari-ants, RoBERTa-QEye-Words which has a word-level feature representation and RoBERTa-QEye-Fixations, which uses a fixation-level representa-tion. Both variants combine a textual input ZWwith eye movements input ZEP .The textual representation ZW is the word em-bedding sequence [CLS; p; SEP; qp; [Ansqp]; SEP],where p is the paragraph, qp is the question,[Ansqp] are optional answers, and SEP is a sep-arator token. The eye movement representation forthe paragraph ZEP = [ZEw1, ..., ZEwn] consists ofa representation for each fixation or word i as:",
  "where Ewi are the eye movement and word prop-erty features and FC is a fully connected layer": "projecting this feature representation to the wordembedding space. Embpos(i) is the positional em-bedding of the i-th word or fixation, initialized tothe models original positional embedding, whichties the eye movement representation to its respec-tive word index. Embeye is an additional learnableembedding marking the presence of eye movementinformation. ZEP is concatenated with the wordembedding representation ZW , separated by a spe-cial token SEPE, initialized as SEP. The combinedsequence [ZEP ; SEPE; ZW ] is passed through thetransformer encoder language model. The resultingCLS token is then provided to a multilayer percep-tron for response prediction.",
  "MAG-QEye": "MAG-QEye, shown in b, modifies thetransformer encoders hidden word representationsbased on eye movement information. It is an adap-tation of the MAG architecture (Rahman et al.,2020) originally developed for multimodal senti-ment analysis. The goal of this model is to empha-size or de-emphasize words based on their respec-tive eye movement features. Formally, for a givenmodel layer k, each hidden token representation inthe paragraph ZkWi is shifted by HWi:",
  "PostFusion-QEye": "PostFusion-QEye, outlined in c, processestext and eye movements separately and combinestheir representations through two cross-attentionmechanisms. The primary objective of these mech-anisms is to transform both text and eye movementdata into a unified space, which we refer to as thereading space while taking into account the readingcomprehension prediction task.The input paragraph is passed through a lan-guage model to obtain contextualized embeddingsZP . The eye movement input features are pro-cessed through two 1D convolution layers, re-sulting in the eye movement representation ZEP .Cross-attention is then applied between the para-graph embedding ZP and ZEP , with eye move-ments as the query and text embeddings as the key and the value. This step modifies the paragraphwords based on the eye movements. The outputis provided along with ZEP to a fully connectedlayer, yielding ZEP +P , a projection of the two intoa shared space. Another cross-attention layer isapplied between ZEP +P as key and value and thequestion embedding ZQ as query, weighting theshared representation by the relevance to the ques-tion. The output of this step is passed to a multi-layer perceptron classifier to predict the response.",
  "Baseline Models": "We compare the proposed models to a number ofeye movement models from prior work. We focuson models that were either designed for readingcomprehension prediction or can be adjusted to thebinary task with minimal modifications. As noneof the prior models allow encoding of answers, wecannot apply them to the multiple-choice task. Logistic Regression(Mzire et al., 2023b)Based on Mzire et al. (2023b) who used linearregression for reading comprehension prediction.We use the same feature set which includes readingspeed, and global averages of standard eye move-ment measures.",
  "CNN(Ahn et al., 2020) Similarly to Mzire et al": "(2023b), this model is based only on eye movementinformation, without the underlying text. It usesthe fixation sequence, represented by x and y coor-dinates on the screen, fixation durations, and pupilsize, which are passed through a ConvolutionalNeural Network (CNN) to predict a binary compre-hension outcome. BEyeLSTM(Reich et al., 2022) A model forpredicting reading comprehension from eye move-ments which represents both the fixation sequenceand text features, combining LSTMs with affinetransformations.BEyeLSTM outperforms theCNN model of Ahn et al. (2020), on the high versuslow comprehension task with SB-SAT.",
  "Eyettention(Deng et al., 2023) This model wasoriginally developed for scanpath prediction. Eyet-tention is a word sequence encoder and a fixation": "sequence encoder that uses a pre-trained BERT(Devlin et al., 2019) and an LSTM (Hochreiter andSchmidhuber, 1997), with a cross-attention mecha-nism for the alignment of the input sequences. Weadjust this model for prediction of reading com-prehension by using global cross-attention insteadof windowed attention, and represent the scanpathusing the last hidden representation. Further detailson this model are provided in Appendix B.",
  "No Eye Movements Baselines": "We further introduce two baselines with no eyemovements. The first is a majority class baseline.The second is Text-only RoBERTa. This baselineis of special importance as it is able to take intoaccount item difficulty as reflected in the item tex-tual characteristics and the distribution of item re-sponses in the training data. To our knowledge, noprevious reading comprehension prediction methodwas benchmarked against this kind of baseline.",
  "New Item & Participant: No prior eyetrack-ing data is available for the participant nor forthe item": "We further report aggregated results across all threeregimes.We perform model training, hyperparameter tun-ing, and evaluation separately for the ordinaryreading and information seeking parts of the data,with 10-fold cross-validation. presentsschematically one of the 10 data splits for a 10-article 60-participant batch. A full data split for areading regime (ordinary reading or informationseeking) is the union of three such splits. In eachsplit, approximately 64% of the data is allocatedfor training, 17% for validation, and 19% for test-ing. The test data is further divided into 9% inthe New Participant, 9% New Item, and 1% NewItem & Participant regimes. In total across the",
  "Train": ": A schematic depiction of a 10-article 60-participant batch split, divided into a train set, a val-idation set, and the three test sets. A full data split for areading regime (ordinary reading or information seek-ing) consists of the union of three batch splits. 10 splits, approximately 90% of the trials in thedataset appear in each of the New Participant andNew Item evaluation regimes, and 10% in the NewItem & Participant regime. Items are assigned tothe train, validation and test portions of each split atthe article level, such that no article is split acrossdifferent data portions, ensuring generalization toitems whose content is unrelated to items seen intraining. See Appendix C for further informationon the splits.Because the data is unbalanced across classes,we use balanced accuracy as the evaluation metric.As prior work has shown considerable differencesin reading behavior between the ordinary readingand information seeking reading conditions (Hahnand Keller, 2023; Malmaud et al., 2020; Shubi andBerzak, 2023), we train and evaluate the models oneach type of trials separately. We perform hyperpa-rameter tuning for each split, and report balancedaccuracy results on the aggregation of the predic-tions across the 10 test sets. We assume that attest time the evaluation regime of the trial is un-known. Model hyperparameter tuning is thereforebased on the entire validation set of the split. Asprior models from the literature were developedfor different tasks and on different datasets, werun a hyperparameter search for each model overa search space that includes the original parametersettings. Hyperparameters are also optimized forthe Text-only RoBERTa baseline. To address the",
  "MajorityNoneNone50.050.050.050.050.050.050.050.0Text-only RoBERTaNoneEmb54.863.155.258.751.863.150.557.1": "Log. Reg. (Mzire et al., 2023b)GlobalNone53.350.853.852.253.252.252.352.7CNN (Ahn et al., 2020)FixationsNone51.051.051.951.151.451.349.251.2BEyeLSTM (Reich et al., 2022)FixationsLing. Feat.50.655.751.153.050.555.155.153.0Eyettention (Deng et al., 2023)FixationsEmb + Word Len.54.860.457.157.650.556.452.353.4 RoBERTa-QEyeWordsEmb + Ling. Feat.55.563.552.159.150.563.851.056.8RoBERTa-QEyeFixationsEmb + Ling. Feat.53.361.357.157.350.360.350.855.1MAG-QEyeWordsEmb + Ling. Feat.54.864.1*53.859.252.562.351.357.1PostFusion-QEyeFixationsEmb + Ling. Feat.54.863.555.058.953.8*62.753.858.0 : Results on balanced accuracy for the main binary reading comprehension prediction task (correct vs incorrectcomprehension). All denotes results for the aggregation of all the trials across the three test regimes. Emb standsfor word embeddings, Ling. Feat. for linguistic word properties. Statistically significant improvements over theText-only RoBERTa baseline, using a paired bootstrap test, chosen based on considerations described in (Dror et al.,2018), are marked with * at p < 0.05.",
  "MAG-QEyeWordsEmb + Ling. Feat.27.9***32.530.4***30.2**26.830.029.028.4PostFusion-QEyeFixationsEmb + Ling. Feat.29.4**31.732.9*30.6*27.5*27.926.727.6": ": Results on balanced accuracy for the multiple-choice specific answer prediction task. Statistically significantimprovements over the Text-only RoBERTa baseline, using a paired bootstrap test, are marked with * at p < 0.05,** at p < 0.01 and *** at p < 0.001. We note that in some cases, higher balanced accuracy scores correspond tolower p-values due to higher variability in the predictions of the minority classes. unbalanced nature of the data, shown in ,we sample the same number of trials from eachanswer class during training. Additional details onfeature normalization, model training, hyperparam-eter search, and number of model parameters areprovided in Appendix D.",
  "Correct vs Incorrect Comprehension": "In , we present trial-level reading compre-hension prediction results for ordinary reading andinformation seeking. The best results are achievedby different models under the different evaluationregimes. MAG-QEye achieves the highest overallbalanced accuracy in ordinary reading with a scoreof 59.2, while PostFusion-QEye performs best ininformation seeking, with a score of 58.0. In all theevaluation regimes, the best performing model out-performs the Text-only RoBERTa baseline. In allbut the New Item & Participant evaluation regime,the best performing model is one of our proposedmodels. Text-only RoBERTa turns out to be a keybenchmark, whereby most models are below thisbaseline especially in the New Participant regime. We note several key trends in the results. First,results in the New Participant regime tend to behigher than in the New Item regime, highlightingthe importance and the challenge of generaliza-tion to new items. The strong performance of theRoBERTa text-only baseline in the New Partici-pant regime suggests that much of the gains in thisregime do not stem from eye movement informa-tion, but rather from item properties and statistics.This highlights the importance of benchmarkingagainst such a baseline for assessing the contri-bution of eye movement information. It furtherunderscores the importance of explicit representa-tion of the text; the Logistic Regression, CNN andBEyeLSTM models, which do not include sucha representation, perform poorly in the New Par-ticipant regime. Finally, for any given model, theordinary reading regime tends to yield higher accu-racies compared information seeking. We hypothe-size that this difference could be related to highervariability in reading strategies in information seek-ing across participants (Shubi and Berzak, 2023).We leave a detailed investigation of this hypothesisto future work.",
  "Multiple-Choice Task": "In we use our models, MAG-QEye andPostFusion-QEye, and the two RoBERTa-QEyevariants to predict participants specific answer re-sponse among the four provided answers. As men-tioned above, prior models from the literature arenot applicable for this task. We find that all themodels outperform the Text-only RoBERTa base-line in the two regimes that involve new items,but not in the New Participant regime. The bestperforming model in the overall evaluations isRoBERTa-QEye-Fixations. The general trends re-garding higher performance in the New Participantregime compared to the New Item regime, as wellas the stronger within-model performance in or-dinary reading compared to information seeking,extend to this evaluation.",
  "Summary and Discussion": "This paper presents a systematic evaluation of theability to predict reading comprehension from eyemovements in reading at the level of a single ques-tion over a single paragraph. We address this taskusing a range of existing and new models, appliedto large scale data across several task variants andevaluation regimes. Our experiments indicate thatthe task at hand is highly challenging, and furtherhighlight the importance of text-only baselines forassessing the added value of eye movements in-formation. However, we do find that small im-provements over a strong text-only baseline areachievable with the proposed and some of the pastmodeling approaches.Given the presented results, the extent to whichspecific aspects of reading comprehension can bereliably decoded from eye movements signal re-mains an open question. It is possible that eyemovements simply do not contain sufficient infor-mation for decoding comprehension at high accu-racy rates for the examined level of granularity.Alternatively, it may be the case that current mod-eling techniques do not represent or process eye movements data effectively enough for this task.Another factor whose role in task difficulty needsto be investigated in more detail is the imbalancednature of the data, where only a relatively smallfraction of the responses are incorrect.Additional work on eye movement data analysis,new model architectures, feature representationsand training regimes is needed for making furtherprogress on this task. Additionally, new datasetswith other task variants and other populations suchas children and L2 readers are required to study theproblem in a more comprehensive manner. We en-vision that the models, tasks, evaluation protocols,and data presented here will serve as a steppingstone for such work, as well as a broader scientificinvestigation of the relations between eye move-ments and reading comprehension.",
  "Ethical Considerations": "The eyetracking data used in this work was col-lected by Malmaud et al. (2020) under an institu-tional IRB protocol. All the participants providedwritten consent prior to participating in the eye-tracking study. The data is anonymized. Analy-ses of the relations between eye movements andreading comprehension, and predictive models ofcomprehension are among the primary use casesfor which the data was collected.Automatic reading comprehension assessmentsfrom eye movements can potentially address short-comings of standard assessment methodologies byreducing test development and test taking costs,and enhancing test availability.However, theyalso introduce potential risks for biased and inaccu-rate assessments that may put various populationsand individuals at a disadvantage. These includenon-native speakers, older participants, participantswith cognitive impairments, disabilities, eye condi-tions and others. Much higher model performancethan the current state-of-the-art and a thorough ex-amination of potential biases due to factors unre-lated to reading comprehension are needed beforeconsidering deploying such assessments.It has previously been shown that eye move-ments can be used for user identification (e.g. Bed-narik et al., 2005; Jger et al., 2020). We do notperform user identification in this study. We fur-ther emphasize that future reading comprehensionassessment systems are to be used only with ex-plicit consent from potential users to have their eyemovements collected and analyzed for this purpose.",
  "Limitations": "Our work has a number of limitations which are re-lated to the experimental design of OneStop. First,the textual data consists of articles with 4-7 para-graphs. Each question is over the content of asingle paragraph. Longer and shorter texts, as wellas questions that require integration of informationfrom several paragraphs, are not covered. The ex-perimental design does not allow participants to goback and forth between the question and passage,which is common in question answering tasks. Fur-ther, participant expectations for upcoming readingcomprehension questions, as well as the settingof an in-lab experiment may result in reading pat-terns that deviate from reading in everyday settings(Huettig and Ferreira, 2022) and could impact thepredictive performance of the model.While our work examines the feasibility of auto-mated assessment of reading comprehension fromeye movements, the accuracy of the models pre-sented is still very far from being relevant for de-ployment in real world scenarios. Our results arefurther limited to the equipment at hand. Our ap-proach has only been tested using a state-of-the-arteyetracker (Eyelink 1000 Plus) at a sampling rateof 1000Hz. This allows extracting gaze positionand duration at a very high temporal resolutionand character-level precision. While studies suchas Ishimaru et al. (2017) and Chen et al. (2023)have demonstrated predictive modeling capabili-ties using lower spatial and temporal resolution eyetracking systems, additional work is required to testthe feasibility of reading comprehension predictionusing such equipment.Although we use the largest eyetracking for read-ing comprehension dataset to date, OneStop wascollected from adult L1 English speakers, with nocognitive impairments, and in the large majority ofcases no eye conditions. We acknowledge that thispool of participants excludes multiple populations,including children, elderly, participants with cogni-tive and physical impairments and others. Futuredata collection and analysis work is required to testthe generalization capabilities and potential biasesof the models in other populations.In this work we assume the availability of bothsuitable eyetracking data and a pretrained languagemodel for the language at hand. Although languagemodels for lower-resource languages (e.g. Chriquiand Yahav, 2022; Vamvas et al., 2023) and multilin-gual models (e.g. Lai et al., 2023) have been made available, many languages still lack such models.Similarly, to the best of our knowledge, no eye-tracking data with a substantial reading comprehen-sion component is currently available for languagesother than English. This limits the generality ofthe results. More eyetracking data collection andlanguage model development work is required toinclude additional languages.",
  "This work was supported by ISF grant 1499/22": "Seoyoung Ahn, Conor Kelton, Aruna Balasubramanian,and Greg Zelinsky. 2020. Towards predicting read-ing comprehension from gaze behavior. In ACMSymposium on Eye Tracking Research and Applica-tions, ETRA 20 Short Papers, New York, NY, USA.Association for Computing Machinery. Roman Bednarik, Tomi Kinnunen, Andrei Mihaila, andPasi Frnti. 2005. Eye-movements as a biometric.In Image Analysis: 14th Scandinavian Conference,SCIA 2005, Joensuu, Finland, June 19-22, 2005. Pro-ceedings 14, pages 780789. Springer. Yevgeni Berzak, Boris Katz, and Roger Levy. 2018. As-sessing Language Proficiency from Eye Movementsin Reading. In Proceedings of the 2018 Conferenceof the North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long Papers), pages 19861996,Stroudsburg, PA, USA. Association for Computa-tional Linguistics.",
  "Vera Demberg and Frank Keller. 2008. Data from eye-tracking corpora as evidence for theories of syntacticprocessing complexity. Cognition, 109(2):193210": "Shuwen Deng, David R. Reich, Paul Prasse, PatrickHaller, Tobias Scheffer, and Lena A. Jger. 2023.Eyettention:An attention-based dual-sequencemodel for predicting human scanpaths during reading.In Proceedings of the ACM on Human-Computer In-teraction, pages 124. Association for ComputingMachinery. Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofDeep Bidirectional Transformers for Language Un-derstanding. In Proceedings of the 2019 Conferenceof the North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics. Rotem Dror, Gili Baumer, Segev Shlomov, and RoiReichart. 2018. The hitchhikers guide to testingstatistical significance in natural language processing.In Proceedings of the 56th annual meeting of theassociation for computational linguistics (volume 1:Long papers), pages 13831392.",
  "Falk Huettig and Fernanda Ferreira. 2022. The Myth ofNormal Reading. Perspectives on Psychological Sci-ence, page 17456916221127226. Publisher: SAGEPublications Inc": "Shoya Ishimaru, Kensuke Hoshika, Kai Kunze, KoichiKise, and Andreas Dengel. 2017. Towards readingtrackers in the wild: detecting reading activities byEOG glasses and deep neural networks. In Proceed-ings of the 2017 ACM International Joint Conferenceon Pervasive and Ubiquitous Computing and Pro-ceedings of the 2017 ACM International Symposiumon Wearable Computers, UbiComp 17, pages 704711, New York, NY, USA. Association for Comput-ing Machinery. Lena A Jger, Silvia Makowski, Paul Prasse, SaschaLiehr, Maximilian Seidler, and Tobias Scheffer. 2020.Deep eyedentification: Biometric identification usingmicro-movements of the eye. In Machine Learn-ing and Knowledge Discovery in Databases: Eu-ropean Conference, ECML PKDD 2019, Wrzburg,Germany, September 1620, 2019, Proceedings, PartII, pages 299314. Springer.",
  "Marcel Adam Just and Patricia A. Carpenter. 1980. Atheory of reading: From eye fixations to comprehen-sion. Psychological Review, 87(4):329": "Reinhold Kliegl, Ellen Grabner, Martin Rolfs, and RalfEngbert. 2004. Length, frequency, and predictabil-ity effects of words on eye movements in reading.European Journal of Cognitive Psychology - EUR JCOGN PSYCHOL, 16:262284. Viet Lai, Nghia Ngo, Amir Pouran Ben Veyseh, HieuMan, Franck Dernoncourt, Trung Bui, and ThienNguyen. 2023. ChatGPT beyond English: Towardsa comprehensive evaluation of large language mod-els in multilingual learning. In Findings of the As-sociation for Computational Linguistics: EMNLP2023, pages 1317113189, Singapore. Associationfor Computational Linguistics.",
  "Ilya Loshchilov and Frank Hutter. 2018. DecoupledWeight Decay Regularization. In International Con-ference on Learning Representations": "Silvia Makowski, Lena A Jger, Ahmed Abdelwahab,Niels Landwehr, and Tobias Scheffer. 2019. A dis-criminative model for identifying readers and as-sessing text comprehension from eye movements.In Machine Learning and Knowledge Discovery inDatabases: European Conference, ECML PKDD2018, Dublin, Ireland, September 1014, 2018, Pro-ceedings, Part I 18, pages 209225. Springer. Jonathan Malmaud, Roger Levy, and Yevgeni Berzak.2020. Bridging Information-Seeking Human Gazeand Machine Reading Comprehension. In Proceed-ings of the 24th Conference on Computational Natu-ral Language Learning, pages 142152, Stroudsburg,PA, USA. Association for Computational Linguistics.",
  "Pascual Martnez-Gmez and Akiko Aizawa. 2014": "Recognition of understanding level and languageskill using measurements of reading behavior. InProceedings of the 19th International Conference onIntelligent User Interfaces, IUI 14, page 95104,New York, NY, USA. Association for ComputingMachinery. Marius Mosbach, Maksym Andriushchenko, and Diet-rich Klakow. 2021. On the stability of fine-tuning{bert}: Misconceptions, explanations, and strongbaselines. In International Conference on LearningRepresentations.",
  "Diane C. Mzire, Lili Yu, Erik D. Reichle, GenevieveMcArthur, and Titus von der Malsburg. 2023a. Scan-path regularity as an index of reading comprehension.Scientific Studies of Reading": "Diane C. Mzire, Lili Yu, Erik D. Reichle, Titus vonder Malsburg, and Genevieve McArthur. 2023b. Us-ing eye-tracking measures to predict reading com-prehension. Reading Research Quarterly, 58(3):425449. Nicki Skafte Detlefsen, Jiri Borovec, Justus Schock,Ananya Harsh, Teddy Koker, Luca Di Liello, DanielStancl, Changsheng Quan, Maxim Grechkin, andWilliam Falcon. 2022. TorchMetrics - MeasuringReproducibility in PyTorch. Adam Paszke, Sam Gross, Francisco Massa, AdamLerer, James Bradbury, Gregory Chanan, TrevorKilleen, Zeming Lin, Natalia Gimelshein, LucaAntiga, Alban Desmaison, Andreas Kopf, EdwardYang, Zachary DeVito, Martin Raison, Alykhan Te-jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,Junjie Bai, and Soumith Chintala. 2019. PyTorch:An Imperative Style, High-Performance Deep Learn-ing Library. In Advances in Neural Information Pro-cessing Systems 32, pages 80248035. Curran Asso-ciates, Inc. Fabian Pedregosa, Gal Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, Olivier Grisel,Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-cent Dubourg, Jake Vanderplas, Alexandre Passos,David Cournapeau, Matthieu Brucher, Matthieu Per-rot, and douard Duchesnay. 2011. Scikit-learn: Ma-chine Learning in Python. Journal of Machine Learn-ing Research, 12(85):28252830.",
  "Keith Rayner. 1998. Eye movements in reading andinformation processing: 20 years of research. Psy-chological Bulletin, 124(3):372422": "Keith Rayner, Jane Ashby, Alexander Pollatsek, andErik D Reichle. 2004. The effects of frequency andpredictability on eye fixations in reading: implica-tions for the ez reader model. Journal of Experimen-tal Psychology: Human Perception and Performance,30(4):720. Keith Rayner, Elizabeth R Schotter, Michael EJ Masson,Mary C Potter, and Rebecca Treiman. 2016. So muchto read, so little time: How do we read, and can speedreading help? Psychological Science in the PublicInterest, 17(1):434. Keith Rayner, Timothy J Slattery, Denis Drieghe, andSimon P Liversedge. 2011. Eye movements and wordskipping during reading: Effects of word length andpredictability. Journal of Experimental Psychology:Human Perception and Performance, 37(2):514. David R. Reich, Paul Prasse, Chiara Tschirner, PatrickHaller, Frank Goldhammer, and Lena A. Jger. 2022.Inferring native and non-native human reading com-prehension and subjective text difficulty from scan-paths in reading. In Symposium on Eye TrackingResearch and Applications, ETRA 22. Associationfor Computing Machinery.",
  "Jannis Vamvas, Johannes Gran, and Rico Sennrich.2023. Swissbert: The multilingual language modelfor switzerland. In Proceedings of the 8th edition ofthe Swiss Text Analytics Conference, pages 5469": "Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is Allyou Need. In Advances in Neural Information Pro-cessing Systems, volume 30. Curran Associates, Inc. Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-icz, Joe Davison, Sam Shleifer, Patrick von Platen,Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,Teven Le Scao, Sylvain Gugger, Mariama Drame,Quentin Lhoest, and Alexander Rush. 2020. Trans-formers: State-of-the-art natural language processing.In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing: SystemDemonstrations, pages 3845, Online. Associationfor Computational Linguistics.",
  "Word-Level Eye Movement Features": "IA_DWELL_TIMEThe sum of the duration across all fixations that fell in the current interest areaIA_DWELL_TIME_%Percentage of trial time spent on the current interest area (IA_DWELL_TIME / TRIAL_DWELL_TIME).IA_FIXATION_%Percentage of all fixations in a trial falling in the current interest area.IA_FIXATION_COUNTTotal number of fixations falling in the interest area.IA_REGRESSION_IN_COUNTNumber of times interest area was entered from a higher IA_ID (from the right in English).IA_REGRESSION_OUT_FULL_COUNTNumber of times interest area was exited to a lower IA_ID (to the left in English).IA_RUN_COUNTNumber of times the Interest Area was entered and left (runs).IA_FIRST_FIX_PROGRESSIVEChecks whether the first fixation in the interest area is a first-pass fixation.IA_FIRST_FIXATION_DURATIONDuration of the first fixation event that was within the current interest areaIA_FIRST_FIXATION_VISITED_IA_COUNTThis reports the number of different interest areas visited so far before the first fixation is made to the current interest area.IA_FIRST_RUN_DWELL_TIMEDwell time of the first run (i.e., the sum of the duration of all fixations in the first run of fixations within the current interest area).IA_FIRST_RUN_FIXATION_COUNTNumber of all fixations in a trial falling in the first run of the current interest area.IA_SKIPAn interest area is considered skipped (i.e., IA_SKIP = 1) if no fixation occurred in first-pass reading.IA_TOPY coordinate of the top of the interest area.IA_LEFTX coordinate of the left-most part of the interest area.normalized_Word_IDPosition in the paragraph of the word interest area, normalized from zero to one.IA_REGRESSION_PATH_DURATIONThe summed fixation duration from when the current interest area is first fixated until the eyes enter an interest area with a higher IA_ID.IA_REGRESSION_OUT_COUNTNumber of times interest area was exited to a lower IA_ID (to the left in English) before a higher IA_ID was fixated in the trial.IA_SELECTIVE_REGRESSION_PATH_DURATIONDuration of fixations and refixations of the current interest area before the eyes enter an interest area with a higher ID.IA_LAST_FIXATION_DURATIONDuration of the last fixation event that was within the current interest area.IA_LAST_RUN_DWELL_TIMEDwell time of the last run (i.e., the sum of the duration of all fixations in the last run of fixations within the current interest area).PARAGRAPH_RTReading time of the entire paragraph.total_skipBinary indicator whether the word was fixated on.",
  "Fixation-level Eye Movement Features": "CURRENT_FIX_INDEXThe position of the current fixation in the trial.CURRENT_FIX_DURATIONDuration of the current fixation.CURRENT_FIX_PUPILAverage pupil size during the current fixation.CURRENT_FIX_XX coordinate of the current fixation.CURRENT_FIX_YY coordinate of the current fixation.NEXT_FIX_ANGLE, PREVIOUS_FIX_ANGLEAngle between the horizontal plane and the line connecting the current fixation and the next/previous fixation.NEXT_FIX_DISTANCE, PREVIOUS_FIX_DISTANCEDistance between the current fixation and the next/previous fixation in degrees of visual angle.NEXT_SAC_AMPLITUDEAmplitude of the following saccade in degrees of visual angle.NEXT_SAC_ANGLEAngle between the horizontal plane and the direction of the next saccade.NEXT_SAC_AVG_VELOCITYAverage velocity of the next saccade.NEXT_SAC_DURATIONDuration of the next saccade in milliseconds.NEXT_SAC_PEAK_VELOCITYPeak values of gaze velocity (in visual degrees per second) of the next saccade.",
  "Feature NameDescription": "Surprisal(Hale, 2001; Levy, 2008), formulated as log2(p(word|context)) for each word given the preceding textual content of theparagraph as context, probabilities extracted from the GPT-2-small language model (Radford et al., 2019; Wolf et al., 2020).Wordfreq_FrequencyFrequency of the word based on the Wordfreq package (Speer, 2022), formulated as log2(p(word)).LengthLength of the word in characters.start_of_lineBinary indicator of whether the word appeared at the beginning of a line.end_of_lineBinary indicator of whether the word appeared at the end of a line. Is_Content_WordBinary indicator of whether the word is a content word.A content word is defined as a word that has a part-of-speech tag of either PROPN, NOUN, VERB, ADV, or ADJ.n_LeftsThe number of leftward immediate children of the word in the syntactic dependency parse.n_RightsThe number of rightward immediate children of the word in the syntactic dependency parse.Distance2HeadThe number of words to the syntactic head of the word.",
  "B.1MAG": "We replace the vision and acoustic input with word-level eye movement features. To align them withthe tokenized text, we duplicate the word-level features for each subword token. Additionally, for a faircomparison with other models, we replace BERT with RoBERTaLARGE as the textual backbone model.Formally, each token embedding Zi is displaced by Hi.",
  "B.2Eyettention": "We adjust the prediction objective of the model from next fixation to trial-level classification. To thisend, we use global cross attention between the word sequence and the scanpath sequence instead of fixedwindow cross attention, as suggested in Deng et al. (2023). We then represent the whole scanpath usingthe last hidden representation of the scanpath LSTM. We further replace BERT, with RoBERTaLARGE forconsistency with the other models.",
  "B.3BEyeLSTM": "First, we employ SpaCy tokenization based on paragraph-level input rather than word-level input, resultingin a more precise tokenization. Second, the textual materials used here include a more fine-grained setof part-of-speech tags and named entities, which results in a larger final feature set. Lastly, we omit the\"words in fixed context on unigrams\" feature, as it presupposes that all the participants read the sametexts, which is not the case in OneStop.",
  "B.4CNN": "Ahn et al. (2020) resort to artificially subdividing SB-SAT texts into smaller segments in order generate asufficient number of training examples to make the dataset usable for their task of predicting low versushigh comprehension over multiple items. This heuristic is problematic in general, and not applicable tothe single item task addressed here. In the current work we use the entire fixation sequence as the input tothe model.",
  "CCross Validation Splits": "Each split guarantees an equal number of participants from each OneStopQA batch in each portion ofthe split, and is approximately stratified by answer type. Recall that each participant is presented with aspecific combination of a paragraph and one of its three associated questions. Due to the stratification byanswer type, it is not guaranteed that the appearances of any given paragraph will be balanced across thethree possible questions in any of the split portions. Note that across the 10 test sets, not all participant item combinations are covered in the test sets, as this would require 100 data splits.",
  "DFeature Standardization and Hyperparameter Tuning": "We apply standardization for each feature in EP , where the statistics are computed on the train set andapplied to the validation and test sets, separately for each split. Feature normalization is performed usingScikit-learn (Pedregosa et al., 2011).For all the neural models, we use the AdamW optimizer (Loshchilov and Hutter, 2018) with a batchsize of 16, a linear warmup ratio of 0.1, and a weight decay of 0.1, following best practice recom-mendations from Liu et al. (2019) and Mosbach et al. (2021). The search space for learning rates is{0.00001, 0.00003, 0.0001} and for dropout {0.1, 0.3, 0.5}.",
  "In PostFusion-QEye, the 1D convolution layers have a kernel size of three, stride 1, and padding 1": "All neural networks are trained using the Pytorch Lighting library (Falcon and The PyTorch Lightningteam, 2019; Paszke et al., 2019) and evaluated using torch-metrics (Nicki Skafte Detlefsen et al., 2022)on a NVIDIA A100-40GB and A40-48GB GPUs. We adapt Huggingfaces RoBERTa implementation(Wolf et al., 2020). The baselines described in .5 are reimplemented in this framework as well.A single training epoch took approximately 5 minutes. We train for a maximum of ten epochs, stoppingafter three epochs without improvement on the validation set.The number of model parameters is 355M for the RoBERTaLARGE backbone, and an additional 1.1Mfor MAG-QEye and RoBERTa-QEye, and 9M for PostFusion-QEye.",
  "EThe Role of Linguistic Word Property Features": "Our proposed models tend to outperform the Text-only RoBERTa baseline, especially in the two evaluationregimes that involve new items. Note however, that in addition to eye movements, these models alsoinclude linguistic word properties, which may provide information on the textual item that is not fullyencoded in word embeddings. Some of them (e.g. word length, frequency and surprisal) are also knownto be predictive of reading times.What is the effect of these features on model performance? To examine this question, we carry outtwo ablation experiments. In the first experiment, we ablate the linguistic word property features. In thesecond experiment we ablate the eye movement features. The latter ablation is not possible with fixationbased models, because even with the eye movement features removed, these models still have informationabout the gaze trajectory through the order and word identity of the fixations. We therefore perform theseexperiments only with the word based models RoBERTa-QEye-Words and MAG-QEye. in Appendix E presents the ablation results for the binary task. In the first experiment, removal oflinguistic word properties does not substantially affect model performance. This outcome does not matchour expectation regarding the potential benefits of allowing models to learn eye movement linguisticword property interactions. In the second experiment, overall, we again do not observe performancedegradation when ablating the eye movement features. While this experiment is not sufficient for drawinggeneral conclusions regarding the value of eye movement information for our task, it suggests that in ourtwo instances of word-based models, eye movements do not seem to provide substantial performancegains above and beyond features that can be readily extracted from the text. We leave a more extensiveinvestigation regarding the impact of linguistic features on model performance to future work.",
  "RoBERTa-QEye-Words55.563.552.159.150.563.851.056.8RoBERTa-QEye-Words w/o Ling. Feat55.463.356.359.251.162.750.756.6RoBERTa-QEye-Words w/o Eyes56.7*63.757.560.0** 49.363.251.256.0": ": The effect of ablating word-level eye movement features () and linguistic word properties ()on balanced accuracy for binary classification of the word based models MAG-QEye and RoBERTa-QEye-Words.Statistically significant improvements over Text-only RoBERTa, using a paired bootstrap test, are marked with *at p < 0.05, ** at p < 0.01 and *** at p < 0.001.",
  "FTextual Backbone Variants": "Our models use RoBERTa as a textual backbone model, and the parameters of this backbone are subjectedto change during model training. Other choices for this model component are possible. For example, onecan pre-train the model on multiple choice question answering, freeze the textual backbone parametersduring model training, or choose a different textual backbone model altogether. Preliminary experimentswith MAG-QEye in Appendix F do not show a consistent effect of these choices on modelperformance in the main prediction task. We leave a comprehensive investigation of textual backbonemodel choice and training to future work.",
  "MAG-QEye BackboneNewItemNewParticipantNew Item& ParticipantAllNewItemNewParticipantNew Item& ParticipantAll": "RoBERTa Large54.864.153.859.252.562.351.357.1RoBERTa Large Frozen54.361.451.457.551.960.053.355.8RoBERTa Large Trained for QA on RACE54.864.652.759.348.362.744.954.9RoBERTa Base52.864.056.958.350.863.5*51.656.9 : Balanced accuracy performance comparison of different backbone architectures and training strategies forMAG-QEye. Statistically significant improvements compared to an unfrozen RoBERTa Large backbone are markedwith * at p < 0.05, ** at p < 0.01 and *** at p < 0.001 using a paired bootstrap test."
}