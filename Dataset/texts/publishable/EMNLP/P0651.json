{
  "Abstract": "Questionansweringbasedonretrieval-augmentedgeneration(RAG-QA)isanimportant research topic in NLP and hasa wide range of real-world applications.However, most existing datasets for thistask are either constructed using a singlesource corpus or consist of short extractiveanswers, which fall short of evaluating largelanguage model (LLM) based RAG-QAsystemsoncross-domaingeneralization.To address these limitations,we createLong-form RobustQA (LFRQA), a new datasetcomprising human-written long-form answersthat integrate short extractive answers frommultiple documents into a single, coherentnarrative, covering 26K queries and largecorpora across seven different domains. Wefurther propose RAG-QA ARENA by directlycomparing model-generated answers againstLFRQAs answers using LLMs as evaluators.We show via extensive experiments thatRAG-QAARENAevaluation and humanjudgments on answer quality are highlycorrelated. Moreover, only 41.3% of the mostcompetitive LLMs answers are preferred toLFRQAs answers, demonstrating RAG-QAARENA as a challenging evaluation platformfor future research.1",
  "Introduction": "Traditional reading comprehension task is con-strained to fixed contexts (Rajpurkar et al., 2016;Kocisk et al., 2018; Huang et al., 2019). It is in-adequate at addressing real-world questions, whereno context is readily provided for a system to findanswers. Such open-ended questions require a sys-tem to identify answers in an enormous knowledgebase (e.g., Wikipedia) that is computationally pro-hibitive to feed into question answering systems",
  "Work done at AWS AI Labs.1Code:": ": LFRQA annotation example. There are threedocuments (some text removed for brevity) relevantto the query. We instruct annotators to combine RO-BUSTQAs answers into a coherent long-form answerwith added text if necessary. Citations , and indicate the supporting documents of each sentence. such as large language models (LLMs). Retrieval-augmented generative question answering (RAG-QA) becomes an effective tool to filter out massiveamounts of noise and select only a few highly rele-vant passages for LLM-based QA models.The wide applications of RAG-QA (Gao et al., 2023) necessitate the evaluation of systems out-of-domain (OOD) performances, because a real-worldsystem often confronts new data unseen duringtraining. Existing popular benchmark datasets suchas Natural Questions (NQ) (Kwiatkowski et al.,2019) and TriviaQA (Joshi et al., 2017) consistsolely of Wikipedia or Web documents, which fallshort at measuring OOD performances.ROBUSTQA (Han et al., 2023) was the firstdataset created to benchmark cross-domain robust-ness for RAG-QA. However, as illustrated by theyellow highlights in , ROBUSTQA followsNQs annotation format with short answer spansextracted from the documents. Such data formatis not the most suitable reference answer to eval-uate the current leading LLMs that typically gen-",
  "ROBUSTQA(Han et al., 2023)16.1KNQ (Kwiatkowski et al., 2019)3.6KMULTIHOP-RAG (Tang and Yang, 2024)2.5KASQA (Stelmakh et al., 2022)1.0KLONGFACT (Wei et al., 2024)2.3KELI5 (Fan et al., 2019)25.0K": ": Comparison of datasets. LFRQA distinguishes from previous work by uniquely encompassing seven features:1) RAG-QA dataset with answers annotated based on underlying corpus; 2) Long-form answers of paragraph length;3) Multiple documents that provide different facts/views; 4) Coherent answers that handle conflicting information; 5)Multiple-domain corpus to benchmark domain robustness; 6) Human annotated high-quality answers; 7) Large-scaleevaluation set. erate long-form responses with multiple pieces ofinformation combined in one coherent narrative(Brown et al., 2020; Ouyang et al., 2022; OpenAI,2024). Consequently, token overlap metrics usedin the extractive QA era (Karpukhin et al., 2020;Izacard et al., 2024) will penalize unfairly on theadditional supporting tokens generated by LLMs,resulting in extremely low overlap scores. As anexample, s extractive answers have poorExact-Match or F1 scores with the final long-formanswer. To create a long reference answer, onecould simply concatenate these short answers, butthe synthesized answers are either incoherent or ill-formatted, as illustrated by examples in Sec. 3.1.To address these drawbacks, we propose long-form RobustQA (LFRQA) that integrates multi-ple short extractive answers into a coherent long-form answer. shows an annotation wherethree extractive answers are combined by annota-tors to create a comprehensive answer. summarizes seven features in LFRQA that makeit uniquely beneficial for RAG-QA evaluations.ASQA (Stelmakh et al., 2022) and ELI5 (Fanet al., 2019) are the most similar datasets to LFRQA.However, they are either not directly annotatedagainst the underlying corpus (thus, not RAG-QA),or rely on single-domain data, which is insuffi-cient to benchmark systems cross-domain perfor-mances.With LFRQA annotations, we propose RAG-QAARENA that leverages model-based evaluators to di-rectly compare LLMs answers with LFRQA with-out the necessity to examine long and potentiallynoisy retrieved passages. By demonstrating thehigh correlation with human judges following thesame instruction and rubrics, we show that RAG-",
  "QA ARENA is an efficient and accurate frameworkto benchmark the RAG-QA systems cross-domainperformances. In this work, we primarily focus onthe LLMs used for the QA component, but RAG-": "QA ARENA can be easily extended to study re-trievals impact on answer generation quality.We summarize our contributions: 1) We presentLFRQA, the first high-quality and large-scale multi-domain human annotations with coherent long-form answers for RAG-QA. 2) We propose an ef-ficient model-based evaluation framework, RAG- QA ARENA that enables users to directly com-pare LLMs answers with ground-truth answersin LFRQA. 3) We build a dashboard incorporat-ing a wide range of leading LLMs and conductin-depth analysis to show that LFRQAs answersare preferred significantly more to the best LLMswith long context. Therefore, we believe RAG-QAARENA will serve as a challenging and robust eval-uation benchmark for future RAG-QA research.",
  "We briefly introduce the RAG-QA task in thissection.Passage retrieval is the first step of aRAG-QA pipeline. Following the passage retrievalset-up in DPR (Karpukhin et al., 2020) and RO-": "BUSTQA (Han et al., 2023), we denote a collec-tion of documents as D. We split each documentdi D with a fixed length N tokens and obtaina collection of M ( |D|) passages denoted asC = {p1, p2, ...pm, ...pM}, where pm is a passage.Given a question q, the passage retrieval task isto select K most relevant passages for q with aretriever R from C. Formally, R(q, C) Cq.Upon receiving top K passages or Cq, a QAmodel reads them as context to generate an answerfor the query. Unlike the extractive QA setting inROBUSTQA and NQ, we adopt generative QA asit is most compatible with the generative nature ofthe leading LLMs with the flexibility to producefree-form answers. The answer generation task canbe modeled as T1 Pq(wt|w0:t1; Cq), where P isan LLM. We focus on the variations of P and fix",
  ": LFRQA v.s. ROBUSTQA. Citations are removed in LFRQAs answers, and a few answer spans are removedfor clarity. Green and orange texts represent positive and negative opinions, respectively": "R in this work.In real-world applications, we deploy RAG-QAsystems into various domains such as healthcare, fi-nance, and technology whose corpus and querytypes may not be well covered in a trained re-triever and LLM. Lewis et al. (2021) and Hanet al. (2023) show significant performance gaps be-tween in-domain and out-of-domain data for RAG-QA systems, further verifying domain adaptationproblems. Therefore, it is crucial to gauge the do-main robustness of RAG-QA based on LLMs, andLFRQA helps achieve this evaluation goal.",
  "Annotated Data": "Following ROBUSTQA (Han et al., 2023), LFRQAsnew annotations are also based on the LoTTE andFiQA queries and corpus. LOTTE was proposedin the ColBERTv2 paper (Santhanam et al., 2022)and consists of information retrieval (IR) datasetsacross five domains: lifestyle, recreation, technol-ogy, writing, and science, each can have relevantanswers coming from either web search or on-lineforum. FIQA (Maia et al., 2018) proposes a task,Opinion-based QA over financial data that an-swers finance-related questions from financial cor-pora such as microblogs, reports, and news. It isimportant to note that both FiQA and LoTTE areIR datasets with answers as long documents, whichmay include a large amount of irrelevant informa-tion to the query.As IR datasets, both FiQA and LoTTE couldonly provide relevant documents to users, as there are no precise answer annotations. ROBUSTQA ad-dresses this short-coming by extracting short an-swer spans from the long documents in the similarformat of NQ, which serves as a high-quality bench-mark for extractive RAG-QA. shows anexample where the yellow highlights in Documents1-3 are the extracted answers to the question.Limitations of extractive RAG-QA. In the eraof LLMs, models responses to user queries areoften long and comprehensive (OpenAI, 2024; An-thropic, 2024; MetaAI, 2024; Jiang et al., 2024),which the short, extractive reference answers in RO- BUSTQA are no longer the most compatible formatto evaluate against. First, in ROBUSTQA, anno-tators are limited to only taking 3 answer spansper relevant document, each with no more than16 words. This process could result in a loss ofuseful information to help answer the query. Sec-ond, to reconcile multiple extractive references formodel evaluation, prior work in extractive RAG-QA (Karpukhin et al., 2020; Han et al., 2023; Izac-ard et al., 2024) adopt the maximum of token over-laps between a model prediction and a list of refer-ences to compute EM or F1 metrics, which penal-izes unfairly the long-form responses from modernLLMs. Finally, if we naively concatenate or listall short answer spans as shown by examples in, the combined answers are often too ill-formatted or incoherent as ground-truth answers.LFRQA addresses all of these drawbacks by in-structing the annotators to integrate all short an-swers in ROBUSTQA into a coherent long-formanswer. Below, we show a summary of our annota-tion instruction and quality control mechanism.Annotation instruction. As shows, a query,all relevant documents, and original short answers(highlighted in the documents) are presented to an-notators on a single annotation page. Annotatorsneed to combine all highlighted answers into a sin-gle complete and coherent answer. All highlighted",
  "DomainSourceLabel|Q||D||P|A/QW/AA/QW/A": "BiomedicalBioASQ[BI]1,95615,559,02637,406,8802.62.41.030.0FinanceFiQA[FI]3,61257,638105,7773.09.41.069.1LifestyleLoTTE[LI]2,208119,461241,7805.78.71.099.5RecreationLoTTE[RE]2,094166,975315,2033.27.21.060.3TechnologyLoTTE[TE]2,111638,5091,252,4026.08.71.099.7ScienceLoTTE[SC]1,4231,694,1643,063,9165.37.81.092.0WritingLoTTE[WR]2,695199,994347,3226.26.61.088.0 : Data (test set) summary: LFRQA v.s. ROBUSTQA. |Q|, |D|, |P|, A/Q, and W/A represent numbers ofquestions, documents, passages, answers per question, and words per answer, respectively. Each passage consistof 100 words at most. LFRQA has only one answer per query as we integrate multiple answers from ROBUSTQA,which results in more words in (long-form) answers. Dev set statistics can be found in Appendix . answers MUST be included; otherwise, the anno-tation is considered as a failure. Annotators areencouraged to include more information in the doc-uments if it helps to answer the queries. To ensureannotators faithfully use the document information,we request annotators to provide citations after eachanswer sentence. For example, the first sentence in is composed using information from Docu-ments 2 and 3. Annotators should add \"\" afterthat sentence. We use these citations primarily fordata quality control and remove them during theanswer evaluation. The actual annotation UI canbe found in Appendix .Quality control.The data annotations are per-formed by contracted data professionals. We alsohave a dedicated team of data linguists to validatethe annotation quality. Specifically, our data lin-guists randomly audit 10% of each batch of theannotations, and if the valid answer ratio is < 90%,we send the batch back to the annotators for re-work. The process iterates until the valid answerratio exceeds 90%. Here is a list of failure cases:1. Incompleteness: Answers do not include allhighlighted answers, or there is clear relevant infor-mation in the documents, but not included in theanswer. 2. Redundancy: Clear irrelevant infor-mation is included in the answer. 3. Incoherence:Answers are not coherent or not written in naturalEnglish. 4. Citation Error: Wrong/missing cita-tions, which indicate annotators do not use correctinformation from the right documents.",
  "Adapted Data": "For the biomedical domain, LFRQA leverages thesame set of test queries as in ROBUSTQA, butuses the complete rather than span answers in theBioASQ dataset. The original BioASQ annota-tions provide two types of answer formats: 1) ex-act answer, which is the short extractive answers used in ROBUSTQA; 2) ideal answer, which is along-form abstractive answer to be consistent withother datasets in this work. We did not performfurther annotations.We notice in thatBioASQs answers are shorter compared with otherdatasets. This is due to its dominant amount of fac-toid queries, which do not require elaborated expla-nations as in other datasets with more open-endedreasoning questions (Han et al., 2023).We drop SearchQA in ROBUSTQA as this datasetonly has short-form extractive answers, and its doc-uments contain a significant amount of text omis-sion (...) that prevents us from re-constructinglong-form answers.",
  "Data Statistics and Analysis": "summarizes the statistics for the test set,which consists of 16K queries across 7 domains.We filter out queries with more than 80 ground-truth documents, resulting in 73 fewer queries com-pared with ROBUSTQA. Since LFRQA combinesmultiple short answers, the answer per query ra-tio (A/Q) is always 1, and the word per answerratio (W/A) is substantially higher compared withROBUSTQA. We also annotate a dev set with 10Kqueries for future model development purposes,and the statistics can be found in Appendix .We conduct further analysis below to demonstratethe unique contributions of LFRQA in .Answers over multiple documents. il-lustrates the distribution of number of documentsused by LFRQAs answers. Specifically, ashows that around 65% of the answers use 2 doc-uments information. 4.9% of the answers consistof information from 10 or more documents (max-imum = 80). In b, we divide long-formanswers into sentences and show the distribution ofthe number of documents used per answer sentence.Nearly 22% of the answer sentences combine in- 123456789 >=10 35.3 21.5 13.6 8.8 5.5 3.9 2.7 2.1 1.6 4.9",
  "(b) # of documents used peranswer sentence": ": Distribution of number (#) of documents usedin LFRQAs answers. All numbers are %.formation from multiple documents. Both showthat LFRQAs answers effectively combine infor-mation across multiple ground-truth documents.This makes LFRQA challenging for RAG-QA, asit requires identification and aggregation of infor-mation across sources.Coherent answers. LFRQAs answers further or-ganize facts and views across multiple documentsin a coherent paragraph. Answers with multipleviews are common in the original ROBUSTQAsanswers. Conducting a string match of both yesand no as a leading word in an answer list, wefound more than 200 examples with such conflict-ing information. This does not account for moresubtle cases where answers semantically contra-dict each other. In , we show 2 exampleswith conflicting views. ROBUSTQAs annotationssimply list them (separated by new-lines), whereasLFRQAs answers organize them as coherent nar-ratives, with conflicting information reconciled inhelpful context.Fluency. ROBUSTQAs answers are extracted fromdocuments and often cut off unnaturally to satisfythe limit of 16 words. In , ROBUSTQAsanswers, such as many ways to compromise youridentity and mostly anecdotal evidence here sug-gest no are incomprehensible without further con-text, whereas LFRQAs answers are all well writtenin complete sentences.All of these features show that LFRQA providesboth challenging and high-quality annotations forevaluating RAG-QA systems.",
  "RAG-QA Arena": "Finally, we show RAG-QA ARENAs benchmarkresults. In we report each models win andwin+tie rate against LFRQA.Dashboard leaders.GPT-4O leads the dash-board, with GPT-4-TURBO and MIXTRAL-8X22B-INSTRUCT as close runners-up. GPT-4O performsthe best for [BI], [LI], [RE] and [WR] domains,MIXTRAL-8X22B-INSTRUCT leads in [FI] and[SC], and GPT-4-TURBO champions in [TE].Impact of no answer found.In RAG-QA,we rely on a passage retriever to provide context,which could be irrelevant. Our prompt (Appendix) asks an LLM to refrain from answeringif it couldnt find an answer. When we use thisanswer generation prompt with CoT (the last twolines in the prompt), GPT-4O produces 48.3% Icouldnt find an answer responses (Appendix Ta-ble 9). We randomly sample 20 such examples, andsurprisingly found that in 16 cases, GPT-4O putsan answer in its <thinking> process, but continuesto generate I couldnt find an answer. -9show four such examples in comparison with otherLLMs answers with the same prompt, and GPT-4Os new answers without CoT. As the answer gen-eration prompt with CoT only fails for GPT-4O,we remove CoT for GPT-4O, which improves itsanswer format and reduces the no-answer ratioto the level similar to other competitive models.",
  ": RAG-QA ARENA framework. Green blocksare LLMs inputs to generate answers. Orange blocksare LLM and LFRQAs answers presented to both hu-man and LLM judges to determine pairwise preferences": "has been partly justified in Sec. 3 for 1) Complete-ness: the annotation process encourages the inclu-sion of as much relevant information as possible. 2)Coherence: its answers are written more coherentlyand naturally than ROBUST-QA as references forLLM generations. Complete and coherent answerscan be considered as a comprehensive summary ofall relevant information in the entire corpus. Thisallows us to evaluate generated answers againstLFRQA answers only, which is much more infor-mative and concise than using retrieved passages,potentially with a large amount of noise.We implement human and model-based evalu-ations with the same instructions and report theircorrelations. We will show results in Sec. 6.1 thatfurther justify using LFRQA as evaluation targets.",
  "Human Evaluation": "We present a query and a pair of answers (one fromLFRQA and one from an LLM), to human anno-tators. We instruct them to rate their preferencesbased on three aspects. 1. Helpfulness:infor-mation that is helpful/relevant to answer the query.(Touvron et al., 2023; Bai et al., 2022). 2. Truth-fulness:information that is correct to answerthe query. By our definition, truthful informationshould also be helpful information (Stephanie Lin,2021; Aisha Khatun, 2024). 3. Completeness:include as much truthful and helpful informationas possible. We further instruct annotators to useTruthfulness (being both truthful and helpful) asthe primary criterion since it is stricter than Helpful-ness. Helpfulness is used when a decision cannotbe made by Truthfulness alone. More details in-cluding the definition of aspects, rating categories,and step-by-step guidelines can be found in Ap-pendix A.6 and -7 (annotation interface).",
  "Model-Based Evaluation": "Since human evaluation is too costly, we adoptmodel-based evaluators for scalable evaluation ofLLMs on the entire LFRQA test set.As for the evaluation approach, we provide LLM-based evaluators with a query and a pair of answers (including one from LFRQA). Similar to humanevaluation, we prompt LLMs to rate their prefer-ences based on the same three aspects above. Weonly modify the human instruction slightly to becompatible with LLM readable input text, but themajority of the prompt, especially the input data,and rubric, stay the same (Appendix -15).For both human and model-based evaluations,we allow tie\" (no preference) as an option. For hu-man evaluation, we take the majority votes from 3annotators to mitigate biases. If there is no majorityvote, we default the label to tie.",
  "Experimental Setup": "In this section, we discuss our retriever, LLMsexperimented for both answer generation and pair-wise evaluations and their prompts in more detail.Retrieval setting. We employ COLBERTV2 (San-thanam et al., 2022) as our passage retriever, con-sidering its superior performance on the underlyingcorpus for both ROBUSTQA and LFRQA as shownin Han et al. (2023). We follow the same retrievalsetting and split passages into text chunks with100 consecutive words. We use the top 5 retrievedpassages for our main results in and experi-ment with the top 10 passages for further analysis.Answer generation.We consider LLMsranked top 252 in the Chatbot Arena (Chianget al., 2024) and their smaller version to showthe impact of model sizes.Due to resourceand legal constraints, for proprietary LLMs, weonly use OpenAI models:a) GPT-4-TURBO(2024-04-09), b) GPT-4O and c) GPT-4-0125- PREVIEW). For public models, we experiment with1) MIXTRAL-8X22B-INSTRUCT and MIXTRAL-8X7B-INSTRUCT (Jiang et al., 2024); 2) LLAMA-3-70B-INSTRUCT and LLAMA-3-8B-INSTRUCT(MetaAI, 2024); 3) COMMAND R+ and COM-",
  "Ranking at the time of paper writing": "and in-context-learning examples. We shuffle theorder of the answer pairs so that both human andmodel judges are not biased by the position ofan answer. We select the LLM with the highestcorrelation with human judgments as the evaluator(Appendix ).We use OpenAI API torun GPT-4 models. We download public modelsfrom HuggingFace Hub (HuggingFace, 2024) andrun them on up to 8 Nvidia A100 GPUs withPyTorch (1.13.0) and Transformers (4.41.0) whosetokenizer.apply_chat_template()functioncan help adapt the generic prompts to differentLLMs input formats.We follow OpenAIs recommendation3 to designprompts with chain-of-thoughts (CoT) (Wei et al.,2022), in-context learning (Dong et al., 2023), andHTML tags as delimiters. We remove the thinkingprocess in model outputs as final answers.",
  "LFRQA v.s. RobustQA": "In Sec. 3.3, we demonstrate LFRQAs advantagesvia data statistics. Here, we show a more rigorousstudy to highlight the benefits. We subsample 700queries (100 from each of the 7 domains) and con-duct pairwise preference comparisons using bothhuman and model-based evaluations. We comparethree types of answers: 1) ROBUSTQA: concatena-tion of its extractive answers, separated by \"\\n\"; 2)LFRQA: long-form answers in this work; 3) GPT-4s answers based on the top 5 retrieved passages. shows that when compared directlyin Row (1),LFRQA dominates ROBUSTQA.When comparing with GPT-4 in Row (2)-(3),LFRQA significantly out-performs GPT-4, but RO-",
  "QWEN1.5-110B-CHAT #433.437.836.244.042.646.922.325.134.140.734.837.540.846.122.525.2QWEN1.5-32B-CHAT #532.837.134.942.843.247.320.723.732.338.334.037.140.844.822.625.2": ": Evaluation results on LFRQA test set. W and W+T indicate win and win+tie rate against LFRQAs answers.LLMs answers are generated based on the top 5 passages. bold and underline indicate the best and runner-upresults. * means using the answer generation prompt w/o CoT. #n indicates the Elo ranking in Appendix .",
  "Tie11.04.0": ": Pairwise comparisons between ROBUSTQA,LFRQA, and LLMs. GPT-4: GPT-4-0125-PREVIEW;MIXTRAL: MIXTRAL-8X22B-INSTRUCT; LLAMA-3:LLAMA-3-70B-INSTRUCT. Answer generated basedon top 5 passages using ColBERT-v2. MBE stands forthe model-based evaluator. All numbers are % exceptfor Pearson Corr. and Cohens Kappa. *** indicatesstrong correlation with p-values 0.001. various LLMs. We rely on model-based evaluationto achieve this goal. Before showing the final dash-board results, we check the quality of our selectedevaluator (GPT-4-0125-PREVIEW) in .To alleviate model bias, we use three LLMsanswers as benchmark data, and the query set is thesame 700 subsample above. Row (3)-(5) use GPT-4-0125-PREVIEW, MIXTRAL-8X22B-INSTRUCTand LLAMA-3-70B-INSTRUCT, respectively. Allanswers are generated based on the top 5 passages. We observe that LLM evaluators numbers alignwell with the average human scores, except thatLLMs tend to predict less tie. Most importantly,all Pearson Correlation (Freedman et al., 2007) areabove 0.52 (with p-values 0.01), and all Cohens Kappa (Cohen, 1960) are above 0.43, both showingstrong agreement between model and human judg-ments. In Appendix , we show correlationnumbers using alternative LLMs as evaluators, butnone of them works better than a single GPT-4-0125-PREVIEW model, which we select as our bestquality evaluator for RAG-QA ARENA.",
  "RAG-QA ARENA RankingRating95% CIVotesRating95% CIVotesRating95% CIVotes": "LFRQA1144+1/-1176.7K1145+1/-1176.7K1146+1/-1176.7KGPT-4O1066+5/-516.1K1081+4/-423.1K1085+3/-330.1KGPT-4-TURBO1050+5/-416.1K1058+4/-323.1K1065+3/-230.1KMIXTRAL-8X22B1049+4/-416.1K1059+3/-323.1K1063+3/-330.1KQWEN1.5-110B-CHAT1041+4/-416.1K1047+4/-323.1K1052+3/-330.1KQWEN1.5-32B-CHAT1036+6/-416.1K1034+4/-323.1K1037+3/-330.1KGPT-4-0125-PREVIEW1008+6/-516.1K1005+4/-423.1K1008+3/-330.1KMIXTRAL-8X7B991+4/-416.1K991+3/-423.1K987+3/-330.1KLLAMA-3-70B939+4/-516.1K931+4/-423.1K930+2/-330.1KCOMMAND R+938+5/-516.1K931+3/-423.1K924+3/-330.1KLLAMA-3-8B924+6/-616.1K910+4/-423.1K903+4/-330.1KCOMMAND R816+8/-616.1K802+5/-523.1K796+4/-430.1K : Elo rating including couldnt find answer responses. LFRQA only indicates the pairs that alwaysinclude an LFRQA answer. LFRQA + N complete pairs means we subsample N additional pairs evenly across 7domains and conduct comparison for all pairs of models. We have 11 models, so the total new pairs are N1110",
  "and as we increase the pairs, the 95% CI can finallyseparate different models.In general, the total added preference pairs areNK(K1)": "2, where K is the number of models in, and N=700 and 1400 for Column (B) and(C), respectively. With 43.6% increase of total pairs(and thus the compute), the final ranking is identicalwith RAG-QA ARENA based on win ratio, and onlydiffers only slightly with the win+tie ranking in by flipping the order of LLAMA-3-70B andCOMMAND R+. These results present additionalevidence that our approach of using LFRQA onlyfor pairwise comparisons is reliable. Furthermore,it reduces the computational costs from O(K2) toO(K) as we now only need to compare each LLMresponse once with the ground-truth in LFRQA.Impact of the number of passages. In Appendix, we compare the top 3 LLMs by increas-ing the number of retrieved passages from 5 to10. Doubling the number of passages (with ex-tra costs) increases RAG-QA performances signif-icantly. We also find that both GPT-4 modelsimprovements are greater than MIXTRAL-8X22B-INSTRUCT, showing their superior capability tounderstand long context and identify useful infor-mation from noise. The best win rate of GPT-4Oagainst LFRQA is 41.3%, which is 13.7% pointslower than LFRQA answers win rate against GPT-4O. This result shows that LFRQAs answer qualityis difficult to surpass, further justifying using it asan evaluation target.Impact of model sizes. For the non-GPT LLMfamily, more parameters lead to better perfor-mances, but a larger increase in model sizes doesnot always indicate greater performance gains in",
  "LLM as Annotators": "Using large language models to provide annota-tions has been explored in previous works (Tanet al., 2024). It could provide a more scalable so-lution than human annotations but can suffer fromhallucination and accuracy issues that require hu-man validations (Huang et al., 2023). We also ex-perimented with LLM as annotators before we starthuman annotations. We subsample 100 queriesfrom LFRQA and prompt GPT-4-0125-PREVIEWto follow the similar procedure in Sec. 3.1 to com-bine answers (Appendix ). Then we re-quest our data linguists to compare LFRQA andGPT-4 annotations based on 1) Completeness:whether all ROBUSTQA answers are integrated intothe final answers; 2) Citation Accuracy: whether ci-tations in answers pointing to the right documents;3) Helpfulness: defined the same as in Sec. 4. Ta-ble 6 shows LFRQA out-performs GPT-4 annota-tions by 15.5%, 23.4% and 12.9% for the threedimensions, respectively, suggesting human anno-tations are both valuable and necessary for our task.",
  "Alternative Evaluation Approaches": "Using retrieved passages. RAG-QA ARENA lever-ages only LFRQAs annotations as ground-truth todirectly evaluate LLM responses, and we explainthis design choice in Sec. 4 that LFRQA consists ofcomplete and coherent answers that can be viewedas high-quality summary of all available answersin the entire corpus. This enables us to not show re-trieved passages as they 1) increase the input lengthand thus the latency of an evaluator; and 2) theycould contain incorrect information due to retrievalerror, which mislead evaluators.Using LFRQA as references.We can also useLFRQAs annotations as references when construct- ing the prompt for pairwise evaluation. That is, wecan potentially compare a pair of LLMs responsesby comparing them both against the references ina single trial. However, this approach would stillrequire the similar O(K2) pairs as in the Elo rating,which is not as efficient as our proposed RAG-QAARENA framework.For these reasons, we do not adopt the abovetwo evaluation approaches. It is conceivable thatprompt engineering, in-context example selectionsand even task specific evaluator training could fur-ther enhance alignments with human judges. Weleave them for future research efforts.",
  "Related Work": "RAG-QA has been widely studied. Prior datasetsare limited in the evaluation as their corpus re-lies heavily on Wikipedia and the answers aremostly short and extractive (Rajpurkar et al., 2016;Kwiatkowski et al., 2019; Amouyal et al., 2019).ROBUSTQA and MULTIHOP-RAG (Tang andYang, 2024) address the single domain issue, butstill adopt short, extractive answers, which is not assuitable as LFRQA to evaluate modern LLMs thatgenerate long-form answers.Longform QA datasets have been proposed inprior work. ELI5 (Fan et al., 2019) and LONG-FACT (Wei et al., 2024) contain answers that areeither not annotated directly on the corpus, andor not created by humans. Krishna et al. (2021)also points out that ELI5s small validation set hassignificant leakage from its train set. ASQA (Stel-makh et al., 2022) is the most similar data to ourwork, but its corpus is in the single Wikipedia do-main. LFRQA is by far the RAG-QA dataset withthe most comprehensive long-form answers.Pairwise preference is now a standard way to eval-uate LLMs. It allows direct comparison betweentwo responses (Chiang et al., 2024; Lin et al., 2024).RAG-QA ARENA is unique by always includinga high-quality human annotated LFRQA answer,thereby making the evaluation more trustworthy.",
  "Limitations": "We discuss some limitations of this work for futureresearch efforts. RAG-QA ARENA can potentiallycover more models. We didnt include some lead-ing LLMs, such as Claude (Anthropic, 2024) andGemini (Google, 2023) models, due to legal andresource constraints, but we plan to add them to theleaderboard in the future. Evaluation using GPT-4-0125-PREVIEW is not cheap. It costs on average300 U.S. dollars per model on the full LFRQAs testset. We plan to subsample 10-20% of the queriesfor the final public leaderboard, which will be morecost-friendly for future users. Future research canalso study training smaller but equally accuratemodels as evaluators. Finally, we mainly focuson different LLMs for RAG-QA in this work, butfuture research can study the impact of different re-trievers or joint retrievers and LLM training usingRAG-QA ARENA.",
  "Ethics Statement": "The authors of this paper are committed to conduct-ing research ethically. We are leveraging existingLLMs to generate answers for LFRQA, which in-clude many open-ended questions. LLM-generatedanswers could be incorrect or unfaithful, as retriev-ers could find irrelevant passages and LLM canhallucinate (Huang et al., 2023). These are knownissues in the AI research community, and that isthe reason we created LFRQA to better evaluateRAG-QA systems. The additional risks and po-tential harms are discussed in numerous previousworks (Bender et al., 2021; Weidinger et al., 2021).The authors strive to ensure that the research andits results do not cause harm.Data used in this work have been collected frompublic sources and used in accordance with all ap-plicable laws and regulations. We use contracteddata professionals for LFRQA annotations, and Ap-pen platform4 for human pairwise preference an-notations. In both cases, we ensure our hourly rateis higher than 15 U.S. dollars per local minimumwage standard. The intended usage of LFRQA iscompatible with the underlying datas access con-ditions (Appendix A.2)",
  "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, and et al. 2023. Qwen technicalreport. arXiv:2309.16609": "Yuntao Bai, Andy Jones, Kamal Ndousse, AmandaAskell, Anna Chen, Nova Dassarma, Dawn Drain,Stanislav Fort, Deep Ganguli, Tom Henighan,Nicholas Joseph, Saurav Kadavath, John Kernion,Tom Conerly, Sheer El-Showk, Nelson Elhage, ZacHatfield-Dodds, Danny Hernandez, Tristan Hume,Scott Johnston, Shauna Kravec, Liane Lovitt, NeelNanda, Catherine Olsson, Dario Amodei, Tom B.Brown, Jack Clark, Sam McCandlish, ChristopherOlah, Benjamin Mann, and Jared Kaplan. 2022.Training a helpful and harmless assistant with re-inforcement learning from human feedback. ArXiv,abs/2204.05862. Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On thedangers of stochastic parrots: Can language modelsbe too big? In Proceedings of the 2021 ACM Confer-ence on Fairness, Accountability, and Transparency. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020.Language models are few-shot learners.In Ad-vances in Neural Information Processing Systems,volume 33, pages 18771901. Curran Associates,Inc. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anasta-sios Nikolas Angelopoulos, Tianle Li, Dacheng Li,Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E.Gonzalez, and Ion Stoica. 2024. Chatbot arena: Anopen platform for evaluating llms by human prefer-ence. ArXiv, abs/2403.04132.",
  "Gemini Team Google. 2023.Gemini:A fam-ilyofhighlycapablemultimodalmodels.arXiv:2312.11805": "Rujun Han, Peng Qi, Yuhao Zhang, Lan Liu, JulietteBurger, William Yang Wang, Zhiheng Huang, BingXiang, and Dan Roth. 2023. RobustQA: Benchmark-ing the robustness of domain adaptation for open-domain question answering. In Findings of the As-sociation for Computational Linguistics: ACL 2023,pages 42944311, Toronto, Canada. Association forComputational Linguistics. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,Zhangyin Feng, Haotian Wang, Qianglong Chen,Weihua Peng, Xiaocheng Feng, Bing Qin, and TingLiu. 2023. A survey on hallucination in large lan-guage models: Principles, taxonomy, challenges, andopen questions. ArXiv, abs/2311.05232. Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, andYejin Choi. 2019. Cosmos QA: Machine readingcomprehension with contextual commonsense rea-soning. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processingand the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), pages23912401, Hong Kong, China. Association for Com-putational Linguistics.",
  "Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux,Arthur Mensch, and et al. 2024. Mixtral of experts.arXiv:2401.04088": "Mandar Joshi, Eunsol Choi, Daniel Weld, and LukeZettlemoyer. 2017. TriviaQA: A large scale distantlysupervised challenge dataset for reading comprehen-sion. In Proceedings of the 55th Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 16011611, Vancouver,Canada. Association for Computational Linguistics. Vladimir Karpukhin, Barlas Oguz, Sewon Min, PatrickLewis, Ledell Wu, Sergey Edunov, Danqi Chen, andWen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the2020 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 67696781,Online. Association for Computational Linguistics. Tom Kocisk, Jonathan Schwarz, Phil Blunsom, ChrisDyer, Karl Moritz Hermann, Gbor Melis, and Ed-ward Grefenstette. 2018. The NarrativeQA readingcomprehension challenge. Transactions of the Asso-ciation for Computational Linguistics, 6:317328.",
  "Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021": "Hurdles to progress in long-form question answering.In Proceedings of the 2021 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 49404957, Online. Association for Computa-tional Linguistics. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-field, Michael Collins, Ankur Parikh, Chris Alberti,Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-ton Lee, Kristina Toutanova, Llion Jones, MatthewKelcey, Ming-Wei Chang, Andrew M. Dai, JakobUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-ral questions: A benchmark for question answeringresearch. Transactions of the Association for Compu-tational Linguistics, 7:452466. Patrick Lewis, Pontus Stenetorp, and Sebastian Riedel.2021. Question and answer test-train overlap in open-domain question answering datasets. In Proceedingsof the 16th Conference of the European Chapter ofthe Association for Computational Linguistics: MainVolume, pages 10001008, Online. Association forComputational Linguistics. Bill Yuchen Lin, Khyathi Chandu, Faeze Brahman, Yun-tian Deng, Abhilasha Ravichander, Valentina Pyatkin,Ronan Le Bras, and Yejin Choi. 2024. Wildbench:Benchmarking language models with challengingtasks from real users in the wild. Macedo Maia, Siegfried Handschuh, Andr Freitas,Brian Davis, Ross McDermott, Manel Zarrouk, andAlexandra Balahur. 2018. WWW18 open challenge:Financial opinion mining and question answering.page 19411942, Republic and Canton of Geneva,CHE. International World Wide Web ConferencesSteering Committee.",
  "OpenAI.2024.GPT-4technicalreport.arXiv:2303.08774": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, JohnSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,Maddie Simens, Amanda Askell, Peter Welinder,Paul F Christiano, Jan Leike, and Ryan Lowe. 2022.Training language models to follow instructions withhuman feedback. In Advances in Neural InformationProcessing Systems, volume 35, pages 2773027744.Curran Associates, Inc. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, andPercy Liang. 2016. SQuAD: 100,000+ questions formachine comprehension of text. In Proceedings ofthe 2016 Conference on Empirical Methods in Natu-ral Language Processing, pages 23832392, Austin,Texas. Association for Computational Linguistics. Keshav Santhanam, Omar Khattab, Jon Saad-Falcon,Christopher Potts, and Matei Zaharia. 2022. Col-BERTv2:Effective and efficient retrieval vialightweight late interaction. In Proceedings of the2022 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 37153734, Seat-tle, United States. Association for ComputationalLinguistics. Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. 2022. ASQA: Factoid questions meetlong-form answers. In Proceedings of the 2022 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 82738288, Abu Dhabi, UnitedArab Emirates. Association for Computational Lin-guistics.",
  "Yixuan Tang and Yi Yang. 2024. Multihop-rag: Bench-marking retrieval-augmented generation for multi-hop queries. ArXiv,": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, and et al. 2023. Llama 2: Open foundationand fine-tuned chat models. arXiv:2307.09288. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,and Denny Zhou. 2022. Chain-of-thought prompt-ing elicits reasoning in large language models. InAdvances in Neural Information Processing Systems,volume 35, pages 2482424837. Curran Associates,Inc. Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu,Nathan Hu, Jie Huang, Dustin Tran, Daiyi Peng,Ruibo Liu, Da Huang, Cosmo Du, and Quoc V. Le.2024. Long-form factuality in large language models.ArXiv, arXiv:2403.18802. Laura Weidinger, John Mellor, Maribeth Rauh, ConorGriffin, Jonathan Uesato, Po-Sen Huang, MyraCheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,et al. 2021. Ethical and social risks of harm fromlanguage models. arXiv preprint arXiv:2112.04359.",
  "A.6Human Evaluation Instructions": "Helpfulness: Information that is helpful/relevantto answer the query. An ideal answer consists ofonly information that is helpful/relevant to answerthe query (Touvron et al., 2023; Bai et al., 2022).Truthfulness: Information that is correct to an-swer the query. By our definition, truthful infor-mation should also be helpful information. If itis difficult to determine the truthfulness of someinformation, we consider it untruthful. Sometimes,this is due to not enough context provided in theanswer. Another source of untruthfulness is whenconflicting information is presented, and the answerdoes not coherently reconcile them (Stephanie Lin,2021; Aisha Khatun, 2024).Completeness: include as many helpful and truth-ful information.Here are the details of our instructions.1. If one answer has all truthful information whilethe other has some untruthful information, preferthe all-truthful one. 2. If both have some untruth-ful information, prefer the one with less untruthfulinformation. 3. If both have all truthful informa-tion, prefer the one with more truthful or helpfulinformation. 4. If two answers look equally good,or it is too hard to differentiate, choose Not sure.As the annotation UI shows, the actual ratingsare Better, Slightly Better, Tie, SlightlyWorse and Worse.We merge Better andSlightly Better, and Slightly Worse and Worsewhen computing correlation with model-based eval-uators.",
  "First, think step by step, and put your thinking in <thinking> tags. Your thinking must be shorter than 50 words.Then, provide your answer": ": Prompt for answer generation. {x.*} indicates a component input that can be replaced by actual data. Wemodify this prompt slightly to be compatible with different LLMs input formats, but the majority of the prompt,particularly instructions, remain the same. * The last section starting with First, think step by step... is what werefer to as CoT prompt. We remove it for GPT-4O only.",
  "Annotation Generation Prompt": "Provide a response around 100 words to the query in the <query></query> tags based on the passages. Passages areinside <passage></passage> tags. The response must incorporate all candidate answers in the <ans></ans>, andyou are allowed to rephrase these answers in order to make your final response natural. The response should notinclude any information outside passages.You should cite the passage number (indices) in the format of , , , etc. at the end of each sentence.",
  "First, try your best to determine whether the information in an answer can help truthfully answer the query. Then rateyour preference based on Helpfulness and Truthfulness": "- Helpfulness: information that is helpful/relevant to answer the query. An ideal answer consists of only informationthat is helpful/relevant to answer the query.- Truthfulness: information that you believe is correct to answer the query. By our definition, truthful informationshould be helpful information. If you find it difficult to determine the truthfulness of some information,consider it untruthful. Often time, this is due to not enough context provided in the answer. Anothersource of untruthfulness is when conflicting information presented, and the answer does not reconcilethem in a coherent way. <rubric>Here is how you judge (in the order of importance),- If one answer has all truthful information while the other has some untruthful information, prefer the all truthful one.- If both have some untruthful information, prefer the one with less untruthful information.- If both have all truthful information, prefer the one with more truthful or helpful information.- If two answers look equally good, or it is too hard to judge using the 3 cases above, then you are our \"not sure\" which oneis better.</rubric>",
  "Answer 1": "The question of whether UTF-16 should be considered harmful is subject to differing opinions. Oneperspective suggests that UTF-16 is harmful due to a historical misconception about character encoding andrecommends UTF-8 as the superior choice for various text interfaces. Another viewpoint argues thatUTF-16 is not inherently harmful, emphasizing its utility in specific scenarios where it serves as acompromise between simplicity and compactness. The choice between UTF-16 and other encodings likeUTF-8 depends on the specific requirements of the application, such as compatibility with ASCII orthe need to efficiently encode certain character sets.",
  "Answer 2": "UTF-16 should not be considered harmful.However, contrasting views argue that UTF-16 should indeed be considered harmful.Some argue that the very reason UTF-16 exists is because some time ago there used to be a misguided beliefthat WideChar is going to be what UCS-4 now is.Additionally, the harmfulness of UTF-16 is tied to issues with exercising code.",
  ": In-context examples for pairwise evaluation. Labels 1, 2, and 3 mean \"answer 1 is better\", \"answer 2 isbetter\" and \"tie\", respectively. \"Reason\" is a models chain-of-thought output": ": GPT-4Os original I couldnt find an answer response compared with GPT-4-TURBO and MIXTRAL-8X22B, and its answer without CoT (*). Blue highlights in the passages indicate helpful information to answer thequery. <answer> tags are added to help differentiate from <thinking>. : GPT-4Os original I couldnt find an answer response compared with GPT-4-TURBO and MIXTRAL-8X22B, and its answer without CoT (*). Blue highlights in the passages indicate helpful information to answer thequery. <answer> tags are added to help differentiate from <thinking>."
}