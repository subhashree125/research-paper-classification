{
  "Abstract": "Large language model agents have exhibitedexceptional performance across a range of com-plex interactive tasks. Recent approaches haveutilized tuning with expert trajectories to en-hance agent performance, yet they primarilyconcentrate on outcome rewards, which maylead to errors or suboptimal actions due tothe absence of process supervision signals. Inthis paper, we introduce the Iterative step-levelProcess Refinement (IPR) framework, whichprovides detailed step-by-step guidance to en-hance agent training. Specifically, we adoptthe Monte Carlo method to estimate step-levelrewards. During each iteration, the agent ex-plores along the expert trajectory and generatesnew actions. These actions are then evaluatedagainst the corresponding step of expert trajec-tory using step-level rewards. Such compari-son helps identify discrepancies, yielding con-trastive action pairs that serve as training datafor the agent. Our experiments on three com-plex agent tasks demonstrate that our frame-work outperforms a variety of strong baselines.Moreover, our analytical findings highlight theeffectiveness of IPR in augmenting action effi-ciency and its applicability to diverse models.",
  "Introduction": "The advancements in large language models(LLMs), such as GPT-3.5 (Ouyang et al., 2022),GPT-4 (Achiam et al., 2023), LLaMA (Touvronet al., 2023) have paved ways for LLM-basedagents to excel in handling complex interactivetasks, including online shopping (Yao et al., 2022a)and embodied housework (Shridhar et al., 2020).To accomplish these tasks, LLM agents explorethe environment step by step, achieving sub-goalsalong action trajectories (Ma et al., 2024). Theefficacy of this task-solving process is pivotal toagents overall performance.",
  "*Corresponding Authors.Code & Data:": ": Comparison of three different agent trainingparadigms. Green and red circles represent correct andincorrect actions, while check and cross marks indicatethe final outcome. Compared to the other methods, IPRcan provide step-level process supervision. Initial efforts in the task-solving process foragents involve generating trajectories by directlyleveraging the planning ability of LLMs, such asReAct (Yao et al., 2022b) and Reflexion (Shinnet al., 2024).To further enhance LLM agentabilities, several studies focus on trajectory tun-ing (Chen et al., 2023; Yin et al., 2023; Zeng et al.,2023). Chen et al. (2023) and Yin et al. (2023)construct agent trajectory data from teacher agents(e.g., GPT-4) and fine-tune open-source LLMs forspecific agent abilities, such as reasoning. Con-versely, Zeng et al. (2023) employ a multi-tasksupervised fine-tuning (SFT) approach, which doesnot significantly improve generalized agent capabil-ities. Observing that the SFT-based works predom-inantly rely on expert success trajectories ((a)), Song et al. (2024) utilize failure trajectoriesand propose the exploration-based trajectory opti- mization (ETO) method to learn the task-solvingprocess ((b)). Although these methodspresent a promising avenue for enhancing agent ca-pabilities, they treat an entire trajectory as a singleentity during training and prioritize the final rewardof a trajectory over the process, thus overlookingthe potentially exploitable information throughoutinteraction process.Regarding agent trajectories, it is well-knownthat alongside those with correct outcomes, thereare trial-and-error paths with detours and erroneousones that achieve accidental success. Step-levelprocess supervision can offer granular guidanceat each step hence is beneficial for task resolution(Lightman et al., 2023). Nevertheless, the appli-cation of step-level optimization to LLM agentsencounters two practical challenges. Firstly, themajority of existing LLM agent environments (Yaoet al., 2022a; Shridhar et al., 2020; Yang et al.,2024) provide only final outcome feedback. Evenin cases where environments offer sub-goal levelfeedback (Ma et al., 2024), the information is of-ten too sparse. Secondly, the question of how toeffectively utilize step rewards to enhance agenttraining, particularly for tasks with long trajectoriesand complex action spaces, remains unexplored.In this paper, we address these challengesby introducing the Iterative step-level ProcessRefinement (IPR) framework ( 3) , which en-compasses two principal mechanisms: Step-levelReward Acquisition ( 3.2) and Iterative Agent Op-timization ( 3.3). More specifically, to constructthe step reward within the agent environment, weemploy Monte Carlo (MC) method to estimate re-wards via sampling. The Iterative Agent Optimiza-tion component aims to refine the agents actionsthrough a cyclical process. During each cycle, theagent navigates the expert trajectory and generatenew actions. These actions are then compared withthe corresponding step of the expert trajectory us-ing step-level rewards to pinpoint errors, resultingin contrastive step pairs. Subsequently, we train theagent using an arrangement of outcome-level directpreference optimization (DPO), step-level DPO,and SFT losses, thereby enhancing the agents ac-tion capabilities at each step ((c)).We assess our IPR framework on three represen-tative benchmarks: online shopping environmentWebShop (Yao et al., 2022a), interactive SQL envi-ronment InterCodeSQL (Yang et al., 2024) and tex-tual embodied environment ALFWorld (Shridharet al., 2020). The experimental results, detailed in 4.2, reveal that our method surpasses the currentleading method by margins of 5.8%, 7.2% and 3.2%on WebShop, InterCodeSQL, and ALFWorld, re-spectively. Moreover, we present a comprehensiveanalysis to substantiate the efficacy of our methodfrom various perspectives.",
  "The primary scope of this study is the task-solvingof LLM agents interacting with the environmentand receiving feedback.Following Song et al": "(2024), we formulate the task as a partially observ-able Markov decision process (POMDP) definedby the elements (U, S, A, O, T , R). Here, U de-notes the instruction space, S the state space, Athe action space, O the observation space, T thetransition function (T : S A S), and R thereward function (R : S A ). In the con-text of our LLM-based agent, U, A, O are subsetsof natural language space. At time step t, the LLM agent receives the ob-servation ot1 O from the environment and takesan action at A following the policy (|et1),where et1 = (u, a1, o1, ..., at1, ot1) representsthe historical trajectory.The action leads to achange in the state space st S, and receivesexecution feedback as observation ot O. The in-teraction loop continues until the task is completedor the maximum steps are reached. The final tra-jectory is en = (u, a1, o1, ..., an, on), where n de-notes the trajectory length, and the outcome rewardis ro(u, en) . For the convenience of subse-quent content, we define et:n = (at, ot, ..., an, on)to represent the trajectory after time step t. : The overall architecture of IPR in a single iteration. The agent trained after SFT first explores new actionsalong the expert trajectory. Then we use the scorer to reward each step and construct contrastive action data. Finallywe optimize the agent with a mixed loss.",
  "Method": "The overall architecture of our method is depictedin . Initially, we empower the languagemodel with fundamental agent capabilities via su-pervised learning ( 3.1). Subsequently, we de-velop the MC method to estimate the step-wiserewards within the agents environment ( 3.2). Inthe final stage, we enhance the agents performancethrough iterative optimization ( 3.3): by construct-ing contrastive action pairs and executing mixturetrajectory optimization.",
  "Step-level Reward Acquisition": "Step-level process reward provide precise feedbackby pinpointing the exact location of potential er-rors, offering a valuable signal for agent learning.However, most agent environments are limited tooutputting only final outcome reward. Prior stud-ies (Uesato et al., 2022; Lightman et al., 2023) relyon human annotators for step supervision annota-tions, rendering the acquisition of step rewards alabor-intensive process. To circumvent this, weadopt an exploration-based method to estimate thereward for action at at step t.It is intuitive that a more accurate action wouldcontribute to a higher reward. Therefore, we de-fine the step reward rs(st, at) as the anticipatedoutcome reward from subsequent exploration start-ing at step t, with st being the current state of theenvironment. A dedicated scorer s with fixed pa-rameters is employed to generate new subsequenttrajectory et:m from step t, based on the histori-cal trajectory et1. The probability of generatinget:m is given by s(et:m|et1), and the environ-ment assigns an outcome reward ro(u, em) for thetrajectory. The step reward can be calculated as:",
  "Iterative Agent Optimization": "Agent tasks typically involve long action sequencesand large decision spaces. Suppose we have a baseagent trained through SFT. Given an instructionu, the agent interacts with the environment to pro-duce a trajectory e = (u, a1, o1, ..., an, on). If theagent makes an error action at at step t, a straight-forward approach would be to use reinforcementlearning methods like proximal policy optimization(PPO, Schulman et al., 2017) to optimize the actionat step t. However, applying online reinforcementlearning directly to the LLM agent may cause prac-tical issues such as instability (Shen et al., 2023;Rafailov et al., 2024). To address this issue, weperform offline learning on the contrastive actionpairs data instead, which ensures stability. Step-wise Trajectory ConstructionTo gener-ate contrastive action pairs data, we allow the baseagent to explore on the expert trajectory. Thisapproach has two benefits: Firstly, upon identify-ing an incorrect action by the agent, we can easilyacquire a correct action for contrastive learning pur-poses. Secondly, it prevents arbitrary explorationby the agent, thereby yielding a more informativetrajectory. For the task instruction u with experttrajectory en = (u, a1, ..., on1, an), we use thefirst t 1 steps (u, a1, ..., at1, ot1) as historicaltrajectory et1. The agent then predict the actionsfrom step t to get the trajectory:",
  "The rewards for at and at are rs(st, at) andrs(st, at), respectively. We use a threshold tofilter actions. If the reward of at is lower than that": "of at by a margin greater than , and the outcomereward of em is lower than that of en, we considerthe agent to have made a mistake at step t. Wethen contrast the subsequent trajectory from thatstep ewt:n elt:m | et1. Here, ew and el repre-sent win/lose trajectories with higher and lower re-wards. We perform exploration across the entire ex-pert trajectory set and obtain the contrastive action",
  "ref(elt:m|et1)),": "(8)As demonstrated by Yuan et al. (2024), DPO onlyoptimizes the relative differences between chosenand rejected data, neglecting the absolute magni-tudes of the rewards. This oversight can be prob-lematic in agent tasks where the space of correctactions is significantly narrower than that of incor-rect ones. To mitigate this issue, we add the SFTloss, aiming to directly increase the likelihood ofthe success trajectory:",
  "Experiment Settings": "DatasetsWe evaluate our method on three rep-resentative agent datasets: WebShop (Yao et al.,2022a) for web navigation, InterCodeSQL (Yanget al., 2024) for SQL database querying, and ALF-World for embodied agent tasks. Both WebShopand InterCodeSQL provide a dense reward scalefrom 0 to 1 to gauge task completion, while ALF-World only provides a binary reward to indicatewhether the task is completed. We employ the av-erage reward as the evaluation metric for all tasks.To collect training expert trajectories, we promptGPT-4 to interact with the environment in ReActpattern. We then filter the results based on thefinal outcome rewards to retain only the correcttrajectories. Please refer to Appendix D for moredetails. The statistical information of the datasetis summarized in , and more details canbe found in Appendix A. Note the ALFWorld testset is divided into 140 seen cases and 134 unseencases, evaluating the agents in-domain and out-of-domain proficiencies, respectively. Implementation DetailsWe utilize Llama-2-7B (Touvron et al., 2023) as the base modelto train LLM agents.The training epoch is 3and with a batch size of 48. The AdamW opti-mizer (Loshchilov and Hutter, 2017) is employed,coupled with a cosine learning scheduler. For step-level rewards acquisition via the scorer, we set thetemperature to 1 and the number of samples N to 5,promoting diversity in sampling. In the generationof contrastive action pairs, the base agents temper-ature is fixed at 0, while the filtering threshold isadjusted to 0.5 for ALFWorld, 0.01 for WebShopand 0.1 for InterCodeSQL. All the generations arecarried using vllm (Kwon et al., 2023). During the mixture trajectory optimization phase, we searchfor the learning rate from 1e-5 to 5e-5, and forthe DPO loss from 0.1 to 0.5. The iteration cap isset to 4. All experiments are conducted on a suiteof 8 NVIDIA A100 80G GPUs. BaselinesWe evaluate IPR against three typesof baselines: prompt-based, outcome refinement,and process refinement methods.For prompt-based methods, we compare the efficacy of GPT-4 (Achiam et al., 2023), GPT-3.5-turbo (Ouyanget al., 2022), and the untrained Llama-2-7B-Chat (Touvron et al., 2023) utilizing ReAct prompt-ing paradigm.These baselines are tested in aone-shot context. Regarding outcome refinementmethods, four tuning strategies are juxtaposed: (1)SFT (Chen et al., 2023) tunes the agent usingsolely expert trajectories, which is the base agentof other baselines; (2) PPO (Schulman et al., 2017)is a reinforcement learning (RL) technique thatdirectly optimizes the agents to maximize the out-come reward; (3) RFT (Rejection sampling Fine-Tuning) (Yuan et al., 2023) augments the experttrajectory dataset with successful trajectories, sub-sequently training the agent on the enriched dataset;and (4) ETO (Song et al., 2024) contrasts successand failure trajectories via DPO (Rafailov et al.,2024). For process refinement methods, we com-pare the Step-PPO method, which optimizes theagents to maximize the step-level process reward.",
  "Results": "illustrates that, in comparison to outcomerefinement and process refinement methods, bothopen-source and proprietary models under prompt-based methods perform significantly worse. Thisdiscrepancy is particularly evident with the un-trained Llama-2-7B, which struggles to completethe InterCodeSQL and ALFWorld tasks. However,after training with our IPR method, there is a re-markable increase in the average reward from 5.5to 69.4, surpassing the best performance of GPT-4. Regarding outcome refinement baselines, ourmethod outperforms the previous state-of-the-art(SOTA) method ETO by margins of 5.8%, 7.2%,2.5% and 3.2% on WebShop, InterCodeSQL, ALF-World (seen), and AFLWorld (unseen) respectively,with an average improvement of 4.5%. This un-derscores the superiority of integrating process su-pervision in enhancing agent performance.Asfor process refinement baselines, while Step-PPOperforms well on InterCodeSQL, surpassing both",
  "Process RefinementLlama-2-7B + Step-PPO64.060.265.769.464.8Llama-2-7B + IPR (ours)71.361.370.374.769.4": ": Performance of different methods on three agent datasets. IPR shows superiority over prompt-based andoutcome refinement methods. For ETO and IPR, we report the best performance across all iterations. prompt-based and outcome refinement baselines,its instability within RL optimization proceduresresults in poor performance on the other two tasks.In contrast, IPR significantly enhances agent per-formance, outperforming all baselines across thethree complex interactive agent tasks. We alsopresent case studies to delineat the task-solvingtrajectories of our method in Appendix E. More-over, IPR showcases robustness on the ALFWorldunseen task, affirming its generalization capabili-ties. We have also included an analysis on trainingefficiency. Please refer to Appendix C for moredetails.",
  "Different Base Models": "To further substantiate the efficacy of our method,we conduct validations across a variety of basemodels. We select Mistral-7B (Jiang et al., 2023a),Llama-2-13B (Touvron et al., 2023) and Llama-3-8B (Meta, 2024) as our base LLMs, employingWebShop and InterCodeSQL as evaluation datasets.We juxtapose the performance of IPR with that ofETO and SFT. The comparative results are summa-rized in . IPR consistently outperforms ETOand SFT across all models and datasets. Notably,on the Mistral model, where SFT performance isrelatively poor, our method realizes a significant im-provement, demonstrating that our approach can ef-fectively enhance the performance of weaker mod-els. Furthermore, we observe that on the WebShoptask, Llama-2-13B achieves the best performanceafter SFT and maintains its leading position afterIPR. Similarly, Llama-3-8B shows superior per-formance on the InterCodeSQL task. This patternindicates that base agents with higher initial perfor-mance are prone to achieve more pronounced final",
  "Ablation Study": "We conduct ablation experiments on the trainingmethods and iteration rounds for IPR. For ALF-World, we evaluate performance on the unseen testset. As shown in , removing each moduleresults in a clear drop in the agents performance,underscoring the power of our method. For the ab-lation on training methods, we discern that the re-moval of SFT loss engenders the most pronouncedperformance drop in the agent. Additionally, wefind that removing the step-DPO loss induce a moresubstantial performance decline than that of remov-ing the outcome-DPO loss, suggesting the necessityof process supervision.The iteration ablation results show that in theinitial rounds of iteration, the agent continuallyrefine its performance by learning from incorrectactions. However, excessive iterations can result ina decrease in performance. This decline might beattributed to overfitting, a consequence of excessiveexploration of the training set.",
  "Step Reward Estimation Quality": "The employment of a scorer agent to estimate pro-cess rewards may introduce some noise. To eval-uate the accuracy of step rewards, we conduct anexperimental analysis on WebShop. In WebShop,each action navigates to a new web page, and scor-ing rules are established to calculate the final re-ward for purchasing a product. Ma et al. (2024)heuristically expands the product scoring rules toassign scores at different web pages, thereby scor-ing each action. This helps us evaluate the qualityof two different actions taken from the same state.Please refer to Appendix B for more details. Wedefine accuracy as the ratio of our constructed con-trastive action pairs order that satisfy the scoringfunction introduced by Ma et al. (2024). We an-alyze the impact of using different LLM agentsas scorers and varying the Monte Carlo samplingtimes on the accuracy of step reward estimation.When constructing the contrastive action pairs, weset the reward threshold as 0.35 for all base mod-els. illustrates that, despite inherent noise,the sampling approach yields satisfactory processreward estimations, achieving an accuracy of upto 82% .The accuracy is influenced by the WebShopIntercodeSQLALFWorld Avg. Reward",
  ": The average reward per step": "base models performance on the task. For ex-ample, with the same sample count, Llama-2-13Bachieves the highest quality in step reward estima-tion. This suggests that using a more powerfulbase model () can improve the quality ofstep reward annotations. Additionally, the numberof samples affects step reward estimation quality.Increasing samples can improve scoring accuracybut raise time costs. Despite the efficiency con-cerns with MC method, we can balance samplesize and scoring accuracy. For WebShop, settingthe sampling number N = 5 achieves performancecomparable to a larger sample size. Without in-creasing inference time costs, IPR achieves nearlya 6% performance improvement at the expense ofthree times the ETO training duration.",
  "Average Reward Per Step": "The purpose of IPR is to provide process-level su-pervision to the agent, enabling it to take moreaccurate actions at each step. Here, we evaluatethe changes in the average reward per step aftertraining. The reward for each step is estimated ac-cording to the procedure in .2. We calcu-late the average rewards for all actions within eachtrajectory and then average these values across theentire test set. illustrates the significantimprovements in average step rewards achieved byour IPR method compared to SFT and ETO acrossthree tasks. It can also be observed that for datasetswhere SFT training has a higher average step re-ward, such as InterCodeSQL, the improvement instep reward is even more pronounced. These resultsunderscore the superior performance of IPR, con-firming its effectiveness in enhancing the accuracyand efficacy of agent actions.",
  ": The performance of different step reward ac-quisition methods": "ward model, which can reduce the training timefor new models within that environment. Giventhe historical trajectory et1 and the current ac-tion at, the reward model outputs a score as thestep reward. We conduct experiments on Web-Shop, using Llama-2-7B to build the reward model.We collect 70k actions generated by Llama-2-7Band Llama-2-13B as training data, with the steprewards estimated using the MC method. We trainthe reward model with MSE loss. To evaluate theeffectiveness of the reward model, we replace thescorer in .2 with the reward model andcompare the results against ETO (which does notuse step rewards) and the MC method. As shown in, the reward model can enhance the perfor-mance of Llama-3-8B, even though its actions arenot included in the training data. This indicates thegeneralization and robustness of the reward model.However, despite outperforming ETO, the resultsstill fall short of the MC method. This may be at-tributed to the models less accurate estimation ofstep rewards within the environment, suggestingthe need for further improvement.",
  "LLM as Agents": "The emerging reasoning and instruction-followingcapabilities of LLMs (Wei et al., 2022) enable themto act as adept agents, particularly in zero-shot gen-eralization across new tasks and problems (Yaoet al., 2022b; Richards, 2023; Wang et al., 2023a).The key technique involves formulating promptsthat furnish LLMs with instructions and contextabout the environment, thereby enabling them togenerate executable actions and leverage externaltools for complex task-solving (Song et al., 2023;Xie et al., 2023). To enhance the capabilities ofopen-source LLMs as agents, recent efforts haveadopted fine-tuning methods (Chen et al., 2023;Zeng et al., 2023; Yin et al., 2023). These methodsenables agent learn from successful trajectories orutilize contrastive information with failed trajecto-ries (Song et al., 2024). However, these approaches",
  "Step-level Process Supervision": "In the resolution of complex tasks, even SOTAmodels may still make mistakes at intermediatesteps. To monitor the task completion process andavoid such errors, some approaches (Uesato et al.,2022; Lightman et al., 2023) employ process-basedmethods which can provide step-level guidance. Toavoid the high cost of manually collecting processsupervision, recent works (Liu et al., 2023; Wanget al., 2023b; Havrilla et al., 2024; Wang et al.,2024) construct pseudo-labels, using the modelspotential to complete the task given the previoussteps as process labels. These methods (Ma et al.,2023; Luong et al., 2024) use PPO to optimize themodel but suffer from training efficiency and insta-bility issues. Our approach, designed with mixturetrajectory optimization, effectively enhances theagents performance.",
  "Self-Improvement": "To compensate for the scarcity of high-quality train-ing data (Tao et al., 2024), self-improvement meth-ods empower the model to autonomously acquire,refine, and learn from self-generated experiences.Certain works (Jiang et al., 2023b; Singh et al.,2023; Zelikman et al., 2023; Chen et al., 2024) fo-cus on alignment, refining the model by discerningthese self-generated responses from those obtainedfrom human-annotated data. Others concentrate onLLM agents utilized for task-solving and interac-tion in dynamic environments. They enhance theagents capabilities in planning (Qiao et al., 2024),tool using (Bousmalis et al., 2023; Zhu et al., 2024),and communication (Ulmer et al., 2024). These en-deavors demonstrate that models can refine them-selves through exploration in diverse domains. Ourwork aims to amplify this self-improvement pro-cess by providing fine-grained guidance.",
  "Conclusion": "In this paper, we present IPR, a novel frameworkdesigned to elevate the capabilties of LLM agentsin complex interaction tasks. Our approach inte-grates process-level supervision, enabling agentsto learn from contrast action pairs. To provide fine-grained guidance in environments where only out-come rewards are available, we use the MC method to automatically calculate step rewards. By em-ploying iterative agent optimization, IPR providesan effective way to optimize agent decision-makingtrajectories.Experiments on three benchmarksdemonstrate that our framework consistently out-performs existing baselines. Subsequent analysesvalidate the efficacy of each part of the frameworkand action efficiency. We believe the IPR frame-work can serve as a potent tool for enhancing agentperformance at the action level, thereby catalyzingfuture progress in intelligent agent development.",
  "Limitations": "Despite achieving the best performance comparedto other baselines, it is important to acknowledgeseveral limitations of this work. 1) Our methodprovides fine-grained supervision for the agentsself-improvement process. However due to limitedtraining data, which is a quite common scenario,iterative preference learning on self-generated sam-ples can lead to overfitting. Future work couldexplore the augmentation of training tasks usingGPT-4 to mitigate this issue. 2) Our method onlyexplores identifying error actions and creating con-trastive datasets through step rewards. However, itdoes not fully exploit the potential of these rewards.The numerical values of step rewards could indi-cate the severity of errors at each step. For instance,adopting the curriculum learning approach (Wanget al., 2021), where more severe errors are correctedfirst before addressing less significant ones, mightfurther enhance agent performance. 3) Our stepreward model is only trained on a single agent task,which affects its generalizability across differenttasks. Future work could develop a general agentstep reward model applicable to various tasks.",
  "Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji,and Quanquan Gu. 2024. Self-play fine-tuning con-verts weak language models to strong language mod-els. arXiv preprint arXiv:2401.01335": "Alex Havrilla, Sharath Raparthy, Christoforus Nalm-pantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi,Eric Hambro, and Roberta Railneau. 2024. Glore:When, where, and how to improve llm reasoningvia global and local refinements.arXiv preprintarXiv:2402.10963. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023a. Mistral7b. arXiv preprint arXiv:2310.06825.",
  "Shuyang Jiang, Yuhao Wang, and Yu Wang. 2023b.Selfevolve: A code evolution framework via largelanguage models. arXiv preprint arXiv:2306.02907": "Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, YingSheng, Lianmin Zheng, Cody Hao Yu, Joseph E.Gonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-cient memory management for large language modelserving with pagedattention. In Proceedings of theACM SIGOPS 29th Symposium on Operating SystemsPrinciples. Hunter Lightman, Vineet Kosaraju, Yura Burda, HarriEdwards, Bowen Baker, Teddy Lee, Jan Leike,John Schulman, Ilya Sutskever, and Karl Cobbe.2023.Lets verify step by step.arXiv preprintarXiv:2305.20050. Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru,Yejin Choi, Hannaneh Hajishirzi, and Asli Celiky-ilmaz. 2023. Dont throw away your value model!making ppo even better via value-guided monte-carlotree search decoding. arXiv e-prints, pages arXiv2309.",
  "Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, PengSun, Xiaoran Jin, and Hang Li. 2024. Reft: Rea-soning with reinforced fine-tuning. arXiv preprintarXiv:2401.08967": "Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang,Yujiu Yang, Yaohui Jin, Zhenzhong Lan, LingpengKong, and Junxian He. 2024. Agentboard: An analyt-ical evaluation board of multi-turn llm agents. arXivpreprint arXiv:2401.13178. Qianli Ma, Haotian Zhou, Tingkai Liu, Jianbo Yuan,Pengfei Liu, Yang You, and Hongxia Yang. 2023.Lets reward step by step: Step-level reward modelas the navigators for reasoning.arXiv preprintarXiv:2310.10080.",
  "AI Meta. 2024. Introducing meta llama 3: The mostcapable openly available llm to date. Meta AI": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al.2022. Training language models to follow instruc-tions with human feedback. Advances in neural in-formation processing systems, 35:2773027744. Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo,Wangchunshu Zhou, Yuchen Eleanor Jiang, ChengfeiLv, and Huajun Chen. 2024. Autoact: Automaticagent learning from scratch via self-planning. arXivpreprint arXiv:2401.05268. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-pher D Manning, Stefano Ermon, and Chelsea Finn.2024. Direct preference optimization: Your languagemodel is secretly a reward model. Advances in Neu-ral Information Processing Systems, 36.",
  "Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu,Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu,and Deyi Xiong. 2023. Large language model align-ment: A survey. arXiv preprint arXiv:2309.15025": "Noah Shinn, Federico Cassano, Ashwin Gopinath,Karthik Narasimhan, and Shunyu Yao. 2024. Re-flexion: Language agents with verbal reinforcementlearning. Advances in Neural Information Process-ing Systems, 36. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Ct,Yonatan Bisk,Adam Trischler,and MatthewHausknecht. 2020. Alfworld: Aligning text and em-bodied environments for interactive learning. arXivpreprint arXiv:2010.03768. Avi Singh, John D Co-Reyes, Rishabh Agarwal, AnkeshAnand, Piyush Patil, Peter J Liu, James Harri-son, Jaehoon Lee, Kelvin Xu, Aaron Parisi, et al.2023. Beyond human data: Scaling self-trainingfor problem-solving with language models. arXivpreprint arXiv:2312.06585. Yifan Song, Weimin Xiong, Dawei Zhu, Cheng Li,Ke Wang, Ye Tian, and Sujian Li. 2023.Rest-gpt: Connecting large language models with real-world applications via restful apis. arXiv preprintarXiv:2306.06624.",
  "Yifan Song, Da Yin, Xiang Yue, Jie Huang, SujianLi, and Bill Yuchen Lin. 2024.Trial and error:Exploration-based trajectory optimization for llmagents. arXiv preprint arXiv:2403.02502": "Zhengwei Tao, Ting-En Lin, Xiancai Chen, HangyuLi, Yuchuan Wu, Yongbin Li, Zhi Jin, Fei Huang,Dacheng Tao, and Jingren Zhou. 2024. A surveyon self-evolution of large language models. arXivpreprint arXiv:2404.14387. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Jonathan Uesato, Nate Kushman, Ramana Kumar, Fran-cis Song, Noah Siegel, Lisa Wang, Antonia Creswell,Geoffrey Irving, and Irina Higgins. 2022.Solv-ing math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275.",
  "Dennis Ulmer, Elman Mansimov, Kaixiang Lin, JustinSun, Xibin Gao, and Yi Zhang. 2024. Bootstrappingllm-based task-oriented dialogue agents via self-talk.arXiv preprint arXiv:2401.05033": "Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-dlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and An-ima Anandkumar. 2023a. Voyager: An open-endedembodied agent with large language models. arXivpreprint arXiv:2305.16291. Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, DamaiDai, Yifei Li, Deli Chen, Y Wu, and Zhifang Sui.2023b. Math-shepherd: A label-free step-by-stepverifier for llms in mathematical reasoning. arXivpreprint arXiv:2312.08935.",
  "Xin Wang, Yudong Chen, and Wenwu Zhu. 2021.A survey on curriculum learning.IEEE transac-tions on pattern analysis and machine intelligence,44(9):45554576": "Zihan Wang, Yunxuan Li, Yuexin Wu, Liangchen Luo,Le Hou, Hongkun Yu, and Jingbo Shang. 2024.Multi-step problem solving through a verifier: Anempirical analysis on model-induced process super-vision. arXiv preprint arXiv:2402.02658. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,Barret Zoph, Sebastian Borgeaud, Dani Yogatama,Maarten Bosma, Denny Zhou, Donald Metzler, et al.2022. Emergent abilities of large language models.arXiv preprint arXiv:2206.07682. Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Lu-oxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao,Qian Liu, Che Liu, et al. 2023. Openagents: Anopen platform for language agents in the wild. arXivpreprint arXiv:2310.10634. John Yang, Akshara Prabhakar, Karthik Narasimhan,and Shunyu Yao. 2024. Intercode: Standardizing andbenchmarking interactive coding with execution feed-back. Advances in Neural Information ProcessingSystems, 36. Shunyu Yao, Howard Chen, John Yang, and KarthikNarasimhan. 2022a. Webshop: Towards scalablereal-world web interaction with grounded languageagents. Advances in Neural Information ProcessingSystems, 35:2074420757.",
  "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, IzhakShafran, Karthik Narasimhan, and Yuan Cao. 2022b.React: Synergizing reasoning and acting in languagemodels. arXiv preprint arXiv:2210.03629": "Da Yin, Faeze Brahman, Abhilasha Ravichander, Khy-athi Chandu, Kai-Wei Chang, Yejin Choi, andBill Yuchen Lin. 2023. Lumos: Learning agentswith unified data, modular design, and open-sourcellms. arXiv preprint arXiv:2311.05657. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-ing Yao, Shanelle Roman, et al. 2018. Spider: Alarge-scale human-labeled dataset for complex andcross-domain semantic parsing and text-to-sql task.arXiv preprint arXiv:1809.08887. Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding,Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen,Ruobing Xie, Yankai Lin, et al. 2024. Advancing llmreasoning generalists with preference trees. arXivpreprint arXiv:2404.02078. Zheng Yuan, Hongyi Yuan, Chengpeng Li, GuantingDong, Chuanqi Tan, and Chang Zhou. 2023. Scal-ing relationship on learning mathematical reason-ing with large language models.arXiv preprintarXiv:2308.01825.",
  "ADataset Details": "WebShopWebShop (Yao et al., 2022a) isa network-based simulation environment for e-commerce experiences, features a website with1.8 million actual products, each with distinct la-bels and attributes. In this environment, the agentis allowed to interact with the system through\"search[QUERY]\" or \"click[ELEMENT]\" actionsto purchase products matching the instructions.Once the agent clicks the \"buy\" option, the environ-ment provides a final reward, which is calculatedbased on the matching heuristics of the productsattributes and price. InterCodeSQLInterCodeSQL is an interactivedatabase environment within InterCode bench-mark (Yang et al., 2024), where the agent inter-acts with the environment to retrieve necessary ta-ble information and complete the correspondingSQL queries. The database is constructed fromthe Spider (Yu et al., 2018) dataset, a large-scalecross-domain dataset originally designed for evalu-ating SQL query generation from natural languagequestions. We have modified InterCodeSQL to fitfor our evaluation framework. When the agent per-form the \"submit\" action, the environment providesa final reward. The reward is calculated using theIntersection over Union (IoU) metric to quantifythe correctness of the submitted execution outputgenerated by the against the gold output, with bothoutputs being lists of records. ALFWorldALFWorld (Shridhar et al., 2020)are household tasks that require agents to explorerooms and use commonsense reasoning to performtasks, such as \"put a pencil on the desk\". The en-vironment provides the outcome on whether theagent successfully completes the task within givensteps. The original ALFWorld dataset comprisesboth seen and unseen evaluation sets. The seen setis designed to assess in-distribution generalization,whereas the unseen set with new task instancesmeasures out-of-distribution generalization of theagents.",
  "f = ftype |UattYatt|+|UoptYopt|+1[ypriceuprice]": "|Uatt|+|Uopt|+1,(11)where ftype = TextMatch(y, y). Following Maet al. (2024), we expand the product scoring rules toderive the score for each action. Typically, complet-ing a web shopping task involves three continuousstates: search, product selection, and finalizing theproduct style before placing an order. Each actionleads to deterministic state change in the environ-ment. Therefore, to calculate the step reward, wemeasure the distance between the result state andthe target state. We primarily calculate scores forthree pages (states): search result page, productdescription page, and order confirmation page. Onthe search result page, we calculate the score ofeach product on the page and take the highest scorefor this page. On the product description page, wecompute the highest score for the product undervarious options as the page score. On the orderconfirmation page, the score of the finally selectedproduct is considered as the score for that page.",
  "CTraining Efficiency Analysis": "Here, we compare the time consumption of differ-ent methods on WebShop in . Since ourmethod can achieve state-of-the-art performanceafter three rounds of iteration, we use the time forthree rounds of iteration as the measure of trainingtime. The time consumption results are as follows:SFT requires 1 hour, ETO requires 2.5 hours, andIPR requires 5.3 hours. Furthermore, although theMonte Carlo method necessitates sampling to ob-tain the process information of step rewards, withthe support of vllm (Kwon et al., 2023), we haveindeed been able to construct the step rewards inan efficient and parallel manner. Without increas-ing inference time costs, IPR achieves nearly a 6%performance improvement at the expense of a train-ing duration less than three times that of ETO. Webelieve that this time cost is acceptable.",
  "ECase Study": "Here, we provide a detailed comparison of the tra-jectories of IPR and ETO on the same tasks withinWebShop and ALFWorld, which illustrates howIPR provides fine-grained guidance to correct erro-neous actions and improves the efficacy of agentactions. The cases are shown in and 6.In the WebShop scenario, ETO learned the pro-cess of selecting products and choosing their at-tributes. However, when selecting a product, itoverlooked the requirement for product type and di-rectly purchased the first item on the search page, adress shirt, ignoring the request for a tank top.In contrast, our IPR, benefiting from fine-grainedprocess supervision, corrected this error and chosethe correct tank top product.In the ALFWorld scenario, ETO engaged inblind exploration to find the desired item potato,which introduced a lot of irrelevant information andunnecessary context. In contrast, our IPR improvedthe efficiency of each action, directly navigatingto the sinkbasin to locate the potato. Moreover,ETO made a mistake in the details by cooling thepotato down without picking it up, whereas IPRrectified this action.",
  "ETO": "Task Instruction: You are in the middle of aroom. Looking quickly around you, you see acabinet 6, a cabinet 5, a cabinet 4, a cabinet 3,a cabinet 2, a cabinet 1, a coffeemachine 1, acountertop 3, a countertop 2, a countertop 1, adrawer 3, a drawer 2, a drawer 1, a fridge 1, agarbagecan 1, a microwave 1, a shelf 3, a shelf2, a shelf 1, a sinkbasin 1, a stoveburner 4, astoveburner 3, a stoveburner 2, a stoveburner1, and a toaster 1. Your task is to: cool somepotato and put it in microwave.# Actions of blind explorationThought: I need to cool a potato before puttingit in the microwave. The fridge is the best placeto cool food items quickly.Action: go to fridge 1Observation: The fridge 1 is closed.Thought: The fridge is closed. I need to openit to access the cooling space inside.Action: open fridge 1Observation: You open the fridge 1.Thefridge 1 is open. In it, you see a bowl 1, a cup1, a lettuce 1, a plate 2, and a tomato 1.Thought: Ive opened the fridge, but I realizeI dont need to put the potato inside since Ican cool it while holding it. I should close thefridge to conserve energy.Action: close fridge 1Observation: You close the fridge 1.Thought: I need to cool the potato, but I cantuse the fridge since I closed it. I should findanother way to cool it down quickly.Action: go to sinkbasin 1Observation: On the sinkbasin 1, you see abowl 3, a butterknife 2, a egg 2, a fork 2, aknife 1, and a potato 2.# Making a mistake in the details by coolingdown without picking up the potatoThought: Im at the sink basin, which is acommon place to cool food items by placingthem in cold water. I can use this method tocool the potato.Action: cool potato 2 with sinkbasin 1Observation: Nothing happens.Thought: Now that the potato is cooled, I needto put it in the microwave. I should go to themicrowave to proceed with the task.",
  "IPR": "Task Instruction: You are in the middle of aroom. Looking quickly around you, you see acabinet 6, a cabinet 5, a cabinet 4, a cabinet 3,a cabinet 2, a cabinet 1, a coffeemachine 1, acountertop 3, a countertop 2, a countertop 1, adrawer 3, a drawer 2, a drawer 1, a fridge 1, agarbagecan 1, a microwave 1, a shelf 3, a shelf2, a shelf 1, a sinkbasin 1, a stoveburner 4, astoveburner 3, a stoveburner 2, a stoveburner1, and a toaster 1. Your task is to: cool somepotato and put it in microwave.Thought: I need to cool a potato and then putit in the microwave. First, I should locate thepotato.Action: go to sinkbasin 1Observation: On the sinkbasin 1, you see abowl 3, a butterknife 2, a egg 2, a fork 2, aknife 1, and a potato 2.Thought: Theres a potato here on the sinkbasin. I should pick it up to proceed with thetask.Action: take potato 2 from sinkbasin 1Observation: You pick up the potato 2 fromthe sinkbasin 1.Thought: To cool the potato, I need to use thefridge. I should go to the fridge now.Action: go to fridge 1Observation: The fridge 1 is closed.Thought: The fridge is closed. I need to openit to cool the potato inside.Action: open fridge 1Observation: You open the fridge 1.Thefridge 1 is open. In it, you see a bowl 1, a cup1, a lettuce 1, a plate 2, and a tomato 1.Thought: The fridge is open now. I can use itto cool the potato by placing the potato inside.Action: cool potato 2 with fridge 1Thought: The potato is cooled. The next stepis to put it in the microwave, so I need to go tothe microwave.Action: go to microwave 1Observation: The microwave 1 is closed.Thought: The microwave is closed. I need toopen it to place the potato inside.Action: open microwave 1Observation: You open the microwave 1. Themicrowave 1 is open. In it, you see a egg 1. Action: go to microwave 1Observation: The microwave 1 is closed.Thought: The microwave is closed. I need toopen it to place the potato inside.Action: open microwave 1Observation: You open the microwave 1. Themicrowave 1 is open. In it, you see a egg 1.Thought: Ill proceed with the given instruc-tions and place the potato in the microwave.Action: put potato 2 in/on microwave 1Observation: Nothing happensThought: With the potato in the microwave,the next step is to close it to complete the task.Action: close microwave 1Observation: You close the microwave 1.Thought: The task is complete with the potatoplaced in the microwave. I should ensure themicrowave is closed before finishing.Action: put potato 2 in/on microwave 1Observation: You cool the potato 2 using thefridge 1.....Reward: 0.0",
  "Instruction Prompt for WebShop": "You are doing a web shopping task. I will give you instructions about what to do. You have tofollow the instructions. Every round I will give you an observation and a list of available actions,you have to respond to an action based on the state and instruction. You can use search action ifsearch is available. You can click one of the buttons in clickables. An action should be one of thefollowing structure: search[keywords] or click[value] If the action is not valid, perform nothing. Keywords in search are up to you, but the value in clickmust be a value in the list of available actions. Remember that your keywords in search should becarefully designed.",
  "Instruction Prompt for InterCodeSQL": "You are a helpful assistant assigned with the task of problem-solving. To achieve this, you willinteract with a MySQL Database system using SQL queries to answer a question.At each turn, you should first provide your step-by-step thinking for solving the task. Your thoughtprocess should start with \"Thought: \", for example: Thought: I should write a SQL query that getsthe average GNP and total population from nations whose government is US territory. After that, you have two options:1) Interact with a mysql programming environment and receive the corresponding output. Yourcode should start with \"Action: \" , for example: Action: SELECT AVG(GNP), SUM(population)FROM nations WHERE government = US Territory2) Directly submit the result, for example: Action: submit.",
  "You should use this format:Thought: your thoughtAction: <the mysql command>": "You will receive the corresponding output for your sql command. Your output should contain onlyone \"Action\" part. The \"Action\" part should be executed with a mysql interpreter or propose ananswer. Any natural language in it should be commented out. The SQL query and submit partscan not appear in your output simultaneously.",
  "Instruction Prompt for ALFWorld": "Interact with a household to solve a task. Imagine you are an intelligent agent in a householdenvironment and your target is to perform actions to complete the task goal. At the beginning ofyour interactions, you will be given a detailed description of the current environment and yourgoal to accomplish.For each of your turn, you will be given the observation of the last turn. You should first thinkabout the current condition and plan for your future actions, and then output your action in thisturn. Your output must strictly follow this format:\"Thought: your thoughts. Action: your nextaction\". The available actions are:1. go to recep2. task obj from recep3. put obj in/on recep4. open recep5. close recep6. toggle obj recep7. clean obj with recep8. heat obj with recep9. cool obj with recepwhere obj and recep correspond to objects and receptacles.After each turn, the environment will give you immediate feedback based on which you plan yournext few steps. if the environment outputs \"Nothing happened\", that means the previous action isinvalid and you should try more options."
}