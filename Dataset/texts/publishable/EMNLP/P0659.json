{
  "Abstract": "Vision Large Language Models (VLLMs) aretransforming the intersection of computer vi-sion and natural language processing. Nonethe-less, the potential of using visual prompts foremotion recognition in these models remainslargely unexplored and untapped. Traditionalmethods in VLLMs struggle with spatial local-ization and often discard valuable global con-text. To address this problem, we propose aSet-of-Vision prompting (SoV) approach thatenhances zero-shot emotion recognition by us-ing spatial information, such as bounding boxesand facial landmarks, to mark targets precisely.SoV improves accuracy in face count and emo-tion categorization while preserving the en-riched image context. Through a battery ofexperimentation and analysis of recent com-mercial or open-source VLLMs, we evaluatethe SoV models ability to comprehend facialexpressions in natural environments. Our find-ings demonstrate the effectiveness of integrat-ing spatial visual prompts into VLLMs for im-proving emotion recognition performance.",
  "Introduction": "As the integration of computer vision and naturallanguage processing progresses, VLLMs (Dai et al.,2024) are revolutionizing the way machines inter-pret visual and textual data. Emotion recognition isgaining considerable interest across multiple disci-plines and presents distinct challenges (Yang et al.,2023a). It requires the decoding of emotions fromnuanced indicators like facial expressions, bodylanguage, and contextual details.Previous methods, such as those in (Xenos et al.,",
  "Left Eye": ": Proposed Set-of-Vision (SoV) promptingapproach for enhancing facial expression recognitionin Vision-Language Large Models (VLLMs). SoVprogressively incorporates (1) bounding boxes to iden-tify and locate faces, (2) numbered boxes to ground anddifferentiate faces, and (3) facial landmarks to analyzespatial relationships for fine-grained emotion classifica-tion. This multi-stage visual prompting strategy enablesVLLMs to accurately detect and recognize emotions inreal-world images while preserving global context. methods overlook the spatial relationships betweendifferent people and facial features within a sin-gle face. These spatial relationships can be la-beled with numbers and bounding boxes in ourSoV prompts to guide VLLMs. A person nearbymay have similar facial expressions, while a personfarther away may show different facial expressions.Additionally, the relationships between the eyes,mouth, and nose features can be highlighted byfacial landmarks in SoV prompts to guide VLLMs.",
  "Recent studies (Yang et al., 2024; Zou et al.,": "2024; Yang et al., 2023b; Zou et al., 2024) have ex-plored Visual Prompting, a technique employed inimage-language tasks to guide LLMs by incorporat-ing markers such as colorful boxes or circles to em-phasize specific targets within an image. ReCLIP(Subramanian et al., 2022) adds colorful boxes di-rectly onto an image to highlight specific targetsand blurs other irrelevant areas to reduce the per-formance gap with supervised models on both realand synthetic datasets. Additionally, RedCircle(Shtedritski et al., 2023) employs visual prompt en-gineering, specifically drawing a red circle aroundan object in an image, to direct a Vision-Language",
  "Answer:": "In the image, there are 22 visible faces. Emotions can be grouped into three main categories.Neutral Emotion: Approximately 16 Faces. This neutral expression is typical in settings requiring attention to a speaker or event, reflecting an engaged yet reserved emotional state.Mildly Positive Emotion: Approximately 5 Faces. This could be a response to somethinginteresting or mildly amusing happening at the event, but their reactions are subdued, in keeping with the formal setting.Happy: 1 Face. This expression stands out as more joyful or enthusiastic about the proceedings.",
  "SOV": ": Comparative analysis of emotion recognition methods in a group setting: assessing the precisionof facial emotion categorization and face detection using plain text prompts versus Set-of-Vision (SoV) promptsincorporating facial landmarks, bounding boxes, and face enumeration. Top: Results using plain text prompts.Bottom: Results using Set-of-Vision (SoV) prompts. The use of SoV prompts, such as numbering each face, placingbounding boxes, and identifying facial landmarks, allows for a more precise analysis. Models attention to that region and enhance itsperformance in tasks like zero-shot keypoint local-ization. However, both of these approaches focuson local objects and ignore spatial context informa-tion. Yang et al. (Yang et al., 2024) propose usingfine-grained visual prompts, such as segmentationmasks, and enhancing focus on relevant areas witha Blur Reverse Mask that blurs regions outside thetarget mask to minimize distractions and maintainspatial context. Although visual prompting tech-niques have garnered interest, their full potentialremains unexplored for emotion recognition tasks.Current approaches rely solely on coarse mark-ers like colorful boxes, circles, or masks, whichcan introduce ambiguity, blur the face images, andpose challenges for accurate recognition tasks. Thispaper addresses these issues by systematically or-ganizing and investigating various forms of visualprompting. Furthermore, we propose a new prompt-ing approach called Set-of-Vision prompting (SoV)in , which utilizes spatial information such asnumbers, bounding boxes, and facial landmarks toprecisely mark each target while maintaining back-ground context, thereby enhancing the zero-shotperformance of facial expression recognition.The top of , shows an approach wherespecific vision prompts are not used. As a result, the analysis inaccurately counts 22 visible facesand misclassifies persons emotions into incorrectcategories, with a number of faces labeled underNeutral Emotion and fewer under Mildly Posi-tive Emotion and Happy. This misclassificationand miscount demonstrate the limitations when de-tailed visual cues are not utilized in the analysis.In the bottom of , the use of SoV prompts,such as numbering each face, placing boundingboxes, and identifying facial landmarks, allowsfor a more precise analysis. The correct numberof faces is identified (18), and the emotions areaccurately categorized into more nuanced groups:Neutral Emotion, Mildly Positive Emotion, andSmiling or Happy. This method provides a clearerand more detailed breakdown of each individualsemotional state based on visible facial expressions.This comparison highlights the importance and ef-fectiveness of integrating visual prompts in VLLMsanalysis for more accurate and detailed recognitionand categorization of human emotions in images.To summarize, our main contributions are: (1)The paper introduces a novel visual promptingmethod (SoV) that highlights facial regions di-rectly within the entire image. This preserves back-ground context, enhancing the ability of VLLMsto perform accurate emotion recognition without",
  "Output 2: Face1: Happy Face2: Neutral": ": Workflow diagram for enhanced face recognition and emotion analysis using the Set-of-Vision(SoV) prompting approach: a multi-step process involving face detection, face numbering, landmark extraction,and spatial relationship analysis for emotion classification. Each detected face is analyzed and identified by faciallandmarks on the face, such as the positions of the nose, eyes, mouth, and other facial features. the need for cropping faces, thus maintaining theholistic view of the image. (2) The proposed faceoverlap handling algorithm effectively addressesconflicts arising from overlapping face detections,especially in images with dense face clusters. Byprioritizing larger faces and iteratively checking foroverlaps, the algorithm ensures that non-occludedfaces are retained for subsequent emotion analy-sis. (3) Our results show that incorporating spatialvisual prompts (SoVs) into VLLMs can enhancetheir performance in recognizing emotions.",
  "Vision Large Language models": "LLMs such as LLaMA (Touvron et al., 2023),ChatGPT-3 (Brown et al., 2020), ChatGPT-4(Achiam et al., 2023), and PaLM (Chowdheryet al., 2023) have demonstrated remarkable zero-shot transfer capabilities in natural language pro-cessing. Recently, VLLMs, which leverage image-text data pairs from the web, have gained promi-nence in the computer vision domain. MiniGPT-4(Zhu et al., 2023), a model that combines a visualencoder with an advanced language model, can en-able multi-modal capabilities such as generatingdetailed image descriptions and designing websitesfrom sketches. Video-LLaVA (Zhang et al., 2023a) is a multi-modal framework that enhances LargeLanguage Models with the ability to understandand generate responses based on both visual and au-ditory content in videos. LLaVA (Liu et al., 2023)is a newly developed, end-to-end trained, large mul-timodal model that combines a vision encoder witha language model, demonstrating promising abili-ties in multimodal chat. Although VLLMs exhibitremarkable capabilities in vision-based tasks suchas image segmentation and object detection, theytypically require fine-tuning of the vision and textencoders using existing open vocabulary methodswhen applied to specific tasks. In contrast, this pa-per proposes a zero-shot architecture for emotionrecognition, overcoming the need for task-specificfine-tuning.",
  "Prompting methods": "Prompt engineering is a widely employed tech-nique in the field of NLP (Strobelt et al., 2022;Zhou et al., 2024). AdbGPT (Feng and Chen, 2024)is a novel, lightweight approach that leverages few-shot learning and chain-of-thought reasoning inLarge Language Models to automatically repro-duce bugs from bug reports, mimicking a devel-opers problem-solving process without the needfor training or hard-coding. Although prompts for large language models have been extensively ex-plored, prompts for vision tasks have received lessattention and investigation. Yang et al. (Yang et al.,2024) propose using fine-grained visual promptslike segmentation masks and a Blur Reverse Maskstrategy to focus on relevant areas. The Image-of-Thought (IoT) prompting method (Zhou et al.,2024) enhances Multimodal Large Language Mod-els by guiding them to extract and refine visualrationales step-by-step from images, combiningvisual and textual insights to improve zero-shotperformance on complex visual reasoning tasks.Although these methods have shown promise intasks like semantic segmentation and object ground-ing, their performance in emotion recognition hasbeen less effective. This is largely because theytend to analyze individual objects in isolation, over-looking global information and specific facial fea-tures, which are crucial for accurately interpretingemotions. To address these issues, the proposedapproach directly focuses on the fine-grained fa-cial features present in the entire image, preservingspatial information by utilizing bounding boxes,numbers, and facial landmarks.",
  "Problem Definition": "The task of matching images to emotions foreach visible face in a given image involves sev-eral sophisticated steps, combining face detec-tion and emotion recognition.Typically, theVLLMs, denoted as , will take an image I RHW3 and a text question of length li, Qi =[qi1, qi2, ..., qili], as input. The output is a sequenceof answers with length lo, containing emotions,Ao = [ao1, ao2, ..., aolo], which can be formulated as(Eq. 1):Ao = (I, Qi)(1) In our task, we aim to find the best matchingimage-emotion pairs (I, Ao) for each visible face.Traditionally, this involves cropping the face fromthe image using face detectors. However, with theintroduction of visual prompting, faces can be di-rectly marked on the entire image, highlighting thefacial region while preserving the background con-text and avoiding the obscuration of faces. Withthis in mind, we have developed Set-of-Visionprompts (SoV), a simple method of overlaying anumber of visual prompts on the facial regions inan image.",
  "Box detection": "Once the image is obtained, we need to generatevisual prompts for the image that will be utilized byVLLMs for emotion recognition. We employ theRetinaFace (Deng et al., 2020) algorithm to detectfaces within the image. Let B = {b1, b2, . . . , bn}denote the set of detected face bounding boxes,where bi represents the i-th face bounding box. Theprocess can be formulated as (Eq. (3)):",
  ", where I is the input image; i represents the hy-perparameters for the RetinaFace model D; and bicorresponds to the i-th face bounding box": "3.2.2Box Overlap Handling AlgorithmHowever, this face detection algorithm inevitablyintroduces some overlaps or conflicts that confuseVLLMs, especially in images with densely pop-ulated faces, such as when two faces overlap inone area or one face is obscured by another. Thisis illustrated in . To mitigate this problem,we propose a face overlap handling algorithm, asshown in Algorithm. 1. Given the set of boxesB = {b1, b2, . . . , bn}, we first calculate the areafor each bounding box bi, then sort the detectedfaces bi by their area in descending order (line 12)(Eq. (4)):",
  "Ffinal Ffinal { F[k]}(7)": "It ensures that only the faces that are close to thecamera and not obstructed remain in the final list.Once we determine the location of boxes B ={b1, b2, . . . , bn} for each face, We need to as-sign a unique ID to each face. The unique IDN = {1, 2, . . . , n} will be used to locate each facein the image, where n is the number of detectedfaces. Thus, the set of vision prompts becomesdistinguishable and can be effectively interpretedby VLLMs.",
  "Plain Text Prompts": "This method is exemplified on the left side in . It involves asking a general question about theemotional state of a group of people without ref-erencing specific individuals. For example, thequestion \"What is the emotion for this group ofpeople?\" yields an answer that considers the over-all mood and setting of the group. This approachis useful for understanding group dynamics or thegeneral atmosphere of a scene.",
  "Combined Text-Vision Prompts": "Shown on the right side in , this method in-volves more detailed prompts that focus on indi-vidual persons within the group. This allows fora more nuanced analysis of specific peoples emo-tions and actions. For instance, questions such asWhat is Person 1s emotion? or What food is Per-son 3 eating and drinking? prompt answers thatdelve into specific details regarding individuals fa-cial expressions, body language, and interactionswith objects, like food and drinks.",
  "What is the emotion for this group of people?": "In the image, we see a group of four people enjoying a picnic outdoors, which suggests a relaxed and pleasantsetting.Person 1 (Man on the left): He is smiling and seems to be engaging in conversation, suggesting he is happy and enjoying the moment.Person 2 (Young girl on the right): She has a neutral expression while listening or looking towards another family member, possiblyindicating contentment or attentiveness.Person 3 (Woman in the center): She is smiling and appears to be laughing, indicating she is happy and enjoying the company.Person 4 (Boy in the center right): He also has a slight smile, suggesting he is happy and enjoying the picnic.",
  "Is Person 1's face visible? What is Person 1's emotion?": "In the image, Person 1 (the man on the left) is partially visible; his face is shown in profile view, and he appears to be smiling. This suggests that he is feeling happy and engaged, likely enjoyingthe conversation or the company he is with. Even though we cannot see his full face directly, the visible portion of his smile and the overall relaxed setting indicate a positive emotion.",
  "What food is Person 3 eating and drinking?": "In the image, Person 3 appears to be holding a glass, likely containing a lemonade or similar light-colored drink. There is no clear indication of her eating any specific food in the scene; however, there are various foods visible on the picnic blanket, including fruits and possibly some sandwiches or snacks, which any of the picnic participants, including her, might be enjoying.",
  "User:Do Person 3 and Person 4 have similar emotion?": "In the image, Person 3 and Person 4 both appear to be happy. Person 3 is smiling broadly,suggesting she is enjoying the moment and possibly laughing or engaged in a cheerfulconversation. Person 4, although his smile is less pronounced than Person 3's, also displays a happy demeanor, suggesting contentment and enjoyment of the picnic. : We use two types of prompt methods. Left: plain text prompts, which can be used for group emotionrecognition. Right: combined text-vision prompts, which can be used for analyzing specific individuals emotions.These prompts can be used to evaluate emotional interpretation in social interactions based on facial expressions,body language, and contextual cues. : Comparison of zero-shot emotion recognition methods, including MiniGPT-4 (Zhu et al., 2023), LLaVA(Liu et al., 2023), Video-LLaVA (Zhang et al., 2023a), GPT-4V (Achiam et al., 2023), and SoV-Enhanced GPTModels, across datasets with varying difficulty levels (Easy, Medium, and Hard): A Comparative Analysis ofAccuracy and Top-1 Recall (R@1).",
  "Models and Settings": "We do not need to train any models for our method.We evaluate the models performance in a zero-shotmanner using VLLMs. We include both commer-cial models such as GPT-4V-turbo (Achiam et al.,2023)1 and GPT-4o-2024-05-13 (Achiam et al.,2024) as well as open-sourced models includingMiniGPT-4-Vicuna (Zhu et al., 2023)2, LLaVA-1.5-7B (Liu et al., 2023)3, Video-LLaVA-7B (Zhanget al., 2023a)4.",
  "Dataset details": "We collect original images from ABC News web-site5. Following the collection, we undertake metic-ulous preprocessing, initially removing any iden-tical and blurry images through deduplication. Tominimize human effort and cost in data annotation,we employ DeepFace (Serengil and zpnar, 2024)for emotion annotation. Subsequently, two humanannotators revise and refine the image labels. Fi-nally, to finalize the labels, we involved a thirdannotator who has a professional background inpsychology to verify correctness of facial expres-sions with their domain knowledge. This proce-dure guarantees the quality of the annotated dataused to construct benchmarks. in appendixA.1 presents the dataset details used for testing a",
  "Quantitative Results": "provides a detailed analysis of differentzero-shot emotion recognition methods. MiniGPT-4 (Zhu et al., 2023) exhibits low performance, withaccuracy ranging from 15.78% to 30.45% and Re-call@1 from 12.85% to 16.17%. LLaVA (Liu et al.,2023) and Video-LLaVA (Zhang et al., 2023a) per-form better in simpler categories but struggle signif-icantly in the Hard category, where accuracy plum-mets to 3.50% and Recall@1 to 1.58%. In con-trast, GPT-4V (Achiam et al., 2023) demonstratesrobust performance across all levels, markedly im-proved by the SoV prompts. Specifically, GPT-4V+SoV achieves an impressive 60.91% accuracyand 41.96% Recall@1 in the Easy category, main-taining 50.00% accuracy and 18.97% Recall@1even in the Hard category. These results underlineSoVs effectiveness in boosting the models abilityto accurately interpret emotions across differentcomplexities.In , the chart highlights the varied effi-cacy of different VLLMs in emotion recognitiontasks.GPT-4V+Ours consistently outperformsother models across nearly all emotions, partic-ularly excelling in neutral and angry emotions.This highlights the specialized capabilities of GPT-4V+Ours in capturing more nuanced and variedemotional states. Meanwhile, other models showselective strengths and general weaknesses, partic-",
  "Visual Prompting": "Table. 2 presents a comparative analysis of state-of-the-art methods for zero-shot emotion recog-nition across datasets categorized by varying lev-els of difficulty: Easy, Medium, and Hard. It de-tails performance metrics such as Accuracy andRecall, comparing the effectiveness of differentvisual prompting strategies utilized by GPT-4V.The baseline method (GPT-4V), which uses plaintext prompts, demonstrates moderate effectiveness,with an overall accuracy of 44.44% and a Recallof 22.11%. Methods such as ReCLIP, RedCircle,and SoV employ more complex combinations of vi-sual prompts. SoV (Ours), incorporating Numbers,Boxes, and Facial Landmarks, achieves the highestoverall accuracy and recall scores of 55.33% and28.69%, respectively. This suggests that the integra-tion of multiple visual cues, particularly those thatenhance the recognition of facial features, signifi-cantly improves performance across all difficultylevels, especially in more challenging datasets. visually represents how each visualprompting approach modifies the image to focus onemotion-relevant features. ReCLIP and RedCircleblur out non-facial areas and highlight faces withrectangles and circles, respectively. SoV appliesa combination of visual prompts to emphasize fa-cial areas while maintaining background context,which is critical for emotion recognition. reveals a comparative analysis of the per-formance across different emotional categories us-ing three different visual prompts for GPT-4V: SoV(Ours), RedCircle, and ReCLIP. In this assessment, : Comparison of SOTA methods for zero-shot emotion recognition across datasets with varying levelsof difficultyEasy, Medium, and Hard. The types of visual prompts used by previous approaches are: P: Crop,B: Box, R: Blur Reverse, C: Circle, N: Number, F: Facial Landmarks.",
  "ImageReCLIPRedCircleSoV": ": Visualization of the SOTA visual prompting approaches such as ReCLIP (Subramanian et al., 2022),RedCircle (Shtedritski et al., 2023) and our SoV prompts. ReCLIP and RedCircle blur out non-facial areas andhighlight faces with rectangles and circles, respectively. SoV(Ours) applies a combination of visual prompts toemphasize facial areas while maintaining background context.",
  ": The bar chart displayed in the image illustratesthe performance of different vision promptsGPT-4V+SoV, Box + Number, Box, Baseline in emotionrecognition across seven different emotional categories": "SoV outperforms the other methods in most emo-tional categories. Notably, SoV scores exception-ally well for Happy and Neutral emotions, sug-gesting a robust capability in recognizing theseemotions accurately. RedCircle and ReCLIP showsimilar performance for Fear and Sad, but lagsignificantly behind SoV in Happy and Neutral.For Angry, SoV and ReCLIP perform almost",
  "Ablation Study": "We investigate the effects of different visionprompts on GPT-4V. The data from demon-strates how various vision prompts affect an emo-tion recognition systems performance across dif-ferent difficulty levels. Baseline prompt showsmoderate effectiveness with an accuracy of 48.85%in Easy, 47.95% in Medium, but drops to 32.45%in Hard scenarios. The overall accuracy is 44.44%,with Recall@1 similarly distributed, suggestingit performs consistently across different complex-ities but struggles with harder categories. Intro-duction of bounding boxes slightly reduces accu-racy in simpler categories but improves Recall@1,suggesting enhanced precision in pinpointing rele-vant emotions. Notably, combining box and num-ber prompts markedly boosts performance acrossall categories, particularly in challenging envi-ronments where accuracy rises significantly from32.45% to 42.10% and Recall@1 from 11.36% to15.57%. The SoV prompt outperforms all othermethods, achieving peak accuracies of 60.91% inEasy and 50.00% in Hard, along with the highest : Ablation study for vision prompts on GPT-4V. Baseline: represents the models performance withoutany additional prompts. Box: indicates a visual prompt that uses bounding boxes. Box+Number: adding numericalidentifiers to the bounding boxes. SoV: adding facial landmarks to each face with additional numerical identifiers tothe bounding boxes.",
  "What is the emotion for each face?": ": The impacts of segmentation masks for emo-tion recognition. Top: SoV provides a clearer view foremotion recognition. Bottom: the segmentation masksobscure parts of their faces, making it more challengingto accurately discern these emotions, especially for Per-son 2. In addition, the added segmentation masks alsoresult in a lack of precise context. Recall@1 figures, confirming its effectiveness inaccurately interpreting emotions even in the mostdifficult settings. showcases an ablation study that com-pares the effectiveness of different vision promptson GPT-4V. The GPT-4V+SoV configuration con-sistently outperforms the other methods in nearlyall emotional categories, particularly excelling inNeutral and Angry. While the Box + Numberprompts demonstrates moderate success in Happyand Surprise, it still falls short compared to SoVprompts in other emotion categories. This resulthighlights that after adding extra facial landmarks,the VLLMs can capture more nuanced and variedemotional states.",
  "Limitations": "We suggest the implementation of Set-of-Visionprompting to bridge visual and textual prompts.However, a challenge arises as it is difficult to pre-cisely describe visual prompts, such as the shape,location, or color of a bounding box and facial land-marks, in language. This issue might require encod-ing the visual prompts and fine-tuning the entiremodel for better accuracy. Moreover, this methodis computationally intensive, potentially limitingits scalability and practicality in real-time appli-cations, especially when handling large datasetsor streaming videos such as tracking one personsemotion in different video frames.",
  "Conclusion": "In conclusion, our Set-of-Vision prompting (SoV)approach significantly advances the field of emo-tion recognition within VLLMs by addressing criti-cal challenges in spatial localization and global con-text preservation. By leveraging spatial informationsuch as bounding boxes and facial landmarks, SoVenhances zero-shot emotion recognition accuracy,ensuring precise face count and emotion catego-rization. Our face overlap handling algorithm andcombined text-vision prompting strategy furtherrefine the recognition process, highlighting the effi-cacy of integrating visual prompts in VLLMs formore accurate and detailed emotion analysis. Thisapproach not only preserves the enriched imagecontext but also offers a solution for detailed andnuanced emotion recognition, underscoring its po-tential impact on various applications within com-puter vision and natural language processing. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. GPT-4 technical re-port. arXiv preprint arXiv:2303.08774.",
  "Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2024. Openai. ChatGPT-4o": "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in Neural Information ProcessingSystems, 33:18771901. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts, PaulBarham, Hyung Won Chung, Charles Sutton, Sebas-tian Gehrmann, et al. 2023. Palm: Scaling languagemodeling with pathways. Journal of Machine Learn-ing Research, 24(240):1113. Wenliang Dai, Junnan Li, Dongxu Li, AnthonyMeng Huat Tiong, Junqi Zhao, Weisheng Wang,Boyang Li, Pascale N Fung, and Steven Hoi.2024. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advancesin Neural Information Processing Systems, 36. Jiankang Deng, Jia Guo, Evangelos Ververas, IreneKotsia, and Stefanos Zafeiriou. 2020. Retinaface:Single-shot multi-level face localisation in the wild.In IEEE/CVF Conference on Computer Vision andPattern Recognition. Sidong Feng and Chunyang Chen. 2024.Prompt-ing is all you need: Automated android bug replaywith large language models. In Proceedings of theIEEE/ACM International Conference on SoftwareEngineering, pages 113.",
  "Sefik Serengil and Alper zpnar. 2024. A benchmarkof facial recognition pipelines and co-usability per-formances of modules. Bilisim Teknolojileri Dergisi,17(2):95107": "Aleksandar Shtedritski, Christian Rupprecht, and An-drea Vedaldi. 2023. What does clip know about ared circle? visual prompt engineering for vlms. InProceedings of the IEEE International Conferenceon Computer Vision, pages 1198711997. Hendrik Strobelt, Albert Webson, Victor Sanh, Ben-jamin Hoover, Johanna Beyer, Hanspeter Pfister, andAlexander M Rush. 2022.Interactive and visualprompt engineering for ad-hoc task adaptation withlarge language models. IEEE Transactions on Visu-alization and Computer Graphics. Sanjay Subramanian, William Merrill, Trevor Darrell,Matt Gardner, Sameer Singh, and Anna Rohrbach.2022. Reclip: A strong zero-shot baseline for refer-ring expression comprehension. In Proceedings ofthe Annual Meeting of the Association for Computa-tional Linguistics, pages 51985215. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro,Faisal Azhar, et al. 2023. Llama: Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971. Alexandros Xenos, Niki Maria Foteinopoulou, IoannaNtinou, Ioannis Patras, and Georgios Tzimiropoulos.2024. Vllms provide better context for emotion un-derstanding through common sense reasoning. arXivpreprint arXiv:2404.07078. Dingkang Yang, Zhaoyu Chen, Yuzheng Wang, ShunliWang, Mingcheng Li, Siao Liu, Xiao Zhao, ShuaiHuang, Zhiyan Dong, Peng Zhai, et al. 2023a. Con-text de-confounded emotion recognition.In Pro-ceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1900519015.",
  "Hang Zhang, Xin Li, and Lidong Bing. 2023a. Video-llama: An instruction-tuned audio-visual languagemodel for video understanding.arXiv preprintarXiv:2306.02858": "Sitao Zhang, Yimu Pan, and James Z Wang. 2023b.Learning emotion representations from verbal andnonverbal communication. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 1899319004. Qiji Zhou, Ruochen Zhou, Zike Hu, Panzhong Lu,Siyang Gao, and Yue Zhang. 2024. Image-of-thoughtprompting for visual reasoning refinement in mul-timodal large language models.arXiv preprintarXiv:2405.13872.",
  "A.1Dataset details": "In the dataset , the \"easy\" dataset includespictures with three or fewer faces. The \"medium\"dataset includes pictures with 3 to 7 faces. The\"hard\" dataset includes pictures with more than 7faces. Easy dataset contains 76 images with a to-tal of 174 faces. Medium dataset consists of 34images featuring 171 faces. Hard dataset is thesmallest set, comprising 9 images but still contain-ing a significant number of faces (114). The tablecategorizes the datasets based on the complexityand density of faces in the images, which likelyaffects the challenge level for the models emotionrecognition capabilities. The usage of SoV promptsacross all categories suggests a consistent testingapproach, aiming to evaluate how well the modelcan interpret and predict emotions without priorspecific training on these images (zero-shot learn-ing). The metrics, Accuracy and Recall, are chosento assess the models precision in correctly identi-fying emotions and its ability to retrieve relevantinstances across the datasets, respectively.: The table presents the dataset details usedfor testing a model on the task of zero-shot emotionrecognition, structured across three different levels ofdifficulty: Easy, Medium, and Hard.",
  "A.2Scene based emotion recognition": "The image in captures a moment from asports event, specifically a match between teamsfrom Australia and the Philippines, with Australialeading based on the scoreboard (\"PHI 0 - 1 AUS\").The scene includes various spectators and players,each showing distinct emotions which can aid inunderstanding the context and overall sentimentrelated to the ongoing game. Including scene con-text can significantly enhance the performance oflanguage and image models (LLMs) in recogniz-ing emotions. By understanding not just the facialexpressions but also the situational context (suchas the score in a sports game), models can makemore accurate inferences about the probable emo-tions being displayed. In addition, environmentalcues like scoreboards, team colors, and body lan-guage provide additional data points that help inaccurately deducing the emotional state of indi-viduals in group settings. Our SoV approach can bridge the gap between purely facial expression-based recognition and a more situation-aware un-derstanding, leading to more nuanced and accurateemotion recognition capabilities in VLLMs.In , the SoV method, by focusing onspecific individuals within the image and retainingthe clarity of the background context, offers a com-prehensive approach to emotion recognition. Thisallows for a detailed analysis of not just the visiblefacial expressions, but also the situational contextand interactions among individuals, providing amore nuanced understanding of the emotions beingconveyed. For example, in the case of Person 2,despite a partially visible face, the SoV approachleverages additional contextual cues to deduce theemotion more accurately. In contrast, the RedCir-cle approach primarily relies on clear facial expres-sions by blurring background details, potentiallyoverlooking subtler emotional cues embedded inthe environment and body language.",
  "A.3Chain of thought for emotion recognition": "The \"Chain of Thought\" approach, as illustrated in, offers advantages by breaking down thethought process into sequential, logical steps. Thismethod enhances accuracy in interpreting emotionsby considering various contextual clues such as ac-tivities, and body language, which are then system-atically analyzed. For example, the analysis firstidentifies the activity (Halloween trick-or-treating)and setting, which sets the emotional backdrop.Subsequently, it classifies each individual based onvisible attributes and costumes, leading to more nu-anced emotion recognition. This step-by-step rea-soning mirrors human cognitive processes, allow-ing for more refined and contextually appropriateinterpretations of emotions, such as distinguishingneutral expressions from smiles, even in a complexsocial setting like Halloween, where expressionsmight otherwise be ambiguous.",
  "User: Is the team with the red cloth girl winning or losing? What should I say to her? What's emotion for the girl label \"1\" and '2' and '3', and '4 ?": "GPT-4V + SoV: Answer: The scoreboard in the image shows \"PHI 0 - 1 AUS,\" indicating that the team associated with Australia (presumably the team in light blue) is winning, and the team associated with the Philippines (possibly the team the girl in red is supporting, if she is a fan or player of this team) is losing.If you wish to speak to the girl in red and she is supporting or part of the Philippine team, here are a few supportive things you could say:1. Encouraging her spirit: \"The game's not over yet! There's still time for a comeback.\"2. Showing empathy and support: \"It's a tough game, but you all are showing great spirit! Keep cheering!\"3. Acknowledging the effort: \"It's been a challenging match, but I admire the team's resilience. How are you holding up?\"These comments are supportive and considerate, acknowledging the situation while remaining positive and encouraging.Based on their expressions in the image:1. The girl labeled \"1\" seems focused and somewhat intense, likely reflecting her determination in the game given that her team is currently losing.2. The girl labeled \"2\" appears joyful and is smiling, which suggests she is happy, likely because her team is in the lead.3. The person labeled \"3\" is also smiling and appears to be enjoying the moment, which is consistent with a positive emotion like happiness or contentment.4. The person labelled \"4 (in the inset) has a neutral expression, which might suggest he is calm or composed, possibly observing the game with interest or concentration.",
  "Compare RedCircle and SoV prompts": "GPT-4V + SoV: Answer: In this image set within a home dining scene:1.Person 1 (woman in purple square): She shows a joyful and welcoming expression, likely enjoying the conversation and the meal.2.Person 2 (boy in red square): He looks attentive and possibly listening or thinking deeply about the conversation. His expression suggests engagementrather than emotional display.3.Person 3 (man in green square): He is smiling as he eats, reflecting happiness and satisfaction, perhaps with the meal and the company.4.Person 4 (girl in blue square): She also shows a happy demeanor, smiling broadly, indicating she is having a good time.Overall, the scene conveys a warm and cheerful family mealtime, where everyone seems to be enjoying both the food and each others company.",
  "User:Step 1: What are they planning to do?Step 2: How many people are there in the image?Step 3: Can you give me the related labels for these person?Step 4: What are their emotions?": "GPT-4V + SoV: Question 1: The group in the image seems to be participating in a Halloween trick-or-treating event. They are dressed in costumes and carrying buckets for collecting candy, which is a traditional activity where participants visit houses to ask for treats. The setting, with residential cars and houses in the background, further supports that they are likely walking through a neighborhood to gather sweets from different homes.Question 2: There are four people in the image.Question 3: In the image:1. Person 1 (on the far right) - labeled as \"1\", wearing a black shirt with white skeleton bones printed on it and carrying an orange pumpkin bucket.2. Person 2 (second from the right) - labeled as \"2\", dressed in casual clothes, holding documents and an orange bucket.3. Person 3 (second from the left) - labeled as \"3\", dressed in a black tutu and carrying an orange bucket.4. Person 4 (on the far left) - labeled as \"4\", also dressed in a black tutu, but with a blue bucket instead.Each label corresponds to the numbered box drawn over their faces in the image.Question 4: From the visible expressions in the image:1. Person 1 (on the far right): Appears to have a neutral expression.2. Person 2 (second from the right): Their face is not visible in the image.3. Person 3 (second from the left): Seems to be smiling, indicating happiness or cheerfulness.4. Person 4 (on the far left): Appears to have a neutral expression.These are the inferred emotions based on the limited facial expressions visible in the image.",
  "Detect negative emotions for group of people": "GPT-4V + SoV: If Sally mentioned seeing a very sad news video that deeply affected her, it's likely she referred to the image depicting people in a scene of destruction or after a disaster. This would be the first image of the three presented in your query, where individuals are seen amidst rubble and damage. Such scenes typically involve serious issues like natural disasters, conflicts, or other tragic events that can have a profound emotional impact on viewers. This type of content aligns with a description of a \"very sad news video.\"The other two images show more casual or celebratory scenarios that are less likely to be described as deeply affecting in a sad context.",
  "User:What are the face emotion of the people in the first image?": "GPT-4V + SoV: In the image where individuals are amidst rubble, the emotions of the people can be described as follows:1.Person 1: Appears to be in distress or discomfort, potentially indicative of shock or sadness.2.Person 2: Looks serious and concerned, likely reflecting the gravity of the situation.3.Person 3: His expression is hard to discern fully due to the angle, but he seems to be focused and possibly worried.4.Person 4: Shows a pensive or somber expression, which might suggest contemplation or distress.5.Person 5: Seems to have a neutral or tired expression, which might reflect fatigue or resignation in the face of the situation.Overall, the expressions suggest a mixture of concern, sadness, and seriousness, aligning with a response to a serious or tragic event.",
  "Grounding ability": "The person labeled \"2\"in the image appears to be smiling and engaging positivelywith the interviewer, which suggests that he is feeling happy or content during the interview. His overall demeanor and facial expression convey a sense of enjoyment or satisfaction, likely related to the context of the sports event. In the image you provided from a sporting event, the person labeled \"2\" would be the most appropriate individual for a reporter to ask questions about the game. This person is likely a coach or player involved in the game, given that he is in uniform and holding a child who is also in a similar uniform. This suggests he is directly related to the team and would have insights about the game's specifics."
}