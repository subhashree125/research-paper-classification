{
  "Abstract": "Despite recent advances in LLM quantization,activation quantization remains to be challeng-ing due to the activation outliers. Conventionalremedies, e.g., mixing precisions for differentchannels, introduce extra overhead and reducethe speedup. In this work, we develop a sim-ple yet effective strategy to facilitate per-tensoractivation quantization by preventing the gen-eration of problematic tokens. Precisely, wepropose a method to nd a set of key-valuecache, coined CushionCache, which mitigatesoutliers in subsequent tokens when inserted asa prex. CushionCache works in two steps:First, we greedily search for a prompt tokensequence that minimizes the maximum activa-tion values in subsequent tokens. Then, wefurther tune the token cache to regularize theactivations of subsequent tokens to be morequantization-friendly.The proposed methodsuccessfully addresses activation outliers ofLLMs, providing a substantial performanceboost for per-tensor activation quantizationmethods. We thoroughly evaluate our methodover a wide range of models and benchmarksand nd that it signicantly surpasses the es-tablished baseline of per-tensor W8A8 quanti-zation and can be seamlessly integrated withthe recent activation quantization method.",
  "Introduction": "Tremendous capabilities of large language models(LLMs) come with a tremendous computationalcost. Modern language models often have overhundreds of billions of parameters, requiring sig-nicant memory and computation for predictionand training. For instance, OPT-175B (Zhang et al.,2022), one of the most popular open-sourced lan-guage models, requires at least 350GB of memoryand the order of 1018 oating point operations togenerate a new token1 (Hoffmann et al., 2022).",
  "*Work done during internship at Google.Contact: the context length 2048": "Quantization is an effective strategy to reducethe computational cost of LLMs. Recent worksdemonstrate that the precision of LLM weight pa-rameters can be greatly reduced by post-trainingquantization (PTQ), with minimal degradations inits generation quality. For example, Huang et al.(2024) shows that one can quantize the weights ofthe LLaMA3-70B to 4 bits, with less than 0.5%pdrop in its zero-shot prediction accuracy. Roughly,the reduced precision translates into 4 increase inthe generation throughput, and similar reduction inmemory requirements (Lin et al., 2024).LLM activations, however, remain challengingto be quantized. The key obstacle is the activationoutlier, i.e., a small number of activations that aresubstantially larger than others (Bondarenko et al.,2021; Dettmers et al., 2022; Sun et al., 2024). Suchoutliers elongate the quantization range and attensout most non-outlier activations, leading to largeperformance losses even at W8A8 quantization.To address this issue, recent works propose tomitigate outliers based on various relaxations ofthe stringent static, per-tensor quantization. Oneline of work applies quantization separately to eachchannel depending on the outlier proneness (Bon-darenko et al., 2021; Dettmers et al., 2022). Thesemethods, however, are difcult to be implementedon conventional hardwares. Another line of workreparameterizes the activations and weights in away that the impact of outliers are amortized (Xiaoet al., 2023; Ashkboos et al., 2024). These algo-rithms focuses on attaining high generative qual-ity by adopting per-token or dynamic quantization,leaving the most hardware-friendly optionstaticper-tensor quantizationless explored.To ll this gap, we take a novel approach for mit-igating activation outliers in LLMs. In particular,we focus on answering the following key question:",
  "Can we nd a good prex that mitigatesthe activation outliers in the subsequent": ": Activation magnitudes in LLaMA2-7B, before and after CushionCache. CushionCache mitigatesthe activation outliers in LLMs by inserting and tuning the several prex tokens to the model, which acts as anattention sink. Adding such sink tokens alleviates outliers in the subsequent tokens and enables a better activationquantization of the model with coarse quantization granularities.",
  "tokens on a pretrained LLM?": "Our answer is positive; we develop a very sim-ple yet effective method, coined CushionCache, todiscover a prex2 which reduces the outlier in thefollowing tokens processed by the given LLM. Byinserting this prex, one then can quantize the acti-vations of the LLM with much smaller quantizationerror, leading to an improved generation quality.To design our method, we draw inspirations froma recent observation that the outliers may originatefrom attention sinks (Bondarenko et al., 2023)theno-operation tokens that receive much attentionfrom other tokens (Xiao et al., 2024). By addingsink-like tokens as a prex, one may be able toseparate out outlier activations as well, renderingthe subsequent tokens outlier-free. In a nutshell,our method works in two steps.1. Greedy initialization. We search for a sequenceof sink-like prompt tokens in a greedy manner,so that the activations of the subsequent tokensare less prone to outliers (.1). 2. Quantization-aware prex tuning. We train thegreedily initialized prex further to minimizethe combined loss of the prediction loss andquantization error (.2).Our experiments demonstrate that the proposedCushionCache is highly effective in making LLMsmore quantizable. The technique is versatile, con-sistently improving the quantized performance ofLLMs under various scenarios, from per-token toper-tensor static quantization. The method can also",
  "Related Work": "Outliers in LLMs.The fact that there exists usu-ally large entries in LLM activations, or outliers,has been reported by multiple works. Kovalevaet al. (2021) and Timkey and van Schijndel (2021)report the existence of outliers in large transformer-based language models (e.g., BERT), and nd thatthey appear mostly in a small number of chan-nels and layers. Bondarenko et al. (2021) makea similar observation in the context of quantiza-tion, and nds that quantizing the activations leadto a large degradation in generation quality; thework also reports that semantically meaninglesstokens can have higher tendencies to have outliers. Dettmers et al. (2022) conrm the same ndingwhile quantizing GPT-scale models, and studieshow the model scale affects the prevalence of out-liers over tokens and layers. More recently, Sunet al. (2024) investigates a similar phenomenonin newer LLM variants and conrms that certaintokens are more likely to suffer from outliers. Per-channel activation quantization.A line ofwork proposes to mitigate outliers in LLM acti-vation quantization by applying different scalingfactors or precision to each channel. Bondarenkoet al. (2021) splits activation channels into severalgroups and perform quantization on each group.LLM.int8() (Dettmers et al., 2022) applies higherprecision (e.g., FP16) to a small number of outlier-prone channels, while quantizing the other chan-nels to lower bits (e.g., INT8). These works, how-ever, are difcult to be implemented in conven-tional hardwares, as they requires scaling along thecontracting dimension of matrix multiplication. Per-token, with reparameterization.Anotherline of work proposes to quantize the activationsper-token to reduce the impact of outliers with bet-ter hardware acceleration. Many of these worksadopt reparameterization of weights to mitigatethe outliers further. ZeroQuant (Yao et al., 2022)applies per-tensor quantization and knowledge dis-tillation to achieve reasonable INT8 quantizationperformance. SmoothQuant (Xiao et al., 2023),Outlier Suppression+ (Wei et al., 2023), and Om-niQuant (Shao et al., 2024) migrates the activationmagnitudes to the weights to normalize the scalesof the activations. More recently, QuaRot (Ashk-boos et al., 2024) rotates the activations so thatthe outlier magnitudes are distributed over multi-ple axes in the reparametrized space. While thesemethods are effective, per-token quantization aretypically slower than per-tensor quantization at thesame quantization precision as it requires largerscale which has a size of the number of tokens.",
  "Attention sinks and outliers.Recent works re-port an intriguing phenomenon in large transform-": "ers, termed attention sink. Xiao et al. (2024) ndthat a small number of semantically meaninglesstokens, usually at the beginning of the sequence,tend to receive unusually large attention. Darcetet al. (2024) make a similar observation for vi-sion transformers, and show that training ViTs withadditional meaningless tokens can help make theattention structures more semantically meaning-ful. Bondarenko et al. (2023) hypothesize that thesink tokens may be the root cause of the activationoutliers, and propose a new architecture that pre-vents the outliers from emerging when pretrainedfrom scratch. Our work shares a similar intuition,but critically differs in that we mitigate outliersby ne-tuning the pretrained LLM. This means nomodication to the network architecture is neededand does not need to train the model from scratch.",
  "tn+1 = f(t1, t2, . . . , tn)(1)": "where t1, , tn are the preceding tokens used ascontext. LLM iteratively applies Eq. (1) autoregres-sively to generate text as a sequence of tokens.As the context length grows, the computationalcost to process all previous tokens also grows larger,slowing down the generation signicantly. A pop-ular solution is to cache and reuse the keys andvalues of the preceding tokens computed duringthe previous iteration. This trick relies on the factthat preceding tokens affect the outcome of thecurrent token only through their keys and values:",
  "tn+l+1 = f(tn+1:n+l | k1:n, v1:n),(2)": "where tn+1:n+l denotes l tokens given at the cur-rent step and k1:n, v1:n are the keys and values ofthe preceding context of length n, computed duringthe previous iteration. During the prell phase, lmay be the length of the prompt, and during the de-coding phase, we can simply use l = 1, processingonly a single token at a time. Quantization.Quantization is an act of castinga high-precision tensor (typically FP) into a lower-precision tensor (typically INT), to save the mem-ory to store and computation to process the tensor.In neural network quantization, a popular choice isthe linear quantization, which performs",
  "WfpXfp sWsX WintXint,(5)": "where the right-hand side can be computed usingan integer matrix multiplication, and a single multi-plication of FP16/32 quantities (for scaling factors).The combined scaling factor need not be multipliedback to the matrix immediately, and can be used inthe subsequent operations directly.In many cases, the scaling factors sW, sX canbe pre-computed based on the validation set statis-tics. This method, called static-range quantization,enables more acceleration than computing thesevalues dynamically during the inference. Outliers and complications.In LLMs, the ac-tivation Xfp tends to have a very large entry(Dettmers et al., 2022; Sun et al., 2024). In suchcase, the magnitude of max(Xfp) and min(Xfp)will be very large, making the scaling factor sX very large. This leads to a high sparsity in the ten-sor Xint, and a much degraded generation quality.This problem can be alleviated in various ways:One can change the scaling factor dynamically overtime (i.e., per-tensor dynamic quantization), or ap-ply different scaling factors for each channel ortoken (i.e., per-channel/token quantization). Asthese methods require on-the-y computations ofscaling factors, the methods are typically slower. Granularity and the communication cost.Thedrawback of ner quantization granularity becomesmore signicant in the distributed setup, as it affectthe communication cost between nodes.To see this, consider the case of multiplying ma-trices with tensor parallelism, e.g., Megatron-LM(Shoeybi et al., 2019). Comparing with the per-tensor static quantization, per-tensor dynamic quan-tization requires an additional AllReduce operationover the nodes to aggregate the (high-precision)scaling factor. The overhead is even more signif-icant for per-token dynamic quantization, as thenumber of scaling factors is multiplied by the num-ber of tokens, increasing the cost of AllReduce.",
  "Method": "We now describe CushionCache, an algorithm tond a prex which can mitigate activation outliersin the subsequent tokens, thereby alleviating thequality degradation from activation quantization.CushionCache aims to nd a set of prex thatminimizes the quantization error of the activations.More concretely, let Xi denote activation of a trans-former block for the input token ti. Our goal is tominimize the squared difference between the origi-nal and the quantized activations, i.e.,",
  "i=1Xi q(Xi)22,(6)": "where q() denotes the quantization function, spec-ied as q(X) = s round((X z)/s) + z. Inpractice, we consider the summation of the er-ror Lq of all transformer blocks, but we omit thisfor the notational simplicity. Similarly, we deneLq(t1:n|p1:m) as the sum of squared error for t1:ngiven the prex p1:n, where the scaling factor s andzero-point z are determined for t1:n only.We hypothesize that there exist prex tokens pthat can reduce the expected activation quantizationerror of the tokens. That is, we nd",
  "Greedy Prex Search": "We carefully initialize the prex as the prex tun-ing is known to be very sensitive to initial values.We follow Li and Liang (2021) to search for theprex that are activations of hard prompt tokens,i.e., input tokens that correspond to real text. Asthe search complexity grows exponentially withrespect to the embedding size, we propose to use agreedy search algorithm with tailored heuristics.In a nutshell, our method is a greedy search withearly stopping. We add new tokens to the promptone-by-one, selected to minimize the quantizationerror. If the new token does not decrease the errormuch, we stop adding to prevent overtting andcomputational overhead from long prompts.Concretely, at each step, we rst draw a singlesample text t1:n from the dataset; we use the C4dataset (Raffel et al., 2020), which is commonlyused for calibration or validation purposes, to drawa sentence of length n = 512. Then, based on thecurrent state of prompts p1:k, we search for the nextprompt token pk+1 by solving",
  ": return p": "Note that this algorithmic design provides someexibility. More specically, one can initialize theprompt with nonempty sequence before the search,as a heuristic that can help speed up the promptsearch procedure. We nd that lling in nonseman-tic words, e.g., <bos> or \\n, is particularly useful;this observation is well-aligned with the ndingsof Bondarenko et al. (2021); Sun et al. (2024).",
  "L = Lpred + Lq(11)": "where Lpred is the cross entropy loss for the next-token prediction and is a hyperparameter thatbalances two losses. Here, we apply stop-grad toscaling factors and zero-points of the quantizationfunction, as is typical in quantization-aware train-ing literature (Jacob et al., 2018).By optimizing this loss function, we ensure thatthe CushionCache not only improves the predictionaccuracy but also minimizes the quantization error.This tuning does not require excessive amount ofmemory, as we only train the prex.",
  ": Average zero-shot accuracies of W8A8-quantized LLMs. We average over LAMBADA, HellaSwag,PIQA, WinoGrande, OpenBookQA, RTE, and COPA. Green is the accuracy gain and red is the drop": "Datasets.We measure the perplexity on the held-out set of WikiText-2 validation dataset (Merityet al., 2016). For zero-shot evaluation, we use seventasks from the LM evaluation harness benchmarkby EleutherAI (Gao et al., 2023). Precisely, weuse LAMBADA, HellaSwag, PIQA, WinoGrande,OpenBookQA, RTE, and COPA datasets. Base algorithms.We apply CushionCache ontwo base activation quantization algorithms: Naveactivation quantization and SmoothQuant (Xiaoet al., 2024). We consider three different scenar-ios: Per-tensor static, per-tensor dynamic, and per-token dynamic quantization. Note that for eachcase, the SmoothQuant has a corresponding ver-sion, called O3, O2, and O1, respectively. Conguration: Quantization.We mostly fol-low the setup of Li et al. (2024) and the TensorFlowdefault. We use symmetric group-wise quantizationfor model weights, and asymmetric quantizationfor the activations. For SmoothQuant, we use themigration strength = 0.8, which worked consis-",
  "Main Results: W8A8 Quantization": "In Tables 1 and 2, we provide the performanceachieved by the quantized language models, quan-tized with and without the proposed CushionCache.We report the WikiText perplexity and zero-shotaccuracy in the tables, respectively.For per-tensor static range quantization, Cush-ionCache successfully improves the performanceof the model; the boost is quite substantial inLLaMA and Mistral, often providing over 30%pgains in terms of zero-shot accuracies. Intriguingly,the gain is much more pronounced in LLaMA-stylemodels, which adopt the pre-LayerNorm and gatedlinear units. For per-tensor dynamic range quantiza-tion, similarly, we make consistent improvements",
  "Ablation Study": "In , we sequentially add our key algorithmiccomponents to validate their efcacy. In particu-lar, the components are (1) greedy-searched initialvalue, (2) prex tuning, and (3) the quantization-error-based regularizer.We observe that each component makes nontriv-ial contributions for achieving near-FP16 zero-shotaccuracy. Interestingly, we nd that the greedy-searched initialization is especially effective, con-tributing 91% of the accuracy gain. This sug-gests that our search mechanism can be used asa compute-light standalone method in the caseswhere it is difcult to conduct prex-tuning, due toa limited on-device memory.",
  "Change of Activation Magnitudes": "In , we report various order statistics of theactivation magnitudes that appear in LLaMA2/3and Mistral. In particular, we focus on the input ac-tivations to the last transformer block of these mod-els, and measure the top-1, top 10%, and median(i.e., top 50%) activation magnitude. We averageover ten samples, with a sequence length 4096.The effect of CushionCache is quite dramatic.In particular, we nd that the CushionCache canreduce the scale of the activation outlier to 1-2%of the previous value. The ratio between the top-1and the median decreases from roughly 10,000:1 to100:1. We also note that the other order statistics,i.e., top 10% and median, remains roughly the samebefore and after the CushionCache. : Top-1/2/3 and median activation magnitudes at each layer of LLaMA3-8B. The left panel shows theactivations without CushionCache, having signicant outliers except for initial layers. The right panel shows theactivation with CushionCache, having signicantly reduced outliers in every layers. : Attention patterns before and after applying CushionCache in LLaMA3-8B and Mistral-7B. Therst and third panels show the attention patterns in models without CushionCache, where the attention sinks arequite prevalent in the generated token sequence. The second and fourth panels illustrate the attention patterns afterinserting CushionCache. By adding the CushionCache, the attention is redirected toward the CushionCache tokens,preventing the attention sink from arising in the subsequent tokens. In , we visualize the top-1/2/3 activationsand median for each layer of LLaMA3-8B. The leftpanel plots the magnitude of the median and top-3activations that occur during the standard operationof LLaMA3-8B. We observe that the median is al-most zero, indicating that a signicant fraction ofall activations are close to zero, with only a few sig-nicantly large outliers. On the right panel, we plotthe same values after applying the proposed Cush-ionCache algorithm. We observe that the size of thetop-3 activations have dramatically decreased, lead-ing to a conclusion that CushionCache effectivelyremoves the activation outliers.",
  "Attention on CushionCache": "In , we visualize the attention patterns ofLLaMA2 and Mistral, before and after applyingthe CushionCache. Attention sinks, as identied byXiao et al. (2024); Sun et al. (2024), are tokens thatdisproportionately attract attention. By insertingCushionCache, we observe that the CushionCachetends to dominate most of the attention from othertokens, removing the sinks in other tokens.",
  "Time Needed to Search CushionCache": "In , we report the wall-clock time spent forperforming the greedy search and prex tuning ofCushionCache. We observe that the greedy prexsearch can be quite time-consuming, highly depen-dent on the side of the embedding table; LLaMA3-8B has a large embedding table. Another obser-vation is that the quantization-aware prex tuningstep takes relatively small time for all models.",
  "Conclusion": "In this paper, we present CushionCache, a novel ap-proach for mitigating activation outliers in LLMsto improve activation quantization performance.Through extensive experiments, we demonstratethat CushionCache consistently enhances the per- formance of per-tensor activation quantization. Ouranalysis shows that CushionCache effectively re-duces the magnitude of activation outliers andredirects attention sinks, leading to more uniformand quantization-friendly activations. In contrastwith other approaches to faciliate activation quan-tization, CushionCache is the rstup to ourknowledgeto fundamentally alter the activationdistribution itself without extensive training, mak-ing activations easier to quantize.",
  "Limitations": "A limitation of our study is that our method is de-signed for LLMs with the decoder-only transformerstructure. An extension to encoder-decoder LLMs(Raffel et al., 2020) may require further modi-cations to the algorithm. Another limitation is thelack of a principled mechanism to determine the hy-perparameter , which decides when to stop addingnew tokens. An extensive tuning may incur a non-negligible computational cost, especially when thetarget model is extremely large.",
  "Acknowledgements": "This work was supported in part by the NationalResearch Foundation of Korea (NRF) grant fundedby the Korea government (MSIT) (No. RS-2023-00213710), and in part by the Korea government(MSIT) (No. RS-2024-00453301). JL also thanksHong-Seok Kim and Radha Chandika for their gen-erous support during his visit at Google. Saleh Ashkboos, Amirkeivan Mohtashami, Maxim-ilian L Croci, Bo Li, Martin Jaggi, Dan Alis-tarh, Torsten Hoeer, and James Hensman. 2024.QuaRot:Outlier-free 4-bit inference in rotatedLLMs. arXiv preprint 2404.00456. Yelysei Bondarenko,Markus Nagel,and TijmenBlankevoort. 2021. Understanding and overcomingthe challenges of efcient transformer quantization.In Conference on Empirical Methods in Natural Lan-guage Processing.",
  "Tim Dettmers, Mike Lewis, Younes Belkada, and LukeZettlemoyer. 2022. LLM.int8(): 8-bit matrix multi-plication for transformers at scale. In Advances inNeural Information Processing Systems": "Leo Gao, Jonathan Tow, Baber Abbasi, Stella Bider-man, Sid Black, Anthony DiPo, Charles Foster,Laurence Golding, Jeffrey Hsu, Alain Le Noach,Haonan Li, Kyle McDonell, Niklas Muennighoff,Chris Ociepa, Jason Phang, Laria Reynolds, HaileySchoelkopf, Aviya Skowron, Lintang Sutawika, EricTang, Anish Thite, Ben Wang, Kevin Wang, andAndy Zou. 2023.A framework for few-shot lan-guage model evaluation. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,Elena Buchatskaya, Trevor Cai, Eliza Rutherford,Diego de Las Casas, Lisa Anne Hendricks, JohannesWelbl, Aidan Clark, Tom Hennigan, Eric Noland,Katie Millican, George van den Driessche, BogdanDamoc, Aurelia Guy, Simon Osindero, Karen Si-monyan, Erich Elsen, Oriol Vinyals, Jack W. Rae,and Laurent Sifre. 2022.An empirical analysisof compute-optimal large language model training.In Advances in Neural Information Processing Sys-tems. Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng,Chengtao Lv, Hong Chen, Jie Luo, Xiaojuan Qi, Xi-anglong Liu, and Michele Magno. 2024. How goodare low-bit quantized LLaMA3 models? an empiri-cal study. arXiv preprint 2404.14047. Benoit Jacob, Skirmantas Kligys, Bo Chen, Meng-long Zhu, Matthew Tang, Andrew Howard, HartwigAdam, and Dmitry Kalenichenko. 2018.Quanti-zation and training of neural networks for efcientinteger-arithmetic-only inference.In IEEE/CVFConference on Computer Vision and Pattern Recog-nition. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel,Guillaume Lample, Lucile Saulnier, et al. 2023.Mistral 7B. arXiv preprint 2310.06825. Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers,and Anna Rumshisky. 2021. BERT busters: Outlierdimensions that disrupt transformers. In Findings ofthe Association for Computational Linguistics: ACL-IJCNLP 2021. Teven Le Scao, Angela Fan, Christopher Akiki, El-lie Pavlick,Suzana Ilic,Daniel Hesslow,Ro-man Castagn, Alexandra Sasha Luccioni, FranoisYvon, et al. 2022.BLOOM: A 176B-parameteropen-access multilingual language model.arXivpreprint 2211.05100.",
  "Xiang Lisa Li and Percy Liang. 2021. Prex-tuning:Optimizing continuous prompts for generation. InProceedings of the Annual Meeting of the Associa-tion for Computational Linguistics": "Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang,Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao,Xingyu Dang, Chuang Gan, and Song Han. 2024.AWQ: Activation-aware weight quantization for on-device LLM compression and acceleration. In Con-ference on Machine Learning and Systems. Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong,Zhaozhuo Xu, Vladimir Braverman, Beidi Chen,and Xia Hu. 2024. Kivi: A tuning-free asymmetric2bit quantization for KV cache. In Proceedings ofthe International Conference on Machine Learning.",
  "Stephen Merity, Caiming Xiong, James Bradbury, andRichard Socher. 2016. Pointer sentinel mixture mod-els. arXiv preprint 1609.07843": "Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J Liu. 2020. Exploring the limitsof transfer learning with a unied text-to-text trans-former. Journal of Machine Learning Research. Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, PengXu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, PengGao, Yu Qiao, and Ping Luo. 2024.OmniQuant:Omnidirectionally calibrated quantization for largelanguage models.In International Conference onLearning Representations. Mohammad Shoeybi, Mostofa Patwary, Raul Puri,Patrick LeGresley, Jared Casper, and Bryan Catan-zaro. 2019.Megatron-LM: Training multi-billionparameter language models using model parallelism.arXiv preprint 1909.08053.",
  "Mingjie Sun, Xinlei Chen, J Zico Kolter, and ZhuangLiu. 2024.Massive activations in large languagemodels. arXiv preprint 2402.17762": "William Timkey and Marten van Schijndel. 2021. Allbark and no bite: Rogue dimensions in transformerlanguage models obscure representational quality.In Conference on Empirical Methods in Natural Lan-guage Processing. Hugo Touvron, Louis Martin, Kevin Stone, PeterAlbert, Amjad Almahairi, Yasmine Babaei, Niko-lay Bashlykov, Soumya Batra, Prajjwal Bhargava,Shruti Bhosale, et al. 2023. Llama 2: Open foun-dation and ne-tuned chat models. arXiv preprint2307.09288. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. In Advances in Neural Information Pro-cessing Systems. Xiuying Wei, Yunchen Zhang, Yuhang Li, XiangguoZhang, Ruihao Gong, Jinyang Guo, and XianglongLiu. 2023. Outlier suppression+: Accurate quanti-zation of large language models by equivalent andeffective shifting and scaling. In Conference on Em-pirical Methods in Natural Language Processing. Guangxuan Xiao,Ji Lin,Mickael Seznec,HaoWu,JulienDemouth,andSongHan.2023.SmoothQuant: Accurate and efcient post-trainingquantization for large language models. In Proceed-ings of the International Conference on MachineLearning.",
  "Guangxuan Xiao, Yuandong Tian, Beidi Chen, SongHan, and Mike Lewis. 2024. Efcient streaming lan-guage models with attention sinks. In InternationalConference on Learning Representations": "Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang,Xiaoxia Wu, Conglong Li, and Yuxiong He. 2022.ZeroQuant: Efcient and affordable post-trainingquantization for large-scale transformers.In Ad-vances in Neural Information Processing Systems. Susan Zhang, Stephen Roller, Naman Goyal, MikelArtetxe, Moya Chen, Shuohui Chen, Christopher De-wan, Mona Diab, Xian Li, Xi Victoria Lin, et al.2022. OPT: Open pre-trained transformer languagemodels. arXiv preprint 2205.01068.",
  "A.2Generation latency": "We have measured the average latency of generat-ing each token. We have experimented with W8A8-quantized LLaMA-3B, using the SmoothQuant ker-nel on a single A6000 GPU; not that this may notbe the best hardware-optimized kernel for our hard-ware, but can be meaningful in terms of providinga comparison. We have used the prompts of length500, and averaged over 1000 generated tokens. Wecompare both the time to the rst token (TTFT;prell phase) and the time per output token (TPOT;generation phase). in .We observe that CushionCache only adds negli-gible latency, while enabling a much better adop-tion of the per-tensor static-range quantization tech-niques which provides a much faster decoding. Inparticular, we observe that adding CushionCacheadds only 0.010.3ms in TTFT or TPOT, which isless than 0.5% of the total latency. Furthermore,as the CushionCache makes the faster option (e.g.,per-tensor static) a viable option, it can even beviewed as enabling an overall speedup up to a fewmilliseconds.",
  "QuaRot-4bit (Ashkboos et al., 2024): A recentweight+activation+cache quantization algorithm,for demonstrating that CushionCache works wellwith SOTA quantization algorithms": "KIVI-2bit (Liu et al., 2024): A recent KV cachequantization algorithm, for demonstrating thatthe KV cache of the CushionCache-quantizedmodel can be compressed well with KV cachequantization methods.The results are given in , where we observethat the CushionCache is indeed versatile, beingable to be combined well with many different quan-tization methods. Note that for KIVI, we measurethe GSM8K results, as the original paper does notreport perplexity."
}