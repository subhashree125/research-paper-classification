{
  "Abstract": "Recently, Large Language Models (LLMs)and Vision Language Models (VLMs) havedemonstrated aptitude as potential substitutesfor human participants in experiments testingpsycholinguistic phenomena.However, anunderstudied question is to what extent modelsthat only have access to vision and text modalitiesare able to implicitly understand sound-basedphenomena via abstract reasoning from orthog-raphy and imagery alone. To investigate this,we analyse the ability of VLMs and LLMs todemonstrate sound symbolism (i.e., to recognise anon-arbitrary link between sounds and concepts)as well as their ability to hear via the interplayof the language and vision modules of open andclosed-source multimodal models. We performmultiple experiments, including replicating theclassic Kiki-Bouba and Mil-Mal shape and mag-nitude symbolism tasks and comparing humanjudgements of linguistic iconicity with that ofLLMs. Our results show that VLMs demonstratevarying levels of agreement with human labels,and more task information may be requiredfor VLMs versus their human counterparts forin silico experimentation. We additionally seethrough higher maximum agreement levels thatMagnitude Symbolism is an easier pattern forVLMs to identify than Shape Symbolism, andthat an understanding of linguistic iconicity ishighly dependent on model size.",
  "Introduction": "Sound symbolism refers to a perceived similaritybetween speech sounds and the conceptual meaningsof the words they comprise. Evidence of this can befound in linguistic devices such as onomatopoeia (e.g.,bang, shriek, and bellow), where a word imitatesthe concept it describes via its phonetic form. Theability of Large Language Models (LLMs) and VisionLanguage Models (VLMs) to reflect a sense of soundsymbolism would therefore suggest that these models",
  "*Corresponding author": ": Illustration of the 3 main experiments weperform. Firstly, Shape Symbolism is a binary choicebetween two pseudowords to best describe an object that isspiky or rounded. Magnitude Symbolism involves a binarychoice between two pseudowords to best describe an objectthat is small or large. Finally, Iconicity involves ratingthe perceived iconicity of words, or how much their writ-ten/phonetic form is representative of what they describe. are capable of acquiring phonetic knowledgeindirectly through only the written orthographic formof a language via patterns of grapheme combinations(Loakman et al., 2024, 2023) and meta-level textualdiscussion of sound in training data, which has im-plications for the potential future use of LLM/VLMsin perceptual studies usually reserved for humans(Jain et al., 2023; Dillion et al., 2023; Aher et al.,2023). In this work, we further explore the capabilityof LLMs/VLMs to demonstrate human-like char-acteristics in a range of psycholinguistic perceptual tests investigating 3 main areas of sound symbolism:(1) Shape Symbolism (i.e., the Kiki-Bouba effect,Ramachandran and Hubbard, 2001), where a forcedchoice must be made between two pseudowords asto which is the most appropriate to describe shapesand entities that are spiky or rounded; (2) MagnitudeSymbolism (i.e., the Mil-Mal Effect, Sapir, 1929), asimilar test to (1), but where the entities are small orlarge (rather than spiky or rounded); and (3) IconicityRating (Winter et al., 2023), where LLMs are askedto rate a series of English words on their perceivediconicity (i.e., to what extent a words form isperceived to be analogous to the concept or entity itdescribes). We illustrate these experiments in .By extending these experiments to LLMs/VLMs,our study aims to shed light on the processes under-lying multimodal perception in language models.1 Moreover, the presence of sound symbolism in suchmodels could inform the development of more effec-tive natural language processing algorithms, aidingtasks such as sentiment analysis, emotion recognition,and content generation that take into account more ab-stract layers of human reasoning and perception suchas abstract connotations between words rather than se-mantics alone (Manzoor et al., 2023). An understand-ing of sound symbolism also has the potential to havea profound effect on creative generation, including lan-guage forms such as poetry and narratives (and theiraccompanying illustrations). Additionally, sound sym-bolism is a prevalent strategy used in marketing prod-ucts to create desirable associations in potential cus-tomers, and LLMs capable of understanding this phe-nomenon could be used as pilot testing before using fo-cus groups to reduce time and monetary costs (Ketronand Spears, 2021; Motoki et al., 2020; Spence, 2012).We summarise our main contributions as follows:2 We perform replications of the classic psy-cholinguistic Kiki-Bouba Shape Symbolism andMil-Mal Magnitude Symbolism studies witha range of open and closed-source VLMs toinvestigate if they understand the associationbetween speech sounds/orthographic forms andthe characteristics of entities.",
  "Related Works": "The work of early linguists, such as Saussure, touchedupon the topic of whether or not the link betweenthe \"sign\" (i.e., a word) and the \"signified\" (i.e., theentity/concept to which the sign relates) is arbitrary,with there being nothing more boat-esque aboutthe word boat than any other combination ofphonotactically legal sounds (de Saussure and Baskin,2011). However, there are many types of languagewhere this association is seen to be non-arbitrary suchas in the onomatopoeia commonly used in literaryworks (e.g., bang for a loud noise, or shriek fora high-pitched wail), where the phonetic realisationmirrors the concept it describes. These phenomenaas a whole are known as sound symbolism, wherethere is thought to be a non-arbitrary link betweenthe sign and the signified, in contrast with the popularstance of early linguistics.Outside of onomatopoeia, sound symbolism is be-lieved to have a range of effects on human perception,including applying to nonsense pseudowords. Forexample, even if a word was not created to explicitlydenote a known concept or entity (and therefore hasno true denotative meaning), it is nevertheless ableto manifest a connotative meaning in the mind of thereader based on its phonological representation and/orphonetic realisation. The first identification of thesepatterns is frequently attributed to Usnadze (1924),who gave 10 participants a series of pseudowordsalongside drawings and found a higher-than-chancelevel of agreement between evaluators for whichnonsense word best described which drawing.Perhaps the most famous example of this is in theKiki-Bouba effect (Sidhu et al., 2021; Ramachandranand Hubbard, 2001; Khler, 1929) which concernsthe allocation of the name Kiki to sharp, hard-edgedentities, and Bouba to more soft and round-edgedentities (which in the original works consistedexclusively of 2D shapes). A similar relationship hasalso been noted between the words Mil and Mal,where the changing vowel in the phonological min- imal pairs3 has a relationship to perceived size, in aphenomenon known as magnitude symbolism (wherevowels with higher frequency content are associatedwith smaller entities due to the relationship betweenvocal tract length and vocal productions) (Sapir,1929). Extensive research has been performed in thearea of sound symbolism, demonstrating interestingfindings such as these patterns being largely languageagnostic (Cwiek et al., 2022) as well as being weakerin neurodivergent individuals (Occelli et al., 2013)and not being yet developed in very early childhood(Sidhu et al., 2023). Other research has also investi-gated the exact requirements and limits of the effects(Sidhu and Vigliocco, 2023; Passi and Arun, 2022;Styles and Gawne, 2017; Nielsen and Rendall, 2013).Recently, Alper and Averbuch-Elor (2023) haveinvestigated the ability of language models toexhibit the Kiki/Bouba effect using CLIP and StableDiffusion by generating images from sound-symbolicprompts and provide positive evidence for thisassociation to be present. We build upon this work in3 by introducing a wider range of VLMs in a forcednaming task for real entities as opposed to abstractshapes, and additionally extend this to magnitudesymbolism via the Mil-Mal test in 4. We furtherdifferentiate our work by focussing on the task ofassigning pseudowords to provided visual stimuli,in contrast to Alper and Averbuch-Elor (2023) whoinvestigate the effects of different pseudowords onthe outputs of image generation models.Additionally, in recent times, large-scale effortshave been made to collect ratings of linguistic iconic-ity (i.e., the level of symbolism a particular word has),with Winter et al. (2023) collecting ratings of over 14kEnglish words. Some effort has been made to analysewhether or not similar ratings would be assigned byan LLM, where Trott (2024) used GPT-4 and reports amoderate positive correlation across ratings. We buildupon this work in 5 by introducing a wider range ofVLMs, including open-source alternatives. Numerouscomputational works in NLP have investigated otheraspects of iconicity and sound symbolism, withAbramova and Fernndez (2016) investigating theword embeddings of different aspects of morphologyin relation to symbolism (see also Yamshchikovet al., 2019; Liu et al., 2018). Additionally, Sabbatinoet al. (2022) investigated the emotional intensity ofnonsense words using NLP methods to determinewhich phoneme combinations were most responsible.",
  "A Minimal Pair refers to a pair of words that differ only inone phonological segment, such as cat /kat/ versus bat /bat/": ": Examples of Kiki-style (spiky) and Bouba-style (rounded) generations with DALL-E 3. In total, 50images were generated, with 25 per condition (the entitiesremaining constant). The ground truth is taken as themajority human vote. Several works in similar areas have also exemplifiedthe ability of LLMs to demonstrate perceptualbehaviour akin to humans and the potential for thesemodels to replace human participants in pilot studies,as well as facilitating the scaling of evaluation in-silico(Jain et al., 2023; Aher et al., 2023; Dillion et al., 2023;Ramezani and Xu, 2023; Coda-Forno et al., 2023).",
  "Shape Symbolism": "In this section, we perform a replication of the classicKiki-Bouba Effect experiment (Ramachandran andHubbard, 2001) using a range of multi-modal LLMs.Within the traditional set-up for the Kiki/Boubatest, human participants are presented with either arounded soft-edged shape or a spiky sharp-edgedshape and asked to assign one of two pseudowords toeither. In numerous experiments (Sidhu et al., 2021;Ramachandran and Hubbard, 2001; Khler, 1929),words such as Bouba and Maluma are preferredfor the latter rounded shapes, whilst Kiki andTakete are preferred for the spiked shapes. Thesefindings are thought to demonstrate a non-arbitrarylink between particular speech sounds and the physicalcharacteristics of the shapes to which they refer.",
  "Methodology": "Image DatasetWe follow the same process as 3.1,but use the characteristics of tiny and huge withthe following prompt: \"Generate an image of a/an[ENTITY] in isolation, with something else to helpjudge scale/perspective\". We use the same noun en-tities as in 3.1. Example generations are in . PseudowordsAs in the Shape Symbolism experi-ment (3), we wish to mitigate potential bias from thememorisation capability of the VLMs. To this end, weuse the Mil and Mal often associated with this test(Sapir, 1929) in addition to other phonetically similarpseudowords. Additionally, to avoid gross extraneousfactors arising from using words that are meaningfulin English (such as mil referring to millilitres, andmal being associated with badness, i.e., malprac-tice/malnourishment), we create the minimal pairsDil/Dal, Zil/Zal, Geel/Gaal, Beel/Baal, Weel/Waal,and Leel/Laal. The former three exploit the contrastbetween /I/ and /a/, whilst the latter exploit /i/ and/A/, with a range of consonants for variation.",
  "Results": "Correlations between each LLM and human judge-ments are presented in . We observe that theability of LLMs to rate the iconicity of English wordsappears to be dependent on model size. For instance,we see no true correlation with any of our FLANT5 models (base = 250M, XL = 3B) or our smallestLLaMA-2 variant (7B), but we begin to see significantcorrelations with another 7B parameter model, thatbeing instruction-tuned Mistral-7B. When investi-gating our larger models, we see the 13B LLaMA-2variant demonstrate Spearman/Pearson correlationsof .379 and .381, respectively. Interestingly, however,our largest LLaMA-2 model with 70B parameters per-forms worse at this task than the 13B variant as seenpreviously, with correlations of .304/.332. RegardingOpenAI models, GPT-3.5-Turbo demonstrates a moderate correlation with humans at .420/.439.Finally, GPT-4 presents the strongest correlations.However, we observe that (on the first 7k entries),the version of GPT-4 we use (late November 2023)performs worse than the earlier version presented byTrott (2024), further demonstrating how continuouslyupdated models also require continuous evaluationdue to receiving additional training data (Spearmancorrelations of 0.537 vs 0.575, respectively).",
  "Evaluators were recruited in different waves followingrevisions. All evaluators were paid above the current UK LivingWage per hour": ": Results of the Shape Symbolism experiments per pseudoword pair. Fleiss (Fleiss, 1971) for inter-annotatoragreement between humans is presented next to each pseudoword pair. Arrows indicate the direction of agreementchange from the standard prompt to the informed prompt. The dashed line represents 50%, akin to chance-level agreement.Full results in table form are presented in within Appendix A.2. In all cases, we are comparing with the humanmajority vote. indicating that once the VLMs are aware of thecharacteristic of interest (i.e., the shape of the entity),the VLMs are more likely to agree with humanperception. However, we do see a few cases (e.g.,Zaki/Umbu with LLaVA 7/13b, Kitaki/Gugagu withGPT-4, and Penape/Gugagu with Gemini, GPT-4,and LLaVA 13b) where performance decreases in theinformed condition, but these are usually only minordecreases except for cases where there is alreadyvery low agreement with humans. We see varyingperformance across open and closed-source models.Whilst LLaVA outperforms GPT-4 and Gemini inmany conditions, this is often close to chance-levelagreement and may be the result of label bias, versusthe closed models systematic disagreement.5 Finally,regarding our open-source LLaVA models at differentsizes, we interestingly see that the 13B modeloutperforms the 7B and 34B models at times, mostnoticeably in Tiki-Giba. However, the largest 34b",
  "Magnitude Symbolism": "Whilst the Kiki-Bouba effect demonstrates sound sym-bolism in relation to the perceived spikiness/roundnessof an object, magnitude symbolism refers to thenon-arbitrary relationship between certain vowels andthe perceived physical size of the entity they refer toand is commonly demonstrated through the namesMil and Mal, where the high front vowel in Milis associated with small entities, and the low backvowel of Mal is associated with larger entities. : Examples of Mil-style (tiny) and Mal-style(huge) generations with DALL-E 3. In total, 50 imageswere generated, with 25 per condition (the entitiesremaining constant). The ground truth is taken as themajority human vote.",
  "Task SettingWe use the same setup as in 3.1 butpresent the models with one candidate from either": "Magnitude-based name category (i.e. Mil-esque orMal-esque pseudowords), rather than Kiki/Boubarelated pseudowords. We additionally provide anextended prompt we call informed, which prepends\"This task is related to the phenomenon of MagnitudeSymbolism, which is a non-arbitrary relationshipbetween the sound of a word and its association withsize and scale\" to give additional task knowledge.",
  "We treat this as VLMs imitating an understanding ofsound symbolism, as they of course cannot actually hear": ": Results of the Magnitude Symbolism experiments per pseudoword pair.Fleiss (Fleiss, 1971) forinter-annotator agreement between humans is presented next to each pseudoword pair. Arrows indicate the direction ofagreement change from the standard prompt to the informed prompt. The dashed line represents 50%, akin to chance-levelagreement. Full results in table form are presented in within Appendix A.2. In all cases, we are comparing withthe human majority vote. Existing work by Trott (2024) has investigatedwhether or not GPT-4 is able to reflect human judge-ments and reported a Spearman correlation of 0.59.However, in trying to verify these findings, we see thatour ratings differ quite strongly. For this reason, were-run this experiment on the first 7k entries of Winteret al. (2023) and compare GPT-4 ratings as of lateNovember 2023 with the human judgements in Win-ter et al. (2023) as well as the GPT-4 judgements fromTrott (2024) that use an earlier version of GPT-4.7 Inthe other cases, we use the entire 14,772 entry dataset,except for GPT-3.5-Turbo, where we remove 209words that triggered OpenAIs safeguarding filters.",
  "Discussion": "The Source of Sound SymbolismAcross ourexperiments, we see evidence that LLM/VLMs arecapable of making decisions that are similar to thoseof humans in sound symbolism tasks, whilst onlyhaving access to textual and visual modalities, whilehuman decisions are believed to be grounded in sound.We hypothesise several reasons for the emergence ofsound symbolism in LLM/VLMs.Firstly,due to human languages exhibitingmostly regular orthography, auditory informationin speech is moderately reflected in the spellings ofwords via grapheme sequences (a characteristic thatgrapheme-to-phoneme conversion models have longexploited). Through this, text-based models are ableto learn associations between grapheme sequencesand semantics, based on more abstract characteristicsthan morpheme combinations alone, such as phonaes-themes (Kaushal and Mahowald, 2022). Whilst suchmodels have no embodied understanding of sound,such statistical patterns pose a viable signal for theimplicit learning of sound-based phenomena.Secondly, such associations between sounds (orgrapheme combinations) and physical characteristicsare naturally present in language, such as in poetry, narratives, or descriptions of entities that are cute,scary, small, or large, and are consequently pairedwith relevant visual stimuli in image captions whentraining vision modules for multimodal systems.However, such associations are subtle and not entirelyubiquitous. For example, whilst the high front vowel/I/ typically associated with small entities is presentin \"tiny\" and \"mini\", the word \"small\" itself possessesa low back vowel /O:/.As a result, the relatively weak performance of ourtested models could also be explained by the relativelack of sound-symbolism-heavy language in themodels training data which is overshadowed by moreprosaic language forms that do not exploit these phe-nomena as readily. This in turn would explain why theclosed-source models we tested (e.g. GPT-4/Gemini)outperform open-source models due to the significant(assumed) differences in parameter size, allowingthe larger closed models to retain more informationregarding sound symbolism within the weights, inaddition to being continuously updated with RLHF.The results of our multimodal experiments addition-ally demonstrate that under certain conditions, VLMsshow systematic disagreement with human labels,indicating the potential interference of additionalknowledge contained within language model trainingdata that influences the associations made betweenpseudowords and images that are not present inhumans. However, it is important to note that in ourexperiments we compare language model selectionsagainst the majority vote or mean scores assigned byhumans. Consequently, this results in a comparisonto an \"ideal\" human by necessity, overlookingindividual differences in perception across humans(where for a decision to be \"human-like\", it has tomatch a choice made by any human, rather than themajority). Consequently, higher agreement levelscan be observed when compared to the choices ofindividual humans, as inter-human agreement is notperfectly aligned in these tests. Future DirectionsAs a result of LLM/VLMs notbeing able to fully reflect human preferences in tasksregarding sound symbolism, it remains a promisingfuture direction to explicitly pre-train languagemodels on more sound-symbolism heavy datasetsor explicitly include sound-symbolism-related tasksinto the training or finetuning of these models foruse on related downstream tasks (such as creativewriting and marketing). Additionally, investigatingthe reason behind model predictions is a promisingdirection, such as through additional prompting to generate textual justifications, or investigating thevisual attention of VLMs to investigate whether theyare attending to characteristics closely associated withthe concepts being tested (e.g., spikes).",
  "Conclusion": "We have shed light on the processes underlying multi-modal perception and understanding in language mod-els. To do so, we performed a series of tests on modernVLM/LLMs regarding their ability to exhibit an under-standing of sound symbolism. Through comparisonwith human judgements, we see that VLMs are able toapproximate human perception in sound symbolismtests under certain conditions, such as when informedof the nature of the study (via the informed prompts),but struggle overall. We additionally see that magni-tude symbolism potentially presents an easier patternfor VLMs to recognise than shape symbolism, withselections having a higher agreement with humans onMagnitude Symbolism tests than Shape Symbolism.We also see that the ability of LLMs to emulatehuman judgements of iconicity scales more linearlywith model size. These findings indicate room forfuture research on more explicit inclusion of abstractperceptual properties into language model trainingin order to facilitate better in silico experimentationand improve performance on other downstream tasks.",
  "Limitations": "Owing to the relatively small sample sizes (i.e., thenumber of pseudoword pairs in the VLM-relatedtasks), we treat this work as a proof-of-conceptas to the ability of LLMs to perform well in thetasks we present and encourage other parties toengage in similar research at scale if their situationpermits. Additionally, whilst sound symbolism isbelieved to be largely language agnostic, we only usenative English speakers and pseudowords that arephonotactically legal in English in the present work.Additionally, some of our chosen pseudowords aretaken from existing literature. Whilst we investigatedthe prevalence of these words in the context of soundsymbolism within internet resources in order tomitigate memorisation from training data, it remainspossible that some level of data contamination maybe present (although the overall low performancecasts doubt on this). Furthermore, we present onlythe orthographic forms of the pseudowords to humanparticipants, resulting in potential variation betweenspeakers regarding phonetic realisation.",
  "Ethics Statement": "We believe in and firmly adhere to the Code of Con-duct in the performance of this work and the methodsinvolved. All of our imagery generations were pro-vided via accessing the respective OpenAI APIs, andin discovering imagery that triggered OpenAIs built-in guardrails, we replaced these images with otheroptions. All human evaluation was performed by con-senting adult participants who were provided with aparticipant information sheet and subsequently signeda consent form in line with the Ethics procedures of theprimary authors institution (who approved the ethicalvalidity of the study performed herein). Additionally,we present this work as a demonstration of interestingbehaviours within (very) large LLMs, but do not con-done the wholesale replacement of human participantsin related psycholinguistic/cognitive/psychologicalexperimentation, but rather view in silico experimen-tation as a useful tool primarily for prototyping. AcknowledgmentsTyler Loakman is supported by the Centre for Doc-toral Training in Speech and Language Technologies(SLT) and their Applications funded by UK Researchand Innovation [grant number EP/S023062/1].Visual elements in were taken fromFlaticon.Specifically, banners (SANB), brain(Freepik), GPU (Taufik Ramadhan), \"versus\" (AfifFudin), descending/ascending (Mie Nakae), andthought bubble (Aranagraphics).",
  "Ekaterina Abramova and Raquel Fernndez. 2016": "Questioning arbitrariness in language: a data-drivenstudy of conventional iconicity. In Proceedings of the2016 Conference of the North American Chapter ofthe Association for Computational Linguistics: HumanLanguage Technologies, pages 343352, San Diego,California. Association for Computational Linguistics. Gati Aher, Rosa I. Arriaga, and Adam Tauman Kalai.2023. Using large language models to simulate multiplehumans and replicate human subject studies.InProceedings of the 40th International Conference onMachine Learning, ICML23. JMLR.org. Morris Alper and Hadar Averbuch-Elor. 2023. Kiki orbouba?sound symbolism in vision-and-languagemodels. In Advances in Neural Information ProcessingSystems, volume 36, pages 7834778359. CurranAssociates, Inc.",
  "Casey Chu, Yunxin Jiao, and Aditya Ramesh. 2023.Improving image geneartion with better captions": "Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, AlbertWebson, Shixiang Shane Gu, Zhuyun Dai, MiracSuzgun, Xinyun Chen, Aakanksha Chowdhery, AlexCastro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter,Sharan Narang, Gaurav Mishra, Adams Yu, VincentZhao, Yanping Huang, Andrew Dai, Hongkun Yu, SlavPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, AdamRoberts, Denny Zhou, Quoc V. Le, and Jason Wei.2022. Scaling instruction-finetuned language models.",
  "Jacob Cohen. 1960.A coefficient of agreement fornominal scales.Educational and PsychologicalMeasurement, 20(1):3746": "Aleksandra Cwiek, Susanne Fuchs, Christoph Draxler,Eva Liina Asu, Dan Dediu, Katri Hiovain, ShigetoKawahara, Sofia Koutalidis, Manfred Krifka, PrtelLippus, Gary Lupyan, Grace E. Oh, Jing Paul, CaterinaPetrone, Rachid Ridouane, Sabine Reiter, NathalieSchmchen, dm Szalontai, zlem nal-Logacev,Jochen Zeller, Marcus Perlman, and Bodo Winter.2022. The bouba/kiki effect is robust across culturesand writing systems. Philosophical Transactions ofthe Royal Society B: Biological Sciences, 377(1841).Publisher Copyright: 2021 The Authors.",
  "Journal of Experimental Psychology, 12(3):225239": "David M. Sidhu, Angeliki Athanasopoulou, Stephanie L.Archer, Natalia Czarnecki, Suzanne Curtin, andPenny M. Pexman. 2023. The maluma/takete effectis late: No longitudinal evidence for shape soundsymbolism in the first year. PLOS ONE, 18(11):123. David M. Sidhu and Gabriella Vigliocco. 2023. I dontsee what youre saying: The maluma/takete effect doesnot depend on the visual appearance of phonemes asthey are articulated. Psychonomic Bulletin & Review,30(4):15211529. David M. Sidhu, Chris Westbury, Geoff Hollis, andPenny M. Pexman. 2021. Sound symbolism shapesthe english language: The maluma/takete effect inenglish nouns.Psychonomic Bulletin & Review,28(4):13901398.",
  "A.1Implementation Details": "Kiki-Bouba & Mil-MalFor the Kiki-Boubaand Mil-Mal sound symbolism experiments, weaccess gemini-pro-vision via the Google GeminiAPI. For GPT-4, we use gpt-4-vision-preview via theOpenAI Chat Completions API. For our open-sourceLLaVA models at various sizes, we specifically usellava-v1.6-vicuna-7b-hf, LLaVA-v1.6-vicuna-13b-hfandLLaVA-v1.6-vicuna-34b-hffrompubliclyavailable checkpoints on Hugging Face.We usedefault hyperparameters for all models to testtheir off-the-shelf capability. Human participantswere shown generated imagery with the binarypseudo-word options via Google Forms. The orderof image presentation was randomised, whilst theorder of the pseudowords was kept static. SeparateGoogle Forms were used for each pseudoword pair.Participants were able to complete the forms at theirown pace within a period of approximately 2 weeks. Iconicity RatingsFor our iconicity rating ex-periments, we use the following models fromHugging Face:FLAN-T5 Base (google/flan-t5-base), FLAN-T5 XL (google/flan-t5-xl) Mistral-7B(mistralai/Mistral-7B-Instruct-v0.2), LLaMA-2 (meta-llama/Llama-2-7b-hf,meta-llama/Llama-2-13b-hf,meta-llama/Llama-2-70b-hf). We access GPT-3.5-Turbo and GPT-4 via the OpenAI Chat CompletionsAPI. We also keep all model hyperparameters atdefault settings. For the LLaVA models, we modifythe prompt slightly by adding choice labels (A/B)rather than requesting the pseudoword itself to bereturned in order to directly access single-tokenoutput probabilities. EntitiesWithin our DALL-E 3 generations in theaforementioned experiments, we select the followinglist of entities in order to have a range of characteristics(including animate and inanimate entities): alien, bed, :Examples of ImageNet and DALL-E 3image pairings for both the soundscape description andbacktranslation experiments. Here, the GPT-4 descriptionof the ImageNet image has been used in a prompt togenerate the novel DALL-E 3 image.",
  "Real-WorldFirstly, due to requiring high-qualitypublicly available images to represent the real-life": "condition, we utilise a subset of ImageNet.8 To selectour candidate images, 2 authors of this work selecteda list of images that are believed to represent a widerange of soundscapes (e.g. a peaceful beach, violentwaves, a desert, a car, a plane, etc.), These 50 were se-lected from a unique set of images that all representeddifferent classes under the ImageNet taxonomy. Im-portantly, some of the chosen images were not usablewith GPT-4 due to containing entities that triggerOpenAIs safeguarding restrictions (such as an imageof a baby in a cot, or a couple of hunters with rifles).In such cases, we replace these images with alternativeselections that the 2 authors agree are high quality. 9 Generative AIFor our GenAI-based imagery,we use scene descriptions from GPT-4 (which aregenerated as part of the output for this task) of theImageNet imagery and prompt DALL-E 3 to generateimages from these descriptions via the OpenAIAPI. In doing so, we then create a parallel datasetof 25 real-world images, 25 LLM descriptions ofsaid images, 25 DALL-E 3 generations using theaforementioned LLM descriptions, and finally, 25LLM descriptions of the DALL-E 3 generations.This therefore allows us to ensure that our testing isrobust to novel images, as ImageNet imagery is likelyto have been a part of the training set for GPT-4svision module. Additionally, this also facilitates aninvestigation as to how consistent GPT-4 is at scenedescription and generation (analogous to testingNeural Machine Translation via back-translation). Task SettingWe perform this task in the followingway. For each condition (ImageNet/DALL-E), wepresent GPT-4 with the following prompt: \"Imaginethat the provided image is a window to another world.Describe the scene in 3 paragraphs discussing thefollowing aspects: Paragraph 1: Describe whatyou see in the image, including the entities and theperceived environment. Paragraph 2: Describe whatyou hear in the image (i.e. the soundscape), includingsounds from the identified entities, as well as theperceived environment. Paragraph 3: In reference tothe sounds mentioned in Paragraph 2, describe thesesounds using onomatopoeia (i.e. words that sound likethe sounds you are trying to describe). Provide youranswer to Paragraph 3 as a series of bulletpoints.\".",
  "Informed57.33% ( .147)48.17% ( -.036)50.17% ( .003)51.50% ( .034)51.83% ( .037)": ": Results of the Shape Symbolism experiments per pseudoword pair. Fleiss (Fleiss, 1971) for inter-annotatoragreement between humans is presented next to each pseudoword pair. For each VLM and word pair, we present Cohens for agreement between the models and the human majority vote (Cohen, 1960). The model with the highest agreementper prompt is in bold, and the best performing open-source model (i.e., LLaVA variant) is underlined. Arrows next tomodel names indicate the direction of agreement change from the standard prompt to the informed prompt. Following this, we ask 5 human evaluators (asubset from the main experiments), to evaluate the 3paragraphs on a 1-5 scale, where 1 = very bad, and 5= excellent (i.e., one rating for the visual description,one for the soundscape description, and one for theassignment of onomatopoeia to the soundscape). Theinstructions presented to human participants for thistask are presented in Appendix C.3.10 The order ofimage presentation to participants is randomised toavoid order effects. 10We present detailed instructions in order to moderate theunderstanding of what we would consider the different ratingsto be indicative of in order to minimise individual perceptionsof the instructions.",
  "B.2Results": "The results of the soundscape description task can beseen in and an example generation is presentedin . Overall, it can be seen that human evalua-tors thought positively of all 3 elements asked for fromGPT-4, including the visual description (which wouldexplain performance in the following section), sound-scape description and onomatopoeia, with all criteriaaveraging at least 4. This therefore demonstratesthat GPT-4 is able to provide convincing descriptionsof auditory experiences when provided with a validimage. One key thing to note is that whilst the stan-dard deviations are consistently low, onomatopoeiademonstrates the lowest consistently. This is to be ex-",
  "Gemini GPT-4 LLaVA-7b LLaVA-13b LLaVA-34b Standard52.17% ( .048)58.50% ( .169)51.00% ( .019)56.17% ( .127)50.00% ( .000)Informed61.83% ( .243)76.50% ( .528)50.00% ( .000)58.33% ( .168)51.83% ( .037)": ": Results of the Magnitude Symbolism experiments per pseudoword pair. Fleiss (Fleiss, 1971) for inter-annotatoragreement between humans is presented next to each pseudoword pair. For each VLM, we present Cohens foragreement between the models and the human majority vote (Cohen, 1960). The model with the highest agreement perprompt is in bold, and the best performing open-source model (i.e., LLaVA variant) is underlined. Arrows next to modelnames indicate the direction of agreement change from the standard prompt to the informed prompt. pected when evaluating a literary device, as differentpeople may have different preferences regarding ono-matopoeia they would use for certain circumstances.Additionally, there may be cases where GPT-4 hasdescribed something such as a stream and assignedthe onomatopoeia whoosh, which to one individualmay sound too aggressive and resemble fast-movingwater, when their own interpretation of a stream ismore gentle (perhaps better suiting lap lap).",
  ": Average ratings given to the visual, soundscape,and onomatopoeia descriptions given by GPT-4 across 2conditions. IN refers to images from ImageNet, and D3refers to DALL-E 3 generations": ": An example output from GPT-4 when asked to describe the visuals, soundscape, and perceived onomatopoeiaof an image. In this case, the image is the ImageNet generation of a teddy solving a crossword seen in . onorefers to onomatopoeia. In effect, our newly generated images demonstratea process analogous to the back-translation used inNeural Machine Translation. To test the consistencyof this process, we ask our evaluators to rate thegenerations on the following criteria: To what extentdoes the DALL-E 3 generated image present thesame visual scene as the original ImageNet sourceimage (on a scale of 1 to 5, where 1 = barely related,and 5 = all the main elements are captured).12",
  "C.1Results": "presents the results of our human evaluation.As we can see, the consistency of the pipelineis viewed favourably, with a mean rating of 4.18across the 25 image pairings and a low standarddeviation of 0.49. Importantly, no comparison wasrated lower than 3 by any evaluator. This result isquite surprising given the 100-word descriptionsprovided by GPT-4, indicating that GPT-4-vision ishighly capable of noticing the most salient aspectsof any image for recreation. The result of automaticevaluation comparing the descriptions from ImageNetand DALL-E 3 images are presented in ,echoing a similar pattern to the human evaluation."
}