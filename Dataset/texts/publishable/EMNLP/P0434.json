{
  "Abstract": "While learning with limited labelled data caneffectively deal with a lack of labels, it is alsosensitive to the effects of uncontrolled random-ness introduced by so-called randomness fac-tors (i.e., non-deterministic decisions such aschoice or order of samples). We propose andformalise a method to systematically investi-gate the effects of individual randomness fac-tors while taking the interactions (dependence)between them into consideration. To this end,our method mitigates the effects of other factorswhile observing how the performance variesacross multiple runs. Applying our method tomultiple randomness factors across in-contextlearning and fine-tuning approaches on 7 rep-resentative text classification tasks and meta-learning on 3 tasks, we show that: 1) disregard-ing interactions between randomness factors inexisting works led to inconsistent findings dueto incorrect attribution of the effects of random-ness factors, such as disproving the consistentsensitivity of in-context learning to sample or-der even with random sample selection; and 2)besides mutual interactions, the effects of ran-domness factors, especially sample order, arealso dependent on more systematic choices un-explored in existing works, such as number ofclasses, samples per class or choice of promptformat.",
  "Introduction": "Learning with limited labelled data, such as in-context learning, fine-tuning or meta-learning, isan umbrella term for approaches designed to workwhen enough labels are lacking. Although suchapproaches can effectively deal with limited la-bels, they were observed to be notably sensitive tothe effects of uncontrolled randomness. Such ran-domness is introduced by the randomness factors,which represent the non-deterministic decisions inthe training process, such as order of samples, themodel initialisation or sample choice (Pham et al.,2021; Gundersen et al., 2022; Pecher et al., 2024b). The randomness in the training process can havemassive impact, leading to large deviation in theperformance over multiple training runs. In-contextlearning was found to be sensitive to the order ofsamples, where changing only order of samplesleads from state-of-the-art predictions to randomguessing (Lu et al., 2022; Zhao et al., 2021b). Sim-ilarly, repeating fine-tuning and evaluation multi-ple times with different random seeds can resultin smaller language models outperforming theirlarger counterparts (Dodge et al., 2020). If the ran-domness is not properly addressed, it can have non-negligible negative consequences even with enoughlabelled samples (Reimers and Gurevych, 2017;McCoy et al., 2020). It was identified as a majorobstacle to reproducibility (Albertoni et al., 2023)that can prohibit objective comparison and cause amethod to be incorrectly denoted as state-of-the-artonly based on more favourable chance (Reimersand Gurevych, 2017). The uncontrolled random-ness can also unintentionally (but, unfortunately,also intentionally by cherry-picking) create animaginary perception of research progress.A lot of focus is dedicated to investigating andmitigating the effects of randomness and sensitiv-ity of learning with limited labelled data (Mosbachet al., 2021; Pecher et al., 2024b), especially for in-context learning (Lu et al., 2022; Zhao et al., 2021b;Chang and Jia, 2023; Li and Qiu, 2023; Kksalet al., 2023). However, the existing research is of-ten limited in its extent (in terms of randomnessfactors, approaches or settings) and at times leads tocontradictory or inconsistent results. For example,in-context learning was believed to be consistentlysensitive to the order of the randomly selected sam-ples (Lu et al., 2022; Zhao et al., 2021b), however,it was later observed that this sensitivity disappearswhen a more sophisticated sample selection strat-egy is used (Zhang et al., 2022; Chang and Jia,2023; Li and Qiu, 2023).We argue that the observed inconsistencies are caused by disregarding the interactions betweenrandomness factors, which leads to incorrectly at-tributing the performance deviations to differentrandomness factors. Such interactions are so farpartially or even completely overlooked in the exist-ing works, resulting in the misleading findings. Inaddition, we hypothesise that the sensitivity of in-context learning is not only affected by the interac-tions with other factors but also by other systematicchoices, which are not thoroughly controlled andexplored in the existing works, such as the numberof classes, shots per class and prompt format.Our main contributions are as follows1: We propose a novel method for investigationof randomness factors effects that, in contrastto the existing works, is thoroughly formalisedand explicitly addresses interactions betweenthem by mitigating the effects of other non-investigated factors. In addition, it measuresthe relative importance of factors, by calcu-lating what fraction of the overall deviationin the performance (estimated by a goldenmodel) the investigated factor contributes incomparison to all other factors, which allowsfor more in-depth analysis across factors, mod-els, datasets and experimental settings. Using the proposed method, we investigate5 randomness factors and their effects onin-context learning and fine-tuning across 7representative text classification datasets, andmeta-learning across 3 datasets. The resultsshow that the in-context learning models arenot consistently sensitive to the order of sam-ples, confirming our hypothesis that the inter-actions play a role in the incorrect attributionof the effects of randomness factors. We further analyse how the more systematicchoices influence the importance of the ran-domness factors. We find the following keyinsights: 1) predicting a higher number ofclasses leads to increased importance of sam-ple order for in-context learning and reducedimportance of sample order and model initial-isation for fine-tuning approaches; 2) increas-ing the number of in-context samples reduces",
  "Related Work": "The main strategy for investigating the effects ofrandomness factors is to repeat the training andevaluation multiple times, changing specific non-deterministic decisions of the training, such aschanging what data is used and observing thechange in results (i.e., Random strategy) (McCoyet al., 2020; Dodge et al., 2020; Bouthillier et al.,2019; Agarwal et al., 2021). As such investigationmay be affected by the interactions with other fac-tors, another possibility is to perform the investiga-tion by fixing all the other factors to a specific state(i.e., Fixed strategy), either chosen randomly (Bo-quet et al., 2019; Pham et al., 2021; Zhao et al.,2021a) or as a result of a mitigation strategy (Liand Qiu, 2023; Chang and Jia, 2023). Another in-vestigation strategy is to vary all the investigatedfactors at the same time and then decouple their ef-fects in evaluation (Dodge et al., 2020; Bouthillieret al., 2021; Sellam et al., 2022; Weber et al., 2023;Webson and Pavlick, 2022), which accounts for theinteractions but introduces a significant increase incomputation costs (Bouthillier et al., 2021).Majority of the focus on investigating and miti-gating effects of randomness is on in-context learn-ing, which was found to be especially sensitiveto the choice of samples (Liu et al., 2022; Zhanget al., 2022; Chang and Jia, 2023; Li and Qiu, 2023;Kksal et al., 2023) and their order (Lu et al., 2022;Zhao et al., 2021b; Nguyen and Wong, 2023). How-ever, it was observed that the sensitivity to sampleorder disappears when using a more sophisticatedsample selection strategy instead of random selec-tion (Zhang et al., 2022; Chang and Jia, 2023; Liand Qiu, 2023), hinting at interactions betweenthese factors that may lead to inconsistent results.In addition, the performance of in-context learn-ing was found to be sensitive to more systematicchoices as well (Weber et al., 2023), such as theformat of the prompt (Sclar et al., 2023; Voronovet al., 2024) or number of shots (Liu et al., 2022;Mavromatis et al., 2023). However, the impact ofthese systematic choices on the effects of random-ness factors is not thoroughly investigated. Besides order of in-context examples, large language mod-els were found to be especially sensitive to theorder of choices in multi-choice question answer-ing (Zong et al., 2023; Wei et al., 2024). Althoughthe remaining approaches and randomness factorsreceive only limited focus, they were still foundto be sensitive to the effects of randomness, suchas fine-tuning being sensitive to the random seeds(that influence model initialisation and order ofsamples) (Dodge et al., 2020; McCoy et al., 2020;Mosbach et al., 2021; Zhao et al., 2021a; Zhonget al., 2021), meta-learning being sensitive to thechoice of adaptation samples or how they are splitinto tasks (Agarwal et al., 2021; Setlur et al., 2021;Cioba et al., 2022; Ye et al., 2021), or the overallmachine learning being sensitive to factors such asthe impact of framework and hardware implemen-tation (Boquet et al., 2019; Pham et al., 2021), orthe data split (Bouthillier et al., 2019, 2021).In majority of the cases, the effects of random-ness are evaluated based on a single aggregatedmetric from multiple runs (e.g., mean, standard de-viation, or the difference between best and worstrun), with the importance being determined in abinary fashion by comparing this metric to a thresh-old, which allows only for simple analysis (McCoyet al., 2020; Ye et al., 2021; Zhang et al., 2022)).A slightly more nuanced analysis is possible onlyin specific cases, where statistical approaches areused, such as grouping runs and aggregating ongroup level (Dodge et al., 2020), decoupling inter-actions (Boquet et al., 2019) or estimating distribu-tion from lower number of training runs (Sellamet al., 2022). However, almost no studies analysethe importance of the effects in a way that wouldallow for easy comparison across different settings,such as what fraction of the overall variance thespecific factor contributes.We build on the ideas from the existing works,mainly from (Dodge et al., 2020; Bouthillier et al.,2021; Zhao et al., 2021a), to explicitly take interac-tions into consideration and analyse the importanceof the found effects. In addition, we fill the identi-fied research gap by analysing the impact of moresystematic choices on the randomness factors.",
  ": end if": "into consideration, and which is designed to mea-sure the importance of the found effects.Thesteps of the method are compiled in Algorithm 1,with further supplementary details included in Ap-pendix B). Setup.First, a set RF (|RF| = K) is defined,which includes all the factors that will be consid-ered in the investigation. Each randomness factoris characterised by a set of its randomness fac-tor configurations, Cj, specifying all the possiblestates the factor can appear in. For example, the different permutations of samples represent the con-figurations of the data order randomness factor. Foreach factor i, the investigated factor configurationsset IFCi, containing the configurations used for theinvestigation, is defined as IFCi = Ci, and the mit-igated factor configurations set MFCi, containingthe joint configurations of the remaining random-ness factors, is defined as a cross product betweenall the sets of randomness factor configurations,except for the investigated randomness factor (Ci):",
  "MFCi = C1 ... Ci1 Ci+1 ... CK (1)": "Investigating effects.At its core, the investiga-tion of factor i is done by observing how the perfor-mance changes across the different configurationsthe randomness factor can appear in. In a singleinvestigation run, the training and evaluation ofa model is repeated N times (N = |IFCi|), eachtime with a different configurations n of the factor i,while keeping the configurations of the remainingfactors fixed to a randomly chosen configurationm from MFCi. For each repeat, the model per-formance (rm,n) is determined. The standard de-viation p_stdm (called partial standard deviation)across these N runs (p_stdm = std(rm,)) repre-sents the effects of the investigated randomnessfactor that are still affected by the interactions. Mitigating interactions.To remove the effectsof other randomness factors, the investigation runis repeated multiple (M) times each time with adifferent fixed configuration m from MFCi. Eachsuch repeat is called mitigation run and results in aseparate partial standard deviation. After perform-ing enough mitigation runs (i.e., searching throughenough configurations m of the non-investigatedrandomness factors), the partial standard deviations(p_stdm) are averaged to produce the contributedstandard deviation (c_std = p_std), which repre-sents the final adjusted effects of the investigatedfactor i (i.e., it represents the deviation the investi-gated randomness factor contributes to the overalldeviation in results). Calculating importance score.To assess the im-portance of the factor, the contributed standarddeviation is compared with two additional values:1) mitigated standard deviation (m_std); and 2)golden model standard deviation (gm_std). Themitigated standard deviation represents the jointeffects of all the non-investigated randomness fac-tors (i.e., standard deviation contributed by non-investigated factors). To obtain this value, a partial mean (p_meanm) is calculated for each investiga-tion run, which represents the expected averagemodel performance for the given combination ofconfigurations of the non-investigated factors. Themitigated standard deviation is then calculated asthe standard deviation across these partial means(m_std = std(p_mean)).The golden model standard deviation (gm_std)represents an objective estimate of the deviationin the model performance. To get this estimate, agolden model configuration set GMC (|GMC| =L) is defined, as a cross product between the setsof all the randomness factor configurations:",
  "GMC = C1 C2 ... CK1 CK(2)": "Afterwards, a model is trained and evaluated Ltimes each time with different configuration g fromGMC, the model performance rg is determined andthe standard deviation across these runs representsthe golden model standard deviation gm_std.The final importance score of the factor is de-fined as the portion of the golden model stan-dard deviation the investigated factors contributeover the non-investigated ones (importance =(c_std m_std)/gm_std). Any randomness fac-tor with an importance value over 0 is consideredimportant, as it contributes the same amount of de-viation as all the remaining factors combined. Thesize of the score determines the relative importancebetween the factors (e.g., factor with importancescore of 0.6 is more important than one with scoreof 0.1) and can be used for further analysis and com-parison across different factors, models, datasetsand experimental settings (e.g., how the importanceof specific factor changes if the number of samplesis increased or a different dataset is used). Choosing values for parameters N, M and L.The number of investigation runs (N) and the mit-igation runs (M) provide a trade-off between thefeasibility (or computation costs) of the investiga-tion and the precision of the results (how well theeffects are estimated and interactions mitigated).Below, we provide a set of heuristics to achieve agood trade-off (and provide full method for select-ing the values of the parameters in Appendix B.3):",
  "teractions, increasing value of M should bepreferred over increasing the number of inves-tigation runs (N)": "3. L = N M; to guarantee the importancescore is calculated from distributions of thesame sizes and characteristics, the number ofruns in the golden model should be equal tothe overall number of runs in the investigation. Validation of the proposed method.We evalu-ate the validity of the proposed method indirectly(as there is no ground-truth to compare against)using the following experiments: 1) comparingthe method to two existing baselines (i.e., Randomand Fixed investigation strategy) and evaluating theproperties and benefits of our method, specificallythe handling of interactions that may lead to un-derestimation or overestimation of the effects inspecific cases, and the importance score that allowsfor more in-depth analysis and comparison acrossdifferent experimental settings; 2) exploring thedependence of how well the effects are estimatedand their interactions mitigated by our method tothe number of investigation and mitigation runs,where we found that the results of our methods arestable already with a low number of runs (20 miti-gation and 10 investigation runs); and 3) observingthe consistency of the results and findings whenapplying the method to different settings (factors,approaches, datasets). The full description of thevalidation results is in Appendix E.",
  "Experiments": "Datasets.The experiments are conducted on7 text classification datasets composed of dif-ferent tasks with different number of classes.We focus on 3 binary classification datasetsfrom the GLUE benchmark (Wang et al., 2018):SST2 (Socher et al., 2013) for sentiment classifica-tion, CoLA (Warstadt et al., 2019) for determiningthe grammatical acceptability of a sentence, andMRPC (Dolan and Brockett, 2005) for determiningthe semantic equivalence relationship between twosentences. In addition, we use 4 multi-class textdatasets: AG News (Zhang et al., 2015) for newsclassification, TREC (Voorhees and Tice, 2000) forquestion classification, DB-Pedia (Lehmann et al.,2015) for topic classification and SNIPS (Couckeet al., 2018) for intent classification.",
  "Approaches.The main focus of the investiga-tion is on the in-context learning using the Flan-": "T5 (Chung et al., 2022) base, LLaMA-2 (Tou-vron et al., 2023) 13B instruction optimised model,Mistral-7B (Jiang et al., 2023) and Zephyr-7B (Tun-stall et al., 2023). In addition, we also focus on fine-tuning, using the BERT (Devlin et al., 2019) andRoBERTa (Liu et al., 2019) base models. Finally,we also investigate the meta-learning approachesMAML (Finn et al., 2017), Reptile (Nichol et al.,2018) and the Prototypical Networks (Snell et al.,2017), but only on the binary datasets. Randomness Factors.In the experiments, weevaluate following randomness factors: 1) LabelSelection used to determine the samples consid-ered as labelled during training; 2) Data Split usedto split the data into training, validation and testsets; 3) Data Order that determines the order ofsamples in training (order of in-context examplesin prompts for in-context learning, order in whichsamples appear in batches for fine-tuning or tasks inmeta-learning); 4) Sample Choice (not relevant forfine-tuning) that determines the randomly chosensamples used as in-context examples for in-contextlearning (or adaptation samples for meta-learning);and 5) Model Initialisation (not relevant for in-context learning) related to the randomly initialisedweights and other parameters in the models. Method Setup.For each randomness factor, thenumber of the investigation runs (N) is set to 10,the number of mitigation runs (M) is set to 100for fine-tuning, meta-learning and 20 for in-contextlearning. The golden model uses the same over-all number of runs (L) (1 000 for fine-tuning andmeta-learning, 200 for in-context learning). Thesevalues, selected based on an Ablation Study (in-cluded in Appendix C), provide a balance betweenthe coverage of the configurations state space andthe computation costs. Experimental Setup.We focus on a setting withlimited labelled data, which represents a practicalreal-world scenario where a limited budget requiresus to choose what data we label (a common case formany NLP supervised tasks). To simulate the un-availability of labels, we randomly select 1000 trainsamples from a sufficiently large labelled datasetand consider only these to be labelled. Beforechoosing this subset of samples, each dataset issplit into train and test using 80-20 split. In addi-tion, 20% of the labelled train samples are used asa validation set. As such, we use different training,validation and test samples across different runs. We report the performance using the F1 macro met-ric. If not specified otherwise, we run in-contextlearning in a 2-shot setting with the first prompt for-mat from . All prompt formats and furtherexperimental details are included in Appendix D.",
  "GOLDEN MODEL1.0431.0431.043LABEL SELECT.(*) 1.122 (*) 1.004(*) 0.863DATA SPLIT(*) 1.1850.402(*) 0.664DATA ORDER(*) 1.138 (*) 0.9570.456SAMPLE CHOICE (*) 1.0520.406(*) 0.744": ": Comparison of different investigation strate-gies for the Flan-T5 and Zephyr-7B models on the SST2dataset based on the F1 macro standard deviation. Fac-tors considered important for different strategies aredenoted using the (*) symbol. We observe that interac-tions between factors may cause some factors to havetheir importance overestimated (denoted in bold) orunderestimated (denoted in italics).",
  "Interactions Between Randomness Factors": "In this section, our goal is to answer the followingresearch question: RQ1: How do the interactionsbetween randomness factors affect their individualimportance? To answer this question, we com-pare our proposed method (Interactions) with thecommonly used investigation strategies: 1) Ran-dom, which varies the overall random seed in theinvestigation without any constraint on the config-urations of other factors; and 2) Fixed, where thenon-investigated randomness factors are fixed to asingle configuration for all runs of the investigation.For these strategies, we consider the effects of fac-tor to be important when it contributes at least 50%of the golden model standard deviation. The resultsfrom this comparison are shown in and usedfor validation of our method in Appendix E.Effects of randomness factors may be overesti-mated or underestimated when interactions arenot taken into consideration. The Random strat-egy leads to a deviation similar to the one from thegolden model across all investigated randomnessfactors. Such result indicates that all randomnessfactors are equally important, leading to a signifi-cant importance overestimation in some cases (e.g., Data Order factor for both Flan-T5 and Zephyrmodels). Even though the Fixed strategy producesmore reliable results, it is still affected by the ran-dom choice of the single factor configuration (e.g.,Data Order contributing deviation of 3.014, whichis much higher than the deviation of 2.244 fromthe golden model). As such, we observe underes-timation of the results (e.g., Sample Choice andData Split with the Zephyr-7B model not beingconsidered important with a deviation of 0.406 and0.402) as well as overestimation (e.g., Data Or-der being considered important with a deviation of3.014 for Flan-T5 and 0.957 for Zephyr-7B). Tak-ing the interactions into consideration, we observethat the Data Order randomness factor is notconsistently important for in-context learningeven when choosing samples randomly, whichconfirms the impact of interactions on incorrect at-tribution of effects of different randomness factors.",
  "Importance of Randomness Factors": "In this section, we want to answer the followingresearch question: RQ2: What randomness factorsare important for different approaches for learn-ing with limited labelled data? We analyse theresults of our method on different datasets to iden-tify the consistently important factors. The resultsare included in for in-context learning andfine-tuning and in Appendix F.1 for meta-learning.Sample Choice represents the most importantfactor for in-context learning. For the majorityof the investigated models, the Sample Choice fac-tor is considered important for almost all of thedatasets, achieving an average importance scoreof 0.25 across the models and datasets. A notableexception is Flan-T5 on the multi-class datasets(average importance score of 0.39) or Zephyr onthe MRPC dataset (importance score of 0.43),where the factor is not considered important.Importance of Data Order is dataset andmodel dependent for in-context learning. Major-ity of the in-context learning models do not showsensitivity to the Data Order randomness factoron binary datasets (average importance score of0.28). At the same time, the importance of DataOrder becomes consistently higher on multi-classdatasets for all models (average importance scoreof 0.16), with the exception of the Zephyr-7B.General randomness factors, Label Selectionand Data Split, show consistent importance forthe majority of the models and datasets. In caseof fine-tuning, the Label Selection and Data Split : Importance of the investigated randomness factors for all investigated approaches and datasets whiletaking the interactions between factors into consideration. The legend indicates the number of classes for eachdataset. As the Flan-T5 model predicts the same class for every sample on the DB-Pedia dataset, we do not includethese results. Increasing the number of classes in datasets results in increased importance of the Data Order factorfor in-context learning and reduced importance of Data Order and Model Initialisation for fine-tuning approaches. : The change in importance of the Data Order and Sample Choice randomness factors as the number ofin-context examples increases. Increasing the number of samples per class does not have a consistent effect on theimportance of the Data Order factor, while the importance of the Sample Choice factor decreases. randomness factors show the highest level of impor-tance across all datasets when compared to otherrandomness factors (average importance score of0.34 and 0.52). For in-context learning, we donot observe such consistent results, with the im-portance changing based on the dataset and modelused. However, these factors are considered impor-tant in more than half of the cases (16 out of 27 forLabel Selection and 22 out of 27 for Data Split). Importance of Data Order and Model Ini-tialisation is dataset and model dependent forfine-tuning. For the binary datasets, these factorsare considered important for both models (averageimportance of 0.25 for Data Order and 0.19 forModel Initialisation). However, on the multi-classdatasets, the importance of Sample Order for bothmodels (average importance score of 0.14) andModel Initialisation for the BERT model (averageimportance score of 0.30 for BERT and 0.04 forRoBERTa) drops significantly.",
  "Effects of Variable Number of Classes andIn-Context Samples": "In this section, our focus is on answering the follow-ing research question: RQ3: How does the impor-tance of data-specific randomness factors changebased on the number of classes and in-context sam-ples? As we observe different effects of random-ness factors on binary and multi-class datasets, ourmain focus is to determine whether the change inimportance is caused by the increased number ofin-context examples in the prompt, by the largernumber of options that can be predicted or by acombination of both. The results from changingthe number of classes are included in , andfrom changing the number of shots for in-contextlearning are included in .The importance of Data Order randomnessfactor for in-context learning increases at highernumber of classes. The Data Order randomnessfactor is not considered important for any of thein-context learning models on the SST2 dataset,achieving importance of 0.47, 0.53, 0.16 and : Effect of different prompt formats on the importance of randomness factors for in-context learning. Thechoice of format has a significant effect on the importance of different factors, with the minimal formats oftenleading to higher importance. At the same time, the larger models show lower sensitivity to prompt format. 0.44 respectively for the Flan-T5, LLaMA-2,Mistral and Zephyr models. On the remaining bi-nary datasets, the importance either gradually in-creases (LLaMA-2 or Zephyr) or decreases (Flan-T5 and Mistral). However, on the datasets withthe higher number of classes, the importance ofthe Data Order factor gradually increases (withthe exception of the Zephyr model), achieving im-portance as high as 0.25 for Flan-T5 and 0.29 forZephyr on the SNIPS dataset, and 0.41 for LLaMA-2 and 0.18 for Mistral model on DB-Pedia dataset. The importance of the Sample Choice for in-context learning is not consistently affected bythe number of classes. In case of Flan-T5 andZephyr, the importance of Sample Choice graduallydecreases as we increase the number of classes(from 0.57 on SST2 to 0.57 on SNIPS for Flan-T5, or from 0.39 on SST2 to 0.17 on DB-Pedia forZephyr). For the LLaMA-2 model, the decrease isnot as consistent, with the importance being muchlower on the TREC than on the SNIPS dataset.Finally, the Sample Choice randomness factor isconsistently important across all datasets for theMistral model, with no apparent tendency. The importance of Data Order and ModelInitialisation randomness factors for fine-tuningdecreases with higher number of classes. ForBERT model, we observe a gradual decrease ofimportance for both factors as we increase the num-ber of classes, going from 0.45 and 0.33 on SST2to 0.55 and 0.50 on DB-Pedia, respectively forData Order and Model Initialisation. Similarly, weobserve a gradual decrease of Data Order impor-tance for RoBERTa model, going from 0.46 onSST2 to 0.19 on DB-Pedia. However, Model Ini-tialisation does not show a consistent tendency forRoBERTa, with the importance staying approxi-mately the same across the majority of the datasets. Number of in-context samples has no consis-tent effect on the importance of Data Order. Theimportance of Data Order remains consistent, oreven is lowered, across all models, datasets andnumber of shots per class. On the other hand, in-creasing the number of shots reduces the impor-tance of Sample Choice factor for all models anddatasets. For example, the importance of SampleChoice for Zephyr drops from 0.39 on 2-shot set-ting to 0.02 on 10-shot setting on the SST2 dataset.",
  "Impact of Prompt Format": "In this section, we aim to answer the followingresearch question: RQ4: How does the promptformat affect the importance of randomness fac-tors? As the previous works observed a significantsensitivity of in-context learning to prompt formatour goal is to investigate whether such sensitivityaffects the importance of randomness factors aswell. To achieve this, we compare our optimisedprompt format (Format A) with 3 minimal promptformats (Formats B, C and D) defined in (Li andQiu, 2023; Gao et al., 2021; Kksal et al., 2023).All the prompt formats are described in detail in in Appendix D. The results from the inves-tigation are illustrated in , with full resultsincluded in Appendix F.Minimal formats lead to significant changes inthe importance of randomness factors over theoptimised format. The Data Order randomnessfactor shows the highest sensitivity to the promptformat, becoming significantly important in manycases even when the interactions are taken into con-sideration. At the same time, Sample Choice isnot as sensitive to the prompt format. The remain-ing randomness factors, Label Selection and DataSplit, are affected only when using specific formats using the last format, we observe a significant change in the importance of these randomness fac-tors across all models and datasets. The largermodels, Mistral and Zephyr, show lower sensitiv-ity to prompt format change, as the importanceof all randomness factors remains consistent acrossformats. On the other hand, in case of the Flan-T5 model, the importance of randomness factorschanges significantly across different formats.",
  "Discussion": "Besides understanding the sensitivity, an importantaspect is predicting a good configurations of themost important randomness factors to guaranteestability and generalisability of the approaches. Asopposed to the hyperparameter tuning, finding thisconfiguration is not as straightforward as it is notso systematic. First, as we show in this work, theimportance and the best configuration is stronglyaffected by the interactions between randomnessfactors and other systematic choices. Second, thereis no metric that can serve as an estimate for thequality of the configuration besides the observedperformance for example when finding an op-timal prompt, the best performing and worst per-forming one can differ only in a single word (Zhanet al., 2024).One way how to determine the optimal config-uration are the mitigation strategies that recentlystarted to attract a research attention (see the re-cent survey on stability of learning with limitedlabelled data (Pecher et al., 2024b) for more infor-mation). While the mitigation strategies are oftenfactor-specific, such as sample selection strategiesfor in-context learning (Li and Qiu, 2023; Changand Jia, 2023; Kksal et al., 2023; Pecher et al.,2024c), also more general strategies based on en-sembling and further model training have been de-veloped (Pecher et al., 2024a; Pezeshkpour and Hr-uschka, 2023; Summers and Dinneen, 2021; Wanget al., 2023; Allingham et al., 2023; Voronov et al.,2024). Uncovering the most important randomnessfactors through systematic investigation and designof new and effective mitigation strategies to reducethe sensitivity to the effects of randomness is an im-portant future directions of the field (Pecher et al.,2024b; Liu et al., 2023).",
  "In this work, we have proposed a novel method thatexplicitly takes interactions between different ran-domness factors into consideration by mitigating": "the effects of the other, non-investigated, random-ness factors. In addition, our method is designed todetermine the importance of the investigated ran-domness factor by measuring what fraction of theoverall deviation of the model (represented usinga golden model) it contributes over the mitigatedrandomness factors, allowing for in-depth analysisacross experimental settings.Applying our proposed method to investigate theeffects of randomness factors on in-context learn-ing, fine-tuning and meta-learning, we confirm ourhypothesis that interactions between randomnessfactors may cause incorrect attribution of effectsof one factor to another, leading to inconsistent re-sult. Contrary to the previous works, after takinginteractions into consideration, we do not observea consistent sensitivity of in-context learning ap-proaches to the sample order even when choosingsamples at random. Instead, we observe that theimportance of randomness factors, especially thesample order, is affected by the interactions withother factors and by the systematic choices suchas the number of predicted classes, the number ofsamples per class and the choice of prompt format.The proposed method can be applied to otherNLP tasks as well, such as question answering,with minimal modifications. Only requirement isto define the randomness factors and their configu-rations, such as order of choices in the questions orthe symbols used for the answers. Extending ourinvestigation to other tasks represents an interestingpotential for future work.",
  "Acknowledgements": "This work was partially supported by the projectsfunded by the European Union under the EU Hori-zon 2020: TAILOR, GA No. 952215, by the Euro-pean Union under the Horizon Europe: DisAI, GANo. 101079164 and vera.ai, GA No. 101070093,and by the EU NextGenerationEU through the Re-covery and Resilience Plan for Slovakia under theproject No. 09I03-03-V03-00020.Part of the research results was obtained us-ing the computational resources procured inthe national project National competence centrefor high performance computing (project code:311070AKF2) funded by European Regional De-velopment Fund, EU Structural Funds Informati-zation of Society, Operational Program IntegratedInfrastructure.",
  "Limitations": "The effects of randomness factors are investigatedon a selection of models from different approachesfor learning with limited labelled data. However,in order to provide a more extensive and in-depthanalysis of the interactions and the more systematicchoices without a significant increase in computa-tion costs, the effects are investigated on models ofsmaller sizes we use the base versions of BERT,RoBERTa and Flan-T5 models, and a 4-bit quan-tised versions of the LLaMA-2-13B, Mistral-7Band Zephyr-7B models. As such, the observed ef-fects may not be as representative for larger models.However, similar to related work, we observed thelarger models to be more susceptible to the effectsof randomness and so the results of our investiga-tion may underestimate the importance of differentfactors (instead of their over-estimation).The number of investigation and mitigation runsused in our investigation is selected based on anablation study (in Appendix 3). In addition, fol-lowing related work (e.g., (Gao et al., 2021; Changand Jia, 2023; Sclar et al., 2023; Li and Qiu, 2023;Kksal et al., 2023)) and the results of our ablationstudy, we also evaluate each run using only 1 000test samples. In both cases, the decision representsa trade-off between the reliability of the results andtheir feasibility. Although this represents an opti-mal trade-off, increasing the number of runs andthe number of test samples could potentially leadto better estimation of the effects and mitigation oftheir interactions (especially on larger datasets), butat the cost of significant computation costs increase.At the same time, the number of investigation andmitigation runs utilised in this paper still representsa significant improvement over the existing stud-ies, as it is a common practice to investigate theeffects using very low numbers of runs. As futurework, we plan to explore the possibilities for ef-fective mitigation strategies for all the randomnessfactors to mitigate their effects and further reducecomputation costs.Similarly, we investigate the effects on a smallerset of training labelled samples (using only 1 000labelled samples). This setup may lead to largereffects of randomness and lower stability for fine-tuning and meta-learning while having negligibleimpact on in-context learning (which works with asmaller subset of these samples). However, as thisrepresents a real-world scenario with a limited bud-get, we do not consider this to be a significant lim- itation, as the effects of randomness factors werepreviously found to be significant even with theuse of large datasets (Mosbach et al., 2021; Dodgeet al., 2020; Reimers and Gurevych, 2017).Even though the effects of implementation levelrandomness factors (e.g., framework implemen-tation and hardware scheduling) were previouslyobserved to affect the models performance, weconsider their effects only partially by mitigatingthem as much as possible (e.g., setting CUDA to bedeterministic, using the same version of librariesand the same system for all experiments). Inves-tigating their effects fully is out of scope for thispaper due to the specifics required to thoroughlyexplore these effects (e.g., using a single worker,deterministic implementation, or running on a sin-gle CPU thread (Pham et al., 2021)).Although we perform basic prompt engineeringto obtain our optimised prompt for each dataset, theprompt format could be theoretically improved us-ing automatic prompt-tuning methods. As we haveobserved the impact of prompt format on the effectsof different randomness factors, the use of such for-mat may lead to different findings, especially forthe Flan-T5 model. However, we still observed themain in-context learning models (Mistral-7B andZephyr-7B) to be sufficiently robust to this changeand their results should stay the same. At the sametime, our main prompt was designed based on therecommendations and prompt formats from relatedwork (Sun et al., 2023; Li and Qiu, 2023; Gaoet al., 2021; Kksal et al., 2023), so we do not ex-pect significant changes when using more promptsobtained through prompt-tuning.Finally, we are not sure whether the datasets weuse in our experiments have been used to train themodels we use for in-context learning, which mayaffect our findings and results on these models. Welimit this effect by using our own optimised promptacross the majority of the experiments. However,we cannot guarantee it is enough to provide unbi-ased results as this limitation is part of the recentlyrecognised LLM validation crisis (Li and Flanigan,2023) and we would need to train the model fromscratch to address it properly, which is out of scopefor this paper.",
  "RiccardoAlbertoni,SaraColantonio,PiotrSkrzypczynski,andJerzyStefanowski.2023.Reproducibility of machine learning: Terminology,recommendations and open issues. arXiv preprintarXiv:2302.12691": "James Urquhart Allingham, Jie Ren, Michael W Dusen-berry, Xiuye Gu, Yin Cui, Dustin Tran, Jeremiah ZheLiu, and Balaji Lakshminarayanan. 2023. A simplezero-shot prompt weighting technique to improveprompt ensembling in text-image models. In Pro-ceedings of the 40th International Conference onMachine Learning, volume 202 of Proceedings ofMachine Learning Research, pages 547568. PMLR.",
  "Xavier Bouthillier, Csar Laurent, and Pascal Vincent.2019. Unreproducible research is reproducible. In In-ternational Conference on Machine Learning, pages725734. PMLR": "Ting-Yun Chang and Robin Jia. 2023. Data curationalone can stabilize in-context learning. In Proceed-ings of the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 81238144, Toronto, Canada. Association forComputational Linguistics. Hyung Won Chung, Le Hou, Shayne Longpre, Bar-ret Zoph, Yi Tay, William Fedus, Eric Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, et al.2022. Scaling instruction-finetuned language models.arXiv preprint arXiv:2210.11416. Alexandru Cioba, Michael Bromberg, Qian Wang,Ritwik Niyogi, Georgios Batzolis, Jezabel Garcia,Da-shan Shiu, and Alberto Bernacchia. 2022. Howto Distribute Data across Tasks for Meta-Learning?Proceedings of the AAAI Conference on ArtificialIntelligence, 36(6):63946401. Number: 6. Alice Coucke, Alaa Saade, Adrien Ball, ThodoreBluche, Alexandre Caulier, David Leroy, ClmentDoumouro, Thibault Gisselbrecht, Francesco Calta-girone, Thibaut Lavril, et al. 2018. Snips voice plat-form: an embedded spoken language understandingsystem for private-by-design voice interfaces. arXivpreprint arXiv:1805.10190. Verna Dankers and Ivan Titov. 2022.Recursiveneural networks with bottlenecks diagnose (non-)compositionality. In Findings of the Associationfor Computational Linguistics: EMNLP 2022, pages43614378, Abu Dhabi, United Arab Emirates. As-sociation for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics. Jesse Dodge, Gabriel Ilharco, Roy Schwartz, AliFarhadi, Hannaneh Hajishirzi, and Noah Smith. 2020.Fine-Tuning Pretrained Language Models: WeightInitializations, Data Orders, and Early Stopping.ArXiv:2002.06305 [cs].",
  "Odd Erik Gundersen, Kevin Coakley, and ChristineKirkpatrick. 2022.Sources of irreproducibilityin machine learning: A review.arXiv preprintarXiv:2204.07610": "Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023. Mistral7b. arXiv preprint arXiv:2310.06825. Abdullatif Kksal, Timo Schick, and Hinrich Schuetze.2023. MEAL: Stable and active learning for few-shotprompting. In Findings of the Association for Compu-tational Linguistics: EMNLP 2023, pages 506517,Singapore. Association for Computational Linguis-tics.",
  "Changmao Li and Jeffrey Flanigan. 2023. Task con-tamination: Language models may not be few-shotanymore. arXiv preprint arXiv:2312.16337": "Xiaonan Li and Xipeng Qiu. 2023. Finding supportexamples for in-context learning. In Findings of theAssociation for Computational Linguistics: EMNLP2023, pages 62196235, Singapore. Association forComputational Linguistics. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,Lawrence Carin, and Weizhu Chen. 2022.Whatmakes good in-context examples for GPT-3?InProceedings of Deep Learning Inside Out (DeeLIO2022): The 3rd Workshop on Knowledge Extrac-tion and Integration for Deep Learning Architectures,pages 100114, Dublin, Ireland and Online. Associa-tion for Computational Linguistics. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,Hiroaki Hayashi, and Graham Neubig. 2023. Pre-train, prompt, and predict: A systematic survey ofprompting methods in natural language processing.ACM Computing Surveys, 55(9):135. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A robustly optimized bert pretraining ap-proach. arXiv preprint arXiv:1907.11692. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,and Pontus Stenetorp. 2022. Fantastically orderedprompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the60th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages80868098, Dublin, Ireland. Association for Compu-tational Linguistics. Costas Mavromatis,Balasubramaniam Srinivasan,Zhengyuan Shen, Jiani Zhang, Huzefa Rangwala,Christos Faloutsos, and George Karypis. 2023.Which examples to annotate for in-context learn-ing? towards effective and efficient selection. arXivpreprint arXiv:2310.20046. R. Thomas McCoy, Junghyun Min, and Tal Linzen.2020. BERTs of a feather do not generalize together:Large variability in generalization across models withsimilar test set performance. In Proceedings of theThird BlackboxNLP Workshop on Analyzing and In-terpreting Neural Networks for NLP, pages 217227,Online. Association for Computational Linguistics. Marius Mosbach, Maksym Andriushchenko, and Diet-rich Klakow. 2021. On the Stability of Fine-tuningBERT: Misconceptions, Explanations, and StrongBaselines. In International Conference on LearningRepresentations.",
  "Alex Nichol, Joshua Achiam, and John Schulman.2018. On first-order meta-learning algorithms. arXivpreprint arXiv:1803.02999": "Branislav Pecher, Jan Cegin, Robert Belanec, JakubSimko, Ivan Srba, and Maria Bielikova. 2024a. Fight-ing randomness with randomness: Mitigating op-timisation instability of fine-tuning using delayedensemble and noisy interpolation. arXiv preprintarXiv:2406.12471. Branislav Pecher, Ivan Srba, and Maria Bielikova.2024b. A survey on stability of learning with limitedlabelled data and its sensitivity to the effects of ran-domness. ACM Computing Surveys. Just Accepted.",
  "Pouya Pezeshkpour and Estevam Hruschka. 2023.Large language models sensitivity to the order ofoptions in multiple-choice questions. arXiv preprintarXiv:2308.11483": "Hung Viet Pham, Shangshu Qian, Jiannan Wang,Thibaud Lutellier, Jonathan Rosenthal, Lin Tan, Yao-liang Yu, and Nachiappan Nagappan. 2021. Prob-lems and opportunities in training deep learning soft-ware systems: an analysis of variance. In Proceed-ings of the 35th IEEE/ACM International Conferenceon Automated Software Engineering, ASE 20, pages771783, New York, NY, USA. Association for Com-puting Machinery. Nils Reimers and Iryna Gurevych. 2017. Reportingscore distributions makes a difference: Performancestudy of LSTM-networks for sequence tagging. InProceedings of the 2017 Conference on EmpiricalMethods in Natural Language Processing, pages 338348, Copenhagen, Denmark. Association for Compu-tational Linguistics. Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and AlaneSuhr. 2023. Quantifying language models sensitiv-ity to spurious features in prompt design or: How ilearned to start worrying about prompt formatting.arXiv preprint arXiv:2310.11324. Thibault Sellam, Steve Yadlowsky, Ian Tenney, JasonWei, Naomi Saphra, Alexander DAmour, Tal Linzen,Jasmijn Bastings, Iulia Turc, Jacob Eisenstein, Dipan-jan Das, and Ellie Pavlick. 2022. The MultiBERTs:BERT Reproductions for Robustness Analysis. In In-ternational Conference on Learning Representations,page 30.",
  "Amrith Setlur, Oscar Li, and Virginia Smith. 2021. IsSupport Set Diversity Necessary for Meta-Learning?ArXiv:2011.14048 [cs, stat]": "Jake Snell, Kevin Swersky, and Richard Zemel. 2017.Prototypical networks for few-shot learning. In Pro-ceedings of the 31st International Conference on Neu-ral Information Processing Systems, NIPS17, page40804090, Red Hook, NY, USA. Curran AssociatesInc. Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Ng, andChristopher Potts. 2013. Recursive deep models forsemantic compositionality over a sentiment treebank.In Proceedings of the 2013 Conference on Empiri-cal Methods in Natural Language Processing, pages16311642, Seattle, Washington, USA. Associationfor Computational Linguistics. Cecilia Summers and Michael J. Dinneen. 2021. Non-determinism and Instability in Neural Network Op-timization. In Proceedings of the 38th InternationalConference on Machine Learning, pages 99139922.PMLR. ISSN: 2640-3498. Xiaofei Sun, Linfeng Dong, Xiaoya Li, Zhen Wan,Shuhe Wang, Tianwei Zhang, Jiwei Li, Fei Cheng,Lingjuan Lyu, Fei Wu, et al. 2023.Pushing thelimits of chatgpt on nlp tasks.arXiv preprintarXiv:2306.09719. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Lewis Tunstall, Edward Beeching, Nathan Lambert,Nazneen Rajani, Kashif Rasul, Younes Belkada,Shengyi Huang, Leandro von Werra, ClmentineFourrier, Nathan Habib, et al. 2023. Zephyr: Di-rect distillation of lm alignment.arXiv preprintarXiv:2310.16944. Ellen M. Voorhees and Dawn M. Tice. 2000. Buildinga question answering test collection. In Proceedingsof the 23rd Annual International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval, SIGIR 00, page 200207, New York, NY,USA. Association for Computing Machinery.",
  "Lucas Weber, Elia Bruni, and Dieuwke Hupkes. 2023": "Mind the instructions: a holistic evaluation of con-sistency and interactions in prompt-based learning.In Proceedings of the 27th Conference on Computa-tional Natural Language Learning (CoNLL), pages294313, Singapore. Association for ComputationalLinguistics. Albert Webson and Ellie Pavlick. 2022. Do prompt-based models really understand the meaning of theirprompts? In Proceedings of the 2022 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 23002344, Seattle, United States.Association for Computational Linguistics.",
  "Mengjie Zhao, Yi Zhu, Ehsan Shareghi, Ivan Vulic,Roi Reichart, Anna Korhonen, and Hinrich Schtze": "2021a. A closer look at few-shot crosslingual trans-fer: The choice of shots matters. In Proceedingsof the 59th Annual Meeting of the Association forComputational Linguistics and the 11th InternationalJoint Conference on Natural Language Processing(Volume 1: Long Papers), pages 57515767, Online.Association for Computational Linguistics. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, andSameer Singh. 2021b. Calibrate Before Use: Im-proving Few-shot Performance of Language Models.In Proceedings of the 38th International Conferenceon Machine Learning, pages 1269712706. PMLR.ISSN: 2640-3498. Ruiqi Zhong, Dhruba Ghosh, Dan Klein, and JacobSteinhardt. 2021. Are larger pretrained languagemodels uniformly better? comparing performanceat the instance level. In Findings of the Associationfor Computational Linguistics: ACL-IJCNLP 2021,pages 38133827, Online. Association for Computa-tional Linguistics.",
  "AEthical Considerations and ImpactStatement": "The experiments in this paper work with publiclyavailable benchmark dataset GLUE, and publiclyavailable datasets AG News, TREC, SNIPS andDB-Pedia, citing the original authors. As we werenot able to determine the license for the tasks anddatasets used, we opted to use them in as limitedform as possible, adhering to the terms of use (noannotation of the test set) for the GLUE benchmarkdataset and applying it to other datasets as well. Wedo not work with any personally identifiable infor-mation or offensive content and perform no crowd-sourcing for further data annotation. In addition,we are not aware of any potential ethical harms ornegative societal impacts of our work, apart fromthe ones related to the advancement of the fieldof Machine Learning and Learning with LimitedLabelled Data, which includes the in-context learn-ing, transfer learning, meta-learning and languagemodel subsets. Finally, we follow the license termsfor all the models we use (such as the one requiredfor the use of the LLaMA-2 model) all modelsand datasets allow their use as part of research. It ispossible the large language models we used (Flan-T5, LLaMA-2, Mistral and Zephyr) contain biasesand may generate potentially offensive or harmfulcontent. However, the authors of the models reduce this potential bias as much as possible when train-ing the models, while at the same time we limitthe output to few tokens and do not release anyoutput of the models, which should further reducethe potential bias and negative impact. Impact Statement: CO2 Emissions Related toExperimentsThe experiments presented in thispaper used significant compute resources as theyrequired multiple training and evaluation runs ofmultiple models, as well as using large languagemodels that require a lot of computation even justfor the inference. Overall, the experiments wereconducted using a private infrastructure, which hasa carbon efficiency of 0.432 kgCO2eq/kWh (de-fault value used as the actual efficiency of our HWinstance was not measured). A cumulative of 1440hours of computation was performed on hardwareof type RTX 3090 (TDP of 350W) and a cumulativeof 4000 hours of computation was performed onhardware of type A100 PCIe 40GB (TDP of 250W).The hours of computation used are only a crudeapproximation, as the machine used was sharedamong multiple projects. Total emissions are es-timated to be 217.73 kgCO2eq (for the first set ofhardware) and 432 kgCO2eq (for the second set ofhardware), of which 0 percents were directly offset.These estimations were conducted using the Ma-chineLearning Impact calculator presented in (La-coste et al., 2019). Whenever possible, we tried toreduce the compute resources used as much as pos-sible. The most compute resources were used bythe large language model LLaMA-2, Mistral-7Band Zephyr-7B. To reduce the computation costsand resources used, we decided to evaluate themodel on lower number of runs (10 investigationand 20 mitigation, resulting in 200 runs for eachrandomness factors) using only 1 000 test samplesfor evaluation. Even in this reduced evaluation, theexperiments using these models used the most GPUhours. To further reduce the compute resources, weuse the 4-bit quantised versions of these models,while also opting to use smaller models for themore detailed analyses and ablation studies, eitherin case of in-context learning (e.g., using Flan-T5and Mistral-7B instead of LLaMA-2 for studyingthe impact of number of shots that significantlyincreased the required computation resources andinference time), but also in case of transfer learningand meta-learning (e.g., using base versions of theBERT and RoBERTa models).",
  "BAdditional Resources Describing theProposed Investigation Method": "In this Appendix, we provide additional supplemen-tary resources that should allow for easier under-standing of how the method operates. The proposedinvestigation method is designed for investigatingeffects of any randomness factor, while explicitlytaking interactions with effects of other factors intoconsideration, and measuring the importance of thefound effects. Overall, the effects are investigatedby observing how the performance changes acrossthe different states the investigated randomness fac-tors can appear in. To deal with the interactions,the effects of remaining randomness factors aremitigated (i.e., the deviation they contribute is re-duced as much as possible). To determine the im-portance, we compare the contributed deviation ofthe investigated randomness factor with effects ofother factors, and with the overall deviation from agolden model. The golden model represents the ob-jective estimate of the performance deviation andis obtained by training a model while mitigating allthe randomness factors at the same time. The finalimportance score is then determined as the frac-tion of the overall deviation (represented using thegolden model) the investigated factor contributesover all the remaining, non-investigated factors.The following section provides a high-leveloverview of the method with references to the Al-gorithm 1 (Appendix B.1), the illustration of howthe method operates and the results it computes(Appendix B.2). We also provide a method for se-lecting the number of investigation and mitigationruns in Appendix B.3 (this method was used toselect the samples in this paper using the AblationStudy in Appendix 3 and to produce the heuristicsat the end of ).",
  "B.1Algorithmic Description of the Method": "To allow for better understanding of our proposedinvestigation method, we provide more informaldescription of the steps composed in Algorithm 1,along with references to individual lines in it andpossible avenues for extension of our method. In-formally our proposed method works in a followingway: 1. A set of randomness factors for investigationis first identified along with their configura-tions. In case of mitigated randomness factors,a complete set of factors and their configura-tions is not required to prevent introduction of biases into the results, as the randomness fac-tors can be controlled on the group level. Allthe algorithmic factors (order, initialisation,model randomness, etc.) can be controlled byglobally setting the seed, while the implemen-tation/hardware level factors can be controlledusing the same setup across all experiments(same library versions, architectures, GPUs,etc.). 2. A single investigation run is performed for aselected investigated randomness factor (re-peating and evaluating training multiple times,each time with different configuration of theselected randomness factor, e.g., with differ-ent split of data, choice of data, or their order),while keeping the configuration of all other(non-investigated) randomness factors fixed.(inner loop; lines 5-7 in the Algorithm 1) 3. The method can be easily extended to investi-gate effect of multiple factors at the same time,by simply changing the definition of the in-vestigated factor configuration set, to includea cross product between the configurationsof multiple factors (similarly to the mitigatedfactor configuration set). (lines 2 and 3 in theAlgorithm 1)",
  ". The single investigation run is evaluated toobtain partial standard deviation and partialmean. (lines 8-9 in the Algorithm 1)": "5. The configuration of all other randomness fac-tors is fixed to a new value and the investi-gation run is repeated again to mitigate theeffects of non-investigated randomness fac-tors (each such repeat is called mitigation run).(outer loop; lines 4-10 in the Algorithm 1) 6. Instead of repeating multiple mitigation runs,the method can be extended to use a specificmitigation strategy (such as sample selectionmethod for in-context learning). Using suchmethod, the set of configurations for the givenrandomness factor is simply replaced by theresults of the mitigation strategy (either a setof single value or a subset that is significantlysmaller). The rest of the method remains un-changed. (line 3 in the Algorithm 1)",
  ". Afterenoughconfigurationsofnon-investigatedrandomnessfactors(i.e.,mitigation runs) are searched through and": "enough runs of training and evaluation areperformed, the partial standard deviationsare averaged to produce the contributedstandard deviation, and the partial meansare aggregated (by taking their standarddeviation) to produce the mitigated standarddeviation. (lines 11-12 in the Algorithm 1) 8. The golden model standard deviation is calcu-lated by simply performing training and evalu-ation multiple times with differently fixed con-figuration of all randomness factors. If enoughoverall runs are used, the golden model stan-dard deviation can be replaced by simply tak-ing the standard deviation over all the runs inthe investigation. However, this may lead toincorrect results. (final loop; lines 13-17 inthe Algorithm 1) 9. The importance score of the investigated fac-tor is calculated as a fraction of the goldenmodel standard deviation of the differencebetween contributed standard deviation andthe mitigated standard deviation (to determinehow much more the investigated factor con-tributes over all the mitigated ones). Any ran-domness factor with importance score over 0is considered significantly important, as suchfactors contribute the same amount of devia-tion as the combination of all the remainingfactors. At the same time, the size of the im-portance value determines the overall impor-tance of the model (i.e., factor with impor-tance score of 0.6 is more important than theones with score of 0.1). (the final check; lines18-21 in the Algorithm 1)",
  "B.2Illustration of the Method and its Results": "In this section, we provide the visualisation of themethod in a form of table. In essence, when in-vestigating the specific randomness factor, whilemitigating the effects of other randomness factors,we fill in such table as illustrated in . Thecolumns represent the different configurations forthe investigated factor. Observing how the per-formance changes across these columns, we candetermine the effects of the randomness factors aggregating across these columns we obtain the par-tial mean p_mean and partial standard deviationp_std.However, having only a single row would notdeal with the interactions. Therefore we performthis investigation multiple times, each time with different randomly fixed combination of configura-tions for all the other, non-investigated randomnessfactors. Each such repeat of the investigation runrepresents a single row in the table, each with itsown partial mean p_meanm and partial standarddeviation p_stdm.To get the final contributed standard deviationc_std for the investigated randomness factor, weaggregate over these different partial standard devi-ations (c_std = p_std). In addition, to obtain themitigated standard deviation m_std we aggregateover the partial means (m_std = std(p_mean)).",
  "B.3Selecting Number of Investigation andMitigation Runs": "When selecting the number of investigation (N)and the number of mitigation (M) runs, we needto find a balance between how well the effects ofthe factors are estimated and how well the interac-tion between the effects of different randomnessfactors are mitigated, and how much computationalresources are required to get to this estimation andmitigation. An optimal solution is to use the lowestnumber of overall runs (that lead to lowest compu-tational resources) after which the change in the re-sults (the contributed/mitigated standard deviationor the normalised importance score) is under anacceptable threshold . The value of this threshold depends on the setup of the experiment and thegoal of our investigation, as in some cases higherchange in the standard deviation may be acceptable,while in others we require a more strict setting.In this section, we describe a simple method tosearch for this optimal point that can be used in-stead of the heuristics at the end of (whichwere a result of our analysis using the followingmethod). The method is composed of followingsteps: 1. The threshold of smallest acceptable change ,and the starting number of investigation runsN are selected. The number of investigationruns should be sufficiently high from the start(following recommendations in ) tomake the search faster. 2. A new mitigation run should be performedusing a randomly selected configuration ofthe non-investigated randomness (or the num-ber of investigation runs should be increased,running the new investigation runs for all thealready performed mitigation runs).",
  "std(p_mean)p_std": ": The effects of a randomness factor i are determined by observing the variability in results over itsconfigurations, while mitigating the effects of other randomness factors. The results are first grouped by themitigated factor configurations m and a partial mean (p_meanm) and standard deviation (p_stdm) is calculated.These values are then aggregated into contributed standard deviation (c_std), representing the effects of investigatedrandomness factor, by calculating a mean over the p_stdm, and into mitigated standard deviation (m_std),representing the remaining effects of mitigated randomness factors, by calculating a standard deviation overp_meanm. 3. The new values of the relevant metrics (con-tributed standard deviation, mitigated standarddeviation, or the importance score) should bedetermined and the difference to previous val-ues calculated. 4. If the observed change is lower than the thresh-old the current values of hyperparameters Nand M represent the optimal point and shouldbe used. Otherwise, continue to step 2 (in-creasing either the value of N or M). In case our goal is to use the results of the investi-gation and the importance score for a more in-depthanalysis and comparison across different factors,models, datasets or other experimental settings, themethod should be repeated for every setting andthe highest values of N and M should be used to guarantee that the comparison and analysis isdone on the same number of overall runs and to notintroduce any possible biases into the comparison.",
  "CAblation Study: Reducing Number ofMitigation Runs and Test Data Size": "As mentioned in , there is a trade-off be-tween feasibility (computations costs) of the inves-tigation and precision (reliability) of the investiga-tion results. This trade-off mainly depends on thenumber of mitigation runs (i.e., the number of con-figurations explored for the non-investigated ran-domness factors). To determine the optimal num-ber of mitigation runs, we explore this trade-offusing a modified version of the method describedin Appendix B.3: we run the investigation for alarger number of mitigation runs (observing thebehaviour even after the optimal point) and explore the effects of reducing the number of mitigationruns (M) and the number of test samples usedfor evaluation on the results and how well they es-timate the overall contributed effects and how wellthe interactions are mitigated. We perform this ab-lation study for the Flan-T5 model on the SST2dataset and report only specific interesting points. As the baseline for this ablation study we workwith the setting of using 100 mitigation runs (with10 investigation runs) and 100% of test samples.For the number of mitigation runs, we explore: 1)increasing the number significantly (to 500); 2) re-ducing the number to 10% (10 mitigation runs).For the number of test samples, we explore reduc-ing the set to: 1) 1 000 samples (which representsapproximately 10% of overall test samples); and2) 500 samples (representing approximately 5% ofoverall test samples). We also explore the combi-nation of both reductions (in relevant cases). Theresults of this ablation study are available in Ta-ble 3. Compared to our baseline setting for the exper-iments (100 mitigation runs, with 100% of testsamples used), increasing the number of mitigationruns by 500% does not lead to a significantly morereliable results. We can observe a slight change inoverall standard deviation in the model (rangingfrom a change of 0.01 to change of 0.21). Simi-larly, the observed contributed standard deviation,as well as the mitigated standard deviation staysapproximately the same (the change ranging from0.005 to 0.1). In addition, the change in importancescore is negligible for the different factors. All inall, we can conclude that increasing the number ofmitigation runs any further does not make sense in",
  "GOLDENF1 macro (%)78.2378.1778.2578.1878.1378.16MODELF1 std2.312.242.092.502.352.97": "LABELF1 macro (%)78.2678.1478.1778.0777.8777.72SELECTIONF1 std2.282.412.442.612.943.20Contributed std2.0732.1672.1352.2042.1742.188Mitigated std0.7970.9040.9461.2781.8062.193Importance0.550.560.570.370.16-0.00 DATAF1 macro (%)78.1878.2477.9878.3978.2278.33SPLITF1 std2.292.302.302.552.592.85Contributed std2.1122.1282.1382.3722.4222.670Mitigated std0.7120.6930.6620.7080.7290.788Importance0.610.640.710.670.720.63 DATAF1 macro (%)78.1478.2877.2978.2277.1076.82ORDERF1 std2.282.152.592.343.183.25Contributed std0.8460.8690.9820.9021.0951.149Mitigated std2.0891.9282.3342.1172.9322.988Importance-0.54-0.47-0.65-0.49-0.78-0.62 SAMPLEF1 macro (%)78.2278.1978.1578.1477.9277.80CHOICEF1 std2.142.352.642.552.873.05Contributed std2.1382.1232.3612.1522.3372.325Mitigated std0.8180.8441.0011.2481.5531.906Importance0.570.570.650.360.330.14 : The effects of changing the number of mitigation runs and the number of samples used for evaluation onthe results of our proposed investigation method when applied to Flan-T5 model used with SST-2 dataset. Thecolumn with 100 mitigation runs and 100% test data represents our baseline setting. With decreasing number ofmitigation runs and the size of test data used, the mitigated standard deviation, as well as the overall standarddeviation increases, while the contributed standard deviation stays approximately the same. This leads to lowerprecision of the results and change in the importance score of the different factors, and even can lead to incorrectresults in extreme cases (Label Selection not being considered important when using 0.5% of data as compared toour baseline setting). Even with 1% of computation (combination of mitigation runs and test sample reduction)the findings can be considered sufficiently reliable in this setting.",
  "regards to the reliability-feasibility trade-off": "On the other hand, reducing the number of mit-igation runs and the number of test samples usedfor evaluation, we can observe more significantchanges in the overall variance in the model andthe importance score of the factors. We can ob-serve a progressive increase in the overall goldenmodel standard deviation (from 2.24 up to 2.97 inthe most extreme setting). At the same time, wealso observe significant increase in the mitigatedstandard deviation (going from as low as 0.904 inthe Label Selection randomness factor up to 2.193for the same factor in the most extreme setting),which can be expected as the number of mitigationruns governs the mitigation of non-investigated,interacting randomness factors in our method. Sim-ilarly, we can observe change in the importancescore as well, with the importance of different fac-tors being lower with lower number of mitigationruns (with the exception of Data Split random- ness factor). In the most extreme setting (using10 mitigation runs and 500 test samples, which rep-resents 0.5% of baselines setting data) we caneven observe a change in the findings regarding theLabel Selection randomness factor it becomesnon-important as it is overshadowed by the mit-igated randomness factors, with the importancescore being slightly below the 0 value. However,the less extreme setting, where 1% of the base-line setting data is used (10 mitigation runs and1000 samples), the results are still reliable enough(even though the importance score is lower in thiscase). In addition, the difference in importancescore when using smaller amount of test samples ismore significant than when using smaller numbermitigation runs (i.e., 0.36 and 0.33 importance with10% test data while using 100 and 10 mitigationruns respectively, as compared to 0.57 and 0.65when using full test data and 100 and 10 mitigationruns respectively). All in all, we can conclude that our proposed method is not as dependent on thenumber of mitigation runs and not as computation-ally expensive as can be expected, making it moreeasily usable on more computationally expensivesettings (e.g., having large labelled datasets or us-ing more computationally expensive models). Atthe same time, the importance score is dependenton the number of test samples used for evaluation,which needs to be taken into consideration when us-ing it on setting such as in-context learning, wherethe inference itself is quite expensive. Even when reducing the computation cost ofthe proposed method to 1% of our baseline set-ting (reducing the number of mitigation runs to10 and using only 1 000 test samples for evalu-ation) the findings can be considered sufficientlyreliable. Therefore, if the precision of the results isnot as paramount, the proposed method can be usedeven in this reduced setting (although one needsto be aware of the implications). To produce moreprecise results, and due to the significant computa-tion costs of running the larger in-context learningmodels (LLaMA-2, Mistral and Zephyr), we havedecided to run the investigation using 20 mitigationruns and 1 000 test samples (following the practicein related work (Gao et al., 2021; Chang and Jia,2023; Sclar et al., 2023; Li and Qiu, 2023; Kksalet al., 2023)). As such, the observed importancescores for different factors may be affected by thischoice, but the findings regarding the importanceshould still hold. In addition, to keep the compar-ison between models as unbiased as possible, weuse the same amount of test data for all the modelsand all the datasets and across all experiments. Based on the observed behaviour, we can de-termine which factor affects the variability of themodel results the most Data Split. For all therandomness factors, except for the Data Split, onlythe mitigated standard deviation increases whenreducing the number of mitigation runs and/or thenumber of samples, while the contributed standarddeviation stays approximately the same. However,for the Data Split randomness factor, the exact op-posite happens (contributed std increases, whilemitigated std stays the same). In essence, havingmore mitigation runs and/or using more test sam-ples for evaluation leads to a significant mitigationof the variance from the data split randomness fac-tor.",
  "DExperimental Setup andImplementation Details": "All the experiments in this paper are using En-glish only datasets from the GLUE (Wang et al.,2018) benchmark suite and other publicly avail-able datasets. The datasets from GLUE benchmark,SST2 (Dankers and Titov, 2022), CoLA (Warstadtet al., 2019) and MRPC (Dolan and Brockett,2005), are all binary classification datasets us-ing only 2 classes. The remaining datasets rep-resent a multi-class classification problems, withthe AG News (Zhang et al., 2015) dataset consist-ing of 4 classes, TREC (Voorhees and Tice, 2000)dataset consisting of 6 classes, SNIPS (Couckeet al., 2018) dataset consisting of 7 classes and DB-Pedia (Lehmann et al., 2015) dataset consisting of14 classes.Based on the ablation study (included in Ap-pendix 3), we use 10 investigation and 20 miti-gation runs (resulting in overall 200 training andevaluation runs) for the in-context learning (Flan-T5, LLaMA-2, Mistral-7B and Zephyr-7B) and 100mitigation runs (results in overall 1 000 trainingand evaluation runs) for the other approaches thatuse smaller models (BERT, RoBERTa). Followingthe practice from the related work (e.g., (Gao et al.,2021; Chang and Jia, 2023; Sclar et al., 2023; Liand Qiu, 2023; Kksal et al., 2023)) and the resultsof our ablation study, we evaluate each run usingonly 1 000 test samples (the selection is governedby the Label Selection randomness factor). Themain reason is the computation cost of the infer-ence for the large language models. To preventintroduction of any biases into the comparison, weuse the same amount of test samples for the transferlearning and meta-learning as well (although weuse larger number of runs results in larger distribu-tions in those cases). These decisions representsthe trade-off between feasibility/required computa-tion costs to achieve the results and how well theeffects of randomness factors are estimated and theinteractions between them mitigated.Besides the factors that we focus our investiga-tion on (Label Selection, Data Split, Model Initial-isation, Data Order and Sample Choice), we alsofocus on mitigating other factors that we call asModel Randomness. This group of factors en-compasses the randomness originating from useof non-deterministic operations in the model (e.g.,dropout or sampling in the in-context learning mod-els that generate text) and from implementation",
  "DatasetID VerbaliserPrompt Format": "SST-2A{Negative, Positive}Determine sentiment of the sentence using following options: 1)[Class 1] 2) [Class 2].[Input][Output]BSame as above[Input] Sentiment? [Output]CSame as above[Input] Sentiment is [Output]D{terrible, great}[Input] It was [Output] CoLAA{No, Yes}Determine grammatical acceptability of the sentence using fol-lowing options: 1) [Class 1] 2) [Class 2].[Input][Output]BSame as above[Input] Grammatically acceptable? [Output]C{Yes, No}[Input] Grammar problems? [Output]D{not acceptable, acceptable}[Input] It is [Output] MRPCA{No, Yes}Determine whether the sentence pair is semantically equivalentusing following options: 1) [Class 1] 2) [Class 2].[Input][Output]BSame as above[Input] Semantically equivalent sentences? [Output]C{Yes, No}[Input] Semantically different sentences? [Output]D{not equivalent, equivalent}[Input] Sentences are [Output] AG News A{World, Sports, Business, Science and Tech-nology}Determine topic of the sentence using following options: 1)[Class 1] 2) [Class 2] ... N) [Class N].[Input][Output]BSame as above[Input] Topic? [Output]CSame as above[Input] Topic is [Output]DSame as aboveUser: [Input] This is about [Output] TRECA{Expression, Entity, Description, Human, Lo-cation, Number}Determine topic of the sentence using following options: 1)[Class 1] 2) [Class 2] ... N) [Class N].[Input][Output]BSame as above[Input] Topic? [Output]CSame as above[Input] Topic is [Output]DSame as aboveUser: [Input] This is about [Output] SNIPSA{Playlist, Weather, Event, Musing, CreativeWork, Rate Book, Book Restaurant}Determine intent of the sentence using following options: 1)[Class 1] 2) [Class 2] ... N) [Class N].[Input][Output]BSame as above[Input] Intent? [Output]CSame as above[Input] Intent is [Output]DSame as aboveUser: [Input] User requested [Output]",
  "DB-Pedia A{Company, Educational Institution, Artist,Athlete, Office Holder, Transportation, Build-ing, Natural Place, Village, Animal, Plant,Album, Film, Written Work}": "Determine topic of the sentence using following options: 1)[Class 1] 2) [Class 2] ... N) [Class N].[Input][Output]BSame as above[Input] Topic? [Output]CSame as above[Input] Topic is [Output]DSame as aboveUser: [Input] This is about [Output] : Prompt formats and verbalisers used for different datasets in the paper. The [Class 1-N] are replaced withthe names of the classes as defined by the verbaliser. The [Input] is replaced by the sentence of the samples and the[Output] is replaced with the name of class as defined by the verbaliser. The [Input] and [Output] are repeated foreach in-context sample, while the final [Output] is used to determine the predicted class. The same format is usedfor all the language models (Flan-T5, LLaMA-2-13B, Mistral-7B and Zephyr-7B).",
  "versions and the same GPUs throughout the experi-ments (one exception are the meta-learning experi-ments which were done on a separate GPU), whilealso setting a specific random seed that governs the": "non-deterministic operations in the models duringtraining and inference (this seed is explored usingthe mitigation runs, so each experiment explored20 or 100 different sets of this non-determinism).For the in-context learning models, we use theFlan-T5 base model2, the LLaMA-2 13B instruc-tion optimised model3, Mistral-7B instruct fine-tuned model4 and Zephyr-7B instruct fine-tunedmodel5 (alpha version as it worked better on theclassification tasks than the beta model, due to thebeta model generating large quantities of text andmultiple classes at the same time). The LLaMA-2, Mistral and Zephyr models are all used in the4-bit quantised setting. All of these models are setto produce deterministic output, while the numberof tokens they can generate is limited to 10. Inthe majority of the setting, we use 2 samples perclass, which are randomly sampled from the traindataset. We use only 2 samples, as the Flan-T5model falls apart and starts predicting a single classfor every test sample when using larger number ofsamples. We perform only a basic prompt engi-neering for these models (exploring also optimalprompt formats from related research papers (Liand Qiu, 2023; Gao et al., 2021; Kksal et al.,2023), the prompt format recommended for theLLaMA-2 model, and taking inspiration from (Sunet al., 2023)), while also using the meta-tags thatspecify instruction for the models. The optimalprompt-format, as well as other formats used inthe analyses, is illustrated in Tabled 4. In casethe models produce multiple words that can bemapped to multiple classes (with the exception ofspecific prompts where some classes are subsets ofeach other), we treat the output as incorrect withthe assumption the model is just hallucinating (al-though we noticed the Mistral and Zephyr modelsprovide more detailed answers, especially on theSST2 dataset, which may lower their performancein this case).Forthefine-tuningmodels,BERT6andRoBERTa7, we use the base version of the pre-trained models from HuggingFace (Wolf et al., 2019). Both models are trained in full (withoutfreezing the pre-trained part) on all datasets usinglearning rate of 1e-5 for 5 epochs on binary and 10epochs on multi-class dataset, using early stopping,AdamW optimiser with warmup for 10% of thesteps and batch size 8.As the basis for the meta-learning approaches,we use the implementation released by the authorsof the specific papers when possible, while the indi-vidual implementations are extended and modifiedto better work with our proposed method for inves-tigation. In case of the Prototypical Networks, wedirectly use the code released by the authors8. Incase of Model Agnostic Meta-Learning, we use theimplementation from the Torchmeta library9. Incase of Reptile, we use our own implementationbased on the code released for the approach10. Formeta-learning, we use the same base model acrossall the meta-learning approaches. This model is asimple fully-connected layer with 128 neurons anda final classification layer on top of the BERT basemodel. Each meta-learning approach is trainedin a 2-way 5-shot learning setup. For evaluation,the meta-learning models are first adapted usinga single set of examples in 2-way 15-shot setting(examples are chosen based on the sample choicerandomness factor) and then evaluated on the wholetest dataset.All the hyperparameters for all the models areset using a separate hyperparameter optimisationfor both fine-tuning and meta-learning (we run nohyperparameter optimisation for in-context learn-ing) using the validation data selected from the 1000 training samples. This hyperparameter opti-misation is done in a two-level fashion. First, theoptimisation is run using large differences in thehyperparameter values, to find the approximate setof hyperparameters that should provide good per-formance on the given dataset. In the second step,we explore the hyperparameter space around theseapproximate hyperparameters, to find the optimalset of parameters. However, it is important to notethat the hyperparameter search is performed on afixed set of labelled samples, chosen beforehand,and on a single split, which may affect the opti-mal set of hyperparameters and lead to sub-optimal hyperparameters, especially in meta-learning.When choosing the hyperparameter values in thefirst level, we draw inspiration from related work,using the optimal parameters reported in papers thatpropose, or use these approaches (such as (Dodgeet al., 2020; McCoy et al., 2020; Mosbach et al.,2021; Sellam et al., 2022). However, we also searchthrough additional hyperparameter values besidesthose reported in related works to better explorethe parameter space and obtain as precise resultsfrom the investigation as possible.",
  "EValidating the Proposed InvestigationMethod": "In this Appendix, we provide further informationon how the proposed investigation method was val-idated. As there is no ground-truth of the effectsof randomness to compare against, the validity ofmethods for investigating the effects of randomnesscan be evaluated only indirectly. In this paper, weperform such indirect evaluation/validation by: 1. Evaluating the properties and benefits ofthe proposed methods by comparing it tothe existing ones. As discussed in the maincontent of the paper, the benefits of the pro-posed methods are: 1) importance score thatcan be used for more in-depth analysis (rela-tive ordering of randomness factors and com-parison across models, datasets and other ex-perimental settings), as opposed to determin-ing the importance only in binary fashion fromprevious works (factor is or is not important);and 2) handling interactions between effectsof randomness factors, which, when previ-ously ignored or not being addressed suffi-ciently caused inconsistencies in findings. Wediscuss this validation further in Appendix E.1(which we consider as the main validation ofthe method). 2. Exploring how the results and findingschange as we change the overall number ofruns. The results and findings of the methodare dependent on the choice of how many in-vestigation and mitigation runs are used. Weobserve a trade-off between how well the re-sults are estimated (higher number of investi-gation runs leads to better estimation) and theinteractions mitigation (higher number of miti-gation runs leads to better mitigation), and thecomputation costs required to achieve the re-sults (increasing the number of runs increases",
  "the overall costs). We discuss this validationfurther in Appendix E.2": "3. Applying the method to different settings(factors, models, datasets) and observingthe consistency of its results and findings.As the investigation method is designed tobe general, it should be applied across dif-ferent experimental settings without showingany problems (i.e., working out-of-the-box onmultiple factors, models and datasets). We dis-cuss this validation further in Appendix E.3.",
  "E.1Additional Results: Validation of MethodThrough Comparison with TypicalInvestigation Strategies": "To showcase the impact of interactions betweenrandomness factors on the investigation of the ef-fects of different randomness factors, and to show-case the properties and benefits of our proposedmethod, we provide a comparison between the typ-ical investigation strategies from related work andour proposed method: Random investigation strategy without anyconstraints on the randomness factor configu-rations. For each training and evaluation runof the model, all the randomness factors arevaried, while only the impact of a specific fac-tor is observed. For example, each trainingand evaluation is done on a different set oftraining and testing data, with different orderin training and with different random modelinitialisation, regardless which randomnessfactor is investigated. This represents the typ-ical investigation process when consideringonly the random seed randomness factor. Thisinvestigation strategy does not consider anyimpact of interactions between randomnessfactors. As there is no change in how the indi-vidual randomness factor is investigated, weexpect most skewed results from this investi-gation strategy, with each randomness factorsshowing approximately similar effects. Fixed investigation strategy where the in-teractions are addressed by fixing the non-investigated randomness factors to a singlerandomness factor configuration. For exam-ple, each training and evaluation is done onthe same set of training and testing data, withthe data in the same order, but each time withdifferent random initialisation of the model.",
  "BERTRANDOMFIXEDINTERACTIONSRANDOMFIXEDINTERACTIONSRANDOMFIXEDINTERACTIONS": "GOLDEN MODEL1.2391.2391.2391.6671.6671.6670.4860.4860.486LABEL SELECT.(*) 1.202(*) 0.923(*) 0.979(*) 1.600(*) 1.513(*) 1.348(*) 0.559(*) 0.405(*) 0.308DATA SPLIT(*) 1.462(*) 1.365(*) 1.164(*) 1.502(*) 1.513(*) 1.568(*) 0.401(*) 0.426(*) 0.294MODEL INIT.(*) 1.142(*) 1.0470.693(*) 1.926(*) 1.1080.939(*) 0.479(*) 0.6350.121DATA ORDER(*) 1.335(*) 0.7140.686(*) 1.666(*) 1.3911.019(*) 0.4710.1730.103 : Comparison of different investigation strategies for the Flan-T5, Zephyr-7B and BERT fine-tuning on thebinary datasets (SST2, CoLA and MRPC) and the multi-class datasets (AG News, TREC and SNIPS). Comparisonfor the DB Pedia dataset is not included as Flan-T5 model shows poor performance on this particular dataset. TheRandom strategy simply repeats the training and evaluation multiple times without any constraints. In the Fixedstrategy, the randomness factor configuration is kept fixed to a single state during investigation. We compare theseinvestigation strategies with our proposed method. We run each investigation strategy the same number of times(number of runs is governed by our method). Our method (Interactions) takes the interactions into consideration.Factors considered important for different strategies are denoted using the (*) symbol. We observe that interactionsbetween factors may cause some factors to have their importance overestimated (denoted in bold) or underestimated(denoted in italics). However, as only a single randomness factorconfiguration is used for the non-investigatedrandomness factors, the effects of the investi-gated randomness factor may still be affectedby the interactions (due to the randomly cho-sen point in the randomness factor configu- ration state space). Therefore we expect theresults to represent the effects of different ran-domness factors more accurately, but still canunder-estimate or over-estimate some effectsdue to the still present randomness in the in-vestigation. Interactions(Our)theinvestigationmethod proposed in this paper. In essencecan be viewed as repeating the Fixed investi-gation strategy multiple times, each time withdifferently fixed randomness factor configura-tions, and averaging over these repeats. To prevent introduction of any biases into thecomparisons between the strategies, we performsame number of training and evaluation runs foreach method. For each strategy, we repeat the train-ing and evaluation 1 000 times (or 200 times, asgoverned by the number of runs in our proposedmethod). The full results are presented in (except for DB Pedia dataset where the Flan-T5model does not work well). We focus on 2 mainaspects in the comparison: 1) determining impor-tance of the factors; 2) how interactions affect thefindings. Determining importance of the factors.As theRandom and Fixed strategies results only in a sin-gle score (deviation in the results), we consider thefactor to be important when it contributes at least50% of the golden model standard deviation. Assuch, the importance of the randomness factors canbe determined only in a binary fashion (factor isor is not important). Such setting allows only for alimited analysis (only relative ordering of factorsbased on the deviation withing the same setup) andcannot be easily used to compare the importanceacross different models. On the other hand, our pro-posed method provides an importance score thatcan be used for more in-depth analysis, such asthe relative ordering of randomness factors basedon their importance, or comparison across models,datasets and experimental settings (as the impor-tance score is normalised with the overall deviationin the results from the golden model). This benefitcan be illustrated using following example using and the Random/Fixed strategy, we cannotsay with good conscience that the Sample Choiceis more important for the Flan-T5 model than forthe Zephyr-7B model based only on their standarddeviation (2.370 vs. 1.052 using Random; 3.191vs. 0.406 using Fixed) as the overall deviation in re-sults is higher, but can be done so using our method(importance score from or and 9 of0.57 vs. 0.39) as the score is normalised. Or simi-larly for Data order (2.131 vs. 1.138 for Random;3.014 vs. 0.957 for Fixed; 0.47 vs. 0.44 for ourmethod) - using the importance score we see theimportance of Data Order is similar (slightly higher",
  "for Zephyr-7B) for both models, while other inves-tigation strategies show a large difference (higherimportance for Flan-T5)": "Handling interactions.The existing strategieseither ignore the interactions completely (Random)or do not addresses the sufficiently (i.e., in a waythat strongly depends on randomness in the Fixedstrategy). As such, the baseline strategies often leadto incorrect attribution of the effects of differentfactors, either due to overestimating the impact ofnon-important randomness factors, or underestimat-ing the impact of important factors. For example, inthe case of Flan-T5 in-context learning, these inves-tigation strategies indicate that all the randomnessfactors are equally important (as they contributesimilar deviation to the golden model), which isnot the case when the interactions are taken intoconsideration (when interactions are considered,the impact of data order falls off). In case of theRandom strategy, this behaviour stems from thestrategy consistently leading to the same overalldeviation/importance for all the investigated ran-domness factors (which is similar to the deviationof the Golden Model). Even though using the Fixedinvestigation strategy produces more reliable re-sults (which are more distributed and handle theinteractions to a certain extent), it is still affectedby the randomness caused by the choice of the sin-gle randomness factor configurations for the non-investigated factors. The results still show bothoverestimation and underestimation of effects forthe randomness factors. On the other hand, ourmethod is specifically designed to handle the in-teractions using the mitigation runs. Handling theinteractions this way, we observe that the findingthat the long believed sensitivity of in-context learn-ing to Data Order is actually a sensitivity to Sam-ple Choice (and potentially the choice of promptformat) when choosing samples in a more sophisti-cated manner, holds even when choosing samplesat random.All in all, our proposed method provides 2 sig-nificant benefits over the baseline strategies, whichindirectly validates its use: 1) allowing for morein-depth analysis and comparison across differentfactors, models, datasets and experimental setupsthat leads to actionable findings and read-to-applytake-away messages and suggestions (describedin experimental results in Sections 4.2, 4.3 and4.4, such as increasing the number of shots forin-context learning reduces the importance of sam-",
  "E.2Additional Results: Validation of Methodby Exploring the Changes Due toDifferent Number of Runs": "The results and findings from investigation areheavily dependent on the overall number of runs.As opposed to the baseline strategies, our proposedmethod introduces another parameter, number ofmitigation runs, to handle the interactions. Weprovide results from exploring how changing thenumber of investigation and mitigation runs affectthe results and findings (i.e., how well the effectsare estimated and the interactions mitigated) in Ap-pendix B.3 and Appendix C, while in this section,we provide a summary of relevant results.The effects of randomness factors can be esti-mated using a relatively low number of investiga-tion runs (around 6 to 8). Increasing the numberof investigation runs further does not lead to con-siderable changes in the estimated effects (the con-tributed standard deviation changes only in seconddecimal place).On the other hand, increasing the number of miti-gation runs has a larger impact on the overall resultsand findings (and the different metrics we use), asit represents the main avenue for mitigating the in-teractions. Any change to the number of mitigationruns changes all the metrics (contributed std, miti-gated std, and the importance score). In addition,the number of mitigation runs also depends on theapproach, model and dataset used. As such, it isimportant to find the optimal point, where the inter-actions are sufficiently handled without requiringextensive computation costs. To find this optimalpoint, we provide heuristics and a simple searchmethod in Appendix B.3. However, the overallnumber of required mitigation runs is still relativelylow in our experiments, we observed that using20 mitigation runs provides sufficient mitigation ofinteractions and estimation of the overall effects.Finally, we observed that the number of test sam-ples used for evaluation is the most important fac-tor influencing the estimation of the effects. Inour experiments, we observed that using 1000 testsamples for evaluation provides a good trade-offbetween the feasibility of larger scale experiments(due to computation costs) and the validity of theresults.",
  "E.3Additional Results: Validation of Methodby Observing Consistency of Results andFindings Across Different Settings": "The proposed investigation method is designedto not be dependent on any specific experimentalsetup, so that it can be used across any randomnessfactors, model, dataset or other systematic changes(e.g., number of samples, or prompt formats). Tovalidate this property of the method, we apply itacross various settings and observe how consistentare the results and findings (the full results pre-sented throughout the paper, such as in Tables 6-14,or Figures 1, 2, 3, or in Appendix F): Different randomness factors that require dif-ferent configuration setup for investigation(e.g., different choice of data, order of sam-ples, initialisation, etc.), but also differentsetup for their mitigation. As discussed inAppendix B.1, the mitigation can be done ongroup level (effectively mitigating multiplerandomness factors at the same time), whilealso allowing for further extensions (such asusing different mitigation strategies). Different approaches, namely in-context learn-ing, fine-tuning and meta-learning, and differ-ent models in these approaches. Althougheach approach works differently (e.g., fine-tuning using optimisation, while in-contextlearning uses only inference with prompts),the proposed method works with any such ap-proach. The only limitation is that the modelsand approaches used have an option to allowfor deterministic behaviour. Without this op-tion, the method can still be applied but mayproduce inconsistent and non-reproducible re-sults and findings (i.e., the importance score insuch cases is affected by the non-determinismof the model and so cannot be trusted fully).In addition, we apply the proposed method tomodels that lead to different performance andshow different overall deviation in the results.In all the cases, the produced importance scorecan be used for the analysis and comparison,even in cases when the impact of the random-ness factor is significant (e.g., PrototypicalNetworks on the SST2 datasets with Data Or-der randomness factor, where we observe asignificant drop in performance and increasein overall deviation as opposed to the goldenmodel in which case the method correctly",
  "Datasets with different characteristics and dif-ferent experimental setups, such as differentnumber of classes, samples, different promptformats": "In all of these cases, the proposed method pro-duces consistent results and findings without anyobvious shortcomings.Although the baselinestrategies can also be applied across all the set-tings, they often lead to inconsistent results due tothe mishandling of interactions (i.e., Random con-sistently leads to results similar to Golden Modelfor all the randomness factors, while using Fixedstrategy the importance of different factors changesquite often across different models, approaches,datasets and experimental settings).",
  "FAdditional Results from Investigation": "In this Appendix, we provide additional resultsfrom the investigation experiments. This includesthe investigation of randomness factor importancefor the meta-learning approaches on the binarydatasets (Appendix F.1), the full results from in-vestigating the impact of prompt format on the im-portance across all datasets (Appendix F.2, and thefull results from the main investigation in a formof tables in order to present the performance andthe deviation of different models and randomnessfactors (Appendix F.3).",
  "F.1Additional Results: Meta-LearningRandomness Factor Importance onBinary Datasets": "In this Appendix, we include the results of therandomness factor importance investigation for themeta-learning approaches on the binary datasets.The results are presented in .For the majority of the approaches and the in-vestigated datasets, the Data Order randomnessfactor is the most important, with the factorsachieving importance score of 1.0 in some cases,which represents the situation, when the factor con-tributes all the deviation in the model. Even thoughthis importance is due to the factor actually leadingto significantly lower performance and significantlyhigher overall deviation when set to only specificsubset, this only reinforces the finding that the DataOrder factor is the most important. In addition, we observe a consistent impor-tance of the Data Split and Label Selection ran-domness factors for the meta-learning approachesacross all the binary datasets. This follows thefindings of transfer learning, which also performsoptimisation/training and is not only composed ofinference (as is the case with in-context learning).As such, we can conclude that the way the data issplit and which samples are considered labelled hasa significant impact on the approaches that requiretraining. One possible reason is that the differentsplits and data labelling lead to different data dis-tribution which severely affects the training.Finally, the Model Initialisation and SampleChoice (and task choice) randomness factors donot show consistent importance across the meta-learning approaches and the datasets. However,the finding regarding Sample Choice may be dueto the binary setting and may be different whenusing the meta-learning approaches in the true few-shot setting (i.e., using them to adapt to previouslyunseen classes and tasks).",
  "F.2Additional Results: Impact of PromptFormat For All Datasets": "This Appendix contains the full results from in-vestigating the impact of the prompt format on theeffects of different randomness factors and their im-portance. The results for the Flan-T5 and Mistral-7B model across all the datasets are included in.As already discussed in .4, the formatof the prompt used can have significant impact onthe importance of different randomness factors. Us-ing the minimal formats, we observe significantchanges in the importance of different randomnessfactors, with them being not considered signifi-cantly important when using one format (e.g., DataOrder on SST2 dataset using format B) and at thesame time significantly important when using dif-ferent format (e.g., Data Order on SST2 datasetusing format D).In addition, the large language models are morerobust to this change of prompt format. This find-ing is more evident on the multi-class datasets,where in comparison to Flan-T5 model, the im-portance score of the Mistral-7B remains more orless constant, while the importance score of Flan-T5 model oscillates significantly. On the binarydatasets, the larger model is not as robust, butstill the changes to the importance score are lesssignificant than in the Flan-T5 model. Analysing : Importance of the investigated randomness factors for the meta-learning approaches on binary datasets,while taking the interactions between factors into consideration. The legend indicates number of classes for eachdatasets. We can observe consistent importance of the majority of the factors, with the exception of the SampleChoice and Model Initialisation factors. At the same time, the Data Order randomness factors appears to be themost important one for all the approaches. the predictions further, we observe that the largermodel provides more in-depth answers on the bi-nary datasets (e.g., not providing only an answerbut also an explanation for the answer, for exam-ple generating \"positive (in a negative context)\"instead of \"positive\", or often predicting neutralsentiment on the SST2 dataset, which is consideredas incorrect answer), which may lead to the sig-nificant changes in the importance of the differentrandomness factors.These findings only further highlights the im-portance of prompt-tuning as the format hassignificant impact on the words generated andtherefore also the assigned classes and the impor-tance scores of the different randomness factors.",
  "This Appendix contains the full results from themain investigation of the importance for the effectsof different randomness factors in this work (which": "were included as ), in a form of tables withall the values included (performance, deviation,contributed deviation, mitigated deviation and im-portance for each investigated randomness factor).We believe that including these results allows formore in-depth analysis, exploration of the resultsand its further extension. In addition to the results,we provide a brief summary overview based onthese results, which main not necessarily be con-nected only to the importance for different factors,but instead to the overall stability of the modelsand their ability to perform the different tasks.The results are included as follows:",
  "BERT results for all datasets in": ": Effect of different prompt formats on the importance of randomness factors for in-context learning. Thechoice of format has significant effect on the importance of different factors, with the minimal formats often leadingto higher importance. At the same time, the larger, more optimised models, show lower sensitivity to prompt format.",
  "Based on these results, we can determine theoverall stability of the different models. Specifi-": "cally, we can observe the smaller in-context learn-ing model (Flan-T5) shows better stability thanthe larger ones (LLaMA-2, Mistral-7B and Zephyr-7B), leading to significantly lower overall deviationacross majority of the datasets. At the same time,we can observe that with increasing number ofpredicted classes, the performance of the Flan-T5model drops significantly (from 83.85 F1 on AGNews dataset with 4 classes to 44.25 on the SNIPSdataset with 7 classes), while retaining its stability(the overall deviation staying approximately thesame with 3.09 on AG News and 2.28 on SNIPS).On the other hand, the larger language modelsachieve similar performance, but different stability,across the majority of the investigated datasets re-gardless of the number of predicted classes. Thesignificant increase of performance and stability incase of the DB-Pedia dataset and SNIPS dataset (toa certain extent), may point to the fact that the mod-els may have been trained on these datasets and sothe results and findings on them may be biased we discuss this as a limitation based on the recentlyobserved large language model validation crisis (Liand Flanigan, 2023).The fine-tuning approaches appear to be the moststable and best performing approaches in our inves-tigation, leading to F1 score as high as 98% andoverall deviation as low as 0.36. Surprisingly, theperformance on the multi-class datasets is higherthan on the binary datasets, which may indicate theoverall hardness of the different datasets we usein this work, or point to specific problems in thebinary datasets (such as the single word sentenceswithout any sentiment in the SST2 dataset).Finally, the meta-learning approaches appear tobe significantly dataset dependent, with the over-all performance and the overall deviation chang-ing significantly across different binary datasets.One possibility for this is their significant sensi-tivity to the setup of the hyperparameters, withthe performance and deviation changing signifi-cantly with even small changes in the hyperparam-eter setup, which we observed when trialling themeta-learning models on the multi-class datasetsas well."
}