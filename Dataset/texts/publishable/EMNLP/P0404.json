{
  "Abstract": "The quality of the dataset is crucial for ensuringoptimal performance and reliability of down-stream task models. However, datasets oftencontain noisy data inadvertently included dur-ing the construction process. Numerous at-tempts have been made to correct this issuethrough human annotators. However, hiringand managing human annotators is expensiveand time-consuming. As an alternative, recentstudies are exploring the use of large languagemodels (LLMs) for data annotation. In this study, we present a case study that ex-tends the application of LLM-based data anno-tation to enhance the quality of existing datasetsthrough a cleansing strategy. Specifically, weleverage approaches such as chain-of-thoughtand majority voting to imitate human anno-tation and classify unrelated documents fromthe Multi-News dataset, which is widely usedfor the multi-document summarization task.Through our proposed cleansing method, weintroduce an enhanced MULTI-NEWS+. By em-ploying LLMs for data cleansing, we demon-strate an efficient and effective approach to im-proving dataset quality without relying on ex-pensive human annotation efforts.",
  "Introduction": "The significance of dataset quality in deep learningapplications cannot be overstated as mislabeled ornoisy data can severely degrade performance (Songet al., 2023). Datasets with incorrect labels, noise,or inconsistencies undermine the consistency andstability of model training. Cleansing these datasetscontributes to enhancing model performance andgeneralization capabilities. Hence, ensuring thequality of the dataset by identifying and eliminat-ing noisy data is imperative. In the realm of naturallanguage processing, several researchers have at-tempted to improve the quality of noisy datasets(Jiang et al., 2020, 2022). For example, ReDo-cRED (Tan et al., 2022) addressed issues such as Source 1Starting in 1996, alexa internet has been donating theircrawl data to the internet archive. Flowing in every day,these data are added to the wayback machine after anembargo period.Source 2... For the first time in decades, researchers trying to de-velop a vaccine for malaria have discovered a new targetthey can use to attack this deadly and common parasite...Source 3Focused crawls are collections of frequently-updatedwebcrawl data from narrow ( as opposed to broad orwide ) web crawls, often focused on a single domain orsubdomain.SummaryResearchers think theyve found a promising new potentialweapon in the fight against malaria in a fairly unlikelyplace: the blood of toddlers. In a paper published in sci-ence today, ...",
  ": Examples of noisy documents in Multi-Newsdataset. Sources 1 and 3 do not contribute to the sum-mary. We aim to identify such noisy documents withouta human annotator": "false negatives in DocRED (Yao et al., 2019), awidely used dataset for relation extraction. Simi-larly, annotation inconsistencies were found in theMultiWOZ dataset (Budzianowski et al., 2018) fordialogue state tracking (Qian et al., 2021), leadingto efforts to rectify these issues (Eric et al., 2020;Zang et al., 2020; Han et al., 2021; Ye et al., 2022a).Despite these efforts, relying on human annota-tors to enhance datasets poses challenges such ashigh costs and time constraints. The quality of theannotation might also be affected by potential vari-ations, such as subjective bias and the proficiencyof the annotator (Rashtchian et al., 2010). Further-more, cleansing a noisy dataset typically requiresa larger budget, often involving majority voting bymultiple annotators or validation by experts (Tanet al., 2022). Given the significance and neces-sity of enhancing the quality of existing datasets,these obstacles hinder practical efforts to cleansedatasets efficiently. Therefore, it is crucial to ex-plore cost-effective methods that can cleanse the",
  ": Overall framework for cleansing data and composing MULTI-NEWS+": "existing dataset, minimizing human involvement.In this study, we propose leveraging large lan-guage model (LLM)-based annotation for datasetcleansing. Researchers have explored cost-efficientalternatives to human annotators by employingLLMs across various tasks (Wang et al., 2021; Dinget al., 2023; He et al., 2024; Bansal and Sharma,2023; Zhang et al., 2023; Choi et al., 2024). How-ever, the real-world applicability of LLM-basedannotation on existing datasets is still less explored.Building on these insights, we extend the appli-cation of LLM-based annotations to denoise theexisting dataset and improve its quality. Specifi-cally, we conduct a case study to cleanse the Multi-News (Fabbri et al., 2019), a dataset for multi-document summarization tasks. This dataset con-sists of news articles crawled from the internet andis widely used in multi-document summarizationresearch. However, as shown in , we iden-tify several issues related to the noise in the dataset.For instance, the set of documents contained sys-tem messages from platforms such as Twitter, Way-back Machine, or Dow Jones that are unrelated tothe summary and degrade the dataset quality.To accomplish our purpose, we utilize LLMs toanalyze the summary and associated documents,identifying and excluding any documents that arenot relevant to the summary. Specifically, we em-ploy approaches such as chain-of-thought (CoT),providing the rationale for decision-making withenhanced transparency and facilitating human in-vestigation. We further enhance our cleansing pro-cess by incorporating self-consistency considera-tions, which mimic the majority voting processused by human annotators (Wang et al., 2023b).Based on our carefully designed framework, weintroduce MULTI-NEWS+, an enhanced version of the existing Multi-News dataset, achieved throughour LLM-based cleansing strategy. To the best ofour knowledge, this is the first attempt to exploitLLMs to enhance the quality of real-world datasets.Our experiments demonstrate the effectiveness ofMULTI-NEWS+, providing a valuable resource forfuture research. We make MULTI-NEWS+ and oursource code publicly available for further study.",
  "Related Work": "Dataset quality has been an interest to researchersbecause of its importance in ensuring the qual-ity of the model trained with the dataset (Budachet al., 2022). Previous studies found that largeamounts of data automatically crawled from theweb may contain noisy documents, and properfiltering procedures can be an efficient solutionagainst them (Xu and Koehn, 2017; Khayrallahand Koehn, 2018; Kryscinski et al., 2019; Luccioniand Viviano, 2021; Kreutzer et al., 2022). Accord-ingly, several studies in text summarization inves-tigated various strategies to filter out noisy data(Matsumaru et al., 2020; Nan et al., 2021; Guoet al., 2022) and released new datasets with betterquality (Grusky et al., 2018; Urlana et al., 2022).However, their strategies are primarily composedof coarse rule-based methods and less interpretablemodel output, or costly human investigation hasbeen applied for constructing new datasets. Fur-thermore, such strategies have not been applied tomulti-document summarization datasets.In the meantime, with the advancement of LLMs(Zhao et al., 2023), researchers have explored theusage of LLMs for data annotation, a task thattraditionally relied on human annotators. Initialattempts have revealed the potential capabilitiesof models like GPT-3 for data annotation (Wang",
  ": Histogram comparing the amount of inputarticles in each dataset": "et al., 2021). These studies indicate that GPT-3can annotate datasets more efficiently and cost-effectively than human annotators. This results inenhanced downstream task performance, with themodel trained on the GPT-3 annotated dataset out-performing the one trained on the human-annotateddataset. Subsequent studies have further demon-strated the capabilities of GPT-3, showing its abilityto generate labeled data using external knowledgeor instructions about desired labels and domains(Ding et al., 2023). Additionally, researchers haveexamined the usefulness of newer models like GPT-3.5 and evaluated the effectiveness of CoT in im-proving annotation quality (He et al., 2024). LLM-based annotation has also been extended to low-resource languages where hiring human annotatorsis challenging (Choi et al., 2024).In this work, we introduce a novel approachto filtering noisy documents from multi-documentsummarization dataset by extending cost-efficientLLM-based annotation beyond traditional dataannotation tasks.By leveraging the capabili-ties of LLMs, our study facilitates real-worlddataset cleansing, enhancing the quality of existingdatasets. This attempt is noteworthy as it broadensthe scope of LLM applications, offering effectivesolutions for improving dataset quality and stream-lining its cleansing process, minimizing relianceon human annotations.",
  "MULTI-NEWS+": "The previous Multi-News dataset plays an im-portant role in multi-document summarization re-search. It consists of sets of documents and theircorresponding summaries. However, as shown in and detailed in Appendix G and H, the Multi-News dataset contains several noisy and ir-relevant articles that are unrelated to the summaryor other documents. This issue arises from theirconstruction process, which relies on automatedcrawling from the Internet Archive.To solve this issue and cleanse the dataset, wedefined our problem as a classification task deter-mining whether each document is relevant to thesummary. To this end, we designed the promptfor the model as shown in Appendix J. We inte-grated CoT to enhance the models performance byevaluating the relevance of each document to thesummary. Thus, a rationale for the decision canbe made available, which marks the difference be-tween LLM-based and human annotations. Whiletraditional human annotation through crowdsourc-ing platforms like Amazon Mechanical Turk usu-ally produces annotation results without underlyingreasons due to additional costs, LLM-based anno-tators can easily offer explanations through CoT.These rationales can assist human managers in re-viewing results and rectifying erroneous decisions.Furthermore, we imitated the conventionaldataset cleansing procedure which typically in-volves multiple human annotators and their col-lective judgments, primarily through majority vot-ing. Similarly to the majority voting process usedby human annotators, we applied this approachto the LLM-based annotators. In particular, wegenerated five individual LLM agents to read thesummary and documents and determine if the doc-ument is relevant to the summary. This strategybased on self-consistency can boost the quality ofannotations, by rectifying potential errors made byindividual agents (Wang et al., 2023b). presents the summary of the overall process.Based on the proposed method, we utilizedfive LLM agents to individually annotate 56,216sets of summaries and documents from the Multi-News dataset.Specifically, we employed theGPT-3.5-turbo-0125 model1, the most re-cent model at the time of this study. With a promptdesigned for a 3-shot CoT, approximately 3,500 to-kens were required to annotate the input summariesand articles, along with around 100 tokens for gen-erating reasoning processes and annotation results.The cost per annotation sample amounted to ap-proximately 0.01$ (0.002$ per agent), resulting ina total cost of approximately 550$ to annotate the",
  "Multi-News40.1113.9021.580.6003-2.407MULTI-NEWS+40.4514.1721.840.6027-2.362Ablation (Urlana et al., 2022)39.3013.6521.420.5967-2.457": ": Performance comparison of the Multi-News and MULTI-NEWS+ datasets on two models. The Ablationrow represents a version of the Multi-News dataset that has been cleansed using methods from previous study(Urlana et al., 2022). entire Multi-News dataset.After annotation, we found that 27,052 of the153,091 articles can be considered noisy documentsand do not contribute to the summarization. Sub-sequently, we constructed MULTI-NEWS+ by re-moving these noisy documents from Multi-Newswhile preserving the train/valid/test split. presents the comparison of the Multi-News andMULTI-NEWS+ datasets in terms of the number ofdocuments per set. More than 15% of the docu-ments in Multi-News are irrelevant, diminishingthe datasets quality and degrading the models per-formance. Furthermore, 379 sets have no relevantsource articles, as shown in Appendix H. In con-trast, by deleting noisy documents, MULTI-NEWS+",
  "Experimental Design": "To validate the efficacy of data cleansing and thedevelopment of MULTI-NEWS+ in filtering outnoisy documents and improving the performanceof downstream task models, we measured the multi-document summarization performance of modelstrained on each dataset, similar to previous study(Guo et al., 2022). Enhanced model performanceindicates superior dataset quality (Ye et al., 2022b;Choi et al., 2024). We fine-tuned two differentmodels, BART (Lewis et al., 2020) and T5 (Raffelet al., 2020) on Multi-News and MULTI-NEWS+.Performance evaluation metrics included the fol-lowing metrics: ROUGE (Lin, 2004), BERTScore(Zhang et al., 2020), and BARTScore (Yuan et al.,2021). For a fair comparison, we used the test setof MULTI-NEWS+ for each model and reported theaverage performance across three random seeds.",
  "Result": "The results in demonstrate the superiorityof the MULTI-NEWS+ dataset in enhancing the per-formance of summarization models compared tothe original Multi-News dataset. Across variousmetrics, models trained on MULTI-NEWS+ con-sistently outperform those trained on Multi-News,indicating better summarization quality with therefined dataset. This highlights the effectiveness ofdataset cleansing in removing noisy and irrelevantdocuments, thereby enhancing the overall perfor-mance of summarization models. Additionally, weperformed a human evaluation on the output of379 sets that are classified as having no relevantsource articles and found that 356 sets are correctlyclassified, which represents 93.9% of the human-machine agreement rate. We provide an exampleof error analysis in Appendix I.Additionally, we conducted an ablation study us-ing the cleansing method proposed by a previousstudy (Urlana et al., 2022), detailed in Appendix F.Our findings indicate that this method is ineffec-tive in improving downstream task performance onthe Multi-News dataset, which focuses on multi-document summarization and differs from the con-figuration used in the prior study. This underscoresthe effectiveness of our proposed method and thevalue of MULTI-NEWS+.",
  "Discussion and Future Works": "In this section, we discuss recent advancements inthe field since the submission of the manuscriptand propose strategies for incorporating them infuture research.Cutting-edge models. Although we employedfive GPT-3.5-turbo-0125 models for our ex-periments, the field has seen the release of more advanced models, such as GPT-4o (OpenAI,2024b), GPT-4o-mini (OpenAI, 2024a), andOpenAI O1 (OpenAI, 2024c), along with the con-tinued development of open-source models likeLLaMA-3 (Dubey et al., 2024), Gemma-2 (Teamet al., 2024), and Mistral Nemo (Mistral, 2024).Models such as GPT-4o-mini and other open-source alternatives offer reduced costs compared toGPT-3.5-turbo-0125, making their adoptionpromising for both lowering the expense of datasetcleansing and improving the accuracy of detectingnoisy documents.Weighted majority voting.The availabil-ityofhigh-performanceyetcost-effectivemodelslikeGPT-4opresentstheoppor-tunitytousethemasexpertannotators,giventheirsuperiorcapabilitiescomparedto models like GPT-3.5-turbo-0125 orGPT-4o-mini. For example, rather than usingfive GPT-3.5-turbo-0125 models, we couldemploy three GPT-3.5-turbo-0125 modelsalongside one GPT-4o, with GPT-4o carryingdouble the weight of a GPT-3.5-turbo-0125annotator. This approach positions GPT-4o asan expert, where agreement between at least oneGPT-3.5-turbo-0125 model and GPT-4owould trigger document deletion.Supervision from superior models. Another po-tential approach involves using more capable mod-els to verify annotation results. In this scenario,GPT-4o would not participate in the initial annota-tion process but would instead verify the outcomesproduced by GPT-3.5-turbo-0125 models.By taking the documents, summaries, and anno-tation results as input, GPT-4o acts as an expertreviewer overseeing the outputs of standard anno-tators.Cost-efficient cleansing via pre-screening. In thispaper, we applied the data cleansing strategy toevery document in the dataset. However, a morecost-efficient approach could involve performingthe annotation procedure only on documents likelyto contain noise. Techniques such as dataset car-tography (Swayamdipta et al., 2020) could serve asa pre-screening method to identify cleansing candi-dates, thereby reducing the overall cost of datasetcleansing.",
  "In this study, we suggest deploying cost-efficientLLM-based data annotation to cleanse real-world": "datasets by identifying and excluding irrelevantand noisy data. We conducted a case study us-ing this strategy to cleanse the Multi-News datasetand proposed the improved MULTI-NEWS+ dataset.Our case study revealed that MULTI-NEWS+ pro-vides superior data quality compared to the orig-inal Multi-News dataset. Additionally, we havemade MULTI-NEWS+ publicly available, therebysupporting further research in the field of multi-document summarization.Our work paves the road to extending our datacleansing strategy to other datasets, broadening thescope of utilizing LLMs. This extension wouldenhance the quality of existing datasets across var-ious domains without the need to construct newdatasets from scratch.As such, our approachnot only contributes to the advancement of multi-document summarization research but also offers acost-efficient solution for enhancing dataset quality.We are committed to extending our LLM-basedmethod to other datasets, further solidifying its ap-plicability to other tasks.",
  "Limitations": "We acknowledge several limitations regarding ourproposed method. First, our method is primarilylimited by the possibility of wrong classificationeven with majority voting and CoT. In the future,we may adopt various LLMs as agents and applyweighted majority voting according to their perfor-mance to alleviate this issue, as discused in Sec-tion 5.Secondly, the nature of the Multi-News datasetmight exhibit a real-world case of automatic collec-tion of documents from the web that are not alwaysrelevant to the summary. In other words, the in-clusion of noisy documents might demonstrate thecharacteristics of real-world automatic crawling.For instance, the model trained on the Multi-Newsdataset may be more suitable for a real-time sys-tem that automatically crawls data from the weband summarizes them. However, we believe such apossibility can be dealt with through the reciprocalusage of our MULTI-NEWS+ and previous Multi-News dataset. For instance, one could utilize a pre-vious Multi-News dataset when the trained modelis expected to consistently deal with noisy docu-ments for inference and there are no pre-definedstrategies for filtering out these noisy documentsat inference time. Otherwise, for cases where themodel is expected to only handle clean documents,",
  "Ethics Statement": "As we are exploiting LLMs for classifying irrel-evant documents rather than text generation, theethical concern with our method is smaller thanthat of studies that utilize LLMs to generate texts.Nonetheless, recent studies suggest that the CoTtechnique may induce ethical bias in LLM (Shaikhet al., 2023). In future work, we plan to investigatethis phenomenons appearance in our method.",
  "Parikshit Bansal and Amit Sharma. 2023. Large lan-guage models as annotators: Enhancing generaliza-tion of nlp models at minimal cost. arXiv preprintarXiv:2306.15766": "Lukas Budach, Moritz Feuerpfeil, Nina Ihde, AndreaNathansen, Nele Noack, Hendrik Patzlaff, Felix Nau-mann, and Hazar Harmouch. 2022. The effects ofdata quality on machine learning performance. arXivpreprint arXiv:2207.14529. Pawe Budzianowski, Tsung-Hsien Wen, Bo-HsiangTseng, Iigo Casanueva, Stefan Ultes, Osman Ra-madan, and Milica Gasic. 2018. Multiwoz-a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling.In Proceedings ofEMNLP, pages 50165026.",
  "Bosheng Ding, Chengwei Qin, Linlin Liu, Yew KenChia, Boyang Li, Shafiq Joty, and Lidong Bing. 2023.Is GPT-3 a good data annotator? In Proceedings ofACL, pages 1117311195": "Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,Akhil Mathur, Alan Schelten, Amy Yang, AngelaFan, et al. 2024. The llama 3 herd of models. arXivpreprint arXiv:2407.21783. Mihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi,Sanchit Agarwal, Shuyang Gao, Adarsh Kumar, AnujGoyal, Peter Ku, and Dilek Hakkani-Tur. 2020. Mul-tiwoz 2.1: A consolidated multi-domain dialoguedataset with state corrections and state tracking base-lines. In Proceedings of LREC, pages 422428. Alexander Richard Fabbri, Irene Li, Tianwei She, SuyiLi, and Dragomir Radev. 2019. Multi-news: A large-scale multi-document summarization dataset and ab-stractive hierarchical model. In Proceedings of ACL,pages 10741084.",
  "Newsroom: A dataset of 1.3 million summarieswith diverse extractive strategies. In Proceedingsof NAACL, pages 708719": "Yanzhu Guo, Chlo Clavel, Moussa Kamal Eddine, andMichalis Vazirgiannis. 2022. Questioning the valid-ity of summarization datasets and improving their fac-tual consistency. In Proceedings of EMNLP, pages57165727. Ting Han, Ximing Liu, Ryuichi Takanabu, Yixin Lian,Chongxuan Huang, Dazhen Wan, Wei Peng, and Min-lie Huang. 2021. Multiwoz 2.3: A multi-domaintask-oriented dialogue dataset enhanced with anno-tation corrections and co-reference annotation. InProceedings of NLPCC, pages 206218. Xingwei He, Zhenghao Lin, Yeyun Gong, A-Long Jin,Hang Zhang, Chen Lin, Jian Jiao, Siu Ming Yiu, NanDuan, and Weizhu Chen. 2024. Annollm: Makinglarge language models to be better crowdsourced an-notators. In Proceedings of NAACL (Industry Track),pages 165190. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023. Mistral7b. arXiv preprint arXiv:2310.06825.",
  "Kun Qian, Ahmad Beirami, Zhouhan Lin, Ankita De,Alborz Geramifard, Zhou Yu, and ChinnadhuraiSankar. 2021. Annotation inconsistency and entitybias in multiwoz. In Proceedings of SIGDIAL, pages326337": "Colin Raffel, Noam Shazeer, Adam Roberts, Kather-ine Lee, Sharan Narang, Michael Matena, YanqiZhou, Wei Li, and Peter J. Liu. 2020. Exploring thelimits of transfer learning with a unified text-to-texttransformer. Journal of Machine Learning Research,21(140):167. Cyrus Rashtchian, Peter Young, Micah Hodosh, and Ju-lia Hockenmaier. 2010. Collecting image annotationsusing amazons mechanical turk. In Proceedings ofNAACL 2010 Workshop on Creating Speech and Lan-guage Data with Amazons Mechanical Turk, pages139147. Omar Shaikh, Hongxin Zhang, William Held, MichaelBernstein, and Diyi Yang. 2023. On second thought,lets not think step by step! bias and toxicity in zero-shot reasoning. In Proceedings of ACL, pages 44544470. Hwanjun Song, Minseok Kim, Dongmin Park, YoojuShin, and Jae-Gil Lee. 2023. Learning from noisylabels with deep neural networks: A survey. IEEETransactions on Neural Networks and Learning Sys-tems, 34(11):81358153. Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie,Yizhong Wang, Hannaneh Hajishirzi, Noah A Smith,and Yejin Choi. 2020. Dataset cartography: Mappingand diagnosing datasets with training dynamics. InProceedings of EMNLP, pages 92759293. Qingyu Tan, Lu Xu, Lidong Bing, Hwee Tou Ng, andSharifah Mahani Aljunied. 2022. Revisiting docred-addressing the false negative problem in relation ex-traction. In Proceedings of EMNLP, pages 84728487. Gemma Team, Morgane Riviere, Shreya Pathak,Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupati-raju, Lonard Hussenot, Thomas Mesnard, BobakShahriari, Alexandre Ram, et al. 2024. Gemma 2:Improving open language models at a practical size.arXiv preprint arXiv:2408.00118. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288.",
  "Shuohang Wang, Yang Liu, Yichong Xu, ChenguangZhu, and Michael Zeng. 2021. Want to reduce la-beling cost? gpt-3 can help. In Findings of EMNLP,pages 41954205": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,Ed H Chi, Sharan Narang, Aakanksha Chowdhery,and Denny Zhou. 2023b. Self-consistency improveschain of thought reasoning in language models. InProceedings of ICLR. Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Rmi Louf, Morgan Funtow-icz, et al. 2020. Transformers: State-of-the-art natu-ral language processing. In Proceedings of EMNLP(Demo Track), pages 3845.",
  "Hainan Xu and Philipp Koehn. 2017. Zipporah: a fastand scalable data cleaning system for noisy web-crawled parallel corpora. In Proceedings of EMNLP,pages 29452950": "Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin,Zhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou,and Maosong Sun. 2019.Docred: A large-scaledocument-level relation extraction dataset. In Pro-ceedings of ACL, pages 764777. Fanghua Ye, Jarana Manotumruksa, and Emine Yilmaz.2022a. Multiwoz 2.4: A multi-domain task-orienteddialogue dataset with essential annotation correctionsto improve state tracking evaluation. In Proceedingsof SIGDIAL, pages 351360. Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, JiangtaoFeng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.2022b. Zerogen: Efficient zero-shot learning viadataset generation. In Proceedings of EMNLP, pages1165311669.",
  "BConstruction Process of Multi-News": "In this section, we briefly explain the construc-tion process of the Multi-News dataset. Multi-News is based on data from newser.com2 that offershuman-written summaries of news articles. Eachsummary is written by professional human editorsand involves several outlinks to the original arti-cles and relevant websites. Multi-News collectedthis human-written summary and documents fromits outlinks, which behave as source documentsfor summarization. Notably, the authors of Multi-News archived every article leveraging WaybackMachine3, a system that supports archiving of thecircumstances of a given website, to ensure the re-producibility and support future investigation. Con-tents of each document have been accessed andcrawled from these Wayback-archived links. However, this affected problems regarding thequality of the dataset. As shown in examples ofnoisy documents in Appendix G, several noisy doc-uments consist of a message from Wayback Ma-chine. Moreover, the failure to crawl the contentof the webpage caused other problems. We investi-gated the case shown in Appendix H and foundthat it is a result of the crawling of the wrongpart of the website. clearly showcasesthis phenomenon where the content in the red boxis crawled instead of the content in the blue box,which is desired. Even though the content in theblue box is different for each article, the systemwrongly crawled the shared red box, which resultedin five noisy documents that share the same contentand do not contribute to the summary.From the example above, we revealed the pres-ence of the wrongly crawled documents, that af-fect the quality of the dataset. We believe suchphenomena would be alleviated with the advance-ment of LLM-based autonomous agents (Wanget al., 2023a), as they could visit the website andonly crawl the text relevant to the summary. Eventhough we leave this as future work, this researchdirection should be prompted.",
  "CImplementation Details": "We utilized PyTorch (Paszke et al., 2019) and Hug-gingface Transformers (Wolf et al., 2020) to im-plement and evaluate the model. Specifically, weemployed facebook/bart-large-cnn4 and google-t5/t5-base, with 406M and 220M parameters, re-spectively, for BART and T5. Each model wastrained using Adam (Kingma and Ba, 2015) witha learning rate of 2e-5 over 3 epochs. We useda batch size of 4 and implemented a gradientaccumulation step of 4, resulting in a practicalbatch size of 16.For evaluation, we utilizedbert-base-uncased and facebook/bart-large-cnn forBERTScore and BARTScore, respectively. We re-ported BERTScore-F1 in . ROUGE scoreswere measured using the rouge-score5 library, withthe F1 score of each metric. The training was con-ducted on a single NVIDIA A100 40GB GPU. Weprovide the source code and dataset to the public.6",
  "No Noisy Example0.6038-2.507One Noisy Example0.6022-2.521Two Noisy Examples0.6016-2.539": ": Performance of LLM-based summarizationof Multi-News with different amounts of noisy exam-ples. We only report two model-based metrics as thehuman-generated reference summary has a differentform compared to the LLM-generated summary. unteers and individually asked them to determinewhether the decision of the model was correct ornot given the summary, original articles, and ratio-nale of the model. We defined the model made anincorrect decision when at least one human evalua-tor flagged the output as an incorrect classification.",
  "False Negative (FN): Relevant documents in-correctly classified as not relevant": "Upon review, we found that 127 documents wereclassified as TP, 24 as TN, and 2 as FN. The anno-tation framework identified 26 documents as irrele-vant and noisy, which accounts for approximately17% of the total 153 documents. This aligns closelywith the statistics in of Appendix A, whichindicates that 18.6% of documents in the validationset were classified as noisy. From these results, the precision is 1.0, as therewere no FP documents, while the recall is approxi-mately 0.984. Additionally, we observed that 17 ofthe 24 TN documents could be classified as noisysystem messages, such as This will appear nextto all of your comments; this will not appear any-where on Newser, as illustrated in Appendix G.The remaining 7 documents were irrelevant to thesummary.Furthermore, we investigated the two FN cases.In one instance, the summary included a portionrelated to the misclassified document at the veryend. In the other, the misclassified document pro-vided context for the summary but was not directlyconnected to it. These cases are consistent with theerror patterns discussed in Appendix I.It is important to note that while individual anno-tators occasionally made incorrect classifications,the majority voting process effectively correctedthese errors. This highlights the efficacy of our pro-posed method in improving data annotation qualityand ensuring thorough dataset cleansing.",
  "EAdditional Experiment with LargeLanguage Models": "This section introduces our additional experimentthat investigates the influence of noisy examples forLLMs in a few-shot learning scheme. For this pur-pose, we used 7B-sized, instruction-tuned Llama2(Touvron et al., 2023) and Mistral (Jiang et al.,2023). Specifically, we used meta-llama/Llama-2-7b-chat-hf and mistralai/Mistral-7B-Instruct-v0.2from Transformers (Wolf et al., 2020). In this ex-periment, we prompted the model to summarizethe documents in the test set of Multi-News withtwo-shot examples selected from the training setof Multi-News. Additionally, we differentiated thenumber of noisy documents in the examples givenas the prompt. presents the experimentalresult. The result demonstrates that the inclusionof the noise in the example degrades the quality ofthe summary generated by the LLM. This suggeststhe significance of the exclusion and filtering of thenoise for LLMs, which underscores the necessityof dataset cleansing presented in this paper.",
  ": The result of analysis of Multi-News datasetwith rule-based filtering methods (Urlana et al., 2022).We concatenated every source document to measuretheir average word and sentence length": "sis. First, we found that 0.7% of total source docu-ments can be considered noisy documents as it isempty or duplicated from other source documentswithin the same set. Second, we found previousrule-based filtering methods are not very effectivestandards for the Multi-News dataset. For instance,there were no sets that had empty summaries, sum-maries that were duplicated with other summaries,or summaries that repeated the first few sentencesof source documents. The only exception is Com-pression < 50%, which identified more than half ofthe dataset. However, it should be noted that Multi-News is a multi-document summarization dataset,which is different from datasets for previous stud-ies. For instance, average compression is signifi-cantly lower than other single-document summa-rization datasets reported in the previous study(Urlana et al., 2022), as multiple source documentsin Multi-News involve more information comparedto the source document of single-document sum-marization datasets. In conclusion, this analysisdemonstrates that previous filtering strategies areless practical for multi-document summarizationdatasets such as Multi-News and enlightens thenecessity of novel approaches for these datasets.",
  "This crawl of online resources of the 115th us congress was performed on behalf of the united statesnational archives &amp; records": "The seed for this crawl was a list of every host in the wayback machine this crawl was run at a level 1( urls including their embeds, plus the urls of all outbound links including their embeds ) the warcfiles associated with this crawl are not currently available to the general public. These crawls are part of an effort to archive pages as they are created and archive the pages that theyrefer to. That way, as the pages that are referenced are changed or taken from the web, a link to theversion that was live when the page was written will be preserved.then the internet archive hopes thatreferences to these archived pages will be put in place of a link that would be otherwise be broken, or Please enable cookies on your web browser in order to continue. The new european data protectionlaw requires us to inform you of the following before you use our website: we use cookies and othertechnologies to customize your experience, perform analytics and deliver personalized advertisingon our sites, apps and newsletters and across the internet based on your interests. By clicking iagree below, you consent to the use by us and our third-party partners of cookies and data gatheredfrom your use of our platforms. See our privacy policy and third party partners to learn more aboutthe use of data and your rights. You also agree to our terms of service. Thank you for reading. Please purchase a subscription to continue reading. A subscription isrequired to continue reading. Thank you for reading 5 free articles. You can come back at the end ofyour 30-day period for another 5 free articles, or you can purchase a subscription and continue toenjoy valuable local news and information. If you are a current 7-day subscriber you are granted anall-access pass to the website and digital newspaper replica. Please click sign up to subscribe, orlogin if you are already a member. Thank you for reading 5 free articles. You can come back at theend of your 30-day period for another 5 free articles, or you can purchase a subscription and continueto enjoy valuable local news and information. If you are a current 7-day subscriber you are grantedan all-access pass to the website and digital newspaper replica. Please click below to get started. Add a location to your tweets when you tweet with a location, twitter stores that location. You canswitch location on/off before each tweet and always have the option to delete your location history.Learn more",
  "HExtreme Cases of Noisy Documents": "In addition to examples of noisy documents, we discovered the following extreme case of noisy data inthe Multi-News dataset. In this example, five documents have the same content but offer no informationon the summary. Thus, it cannot generate a reasonable summary based on the given documents. Wewitnessed 379 similar cases during the dataset cleansing process, as reported in . While they wereexcluded from training and testing, we included them in the dataset file for future investigation. SummaryNote to tweeting politicians: watch what you post, because politwoops will remember it forever. Thetransparency-minded website is safeguarding politiciansdeleted tweets, enabling the rest of us to giggleor ponder over them at our leisure, the atlantic reports. The sites current 6-month stash includes a fewdoozey deletions, including john mccain mocking vladimir putins tears and rep. Jeff miller posting a linkto a poll that asked, \" was obama born in the united states? \" a few deletions are more odd than obvious,begging us to ask what politicians were thinking. Why, for example, did rep. Tom graves remove a tweetabout going out one night with his wife? or rep. Kathy hochul delete one about her visit to a cancerinstitute? perhaps rep. Stephen finchers tweet comparing the bachelor to the hunger games is a moreobvious case, but the online avenues of a politicians mind can be dimly lit indeed. Document 1An archive of the public statements deleted by u.s. Politicians. Explore the tweets they would prefer youcouldnt see. If you arent an elected official or running for office and feel your account is being trackedby mistake then please contact us. Document 2An archive of the public statements deleted by u.s. Politicians. Explore the tweets they would prefer youcouldnt see. If you arent an elected official or running for office and feel your account is being trackedby mistake then please contact us. Document 3An archive of the public statements deleted by u.s. Politicians. Explore the tweets they would prefer youcouldnt see. If you arent an elected official or running for office and feel your account is being trackedby mistake then please contact us. Document 4An archive of the public statements deleted by u.s. Politicians. Explore the tweets they would prefer youcouldnt see. If you arent an elected official or running for office and feel your account is being trackedby mistake then please contact us. Document 5An archive of the public statements deleted by u.s. Politicians. Explore the tweets they would prefer youcouldnt see. If you arent an elected official or running for office and feel your account is being trackedby mistake then please contact us.",
  "IError Analysis": "Following the form of the previous study (Choi et al., 2024), we provide an error analysis to provide amore balanced view of the behavior and limitations of our proposed method. In the first example, we canobserve that while Document 1 can be regarded as irrelevant to the summary except that there is a mentionof fusion tv, Document 2 contains information about Mike Tyson and his new TV documentary series.However, the model predicted both documents are irrelevant to the summary, primarily because the modelconcentrated on the mention of the world team tennis exhibition from Document 2. From this insight,we hypothesize GPT-3.5 suffers from a mixture of irrelevant and relevant information in one document. SummaryOver his career, former heavyweight champion mike tyson recorded 50 wins and six losses. But herecently notched another big loss in latin america this time as a coach of a bird, reports the ap. Tysontraveled to suriname as part of the new fusion tv documentary series outpost, and was soundly beatenwhen he entered a bird in a songbird contest, a cherished local tradition. Cameras captured iron mike ashe learned about the contest, located a bird to enter he dubbed the tiny guy \" little mike \" but thensuffered a tko when a competing champion cheeped and peeped more than his bird did in the same15-minute period. \" little mike let us down, man. I was in his corner, though, \" said tyson. \" it was justamazing meeting the people, meeting the culture i had a great time. \" the series, kicking off on sundaywith tysons episode, mixes travel adventure, history, and journalism to shine a light on global stories.The first season focuses on latin america and includes as hosts the late show with stephen colbertbandleader jon batiste, brain games star jason silva, and transgender model carmen carrera. Spanishversions air on unimas. Tyson was lured onto the show by the chance to visit a country hed never heardof and his love of birds. The former boxer has loved pigeons and kept them since he was a kid inbrooklyn. ( sundays show recorded the moment tyson lovingly released his bird in suriname. ) \" my wifealways says the reason i keep my pigeons is they connect me to my childhood, \" tyson said. \" once its inyour blood, it never leaves. Its just who you are. \" Document 1Starting in 1996, alexa internet has been donating their crawl data to the internet archive. Flowing inevery day, these data are added to the wayback machine after an embargo period. [Abbreviated duplicatedtext] Outpost shows you the world like youve never seen it. The series lives at the intersection ofinvestigative journalism and adventure travel, bringing you a local perspective on faraway places andinviting you to explore. The series premieres march 26 @ 8 and 11 pm on fusion tv. In the first episode,transgender model carmen carrera travels to brazil, a place where rates of violence against lgbt people aresome of the highest in the world, to find out whats happening, what life is like for young transgenderedpeople in brazil, and what the future might hold. Gabriel leigh takes us to el alto, bolivia, where some ofthe craziest architecture on earth is taking shape as part of a surge in indigenous purchasing power. Document 2[Abbreviated duplicated text] file - in this monday, oct. 10, 2016, file photo, mike tyson attends a worldteam tennis exhibition to benefit the elton john aids foundation in las vegas. Tyson traveled to suriname aspart of the new fusion tv documentary series \"outpost \" and was soundly beaten when he entered a bird ina songbird... ( associated press ) [Abbreviated duplicated text] new york ( ap ) over his career, formerheavyweight champion mike tyson recorded 50 wins and six losses. But he recently notched another bigloss in latin america this time as a coach of a bird. Tyson traveled to suriname as part of the new fusiontv documentary series \" outpost \" and was soundly beaten when he This second example also showcases the characteristics of GPT-3.5 model we used. In this example, it isobvious that Document 2 is less relevant to the summary, which is mainly about the relationship betweenGwyneth Paltrow and Chris Martin. However, while it is not the main content of the document as wellas Document 2, Document 1 contains a sentence that mentions the relationship between the two (heramicable split from husband chris martin of coldplay). Nonetheless, the model predicted Document 1 isalso irrelevant to the summary, implying the model is stringent to the partial contribution of the documentto the summary. However, it is important to note that we categorized these instances as errors based onrigorous human evaluation, and such errors constituted fewer than 10% of the total classifications, wherea single flag by multiple human evaluators was sufficient to deem it an error. We are planning to manuallyrevise these errors in the released version of MULTI-NEWS+. SummaryGwyneth paltrow continues to paint the sunniest of pictures of her post-conscious-uncoupling life withchris martin, but the description she gives glamour in a new interview may be the most interesting one sofar. \" were still very much a family, even though we dont have a romantic relationship. Hes like mybrother, \" she says, explaining that the two of them and their two kids still spend quite a bit of timetogether, even staying in one anothers houses and spending holidays together ( not to mentioncollaborating on songs together ). \" the ideal is to stay married. But if you cant stay married, wouldntthe ideal be that you could still be a family and you could put aside your own stuff long enough to explore what is this new family and who am i in it? \" paltrow muses. \" and chris is a great ex-husband causehes a very, very willing partner in how to do that. \" she adds that, though shes \" very independent, \" shedoes see the value in having a husband, and though shes not quite divorced yet, she could perhaps seeherself getting married again someday. ( click to see what she has to say about her other famous exes. ) Document 1Gwyneth paltrow is in a state of deep focus. The new goop office is under construction \"its like a dustbowl, \" she says with a laugh so today shes helming her company from the kitchen island of her losangeles home. Fitting, considering it was at her kitchen table ( then in london ) that paltrow, 43, startedgoop as a newsletter to friends nearly eight years ago. Since then, she has built goop into a global brand:it has produced sought-after collaborations with valentino and stella mccartney; opened pop-up shops;and brought terms like conscious uncoupling and vaginal steaming to the masses ( the first a descriptionof her amicable split from husband chris martin of coldplay; the second, a way to cleanse ones uterus dont try it at home ). Her presence has also unwittingly exposed a dirty little secret: as fans, we provideactresses with wealth and fame, only to scoff when they actually lead that rich and famous lifestylepublicly. We want these stars to be \"just like us. \" but paltrows life simply isnt. She wont pretend thatshe shops at the dollar store for beauty products or feeds her kids, apple, 11, and moses, 9, a steady diet offast food; Document 2Gwyneth paltrow was definitely in the mood to share during her appearance on howard sterns siriusxmradio show on wednesday.... Especially when it came to her a-list exes. In the hour-long chat, stern ofcourse wanted to know all about paltrows ex-fiance brad pitt, who the shakespeare in love star wasengaged to when she was 24 years old. The beautiful blondes eventually called it quits in 1997 after threeyears together. Getty images \" i didnt think about it at the time, but i m sure it did help with my career, \"the now 42-year-old actress admits about the start of all the paparazzi attention when the two got togetheron the set of seven. \" i definitely fell in love with him. He was so gorgeous, and sweet i mean, he wasbrad pitt, you know? \" video: a history of gwyneths former flames her parents, the late bruce paltrow andactress blythe danner, also 100 percent approved. \" my father was so devastated when we broke up, \" shedishes. \" my father loved him like he was his son. \" in hindsight, she blames the demise of theirrelationship on her youth. \" i was such a kid, i was 22 when we met, \" she explains.",
  "JPrompt": "This section describes the prompt used for dataset cleansing. We truncated several parts to save space andonly displayed a 1-shot CoT example. Please refer to the released source code for the exact prompt. SystemYou are a helpful assistant. Your job is to identify the document that is not relevant to the summary. Therelevance is determined by the mention of the key points in the summary. For example, the alert messagefrom the twitter, the location information, and the unrelated information are considered irrelevant. Pleaseresponse with the resaoning process and the result as document number or None. Do not end theresponse with a period. If multiple documents are irrelevant to the summary, please resonse in the form ofDocument 1|Document 3 UserGiven the summary and documents, please identify the document that is not relevant to the summary suchas advertisements. If every document is relevant to the summary, output None. Input document will begiven in next dialogue.",
  "User[Summary]# The content of summary[Document 1]# The content of document 1[Document 2]# The content of document 2[Document 3]# The content of document 3[Document 4]# The content of document 4": "AssistantThe summary discusses Tucker Carlsons controversial tweet regarding Sarah Palin and the subsequentbacklash he faced from conservative women. Document 1 is related to Tucker Carlsons controversialtweet. Document 2 is unrelated as it contains information about adding location information to tweets anddoes not contribute to the discussion about Tucker Carlsons tweet and the reaction it received. Document3 is a tweet message that calls Tucker Carlson a misogynist, related to the summary. Document 4 isrelated to the summary as it deals with Tucker Carlsons movement. Therefore, the irrelevant document is:Document 2"
}