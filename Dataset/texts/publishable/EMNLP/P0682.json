{
  "Abstract": "Electronic healthcare records are vital for pa-tient safety as they document conditions, plans,and procedures in both free text and medicalcodes. Language models have significantly en-hanced the processing of such records, stream-lining workflows and reducing manual data en-try, thereby saving healthcare providers signif-icant resources. However, the black-box na-ture of these models often leaves healthcareprofessionals hesitant to trust them. State-of-the-art explainability methods increase modeltransparency but rely on human-annotated ev-idence spans, which are costly. In this study,we propose an approach to produce plausibleand faithful explanations without needing suchannotations. We demonstrate on the automatedmedical coding task that adversarial robustnesstraining improves explanation plausibility andintroduce AttInGrad, a new explanation methodsuperior to previous ones. By combining bothcontributions in a fully unsupervised setup, weproduce explanations of comparable quality, orbetter, to that of a supervised approach. Werelease our code and model weights. 1",
  "Introduction": "Explainability in natural language processing re-mains a largely unsolved problem, posing signif-icant challenges for healthcare applications (Lyuet al., 2023). For every patient admission, a health-care professional must read extensive documenta-tion in the healthcare records to assign appropri-ate medical codes. A code is a machine-readableidentifier for a diagnosis or procedure, pivotalfor tasks such as statistics, documentation, andbilling. This process can involve sifting throughthousands of words to choose from over 140,000possible codes (Johnson et al., 2016), making med-",
  ": Example of an input, prediction, and featureattribution explanation highlighted in the input": "ical coding not only time-consuming but also error-prone (Burns et al., 2012; Tseng et al., 2018).Automated medical coding systems, poweredby machine learning models, aim to alleviate theseburdens by suggesting medical codes based on free-form written documentation. However, when re-viewing suggested codes, healthcare professionalsmust still manually locate relevant evidence in thedocumentation. This is a slow and strenuous pro-cess, especially when dealing with extensive docu-mentation and numerous medical codes. Explain-ability is essential for making this process tractable. Feature attributions, a common form of explain-ability, can help healthcare professionals quicklyfind the evidence necessary to review a medicalcode suggestion (see ). Feature attribu-tion methods score each input feature based on itsinfluence on the models output. These explana-tions are often evaluated through plausibility andfaithfulness (Jacovi and Goldberg, 2020). Plausi-bility measures how convincing an explanation isto human users, while faithfulness measures theexplanations ability to reflect the models logic.While previous work has proposed feature at-tribution methods for automated medical coding,they only evaluated attention-based feature attri-bution methods.Furthermore, the state-of-the-art method uses a supervised approach relying oncostly evidence-span annotations. This reliance onmanual annotations significantly limits practicalapplicability, as each code system and its versions require separate manual annotations (Mullenbachet al., 2018; Teng et al., 2020; Dong et al., 2021;Kim et al., 2022; Cheng et al., 2023).In this study, we present an approach for produc-ing explanations of comparable quality to the su-pervised state-of-the-art method but without usingevidence span annotations. We implement adver-sarial robustness training strategies to decrease themodels dependency on irrelevant features, therebyavoiding such features in the explanations (Tsipraset al., 2018). Moreover, we present more faithfulfeature attribution methods than the attention-basedmethod used in previous studies. Our key contribu-tions are:",
  "Explainable automated medical coding": "Automated medical coding is a multi-label classi-fication task that aims to predict a set of medicalcodes from J classes based on a given medicaldocument (Edin et al., 2023). In this context, theobjective of explainable automated medical codingis to generate feature attribution scores for each ofthe J classes. These scores quantify how mucheach input token influences each classs prediction.Most studies in explainable automated medicalcoding use attention weights as feature attributionscores without comparing to other methods (Mul-lenbach et al., 2018; Teng et al., 2020; Dong et al.,2021; Feucht et al., 2021; Cheng et al., 2023). How-ever, two studies suggest alternative feature attri-bution methods. Xu et al. (2019) propose a featureattribution method tailored to their one-layer CNNarchitecture but do not compare performance with other methods. Kim et al. (2022) train a linear med-ical coding model using knowledge distillation anduse its weights as the explanation. However, theirmethod does not improve over the explanations ofthe popular attention approach. Cheng et al. (2023)improve the plausibility of the attention weights ofthe final layer by training them to align with evi-dence span annotations. However, obtaining suchannotations is costly.Previous work focused on plausibility using hu-man ratings (Mullenbach et al., 2018; Teng et al.,2020; Kim et al., 2022), example inspection (Donget al., 2021; Feucht et al., 2021; Liu et al., 2022),or evidence span overlap metrics (Xu et al., 2019;Cheng et al., 2023). Notably, no studies have as-sessed the faithfulness of the explanations nor com-pared the attention-based methods with other estab-lished methods.",
  "Adversarial robustness and explainability": "Adversarial robustness refers to the ability of amachine learning model to maintain performanceunder adversarial attacks, which involve makingsmall changes to input data that do not significantlyaffect human perception or judgment (e.g., a smallamount of image noise). Tsipras et al. (2018) andIlyas et al. (2019) demonstrate that adversarial ex-amples exploit the models dependence on fragile,non-robust features. Adversarial robustness train-ing embeds invariances to prevent models relyingon such non-robust features, with regularizationand data augmentation as main strategies (Tsipraset al., 2018; Ros and Doshi-Velez, 2018).Previous work in image classification shows thatadversarially robust models generate more plausi-ble explanations (Ros and Doshi-Velez, 2018; Chenet al., 2019; Etmann et al., 2019). These studiesdemonstrate this phenomenon for three adversarialtraining strategies: 1) input gradient regularization,which improves the Lipschitzness of neural net-works (Drucker and Le Cun, 1992; Ros and Doshi-Velez, 2018; Chen et al., 2019; Etmann et al., 2019;Rosca et al., 2020; Fel et al., 2022; Khan et al.,2023), 2) adversarial training, which trains modelson adversarial examples, thereby embeds invari-ance to adversarial noise (Tsipras et al., 2018), and3) feature masking, which masks unimportant fea-tures during training to embed invariance to suchfeatures (Bhalla et al., 2023).The relationship between model robustness andexplanation plausibility in Natural Language Pro-cessing (NLP) remains unclear, primarily due to the fundamental differences between textual to-kens and image pixels. To date, only two stud-ies, Yoo and Qi (2021) and Li et al. (2023), haveexplored this relationship in depth. These studiessuggest that robust models produce more faithfulexplanations compared to their non-robust counter-parts. However, their conclusions may be question-able due to the use of the Area Over the Perturba-tion Curve (AOPC) metric, which is unsuitable forcross-model comparisons (Edin et al., 2024). Thisinappropriate application of AOPC may have ledto potentially misleading conclusions.",
  "Adversarial robustness training strategies": "We implemented three adversarial training strate-gies, which we hypothesized could decrease ourmedical coding models reliance on irrelevant to-kens: Input gradient regularization, projected gra-dient descent, and token masking. We chose thesestrategies because they have been shown to improveplausibility in image classification and faithfulnessin text classification (Li et al., 2023) Input gradient regularization (IGR)encour-ages the gradient of the output with respect to theinput to be small. This aims to decrease the num-ber of features on which the model relies, encour-aging it to ignore irrelevant words (Drucker andLe Cun, 1992). We adapt IGR to text classifica-tion by adding to the tasks binary cross-entropyloss, LBCE, the 2 norm of the gradient of LBCE wrt.the input token embedding sequence X RND.This yields the total loss,",
  "where y RJ is a binary target vector representingthe J medical codes, 1 is a hyperparameter, andf : RND RJ is the classification model": "Projected gradient descent (PGD)increasesmodel robustness by training with adversarial ex-amples, thereby promoting invariance to such in-puts (Madry, 2018). We hypothesized that PGD re-duces the models reliance on irrelevant tokens, as adversarial examples often arise from the modelsuse of such unrobust features (Tsipras et al., 2018).PGD aims to find the noise RND that maxi-mizes the loss LBCE(f(X +), y) while satisfyingthe constraint , where is a hyperparam-eter. PGD was originally designed for image classi-fication; we adapted it to NLP by adding the noiseto the token embeddings X. We implemented PGDas follows,",
  "where 2 is a hyperparameter": "Token masking (TM)teaches the model to pre-dict accurately while using as few features as pos-sible, thereby encouraging the model to ignore ir-relevant words. TM uses a binary mask to occludeunimportant tokens and train the model to rely onlyon the remaining tokens (Bhalla et al., 2023; Tomaret al., 2023). Inspired by Bhalla et al. (2023), weemployed a two-step teacher-student approach. Weused two copies of the same model already trainedon the automated medical coding task: a teacher ftwith frozen model weights and a student fs, whichwe fine-tuned. For each training batch, the firststep was to learn a sparse mask M ND,that still provided enough information to predictthe correct codes by minimizing:",
  "xm(X, M) = B (1 M) + X M ,": "where B RND is the baseline input. We choseB as the token embedding representing the starttoken, followed by the mask token embedding re-peated N 2 times, followed by the end tokenembedding. After optimization, we binarized themask M = round( M), where around 90% of thefeatures were masked. Finally, we tuned the modelfs using the following training objective:",
  "Feature attribution methods": "We evaluated several feature attribution meth-ods for automated medical coding, categorizingthem into three types: attention-based, gradient-based, and perturbation-based (more details in Ap-pendix B). Attention-based methods like Atten-tion (Mullenbach et al., 2018), Attention Roll-out (Abnar and Zuidema, 2020), and AttGrad (Ser-rano and Smith, 2019) rely on the models attentionweights. Gradient-based methods such as InputX-Grad (Sundararajan et al., 2017), Integrated Gra-dients (IntGrad) (Sundararajan et al., 2017), andDeeplift (Shrikumar et al., 2017) use backpropaga-tion to quantify the influence of input features onoutputs. Perturbation-based methods, includingLIME (Ribeiro et al., 2016), KernelSHAP (Lund-berg and Lee, 2017), and Occlusion@1 (Ribeiroet al., 2016), measure the impact on output confi-dence by occluding input features.Our investigation into feature attribution meth-ods revealed an intriguing pattern: while individualmethods often produced unreliable explanations,their shortcomings rarely overlapped. Attention-based methods and gradient-based approaches likeInputXGrad frequently disagreed on which tokenswere most important, yet both contributed valuableinsights in different scenarios. This observationsparked a key question: could we leverage the com-plementary strengths of these methods to create amore robust attribution technique?To address this question, we propose AttInGrad,a novel feature attribution method that combinesAttention and InputXGrad. AttInGrad multipliestheir respective attribution scores, aiming to am-plify the importance of tokens deemed relevant byboth methods while down-weighting those high-lighted by only one or neither method.We formalize the AttInGrad attribution scoresfor class j using the following equation:",
  "Data": "We conducted our experiments using the open-access MIMIC-III and the newly released MDACEdataset (Johnson et al., 2016; Cheng et al., 2023).MIMIC-III2 includes 52,722 discharge summariesfrom the Beth Israel Deaconess Medical CentersICU, collected between 2008 and 2016 and anno-tated with ICD-9 codes. MDACE comprises 302 re-annotated MIMIC-III cases, adding evidence spansto indicate the textual justification for each medicalcode. Not all possible evidence spans are anno-tated; for example, if hypertension is mentionedmultiple times, only the first mention might be an-notated, leaving subsequent mentions unannotated.We focused exclusively on discharge summaries, asmost previous medical coding studies on MIMIC-III (Teng et al., 2022). Statistics are in .For dataset splits, we used MIMIC-III full, a pop-ular split by Mullenbach et al. (2018), and MDACE,introduced by Cheng et al. (2023) for training andevaluating explanation methods. All MDACE ex-amples are from the MIMIC-III full test set, whichwe excluded from this test set when using MDACEin our training data.",
  "We decided to use MIMIC-III instead of the newerMIMIC-IV because we wanted to use the same dataset asCheng et al. (2023)": "issues caused by numerical overflow in the de-coder of the original model, we replaced the label-wise attention mechanism with standard cross-attention (Vaswani et al., 2017). This adjustmentnot only stabilized training but also slightly im-proved performance. We provide further details onthe architecture modifications in Appendix A.We compared five models: BU, BS, IGR, PGD,and TM. All models used our modified PLM-ICDarchitecture but were trained differently. BU wastrained unsupervised with binary cross-entropy,whereas BS employed a supervised auxiliary train-ing objective that minimized the KL divergence be-tween the models cross-attention weights and an-notated evidence spans, as per Cheng et al. (2023).IGR, PGD, and TM training is as in .2.Best hyperparameters are in Appendix D.",
  "Experiments": "We trained all five models with ten seeds on theMIMIC-III full and MDACE training set. The su-pervised training strategy BS used the evidencespan annotations, while the others only used themedical code annotations. For each model, weevaluated the plausibility and faithfulness of the ex-planations generated by every explanation method.We aimed to demonstrate a similar explanationquality as a supervised approach but without train-ing on evidence spans. Therefore, after evaluatingthe models and explanation methods, we comparedour best combination with the supervised strategyproposed by Cheng et al. (2023), who used the BSmodel and the Attention explanation method. Wealso compared our best combination with the un-supervised strategy used by most previous works(see .1), comprising the BU model and theAttention explanations method.",
  "Evaluation metrics": "We measured the explanation quality using metricsestimating plausibility and faithfulness. Plausibil-ity measures how convincing an explanation is tohuman users, while faithfulness measures how ac-curate an explanation reflects a models true rea-soning process (Jacovi and Goldberg, 2020). Plausibility metricsOur plausibility metricsmeasured the overlap between explanations andannotated evidence-spans. We assumed that a highoverlap indicated plausible explanations for medi-cal coders. We identified the most important tokensusing feature attribution scores, applying a decision boundary for classification metrics, and selectingthe top K scores for ranking metrics.For classification metrics, we used Precision(P), Recall (R), and F1 scores, selecting the de-cision boundary that yielded the highest F1 scoreon the validation set (Cheng et al., 2023). Addition-ally, we included four more classification metrics:Empty explanation rate (Empty), Evidence spanrecall (SpanR), Evidence span cover (Cover), andArea Under the Precision-Recall Curve (AUPRC).Empty measures the rate of empty explanationswhen all attribution scores in an example are belowthe decision boundary. SpanR measures the per-centage of annotated evidence spans where at leastone token is classified correctly. Cover measuresthe percentage of tokens in an annotated evidencespan that are classified correctly, given that at leastone token is predicted correctly. AUPRC representsthe area under the precision-recall curve generatedby varying the decision boundary from zero to one.For ranking metrics, we selected the top Ktokens with the highest attribution scores, usingRecall@K, Precision@K, and Intersection-Over-Unions (IOU) (DeYoung et al., 2020). Faithfulness metricsWe use two metrics to ap-proximate faithfulness: Sufficiency and Compre-hensiveness (DeYoung et al., 2020); more detailsare in Appendix C. Faithful explanations yield highComprehensiveness and low Sufficiency scores. Ahigh Sufficiency score indicates that many impor-tant tokens are incorrectly assigned low attributionscores, while a low Comprehensiveness score sug-gests that many non-important tokens are incor-rectly assigned high attribution scores.",
  "Next, we present experimental results for the differ-ent training strategies and explainability methods": "Rivaling supervised methods in explanationqualityThe objective of this paper was to pro-duce high-quality explanations without relying onevidence span annotations. In , we com-pare the plausibility of our approach (Token mask-ing and AttnInGrad) with the unsupervised ap-proach (BU and Attention) and supervised state-of-the-art approach (BS and Attention). Our ap-proach was substantially more plausible than theunsupervised on all metrics. Compared with thesupervised, our approach achieved similar F1 andRecall@5 and substantially better Empty scores. Attention+BS Attention+BU AttInGrad+TM F1 Score (%) SupervisedUnsupervised",
  "(c) Recall@5": ": Comparison of plausibility across various combinations of explanation methods and models from thisstudy and previous work. Most previous studies used Attention and a standard medical coding model (BU). Chenget al. (2023) instead used a supervised model trained on evidence-span annotations (BS). We proposed AttInGradand an adversarial robust model (TM). The supervised approach achieved similar plausi-bility to ours on most metrics (see ). Ourapproach also achieved the highest comprehensive-ness and lowest sufficiency scores (see ).The difference was larger in the sufficiency scores,where the supervised score was twice as high asours. Adversarial robustness improves plausibilityWe evaluated the explanation plausibility of ev-ery model and explanation method combinationin . IGR and TM outperformed the base-line model BU on most metrics and explanationmethods. In Appendix E.5, we compare the un-supervised models on a bigger test set and seesimilar results. The supervised model BS yieldedbetter results for attention-based explanations butwas weaker than the robust models when using thegradient-based explanation methods: InputXGrad,IG, and Deeplift. AttInGrad is more plausible and faithful thanAttentionAttInGrad was more plausible than allother explanation methods across all training strate-gies and metrics. Notably, plausibility improve-ments were particularly significant, with relativegains exceeding ten percent in most metrics (see and Appendix E.2). For instance, for BU,AttInGrad reduced the Empty metric from 21.1%to 3.0% and improved the Cover metric from 63.4%to 74.9%. However, these enhancements were lesspronounced for BS, the supervised model.AttInGrad was also more faithful than Atten-tion (see ). However, while AttInGradsurpassed the gradient-based methods in compre-hensiveness, its sufficiency scores were slightlyworse. Analysisofattention-basedexplanationsWhile AttInGrad and Attention were more plausi-ble than the gradient-based explanations, they hada three-fold higher inter-seed variance. We foundthat they often attributed high importance to tokensdevoid of alphanumeric characters such as G[, *,and C, which we classify as special tokens. Thesespecial tokens, such as punctuation and byte-pairencoding artifacts, rarely carry semantic meaning.In the MDACE test set, they accounted for 32.2%of all tokens, compared to just 5.8% within theannotated evidence spans, suggesting they areunlikely to be relevant evidence.In , we analyze the relationship betweenexplanation quality (y-axis) and the proportion ofthe top five most important tokens that are specialtokens (x-axis). Each data point represents the av-erage statistics across the MDACE test set for oneseed/run of the BU model. Figures 4a and 4b showF1 (plausibility) and comprehensiveness (faithful-ness) respectively. For Attention and AttInGrad,we see strong negative correlations for both metricswith a large inter-seed variance. The regressionlines fitted on Attention and AttInGrad overlap,with the data points from AttInGrad shifted slightlytowards the upper left, indicating attribution of lessimportance to special tokens.Conversely, for InputXGrad, we see a moder-ate negative correlation for the F1 score and nocorrelation for comprehensiveness. Furthermore,InputXGrad demonstrates a small inter-seed vari-ance, where the proportion of special tokens moreclosely mirrors that observed in the evidence spans.We hypothesized that AttInGrads improvementsover Attention stem from InputXGrad reducing spe-cial tokens attribution scores. We tested this byzeroing out these tokens scores. While it sub- : Plausibility of Attention, InputXGrad, Integrated Gradients (IntGrad), Deeplift, and AttInGrad on theMDACE test set. Each experiment was run with ten different seeds. We show the mean of the seeds as thestandard deviation. All the scores are presented as percentages. Bold numbers outperform the unsupervised baselinemodel, while underlined numbers outperform the supervised model. We included more feature attribution methodsin Appendix E.2.",
  "AttInGrad": "BS40.62.245.93.143.01.938.82.11.20.563.92.979.01.945.72.633.31.446.61.9BU40.72.942.54.441.53.237.13.63.01.359.35.474.95.243.54.032.02.144.93.0IGR38.84.544.04.041.24.037.24.52.30.760.35.374.54.643.34.931.92.744.73.8TM40.23.043.94.841.93.437.83.92.81.660.55.875.95.344.03.932.52.345.53.2PGD37.18.242.55.739.37.134.78.62.30.957.79.172.77.439.610.230.94.843.36.7 stantially enhanced Attentions F1 score, Attentionremained lower than AttInGrad (see ). IfAttInGrads sole contribution were filtering specialtokens, we would expect similar F1 scores afterzeroing their attributions. The fact that AttInGradstill outperforms Attention after controlling for spe-cial tokens suggests that there are additional factorsbeyond special token filtering contributing to At-tInGrads improved performance.",
  "Discussion": "Do we need evidence span annotations?Wedemonstrated that we could match the explanationquality of Cheng et al. (2023) but without super-vised training with evidence-span annotations (see). This raises the question: are evidence-span annotations unnecessary?Intuitively, training a model on evidence spansshould encourage it to use features relevant to hu-mans, thereby making its explanations more plausi-ble. However, we hypothesize that the training strat-egy used by Cheng et al. (2023) primarily addressesthe shortcomings of attention-based explanationmethods rather than enhancing the models underly- ing logic. The model BS only produced more plau-sible explanations with attention-based feature at-tribution methods (see ). If the model trulyleveraged more informative features, we would ex-pect to see improvements across various featureattribution methods. Additionally, the differencesbetween Attention and AttInGrad were negligiblefor BS compared to the other models. This maysuggest that the supervised training might have cor-rected some of the inherent issues in the Attentionmethod, similar to what AttInGrad achieves. Adversarial robustness training strategies im-pact on explanation plausibilityWhile IGR andTM generated more plausible explanations than BU,our evidence is insufficient to conclude whether theimprovements were caused by our adversarially ro-bust models relying on fewer irrelevant features.The adversarial robustness training strategies, espe-cially PGD, had a larger impact on the plausibilityof the explanations in previous image classificationstudies (Tsipras et al., 2018). We speculate that thisdiscrepancy is caused by the inherent differences inthe text and image modalities, causing techniquesdesigned for image classifiers to be less effective",
  "for text classifiers (Etmann et al., 2019)": "Limitations of attention-based explanationsDespite Attention and AttInGrad outperformingother methods in plausibility and faithfulness, theyexhibited significant shortcomings, including highsufficiency and inter-seed variation. These findingsalign with previous research questioning the faith-fulness of solely relying on final layer attentionweights (Jain and Wallace, 2019).We hypothesize these limitations stem from mis-alignment between the positions of the originaltokens and their encoded representations. Our anal-ysis () suggests the encoder may store con-textual information in uninformative tokens, suchas special tokens, which are then used by the fi- Special Tokens in Top 5 (%) F1 Score (%) r =0.89 r =0.50 r =0.71AttentionInputXGradAttInGrad",
  "(b) Comprehensiveness": ": The relationship between explanation qual-ity and the proportion of the top five most importanttokens that are special tokens (tokens devoid of alphanu-meric characters). Each data point is the average statisticon the MDACE test set for a seed of BU. We fitted alinear regression for each explanation method and cal-culated the Pearson correlation (r). The dotted verticallines represent the proportion of special tokens in theevidence-span annotations. nal attention layer for classification. As the train-ing loss does not penalize where contextualizedinformation is placed, this location can vary acrosstraining iterations, leading to the observed highinter-seed variance in attention-based explanations.Training strategies that enforce alignment be-tween original tokens and their encoded represen-tations could alleviate the limitations of Attentionand AttInGrad. This alignment might explain thebenefits of the supervised training strategy pro-posed by Cheng et al. (2023). However, ratherthan restricting the model, future research shouldexplore feature attribution methods that incorporateinformation from all transformer layers, not justthe final one (Kobayashi et al., 2021). Althoughattention rollout, a method incorporating all atten-tion layers, proved unsuccessful in our experiments(see Appendix E.2), recent studies have highlighted",
  "its shortcomings and proposed alternative featureattribution methods that may be more suitable forour task (Modarressi et al., 2022, 2023)": "RecommendationsSimilar to Lyu et al. (2023),we advocate that future research on feature attribu-tion methods prioritize enhancing their faithfulness,as focusing solely on plausibility can yield mislead-ing explanations. When models misclassify or relyon irrelevant features, explanations can only appearplausible if they ignore the models actual reason-ing process. Overemphasizing plausibility mayinadvertently lead researchers to favor approachesthat produce explanations disconnected from themodels true reasoning.Instead, we propose that researchers prioritizeimproving the faithfulness of feature attributionmethods while also working to align the modelsreasoning process with that of humans. This ap-proach not only enhances the plausibility and faith-fulness of explanations but also contributes to theaccuracy and robustness of model classifications.",
  "Conclusion": "Our goal was to enhance the plausibility and thefaithfulness of explanations without evidence-spanannotations. We found that training our model us-ing input gradient regularization or token maskingresulted in more plausible gradient-based expla-nations. We proposed a new explanation method,AttInGrad, which was substantially more plausibleand faithful than the attention-based explanationmethod used in previous studies. By combining thebest training strategy and explanation method, weshowed results of similar quality to a supervisedbaseline (Cheng et al., 2023).",
  "Limitations": "Our study did not conclusively show why adversar-ial robustness training strategies improved the ex-planation plausibility. We hypothesized that thesestrategies force the model to rely on fewer featuresthat weakly correlate with the labels, and such fea-tures are less plausible. However, validating thishypothesis proved challenging. Our analysis of fea-ture attributions entropy was inconclusive, as de-tailed in Appendix E.4. Moreover, we did not knowwhich features the model relied on because thiswould require a perfect feature attribution method,which is what we aimed to develop. Despite thesechallenges, we demonstrated that the adversarial ro-bust models produced more plausible explanations. We believe that our work has laid a solid founda-tion for future research into how model trainingstrategies can impact explanation plausibility.Our studys scope was constrained to a singledata source (Beth Israel Deaconess Medical Cen-ters MIMIC-III and MDACE) and one model ar-chitecture (PLM-ICD). The effectiveness of Atten-tion and AttInGrad may vary with different archi-tectures, particularly those employing multi-headattention and skip connections in the final layer.Further research is needed to investigate the gen-eralizability of our findings across diverse modelarchitectures, medical coding systems, languages,and healthcare institutions.Furthermore, the limited size of the MDACEtest set constrained our study, resulting in low sta-tistical power for many experiments. Despite thedesire to conduct more trials with various seeds, welimited ourselves to ten seeds per training strategydue to the high computational costs involved. Con-ducting more experiments or expanding the test setmight have revealed nuances and differences thatour initial setup failed to detect. Nevertheless, ourresults across runs, explanation methods, and analy-sis point in the same direction. Moreover, while thetest set in the main paper only comprises 61 exam-ples, each example contains 14 medical codes, eachannotated with multiple evidence spans, providinggreater statistical power. Finally, our comparisonof the unsupervised approaches on the larger testset in Appendix E.5 demonstrated similar resultsas on the smaller test set in the main paper. We,therefore, believe that our claims in this paper arewell substantiated with empirical evidence.",
  "Ethics statement": "Healthcare costs are continuously increasing world-wide, with administrative costs being a significantcontributing factor (Tseng et al., 2018). In thispaper, we propose methods that may help reducethese administrative costs by making the review ofmedical code suggestions easier and faster. Theaim of this paper was to develop technology toassist medical coders in performing tasks fasterinstead of replacing them.Plausible but unfaithful explanations may riskconvincing medical coders to accept medical codesuggestions that are incorrect, thereby risking thepatients safety (Jacovi and Goldberg, 2020). We,therefore, advocate faithfulness to be of higher pri-ority than in previous studies. Electronic healthcare records contain privateinformation. The healthcare records in MIMIC-III, the dataset used in this paper, have beenanonymized and stored in encrypted data storageaccessible only to the main author, who has a li-cense to the dataset and HIPAA training.",
  "Acknowledgements": "This research was partially funded by the Innova-tion Fund Denmark via the Industrial Ph.D. Pro-gram (grant no. 2050-00040B) and Research Coun-cil of Finland (grant no. 322653). We thank JonasLyngs for insightful discussions and code for dat-aloading. Furthermore, we thank Simon Flachs,Lana Krumm, and Andreas Geert Motzfeldt forrevisions. Samira Abnar and Willem Zuidema. 2020. Quantify-ing Attention Flow in Transformers. In Proceedingsof the 58th Annual Meeting of the Association forComputational Linguistics, pages 41904197, On-line. Association for Computational Linguistics. Usha Bhalla, Suraj Srinivas, and Himabindu Lakkaraju.2023. Discriminative Feature Attributions: BridgingPost Hoc Explainability and Inherent Interpretabil-ity. In Advances in Neural Information ProcessingSystems 36 (NeurIPS 2023).",
  "E.M. Burns, E. Rigby, R. Mamidanna, A. Bottle,P. Aylin, P. Ziprin, and O.D. Faiz. 2012. System-atic review of discharge coding accuracy. Journal ofPublic Health (Oxford, England), 34(1):138148": "Jiefeng Chen, Xi Wu, Vaibhav Rastogi, Yingyu Liang,and Somesh Jha. 2019. Robust Attribution Regu-larization. In 33rd Conference on Neural Informa-tion Processing Systems (NeurIPS 2019), Vancouver,Canada. Hua Cheng, Rana Jafari, April Russell, Russell Klopfer,Edmond Lu, Benjamin Striner, and Matthew Gorm-ley. 2023. MDACE: MIMIC Documents Annotatedwith Code Evidence. In Proceedings of the 61st An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 75347550, Toronto, Canada. Association for Computa-tional Linguistics. Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani,Eric Lehman, Caiming Xiong, Richard Socher, andByron C. Wallace. 2020. ERASER: A Benchmark toEvaluate Rationalized NLP Models. In Proceedingsof the 58th Annual Meeting of the Association forComputational Linguistics, pages 44434458, Online.Association for Computational Linguistics. Hang Dong, Vctor Surez-Paniagua, William White-ley, and Honghan Wu. 2021. Explainable automatedcoding of clinical notes using hierarchical label-wiseattention networks and label embedding initialisation.Journal of Biomedical Informatics, 116:103728.",
  "H. Drucker and Y. Le Cun. 1992. Improving general-ization performance using double backpropagation.IEEE Transactions on Neural Networks, 3(6):991997": "Joakim Edin, Alexander Junge, Jakob D. Havtorn, LasseBorgholt, Maria Maistro, Tuukka Ruotsalo, andLars Maale. 2023. Automated Medical Coding onMIMIC-III and MIMIC-IV: A Critical Review andReplicability Study. In Proceedings of the 46th In-ternational ACM SIGIR Conference on Research andDevelopment in Information Retrieval, SIGIR 23,pages 25722582, New York, NY, USA. Associationfor Computing Machinery. Joakim Edin, Andreas Geert Motzfeldt, Casper L. Chris-tensen, Tuukka Ruotsalo, Lars Maale, and MariaMaistro. 2024. Normalized AOPC: Fixing Mislead-ing Faithfulness Metrics for Feature Attribution Ex-plainability. Preprint, arXiv:2408.08137. Christian Etmann, Sebastian Lunz, Peter Maass, andCarola Schoenlieb. 2019. On the Connection Be-tween Adversarial Robustness and Saliency Map In-terpretability. In Proceedings of the 36th Interna-tional Conference on Machine Learning, pages 18231832. PMLR. Thomas Fel, David Vigouroux, Remi Cadene, andThomas Serre. 2022. How Good is your Explana-tion? Algorithmic Stability Measures to Assess theQuality of Explanations for Deep Neural Networks.In 2022 IEEE/CVF Winter Conference on Applica-tions of Computer Vision (WACV), pages 15651575,Waikoloa, HI, USA. IEEE. Malte Feucht, Zhiliang Wu, Sophia Althammer, andVolker Tresp. 2021. Description-based Label Atten-tion Classifier for Explainable ICD-9 Classification.In Proceedings of the Seventh Workshop on NoisyUser-generated Text (W-NUT 2021), pages 6266,Online. Association for Computational Linguistics. Chao-Wei Huang, Shang-Chi Tsai, and Yun-Nung Chen.2022. PLM-ICD: Automatic ICD Coding with Pre-trained Language Models. In Proceedings of the4th Clinical Natural Language Processing Workshop,pages 1020, Seattle, WA. Association for Computa-tional Linguistics. Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Lo-gan Engstrom, Brandon Tran, and Aleksander Madry.2019. Adversarial Examples Are Not Bugs, TheyAre Features. In Advances in Neural InformationProcessing Systems, volume 32. Curran Associates,Inc.",
  "Zulqarnain Khan, Davin Hill, Aria Masoomi, JoshuaBone, and Jennifer Dy. 2023. Analyzing ExplainerRobustness via Lipschitzness of Prediction Functions.Preprint, arXiv:2206.12481": "Byung-Hak Kim, Zhongfen Deng, Philip S. Yu, andVarun Ganapathi. 2022. Can Current ExplainabilityHelp Provide References in Clinical Notes to Sup-port Humans Annotate Medical Codes?Preprint,arXiv:2210.15882. Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, andKentaro Inui. 2021. Incorporating Residual and Nor-malization Layers into Analysis of Masked LanguageModels. In Proceedings of the 2021 Conference onEmpirical Methods in Natural Language Processing,pages 45474568, Online and Punta Cana, Domini-can Republic. Association for Computational Lin-guistics. Narine Kokhlikyan, Vivek Miglani, Miguel Martin,Edward Wang, Bilal Alsallakh, Jonathan Reynolds,Alexander Melnikov, Natalia Kliushkina, CarlosAraya, Siqi Yan, and Orion Reblitz-Richardson. 2020.Captum: A unified and generic model interpretabilitylibrary for PyTorch. Preprint, arXiv:2009.07896.",
  "AlexandreLacoste,AlexandraLuccioni,VictorSchmidt, and Thomas Dandres. 2019.Quantify-ing the Carbon Emissions of Machine Learning.Preprint, arXiv:1910.09700": "Patrick Lewis, Myle Ott, Jingfei Du, and VeselinStoyanov. 2020. Pretrained Language Models forBiomedical and Clinical Tasks: Understanding andExtending the State-of-the-Art. In Proceedings of the3rd Clinical Natural Language Processing Workshop,pages 146157, Online. Association for Computa-tional Linguistics. Dongfang Li, Baotian Hu, Qingcai Chen, and ShanHe. 2023. Towards Faithful Explanations for TextClassification with Robustness Improvement and Ex-planation Guided Training. In Proceedings of the 3rdWorkshop on Trustworthy Natural Language Process-ing (TrustNLP 2023), pages 114, Toronto, Canada.Association for Computational Linguistics. Leibo Liu, Oscar Perez-Concha, Anthony Nguyen,Vicki Bennett, and Louisa Jorm. 2022. Hierarchicallabel-wise attention transformer model for explain-able ICD coding. Journal of Biomedical Informatics,133:104161.",
  "Zico Kolter and Aleksander Madry. 2018. AdversarialRobustness - Theory and Practice": "Ali Modarressi, Mohsen Fayyaz, Ehsan Aghazadeh,Yadollah Yaghoobzadeh, and Mohammad Taher Pile-hvar. 2023. DecompX: Explaining Transformers De-cisions by Propagating Token Decomposition. InProceedings of the 61st Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 26492664, Toronto, Canada.Association for Computational Linguistics. AliModarressi,MohsenFayyaz,YadollahYaghoobzadeh,andMohammadTaherPile-hvar. 2022. GlobEnc: Quantifying Global TokenAttribution by Incorporating the Whole EncoderLayer in Transformers. In Proceedings of the 2022Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 258271, Seattle,UnitedStates.AssociationforComputationalLinguistics. James Mullenbach, Sarah Wiegreffe, Jon Duke, JimengSun, and Jacob Eisenstein. 2018. Explainable Pre-diction of Medical Codes from Clinical Text. In Pro-ceedings of the 2018 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, Volume1 (Long Papers), pages 11011111, New Orleans,Louisiana. Association for Computational Linguis-tics. Marco Tulio Ribeiro, Sameer Singh, and CarlosGuestrin. 2016. \"Why Should I Trust You?\": Explain-ing the Predictions of Any Classifier. In Proceedingsof the 22nd ACM SIGKDD International Conferenceon Knowledge Discovery and Data Mining, pages11351144, San Francisco California USA. ACM. Andrew Slavin Ros and Finale Doshi-Velez. 2018. Im-proving the adversarial robustness and interpretabil-ity of deep neural networks by regularizing theirinput gradients.In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligenceand Thirtieth Innovative Applications of Artificial In-telligence Conference and Eighth AAAI Symposiumon Educational Advances in Artificial Intelligence,AAAI18/IAAI18/EAAI18, pages 16601669, NewOrleans, Louisiana, USA. AAAI Press.",
  "Mihaela Rosca, Theophane Weber, Arthur Gretton, andShakir Mohamed. 2020. A case for new neural net-work smoothness constraints. pages 2132. PMLR": "Sofia Serrano and Noah A. Smith. 2019. Is Attention In-terpretable? In Proceedings of the 57th Annual Meet-ing of the Association for Computational Linguistics,pages 29312951, Florence, Italy. Association forComputational Linguistics. Avanti Shrikumar, Peyton Greenside, and Anshul Kun-daje. 2017.Learning important features throughpropagating activation differences. In Proceedingsof the 34th International Conference on MachineLearning - Volume 70, ICML17, pages 31453153,Sydney, NSW, Australia. JMLR.org.",
  "Fei Teng, Wei Yang, L. Chen, Lufei Huang, and QiangXu. 2020. Explainable Prediction of Medical CodesWith Knowledge Graphs. Frontiers in Bioengineer-ing and Biotechnology": "Manan Tomar, Riashat Islam, Matthew E. Taylor, SergeyLevine, and Philip Bachman. 2023. Ignorance isBliss: Robust Control via Information Gating. InAdvances in Neural Information Processing Systems36 (NeurIPS 2023). arXiv. Phillip Tseng, Robert S. Kaplan, Barak D. Richman, Ma-hek A. Shah, and Kevin A. Schulman. 2018. Admin-istrative Costs Associated With Physician Billing andInsurance-Related Activities at an Academic HealthCare System. JAMA, 319(7):691697.",
  "Dimitris Tsipras, Shibani Santurkar, Logan Engstrom,Alexander Turner, and Aleksander Madry. 2018. Ro-bustness May Be at Odds with Accuracy. In Interna-tional Conference on Learning Representations": "Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is Allyou Need. In Advances in Neural Information Pro-cessing Systems, volume 30. Curran Associates, Inc. Keyang Xu, Mike Lam, Jingzhi Pang, Xin Gao, Char-lotte Band, Piyush Mathur, Frank Papay, Ashish K.Khanna, Jacek B. Cywinski, Kamal Maheshwari,Pengtao Xie, and Eric P. Xing. 2019. MultimodalMachine Learning for Automated ICD Coding. InProceedings of the 4th Machine Learning for Health-care Conference, pages 197215. PMLR.",
  "AModel architecture details": "PLM-ICD is a state-of-the-art automated medi-cal coding model (Huang et al., 2022; Edin et al.,2023). It comprises 131 million parameters. We ex-perienced that PLM-ICD occasionally crashed dur-ing training. Therefore, we modified the architec-ture and called it pre-trained language model withclass-wise cross attention (PLM-CA) (Huang et al.,2022). Our architecture comprises an encoder anda decoder (see ). The encoder transformsa sequence of tokens indices t {0, 1, . . . , V }N into a sequence of contextualized token represen-tations H RND. Both PLM-ICD and PLM-CA use RoBERTa-PM, a transformer pre-trainedon PubMed articles and clinical notes, as the en-coder (Lewis et al., 2020).Our decoder takes the token representations Has input and outputs a sequence of output probabil-ities y J. It computes the output probabili-ties from the contextualized token representationsusing the following equation:",
  "studies in automated medical coding (Mullenbachet al., 2018; Kim et al., 2022; Dong et al., 2021;Teng et al., 2020; Cheng et al., 2023)": "Attention Rollout (Rollout)The attention ma-trix in the cross-attention layer extracts informa-tion from the contextualized token representationsencoded by RoBERTa (see ). The tokenrepresentations are not guaranteed to be alignedwith the input tokens. A token representation atposition n could represent any and multiple tokensin the document. Attention rollout considers allthe models attention layers to calculate the featureattributions (Abnar and Zuidema, 2020). First, theattention matrices in each layer are averages acrossthe heads. Then, the identity matrix is added toeach layers attention matrix to represent the skipconnections. Finally, the attention rollout is calcu-lated recursively using Equation (4).",
  "A(l) A(l1)if l > 0A(l)if l = 0(4)": "where A RNN is the rollout attention, andA RNN is the attention averaged across headswith the added identity matrix. We calculated thefinal feature attribution score by multiplying therollout attention from the final layer with the atten-tion matrix from the cross-attention layer: A A(L),where L is the number of attention layers.",
  "Occlusion@1Occlusion@1 calculates each fea-tures score by occluding it and measuring thechange in output confidence. The change of outputwill be the features score (Ribeiro et al., 2016)": "LIMELocal Interpretable Model-agnostic Ex-planations (LIME) randomly occlude sets of tokensfrom a specific input and measure the change inoutput confidence. It uses these measurements totrain a linear regression model that approximatesthe explained models reasoning for that particularexample. It then uses the linear regression weightsto approximate each features influence (Ribeiroet al., 2016). KernelSHAPShapley Additive Explanations(SHAP) is based on Shapley values from coop-erative game theory, which fairly distributes thepayout among players by considering each playerscontribution in all possible coalitions.Ideally,SHAP quantifies all possible feature combinationsin an input by occluding them and measuring theimpact.However, this would result in N for-wards passes. KernelSHAP employs the LIMEframework to approximate Shapley values using aweighted linear regression approach efficiently. Werefer the reader to the seminal paper introducingSHAP and KernelSHAP for more details (Lund-berg and Lee, 2017). InputXGradInputXGradient multiplies the in-put gradients with the input (Shrikumar et al.,2017). We used the L2 norm to get the final fea-ture attribution scores. We calculated the featureattribution scores for class J as follows:",
  "(5)": "where X RND is the input token embeddings, is the element-wise matrix multiplication oper-ation, D is the embedding dimension, N are thenumber of tokens in a document, and J is the num-ber of classes. Integrated Gradients (IntGrad)Integrated Gra-dients (IntGrad) assigns an attribution score to eachinput feature by computing the integral of the gra-dients along the straight line path from the baselineB to the input X (Sundararajan et al., 2017). Simi-lar to InputXGradient, we used the L2-norm of theoutput to get the final attribution scores. DeepliftDeepLIFT (Deep Learning ImportantFeaTures) backpropagates the contributions of allneurons in the model to every input feature (Shriku-mar et al., 2017). It compares each neurons activa-tion to its baseline activation and assigns attributionscores according to the difference.",
  "DTraining details": "We used the same hyperparameter as Edin et al.(2023).We trained for 20 epochs with theADAMW optimizer (Loshchilov and Hutter, 2022),learning rate at 5 105, dropout at 0.2, no weightdecay, and a linear decay learning rate schedulerwith warmup. We found the optimal hyperparame-ters for the auxiliary adversarial robustness trainingobjectives through random search. For each train-ing strategy, we searched the following options:learning rate: {5 105, 1 105}, 1, 2, 3, :{1.0, 0.5, 0.1, 102, 103, 104, 105, 106}, and: {103, 104, 105, 106}. We found these hy-perparameters to be optimal: 1 = 105, 2 = 0.5,3 = 0.5, = 105, and = 0.01. The learn-ing rate was optimal at 5 105 for all trainingstrategies except for token masking, where 1 105",
  "was optimal. We optimized the token mask andadversarial noise using the ADAMW optimizer.In token-masking, we initialized the student andteacher model from a trained BU. We fine-tuned the": "student for one epoch. We used the same hyperpa-rameters as Cheng et al. (2023) for the supervisedtraining strategy.We did not preprocess the text except to truncatethe documents to a maximum length of 6000 tokensto reduce memory usage. Truncation is a commonstrategy in automated medical coding and has anegligible negative impact because few documentsexceed the 6000 token limit (Edin et al., 2023).",
  "E.1Advesarial training does not affect codeprediction performance": "Previous papers have demonstrated that adversarialrobustness often comes at the cost of accuracy (Liet al., 2023; Tsipras et al., 2018). Therefore, weevaluated whether the training strategies impactedthe models medical code prediction capabilities.As shown in , all models performed sim-ilarly on the MDACE test set. We also observednegligible performance differences on the MIMIC-III full test set.",
  "BS67.50.451.51.172.00.7BU67.70.351.60.772.00.4IGR68.00.652.21.272.41.0TM68.10.651.81.572.30.4PGD68.1 0.552.1 1.272.0 0.7": "AttGrad, Attention Rollout (Rollout), InputXGrad,Integrated gradients (IntGrad), Deeplift, and AttIn-Grad. We compare these methods with a randombaseline (Rand), which randomly generates attribu-tion scores. We present the plausibility results in, and the faithfulness results in .We did not include Occlusion@1, LIME, andKernelSHAP in these tables because they were tooslow to calculate. We used the Captum implemen-tation of the algorithms (Kokhlikyan et al., 2020).It took around 45 minutes on an A100 GPU to cal-culate the explanations for a single example withLIME and KernelSHAP. Therefore, we only evalu-ated these methods on a single trained instance ofBU. We present the results in .",
  "E.4Entropy of explanation methods": "We calculated the entropy of the feature attribu-tion distributions to test our hypothesis that robusttraining strategies reduce the number of featuresthe model uses (see ). The training strate-gies did not reduce the entropy. While we wouldexpect a reduced entropy if the model used fewerfeatures, other feature attribution distribution dif-ferences may simultaneously increase the entropy.The analysis is, therefore, inconclusive.",
  "E.5Unsupervised comparison on bigger testset": "We included additional experiments on the unsu-pervised training strategies on a bigger test set.Since only the supervised training strategy requiredevidence-span annotations in the training set, weretrained our unsupervised methods on the MIMIC-III full training set and evaluated them on theMDACE training and test set (242 examples).We present the plausibility results in .We observe that the results are similar to those ofthe main paper. However, the IGR produced sub-stantially better attention-based explanations thanin the main paper. In , we inspect theinter-seed variance. We observe that IGR has nooutliers. We, therefore, attribute the differences be-tween this comparison and that in the main paper tonone of the ten IGR runs happening to produce anoutlier model. These results highlight the fragilityof evaluating the attention-based feature attributionmethods.",
  "FCO2 emissions": "Experiments were conducted using a private infras-tructure, which has a carbon efficiency of 0.185kgCO2eq/kWh. To train BU or BS, a cumulativeof 8 hours of computation was performed on hard-ware of type A100 PCIe 40/80GB (TDP of 250W).Total emissions for one run are estimated to be0.37 kgCO2eq. The adversarial robustness train-ing strategies required more hours of computation,therefore causing higher emissions. Input gradientregularization and projected gradient regulariza-tion required approximately 36 hours each (1.67kgCO2eq), while token masking required 2.5 hoursof fine-tuning of BU (0.09 kgCO2eq). We ran each experiment 10 times, resulting in total emissionsof 41.86 kgCO2eq, which is equivalent to burning20.9 Kg of coal. Estimations were conducted usingthe MachineLearning Impact calculator (Lacosteet al., 2019).",
  "GLicenses": "We used MIMIC-III version 1.4, which is dis-tributed under a non-commercial license, as de-tailed here: model weights released in this paper are alsorestricted to non-commercial use. However, theMDACE annotations and our code are availableunder the MIT License. We provide instructions tohow to obtain the datasets in our GitHub repository. : Plausibility comparison of Attention, AttGrad, Attention Rollout (Rollout), InputXGrad, Integratedgradients (IntGrad), Deeplift, and AttInGrad on the MDACE test set. Rand randomly generated attribution scores.Each experiment was run with ten different seeds. We show the mean of the seeds as the standard deviation.All the scores are presented as percentages. Bold numbers outperform the unsupervised baseline model, whileunderlined numbers outperform the supervised model.",
  "BUTMIGR": ": F1 scores and IOU of the unsupervised approaches on the bigger test set. Notice that there are no outliersfor the attention-based explanations produced by IGR, which explains its higher mean scores in . : Evaluation of perturbation-based feature attribution methods. We chose a random seed of BU and comparedall feature attribution methods for that model. We have divided the feature attribution into attention-based, gradient-based, and perturbation-based. The highest values are bold, and the second highest are underlined."
}