{
  "Abstract": "In this paper, we study how open-source largelanguage models (LLMs) can be effectivelydeployed for improving query rewriting inconversational search, especially for ambigu-ous queries. We introduce CHIQ, a two-stepmethod that leverages the capabilities of LLMsto resolve ambiguities in the conversation his-tory before query rewriting. This approachcontrasts with prior studies that predominantlyuse closed-source LLMs to directly generatesearch queries from conversation history. Wedemonstrate on five well-established bench-marks that CHIQ leads to state-of-the-art re-sults across most settings, showing highly com-petitive performances with systems leveragingclosed-source LLMs. Our study provides a firststep towards leveraging open-source LLMs inconversational search, as a competitive alter-native to the prevailing reliance on commer-cial LLMs for query rewriting. Our code ispublicly available at",
  "Introduction": "Conversational search enables users to interact withthe system in a multi-turn fashion to satisfy theircomplex information needs (Gao et al., 2022; Za-mani et al., 2023). One of the crucial steps is tocompose adequate search queries for each context-dependent utterance. Recent advancements in thetask-solving capabilities of Large Language Mod-els (LLMs) (Ouyang et al., 2022; Chen et al.,2024b; Wang et al., 2024a; Huang et al., 2024)have motivated researchers to integrate these mod-els into existing conversational search systems.Most recent studies (Mao et al., 2023a; Ye et al., 2023) leverage LLMs to directly generate searchqueries based on the context of the conversationhistory. Although seemingly straightforward, thistechnique is shown to achieve higher effectivenessin query rewriting than fine-tuning a smaller lan-guage model, such as T5 (Raffel et al., 2020; Chung et al., 2022). However, these performance gainsare primarily achieved through the use of commer-cial, closed-source LLMs (OpenAI, 2023). Thisis primarily because closed-source LLMs can bet-ter perform complex reasoning tasks (Gudibandeet al., 2023; Kaddour et al., 2023) compared toopen-source models.One of the main challenges in conversationalsearch resides in the ambiguous nature of the con-versation history. illustrates an exam-ple where solving co-reference relation in u4 andelaborating the response in r3 can help generatean adequate search query. Intuitively, performingthese tasks requires basic NLP task-solving capa-bilities, which even small-scale open-source LLMs(e.g., 7B) possess (Touvron et al., 2023; Jiang et al.,2023). The key challenge is to unlock the full ca-pabilities of open-source LLMs for conversationalsearch. This requires carefully preparing the con-versation history to enhance its quality, rather thandirectly using it to generate the search query.In this paper, we propose CHIQ, a method thataims to enhance the quality of contextual historyfor improving query rewriting. As illustrated in Fig-ure 1, we leverage the NLP capabilities of LLMs(e.g. solving coreference relation or expanding thecontext) to make the conversational history lessambiguous, consequently enhancing the relevanceof the generated search query. We investigate vari-ous methods for integrating refined conversationalhistory into existing frameworks, including ad-hocquery rewriting, generating pseudo supervision sig-nals for fine-tuning query rewriting models, andthe fusion of both approaches.We conduct extensive experiments using theopen-source LLM, LLaMA-2-7B (Touvron et al.,2023), across five well-established conversationalsearch benchmarks under both dense and sparseretrieval settings. The experimental results indi-cate that enhancing the conversational history usingour method achieves state-of-the-art performance Original Conversation History: Who did George Harrisonwrote the song \"Something\"for?Pattie Boyd. Who was she?She is an English model andphotographer. Who sang the abovementioned song?Joe Cocker. Which album is it a part of? Gold Positive Passage: \"Something\" is a song by band the Beatles from their 1969 album \"Abbey Road\". It was written by George Harrison, the band's leadguitarist [] as a composer to the level of the Beatles' principal songwriters, John Lennon and Paul McCartney. Did George Harrison write \"Something\" as a song specifically for whom?George Harrison wrote the song \"Something\" for his wife Pattie Boyd []Which woman was George Harrison writing the song \"Something\"specifically?George Harrison wrote the song \"Something\" for his wife Pattie Boyd []Who sang \"Something\" originally, written by George Harrison for Pattie Boyd?George Harrison wrote the song \"Something\" [] it was covered by JoeCocker []Which album does \"Something\" by George Harrison, sung by Joe Cockerfrom?The song \"Something\" written by [] for [] on the album \"Abbey Road\" byThe Beatles. Enhanced Conversation History:Search Query: QR on original history:In which album thesong composed forPattie Boyd? CHIC-FT:George Harrison wrote'Something' for whichalbum? CHIC-AD:Which album doesSomething by GeorgeHarris [] is a part ofthe album AbbeyRoad by The Beatles[] covered by JoeCocker. : An illustrative example of a conversational history (left box) and the gold positive passage relevant to thelast user turn. The enhanced history obtained using our method described in 3.2 is in the middle box. The right boxshows the three search queries generated by LLaMA-2-7B conditioned on the original history, and our CHIQ-FTand CHIQ-AD methods described in 3.4 and 3.3, respectively. Underlined terms in the gold passages are thosethat appear in the query generated by our approaches, which is conditioned on the enhanced history and did notappear in the query generated by the method that uses the original history. across most settings, often surpassing systems pow-ered by closed-source LLMs. Our analysis revealsthat although closed-source LLMs benefit fromenhancing the history, the gap with open-sourcemodels is narrower when using the enhanced his-tory with different facets compared to the original.Our contributions are summarized as follows:",
  "We propose a two-step method for queryrewriting that relies on open-source languagemodels: enhancing the conversation historyand then generating the search query": "We introduce three approaches for generat-ing the search query on top of the enhancedconversation history: ad-hoc query rewriting(CHIQ-AD), fine-tuning a small LM for thetask (CHIQ-FT), and a fusion of both ap-proaches (CHIQ-Fusion). Experiments conducted on five conversationalsearch benchmarks demonstrate that CHIQ,using open-source LLMs, achieves state-of-the-art performance across most settings, of-ten surpassing systems that rely on closed-source LLMs.",
  "Different from traditional ad-hoc retrieval, whichassumes users submit a stand-alone query, conver-sational search provides a conversational interface": "so that users can elaborate more complex search re-quirements, and interactively perform search. Themain challenge lies in accurately understandingthe users real search intent, which may be em-bedded within a longer, noisy, and more complexconversational context history. There are two well-established approaches in the literature for conver-sational search: Conversational Dense Retrieval(CDR) (Qu et al., 2020; Yu et al., 2021; Mao et al.,2024; Mo et al., 2024a) and Conversational QueryRewriting (CQR) (Elgohary et al., 2019). The CDR systems aim to fine-tune an end-to-end conversational dense retriever that can directlymodel the entire conversational history to returnrelevant documents (Kim and Kim, 2022; Mo et al.,2024b). Conversely, the CQR systems focus onformulating an adequate search query based on theconversational history. This query can then serve asthe input to an existing, well-established retriever-ranker framework. We base our solution on CQR,leveraging its ability to integrate with existing ad-hoc search models, which has demonstrated signif-icant practical value (Dalton et al., 2022). Earlier approaches to CQR attempted to selectuseful tokens from the conversation context (Ku-mar and Callan, 2020; Voskarides et al., 2020; Fanget al., 2022) or to train generative rewriter modelswith conversational sessions to mimic the human-rewritten query (Yu et al., 2020; Lin et al., 2020;Vakulenko et al., 2021). To optimize query rewrit- ing, some studies have adopted reinforcement learn-ing (Wu et al., 2022; Chen et al., 2022), or usedthe ranking signals with the rewriting model train-ing (Qian and Dou, 2022; Mo et al., 2023a; Maoet al., 2023b). In addition, there have been en-deavors to improve the conversion history qual-ity through context denoising (Lin et al., 2021b;Mao et al., 2022a; Krasakis et al., 2022; Mo et al.,2023b; Mao et al., 2023c), and data augmenta-tion (Dai et al., 2022; Mao et al., 2022b; Mo et al.,2024c). Unlike them, we enhance query rewritingby leveraging the NLP capabilities of open-sourceLLMs to reduce the ambiguity of conversationalhistory. There have been multiple endeavors tointegrate LLMs to solve traditional ad-hoc searchsub-tasks (Zhu et al., 2023), such as query expan-sion (Wang et al., 2023; Gao et al., 2023), denseretrieval (Ma et al., 2023; Wang et al., 2024b), andre-ranking (Sun et al., 2023). About conversationalsearch, Jin et al. (2023), Jang et al. (2023), andChen et al. (2024a) attempt to improve CDR withunsupervised fine-tuning. Mao et al. (2023a), Moet al. (2024d), and Ye et al. (2023) explore howLLMs can understand users contextualized searchintents via CQR. Unlike direct rewriting the queryusing LLMs, we investigate various approaches forintegrating refined conversational history into CQRframeworks.",
  "Task Formulation": "Let H = {uk, rk}nk=1 represent the user-systemconversational history, where uk and rk are theuser question and the system response at the k-thturn. Given a new user question un+1, the goal ofa conversational search system is to return a setof passages Pn+1 that are relevant to H and un+1,which would eventually help generate the modelresponse rn+1. To solve the main challenge of un-covering the real search intents hidden in the userscontext-dependent query, a conversational queryrewriting (CQR) module has been commonly em-ployed as an intermediate step to obtain a rewrittenquery qn+1, which in turn is used as input to an off-the-shelf retriever. Recently, LLMs have becomethe default option for obtaining qn+1 as follows:",
  "where denotes concatenation and ICQR is amanually-engineered instruction prompt describing": "the CQR task. The choice of LLM(.) has predom-inantly favored closed-source commercial models,mainly CHATGPT, some of which require multi-ple iterations to get the optimal query (Mao et al.,2023a; Ye et al., 2023). In this work, we leveragethe basic NLP capabilities of open-source LLMs togenerate H, a clearer and less noisy version of H.This refined version can then serve as a substitutefor H in Eq. 1, potentially improving the quality ofqn+1 using an open source LLM.",
  "History Enhancement": "In this section, we propose five approaches to tacklethe ambiguity problems inherent in conversationalhistory H and map each of them to a fundamen-tal NLP task ability. Then, we explain how wedesign prompts for an LLM to make part or theentire history clearer. The exact prompts we used,along with illustrative examples for each case, arepresented in Appendix A.",
  "Question Disambiguation": "Users often expect a human-to-human level of inter-action with modern conversational systems. Theyoften use acronyms, ambiguous words, or corefer-ence substitutes when asking questions, expectingnative understanding and default reasoning capa-bilities from these systems. For a search system,the search intent is often unclear and ambiguous.Therefore, we propose a prompt to an LLM, de-noted as IQD, that takes the conversational historyH and the subsequent user question un+1 as input,to generate un+1, a self-contained and unambigu-ous version of un+1 which can substitute it in Eq. 1.",
  "Response Expansion": "In conversation sessions, it is common for modelresponses to be short and concise, especially forthe factoid query. While brevity is often conve-nient for the user to acquire needed information, itmakes the response less informative for a searchsystem, which requires abundant rewrite/expansionresources. To handle this issue, we design a promptIRE which instructs an LLM to enrich the contentof the last model response. The goal is to makeit self-contained by leveraging the preceding con-versational history. The enhanced history H isobtained by replacing the original response by rn.",
  "Given that LLMs have been demonstrated to encap-sulate human knowledge, one could employ them": "to speculate on potential responses directly. Theintuition is that, even if the response includes somenoise, it may still contain relevant terms, particu-larly when the LLM is prompted to produce a self-contained answer. Therefore, we design a promptIPR that takes the conversational history H andun+1 as inputs to generate a pseudo-response rn+1.The latter can be used to expand the input of Eq. 1to improve the quality of the query generation.",
  "Topic Switch": "It is natural in a conversation that different turnsmay focus on different aspects. Some of them arerelevant to the current turn, while others may not.This is especially the case when conversations arelong. In such cases, using the full history is highlylikely to distract the CQR module, leading to poorquery generation. Therefore, we design a promptITS that instructs the LLM to determine whethera topic switch happens between un+1 and H. Ifa switch is identified, the enhanced history wouldonly include the last turn to maintain the transitionas H ={un, rn}. The other turns in H are deemedto be irrelevant for generating qn+1 and ignored.",
  "History Summary": "As the conversation goes on, the historical contextbecomes longer, which contains more irrelevan-t/noisy parts. A summary of history context isexpected to contain only the most useful informa-tion of the original long context and better servefor query expansion. Thus, we propose the promptIHS, which takes a history context (original oreven enhanced) and generates a summary of theconversation H.",
  "Ad-hoc Query Rewriting": "A straightforward method to obtain the enhancedrewritten query qn+1 is to independently utilize theoutputs of the five methods described previously.However, by complementing each other, the out-puts of these methods can collectively contributeto a more enhanced conversational history, therebysignificantly improving the retrieval performanceby generating a better query. We intuitively definemultiple combinatory configurations for updatingthe input in Eq. 1, which are denoted by differentsymbols. The addition of +QD or +RE indicatesthat we replace un+1 and rn with un+1 un+1( 3.2.1) and rn ( 3.2.2) in H, respectively. +PRsignifies that rn+1 ( 3.2.3) is concatenated to theinput of Eq. 1; Lastly, +TS indicates that H should omit the previous turns except the last one if a topicswitch is detected. Lastly, +HS means that H isoverwritten by the entire H obtained in 3.2.5. Inour default configuration, we first check for topic-switch (TS). If the result is affirmative, we onlyuse the QD+RE+PR configuration on the top ofthe truncated history in 3.2.4. Otherwise, weapply QD+RE+PR on top of the original history,followed by +HS which consists of obtaining thehistory summary on top of the enhanced history.We report the results of models using this configu-ration in the remaining sections.",
  "Search-Oriented Fine-tuning": "In existing studies, fine-tuning conversational querygenerators based on a small-scale language model,such as T5-base, have proven to be both effec-tive and efficient (Lin et al., 2020). These mod-els consist of using human-rewritten query (Wuet al., 2022) or LLM-generated query (Jang et al.,2023) to serve as supervision signals and take Hand un+1 as input. However, they do not take theranking signals into account during the training andthe supervision signals might be sub-optimal (Linet al., 2021b; Mo et al., 2023a).Considering that oracle search queries are typ-ically unavailable and costly to annotate, we pro-pose extending the existing approach to generatepseudo-supervision signals for query generation byleveraging the outputs produced in 3.2. Moreprecisely, we propose three modifications to Eq. 1to obtain a search-oriented qn+1 as follow:",
  "qn+1 arg maxqS(Qn+1, pn+1),q Qn+1 (3)": "Then, we select qn+1, the one with the highestretrieval score S determined by an off-the-shelfretriever and relevance judgment from the set ofpseudo-queries Qn+1 as Eq 3, which is used as thesupervision signal to fine-tune a rewriting modelM(H un+1) = qn+1 by maximum likelihoodestimation. It is important to mention that the pro-cess described in this section is conducted offline and performed once, for the purpose of generat-ing pseudo-labeled queries to fine-tune a search-oriented query rewriter. During inference, H andun+1 serve as inputs for the fine-tuned model togenerate the query qn+1, and no calls are made tothe LLM, so that the latency is not much affected.",
  "Datasets and Evaluation Metrics": "To be comparable with state-of-the-art systems (Moet al., 2023a; Mao et al., 2023a), we consider twostandard benchmarks for conversational search:TopiOCQA (Adlakha et al., 2022), QReCC (Anan-tha et al., 2021). TopiOCQA focuses on the chal-lenge of the topic switch under the conversationalsetting, while QReCC focuses on the query rewrit-ing problem. We run experiments on the officialtrain-test splits and report MRR, NDCG@3, andRecall@10 to evaluate the passage retrieval resultsas in previous works. In addition, we evaluate threeCAsT datasets (Dalton et al., 2020, 2021, 2022)which are used solely as test sets, to further vali-date the zero-shot or transfer learning ability of ourapproach, e.g., when CQR models are trained onTopiOCQA and tested on CAsTs.",
  "CHIQ-Fusion We fuse the rank list retrievedby CHIQ-AD and CHIQ-FT using the result-level fusion technique (Lin et al., 2021b).1": "We compared our methods with a variety ofsystems that can mainly be classified into threecategories.More precisely, we first compareagainst traditional systems that fine-tune small-scale CQR models (e.g., T5-base) including:QuReTeC (Voskarides et al., 2020) T5QR (Lin et al.,2020), CONQRR (Wu et al., 2022), ConvGQR (Moet al., 2023a), EDIRCS (Mao et al., 2023b). Then,",
  "It consists of aggregating multiple ranked lists retrievedby each query into a single list to produce Pn+1": "we compare with the systems that fine-tune anLLM-based retriever, e.g., create the query anddocument representation by the ending token froma decoder-only model, including RepLLaMA (Maet al., 2023), E5-Mistral (Wang et al., 2024b), andLLM-Embedder (Zhang et al., 2023), or fine-tunean LLM-based CQR model as RETPO (Yoon et al.,2024) and IterCQR (Jang et al., 2023). Besides, weinclude the systems that directly obtain the rewrit-ten query by prompting LLMs such as LLM-AidedIQR (Ye et al., 2023), HyDE (Gao et al., 2023),Query2doc (Wang et al., 2023) and LLM4CS (Maoet al., 2023a). Although not directly comparable,we report results of systems that fine-tune an ad-hoc search retriever for conversational scenarios,including the one without LLMs ConvDR (Yu et al.,2021) and with LLMs InstructorR (Jin et al.,2023). A detailed description of each aforemen-tioned baseline is presented in Appendix B.3.",
  "Implementation Details": "We conduct experiments with the instruct-tuningvariants2 of both LLaMA-2-7B (Touvron et al.,2023) and Mistral-2-7B (Jiang et al., 2023) asLLM(.) in Eq. 1 and Eq. 2. We experiment withboth BM25 (Robertson et al., 2009) sparse retrieverand ANCE dense retriever (Xiong et al., 2020). Inaddition, we use FlanT5-base3 (Chung et al., 2022)and large models as the backbone when fine-tuninga CQR model on TopiOCQA and QReCC. Thefine-tuning process consists of 10 epochs with alearning rate of 1e-5 and a batch size of 8 for bothdatasets. More implementation details can be foundin Appendix B.2.",
  "Main Results": "shows both the dense and sparse retrievalperformances of systems with diverse properties onthe TopiOCQA and QReCC. We report the resultsof our systems using LLaMA-2-7B as the back-bone LLM to make the results comparable withprevious work. First, we observe that using our en-hanced conversation history significantly improvesperformance over vanilla baselines that use theoriginal history, for both ad-hoc QR (LLM4CS) andfine-tuning a small QR model (T5QR). For dense 2Concretely, the version of LLMs we used are meta-llama/Llama-2-7b-chat-hf and mistralai/Mistral-7B-Instruct-v0.2 on respectively.3We report our main results using FlanT5-base to ensurethe results are comparable with previous studies.",
  "CHIQ-Fusion25.623.544.754.351.978.5": ": Performance of dense and sparse retrieval on TopiOCQA and QReCC with different systems. We listthe attributes of the reported baseline systems, which include: DR based on conversational dense retrieval, QRperform query rewriting, CS leverage close-source LLMs (e.g., ChatGPT or GPT-4), OS leverage open-sourceLLMs (mainly LLaMA-2-7B), FT fine-tune a small LM (mainly T5-base) for QR, and QF fuse multiple queries forretrieval. RETPO involves high-cost supervised fine-tuning an LLM for QR. denotes significant improvementswith t-test at p < 0.05 over all compared baselines (except CONQRR, RETPO, and IterCQR). Bold and underlineindicate the best and the second-best results within the categories of dense and sparse retrieval.",
  "CHIQ-FTLLaMA-2-7B68.545.111.946.331.615.953.936.020.4CHIQ-ADLLaMA-2-7B70.847.611.951.034.417.957.742.022.6CHIQ-FusionLLaMA-2-7B73.350.512.954.038.019.362.946.525.2": ": Zero-shot retrieval performances of the systems involved with LLMs under the dense retrieval (ANCE). denotes significant improvements with t-test at p < 0.05 over all compared baselines. Bold and underline indicatethe best and the second-best results, respectively. retrieval, CHIQ-AD outperforms LLM4CS by 5.5%and 2.2% MRR on TopiOCQA and QReCC re-spectively, while CHIQ-FT reports a gain of 7.0%and 1.9% over T5QR on the same datasets. Similargains are also observed using the sparse retriever,indicating the strong effectiveness of our methods.Second, we notice that vanilla QR systems ontop of an enhanced history can outperform systemsthat utilize additional training techniques and so-phisticated modules. For instance, CHIQ-AD outper-forms both ConvDR and IntructorR, which needrelevance judgments to fine-tune a conversationaldense retriever on the raw input; ReTPO, which fine-tunes an LLM for QR and in addition leveragesGPT-4 for data augmentation. While CHIQ-FT out-performs its direct competitors, primarily ConvGQRand IterCQR that refine the supervision signals onTopiOCQA, it underperforms on QReCC mainlybecause previous fine-tuned QR models rely onQReCCs human-rewritten queries. The contrast-ing observations between the two datasets suggestthat enhancing the history is crucial for perfor-mance when no QR-supervised annotations exist.Third, we observe that by systematically fus-ing the outputs of our approaches, CHIQ-Fusionoutperforms the models that use each componentseparately, achieving the best performance acrossmost settings. The gains are more significant onthe topic-mixed and more challenging TopiOCQAcompared to QReCC, with 4.8% and 0.2% MRRscore improvements on each dataset. Interestingly,this occurs even though CHIQ-FT significantly un-derperforms compared to CHIQ-AD, suggesting thatCHIQ-FT still generates query content that is com-plementary and not captured by CHIQ-AD.",
  "Zero-shot Results": "We compare the dense retrieval performances ofdifferent systems that leverage LLMs under azero-shot manner on three CAsT datasets in Ta-ble 2. We observe a consistent pattern as previ-ous results in when comparing the perfor-mances within our approaches. More precisely,although CHIQ-FT performs slightly worse com-pared to CHIQ-AD, fusing their outputs systemati-cally leads to better performances across all threedatasets. Besides, we can see that CHIQ-AD outper-forms most systems either utilizing open-sourceor close-source LLMs and yields results com-petitive with the state-of-the-art system LLM4CS,which requires multiple calling for each query turn. Specifically, CHIQ-AD surpasses LLM4CS withLLaMA-2-7B, on CAsT-19 and CAsT-21. In addi-tion, our top-performing approach, CHIQ-Fusion,outperforms all compared systems, except theLLM4CS with close-source ChatGPT-3.5 on CAsT-20 and CAsT-21, indicating the superior effective-ness of our approaches.We also find that ad-hoc fine-tuned LLM-based retrievers (RepLLaMA,E5-Mistral, and LLM-Embedder) underperformthe systems with LLM-based query generation(HyDE and Query2doc) and the InstructorR withconversational fine-tuning adaption. These systemsalso underperform CHIQ-FT, which only fine-tunesa small LM on TopiOCQA with enhanced supervi-sion signals. The observation indicates the impor-tance of improving the generalization capabilitiesof the models to handle complex and diverse con-versational scenarios.",
  "Open vs. Close Source LLMs": "In addition to conducting experiments with open-source LLMs, we also deploy the closed-sourceLLM ChatGPT-3.5 to isolate the effects of historyenhancement. shows the dense retrievalperformances on three CAsT test sets when queryrewriting (QR) is performed by CHIQ-AD approachon both the original history and the enhanced one.We observe that ChatGPT-3.5 benefits from QR onenhanced history, with NDCG@3 score improve-ments of 5.4%, 4.3%, and 0.9% across CAsT-19,CAsT-20, and CAsT-21, respectively. Such resultsindicate that despite the superior reasoning abili-ties of closed-source LLM, enhancing the conver-sational history is deemed important for handlingcomplex queries within conversational scenarios.",
  "Also, we find that conducting QR on enhancedconversational history helps to narrow the perfor-mance gap between open-source and closed-source": "LLMs. For instance, the gap of MRR score be-tween LLaMA and ChatGPT-3.5 on the original his-tory is 1.9%, 12.1%, and 7.6% across three CAsTtest sets, respectively. In contrast, when utilizingenhanced history, the gaps reduced significantly to0.9%, 4.7%, and 4.5%, indicating that our designedapproach can adequately leverage the capacity ofopen-source LLMs for conversational search andbe competitive with close-source ones.",
  "Search-Oriented Fine-tuning Ablation": "We analyze the potential choices for generatingsearch queries in Eq. 2 and Eq. 3 as supervisionsignals for CHIQ-FT models. presents thedense retrieval performances via the queries gen-erated by CHIQ-FT models, which are fine-tunedusing manually rewritten queries or the variantsof the approach outlined in 3.4. The ablationsare based on the results of either without using en-hanced history, without generating multiple queries,or not conditioning on the gold passage. We ob-serve that using the queries generated by LLMs assupervision signals outperform the one using man-ual annotation, which is consistent with previousstudies that have identified human-written queriesas sub-optimal (Wu et al., 2022; Mo et al., 2023a).",
  "CHIQ-FT30.028.936.934.0w.o. H27.626.735.433.8w.o. Qn+124.223.433.431.7w.o. pn+118.217.226.823.9": ": Dense retrieval performances of fine-tunedQR models, which utilize different supervised signals.These include human-written queries and variants fromCHIQ-FT, either without enhanced history, without mul-tiple queries, or not conditioned on the gold passage. Additionally, we observe that all of our proposedadaptive modifications significantly enhance the fi-nal performance of the system, especially applyingthem all to generate the pseudo supervision sig-nals. Such results indicate that improving the qual-ity of supervision signals is crucial for QR modelfine-tuning and justifying the effectiveness of ourapproaches for search-oriented fine-tuning.",
  ": Dense and sparse retrieval results of ablatingCHIQ-AD by not using one history enhancement promptat each line on TopiOCQA and QReCC datasets": "We observe that all our proposed enhancementsto the history context contribute positively to theperformance of the CHIQ-AD method, althoughsome enhancements are more effective than oth-ers. On one hand, detecting topic switching isparticularly crucial on TopiOCQA, leading to per-formance improvements of 13.2% and 5.2% MRRscores in dense and sparse retrieval, respectively.This is mainly due to the multi-topic focus designof the dataset within the same conversation. Onthe other hand, we notice that while question dis-ambiguation (QD) improves performance, it is lesscritical compared to predicting a pseudo response(PR) or enhancing the quality of the last systemresponse (RE). In addition, we notice that all ourproposed enhancements contribute similarly to thegenerated search queries across both dense andsparse retrieval settings.",
  "Case Analysis": "We manually analyze the content of the enhancedhistory to better understand the mechanisms andlimitations of our approach. This analysis showsthe complementary roles each enhancement promptplays in improving the quality of the original his-tory. QD and RE primarily assist in resolving coref-erences and clarifying acronyms to full names, TShelps remove irrelevant content, PR speculates onrelevant terms that may occur in the response, andHS not only converts the conversation into plaintext but also ensures that key terms from the con-versation are preserved. While prompts such as PR and RE generation generally aid in retrieval,they may also introduce noisy terms due to thewrong fact generated by LLMs that hurt the rank-ing results. Finally, we also notice that the queriesgenerated by CHIQ-AD and CHIQ-FT are of differentstyles. The first focuses on expanding more rele-vant terms to increase the matching scores, whilethe latter queries are more concise with higher ef-ficiency for retrieval. Nevertheless, aggregatingthe output rank lists from both approaches helpsrefine the final results by ranking the relevant pas-sages higher. The concrete examples of these caseanalyses are presented in Appendix D.",
  "Conclusion": "In this paper, we propose CHIQ, an approach thatleverages the basic NLP capabilities of LLMs toenhance the quality of contextual history for im-proving the query rewriting performance in termsof conversational search. Despite its simplicity, ourapproach achieves superior performance across var-ious datasets and settings, using open-source LLMscompared to closed-source alternatives. This studyshows that instead of simply ask an LLM to gener-ate a search query, it is critical to design strategiesto generate different facets of enhancement in viewof finding the target information.",
  "Limitations": "Potential limitations of this work include not ex-perimenting with larger open-source LLMs, suchas the 56B Mixtral (Mixtral AI team, 2023) or70B LLaM a, as well as other recent models likeGemma (Team et al., 2024). Additionally, the studydid not incorporate more closed-source modelssuch as GPT-4 (OpenAI, 2023) or Claude (Anthrop-icAI, 2023) to further study the impact of historyenhancement. This is mainly due to limitationsin computation (open source) and financial (closesource) resources. Despite the straightforward andsignificant gains, some design choices could befurther analyzed to potentially boost the perfor-mance even more. For instance, adding a backofffiltering strategy could detect when the LLM is pro-ducing noisy outputs, or exploring approaches thatinterpolate between the use of human and pseudo-queries when its higher quality as training signalsfor CHIQ-FT. Besides, we have considered 5 direc-tions of enhancement in this paper. More strategiescan be incorporated so that other useful enhance-ments can be integrated. Vaibhav Adlakha, Shehzaad Dhuliawala, Kaheer Sule-man, Harm de Vries, and Siva Reddy. 2022. Topi-ocqa: Open-domain conversational question answer-ing with topic switching. Transactions of the Associ-ation for Computational Linguistics, 10:468483. Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu,Shayne Longpre, Stephen Pulman, and SrinivasChappidi. 2021. Open-domain question answeringgoes conversational via question rewriting. In Pro-ceedings of the 2021 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages520534.",
  "Zhiyu Chen, Jie Zhao, Anjie Fang, Besnik Fetahu,Rokhlenko Oleg, and Shervin Malmasi. 2022. Rein-forced question rewriting for conversational questionanswering": "Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, et al.2022. Scaling instruction-finetuned language models.arXiv preprint arXiv:2210.11416. Zhuyun Dai, Arun Tejasvi Chaganty, Vincent Y Zhao,Aida Amini, Qazi Mamunur Rashid, Mike Green,and Kelvin Guu. 2022. Dialog inpainting: Turningdocuments into dialogs. In International Conferenceon Machine Learning, pages 45584586. PMLR.",
  "Jeffrey Dalton, Chenyan Xiong, and Jamie Callan. 2022.Trec cast 2021: The conversational assistance trackoverview. In In Proceedings of TREC": "Ahmed Elgohary, Denis Peskov, and Jordan Boyd-Graber. 2019. Can you unpack that? learning torewrite questions-in-context. In Proceedings of the2019 Conference on Empirical Methods in Natu-ral Language Processing and the 9th InternationalJoint Conference on Natural Language Processing(EMNLP-IJCNLP), pages 59185924. Hung-Chieh Fang, Kuo-Han Hung, Chen-Wei Huang,and Yun-Nung Chen. 2022. Open-domain conver-sational question answering with historical answers.In Findings of the Association for ComputationalLinguistics: AACL-IJCNLP 2022, pages 319326.",
  "Arnav Gudibande, Eric Wallace, Charlie Snell, XinyangGeng, Hao Liu, Pieter Abbeel, Sergey Levine, andDawn Song. 2023. The false promise of imitatingproprietary llms. arXiv preprint arXiv:2305.15717": "Kaiyu Huang, Fengran Mo, Hongliang Li, You Li,Yuanchi Zhang, Weijian Yi, Yulong Mao, JinchenLiu, Yuzhuang Xu, Jinan Xu, et al. 2024. A sur-vey on large language models with multilingualism:Recent advances and new frontiers. arXiv preprintarXiv:2405.10936. Yunah Jang, Kang-il Lee, Hyunkyung Bae, Seung-pil Won, Hwanhee Lee, and Kyomin Jung. 2023.Itercqr: Iterative conversational query reformula-tion without human supervision.arXiv preprintarXiv:2311.09820. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode Las Casas, Florian Bressand, Gianna Lengyel,Guillaume Lample, Lucile Saulnier, Llio Re-nard Lavaud, Marie-Anne Lachaux, Pierre Stock,Teven Le Scao, Thibaut Lavril, Thomas Wang, Timo-the Lacroix, and William El Sayed. 2023. Mistral7b. CoRR, abs/2310.06825. Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, andJun Zhao. 2023. Instructor: Instructing unsupervisedconversational dense retrieval with large languagemodels. In Findings of the Association for Computa-tional Linguistics: EMNLP 2023, pages 66496675.",
  "Antonios Minas Krasakis, Andrew Yates, and Evan-gelos Kanoulas. 2022. Zero-shot query contextu-alization for conversational search. arXiv preprintarXiv:2204.10613": "Vaibhav Kumar and Jamie Callan. 2020. Making in-formation seeking easier: An improved pipeline forconversational search. In Findings of the Associationfor Computational Linguistics: EMNLP 2020, pages39713980. Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, and Rodrigo Nogueira.2021a. Pyserini: A python toolkit for reproducibleinformation retrieval research with sparse and denserepresentations. In Proceedings of the 44th Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval, pages 23562362. Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin.2021b. Contextualized query embeddings for con-versational search. In Proceedings of the 2021 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 10041015. Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira,Ming-Feng Tsai, Chuan-Ju Wang, and Jimmy Lin.2020.Conversational question reformulation viasequence-to-sequence architectures and pretrainedlanguage models. arXiv preprint arXiv:2004.01909.",
  "Kelong Mao, Zhicheng Dou, Haonan Chen, FengranMo, and Hongjin Qian. 2023a. Large language mod-els know your contextual search intent: A promptingframework for conversational search": "Kelong Mao, Zhicheng Dou, Bang Liu, Hongjin Qian,Fengran Mo, Xiangli Wu, Xiaohua Cheng, and ZhaoCao. 2023b. Search-oriented conversational queryediting. In Findings of the Association for Computa-tional Linguistics: ACL 2023, pages 41604172. Kelong Mao, Zhicheng Dou, and Hongjin Qian. 2022a.Curriculum contrastive context denoising for few-shot conversational dense retrieval. In Proceedingsof the 45th International ACM SIGIR Conference onResearch and Development in Information Retrieval,pages 176186. Kelong Mao, Zhicheng Dou, Hongjin Qian, FengranMo, Xiaohua Cheng, and Zhao Cao. 2022b. Con-vtrans: Transforming web search sessions for con-versational dense retrieval. In Proceedings of the2022 Conference on Empirical Methods in NaturalLanguage Processing, pages 29352946. Kelong Mao, Hongjin Qian, Fengran Mo, ZhichengDou, Bang Liu, Xiaohua Cheng, and Zhao Cao.2023c. Learning denoised and interpretable sessionrepresentation for conversational search. In Proceed-ings of the ACM Web Conference 2023, pages 31933202.",
  "Fengran Mo, Kelong Mao, Yutao Zhu, Yihong Wu,Kaiyu Huang, and Jian-Yun Nie. 2023a. Convgqr:Generative query reformulation for conversationalsearch. arXiv preprint arXiv:2305.15645": "Fengran Mo, Jian-Yun Nie, Kaiyu Huang, Kelong Mao,Yutao Zhu, Peng Li, and Yang Liu. 2023b. Learningto relate to previous turns in conversational search.In 29th ACM SIGKDD Conference On KnowledgeDiscover and Data Mining (SIGKDD). Fengran Mo, Chen Qu, Kelong Mao, Yihong Wu, ZhanSu, Kaiyu Huang, and Jian-Yun Nie. 2024a. Align-ing query representation with rewritten query andrelevance judgments in conversational search. arXivpreprint arXiv:2407.20189.",
  "Fengran Mo, Chen Qu, Kelong Mao, Tianyu Zhu, ZhanSu, Kaiyu Huang, and Jian-Yun Nie. 2024b. History-aware conversational dense retrieval. arXiv preprintarXiv:2401.16659": "Fengran Mo, Bole Yi, Kelong Mao, Chen Qu, KaiyuHuang, and Jian-Yun Nie. 2024c. Convsdg: Sessiondata generation for conversational search. In Com-panion Proceedings of the ACM on Web Conference2024, pages 16341642. Fengran Mo, Longxiang Zhao, Kaiyu Huang, Yue Dong,Degen Huang, and Jian-Yun Nie. 2024d. How toleverage personal textual knowledge for personalizedconversational information retrieval. arXiv preprintarXiv:2407.16192.",
  "OpenAI. 2023.Gpt-4 technical report.ArXiv,abs/2303.08774": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al.2022. Training language models to follow instruc-tions with human feedback.Advances in NeuralInformation Processing Systems, 35:2773027744. Hongjin Qian and Zhicheng Dou. 2022. Explicit queryrewriting for conversational dense retrieval. In Pro-ceedings of the 2022 Conference on Empirical Meth-ods in Natural Language Processing, pages 47254737.",
  "Stephen Robertson, Hugo Zaragoza, et al. 2009. Theprobabilistic relevance framework: Bm25 and be-yond. Foundations and Trends in Information Re-trieval, 3(4):333389": "Weiwei Sun, Lingyong Yan, Xinyu Ma, ShuaiqiangWang, Pengjie Ren, Zhumin Chen, Dawei Yin, andZhaochun Ren. 2023. Is chatgpt good at search?investigating large language models as re-rankingagents. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing,pages 1491814937. Gemma Team, Thomas Mesnard, Cassidy Hardin,Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,Laurent Sifre, Morgane Rivire, Mihir Sanjay Kale,Juliette Love, et al. 2024. Gemma: Open modelsbased on gemini research and technology.arXivpreprint arXiv:2403.08295. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurlien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288. Svitlana Vakulenko, Shayne Longpre, Zhucheng Tu,and Raviteja Anantha. 2021. Question rewriting forconversational question answering. In Proceedingsof the 14th ACM International Conference on WebSearch and Data Mining, pages 355363.",
  "Zeqiu Wu, Yi Luan, Hannah Rashkin, David Reitter,and Gaurav Singh Tomar. 2022. Conqrr: Conversa-tional query rewriting for retrieval with reinforcementlearning": "Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,Jialin Liu, Paul N Bennett, Junaid Ahmed, andArnold Overwijk. 2020. Approximate nearest neigh-bor negative contrastive learning for dense text re-trieval. In International Conference on LearningRepresentations. Fanghua Ye, Meng Fang, Shenghui Li, and Emine Yil-maz. 2023. Enhancing conversational search: Largelanguage model-aided informative query rewriting.In Findings of the Association for ComputationalLinguistics: EMNLP 2023, pages 59856006. Chanwoong Yoon, Gangwoo Kim, Byeongguk Jeon,Sungdong Kim, Yohan Jo, and Jaewoo Kang. 2024.Ask optimal questions: Aligning large languagemodels with retrievers preference in conversationalsearch. arXiv preprint arXiv:2402.11827. Shi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong, PaulBennett, Jianfeng Gao, and Zhiyuan Liu. 2020. Few-shot generative conversational query rewriting. InProceedings of the 43rd International ACM SIGIRconference on research and development in Informa-tion Retrieval, pages 19331936. Shi Yu, Zhenghao Liu, Chenyan Xiong, Tao Feng, andZhiyuan Liu. 2021. Few-shot conversational denseretrieval. In Proceedings of the 44th InternationalACM SIGIR Conference on Research and Develop-ment in Information Retrieval, pages 829838.",
  "AHistory Enhancement and QueryRewriting Prompts": "In this section, we list the prompts that we havecarefully designed to enhance different parts of theconversation history, as well as the prompt usedfor query rewriting and pseudo supervision signalsfor search-originated fine-tuning. For each prompt,we designed the instruction part through trial anderror iterations until we confirmed that both mod-els (LLaMA-2-7B and Mistral-v0.2-7B) could fol-low the instructions and generate outputs in therequired format. We observed no benefits fromusing in-context examples for any model, as theoutputs remained mostly stable, with minor to nochanges in the model responses even after addingthese examples.",
  ": Statistics of conversational search datasets": "The statistics of each dataset are presented in. We discard the samples without gold pas-sages. The manually rewritten query for each turnis provided in all datasets except TopiOCQA. Therelevance judgments in the CAsT datasets are madeby experts with multi-level annotations. The rele-vance judgment thresholds are set at 1, 2, and 2 forCAsT-19, CAsT-20, and CAsT-21, respectively.",
  "B.2Implementation Details": "We implement the retrieval evaluation metrics fromthe pytrec_eval tool (Van Gysel and de Rijke,2018). We leverage the Pyserini (Lin et al., 2021a)and Faiss (Johnson et al., 2019) libraries for im-plementing the BM25 and ANCE retrievers, re-spectively. Following previous works (Lin et al.,2021b; Mo et al., 2023a), we set BM25 parametersas follows: k1 = 0.9, b = 0.4 on TopiOCQA andk1 = 0.82, b = 0.68 on the QReCC. The lengthsof the query, concatenated input, and passage aretruncated to 32, 512, and 384 tokens, respectively.In all experiments, we use sampling and set thetemperature to 0.7 when generating with LLMs. For the search-oriented fine-tuning, we use theNDCG@3 score as the standard metric to select thegenerated query as the pseudo supervision signals,while we set the maximum length for the gener-ated query is set to 32, which is the same as (Linet al., 2020; Wu et al., 2022; Mo et al., 2023a). Forthe rank-list fusion, we set the balance factor inLin et al. (2021b) as 1, which indicates the sameimportance of different retrieved results.",
  "B.3Baselines Details": "We provide a more detailed introduction to the fol-lowing baselines used for comparison:QuReTeC (Voskarides et al., 2020): A tradi-tional sequence tagger query rewriting approachfine-tuned with weakly supervision signals to deter-mine whether a term in a historical context shouldbe expanded to the current query.T5QR (Lin et al., 2020): A query rewritingapproach fine-tuned with manual annotations pro-vided in QReCC as the supervised signals via theT5-base model.CONQRR (Wu et al., 2022): A query rewrit-ing approach fine-tuned with manual annotationsprovided in QReCC and the ranking signals usingreinforcement learning via the T5-base model.ConvGQR (Mo et al., 2023a): A unified frame-work that integrates query rewriting and queryexpansion mechanisms by two T5-base modelsand fine-tuned them with manual annotations andground-truth response, respectively.EDIRCS (Mao et al., 2023b): A query rewrit-ing approach based on text editing technique withranking signals fine-tuned on the T5-base model.IterCQR (Jang et al., 2023): An iterative queryrewriting method using the initial query generatedby the ChatGPT-3.5 and refining a query turn multi-ple times according to the ranking signals feedbackduring the training stage.LLM-Aided (Ye et al., 2023): An informa-tive conversational query rewriting by directlyprompting ChatGPT-3.5 as both query rewritersand rewrite editors twice to incorporate all the de-sirable properties for producing the final rewrittenqueries.RETPO (Yoon et al., 2024): A retriever pref-erence adapted query rewriting method that fine-tunes LLaMA-2-7B as a QR model with an externalQR dataset generated by GPT-4.ConvDR (Yu et al., 2021):A traditionalconversational dense retrieval method that uses knowledge distillation to learn the session embed-dings with relevance judgments from the human-rewritten queries based on the ANCE model.InstructorR (Jin et al., 2023): A LLM-basedgeneral conversational dense retriever tailored tovarious tasks and domains by fine-tuned with var-ious task-specific instructions and relevance judg-ments based on FlanT5-XL model.RepLLaMA (Ma et al., 2023): A large ad-hocdense retriever fine-tuned on top of the LLaMA-7Bmodel on the MSMARCO dataset.E5-Mistral (Wang et al., 2024b): A large ad-hocretriever fine-tuned on top of Mistral-7B model onthe synthetic dataset generated by ChatGPT-3.5and MSMARCO.LLM-Embedder (Zhang et al., 2023): A uni-fied retrieval model that can support diverse re-trieval augmentation needs of LLMs, which is fine-tuned on various tasks and datasets such as MS-MARCO, NQ, ToolLLM, QReCC, FLAN, Books3,and Multi-Session Chat.HyDE (Gao et al., 2023): A zero-shot retrievalmethod, which adopts ChatGPT-3.5 to generate hy-pothetical documents for the query, then retrievesreal documents with hypothetical documents.Query2doc (Wang et al., 2023): A zero-shotquery expansion approach, which expands the orig-inal query with the generated documents fromChatGPT-3.5.LLM4CS (Mao et al., 2023a): A state-of-the-artLLM-based prompting method for conversationalquery rewriting. We implement it with full aggre-gation by calling LLMs five times for the queryand response generation but without the chain-of-thought (CoT) content because of the efficient an-notation consideration in practical scenarios.",
  "CResults of Mistral-2-7B": "and show the performances ofour methods using Mistral-2-7B as the backboneLLM, replacing LLama-2-7B as previously shownin and , respectively. These re-sults exhibit trends similar to those observed withLLama-2-7B, as discussed in Sections 5.1 and 5.2.More specifically, CHIQ-AD consistently outper-forms CHIQ-FT across all settings, and combiningthe results of both methods (CHIQ-Fusion) yieldsthe best performance. It is worth noting that resultswith Mistral-2-7B are systematically lower than theones with LLama-2-7B on most datasets (except onCAsT-21). Besides, by comparing the fusion per-",
  ": Performance of dense retrieval on three CAsT datasets based on Mistral-2-7B model. The system propertiesand the settings are inherited from the": "formance between Mistrial and LLaMa, we noticethat in most cases when the gap between CHIQ-ADand CHIQ-FT is large, CHIQ-Fusion results are ei-ther slightly better or worse than CHIQ-AD. Thisis mainly because the poor quality of the rank listobtained by CHIQ-FT negatively impacts the onefrom CHIQ-AD. However, when the gap is smaller,we notice a significant gain for CHIQ-Fusion, sug-gesting that both variants are generating good andcomplementary rank lists. It will be interesting toinvestigate how we can better take advantage ofCHIQ-AD and CHIQ-FT in an adaptive fusion. Nev-ertheless, performing QR on top of the enhancedhistory with our approach still outperforms mostother settings and datasets.",
  "DCase Analysis": "and showcase two examples thatsupport the case study analysis conducted in 5.6.In the example of , QD and RE QD andRE contribute by adding the terms hormone intocortisol, glucagon, adrenaline, cytokines orexin,and melatonin to the enhanced history. In addi-tion, PR enriches the emotional context by includ-ing excitement, anxiety, or fear, which co-occursin the gold positive passage, thereby improving thescores. Since a topic switch (TS) is detected, earlierturns containing noisy terms, such as Adenosinetriphosphate (ATP), are dropped from the historysummary (HS). Therefore, performing QR with ourmethods on the enhanced history results in top pas- sages being ranked higher compared to those basedon the original history (LLM-QR). Similar trendsare observed in the second case shown in ,where CHIQ-FT and CHIQ-AD outperform QR withthe original history. This improvement may be at-tributed to the enhanced history increasing seman-tic similarity through references to titles of Tjadersartworks from PR and HS, such as Inaugurationof the Pleasure Dome. However, names of var-ious collaborators like Kenneth Anger and StanBrakhage may introduce noise in PR.",
  "u1: The primary high energy mole-QD: Which hormone among cortisol, glucagon, adrenaline, cytokines orexin,": "cule in human metabolism is?and melatonin is associated with an emotional response?TS: Yesr1: Adenosine Triphosphate (ATP).RE: These hormones, including cortisol, glucagon, adrenaline, cytokines, ore- u2: What is catabolism?xin, and melatonin, play various roles in regulating metabolic processes invo-r2: It is the set of metabolic processeslving the breakdown of larger molecules to produce ATP during catabolism.that breaks down large molecules.PR: The emotional responses to hormones can vary greatly among individuals.u3: Which hormones are related to it?Adrenaline (epinephrine) is often associated with excitement, anxiety, or fear. r3: Cortisol, Glucagon, Adrenaline,HS: Adenosine triphosphate (ATP) serves as the primary high energy moleculeCytokines, Orexin, and Melatonin.in human metabolism. Catabolism refers to the metabolic processes that breaku4: What is the emotional responsedown large molecules into smaller ones. Cortisol, glucagon, adrenaline, cyto-due to the third one?kines, orexin, and melatonin are associated hormones involved in this process.",
  "Rewritten Query q4Gold Positive Passage p4": "LLM-QR: Which album did Cal Tjader record withThe most obvious deviation from Tjaders Latin jazz sound wasJack Weeks and John Marabuto in California?(Rank:16)Several Shades of Jade and the follow-up Breeze From the East.CHIQ-AD: QD + PR + HS (Rank: 7)Both albums attempted to combine jazz and Asian music, much asCHIQ-FT: Which specific album title did Cal TjaderTjader and others had done with Afro-Cuban. The result was ...record with Jack Weeks on bass and either JohnOther experiments were not so easily dismissed. Tjader teamed upwith Marabuto or Vince Guaraldi on piano? (Rank: 10)New Yorker Eddie Palmieri in 1966 to produce El Sonido",
  "CHIQ-Fusion (Rank: 1)to fear films compared to a control group": ": The first case for analyzing the content between the original and our enhanced history, as well as therewritten queries generated on top of them. The blue tokens and the orange tokens stand for the effective and noisypatterns for matching, respectively. The underline tokens denote the coreference relation between rewritten queriesand the original context. The Rank indicates the ranking position of the gold positive passage w.r.t each query.",
  "Original HistoryOur Enhanced History": "u1: Are there any interesting aspects about Cal Tjader?QD: What was the outcome or reception of Kenneth Angers collab-r1: Cal Tjader entered the United States Navy in 1943oration with Stan Brakhage on their film project in the United States?served as a medical corpsman in the Pacific TheaterPR: It is unclear if Kenneth Anger and Stan Brakhages confiscateduntil March 1946.film was successful as it is presumed to have been destroyed.u2: Were any albums released?However, Angers later film, Inauguration of the Pleasure Dome r2: At San Francisco State he met Dave Brubeck, a(1954), received critical acclaim for its themes.TS: Noyoung pianist fresh from a stint in the Army. BrubeckHS: In 1953, Kenneth Anger returned to the United States followingintroduced Tjader to Paul Desmond.his mothers death to handle her estate. There, he befriended Stanu3: What were the jazz album released for?Brakhage and together they produced a controversial film, which wasr3: Cal Tjader continued the trio work in Californiaconfiscated and likely destroyed due to its obscene content. Despitewith bassist Jack Weeks and pianists John Marabutothis setback, Anger went on to create his groundbreaking 38-minute... recording his first LP with them for Fantasy Records.surrealist work, \"Inauguration of the Pleasure Dome,\" in 1954,u4: What was a title of one of the albums?showcasing Crowleyan and Thelemite themes."
}