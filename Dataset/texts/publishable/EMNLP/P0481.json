{
  "Abstract": "Aligning language models (LMs) based onhuman-annotated preference data is a crucialstep in obtaining practical and performant LM-based systems. However, multilingual humanpreference data are difcult to obtain at scale,making it challenging to extend this frame-work to diverse languages. In this work, weevaluate a simple approach for zero-shot cross-lingual alignment, where a reward model istrained on preference data in one source lan-guage and directly applied to other target lan-guages. On summarization and open-endeddialog generation, we show that this method isconsistently successful under comprehensiveevaluation settings, including human evalua-tion: cross-lingually aligned models are pre-ferred by humans over unaligned models on upto >70% of evaluation instances. We moreovernd that a different-language reward modelsometimes yields better aligned models thana same-language reward model. We also iden-tify best practices when there is no language-specic data for even supervised netuning,another component in alignment.",
  "Introduction": "Alignment has become an indispensable stage forbuilding practical language models (LMs) adjustedto human preferences. This additional step, how-ever, makes it challenging to develop LMs for manylanguages: unlike for autoregressive language mod-eling where multilingual unlabeled data may beeasy to obtain (Joshi et al., 2020), such as religioustexts (Christodouloupoulos and Steedman, 2015),labeled preference data can be expensive to gather.How do we align a LM in a target language withoutany preference data in that language?We propose a novel reward model (RM) trans-fer setup, where we re-purpose a trained RM forsome source language to align a LM in a target lan-guage (), and investigate the effectiveness",
  "Work done while ZW was a part-time intern at Google": ": Cross-lingual reward model (RM) transfer.To align in a target language (in this example, Spanish),common monolingual alignment uses a RM for thattarget language. Instead, we re-purpose a RM for adifferent source language (in this example, English). : Performing target-language alignment us-ing a RM for a different source language improvesperformance, when evaluated exclusively in the tar-get language. This improvement is sometimes evenlarger than using the target-language RM (monolin-gual alignment). Here we measure the win rate againstthe target-language (unaligned) SFT model judged byhumans, and the 95% condence interval across valida-tion instances. sourcetarget denotes using a source-language RM to drive alignment in the target language. of this simple recipe. Across two tasks (summa-rization and open-ended dialog generation), tworeward optimization methods (reinforcement learn-ing and best-of-n reranking), and various eval-uation settings, we demonstrate substantial andconsistent zero-shot cross-lingual utility of RMs.Surprisingly, alignment using a different-languageRM sometimes outperforms using a same-languageRM, both when judged by humans and LMs. Wealso show that our RM transfer framework is use-ful even when target-language data for supervisednetuning (SFT), another component in alignment,is inaccessible.Our results show that RM signals are generaliz-able and robust to input distribution changes, whichcould be leveraged for more future applications.Practically, our ndings pave the path towards low-ering the costs for training and deploying LMs thatmore equitably serve users around the world.",
  "Background: Alignment From HumanFeedback": "In addition to traditional unsupervised LM pretrain-ing, many recent LMs also include an alignmentphase to improve helpfulness, harmlessness, etc.,supervised by human feedback (Bai et al., 2022a;Ouyang et al., 2022; i.a.). A common recipe in-cludes three stages: supervised netuning (SFT),reward modeling (RM), and reward optimization.We give an overview of each and refer readers toOuyang et al. (2022) and Bai et al. (2022a) for de-tails. We assume a base model already pretrainedusing a usually next-token prediction objective. The SFT stageinitializes from the base modeland takes task inputs x X to train the model tosimulate example outputs y Y. Specically, itoptimizes the conditional log-likelihood of y givensome input x, similar to regular language modeling.We denote the trained SFT model using SFT. The RM stagetrains a model r : X Y R asa proxy for human-judged quality of y under x. Itinitializes from SFT and is trained using a datasetof human judgments of generations. We considertwo types of feedback to train the RM: 1. Pointwise feedback judges the quality of a sin-gle generation; in particular we only considerbinary (good or bad) pointwise judgments. De-noting it as z {0, 1} and letting DRM be adataset of judgments, the RM can be a standard",
  "It is also generalizable to more than two outputs": "The reward optimization stagealso initializesfrom SFT and further adjusts the model outputsusing human feedback (as captured by the RM).Two common methods are reinforcement learning(RL) and best-of-n. Best-of-n is an inference-timeprocedure that does not change the underlyingmodel, where multiple generations are sampledfrom SFT and then reranked using the RM; thehighest-scoring generation is returned as the output.In RL, the model itself is changed such that its sam-ples are scored highly by the RM, with the objective",
  "Reward Model Transfer forCross-Lingual Alignment": "The pipeline in 2 is usually performed monolin-gually, commonly in English. Aligning for a newlanguage requires both SFT data and RM data inthat language. While the former may be relativelyeasier to obtain due to automatic construction meth-ods, such as by re-purposing existing multilingualdatasets (Muennighoff et al., 2023) or by elicitingfrom LMs (Wang et al., 2023c), RM data for anew language can be more expensive to gather, asit in principle requires human judgments. Addi-tionally, RM data should ideally be periodicallyre-collected to avoid over-optimization (Bai et al.,2022a), further increasing data demand. Thus, weare mainly interested in alignment without target-language RM data, though, in 5.3, we investigatedispensing with target-language SFT data too. We propose to perform reward optimization us-ing a RM trained for a different language (Fig-ure 1). Intuitively, assuming model generation qual-ity transfers cross-lingually (e.g., good English gen-erations are still good when translated into Span-ish1), a model that can judge the output quality inone language should generalize to others, as longas the RM understands the languages, which is en-abled by multilingual base model training. Thisgeneralizability is often observed for other tasks inthe zero-shot cross-lingual transfer literature (Wuand Dredze, 2019; Pires et al., 2019; Conneau et al.,2020b; Hu et al., 2020; i.a.), and we expect it towork for RMs too. A simple baseline would beto use automatically translated RM data, to whichwe compare in 5.1. In this paper, we use sourcelanguage to denote the RM language, and targetlanguage for the language of the aligned model.",
  "Experimental Setup": "We consider two tasks: summarization, common inalignment research (Stiennon et al., 2020; Ziegleret al., 2020; Lee et al., 2023; i.a.), and open-endeddialog generation, with substantial real-world rel-evance. A describes dataset details and statistics.B includes training details. G.1 contains our taskinstructions. Summarization.The Seahorse dataset (Clarket al., 2023) contains documents and summaries insix languages (German, English, Spanish, Russian,Turkish, and Vietnamese) with pointwise humanratings which we use. For SFT, we gather the datasources of Seahorse: XSum (Narayan et al., 2018),XL-Sum (Hasan et al., 2021), MLSum (Scialomet al., 2020), and WikiLingua (Ladhak et al., 2020).We use mT5-XL (Xue et al., 2021) as our multilin-gual base model, with 3.7B parameters.",
  "Open-Ended Dialog Generation.We use theOpenAssistant dataset (Kpf et al., 2023) with mul-tilingual, pairwise human-rated chat transcripts.2": "For the SFT data, we use the human-preferred re-sponse in each pair to netune the model. Manylanguages in OpenAssistant have only limited data,so we only consider three languages with the mostamounts of data: English, Spanish, and Russian. 1We believe this is a weak assumption, though for tasks andinstances more subject to culture-specic factors, generationsmay be judged more differently across languages (Costa et al.,2014; Hershcovich et al., 2022; Shwartz, 2022).2In We use PaLM-2-XXS as the base model (Anil et al.,2023). The authors of OpenAssistant found RL tobe ineffective for this dataset (Kpf et al., 2023),which we conrmed in our experiments ().We therefore focus on best-of-n for this task. Evaluation.We assess model quality across sev-eral settings. First, we use the target-languageRM, which is by design netuned to judge target-language generation quality. But because of poten-tial RM biases (Gao et al., 2023; Coste et al., 2023;Eisenstein et al., 2023), we also include two zero-shot-prompted evaluation models with much largerbackbonesGPT-4 (OpenAI, 2023) and PaLM-2-L (Anil et al., 2023). This latter evaluation setup iscommon in prior work and has been demonstratedto correlate well with human judgments (Lee et al.,2023; Rafailov et al., 2023; An et al., 2023; Muet al., 2023; i.a.). We also conrm its validity in5.1 and C. Importantly, both evaluation LMssupport multilingual texts. Finally, we also per-form human evaluations by self-reported native oradvanced speakers, though only for a subset oflanguage pairs and 250 (RL) / 100 (best-of-n) in-stances per pair due to its cost. For both humanand LM evaluation, we elicit pairwise judgmentsto compare responses from the aligned model andthe SFT model (Bai et al., 2022b; Lee et al., 2023;i.a.). We measure the win rate, i.e., how often thejudge prefers the former. A 50% win rate indicatesno improvement from alignment. G.2 includesmore details such as the evaluation prompts andpositional bias control.",
  "Cross-Lingual Alignment Is Effective": "When evaluated by the netuned target-languageRM, shows that monolingual best-of-n or RL always improves model quality, as ex-pected. Encouragingly, cross-lingual reward opti-mization improves over the SFT model in all casestoo. Similarly, when judged by a general-purposeLM, PaLM-2-L in and GPT-4 in D, in-language and cross-lingual reward optimizationboth generally improve model quality. Importantly,we observe high agreement between the two LMs:on an instance level, they agree >70% across setups(see D); if we consider how often they agree in the relative ranking of two source languages, theyagree 78% for summarization (both best-of-n andRL) and 100% for dialog generation (best-of-n).This indicates the reliability of a LM judge.Human evaluation () reveals the sametrend, though with larger condence intervals dueto the cost. Human evaluation results also validateand justify LM-based evaluation: For summariza-tion, PaLM-2-L (GPT-4) agrees with humans 65%(69%) of the time in English and 66% (62%) inSpanish, matching the 63% human-human agree-ment for English reference summaries and 67%for Spanish in Seahorse (Clark, personal commu-nication, April 15, 2024). For dialog, PaLM-2-L(GPT-4) agrees with humans 69% (59%) of thetime in English and 62% (60%) in Spanish, againsimilar to the 63% human-human agreement in Baiet al. (2022a) and 66% in Dubois et al. (2024). Withfurther evidence in C, we believe our LM judgesreasonably reect output quality.We also compare our cross-lingual transfersetup to an alternative strategy, sometimes dubbedtranslate-train (Conneau et al., 2018; i.a.), thatrst trains a silver target-language RM by automat-ically translating the source-language data and thenusing the silver RM for target-language alignment.Averaged across all 30 (= 62 6) cross-lingual lan-guage pairs, under best-of-n and judged by PaLM-2-L, our RM transfer strategy outperforms translate-train3 (average win rate 58.8 vs. 57.5; see and 17 for raw numbers). RM transfer also has anefciency advantage: to align in multiple target lan-guages, it sufces to train one source-language RM,rather than different ones for each target language.In F, we also explore alignment using bilingualRMs with two source languages (Mulcaire et al.,2019), though without noticeable improvements.",
  "Cross-Lingual Alignment SometimesOutperforms Monolingual Alignment": "Remarkably, cross-lingual reward optimization of-ten yields an even better model than using thetarget-language RM. This is validated by (1) theconsistent trend when evaluated by PaLM-2-L,GPT-4, and humans, (2) their instance-level andranking-level agreement (5.1), and (3) the smallcondence intervals. This may be due to a regular-ization effect: the target-language RM may possesslanguage-specic spurious artifacts, to which thetarget-language policy model can overt (Gao et al.,",
  "Summarization": ": Source-language RM generalizability evalu-ated by increases in scores they assign to target-languagegenerations after monolingual target-language align-ment (best-of-n or RL). We show all (source, target)language pairs where the two languages differ as den-sity in (a) and lines in (b). RL is difcult to train forOpenAssistant (4), so we omit it here, since the as-sumption that the RLed model is better would not hold.In most cases, the source-language RM assigns ahigher score (>0 increase) to aligned models, demon-strating cross-lingual RM generalizability.",
  "So far we assumed access to target-language SFTdata since, as 3 argues, SFT data could be moreeasily obtained than RM data. We now relax this as-sumption and instead translate the source-language": "SFT data into the target language using GoogleTranslate. We investigate if it, combined with RMtransfer, still enables cross-lingual alignment. Asa case study, we only consider summarization andwhen English is the source or target language.Using translated SFT data substantially degradesthe quality of the SFT model ((a)) and thebest-of-n-aligned LM ((b)). There are how-ever two factors: (1) quality loss due to translation,and (2) domain/style mismatch. For (2), we notethat different languages have SFT data composed ofdifferent datasets, following Seahorse ().4 And these datasets differ stylistically: for example,while XSum includes news articles, WikiLinguaconsists of how-to articles and with more formulaicsummaries. There would thus be a domain differ-ence between using organic target-language SFTdata vs. data translated from a different language.To account for this, we employ round-trip back-translation, rst translating the target-language SFTdata into the source language and then back to thetarget language. This setup is not practically usefulbut it upper-bounds the effect of translation errorsalone. (a) shows that this bridges most ofthe gap, sometimes leading to models that win overthe SFT model >50% of the time. Alternatively, wecontrol for domain by repeating our experimentssolely using WikiLingua for both SFT and RM as",
  "SFT data quantity may also be a confounder, but we con-sider directions both from and to English, and the degradationis substantial in both. So quantity is not the biggest factor": "en deen esen ruen tren vide enes enru entr envi en ROUGE-L (a) Summarization, unaligned SFT model Target-Language SFT Data Translated Source-Language SFT Data Back-Translated SFT Data en deen esen ruen tren vide enes enru entr envi en Win Rate Against SFT (%) (b) Summarization, best-of-n-aligned en deen esen ruen tren vide enes enru entr envi en Win Rate Against SFT (%) (c) Summarization, best-of-n-aligned, WikiLingua only en deen esen ruen tren vide enes enru entr envi en Win Rate Against SFT (%) (d) Summarization, RL-aligned : Cross-lingual alignment results without target-language SFT data using various strategies and on differentdata. Training the SFT model using data translated from another language can be helpful when aligningusing RL ((d)), but domain match is important for best-of-n ((c) and the back-translation results). it is present for all languages. From (c),the gap indeed reduces, with the translated SFTmodels sometimes even outperforming the origi-nal, and back-translation is no longer consistentlybenecial.Other than genre control, we also hypothesizethat the gap would be smaller for RL than best-of-n because the RM, whose transferability weveried (5), intuitively plays a bigger role in theRL pipeline. Best-of-n, on the other hand, is morereliant on the SFT model quality, as reected bythe high resemblance between the transfer perfor-mance patterns in (b) and the SFT modelquality in (a). (d) indeed showsthat the translated models have little performancedrop, except for cases where the former degen-erates.5 Again, apart from the degenerate cases,back-translation is not helpful.To summarize,6 cross-lingual alignment couldstill be helpful even without target-language SFTdata, though care needs to be taken when training",
  "Practical Recommendations": "Our ndings suggest that, for SFT, it is alwaysbenecial to use organic target-language data, butwhen inaccessible, automatic translation may be aremedy, though one should be mindful of the datadistribution match between the data source and theapplication, or relying more on RL.For RM, cross-lingual transfer is often success-ful, but how does one select the source RM lan-guage to align in a new target language? In Fig-ure 6, we show the source languages ranked bytransfer effectiveness for each target language. Therankings across target languages are generally sta-ble, especially for best-of-n: if a source languageis effective for one target language, it is usuallyeffective for others too. Therefore, one may selectthe source language by extrapolating from its per-formance on other target languages. In particular,English RMs are usually the most accessible in de en es ru tr vi",
  ": PaLM-2-L-judged rankings of source lan-guage effectiveness when driving alignment in differenttarget languages. English is generally a good source": "practice. Our results show that it is a decent strat-egy to use them as the source: English is often ahighly-ranked source language, most frequently thebest, perhaps due to the relatively higher annotatorquantity and quality (Yu et al., 2022) or implicitmodeling assumptions (Dyer et al., 2019). Beyondthis empirical observation, we try to causally pre-dict the pairwise transferability from various fea-tures in 6, but without success.",
  "Analysis": "The effectiveness of cross-lingual alignment mo-tivates us to better understand how it relates tovarious factors. We show that while RM general-izability within the original reward modeling taskis a prerequisite, it does not uniquely explain thedownstream success. Similarly, we also show thatthe pairwise win rates (judged by PaLM-2-L unlessotherwise mentioned) cannot be fully explained by,and thereby not predictable from, language featuresor the KL-divergence from the SFT model.",
  "Impact of RM Generalizability WithinReward Modeling": "The RMs cross-lingual utility in downstream align-ment is predicated on their generalizability withinthe original reward modeling task, but the latteris not sufcient for the former.So how muchdoes this generalizability explain the alignment suc-cess? We analyze this generalizability followingthe cross-lingual transfer tradition, zero-shot apply-ing a source-language RM to the target-languagevalidation data and computing accuracy (Wu andDredze, 2019, 2020; Pires et al., 2019; i.a.). Wealso consider a majority baseline and a length base-line to check if the RMs are only supercially cap- turing generation length (Wang et al., 2023b; Sing-hal et al., 2023). To compute this length baseline:for dialog generation, a pairwise task, all longer, orshorter, responses in each pair are chosen, depend-ing on which (long or short) yields higher trainingset accuracy. For summarization, a pointwise task,all responses longer (or shorter) than a thresholdare chosen. The direction (long or short) and thethreshold are also selected using the training set. conrms cross-lingual RM generaliz-ability: cross-lingual RMs often perform abovethe majority baseline for summarization and ran-dom performance (50%) for dialog. E veries thiscross-lingual generalizability with another setup.Nevertheless, the improvements over the majori-ty/random baselines are modest. The dialog modelseven sometimes underperform the length baseline(though this does not mean the RMs only rely onlength7). Part of this is due to the high subjectivityof the reward modeling task: the RM accuracieshere are near the human agreement level for Sea-horse (Clark et al., 2023), plotted in , andgenerally match the human agreement numbers indialog generation work (Bai et al., 2022a; Duboiset al., 2024). But it is still interesting that seeminglyweak RMs, like the Vietnamese RM which per-forms similarly to the majority baseline when usedmonolingually or the dialog RMs which are oftensurpassed by the length baseline, can achieve highcross-lingual alignment effectiveness ().Furthermore, the results here do not match theirdownstream utility, regardless of whether we con-sider the quality of the RMs as measured by their in-language validation accuracy (Turkish, for example,is the best in , but not so in ), thegeneralizability of the RMs which we operational-ize as the difference between in-language trainingand validation loss (or accuracythey yield thesame ranking: Russian, German, English, Turkish,Vietnamese, and Spanish, from the least amountof overtting to the most, again different from Fig-ure 6), or the specic pairwise transfer effective-ness (for each target language, we compare theeffectiveness of source languages ranked by thereward modeling task generalizability here vs. bydownstream alignment win rate; on summariza-tion, averaged across target languages, Kendalls = 0.1 (same with best-of-n or RL), indicat- 7The RMs agree with the length baseline on 72.6% of thevalidation instances, higher than the baseline agreement levelof 56.6% (how often two random models at their accuracylevels agree on average), but far from full agreement. : Source-language RM generalizability within the original reward modeling task and the 95% condenceinterval across validation instances. sourcetarget denotes training a source-language RM and measuring itsaccuracy on the target language validation data. The baselines are explained in 6.1. Dialog generation, a pairwisetask, does not have a majority baseline; the dataset authors also did not report human agreement. RMs generallyexhibit cross-lingual generalizability, exceeding the majority baseline and often the length baseline.",
  "Impact of Language Features": "Can the cross-lingual alignment performance bepredicted from simple language features, such astheir frequency in the pretraining corpus or typo-logical similarity? The summarization languagesranked by frequency in the mT5 corpus, the basemodel for this task, are: English, Russian, Spanish,German, Turkish, Vietnamese (Xue et al., 2021).This does not match the transfer utility ranking in. Similarly, neither does the ranking matchthe SFT data quantity or RM data quantity (in A).Linguistic typology and orthography are alsocommon predictors of cross-lingual transferabil-ity (Gerz et al., 2018; K et al., 2020; Muller et al.,2021; i.a.). This, however, is not the case for us ei-ther: for summarization RL, for example, Englishbenets from Vietnamese the most, but they be-long to disparate language families. Orthographymay be playing a role: Russian overall does nottransfer well to other languages, and it is the onlylanguage that does not use the Latin script, but thistrend is not clear. Systematically, we compute thecorrelation between alignment utility and WALSfeatures of linguistic typology (Dryer and Haspel-math, 2013). For each WALS feature present for all6 summarization languages, we divide all win ratesinto two groups: those between language pairs thathave the same, or different, feature values. Undera one-sided unpaired t-test, no feature shows sta-tistical signicance at = 0.05 with Bonferronicorrection (Dunn, 1961).8 Therefore, alignment",
  "Impact of Policy Divergence": "From a learning angle, it has been shown that thereward that a learned policy can obtain stronglycorrelates with its KL-divergence from the base(SFT) policy (Bai et al., 2022a). This could beconcerning, if the model deviates from the basepolicy to hack the reward (Gao et al., 2023; Costeet al., 2023; Eisenstein et al., 2023), but not if theevaluation metric is robust. As we perform humanevaluation and also veried that our LM judgescorrelate with human judgments, this is less of a",
  "Related Work": "Zero-shot cross-lingual transfer.There is along line of research on cross-lingual representa-tion generalizability, such as with sentence em-beddings (Conneau et al., 2018) or more recently,LMs (Wu and Dredze, 2019, 2020; Pires et al.,2019; Siddhant et al., 2020). Commonly, a mul-tilingual LM (Devlin et al., 2019; Conneau andLample, 2019; Conneau et al., 2020a; i.a.) is ne-tuned on a task in a source language and evaluatedon the tasks test set in a different language. Thisis generally effective. Our RM transfer setup canbe viewed under this framework, but we go fur-ther and show that this generalizability is useful fordownstream tasks, in our case alignment. Shahamet al. (2024) and Chirkova and Nikoulina (2024)are close to us in studying cross-lingual generaliz-ability in alignment, but only focusing on SFT andonly using translated data. Multilingual Alignment.For SFT, it is commonto assemble existing multilingual task datasets intoinstruction datasets (Muennighoff et al., 2023; Asaiet al., 2023; Ahuja et al., 2023). Some have directlycollected SFT data for non-English languages, ei-ther on a per-language basis (Zhang et al., 2023;Xu et al., 2023b; Ni et al., 2023; i.a.) or multi-lingually (Zhao et al., 2024; Singh et al., 2024),though this can be expensive. Past work has alsoused automatic translation for SFT (Li et al., 2023a;Lai et al., 2023; Shaham et al., 2024; i.a.) andRM data (Lai et al., 2023; Shen et al., 2024). Wealso use translation for SFT, but showed that cross-lingual transfer outperforms translation for RM.",
  "Conclusion": "We showed through two different tasks that we canperform alignment using a different-language RM.Surprisingly, we nd this to be sometimes moreeffective than using a same-language RM. We alsoidentied issues and remedies when we dispensewith target-language SFT data. We hope our nd-ings can motivate future work to build better LMsfor more languages. Adapting our RM transfersetup to other settings such as domain generaliza-tion would also be exciting future directions.",
  "Limitations": "Free-form generation is challenging to evaluate, es-pecially in a cross-lingual setup. As we mentioned,neither the netuned target-language RM evalua-tor scores nor pairwise evaluation from humans orLMs are perfect (Wang et al., 2023b; Zheng et al.,2023; Hosking et al., 2024; i.a.). Nevertheless, webelieve the consistent cross-lingual transferabilityobserved across our many evaluation settings sug-gests that it would hold more generally. Similarly,it is not possible to comprehensively study the myr-iad of reward optimization methods (Rafailov et al.,2023; Azar et al., 2023; i.a.), some of which maynot enjoy the same cross-lingual RM transfer bene-t (in fact, the notion of a RM do not even exist insome, though analogous ideas may be applicable).However, the two that we study, best-of-n and PPO,are representative of current common practices, es-pecially given the strong empirical performance ofbest-of-n (Gao et al., 2023; Mudgal et al., 2023;Rafailov et al., 2023; i.a.). Somewhat orthogo-nally, past work has argued that it is limiting touse one single scalar to represent generation qual-ity (Xu et al., 2023a; Krishna et al., 2023; Hoskinget al., 2024) and that more ne-grained rewardscould be benecial (Wu et al., 2023). We followthe convention to use one single score to more eas-ily measure and compare cross-lingual transfer inmany setups, but a similar but more ne-grainedstudy would be valuable future work. It has alsobeen shown that it is more challenging to train re-ward models for low-resourced languages (Shenet al., 2024). We only considered relatively high-resourced languages in this work, and it is possiblethat the pattern would differ when using lower-resourced source languages for transfer. Finally,our motivating assumption that generation qualitybeing language-agnostic does not always hold, es-pecially when facing culture-specic tasks or taskinstances. In those cases, we believe we would seereduced cross-lingual generalizability. We would like to thank Jonathan Berant, JilinChen, Elizabeth Clark, Daphne Domansi, Jie Fan,Han Guo, Henry Hand, Harrison Lee, Jong Lee,Alisa Liu, Ana Marasovic, Usha Rani Markuk,Joshua Maynez, Kathy Meier-Hellstern, ChiragNagpal, Flavien Prost, Linlu Qiu, Kevin Robinson,Alexis Ross, Shannon Zejiang Shen, Bailin Wang,Xinyan Velocity Yu, and the T5X team Google for",
  "Chenxin An, Shansan Gong, Ming Zhong, XingjianZhao, Mukai Li, Jun Zhang, Lingpeng Kong, andXipeng Qiu. 2023. L-Eval: Instituting standardizedevaluation for long context language models": "Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-son, Dmitry Lepikhin, Alexandre Passos, SiamakShakeri, Emanuel Taropa, Paige Bailey, ZhifengChen, Eric Chu, Jonathan H. Clark, Laurent ElShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-rav Mishra, Erica Moreira, Mark Omernick, KevinRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,Yuanzhong Xu, Yujing Zhang, Gustavo HernandezAbrego, Junwhan Ahn, Jacob Austin, Paul Barham,Jan Botha, James Bradbury, Siddhartha Brahma,Kevin Brooks, Michele Catasta, Yong Cheng, ColinCherry, Christopher A. Choquette-Choo, AakankshaChowdhery, Clment Crepy, Shachi Dave, MostafaDehghani, Sunipa Dev, Jacob Devlin, Mark Daz,Nan Du, Ethan Dyer, Vlad Feinberg, FangxiaoyuFeng, Vlad Fienber, Markus Freitag, Xavier Gar-cia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, JoshuaHowland, Andrea Hu, Jeffrey Hui, Jeremy Hur-witz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-ski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,Sneha Kudugunta, Chang Lan, Katherine Lee, Ben-jamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li,Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,Frederick Liu, Marcello Maggioni, Aroma Mahendru,Joshua Maynez, Vedant Misra, Maysam Moussalem,Zachary Nado, John Nham, Eric Ni, Andrew Nys-trom, Alicia Parrish, Marie Pellat, Martin Polacek,Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,Bryan Richter, Parker Riley, Alex Castro Ros, Au-rko Roy, Brennan Saeta, Rajkumar Samuel, ReneeShelby, Ambrose Slone, Daniel Smilkov, David R.So, Daniel Sohn, Simon Tokumine, Dasha Valter,Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang,Pidong Wang, Zirui Wang, Tao Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, LintingXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, StevenZheng, Ce Zheng, Weikang Zhou, Denny Zhou, SlavPetrov, and Yonghui Wu. 2023. PaLM 2 technicalreport.",
  "Mohammad Gheshlaghi Azar, Mark Rowland, BilalPiot, Daniel Guo, Daniele Calandriello, MichalValko, and Rmi Munos. 2023. A general theoret-ical paradigm to understand learning from humanpreferences": "Yuntao Bai, Andy Jones, Kamal Ndousse, AmandaAskell, Anna Chen, Nova DasSarma, Dawn Drain,Stanislav Fort, Deep Ganguli, Tom Henighan,Nicholas Joseph, Saurav Kadavath, Jackson Kernion,Tom Conerly, Sheer El-Showk, Nelson Elhage, ZacHateld-Dodds, Danny Hernandez, Tristan Hume,Scott Johnston, Shauna Kravec, Liane Lovitt, NeelNanda, Catherine Olsson, Dario Amodei, TomBrown, Jack Clark, Sam McCandlish, Chris Olah,Ben Mann, and Jared Kaplan. 2022a. Training ahelpful and harmless assistant with reinforcementlearning from human feedback. Yuntao Bai,Saurav Kadavath,Sandipan Kundu,Amanda Askell, Jackson Kernion, Andy Jones, AnnaChen, Anna Goldie, Azalia Mirhoseini, CameronMcKinnon, Carol Chen, Catherine Olsson, Christo-pher Olah, Danny Hernandez, Dawn Drain, DeepGanguli, Dustin Li, Eli Tran-Johnson, Ethan Perez,Jamie Kerr, Jared Mueller, Jeffrey Ladish, JoshuaLandau, Kamal Ndousse, Kamile Lukosuite, LianeLovitt, Michael Sellitto, Nelson Elhage, NicholasSchiefer, Noemi Mercado, Nova DasSarma, RobertLasenby, Robin Larson, Sam Ringer, Scott John-ston, Shauna Kravec, Sheer El Showk, Stanislav Fort,Tamera Lanham, Timothy Telleen-Lawton, Tom Con-erly, Tom Henighan, Tristan Hume, Samuel R. Bow-man, Zac Hateld-Dodds, Ben Mann, Dario Amodei,Nicholas Joseph, Sam McCandlish, Tom Brown, andJared Kaplan. 2022b. Constitutional AI: Harmless-ness from AI feedback.",
  "Elizabeth Clark, Shruti Rijhwani, Sebastian Gehrmann,Joshua Maynez, Roee Aharoni, Vitaly Nikolaev,Thibault Sellam, Aditya Siddhant, Dipanjan Das, and": "Ankur Parikh. 2023. SEAHORSE: A multilingual,multifaceted dataset for summarization evaluation.In Proceedings of the 2023 Conference on Empiri-cal Methods in Natural Language Processing, pages93979413, Singapore. Association for Computa-tional Linguistics. Alexis Conneau, Kartikay Khandelwal, Naman Goyal,Vishrav Chaudhary, Guillaume Wenzek, FranciscoGuzmn, Edouard Grave, Myle Ott, Luke Zettle-moyer, and Veselin Stoyanov. 2020a. Unsupervisedcross-lingual representation learning at scale. In Pro-ceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 84408451, Online. Association for Computational Lin-guistics.",
  "Alexis Conneau and Guillaume Lample. 2019. Cross-lingual language model pretraining. In Advances inNeural Information Processing Systems, volume 32.Curran Associates, Inc": "Alexis Conneau, Ruty Rinott, Guillaume Lample, AdinaWilliams, Samuel Bowman, Holger Schwenk, andVeselin Stoyanov. 2018. XNLI: Evaluating cross-lingual sentence representations. In Proceedings ofthe 2018 Conference on Empirical Methods in Nat-ural Language Processing, pages 24752485, Brus-sels, Belgium. Association for Computational Lin-guistics. Alexis Conneau, Shijie Wu, Haoran Li, Luke Zettle-moyer, and Veselin Stoyanov. 2020b.Emergingcross-lingual structure in pretrained language mod-els. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics, pages60226034, Online. Association for ComputationalLinguistics.",
  "Chris Dyer, Gbor Melis, and Phil Blunsom. 2019. Acritical analysis of biased parsers in unsupervisedparsing": "Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ah-mad Beirami, Alex DAmour, DJ Dvijotham, AdamFisch, Katherine Heller, Stephen Pfohl, Deepak Ra-machandran, Peter Shaw, and Jonathan Berant. 2023.Helping or herding? Reward model ensembles miti-gate but do not eliminate reward hacking. Leo Gao, John Schulman, and Jacob Hilton. 2023. Scal-ing laws for reward model overoptimization. In Pro-ceedings of the 40th International Conference onMachine Learning, volume 202 of Proceedings ofMachine Learning Research, pages 1083510866.PMLR. Sebastian Gehrmann, Tosin Adewumi, KarmanyaAggarwal,PawanSasankaAmmanamanchi,AnuoluwapoAremu,AntoineBosselut,Khy-athi Raghavi Chandu, Miruna-Adriana Clinciu,Dipanjan Das, Kaustubh Dhole, Wanyu Du, EsinDurmus, Ondrej Duek, Chris Chinenye Emezue,VarunGangal,CristinaGarbacea,TatsunoriHashimoto, Yufang Hou, Yacine Jernite, Harsh Jham-tani, Yangfeng Ji, Shailza Jolly, Mihir Kale, DhruvKumar, Faisal Ladhak, Aman Madaan, MounicaMaddela, Khyati Mahajan, Saad Mahamood, Bod-hisattwa Prasad Majumder, Pedro Henrique Martins,Angelina McMillan-Major, Simon Mille, Emiel vanMiltenburg, Moin Nadeem, Shashi Narayan, VitalyNikolaev, Andre Niyongabo Rubungo, SalomeyOsei,AnkurParikh,LauraPerez-Beltrachini,Niranjan Ramesh Rao, Vikas Raunak, Juan DiegoRodriguez,SashankSanthanam,JooSedoc,Thibault Sellam, Samira Shaikh, Anastasia Shimo-rina, Marco Antonio Sobrevilla Cabezudo, HendrikStrobelt, Nishant Subramani, Wei Xu, Diyi Yang,Akhila Yerukola, and Jiawei Zhou. 2021.TheGEM benchmark:Natural language generation,its evaluation and metrics. In Proceedings of the1st Workshop on Natural Language Generation,Evaluation, and Metrics (GEM 2021), pages 96120,Online. Association for Computational Linguistics. Daniela Gerz, Ivan Vulic, Edoardo Maria Ponti, RoiReichart, and Anna Korhonen. 2018. On the rela-tion between linguistic typology and (limitations of)multilingual language modeling. In Proceedings ofthe 2018 Conference on Empirical Methods in Natu-ral Language Processing, pages 316327, Brussels,Belgium. Association for Computational Linguistics. Tahmid Hasan, Abhik Bhattacharjee, Md. Saiful Is-lam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang,M. Sohel Rahman, and Rifat Shahriyar. 2021. XL-sum: Large-scale multilingual abstractive summariza-tion for 44 languages. In Findings of the Associationfor Computational Linguistics: ACL-IJCNLP 2021,pages 46934703, Online. Association for Computa-tional Linguistics. Daniel Hershcovich, Stella Frank, Heather Lent,Miryam de Lhoneux, Mostafa Abdou, StephanieBrandl, Emanuele Bugliarello, Laura Cabello Pi-queras, Ilias Chalkidis, Ruixiang Cui, ConstanzaFierro, Katerina Margatina, Phillip Rust, and AndersSgaard. 2022. Challenges and strategies in cross-cultural NLP. In Proceedings of the 60th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 69977013,Dublin, Ireland. Association for Computational Lin-guistics.",
  "Human feedback is not gold standard": "Junjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-ham Neubig, Orhan Firat, and Melvin Johnson.2020. XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual gener-alisation. In Proceedings of the 37th InternationalConference on Machine Learning, volume 119 ofProceedings of Machine Learning Research, pages44114421. PMLR. Pratik Joshi, Sebastin Santy, Amar Budhiraja, KalikaBali, and Monojit Choudhury. 2020. The state andfate of linguistic diversity and inclusion in the NLPworld. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics, pages62826293, Online. Association for ComputationalLinguistics.",
  "Karthikeyan K, Zihan Wang, Stephen Mayhew, andDan Roth. 2020. Cross-lingual ability of multilin-gual BERT: An empirical study. In InternationalConference on Learning Representations": "Kalpesh Krishna, Erin Bransom, Bailey Kuehl, MohitIyyer, Pradeep Dasigi, Arman Cohan, and Kyle Lo.2023. LongEval: Guidelines for human evaluation offaithfulness in long-form summarization. In Proceed-ings of the 17th Conference of the European Chap-ter of the Association for Computational Linguistics,pages 16501669, Dubrovnik, Croatia. Associationfor Computational Linguistics. Andreas Kpf, Yannic Kilcher, Dimitri von Rtte,Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,Abdullah Barhoum, Nguyen Minh Duc, OliverStanley, Richrd Nagy, Shahul ES, Sameer Suri,David Glushkov, Arnav Dantuluri, Andrew Maguire,Christoph Schuhmann, Huu Nguyen, and Alexan-der Mattick. 2023. OpenAssistant conversations democratizing large language model alignment. Faisal Ladhak, Esin Durmus, Claire Cardie, and Kath-leen McKeown. 2020. WikiLingua: A new bench-mark dataset for cross-lingual abstractive summariza-tion. In Findings of the Association for Computa-tional Linguistics: EMNLP 2020, pages 40344048,Online. Association for Computational Linguistics. Viet Lai, Chien Nguyen, Nghia Ngo, Thuat Nguyen,Franck Dernoncourt, Ryan Rossi, and Thien Nguyen.2023. Okapi: Instruction-tuned large language mod-els in multiple languages with reinforcement learning from human feedback. In Proceedings of the 2023Conference on Empirical Methods in Natural Lan-guage Processing: System Demonstrations, pages318327, Singapore. Association for ComputationalLinguistics. Harrison Lee, Samrat Phatale, Hassan Mansoor, ThomasMesnard, Johan Ferret, Kellie Lu, Colton Bishop,Ethan Hall, Victor Carbune, Abhinav Rastogi, andSushant Prakash. 2023. RLAIF: Scaling reinforce-ment learning from human feedback with ai feed-back.",
  "Learning to compress prompts with gist tokens": "Sidharth Mudgal,Jong Lee,Harish Ganapathy,YaGuang Li, Tao Wang, Yanping Huang, ZhifengChen, Heng-Tze Cheng, Michael Collins, TrevorStrohman, Jilin Chen, Alex Beutel, and AhmadBeirami. 2023. Controlled decoding from languagemodels. Niklas Muennighoff, Thomas Wang, Lintang Sutawika,Adam Roberts, Stella Biderman, Teven Le Scao,M Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai-ley Schoelkopf, Xiangru Tang, Dragomir Radev,Alham Fikri Aji, Khalid Almubarak, Samuel Al-banie, Zaid Alyafeai, Albert Webson, Edward Raff,and Colin Raffel. 2023.Crosslingual generaliza-tion through multitask netuning. In Proceedingsof the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 1599116111, Toronto, Canada. Associationfor Computational Linguistics. Phoebe Mulcaire, Jungo Kasai, and Noah A. Smith.2019. Polyglot contextual representations improvecrosslingual transfer. In Proceedings of the 2019Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, Volume 1 (Long and ShortPapers), pages 39123918, Minneapolis, Minnesota.Association for Computational Linguistics. Benjamin Muller, Antonios Anastasopoulos, BenotSagot, and Djam Seddah. 2021. When being un-seen from mBERT is just the beginning: Handlingnew languages with multilingual language models.In Proceedings of the 2021 Conference of the North",
  "OpenAI. 2023. GPT-4 technical report": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, JohnSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,Maddie Simens, Amanda Askell, Peter Welinder,Paul F Christiano, Jan Leike, and Ryan Lowe. 2022.Training language models to follow instructions withhuman feedback. In Advances in Neural InformationProcessing Systems, volume 35, pages 2773027744.Curran Associates, Inc.",
  "Telmo Pires, Eva Schlinger, and Dan Garrette. 2019": "How multilingual is multilingual BERT? In Proceed-ings of the 57th Annual Meeting of the Association forComputational Linguistics, pages 49965001, Flo-rence, Italy. Association for Computational Linguis-tics. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-pher D Manning, Stefano Ermon, and Chelsea Finn.2023. Direct preference optimization: Your languagemodel is secretly a reward model. In Thirty-seventhConference on Neural Information Processing Sys-tems.",
  "Uri Shaham, Jonathan Herzig, Roee Aharoni, IdanSzpektor, Reut Tsarfaty, and Matan Eyal. 2024. Mul-tilingual instruction tuning with just a pinch of multi-linguality": "Noam Shazeer and Mitchell Stern. 2018. Adafactor:Adaptive learning rates with sublinear memory cost.In Proceedings of the 35th International Conferenceon Machine Learning, volume 80 of Proceedingsof Machine Learning Research, pages 45964604.PMLR. Lingfeng Shen, Weiting Tan, Sihao Chen, Yunmo Chen,Jingyu Zhang, Haoran Xu, Boyuan Zheng, PhilippKoehn, and Daniel Khashabi. 2024. The languagebarrier: Dissecting safety challenges of llms in multi-lingual contexts. Vered Shwartz. 2022. Good night at 4 pm?! Time ex-pressions in different cultures. In Findings of the As-sociation for Computational Linguistics: ACL 2022,pages 28422853, Dublin, Ireland. Association forComputational Linguistics. Aditya Siddhant, Melvin Johnson, Henry Tsai, NaveenAri, Jason Riesa, Ankur Bapna, Orhan Firat, andKarthik Raman. 2020. Evaluating the cross-lingualeffectiveness of massively multilingual neural ma-chine translation. Proceedings of the AAAI Confer-ence on Articial Intelligence, 34(05):88548861. Shivalika Singh, Freddie Vargus, Daniel Dsouza,Brje F. Karlsson, Abinaya Mahendiran, Wei-YinKo, Herumb Shandilya, Jay Patel, Deividas Mat-aciunas, Laura OMahony, Mike Zhang, RamithHettiarachchi, Joseph Wilson, Marina Machado,Luisa Souza Moura, Dominik Krzeminski, HakimehFadaei, Irem Ergn, Ifeoma Okoh, Aisha Alaagib,Oshan Mudannayake, Zaid Alyafeai, Vu Minh Chien,Sebastian Ruder, Surya Guthikonda, Emad A. Al-ghamdi, Sebastian Gehrmann, Niklas Muennighoff,Max Bartolo, Julia Kreutzer, Ahmet stn, MarziehFadaee, and Sara Hooker. 2024. Aya dataset: Anopen-access collection for multilingual instructiontuning.",
  "Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu,Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, andZhifang Sui. 2023a. Large language models are notfair evaluators": "Yizhong Wang, Hamish Ivison, Pradeep Dasigi, JackHessel, Tushar Khot, Khyathi Chandu, David Wad-den, Kelsey MacMillan, Noah A. Smith, Iz Beltagy,and Hannaneh Hajishirzi. 2023b. How far can camelsgo? Exploring the state of instruction tuning on openresources. In Thirty-seventh Conference on NeuralInformation Processing Systems Datasets and Bench-marks Track. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, AlisaLiu, Noah A. Smith, Daniel Khashabi, and HannanehHajishirzi. 2023c. Self-instruct: Aligning languagemodels with self-generated instructions. In Proceed-ings of the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 1348413508, Toronto, Canada. Associationfor Computational Linguistics. Shijie Wu and Mark Dredze. 2019. Beto, bentz, becas:The surprising cross-lingual effectiveness of BERT.In Proceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the 9thInternational Joint Conference on Natural LanguageProcessing (EMNLP-IJCNLP), pages 833844, HongKong, China. Association for Computational Linguis-tics. Shijie Wu and Mark Dredze. 2020. Are all languagescreated equal in multilingual BERT? In Proceedingsof the 5th Workshop on Representation Learning forNLP, pages 120130, Online. Association for Com-putational Linguistics. Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, AlaneSuhr, Prithviraj Ammanabrolu, Noah A. Smith, MariOstendorf, and Hannaneh Hajishirzi. 2023. Fine-grained human feedback gives better rewards for lan-guage model training. In Thirty-seventh Conferenceon Neural Information Processing Systems. Fangyuan Xu, Yixiao Song, Mohit Iyyer, and EunsolChoi. 2023a. A critical evaluation of evaluationsfor long-form question answering. In Proceedingsof the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 32253245, Toronto, Canada. Association forComputational Linguistics. Guohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, JinghuiSi, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang,Rong Zhang, Ji Zhang, Chao Peng, Fei Huang, andJingren Zhou. 2023b. CValues: Measuring the val-ues of chinese large language models from safety toresponsibility. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale,Rami Al-Rfou, Aditya Siddhant, Aditya Barua, andColin Raffel. 2021. mT5: A massively multilingualpre-trained text-to-text transformer. In Proceedingsof the 2021 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 483498, On-line. Association for Computational Linguistics. Xinyan Yu, Trina Chatterjee, Akari Asai, Junjie Hu,and Eunsol Choi. 2022. Beyond counting datasets:A survey of multilingual dataset construction andnecessary resources. In Findings of the Associationfor Computational Linguistics: EMNLP 2022, pages37253743, Abu Dhabi, United Arab Emirates. As-sociation for Computational Linguistics.",
  "ADataset Details and Statistics": "We report dataset statistics in , 2, 3, and 4.We reuse the SFT data for reward optimization (forboth training and evaluation for RL, and for onlyevaluation for best-of-n since it does not have atraining stage), but only the input x, without refer-ence generations y.The summarization SFT datasets, reported in Ta-ble 1, are the original data sources of Seahorse,which we take from the GEM release (Gehrmannet al., 2021). They are evenly mixed at the in-stance level for both SFT training and RL training.For evaluation of the aligned model, we macro-average the per-dataset metrics (e.g., win rate) for alanguage-level score. Because the Seahorse datasetwas created using the validation and test instancesof the original summarization datasets, to be clean,we exclude the Seahorse training instances fromthese splits when performing SFT and reward opti-mization. OpenAssistant does not have this issueand has clean split separations. The Seahorse sum-maries are human-rated along six axes, and we onlyuse the sixth axis for our pointwise reward as it en-capsulates previous axes (Clark et al., 2023). Welimit the maximum length of model inputs to 1,024tokens and outputs to 512 tokens. See also G.1for instructions we attach to the dataset instancesduring training and inference.",
  "RM.The model is trained using Adafactor witha constant learning rate at 104 after 1,000 linearwarm-up steps, batch size 32, and dropout 0.1. Weperform checkpoint selection using validation loss": "RL.We use PPO for RL training with a constantlearning rate at 104, batch size 32, for 3,000 stepsfor summarization and 2,500 steps for dialog gen-eration. The value model has 1,000 linear warm-upsteps and we only start training the policy modelafter 2,000 steps elapse. We set the regularizationcoefcient at = 0.1.",
  ": The accuracy of evaluating the PaLM-2-L judge on the RM validation data. We also report the number ofcomparisons based on which the accuracy is calculated": ": Alignment effectiveness, compared to the target-language SFT model judged by GPT-4, and the 95%condence interval across validation instances. sourcetarget denotes a source-language RM driving alignmentin the target language. Cross-lingual alignment is generally effective, sometimes outperforming monolingualalignment. RL is hard to train for OpenAssistant, in line with what its authors found (Kpf et al., 2023).",
  "CLM Judge Accuracy on Ground-truthReward Modeling Data": "We verify the validity of using LM as a judge forour tasks by computing its accuracy on the valida-tion splits of the RM datasets we used. We onlyconsider PaLM-2-L as a case study. For OpenAssis-tant, a pairwise dataset, we simply check if the RMranks the candidate generations correctly accord-ing to human preference. For Seahorse, a point-wise dataset, we group summaries for the samesource document, and for each summary pair insuch groups, we compute the ranking correctness.We show the results in . The accura-cies generally match the human agreement in Sea-horse (Clark et al., 2023), and while human agree-ment was not reported in OpenAssistant, they gen-erally match the human agreement numbers inpast work on dialog generation (Bai et al., 2022a;Dubois et al., 2024) too (see 5.1 for reference hu-",
  "DGPT-4 as a Judge Results": "In this section, we present the alignment evalua-tion results as judged by GPT-4, specically thegpt-4-0125-preview model. Due to its high cost,we cap the number of evaluation instances for eachdataset at 1,000 (i.e., for each row of and3). The results are shown in . We observethe same trends as in 5.1, where cross-lingual re-ward optimization is generally effective, sometimeseven more so than when done monolingually. Com-pared to PaLM-2-L, the two LMs agree on 72%of the instances in English and 71% in Spanishfor summarization, and 75% and 73% for theselanguages for dialog. These are higher than the 0.00.51.01.52.0 Source-Lg. RM Score Increase",
  "In 6.1, we observed RM generalizability on theoriginal reward modeling task, which would bea necessary condition for successful downstream": "cross-lingual alignment. There, we showed thatthe source-language RMs assign higher scores tobetter target-language generations than worse ones.Here, we consider an alternative setup to study thesame problem: instead of relying on existing RMdatasets for the better and worse generations, wetake generations from monolingually-aligned mod-els as better ones than those from unaligned (i.e.,SFT) models. The assumption here is that mono-lingual alignment improves model quality, whichis indeed the case as illustrated in and9. Like in 6.1, we indeed see from that source-language RMs assign higher scores tomonolingually-aligned models than unaligned SFTmodels. Under RL, this score difference also in-creases throughout training. These results conrmthe RMs cross-lingual generalizability within thereward modeling task.",
  "FAlignment Using Bilingual RMs": "Seeing the benet of cross-lingual RM transfer-ability in 5, we hypothesize that bilingual RMscould bring further improvements since the result-ing reward could be encouraged to be more lan-guage agnostic (Mulcaire et al., 2019). It would becomputationally expensive to experiment with allpossible language congurations (there would be acubic number of them with pairwise sources), so,for simplicity, we take the best-performing sourcelanguages under the summarization best-of-n setupas judged by PaLM-2-L, English and German (Fig-ure 6), and see if a bilingual RM based on themwould lead to further performance improvement.Specically, we rst train a bilingual SFT modelby pooling the SFT data for both languages, andsimilarly for the RM, which initializes from thisbilingual SFT model. does not show an improvement fromthe bilingual RM, which always achieves similarperformance to the English RM, the better of thetwo monolingual RMs. Nevertheless, if this trendholds consistently, that the bilingual RM matchesthe performance of the better monolingual RM,this could be useful as an alternative to having toperform source language selection. We leave amore systematic validation of this phenomenon tofuture work.",
  "G.1Task Instructions": "We prepend the following task-specic instructionsto inputs for SFT and reward optimization. Alloccurrences of [LANGUAGE] are substituted withthe target language. The RM stage does not includesuch prompts, where we simply concatenate thetexts with delimiters.Summarization: Summarize the followingtext in [LANGUAGE]:Dialog generation: You are given a dialogbetweenahumanandanassistantin[LANGUAGE]. Please write one turn of theassistant side in [LANGUAGE].\\n\\n",
  "G.2Evaluation Prompts": "We use the following prompts to elicit pairwisegeneration judgments for both human and LMjudge evaluation. All occurrences of [LANGUAGE],[INPUT], [GENERATION1], and [GENERATION2]are substituted with the respective content. Forboth tasks, we compare the probability of the to-kens 1 and 2. To control for the positional biasof LMs (Wang et al., 2023a; Pezeshkpour and Hr-uschka, 2023; Zheng et al., 2023) and potentially ofour human annotators, we randomly shufe the twogenerations for human evaluation and the GPT-4judge. For the PaLM-2 judge for which we haveprobability access, we prompt the LM judge twicewith both orderings of the generations and computethe accuracy by averaging the probabilities of the1 and 2 tokens.",
  "conveys the key information from theoriginal post. Below we define fourevaluation axes for summary quality:coherence, accuracy, coverage, and overallquality": "Coherence: This axis answers the questionhow coherent is the summary on its own?A summary is coherent if it's easy tounderstand when read on its own and free ofEnglish errors. A summary is not coherentif it's difficult to understand what thesummary is trying to say. Generally, it'smore important that the summary isunderstandable than it being free ofgrammar errors. Accuracy: This axis answers the questiondoes the factual information in thesummary accurately match the post? Asummary is accurate if it doesn't saythings that aren't in the article, itdoesn't mix up people, and generally isnot misleading. Coverage: This axis answers the questionhow well does the summary cover theimportant information in the post? Asummary has good coverage if it mentionsthe main information from the post that'simportant to understand the situationdescribed in the post. A summary has poorcoverage if someone reading only thesummary would be missing several importantpieces of information about the situationin the post. A summary with good coverageshould also match the purpose of the",
  "original post (e.g. to ask for advice)": "Overall quality: This axis answers thequestion how good is the summary overallat representing the post? This canencompass all of the above axes of quality,as well as others you feel are important.If it's hard to find ways to make thesummary better, the overall quality isgood. If there are lots of different waysthe summary can be made better, the overallquality is bad. You are an expert summary rater and areknowledgeable in [LANGUAGE]. Given apiece of text in [LANGUAGE] and two of itspossible summaries, also in [LANGUAGE],output 1 or 2 to indicate which summarybest adheres to coherence, accuracy,coverage, and overall quality as definedabove.",
  "LgEn36.6 26.6 29.8 37.5 31.8EnLg14.4 43.5 43.9 47.1 41.6EnLgEn42.7 43.2 40.1 41.4 37.1LgEnLg45.3 54.0 60.1 61.7 51.1": ":Alignment performance using best-of-n,measured in the win rate against the monolingual targetlanguage SFT model as judged by PaLM-2-L, when theSFT model is trained using different strategies. Therst section uses a SFT model that is trained on target-language datasets (same as ), while the sec-ond uses translated or back-translated SFT data (Fig-ure 5(b)).",
  "LgEn40.5 29.1 33.2 26.0 19.4EnLg45.7 50.3 60.3 37.1 67.6EnLgEn31.4 33.9 34.0 40.8 31.7LgEnLg40.3 31.2 40.1 45.9 61.4": ":Alignment performance using best-of-n,measured in the win rate against the monolingual targetlanguage SFT model as judged by PaLM-2-L, when theSFT model is trained using different strategies. Therst section uses a SFT model that is trained on target-language datasets, while the second uses translated orback-translated SFT data. Here, we only consider theWikiLingua dataset for both SFT and RM ((c))."
}