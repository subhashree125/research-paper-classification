{
  "Abstract": "In this work, we develop a pipeline forhistorical-psychological text analysis in clas-sical Chinese. Humans have produced texts invarious languages for thousands of years; how-ever, most of the computational literature isfocused on contemporary languages and cor-pora.The emerging field of historical psy-chology relies on computational techniques toextract aspects of psychology from historicalcorpora using new methods developed in nat-ural language processing (NLP). The presentpipeline, called Contextualized Construct Rep-resentations (CCR), combines expert knowl-edge in psychometrics (i.e., psychological sur-veys) with text representations generated viaTransformer-based language models to mea-sure psychological constructs such as tradition-alism, norm strength, and collectivism in clas-sical Chinese corpora. Considering the scarcityof available data, we propose an indirect super-vised contrastive learning approach and buildthe first Chinese historical psychology corpus(C-HI-PSY) to fine-tune pre-trained models.We evaluate the pipeline to demonstrate its su-perior performance compared with other ap-proaches. The CCR method outperforms word-embedding-based approaches across all of ourtasks and exceeds prompting with GPT-4 inmost tasks. Finally, we benchmark the pipelineagainst objective, external data to further verifyits validity.1",
  "Our code and data are available at": ": Comparison of the best performance amongthe DDR, CCR, and prompting methods on three tasksin the C-HI-PSY test set. (STS: Semantic Textual Simi-larity, PM: Psychological Measure, QIC: QuestionnaireItem Classification) et al., 2021). The emerging field of historical psy-chology has been developed to understand howdifferent aspects of psychology vary over historicaltime and how the origins of our contemporary psy-chology are rooted in historical processes (Atariand Henrich, 2023; Muthukrishna et al., 2021; Bau-mard et al., 2024). Since we cannot access deadminds directly but can access their textual remains,natural language processing (NLP) is the primarymethod to extract aspects of psychology from his-torical corpora. Previous works, however, are oftenmonolingual and in English (Blasi et al., 2022).In addition, much of the literature at the intersec-tion of psychology and NLP has relied on bag-of-words or word embedding models, focusing onnon-contextual word meanings rather than a holis-tic approach to language modeling.Recently, more research attention in the NLPcommunity has been directed to historical and an-cient languages (Johnson et al., 2021), including",
  ": Pipeline for cross-lingual questionnaire conversion and contextualized construct representation": "but not limited to English (Manjavacas Arevalo andFonteyn, 2021), Latin (Bamman and Burns, 2020),ancient Greek (Yousef et al., 2022), and ancientHebrew (Swanson and Tyers, 2022). While allthese languages have historical significance, classi-cal Chinese is particularly important in the quantita-tive study of history. China has a long history span-ning thousands of years, largely recorded in clas-sical Chinese. The language served as a mediumfor expressing and disseminating influential philo-sophical and religious ideas. Confucianism, Dao-ism, and later Buddhism (through translations fromSanskrit) all found expression in classical Chinese,profoundly shaping Chinese thought, ethics, gov-ernance, and norms. As more resources becomereadily available for classical Chinese, scholarsof ancient China can test more specific hypothe-ses using computational methods (Liu et al., 2023;Slingerland, 2013; Slingerland et al., 2017). Due to its historical significance and geograph-ical coverage, classical Chinese represents one ofthe most important languages in historical psy-chology (Atari and Henrich, 2023). Prior workin social science has often relied on bag-of-wordsapproaches (Zhong et al., 2023) or bottom-uptechniques such as topic modeling (Slingerlandet al., 2017).In the NLP community, variousTransformer-based models for classical Chinesehave been developed (Tian et al., 2021; Wangand Ren, 2022; Yan and Chi, 2020; Wang et al.,2023a), primarily for tasks like punctuation pre- diction (Zhou et al., 2023), poem generation (Tianet al., 2021), and translation (Wang et al., 2023b).However, they have not been applied to theory-driven psychological text analysis for extractingpsychological constructs (e.g., moral values, norms,cultural orientation, mental health, religiosity, emo-tions, and thinking styles) from historical data. Transformer-based language models (Vaswaniet al., 2017) are crucial for psychological text anal-ysis because psychological constructs are oftencomplex, and sentence-level semantics (and above)will more effectively capture psychological mean-ings than isolated words (Demszky et al., 2023) ornon-contextual word embedding models (Kennedyet al., 2021). Here, we create a pipeline called Contextual-ized Construct Representation (CCR) for historical-psychological text analysis in classical Chinese. Al-though similar method has recently been developedfor contemporary psychological text analysis (Atariet al., 2023b), it can be adapted for historical NLP.Taking advantage of contextual language models,CCR does not require selecting a priori lists ofwords to represent a psychological construct (e.g.,the popular Linguistic Inquiry and Word Count pro-gram, Boyd et al., 2022); instead, making use ofthousands of existing questionnaires (which typi-cally include face-valid declarative sentences withwhich participants agree or disagree) that have beendeveloped and validated in psychology over the lastcentury. The pipeline of CCR for classical Chinese pro-ceeds in five steps: (1) selecting a questionnairefor the psychological construct of interest; (2) con-verting the questionnaire, usually in English, intoclassical Chinese; (3) representing questionnaireitems as embeddings using a contextual languagemodel; (4) generating the embedding of the tar-get text using a contextual language model; (5)computing the cosine similarity between the itemand text embeddings. This straightforward pipelineis particularly useful for social science, whereinresearchers are interested in interpretability andhypothesis testing.There are two main challenges of using the CCRpipeline in analyzing Chinese historical texts: (1)popular self-report questionnaires, widely acceptedby psychologists, are often in English, makingit difficult to align them with classical Chinesetexts; (2) there is a lack of psychology-specificTransformer-based models for classical Chinese,making it difficult to obtain high-quality represen-tations of Chinese historical texts. To address thefirst challenge, we propose a pipeline that usesa multilingual quotation recommendation model(Qi et al., 2022) to convert contemporary Englishquestionnaires into contextually meaningful clas-sical Chinese sentences (3.1). To tackle the sec-ond challenge, we build the first Chinese historicalpsychology corpus (C-HI-PSY) and introduce anapproach based on indirect supervision (He et al.,2021; Yin et al., 2023; Xu et al., 2023a) and con-trastive learning (Chopra et al., 2005; Schroff et al.,2015; Gao et al., 2021; Chuang et al., 2022) tofine-tune pre-trained models (3.2).",
  "Related Work": "Psychological Text AnalysisTextual corporacontain imprints of historical biases that can be cap-tured by machine-learning models (Caliskan et al.,2017; Garg et al., 2018). The changes in thesebiases can also serve as clues for understandingthe shifts in human sociality. Given the increasingamount of online textual data, many social scien-tists are turning to NLP to test their theories. Un-like in some computational fields, social scientiststraditionally give primacy to theory rather thanprediction (Yarkoni and Westfall, 2017). Hence,theory-driven text analysis is the first methodolog-ical choice in social sciences, including psychol-ogy (Jackson et al., 2021; Wilkerson and Casas,2017; Boyd and Schwartz, 2021). Given the impor- tance of theory development and hypothesis testing,many social scientists have developed dictionariesto assess psychological constructs as diverse asmoral values (Graham et al., 2009), stereotypes(Nicolas et al., 2021), polarization (Simchon et al.,2022), and threat (Choi et al., 2022). Distributed Dictionary Representation (DDR)Aiming to integrate psychological theories withthe capabilities of word embeddings, Garten et al.(2018) proposed the Distributed Dictionary Repre-sentation (DDR) as a top-down psychological text-analytic method. This method involves (a) defininga concise list of words by experts to capture a spe-cific concept, (b) using a word-embedding modelto represent these individual words, (c) computingthe centroid of these word representations to definethe dictionarys representation, (d) determining thecentroid of the word embeddings within a givendocument, and (e) assessing the cosine similaritybetween the dictionarys representation and that ofthe document. DDR has been a useful approach inmeasuring moral rhetoric (Wang and Inbar, 2021),temporal trends in politics (Xu et al., 2023b), andsituational empathy (Zhou et al., 2021). Sentence TransformerWhile BERT (Devlinet al., 2018) can identify sentences with similarsemantic meanings, this process can be resource-intensive. To enhance the performance of BERT-like models for tasks like semantic similarity as-sessments, Reimers and Gurevych (2019) devel-oped Sentence Transformer with a Siamese net-work structure, which outperforms conventionalmodels in tasks related to sentences and signifi-cantly reduces the time needed for computations.It is engineered to generate sentence embeddingsthat capture the core semantic content, ensuringthat sentences with comparable meanings are rep-resented by closely positioned embeddings in thevector space. Therefore, Sentence Transformer pro-vides an efficient and less computationally demand-ing method for evaluating semantic similarities be-tween sentences, making it particularly useful infields such as psychology (Juhng et al., 2023; Senet al., 2022).",
  "Cross-lingual Questionnaire Conversion": "In order to calculate semantic similarities betweenquestionnaires, typically in English, and the Chi-nese historical texts to be measured, typically inclassical Chinese, we introduce a novel workflowfor Cross-lingual Questionnaire Conversion (CQC).Considering the gaps between the two languagesand cultures, we employ quotations from authentichistorical texts, as they can integrate more naturallywithin the context of classical Chinese, instead ofrelying on translations.The process of converting a contemporary En-glish questionnaire Q into a classical Chinese ques-tionnaire Q is illustrated in the right panel of . For each questionnaire item (qi Q), the mul-tilingual quote recommendation model, QuoteR(Qi et al., 2022), which is trained on a dataset thatincludes English, modern Standard Chinese, andclassical Chinese, can identify a set of quotations{q}i in classical Chinese that are semantically sim-ilar to the English sentence qi. All the items areentered into the model for each questionnaire, re-sulting in a pool of corresponding quotations. Then,manual filtering is followed to eliminate quotationsof low quality, which can be either inappropriateor not explicitly relevant to the psychological con-struct. Ultimately, the most similar quotations qiare selected, substituting for every English qi toconstruct Q in classical Chinese.",
  "Indirect Supervised Contrastive Learning": "To obtain better psychology-specific representa-tions for CCR in Chinese historical texts, we in-troduce an indirect supervised contrastive learningapproach to finetune pre-trained Transformer-basedmodels, as shown in . A detailed exampleis shown in in the Appendix. Historical Psychology CorpusWe assemble arefined corpus named Chinese Historical Psychol-ogy Corpus (C-HI-PSY), which is comprised of21,539 paragraphs (S) from 667 distinct historicalarticles and book chapters in classical Chinese (Ta-ble 3 in the Appendix) that meet specific criteria.",
  ": Pipeline of triplet sampling and contrastivelearning. CLM stands for contextual language model": "Our selection criteria included (a) whether the arti-cle revolved around a theme related to psychologyand (b) whether the articles title revealed the maintheme. The titles of these works (T , |T | |S|),each carefully selected for their relevance to moralvalues, were labeled by the ancients for us and thuscan serve as pseudo for their topics, including (moral integrity), (filial piety and frater-nal duty), (utmost loyalty), (senseof shame), (pure and incorruptible), and (love oneself), among others.We divide our data into training, validation, andtesting sets, allocating 60%, 20%, and 20% of thedata to each set, respectively. The distribution ofparagraph lengths across different sets is consistent,as shown in in the Appendix. Pseudo Ground Truth from TitlesSince thetitle (ti T ) of a paragraph (si S) is a con-cise summary of the moral values reflected in theparagraph, the semantic similarity between titles,sim(ti, tj), can be considered as the pseudo groundtruth for the semantic similarity between corre-sponding paragraphs, sim(si, sj). The similaritybetween titles can be obtained by embedding thetitles via ET () and calculating their cosine similar-ity cos(ET (ti), ET (tj)). To perform word embed-ding on the titles, we trained five word vector mod-els on a large classical Chinese corpus containingover a billion tokens using different architectures,and picked the best-performing one (Appendix B). Positive and Negative SamplingWe calculatethe cosine similarities between the title embed-dings cos(ET (ti), ET (tj)), obtained through theword vector model, of all title pairs (the Cartesianproduct T T ) in the corpus. The distribution oftitle similarities is illustrated in in the Ap-pendix. We obtain positive and negative paragraphpairs by thresholding the similarities of title pairs.Paragraphs whose titles have similarities exceedingthe upper threshold +, as well as those with identi-cal titles, were identified as positive pairs (S S)+,that is,",
  "{(si, sj) | sim(ET (ti), ET (tj)) < }": "We experiment with several threshold settings,including 0.5th/99.5th, 1st/99th, 10th/90th, and25th/75th percentiles, on the C-HI-PSY valida-tion set using the base model bert-ancient-chinese(Wang and Ren, 2022). Our findings demonstratethat the 10th/90th percentile threshold yields thebest performance, see . Hence, for the fol-lowing experiments, if not specified, the thresholdsetting has been taken as 10th/90th. Triplet SamplingWe implement two strategies,random sampling and hard sampling, to con-struct triplets of anchor-positive-negative para-graphs (sA, s+A, sA) from the training set. In ran-dom sampling, we select one positive instance s+Aand one negative instance sA randomly from the re-spective positive pairs (sAS)+ and negative pairs(sA S) of the anchor sA. In hard sampling, weutilize the pre-trained model f(), which is laterfine-tuned on these triplets, to embed paragraphsand calculate cosine similarities between the pos-itive and negative pairs as cos(f(sA), f(s+/A)).For the positive instance, we choose the paragraphwith the lowest similarity to the anchor from itspositive pairs, that is,",
  "(sA, s) (sA S)}": "To prevent the model from over-fitting, we ensurethat each paragraph is used as an anchor only once,applying this rule across both random and hardsampling strategies. We also compare the two sam-pling procedures in with respect to eachpositive-negative splitting threshold. We find thatthe random sampling procedure is better than hardsampling ever since the threshold is higher/lowerthan 0.5th/99.5th; we note that the case could bedue to the noise inevitably caused by the indirect su-pervised learning approach, which drove the hardsampling procedure to fail at finding helpful in-stances (see Limitation). Fine-tuning with Contrastive LearningWefine-tune several pre-trained Transformer-basedmodels (Wang and Ren, 2022; Yan and Chi, 2020;Reimers and Gurevych, 2019; Xu, 2023) on theC-HI-PSY training set, using a triplet loss function(Schroff et al., 2015),",
  "sASmax{D+ D + , 0}": "where denotes the pre-trained weights to befine-tuned, D+ denotes the distance between thepositive pair, i.e.f(sA) f(s+A)2, D de-notes the distance between the negative pair, i.e.f(sA) f(sA)2, and stands for a marginbetween positive and negative pairs to ensure thatthe model does not trivially satisfy the conditionby making the embeddings of the anchor, positive,and negative samples equal to each other. Thisloss function aims to minimize the squared Eu-clidean norm between the anchor and positive, andmaximize the squared Euclidean norm between theanchor and negative.We construct paragraph pairs from the C-HI-PSY validation set through random sampling tovalidate the models during training, using the sim-ilarities between titles as pseudo ground truth togauge the similarities between paragraphs. We per-form a hyperparameter sweep (), to selectthe best-performing configuration for each model,as shown in in the Appendix. : Comparison of model performance using theCCR method on the three tasks in the C-HI-PSY test setbefore and after fine-tuning. (Model A: bert-ancient-chinese, B: guwenbert-base, C: guwenbert-large, D:paraphrase-multilingual-MiniLM-L12-v2, E: text2vec-base-chinese, F: text2vec-base-chinese-paraphrase, G:text2vec-large-chinese)",
  "Semantic Understanding": "Understanding of Historical Text: Semantic Tex-tual SimilarityFor the CCR method, we embedwhole paragraphs with Sentence Transformer mod-els, and then calculate the cosine similarity betweeneach pair of paragraphs. For the DDR method, weaverage the word vectors of all the words in theparagraph, and then calculate the cosine similaritybetween each pair of paragraphs. For the LLM-prompting method, we craft a few-shot prompt(Brown et al., 2020; Si et al., 2023) ( inthe Appendix) asking for a similarity score, rangingfrom 0 to 1, between each pair of paragraphs. Asmentioned, similarities between the titles of eachpair of paragraphs are used as the pseudo groundtruth.We construct paragraph pairs for evaluation fromthe C-HI-PSY test set using two sampling methods:(1) random sampling, where paragraphs are ran-domly paired, and (2) threshold sampling, whichpairs paragraphs with either positive or negativesamples based on a specific threshold (10th/90th).Threshold sampling produces distinctly positiveor negative pairs; thus, we refer to it as the EasyTask. Conversely, random sampling can result inambiguous pairs, making for a more challengingHard Task. Understanding of Questionnaire Item: TextClassificationWe convert several broadly ac-cepted questionnaires from English into classi-cal Chinese, including Collectivism, Individual-ism (Oyserman, 1993), Norm Tightness and NormLooseness (Gelfand et al., 2011), by employing theCQC approach described in 3.1. A good classifica-tion of items from different questionnaires, e.g. Iaccept the decisions made by my group from Col-lectivism questionnaire, and The best decisionsare the ones I make on my own from Individu-alism questionnaire, can demonstrate the modelsability to understand and distinguish the psycholog-ical constructs represented by these questionnaires.For both the CCR and DDR methods, all theitems from these questionnaires are embedded.Then we conduct 10-fold cross-validation, usingSupport Vector Machines (SVM) as the classifier,and text embeddings or averaged word vectors as",
  "Psychological Measure": "For the CCR method, we calculate the average co-sine similarity between each paragraph in the C-HI-PSY test set and all the items in the questionnaire,representing the loading score of the paragraphon the questionnaire. For the DDR method, webuild a corresponding dictionary for each psycho-logical construct (Appendix C), and calculate thecosine similarity between the centroid of words ineach paragraph and the centroid of words in thedictionary. For the prompting method, we craft afew-shot prompt ( in the Appendix) torequest a score from 0 to 1 for each paragraph oneach questionnaire. Items in each questionnaire areprovided in the prompt. Average similarities be-tween the title of each paragraph and all the wordsin the dictionary, calculated by the word vectormodel, are used as the pseudo ground truth.",
  "Results": "For the Semantic Textual Similarity (STS) task, weevaluate the DDR and CCR methods through a rig-orous process involving 20 rounds of random sam-pling. In each round, 4,308 random paragraph pairsare constructed from the C-HI-PSY test set. Aftercompleting these 20 evaluations, we calculate theaverage scores along with standard errors. Whenevaluating the prompting method, due to the highcosts, we only conduct a single round of randomsampling. For the Questionnaire Item Classifica-tion (QIC) task, we utilize 60 items from question-naires on Collectivism, Individualism (Oyserman,1993), Norm Tightness and Looseness (Gelfandet al., 2011), selecting 15 items from each ques-tionnaire. For the Psychological Measure (PM)task, we measure the loading scores of all 4,308paragraphs in the C-HI-PSY test set across the ques-tionnaires mentioned above, and report the averagescores along with standard errors. illustrates that the performance metricsof almost all pre-trained models (Wang and Ren,2022; Yan and Chi, 2020; Reimers and Gurevych,2019; Xu, 2023) in the CCR baseline have sub-stantially improved after fine-tuning. As shown in, the CCR method outperforms the DDRmethod across all tasks and surpasses the prompt-ing method with GPT-4 (version 2024-04-09) in",
  "To address the lack of benchmark datasets relatedto psychological measurement in classical Chinese,we further validate the effectiveness of the CCRmethod using externally annotated data": "Officials Attitudes toward ReformsMoral val-ues and political orientations are closely inter-twined (Federico et al., 2013; Kivikangas et al.,2021). For example, the attitude of individualstoward reforms, policy changes, and new legisla-tion often reflects traditionalism, conservatism, andrespect for authority (Hackenburg et al., 2023; Kol-eva et al., 2012). Those with stronger traditionalistviews are more likely to identify with the existingsocial order and resist changes to the status quo(Osborne et al., 2023; Jost and Hunyady, 2005).Throughout Chinese history, there have been nu-merous instances of significant reforms, one of themost notable of which being the Wang AnshisNew Policies (Anderson, 2001) in the 11th century,which faced mixed reactions from officials. Wedraw upon a dataset manually compiled by Wang(2022), who annotated the attitudes of 137 majorofficials toward the reform. Unlike data easily in-fluenced by annotators subjectivity (Davani et al.,2022), officials attitudes toward reforms are ob-jective and less controversial, making them highlysuitable for benchmarking our pipeline. Measure of Traditionalism and AuthorityWeextract writings of officials documented in the Com-plete Prose of the Song Dynasty (Zeng and Liu,2006). Questionnaires of traditionalism (Samoreet al., 2023) and authority (Atari et al., 2023a) areconverted from English into classical Chinese, byemploying the CQC approach (3.1). Employingthe best-performing fine-tuned model, we use ourCCR pipeline to measure the levels of tradition-alism and authority expressed in texts. For eachindividual official, results are aggregated by calcu-lating the average score of corresponding writings. ResultsWe find a significant negative correlation() between officials attitudes toward thereforms and the levels of traditionalism and author-ity measured through CCR. Officials with greatertraditionalism and respect for existing authority are",
  ": Performance on the test set across three tasks using three methods: DDR, LLM Promping, and CCR": ": Correlation between traditionalism, author-ity and officials attitudes toward reforms. (a) and (c)present the average psychological measure scores withstandard errors, using an ordinal variable where -1 signi-fies opposition to the reform, 0 indicates a neutral or noexplicit attitude, and 1 denotes support for the reform(N = 108). (b) and (d) depict the linear regression linesaccompanied by 95% confidence intervals, employing acontinuous variable that ranges from 0 to 1 to quantifyofficials degree of support for the reform (N = 56).",
  ": Spearman correlation between CCR-based mea-sure of moral values and actual attitude toward reformof officials. *p < 0.05, **p < 0.01, ***p < 0.001": "more likely to oppose reform (), which isin line with the theoretical assumptions.We also conduct Ordinary Least Squares (OLS)regressions of officials attitudes toward reformson traditionalism and authority. To ensure robust-ness, we include the officials rank as a controlvariable and the fixed effects of the officials home-towns. The results ( in the Appendix) showthat psychological indicators of traditionalism andauthority measured through CCR, can be used topredict the corresponding officials attitudes towardreforms, proving the effectiveness of our method.This benchmarking against historically verifieddata supports the validity of CCR as a valid compu-tational pipeline to extract meaningful psychologi-cal information from classical Chinese corpora.",
  "Discussion and Conclusion": "Historical-psychological text analysis is a new lineof research focused on extracting different aspectsof psychology from historical corpora using state-of-the-art computational methods (Atari and Hen-rich, 2023). Here, we create a new pipeline, CCR,as a helpful tool for historical-psychological textanalysis. Evaluating our model against word em-bedding models (e.g., DDR) and more recent LLMs(e.g., GPT-4), we demonstrate that CCR performsbetter than these alternatives while keeping its highlevel of interpretability and flexibility. ClassicalChinese is of great historical significance, and theproposed approach can be particularly helpful intesting new insights about the dead minds wholived centuries or even millennia prior. We hopeour tool motivates future work at the intersectionsof psychology, quantitative history, and NLP. Im-portantly, benchmarking historical-psychologicaltools, especially in ancient languages, is difficultbecause obtaining ground truth is challenging anddependent upon the quality of historical data. Thatsaid, we validate CCR against a historically veri-fied knowledge base about attitudes toward reformand traditionalism.",
  "Limitation": "Due to the lack of fine-grained data available fortraining in the context of classical Chinese and withhistorical-psychological texts, we propose an indi-rect supervised learning approach where the simi-larities between titles are used as the pseudo groundtruth for similarities between paragraphs. This ap-proach may lead to the model learning some noisefrom the data, negatively affecting the models per-formance in downstream tasks, but it can save asignificant amount of resources, especially for low-resource ancient languages.Our experiments show that hard sampling iscounterintuitively worse than random sampling onour dataset (). This is the case becausealthough the title of a text represents the main ideaof most of the content, there may still be parts ofthe text that are unrelated to the title. For example,in a pair of paragraphs that are identified as positivesamples due to their highly similar titles, one para-graph might be irrelevant to the title. Consequently,the text similarity calculated after embedding by apre-trained model might not be high for this pairof paragraphs. The difference between the similar-ity prediction made by the pre-trained model and the pseudo ground truth based on title similaritymay result in these paragraph pairs being identi-fied as hard samples. However, in such cases, thepre-trained models prediction could be more ac-curate than the pseudo ground truth derived fromtitle similarity. It is the noise caused by the indirectsupervised approach that makes the hard samplingfail to find helpful instances.Additionally, it is worth mentioning that, due tothe unavoidable survivorship bias in existing histor-ical texts, we may only have access to texts that arephysically accessible for analysis. Moreover, thepsychological indicators measured from historicalcorpora may not represent the psychological stateof the majority of people living in a specific pe-riod and location, but rather the psychological stateof the specific class who wrote these documents,often the elite and intellectuals. We propose anobjective measurement approach, but the resultsobtained from this method need to be subjected tomore rigorous benchmarking and statistical analy-sis, taking into account population characteristicsand recording efforts. The findings should be inter-preted based on the domain knowledge of histori-ans and psychologists.Our future efforts will be directed toward assem-bling datasets with expert annotations to addressthese issues. Moreover, we aim to contribute toboth historical psychology and NLP by compilingnew open-source datasets for benchmarking pur-poses.",
  "Ryan L Boyd, Ashwini Ashokkumar, Sarah Seraj, andJames W Pennebaker. 2022. The development andpsychometric properties of liwc-22. Austin, TX: Uni-versity of Texas at Austin, pages 147": "Ryan L Boyd and H Andrew Schwartz. 2021. Natu-ral language analysis and the psychology of verbalbehavior: The past, present, and future states of thefield. Journal of Language and Social Psychology,40(1):2141. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020.Language models are few-shot learners.In Ad-vances in Neural Information Processing Systems,volume 33, pages 18771901. Curran Associates,Inc.",
  "Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan.2017. Semantics derived automatically from lan-guage corpora contain human-like biases. Science,356(6334):183186": "Virginia K Choi, Snehesh Shrestha, Xinyue Pan, andMichele J Gelfand. 2022. When danger strikes: A lin-guistic tool for tracking americas collective responseto threats. Proceedings of the National Academy ofSciences, 119(4):e2113891119. S. Chopra, R. Hadsell, and Y. LeCun. 2005. Learninga similarity metric discriminatively, with applicationto face verification. In 2005 IEEE Computer SocietyConference on Computer Vision and Pattern Recog-nition (CVPR05), volume 1, pages 539546 vol. 1. Yung-Sung Chuang, Rumen Dangovski, Hongyin Luo,Yang Zhang, Shiyu Chang, Marin Soljacic, Shang-Wen Li, Scott Yih, Yoon Kim, and James Glass. 2022.DiffCSE: Difference-based contrastive learning forsentence embeddings. In Proceedings of the 2022Conference of the North American Chapter of theAssociation for Computational Linguistics: Human",
  "Language Technologies, pages 42074218, Seattle,United States. Association for Computational Lin-guistics": "Aida Mostafazadeh Davani, Mark Daz, and Vinodku-mar Prabhakaran. 2022. Dealing with disagreements:Looking beyond the majority vote in subjective an-notations. Transactions of the Association for Com-putational Linguistics, 10:92110. Dorottya Demszky, Diyi Yang, David S Yeager, Christo-pher J Bryan, Margarett Clapper, Susannah Chand-hok, Johannes C Eichstaedt, Cameron Hecht, JeremyJamieson, Meghann Johnson, et al. 2023. Using largelanguage models in psychology. Nature Reviews Psy-chology, 2(11):688701.",
  "Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021": "SimCSE: Simple contrastive learning of sentence em-beddings. In Proceedings of the 2021 Conferenceon Empirical Methods in Natural Language Process-ing, pages 68946910, Online and Punta Cana, Do-minican Republic. Association for ComputationalLinguistics. Nikhil Garg, Londa Schiebinger, Dan Jurafsky, andJames Zou. 2018. Word embeddings quantify 100years of gender and ethnic stereotypes. Proceedingsof the National Academy of Sciences, 115(16):E3635E3644. Justin Garten, Joe Hoover, Kate M Johnson, ReihaneBoghrati, Carol Iskiwitch, and Morteza Dehghani.2018. Dictionaries and distributions: Combiningexpert knowledge and large scale textual data con-tent analysis: Distributed dictionary representation.Behavior research methods, 50:344361. Michele J. Gelfand, Jana L. Raver, Lisa Nishii,Lisa M. Leslie, Janetta Lun, Beng Chong Lim,Lili Duan, Assaf Almaliach, Soon Ang, JakobinaArnadottir, Zeynep Aycan, Klaus Boehnke, PawelBoski, Rosa Cabecinhas, Darius Chan, JagdeepChhokar, Alessia DAmato, Montserrat Subirats Fer-rer, Iris C. Fischlmayr, Ronald Fischer, Marta Flp,James Georgas, Emiko S. Kashima, YoshishimaKashima, Kibum Kim, Alain Lempereur, PatriciaMarquez, Rozhan Othman, Bert Overlaet, PennyPanagiotopoulou, Karl Peltzer, Lorena R. Perez-Florizno, Larisa Ponomarenko, Anu Realo, VidarSchei, Manfred Schmitt, Peter B. Smith, NazarSoomro, Erna Szabo, Nalinee Taveesin, MidoriToyama, Evert Van de Vliert, Naharika Vohra,",
  "Jesse Graham, Jonathan Haidt, and Brian A Nosek.2009. Liberals and conservatives rely on differentsets of moral foundations. Journal of personality andsocial psychology, 96(5):1029": "Kobi Hackenburg, William J Brady, and Manos Tsakiris.2023. Mapping moral language on us presidentialprimary campaigns reveals rhetorical networks of po-litical division and unity. PNAS nexus, page pgad189. Hangfeng He, Mingyuan Zhang, Qiang Ning, and DanRoth. 2021. Foreseeing the benefits of incidentalsupervision. In Proceedings of the 2021 Conferenceon Empirical Methods in Natural Language Process-ing, pages 17821800, Online and Punta Cana, Do-minican Republic. Association for ComputationalLinguistics. Joshua Conrad Jackson, Joseph Watts, Johann-MattisList, Curtis Puryear, Ryan Drabble, and Kristen A.Lindquist. 2021. From text to thought: How analyz-ing language can advance psychological science. Per-spectives on Psychological Science, 17(3):805826. Kyle P. Johnson, Patrick J. Burns, John Stewart, ToddCook, Clment Besnier, and William J. B. Mattingly.2021. The Classical Language Toolkit: An NLPframework for pre-modern languages. In Proceed-ings of the 59th Annual Meeting of the Association forComputational Linguistics and the 11th InternationalJoint Conference on Natural Language Processing:System Demonstrations, pages 2029, Online. Asso-ciation for Computational Linguistics.",
  "Enrique Manjavacas Arevalo and Lauren Fonteyn. 2021": "MacBERTh: Development and evaluation of a histor-ically pre-trained language model for English (1450-1950). In Proceedings of the Workshop on NaturalLanguage Processing for Digital Humanities, pages2336, NIT Silchar, India. NLP Association of India(NLPAI). Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean. 2013. Efficient estimation of word representa-tions in vector space. In 1st International Conferenceon Learning Representations, ICLR 2013, Scottsdale,Arizona, USA, May 2-4, 2013, Workshop Track Pro-ceedings.",
  "Daphna Oyserman. 1993.The lens of personhood:Viewing the self and others in a multicultural so-ciety. Journal of Personality and Social Psychology,65(5):9931009": "Jeffrey Pennington, Richard Socher, and ChristopherManning. 2014. GloVe: Global vectors for wordrepresentation. In Proceedings of the 2014 Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP), pages 15321543, Doha, Qatar.Association for Computational Linguistics. Fanchao Qi, Yanhui Yang, Jing Yi, Zhili Cheng,Zhiyuan Liu, and Maosong Sun. 2022. QuoteR: Abenchmark of quote recommendation for writing. InProceedings of the 60th Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 336348, Dublin, Ireland. Asso-ciation for Computational Linguistics. Fanchao Qi, Lei Zhang, Yanhui Yang, Zhiyuan Liu, andMaosong Sun. 2020. Wantwords: An open-sourceonline reverse dictionary system. In Proceedings ofthe 2020 Conference on Empirical Methods in Nat-ural Language Processing: System Demonstrations,pages 175181. Nils Reimers and Iryna Gurevych. 2019.Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processingand the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), pages39823992, Hong Kong, China. Association for Com-putational Linguistics. Theodore Samore, Daniel M. T. Fessler, Adam MaxwellSparks, Colin Holbrook, Lene Aare, Carmen Glo-ria Baeza, Mara Teresa Barbato, Pat Barclay, Re-natas Berniunas, Jorge Contreras-Garduo, BernardoCosta-Neves, Maria del Pilar Grazioso, Pnar El-mas, Peter Fedor, Ana Maria Fernandez, ReginaFernndez-Morales, Leonel Garcia-Marques, PaulinaGiraldo-Perez, Pelin Gul, Fanny Habacht, YoussefHasan, Earl John Hernandez, Tomasz Jarmakowski,Shanmukh Kamble, Tatsuya Kameda, Bia Kim,Tom R. Kupfer, Maho Kurita, Norman P. Li, Jun-song Lu, Francesca R. Luberti, Mara Andre Maegli,Marins Mejia, Coby Morvinski, Aoi Naito, Al-ice Nganga, Anglica Nascimento de Oliveira,Daniel N. Posner, Pavol Prokop, Yaniv Shani, Wal-ter Omar Paniagua Solorzano, Stefan Stieger, An-gela Oktavia Suryani, Lynn K. L. Tan, Joshua M.Tybur, Hugo Viciana, Amandine Visine, Jin Wang,and Xiao-Tian Wang. 2023. Greater traditionalismpredicts covid-19 precautionary behaviors across 27societies. Scientific Reports, 13(1). Florian Schroff, Dmitry Kalenichenko, and JamesPhilbin. 2015. Facenet: A unified embedding forface recognition and clustering. In Proceedings ofthe IEEE conference on computer vision and patternrecognition, pages 815823. Indira Sen, Daniele Quercia, Marios Constantinides,Matteo Montecchi, Licia Capra, Sanja Scepanovic,and Renzo Bianchi. 2022. Depression at work: ex-ploring depression in major us companies from on-line reviews. Proceedings of the ACM on Human-Computer Interaction, 6(CSCW2):121. Chenglei Si, Zhe Gan, Zhengyuan Yang, ShuohangWang, Jianfeng Wang, Jordan Boyd-Graber, and Li-juan Wang. 2023. Prompting gpt-3 to be reliable. InInternational Conference on Learning Representa-tions (ICLR).",
  "Edward Slingerland. 2013. Body and mind in earlychina: An integrated humanitiesscience approach.Journal of the American Academy of Religion,81(1):655": "Edward Slingerland, Ryan Nichols, Kristoffer Neilbo,and Carson Logan. 2017. The distant reading ofreligious texts: A big data approach to mind-bodyconcepts in early china. Journal of the AmericanAcademy of Religion, 85(4):9851016. Daniel Swanson and Francis Tyers. 2022. Handlingstress in finite-state morphological analyzers for An-cient Greek and Ancient Hebrew. In Proceedings ofthe Second Workshop on Language Technologies forHistorical and Ancient Languages, pages 108113,Marseille, France. European Language ResourcesAssociation. Huishuang Tian, Kexin Yang, Dayiheng Liu, andJiancheng Lv. 2021. Anchibert: A pre-trained modelfor ancient chinese language understanding and gen-eration. In 2021 International Joint Conference onNeural Networks (IJCNN), pages 18. IEEE. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processingsystems, 30. Dongbo Wang, Chang Liu, Zhixiao Zhao, Si Shen, LiuLiu, Bin Li, Haotian Hu, Mengcheng Wu, Litao Lin,Xue Zhao, and Xiyu Wang. 2023a. Gujibert andgujigpt: Construction of intelligent information pro-cessing foundation language models for ancient texts.Preprint, arXiv:2307.05354. Jiahui Wang, Xuqin Zhang, Jiahuan Li, and Shu-jian Huang. 2023b. Pre-trained model in Ancient-Chinese-to-Modern-Chinese machine translation. InProceedings of ALT2023: Ancient Language Trans-lation Workshop, pages 2328, Macau SAR, China.Asia-Pacific Association for Machine Translation. Pengyu Wang and Zhichen Ren. 2022. The uncertainty-based retrieval framework for ancient chinese cwsand pos. In Proceedings of the Second Workshop onLanguage Technologies for Historical and AncientLanguages, pages 164168.",
  "Tal Yarkoni and Jacob Westfall. 2017. Choosing predic-tion over explanation in psychology: Lessons frommachine learning. Perspectives on PsychologicalScience, 12(6):11001122": "Wenpeng Yin, Muhao Chen, Ben Zhou, Qiang Ning,Kai-Wei Chang, and Dan Roth. 2023.Indirectlysupervised natural language processing. In Proceed-ings of the 61st Annual Meeting of the Association forComputational Linguistics (Volume 6: Tutorial Ab-stracts), pages 3240, Toronto, Canada. Associationfor Computational Linguistics. Tariq Yousef, Chiara Palladino, David J. Wright, andMonica Berti. 2022. Automatic translation align-ment for Ancient Greek and Latin. In Proceedings ofthe Second Workshop on Language Technologies forHistorical and Ancient Languages, pages 101107,Marseille, France. European Language ResourcesAssociation.",
  "Lei Zhang, Fanchao Qi, Zhiyuan Liu, Yasheng Wang,Qun Liu, and Maosong Sun. 2020. Multi-channelreverse dictionary model. In Proceedings of the AAAIConference on Artificial Intelligence, pages 312319": "Ying Zhong, Valentin Thouzeau, and Nicolas Baumard.2023. The evolution of romantic love in chinesefiction in the very long run (618 - 2022): A quan-titative approach. In Workshop on ComputationalHumanities Research. Bo Zhou, Qianglong Chen, Tianyu Wang, XiaomiZhong, and Yin Zhang. 2023. WYWEB: A NLPevaluation benchmark for classical Chinese. In Find-ings of the Association for Computational Linguis-tics: ACL 2023, pages 32943319, Toronto, Canada.Association for Computational Linguistics.",
  "A.1Distribution of Paragraph Lengths": "To ensure the inclusion of sufficient semantic in-formation, paragraphs containing fewer than 50characters have been merged with the precedingparagraph of the article or chapter, wherever pos-sible. To accommodate the token limitations ofmodels such as BERT, paragraphs that exceed 500characters have been divided into segments withfewer than 500 characters each, while maintainingthe integrity of the original sentence structure asmuch as possible. The average length of paragraphsis 195 characters.",
  "CDictionary Details": "We build a dictionary for each classical Chinesequestionnaire by using an open-source dictionarysystem named WantWords (Qi et al., 2020),which is based on a multi-channel reverse dictio-nary model (MRDM) (Zhang et al., 2020) andtakes sentences (descriptions of words) as inputand yields words semantically matching the inputsentences.The process involves three steps: (1) we employthe WantWords model to obtain the top n mostsimilar words to each sentence in the questionnaire;(2) a process of deduplication is then conducted; (3)the words are labeled manually by a native Chinesespeaker with relevant or irrelevant to the corre-sponding topic, after which all irrelevant words arediscarded.",
  "TextTitleSource": "It is said: \"When a subject serves his ruler, a son serves his father,and a wife serves her husband, if all these are in harmony, then theworld is in order. If they are contrary, the world is in chaos. This isthe eternal way of the world, and wise kings and virtuous ministersdo not deviate from it.\" Even if a ruler is unworthy, a subject daresnot encroach.",
  "Han Feizi": "A gentleman considers a woman of virtue and righteousness tohold loyalty and kindness above all else. To value righteousnessand morality over life and death is the act of the noble. Confuciussaid: \"A gentleman would die to achieve righteousness, withoutseeking to live at the expense of righteousness.\" This is what itmeans. The ode says: \"In the capital, a woman of principle, whenbandits threatened her father, demanded her in exchange, she couldnot refuse. When the arrangement was made, she died to upholdrighteousness, her virtue renowned under the heavens.\"",
  "Biographies ofExemplaryWomen": "Kang Sengyuan in Yuzhang, some tens of li away from the settle-ment, established a hermitage. Beside a ridge, along a long river,fragrant forests lined the pavilion, and clear streams rushed bythe hall. He then lived in seclusion to study and lecture, seekingthe essence of philosophy, attracting many, including Yu Gong, tovisit. Observing his breathing techniques and the transformationof his demeanor, his grace and elegance stood out. Living in suchcontentment, he found fulfillment.",
  "Observations1085610856": ": Ordinary Least Squares (OLS) regressions of officials towards reforms on CCR-based measures oftraditionalism and authority, with control variable of official rank and fixed effects for officials hometown provinces.Models 1 and 3 use an ordinal variable for officials attitudes towards reform, where -1 signifies opposition to thereform, 0 indicates a neutral or no explicit stance, and 1 denotes support for the reform. Models 2 and 4 use acontinuous variable that ranges from 0 to 1 to quantify the degree of officials support for the reform. *p < 0.05, **p< 0.01, ***p < 0.001"
}