{
  "Abstract": "Large language models demonstrate impres-sive reasoning abilities but struggle to providepersonalized content due to their lack of in-dividual user preference information. Exist-ing methods, such as in-context learning andparameter-efficient fine-tuning, fall short in cap-turing the complexity of human preferences,especially given the small, personal datasetsindividuals possess. In this paper, we propose anovel approach utilizing small parameter mod-els as preference agents to generate naturallanguage rules that guide a larger, pre-trainedmodel, enabling efficient personalization. Ourmethod involves a small, local \"steering wheel\"model that directs the outputs of a much largerfoundation model, producing content tailoredto an individuals preferences while leverag-ing the extensive knowledge and capabilities ofthe large model. Importantly, this personaliza-tion is achieved without the need to fine-tunethe large model. Experimental results demon-strate that our technique significantly outper-forms baseline personalization methods. Byallowing foundation models to adapt to individ-ual preferences in a data and compute-efficientmanner, our approach paves the way for highlypersonalized language model applications.",
  "Introduction": "Large Language Models have revolutionized vari-ous domains with their impressive capabilities inreasoning, knowledge retrieval, and content gener-ation. However, a crucial aspect where they oftenfall short is the ability to adapt their outputs to in-dividual user preferences. While LLMs excel atgenerating content that caters to a broad audience,they struggle to produce outputs that resonate withthe unique stylistic and contextual needs of indi-vidual users (Wozniak et al., 2024). This limitationstems from the fact that LLMs are typically trainedon massive, generic datasets, promoting neutralityand hindering their capacity to learn and adapt toindividual nuances (Berliner et al., 2016). Existing personalization techniques, such as in-context learning (ICL) (Brown et al., 2020) andparameter-efficient fine-tuning (PEFT) (Hu et al.,2021; Dettmers et al., 2023), have shown promisein adapting LLMs to specific tasks. However, thesemethods encounter significant challenges when ap-plied to the domain of human preference learning.ICL, while effective in providing general guidance,struggles to capture the intricate and often contra-dictory nature of human preferences, especiallywhen relying on limited in-context examples (Penget al., 2023). Similarly, PEFT methods, while effi-cient in terms of compute and storage, face difficul-ties in generalizing to new preferences, particularlywhen users only possess small, personal datasets(Balne et al., 2024). This raises a fundamental ques-tion: how can we efficiently and effectively alignpowerful LLMs to individual user preferences,especially when personalized data is scarce?To address this challenge, we propose a novelapproach based on the concept of preference agents.These agents are small, locally trainable languagemodels designed to learn and encode individualuser preferences into concise natural language rules.These agents act like a small \"steering wheel,\" guid-ing the output of a much larger, generic LLM to-wards the desired personalized style and content.This modular architecture decouples the task ofpreference learning from the generic LLM, allow-ing users to efficiently fine-tune a small agent ontheir personal data without the need for modifyingthe weights of the larger model.Our approach represents a significant depar-ture from conventional preference learning meth-ods, offering a potential solution for unsuper-vised human preference learning. We evaluate ourmethod across three diverse datasets encompass-ing human-generated content: emails, news articlesand product reviews. Our results demonstrate thatpreference-guided LLMs significantly outperformboth fine-tuning baselines and standard prompting",
  "We seek to enable users to generate high qual-ity, personalized responses as our goal, which arebounded by some constraints and assumptions:": "Constraint 1: The size of the dataset D is notlarge enough to permit effective full parametermodel fine-tuning. Given that individual userstypically possess small, personal datasets, itis impractical to expect these datasets to besufficient for extensive fine-tuning of a largelanguage model. Constraint 2: The small model, denoted asMS, must be lightweight enough to operate(w.r.t both training and inference) on lowerpower end-user devices. This requirementensures that users can generate and applytheir preferences without the need for high-performance computing resources. This al-lows for local inference, making the person-alization process more accessible and conve-nient. Constraint 3: The large model, referred toas ML, is either too large to run inferencelocally or is a closed-source API model. Con-sequently, it is not feasible, or cost effectiveto fine-tune or align ML by altering its modelweights.",
  "P = ML(Yz, G)(2)": "Here, P represents the set of preference rulesderived for each training example. We hypothesizethat ML can effectively identify these rules withoutprior knowledge of the specific users preferences,just by observing the differences between the zeroshot completion and the ground truth.Finally, we train the smaller model, MS, to learnto generate these preference rules. The trainingdata for MS consists of input-preference rule pairs:",
  "Quantifying Alignment": "We utilize an evaluation function Eval(ya, yz|x) onan unseen test set T . For each example in T , thefunction compares the preference-aligned outputya with the zero-shot output yz generated by MLwithout preference rules. The evaluation functionassigns a score indicating the preference betweenya and yz, given the input x. A positive scoreindicates a preference for the aligned output ya,suggesting better alignment with the users likelypreference, while a negative score favors the zero-shot output. We aggregate these scores across allexamples in T to obtain an overall alignment score:",
  "Model Choice": "We employ Llama-3-8B-Instruct as our smaller,locally inferrable model (MS) due to its strong ca-pabilities and suitability for QLoRA fine-tuning onconsumer hardware (Dettmers et al., 2023). For ourlarger reasoning model (ML), we utilize Llama-3-70B-Instruct (AI@Meta, 2024), Claude 3.5 Son-net (Anthropic, 2024), and Gemini 1.5 Pro (Teamet al., 2024). This diverse selection allows us toevaluate our approach with both a strong open-source model (Llama-3-70B-Instruct) and power-ful closed-source API models (Claude 3.5, Gemini1.5), to demonstrate generalizability.",
  "Datasets": "Our evaluation spans three datasets encompassingsingle and multi-user preference information:Enron Email Corpus. For evaluating short-form writing personalization, we utilize the Enronemail corpus (Klimt and Yang, 2004), comprisingemails from approximately 150 users, primarilysenior management at Enron. We sample 15 usersto analyze the reproducibility of individual writing styles. Each users subset is split into an 80-20train-test split.New Yorker. To assess performance on long-form creative writing, we employ a subset of theAll the News 2.0 dataset (Thompson, 2020), specifi-cally articles from The New Yorker magazine. Thissubset, containing approximately 3,500 articles,provides a rich source of author preference infor-mation. We investigate whether our preferenceagents can reproduce the unique style of The NewYorker using natural language rules. We split thisdataset into a 50-50 train-test split.Amazon Review Subset (LAMP 3U). To fur-ther assess the generalizability of our frameworkbeyond long-form content, we incorporate theLAMP 3U dataset (Salemi et al., 2024), whichconsists of Amazon product reviews grouped byuser. We sampled 15 random users from LAMP3U and then generated review intents and rules foreach user following the same process employed forthe Enron dataset.Refer to Appendix A for details regarding datasetpreparation and sampling.",
  "Dataset Augmentation": "3.3.1Synthetic Intent GenerationWe aimed to develop a fully unsupervised approachthat avoids manual collection of human intents,which can be time-consuming and costly.Toachieve this, we leveraged the large language model(ML) to automatically extract the core content ofeach email or article into bullet points, emulatinguser input or intent. To ensure the quality of thesesynthetic intents, we randomly sampled a subset and subjected them to manual human evaluation.Our findings indicated a high degree of fidelity,with over 95% of the synthetic intents rated highlyby humans. To introduce variance and simulatedifferent user styles, we generated three intent vari-ants for each data point at temperatures of 0.7,1.0, and 1.2. These intents were then randomlysampled to create intent-annotated versions of ourdatasets. Examples of generated intents can befound in Appendix B.3. 3.3.2Rule GenerationTo capture the nuanced stylistic preferences inher-ent in our datasets, we employed the large rea-soning model (ML) to generate natural languagepreference rules. These rules distill the essence ofthe desired style by highlighting the discrepanciesbetween a zero-shot baseline generated by ML andthe ground truth email or article.We investigated three distinct strategies for gen-erating these preference rules: Distillation-Based Rule Generation: Thisapproach leverages a distillation process. MLfirst generates a zero-shot baseline responsefor the given input. By analyzing the differ-ences between this baseline and the groundtruth, the model identifies missing stylistic andpreference elements and generates targetedrules to bridge the gap. Direct Rule Generation: We also explore di-rectly prompting ML to generate rules basedsolely on the ground truth email or article.This approach, while simpler, lacks the tar-geted feedback mechanism inherent in thedistillation-based method. Rule Generation with Thinking Tokens:To further enhance the rule generation pro-cess, we incorporate \"thinking tokens\" into theprompts. These tokens encourage the modelto engage in more deliberate reasoning beforegenerating rules, potentially leading to moreinsightful and effective guidance.",
  "The preference agents, based on Llama-3-8B-Instruct, are trained using Quantized Low-Rank": "Adaptation (QLoRA) (Dettmers et al., 2023), aparameter-efficient fine-tuning (PeFT) method. Wechoose QLoRA over full fine-tuning due to itsscalability and feasibility for local deployment onuser devices. All model training procedures aredesigned to be accommodated within 16GB ofVRAM, making our approach accessible to stan-dard consumer-grade devices.We employ a consistent set of hyperparametersacross all experiments. This simplified configura-tion, while not optimal, serves to demonstrate theeffectiveness of our method even with straightfor-ward hyperparameter choices. A detailed analysisof our fine-tuning procedure, including further ex-ploration of hyperparameter search and impact onperformance, can be found in Appendix D.To establish a performance baseline, we also nai-ve finetune a model (MF ) using the same setup.This model is trained directly on input-output pairs(user intent and task metadata as input, ground truthtext as output). This ensures a fair comparison byisolating the impact of our proposed rule-basedfine-tuning approach from potential differences inmodel architecture or training configurations.",
  "Naive fine-tuned agent (MF ), where MS isdirectly fine-tuned on input-output pairs usingQLoRA with the same hyperparameters as ourpreference agent training": "Automated Evaluation. We leverage GPT-4oas our automated evaluator due to its demonstratedcapabilities in assessing human-written text andcapturing stylistic nuances (Naismith et al., 2023;Zheng et al., 2023; Shashidhar et al., 2023). Ourprimary automated metric is the win percentage, which quantifies how often a methods output isselected by GPT-4o as the best match to the groundtruth. GPT-4os evaluations are based on crite-ria such as similarity in style, tone, characteris-tic phrases, and overall resemblance to the groundtruth content.Human Evaluation: We complement the auto-mated evaluation with a human evaluation study ona subset of each dataset. Participants are presentedwith the original input, the ground truth output, andthe outputs generated by each method. They arethen asked to select the response that they believebest aligns with the ground truth, using the samecriteria as the GPT-4o evaluation. This human eval-uation provides valuable insights into the alignmentof model outputs with human preferences. Detailedinformation on the human evaluation protocol canbe found in Appendix I.Choice of Metrics. We have deliberately chosennot to rely on traditional similarity metrics such asBLEU (Papineni et al., 2002) and ROUGE (Lin,2004). While these metrics are valuable for assess-ing lexical overlap, they are less effective in captur-ing the nuanced aspects of stylistic similarity thatare central to our evaluation. For example, considertwo emails from the Enron dataset with similar con-tent but distinct styles. One email might be writ-ten in a formal, professional tone, while the otheradopts a more casual, conversational style. BLEUand ROUGE, focusing primarily on the presenceor absence of specific words and phrases, might as-sign similar scores to both emails despite the cleardifference in style perceived by a human reader.This discrepancy arises because these metrics donot adequately account for the semantic meaningand contextual usage of words, which are crucialfor evaluating stylistic resemblance. This decisionis further elaborated upon in Appendix E.",
  "Results": "We evaluated the performance of our fine-tunedpreference agents against several baselines usingGPT-4o and human evaluation.We report thewin rates the percentage of instances where ourmethod outperforms the baseline in . Ourbaselines include zero-shot generations from bothMS and ML, few-shot generations using ML, anda naive fine-tuned agent (MF ). We compare thesebaselines against our preference agent, trained withzero-shot baseline rules, and a no-baseline agenttrained without using zero-shot information.",
  ": Win Rates of Llama3 8B Ms combined with various ML, evaluated by GPT4o and human evaluation": "Small Model Baseline. Both GPT-4o and hu-man evaluations agree that the small model baseline(MS) performs significantly worse than our prefer-ence agent. This highlights the limitations of usingsmall language models alone for tasks requiring adeep understanding of user preferences. Humanevaluations show an even larger performance gapcompared to GPT-4o, as humans are more adept atdetecting subtle differences in style and content.Large Model Baseline. While the baseline pro-duced by the large model improves, the improve-ment is most noticeable in domains with lowerdegrees of available personalization (such as arti-cles and product reviews), and less noticeable whendirect preferences are involved (such as email writ-ing)Few Shot. While our method consistently out-performs the few-shot baseline across all datasets,the performance gap is more pronounced in theNew Yorker dataset, compared to the LAMP3U orEnron datasets. We hypothesize that this differencestems from the nature of the tasks. Few-shot exam-ples are likely more effective for email writing orproduct review writing, a relatively structured andconcise format, than for long-form article writing,where capturing stylistic nuances requires morethan a few examples.Naive Finetune. Human evaluators exhibiteda stronger preference for preference agent out-puts over naive fine-tuning compared to GPT-4o.Post-annotation interviews revealed that naive fine-tuning often resulted in hallucinations of crucialinformation, a phenomenon not effectively penal-ized by automated metrics but disliked by humans.No Baseline Agent.The preference agenttrained without access to zero-shot baseline infor-mation exhibits competitive performance, particu-larly when considering the marginal reduction in in- ference cost it offers. This makes it a viable alterna-tive in scenarios where minimizing inference cost isa priority, even if it comes with a slight compromisein performance compared to the distillation-basedapproach.Both automated and human evaluations confirmthat our preference agent significantly improvesthe alignment of LLM outputs with individual userstyles and preferences, as discussed in .1.The strong and consistent performance across di-verse datasets and LLMs highlights the general-izability utilizing preference agents. Qualitativeexamples and human annotation samples of theresults are provided in Appendix J.",
  "Enhanced Fine-Tuning throughPreference Rules": "Our experiments () reveal that fine-tuningthe preference agent on natural language rules, asopposed to directly on input-output pairs, leads toa more effective learning process. This result indi-cates that structured rules provide a more efficientlearning signal for the preference agent.We hypothesize that this difference stems fromthe inherent complexity and diversity of the tar-get data versus the clear structure of the prefer-ence rules. When learning from raw input-outputpairs, the model must adapt to the nuances of thetarget task, which can be challenging given thediversity of writing styles and content. Specifi-cally, instruction-finetuned language models oftenexhibit a \"chatbot\" style, characterized by conver-sational and explanatory responses (Ouyang et al.,2022). This style can be significantly different fromthe desired output style for specific tasks like emailwriting, which often requires a more direct and concise approach. Adapting the model to such spe-cific styles directly through fine-tuning can be chal-lenging, especially under the constraints of PeFTmethods. In contrast, the structured format of therules enables the model to discern patterns moreeasily, facilitating faster and more effective learn-ing. This translates to improved sample efficiency,as the model requires fewer examples to grasp theunderlying preference information.Furthermore, this approach promotes a smallerdistribution shift during fine-tuning. Naive fine-tuning necessitates a substantial adaptation to thenew tasks distribution, which can involve shiftingthe models overall output style from a conversa-tional \"chatbot\" approach to a more task-specificstyle. PEFT methods, while effective in adapt-ing models to new tasks or domains, may be lesseffective in inducing such significant changes inthe models fundamental language generation style(Balne et al., 2024). On the other hand, rule-basedfine-tuning focuses on learning a more specificmapping from input to preference rules lever-aging the pre-trained language models existingcapabilities for task completion. Crucially, naturallanguage rules, are closer to the LMs existing out-put distribution compared to the diverse and poten-tially drastically different output styles of diverse,specific tasks. This makes them more suitable forPEFT adaptation, as the model can learn to gener-ate rules without having to undergo a substantialshift in its underlying parameters.his decoupling of preference learning from thecore task allows for more efficient adaptation, espe-cially in multi-task settings where the model mightneed to switch between different domains and writ-ing styles. By focusing on learning user prefer-ences, we can leverage the larger models general-izability and extensive knowledge base for superiorperformance across diverse tasks.",
  "Model-Specific Semantic Understanding": "Our findings suggest that models within the samefamily (e.g., Llama) exhibit a higher degree of se-mantic alignment compared to models from dif-ferent families (e.g., GPT-4).Specifically, weobserved that Llama-3 70B demonstrates a bet-ter understanding of rules generated by itself orthe smaller Llama-3 8B model, compared to rulesgenerated by GPT-4. While GPT-4 generated well-structured and seemingly comprehensive rules, theywere less effective in guiding the Llama models.This indicates that semantic understanding, even",
  ": On the New Yorker dataset, naive fine-tuningplateaus at a loss above 1.5, whereas fine-tuning withstructured preference rules reduces the loss below 1.0with identical hyperparameters": "when expressed through seemingly universal natu-ral language, can be model-specific.This observation is further supported by exper-iments with human-written rules. Despite beingcrafted by expert annotators to be as clear andspecific as possible, human-generated rules ledto a 16.8% performance degradation compared tomodel-generated rules. This suggests that subtledifferences in the way models and humans interpretlanguage can significantly impact the effectivenessof rule-based guidance. For instance, models mightinterpret terms like \"precise,\" \"concise,\" and \"infor-mal\" differently than humans, leading to discrepan-cies between intended and actual outcomes.These findings highlight the potential impor-tance of model-specific semantic understandingin aligning LLMs with human preferences. Au-tomated rule generation, leveraging the modelsown internal representations and semantic under-standing, is a more effective approach than relyingon human-generated rules or prompts. However,further research is needed to fully understand thenature of these semantic differences and developstrategies for mitigating their impact.",
  "Enhancing Rule Generation throughDeliberative Prompts": "Humans often engage in a process of deliberationbefore formulating responses, particularly whenfaced with complex or nuanced tasks. This internaldialogue, where we weigh different options andconsider various perspectives, contributes to more thoughtful and well-reasoned answers. Drawinginspiration from this human cognitive process, weexplored the use of \"deliberation\" during rule gen-eration inference by the preference agent. We in-clude specially designed \"thinking tokens,\" whichencourage the model to engage in a similar formof internal reasoning before generating the natu-ral language rules that guide the larger LLM. Thisencourages the model to decompose the task ofpreference extraction into smaller, more manage-able steps. Our empirical results demonstrate thatincorporating these deliberative prompts leads toa notable improvement in the quality of generatedrules, resulting in better alignment between thelarge LLMs outputs and individual user prefer-ences.We hypothesize that these thinking tokens func-tion as a form of cognitive scaffolding, providingthe model with a structured space to isolate and pro-cess critical preference information. By explicitlyprompting the model to \"think\" before generatingrules, we aim to enhance its ability to identify sub-tle patterns in user preferences and translate theminto effective guidance for the larger model. Thisapproach aligns with findings from previous re-search, which demonstrates that prompting LLMsto engage in step-by-step reasoning can signifi-cantly improve their performance on various tasks(Kojima et al., 2023; Zelikman et al., 2024; Goyalet al., 2024).",
  "Evidence of Personalization": "A key objective of our approach is to learn indi-vidual writing styles rather than merely improvinggeneral task performance (e.g., email writing). Toinvestigate this, we conducted a permutation analy-sis using preference agents trained on distinct emailsenders from the Enron dataset. We trained fiveagents on data from five different senders and thenapplied each agent to the test data of all five senders,generating emails for every agent-sender combina-tion. This allowed us to assess whether an agenttrained on a specific senders style is more effec-tive at generating emails resembling that senderswriting compared to other senders.We quantified the similarity between the gener-ated emails and the ground truth using the normal-ized BERT Score (Reimers and Gurevych, 2019),which provides a measure of semantic similaritysuitable for analyzing large text corpora like emails.Our analysis, depicted in , reveals a strongtrend along the diagonal. This indicates that the",
  "Cost Effectiveness": "While our approach necessitates an inference stepwith ML during rule generation and at inferencetime, the cost, Ci(ML), is relatively small due tothe concise nature of the rule sequences. For in-stance, most rule sequences generated range from100 - 150 extra tokens. This results in a combinedcost of Cf(MS) + Ci(ML). Although this com-bined cost is marginally higher than the cost ofnaive fine-tuning (Cf(MF )), the significant perfor-mance gains offered by our method, as evidencedby our experimental findings, justify this trade-off.Moreover, the inference cost associated with rulegeneration is a one-time expense during trainingdata preparation, further diminishing its impact.Our decision to avoid fine-tuning ML providessignificant flexibility as we avoid the sunk costassociated with fine-tuning a large model, en-abling seamless integration of newer, more power-ful LLMs as they become available.",
  "Traditional Methods of Alignment. Aligning lan-guage models to human preferences often employstechniques like Reinforcement Learning from Hu-man Feedback (RLHF) (Ouyang et al., 2022) and": "its variant, Reinforcement Learning from AI Feed-back (RLAIF) (Bai et al., 2022), which leveragesfine-tuned LLMs as annotators. While effective,RLHF requires substantial human annotation andcomplex distributed training. Direct PreferenceOptimization (DPO) (Rafailov et al., 2023) offersan alternative by using preference pairs, reduc-ing computational complexity. However, DPOsreliance on contrasting pairs may not fully cap-ture the nuances of overlapping human preferences.In-context learning methods (Kojima et al., 2022;Wozniak et al., 2024), while showing promise, arelimited by context length restrictions, hinderingtheir ability to generalize effectively.Agent-based Alignment. To address the com-putational demands of training large models, agent-based architectures have emerged as a promisingavenue for compute-constrained environments. Forinstance, Li et al. (2023) utilize a fine-tuned T5policy model to guide large models via stimulusprompting. However, this approach necessitatesfull-parameter Supervised Fine-Tuning (SFT) andRL optimization, introducing computational over-head and yielding limited performance gains intasks like dialogue generation. Similarly, Aligner(Ji et al., 2024) employs full-parameter SFT andrelies on a substantial custom dataset for preferencelearning, posing challenges in terms of data require-ments and VRAM usage. Tan et al. (2024) proposeParameter-Efficient Fine-Tuning (PEFT) methodsto personalize agents based on user history andpreference retrieval. While computationally effi-cient, this approach is constrained by the reasoningcapabilities of the smaller fine-tuned agent. Theseapproaches often rely on automatic metrics likeBLEU and ROUGE, which predominantly capturelexical similarity without fully encapsulating thenuances of human preferences. Gao et al. (2024)introduce an agent trained on human edits to alignzero-shot outputs. However, this approach requiresmultiple inference rounds for each query, increas-ing latency and computational costs. Moreover,human edit history may not consistently reflectgenuine preferences, and relying solely on edit dis-tance as a measure of alignment can be unreliable.Yang et al. (2024) propose a framework for align-ing LLMs through Multi-perspective User Pref-erence Ranking-based Feedback. This approach,however, involves an initial SFT phase, along withMulti-Perspective Ranking Aggregation (MPRA)and Reward Imitation Learning (RIL), leading tosignificant training overhead and the use of metrics like BLEU that may not accurately capture humanpreferences.Comparison with Aligner. While both Aligner(Ji et al., 2024) and our method utilize a smallmodel trained with full-parameter SFT, our ap-proaches differ significantly. Aligner focuses oncorrecting model outputs post-generation, whileour preference agent proactively generates rules tosteer the large models initial output. This allowsus to leverage the large models reasoning capabili-ties by providing preference information upfront,rather than correcting its output afterwards. WhileAligner demonstrates strong performance on taskslike text summarization and dialogue generation,its design is geared towards making smaller ad-justments to large model outputs. Our task, on theother hand, often requires more substantial changesto align with user preferences, potentially necessi-tating complete rewrites of emails or articles. AnAligner-style approach or naive fine-tuning wouldface challenges in our setting, as a small modelmight struggle to accurately make drastic changeswhile preserving the large models knowledge andcoherence. This would also complicate the fine-tuning objective, as the patterns to be learned wouldbe less well-defined and vary significantly acrossexamples.",
  "Conclusion": "This work introduces a novel paradigm for aligninglarge language models (LLMs) with individual userpreferences using limited data. We leverage small,locally trainable \"preference agents\" to efficientlyguide larger LLMs without resource-intensive fine-tuning. Our approach generates natural languagerules that encapsulate user preferences, acting as a\"steering wheel\" to direct the larger models outputtowards desired styles and content.This framework introduces a new preferencefine-tuning objective: learning from implicit prefer-ence information found in the differences betweena baseline LLM output and the users desired out-put. This allows the agent to distill user preferencesinto actionable rules, enabling efficient personaliza-tion without modifying the larger models weights.Our empirical findings across diverse datasetsdemonstrate that preference agents significantly im-prove alignment with user preferences comparedto existing methods in a compute-efficient man-ner, highlighting the potential for building highlypersonalized LLM applications at scale.",
  "Limitations": "While our proposed method demonstrates signif-icant improvements, there are a few areas for po-tential refinement. One consideration is the timerequired for the large model to process the prefer-ence agents output before the first token can begenerated. This could lead to a slightly higher Timeto First Token (TTFT) at inference time. However,we believe the substantial performance gains of-fered by our approach outweigh this trade-off.As discussed in C, our most performant rulegeneration strategy incurs an additional computa-tional cost compared to the alternative methods dueto an extra zero-shot inference step. This cost is off-set by the superior performance it enables. We alsoprovide a highly competitive \"no-baseline\" rulegeneration method which offers good performanceat a lower inference cost.Furthermore, our rule generation strategy lever-ages thinking tokens, which can lead to slightlylonger outputs. If output length is a strict constraint,this step can be omitted with minimal impact onthe frameworks effectiveness. Importantly, theinference cost associated with rule generation isa one-time expense incurred during training datapreparation.Finally, as noted in 5.5, using ML for prefer-ence agent rule generation introduces an additionalinference iteration compared to naive fine-tuning.While our current research focuses on text-basedpreferences, future work could explore extendingthis approach to other modalities, such as image oraudio generation. Additionally, investigating the in-tegration of multimodal preferences and the devel-opment of more sophisticated rule generation strate-gies could further enhance the capabilities of pref-erence agents. We believe that this research opensexciting new avenues for personalized LLM appli-cations, paving the way for a future where powerfullanguage models can be seamlessly tailored to indi-vidual needs and preferences, ultimately enhancinguser experience and fostering more engaging andhuman-centric interactions.",
  "Ethical Considerations": "In this work, we have taken several steps to ensurethat our research adheres to ethical principles andrespects the rights of all parties involved. We arecommitted to the responsible and ethical use ofAI technology and have implemented measures toprevent potential misuse of our work. DatasetLicensingandAttribution.Bothdatasetsusedinthisresearchwillbere-leased under the Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)license.The Enron email dataset (Klimt and Yang, 2004)is available for educational and research purposesunder the principles of fair use. We have creditedthe original dataset creators and adhered to theterms of its usage.The New Yorker dataset is based on the All theNews 2.0 dataset by Andrew Thompson (Thomp-son, 2020), which is licensed for non-commercial,research purposes only. We have made modifica-tions and enhancements to the dataset, and thesechanges are also licensed under the CC BY-NC 4.0license. We have properly attributed the originaldataset and its creator. Model Release.In compliance with the terms ofthe All the News 2.0 dataset license, we will notbe releasing the fine-tuned agents trained on theNew Yorker dataset. The license explicitly statesthat the dataset is to be used for research purposesonly and not for the release of commercial genera-tive models.Similarly, we will not release the agent fine-tuned on the Enron email corpus. This decisionwas made to ensure that our models are not used toimpersonate the senders in the Enron email corpuswithout their explicit permission. We believe thatreleasing such a model could potentially infringeupon the privacy rights of the individuals involved.However, for research purposes only, we willmake the models available upon request. Citation and Acknowledgment.We have takenextensive care to ensure that we comply with alllicenses and have appropriately cited any of ourwork that is a derivative of another project. Weacknowledge the original creators and their contri-butions to the field. Potential Misuse.We acknowledge that ourdatasets, though open-source, can potentially beused to train AI assistants or models for maliciouspurposes. We strongly condemn any misuse ofour work and explicitly support the safe and re-sponsible use of AI technology. Our intention is toadvance the field of AI research while adhering toethical principles and preventing harm.",
  "David Berliner, Michael Lambek, Richard Shweder,Richard Irvine, and Albert Piette. 2016. Anthropol-ogy and the study of contradictions. HAU: Journalof Ethnographic Theory, 6(1):127": "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020.Language models are few-shot learners.In Ad-vances in Neural Information Processing Systems,volume 33, pages 18771901. Curran Associates,Inc.",
  "Ben Naismith, Phoebe Mulcaire, and Jill Burstein. 2023": "Automated evaluation of written discourse coherenceusing GPT-4. In Proceedings of the 18th Workshopon Innovative Use of NLP for Building EducationalApplications (BEA 2023), pages 394403, Toronto,Canada. Association for Computational Linguistics. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-roll L. Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, JohnSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,Maddie Simens, Amanda Askell, Peter Welinder,Paul Christiano, Jan Leike, and Ryan Lowe. 2022.Training language models to follow instructions withhuman feedback. Preprint, arXiv:2203.02155. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalu-ation of machine translation. In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics, pages 311318, Philadelphia,Pennsylvania, USA. Association for ComputationalLinguistics.",
  "via hierarchical deep reinforcement learning. In Pro-ceedings of the 2017 Conference on Empirical Meth-ods in Natural Language Processing. Association forComputational Linguistics": "Hao Peng, Xiaozhi Wang, Jianhui Chen, Weikai Li, Yun-jia Qi, Zimu Wang, Zhili Wu, Kaisheng Zeng, Bin Xu,Lei Hou, and Juanzi Li. 2023. When does in-contextlearning fall short and why? a study on specification-heavy tasks. Preprint, arXiv:2311.08993. Rafael Rafailov, Archit Sharma, Eric Mitchell, StefanoErmon, Christopher D. Manning, and Chelsea Finn.2023.Direct preference optimization: Your lan-guage model is secretly a reward model. Preprint,arXiv:2305.18290.",
  "Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020": "BLEURT: Learning robust metrics for text genera-tion. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics, pages78817892, Online. Association for ComputationalLinguistics. Sumuk Shashidhar, Abhinav Chinta, Vaibhav Sahai,Zhenhailong Wang, and Heng Ji. 2023. Democra-tizing LLMs: An exploration of cost-performancetrade-offs in self-refined open-source models.InFindings of the Association for Computational Lin-guistics: EMNLP 2023, pages 90709084, Singapore.Association for Computational Linguistics.",
  "Zhaoxuan Tan, Qingkai Zeng, Yijun Tian, Zheyuan Liu,Bing Yin, and Meng Jiang. 2024. Democratizinglarge language models via personalized parameter-efficient fine-tuning. Preprint, arXiv:2402.04401": "Gemini Team, Petko Georgiev, Ving Ian Lei, RyanBurnell, Libin Bai, Anmol Gulati, Garrett Tanzer,Damien Vincent, Zhufeng Pan, Shibo Wang, SorooshMariooryad, Yifan Ding, Xinyang Geng, Fred Al-cober, Roy Frostig, Mark Omernick, Lexi Walker,Cosmin Paduraru, Christina Sorokin, Andrea Tac-chetti, Colin Gaffney, Samira Daruki, Olcan Ser-cinoglu, Zach Gleicher, Juliette Love, Paul Voigt-laender, Rohan Jain, Gabriela Surita, Kareem Mo-hamed, Rory Blevins, Junwhan Ahn, Tao Zhu, Korn-raphop Kawintiranon, Orhan Firat, Yiming Gu, Yu-jing Zhang, Matthew Rahtz, Manaal Faruqui, NatalieClay, Justin Gilmer, JD Co-Reyes, Ivo Penchev, RuiZhu, Nobuyuki Morioka, Kevin Hui, Krishna Hari-dasan, Victor Campos, Mahdis Mahdieh, Mandy Guo,Samer Hassan, Kevin Kilgour, Arpi Vezer, Heng-Tze Cheng, Raoul de Liedekerke, Siddharth Goyal,Paul Barham, DJ Strouse, Seb Noury, Jonas Adler,Mukund Sundararajan, Sharad Vikram, Dmitry Lep-ikhin, Michela Paganini, Xavier Garcia, Fan Yang, Dasha Valter, Maja Trebacz, Kiran Vodrahalli, Chu-layuth Asawaroengchai, Roman Ring, Norbert Kalb,Livio Baldini Soares, Siddhartha Brahma, DavidSteiner, Tianhe Yu, Fabian Mentzer, Antoine He,Lucas Gonzalez, Bibo Xu, Raphael Lopez Kauf-man, Laurent El Shafey, Junhyuk Oh, Tom Hennigan,George van den Driessche, Seth Odoom, Mario Lucic,Becca Roelofs, Sid Lall, Amit Marathe, Betty Chan,Santiago Ontanon, Luheng He, Denis Teplyashin,Jonathan Lai, Phil Crone, Bogdan Damoc, LewisHo, Sebastian Riedel, Karel Lenc, Chih-Kuan Yeh,Aakanksha Chowdhery, Yang Xu, Mehran Kazemi,Ehsan Amid, Anastasia Petrushkina, Kevin Swersky,Ali Khodaei, Gowoon Chen, Chris Larkin, MarioPinto, Geng Yan, Adria Puigdomenech Badia, PiyushPatil, Steven Hansen, Dave Orr, Sebastien M. R.Arnold, Jordan Grimstad, Andrew Dai, Sholto Dou-glas, Rishika Sinha, Vikas Yadav, Xi Chen, Elena Gri-bovskaya, Jacob Austin, Jeffrey Zhao, Kaushal Patel,Paul Komarek, Sophia Austin, Sebastian Borgeaud,Linda Friso, Abhimanyu Goyal, Ben Caine, KrisCao, Da-Woon Chung, Matthew Lamm, Gabe Barth-Maron, Thais Kagohara, Kate Olszewska, Mia Chen,Kaushik Shivakumar, Rishabh Agarwal, HarshalGodhia, Ravi Rajwar, Javier Snaider, Xerxes Doti-walla, Yuan Liu, Aditya Barua, Victor Ungureanu,Yuan Zhang, Bat-Orgil Batsaikhan, Mateo Wirth,James Qin, Ivo Danihelka, Tulsee Doshi, MartinChadwick, Jilin Chen, Sanil Jain, Quoc Le, Ar-jun Kar, Madhu Gurumurthy, Cheng Li, RuoxinSang, Fangyu Liu, Lampros Lamprou, Rich Munoz,Nathan Lintz, Harsh Mehta, Heidi Howard, Mal-colm Reynolds, Lora Aroyo, Quan Wang, LorenzoBlanco, Albin Cassirer, Jordan Griffith, DipanjanDas, Stephan Lee, Jakub Sygnowski, Zach Fisher,James Besley, Richard Powell, Zafarali Ahmed, Do-minik Paulus, David Reitter, Zalan Borsos, RishabhJoshi, Aedan Pope, Steven Hand, Vittorio Selo, Vi-han Jain, Nikhil Sethi, Megha Goel, Takaki Makino,Rhys May, Zhen Yang, Johan Schalkwyk, ChristinaButterfield, Anja Hauth, Alex Goldin, Will Hawkins,Evan Senter, Sergey Brin, Oliver Woodman, Mar-vin Ritter, Eric Noland, Minh Giang, Vijay Bolina,Lisa Lee, Tim Blyth, Ian Mackinnon, Machel Reid,Obaid Sarvana, David Silver, Alexander Chen, LilyWang, Loren Maggiore, Oscar Chang, Nithya At-taluri, Gregory Thornton, Chung-Cheng Chiu, Os-kar Bunyan, Nir Levine, Timothy Chung, EvgeniiEltyshev, Xiance Si, Timothy Lillicrap, DemetraBrady, Vaibhav Aggarwal, Boxi Wu, Yuanzhong Xu,Ross McIlroy, Kartikeya Badola, Paramjit Sandhu,Erica Moreira, Wojciech Stokowiec, Ross Hems-ley, Dong Li, Alex Tudor, Pranav Shyam, ElaheRahimtoroghi, Salem Haykal, Pablo Sprechmann,Xiang Zhou, Diana Mincu, Yujia Li, Ravi Addanki,Kalpesh Krishna, Xiao Wu, Alexandre Frechette,Matan Eyal, Allan Dafoe, Dave Lacey, Jay Whang,Thi Avrahami, Ye Zhang, Emanuel Taropa, HanzhaoLin, Daniel Toyama, Eliza Rutherford, Motoki Sano,HyunJeong Choe, Alex Tomala, Chalence Safranek-Shrader, Nora Kassner, Mantas Pajarskas, MattHarvey, Sean Sechrist, Meire Fortunato, ChristinaLyu, Gamaleldin Elsayed, Chenkai Kuang, JamesLottes, Eric Chu, Chao Jia, Chih-Wei Chen, Pe- ter Humphreys, Kate Baumli, Connie Tao, Rajku-mar Samuel, Cicero Nogueira dos Santos, AndersAndreassen, Nemanja Rakicevic, Dominik Grewe,Aviral Kumar, Stephanie Winkler, Jonathan Caton,Andrew Brock, Sid Dalmia, Hannah Sheahan, IainBarr, Yingjie Miao, Paul Natsev, Jacob Devlin, Fer-yal Behbahani, Flavien Prost, Yanhua Sun, ArtiomMyaskovsky, Thanumalayan Sankaranarayana Pillai,Dan Hurt, Angeliki Lazaridou, Xi Xiong, Ce Zheng,Fabio Pardo, Xiaowei Li, Dan Horgan, Joe Stanton,Moran Ambar, Fei Xia, Alejandro Lince, MingqiuWang, Basil Mustafa, Albert Webson, Hyo Lee, Ro-han Anil, Martin Wicke, Timothy Dozat, AbhishekSinha, Enrique Piqueras, Elahe Dabir, Shyam Upad-hyay, Anudhyan Boral, Lisa Anne Hendricks, CoreyFry, Josip Djolonga, Yi Su, Jake Walker, Jane La-banowski, Ronny Huang, Vedant Misra, JeremyChen, RJ Skerry-Ryan, Avi Singh, Shruti Rijh-wani, Dian Yu, Alex Castro-Ros, Beer Changpinyo,Romina Datta, Sumit Bagri, Arnar Mar Hrafnkels-son, Marcello Maggioni, Daniel Zheng, Yury Sul-sky, Shaobo Hou, Tom Le Paine, Antoine Yang,Jason Riesa, Dominika Rogozinska, Dror Marcus,Dalia El Badawy, Qiao Zhang, Luyu Wang, HelenMiller, Jeremy Greer, Lars Lowe Sjos, Azade Nova,Heiga Zen, Rahma Chaabouni, Mihaela Rosca, JiepuJiang, Charlie Chen, Ruibo Liu, Tara Sainath, MaximKrikun, Alex Polozov, Jean-Baptiste Lespiau, JoshNewlan, Zeyncep Cankara, Soo Kwak, Yunhan Xu,Phil Chen, Andy Coenen, Clemens Meyer, KaterinaTsihlas, Ada Ma, Juraj Gottweis, Jinwei Xing, Chen-jie Gu, Jin Miao, Christian Frank, Zeynep Cankara,Sanjay Ganapathy, Ishita Dasgupta, Steph Hughes-Fitt, Heng Chen, David Reid, Keran Rong, HongminFan, Joost van Amersfoort, Vincent Zhuang, AaronCohen, Shixiang Shane Gu, Anhad Mohananey,Anastasija Ilic, Taylor Tobin, John Wieting, AnnaBortsova, Phoebe Thacker, Emma Wang, EmilyCaveness, Justin Chiu, Eren Sezener, Alex Kaskasoli,Steven Baker, Katie Millican, Mohamed Elhawaty,Kostas Aisopos, Carl Lebsack, Nathan Byrd, HanjunDai, Wenhao Jia, Matthew Wiethoff, Elnaz Davoodi,Albert Weston, Lakshman Yagati, Arun Ahuja, IsabelGao, Golan Pundak, Susan Zhang, Michael Azzam,Khe Chai Sim, Sergi Caelles, James Keeling, Ab-hanshu Sharma, Andy Swing, YaGuang Li, ChenxiLiu, Carrie Grimes Bostock, Yamini Bansal, ZacharyNado, Ankesh Anand, Josh Lipschultz, Abhijit Kar-markar, Lev Proleev, Abe Ittycheriah, Soheil Has-sas Yeganeh, George Polovets, Aleksandra Faust,Jiao Sun, Alban Rrustemi, Pen Li, Rakesh Shivanna,Jeremiah Liu, Chris Welty, Federico Lebron, AnirudhBaddepudi, Sebastian Krause, Emilio Parisotto, RaduSoricut, Zheng Xu, Dawn Bloxwich, Melvin John-son, Behnam Neyshabur, Justin Mao-Jones, Ren-shen Wang, Vinay Ramasesh, Zaheer Abbas, ArthurGuez, Constant Segal, Duc Dung Nguyen, JamesSvensson, Le Hou, Sarah York, Kieran Milan, So-phie Bridgers, Wiktor Gworek, Marco Tagliasacchi,James Lee-Thorp, Michael Chang, Alexey Guseynov,Ale Jakse Hartman, Michael Kwong, Ruizhe Zhao,Sheleem Kashem, Elizabeth Cole, Antoine Miech,Richard Tanburn, Mary Phuong, Filip Pavetic, Se-bastien Cevey, Ramona Comanescu, Richard Ives, Sherry Yang, Cosmo Du, Bo Li, Zizhao Zhang,Mariko Iinuma, Clara Huiyi Hu, Aurko Roy, ShaanBijwadia, Zhenkai Zhu, Danilo Martins, RachelSaputro, Anita Gergely, Steven Zheng, Dawei Jia,Ioannis Antonoglou, Adam Sadovsky, Shane Gu,Yingying Bi, Alek Andreev, Sina Samangooei, MinaKhan, Tomas Kocisky, Angelos Filos, Chintu Ku-mar, Colton Bishop, Adams Yu, Sarah Hodkin-son, Sid Mittal, Premal Shah, Alexandre Moufarek,Yong Cheng, Adam Bloniarz, Jaehoon Lee, PedramPejman, Paul Michel, Stephen Spencer, VladimirFeinberg, Xuehan Xiong, Nikolay Savinov, Char-lotte Smith, Siamak Shakeri, Dustin Tran, MaryChesus, Bernd Bohnet, George Tucker, Tamara vonGlehn, Carrie Muir, Yiran Mao, Hideto Kazawa,Ambrose Slone, Kedar Soparkar, Disha Shrivastava,James Cobon-Kerr, Michael Sharman, Jay Pavagadhi,Carlos Araya, Karolis Misiunas, Nimesh Ghelani,Michael Laskin, David Barker, Qiujia Li, AntonBriukhov, Neil Houlsby, Mia Glaese, Balaji Laksh-minarayanan, Nathan Schucher, Yunhao Tang, EliCollins, Hyeontaek Lim, Fangxiaoyu Feng, AdriaRecasens, Guangda Lai, Alberto Magni, Nicola DeCao, Aditya Siddhant, Zoe Ashwood, Jordi Orbay,Mostafa Dehghani, Jenny Brennan, Yifan He, KelvinXu, Yang Gao, Carl Saroufim, James Molloy, XinyiWu, Seb Arnold, Solomon Chang, Julian Schrit-twieser, Elena Buchatskaya, Soroush Radpour, Mar-tin Polacek, Skye Giordano, Ankur Bapna, SimonTokumine, Vincent Hellendoorn, Thibault Sottiaux,Sarah Cogan, Aliaksei Severyn, Mohammad Saleh,Shantanu Thakoor, Laurent Shefey, Siyuan Qiao,Meenu Gaba, Shuo yiin Chang, Craig Swanson, BiaoZhang, Benjamin Lee, Paul Kishan Rubenstein, GanSong, Tom Kwiatkowski, Anna Koop, Ajay Kan-nan, David Kao, Parker Schuh, Axel Stjerngren, Gol-naz Ghiasi, Gena Gibson, Luke Vilnis, Ye Yuan, Fe-lipe Tiengo Ferreira, Aishwarya Kamath, Ted Kli-menko, Ken Franko, Kefan Xiao, Indro Bhattacharya,Miteyan Patel, Rui Wang, Alex Morris, RobinStrudel, Vivek Sharma, Peter Choy, Sayed HadiHashemi, Jessica Landon, Mara Finkelstein, PriyaJhakra, Justin Frye, Megan Barnes, Matthew Mauger,Dennis Daun, Khuslen Baatarsukh, Matthew Tung,Wael Farhan, Henryk Michalewski, Fabio Viola, Fe-lix de Chaumont Quitry, Charline Le Lan, Tom Hud-son, Qingze Wang, Felix Fischer, Ivy Zheng, ElspethWhite, Anca Dragan, Jean baptiste Alayrac, Eric Ni,Alexander Pritzel, Adam Iwanicki, Michael Isard,Anna Bulanova, Lukas Zilka, Ethan Dyer, Deven-dra Sachan, Srivatsan Srinivasan, Hannah Mucken-hirn, Honglong Cai, Amol Mandhane, MukarramTariq, Jack W. Rae, Gary Wang, Kareem Ayoub,Nicholas FitzGerald, Yao Zhao, Woohyun Han, ChrisAlberti, Dan Garrette, Kashyap Krishnakumar, MaiGimenez, Anselm Levskaya, Daniel Sohn, JosipMatak, Inaki Iturrate, Michael B. Chang, Jackie Xi-ang, Yuan Cao, Nishant Ranka, Geoff Brown, AdrianHutter, Vahab Mirrokni, Nanxin Chen, KaishengYao, Zoltan Egyed, Francois Galilee, Tyler Liechty,Praveen Kallakuri, Evan Palmer, Sanjay Ghemawat,Jasmine Liu, David Tao, Chloe Thornton, Tim Green,Mimi Jasarevic, Sharon Lin, Victor Cotruta, Yi-XuanTan, Noah Fiedel, Hongkun Yu, Ed Chi, Alexan- der Neitz, Jens Heitkaemper, Anu Sinha, DennyZhou, Yi Sun, Charbel Kaed, Brice Hulse, Swa-roop Mishra, Maria Georgaki, Sneha Kudugunta,Clement Farabet, Izhak Shafran, Daniel Vlasic, An-ton Tsitsulin, Rajagopal Ananthanarayanan, AlenCarin, Guolong Su, Pei Sun, Shashank V, GabrielCarvajal, Josef Broder, Iulia Comsa, Alena Repina,William Wong, Warren Weilun Chen, Peter Hawkins,Egor Filonov, Lucia Loher, Christoph Hirnschall,Weiyi Wang, Jingchen Ye, Andrea Burns, HardieCate, Diana Gage Wright, Federico Piccinini, LeiZhang, Chu-Cheng Lin, Ionel Gog, Yana Kulizh-skaya, Ashwin Sreevatsa, Shuang Song, Luis C.Cobo, Anand Iyer, Chetan Tekur, Guillermo Gar-rido, Zhuyun Xiao, Rupert Kemp, Huaixiu StevenZheng, Hui Li, Ananth Agarwal, Christel Ngani,Kati Goshvadi, Rebeca Santamaria-Fernandez, Woj-ciech Fica, Xinyun Chen, Chris Gorgolewski, SeanSun, Roopal Garg, Xinyu Ye, S. M. Ali Eslami,Nan Hua, Jon Simon, Pratik Joshi, Yelin Kim, IanTenney, Sahitya Potluri, Lam Nguyen Thiet, QuanYuan, Florian Luisier, Alexandra Chronopoulou, Sal-vatore Scellato, Praveen Srinivasan, Minmin Chen,Vinod Koverkathu, Valentin Dalibard, Yaming Xu,Brennan Saeta, Keith Anderson, Thibault Sellam,Nick Fernando, Fantine Huot, Junehyuk Jung, ManiVaradarajan, Michael Quinn, Amit Raul, Maigo Le,Ruslan Habalov, Jon Clark, Komal Jalan, KaleshaBullard, Achintya Singhal, Thang Luong, BoyuWang, Sujeevan Rajayogam, Julian Eisenschlos,Johnson Jia, Daniel Finchelstein, Alex Yakubovich,Daniel Balle, Michael Fink, Sameer Agarwal, JingLi, Dj Dvijotham, Shalini Pal, Kai Kang, JaclynKonzelmann, Jennifer Beattie, Olivier Dousse, DianeWu, Remi Crocker, Chen Elkind, Siddhartha ReddyJonnalagadda, Jong Lee, Dan Holtmann-Rice, Krys-tal Kallarackal, Rosanne Liu, Denis Vnukov, NeeraVats, Luca Invernizzi, Mohsen Jafari, Huanjie Zhou,Lilly Taylor, Jennifer Prendki, Marcus Wu, TomEccles, Tianqi Liu, Kavya Kopparapu, FrancoiseBeaufays, Christof Angermueller, Andreea Marzoca,Shourya Sarcar, Hilal Dib, Jeff Stanway, Frank Per-bet, Nejc Trdin, Rachel Sterneck, Andrey Khor-lin, Dinghua Li, Xihui Wu, Sonam Goenka, DavidMadras, Sasha Goldshtein, Willi Gierke, Tong Zhou,Yaxin Liu, Yannie Liang, Anais White, Yunjie Li,Shreya Singh, Sanaz Bahargam, Mark Epstein, Su-joy Basu, Li Lao, Adnan Ozturel, Carl Crous, AlexZhai, Han Lu, Zora Tung, Neeraj Gaur, AlannaWalton, Lucas Dixon, Ming Zhang, Amir Glober-son, Grant Uy, Andrew Bolt, Olivia Wiles, MiladNasr, Ilia Shumailov, Marco Selvi, Francesco Pic-cinno, Ricardo Aguilar, Sara McCarthy, Misha Khal-man, Mrinal Shukla, Vlado Galic, John Carpen-ter, Kevin Villela, Haibin Zhang, Harry Richard-son, James Martens, Matko Bosnjak, Shreyas Ram-mohan Belle, Jeff Seibert, Mahmoud Alnahlawi,Brian McWilliams, Sankalp Singh, Annie Louis,Wen Ding, Dan Popovici, Lenin Simicich, LauraKnight, Pulkit Mehta, Nishesh Gupta, ChongyangShi, Saaber Fatehi, Jovana Mitrovic, Alex Grills,Joseph Pagadora, Dessie Petrova, Danielle Eisenbud,Zhishuai Zhang, Damion Yates, Bhavishya Mittal,Nilesh Tripuraneni, Yannis Assael, Thomas Brovelli, Prateek Jain, Mihajlo Velimirovic, Canfer Akbulut,Jiaqi Mu, Wolfgang Macherey, Ravin Kumar, JunXu, Haroon Qureshi, Gheorghe Comanici, JeremyWiesner, Zhitao Gong, Anton Ruddock, MatthiasBauer, Nick Felt, Anirudh GP, Anurag Arnab, DustinZelle, Jonas Rothfuss, Bill Rosgen, Ashish Shenoy,Bryan Seybold, Xinjian Li, Jayaram Mudigonda,Goker Erdogan, Jiawei Xia, Jiri Simsa, Andrea Michi,Yi Yao, Christopher Yew, Steven Kan, Isaac Caswell,Carey Radebaugh, Andre Elisseeff, Pedro Valen-zuela, Kay McKinney, Kim Paterson, Albert Cui, EriLatorre-Chimoto, Solomon Kim, William Zeng, KenDurden, Priya Ponnapalli, Tiberiu Sosea, Christo-pher A. Choquette-Choo, James Manyika, BronaRobenek, Harsha Vashisht, Sebastien Pereira, HoiLam, Marko Velic, Denese Owusu-Afriyie, Kather-ine Lee, Tolga Bolukbasi, Alicia Parrish, Shawn Lu,Jane Park, Balaji Venkatraman, Alice Talbert, Lam-bert Rosique, Yuchung Cheng, Andrei Sozanschi,Adam Paszke, Praveen Kumar, Jessica Austin, Lu Li,Khalid Salama, Wooyeol Kim, Nandita Dukkipati,Anthony Baryshnikov, Christos Kaplanis, Xiang-Hai Sheng, Yuri Chervonyi, Caglar Unlu, Diegode Las Casas, Harry Askham, Kathryn Tunyasuvu-nakool, Felix Gimeno, Siim Poder, Chester Kwak,Matt Miecnikowski, Vahab Mirrokni, Alek Dimitriev,Aaron Parisi, Dangyi Liu, Tomy Tsai, Toby Shevlane,Christina Kouridi, Drew Garmon, Adrian Goedeck-emeyer, Adam R. Brown, Anitha Vijayakumar, AliElqursh, Sadegh Jazayeri, Jin Huang, Sara Mc Carthy,Jay Hoover, Lucy Kim, Sandeep Kumar, Wei Chen,Courtney Biles, Garrett Bingham, Evan Rosen, LisaWang, Qijun Tan, David Engel, Francesco Pongetti,Dario de Cesare, Dongseong Hwang, Lily Yu, Jen-nifer Pullman, Srini Narayanan, Kyle Levin, Sid-dharth Gopal, Megan Li, Asaf Aharoni, Trieu Trinh,Jessica Lo, Norman Casagrande, Roopali Vij, LoicMatthey, Bramandia Ramadhana, Austin Matthews,CJ Carey, Matthew Johnson, Kremena Goranova, Ro-hin Shah, Shereen Ashraf, Kingshuk Dasgupta, Ras-mus Larsen, Yicheng Wang, Manish Reddy Vuyyuru,Chong Jiang, Joana Ijazi, Kazuki Osawa, CelineSmith, Ramya Sree Boppana, Taylan Bilal, YumaKoizumi, Ying Xu, Yasemin Altun, Nir Shabat,Ben Bariach, Alex Korchemniy, Kiam Choo, OlafRonneberger, Chimezie Iwuanyanwu, Shubin Zhao,David Soergel, Cho-Jui Hsieh, Irene Cai, ShariqIqbal, Martin Sundermeyer, Zhe Chen, Elie Bursztein,Chaitanya Malaviya, Fadi Biadsy, Prakash Shroff, In-derjit Dhillon, Tejasi Latkar, Chris Dyer, HannahForbes, Massimo Nicosia, Vitaly Nikolaev, SomerGreene, Marin Georgiev, Pidong Wang, Nina Mar-tin, Hanie Sedghi, John Zhang, Praseem Banzal,Doug Fritz, Vikram Rao, Xuezhi Wang, JiagengZhang, Viorica Patraucean, Dayou Du, Igor Mor-datch, Ivan Jurin, Lewis Liu, Ayush Dubey, AbhiMohan, Janek Nowakowski, Vlad-Doru Ion, NanWei, Reiko Tojo, Maria Abi Raad, Drew A. Hud-son, Vaishakh Keshava, Shubham Agrawal, KevinRamirez, Zhichun Wu, Hoang Nguyen, Ji Liu, Mad-havi Sewak, Bryce Petrini, DongHyun Choi, IvanPhilips, Ziyue Wang, Ioana Bica, Ankush Garg,Jarek Wilkiewicz, Priyanka Agrawal, Xiaowei Li,Danhao Guo, Emily Xue, Naseer Shaik, Andrew Leach, Sadh MNM Khan, Julia Wiesinger, SammyJerome, Abhishek Chakladar, Alek Wenjiao Wang,Tina Ornduff, Folake Abu, Alireza Ghaffarkhah, Mar-cus Wainwright, Mario Cortes, Frederick Liu, JoshuaMaynez, Andreas Terzis, Pouya Samangouei, Ri-ham Mansour, Tomasz Kepa, Franois-Xavier Aubet,Anton Algymr, Dan Banica, Agoston Weisz, An-dras Orban, Alexandre Senges, Ewa Andrejczuk,Mark Geller, Niccolo Dal Santo, Valentin Anklin,Majd Al Merey, Martin Baeuml, Trevor Strohman,Junwen Bai, Slav Petrov, Yonghui Wu, Demis Has-sabis, Koray Kavukcuoglu, Jeffrey Dean, and OriolVinyals. 2024. Gemini 1.5: Unlocking multimodalunderstanding across millions of tokens of context.Preprint, arXiv:2403.05530.",
  "ADatasets Overview": "For the Enron dataset, we began with the origi-nal Enron email corpus. To focus on original con-tent creation, emails containing only forwardedcontent like email threads, blog posts, and articleswere removed. We then dissected the remainingemails into two distinct parts: previous_contextencompassing any preceding email chain or replycontent, and content representing the original mes-sage drafted by the sender. This careful separation,achieved through a specifically designed heuristic,ensured that only self-written content was consid-ered during analysis. After these steps, we releaseour dataset - Enron-42k. Conversely, the New Yorker dataset requiredminimal pre-processing. This dataset, compris-ing articles from the New Yorker publishing house,was already cleaned, pre-processed, and structuredwith the necessary features for our study. As such,we utilized the New Yorker dataset in its originalform. The LAMP 3U Amazon reviews dataset consistsof customer reviews for a specific product. Weselected this dataset to explore the application ofour methods in a product review domain. Similarto the Enron and New Yorker datasets, the goalwas to leverage user-generated content to under-stand preferences and generate tailored responses.We extracted user intents from the reviews andused these intents to create baselines and rules forfine-tuning our preference agents. This approachmirrors the methodology applied to the other twodatasets, allowing for a consistent evaluation frame-work across different domains.",
  "The rules were generated using preference agentsfor the following ground truth email:": "Paul, Here is an updated version of theagreement I sent to Steve on Friday. SorryI didn't cc you to start with. It's prettymuch exactly as we discussed. Let me knowif you have any questions. Thanks, BillAre: 1. **Tone**: Use a conversational toneandlanguagethroughouttheemail.2.**Length**:Keeptheemailconcise,aimingforatotalofaround50-60words.3.**Introduction**:Begintheemailwithabriefintroductionthatsets the context, using a phrase like\"Hereisanupdatedversionoftheagreement\".4.**Specificdetails**:Ifapplicable,mentionanyrelevantbackgroundinformation,suchaswhoelsetheagreementwassenttoandwhen.5.**Apologyphrasing**:Useaconciseapologyphraselike\"SorryIdidn'tccyoutostartwith\"insteadofalonger,moreformalapology.6.**Request for feedback**: Use a brief,informal phrase like \"Let me know if youhave any questions\" to request feedbackorquestionsfromtherecipient.7.**Closing sentence**: End the email witha concise, informal phrase like \"Thanks\"instead of a more formal closing like\"Best\". 8. **Signature**: Use a simple,formal signature with the sender's name.9.**Sentencestructure**:Useshort,simplesentencesthroughouttheemailto maintain a conversational tone. 10.**Language**: Avoid using overly formal",
  "B.2Thinking Tokens": "<thinking>Thegroundtruthemailisaconciseand informative response to the previouscontext,discussingmarkettrendsandtradingactivities.Keyobservationsinclude:1. **Length and Structure**: The emailisrelativelyshort,consistingof8sentences.Itusesastraightforwardstructure,jumpingdirectlyintotheobservationsandinsightswithoutanyformal greetings or sign-offs.2.**ToneandStyle**:Thetoneisinformal and conversational, with a focuson market terminology and shorthand thatwould be understood by someone familiarwith the trading environment. There arecontractions (e.g., \"saw\", \"taking off\"),colloquial phrases (e.g., \"get whacked\"),and specific market terms (e.g., \"VAR\",\"h/j spread\").3. **Common Phrases and Terminology**:Specifictermslike\"bulls\",\"summeragainstlength\",\"mitigatemargins\",\"absolutepositionlimits\",\"VAR\",\"Access\", and \"bearish to neutral trade\"areused.Thesereflecttheinsiderknowledgeandjargonofthetradingcommunity.4. **Observational Detail**: The emailprovides specific details about marketactivities and price changes. Mention keyplayers (e.g., El Paso) and their actionsin the market.5.**ConsistencywithIntent**:Thecontentmatchestheintentclosely,summarizingmarketmovements,tradingstrategies,andpotentialfuturemovements.Reflectthekeypointsmentioned in the intent, such as bulls'behavior,ElPaso'sbuying,andthechange in the H 20 call price.6.**GrammarandPunctuation**:Theemailusescasualgrammarandminimalpunctuation.Avoidunnecessarycapitalizationandformalsentencestructures.</thinking> <rules>1. **Length and Structure**: The emailshouldbeconcise,consistingofapproximately 8 sentences. Avoid addingformal greetings or sign-offs.2. **Tone and Style**: Use an informal andconversational tone. Include contractionsandcolloquialphrasestoreflectthecasual nature of the communication.3.**MarketTerminology**:Incorporatespecificmarket-relatedtermsandshorthand(e.g.,\"VAR\",\"h/jspread\",\"Access\", \"bearish to neutral trade\").Ensurethelanguagereflectsinsiderknowledgeandjargonofthetradingcommunity.4.**ObservationalDetail**:Providespecific details about market activitiesand price changes. Mention key players(e.g., El Paso) and their actions in themarket.5. **Consistency with Intent**: Ensurethe content matches the intent closely,summarizingmarketmovements,tradingstrategies,andpotentialfuturemovements.6.**GrammarandPunctuation**:Usecasual grammar and minimal punctuation.Avoidunnecessarycapitalizationandformal sentence structures.7. **Specific Phrases**: Use phrases like\"sawalotofthebulls\",\"mitigatemargins/absolutepositionlimits/var\",\"taking off spreads\", \"trading equivalentof 180 on access\", \"get whacked\", \"if hsettles \\$20\", \"H 20 call was tradingfor55onmonday\",\"market'sviewofprobability of h going crazy\".8.**FlowandCoherence**:Ensuretheemailflowslogicallyfromonepointtothenext,maintainingcoherencewhile jumping between observations andinsights.</rules>",
  "Best,Jeff---": "Generated Intents:---Variant 1:<thinking>1. Identify the main event mentioned inthe email.2. Note the person involved and theiraction.3. Mention Jeff's skepticism about theconversation.4. Highlight the intention to keep Michaelinformed.5. Ensure the bullet points are conciseand focused on the key information.</thinking><bullet_points>- Received a message from Jerry Bloomwanting to talk- Skeptical about the conversation- Will update Michael on what Jerry says- Will keep Michael informed</bullet_points>---Variant 2:<thinking>1.Identifythemainsurpriseorunexpected event mentioned in the email.2. Note who initiated the contact and whatthey want to do.3.CaptureJeff'sskepticismanduncertainty about the outcome. 4. Mention Jeff's intention to follow upwith Michael.</thinking><bullet_points>- Jerry Bloom left a message for me, wantsto talk- I'm skeptical and wouldn't get hopes up- Will let Michael know what Jerry says- Uncertainty about the outcome</bullet_points>---",
  "CRule Generation Strategies and TheirImplications": "We explored three distinct strategies for rule gener-ation, each offering insights into how best to cap-ture and leverage user preferences for guiding largelanguage models.R1 involved prompting the large language model(ML) to generate rules based on user intent andtask metadata. While this approach provides a ba-sic framework for rule generation, it lacks the depthof analysis needed to accurately distill user prefer-ences.R2 extended R1 by incorporating \"thinking to-kens,\" (see 5.3), prompting ML to engage ina more deliberate reasoning process before gen-erating rules. This resulted in modest improve-ments, highlighting the benefits of encouragingmore thoughtful rule generation.R3, based on a distillation process, proved mosteffective. This strategy leverages MLs zero-shotoutput as a starting point, prompting it to identifydiscrepancies between its initial response and theground truth. By explicitly focusing on these differ-ences, ML generates rules specifically designed toaddress the missing preference information. Thistargeted approach led to significant performancegains, with R3 outperforming both R2 and R1 by65% on the Enron dataset and 69.7% on the NewYorker dataset.By explicitly identifying the gaps in preferencealignment, the distillation process enables the gen-eration of highly targeted and effective rules.While R3 offers the best performance, it is worthnoting that it incurs an additional inference costcompared to R1 and R2 due to the extra zero-shotgeneration step. In scenarios where computationalresources are limited, R2, which leverages think-ing tokens for improved rule generation withoutthe added inference cost, provides a compelling",
  "DFinetuning Hyperparameter SearchFor Rule Generators": "To identify the optimal configuration, we train fourrule generators on our gold-standard rules, vary-ing the ranks in each case. We implement a 1:1mapping between the LoRA rank and Alpha.As anticipated, our results indicate that higherAlpha values and corresponding ranks lead to im-proved training losses. This trend is illustrated in, which shows the relationship between in-creasing Alpha/rank values and the resulting train-ing performance. These findings underscore theimportance of selecting appropriate parameter set-tings to optimize the rule generators effectiveness.",
  "ESimilarity Metrics": "This work evaluates the similarity between re-sponses generated by different methods and theground truth for a given task. Our primary goalis to assess how effectively each method capturesthe users preferences in terms of style, tone, andword choice. While metrics like BLEU, ROUGE,and TFIDF Cosine similarity are commonly used toevaluate the lexical overlap between texts, they fallshort of capturing the nuanced aspects of stylisticsimilarity crucial to our evaluation.Consider the example in , which showsthe TF-IDF cosine similarity scores for differentmethods on the New Yorker dataset. While thereare slight variations in the median scores, the over-all distributions largely overlap. This suggests thatTF-IDF, which primarily relies on term frequency and inverse document frequency, struggles to differ-entiate between outputs that exhibit clear stylisticdifferences to human readers or as evaluated byGPT-4o. Similarly, in the Enron dataset, an emailusing formal language like \"Dear Mr. Smith\" and\"Sincerely\" might receive a similar BLEU score toan email using informal greetings like \"Hey John\"and \"Cheers\" despite the contrasting styles. Thishighlights the limitations of BLEU in capturing thesubtle variations in word choice that contribute toa specific writing style.Several alternative metrics have been proposedto address the shortcomings of traditional lex-ical overlap measures.BERT Score (Reimersand Gurevych, 2019), for instance, leverages pre-trained BERT embeddings to compute semanticsimilarity between sentences, potentially captur-ing stylistic nuances better than BLEU or ROUGE.Similarly, BLEURT (Sellam et al., 2020) is alearned metric that utilizes a large pre-trained lan-guage model to predict human judgments of transla-tion quality, which can be adapted to assess stylisticsimilarity. However, even these advanced metricsmight not fully capture the complexities of humanpreferences for style and tone, which can be sub-jective and context-dependent (Peng et al., 2017).Given these limitations, we prioritized GPT-4oevaluation and human evaluation for our analysis.Human judgment remains the gold standard forevaluating stylistic similarity, as it reflects the in-herent subjectivity of human preferences. GPT-4o,with its advanced language understanding capabili-ties, can serve as a reliable proxy for human judg-ment, particularly in capturing stylistic nuances(Naismith et al., 2023). By combining GPT-4o",
  "FPersonalization Test": "While the diagonal trend generally holds, there areinstances where an agent trained on one sender per-forms well across multiple senders. For example,the agent trained on Benjamin Rogers emails ex-hibits high BERT Scores across all senders. Wehypothesize that this may be due to the diversity ofRogers email interactions and the larger size of histraining set, which allows the model to learn theunderlying task exceptionally well. Consequently,this agent demonstrates strong performance evenwhen generating emails for other senders, high-lighting the models ability to generalize beyondindividual preferences when trained on sufficientlydiverse data.Here are the un-normalized BERT Score valuesfor the personalization test (for 5 Enron employees).Though these arent a perfect metric, they providea generalized view of the large evaluation spacethat we have:",
  "No Baseline Email Rule Generator": "You are an expert rule generator whosetask is to generate a detailed set ofrules given the metadata of an email,previous context, user intent, and theground truth email. First you must gothrough the metadata carefully, analyzingwhothesenderandreceiveris,thesubject of the email, and the user intent.After analyzing this information, pleasegenerate a set of extremely detailed andgranular set of rules that would help amodel generate an email that is exactlyhow the user would intent to write it.Makesuretherulesarespecifictothegivenuserandreceiverpairandpay close attention to the user intent.Please generate these extremely detailed,specific, and granular set of rules.With Baseline Email Rule Generator You are an expert rule generator whosetask is to ensure that a base email can betransformed into the ground truth email.Youareprovidedwiththefollowing:The intents that were used to generatethe base email, the base email and theground truth email. You must analyze thedifferences between the base email andthe ground truth email in great detailanalyzingeverydifference.Youmustfocus on the following while generating these rules: the difference in the lengthof the emails, the tone, style, structure,commonphrases,nicknames,signature,andanythingelsethatyouthinkisvery important. All these factors mustbecloselyanalyzedtogeneratetheseextremely granular set of rules. Pleasealso mention exactly how long the emailshouldbeandgenerateanextremelydetailed and granular set of rules thatshouldbeabletotransformthebaseemail exactly into the ground truth email.Todothispleasefirstthinkdeeplyandanalyzethesedifferenceswithin<thinking></thinking> tags where you canenlist every possible difference betweenthe base and the ground truth email. Oncethis is done please generate an extremelydetailed and granular set of rules thatcan be used to transform the base email.Do not mention the ground truth emailinyoursetofruleswhatsoeveranddo not talk about removing things fromthe base email. The rules should be anextremely detailed guideline to transformthebasetogroundtruthemail.Therulesshouldnotreferencethegroundtruth or base email, and should be astandalone list of detailed rules. Pleaseinclude these detailed set of rules within<rules></rules> tags.",
  "H.2.2New Yorker DatasetNo Baseline Rule Generation": "You are an expert rule generator whosetask is to help a model generate articlesthat are close to the ground truth articlegiven user intent. You are given somemetadata and the user intent which isthe input to generate an article, andthe ground truth article. Your task isto deeply analyze the intents and groundtruth very carefully and generate a setof rules that you think are very importanttofullycapturethenuancesofthegroundtrutharticle.Whileanalyzingthe article please consider the followingfactors: the exact length of the article,thetone,writingstyle,structure,importantphrases,directquotes,andanythingelsethatyouthinkisveryimportant. First start by analyzing the ground truth article extremely carefullyaccounting for all the important factorswithin <thinking></thinking> tokens. Onceyouhavedonethatlistasetofextremelydetailedandgranularrulestoensurethatallnuancesofthegroundtrutharticlearecapturedtoensurethatthegeneratedarticleisexactly the ground truth article. Includeeverythingincludingphrasesthatareimportant and all stylistic informationthatneedstobecapturedinextremedetail. Please enclose these extremelydetailed, specific, and granular set ofrules within <rules></rules>With Baseline Rule Generations You are an expert rule generator whosetask is to ensure that a base articlecanbetransformedintothegroundtrutharticle.Youareprovidedwiththefollowing:Theintentsthatwereusedtogeneratethebasearticle,the base article and the ground trutharticle. You must analyze the differencesbetween the base and the ground truth ingreat detail analyzing every difference.You must focus on the following whilegenerating these rules: the differenceinthelengthofthearticles,thetone, style, structure, common phrases,nicknames, signature, and anything elsethat you think is very important. Allthese factors must be closely analyzedto generate these extremely granular setof rules. Please also mention exactly howlong the article should be and generatean extremely detailed and granular set ofrules that should be able to transform thebase article exactly into the ground trutharticle. To do this please first thinkdeeplyandanalyzethesedifferenceswithin <thinking></thinking> tags whereyou can enlist every possible differencebetween the base and the ground trutharticle. Once this is done please generatean extremely detailed and granular set ofrules that can be used to transform thebase article. Do not mention the groundtruth or base article in your set ofrules whatsoever. The rules should be anextremely detailed guideline to transform",
  "No Baseline Rule Generation": "You are an expert rule generator whosetask is to help a model generate reviewsthat are close to the ground truth reviewgiven user intent. You are given somemetadata and the user intent which is theinput to generate a review, and the groundtruthreview.Yourtaskistodeeplyanalyze the intents and ground truth verycarefully and generate a set of rulesthat you think are very important to fullycapture the nuances of the ground truthreview. While analyzing the review pleaseconsider the following factors: the exactlength of the review, the tone, writingstyle,structure,importantphrases,specificproductdetails,ratings(ifany),personalanecdotes,comparisons,and anything else that you think is veryimportant. First start by analyzing theground truth review extremely carefullyaccounting for all the important factorswithin <thinking></thinking> tokens. Onceyouhavedonethatlistasetofextremelydetailedandgranularrulestoensurethatallnuancesofthegroundtruthreviewarecapturedtoensurethatthegeneratedreviewisexactly the ground truth review. Includeeverythingincludingphrasesthatareimportant and all stylistic informationthatneedstobecapturedinextremedetail. Please enclose these extremelydetailed, specific, and granular set ofrules within <rules></rules> tags.With Baseline Rule Generation You are an expert rule generator whosetask is to ensure that a base review can betransformed into the ground truth review.You are provided with the following: Theintents that were used to generate thebasereview,thebasereviewandthegroundtruthreview.Youmustanalyzethe differences between the base and theground truth in great detail analyzingevery difference. You must focus on the following while generating these rules:thedifferenceinthelengthofthereviews,thetone,style,structure,common phrases, specific product details,ratings(ifany),personalanecdotes,comparisons, and anything else that youthink is very important. All these factorsmustbecloselyanalyzedtogeneratethese extremely granular set of rules.Please also mention exactly how long thereview should be and generate an extremelydetailed and granular set of rules thatshouldbeabletotransformthebasereviewexactlyintothegroundtruthreview. To do this please first thinkdeeplyandanalyzethesedifferenceswithin <thinking></thinking> tags whereyou can enlist every possible differencebetween the base and the ground truthreview. Once this is done please generatean extremely detailed and granular set ofrules that can be used to transform thebase review. Do not mention the groundtruthorbasereviewinyoursetofrules whatsoever. The rules should be anextremely detailed guideline to transformthe base to ground truth review. Pleaseinclude these detailed set of rules within<rules></rules> tags.",
  "H.3.2New Yorker Dataset": "Youareanexpertarticleevaluator.Givenanumberofcandidatearticlesand the ground truth article, your taskis to pick which one of the candidatearticles is closest to the ground trutharticle. During your evaluation, pleasefocus mainly on elements of the articlelike style, tone, common phrases used,length of the articles, factual accuracy,etc. YOU MUST ALWAYS PICK A WINNER. Here is how your evaluation should looklike:<evaluation><!-- Use this to evaluate each candidatearticle and compare it with the groundtruth --></evaluation><winner><!-- Use this pick the winning candidatearticle.Displaytheoptionthatisclosest to the ground truth. ONLY DISPLAYTHE OPTION NUMBER HERE. For example ifarticle_x is the winner, display only x--></winner>",
  "I.1Human Study Details": "To validate our usage of GPT-4o as an evaluator,we collect human preference data for the samematchups presented to GPT-4o. As seen in Fig13, every human evaluator is provided with clearand specific instructions alongside the ground truth.Evaluators are asked to select which of the twooptions best matches the ground truth. To mitigatebiases, all evaluators receive the prompts in thesame order and are allowed to review and makechanges if needed. We randomly sample 200 com-parison examples of our work vs naive finetuning and our work vs no baseline rules alongside 100comparison examples of our work vs small andlarge baselines. The same set of human evaluatorsreviewed and made choices for each subset. Weremove missing judgments (which amount to < 1%of collected data) and measure the raw agreementpercentage between humans on the same subsetfollowed by the agreement between each humanand GPT-4o.",
  "% (8 participants) come from variousother STEM fields, including Biology,Chemistry, and Physics": "All volunteers were thoroughly briefed on thegoals of this work and provided informed con-sent for data collection and its subsequent pub-lication. The diversity in their educational andprofessional backgrounds ensures a comprehensiveand balanced evaluation of our research.We confirm that our study has received IRB ap-proval from our institution for conducting annota-tions and evaluations of this nature. Our lab has anexisting IRB review that covers this type of annota-tion work for evaluation of this form. All annota-tors were fully briefed on the studys purpose andprovided informed consent for anonymized datarelease. We do not collect or release any personallyidentifiable information (PII). For all datasets, weplan to release only anonymized, binary preferenceinformation to support future research efforts. Thisstudy did not receive federal funding.We believe that these safeguards ensure the eth-ical conduct of our research and the responsiblehandling of data. We are committed to uphold-ing the highest standards of research integrity andprotecting the privacy and rights of all individualsinvolved in our study.",
  "She can talk to someone or answer anyquestions you may have": "Ground Truth EmailIn reference to your daughter and St. Ceciliaskindergarten class, let me know if my wife,Megan, who is a second grade teacher at theSchool, can talk to someone or answer anyquestions for you.. Ben Naive FinetuneBernie: Megan is a 2nd Grade teacher at St.Cecilias and she teaches the Kindergarten class atthat school. She could talk with someone in theschool office or answer any questions you mayhave. Megan can give you the information youneed. Please call her to see if she can help withany of these. Megan is Mrs. M. Rogers @776-4853. If you happen to talk to her - let herknow that Ben Rogers is her husband. Talk to youlater. Ben Preference AgentHey Berney, Megan, my wife who teaches secondgrade at St. Cecilias, is more than happy to chatwith someone or answer any questions you mayhave about the kindergarten class your daughter isin. Let me know if thats something youd beinterested in. Ben GPT-4o JudgementEmail 1: The tone here is informal but introducesa formal element by providing a phone numberand specific instructions on contacting Megan,which adds complexity not present in the groundtruth and misclassifies Megan as a kindergartenteacher. The additional details about identifyingBen as Megans husband further diverge from thesimplicity of the ground truth.Email 2: This email maintains a similar informaland personal tone. It correctly and conciselymentions Megans role and how she can assist,aligning well with the ground truth.Winner: 2",
  "Preference AgentHere are the Mid-Columbia last summer prices!Let me know if you need anything else. Ben": "Large Model baselineDear Bryan, I wanted to follow up on yourrequest for the Mid-Columbia last summer prices.Ive managed to obtain the information you werelooking for. Below are the prices you requested.Let me know if you need anything else. Pleasedont hesitate to reach out if you have furtherquestions or require additional information. Bestregards, Benjamin GPT-4o JudgementEmail 1: Simple and to the point, directly statingwhat was provided and offering further assistance,similar to the ground truths direct offer of helpwithout additional fluff. Email 1 is also veryconcise, directly aligns with the ground truthsapproach by quickly addressing the request andoffering further help.Email 2: Contains formal phrases such as \"DearBryan\", \"I wanted to follow up\" and \"Please donthesitate to reach out,\" which add a level offormality not present in the ground truth. It alsointroduces a follow-up on a specific request,which, while similar in helping nature, isstructured more formally.Winner: 1",
  "Preference AgentHi Mike! Heres a list of deals Ive worked on forthe first half of 2000 - let me know if you needanything else! Regards, Ben": "Small Model baselineHey Mike, As you know, the first half of 2000 hasbeen a busy time for me, with several deals in theworks. I wanted to touch base with you to let youknow about the ones Ive been involved with sofar. Those include... [Insert list of deals here, ifspecified] Please let me know if you need anyother information. Best, Benjamin GPT-4o JudgementEmail 1: Closely matches the ground truth incontent, directly presenting the list of deals andoffering further assistance without additionalcontext or preamble.Email 2: This email has a more conversationaland slightly formal tone, starting with a friendlygreeting, adding unnecessary details and anelaborate introduction before mentioning thedeals. It adds the phrase \"Those include... [Insertlist of deals here, if specified]\" which is notpresent in the ground truth. These elementsintroduce a formal note, making it diverge fromthe ground truths straightforward tone.Winner: 1",
  "Preference Agent (Debra Variant)Contracts finished, just waiting on an attorney toreview it - should be ready first thing in themorning": "Human Annotator JudgementBill Variant: Overtly polite, mentions thecontract even though it isnt present in user intentsor the ground truth, ends email with signoffGerald Variant: Takes an angry and urgent tonewhich isnt present in ground truth email. Doesntinform the recipient when the contract will beready.Debra Variant: Maintains a consistent and directtone while getting the main intent across, Alignsclosest with ground truth.",
  "Preference Agent (Benjamin Variant)I am fine with all of their changes. , Gerald,713-345-4169": "Human Annotator JudgementBill Variant: This email utilizes \"we\" while theground truth and intent specify \"I\". The tone isalso overtly polite. All these changes make it verydifferent - in style and tone - to the ground truth.Gerald Variant: Short, concise and doesnt addunnecessary fluff, aligning with the ground truth.Benjamin Variant: While this is concise, itmisses out on 2/3 rds of the intents and adds anuneccesary number.",
  ": Preference Agent For Bill vs Preference Agents For Others": "InputGiven the metadata of the email, the previouscontext, and the intent of the user, please generatean email.Metadata: From: To: Date: Fri, 19 Oct2001 Subject: RE: Meeting with Britt WhitmanPrevious Context: From: Sheppard, KathrynSent: Friday, October 19, 2001 10:02 AM To:Williams III, Bill Subject: Meeting with BrittWhitman Bill, Would you be available to meetwith Britt Whitman, Houston CompetitiveAnalysis, at 11:00 a.m. on Tuesday, 10/23?Thanks, KathyIntent:",
  "Preference Agent (Gerald Variant)Kathy, I will be available on Tuesday, but it wouldhave to be either earlier in the day or later in theday. I could meet with you at 1. 9 AM or 2. 3 PM": "Preference Agent (Dutch Variant)hey kathy, tuesday works for me, but not at 11am.my schedule is pretty open, so how about 9am or3pm instead? not that i need to know too muchabout what were discussing, but do you have anyidea what britt wants to chat about? let me knowwhen works for you, bill Human Annotator JudgementBill Variant: concise - even reorders theproposed reschedule timings to be AM then PM.Gerald Variant: concise and clear but doesntinclude the intent which asks for the purpose ofthis call.Dutch Variant: The tone is friendlier, chattier,and longer than the ground truth. Divergesbecause of additional details.",
  "The exhibition opens on April 3 at MOMAPS1": "Ground Truth ArticleYou may know the ancient Chinese game Go, butthe traditional Plant Contest (above), whichCao Fei staged in 2000, is probably less familiar.The picture is one of the earliest works in theBeijing-based artists show at MOMA PS1,opening on April 3. Cao, who was born in 1978,delves into the fantasies and entertainments of hergeneration, videotaping cosplayers in her nativeGuangzhou and building a virtual city in theonline world Second Life, where her avatar,China Tracy, has served as a philosopher-guide. Preference AgentThe Fantasies of Chinas Generation X Cao Fei isset to unveil an immersive and thought-provokingexhibition at MOMA PS1, which delves into thefantasies and entertainments of ChinasGeneration X. The exhibition, opening on April 3,promises to be a fascinating exploration of thecultural landscape of a generation that grew up ina changing China. One of the highlights of theexhibition is Cao Feis 2000 work, \"PlantContest\", a traditional Chinese performance thatoffers a glimpse into the countrys rich culturalheritage. This work is complemented by CaoFeis videotapes of cosplayers in her nativeGuangzhou, which provide a window into thelives of young Chinese people and theirfascination with fantasy and role-playing. Cao Feihas also created a virtual city in the online worldSecond Life, where her avatar, China Tracy,serves as a philosopher-guide.",
  ": Preference Agents vs Naive Finetune (Page 1 of 2)": "Naive FinetuneIn China, its often said that each generation is asocialist addendum. The Millennials are called theHelloKittyState Generation. Cao Fei was born in1978, smack in the middle of Chinas GenerationX. Her Triple Artist show at MOMA PS1 (April3-June 5) includes a work she made in 2000, asort of Fluxus performance in a Guangzhoubarber shop, in which participants award hereggplants for proper beards. Last year she wasvideotaping her own local cosplayers. Cao Fei(based in Beijing) has also spent time building herown virtual city in the online world of SecondLife: China Tracy serves as a philosopher-guide. GPT-4o JudgementArticle 1 is a clear winner due to its similaritiesin style, tone, and factual accuracy with theground truth article. The structure andorganization of the articles are also similar,starting with an introduction to the artist and theexhibition, followed by a description of thevarious works featured. <...>.Article 2 has a more informal tone and usescolloquial language, which differs from theground truth article. The structure of the article isalso less cohesive, jumping between differenttopics and ideas without a clear flow. It alsomentions the exhibit ending on June 5th whichisnt present in the ground truth. While it doesmention some of the same works as the groundtruth article, the descriptions are brief and lack thedetail and context provided in Candidate Article1.Winner: 1"
}