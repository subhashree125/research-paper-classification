{
  "Abstract": "Large language models have seen widespreadadoption in math problem-solving. However, ingeometry problems that usually require visualaids for better understanding, even the most ad-vanced multi-modal models currently still facechallenges in effectively using image informa-tion. High-quality data is crucial for enhanc-ing the geometric capabilities of multi-modalmodels, yet existing open-source datasets andrelated efforts are either too challenging fordirect model learning or suffer from misalign-ment between text and images. To overcomethis issue, we introduce a novel pipeline thatleverages GPT-4 and GPT-4V to generate rel-atively basic geometry problems with alignedtext and images, facilitating model learning.We have produced a dataset of 4.9K geome-try problems and combined it with 19K open-source data to form our GeoGPT4V dataset.Experimental results demonstrate that the Ge-oGPT4V dataset significantly improves the ge-ometry performance of various models on theMathVista and MathVision benchmarks. Thecode is available at",
  "Introduction": "With large language models (LLMs) demonstratingformidable performance, their application in solv-ing mathematical problems has become an increas-ingly popular trend (Toshniwal et al., 2024; Wanget al., 2023b; Gou et al., 2023; Wang et al., 2023a).Prior research has indicated that humans encountera significant reduction in accuracy when resolvinggeometric problems devoid of visual aids (Chenet al., 2021). Thus, the integration of visual infor-mation from images is imperative for accurately",
  "*These authors contributed equally to this work.Corresponding author.This work is done when Shihao Cai is an intern at Al-ibaba": "solving of such mathematical problems, necessi-tating the visual perception capabilities of multi-modal large language models (MLLMs). However,even the best batch of MLLMs available now (suchas GPT-4V (OpenAI, 2023b), Gemini (Anil et al.,2023)) still lag significantly behind human perfor-mance (Wang et al., 2024). Therefore, researchersare eagerly exploring methods to enhance the geo-metric capabilities of MLLMs.To enhance the geometric capabilities ofMLLMs, an important step is to construct corre-sponding high-quality data (Gao et al., 2023; Zhouet al., 2023b; Chen et al., 2022). Nevertheless, cur-rent data often suffer from two main issues. On theone hand, most open-source datasets are quite chal-lenging, making it difficult for models to directlylearn geometric capabilities from them (Bengioet al., 2009; Xu et al., 2020). For instance, the Uni-GEO (Chen et al., 2022) dataset consists of prob-lems extracted from high school textbooks, but themodels have not been exposed to the correspond-ing foundational knowledge. On the other hand,current data augmentation techniques (Gao et al.,2023), using ChatGPT-3.5 to adjust numerical val-ues in the text, fail to harmonize these changes withthe corresponding values in images. Consequently,mismatches between the altered text and imagescan bewilder the model and impede its learningprocess (Hessel et al., 2021; Yao et al., 2022).In this paper, we address the aforementionedissues by introducing a straightforward and effi-cient pipeline for generating geometric problemdata. Our objectives are two-fold: (1) to creategeometric problems that facilitate the models ac-quisition of basic geometric concepts, and (2) toensure that the image and the text of the generatedgeometric problems are well-aligned. In detail, wefirst employ GPT-4V to create a collection of sim-plified geometric problems based on open-sourcedatasets. Subsequently, we harness the capabilitiesof GPT-4 (OpenAI, 2023a) to generate K individ- ual pieces of Wolfram1 code for each geometricproblem previously crafted. The code is then exe-cuted to produce K distinct geometric images. Fi-nally, GPT-4V is employed to score these images,allowing us to select the best one that optimallyaligns with the associated textual descriptions.Through the above pipeline, we generate adataset comprising 4.9K geometric problems char-acterized by simplicity and image-text match-ing. We then mix our generated problems with19K problems from open-source datasets to for-mulate a dataset with various difficulty levels,named GeoGPT4V. We have conducted compre-hensive experiments on the geometry problem sub-set of MathVista (Lu et al., 2024b) and MathVi-sion (Wang et al., 2024) datasets, two commonlyused datasets for multi-modal math. Our experi-mental results show that models of various sizesand types can achieve significant improvementsin geometric capabilities after training with ourdataset (achieving 58.2% and 33.8% relative im-provement for LLaVA-1.5-7B (Liu et al., 2023b)and ShareGPT4V-7B (Chen et al., 2023a), re-spectively, on Geometry problem solving (GPS)minitest split of MathVista), which validates theeffectiveness of our approach.In conclusion, the contributions of this paper aresummarized as follows: We first introduce a novel pipeline capable ofautomatically generating simple geometric datawith aligned image-text pairs.",
  "The Wolfram is a computational language designed tohandle various computing and data analysis tasks, possessinga formidable capability for geometric visualization": "et al., 2023; OpenAI, 2023b; Liu et al., 2023c).These MLLMs integrate visual information withlinguistic data, enhancing their capabilities sig-nificantly (Lu et al., 2024a; Li et al., 2023; Yeet al., 2023; Dai et al., 2023).Closed-sourcemodels, such as GPT-4V (OpenAI, 2023b), Gem-ini (Anil et al., 2023), and Qwen-VL-Max (Baiet al., 2023), have demonstrated remarkable pro-ficiency in image comprehension and cognitivetasks. For open-source models, LLaVA (Liu et al.,2023c,b, 2024) utilizes linear projection to bridgethe visual encoder and the language model, achiev-ing commendable performance in multi-modaltasks.Building upon the LLaVA architecture,ShareGPT4V (Chen et al., 2023a) employs high-quality instructional data to further enhance modelcapabilities. Moreover, InternVL-Chat (Chen et al.,2023b) upscales its visual encoder to 6 billion pa-rameters. InternLM-XComposer2 (Dong et al.,2024) excels in free-form text-image compositionand understanding. Although these MLLMs haveshown powerful visual capabilities, MLLMs stillconfront challenges when it comes to mathemati-cal problem-solving, as highlighted by recent stud-ies (Wang et al., 2024; Lu et al., 2024b; Yue et al.,2023). Mathematical Problem Solving.The remark-able reasoning capabilities of LLMs have spurredresearchers to harness them for solving mathemati-cal problems (Zhou et al., 2023a; Shao et al., 2024;Lightman et al., 2023; Zhao et al., 2023). In therealm of pure text-based mathematical tasks, Wiz-ardMath (Luo et al., 2023) enhances model perfor-mance by refining instructions through a process ofdownward and upward instruction evolution. Meta-Math (Yu et al., 2023) approaches the challenge bybootstrapping mathematical questions and rewrit-ing them from various perspectives to improve un-derstanding and problem-solving. However, as pre-vious studies have found, humans accuracy signif-icantly decreases when solving geometry problemswithout images (Chen et al., 2021). Therefore, ge-ometry problems necessitate the visual perceptionabilities of multi-modal models to fully compre-hend and solve them. UniGeo (Chen et al., 2022)addresses this by compiling geometry problemsfrom high school textbooks and introducing a uni-fied multitask geometric transformer framework totackle calculation and proving problems simulta-neously in the form of sequence generation. G-LLaVA (Gao et al., 2023) leverages ChatGPT-3.5",
  "Wolfram": ": Pipeline of our geometric data generation. During the first step, we employ GPT-4V to generatesimplified geometric question-answer pairs based on open-source datasets. We highlight the simplified partscompared to the original questions. During the second step, we employ GPT-4 to generate K Wolfram code foreach question-answer pair. During the third step, we execute K code to obtain K images. During the fourth step,we employ GPT-4V to score the degree of alignment between the generated images and the questions. We choosethe image with the highest score. Finally, we can obtain simplified and image-text matching geometric problems. to create geometric question-answer pairs and torewrite the textual content within questions. Nev-ertheless, this approach of textual rewriting alonemay result in discrepancies between images andtext, leading the model to produce incorrect or un-realistic outputs (Liu et al., 2023a). This highlightsthe ongoing challenge of aligning textual and visualinformation in multi-modal mathematical problem-solving.",
  "Method": "In this section, we will elaborate on the pipelinewe have constructed. An overview of our pipelineis depicted in . Specifically, our processincludes: (1) generating new question-answer pairs(.1), (2) producing corresponding geo-metric images (.2), and (3) scoring andfiltering based on the image-text matching degree(.3).Formally, the original data from the open-sourcedatasets can be represented as D = {Q, A, I},where Q represents the question, A represents theanswer, and I represents the image.",
  "Due to the prevalence of more challenging geomet-ric problems in open-source datasets, to facilitate": "our models learning of basic geometric concepts,we initially simplify these difficult problems to gen-erate easier geometric question-answer (QA) pairs.In detail, we utilize GPT-4V (OpenAI, 2023b) togenerate QA pairs from the dataset D = {Q, A, I}.We instruct GPT-4V to craft simplified problemsthat are derived from the original geometric QApairs to acquire QA pairs containing fundamentalgeometric concepts. In detail, we prompt GPT-4Vto consider these three perspectives: (1) generat-ing lead-up problems, (2) generating sub-problems,and (3) incorporating the conclusions from the an-swer into the conditions of the question, which canreduce the complexity of the question. To preventGPT-4V from generating the same simplified ques-tions, we also ask GPT-4V to generate questionsthat are as diverse as possible. Additionally, forefficiency, the instruction also asks GPT-4V to gen-erate textual descriptions of images aimed at sup-porting the subsequent phase of image generation.The detailed prompt can be found in Appendix C.1.In practice, we generate N (N = 3) new datapoints based on a single original data point to im-prove efficiency and reduce API costs. After thisphase, the data we obtain can be formally repre-sented as D1 = { Q, A,Des} whereDes repre-sents the image description.",
  "Geometric Images Generation": "It is important to highlight that the newly generatedQA pairs may not correspond directly to the origi-nal images, which could hurt the models learningprocess. To ensure congruity between the textualcontent and the visual aspects, it is essential to pro-duce new images that align with the generated QApairs. To address this issue, we employ Wolfram, apowerful software tool capable of executing codeto generate geometric images.In detail, we utilize GPT-4 (OpenAI, 2023a) togenerate Wolfram code based on the dataset D1.Firstly, we feed the questions, answers, and imagedescriptions as prompts to GPT-4 to generate Wol-fram code. During the generation process, we in-struct GPT-4 to explicitly name all variables withinthe code, with the aim of facilitating a clearer un-derstanding and assisting GPT-4 in recognizing therelationships between code elements and the givenquestions. The detailed prompt can be found inAppendix C.2. Finally, we execute the Wolframcode, resulting in the generation of new images.In practice, it is noticed that employing GPT-4to generate code is unstable. Thus, we generate K(K = 3) distinct code from the same data to in-crease the probability of obtaining the correct code.Consequently, we can obtain K distinct imagescorresponding to K code. It can be represented asD2 = { Q, A, I(1), I(2), . . . , I(K)}, where I(i) rep-resents the i-th image generated for each question.",
  "Scoring and Filtering": "After generating K images using Wolfram for eachquestion, we need to select the most suitable oneto be used as the final image in our dataset.Concretely, we employ GPT-4V to assign a scoreranging from 0 to 1 that reflects the degree of cor-respondence between an image generated for thequestion and the question itself; a higher score sig-nifies a stronger alignment. To augment the scoringproficiency of GPT-4V, drawing inspiration fromthe Chain-of-Thought (Wei et al., 2022) , we in-struct GPT-4V to articulate the rationale underlyingits evaluation before determining the ultimate score.The detailed prompt can be found in Appendix C.3.Finally, for each question associated with K dis-tinct generated images, we obtain K correspondingscores. For each question, we retain the image withthe highest score as I. Note that, if this score is lessthan 0.9, we consider that the image for this ques-tion has not been well-generated, and we discard",
  "Datasets": "In this study, to minimize costs, we selected thefirst 1,500 samples from the training sets of theUniGEO-Proving (Chen et al., 2022), Geome-try3K (Lu et al., 2021), and GeoQA (Chen et al.,2021) to create UniGEO-Proving_Enhanced, Ge-ometry3K_Enhanced, and GeoQA_Enhanced forvalidating the effectiveness of our method. Sub-sequently, we combine the generated geometricproblems with those from open-source datasets,including ChartQA (Masry et al., 2022), UniGEO-Calculation (Chen et al., 2022), the original Geom-etry3K (Lu et al., 2021), and GeoQA+ (Cao andXiao, 2022), to form a new dataset with variousdifficulty levels, dubbed GeoGPT4V. A detailedbreakdown of the datasets is provided in .",
  "Difficulty Evaluation": "As mentioned in , our pipeline will takeoriginal data D as input and output generated dataD. We aim to generate easier data than the originalone to facilitate model learning of basic geometricknowledge. This section demonstrates the efficacy of our pipeline by comparing the difficulty levelsof D and D.We initiate this by forming a data pair P1 ={D, D} and utilize GPT-4V to assess the relativedifficulty of the data points. To mitigate the biasthat GPT-4V may have due to the presentation or-der, we also consider the pair P2 = { D, D}, ob-tained by swapping the order of the data points. IfGPT-4V produces different outputs based on P1and P2, we conclude that the difficulty of D andD is equal. A detailed prompt can be found inAppendix C.4.In practice, we randomly sample 500 pairs ofgenerated and corresponding original data points.The outcome, presented in a, reveals thatover 80% of the questions in the generated datasetare of equal or lesser difficulty compared to theoriginal questions. This indicates that our pipelineis successful in generating data that is simpler thanthe original dataset.",
  "Image-text Matching Evaluation": "As mentioned in the previous section, the align-ment between text and images is a critical aspect ofgeometric problem data. To illustrate that the gen-erated images are better suited for the simplifiedproblems than the original images, we replace thegenerated images with the original image for eachquestion, resulting in new data D = { Q, A, I}.Consequently, in this section, we will compare thelevel of image-text matching in our generated dataD with D and the QA data produced by priormethods G-LLaVA (Gao et al., 2023). Similarto the score function in .3, we employGPT4-V to score the degree of alignment betweenthe images and the questions.In detail, we randomly select 500 data points foreach dataset and show the average scores of thethree datasets in b. The results indicatethat our generated data, D, exhibits a significantlyhigher degree of image-text matching than D, aswell as the dataset enhanced by G-LlaVA (0.9636for D, 0.7276 for D, and 0.6754 for G-LlaVA).Moreover, it is observed that G-LlaVAs image-textmatching score is the lowest, which confirms ourhypothesis that simply scaling the size of numberswithin problems is an inappropriate approach.",
  "(b)": ": The manual data analysis results. Figure (a)is a manual comparison chart of the difficulty betweenthe generated and original data. In this figure, Easierrepresents that the generated data is easier than the orig-inal data; Harder represents that the generated datais harder than the original data; Equal represents thatthe generated and original data have the same difficultylevel. Figure (b) is a manual comparison chart of theimage-text matching between the generated and originalimages. In this figure, Original represents that theoriginal image better matches the question text; Gener-ated represents that the generated image better matchesthe question text; Equal represents that the generatedimage and the original image match the text to the samedegree.",
  "Experimental Setup": "Benchmarks.We utilize two widely used bench-marks, which encompass numerous multi-modelgeometric problems, to evaluate the effectivenessof our proposed GeoGPT4V dataset. The detailedinformation of these benchmarks is as follows: MathVista (Lu et al., 2024b) is a mathematicalreasoning benchmark in visual contexts. It in-cludes diverse visual contexts, such as naturalimages, geometry diagrams, charts, etc. Math-Vista includes multiple-choice questions as wellas open-ended questions. The MathVista testset comprises 5141 examples without groundtruth answers and provides 1000 examples withground truth answers known as MathVista test-mini.",
  "Closed-source Models": "Qwen-VL-Plus-38.539.338.9017.912.715.48.911.66.410.014.311.3112.06Qwen-VL-Max----19.116.916.412.213.314.219.811.517.315.61Gemini-1.0-Pro-40.441.040.7010.720.120.221.119.119.020.014.320.818.37Gemini-1.0-Ultra-56.255.655.90----------GPT-4V-50.551.050.7532.121.122.114.422.022.220.923.825.622.69 : Overall results of our best model and other open-source and closed-source models on the MathVistaand MathVision. We present the detailed score for all the tasks related to geometry such as GPS and AnaG,as well as the average score over these tasks in two benchmarks denoted as AVG. Due to limited space, weutilize abbreviations for these geometry-related tasks and illustrate the detailed task name in the Appendix A. Boldresults indicate the best results for all models, and the red results indicate the best results among the open-sourcemodels. indicates our re-implemented model without an official checkpoint. indicates our re-implemented testresults missed in benchmarks or origin papers. InternVLrepresents the abbreviation for InternVL-Chat-V1.2-Plus.InternLM-VLrepresents the abbreviation for InternLM-XComposer2-VL. The suffix -G to the model nameindicates a model trained on the GeoGPT4V.",
  "LLaVA-1.5 (Liu et al., 2023c,b) utilizes linear": "layers to connect the vision encoder and the largelanguage model (LLM). In the pre-training stage,LLaVA-1.5 keeps the vision encoder and theLLM frozen, and only trains linear layers. In thefine-tuning stage, it freezes the vision encoderand trains the linear layers and the LLM. ShareGPT4V (Chen et al., 2023a) has an archi-tecture similar to LLaVAs. However, in the pre-training stage of ShareGPT4V, both the visionencoder and the language model remain unfrozen.The training data is high-quality, detailed descrip-tion data generated by GPT-4V. InternVL-Chat-V1.2-Plus (Chen et al., 2023b)utilizes the InternViT (Chen et al., 2023b) as itsvisual encoder, which has 6 billion parameters.Whats more, it scales LLM to 34B and utilizes afine-tuning dataset with 12 million samples. Implementation Details.For data generation,we employ the gpt-4-vision-preview and gpt-4-1106-preview API provided by OpenAI for GPT-4V and GPT-4. For model training, all the modelsare trained on NVIDIA A100 GPUs with PyTorch",
  "GeoGPT4V32.6932.2232.469.5216.889.6221.1119.08 11.06 17.159.4315.4814.37": ": Ablation for image generation and image scoring. - Image Generation denotes the exclusion ofnewly generated geometric images. - Image Scoring signifies the random selection of generated images, ratherthan utilizing GPT4V to score and choose them. For comparison, we also represent the results from the officialLLaVA-1.5-7B model in the first line and GeoGPT4V in the last line. Bold results indicate the best results for allmodels. indicates our re-implemented test results missed in benchmarks or origin papers. version 2.0.1. To ensure a fair comparison, we keepthe training parameters consistent with those spec-ified by the models original authors and train themodels for one epoch. Detail training parametersare demonstrated in Appendix B.",
  "Main Results (RQ1)": "We evaluate the performance of various open-source models on MathVista testmini (short asMathVista) and MathVision test (short as MathVi-sion) benchmarks after training on the GeoGPT4Vdataset to demonstrate our proposed methods ef-fectiveness. For convenience, we append the suffix-G to the model name to indicate a model trainedon the GeoGPT4V dataset, such as LLaVA-1.5-G. Since our method focuses on geometric data,we present detailed scores for all the tasks related togeometry and the average score over these tasks in. The complete set of scores can be found inAppendix D.1 and D.2. In Appendix D.3, we com-pare the geometric capabilities of our best model,InternVL-Chat-V1.2-Plus-GeoGPT4V, with otheropen-source and closed-source models.The experimental results from indicatethat our dataset can effectively improve differentmodels geometric capabilities. First of all, our pro-posed GeoGPT4V has exhibited an improvement inthe average scores across all geometry-related taskson both MathVista and MathVision benchmarks, in-dicating that GeoGPT4V can enhance the modelsgeneral geometry performance. Moreover, our pro-posed GeoGPT4V has brought improvements tomost geometry-related tasks in both benchmarksin all scales and types of models. Furthermore,our GeoGPT4V significantly bridges the gap ingeometric capabilities between open-source andclosed-source models, except InternVL-Chat-V1.2-Plus, which has already employed a substantial",
  "In-depth Analysis": "To comprehensively analyze the effectiveness ofGeoGPT4V, we design a series of analyzing ex-periments from various perspectives. Firstly, wedesign ablation experiments from the standpointof the efficacy of generating new geometric im-ages and selecting generated images with GPT4Vscores. Subsequently, we conduct experiments todemonstrate the substantial performance improve-ment brought by GeoGPT4V stemming from thegenerated data rather than the utilization of open-source data. Due to resource and space limitations,we leverage LLaVA-1.5-7B for analytical experi-ments and conduct evaluations on both MathVistaand MathVision.",
  "Effect of Generating New Images (RQ2)": "We validate the effectiveness of the newly gener-ated geometric images by replacing the imagesgenerated in GeoGPT4V with their original coun-terparts and training the model on them. In detail,we first substitute the newly generated images fromGeoGPT4V with the original images while retain-ing the simplified questions generated, formulatinga new dataset denoted as D. Subsequently, wetrain the LLaVA-1.5-7B model on D and compareits geometric capabilities with the model trained onGeoGPT4V.Based on results demonstrated in , wehave following observations: Firstly, the modeltrained on D exhibits inferior performance com-pared to the model trained on GeoGPT4V, indicat-ing the effectiveness of the newly generated images.Secondly, the model trained on D demonstratesstronger performance than the model trained with-out the use of D, thereby validating the efficacy of",
  "Is Scoring Necessary? (RQ3)": "As mentioned in .3, K images are scored,and the one with the highest score is selected fromthis set. To demonstrate the necessity of scoring,we formulate a new dataset D by directly mod-ifying the selection method to randomly choosefrom the K images while keeping all other aspectsunchanged. Consequently, we analyze the perfor-mance of the LLaVA-1.5-7B trained on D.According to results demonstrated in ,we can find that the model trained on D exhibitsinferior performance on most tasks compared to themodel trained on GeoGPT4V. The results indicatethat the quality of the images obtained via rankingsurpasses those chosen randomly in overall aspects..It is also worth noting that the model trained onD performs better on a few tasks, possibly due tothe relative similarity of the generated images inthese tasks. While using GPT-4V for selection mayintroduce bias, random selection has the potentialto enhance diversity.",
  "Are the Open-source Datasets Enough?(RQ4)": "To demonstrate that the performance improvementsbrought by GeoGPT4V are not solely reliant onopen-source data, we compare the performance ofmodels trained using various combinations of open-source and our generated data. In detail, as illus-trated in , we construct three tiers of datasets.Firstly, we combine all open-source datasets to cre- ate the Base dataset. Subsequently, we replacethe original data from the Base dataset with thedata generated by our pipeline, resulting in the Re-place dataset. Lastly, we mix the generated datawith all the data from the Base dataset to formthe Mix dataset. It is notable that GeoQA is asubset of GeoQA+. Thus we only use GeoQA+ inthese three dataset settings, rather than using bothGeoQA+ and GeoQA.We finetune LLaVA-1.5-7B separately on thesethree datasets and evaluate their performance in, with observations as follows: Althoughthe Base dataset, constructed using open-sourcedata, provides moderate geometric capabilities, ourReplace and Mix datasets exhibit even greaterenhancements in geometric performance. This notonly demonstrates the effectiveness of the data gen-erated by our pipeline but also indicates that the im-provements afforded by GeoGPT4V are not solelyderived from open-source data.",
  "Conclusion": "In this study, we propose a novel pipeline to en-hance the geometric capabilities of MLLMs. Wehave proposed data generation methods for multi-modal geometric tasks involving problem simpli-fication and the generation of images that matchnewly generated text. Specifically, we use GPT4Vand GPT4 to generate sub-problems or lead-upproblems for given geometric tasks, along withthe corresponding Wolfram code that can be ex-ecuted to generate geometric images. Based on the pipeline, we have generated 4.9K simplifiedand image-text matching geometric problems. Wemix our generated data with 19K open-source datato formulate a dataset with various difficulty lev-els, named GeoGPT4V. After training on the Ge-oGPT4V dataset, various models have improved ge-ometric scores on both MathVista and MathVisionbenchmarks. The extensive experimental resultsdemonstrate the effectiveness of the GeoGPT4Vdataset. We have open-sourced the GeoGPT4Vdataset and the checkpoints of models trained onthe GeoGPT4V dataset, with the aim of fosteringthe communitys growth.",
  "Limitations": "This paper focuses on the generation of geometricimages. We employ GPT-4 to generate Wolframcode, which can be executed to generate images.However, this approach is unstable and may resultin poor image quality. Thats why we use GPT-4Vto score the images, which leads to more API callsand increased costs.Whats more, this paper only considers simpli-fying open-source geometric problems. However,generating more complex problems is also worthconsidering, as it will generate more complex geo-metric images and help models improve complexreasoning capabilities. Our future work will ex-plore the more accurate generation of complex ge-ometric images.Finally, multi-modal mathematics is not limitedto geometric problems. It also includes tasks suchas chart question answering and function questionanswering. Generating richer charts and functionimages is also part of our future exploration work.",
  "This work was supported by Alibaba Groupthrough Alibaba Research Intern Program": "Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, JohanSchalkwyk, Andrew M. Dai, Anja Hauth, Katie Mil-lican, David Silver, Slav Petrov, Melvin Johnson,Ioannis Antonoglou, Julian Schrittwieser, AmeliaGlaese, Jilin Chen, Emily Pitler, Timothy P. Lilli-crap, Angeliki Lazaridou, Orhan Firat, James Molloy,Michael Isard, Paul Ronald Barham, Tom Henni-gan, Benjamin Lee, Fabio Viola, Malcolm Reynolds,Yuanzhong Xu, Ryan Doherty, Eli Collins, ClemensMeyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Pi-queras, Maxim Krikun, Iain Barr, Nikolay Savinov,Ivo Danihelka, Becca Roelofs, Anas White, AndersAndreassen, Tamara von Glehn, Lakshman Yagati,Mehran Kazemi, Lucas Gonzalez, Misha Khalman,Jakub Sygnowski, and et al. 2023. Gemini: A fam-ily of highly capable multimodal models. CoRR,abs/2312.11805. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,and Jingren Zhou. 2023.Qwen-vl: A versatilevision-language model for understanding, localiza-tion, text reading, and beyond.arXiv preprint",
  "the 29th International Conference on ComputationalLinguistics, COLING 2022, Gyeongju, Republic ofKorea, October 12-17, 2022, pages 15111520. In-ternational Committee on Computational Linguistics": "Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin,Chongyu Chen, and Xiaodan Liang. 2022. Unigeo:Unifying geometry logical reasoning via reformulat-ing mathematical expression. In Proceedings of the2022 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP 2022, Abu Dhabi,United Arab Emirates, December 7-11, 2022, pages33133323. Association for Computational Linguis-tics. Jiaqi Chen, Jianheng Tang, Jinghui Qin, XiaodanLiang, Lingbo Liu, Eric P. Xing, and Liang Lin.2021.Geoqa: A geometric question answeringbenchmark towards multimodal numerical reasoning.In Findings of the Association for ComputationalLinguistics:ACL/IJCNLP 2021, Online Event,",
  "Processing Systems 2023, NeurIPS 2023, NewOrleans, LA, USA, December 10 - 16, 2023": "Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, KaiDong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhu-oshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng,Hanwei Xu, Zhenda Xie, and Chong Ruan. 2024a.Deepseek-vl: Towards real-world vision-languageunderstanding. CoRR, abs/2403.05525. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu,Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng,Kai-Wei Chang,Michel Galley,and JianfengGao. 2024b.Mathvista: Evaluating mathemati-cal reasoning of foundation models in visual con-texts.In International Conference on Learning",
  "Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan LeBras, and Yejin Choi. 2021. Clipscore: A reference-free evaluation metric for image captioning.In": "Proceedings of the 2021 Conference on EmpiricalMethods in Natural Language Processing, EMNLP2021, Virtual Event / Punta Cana, DominicanRepublic, 7-11 November, 2021, pages 75147528.Association for Computational Linguistics. Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, ShuoZhang, Jingxu Yang, Yabo Sun, Yuliang Liu, andXiang Bai. 2023. Monkey: Image resolution andtext label are important things for large multi-modalmodels. CoRR, abs/2311.06607.",
  "Representations (ICLR)": "Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, SiyuanHuang, Xiaodan Liang, and Song-Chun Zhu. 2021.Inter-gps:Interpretable geometry problem solv-ing with formal language and symbolic reason-ing. In Proceedings of the 59th Annual Meeting ofthe Association for Computational Linguistics andthe 11th International Joint Conference on NaturalLanguage Processing, ACL/IJCNLP 2021, (Volume1: Long Papers), Virtual Event, August 1-6, 2021,pages 67746786. Association for ComputationalLinguistics. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-guang Lou, Chongyang Tao, Xiubo Geng, QingweiLin, Shifeng Chen, and Dongmei Zhang. 2023. Wiz-ardmath: Empowering mathematical reasoning forlarge language models via reinforced evol-instruct.CoRR, abs/2308.09583. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq R.Joty, and Enamul Hoque. 2022. Chartqa: A bench-mark for question answering about charts with visualand logical reasoning. In Findings of the Associationfor Computational Linguistics: ACL 2022, Dublin,Ireland, May 22-27, 2022, pages 22632279. Asso-ciation for Computational Linguistics.",
  "Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, MingjieZhan, and Hongsheng Li. 2024. Measuring mul-timodal mathematical reasoning with math-visiondataset. CoRR, abs/2402.14804": "Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, SichunLuo, Weikang Shi, Renrui Zhang, Linqi Song,Mingjie Zhan, and Hongsheng Li. 2023a. Mathcoder:Seamless code integration in llms for enhanced math-ematical reasoning. CoRR, abs/2310.03731. Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, DamaiDai, Yifei Li, Deli Chen, Y. Wu, and Zhifang Sui.2023b. Math-shepherd: Verify and reinforce llmsstep-by-step without human annotations.CoRR,abs/2312.08935. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,and Denny Zhou. 2022. Chain-of-thought prompt-ing elicits reasoning in large language models. InAdvances in Neural Information Processing Systems35:Annual Conference on Neural Information",
  "Processing Systems 2022, NeurIPS 2022, NewOrleans, LA, USA, November 28 - December 9,2022": "Benfeng Xu, Licheng Zhang, Zhendong Mao, QuanWang, Hongtao Xie, and Yongdong Zhang. 2020.Curriculum learning for natural language understand-ing. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics, ACL2020, Online, July 5-10, 2020, pages 60956104. As-sociation for Computational Linguistics. Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu,Minzhe Niu, Hang Xu, Xiaodan Liang, ZhenguoLi, Xin Jiang, and Chunjing Xu. 2022.FILIP:fine-grained interactive language-image pre-training.In The Tenth International Conference on LearningRepresentations, ICLR 2022, Virtual Event, April25-29, 2022. OpenReview.net. Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, AnwenHu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, andJingren Zhou. 2023. mplug-owl2: Revolutionizingmulti-modal large language model with modality col-laboration. CoRR, abs/2311.04257. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu,Zhengying Liu, Yu Zhang, James T. Kwok, ZhenguoLi, Adrian Weller, and Weiyang Liu. 2023. Meta-math: Bootstrap your own mathematical questionsfor large language models. CoRR, abs/2309.12284. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng,Ruoqi Liu, Ge Zhang, Samuel Stevens, DongfuJiang, Weiming Ren, Yuxuan Sun, Cong Wei, BotaoYu, Ruibin Yuan, Renliang Sun, Ming Yin, BoyuanZheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang,Huan Sun, Yu Su, and Wenhu Chen. 2023. MMMU:A massive multi-discipline multimodal understand-ing and reasoning benchmark for expert AGI. CoRR,abs/2311.16502. James Xu Zhao, Yuxi Xie, Kenji Kawaguchi, JunxianHe, and Michael Qizhe Xie. 2023. Automatic modelselection with large language models for reasoning.In Findings of the Association for ComputationalLinguistics: EMNLP 2023, Singapore, December6-10, 2023, pages 758783. Association for Com-putational Linguistics. Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, SichunLuo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song,Mingjie Zhan, and Hongsheng Li. 2023a. Solvingchallenging math word problems using GPT-4 codeinterpreter with code-based self-verification. CoRR,abs/2308.07921. Chunting Zhou, Pengfei Liu, Puxin Xu, SrinivasanIyer, Jiao Sun, Yuning Mao, Xuezhe Ma, AviaEfrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh,Mike Lewis, Luke Zettlemoyer, and Omer Levy.2023b.LIMA: less is more for alignment.In",
  "D.3Comparison with Other Models": "We compare the performance of our best model,InternVL-Chat-V1.2-Plus-GeoGPT4V, with otheropen-source and closed-source models regardinggeometric capabilities. Detailed results are in Ta-ble 14.For MathVista, our best model achieves the bestgeometric scores among all models. For MathVi-sion, our best model achieves the highest scores foraverage score and most geometric scores amongopen-source models.The experimental resultsdemonstrate the effectiveness of the GeoGPT4Vdataset.",
  "E.1Difficulty Comparison": "We randomly selected 200 generated questions andtheir corresponding original questions, and askedannotators to compare the difficulty between thegenerated question and the original question. Wedisplay the results in a, and the inner agree-ment between the two annotators is 0.74. In the Figure, \"Easier\" indicates that the generated ques-tion is easier than the original question, with othersymbols following the same pattern. Based onthe experimental results, 77.75% of the generatedquestions are easier or of the same difficulty as theoriginal ones, which indicates that our pipeline canreduce the difficulty of the questions.",
  "E.2Image-text Matching Comparison": "We randomly selected 200 generated questions andtheir corresponding original images, and asked an-notators to judge which image, the generated one orthe original one, better matches the generated ques-tion. We display the results in b, and theinner agreement between the two annotators is 0.78.In the Figure, \"Original\" indicates that the originalimage better matches the question text, with othersymbols following the same pattern. Based on theexperimental results, we can observe that the gen-erated images match the generated questions betterthan the original images.",
  ": Training parameters of different models. To make a fair comparison, we keep the training parametersconsistent with those specified by the models original authors and train the models for one epoch": "Please act as a question generator.Give you a question and its answer, along with a corresponding image for the question; please generate new questionsand provide new answers in English. The new questions and new answers must meet the following conditions:1. The new questions are slightly easier than the original ones but shouldnt be too simple.2. Do not merely rephrase the question; you must reduce its difficulty level.3. The new question must include a detailed description of the information in the image, which must be detailed enoughto allow others to redraw the image based on the description.5. The questions should be as diverse as possible.6. The new answers must be correct.Some useful tips:1. You can incorporate information from the original answer into the question.3. You can generate lead-up problems for the original problem.5. You can generate sub-problems for the original problem.4. Imagine that others cannot see the image corresponding to the new question; you must describe it using words.5. For each question, consider it as a standalone item. Others can only view one question at a time, so avoid usingphrases like \"similar to the previous question\" or references such as \"New_Image 1\".Come up with three diverse questions and answers.Input format:Question: <question example>Answer: <answer example>You must follow this output format:New_Question: <new question example>New_Answer: <new answer example>Image_Description: <new image description example> : Prompt for Question-Answer Pairs Generation. We prompt GPT-4V to generate simplified questions.We also prompt GPT-4V to generate questions that are as diverse as possible to prevent GPT-4V from generating thesame questions. You are a teacher creating an exam, and you need to draw images for the questions on the exam.Give you a question, an answer, and an image description, and generate the image corresponding to the question usingMathematica code. Your code must meet the following conditions:1. Only use the Export command at the end of the code to save the generated image to /temp/image.png.2. The image should be clear and correspond to the question, with particular attention to shape and angle.3. You only need to generate the image; there is no need to solve the problem.4. All variables in the code should be named for easy understanding; avoid using terms such as C directly.Some useful tips:1. Focus on the image description.2. You can use the information from the question and answer to help you generate code.Come up with one code.Input format:Question: <question example>Answer: <answer example>Image description: <image description example>You must follow this output format:Code: <code example> : Prompt for Wolfram Code Generation. When prompting GPT-4, we integrate both image descriptionsand question-answer data to refine code generation. Additionally, we prompt GPT-4 to ensure variable namingwithin the code for clarity, aiming to enhance GPT-4s grasp of the codes relationship to the query at hand. Please act as a scorer.Give you a description, along with an image. Please evaluate the degree of match between the image and the descriptionand give a score. The evaluation process must meet the following conditions:1. The score is a decimal between 0 and 1.2. The score reflects the degree of image-description match.3. If the image and the image description do not match, the score should be low.4. The score should be lower if the image is not clear enough or difficult to understand.5. The image should be rated low if it contains only text and numbers, with no geometric shapes or chart forms.6. The image must have clear shapes and labels.Some useful tips:1. Dont always give high scores.2. Only give high scores when the image and the description match very well.3. You can use two decimal places to represent your score.Come up with one score.Input format:Image description: <image description example>You must follow this output format:Reason: <your reason example>Score: <score example> : Prompt for Scoring. We employ GPT-4V to score the degree of alignment between the generated imagesand the questions. Specifically, the score is a decimal that ranges from 0 to 1. We also prompt GPT-4V to give areason first and then give a final score, hoping this can enhance the accuracy of scoring. Please act as a difficulty level evaluator.Give two geometric data, each consisting of a question, an answer, and an image.Please compare these two questions to determine which one is more difficult.If the first one is more difficult, output 1; if the second one is more difficult, output 2.Some useful tips:1. You should consider the complexity and difficulty of the questions and images.2. Dont automatically assume that multiple-choice questions are easier.3. A shorter answer does not mean its easier.Input format:Question_1: <the first question>Answer_1: <the first answer>Question_2: <the second question>Answer_2: <the second answer>The first image corresponds to the first question, and the second image corresponds to the second question.You can only output the number 1 or 2. : Prompt for Difficulty Comparison. We prompt GPT-4V to determine which of the two questions ismore difficult. We instruct GPT-4V not to simplistically assume that multiple-choice questions or shorter answersimply an easier question.",
  "InternVL40B 59.951.761.179.652.557.054.563.261.116.248.655.760.8InternVL-G40B 56.246.1064.4275.2751.9045.8157.3054.9663.6018.9239.5853.2855.81": ": Overall results of different models on the MathVista. For the model trained with GeoGPT4V, scoreincreases are marked in red compared to the original model. indicates our re-implemented test results missedin benchmarks or origin papers. InternVLrepresents the abbreviation for InternVL-Chat-V1.2-Plus. The suffix-G to the model name indicates a model trained on the GeoGPT4V. We present the detailed score for all the taskssuch as FQA and GPS, as well as the overall (All) score for the benchmark. Due to limited space, we utilizeabbreviations for the tasks and illustrate the detailed task name in the Appendix A.",
  "InternVL-G40B 16.12 9.5716.67 15.00 18.1810.71 10.4513.4616.6716.81 23.12 18.4 18.93 11.896.9013.0423.21": ": Overall results of different models on the MathVision. For the model trained with GeoGPT4V, scoreincreases are marked in red compared to the original model. indicates our re-implemented test results missedin benchmarks or origin papers. InternVLrepresents the abbreviation for InternVL-Chat-V1.2-Plus. The suffix-G to the model name indicates a model trained on the GeoGPT4V. We present the detailed score for all the taskssuch as Alg and AnaG, as well as the overall (All) score for the benchmark. Due to limited space, we utilizeabbreviations for the tasks and illustrate the detailed task name in the Appendix A."
}