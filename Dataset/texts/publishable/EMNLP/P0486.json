{
  "Abstract": "When asked to summarize articles or answerquestions given a passage, large language mod-els (LLMs) can hallucinate details and respondwith unsubstantiated answers that are inaccu-rate with respect to the input context. This pa-per describes a simple approach for detectingsuch contextual hallucinations. We hypothe-size that contextual hallucinations are relatedto the extent to which an LLM attends to in-formation in the provided context versus itsown generations. Based on this intuition, wepropose a simple hallucination detection modelwhose input features are given by the ratio ofattention weights on the context versus newlygenerated tokens (for each attention head). Wefind that a linear classifier based on these look-back ratio features is as effective as a richerdetector that utilizes the entire hidden statesof an LLM or a text-based entailment model.The lookback ratio-based detectorLookbackLensis found to transfer across tasks andeven models, allowing a detector that is trainedon a 7B model to be applied (without retrain-ing) to a larger 13B model. We further applythis detector to mitigate contextual hallucina-tions, and find that a simple classifier-guideddecoding approach is able to reduce the amountof hallucination, for example by 9.6% in theXSum summarization task.1",
  "Introduction": "Despite the utility and impressive capabilities oflarge language models (LLMs), their tendency togenerate hallucinations, i.e., content that deviatesfrom facts or contextually relevant information (Jiet al., 2023), presents a significant challenge intheir deployment. In this work, we focus on thescenarios where the model is provided with the cor-rect facts within the input context but still fails togenerate accurate outputs, a phenomenon we termcontextual hallucination. Despite the simplicity of",
  "Source code: github.com/voidism/Lookback-Lens": "this setup, LLMs struggle with contextual halluci-nations, frequently producing errors in tasks suchas summarization and document-based question an-swering (e.g., ), which can cause seriousissues in applications such as retrieval-augmentedgeneration (RAG) (Lewis et al., 2020), even whencorrect documents are retrieved.Most prior studies that propose methods to com-bat hallucination focus on the scenario without anyinput context, where the hallucinations arise fromthe LLMs parametric knowledge. These worksdetect and mitigate hallucinations by generally us-ing the LLMs representations, such as hiddenstates (Burns et al., 2023; Azaria and Mitchell,2023), MLP outputs (Zhang et al., 2024; Simhiet al., 2024), attention block outputs (Zhang et al.,2024; Simhi et al., 2024) and attention head out-puts (Li et al., 2024; Chen et al., 2024b; Simhiet al., 2024). In contrast, the provided contex-tual information plays a key role in detecting con-textual hallucinations. Insofar as attention (moreso than other model internals) provides a human-meaningful measure of how much weight is givento the context during generation, this motivates theuse of signals from the attention maps for halluci-nation detection and mitigation.To leverage signals from attention maps, we startby hypothesizing that contextual hallucinations arerelated to the extent to which an LLM attends tothe provided contextual information. Concretely,we propose a simple feature called lookback ratio,which is computed as the ratio of attention weightson the given context versus the newly generated to-kens. At each time step, we calculate this lookbackratio for each attention head, and train a linear clas-sifier, which we call the Lookback Lens, to detectcontextual hallucinations based on the lookbackratio features, as illustrated in . The Look-back Lens performs on par with, and sometimeseven surpasses, more complex feature-based detec-tors that utilize hidden states from LLMs or text-",
  "H x L": ": An illustration of the Lookback Lens. We extract attention weights and calculate the lookback ratios for alllayers and all heads. We train a linear classifier on the concatenated features to predict truthfulness of the generation. based entailment models trained on extensively an-notated datasets. We can further integrate this de-tector during decoding to derive a Lookback LensGuided Decoding strategy which can reduce con-textual hallucinations by 9.6% from LLaMA-2-7B-Chat in the XSum summarization task. Fur-thermore, our use of higher level attention mapfeatures makes it possible to transfer the detec-tor across models without retraining, allowing aLLaMA2-13B-Chat model to use the same detec-tor that has been trained on LLaMA-2-7B-Chat,and still reduce hallucinations by 3.2% in XSum.These results collectively highlight the potential ofcombating contextual hallucination by leveragingthe information from attention maps.",
  "Lookback Lens": "To detect contextual hallucinations in LLMs, weintroduce a lookback ratio, a measure based onthe attention distribution of a transformer model.Given a transformer with L layers, each with Hheads, the model processes an input sequence ofcontext tokens X = {x1, x2, . . . , xN} of lengthN followed by a set of newly generated tokensY = {y1, y2, . . . , yt1} to generate the next tokenyt. For time step t, and for each head, we calcu-late the ratio of attention weights focused on thecontext tokens versus the newly generated tokens.Formally, for each head h in layer l, we define:",
  "where denotes the sigmoid function, w is theweight vector, and b is the bias term of the classifier": "Defining SpanThe Lookback Lens predicts theprobability of hallucinations over spans. We con-sider two ways to obtain spans for a given sequence:predefined spans or sliding window.1) Predefined Spans: When the hallucinatedand non-hallucinated span annotations are avail-able, we directly train the classifier to differentiatebetween them. This is a clean setting where allspans are either hallucinated or non-hallucinated. 2) Sliding Window: In practice, we do not haveany predefined spans during decoding, thus weneed a sliding window setup that iterates over allpossible spans. Specifically, we process the sen-tences into fixed-sized chunks and train the classi-fier to predict a label of 0 if any hallucinated con-tent exists within a chunk, and 1 otherwise. Here,the annotated data is only used for creating labels,not for the span segmentation. This is more real-istic for classifier-guided decoding, but it presentsgreater challenges because a chunk can containboth hallucinated and non-hallucinated content.",
  "Experimental Setup": "We evaluate Lookback Lens Guided Decoding onthree tasks that involve generating texts condi-tioned on given contexts, including summariza-tion with XSum (Narayan et al., 2018), QA withNQ (Kwiatkowski et al., 2019), and multi-turn con-versations with MT-bench (Zheng et al., 2024).For testing the generalization ability of the Look-back Lens, we only train it with the CNN/DM sum-",
  "Results": "Our results are presented in . We considerboth predefined span segmentation and sliding win-dow with a window size of 8. We include the two-fold validation setting on the source task and theout-of-domain transfer setting on the target task,with the tasks either question answering (QA) orsummarization (Sum.). We find that the LookbackLens achieves slightly better performance than thehidden states-based classifier and significantly out-performs the NLI models (SoTA and our impl.).The advantage of the Lookback Lens over the hid-den states-based classifier is more significant in thesliding window settings, as shown in the right-handside of .Additionally, we observe that the hidden states-based classifier tends to overfit the training setsduring the two-fold validation, and present a sub-stantial performance drop when transferred to out-of-domain tasks. In contrast, Lookback Lens, whilenot always fitting the training set perfectly, consis-tently exhibits better performance when applied toout-of-domain tasks. This contrast highlights theeffectiveness and generalizability of the lookbackratio features we extract from the attention maps.",
  "Lookback Lens Guided Decoding": "To mitigate the impact of contextual hallucinationsidentified by the Lookback Lens, we introduce aclassifier-guided decoding strategy to guide the gen-eration toward more contextually accurate outputs.This approach serves as a robustness test of theLookback Lens ability to handle various text gener-ation scenarios. While prior studies on controllabletext generation adjust the output probabilities usingclassifiers based on the output tokens (Yang andKlein, 2021), our method fundamentally differs bynot using the tokens themselves but rather theirattention maps during generation.We propose Lookback Lens Guided Decoding,which incorporates Lookback Lens (F) into the de-coding process. Since all tokens in the vocabularyshare the same attention pattern during one decod-ing step, F cannot directly influence one-step to-ken choice. Instead, F can evaluate multiple-tokenchunks, as each chunk causes different attention patterns in multiple decoding steps.Given the context and partially generated text,we independently sample a set of k candidatechunks {C1, C2, . . . , Ck} at the same decodingstep t. For each chunk Cj, the associated lookbackratios are averaged to form a feature vector vj. Asshown in , we select the best candidate C",
  "marization dataset from the detection task in Sec-tion 2.2. Thus, only the XSum dataset will be thesame-task transfer setting, while NQ and MT-benchwill be cross-task transfer setting": "XSumTo test the Lookback Lenss effectivenessat transferring across data distributions for the sametask (summarization), we use 1,000 examples sam-pled from the testing set of XSum. Prior stud-ies (Maynez et al., 2020) indicate that traditionalevaluation metrics such as ROUGE (Lin, 2004) orBERTScore (Zhang et al., 2019a) correlated poorlywith human evaluation on faithfulness and factu-ality. Recent studies (Chiang and Lee, 2023; Liuet al., 2023) also show a strong correlation betweenGPT-4 (OpenAI, 2023) evaluation and human eval-uation. Thus, we report the averaged accuracy fromthe binary judgments of GPT-4o, with the promptsin Appendix B.1. We also conduct a pilot studyfor human evaluation on GPT-4os judgment inAppendix B.2, finding that 97% of the GPT-4ojudgments are consistent with human judgment.",
  "Natural QuestionsWe use the NQ data fromthe setup of Liu et al. (2024) we describe in Ap-pendix C.2 and evaluate the best span exact matchfollowing Kandpal et al. (2023); Mallen et al.(2023)": "MT-BenchWe consider a multi-turn conversa-tions setup where the model needs to follow previ-ous chat history. We use MT-bench (Zheng et al.,2024), a multi-turn instruction-following bench-mark covering eight categories. We focus exclu-sively on generating responses for the second turnand use GPT-3.5s responses as the default for thefirst turn. We use GPT-4 to score the models an-swers on a scale of 1 to 10 based on various factors,including helpfulness, relevance, accuracy, depth,creativity, and level of detail of the response.Additionally, since we are particularly interestedin mitigating contextual hallucinations, we furtherexclude math questions and evaluate the remaining50 general questions. We specifically instruct GPT-4o to focus on whether the responses are faithful tothe chat history (see prompt in Appendix B.1). Werefer to this setup as MT-Bench (hallu.). BaselinesTo evaluate the performance of our pro-posed method, we compared it against the follow-ing baselines: 1) Greedy Decoding: generating re-sponses using the LLaMA-2-7B-Chat model (Tou-vron et al., 2023) through greedy decoding. 2)Other Classifier-Guided Decoding: using exactly",
  "Main Results": "We show our results using eight candidates perchunk in a chunk size of eight in , andthe ablation with different chunk sizes is shownin . Lookback Lens Guided Decoding canimprove the performance on both in-domain task(XSum, by 9.6%) and out-of-domain tasks (NQ,by 3%). The original greedy decoding results onXSum achieved 49.0% correct which means 510examples were hallucinated. Our decoding methodsignificantly reduced the number of hallucinatedexamples from 510 to 414, resulting in an 18.8%reduction in the hallucinated examples. This re-sult is on par with using SoTA NLI to guide thedecoding, where SoTA NLI is trained on roughly731k annotated summarization examples, which is700 larger compared to our 1k training set. (SeeAppendix C.1.) In contrast, decoding guided byhidden states-based or the NLI (our implementa-tion) classifiers, both trained on the same data ofour method, can only slightly improve the perfor-mance on NQ, but not for XSum, probably dueto the issue of distribution shift, highlighting theadvantages of Lookback Lens in generalization abil-ity.For MT-bench, we evaluate both settings: theoriginal setting (ori.) and the setting that is specifi-cally for judging contextual hallucinations (hallu.).",
  ": Cross model transfer results on detection tasks": "We do not expect our method can improve on theoriginal setting, because it evaluates many factorssuch as helpfulness, relevance, etc. But we expectto see an improvement on the hallucination setting.The results shown in suggest that our de-coding method can boost the performance on thehallucination setting while maintaining the sameperformance in the original setting, which showsthat our decoding method is effective in reducinghallucinations without compromising the overallgeneration quality.",
  "Cross-model Transfer": "One benefit of using the lookback ratio to capturehigher-level model patterns for hallucination detec-tion is its potential to better transfer across models.A classifier trained with one models lookback ra-tio could potentially be applied to another modelwithout retraining, provided correlation betweenthe target models attention pattern and that of theoriginal model. Here, we show that we can transfera Lookback Lens trained on attention maps fromLLaMA-2-7B-Chat to LLaMA-2-13B-Chat with-out any retraining.Since the total numbers of attention heads aredifferent in 7B and 13B models, and there is noobvious one-to-one mapping between the heads,we use a linear regression model to map the headsfrom the 13B model to the heads in 7B model.Concretely, we have 1024 heads in 7B and 1600heads in 13B. We extract the averaged lookbackratio per head for all the |D| training examples,resulting in a 1024 |D| matrix and a 1600 |D|matrix.2 We then fit a linear regression model tomap the heads to reconstruct the 7B heads from13B heads. After applying the linear transformationto the lookback ratio from 13B, the transformed",
  ": Cross model transfer from LLaMA-2-7B-chatto LLaMA-2-13B-chat using greedy decoding and clas-sifier guided sampling methods with chunk size 8": "heads can be directly used by 7Bs classifiers. Seedetails in Appendix C.1.The detection results are shown in . Wefirst show the same-model (13B13B) + cross-task transfer result, and the cross-model (7B13B)+ cross-task transfer result. Although cross-modeltransfer yields slightly worse results compared tosame-model transfer, the AUROC scores are stillnon-trivially high. Consider that doing cross-model+ cross-task transfer at the same time may be toughto Lookback Lens, we also include one more settingthat does training on 2.5K examples of the NQtraining set3 and then transfer to the NQ testing set.We see the cross-model same-task transfer resultsare even closer to the same-model transfer results.Given promising results on detection tasks,we apply cross-model transfer to Lookback LensGuided Decoding.We conduct the same-tasktransfer setting: NQ-train (7B) to NQ (13B), andCNN/DM (7B) to XSum (13B). In , we ob-serve a performance improvement similar to same-model transfer using 13B itself, or using the SoTANLI model applied on the 13B decoding. How-ever, on cross-task + cross-model transfer settings:CNN/DM (7B) to NQ (13B), we do not observesignificant improvements where we attribute to thelarger distribution shift. We leave this challengingsetting for future work.",
  "The NQ-train 2.5K data is annotated in the same methodto annotate NQ testing set, as described in .2": "the effect of varying chunk sizes, from 4, 8, to 16.We see that there is a slight trend that LookbackLens guided decoding prefers shorter chunk sizefor NQ and longer chunk size for XSum. However,in general the improvements are consistent acrossdifferent chunk sizes, thus reducing the need tooptimize for chunk sizes.",
  "Largest mag.71.282.382.879.280.381.1Most positive65.174.975.466.370.374.4Most negative59.567.574.466.470.273.0": ": Cross-task transfer AUROC using top-k at-tention heads selected according to: coefficients withthe largest magnitude (largest mag.), most positive, andmost negative. We consider k = 10, 50, and 100. Predictive Power of Different HeadsIn theaforementioned experiments, we utilize all atten-tion heads to train the Lookback Lens. We are thusinterested in how the predictive power is distributedamong different heads in making predictions. Thatis, how much performance can we recover if weonly utilize a subset of heads? To answer this, weuse the coefficients in the linear classifier of theLookback Lens (in ) to estimate the impor-tance of each head in detecting hallucinations.In , we show the results on detection tasksachieved by different detectors trained using only asubset of top-k heads with the largest magnitude ofcoefficients in the original Lookback Lens trainedwill all heads. The results show that the predic-",
  ": Cross-task transfer AUROC among layers": "tive power is not concentrated only on a subset ofheads. Using only top-10 heads is worse than usingall heads, and increasing k consistently improvesperformance and top-100 heads largely recover themodels performance using all heads. More interestingly, we also include the resultsthat only select the top-k heads among the headswith most positive/negative coefficients, which arepositive/negatively correlated to factuality. On theheads with positive coefficients, higher lookbackratio (i.e., when the heads attend at the contextmore) indicates higher factuality and less halluci-nation; conversely, heads with negative coefficientssuggest a lower lookback ratio (i.e., attending togenerated tokens more) is more likely to be truth-ful. shows that none of positive or negativeheads alone can be on par with using the top-klargest magnitude heads. This result implies thatboth positive and negative heads are critical for amodel to generate factual responses. We conjecturethat the positive heads may specialize at contextgrounding, and thus higher lookback ratio on theseheads leads to more factual response. On the otherhand, the negative heads may be critical at ensuringconsistency in its own generation, and thus shouldattend to the generated tokens more. We leavefurther investigation on this interesting balance forfuture work. Meanwhile, we visualize the lookbackratio of positive/negative heads in Appendix D.1. Reducing Number of LayersWe experimentwith using only a subset of layers for LookbackLens, as shown in . We can see that thepredictive power is not concentrated in any subsetof layers, as none of them can recover the perfor-mance of the full model that uses all layers. How-ever, we observe that the middle layers (13-16, 17-20) are slightly more useful than other layers. She had the \"biggest year of her career\", playing 95 shows, bringing in an average $2.4m (1.5m) per city, according to Forbes.Endorsement deals with companies like Pepsi and H&M, along with the surprise album she released in December 2013, helped her to the top spot.The self-titled album was released on iTunes with no prior promotion.Beyonce has just announced she'll be releasing a platinum version of that album later this month, which will include new songs and a concert video.Source: ForbesTaylor Swift came in second on the Forbes list, taking home an estimated $64m (40m). [truncated]",
  ": Qualitative example on XSum using the LLaMA-2-7B-Chat model with greedy decoding and LookbackLens Guided Decoding. The numbers in the parenthesis show the predicted scores from the Lookback Lens": "Qualitative StudyWe show qualitative exam-ples from XSum in to illustrate how Look-back Lens guided decoding improves performance.Greedy decoding from LLaMA-2-7B-Chat resultsin a hallucination, i.e. $100m (64m), that does notexist in the input document. However, the Look-back Lens is able to assign low scores for the chunkcandidates that have contextual hallucinations (asmarked in red). Therefore, Lookback Lens GuidedDecoding is able to help the model generate a sum-mary that is factual to the given context.",
  "Related Work": "Hallucinations in LLMsSimhi et al. (2024) de-fined close-book hallucination vs open-book hal-lucination for settings of relying on parametricknowledge vs knowledge in context. We term open-book hallucination as contextual hallucination forbetter clarity. Previous studies in hallucinations pri-marily focus on close-book hallucinations (Chenet al., 2023; Min et al., 2023; Chern et al., 2023) andtheir detection (Azaria and Mitchell, 2023; Simhiet al., 2024) and mitigation (Li et al., 2024; Chuanget al., 2024; Chen et al., 2024a; Zhang et al., 2024).Most of the studies focus on leveraging LLMs in-ternal representations, such as hidden states (Burnset al., 2023; Azaria and Mitchell, 2023), MLP out-puts (Zhang et al., 2024; Simhi et al., 2024), at-tention block outputs (Zhang et al., 2024; Simhiet al., 2024) and attention head outputs (Li et al.,2024; Chen et al., 2024b; Simhi et al., 2024). Ourwork, however, focuses on contextual hallucina- tions, where models produce content inconsistentwith the provided context (Maynez et al., 2020;Fabbri et al., 2021; Shi et al., 2023). Thus, differ-ent from prior studies, we focus on the attentionmaps instead of internal representations, as we be-lieve that the attention maps patterns record howthe LLM process the given contextual information.Most of the prior studies treat detection and miti-gation as two separate tasks, expect for Simhi et al.(2024); Chen et al. (2024a). Our work focuses notonly on detection, but also tries to incorporate thedetector into the decoding process to further miti-gate the contextual hallucinations. Recently, Simhiet al. (2024) also explored detecting and mitigat-ing both close-book and open-book hallucinations.However, their open-book hallucination setting islimited to DisentQA (Neeman et al., 2023), whichcreates knowledge conflicts between parametricknowledge and given context. In contrast, we focuson LLaMA-2s naturally generated responses tocapture general cases where LLMs fail to followthe context, not just due to knowledge conflicts. Classifier Guided GenerationClassifier guidedgeneration aims to control attributes like topic orsentiment in text generation. PPLM (Dathathriet al., 2019) uses gradient ascent to adjust LM prob-abilities via attribute classifiers. FUDGE (Yang andKlein, 2021) uses an attribute predictor on partialsequences to modify LM probabilities. Our methoduniquely guides generation using classifiers on at-tention maps, setting it apart from prior approaches. Self-attention and Model BehaviorThe atten-tion mechanism, initially introduced in RNN-based encoder-decoder for neural machine trans-lation (Bahdanau et al., 2015; Luong et al., 2015),was later adopted in the Transformer modelsself-attention module (Vaswani et al., 2017), en-abling greater parallelization. Self-attentions in-terpretability has led researchers to use it for un-derstanding model behaviors (Clark et al., 2019;Hao et al., 2021; Vashishth et al., 2019).Ourwork demonstrates that attention maps in LLMsare effective for detecting contextual hallucinations,providing a lightweight and interpretable solutioncompared to complex hidden representation meth-ods (Zhang et al., 2024; Chen et al., 2024b).",
  "Conclusion": "We introduce the Lookback Lens, a lightweight clas-sifier designed to detect contextual hallucinationsby utilizing the lookback ratio, which is computedsolely from attention weights. This classifier notonly effectively identifies contextual hallucinationsbut also mitigates them through Lookback LensGuided Decoding from the LLM. Remarkably, themethod is transferable across various tasks, andeven across models after mapping their attentionheads. This research opens up new possibilitiesfor leveraging attention map information to combathallucinations in large language models.",
  "Despite the effectiveness of the Lookback Lens andits decoding, there are several limitations to con-sider": "First, the performance upper bound of Look-back Lens Guided Decoding is limited by thesampling capabilities of the LLM itself. If theLLM fails to sample the correct chunk amongthe eight candidates, the Lookback Lens can-not correct the error. Second, although the Lookback Lens is alightweight classifier with negligible inferencetime, the requirement to sample multiple can-didates from the LLM increases the total in-ference time. We argue that Lookback LensGuided Decoding is a preliminary approachthat demonstrates the feasibility of integratingthe Lookback Lens into the decoding process,as well as a robustness test for the Lookback Lens to handle various text generation scenar-ios. However, other options, such as inter-vening in the attention map mechanism basedon Lookback Lens signals, could potentiallyachieve faster inference, and we leave this forfuture work. Lastly, the Lookback Lens relies on annotatedexamples of around 1k-2k to train the classi-fier. While other end-to-end methods (Chuanget al., 2024) can mitigate close-book halluci-nations without training data, they lack inter-pretability due to the absence of a detectionstep. Nevertheless, we believe that requiring1,000 annotated examples is a feasible setting.",
  "Acknowledgement": "We sincerely thank Philip Schroeder, Huirong Wen,Andrew Rouditchenko, Nishad Gothoskar, AniNrusimha, Howard Chen, Weijia Shi, and NourJedidi for their discussion and help in this project.This research was sponsored by the United StatesAir Force Research Laboratory and the UnitedStates Air Force Artificial Intelligence Acceleratorand was accomplished under Cooperative Agree-ment Number FA8750-19-2-1000. The views andconclusions contained in this document are thoseof the authors and should not be interpreted as rep-resenting the official policies, either expressed orimplied, of the United States Air Force or the U.S.Government. The U.S. Government is authorizedto reproduce and distribute reprints for Governmentpurposes notwithstanding any copyright notationherein. Linlu and Yoon were supported in part byMIT-IBM Watson AI Lab.",
  "Ethics Statement": "In this research, we used publicly available datasetsand we did not collect any personal information.All datasets and models are used in accordancewith their intended use and licenses. Our methodis designed to improve the factuality of large lan-guage models (LLMs), which can have a positiveimpact on various applications, such as question-answering systems, summarization systems, andother applications that rely on LLMs. When de-ployed, however, our approach still carries the is-sues stemming from LLMs, which means that thereis a risk that the LLM can produce biased, harmful,or offensive output. Therefore, caution should beexercised before implementing similar approachesin real-world applications.",
  "Jifan Chen, Grace Kim, Aniruddh Sriram, Greg Durrett,and Eunsol Choi. 2023. Complex claim verificationwith evidence retrieved in the wild. arXiv preprintarXiv:2305.11859": "Shiqi Chen, Miao Xiong, Junteng Liu, Zhengxuan Wu,Teng Xiao, Siyang Gao, and Junxian He. 2024a.In-context sharpness as alerts: An inner represen-tation perspective for hallucination mitigation. arXivpreprint arXiv:2403.01548. Zhongzhi Chen, Xingwu Sun, Xianfeng Jiao, FengzongLian, Zhanhui Kang, Di Wang, and Chengzhong Xu.2024b. Truth forest: Toward multi-scale truthful-ness in large language models through interventionwithout tuning. In Proceedings of the AAAI Con-ference on Artificial Intelligence, volume 38, pages2096720974. I Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, KehuaFeng, Chunting Zhou, Junxian He, Graham Neubig,Pengfei Liu, et al. 2023. Factool: Factuality detec-tion in generative aia tool augmented frameworkfor multi-task and multi-domain scenarios. arXivpreprint arXiv:2307.13528. Cheng-Han Chiang and Hung-Yi Lee. 2023. Can largelanguage models be an alternative to human evalua-tions? In Proceedings of the 61st Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 1560715631. Yung-Sung Chuang, Yujia Xie, Hongyin Luo, YoonKim, James R Glass, and Pengcheng He. 2024. Dola:Decoding by contrasting layers improves factuality inlarge language models. In The Twelfth InternationalConference on Learning Representations.",
  "Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding shar-ing. Preprint, arXiv:2111.09543": "Or Honovich, Roee Aharoni, Jonathan Herzig, HagaiTaitelbaum, Doron Kukliansy, Vered Cohen, ThomasScialom, Idan Szpektor, Avinatan Hassidim, andYossi Matias. 2022.True: Re-evaluating factualconsistency evaluation. In Proceedings of the 2022Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 39053920. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, DanSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, AndreaMadotto, and Pascale Fung. 2023. Survey of halluci-nation in natural language generation. ACM Comput-ing Surveys, 55(12):138. Nikhil Kandpal, Haikang Deng, Adam Roberts, EricWallace, and Colin Raffel. 2023. Large languagemodels struggle to learn long-tail knowledge. In In-ternational Conference on Machine Learning, pages1569615707. PMLR. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-field, Michael Collins, Ankur Parikh, Chris Alberti,Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-ton Lee, et al. 2019. Natural questions: a benchmarkfor question answering research. Transactions of theAssociation for Computational Linguistics, 7:453466. Philippe Laban, Tobias Schnabel, Paul N Bennett, andMarti A Hearst. 2022.Summac: Re-visiting nli-based models for inconsistency detection in summa-rization. Transactions of the Association for Compu-tational Linguistics, 10:163177. Patrick Lewis, Ethan Perez, Aleksandra Piktus, FabioPetroni, Vladimir Karpukhin, Naman Goyal, Hein-rich Kttler, Mike Lewis, Wen-tau Yih, Tim Rock-tschel, et al. 2020. Retrieval-augmented generationfor knowledge-intensive nlp tasks. Advances in Neu-ral Information Processing Systems, 33:94599474. Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-YunNie, and Ji-Rong Wen. 2023. Halueval: A large-scale hallucination evaluation benchmark for largelanguage models. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 64496464. Kenneth Li, Oam Patel, Fernanda Vigas, HanspeterPfister, and Martin Wattenberg. 2024.Inference-time intervention: Eliciting truthful answers froma language model. Advances in Neural InformationProcessing Systems, 36.",
  "Chin-Yew Lin. 2004. Rouge: A package for automaticevaluation of summaries.In Text summarizationbranches out, pages 7481": "Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-jape, Michele Bevilacqua, Fabio Petroni, and PercyLiang. 2024. Lost in the middle: How language mod-els use long contexts. Transactions of the Associationfor Computational Linguistics, 12:157173. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,Ruochen Xu, and Chenguang Zhu. 2023. G-eval:Nlg evaluation using gpt-4 with better human align-ment. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing,pages 25112522. Minh-Thang Luong, Hieu Pham, and Christopher DManning. 2015. Effective approaches to attention-based neural machine translation. In Proceedingsof the 2015 Conference on Empirical Methods inNatural Language Processing, pages 14121421. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,Daniel Khashabi, and Hannaneh Hajishirzi. 2023.When not to trust language models: Investigatingeffectiveness of parametric and non-parametric mem-ories. In Proceedings of the 61st Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 98029822.",
  "Joshua Maynez, Shashi Narayan, Bernd Bohnet, andRyan McDonald. 2020. On faithfulness and factu-ality in abstractive summarization. arXiv preprintarXiv:2005.00661": "Sewon Min, Kalpesh Krishna, Xinxi Lyu, MikeLewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.Factscore: Fine-grained atomic evaluation of factualprecision in long form text generation. arXiv preprintarXiv:2305.14251. Shashi Narayan, Shay B Cohen, and Mirella Lapata.2018. Dont give me the details, just the summary!topic-aware convolutional neural networks for ex-treme summarization. In Proceedings of the 2018Conference on Empirical Methods in Natural Lan-guage Processing, pages 17971807.",
  "Tal Schuster, Adam Fisch, and Regina Barzilay. 2021": "Get your vitamin C! robust fact verification withcontrastive evidence. In Proceedings of the 2021Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 624643, Online. As-sociation for Computational Linguistics. Abigail See, Peter J Liu, and Christopher D Manning.2017. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 10731083. Weijia Shi, Xiaochuang Han, Mike Lewis, YuliaTsvetkov, Luke Zettlemoyer, and Scott Wen-tauYih. 2023.Trusting your evidence: Hallucinateless with context-aware decoding. arXiv preprintarXiv:2305.14739.",
  "AData Creation for Lookback Lens": "Our experimental setup aims to evaluate the abilityof Lookback Lens to detect hallucinations in largelanguage models with attention maps. We considerthe summarization task and question-answering(QA) task in data creation.For the summarization task, we sampled 1,000examples from the CNN/DM dataset (See et al.,2017). For QA, we use 2,655 examples from theNatural Questions (Kwiatkowski et al., 2019) fromthe setup of Liu et al. (2024) to mix the gold docu-ment with irrelevant documents. To keep our focusmore on LLM hallucinations rather than being dis-tracted by assessing LLMs long-context utilizationability, we limited context to three documents perquestion where the gold document containing theanswer was placed in the middle, surrounded bytwo irrelevant documents.We prompt LLaMA-2-7B-Chat (Touvron et al., 2023) to generate correct responses by greedy de-coding for both tasks to ensure that both halluci-nated and non-hallucinated examples derive fromthe same source distribution. The max length ofgeneration is set to 256 tokens, or until the EOStoken is generated.After the annotation was collected, we extracthallucinated and non-hallucinated spans, as wellas the corresponding attention map lookback ratio,from the LLaMA-2-7B-Chat model, to train theLookback Lens classifiers.In the predefined span setting, three types ofspans are considered as non-hallucinated spans: 1)the text segment before the first hallucinated spanin the response 2) the text segment after the lasthallucinated span in the response 3) the responseannotated as non-hallucinated. All the annotatedhallucinated spans are used as negative data to trainthe Lookback Lens.In the sliding window setting, we consider all thepossible fixed sized chunk with size = 8. If a chunkis overlapping with any of the annotated halluci-nated spans, then it is considered as hallucinated,otherwise it is non-hallucinated. Why not use existing data?Initially, we consid-ered using the HaluEval dataset (Li et al., 2023),which was created by prompting GPT-3.5 (OpenAI,2022) to generate hallucinated examples againsthuman-annotated non-hallucinated responses, onsummarization, QA, and dialogue tasks. However,we have concerns that their method introduces abias by creating fundamentally different data distri- butions between hallucinated and non-hallucinatedexamples. This discrepancy could potentially leadthe classifier to learn to distinguish the sources ofresponses rather than accurately detecting halluci-nations.Additionally, we argue that the LLMs attentionweight will be more meaningful if the text is gen-erated by the same LLM itself, not from externalsources and teacher forcing to obtain the attentionweights. To ensure an unbiased and controlled eval-uation environment, we generated our own dataseton summarization and QA tasks.",
  "B.1Evaluation Prompt for GPT-4o": "We show the templates used to prompt GPT-4o(gpt-4o-2024-05-13) in annotating the truthful-ness of a response and the span-level hallucinationsegment prediction in and , respec-tively for CNN/DM and Natural Questions.This prompt is used for 1) collecting the data totrain the Lookback Lens in , and 2) evalu-ating the XSum summarization task in Sections 3,4, and 5. We also provide the approximate cost ofGPT-4o calls (in USD):",
  "B.2Human Evaluation on GPT-4o Evaluation": "SummarizationTo assess the quality of GPT-4os evaluations, we initially conducted a pilotstudy using 70 XSum dataset examples, with na-tive English-speaking authors and colleagues asevaluators.Evaluators received the document,ground truth summary, LLaMA-2-7B-Chats sum-mary, and GPT-4os judgment to provide a binaryjudgment on GPT-4os accuracy. Our interface isdepicted in Appendix B.1 (see ). This ini-tial evaluation affirmed the correctness of GPT-4osjudgments in 68 out of 70 cases. To further verifythese results, we expanded our evaluation throughAmazon MTurk, adding two additional annotationsper example. Across all 210 evaluations (70 initial+ 140 MTurk), only 9 annotations were markedincorrect, and in only 2 cases did a majority ofannotators deem the judgment incorrect (markedincorrect by at least two annotators). With a fi-nal accuracy of 97.1%, and high intra-annotator",
  "agreement, the comprehensive evaluation supportsGPT-4os use as an automatic evaluator for the en-tire dataset": "Question AnsweringWe expand the human eval-uation to Natural Questions dataset using AmazonMTurk. The evaluation interface is copied from thesummarization setup, but changing summary toanswer, as well as adding the question field. We take 50 examples and assign each example tothree different annotators. There are 7 annotationsmarked incorrect out of the 150 annotations. Intotal, 3 of the examples are marked incorrect by atleast two annotators. If applying a majority vote,47 out of 50 examples are correct, resulting in a94.0% accuracy. This suggests that it is generallysufficient to use GPT-4o to verify the generatedresponses on the question-answering task. You will be provided with a document and a proposed summary.Your task is to determine if theproposed summary can be directly inferred from the document. If the summary contains any informationnot found in the document, it is considered false. Even if the summary is different from a groundtruth summary, it might still be true, as long as it doesnt contain false information.For each proposed summary, explain why it is true or false based on the information from thedocument. Focus only on the original documents content, disregarding any external context.After your explanation, give your final conclusion as Conclusion: True if the proposed summary iscompletely accurate based on the document, or Conclusion: False if it contains any incorrect orunsupported information. If your conclusion is False, identify the exact phrases or name entitiesfrom the summary that is incorrect by stating Problematic Spans: [the inaccurate text spans fromthe summary, in Python list of strings format].",
  ": Prompt template for GPT-4o in annotating the truthfulness and predicting span-level hallucinations onsummarization tasks. Used for CNN/DM and XSum": "You will be provided with a document and a proposed answer to a question. Your task is to determineif the proposed answer can be directly inferred from the document.If the answer contains anyinformation not found in the document, it is considered false. Even if the answer is different froma ground truth answer, it might still be true, as long as it doesnt contain false information.For each proposed answer, explain why it is true or false based on the information from the document.Focus only on the original documents content, disregarding any external context.After your explanation, give your final conclusion as Conclusion: True if the proposed answer iscompletely accurate based on the document, or Conclusion: False if it contains any incorrect orunsupported information. If your conclusion is False, identify the exact phrases or name entitiesfrom the answer that is incorrect by stating Problematic Spans: [the inaccurate text spans from theanswer, in Python list of strings format].",
  "C.1Model Details": "State-of-the-art NLI ModelWe give further de-tail on the pretrained SoTA NLI model 5 used asour topline hallucination detector.Specifically,the model is based on DeBERTa-V3-base (Heet al., 2021) and further finetuned on a rangeof NLI and summarization datasets with exam- Please act as an impartial judge and evaluate the faithfulness and consistency of the responseprovided by an AI assistant to the user question displayed below. Your evaluation should considerwhether the assistants answer to the second user question is faithful and consistent to the chathistory. If the answer contains any misinformation not found or not supported by the chat history,it is considered a hallucination.You evaluation should focus on the assistants answer to thesecond user question. Begin your evaluation by providing a short explanation. Be as objective aspossible. After providing your explanation, you must rate the response on a scale of 1 to 10 bystrictly following this format: [[rating]]\", for example: Rating: []\".",
  ": GPT-4o evaluation prompt for MT-bench (hallucination)": "ples annotated with factual consistency, includingFEVER (Thorne et al., 2018), Vitamin C (Schus-ter et al., 2021) and PAWS (Zhang et al., 2019b).Roughly 731k data examples can be collected fromthe training set of the above three datasets. Themodel is reported to have superior performancewhen evaluated on TRUE (Honovich et al., 2022)SummaC Benchmark (Laban et al., 2022) andAnyScale Ranking Test for Hallucinations 6.",
  "DeBERTa-V3-Base is under MIT License": "Inference DetailsWe run all the models onNVIDIA A6000 (48GB) and V100 (32GB) GPUs.We do not train the model, but only run the in-ference part. Each of the examples takes around20-30 seconds for 7B model, 40-60 seconds for13B model to generate responses using our Look-back Lens Guided Decoding. Please check Ap-pendix C.2 to estimate the total running time oneach of the datasets, as it depends on number ofexamples.All the inferences are run with either greedydecoding or sampling using temperature 0.9 andtop-p sampling with p = 0.95. The implementationis based on Huggingface Transformers packages.7",
  "Classifier Training DetailsWe use Scikit-Learnsklearn.linear_model.LogisticRegression8": "Please act as an impartial judge and evaluate the quality of the response provided by an AIassistant to the user question displayed below.Your evaluation should consider factors such asthe helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. Youevaluation should focus on the assistants answer to the second user question. Begin your evaluationby providing a short explanation. Be as objective as possible. After providing your explanation,you must rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\",for example: \"Rating: []\".",
  "<|The End of Assistant As Conversation with User|>": "Please act as an impartial judge and evaluate the quality of the response provided by an AI assistantto the user question.Your evaluation should consider correctness and helpfulness.You will begiven a reference answer and the assistants answer. You evaluation should focus on the assistantsanswer to the second question. Begin your evaluation by comparing the assistants answer with thereference answer. Identify and correct any mistakes. Be as objective as possible. After providingyour explanation, you must rate the response on a scale of 1 to 10 by strictly following this format:\"[[rating]]\", for example: \"Rating: []\".",
  "D.1Visualization": "We visualize the lookback ratio of the top-10 mostpositive/negative heads when LLaMA-2-7B-Chatdecodes the answer for an NQ example. The top-10most positive/negative heads are selected with themost positive/negative coefficients from the clas-sifier. The green rectangle frames the part thatcontains the hallucinations, i.e. and in Germany inthe 14th century. We can see that during the gener-ation of the hallucinated span, the positive heads,",
  "D.2Using Multiple or All Layers for HiddenStates": "MultipleLayerWefollowthepriorstudy(AzariaandMitchell,2023)tousethe layers with the best predictive power inhallucinationdetection:32nd/28th/24th/20thlayers.We concatenate the 4 layer featuresinto a huge feature. Please note that the hiddendimension of LLaMA-7B is 4096, so combining 4layers would result in a 16384-dim feature vector.In contrast, our Lookback Lens feature for the 7Bmodel is only 1024-dim. Thus, the big classifierusing 16384 input features is supposed to be moreeffective given that it uses 10x more features.However, the result shown in indicatesthat concatenating 4 layers is still less effectivecompared to our Lookback Lens. All LayersWe also try to use the hidden statesfrom all layers, but concatenating them all will re-sult in a huge feature vector with dimensions ofmore than 100k and make the classifier extremelyslow in training. Thus, we perform max/averagepooling for the features across different layers, re-sulting in 4096-dim feature vectors as the classifierinputs. The results shown in the table below arestill worse than our Lookback Lens results.The two experiments above indicate that using",
  "D.3Comparing Attention Outputs withHidden States": "Some papers mention that attention block out-puts could be more useful for detecting halluci-nations (Campbell et al., 2023; Li et al., 2024),while our main experiments only consider the hid-den states as input features for detecting contextualhallucinations. Here we include additional experi-ment results that use attention block outputs instead.In , we show that there is no significantdifference when switching to attention block out-puts, and our Lookback Lens still outperforms thesebaselines."
}