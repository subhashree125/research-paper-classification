{
  "Abstract": "Language models can be manipulated by ad-versarial attacks, which introduce subtle per-turbations to input data. While recent attackmethods can achieve a relatively high attacksuccess rate (ASR), weve observed that thegenerated adversarial examples have a differentdata distribution compared with the originalexamples. Specifically, these adversarial ex-amples exhibit reduced confidence levels andgreater divergence from the training data dis-tribution. Consequently, they are easy to de-tect using straightforward detection methods,diminishing the efficacy of such attacks. Toaddress this issue, we propose a Distribution-Aware Adversarial Attack (DA3) method. DA3 considers the distribution shifts of adversarialexamples to improve attacks effectiveness un-der detection methods. We further design anovel evaluation metric, the Non-detectable At-tack Success Rate (NASR), which integratesboth ASR and detectability for the attack task.We conduct experiments on four widely useddatasets to validate the attack effectiveness andtransferability of adversarial examples gener-ated by DA3 against both the white-box BERT-BASE and ROBERTA-BASE models and theblack-box LLAMA2-7B model1.",
  "Introduction": "Language models (LMs), despite their remarkableaccuracy and human-like capabilities in many ap-plications (Thirunavukarasu et al., 2023; Wu et al.,2024; Wang et al., 2024), face vulnerability to ad-versarial attacks and exhibit high sensitivity to sub-tle input perturbations, which can potentially causefailures (Jia and Liang, 2017; Belinkov and Bisk,2018; Liang et al., 2023; Wallace et al., 2019). Re-cently, an increasing number of adversarial attackshave been proposed, employing techniques such as",
  "Negative": ": Toy examples of two adversarial sentencesin a sentiment analysis task. Although both sentencessuccessfully attack the victim model, the top one isflagged by the detector, while the bottom one is notdetected. In our task, we aim to generate adversarialexamples that are hard to detect. insertion, deletion, swapping, and substitution atcharacter, word, or sentence levels (Ren et al., 2019;Jin et al., 2020; Garg and Ramakrishnan, 2020;Ribeiro et al., 2020). These thoroughly crafted ad-versarial examples are imperceptible to humans yetcan deceive victim models, raising concerns regard-ing the robustness and security of LMs. For exam-ple, chatbots may misunderstand user intent or sen-timent, resulting in inappropriate responses (Perezet al., 2022; Dong et al., 2023). However, while existing adversarial attacks canachieve a relatively high attack success rate (Gaoet al., 2018; Belinkov and Bisk, 2018; Li et al.,2020), our experimental observations detailed in 3reveal notable distribution shifts between adversar-ial examples and original examples, rendering highdetectability of adversarial examples. On one hand,adversarial examples exhibit different confidencelevels compared to their original counterparts. Typ-ically, the Maximum Softmax Probability (MSP),a metric indicating prediction confidence, is higherfor original examples than for adversarial exam-ples. On the other hand, there is a disparity in thedistance to the training data distribution betweenadversarial and original examples. Specifically,the Mahalanobis Distance (MD) to training datadistribution for original examples is shorter thanthat for adversarial examples. Based on these twoobservations, we conclude that adversarial exam- ples generated by previous attack methods, suchas BERT-Attack (Li et al., 2020), can be easilydetected through score-based detection techniqueslike MSP detection (Hendrycks and Gimpel, 2017)and embedding-based detection methods like MDdetection (Lee et al., 2018). Thus, the efficacy ofprevious attack methods is diminished when con-sidering Out-of-distribution (OOD) detection, asshown in .To address the aforementioned problems, wepropose a Distribution-Aware Adversarial Attack(DA3) method with Data Alignment Loss (DAL),which is a novel attack method that can gener-ate hard-to-detect adversarial examples. The DA3 framework comprises two phases. Firstly, DA3 fine-tunes a LoRA-based LM by combining the MaskedLanguage Modeling task and the downstream clas-sification task using DAL. This fine-tuning phaseenables the LoRA-based LM to generate adversar-ial examples closely resembling original examplesin terms of MSP and MD. Subsequently, the LoRA-based LM is used during inference to generate ad-versarial examples.To measure the detectability of adversarial ex-amples, we propose a new evaluation metric: Non-detectable Attack Success Rate (NASR), whichcombines Attack Success Rate (ASR) with OODdetection. We conduct experiments on four datasetsto assess whether DA3 can effectively attack white-box LMs using ASR and NASR. Furthermore,given the widespread use of Large Language Mod-els (LLMs) and their costly fine-tuning process,coupled with the limited availability of open-sourcemodels, we also evaluate the attack transferabilityof adversarial examples on black-box LLMs. Theresults show that DA3 achieves competitive attackperformance on the white-box BERT-BASE (De-vlin et al., 2019) and ROBERTA-BASE (Liu et al.,2019) models and superior transferability on theblack-box LLAMA2-7B (Touvron et al., 2023).Our work has the following contributions: We analyze the distribution of adversarial andoriginal examples, revealing the existence ofdistribution shifts in terms of MSP and MD. We propose a novel Distribution-Aware Ad-versarial Attack method with Data AlignmentLoss, which is capable of generating adversar-ial examples that effectively undermine victimmodels while remaining difficult to detect.",
  "Adversarial Attacks in NLP": "Adversarial attacks in Natural Language Process-ing (NLP) have been extensively studied to ex-plore the robustness of LMs. Current methods fallinto character-level, word-level, sentence-level, andmulti-level (Goyal et al., 2023). Character-levelmethods manipulate texts by incorporating typosor errors into words, such as deleting, repeating,replacing, swapping, flipping, inserting, and allow-ing variations in characters for specific words (Gaoet al., 2018; Belinkov and Bisk, 2018). Word-levelattacks alter entire words rather than individualcharacters within words. Common manipulationincludes addition, deletion, and substitution withsynonyms to mislead language models while themanipulated words are selected based on gradi-ents or importance scores (Ren et al., 2019; Jinet al., 2020; Li et al., 2020; Garg and Ramakrish-nan, 2020; Formento et al., 2024). Sentence-levelattacks typically involve inserting or rewriting sen-tences within a text, all while preserving the origi-nal meaning (Zhao et al., 2018; Iyyer et al., 2018;Ribeiro et al., 2020). Multi-level attacks combinemultiple perturbation techniques to achieve bothimperceptibility and a high success rate in the at-tack (Song et al., 2021).",
  "Out-of-distribution Detection in NLP": "Detecting suspicious data in NLP has been stud-ied from various perspectives, such as linguisticanalysis (Zhou et al., 2019; Mozes et al., 2021;Mosca et al., 2022). Our work, however, primar-ily focuses on detecting adversarial data from theout-of-distribution perspective. Out-of-distribution(OOD) detection methods have been widely ex-plored in NLP, like machine translation (Aroraet al., 2021; Ren et al., 2022; Adila and Kang,2022). OOD detection methods in NLP can beroughly categorized into two types: (1) score-based methods and (2) embedding-based methods.Score-based methods use maximum softmax prob-ability (Hendrycks and Gimpel, 2017), perplexityscore (Arora et al., 2021), beam score (Wang et al.,2019b), sequence probability (Wang et al., 2019b), 0.50.60.70.80.91.0",
  ": Visualization of the distribution shift betweenoriginal data and adversarial data generated by BERT-Attack when attacking BERT-BASE regarding MD": "BLEU variance (Xiao et al., 2020), or energy-basedscores (Liu et al., 2020). Embedding-based meth-ods measure the distance to in-distribution datain the embedding space for OOD detection. Forexample, Lee et al. (2018) uses Mahalanobis dis-tance; Ren et al. (2021) proposes to use relativeMahalanobis distance; Sun et al. (2022) proposes anearest-neighbor-based OOD detection method.We select the simple, representative, and widely-used OOD detection methods of these two cate-gories: MSP detection (Hendrycks and Gimpel,2017) and MD detection (Lee et al., 2018), respec-tively. This selection serves to highlight a signif-icant issue within the community the ability todetect adversarial examples using such basic andcommonly employed OOD detection methods un-derscores the criticality of detectability. These twomethods are then incorporated with the ASR to as-sess the robustness and detectability of adversarialexamples generated by different attack models.",
  "cus our analysis on adversarial examples generatedby BERT-Attack on SST-2 (Socher et al., 2013) andMRPC (Dolan and Brockett, 2005); the completeresults are available in Appendix G": "Maximum Softmax Probability (MSP).Max-imum Softmax Probability (MSP) is a metricto evaluate prediction confidence, rendering it awidely used score-based method for OOD detec-tion, where lower confidence values often signifyOOD examples. To assess MSP, we visualize theMSP distribution of adversarial examples gener-ated by BERT-Attack and original examples fromSST-2 and MRPC datasets in . Our obser-vation reveals that in both datasets, the majorityof original examples have an MSP exceeding 0.9,indicating a significantly higher MSP comparedto adversarial examples overall. This distributionshift is particularly notable in the MRPC dataset,whereby most adversarial examples exhibit MSPbelow 0.6, highlighting a clear distinction from theoriginal examples. Mahalanobis Distance (MD).Mahalanobis Dis-tance (MD) is a metric used to measure the distancebetween a data point and a distribution, making ita highly suitable and widespread method for OODdetection. A high MD between an example and thein-distribution data (training data) indicates that theexample is probably an OOD instance. To assessthe MD difference between adversarial and origi-nal examples, we visualize the MD distribution ofadversarial examples generated by BERT-Attackand original examples from the SST-2 and MRPCdatasets in . From , we can ob-serve that distribution shifts exist between originaland adversarial examples in both datasets. This dis-similarity is more noticeable on the SST-2 datasetand not as conspicuous on the MRPC dataset. Summary.These observations regarding MSPand MD highlight clear distinctions between origi-nal and adversarial examples generated by one ofthe state-of-the-art methods, BERT-Attack. Com-pared to the original examples, the adversarial ex-amples exhibit a more pronounced OOD naturein either MSP or MD, meaning that adversarialexamples are easy to detect and the practical effec-tiveness of previous attack methods is diminished.",
  "Xadv": "MSP Loss MDLoss PLM MLMPLMyorig yadv Fine-tuningInference LoRA WdownWup xorig PLM LoRA WdownWup xorig xadv : The model architecture of DA3 comprises two phases: fine-tuning and inference. During fine-tuning,a LoRA-based Pre-trained Language Model (PLM) is fine-tuned to develop the ability to generate adversarialexamples resembling original examples in terms of MSP and MD. During inference, the LoRA-based PLM is usedto generate adversarial examples.",
  "Distribution-Aware Adversarial Attack": "Motivated by the observed distribution shifts ofadversarial examples, we propose a Distribution-Aware Adversarial Attack (DA3) method.Thekey idea of DA3 is to consider the distribution ofthe generated adversarial examples and attempt toachieve a closer alignment between distributionsof adversarial and original examples in terms ofMSP and MD. DA3 is composed of two phases:fine-tuning and inference, as shown in . Fine-tuning Phase.The fine-tuning phase aimsto fine-tune a LoRA-based Pre-trained LanguageModel (PLM) to make it capable of generating ad-versarial examples through the Masked LanguageModeling (MLM) task. We employ LoRA-basedPLM because it is efficient to fine-tune and thefrozen PLM can serve in both MLM and down-stream classification tasks. First, the original sen-tence xorig undergoes the MLM task through aLoRA-based PLM to generate the adversarial em-bedding Xadv, during which the parameters of thePLM are frozen, and the parameters of LORA (Huet al., 2021) are tunable. Then, the generated adver-sarial embedding Xadv is fed into the frozen PLMto perform the corresponding downstream classi-fication task, producing logits of original groundtruth label yorig and adversarial label yadv. The",
  "loss is computed based on Xadv, P(yorig|Xadv, ),and P(yadv|Xadv, ) to update the parameters ofLORA, where is the model parameters. Detailsare discussed in 4.3": "Inference Phase.The inference phase aims togenerate adversarial examples with minimal per-turbation. The original sentence xorig is first tok-enized, and a ranked token list is obtained throughtoken importance (Li et al., 2020). Then, a token isselected from the token list to be masked. Subse-quently, the MLM task of the frozen LoRA-basedPLM is employed to generate a candidate list forthe masked token. A word is then chosen from thelist to replace the masked token until a successfulattack on the victim model is achieved or the candi-date list is exhausted. If the attack is unsuccessful,another token is chosen from the token list untila successful attack is achieved or the terminationcondition is met. The termination condition is setas the percentage of the tokens.",
  "Model Learning": "The Data Alignment Loss, denoted as LDAL, isused to minimize the discrepancy between distribu-tions of adversarial examples and original examplesin terms of MSP and MD. LDAL is composed oftwo losses: MSP loss, denoted as LMSP and MDloss, denoted as LMD.LMSP aims to increase the difference betweenP(yadv|Xadv, ) and P(yorig|Xadv, ). LMSP isformulated as",
  "(Xadv ) 1(Xadv ),": "where and 1 are the mean and covariance em-bedding of the in-distribution (training) data respec-tively. MD is a robust metric for OOD detectionand adversarial data detection. In general, adver-sarial data has higher MD than original data, asshown in . Therefore, minimizing LMDencourages the generated adversarial examples toresemble original examples in terms of MD. LMDis constrained to the logarithmic space for consis-tency with the scale of LMSP .Thus, Data Alignment Loss is represented as",
  "Automatic Evaluation Metrics": "Given the observations of distribution shifts ana-lyzed in , we adopt a widely-used metric Attack Success Rate (ASR) and design a new met-ric Non-detectable Attack Success Rate (NASR) to evaluate attack performance. We also reportthe Percentage of Perturbed Words (%Words) andSemantic Similarity (SS) to evaluate the impact oftext perturbation. Detailed explanations of ASR,%Words, and SS are shown in Appendix A. Non-detectable Attack Success Rate (NASR).Considering the detectability of adversarial exam-ples generated by attack methods, we define a newevaluation metric Non-Detectable Attack SuccessRate (NASR). This metric considers both ASR andOOD detection. Specifically, NASR posits thata successful adversarial example is characterizedby its ability to deceive the victim model whilesimultaneously evading OOD detection methods.We utilize two established and commonly em-ployed OOD detection techniques MSP detec-tion (Hendrycks and Gimpel, 2017) and MD de-tection (Lee et al., 2018). MSP detection relies onlogits and utilizes a probability distribution-basedapproach, while MD detection is a distance-based",
  "|X|,": "where Dk denotes the set of examples that success-fully attack the victim model but are detected bythe detection method k {MSP, MD}.In this context, adversarial examples are consid-ered as OOD examples (positive), while originalexamples are considered as in-distribution exam-ples (negative). To avoid misdetecting original ex-amples as adversarial examples from a defendersview, we use the Negative MSP and MD value at99% False Positive Rate of the training data asthresholds. Values exceeding these thresholds areconsidered positive, while those falling below areclassified as negative.",
  "Experimental Settings": "Attack Baselines.We use two character-levelattack methods, DeepWordBug (Gao et al., 2018)and TextBugger (Jinfeng et al., 2019), and threeword-level attack methods, TextFooler (Jin et al.,2020), BERT-Attack (Li et al., 2020) and A2T (Yooand Qi, 2021). Detailed descriptions are listed inAppendix B.1. Datasets.We evaluate DA3 on four differenttypes of tasks: sentiment analysis task SST-2 (Socher et al., 2013), grammar correctness task CoLA (Warstadt et al., 2019), textual entailmenttask RTE (Wang et al., 2019a), and textual sim-ilarity task MRPC (Dolan and Brockett, 2005).Detailed descriptions and statistics of each datasetare shown in Appendix B.2.",
  "We use the adversarial examples generated by DA3": "with BERT-BASE or ROBERTA-BASE as the back-bone to attack the white-box BERT-BASE andROBERTA-BASE models, respectively. White-boxmodels have been fine-tuned on the correspondingdatasets and are accessible during our fine-tuningphase. Besides, considering that LLMs are widelyused, expensive to fine-tune, and often not opensource, we evaluate the attack transferability of theadversarial examples, which are generated by DA3",
  "DA3 (ours)59.8517.9212.2216.84": "BASE, while its NASRMSP decreases drasticallycompared to ASR on SST-2, RTE, and MRPC. Sim-ilarly, BERT-Attack shows good performance onROBERTA-BASE, while its NASRMSP is notablylower than its ASR, especially on SST-2, RTE, andMRPC. This phenomenon indicates these adver-sarial examples are relatively easy to detect usingMSP detection. Considering the results of both vic-tim models, DA3 consistently produces reasonableand favorable outcomes when attacking white-boxmodels, which proves the effectiveness of DA3.We also report %Words and SS in Appendix C.DA3 achieves best or second-to-best %Wordsand comparable SS compared to baselines acrossdatasets on both victim models.",
  "Transferability to LLMs (RQ2).2 When at-tacking the black-box LLAMA2-7B model, DA3": "performs the best on SST-2, RTE, and MRPC,outperforming baselines in all evaluation metrics.On CoLA, DA3 achieves second-to-best results onNASR. Further analysis and visualization of attackperformance on LLAMA2-7B across five differentprompts are displayed in Appendix F. DA3 consis-tently surpasses all baselines across five prompts. 2We also present results on MISTRAL-7B and the analysison why the generated samples can be transferred to anotherLLMs in Appendix C. The results show DA3 achieves the bestperformance in most cases when attacking MISTRAL-7B. original negativeoriginal positive adversarial negativeadversarial positive",
  "The experimental results underscore the substan-tial advantage of our model when generalizing thegenerated adversarial examples to the black-boxLLAMA2-7B model, compared to baselines": "VisualizationWe use t-SNE visualization to ana-lyze high-level features of the generated adversar-ial examples. We visualize BERT embedding ofBERT-Attack generated adversarial examples andoriginal examples, and DA3 generated adversarialexamples and original samples on SST-2 in Fig-ure 5. In each subfigure, data points are classifiedinto four types: original negative, original posi-tive, adversarial negative, and adversarial positive(negative/positive refer to ground truth labels). Inboth subfigures, original negative points and origi-nal positive points form two separate clusters. In (left), adversarial negative and adversarialpositive points overlap and are dispersed betweenthe original negative and original positive points.In contrast, in (right), adversarial nega-tive and adversarial positive points are relativelyseparated, with adversarial negative points closerto original positive points and adversarial positivepoints closer to original negative points. The vi-sualization shows that DA3 generated adversarialexamples are harder to detect than BERT-Attackgenerated adversarial examples.",
  "Human Evaluation (RQ3)": "Given that our goal is to generate high-quality ad-versarial examples that preserve the original se-mantics and remain imperceptible to humans, weperform human evaluations to assess the adversar-ial examples generated by DA3 using BERT-BASEas the backbone. These evaluations focus on gram-mar, prediction accuracy, and semantic preserva-tion on SST-2 and MRPC datasets. For this pur-",
  "SST-24.124.370.680.740.710.66MRPC4.624.860.680.760.880.84": "pose, three human judges evaluate 50 randomly se-lected original-adversarial pairs from each dataset.Detailed annotation guidelines are in Appendix D.First, human raters are tasked with evaluatingthe grammar correctness and making predictions ofa shuffled mix of the sampled original and adversar-ial examples. Grammar correctness is scored from1-5 (Li et al., 2020; Jin et al., 2020). Then, humanjudges assess the semantic preservation of adversar-ial examples, determining whether they maintainthe original semantics. We follow Jin et al. (2020)and ask human judges to classify adversarial exam-ples as similar (1), ambiguous (0.5), or dissimilar(0) to the original examples. We compare DA3 with the best baseline model, TextFooler, on se-mantic preservation for better evaluation. We takethe average scores among human raters for gram-mar correctness and semantic preservation and takethe majority class as the predicted label.As shown in , grammar correctnessscores of adversarial examples generated byDA3 are similar to those of original examples.While word perturbations make predictions morechallenging, adversarial examples generated byDA3 still show decent accuracy.Compared toTextFooler, DA3 can better preserve semantic simi-larity to original examples. Some generated adver-sarial examples are displayed in Appendix E.",
  "To analyze the effectiveness of different compo-nents of LDAL, we conduct an ablation study onDA3 with BERT-BASE as the backbone. The re-sults are shown in and": "MSP Loss.We ablate LMSP during fine-tuningto assess the efficacy of LMSP .LMSP helpsimprove NASRMSP and MSP Detection Rate(DRMSP ), which is the ratio of |DMSP | to thetotal number of successful adversarial examples,across all datasets. An interesting finding is that onSST-2 and CoLA, although models without LMSPperform better in terms of ASR, the situation dete-riorates when considering detectability, leading tolower NASRMSP and higher DRMSP compared",
  "to the model with LDAL": "MD Loss.We ablate LMD during fine-tuning toassess the efficacy of LMD. LMD helps improveMD Detection Rate (DRMD), which is the ratioof |DMD| to the number of successful adversarialexamples, across all datasets. LMD also improvesNASRMD on all datasets except SST-2. A similarfinding on CoLA exists that although models with-out LMD perform better on ASR, the performanceworsens when considering detectability.The ablation study shows that both LMSP andLMD are effective on most datasets.",
  "Loss Visualization and Analysis (RQ4)": "To better understand how different loss compo-nents contribute to DA3, we visualize the changesof LMSP , LMD, and LDAL throughout the fine-tuning phase of DA3 with BERT-BASE as the back-bone on SST-2 dataset, as illustrated in .We observe that all three losses exhibit oscillat-ing descent and eventual convergence. Althoughthe overall trends of LMSP and LMD are consis-tent, a closer examination reveals that they oftenexhibit opposite trends at each step, especially inthe initial stages. Despite both losses sharing a com-mon goal of reducing distribution shifts betweenadversarial examples and original examples, thisobservation reveals a potential trade-off relation-ship between them. One possible interpretation isthat, on the one hand, minimizing LMSP increasesthe confidence of wrong predictions, aligning with",
  "BASE as the backbone on SST-2. The x-axis representsfine-tuning steps; the y-axis represents the change ofloss compared to the initial loss": "the objective of the adversarial attack task to induceincorrect predictions. On the other hand, minimiz-ing LMD encourages the generated adversarial sen-tences to resemble the original ones more closely,loosely akin to the objective of the masked lan-guage modeling task to restore masked tokens totheir original values. While these two objectivesare not inherently conflicting, an extreme stand-point reveals that when the latter objective is fullysatisfied meaning the model generates identicalexamples to the original ones the former objectivenaturally becomes untenable.",
  "Loss Comparison (RQ5)": "Other than using our LDAL, we also explore otherloss variants: LNCE and LFCE.Minimizing the negative of regular cross-entropyloss (denoted as LNCE) or minimizing the cross-entropy loss of flipped adversarial labels (denotedas LFCE) are two simple ideas as baseline attackmethods. We replace LDAL with LNCE or LFCEduring the fine-tuning phase to assess the efficacyof our loss LDAL. The results in show thatLDAL outperforms the other two losses across allevaluation metrics on RTE and MRPC datasets. OnCoLA dataset, LDAL achieves better or similar per-formance compared to LNCE and LFCE. WhileLDAL may not perform as well as LNCE andLFCE on SST-2, given its superior performanceon the majority of datasets, we believe LDAL ismore effective than LNCE and LFCE generally.",
  "ours0.7499.1674.8624.5193.295.90": "shifts between adversarial examples and originalexamples in terms of MSP and MD. To addressthis problem, we propose a Distribution-AwareAdversarial Attack (DA3) method with the DataAlignment Loss and introduce a novel evaluationmetric, NASR, which integrates out-of-distributiondetection into the assessment of successful attacks.Our experiments validate the attack effectivenessof DA3 on BERT-BASE and ROBERTA-BASE andthe transferability of adversarial examples gener-ated by DA3 on the black-box LLAMA2-7B.",
  "Limitations": "We analyze the distribution shifts between adver-sarial examples and original examples in terms ofMSP and MD, which exist in most datasets. Nev-ertheless, the MD distribution shift is not very ob-vious in some datasets like MRPC. This indicatesthat MD detection may not always effectively iden-tify adversarial examples. However, we believethat since such a distribution shift is present inmany datasets, we still need to consider MD detec-tion. Furthermore, our experiments demonstratethat considering distribution shift is not only effec-tive for NASR but also enhances the performanceof the model in ASR.",
  "Yonatan Belinkov and Yonatan Bisk. 2018. Syntheticand natural noise both break neural machine transla-tion. In International Conference on Learning Rep-resentations": "Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,Nicole Limtiaco, Rhomni St John, Noah Constant,Mario Guajardo-Cespedes, Steve Yuan, Chris Tar,et al. 2018.Universal sentence encoder.arXivpreprint arXiv:1803.11175. Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics.",
  "William B. Dolan and Chris Brockett. 2005. Automati-cally constructing a corpus of sentential paraphrases.In Proceedings of the Third International Workshopon Paraphrasing (IWP2005)": "Xiangjue Dong, Yun He, Ziwei Zhu, and James Caver-lee. 2023. PromptAttack: Probing dialogue statetrackers with adversarial prompts. In Findings ofthe Association for Computational Linguistics: ACL2023, pages 1065110666, Toronto, Canada. Associ-ation for Computational Linguistics. Brian Formento, Wenjie Feng, Chuan-Sheng Foo,Luu Anh Tuan, and See Kiong Ng. 2024. Semrode:Macro adversarial training to learn representationsthat are robust to word-level attacks. In Proceed-ings of the 2024 Conference of the North AmericanChapter of the Association for Computational Lin-guistics: Human Language Technologies (Volume 1:Long Papers), pages 79988021.",
  "Dan Hendrycks and Kevin Gimpel. 2017. A baseline fordetecting misclassified and out-of-distribution exam-ples in neural networks. In International Conferenceon Learning Representations": "Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,et al. 2021. Lora: Low-rank adaptation of large lan-guage models. In International Conference on Learn-ing Representations. Mohit Iyyer, John Wieting, Kevin Gimpel, and LukeZettlemoyer. 2018. Adversarial example generationwith syntactically controlled paraphrase networks. InProceedings of the 2018 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,Volume 1 (Long Papers), pages 18751885, New Or-leans, Louisiana. Association for Computational Lin-guistics. Robin Jia and Percy Liang. 2017. Adversarial exam-ples for evaluating reading comprehension systems.In Proceedings of the 2017 Conference on Empiri-cal Methods in Natural Language Processing, pages20212031, Copenhagen, Denmark. Association forComputational Linguistics. Di Jin, Zhijing Jin, Joey Tianyi Zhou, and PeterSzolovits. 2020. Is bert really robust? a strong base-line for natural language attack on text classificationand entailment. Proceedings of the AAAI Conferenceon Artificial Intelligence, 34(05):80188025. Li Jinfeng, Ji Shouling, Du Tianyu, Li Bo, and WangTing. 2019. Textbugger: Generating adversarial textagainst real-world applications. Proceedings 2019Network and Distributed System Security Symposium. Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin.2018. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. Ad-vances in neural information processing systems, 31. Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue,and Xipeng Qiu. 2020. BERT-ATTACK: Adversar-ial attack against BERT using BERT. In Proceed-ings of the 2020 Conference on Empirical Methodsin Natural Language Processing (EMNLP), pages61936202, Online. Association for ComputationalLinguistics.",
  "Weitang Liu, Xiaoyun Wang, John Owens, and YixuanLi. 2020. Energy-based out-of-distribution detection.Advances in neural information processing systems,33:2146421475": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A robustly optimized bert pretraining ap-proach. arXiv preprint arXiv:1907.11692. Edoardo Mosca, Shreyash Agarwal, Javier RandoRamrez, and Georg Groh. 2022. that is a suspi-cious reaction!: Interpreting logits variation to de-tect nlp adversarial attacks. In Proceedings of the60th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages78067816. Maximilian Mozes, Pontus Stenetorp, Bennett Klein-berg, and Lewis Griffin. 2021. Frequency-guidedword substitutions for detecting textual adversarialexamples. In Proceedings of the 16th Conference ofthe European Chapter of the Association for Compu-tational Linguistics: Main Volume, pages 171186.Association for Computational Linguistics. Nikola Mrkic, Diarmuid Saghdha, Blaise Thomson,Milica Gasic, Lina M Rojas Barahona, Pei-Hao Su,David Vandyke, Tsung-Hsien Wen, and Steve Young.2016. Counter-fitting word vectors to linguistic con-straints. In Proceedings of the 2016 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 142148. Ethan Perez, Saffron Huang, Francis Song, Trevor Cai,Roman Ring, John Aslanides, Amelia Glaese, NatMcAleese, and Geoffrey Irving. 2022. Red teaminglanguage models with language models. In Proceed-ings of the 2022 Conference on Empirical Methodsin Natural Language Processing, pages 34193448,Abu Dhabi, United Arab Emirates. Association forComputational Linguistics. Jie Ren, Stanislav Fort, Jeremiah Liu, Abhijit GuhaRoy, Shreyas Padhy, and Balaji Lakshminarayanan.2021.A simple fix to mahalanobis distance forimproving near-ood detection.arXiv preprintarXiv:2106.09022. Jie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mo-hammad Saleh, Balaji Lakshminarayanan, and Pe-ter J Liu. 2022. Out-of-distribution detection andselective generation for conditional language mod-els. In The Eleventh International Conference onLearning Representations.",
  "Proceedings of the 57th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 10851097, Florence, Italy. Association for ComputationalLinguistics": "Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,and Sameer Singh. 2020.Beyond accuracy: Be-havioral testing of NLP models with CheckList. InProceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 49024912, Online. Association for Computational Lin-guistics. Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Ng, andChristopher Potts. 2013. Recursive deep models forsemantic compositionality over a sentiment treebank.In Proceedings of the 2013 Conference on Empiri-cal Methods in Natural Language Processing, pages16311642, Seattle, Washington, USA. Associationfor Computational Linguistics. Liwei Song, Xinwei Yu, Hsuan-Tung Peng, and KarthikNarasimhan. 2021.Universal adversarial attackswith natural triggers for text classification. In Pro-ceedings of the 2021 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages37243733, Online. Association for ComputationalLinguistics.",
  "Arun James Thirunavukarasu, Darren Shu Jeng Ting,Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan,and Daniel Shu Wei Ting. 2023. Large languagemodels in medicine. Nature medicine, 29(8):19301940": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gard-ner, and Sameer Singh. 2019. Universal adversarialtriggers for attacking and analyzing NLP. In Proceed-ings of the 2019 Conference on Empirical Methodsin Natural Language Processing and the 9th Inter-national Joint Conference on Natural Language Pro-cessing (EMNLP-IJCNLP), pages 21532162, HongKong, China. Association for Computational Linguis-tics. Alex Wang, Amanpreet Singh, Julian Michael, FelixHill, Omer Levy, and Samuel R. Bowman. 2019a.GLUE: A multi-task benchmark and analysis plat-form for natural language understanding. In the Pro-ceedings of ICLR. Chen Wang, Liangwei Yang, Zhiwei Liu, Xiaolong Liu,Mingdai Yang, Yueqing Liang, and S Yu Philip. 2024.Collaborative semantic alignment in recommendationsystems. In 33rd ACM International Conference onInformation and Knowledge Management. Shuo Wang, Yang Liu, Chao Wang, Huanbo Luan, andMaosong Sun. 2019b. Improving back-translationwith uncertainty-based confidence estimation.InProceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the 9thInternational Joint Conference on Natural LanguageProcessing (EMNLP-IJCNLP), pages 791802, HongKong, China. Association for Computational Linguis-tics.",
  "B.1Baselines": "DeepWordBug (Gao et al., 2018) uses two scoringfunctions to determine the most important wordsand then adds perturbations through random sub-station, deletion, insertion, and swapping letters inthe word while constrained by the edit distance. TextBugger (Jinfeng et al., 2019) finds importantwords through the Jacobian matrix or scoring func-tion and then uses insertion, deletion, swapping,substitution with visually similar words, and sub-stitution with semantically similar words. TextFooler (Jin et al., 2020) uses the predictionchange before and after deleting the word as theword importance score and then replaces each wordin the sentence with synonyms until the predictionlabel of the target model changes.",
  ": Different instructions used for different runs": "Dataset PromptSST-2Evaluate the sentiment of the given text.Please identify the emotional tone of this passage.Determine the overall sentiment of this sentence.After examining the following expression, label its emotion.Assess the mood of the following quote. CoLAAssess the grammatical structure of the given text.Assess the following sentence and determine if it is grammatically correct.Examine the given sentence and decide if it is grammatically sound.Check the grammar of the following sentence.Analyze the provided sentence and classify its grammatical correctness. RTEAssess the relationship between sentence1 and sentence2.Review the sentence1 and sentence2 and categorize their relationship.Considering the sentence1 and sentence2, identify their relationship.Please classify the relationship between sentence1 and sentence2.Indicate the connection between sentence1 and sentence2. MRPC Assess whether sentence1 and sentence2 share the same semantic meaning.Compare sentence1 and sentence2 and determine if they share the same semantic meaning.Do sentence1 and sentence2 have the same underlying meaning?Do the meanings of sentence1 and sentence2 align?Please analyze sentence1 and sentence2 and indicate if their meanings are the same. the tokens are masked during the fine-tuning phrase.The rank of the update matrices of LORA is set to8; LORA scaling factor is 32; LORA dropout valueis set as 0.1. The inference termination conditionis set as 40% of the tokens. shows the hyperparameters used in ex-periments.White-box experiments are conducted on twoNVIDIA GeForce RTX 3090ti GPUs, and black-box experiments are conducted on two NVIDIARTX A5000 24GB GPUs.",
  "CMore Automatic Evaluation Results": "Experimental results of %Words and SS onthe white-box victim models BERT-BASE andROBERTA-BASE are shown in and Ta-ble 13.DA3 achieves best or second-to-best%Words and comparable SS compared to baselinesacross datasets on both victim models.The results of the generated adversarial exam-ples by DA3 with BERT-BASE as the backboneon attacking the white-box MISTRAL-7B modelon CoLA, RTE, and MRPC are shown in .Our proposed DA3 outperforms all other baselines.Although BERT-BASE, LLAMA2-7B, andMISTRAL-7B have different structures and param-eters, they are both trained on large text corpora.",
  "GObservation Experiments": "The observation experiments on previous attackmethods TextFooler, TextBugger, DeepWordBug,and BERT-Attack are shown in , ,, , , , Fig-ure 14, and .The distribution shift between adversarial exam-ples and original examples is more evident in termsof MSP across all the datasets. The distributionshift between adversarial examples and originalexamples in terms of MD is clear only on SST-2dataset and MRPC dataset. Although this shift isnot always present in terms of MD, it is imperativeto address this issue given its presence in certaindatasets.",
  "Adv Sentence1: Ms Stewart , the chief executive , was not expected to visiting .<SPLIT>Sentence2:Ms Stewart , 61 , its chief executive officer and chairwoman , did not attend .Not_equivalent": "OriSentence1: Sen. Patrick Leahy of Vermont , the committee s senior Democrat , later said theproblem is serious but called Hatch s suggestion too drastic .<SPLIT>Sentence2: Sen. PatrickLeahy , the committee s senior Democrat , later said the problem is serious but called Hatch sidea too drastic a remedy to be considered .",
  "Equivalent": "Adv Sentence1: Sen. Patrick Leahy of Vermont , the committee s senior Democrat , later said theproblem is serious but called Hatch s suggestion too drastic .<SPLIT>Sentence2: Sen. PatrickLeahy , the committee s senior Democrat , later said the problem is serious but called Hatch sidea too drastic a remedy to be counted ."
}