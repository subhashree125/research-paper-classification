{
  "Abstract": "Embedding models play a pivotal role in mod-ern NLP applications such as document re-trieval. However, existing embedding modelsare limited to encoding short documents of typ-ically 512 tokens, restrained from applicationscenarios requiring long inputs. This paper ex-plores context window extension of existingembedding models, pushing their input lengthto a maximum of 32,768. We begin by evalu-ating the performance of existing embeddingmodels using our newly constructed LONGEM- BED benchmark, which includes two syntheticand four real-world tasks, featuring documentsof varying lengths and dispersed target infor-mation. The benchmarking results highlighthuge opportunities for enhancement in currentmodels. Via comprehensive experiments, wedemonstrate that training-free context windowextension strategies can effectively increase theinput length of these models by several folds.Moreover, comparison of models using Abso-lute Position Encoding (APE) and Rotary Po-sition Encoding (RoPE) reveals the superiorityof RoPE-based embedding models in contextwindow extension, offering empirical guidancefor future models. Our benchmark, code andtrained models will be released to advance theresearch in long context embedding models.",
  "Introduction": "Text embeddings are vector representations of nat-ural language that encode its semantic informa-tion. They play a pivotal role in various natural lan-guage processing (NLP) tasks, including informa-tion retrieval (IR) and retrieval-augmented genera-tion (RAG). However, embedding models for pro-ducing these vector representations still operateswithin a very narrow context window, many sup-porting only 512 input tokens (Wang et al., 2022;Xiao et al., 2023; Ni et al., 2022). This narrow",
  "* Contribution during Daweis internship at MSR Asia.Sujian Li is the corresponding author": "context window has greatly hindered their appli-cation in scenarios requiring long inputs, such aslong Wikipedia articles and meeting scripts (Saad-Falcon et al., 2024).Previous efforts that train a long context embed-ding model from scratch suffer significant compu-tational overhead, due to the combined demand forlarge batch sizes and long sequences. For example,Chen et al. (2024) utilized 96 A100 GPUs to trainBGE-M3 which supports 8k context. Meanwhile,there have been many successes in extending con-text window of existing LLMs in a plug-and-playway or via efficient fine-tuning, pushing their con-text from 4k to 128k (Xiong et al., 2023) and even2 million tokens (Ding et al., 2024). Motivated bythis, instead of training long context embeddingmodels from scratch, this paper explores contextwindow extension of existing embedding models.First, we examine the capability of existing em-bedding models in processing long context. Re-trieval is selected as the proxy task, as it closelymirrors real-world application scenarios. Whilethere have been some retrieval benchmarks such asBEIR (Thakur et al., 2021) and LoCo (Saad-Falconet al., 2024), we identify two major limitations withthese existing benchmarks: 1) limited documentlength, 2) biased distribution of target information.To overcome this, we introduce the LONGEMBEDbenchmark that integrates two synthetic tasks toenable flexible control over document length, andfour real tasks featuring dispersed target informa-tion. Results on LONGEMBED indicates huge roomfor improvement in current embedding models.Based on this, we explore plug-and-play strate-gies to extend embedding models, including par-allel context windows, reorganizing position ids,and position interpolation. Comprehensive exper-iments show that these strategies can effectivelyextend the context window of existing embeddingmodels by several folds, regardless of their origi-nal context being 512 or beyond 4k. Furthermore, QA",
  "(c)": ": (a) Overview of the LONGEMBED benchmark. (b) Performance of current embedding models on passkeyretrieval, with evaluation length ranging from 256 to 32,768 1. / denotes embedding models with 512 / 4kcontext. The greenera cell is, the higher retrieval accuracy this model achieves on the corresponding evaluationlength. (c) Effects of context window extension methods on E5, E5-RoPE, E5-Mistral, measured by improvementsof Avg. Scores on LONGEMBED. SE / NTK is short for SelfExtend / NTK-Aware Interpolation. for models employing absolute position encoding(APE), we show the possibility of harvesting fur-ther improvements via fine-tuning while strictlypreserving original behavior within the short con-text. In this way, we have extended E5Base (Wanget al., 2022) from 512 to 4k (See c).For models utilizing RoPE (Su et al., 2021), sub-stantial enhancements on LONGEMBED are ob-served when employing methods that fully lever-age RoPEs advantages, such as NTK (Peng andQuesnelle, 2023) and SelfExtend (Jin et al., 2024).As illustrated in b and 1c, leveragingNTK extends the context window of E5-Mistralto 32k, achieving close-to-perfect accuracy onpasskey retrieval and state-of-the-art performanceon LONGEMBED. Further, for fair comparisonof APE / RoPE-based embedding models, we pre-train E5-RoPE following the training procedureand data of E5. Thorough comparison of E5 andE5-RoPE reveals the superiority of RoPE-basedembedding models in context window extension.To sum up, our contributions are as follows:",
  "Related Work": "Text Embedding Models.Text embeddingsencode semantic information of text as low-dimensional vectors, enabling numerous NLP ap-plications. Early attempts on embeddings mod-els include latent semantic indexing (Deerwesteret al., 1990) and weighted average of word embed-dings (Mikolov et al., 2013). Modern embeddingmodels (Wang et al., 2022; Xiao et al., 2023; Nee-lakantan et al., 2022) exploit supervision from la-beled query-document pairs, adopting a multi-stagetraining paradigm that pre-trained on large-scaleraw text pairs using contrastive loss, then fine-tunedon small scale but high-quality datasets.Existing efforts in developing long-context em-bedding models typically involve first obtaininga long-context backbone model, either by pre-training with long inputs from scratch (Gntheret al., 2023; Nussbaum et al., 2024; Chen et al.,2024) or using existing ones (Wang et al., 2023b),followed by training the backbone model to pro-duce embeddings. Instead, this paper endows ex- 1For simplicity, we report results from the base versions ofthe included models by default. The supported context lengthof each model is presented in . Inputs exceeding thesupported context length are truncated. isting embedding models with the ability to handlelong context through context window extension.Context Window Extension for LLMs.Due tothe high cost of pre-training an LLM from scratch,there have been many efforts towards extending thecontext window of existing LLMs in a plug-and-play manner. We categorize these efforts as follows:1) Divide-and-conquer, which involves segment-ing long inputs into short chunks, processing eachchunk with the model, and aggregating the results,as demonstrated by PCW (Ratner et al., 2023); 2)Position reorganization, which reorganizes positionids to accommodate longer inputs, as exemplifiedby SelfExtend (Jin et al., 2024), DCA (An et al.,2024). 3) Position interpolation, which introducesnew position embeddings by interpolating existingones, includes PI (Chen et al., 2023), NTK (Pengand Quesnelle, 2023), YaRN (Peng et al., 2023),and Resonance RoPE (Wang et al., 2024a). Ourpaper thoroughly investigates these three lines ofmethods on embedding models. We also acknowl-edge other efforts in context extension, such as to-ken compression (Jiang et al., 2023; Ge et al., 2023;Zhang et al., 2024a) and memory-based transform-ers (Wang et al., 2024b; Xiao et al., 2024). How-ever, the former is not applicable for bidirectionalattention, and the latter requires complex mecha-nisms for accessing encoded content, hence we donot experiment with these two categories.In addition to their plug-and-play usability, fur-ther fine-tuning on top of these methods with longtraining samples has been proven to yield betterperformance (Xiong et al., 2023; Fu et al., 2024;Zhang et al., 2024b; Yen et al., 2024). Address-ing the overhead of training on long inputs and thescarcity of extremely long training data, a line ofresearch investigates simulating long inputs withinshort context, including Randomized Positions (Ru-oss et al., 2023), Positional Skip-wise (PoSE) train-ing (Zhu et al., 2023), and SkipAlign (Wu et al.,2024). This paper also leverage these efforts tosynthesize long training samples from the originaltraining data, facilitating further fine-tuning on topof plug-and-play methods.",
  "Examing Existing Retrieval Benchmarks": "There are two main desiderata for curating a longcontext retrieval benchmark. First, the candidatedocuments should be long enough. Second, thetarget information to answer user query should beas uniformly distributed across the document aspossible. This prevents embedding models fromsolely focusing on specific parts, such as the begin-ning (Coelho et al., 2024), to achieve unreasonablyhigh scores. Based on these criteria, we examineexisting retrieval benchmarks as follows:BEIR Benchmark (Thakur et al., 2021) is a col-lection of 18 information retrieval datasets, rang-ing across ad-hoc web search, question answering,fact verification, etc. However, documents in thisbenchmark contains fewer than 300 words on av-erage (See in Appendix), making it un-suitable for measuring long context retrieval thatusually involves documents of thousands or tens ofthousands of words.LoCo Benchmark (Saad-Falcon et al., 2024) con-sists 12 retrieval tasks that requires long contextreasoning, spanning diverse domains such as lawand finance. However, it still suffers from biaseddistribution of key information, as demonstratedin . With only 512 context length, E5Baseachieves >85% nDCG scores on 3 out of 8 publicly-available LoCo tasks. This severely biased distri-bution of target information undermines its abilityto reflect model performance as context increases.",
  "PasskeySynthetic40080011NeedleSynthetic4008007": ": Overview of the LONGEMBED benchmark. Average word number is rounded to the nearest integer. For needle and passkey test, we have 8 groups of queries and candidate documents, with the documents averaging{0.25, 0.5, 1, 2, 4, 8, 16, 32} 0.75k words, respectively. Passkey Test Examples:Query: What is the pass key for Sky Morrow?Doc1: <prefix> Sky Morrow's passkey is 123. Remember it. 123 is the passkey for Sky Morrow. <suffix>Doc2: <prefix> Cesar McLean's passkey is 456. Remember it. 456 is the passkey for Cesar McLean. <suffix> ... Needle Test Examples:Query: Who discovered the law of gravity?Doc1: <prefix> The law of gravity was discovered by Sir Issac Newton. <suffix>Doc2: <prefix> The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day. <suffix> ... : Example for the passkey and needle test. Forthe passkey test, the <prefix / suffix> are repeats of \"Thegrass is green. The sky is blue. The sun is yellow. Herewe go. There and back again.\" For the needle test, the<prefix> and <suffix> form a long essay. ized passkey retrieval (Wang et al., 2023b), whereeach document contains a unique person name andhis/her passkey at random position. The goal is toretrieve the document containing the given personspasskey from all candidates documents. Needle-in-a-haystack Retrieval.While passkeyretrieval surrounds key information with garbagesentences, needle-in-a-haystack retrieval (Kamradt,2023; Liu et al., 2024) randomly inserts key infor-mation into an arbitrary position of a long essay,making the task more challenging. To tailor thistask for embedding models, we instruct GPT-4 togenerate 100 facts covering a variety of domainsincluding physics, history, geometry, art, etc, and100 queries correspondingly. The facts are subse-quently treated as needles and randomly insertedinto the PaulGrahamEssay to form 100 candidate documents. Our task is to correctly retrieve thedocument that contains corresponding needle giventhe query.The advantage of synthetic data is that wecan flexibly control context length and dis-tribution of target information.For bothtasks, we evaluate a broad context range of{0.25, 0.5, 1, 2, 4, 8, 16, 32} 1, 024 tokens 2. Foreach context length, we include 50 test samples,each comprising 1 query and 100 candidate docu-ments. 3 In this way, we can measure the effectivecontext size of embedding models for up to 32ktokens. Examples for both tasks are in .",
  "Real Tasks in LONGEMBED": "While synthetic tasks offer flexibility in manipulat-ing context length and distributing target informa-tion, they still differ from real-world scenarios. Toconduct a comprehensive evaluation, we have tai-lored following long-form QA and summarizationtasks for long context retrieval. For QA datasets,we use the questions as queries, the set of all inputdocuments as candidate documents. For summa-rization datasets, we use the summaries as queries,and the set of all input documents as candidatedocuments.NarrativeQA (Kocisk et al., 2018) is a QAdataset comprising long stories and correspondingquestions about specific content such as characters, 2Since token numbers vary w.r.t. tokenizers, we use arough estimation that 1 token = 0.75 word, and constraint theword numbers to not exceed {0.25, 0.5, 1, 2, 4, 8, 16, 32} 1, 024 0.75.3The original version of personalized passkey retrieval usesdifferent candidate documents for each query, resulting in 50queries and 5,000 documents to encode for each context length.To speed up evaluation, we share the candidate documents fordifferent queries within each context length. events. As these details are dispersed throughoutthe story, models must process the entire long con-text to get the correct answers.2WikiMultihopQA (Ho et al., 2020) is a multi-hopQA dataset featuring questions with up to 5 hops,synthesized through manually designed templatesto prevent shortcut solutions. This necessitatesthe ability to process and reason over long context,ensuring that answers cannot be obtained by merelyfocusing on a short span within the document.QMSum (Zhong et al., 2021) is a query-basedmeeting summarization dataset that requires select-ing and summarizing relevant segments of meet-ings in response to queries. Due to the involve-ment of multiple participants and topics in the meet-ing, summarization regarding specific queries nat-urally requires aggregating information dispersedthroughout the entire text.SummScreenFD (Chen et al., 2022) is a screen-play summarization dataset comprising pairs of TVseries transcripts and human-written summaries.Similar to QMSum, its plot details are scatteredthroughout the transcript and must be integrated toform succinct descriptions in the summary. presents the overall statistics ofLONGEMBED.Considering the computationalcomplexity that increases quadratically with inputlength, we intentionally restrict the number of can-didate documents in each task to to not exceed 103.In this way, we can efficiently evaluate the basiclong context capabilities of embedding models. Forfurther elaboration on the source and examples foreach dataset, please refer to Appendix C.",
  "Preliminary: APE & RoPE": "Absolute Position Embedding (APE) stands asthe predominant positional encoding strategy forembedding models, as majority of them followsthe BERT architecture (Devlin et al., 2019). APE-based models first embed absolute position idsinto position vectors and add token embeddings totheir corresponding position vectors, before feed-ing them to a stack of transformer layers.Rotary Position Embedding (RoPE) is the mostpervasive position embedding strategy in the era ofLLMs, including LLaMA (Touvron et al., 2023),QWen (Bai et al., 2023a), etc. It encodes posi-tion information of tokens with a rotation matrixthat naturally incorporates explicit relative positiondependency. To elucidate, given a hidden vector",
  "(hd2 + ihd1)eimd/21]": "where j = 100002j/d, j {0, 1, ..., d/2 1},i = 1 is the imaginary unit. Unlike APE thatis directly applied to the input vector x, RoPE isemployed on the query and key vectors at eachlayer. The attention score a(q, k) between a queryq at position m and a key k at position n is:",
  "Extending APE-based Models": "As delineated in , training-free contextextension strategies applicable to embedding mod-els can be classified into 3 categories: 1) Divide-and-conquer; 2) Position reorganization; 3) Posi-tion interpolation. In this section, we introducemethods from each of these categories to assesstheir applicability to embedding models. Furtherfine-tuning on top of these methods is also in-cluded. Let Lo represent the original context length,D = {x1, x2, ..., xLt} denote a long document oftarget context length Lt, and s = Lt/Lo indicatethe context scaling factor. The context extensionmethods we investigated are described below:Parallel Context Windows (PCW).To processa long document with a short-context model, PCWdivides the long document into multiple shortchunks, processes each chunk in parallel, and ag-gregates their results (Ratner et al., 2023; Yen et al.,2024). In practice, we first segment D into chunksof Lo tokens, then average over each chunks em-beddings to represent D. For simplicity, we set theoverlap between adjacent chunks to 0, except forthe last chunk, to ensure it contains Lo tokens.Grouped & Recurrent Positions (GP & RP).Dividing inputs into chunks and processing themseparately sacrifices their interaction in between.By contrast, position reorganization accommodateslonger context by reusing the original position ids.To be specific, we experiment with two simple 0.501 510.5511.5511",
  ") position vectors when further tuning on RP / PI": "strategies: Grouped Positions and Recurrent Po-sitions. The former groups the original positionids as such: fgp(pid) pid/s, while the latterassigns the position ids recurrently, formulated as:frp(pid) pid mod Lo. Linear Position Interpolation (PI).Instead ofreusing position ids, Chen et al. (2023) introducesnew position embeddings via linear interpolationof existing ones. To apply PI on APE-based mod-els, we map the positions ids as such: fpi(pid) pid/s, and assign embeddings for non-integers aslinear interpolation of that of neighboring integers.In practice, we first extend the original positionembedding matrix Eo RLod into Et RLtd,where d stands for hidden size. Next, we assignEt[i s] = Eo[i], i {0, 1, ..., Lo 1}. For non-integer position id j between i and i + 1, we de-termine their embeddings as follows: Et[s j] =((i + 1 j)Et[i s] + (j i)Et[(i + 1) s]). Further Tuning.Except for PCW, which divideslong texts into smaller blocks and processes sepa-rately, GP, RP, and PI can all be seen as extendingthe position embedding matrix. Since APE-basedmodels assign an independent vector to each posi-tion, we can freeze the original model parameterswhile updating only the newly added position em-beddings. In this way, we can strictly maintainmodel ability within 512 context, while harvest-ing further performance gains in handling longcontext as free lunch. Specifically, further fine-tuning on top of RP and PI is explored in this paper,as illustrated in (Right). Since the tradi-tional training data for embedding models are shortqueries and passages not exceeding 512 tokens, wemanipulate position ids to simulate long trainingsamples, as proposed in Zhu et al. (2023). SeeAppendix B for details of further fine-tuning.",
  "Extending RoPE-based Models": "For RoPE-based models, we further explore SelfExtend and NTK, which respectively advances overGP and PI, harnessing the inherent advantages ofRoPE. Since there is no simple strategy for furthertraining while exactly maintaining original perfor-mance like APE, we leave comprehensive explo-ration of training-based context window extensionfor RoPE-based models for future work. Self Extend (SE).Compared with APE, RoPEoperates on the query and key vectors at each layerto encode relative positions, offering enhanced flex-ibility for position reorganization. For each to-ken, instead of assigning grouped relative positionsto all other tokens, SelfExtend (Jin et al., 2024)re-introduces normal relative positions within thenearest neighbor window w, achieving improvedperformance. For example, consider a document of10 tokens {x0, x1, ..., x9} with a neighbor windowsize w = 4 and a group size g = 2. The relativepositions to x0 are {0, 1, 2, 3, 4, 4, 5, 5, 6, 6}. Forx4, the relative positions of the other tokens are{4, 3, 2, 1, 0, 1, 2, 3, 4, 4}. NTK-Aware Interpolation (NTK).Given a scal-ing factor s, PI proportionally down-scales po-sition index m to m/s. In this way, the atten-tion score a(q, k) defined in Equation 1 becomesg(q, k, (m n)/s). This is also equivalent toreducing the frequencies uniformly, which mayprevent the model from learning high-frequencyfeatures, as shown by the Neural Tangent Kernel(NTK) theory (Jacot et al., 2018). To remedy this,NTK-Aware interpolation (Peng and Quesnelle,2023) scales high frequencies less and low frequen-cies more to spread out the interpolation pressureacross multiple dimensions. This is achieved bydirectly altering the original j = 100002j/d intoj = (10000)2j/d, where is conventionallychosen to be slightly greater than s.",
  "Experimental Setup": "Benchmarked Models.We evaluate both open-sourced and proprietary models on LONGEMBED,including E5, GTE, BGE, Contriever, GTR, E5-Mistral, Jina-V2, Nomic-V1, BGE-M3, OpenAI-ada-002. M2 (Saad-Falcon et al., 2024) is not in-cluded in our evaluation, given its training datapartly overlaps with test samples in LONGEMBED. Candidate Models for Extension.From eachof the APE-based and RoPE-based category, weselect 2 candidate models for comprehensive study.The former includes E5Base and GTEBase. The lat-ter includes the 4,096-context E5-Mistral, and anewly trained E5-RoPEBase, which supports 512context (See Appendix A for its training detailsand BEIR results). Note that E5-RoPEBase employsthe same training procedure and training data asE5Base, only with APE substituted with RoPE. Thisfacilitates fair comparison of APE / RoPE-basedmodels in context window extension, as presentedin .4. For implementation details of eachcontext window extension strategies on each model,please refer to Appendix B.",
  "Main Results": "demonstrates the performance of existingembedding models on our LONGEMBED bench-mark.Among the 512-context models, E5Baseachieves the highest average score of 41.0 points,closely followed by E5-RoPEBase and Contriever.As the supported context length increases beyond4k, exemplified by E5-Mistral and Jina-V2, a dis-cernible increase in scores is observed. This veri-fies both the efficacy of these long-context modelsand the validity of LONGEMBED to assess long-context retrieval. Note that even the best perform-ing model attains only 64.4 pts on average, indicat-ing huge room for improvement in current models.In the last row block of , we furtherinclude the best results achieved by E5Base, E5-RoPEBase and E5-Mistral after context window ex-tension. For E5Base and E5-RoPEBase, we extendtheir contexts from 512 to 4,096. For E5-Mistral,we extend its context from 4,096 to 32,768. Com-pared to the original versions, the extended modelsachieve an average score increase of +15.6 / +20.3/ +10.9 points. This indicates the efficacy of thesecontext extension strategies on embedding mod-els, enabling them to handle inputs of several foldslonger. Detailed performance comparison of dif-ferent extension strategies on APE & RoPE-basedembedding models is presented in .3. 0.5k1k2k4k",
  "Comparison of Extension Methods": "APE-based Models. illustrates the im-pact of various context extension strategies onE5Base and GTEBase across different target contextlengths. We observe that plug-and-play methodsincluding GP, RP, PI and PCW strategies yield com-parable results with no significant disparities. Onthe other hand, further tuning consistently yields ad-ditional performance gains for both models, acrossall target context lengths. Particularly noteworthyis GTEBase, which showcases a substantial aver-age score increase of approximately 5 points afterfurther tuning. This suggests that freezing the orig-inal model weights and fine-tuning exclusively theadded position embeddings can effectively extendthe models context window while strictly main-taining models original ability.RoPE-based Models. depicts the out-comes of E5-RoPEBase and E5-Mistral on eachdataset of LONGEMBED after context window ex-tension via PCW, GP, PI, SE and NTK. It is ob-served that RoPE-specific methods including NTKand SE yield significant improvements for both",
  "Analysis": "Tuning on PI vs. RP.a compares fur-ther tuning on top of RP vs. PI. In the formerapproach, the initial 512 position embeddings arefrozen while the remaining embeddings are tuned,whereas for the latter, the frozen / learnable embed-ding vectors are arranged in an interleaved manner.We observe that tuning on PI consistently producessuperior results on both GTEBase and E5Base. A pos-sible explanation is that fixed vectors in PI serveintrinsically as anchors, preventing the learnablevectors from converging to suboptimal values.RoPE vs. APE.We further discuss the potentialof APE / RoPE-based models for context windowextension. E5Base and E5-RoPEBase are selectedas the comparison subjects thanks to their sharedtraining process, training data, and comparable per-formance on BEIR and LONGEMBED benchmarks.At each target context length ({1k, 2k, 4k}), wereport the best scores achieved by each model onLONGEMBED, as illustrated in b. With-out requiring further training, E5-RoPEBase con-sistently demonstrates superior performance com-pared to E5Base across all target lengths. Further-more, as the target window length increases, this superiority becomes more pronounced, even sur-passing the fine-tuned version of E5Base by a largemargin. This suggests that RoPE-based modelscan better extrapolate to to longer context. Conse-quently, we advocate for the use of RoPE in futureembedding models.",
  "Conclusion": "This paper explores context window extension ofexisting embedding models. Through extensiveexperiments on our LONGEMBED benchmark, weshow that training-free context window extensionstrategies can effectively increase the input lengthof these models by several folds. Further, our anal-ysis reveals the superiority of RoPE-based embed-ding models over APE-based ones in context win-dow extension. Hence, we advocate for the use ofRoPE for future embedding models.",
  "Limitations": "As a pioneering work in applying context windowextension on embedding models, this paper is stilllimited in several aspects, particularly in that mostof the context extension strategies explored in thispaper are training-free. As evidenced by previousfindings (Xiong et al., 2023; Fu et al., 2024; Zhanget al., 2024b; Yen et al., 2024), and the additionalperformance gain achieved via tuning on E5Baseand GTEBase, we believe further fine-tuning on topof plug-and-play methods can bring even betterextension results. In the future, we will make com-prehensive exploration of training-based contextwindow extension for embedding models, espe-cially for RoPE-based ones.",
  "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, FeiHuang, et al. 2023a. Qwen technical report. arXivpreprint arXiv:2309.16609": "Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,Jiankai Tang, Zhidian Huang, Zhengxiao Du, XiaoLiu, Aohan Zeng, Lei Hou, et al. 2023b. Longbench:A bilingual, multitask benchmark for long contextunderstanding. arXiv preprint arXiv:2308.14508. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, DefuLian, and Zheng Liu. 2024. Bge m3-embedding:Multi-lingual, multi-functionality, multi-granularitytext embeddings through self-knowledge distillation.arXiv preprint arXiv:2402.03216. Mingda Chen, Zewei Chu, Sam Wiseman, and KevinGimpel. 2022. Summscreen: A dataset for abstrac-tive screenplay summarization. In Proceedings of the60th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages86028615.",
  "Joo Coelho, Bruno Martins, Joo Magalhes, JamieCallan, and Chenyan Xiong. 2024.Dwell inthe beginning: How language models embed longdocuments for dense retrieval.arXiv preprintarXiv:2404.04163": "Scott Deerwester, Susan T Dumais, George W Furnas,Thomas K Landauer, and Richard Harshman. 1990.Indexing by latent semantic analysis. Journal of theAmerican society for information science, 41(6):391407. Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics. Yiran Ding, Li Lyna Zhang, Chengruidong Zhang,Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang,and Mao Yang. 2024. Longrope: Extending llm con-text window beyond 2 million tokens. arXiv preprintarXiv:2402.13753.",
  "Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and FuruWei. 2023. In-context autoencoder for context com-pression in a large language model. arXiv preprintarXiv:2307.06945": "Michael Gnther, Jackmin Ong, Isabelle Mohr, Alaed-dine Abdessalem, Tanguy Abel, Mohammad KalimAkram, Susana Guzman, Georgios Mastrapas, SabaSturua, Bo Wang, et al. 2023. Jina embeddings 2:8192-token general-purpose text embeddings for longdocuments. arXiv preprint arXiv:2310.19923. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,and Akiko Aizawa. 2020.Constructing a multi-hop QA dataset for comprehensive evaluation ofreasoning steps. In Proceedings of the 28th Inter-national Conference on Computational Linguistics,pages 66096625, Barcelona, Spain (Online). Inter-national Committee on Computational Linguistics. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-bastian Riedel, Piotr Bojanowski, Armand Joulin,and Edouard Grave. 2021. Towards unsuperviseddense information retrieval with contrastive learning.arXiv preprint arXiv:2112.09118, 2(3).",
  "Arthur Jacot, Franck Gabriel, and Clment Hongler.2018. Neural tangent kernel: Convergence and gen-eralization in neural networks. Advances in neuralinformation processing systems, 31": "Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, YuqingYang, and Lili Qiu. 2023. Llmlingua: Compressingprompts for accelerated inference of large languagemodels. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing,pages 1335813376. Hongye Jin, Xiaotian Han, Jingfeng Yang, ZhimengJiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen,and Xia Hu. 2024. Llm maybe longlm: Self-extendllm context window without tuning. arXiv preprintarXiv:2401.01325.",
  "Greg Kamradt. 2023. Needle in a haystack - pressuretesting llms": "Vladimir Karpukhin, Barlas Oguz, Sewon Min, PatrickLewis, Ledell Wu, Sergey Edunov, Danqi Chen, andWen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the2020 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 67696781. Tom Kocisk, Jonathan Schwarz, Phil Blunsom, ChrisDyer, Karl Moritz Hermann, Gbor Melis, and Ed-ward Grefenstette. 2018. The NarrativeQA readingcomprehension challenge. Transactions of the Asso-ciation for Computational Linguistics, 6:317328. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-field, Michael Collins, Ankur Parikh, Chris Alberti,Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-ton Lee, et al. 2019. Natural questions: A benchmarkfor question answering research. Transactions of theAssociation for Computational Linguistics, 7:452466. BenjaminLefaudeux,FranciscoMassa,DianaLiskovich,Wenhan Xiong,Vittorio Caggiano,Sean Naren, Min Xu, Jieru Hu, Marta Tintore,Susan Zhang, Patrick Labatut, Daniel Haziza,Luca Wehrstedt, Jeremy Reizenstein, and Grig-ory Sizov. 2022.xformers:A modular andhackable transformer modelling library.",
  "Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and En-rico Shippole. 2023. Yarn: Efficient context windowextension of large language models. arXiv preprintarXiv:2309.00071": "Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram,Inbal Magar, Omri Abend, Ehud Karpas, AmnonShashua, Kevin Leyton-Brown, and Yoav Shoham.2023. Parallel context windows for large languagemodels. In Proceedings of the 61st Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 63836402. Anian Ruoss, Grgoire Deltang, Tim Genewein, JordiGrau-Moya, Rbert Csords, Mehdi Bennani, ShaneLegg, and Joel Veness. 2023. Randomized positionalencodings boost length generalization of transform-ers. In Proceedings of the 61st Annual Meeting of theAssociation for Computational Linguistics (Volume2: Short Papers), pages 18891903.",
  "Jon Saad-Falcon, Daniel Y Fu, Simran Arora, NeelGuha, and Christopher R. 2024. Benchmarking andbuilding long-context retrieval models with loco andm2-bert. arXiv preprint arXiv:2402.07440": "Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, OriYoran, Adi Haviv, Ankit Gupta, Wenhan Xiong,Mor Geva, Jonathan Berant, and Omer Levy. 2022.SCROLLS: Standardized CompaRison over long lan-guage sequences. In Proceedings of the 2022 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1200712021, Abu Dhabi, UnitedArab Emirates. Association for Computational Lin-guistics.",
  "Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,Bo Wen, and Yunfeng Liu. 2021. Roformer: En-hanced transformer with rotary position embedding.arXiv preprint arXiv:2104.09864": "Nandan Thakur, Nils Reimers, Andreas Rckl, Ab-hishek Srivastava, and Iryna Gurevych. 2021. BEIR:A heterogeneous benchmark for zero-shot evaluationof information retrieval models. In Thirty-fifth Con-ference on Neural Information Processing SystemsDatasets and Benchmarks Track (Round 2). Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro,Faisal Azhar, et al. 2023. Llama: Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971. Liang Wang, Nan Yang, Xiaolong Huang, BinxingJiao, Linjun Yang, Daxin Jiang, Rangan Majumder,and Furu Wei. 2022. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprintarXiv:2212.03533. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao,Linjun Yang, Daxin Jiang, Rangan Majumder, andFuru Wei. 2023a. Simlm: Pre-training with repre-sentation bottleneck for dense passage retrieval. InProceedings of the 61st Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 22442258.",
  "Yikai Zhang, Junlong Li, and Pengfei Liu. 2024b. Ex-tending llms context window with 100 samples.arXiv preprint arXiv:2401.07004": "Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, MutethiaMutuma, Rahul Jha, Ahmed Hassan Awadallah, AsliCelikyilmaz, Yang Liu, Xipeng Qiu, and DragomirRadev. 2021.QMSum: A New Benchmark forQuery-based Multi-domain Meeting Summarization.In North American Association for ComputationalLinguistics (NAACL). Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wen-hao Wu, Furu Wei, and Sujian Li. 2023. Pose: Effi-cient context window extension of llms via positionalskip-wise training. In The Twelfth International Con-ference on Learning Representations.",
  ": Hyperparameters for contrastive pre-trainingand fine-tuning of E5Base and E5-RoPEBase": "In this section, we describe the training detailsof E5-RoPEBase. Our training procedure and dataexactly follows that of E5 (Wang et al., 2022),where we first perform contrastive pre-trainingon their collected CCPairs, then perform fine-tuning on the concatenation of 3 datasets: MS-MARCO passage ranking (Nguyen et al., 2016),NQ (Karpukhin et al., 2020; Kwiatkowski et al.,2019), and NLI (Gao et al., 2021). Each exam-ple is paired with 7 hard negatives.We lever-age the mined hard negatives and re-ranker scoresfrom SimLM (Wang et al., 2023a) for the firsttwo datasets. As the NLI dataset only provides1 hard negative per example, we randomly sam-ple 6 sentences from the entire corpus. xForm-ers (Lefaudeux et al., 2022) is used for memoryefficient training. As presented in , traininghyperparameters for E5Base and E5-RoPEBase areidentical, except in two aspects: Initialization.Before contrastive pre-training,E5Base is initialized on BERTBase (Devlin et al.,2019), which employs absolute position em-beddings (APE). For the initialization of E5-RoPEBase, we simply replace the APE part ofBERTBase with RoPE. Its worth noting that theBERTBase model after this replacement cannotfunction properly. We count on the subsequentpre-training phase to adapt the model to RoPE.",
  ": Statistics and performance comparison ofE5Base and E5-RoPEBase on 15 publicly available BEIRtasks. # W/Q. and # W/D. stands for word number perquery and per document, respectively": "allows it to generalize to input sequences of upto 512 tokens, while being trained with a maxtraining length of 192. As for E5-RoPE, replac-ing APE with RoPE during initialization preventsus from directly inheriting the original modelscapability in handling 512 tokens. Consequently,in the pre-training phase of E5-RoPE, we setthe maximum training length to 512, and reducethe batch size to 16k according to memory con-straints. demonstrates results of E5Base and E5-RoPEBase on 15 publicly available BEIR tasks. Weobserve comparable overall scores between bothmodels. This comparable performance, along withtheir shared training process and training data, fa-cilitates fair comparison of APE and RoPE-basedmodelss capabilities in length extrapolation. Notethat the slight performance loss of E5-RoPEBasecould possibly be attributed to the replacement ofposition embedding in the initialization phase, orthe reduced batch size in the pre-training phase, asmentioned before.",
  "E5-RoPEBase": "512 -> 1,024Lo = 512, Lt = 1, 024, s = 2 = 3 (10,000 -> 30,000)g = 3, w = 256512 -> 2,048Lo = 512, Lt = 2, 048, s = 4 = 5 (10,000 -> 50,000)g = 5, w = 128512 -> 4,096Lo = 512, Lt = 4, 096, s = 8 = 10 (10,000 -> 100,000)g = 9, w = 64",
  "E5-Mistral": "4,096 -> 8,192Lo = 4, 096, Lt = 8, 192, s = 2 = 3 (10,000 -> 30,000)g = 3, w = 2, 0484,096 -> 16,384Lo = 4, 096, Lt = 16, 384, s = 4 = 5 (10,000 -> 50,000)g = 5, w = 1, 0244,096 -> 32,768Lo = 4, 096, Lt = 32, 768, s = 8 = 10 (10,000 -> 100,000)g = 9, w = 512",
  ": Hyperparameters for plug-and-play context extension strategies": "Further Tuning.On top of PI and RP, we per-form further tuning on both E5Base and GTEBase,utilizing the fine-tuning dataset mentioned in Ap-pendix A. Following the practice of PoSE (Zhuet al., 2023), we manipulate position ids to simu-late long training samples. Concretely, given aninput document D = {x0, x1, ..., xLo1} of orig-inal context length Lo, we introduce a skippingbias term u at the beginning of D, transferring theoriginal position ids D into {0, 1, ..., Lo 1} into{u, u+1, ..., u+Lo1}. 4 For every piece of train-ing data, u is re-sampled from the discrete uniformdistribution U({0, 1, ..., Lt Lo}). In this way, weensure comprehensive coverage of target contextwindow. The training procedure spans 3 epochson 2 A100 GPUs, with a learning rate of 5e4, abatch size of 512, and 100 steps for warmup. Otherhyperparameters are same as .Inference.In inference time, attention scal-ing (Su, 2021; Chiang and Cholak, 2022) is usedby default for all tested models for better lengthextrapolation ability. Especially for GTEBase andE5Base tuned on PI, we use the original positionids when input length not exceeds 512. This isachived by mapping the position ids {0, 1, ..., l}into {0, s, ..., l s}, where s is the scaling factor,l < 512.",
  ": BM25 Results on LONGEMBED. P, N, NQA,QMS, SFD, WQA is short for Passkey, Needle, Narra-tiveQA, QMSum, SummScreenFD, 2WikiMultihopQA": "adopt their test splits. Note that for 2WikiMulti-hopQA, we adopt the length-uniformly sampledversion from Bai et al. (2023b) to better assessthe models capabilities across various contextlengths. For summarization datasets including QM-Sum and SummScreenFD, we adopt the versionprocessed by SCROLLS (Shaham et al., 2022).Since SCROLLS does not include ground truthsummarization in its test sets, we switch to vali-dation set for these two datasets. Particularly forQMSum, as its validation set only have 60 docu-ments, which is too small for document retrieval,we included the train set as well.",
  "The team wanted to understand how they could combine different linguistic features to make a more robust recognition model. They were []": "Project Manager: Can I close this ?\\nUser Interface: Uh we don't have any changes , do we ?\\nProject Manager: Oh , okay .\\nUser Interface: So no . {vocalsound}\\nProjectManager: {vocalsound} There we go . Okay , here we are again . Detailed design {disfmarker} oh , come on . Well {disfmarker} Ah {gap} s Forgot to insert the minutes []",
  "Where was the director of film The Central Park Five born": "Passage 1:\\nMargaret, Countess of Brienne\\nMarguerited'Enghien (born 1365 - d. after 1394), was the ruling suo jure Countess of Brienne and of Conversano, suo jure Lady of Enghien, and Lady of Beauvois from 1394 until an unknown date. []Passage 2:\\nNocher II, Count of Soissons\\nNocher II (died 1019), Count of Bar-sur-Aube, Count of Soissons. He was the son of Nocher I, Count of Bar-sur-Aube. Nocher's brother Beraud (d. 1052) was Bishop of Soissons.Nocher became Count of Soissons, jure uxoris, upon his marriage to Adelise, Countess of Soissons. []",
  "SummScreenFD": "Scrolls / validPenny gets a new chair, which Sheldon enjoys until he finds out that she picked it up from the street. He constantly pesters Penny to dispose of it, to no avail. Note: Melissa Rauch is absent in this episode. [PREVIOUSLY_ON]\\nYou make jumps you can't explain, Will. The evidence explains. Then help me find some evidence. I wouldn't put him out there! Should he get too close, I need you to make sure he's not out there alone. I don't think the Shrike killed that girl in the field. This girl's killer thought that she was a pig. You think this was a copycat? I think I can help good Will, see his face. Hello? They know.\\n(gunshots)\\nYou said he wouldn't get too close. See?\\n(gunshots)\\n(knocking)\\nJack: We're here!\\n(police radio chatter)\\nWill: Could be a permanent installation in your Evil Minds Museum. []",
  "Passkey- / -what is the passkey for Kyree Mays?": "[] The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue.\\nMalayah Graves's pass key is 41906. Remember it. 41906 is the pass key for Malayah Graves.\\nThe sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. []",
  "Needle- / -What is the best thing to do in San Francisco?": "Aaron Swartz created a scraped feed of the essays page. November 2021(This essay is derived from a talk at the Cambridge Union. ) [] The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.\\nThere's a narrow sense in which it refers to aesthetic judgements and a broader one in which it refers to preferences of any kind. []"
}