{
  "Abstract": "TableQA is the task of answering questionsover tables of structured information, returningindividual cells or tables as output. TableQA re-search has focused primarily on high-resourcelanguages, leaving medium- and low-resourcelanguages with little progress due to scarcityof annotated data and neural models. We ad-dress this gap by introducing a fully automaticlarge-scale table question answering (tableQA)data generation process for low-resource lan-guages with limited budget. We incorporate ourdata generation method on two Indic languages,Bengali and Hindi, which have no tableQAdatasets or models. TableQA models trainedon our large-scale datasets outperform state-of-the-art LLMs. We further study the trainedmodels on different aspects, including math-ematical reasoning capabilities and zero-shotcross-lingual transfer. Our work is the firston low-resource tableQA focusing on scalabledata generation and evaluation procedures. Ourproposed data generation method can be ap-plied to any low-resource language with a webpresence. We release datasets, models, andcode.1",
  "Introduction": "Tables are ubiquitous for storing informationacross domains and data sources such as rela-tional databases, web articles, Wikipedia pages,etc. (Deldjoo et al., 2021). Tables introduce newchallenges in machine comprehension not presentin text as they are are not well-formed sentencesbut a semi-structured collection of facts (numbers,long-tail named entities, etc.) (Iyyer et al., 2017;Jauhar et al., 2016; Jin et al., 2022; Katsis et al.,2022; Liu et al., 2021; Nan et al., 2022; Pal et al.,2022; Zhu et al., 2021). Additionally, tables makeposition (rows/columns) bias (Lin et al., 2023)and entity popularity bias (Gupta et al., 2023) se-vere. The tableQA task introduces novel challenges compared to text-based question answering (text-QA) (Herzig et al., 2020; Liu et al., 2021; Ye et al.,2023; Yu et al., 2018; Zhao et al., 2022). In ad-dition to the semi-structured nature of tables, atabular context leads to a high frequency of fact-based questions, mathematical and logical oper-ations such as arithmetic (Zhu et al., 2021), set,relational (Jiang et al., 2022; Liu et al., 2021),and table operations such as table joins (Pal et al.,2023). Effective tableQA systems not only havemachine comprehension skills, but also numeracyunderstanding (Cheng et al., 2022; Liu et al., 2021;Zhao et al., 2022; Zhu et al., 2021), table reasoning(Liu et al., 2021; Yu et al., 2018), table summariza-tion (Zhang et al., 2024; Zhao et al., 2023a) andanswer table generation ability (Pal et al., 2023).Low-resource tableQA aims to answer questionsover semi-structured tables storing cultural andregion-specific facts in a low-resource language.Joshi et al. (2020) show that most languages strug-gle to be represented and are deprived of advancesin NLP research. As manual data collection is slowand expensive, low-resource languages strugglewith large-scale, annotated data for effective trans-fer learning solutions. The low-resource setting(Hedderich et al., 2021; Ruder, 2019) exacerbatesthe challenges of tableQA with challenges of datasparsity, annotated data costs, and lack of trainedmodels. In contrast to textQA, syntactico-semanticvariations such as agreement and morphology arenot exhibited in tables, but high presence of cultur-ally significant yet long-tail entities makes adaptingexisting high resource datasets and trained mod-els challenging. Research on low-resource tableinference (Minhas et al., 2022) shows that stan-dard approaches of translating English datasets forlow-resource data creation are infeasible for tablesdue to high translation error as tables are not well-formed sentences.",
  "(1) low-resource tableQA data scarcity andunder-representation of cultural facts": "(2) Existing neural models poor alignment inlow-resource languages and a lack of under-standing of table structure.This motivates us to explore low-resource tableQAby designing a low-cost and large-scale automaticdata generation and quality estimation pipeline. Wediscuss the process in detail with a low-resourceIndic language, Bengali (spoken extensively inBangladesh and India, with over 230 million na-tive speakers (Karim et al., 2021)), and exploregeneralizability with Hindi (570 million speakers).Our main contributions are as follows:",
  "Related Work": "TableQA aims to answer a user question from semi-structured input tables. Prior work on tableQA inEnglish can be classified as extractive (Herzig et al.,2020; Yin et al., 2020) or abstractive (Nan et al.,2022; Pal et al., 2022; Ye et al., 2023; Zhao et al.,2023b). While extractive tableQA focuses on rowand cell selection (Herzig et al., 2020), abstrac-tive tableQA generates various types of answerssuch as factoid answers (Liu et al., 2021), sum-maries (Zhang et al., 2024; Zhao et al., 2023b), oranswer tables (Pal et al., 2023). Low-resource set-ting poses challenges for various NLP tasks. Thelow-resource corpus creation (Bhattacharjee et al.,2022; Das and Saha, 2022; Hasan et al., 2020)has used automatic annotation efforts by synthe-sizing a large-scale dataset. Das and Saha (2022)train a Bengali QA system by developing a syn-thetic dataset translated from standard English QAdatasets. Bhattacharjee et al. (2022); Hasan et al.(2020) create low-resource datasets by translatingEnglish datasets to Bengali using neural models.",
  "Task Definition": "We formulate low-resource tableQA as a se-quence generation task.Given a question Qof k tokens q1, q2, . . . , qk, and table T compris-ing of m rows and n columns {h1, . . . , hn, t1,1,t1,2, . . . , t1,n, . . . , tm,1, tm,2, . . . , tm,n} where ti,jis value of the cell at the i-th row and j-th col-umn and hj is the j-th column header; the low-resource tableQA model generates an answer tableTout. The input sequence is the concatenated ques-tion Q, and linearized input table T separated byspecial sentinel tokens. The answer, Tout, is also alinearized sequence. Henceforth, for concreteness,we will use Bengali as the example low-resourcelanguage. The input to such a model is:",
  "Methodology for Dataset Generation": "Effective training of low-resourced tableQA re-quires creation of large-scale datasets of questions,input and answers tables, to align a language modelto the low-resource language and adapt it to semi-structured tables and QA task. We address Chal-lenge 1 by designing an automatic data genera-tion process to generate a large-scale low resourcetableQA corpus of training and validation samples.We follow a 3-step pipeline as follows: (i) tableextraction, (ii) question generation, and (iii) an-swer table extraction. This pipeline applied onBengali, as depicted in , generates theBanglaTabQA dataset.",
  "Table Extraction": "English Wikipedia with 6, 751, 000+ articles isused for English tableQA datasets (Pasupat andLiang, 2015), but is insufficient for non-Latin lan-guages with many cultural topics missing. Thestandard process (Bhattacharjee et al., 2022; Dasand Saha, 2022) of translating English datasets tolow-resource languages is biased due to lack of cul-tural topic/fact representation in English tableQA Bengali-English Code-Mixed SQL select count(` `) from ` ()` where ` ` = \"- -- -\" SQL template select count( column_1) from table where column_1 = value_column_1 Mono-Lingual Bengali SQL \" (` `) ` ()` ` ` =\"- -- -\"(` `) Mono-Lingual Natural Language Question ' ()` \"- -- - ' Bengali-English Code-Mixed SQL (Translation) select count(`road section`) from `9 no. state road(West Bengal)` where `road section` = \"Shimlapal- Krishnapur-Raipur-Phoolkushma-Bengoria\" Mono-Lingual Natural Language Question (Translation) search for the total number of \"Shimlapal- Krishnapur-Raipur-Phoolkushma-Bengoria\" in `9 no. state road (West Bengal)` Step 1: Wikipedia Table Extraction RelationalDatabase count (`road section`) Step 3: Answer Extraction Step 2: Natural Language Question Generation Answer Table (Translation) () SQL keyword Translation Dictionary FROM: , WHERE: , .... Bengali SQL2NQ model : BanglaTabQA Dataset generation: The SQL elements and table elements are color-coordinated torepresent a single SQL/table element. Dotted rectangles represent translations for accessibility to non-native readers. datasets. For example, the named-entity Azirajgaguil (Adhiraj Ganguly), exists only in BengaliWikipedia,2 and not in English. Further, translat-ing English tables with machine translation modelsis error-prone (Minhas et al., 2022) as tables arenot well-formed sentences but collections of facts.To mitigate these issues, we extract tables fromWikipedia dump of the low-resource language.",
  "The question generation is a 2-step process:": "Code-mixed SQL query generation.We auto-matically generate SQL queries over the extractedlow-resourced tables with SQL templates from theSQUALL dataset (Shi et al., 2020). These tem-plates have placeholders of table components suchas table name, column names, etc. which are ran-domly assigned with values from a Wikipedia table.For example, the template select count(c1) from w where c1 = value is instantiated by as-signing a Bengali table name 9 noK rajYo sook (picmbg) to w, column header ejla to c1, and bakuaejla to value. This results in an executable code-mixed query select count(ejla) from 9 noK ra-jYo sook (picm bg) where ejla = \"bakua ejla\",where the SQL keywords are in English but alltable information is in the low-resource language(Bengali). This leads to 13, 345, 000 executableBengali code-mixed queries. Natural language question generation.Weformulate question generation as a sequence-to-sequence task by transforming a code-mixed SQLquery into a natural language question (NQ). Tothe best of our knowledge, there exists no sequencegeneration models which translates code-mixed SQL queries to low-resource natural language ques-tions. To train a model for this conversion, wefirst transform the code-mixed SQL to a mono-lingual SQL-like query in the low-resource lan-guage. As the only linguistic variation exhibitedin the SQL templates is polysemy i.e. a dearth ofone-to-one correspondence between English SQLkeywords and the corresponding low-resource lan-guage translations, we employ native speakers well-versed in SQL to manually create one-to-one map-pings of 27 SQL keywords for linguistic trans-fer of SQL keywords to the corresponding low-resource language. All table-specific words aredirectly copied into the monolingual query. We dis-card FROM keyword and table name from the queryas it is associated with a single input table. Thisleads to a SQL-like monolingual query in the low-resource language which is a well-formed sentence.For example, code-mixed Bengali query select",
  "count( ejla) from 9 noK rajYo sook (picm bg)": "where ejla = \"bakua ejla\", results in a mono-lingual Bengali query inbacon korun gNna( ejla)eJxaen ejla = \"bakua ejla\". In contrast to tableswhich are invalid sentences, queries and NQ arewell-formed sequences and effectively transformed(SQL to question) with existing encoder-decodermodels. We train a SQL-to-NQ (SQL2NQ) model(mbart-50-large (Liu et al., 2020) backbone) bytranslating 68, 512 training and 9, 996 validationsamples from semantic parsing datasets: Spider(Yu et al., 2018), WikiSQL (Zhong et al., 2017),Atis (Dahl et al., 1994; Price, 1990), and Geoquery(Zelle and Mooney, 1996) to the low-resource lan-guage. We use this SQL2NQ model to transformthe queries to NQ. For example, Bengali SQL2NQmodel transforms the aforementioned query to theNQ kbar bakua ejlar Uek Aaeq?.",
  "We employ automatic quality control steps to en-sure quality of the synthetic tableQA data": "Code-mixed query and answer quality control.We discard all code-mixed queries which executeto an error with an SQL compiler. This processfollows the quality control in (Pal et al., 2023) anddiscards invalid and erroneous queries and samples. Natural Language Question quality control.We evaluate the quality of the generated NQ with asentence similarity model to discard questions thathave low similarity score with the correspondingmonolingual queries.We found the standardmethod of quality evaluation in low-resourcelanguages (Bhattacharjee et al., 2022; Rameshet al., 2022) using the sentence similarity model,LaBse (Feng et al., 2022), incompatible forcode-mixed SQL-NQ due to low discriminatingability (0.55 mean similarity score and 0.13standard deviation for Bengali SQL-NQ). Forexample, LaBse assigns low score (0.43) forpositive SQL-NQ pair corresponding to the Ben-gali query SELECT title ORDER BY year DESC",
  "World of Saudamini\" and the unrelated NQ How": "many games scored a total of 4?\". in Appendix A.8 shows more examples.Thisnecessitates fine-tuning LaBse on low-resourcedSQL-NQ samples. First, we use the translatedsemantic parsing samples (68, 512 training and9, 996 SQL-NQ pairs), described in .2,as positive pairs and in-batch negatives withmultiple-negatives ranking loss. We call this theSQL2NQSim model. We select the best checkpointby evaluating SQL2NQSim on 1, 000 randomlyselectedhard-negatives(unrelated/negativeSQL-negative question pairs for which pre-trained 0.20.00.20.40.60.81.0",
  ": Histogram of similarity scores from fine-tunedBengali SQL2NQSim model of 1, 000 random samples": "LaBse assigns a high similarity score (> 0.5)).We use that checkpoint to obtain similarity scoresof the low-resourced tableQA SQL-NQ pairs anddiscard samples with a similarity score lower thana threshold. We select a good threshold by plottinga histogram of scores assigned by the SQL2NQSimmodel on 10, 000 randomly selected positivesand hard-negatives and selecting the inflectionpoint as the threshold. shows the scoreshistogram for BanglaTabQA. We select a strictthreshold of 0.74 (hard-negatives scores taper-offaround 0.7). The final BanglaTabQA dataset, afterquality control, comprises of 2, 050, 296 trainingand 2, 053 validation samples.",
  "Dataset Analysis": "In contrast to textQA, tableQA focuses on mathe-matical questions (Liu et al., 2021; Pal et al., 2023;Zhu et al., 2021). Following (Liu et al., 2021),we analyse BanglaTabQA dataset on question com-plexity, which estimates the difficulty of a ques-tion based on the corresponding SQL query. AstableQA enforces mathematical, logical and tablereasoning questions, we further classify tableQAqueries into different classes of table operationsdetermined by the SQL operators present. Question complexity.Recent work on tableQA(Liu et al., 2021) categorizes SQL queries into diffi-culty levels based on the number of SQL keywords.We follow this approach and count the number ofkeywords for each query. shows that mostof BanglaTabQA queries have 4 SQL keywords.The longest SQL queries are comprised of 10 key-words, and the shortest ones of 3 SQL keywords.",
  ":Histogram of operator classes in theBanglaTabQA dataset": "the question. We utilize the SQL query associatedwith a question to extract all keywords for classifi-cation. We categorize data samples into 6 operatorclasses: arithmetic, sorting, group by, filtering, setoperators, and logical operators. Arithmetic oper-ators comprises of SQL numeric operations suchas sum, count, min, etc. Sorting refers to orderingof the answer values in an ascending or descendingorder. Group by is the SQL operator of groupingrows based on a criterion. Filtering corresponds toSQL operators such as where and having used tofilter the input table. Set operators involve union, intersect, and except. Finally, we classify logi-cal operators to be conjunction (and) and disjunc-tion (or) to combine filtering conditions. It alsoincludes membership operators (in, between, etc.)and string matching operator (like). The classifi-cation of the operators is shown in . shows the distribution of the 6 operator classesfor the BanglaTabQA dataset.",
  "Test Set": "We manually annotate test samples for evaluat-ing low-resource tableQA models on clean data.We select unique tables not present in the train-ing and validation set to avoid data leakage. Toensure question diversity, we select code-mixed SQL representing each of the 6 operator classes(discussed in .5) and distinct from thetraining and validation data. Three native anno-tators well-versed in SQL were employed for an-notation. One annotator was tasked with questiongeneration and given the synthetic SQL query, in-put tables and the answer table, and asked to rewritethe code-mixed query to a natural language ques-tion. The remaining two were tasked with evalu-ation of the question generated by the first anno-tator. The evaluator-annotators were provided thecode-mixed query, input table, answer table, andthe annotated question and asked to rate the ques-tion based on fluency. We estimate the annotatedquestion fluency with a 5-point Likert scale (1-5),where a higher score indicates a better fluency. Thefinal score for each question was computed by av-eraging the scores of the evaluator-annotators. ForBanglaTabQA, we manually annotate 165 test sam-ples. We estimate an inter-annotator agreementwith Fliesss Kappa score (Fleiss, 1971) of 0.82,indicating strong agreement among the annotators.The average fluency score across test set questionswas 4.3, indicating high fluency.",
  "Generalizability of Dataset Methodology": "We study the generalizability of the dataset gener-ation method by repeating the process on anotherIndic language: Hindi (Hi) with more than 602million speakers. To the best of our knowledge,there is no existing tableQA data for Indic lan-guages. Hindi text is in Devanagari script whichis different from Bengali written in Eastern-Nagari(Bengali-Assamese) script. This requires tableQAmodels to be trained on large-scale Hindi datasetsfor good alignment. Following the dataset creationprocess in , we extract 1, 921 Hindi ta-bles from the respective Wikipedia dumps. Wegenerate 82, 00, 000 Hindi code-mixed queries au-tomatically to extract answer tables and generatethe Hindi natural language questions. The final Hin-diTabQA dataset comprises of 643, 434 synthetictraining, 645 synthetic validation samples and 121manually annotated test samples.",
  "models effectiveness in Bengali language, mathe-matical/table operations and generalizability, thusproviding a measure of the dataset quality and con-sequently the dataset creation methodology": "Baselines.We perform 2-shot in-context learn-ing (ICL) to adapt large language model (LLM)sto BanglaTabQA task. We further fine-tune anencoder-decoder model. The demonstrations arethe concatenated question and flattened input ta-ble with the flattened answer table. We use thefollowing models as baselines:(1) En2Bn:We fine-tune an encoder-decodermodel, mbart-50-large, with 25, 000 randomsamples from MultiTabQAs (Pal et al., 2023)pre-training data translated to Bengali usingGoogle translate. MultiTabQA used SQUALLtemplates to generate their queries and havethe same distribution as BanglaTabQA queries.However, the input tables of MultiTabQA areEnglish wiki-tables from WikiTableQuestionsdataset (Pasupat and Liang, 2015) and are notrepresentative of Bengali cultural topics/facts.",
  "(2) OdiaG (Parida et al., 2023) is Llama-7b (Tou-vron et al., 2023) adapter-tuned (LoRA (Huet al., 2022)) on 252k Bengali instruction set.3": "(3) GPT: GPT-3.5 (Brown et al., 2020) per-forms well on English tableQA (Zha et al.,2023).GPT-4 (OpenAI et al., 2023) out-performs other LLMs (Chinchilla (Hoffmannet al., 2022), PaLM (Chowdhery et al., 2022))in low-resource languages, including Bengaliand Hindi, on various tasks (14, 000 multiple-choice problems on 57 subjects in a translatedMMLU benchmark (Hendrycks et al., 2021)). BanglaTabQA models.Bengali tableQA mod-els must understand both Bengali script and nu-merals, crucial for mathematical operations. How-ever, Bengali numbers are not present in many state-of-the-art Indic models (Dabre et al., 2022; Galaet al., 2023)4 vocabulary. To the best of our knowl-edge, there is no open-access generative modelwhich understands both table structure and Bengali.We train the following models on BanglaTabQA asthey support Bengali and Hindi numbers and text:(1) BnTQA-mBart: mbart-50-large (Liu et al.,",
  "Evaluation Metrics": "The answer table requires both table structure andcontent evaluation rendering standard text simi-larity metrics (Rouge, BLEU, etc.) inappropriate.We instead evaluate with tableQA evaluation met-rics (Pal et al., 2023). Henceforth, F1 scores are theharmonic mean of the precision and recall scores.(1) Table Exact Match Accuracy (Tab) measuresthe percentage of generated answer whichmatch exactly to the target answer tables.",
  "TabRowColCellTabRowColCellTabRowColCellTabRowColCell": "En2(Bn/Hi)0.053.060.203.070.004.730.004.730.003.370.473.430.005.038.265.03OdiaG0.003.890.003.890.691.770.691.42OpHathi0.000.000.000.000.000.110.370.74GPT-3.51.144.811.675.146.04 10.069.129.844.818.944.999.718.20 10.297.109.81GPT-40.00 13.575.43 14.65 26.83 38.67 26.74 36.5115.53 22.60 16.02 22.25 11.11 21.49 11.76 20.84 BnTQAHiTQA-llama60.08 68.30 60.47 68.309.41 12.35 9.8511.8714.769.92 14.137.29 13.119.71 11.117.66-mBart56.63 64.10 56.79 64.31 35.88 33.16 35.88 33.1692.09 87.97 92.02 87.97 33.06 43.35 33.88 43.35-M2M45.31 58.07 45.29 58.04 28.05 34.55 28.05 34.5589.55 85.32 89.34 85.15 28.93 33.11 28.92 33.10-BnTQA92.40 88.10 92.42 88.12 41.32 47.26 41.32 47.26",
  "is the percentage of correctly predicted rowsamong all target rows": "(3) Column Exact Match F1 (Col): ColumnEM precision is the percentage of correctlypredicted columns and corresponding headersamong all predicted columns. Column EMrecall is the percentage of correctly predictedcolumns among all target columns. (4) Cell Exact Match F1 (Cell) is the most relaxedmetric. Cell EM precision is the percentage ofcorrectly generated cells among all predictedcells. Cell EM recall is the percentage of cor-rectly predicted cells among all target cells.",
  "Results": "Baselines. As reported in , GPT-4 performsthe best on our test set with a table EM accuracyof 26.83%.GPT-3.5 under-performs GPT-4 butis better than open-sourced LLMs. Open-sourceLLMs, OdiaG is pre-trained on Bengali text databut not on structured table data. The low accuracyof OdiaG (0.69%) can be attributed to the mod-els lack of table understanding and table specificquestion which differs significantly from text-basedtasks on which it has been pre-trained on as shownin examples in Appendix A.6. Baseline encoder-decoder model, En2Bn, fine-tuned on translatedtableQA data, correctly generates 4.73% of rowsand cells and under-performs OdiaG, but is betterthan TableLlama. Although fine-tuning improvesBengali understanding, the low scores can be at-tributed to the erroneous translations of Englishtables in the MultiTabQA dataset which corrobo-rate with (Minhas et al., 2022) that table translationleads to error-propagation to down-stream QA task.Further, a lack of culture-specific tables in the Mul-tiTabQA pre-training dataset leads to downgraded performance on topics in the BanglaTabQA testset. In conclusion, GPT-4 is able to perform tablereasoning in low-resourced Bengali, but is veryexpensive and closed-source, limiting its accessi-bility and utility. GPT-3.5s and all open-accessbaseline models low scores demonstrates the needfor both task and language adaptation with a large-scale dataset for training accessible open-sourcelanguage models for low-resourced tableQA. BanglaTabQA models.Parameter-efficient fine-tuned Llama models, BnTQA-llama, achieves com-parable results to GPT-3.5. shows thatfine-tuned encode-decoder models, BnTQA-mBartand BnTQA-M2M, outperforms GPT-4 on table exactmatch accuracy (EM) and column EM F1, but notfor row and cell EM F1. This can be attributed toincorrect header generation of GPT-4 reflecting incolumn and subsequently table EM scores. Apartfrom GPT-4, all other baseline models underper-form BanglaTabQA encoder-decoder models by alarge margin on all metrics. BnTQA-llama overfitsto the validation set, and does not generalize well tothe test set. The low scores of PEFT compared tofull fine-tuning (FT) can be attributed to insufficientalignment of the frozen parameters of the backboneLlama model and sub-optimal tokenization of Ben-gali which has been observed in SentencePiecetokenizers in non-Latin languages (Banerjee andBhattacharyya, 2018; Cui et al., 2023). The resultsestablishes the quality of the BanglaTabQA datasetand its effectiveness in adapting neural models toboth language and table understanding.",
  "a test score of 41.32%. Similar to BanglaTabQA,": "HiTQA-mBart outperforms HiTQA-M2M with a ta-ble EM test score of 33.06% and 28.93% respec-tively. HiTQA-llama underperforms compared tothe encoder-decoder models. All models trained onthe HindiTabQA dataset outperform the two-shotin-context learning baseline models. The resultsfollow a similar trend to BanglaTabQA models andprove that our data generation process is general-izable and the HindiTabQA dataset is able to alignneural models for tableQA task in Hindi.",
  "Zero-shot Cross-lingual Transfer": "We further study generalizability, by selecting thebest performing language, Bengali, and evaluat-ing the BanglaTabQA models on Hindi test set ina zero-shot setting without training on Hindi data.This setup allows us to study the cross-lingual trans-fer of BanglaTabQA models to Hindi with a dif-ferent script, and evaluate how well the modelsgeneralize to new out-of-distribution input tables.BanglaTabQA models are able to perform tablereasoning in Hindi indicating semantic informa-tion transfer across languages. We demonstratesome examples in the Appendix A.7. Table head-ers and numbers generated from math operationsare often in Bengali instead of Hindi (Example 7).Extractive questions are generated correctly (Ex-ample 8). lists the zero-shot cross-lingualscores using the original predictions (named NoPost-Processing) of the BanglaTabQA models onthe Hindi test set defined in .7. Addition-ally, we perform post-processing of the predictionsto translate the predicted tables values to Hindi.As translating tables, composed of numbers andentities, with machine translation systems is unreli-able (Minhas et al., 2022), we follow an automaticpost-processing pipeline to transform predicted an-swer tables to Hindi. First, all lexical occurrenceof Bengali digits in predictions are replaced withHindi digits using a dictionary. Next, all lexicaloccurrence of SQL keyword in Bengali in the pre-diction headers are replaced with a Bengali-to-SQLkeyword mapping and subsequently with a SQL- to-Hindi mapping described in . Thisfixes most of the Bengali presence in the predic-tions. Finally, we translate the predicted columnnames/values in Bengali to Hindi with Googletranslate. shows that post-processing in-creases the scores, demonstrating the generaliz-ability of BanglaTabQA models table reasoningcapabilities on out-of-domain Hindi tables with un-seen cultural entities. This further demonstrates thequality and utility of the BanglaTabQA dataset andour proposed data generation method and qualityof the trained models.",
  "Conclusion": "Our work introduces tableQA for the low-resourcelanguages. We propose a methodology for large-scale dataset development on limited budget andautomatic quality control which can be applied overany low-resource language with a web-presence.We discuss in detail the application of the method-ology with an Indic Language, Bengali, for whichwe release a large-scale dataset, BanglaTabQA.We further demonstrate generalizability of the pro-cess with another language, Hindi. We assess thedatasets quality by effectively training differentBengali and Hindi tableQA models and conductingvarious experiments on model efficacy. Our studieson different operator classes and zero-shot cross-lingual transfer demonstrate that models trainedwith our dataset generalize well to unseen tables.Our proposed methodology can promote further re-search in low-resource tableQA, while our releaseddataset and models can be used to further exploretableQA for Bengali and Hindi.",
  "Limitations": "We design a scalable automatic tableQA data gen-eration method and apply it on with two low-resourced languages: Bengali and Hindi. We re-lease two tableQA datasets: BanglaTabQA andHindiTabQA and several models as outcome. Ourmain results in demonstrate successfuladaptation of neural models to low-resourcedtableQA task. Our extensive experimentation ongeneralizability in .1 and 6.2 shows thatmodels trained on the BanglaTabQA dataset per-forms well across all operator classes and general-ize to unseen languages and tables, proving gener-alizability of the datasets and methodology.Our dataset methodology is generalizable, butit is limited to languages for which unlabelled ta-bles are available online. For very-low resourcelanguages with low web presence, our method hasonly limited impact. Also, we used SQUALL tem-plates for query generation, which do not supportmulti-table operations or complex queries. Weleave addressing these challenges to future work.",
  "Ethical Considerations": "The task and models proposed in the paper isaimed at closing the gap of resource scarcity inlow-resource languages. To do so, we have usedexisting open-source resources publicly availablein the web under MIT, CC-BY-SA-3.0 and MIT,CC-BY-SA-4.0 licenses. Our dataset is generatedsynthetically data and will be released under MIT,CC-BY-SA-4.0 license. Our synthetic samples usetemplates from the SQUALL dataset also releasedunder MIT, CC-BY-SA-4.0 license. Our test datasplits are manually annotated. We pay each an-notator C13.27/hour for their efforts. Further, wehave utilized Wikipedia tables from HuggingfaceWikipedia dataset. Wikipedia tables contain infor-mation about named-entities, facts and events inthe public domain. We do not use any user-specific or sensitive data and information. Our models arebuilt over open-source encoder-decoder models andclosed-source GPT-3.5. Our work did not explicitlyhandle any bias which exists in the aforementionedpre-trained models or Wikipedia.",
  "Acknowledgements": "We thank Elseviers Discovery Lab for theirsupport throughout this project and fundingthis work.This work was also supportedby Dutch Research Council (NWO), underproject numbers 024.004.022, NWA.1389.20.183,KICH3.LTP.20.006, and VI.Vidi.223.166, and theEuropean Unions Horizon Europe program undergrant agreement No 101070212. All content rep-resents the opinion of the authors, which is notnecessarily shared or endorsed by their respectiveemployers and/or sponsors.",
  "Tamali Banerjee and Pushpak Bhattacharyya. 2018": "Meaningless yet meaningful: Morphology groundedsubword-level NMT.In Proceedings of the Sec-ond Workshop on Subword/Character LEvel Models,pages 5560, New Orleans. Association for Compu-tational Linguistics. Abhik Bhattacharjee, Tahmid Hasan, Wasi Ahmad,Kazi Samin Mubasshir, Md Saiful Islam, AnindyaIqbal, M. Sohel Rahman, and Rifat Shahriyar.2022. BanglaBERT: Language model pretrainingand benchmarks for low-resource language under-standing evaluation in Bangla. In Findings of theAssociation for Computational Linguistics: NAACL2022, pages 13181327, Seattle, United States. Asso-ciation for Computational Linguistics. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020.Language models are few-shot learners.In Ad-vances in Neural Information Processing Systems,volume 33, pages 18771901. Curran Associates,Inc. Zhoujun Cheng, Haoyu Dong, Ran Jia, Pengfei Wu,Shi Han, Fan Cheng, and Dongmei Zhang. 2022.FORTAP: Using formulas for numerical-reasoning-aware table pretraining. In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 11501166, Dublin, Ireland. Association for ComputationalLinguistics. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts,Paul Barham, Hyung Won Chung, Charles Sutton,Sebastian Gehrmann, Parker Schuh, Kensen Shi,Sasha Tsvyashchenko, Joshua Maynez, AbhishekRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-odkumar Prabhakaran, Emily Reif, Nan Du, BenHutchinson, Reiner Pope, James Bradbury, JacobAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,Toju Duke, Anselm Levskaya, Sanjay Ghemawat,Sunipa Dev, Henryk Michalewski, Xavier Garcia,Vedant Misra, Kevin Robinson, Liam Fedus, DennyZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,Barret Zoph, Alexander Spiridonov, Ryan Sepassi,David Dohan, Shivani Agrawal, Mark Omernick, An-drew M. Dai, Thanumalayan Sankaranarayana Pil-lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,Rewon Child, Oleksandr Polozov, Katherine Lee,Zongwei Zhou, Xuezhi Wang, Brennan Saeta, MarkDiaz, Orhan Firat, Michele Catasta, Jason Wei, KathyMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,and Noah Fiedel. 2022. PaLM: Scaling languagemodeling with pathways.",
  "Yiming Cui, Ziqing Yang, and Xin Yao. 2023. Efficientand effective text encoding for Chinese LLaMA andAlpaca. arXiv preprint arXiv:2304.08177": "Raj Dabre, Himani Shrotriya, Anoop Kunchukuttan,Ratish Puduppully, Mitesh Khapra, and Pratyush Ku-mar. 2022. IndicBART: A pre-trained model for indicnatural language generation. In Findings of the As-sociation for Computational Linguistics: ACL 2022,pages 18491863, Dublin, Ireland. Association forComputational Linguistics. Deborah A. Dahl, Madeleine Bates, Michael Brown,William Fisher, Kate Hunicke-Smith, David Pallett,Christine Pao, Alexander Rudnicky, and ElizabethShriberg. 1994. Expanding the scope of the ATIStask: The ATIS-3 corpus. In Human Language Tech-nology: Proceedings of a Workshop held at Plains-boro, New Jersey, March 8-11, 1994.",
  "Arijit Das and Diganta Saha. 2022.Deep learningbased bengali question answering system using se-mantic textual similarity. Multimedia Tools Appl.,81(1):589613": "Yashar Deldjoo, Johanne R. Trippas, and Hamed Za-mani. 2021. Towards multi-modal conversationalinformation seeking. In Proceedings of the 44th In-ternational ACM SIGIR Conference on Research andDevelopment in Information Retrieval, SIGIR 21,page 15771587, New York, NY, USA. Associationfor Computing Machinery. Angela Fan, Shruti Bhosale, Holger Schwenk, ZhiyiMa, Ahmed El-Kishky, Siddharth Goyal, Man-deep Baines, Onur Celebi, Guillaume Wenzek,Vishrav Chaudhary, Naman Goyal, Tom Birch, Vi-taliy Liptchinsky, Sergey Edunov, Edouard Grave,Michael Auli, and Armand Joulin. 2021. BeyondEnglish-centric multilingual machine translation. J.Mach. Learn. Res., 22(1). Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Ari-vazhagan, and Wei Wang. 2022. Language-agnosticbert sentence embedding. Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers).",
  "Joseph L. Fleiss. 1971. Measuring nominal scale agree-ment among many raters. Psychological Bulletin,76:378382": "Jay Gala, Pranjal A Chitale, A K Raghavan, VarunGumma, Sumanth Doddapaneni, Aswanth Kumar M,Janki Atul Nawale, Anupama Sujatha, Ratish Pudup-pully, Vivek Raghavan, Pratyush Kumar, Mitesh MKhapra, Raj Dabre, and Anoop Kunchukuttan. 2023.IndicTrans2: Towards high-quality and accessiblemachine translation models for all 22 scheduled In-dian languages. Transactions on Machine LearningResearch. Vivek Gupta, Pranshu Kandoi, Mahek Vora, ShuoZhang, Yujie He, Ridho Reinanda, and Vivek Sriku-mar. 2023. TempTabQA: Temporal question answer-ing for semi-structured tables. In Proceedings of the2023 Conference on Empirical Methods in NaturalLanguage Processing, pages 24312453, Singapore.Association for Computational Linguistics. Tahmid Hasan, Abhik Bhattacharjee, Kazi Samin, Ma-sum Hasan, Madhusudan Basak, M. Sohel Rahman,and Rifat Shahriyar. 2020. Not low-resource any-more: Aligner ensembling, batch filtering, and newdatasets for Bengali-English machine translation. InProceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 26122623, Online. Association for Computa-tional Linguistics. Michael A. Hedderich, Lukas Lange, Heike Adel, Jan-nik Strtgen, and Dietrich Klakow. 2021. A surveyon recent approaches for natural language process-ing in low-resource scenarios. In Proceedings ofthe 2021 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 25452568,Online. Association for Computational Linguistics. Dan Hendrycks, Collin Burns, Steven Basart, AndyZou, Mantas Mazeika, Dawn Song, and Jacob Stein-hardt. 2021. Measuring massive multitask languageunderstanding. Proceedings of the International Con-ference on Learning Representations (ICLR). Jonathan Herzig, Pawel Krzysztof Nowak, ThomasMller, Francesco Piccinno, and Julian Eisenschlos.2020. TaPas: Weakly supervised table parsing viapre-training. In Proceedings of the 58th Annual Meet-ing of the Association for Computational Linguistics,pages 43204333, Online. Association for Computa-tional Linguistics. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,Elena Buchatskaya, Trevor Cai, Eliza Rutherford,Diego de Las Casas, Lisa Anne Hendricks, JohannesWelbl, Aidan Clark, Tom Hennigan, Eric Noland,Katie Millican, George van den Driessche, BogdanDamoc, Aurelia Guy, Simon Osindero, Karen Si-monyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,and Laurent Sifre. 2022. Training compute-optimallarge language models. Edward J Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. 2022. LoRA: Low-rank adaptation oflarge language models. In International Conferenceon Learning Representations.",
  "Nengzheng Jin, Joanna Siebert, Dongfang Li, and Qing-cai Chen. 2022. A survey on table question answer-ing: Recent advances": "Pratik Joshi, Sebastin Santy, Amar Budhiraja, KalikaBali, and Monojit Choudhury. 2020. The state andfate of linguistic diversity and inclusion in the NLPworld. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics, pages62826293, Online. Association for ComputationalLinguistics. Md. Rezaul Karim, Sumon Kanti Dey, Tanhim Islam,Sagor Sarker, Mehadi Hasan Menon, Kabir Hossain,Md. Azam Hossain, and Stefan Decker. 2021. Deep-HateExplainer: Explainable hate speech detection inunder-resourced Bengali language. Yannis Katsis, Saneem Chemmengath, Vishwajeet Ku-mar, Samarth Bharadwaj, Mustafa Canim, MichaelGlass, Alfio Gliozzo, Feifei Pan, Jaydeep Sen,Karthik Sankaranarayanan, and Soumen Chakrabarti.2022. AIT-QA: Question answering dataset overcomplex tables in the airline industry. Proceedingsof the 2022 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies: Industry Track. Weizhe Lin, Rexhina Blloshmi, Bill Byrne, Adriade Gispert, and Gonzalo Iglesias. 2023. An innertable retriever for robust table question answering.In Proceedings of the 61st Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 99099926, Toronto, Canada.Association for Computational Linguistics.",
  "Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, ZeqiLin, Weizhu Chen, and Jian guang Lou. 2021.TAPEX: Table pre-training via learning a neural SQLexecutor": "Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, SergeyEdunov, Marjan Ghazvininejad, Mike Lewis, andLuke Zettlemoyer. 2020. Multilingual denoising pre-training for neural machine translation.Transac-tions of the Association for Computational Linguis-tics, 8:726742. Bhavnick Minhas, Anant Shankhdhar, Vivek Gupta, Di-vyanshu Aggarwal, and Shuo Zhang. 2022. XIn-foTabS: Evaluating multilingual tabular natural lan-guage inference. In Proceedings of the Fifth Fact Ex-traction and VERification Workshop (FEVER), pages5977, Dublin, Ireland. Association for Computa-tional Linguistics. Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi VictoriaLin, Neha Verma, Rui Zhang, Wojciech Kryscinski,Hailey Schoelkopf, Riley Kong, Xiangru Tang,Mutethia Mutuma, Ben Rosand, Isabel Trindade,Renusree Bandaru, Jacob Cunningham, CaimingXiong, and Dragomir Radev. 2022. FeTaQA: Free-form Table Question Answering. Transactions of theAssociation for Computational Linguistics, 10:3549. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-man, Diogo Almeida, Janko Altenschmidt, Sam Alt-man, Shyamal Anadkat, Red Avila, Igor Babuschkin,Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-ing Bao, Mo Bavarian, Jeff Belgum, Irwan Bello,Jake Berdine, Gabriel Bernadett-Shapiro, Christo-pher Berner, Lenny Bogdonoff, Oleg Boiko, Made-laine Boyd, Anna-Luisa Brakman, Greg Brockman,Tim Brooks, Miles Brundage, Kevin Button, TrevorCai, Rosie Campbell, Andrew Cann, Brittany Carey,Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen,Ruby Chen, Jason Chen, Mark Chen, Ben Chess,Chester Cho, Casey Chu, Hyung Won Chung, DaveCummings, Jeremiah Currier, Yunxing Dai, CoryDecareaux, Thomas Degry, Noah Deutsch, DamienDeville, Arka Dhar, David Dohan, Steve Dowl-ing, Sheila Dunning, Adrien Ecoffet, Atty Eleti,Tyna Eloundou, David Farhi, Liam Fedus, NikoFelix, Simn Posada Fishman, Juston Forte, Is-abella Fulford, Leo Gao, Elie Georges, ChristianGibson, Vik Goel, Tarun Gogineni, Gabriel Goh,Rapha Gontijo-Lopes, Jonathan Gordon, MorganGrafstein, Scott Gray, Ryan Greene, Joshua Gross,Shixiang Shane Gu, Yufei Guo, Chris Hallacy, JesseHan, Jeff Harris, Yuchen He, Mike Heaton, Jo-hannes Heidecke, Chris Hesse, Alan Hickey, WadeHickey, Peter Hoeschele, Brandon Houghton, KennyHsu, Shengli Hu, Xin Hu, Joost Huizinga, ShantanuJain, Shawn Jain, Joanne Jang, Angela Jiang, RogerJiang, Haozhun Jin, Denny Jin, Shino Jomoto, BillieJonn, Heewoo Jun, Tomer Kaftan, ukasz Kaiser,Ali Kamali, Ingmar Kanitscheider, Nitish ShirishKeskar, Tabarak Khan, Logan Kilpatrick, Jong WookKim, Christina Kim, Yongjik Kim, Hendrik Kirch-ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,ukasz Kondraciuk, Andrew Kondrich, Aris Kon-stantinidis, Kyle Kosic, Gretchen Krueger, VishalKuo, Michael Lampe, Ikai Lan, Teddy Lee, JanLeike, Jade Leung, Daniel Levy, Chak Ming Li,Rachel Lim, Molly Lin, Stephanie Lin, MateuszLitwin, Theresa Lopez, Ryan Lowe, Patricia Lue,Anna Makanju, Kim Malfacini, Sam Manning, TodorMarkov, Yaniv Markovski, Bianca Martin, KatieMayer, Andrew Mayne, Bob McGrew, Scott MayerMcKinney, Christine McLeavey, Paul McMillan,Jake McNeil, David Medina, Aalok Mehta, JacobMenick, Luke Metz, Andrey Mishchenko, PamelaMishkin, Vinnie Monaco, Evan Morikawa, DanielMossing, Tong Mu, Mira Murati, Oleg Murk, DavidMly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh,Long Ouyang, Cullen OKeefe, Jakub Pachocki, AlexPaino, Joe Palermo, Ashley Pantuliano, Giambat-tista Parascandolo, Joel Parish, Emy Parparita, AlexPassos, Mikhail Pavlov, Andrew Peng, Adam Perel-man, Filipe de Avila Belbute Peres, Michael Petrov,Henrique Ponde de Oliveira Pinto, Michael, Poko-rny, Michelle Pokrass, Vitchyr Pong, Tolly Pow-ell, Alethea Power, Boris Power, Elizabeth Proehl,Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,Cameron Raymond, Francis Real, Kendra Rimbach,Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-der, Mario Saltarelli, Ted Sanders, Shibani Santurkar,Girish Sastry, Heather Schmidt, David Schnurr, JohnSchulman, Daniel Selsam, Kyla Sheppard, TokiSherbakov, Jessica Shieh, Sarah Shoker, PranavShyam, Szymon Sidor, Eric Sigler, Maddie Simens,Jordan Sitkin, Katarina Slama, Ian Sohl, BenjaminSokolowsky, Yang Song, Natalie Staudacher, Fe-lipe Petroski Such, Natalie Summers, Ilya Sutskever,Jie Tang, Nikolas Tezak, Madeleine Thompson, PhilTillet, Amin Tootoonchian, Elizabeth Tseng, Pre-ston Tuggle, Nick Turley, Jerry Tworek, Juan Fe- lipe Cern Uribe, Andrea Vallone, Arun Vijayvergiya,Chelsea Voss, Carroll Wainwright, Justin Jay Wang,Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,CJ Weinmann, Akila Welihinda, Peter Welinder, Ji-ayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,Clemens Winter, Samuel Wolrich, Hannah Wong,Lauren Workman, Sherwin Wu, Jeff Wu, MichaelWu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-ing Yuan, Wojciech Zaremba, Rowan Zellers, ChongZhang, Marvin Zhang, Shengjia Zhao, TianhaoZheng, Juntang Zhuang, William Zhuk, and BarretZoph. 2023. Gpt-4 technical report. Vaishali Pal, Evangelos Kanoulas, and Maarten de Rijke.2022. Parameter-efficient abstractive question an-swering over tables or text. In Proceedings of theSecond DialDoc Workshop on Document-groundedDialogue and Conversational Question Answering,pages 4153, Dublin, Ireland. Association for Com-putational Linguistics. Vaishali Pal, Andrew Yates, Evangelos Kanoulas, andMaarten de Rijke. 2023. MultiTabQA: Generatingtabular answers for multi-table question answering.In ACL 2023: The 61st Annual Meeting of the Asso-ciation for Computational Linguistics, pages 63226634.",
  "Patti Price. 1990. Evaluation of spoken language sys-tems: the ATIS domain. In Speech and Natural Lan-guage: Proceedings of a Workshop Held at HiddenValley, Pennsylvania, June 24-27,1990": "Gowtham Ramesh, Sumanth Doddapaneni, AravinthBheemaraj, Mayank Jobanputra, Raghavan AK,Ajitesh Sharma, Sujit Sahoo, Harshita Diddee, Ma-halakshmi J, Divyanshu Kakwani, Navneet Kumar,Aswin Pradeep, Srihari Nagaraj, Kumar Deepak,Vivek Raghavan, Anoop Kunchukuttan, Pratyush Ku-mar, and Mitesh Shantadevi Khapra. 2022. Samanan-tar: The largest publicly available parallel corporacollection for 11 Indic languages. Transactions of theAssociation for Computational Linguistics, 10:145162.",
  "tial of lexico-logical alignments for semantic parsingto SQL queries. In Findings of EMNLP": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, FeiHuang, and Yongbin Li. 2023. Large language mod-els are versatile decomposers: Decomposing evi-dence and questions for table-based reasoning. InProceedings of the 46th International ACM SIGIRConference on Research and Development in Infor-mation Retrieval, SIGIR 23, page 174184, NewYork, NY, USA. Association for Computing Machin-ery. Pengcheng Yin, Graham Neubig, Wen-tau Yih, and Se-bastian Riedel. 2020. TaBERT: Pretraining for jointunderstanding of textual and tabular data. Proceed-ings of the 58th Annual Meeting of the Associationfor Computational Linguistics. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-ing Yao, Shanelle Roman, Zilin Zhang, and DragomirRadev. 2018. Spider: A large-scale human-labeleddataset for complex and cross-domain semantic pars-ing and text-to-SQL task. In Proceedings of the 2018Conference on Empirical Methods in Natural Lan-guage Processing, pages 39113921, Brussels, Bel-gium. Association for Computational Linguistics. John M. Zelle and Raymond J. Mooney. 1996. Learn-ing to parse database queries using inductive logicprogramming. In Proceedings of the Thirteenth Na-tional Conference on Artificial Intelligence - Volume2, AAAI96, page 10501055. AAAI Press. Liangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, QingyiHuang, Saisai Yang, Jing Yuan, Changbao Su, XiangLi, Aofeng Su, Tao Zhang, Chen Zhou, Kaizhe Shou,Miao Wang, Wufang Zhu, Guoshan Lu, Chao Ye,Yali Ye, Wentao Ye, Yiming Zhang, Xinglong Deng,Jie Xu, Haobo Wang, Gang Chen, and Junbo Zhao.",
  "Weijia Zhang, Vaishali Pal, Jia-Hong Huang, EvangelosKanoulas, and Maarten de Rijke. 2024. Qfmts: Gen-erating query-focused summaries over multi-tableinputs": "Yilun Zhao, Yunxiang Li, Chenying Li, and Rui Zhang.2022. MultiHiertt: Numerical reasoning over multihierarchical tabular and textual data. In Proceedingsof the 60th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 65886600, Dublin, Ireland. Association forComputational Linguistics. Yilun Zhao, Zhenting Qi, Linyong Nan, Boyu Mi, YixinLiu, Weijin Zou, Simeng Han, Ruizhe Chen, XiangruTang, Yumo Xu, Dragomir Radev, and Arman Cohan.2023a. QTSumm: Query-focused summarizationover tabular data. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 11571172, Singapore. Associa-tion for Computational Linguistics. Yilun Zhao, Zhenting Qi, Linyong Nan, Boyu Mi, YixinLiu, Weijin Zou, Simeng Han, Xiangru Tang, YumoXu, Arman Cohan, and Dragomir Radev. 2023b. QT-Summ: A new benchmark for query-focused tablesummarization.",
  "Victor Zhong, Caiming Xiong, and Richard Socher.2017. Seq2SQL: Generating structured queries fromnatural language using reinforcement learning": "Fengbin Zhu, Wenqiang Lei, Youcheng Huang, ChaoWang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021. TAT-QA: A question answeringbenchmark on a hybrid of tabular and textual con-tent in finance. In Proceedings of the 59th AnnualMeeting of the Association for Computational Lin-guistics and the 11th International Joint Conferenceon Natural Language Processing (Volume 1: LongPapers), pages 32773287, Online. Association forComputational Linguistics.",
  "A.1Bengali SQL2NQSim (LaBse fine-tuning)Results": "We evaluate semantic similarity of the LaBse modeltrained on the translated semantic parsing datasetscomprising of Bengali SQL and it correspondingBengali question (.4) and report the valida-tion set results in . Both datasets show highsemantic similarity among query-question pairs.However, BanglaTabQA have a higher semanticsimilarity on various distance metrics indicatinghigher similarity of the query-question pairs com-pared to HindiTabQA. HindiTabQA lower seman-tic scores can be attributed to the lower recall scoresamong query-question pairs leading to lower F1similarity scores.",
  "A.2Bengali SQL2NQ model Results": "We report the validation scores of the SQL2NQmodels in . The Bengali SQL2NQ modelscores are lower than the Hindi SQL2NQ model.Manual inspection of the generated dataset revealsthat the Hindi questions and query have higherlexical overlap compared to the Bengali questions-query pairs where the questions are more naturalleading to lower lexical overlap with the corre-sponding SQL query.",
  "Prompt A.1: 2-Shot ICL Prompt for GPT-3.5/4": "Aapin Ekjn sHayk sHkar iJin baKla \u0017eS\u001ar Ur ednbaKla eTibl eQek baKlay Ur eTibl tir ker. m sairEbK n klamguilr EkiT eTibl in\u001ailixt pYoaTaen elxaHey: <klam> eTibl eHDar <era 1> man 1,1 . man 1,2. ... man 1,n <era 2> man 2,1 . ... <era m> man m,1 .man m,2 . ... . man m,n UdaHrN: 1) \u0017S\u001a: kTa iSeranam kaUTDaUn? <klam> bqr . iSer-anam . vuimka <era 1> 2006 . is ena Ivl . ejkb guDnaIT ...<era 13> 2016 . kaUTDaUn . elh euainn <era14> 2016 . kaUTDaUn . elh euainn <era 15> 2016 .kaUTDaUn . elh euainn",
  "Prompt A.2: 2-Shot ICL Prompt for GPT-3.5/4(English translation)": "You are a helpful assistant who answers Bengali questionsfrom Bengali tables by generating an answer table. A tableof m rows and n columns is written in the following pattern:<column> table header <row 1> value 1,1 | value 1,2 | ...value 1,n <row 2> value 2,1 | ... <row m> value m,1 | valuem,2 | ... | value m,nExamples:1) Question: How many titles are Countdown? <column>year | Title | Role <row 1> 2006 | See No Evil | Jacob Go ...<row 13> 2016 | Countdown | Le Trunin <row 14> 2016 |Countdown | Le Trunin <row 15> 2016 | Countdown | LeTruninAnswer: <column> count(Title) <row 1> 32) Question: How many years have See no Evil as titles?<column> year | Title | Role <row 1> 2006 | See No Evil| Jacob Good Night <row 2> 2006 | See No Evil | JacobGood Night | <row 3> 2006 | See No Evil | Jacob GoodNight ...Answer: <column> count(year) <row 1> 3",
  "A.6BnTabQA Models Qualitative analysis": "We analyze the output of each model with an ex-ample to identify error patterns and factors thatimpact model predictions. The test set question karnaem fuTsal smbykar AQba \u0017Juigt pircalekr AbQanAaeq? (Who has the position of Futsal Coordina-tor or Technical Director?), involves logical oper-ator or after extracting values for fuTsal smbykar(Fulsal Coordinator) and \u0017Juigt pircalekr (Techni-cal Director) from the column AbQan (Position).The input table is shown in (translation ofeach table cell is italicized and in parenthesis fornon-native readers) with target (English translationitalicized and in parenthesis):",
  "AbQan (Position)nam (Name)": "svapit (Chairman)egRg lak (Greg Clark)sH-svapit (Co-Chairman)eDivD igl (David Gil)sazarN spadk (General Secretary)mak builKHYam (Mark Bullingham)ekaPazY (Treasurer)mak baeras (Mark Burroughs)gNmazYm EbK eJageJag pircalk (Media and Communications Director)luIsa ifyas (Louisa Fiennes)\u0017Juittgt pircalk (Technical Director)els irD (Les Reed)fuTsal smbykar (Futsal Coordinator)maIekl ubala (Michael Skubala)jaty delr ekac (puruP) (National Team Coach (Male))gYaerQ saUQegT (Gareth Southgate)jaty delr ekac (nar) (National Team Coach (Female))ifl enivl (Phil Neville)erfair smbykar (Referee Coordinator)inl bYair (Neil Barry)",
  "A.8Comparison of scores of LaBSE andSQL2NQ models": "We qualitatively compare the sentence similaritymodels LaBse and SQL2NQ with examples shownin . We observe that LaBse scores are lowfor positive samples of Bengali SQL queries andthe corresponding Bengali question. Further, neg-ative samples, i.e., Bengali SQL query and an un-related Bengali question has high similarity scores.This trend is not observed for the sentence simi-larity model, SQL2NQ, trained on Bengali SQLqueries and corresponding Bengali natural ques-tions."
}