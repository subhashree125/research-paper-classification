{
  "Abstract": "Large language models (LLMs) are widelyused in question-answering (QA) systems butoften generate information with hallucinations.Retrieval-augmented generation (RAG) offersa potential remedy, yet the uneven retrievalquality and irrelevant contents may distractLLMs. In this work, we address these issuesat the generation phase by treating RAG asa multi-document QA task.We propose anovel decoding strategy, Dynamic ContrastiveDecoding (DVD), which dynamically ampli-fies knowledge from selected documents dur-ing the generation phase. DVD involves con-structing inputs batchwise, designing new se-lection criteria to identify documents worthamplifying, and applying contrastive decod-ing with a specialized weight calculation toadjust the final logits used for sampling an-swer tokens. Zero-shot experimental results onALCE-ASQA, NQ, TQA and PopQA bench-marks show that our method outperforms otherdecoding strategies.Additionally, we con-duct experiments to validate the effectivenessof our selection criteria, weight calculation,and general multi-document scenarios. Ourmethod requires no training and can be inte-grated with other methods to improve the RAGperformance. Our codes will be publicly avail-able at",
  "Introduction": "The emergence of large language models (LLMs)has significantly advanced various natural languageprocessing tasks (Touvron et al., 2023; Achiamet al., 2023).However, despite their extensiveknowledge base and linguistic capabilities, LLMsfrequently struggle with handling new knowledgeand are susceptible to producing outdated contentand hallucinations (Huang et al., 2023; Jiang et al.,2024). A straightforward resolution involves thecontinue updating of LLMs knowledge via train- ing, but such a process typically demands substan-tial time and computational resources.Retrieval-augmented generation (RAG) offersan alternative solution and has drawn substantiveeffectiveness to mitigate hallucination by introduc-ing external knowledge (Gao et al., 2023b; Asaiet al., 2023b). After document retrieval, RAG canbe treated as a multi-document question answering(MDQA) task. Recent studies (Shi et al., 2023a;Yoran et al., 2024) indicate that the variability indocument quality may cause distractions and im-pair the generation quality. Besides, knowledgeconflicts, such as discrepancies within retrieveddocuments and between parametric and externalnon-parametric knowledge, may hinder the perfor-mance of LLMs (Chen et al., 2022; Jin et al., 2024b;Ni et al., 2024). Thus, addressing the integrationof diverse knowledge during generation remains asignificant challenge for LLMs.The primary method for infusing new knowledgeinto LLMs involves supervised fine-tuning or con-tinued training, which is resource-intensive. Priorresearch in RAG has introduced various improve-ments (Vu et al., 2023), such as improving retrievalquality (Shi et al., 2023d; Xu et al., 2023), refiningresponses through multiple iterations (Peng et al.,2023; Li et al., 2024), using optimized prompts(Ni et al., 2024), and developing new decodingstrategies (Shi et al., 2023b; Zhao et al., 2024).However, these methods typically require retrain-ing or multiple iterations. Contrastive decoding (Liet al., 2023) offers a training-free solution for hallu-cination mitigation and inspires many subsequentworks (Shi et al., 2023b; Zhao et al., 2024), butthey often concentrate on single-document scenar-ios and the resolution of conflicts between internaland external knowledge, overlooking the challengeof integrating multiple documents.In this work, we propose a novel decodingstrategy, termed Dynamic Contrastive Decoding(DVD), to enhance the integration of various knowl- query input Query: Who played the weasley brothers in harry potter? retrieved documents",
  ": The framework of DVD. We propose a new decoding strategy with selection criteria and dynamic weightto incorporate knowledge from all documents and amplify knowledge from selected documents": "edge during the generation. The goal of DVD isto dynamically amplify knowledge from selecteddocuments during integration to improve model-generated responses. The process starts with queryassociated with multiple retrieved documents. Wecreate prompts for each question in no-document,single-document, and multi-documents formats,and feed them into LLM in a single batch. Dur-ing each inference step, the model produces logitsfor each prompt. Our method introduces a novelstrategy for assessing logits from different prompts.These logits are then adjusted using contrastive de-coding to refine the logits that guide the token gen-eration. Furthermore, it investigates dynamicallyadjusting weights during the generation process,rather than relying on static values. See for better illustration. To evaluate the effectiveness of our pro-posed method, we conducted zero-shot exper-iments across diverse datasets, including theALCE-ASQA (Gao et al., 2023a), Natural Ques-tions (Kwiatkowski et al., 2019), TriviaQA (Joshiet al., 2017) and PopQA (Mallen et al., 2022).Our experiments, utilizing the Mistral (Jiang et al.,2023), LLaMA2 (Touvron et al., 2023) and Vi-cuna (Chiang et al., 2023) models, demonstrate thatour method consistently achieves superior responsequality. This enhancement is attributed to our novelapproach of dynamically amplifying knowledgefrom selected documents during the integration of different knowledge. A thorough analysis of ourselection criteria, weight computation, and docu-ment count reveals consistent performance gainsacross all datasets. Importantly, our method is en-tirely plug-and-play, requiring no additional train-ing. Furthermore, it seamlessly synergizes withother techniques, further augmenting the efficacyof the RAG system.",
  "Retrieval Augmented Generation": "Retrieval-augmented generation (RAG) is a promi-nent research area in the development of LLMs, sig-nificantly improving answer accuracy and reducinghallucinations, especially in knowledge-intensivetasks (Gao et al., 2023b; Asai et al., 2023a). RAGoperates by retrieving data from external sourcesand integrating it into response generation acrosstwo main phases: retrieval and generation. Thetraining of the retrieval and generation componentscan be conducted independently, sequentially, orjointly (Asai et al., 2023a). This paper focusessolely on the generation phase, where the generatorprocesses both traditional contextual informationand retrieved text segments. Numerous studiesaim to enhance the quality of generation throughmethods such as information compression (Yanget al., 2023; Xu et al., 2023), document rerank-ing (Ma et al., 2023b; Zhuang et al., 2023; Sachan et al., 2022; Shi et al., 2023a), query rewriting (Maet al., 2023a), structural and optimization modifica-tions (Cheng et al., 2023; Shi et al., 2023c). Othermethods include multi-round feedback (Peng et al.,2023; Asai et al., 2023b; Li et al., 2024), and im-proved prompts (Zheng et al., 2023; Ni et al., 2024).While many strategies necessitate training-specificmodules, this paper emphasizes a plug-and-playdecoding strategy that requires no training and isreadily adaptable to various datasets and models.",
  "Knowledge Conflicts": "The generation phase for LLMs involves integrat-ing both internal parametric and external non-parametric knowledge, which is challenging whenknowledge conflict happens (Xu et al., 2024).Many studies have explored the behavior of LLMsin the presence of knowledge conflicts (Chen et al.,2022; Jin et al., 2024a; Ni et al., 2024; Xie et al.,2024; Tan et al., 2024; Jin et al., 2024b). Thesestudies have identified factors that impact the pref-erence of LLM during generation, such as confir-mation bias, text similarity, semantic completeness(Tan et al., 2024; Xie et al., 2024; Jin et al., 2024a).These works typically create conflict datasets anddevelop strategies for better boundary understand-ing and response generation in LLMs, yet oftenlimited to just a few external documents. Our workexpands on this by incorporating multiple docu-ments, aligning with RAG and practical scenarios,aiming to enhance the integration of diverse inter-nal and external knowledge during generation.",
  "Contrastive Decoding": "Contrastive decoding, introduced by Li et al.(2023), identifies text by maximizing log proba-bility discrepancies between expert and amateurmodels. This training-free method is effective andwidely applicable, inspiring many studies (Zhanget al., 2023; Chuang et al., 2024; Jin et al., 2024a;Kim et al., 2023; Shi et al., 2023b; Zhao et al.,2024). Shi et al. (2023b) introduced context-awaredecoding (CAD) to amplify output disparities withand without context, improving performance acrossdatasets. Zhao et al. (2024) used contrastive decod-ing to merge knowledge from internal and exter-nal documents, incorporating a dynamic weight toadjust logits during generation. However, theseapproaches typically consider only one or two re-trieved documents. In contrast, our work addressesthe incorporation of knowledge from multiple docu-ments, introducing new selection criteria and fusion",
  "Notations": "For each sample, we use q to present the ques-tion. The documents are retrieved based on theirrelevance with q. We neglect the retrieval phaseand assume the retrieved documents as D ={d1, d2, ..., dN}, where di is a single document andN is the overall number of documents. 1 Given qand D, our task is to generate answers for q basedon retrieved documents D. The quality of docu-ments varies, while the language model is supposedto incorporate its internal parametric knowledgeand external knowledge from D to generate accu-rate and comprehensive answers.We use x to present the input of large languagemodels, which is constructed based on q, D, andcertain prompt template T, and the output is indi-cated as y. The large language model is presentedas and generates each token in answer y withauto-regressive style. At each time step t, LLM first generate logits zt for answer token yt, andcompute the probability distribution as follows:",
  "Dynamic Contrastive Decoding": "Contrastive Decoding (Li et al., 2023) is an ef-fective method to enhance the difference betweenlogits with different input x and make the log-its used to generate answer y more reasonable.Previous researches (Zhao et al., 2024; Shi et al.,2023b) only compare the input with single docu-ment (i.e., x = T(q, d1)) or without documents(i.e.,",
  "The overall number of retrieved documents N is not lessthan 5, making it a multiple document setting": "x = T(q)). However, we want to incorporateknowledge from all documents and amplify knowl-edge from certain important documents.We construct the input x in a special style. Weconsider multiple inputs simultaneously and ap-ply different prompt templates to construct them.There are three types of inputs, correspondingto three templates. First, we consider the inputwithout the documents, i.e., x1 = T1(q). Sec-ond, we consider the input with all documentsconcatenating together, x2 = T2(q, D).Last,we consider the input with a single document foreach document in D, i.e., x3 = T3(q, d1), x4 =T3(q, d2), ..., xN+2 = T3(q, dN). In conclusion,we construct N + 2 inputs for each sample, whereN is the number of documents. Inspired by Su(2023), we construct these inputs into a batch andfeed them into the LLM. B = {x1, x2, ..., xN+2}.The LLM generates corresponding N + 2 logitssimultaneously for each sample, which is denotedas Z. Z = {z1, z2, ..., zN+2}.",
  "Z = (B)(4)": "We want to incorporate internal and externalknowledge and amplify or neglect knowledge fromcertain documents, which need criteria to assessthe quality of logits and make selections.Pre-vious work often computes the entropy for eachlogit. However, LLMs tend to assign probabilitiesto numerous tokens in the vocabulary after pre-training, leading to the overall entropy being in-fluenced by the meaningless probabilities of manytokens. Therefore, we emphasize the importanceof head tokens, and only compute the entropy fortokens with top K2 probability. We use the scor-ing function f to compute the following score sifor each logits zi in the batch B and get scoresS = {s1, s2, ..., sN+2}:",
  "K is a hyperparameter and we set K to 10 in main ex-periments. The influence of K is demonstrated in section5.1": "are first used to determine the importance of in-ternal parametric knowledge. We assume that themodel should prioritize the provided documents butcannot entirely disregard the influence of internalknowledge. Only if s1 is more than one order ofmagnitude lower than the value of s2 (i.e., s1 <= s2/ 10), should the LLM retain its reliance on internalknowledge. Otherwise, LLM should depend on theknowledge from documents to answer the questionand eliminate self-interference. This weight thresh-old is related to the characteristics of datasets and issettled in the preliminary experiments. The scoress3 to sN+2 are used to determine the importanceof each document. The documents with the low-est score and highest score are selected to adjustthe logits and amplify knowledge from the specificdocument, denoted as zl and zh respectively. Theofficial formula is as follows:",
  "(9)": "where x1 and x2 are inputs without and withdocuments correspondingly, xl and xh correspondto the two inputs with the lowest and highest scores.Our method can be seen as an extension to CADproposed by Shi et al. (2023b). We consider theinfluence of a single document, amplify knowl-edge from specific documents, and design specialmetrics to select the target document during thegeneration process.In the preliminary experiments, we find that thesetting of hyperparameters and are crucial todownstream performance. It is inconvenient to runlots of experiments to explore the perfect weight forevery dataset and language model. Therefore, wewant to dynamically set these weights at each timestep during the generation. Previous work (Zhaoet al., 2024; Jiang et al., 2021) used the highest from the normalized predicted token probabilitiesprobability for LLM confidence, which is not veryeffective in our experiments (See section 5.2 forfurther details). Inspired by Wang and Zhou (2024),we apply the difference in probability between thetop 2 tokens as the confidence. Specifically,",
  "= max(Cl Ch, 0)(12)": "where p(y1t |zi) refers to the highest probabilityfrom distribution zi and p(y2t |zi) refers to the sec-ond highest probability value. Therefore the dy-namic version of is determined by the confidencedifference between logits with the lowest score andhighest score, while is determined by logits withand without documents jointly.In conclusion, we propose a new decoding strat-egy with selection criteria and dynamic weight toincorporate knowledge from all documents and am-plify knowledge from selected documents.",
  "Experimental Settings": "DatasetsWe conduct the experiments on a zero-shot open-domain QA setting, where documentsare retrieved through retrievers.Since the re-trieval phase is not our focus and to ensure faircomparisons with other work, we utilized pre-processed public datasets.Specifically, we ap-ply the ALCE-ASQA benchmark provided by Gaoet al. (2023a), Natural Questions (NQ) and Triv-iaQA (TQA) datasets pre-processed by Izacardand Grave (2020), and PopQA datasets from hug-gingface community3. It is worth noting that theretrieval quality is not perfect, with a Recall@5(R@5) of less than 1. The details of datasets canbe found in the original paper or Appendix A. ModelsDue to cost considerations, we useMistral-7B-v0.1(Jiang et al., 2023), LLaMA2-7B,LLaMA2-13B (Touvron et al., 2023) and Vicuna-13B (Chiang et al., 2023) for experiments, fromwhich not only can we see the impact of differentscales of the same model, but we can also see theimpact of whether the model has been supervisedfinetuned. We also consider Qwen2-7B(Yang et al.,2024) and the corresponding results are presentedin the appendix C for better format. MetricsOur primary evaluation metric is thequality of answers, which is assessed by check-ing whether the gold answers (provided by thedataset) are exact substrings of the generation (Gaoet al., 2023a). We do not use exact match scores be-tween generated answers and gold answers as met-rics because our experiments are zero-shot settingsand our language models possess certain expansionabilities (especially Vicuna-13B model). They tendto generate sentences rather than single words toanswer the question. Therefore, metrics that checksubstrings are more applicable and indicative, de-noted as str-em for further clarification. BaselinesWe propose a new decoding strategy,so we mainly compare our methods with other de-coding methods, such as regular decoding, CAD(Shi et al., 2023b) and work of Zhao et al. (2024).There are various variants for regular decoding,corresponding to decoding based on input with-out a document, with all documents concatenated,and with a single document, which we denote asRegular-closed, Regular-full, Regular-single. Thesingle document is retrieved from the retriever andranked first. There are also two variants for workof Zhao et al. (2024), corresponding to decodingwith fixed weight and the dynamic weight, andwe only consider dynamic weight and denote it asZ-dynamic.The number of documents N is set to 5 and Kis set to 10 in our main experiments. The influenceof these important hyperparameters is explored insection 5.1 and 5.3. To ensure a fair comparison,all decoding methods differ only in their inputs oradjustments to logits. Subsequent token samplingmethods based on the logits remain the same, withthe temperature set to 1 as Gao et al. (2023a). Ad-ditional experimental details, such as the prompttemplate and the setting of the rest hyperparame-ters, can be found in Appendix B.",
  "Main Results": "The results are presented in table 1. From the re-sults, we can see that: (1) Our proposed decod-ing strategy, DVD, consistently outperforms otherdecoding methods with both fixed and dynamicweights across all models. (2) Our method withfixed and dynamic weights shows comparable per-formance, consistent with findings from Zhao et al.(2024)s work. While the fixed weight approachexhibits better performance in certain instances, thedynamic weight approach outperforms it in others.",
  "Vicuna-13B (Chiang et al., 2023)": "Regular-closed26.6834.8562.7827.56Regular-full36.9456.3472.5649.47Regular-single27.5146.8465.5641.34CAD (Shi et al., 2023b)37.9656.9272.4549.14Z-dynamic (Zhao et al., 2024)28.9148.7870.4342.61DVD-fixed38.2457.6772.9549.91DVD-dynamic38.6856.9873.1550.54 : Str-em results under zero-shot setting. Regular-closed, -full, and -single corresponds to Regular Decodingwithout documents, with all documents concatenated, and single document. DVD-fixed means fixed and whileDVD-dynamic refers to dynamic and . The impact of these weights is further exploredin section 5.2. (3) In our experiments, the zero-shot setting and irrelevant retrieved passages posechallenges. However, the fine-tuned Vicuna-13Bachieves great performance under a zero-shot set-ting, which indicates that fine-tuning can enhancethe models robustness and ability to resist irrele-vant information. This is also demonstrated by theexperiments of Qwen2 in the appendix C. The ex-perimental results also show that our method worksfor models of various sizes, regardless of whetherthe model is fine tuned or what architecture it is. (4)Regular-full outperforms Regular-single in most ofthe cases, which is consistent with intuition andprevious findings that increasing retrieved informa-tion can help models better answer questions. Butfor some distracting datasets, such as the ASQAdataset, which we use retrieval results coming fromDPR without reranking, irrelevant information iscaused to potentially interfere with the models. Inthat case, models that havent undergone furtherfine-tuning and lack the ability to utilize contex-tual information and mitigate irrelevant influencesare impacted. That is why Regular-single outper-forms Regular-full for LLaMA2-13B and Mistral- 7B in some datasets. (5) Additionally, Zhao et al.(2024)s work (Z-dynamic) only considers a pair ofdocuments and uses their difference to adjust finallogits, making it a slight improvement comparedto Regular-single. In contrast, CAD applies thedifference between logits with and without docu-ments, making it more similar to Regular-full. Ourmethod demonstrates universality and can achievebetter results after the incorporation of all knowl-edge and dynamical enhancement of knowledgefrom selected documents. (6) We retain most hy-perparameters used in sampling same for simplicity,such as the number of new tokens, temperature, andsample method, which indicates that there may stillbe room for performance improvement. However,given that all decoding strategies employ the samesampling method, our method consistently outper-forms other decoding methods with both fixed anddynamic weights.",
  "Selection Criteria": "In section 3.2, we propose to use the entropy ofhead tokens with top K probability to assess thelogits and choose the logits that are worth ampli-fying (i.e., zl and zh). To demonstrate the effi-ciency of this selection criteria, we compare it withother selection criteria for choosing zl and zh, suchas choosing randomly and choosing based on theranking of the retrieval system. The results arepresented in table 2.The results show that: (1) Our method outper-forms static selection criteria, such as random se-lection or selection based on retrieval ranking. (2)Using the ranking of the retrieval system directlyto select the logits and amplify knowledge alsoyields great improvement compared to results intable 1, while choosing randomly leads to inferiorresults. This indicates the effect of our motivation,amplifying knowledge from specific documents dy-namically selectively during the incorporation ofall documents can help the model generate betteranswers. While the retrieval system can offer in- sights into selecting certain documents compared torandom selection, choosing the document with thehighest retrieval ranking is not always the optimalchoice.In addition to comparison with static selectioncriteria, we also explore the influence of the numberof tokens K. K determines the calculation rangeof entropy, ranging from a few head tokens to alltokens. We conduct experiments with different Kand present the outcomes in the figure 2.All refers to using all tokens to calculate theentropy, which is equivalent to regular entropy. Theresults align with our motivations that the overallentropy, impacted by the meaningless probabilityof numerous tokens, may not adequately representthe quality of distribution in autoregressive-styleLLMs. Head tokens with high probability deservemore attention and can serve as good indicatorsfor documents worth amplifying. The number oftokens considered impacts the performance of bothfixed and dynamic weights, as it affects the selec-tion criteria across different logits. In our main ex-periments, the best performance is achieved whenthe number of tokens K is set to 10. However, theoptimal K may vary depending on the characteris-tics of the dataset and language models, necessitat-ing additional experiments to determine the idealvalue.",
  "The Design of Weight": "In addition to selection criteria, the value of weightalso impacts the final adjustment of logits that areused to sample tokens. is related to the influenceof internal parametric knowledge, while is relatedto the influence of knowledge from selected knowl-edge. Since the former is well studied in previouswork (Li et al., 2023; Shi et al., 2023b), we mainlydiscuss the influence of different implementationsof in this section.The value of can either be a static hyperparam-eter or determined dynamically during the genera-tion phase, as discussed in section 3.2. For staticapproaches, we conduct experiments with differentfixed values of and present the results in table 3.For dynamic approaches, the calculation processinvolves model confidence. We apply the differ-ence in probability between the top 2 tokens as theconfidence, as demonstrated in equation 10 in sec-tion 3.2. Previous researches often use the highestprobability directly as the confidence, which can bepresented in an official formula as Ci = p(y1t |zi).We also conduct experiments to compare these two",
  ": Str-em performance with different N. N isthe number of documents": "implementations.After the calculation of model confidence, howto use confidence to determine the weight is alsoan important issue, leading to various calculationvariants. We apply the difference of confidence asweights as shown in equation 11 and 12. Thereare also variants like using the average confidence( = (Cl + Ch)/2) or using only the confidencethat needs to be emphasized (Zhao et al., 2024)( = Cl). We conduct experiments on all variantsand present the results in table 3.The results show that: (1) The value of signif-icantly impacts performance. The optimal valueof depends not only on language models butalso on the retrieval system. If the overall qual-ity of retrieval is high, the model should prioritizethe concatenation of all documents. Conversely,if the overall retrieval quality is low and irrele-vant documents are present, the model should am- plify specific knowledge and focus on particulardocuments.In our experiments on the ALCE-ASQA benchmark, is set to 0.4 for LLaMA2-13B to get better performance. (2) For dynamic ap-proaches, while many variants lead to great perfor-mance compared to results in table 1, our design ofCi = p(y1t |zi)p(y2t |zi) and = max(Cl Ch, 0)outperforms other variants. This finding alignswith previous research about using the differencein probability between the top 2 tokens as confi-dence (Wang and Zhou, 2024; Xiang et al., 2024),and is consistent with rationality that Cl and Chshould jointly determine the weight. The designof Ci = p(y1t |zi) and = Cl also performs wellcompared to results in table 1 and those of fixedapproaches, making it applicable when speed andcomputational efficiency are prioritized. Whilethere are more designs and combinations for confi-dence and weight calculation, they are beyond thefocus of this paper.",
  "The Number of Documents": "Our work concentrates on multi-document scenar-ios and construct the input for every document assaid in section 3.2. We investigate our methodwith different values of N to demonstrate its ef-fectiveness in a broader range of situations. Forsimplicity, we utilize the dynamic weight approachto represent our method. We primarily compare ourmethod with Regular-full and CAD, as they applyto various document scenarios and serve as strongbaselines. The results are presented in figure 3.The results show that our proposed method canoutperform regular decoding and CAD across dif- ferent number of documents. As the number ofdocuments N increases, the interference of irrele-vant information for LLM is also increasing, whileour method that amplifying knowledge from spe-cific documents can consistently be helpful.",
  "Conclusion": "In this paper, we propose a decoding strategy thatcan amplify knowledge from the selected docu-ments during the generation phase to adjust thefinal logits used to sample answer tokens. We con-struct the inputs batch-wise with different templatesand instructions, and get corresponding logits fromLLM. We design a new selection criteria that com-putes the entropy of head tokens with high prob-ability to assess the logits and choose the onesthat worth amplifying. The contrastive decodingis used to adjust the logits, where the weights arecalculated based on logits dynamically during thegeneration phase.We explore several selection criteria and calcu-lation of weights to demonstrate the efficiency ofour design. Extensive experiments show that DVDmakes consistent improvement on downstream per-formance and is superior to other decoding strate-gies, such as regular decoding and CAD. DVD ex-plores the usage of contrastive decoding under thesetting of multi-documents, making the incorpora-tion process of knowledge more diverse.In conclusion, our method propose a new decod-ing strategy to incorporate knowledge in a morediscriminative way under the multi-document set-ting. Our method is plug-and-play and doesntrequire any training, and it can be combined withother orthogonal methods to improve the overallperformance of the RAG system.",
  "Limitations": "Our work has the following limitations:(1) Our method is applied on the logit level, ne-cessitating access to each logit in the batch, andsubsequently adjusts the final logits used for sam-pling answer tokens. Consequently, its applicabil-ity may be limited to white-box models that areopen-source and offer access to such information.Closed-source models, such as ChatGPT, GPT4,and others, may not be compatible with our methoddue to the lack of access to the underlying logits.(2) We propose to construct the input in a batchwith different templates and instructions, which canhelp LLM consider multiple inputs simultaneously and incorporate all kinds of knowledge includinginternal parametric knowledge and external non-parametric knowledge from documents. However,this methodology may result in increased resourceutilization during inference, particularly in terms ofhardware consumption. Actual hardware consump-tion is directly proportional to the size of batch, i.e.the number of documents. Therefore, our methodmay require lots of resources when applied in situ-ation where the number of documents exceeds 10.To mitigate this limitation, alternative batch con-struction methods can be explored. For instance,concatenating two or more documents into a singleinput within the batch may reduce memory con-sumption. However, its important to note that thisapproach may compromise the accuracy of docu-ment selection.(3) In this paper, we only consider limited situa-tions such as zero-shot muli-document QA settingand models up to 13B due to the cost consider-ation. We will also conduct experiments to testour method on a wider range of application scenar-ios in the future, such as few-shot settings, biggermodels, and more kinds of datasets. While our ap-proach has demonstrated the effectiveness of ampli-fying knowledge from specific documents duringthe generation phase, its important to acknowledgethe existence of various other selection criteria andfusion methods. Further investigation into thesealternatives may yield additional performance im-provements.",
  "Acknowledgements": "This work was supported by National Science andTechnology Major Project (2022ZD0116308) andNational Natural Science Foundation of China(62036001). We also appreciate the anonymousreviewers for their valuable suggestions. The cor-responding author is Houfeng Wang. OpenAI Josh Achiam, Steven Adler, Sandhini Agarwal,Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-man, Diogo Almeida, Janko Altenschmidt, Sam Alt-man, Shyamal Anadkat, Red Avila, Igor Babuschkin,Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-ing Bao, Mo Bavarian, Jeff Belgum, Irwan Bello,Jake Berdine, Gabriel Bernadett-Shapiro, Christo-pher Berner, Lenny Bogdonoff, Oleg Boiko, Made-laine Boyd, Anna-Luisa Brakman, Greg Brockman,Tim Brooks, Miles Brundage, Kevin Button, TrevorCai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan,Che Chang, Fotis Chantzis, Derek Chen, Sully Chen,Ruby Chen, Jason Chen, Mark Chen, BenjaminChess, Chester Cho, Casey Chu, Hyung Won Chung,Dave Cummings, Jeremiah Currier, Yunxing Dai,Cory Decareaux, Thomas Degry, Noah Deutsch,Damien Deville, Arka Dhar, David Dohan, SteveDowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,Simon Posada Fishman, Juston Forte, Isabella Ful-ford, Leo Gao, Elie Georges, Christian Gibson, VikGoel, Tarun Gogineni, Gabriel Goh, Raphael Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, ScottGray, Ryan Greene, Joshua Gross, Shixiang ShaneGu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,Yuchen He, Mike Heaton, Johannes Heidecke, ChrisHesse, Alan Hickey, Wade Hickey, Peter Hoeschele,Brandon Houghton, Kenny Hsu, Shengli Hu, XinHu, Joost Huizinga, Shantanu Jain, Shawn Jain,Joanne Jang, Angela Jiang, Roger Jiang, HaozhunJin, Denny Jin, Shino Jomoto, Billie Jonn, HeewooJun, Tomer Kaftan, Lukasz Kaiser, Ali Kamali, In-gmar Kanitscheider, Nitish Shirish Keskar, TabarakKhan, Logan Kilpatrick, Jong Wook Kim, ChristinaKim, Yongjik Kim, Hendrik Kirchner, Jamie RyanKiros, Matthew Knight, Daniel Kokotajlo, LukaszKondraciuk, Andrew Kondrich, Aris Konstantini-dis, Kyle Kosic, Gretchen Krueger, Vishal Kuo,Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike,Jade Leung, Daniel Levy, Chak Ming Li, RachelLim, Molly Lin, Stephanie Lin, Mateusz Litwin,Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Ade-ola Makanju, Kim Malfacini, Sam Manning, TodorMarkov, Yaniv Markovski, Bianca Martin, KatieMayer, Andrew Mayne, Bob McGrew, Scott MayerMcKinney, Christine McLeavey, Paul McMillan,Jake McNeil, David Medina, Aalok Mehta, JacobMenick, Luke Metz, Andrey Mishchenko, PamelaMishkin, Vinnie Monaco, Evan Morikawa, Daniel P.Mossing, Tong Mu, Mira Murati, Oleg Murk, DavidMely, Ashvin Nair, Reiichiro Nakano, RajeevNayak, Arvind Neelakantan, Richard Ngo, Hyeon-woo Noh, Ouyang Long, Cullen OKeefe, Jakub W.Pachocki, Alex Paino, Joe Palermo, Ashley Pantu-liano, Giambattista Parascandolo, Joel Parish, EmyParparita, Alexandre Passos, Mikhail Pavlov, AndrewPeng, Adam Perelman, Filipe de Avila Belbute Peres,Michael Petrov, Henrique Pond de Oliveira Pinto,Michael Pokorny, Michelle Pokrass, Vitchyr H. Pong,Tolly Powell, Alethea Power, Boris Power, ElizabethProehl, Raul Puri, Alec Radford, Jack Rae, AdityaRamesh, Cameron Raymond, Francis Real, KendraRimbach, Carl Ross, Bob Rotsted, Henri Roussez,Nick Ryder, Mario D. Saltarelli, Ted Sanders, ShibaniSanturkar, Girish Sastry, Heather Schmidt, DavidSchnurr, John Schulman, Daniel Selsam, Kyla Shep-pard, Toki Sherbakov, Jessica Shieh, Sarah Shoker,Pranav Shyam, Szymon Sidor, Eric Sigler, MaddieSimens, Jordan Sitkin, Katarina Slama, Ian Sohl,Benjamin D. Sokolowsky, Yang Song, Natalie Stau-dacher, Felipe Petroski Such, Natalie Summers, IlyaSutskever, Jie Tang, Nikolas A. Tezak, MadeleineThompson, Phil Tillet, Amin Tootoonchian, Eliz-abeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Ceron Uribe, Andrea Val-lone, Arun Vijayvergiya, Chelsea Voss, Carroll L.Wainwright, Justin Jay Wang, Alvin Wang, BenWang, Jonathan Ward, Jason Wei, CJ Weinmann,Akila Welihinda, Peter Welinder, Jiayi Weng, LilianWeng, Matt Wiethoff, Dave Willner, Clemens Win-ter, Samuel Wolrich, Hannah Wong, Lauren Work-man, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao,Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Woj-ciech Zaremba, Rowan Zellers, Chong Zhang, Mar-vin Zhang, Shengjia Zhao, Tianhao Zheng, JuntangZhuang, William Zhuk, and Barret Zoph. 2023. Gpt-4 technical report.",
  "Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu,Dongyan Zhao, and Rui Yan. 2023. Lift yourselfup: Retrieval-augmented text generation with selfmemory. ArXiv, abs/2305.02437": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E. Gonzalez, IonStoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgptquality. Yung-Sung Chuang, Yujia Xie, Hongyin Luo, YoonKim, James R. Glass, and Pengcheng He. 2024. Dola:Decoding by contrasting layers improves factuality inlarge language models. In The Twelfth InternationalConference on Learning Representations. Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.2023a. Enabling large language models to generatetext with citations. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 64656488, Singapore. Associa-tion for Computational Linguistics. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo,Meng Wang, and Haofen Wang. 2023b. Retrieval-augmented generation for large language models: Asurvey. ArXiv, abs/2312.10997. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,Zhangyin Feng, Haotian Wang, Qianglong Chen,Weihua Peng, Xiaocheng Feng, Bing Qin, and TingLiu. 2023. A survey on hallucination in large lan-guage models: Principles, taxonomy, challenges, andopen questions. ArXiv, abs/2311.05232.",
  "Xuhui Jiang, Yuxing Tian, Fengrui Hua, Chengjin Xu,Yuanzhuo Wang, and Jian Guo. 2024. A survey onlarge language model hallucination via a creativityperspective. ArXiv, abs/2402.06647": "Zhengbao Jiang, Jun Araki, Haibo Ding, and GrahamNeubig. 2021. How can we know when languagemodels know? on the calibration of language modelsfor question answering. Transactions of the Associa-tion for Computational Linguistics, 9:962977. Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Xiao-jian Jiang, Jiexin Xu, Li Qiuxia, and Jun Zhao. 2024a.Tug-of-war between knowledge: Exploring and re-solving knowledge conflicts in retrieval-augmentedlanguage models. In Proceedings of the 2024 JointInternational Conference on Computational Linguis-tics, Language Resources and Evaluation (LREC-COLING 2024), pages 1686716878, Torino, Italia.ELRA and ICCL. Zhuoran Jin, Pengfei Cao, Hongbang Yuan, Yubo Chen,Jiexin Xu, Huaijun Li, Xiaojian Jiang, Kang Liu,and Jun Zhao. 2024b. Cutting off the head ends theconflict: A mechanism for interpreting and mitigat-ing knowledge conflicts in language models. ArXiv,abs/2402.18154. Mandar Joshi, Eunsol Choi, Daniel Weld, and LukeZettlemoyer. 2017. TriviaQA: A large scale distantlysupervised challenge dataset for reading comprehen-sion. In Proceedings of the 55th Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 16011611, Vancouver,Canada. Association for Computational Linguistics.",
  "Taehyeon Kim, Joonkee Kim, Gihun Lee, and Se-YoungYun. 2023. Instructive decoding: Instruction-tunedlarge language models are self-refiner from noisyinstructions. arXiv preprint arXiv:2311.00233": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-field, Michael Collins, Ankur Parikh, Chris Alberti,Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-ton Lee, Kristina Toutanova, Llion Jones, MatthewKelcey, Ming-Wei Chang, Andrew M. Dai, JakobUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-ral questions: A benchmark for question answeringresearch. Transactions of the Association for Compu-tational Linguistics, 7:452466.",
  "When do llms need retrieval augmentation? mitigat-ing llms overconfidence helps retrieval augmenta-tion. ArXiv, abs/2402.11457": "Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng,Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Lidn, ZhouYu, Weizhu Chen, and Jianfeng Gao. 2023. Checkyour facts and try again: Improving large languagemodels with external knowledge and automated feed-back. ArXiv, abs/2302.12813. Devendra Sachan, Mike Lewis, Mandar Joshi, ArmenAghajanyan, Wen-tau Yih, Joelle Pineau, and LukeZettlemoyer. 2022. Improving passage retrieval withzero-shot question generation. In Proceedings ofthe 2022 Conference on Empirical Methods in Nat-ural Language Processing, pages 37813797, AbuDhabi, United Arab Emirates. Association for Com-putational Linguistics. Freda Shi, Xinyun Chen, Kanishka Misra, NathanScales, David Dohan, Ed Huai hsin Chi, NathanaelScharli, and Denny Zhou. 2023a. Large languagemodels can be easily distracted by irrelevant context.In International Conference on Machine Learning. Weijia Shi, Xiaochuang Han, Mike Lewis, YuliaTsvetkov, Luke Zettlemoyer, and Scott Wen-tauYih. 2023b. Trusting your evidence: Hallucinateless with context-aware decoding. arXiv preprintarXiv:2305.14739. Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou,Margaret Li, Victoria Lin, Noah A. Smith, LukeZettlemoyer, Scott Yih, and Mike Lewis. 2023c. In-context pretraining: Language modeling beyond doc-ument boundaries. ArXiv, abs/2310.10638.",
  "Jianlin Su. 2023. Naive bayes-based context extension": "Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang,Qi Cao, and Xueqi Cheng. 2024. Blinded by gen-erated contexts: How language models merge gen-erated and retrieved contexts for open-domain qa?ArXiv, abs/2401.11911. Hugo Touvron, Louis Martin, Kevin R. Stone, PeterAlbert, Amjad Almahairi, Yasmine Babaei, Niko-lay Bashlykov, Soumya Batra, Prajjwal Bhargava,Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cris-tian Cantn Ferrer, Moya Chen, Guillem Cucurull,David Esiobu, Jude Fernandes, Jeremy Fu, WenyinFu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,Naman Goyal, Anthony S. Hartshorn, Saghar Hos-seini, Rui Hou, Hakan Inan, Marcin Kardas, ViktorKerkez, Madian Khabsa, Isabel M. Kloumann, A. V.Korenev, Punit Singh Koura, Marie-Anne Lachaux,Thibaut Lavril, Jenya Lee, Diana Liskovich, YinghaiLu, Yuning Mao, Xavier Martinet, Todor Mihaylov,Pushkar Mishra, Igor Molybog, Yixin Nie, AndrewPoulton, Jeremy Reizenstein, Rashi Rungta, KalyanSaladi, Alan Schelten, Ruan Silva, Eric MichaelSmith, R. Subramanian, Xia Tan, Binh Tang, RossTaylor, Adina Williams, Jian Xiang Kuan, PuxinXu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, An-gela Fan, Melanie Kambadur, Sharan Narang, Aure-lien Rodriguez, Robert Stojnic, Sergey Edunov, andThomas Scialom. 2023. Llama 2: Open foundationand fine-tuned chat models. ArXiv, abs/2307.09288. Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, JerryWei, Jason Wei, Chris Tar, Yun-Hsuan Sung, DennyZhou, Quoc Le, and Thang Luong. 2023. Freshllms:Refreshing large language models with search engineaugmentation. ArXiv, abs/2310.03214.",
  "Rongwu Xu, Zehan Qi, Cunxiang Wang, Hongru Wang,Yue Zhang, and Wei Xu. 2024. Knowledge conflictsfor llms: A survey. CoRR, abs/2403.08319": "An Yang, Baosong Yang, Binyuan Hui, Bo Zheng,Bowen Yu, Chang Zhou, Chengpeng Li, ChengyuanLi, Dayiheng Liu, Fei Huang, Guanting Dong, Hao-ran Wei, Huan Lin, Jialong Tang, Jialin Wang, JianYang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, JinXu, Jingren Zhou, Jinze Bai, Jinzheng He, JunyangLin, Kai Dang, Keming Lu, Ke-Yang Chen, KexinYang, Mei Li, Min Xue, Na Ni, Pei Zhang, PengWang, Ru Peng, Rui Men, Ruize Gao, Runji Lin,Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu,Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng,Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, XipinWei, Xuancheng Ren, Yang Fan, Yang Yao, YichangZhang, Yunyang Wan, Yunfei Chu, Zeyu Cui, ZhenruZhang, and Zhi-Wei Fan. 2024. Qwen2 technicalreport. ArXiv, abs/2407.10671. Haoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang,Ning Cheng, Ming Li, and Jing Xiao. 2023. Prca:Fitting black-box large language models for retrievalquestion answering via pluggable reward-driven con-textual adapter. In Conference on Empirical Methodsin Natural Language Processing. Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Be-rant. 2024. Making retrieval-augmented languagemodels robust to irrelevant context. In The TwelfthInternational Conference on Learning Representa-tions.",
  "repository to evaluate our generated answers. Seetheir repository for more details": "Natural Questions (NQ)is a popular QA datasetproposed by Kwiatkowski et al. (2019) and iswidely used in many open-domain researches. Theretrieval system affect downstream performance.Therefore, we use the retrieval results and pre-processed NQ dataset from Izacard and Grave(2020) directly for simplicity. Since our work fo-cus on zero-shot multi-document setting, we onlyuse the test set with 3610 samples. According totheir repository5, the R@5 value is 73.8, making itsuitable for our experiments that aim at improvingperformance under irrelevant interfere. TriviaQA(TQA)is also a popular RAG datasetproposed by Joshi et al. (2017). Like the Natu-ral Questions dataset, we use the retrieval resultsand pre-processed version from Izacard and Grave(2020) directly. According to their repository, theR@5 value is 77.0. This dataset contains over10000 data, which makes its coverage wider, butrequires more time and resources to test. PopQAis a entity-centric QA dataset proposedby Mallen et al. (2022). The author apply cus-tomized templates to construct questions by replac-ing topics in knowledge triplets. They also definethe popularity based on the monthly Wikipediapage views related to the entity mentioned. In thispaper, we concentrate only on the questions andretrieval documents rather their construction andpopularity. The dataset contains 14k data, which isa huge challenge for our equipment. And its ques-tions are constructed based on templates, makingthem not as natural as NQ and TQA. Therefore, weonly test 4267 data sampled from original datasetand pre-processed by huggingface community6.",
  "BExperimental Details": "We provide more details about our experiments inthis section.First, the prompt templates we use in theexperiments are diverse.As for ALCE-ASQAbenchmark,weapplyasqa_closedbook.jsonas T1 for input without document and applyasqa_default.json as T2 and T3 for input with alldocuments and single document. Both files areprovided by original work (Gao et al., 2023a),and we apply their prompts directly to avoidthe influence of different templates. One of theexample of our constructed input based on thesetemplate is presented in table 5. As for the restbenchmarks, we apply simple prompt, \"Question:{question}\\nAnswer:\" for closed-booksetting, and \"Write a high-quality answerfor the given question using only theprovided search results (some of whichmight be irrelevant).\\n\\n {documents}\\n\\nQuestion:{question}Answer:\"formulti-documentsandsingle-documentsetting, where documents are also formattedas\"Document[{document.index}](Title:{document.title}) {document.text}\".Then, we will list the settings of hyperparameterswe used in the experiments. The seed is set to 42.The generation configuration includes, temperateis set to 1, the value of top_p is set to 0.95 and thenumber of max_new_tokens is 300. The value of is set to 0.25 for all models in the setting of fixedweight.",
  "x1Instruction: Write an accurate, engaging, and concise answer for the given question. Use anunbiased and journalistic tone.\\n\\n Question: Who has the highest goals in world football? \\n\\nAnswer:": "x2Instruction: Write an accurate, engaging, and concise answer for the given question using onlythe provided search results (some of which might be irrelevant) and cite them properly. Use anunbiased and journalistic tone. Always cite for any factual claim. When citing several searchresults, use . Cite at least one document and at most three documents in each sentence. Ifmultiple documents support the sentence, only cite a minimum sufficient subset of the documents.\\n\\n Question: Who has the highest goals in world football? \\n\\n Document (Title: FIFAWorld Rankings) FIFA World Rankings The FIFA World Ranking is a ranking system for mensnational teams in association football, ... \\n Document (Title: FIFA World Rankings) based onthe importance of the match and the strength of the opponent. ... \\n Document (Title: FIFAWorld Rankings) The 19 July 2018 release was cancelled following the new calculation methodimplementation. ... \\n Document (Title: World Football Elo Ratings) Ukraine 26 years, and forMontenegro 11 years. For Croatia and Slovakia th ... \\n Document (Title: FIFA World Rankingsystem (20062018)) match status multipliers are as follows: A win against a very highly rankedopponent is a considerably great... \\n Answer: x3Instruction: Write an accurate, engaging, and concise answer for the given question using onlythe provided search results (some of which might be irrelevant) and cite them properly. Use anunbiased and journalistic tone. Always cite for any factual claim. When citing several searchresults, use . Cite at least one document and at most three documents in each sentence. Ifmultiple documents support the sentence, only cite a minimum sufficient subset of the documents.\\n\\n Question: Who has the highest goals in world football? \\n\\n Document (Title: FIFA WorldRankings) FIFA World Rankings The FIFA World Ranking is a ranking system for mens nationalteams in association football, currently led by Belgium.... \\n Answer: x4Instruction: Write an accurate, engaging, and concise answer for the given question using onlythe provided search results (some of which might be irrelevant) and cite them properly. Use anunbiased and journalistic tone. Always cite for any factual claim. When citing several searchresults, use . Cite at least one document and at most three documents in each sentence. Ifmultiple documents support the sentence, only cite a minimum sufficient subset of the documents.\\n\\n Question: Who has the highest goals in world football? \\n\\n Document (Title: FIFA WorldRankings) based on the importance of the match and the strength of the opponent. ... \\n Answer:",
  "x5...x6": "x7Instruction: Write an accurate, engaging, and concise answer for the given question using onlythe provided search results (some of which might be irrelevant) and cite them properly. Use anunbiased and journalistic tone. Always cite for any factual claim. When citing several searchresults, use . Cite at least one document and at most three documents in each sentence. Ifmultiple documents support the sentence, only cite a minimum sufficient subset of the documents.\\n\\n Question: Who has the highest goals in world football? \\n\\n Document (Title: FIFA WorldRanking system (20062018)) match status multipliers are as follows: A win against a very highlyranked opponent is a considerably great... \\n Answer:"
}