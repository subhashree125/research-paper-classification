{
  "Abstract": "When seeking information from unfamiliar doc-uments, users frequently pose questions thatcannot be answered by the documents. Whileexisting large language models (LLMs) iden-tify these unanswerable questions, they donot assist users in reformulating their ques-tions, thereby reducing their overall utility.We curate COULDASK, an evaluation bench-mark composed of existing and new datasetsfor document-grounded question answering,specifically designed to study reformulatingunanswerable questions. We evaluate state-of-the-art open-source and proprietary LLMson COULDASK. The results demonstrate thelimited capabilities of these models in refor-mulating questions. Specifically, GPT-4 andLlama2-7B successfully reformulate questionsonly 26% and 12% of the time, respectively.Error analysis shows that 62% of the unsuc-cessful reformulations stem from the modelsmerely rephrasing the questions or even gener-ating identical questions. We publicly releasethe benchmark1 and the code to reproduce theexperiments2.",
  "Introduction": "Applying large language models (LLMs) to per-form question answering (QA) over documents,such as legal and medical texts, has become in-creasingly popular (Agrawal et al., 2022; Guhaet al., 2023). However, users limited knowledge ofthese documents often results in the formulation ofunanswerable questions, whose assumptions eitherconflict with or cannot be verified with the infor-mation available in the documents. We will refer tothese assumptions as presupposition errors.3 Gaoet al. (2023) and Yu et al. (2023) found that around30% of information-seeking questions written by",
  "Can you read an article and answer a question?": "In 1986, Chief Justice Warren Burger informed the White House of his intent to retire. [...] Feeling that this might well be Reagan's last opportunity to pick a Supreme Court justice, the president and his advisers chose Scalia over Bork. Many factors influenced the decision. Reagan wanted to appoint the first Italian-American justice. In addition, Scalia was ten years younger and would likely serve longer on the Court. Scalia also had the advantage of not having Bork's \"paper trail\"; the elder judge had written controversial articles about individual rights. Scalia was called to the White House and accepted Reagan's nomination. [...]",
  ": An example of an LLM suggesting an alterna-tive relevant question the user could have asked whoseanswers can be found in the document, besides onlyinforming users with the presupposition errors": "users include presupposition errors. Research inthe field has primarily focused on the detectionof unanswerable questions (Rajpurkar et al., 2018;Tran et al., 2023; Hu et al., 2023) and providingexplanations for why such questions cannot be an-swered (Yu et al., 2023; Kim et al., 2023). However,this goal is insufficient for fostering an effectiveinteraction between users and LLMs. Identifyingunanswerable questions only serves as a startingpoint in question reformulation; without additionalguidance or feedback on how to rephrase the ques-tion, users, especially those unfamiliar with thecontent, might find themselves caught in a repeti-tive cycle of formulating questions. In a large-scaleindustrial experiment, Faustini et al. (2023) haveshown that the practice of rewriting unanswerablequestions users ask virtual assistants significantly enhances the experience for millions of users.This work aims to improve the utility of QAsystems by introducing a new task that requiresboth detecting unanswerable questions and gener-ating questions closely related to the initial queryand grounded in the document. Opting to gen-erate a relevant question rather than a summaryof related information emphasizes a user-centeredapproach. While producing a summary is moredocument-focused, formulating a relevant questiontargets understanding and predicting the users in-tent, aligning the interaction more closely with theusers specific needs and queries. We provide an ex-ample of how to suggest such questions in .Although generating relevant and grounded ques-tions conditioned on initial queries offers greaterutility to users, it remains a difficult task even forthe best models in a few-shot setting.We first characterize human-reformulated ques-tions and describe several different strategies forupdating questions to remove presupposition errors.Motivated by these strategies, we curate COUL- DASK, an evaluation benchmark for document-grounded QA that consists of a combination ofexisting and new datasets to study question refor-mulation in the presence of presupposition errors.We evaluate several prompting methods such asfew-shot prompting and chain-of-thought prompt-ing, employing state-of-the-art open-source andproprietary models. The results illustrate the limita-tions of existing models and prompting techniquesin accurately detecting unanswerable questions andreformulating questions: the F1 scores for identify-ing unanswerable questions range from 41.16% to67.82%, and success rates for reformulating ques-tions range from 7.13% to 26.21%, depending onbase models. Analysis shows that most of the un-successful reformulation come from rephrasing orrepeating the original questions and that LLMsare worse at reformulating questions necessitatingglobal edits compared to those solely requiring lo-cal edits.",
  "Related Work": "Several datasets have been proposed to study unan-swerable questions. Rajpurkar et al. (2018) curatethe first document-grounded QA dataset that fea-tures unanswerable questions. More recently, Yuet al. (2023) and Kim et al. (2023) have collectedquestions with presupposition errors from Googleuser queries and Reddit posts, respectively. How to identify unanswerable questions, espe-cially with off-the-shelf LLMs, has remained un-derstudied. Kim et al. (2021) proposed to first ex-tract presuppositions from a question and then per-form natural language inference (NLI) to check forpresupposition violations. However, this pipelinerequires supervision. In practice, supervision isoften not available; Kim et al. (2023) thus exploreprompting large language models in the chain-of-thought style to identify unanswerable questions.However, the results remain unsatisfying; usingGPT-3 only yields detection accuracy that is onlyslightly better than random guesses. Faustini et al. (2023) investigate unanswerablequestions and their reformulations in the domain ofspoken QA, focusing on issues stemming from dis-fluencies, grammatical errors, and awkward phras-ing. We, however, study unanswerable questionsarising from presupposition errors, which requirea more profound semantic comprehension of bothcontexts and questions.Unanswerable questions are closely related toambiguous questions (Min et al., 2020). Whilethere has been extensive research into reformu-lating ambiguous questions (Rao and Daum III,2018; White et al., 2021; Pyatkin et al., 2023; Ma-jumder et al., 2021), the problem of reformulat-ing unanswerable questions receives little attention.Strategies for rephrasing ambiguous questions of-ten involve making questions more specific by men-tioning precise entities or events (Min et al., 2020).In contrast, as we will show in , reformu-lating unanswerable questions necessitates a widerange of strategies.Finally, we discuss the connection betweendocument-grounded QA and open-domain QA(Kim et al., 2023). Document-grounded QA isessentially open-domain QA with the correct doc-ument retrieved. Reformulating questions basedon the identified document is a separate skill fromretrieving the document. Therefore, question refor-mulation is an interesting task in itself.",
  "Task: Question Reformulation": "To assist users with question reformulation whenreading unfamiliar documents, we define the fol-lowing task. Given a document and a user ques-tion, the system must determine if the question isunanswerable. Upon identifying the unanswerablequestion, it must reformulate the question such thatthe new question is answerable by the documentwhile remaining relevant to the original question.",
  ": An overview of COULDASK. Unans% is the percentage of unanswerable questions": "As this task is challenging to formally define, webegin with a qualitative study over a set of examplereformulations by a human user, shown in .Different strategies are applied for different presup-position errors in the reformulation process. Forhandling presuppositions that are contradictory tothe documents, the human user corrects the presup-positions. When it comes to presuppositions thatare unverifiable given the documents, we observethree strategies. The first strategy takes a step backby asking about a less specific event than the eventin the original question. The second strategy seeksa nearest match question that the document cananswer due to a flaw in the original. The third strat-egy refines the original question by asking aboutsomething more specific that can be verified by thedocument. While these strategies are not exhaus-tive, they demonstrate the challenging nature of theproblem and the necessity for establishing sourcesof ground truth in the document.",
  "Motivated by the need to reformulate questionsboth to rely on verified information and to avoidcontradictions, we develop an evaluation bench-": "mark. We consider two important challenges inconstructing benchmarks for this task. (1) Thebenchmark should cover a wide range of domainsto cover different types of presuppositions. (Ex-isting QA datasets that study unanswerable ques-tions mostly rely on Wikipedia articles (Rajpurkaret al., 2018; Gao et al., 2023; Kim et al., 2023).)(2) The evaluation method should be capable offairly assessing equally good reformulated ques-tions, considering the subjective nature of questionreformulation.",
  "Datasets": "Following the desiderata, we select three existingdatasets SQuADv2, BanditQA (Gao et al., 2023),and QA2 (Kim et al., 2023) and to cover a broaderrange of domains, we create three new datasets inthe domains of news, review, and Reddit, wherethe questions are generated by models and verifiedby crowdworkers. We summarize statistics for alldatasets in .",
  "ExampleRevised questionAnsRel": "Question: When did Chick-fil-A open their firstrestaurant in Pennsylvania?Document: [...] he registered the name Chick-fil-A,Inc. From 1964 to 1967, the sandwich was licensed toover fifty eateries, including Waffle House and theconcession stands of the new Houston Astrodome. TheChick-Fil-A sandwich was withdrawn from sale atother restaurants when the first standalone locationopened in 1967, in the food court of the GreenbriarMall, in a suburb of Atlanta. Since 1997, theAtlanta-based company has been the title sponsor ofthe Peach Bowl, an annual college football bowl gameplayed in Atlanta on New Years Eve.",
  "When were chicken sandwiches invented atChick-fil-A?": ": Different aspects of question reformulation. Ans indicates whether the reformulated question can beanswered by the document, and Rel indicates whether the reformulated question is relevant to the original question. We use the documents from BBC news articles4,Reddit pages5, and Yelp reviews6, respectively. Tonot artificially craft unanswerable questions, weinstruct the question generation model to producequestions normally and later identify the unanswer-able ones. To produce a dataset that is challeng-ing for LLMs, we additionally leverage a questionchecking model7. We search for questions thatconfuse the question checking model. Specifically,we sample the question checking model five timesto produce an answer for each of the questions.We gather questions where the question checkingmodel flags the questions are unanswerable any ofthose five times. We then ask three crowdworkersfrom Amazon Mechanical Turk (MTurk) to inde-pendently verify whether the question is answer-able or not8. The question generation model pro-duces 9500, 9964, and 10000 QA pairs for the BBC,Reddit, and Yelp datasets, respectively. We keep278, 313, and 165 examples that have confusedthe question checking model. Finally, the crowd-workers identify 21.22%, 36.10%, and 30.91% ofthe questions are truly unanswerable. We thus con-struct examples that are both challenging to LLMsand high-quality. 4 is used for both question generation model andquestion checking model.8In particular, to annotate whether a question is answer-able, we instruct the crowdworkers to select a span from thedocument to answer the question. If they cannot identify aspan, the question is deemed unanswerable. To reduce noise,we take the majority vote from three crowdworkers. To furtherensure a low noise level in annotations, we remove the exam-ples where answer spans annotated by different crowdworkersare disjoint from each other. SQuADv2, QA2, and BanditQA.We adaptthese datasets to be used for document-groundedQA. Questions in SQuADv2 are mechanisticallyformulated with the intention of being unanswer-able, whereas the other two datasets feature nat-urally occurring unanswerable questions. QA2 iscomposed of natural Google search queries, andBanditQA includes questions formulated by usersduring interactions with LLMs. More informationon the modifications we make to these datasets canbe found in Appendix A.",
  "Evaluating Reformulation": "To improve the utility of QA systems in respondingto unanswerable questions, the reformulated ques-tions must be (1) answerable by the documents and(2) relevant to the original questions posed by theusers. As illustrated by the examples in ,a reformulation could be answerable but not rele-vant, or relevant but not answerable. A successfulreformulation must satisfy both conditions. Thereare multiple equally good reformulations for eachquestion. For instance, when given the originalquestion, it is hard to determine which of the firsttwo reformulations in would be closer tothe users intent. As a result, we opt for a reference-free evaluation approach, where the evaluation doesnot rely on gold reformulations.Measuring the relatedness of two questions in-volves determining how closely their topics, in-tents, and meanings are aligned. With these goalsin mind, we propose three reference-free relevancemetrics: edit distance, entity overlap ratio9, andcosine similarity between the original question and",
  "ZS63.7355.3767.6145.3138.0432.5050.43ZS CoT65.4250.3668.2739.7234.0739.5349.56FS67.2963.0667.6742.8650.0030.2353.52FS CoT64.2571.1068.6142.0349.6143.4056.50": ": Comparing F1 scores for unanswerable question detection with different prompting methods using bothproprietary and open-source models on COULDASK. The best method for each base model is italicized, and the bestmethod across all base models is bolded. the reformulation to indicate the level of relevance.We consider the reformulation to be unanswerableand have zero relatedness for the unanswerablequestions that are not successfully detected by thesystem. To automatically evaluate (1), we traina Llama2-7B model on COULDASK to classifywhether the reformulation is answerable or not.The classifier achieved 95% accuracy on a held-outvalidation set. We release the classifier on HuggingFace . For (2), we calculate the Levenshtein editdistance, use GPT-4 to tag entities for computingentity overlap ratios, and apply OpenAI embeddingmodels to produce question embeddings for com-puting cosine similarities. Finally, a reformulationthat is answerable but irrelevant, or vice versa, isnot yet helpful. To consolidate the evaluation intoa single unified score, using entity overlap ratiosas an example, we assign a score of 1 to a refor-mulation if it is both answerable and has an entityoverlap ratio of more than 50%; otherwise, we as-sign a score of 0. We refer to this binary score assuccess rate.",
  "-billion-parameter models: Llama210 (Touvronet al., 2023), Mistral11 (Jiang et al., 2023), andZephyr12 (Tunstall et al., 2023)": "Comparisons.We consider several promptingapproaches: zero-shot (ZS) and few-shot (FS)prompting and ZS and FS chain-of-thought (CoT)prompting. We first prompt LLMs to determinewhether the input question cannot be answered bythe provided document. For ZS and FS prompting,we use the prompt provided by SurgeAI13, whichexplicitly instructs the model to not produce an an-swer if the answer cannot be determined from thedocument alone. For ZS and FS CoT prompting,we expand the aforementioned prompt by askingthe model to think step by step to come up with areason to explain and support its decision. Onlyquestions determined to be unanswerable proceedto the question reformulation stage. In this stage,all methods are provided with their previous turnswhere the models determine the questions are unan-swerable. ZS and FS prompting instruct the modelto make minimum edits to the original question tomake it answerable. ZS and FS CoT prompting 10huggingface.co/meta-llama/Llama-2-7b-chat-hf11huggingface.co/mistralai/Mistral-7B-Instruct-v0.212huggingface.co/HuggingFaceH4/zephyr-7b-beta13SurgeAI prompt: Answer the following from the abovepassage alone, and if you cant determine the answer based onthe passage, say that you dont know the answer.",
  "Results": "Detecting Unanswerable QuestionsBeing ableto detect unanswerable questions is a necessaryprecondition for successful question reformulation. presents the F1 scores for unanswerablequestion detection using different prompting ap-proaches with each base model. Performance isoften better for existing datasets than new ones,which indicates our approachs effectiveness in gen-erating more challenging questions. Among allmodels, GPT-4 performs best at identifying unan-swerable questions. Surprisingly, both Mistral andZephyr are more accurate at detecting unanswer-able questions than GPT-3.5. Among all promptingtechniques, FS CoT consistently improves uponZS, with a larger degree of improvement observedin smaller models compared to larger ones. Reformulating Questions presents suc-cess rates for question reformulation vary dramati-cally from domain to domain. News (BBC) appearsto be the least challenging domain, with successrates ranging from 10.17 to 52.54 depending onbase models. Reddit is a challenging domain, withsuccess rates ranging from 4.42 to 14.16. The re-sults on Wikipedia are mixed. While SQuADv2,QA2, and BanditQA are in the Wikipedia domain, LLMs achieve the lowest success rates on Ban-ditQA. We hypothesize that user queries writtenduring interaction require deeper revision.Among all base models, GPT-4 achieves thehighest average success rate (26.21), while GPT-3.5has the lowest average success rate (7.13). Amongall open-source models, Zephyr has the best per-formance. When it comes to prompting methods,there is not a clear winner. Different LLMs canbe improved with different prompting approaches.GPT-3.5, Mistral, and Zephyr benefit the most fromFS CoT prompting, GPT-4 from ZS, Llama2 fromFS. We include individual metrics for question re-formulation in Appendix B.",
  "Analysis": "Qualitative analysisWe randomly sample andanalyze 20 reformulated questions from each basemodel (a total of 100 questions) that cannot be an-swered by the corresponding documents. We sum-marize the results in . We identify three ma-jor types of errors. The most frequent type is thatthe models simply rephrase or generate the samequestions. Most of the errors in this type are con-tributed by open-source models such as Llama2.Another type of error that occurs 14% of the timeis that the models generate a question by copyinga document span that looks similar to the originalquestion. For example, given the original question",
  ": Error analysis on 100 revised questions generated by different models": "about the characteristics of the Classical Romanarchitecture, the model identifies a span that is simi-lar to the original question and generates a questionbased on this span. However, the model does notplan thoroughly about whether the new questioncan be answered or not.For 17% of the time, the models propose ques-tions that are irrelevant to the original questions.For example, in the Snow Patrol question, the sub-ject of the reformulation has entirely changed fromasking about the most famous song to famous mu-sicians. Finally, we group the remaining elevenerrors in the Other type, where there usually is noteven a question produced. Limited benefit from FS CoT on Question Re-formulation.We explore why FS CoT improvesthe detection of unanswerable questions but notquestion reformulation by conducting a qualitativeanalysis of GPT-4 outputs on a BanditQA example,as illustrated in . It is relatively straightfor-ward to determine why a question is unanswerable either the document does not provide the neces-sary information or there is a presupposition thatconflicts with the document. However, questionreformulation demands compositional reasoning.The model needs to first decide on a reformulationstrategy and then plan the specific steps to achievethe reformulation. The reformulated question gen-erated by GPT-4 via FS CoT prompting is closerto being answerable. However, the model missesa subtle detail the document only mentions theexercise habits of 30.8% of residents, not the gen-eral population. Therefore, FS CoT alone, withoutfurther methodological innovation, does not fully Document:In December 2007, Sport England published a surveywhich revealed that residents of St Albans were the 10thmost active in England in sports and other fitness activities.30.8% of the population participate at least 3 times a weekfor 90 minutes.Question:What is St Albans Sport?ZS CoT for unanswerable question detection:The article did not provide information on what St AlbansSport is, so I dont know.ZS CoT for question reformulation:The article discusses the activity level of the residents of StAlbans in sports and fitness but does not detail a specificsport. Instead of asking \"What is St Albans Sport?\" wecould ask, \"How often do residents of St Albans participatein sports and fitness activities?\" to get an answer from thegiven passage.ZS for question reformulation:What is the level of participation in sports and other fitnessactivities in St Albans?",
  "address the challenge of question reformulation": "Compositional Modifications vs. Answerabil-ity.We hypothesize that it is more difficult toreformulate an unanswerable question to be answer-able when it requires compositional modifications,which means making global edits to the questioninstead of making local edits.To quantify edits required, we follow Lee et al. (2020) and annotate the minimum span of a ques-tion to explain why the question cannot be an-swered by the document. We use GPT-4 to annotateunanswerable questions in BanditQA, QA2, andYelp. We divide the questions into two categories. q spanshort span",
  ": Examples of a question with the minimum spanbeing the entire question (q span) and a question wherethe minimum span is a noun (short span)": "The first category comprises questions whose min-imum span is the entire question. We call these qspan. The second category covers questions thathave a shorter minimum span, typically a noun ora noun phrase. We call these short span. We cal-culate how many reformulations are answerable,broken down by the type of minimum span. Weconsider the reformulations generated by zero-shotprompting with GPT-4, GPT-3.5, and Zephyr.The results of this analysis are in . Ourfindings consistently show that fewer reformula-tions are answerable when the minimum span con-stitutes the entire question compared to when theminimum span is shorter, with only one exceptionin zero-shot prompting with Zephyr on Yelp.We present examples for each span type in Ta-ble 7. When the minimum span is the entire ques-tion, we cannot attribute the presupposition error toa smaller segment of the question. As a result, themodification has to be applied to the full question.For example, for the q-span example in ,modifying individual parts of the question cannotcorrect the presupposition error, and therefore themodel needs to make global changes. On the con-trary, when the minimum span is short, it only re-",
  ": The correlation between the tested metricsand human-judged relevance between two questions,evaluated using Fleiss": "quires local edits. For the short-span question in, replacing USB-C with USB is enough tocorrect the question. Future efforts should be moredevoted to the more challenging cases to makeprogress on question reformulation. Sufficiency of entity overlap ratios.Our met-ric is for relevance is based on a minimum entityoverlaps. To judge the sufficiency of this metric,we compare it to human evaluation. As a baselinewe consider Levenshtein edit distance, a method tomeasure the similarity between two questions, as abaseline metric. We have a human annotator evalu-ate 200 pairs of reformulated questions producedby zero-shot prompting with GPT-4 and GPT-3.5on BanditQA. The question pairs are randomlyshuffled to ensure the annotator remains unawareof the model source of each reformulation. Theannotator then selects the more relevant questionfrom each pair, based on alignment with the orig-inal questions intent. Subsequently, we identifythe question with a higher entity overlap ratio andthe question with a lower edit distance as the morerelevant ones, respectively.We calculate the Fleiss score between humanevaluations and each of the metrics, with the resultssummarized in . Compared to edit distance,the entity overlap ratio more accurately representsthe relevance between the original and reformu-lated questions (93.99 vs. 40.96). A Fleiss score of 93.99 also suggests a near-perfect agreementbetween the entity overlap ratio and human judge-ments. We hypothesize that the specific semanticproperties of the questions are more central thanthe surface form represented by edit distance.",
  "Limitations": "While our benchmark offers advantages over exist-ing sources, we acknowledge the following limi-tations. Questions in BBC, Reddit, and Yelp aregenerated by GPT-4, and they may not accuratelyrepresent questions posed by humans. Despite bestefforts to ensure high-quality annotations, occa-sional human errors are possible. Additionally, ourbenchmark only collects English questions and thuslacks language diversity. Finally, regarding evalu-ation, the way we currently measure success ratesonly focuses on mistakes made on unanswerablequestions. If an answerable question is detectedto be unanswerable, we do not evaluate questionreformulation in such cases.",
  "This work was supported by NSF CAREER#2037519 and NSF #1901030. We also thank Lil-lian Lee and the anonymous reviewers for theirhelpful feedback": "Monica Agrawal, Stefan Hegselmann, Hunter Lang,Yoon Kim, and David Sontag. 2022. Large languagemodels are few-shot clinical information extractors.In Proceedings of the 2022 Conference on Empiri-cal Methods in Natural Language Processing, pages19982022, Abu Dhabi, United Arab Emirates. Asso-ciation for Computational Linguistics. Pedro Faustini, Zhiyu Chen, Besnik Fetahu, OlegRokhlenko, and Shervin Malmasi. 2023. Answer-ing unanswered questions through semantic refor-mulations in spoken QA. In Proceedings of the 61stAnnual Meeting of the Association for Computational",
  "Linguistics (Volume 5: Industry Track), pages 729743, Toronto, Canada. Association for ComputationalLinguistics": "Ge Gao, Hung-Ting Chen, Yoav Artzi, and Eunsol Choi.2023. Continually improving extractive QA via hu-man feedback. In Proceedings of the 2023 Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 406423, Singapore. Associationfor Computational Linguistics. Neel Guha, Julian Nyarko, Daniel E. Ho, ChristopherRe, Adam Chilton, Aditya Narayana, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel Rock-more, Diego Zambrano, Dmitry Talisman, EnamHoque, Faiz Surani, Frank Fagan, Galit Sarfaty, Gre-gory M. Dickinson, Haggai Porat, Jason Hegland,Jessica Wu, Joe Nudell, Joel Niklaus, John J Nay,Jonathan H. Choi, Kevin Tobia, Margaret Hagan,Megan Ma, Michael Livermore, Nikon Rasumov-Rahe, Nils Holzenberger, Noam Kolt, Peter Hender-son, Sean Rehaag, Sharad Goel, Shang Gao, SpencerWilliams, Sunny Gandhi, Tom Zur, Varun Iyer, andZehua Li. 2023. Legalbench: A collaboratively builtbenchmark for measuring legal reasoning in largelanguage models. In Thirty-seventh Conference onNeural Information Processing Systems Datasets andBenchmarks Track. Shengding Hu, Yifan Luo, Huadong Wang, XingyiCheng, Zhiyuan Liu, and Maosong Sun. 2023. Wontget fooled again: Answering questions with falsepremises. In Proceedings of the 61st Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 56265643, Toronto,Canada. Association for Computational Linguistics. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023. Mistral7b. arXiv preprint arXiv:2310.06825. Najoung Kim, Phu Mon Htut, Samuel R. Bowman, andJackson Petty. 2023. (QA)2: Question answeringwith questionable assumptions. In Proceedings of the61st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages84668487, Toronto, Canada. Association for Com-putational Linguistics. Najoung Kim, Ellie Pavlick, Burcu Karagol Ayan, andDeepak Ramachandran. 2021. Which linguist in-vented the lightbulb? presupposition verification forquestion-answering. In Proceedings of the 59th An-nual Meeting of the Association for ComputationalLinguistics and the 11th International Joint Confer-ence on Natural Language Processing (Volume 1:Long Papers), pages 39323945, Online. Associationfor Computational Linguistics.",
  "in SQuAD 2.0. In Proceedings of the Twelfth Lan-guage Resources and Evaluation Conference, pages54255432, Marseille, France. European LanguageResources Association": "Bodhisattwa Prasad Majumder, Sudha Rao, Michel Gal-ley, and Julian McAuley. 2021. Ask whats missingand whats useful: Improving clarification questiongeneration using global knowledge. In Proceedingsof the 2021 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 43004312,Online. Association for Computational Linguistics. Sewon Min, Julian Michael, Hannaneh Hajishirzi, andLuke Zettlemoyer. 2020. AmbigQA: Answering am-biguous open-domain questions. In Proceedings ofthe 2020 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), pages 57835797, Online. Association for Computational Lin-guistics. Valentina Pyatkin, Jena D. Hwang, Vivek Srikumar,Ximing Lu, Liwei Jiang, Yejin Choi, and ChandraBhagavatula. 2023. ClarifyDelphi: Reinforced clari-fication questions with defeasibility rewards for so-cial and moral situations. In Proceedings of the 61stAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 1125311271, Toronto, Canada. Association for Computa-tional Linguistics. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.Know what you dont know: Unanswerable ques-tions for squad. In Proceedings of the 56th AnnualMeeting of the Association for Computational Lin-guistics (Volume 2: Short Papers), pages 784789. Sudha Rao and Hal Daum III. 2018. Learning to askgood questions: Ranking clarification questions us-ing neural expected value of perfect information. InProceedings of the 56th Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 27372746, Melbourne, Aus-tralia. Association for Computational Linguistics. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288.",
  "Son Quoc Tran, Gia-Huy Do, Phong Nguyen-ThuanDo, Matt Kretchmar, and Xinya Du. 2023. Agent: Anovel pipeline for automatically creating unanswer-able questions. arXiv preprint arXiv:2309.05103": "Lewis Tunstall, Edward Beeching, Nathan Lambert,Nazneen Rajani, Kashif Rasul, Younes Belkada,Shengyi Huang, Leandro von Werra, ClmentineFourrier, Nathan Habib, et al. 2023. Zephyr: Di-rect distillation of lm alignment.arXiv preprintarXiv:2310.16944. Julia White, Gabriel Poesia, Robert Hawkins, DorsaSadigh, and Noah Goodman. 2021. Open-domainclarification question generation without question ex-amples. In Proceedings of the 2021 Conference onEmpirical Methods in Natural Language Processing,pages 563570, Online and Punta Cana, DominicanRepublic. Association for Computational Linguistics. Xinyan Yu, Sewon Min, Luke Zettlemoyer, and Han-naneh Hajishirzi. 2023. CREPE: Open-domain ques-tion answering with false presuppositions. In Pro-ceedings of the 61st Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 1045710480, Toronto, Canada. As-sociation for Computational Linguistics.",
  "We provide more details about the existing datasetswe include in COULDASK": "SQuADv2is a collection of QA pairs fromWikipedia articles, including questions with pre-supposition errors. Such questions are written bycrowdworkers who are instructed to craft questionsthat are plausible but have presuppositions that can-not be verified by the associated articles. Theseannotators either modify valid questions or createnew questions based on entities or topics related tothe text, ensuring the questions appear relevant anddeceivingly valid. QA2contains QA pairs from general web articles.QA2 sources questions from Googles autocomple-tion API. It calls the API on prefix strings when,where, which, how, what, why, who, whose tocomplete the queries. Crowdworkers are recruitedto (1) search for relevant documents and (2) checkwhether these questions contain presupposition er-rors. QA2 features naturally occurring presuppo-sition errors and diverse document sources. Theoriginal QA2 dataset is open-domain QA and onlyprovides annotations for relevant URLs. To obtainthe context, we scrape the text from their annotatedURL and remove the examples where the contextsare not clear. BanditQAinvestigate whether continual humanfeedback can improve extractive QA systems; werepurpose the dataset to study naturally occurringpresupposition errors. During data collection, usersare presented with Wikipedia passages, and theyare required to write questions about things theyare curious to know. In a later verification step,over 30% of these questions are identified to havepresupposition errors. BanditQA is closest to the",
  "BMore Analysis": "Hacking success rates.We explore the poten-tial for our proposed metrics to be manipulated inorder to artificially achieve perfect scores. We con-sider two approaches. The first approach directlyprompts LLMs to produce an answerable questionwhile leaving all the entities unchanged. In the sec-ond approach, we test a rule-based heuristic thathas the following steps: (1) tag entities in the orig-inal question via LLM prompting, (2) select thesentence in the document that has the highest en-tity overlap ratio with the original question, and (3)replace something in the select sentence to createa wh-question, again using LLM prompting. Theresults, summarized in , indicate that oursuccess rates remain robust, even when faced withmethods explicitly designed to exploit our metric.We seek to understand where performance de-grades in a rule-based heuristic by inspecting indi-vidual metrics used to compute success rates. Wepresent the results in . While using therule-based heuristic leads to similar answerabilityperformance, the entity overlap ratio drops signifi-cantly. We show further qualitative analysis of theerror types in . Individual Metrics in Question ReformulationWe present individual metrics for question refor-mulation. presents the answerability (top)and entity overlap ratios (bottom) achieved by eachmethod using each base model. We additionallyreport cosine similarities in and edit dis-tance in .",
  ": Inter-annotator agreement rates among threeworkers for annotating unanswerable questions": "Assessing the Accuracy of Entity Tagging Mod-els through Human EvaluationTo determinethe entity overlap ratio, we first utilize GPT-4 toidentify entities within both the original and revisedquestions. To ensure the credibility of the taggedentities, we conduct a human evaluation on a setof 100 questions. The findings of this evaluationare shown in . This analysis reveals thatGPT-4 exhibits both high precision and recall in theidentification of entities, affirming its effectivenessfor this task.",
  "CMore Annotation Details": "Annotation GuidelinesWe present annotationguideline for annotating unanswerable questions in. The crowdworkers are those who wereidentified to contribute high-quality annotationsfrom our previous annotation tasks. For every com-pleted HIT, we pay the crowdworker USD 0.5. AnnotationAgreementWereportinter-annotator agreement rates between crowdworkersin . Specifically, we compute how oftenall three workers agree with each other.Theagreement rates on the three datasets are close to",
  "each other": "Failure case 1: Not all entities present in the same sen-tenceOriginal unanswerable question: What are balance inan open system of particles?Entities: balance, open system, particlesMost-overlapped sentence in the document: This meansthat in a closed system of particles, there are no internalforces that are unbalanced.Reformulated Question: What does this mean about in-ternal forces in a closed system of particles?Note: The three entities are mentioned in different sen-tences. Therefore, the most overlapped sentence only hasone overlapped entity. Although this question can be an-swered, the reformulation is not successful due to the lowentity overlap ratio. Note that most of our questions havean average number of two to three entities.Failure case 2: Reformulated questions are still unan-swerableOriginal unanswerable question: What job requires noqualifications?Entities: job, qualificationsMost-overlapped sentence in the document: As in theHouse of Commons, a number of qualifications apply tobeing an MSP.Reformulated Question: What qualifications apply tobeing an MSP as in the House of Commons?Note: Even though a part of the sentence was replacedwith a what question, this process does not necessarilymake the reformulated question become answerable. Thearticle does not mention what specific qualifications, andsuccessful reformulations require a deeper understandingof the document as a whole."
}