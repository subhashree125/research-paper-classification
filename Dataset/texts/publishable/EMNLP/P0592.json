{
  "Abstract": "Continual learning (CL) is crucial for languagemodels to dynamically adapt to the evolvingreal-world demands.To mitigate the catas-trophic forgetting problem in CL, data replayhas been proven a simple and effective strat-egy, and the subsequent data-replay-based dis-tillation can further enhance the performance.However, existing methods fail to fully exploitthe knowledge embedded in models from previ-ous tasks, resulting in the need for a relativelylarge number of replay samples to achievegood results. In this work, we first exploreand emphasize the importance of attentionweights in knowledge retention, and then pro-pose a SElective attEntion-guided KnowledgeRetention method (SEEKR) for data-efficientreplay-based continual learning of large lan-guage models (LLMs). Specifically, SEEKRperforms attention distillation on the selectedattention heads for finer-grained knowledgeretention, where the proposed forgettability-based and task-sensitivity-based measures areused to identify the most valuable attentionheads.Experimental results on two contin-ual learning benchmarks for LLMs demon-strate the superiority of SEEKR over the ex-isting methods on both performance and effi-ciency. Explicitly, SEEKR achieves compa-rable or even better performance with only1/10 of the replayed data used by other meth-ods, and reduces the proportion of replayeddata to 1%. The code is available at",
  "Perplexity": "Old Task ModelFinal ModelFinal Model with Grafted AttentionFinal Model with SEEKR (ours) : Demonstration of the critical role of atten-tion weights in knowledge retention. We apply DER++(Buzzega et al., 2020) for continual learning on theTRACE benchmark (Wang et al., 2023c) to obtain mul-tiple old task models and the final model. Grafting theattention weights of the old models onto the final modelat inference can maintain better performance on the oldtasks. Moreover, the final model obtained by our contin-ual learning method, SEEKR, achieves similar results. without the need for costly retraining. However,sequentially finetuning the LLMs with new datacan lead to catastrophic forgetting (McCloskey andCohen, 1989), impairing the general ability of themodel and its performance on previous tasks. Among the array of continual learning meth-ods (Ke and Liu, 2022), data replay stands out asthe most widely adopted strategy in practice dueto its simplicity and efficacy (Wang et al., 2024).Based on it, replay-based distillation methods, in-cluding DER++ (Buzzega et al., 2020) and subse-quent techniques (Qin and Joty, 2021; Kang et al.,2022; Gu et al., 2023), further boost the perfor-mance by utilizing memories from both data andmodel perspectives. Specifically, Buzzega et al.,2020; Qin and Joty, 2021; Gu et al., 2023 distill theoutput logits of old models for knowledge transfer,and Kang et al., 2022 restrict the changes in impor-tant feature maps in the image encoders. However,these works have not fully exploited the potentialof knowledge distillation in continual learning forLLMs. They focus on the outputs of network lay- ers while neglecting the preservation of intricateinternal functions. Consequently, a relatively largeamount of replay data is required by these methodsto achieve good results.Recently, many studies have investigated the at-tention weights of different heads to analyze the in-terpretability of the internal mechanisms in LLMs(Vig and Belinkov, 2019; Wang et al., 2023a).Inspired by this, we explore whether attentionweights play a critical role in knowledge retentionduring continual learning in LLMs. As shown in, grafting the attention weights from theLLM of the old tasks to the final LLM after con-tinual learning can maintain better performance onold tasks, which suggests that the attention weightscould be crucial to alleviate the catastrophic for-getting problem and achieve more comprehensiveknowledge retention1. However, naively preserv-ing the attention weights of all heads in the LLMby distillation introduces significant computationalcosts. Previous studies have observed a functionalspecialization phenomenon among attention headsin LLMs (Vig and Belinkov, 2019; Jo and Myaeng,2020; Li et al., 2023), which indicates the suscep-tibility of attention heads to forgetting and theirimportance to previous tasks vary. This propertyallows us to selectively focus on the valuable atten-tion heads for efficient knowledge retention.To this end, we propose a finer-grained modeldistillation method called SElective attEntion-guided Knowledge Retention (SEEKR) for con-tinual learning of large language models, whichemploys attention distillation on the most valu-able heads in LLMs to achieve efficient knowledgeretention.Specifically, we develop knowledge-retention-oriented head importance measures,which consider both forgettability and task sen-sitivity, to identify the most valuable heads for dis-tillation. The forgettability, measured by the cumu-lative changes in attention weights during continuallearning, indicates the generality of knowledge andthe necessity of distillation. An attention head withhigher forgettability indicates a greater need forknowledge retention. The task sensitivity, calcu-lated as the first-order derivative of the task loss,evaluates the importance of maintaining the atten-tion weights of an attention head for a given task.An attention head with greater sensitivity should 1Attention grafting can only be used during inference withboth the source and target models, which is an infeasiblesolution for continual learning. We employ this techniquesolely for exploratory experiments. be prioritized to restrict variations in its attentionweights. Using the above two importance scores,SEEKR designs a hierarchical budget allocationmechanism to adaptively select the most valuableattention heads for distillation in a controllable way,which can efficiently regulate the training cost. Byusing SEEKR, the performance of old tasks can befurther maintained as shown in .Extensive experiments are conducted on the re-cently developed continual learning benchmark forLLMs (Wang et al., 2023c) and the continual learn-ing benchmark on traditional NLP tasks (Wanget al., 2022a). The results consistently demon-strate the superiority of SEEKR in mitigating catas-trophic forgetting and maintaining the general ca-pabilities of LLMs. Moreover, as a replay-basedmethod, SEEKR exhibits excellent data efficiency,achieving comparable or better performance withjust 1/10 of the replayed data used by the existingmethods, reducing the replayed data proportion toonly 1%.Our main contributions are summarized as fol-lows: We explore and emphasize the importanceof attention weights for knowledge retention,and devise knowledge-retention-oriented mea-sures to identify important attention heads fordistillation. The proposed method, SEEKR,can efficiently preserve the finer-grainedknowledge in the selected attention heads. Extensive experiments validate the superiorityof SEEKR, showcasing its data efficiency byusing just 1% of replay samples to achieve thecomparable or better performance that othermethods reach with 10% of replay samples.",
  "Ltask = E(x,y)Di log p(y|x)(1)": "where x, y are the instruction and true answer, re-spectively. Hereafter, we assume the current task isi and omit the corresponding subscript. In this pa-per, we study a more common scenario in practice where a small amount of data from the old tasks{R1, ..., RN} can be stored in the memory buffer toaid the continual learning process. During trainingon the current task, replay data are acquired fromthe memory buffer, and the model is optimized fortheir previous tasks:",
  "Al,h = softmax(Ql,hKTl,hdk+ Mcausal)(4)": "where Q and K represent the query vectors andthe key vectors in the self-attention operation, re-spectively. Mcausal is the casual attention maskin LLMs. We use t to index the attention distribu-tion of the t-th query in Al,h and denote it as Al,h,t.The attention distributions of query t from each old",
  "t=1DKL(Akl,h,tAl,h,t)": "(5)where U stands for the set of all attention headsin all layers. x y is the concatenated sequenceof x and y, and |x y| means the length of thewhole sequence. In SEEKR, the knowledge distilla-tion is performed at the head level, which can offermore direct and refined regulation on the intricateinternal functions of LLMs, achieving a more com-prehensive and efficient utilization of the limitedreplay data.",
  "Important Head Identification": "In practice, distilling all the attention heads in anLLM is costly and unnecessary, as different headsexhibit varying levels of task sensitivity and forget-tability. Therefore, we propose a two-dimensionalmeasure to identify the most valuable attentionheads for knowledge retention. 3.2.1Task Sensitivity MeasureFor a model adapted to task k, we assess to whichextent changes in the attention weights of each headaffect the task performance. Following commonpractice, we resort to Taylor expansion to formalizethis influence (Kang et al., 2022):",
  "(6)": "where , F and F denote the Frobenius innerproduct and Frobenius norm, respectively. Thisinequality demonstrates the upper bound on theincrease in task loss due to changes in the attentionweights, i.e. Al,h. A larger coefficient indicatesa higher upper bound for the same changes in Al,h.This implies that changes in these attention weightsare more likely to increase task loss or degradetask performance, making it crucial to keep themunchanged. Therefore, we take the coefficient toestimate the sensitivity of the task k to Akl,h, whichis formulated as:",
  "k=1Skl,h(8)": "3.2.2Forgettability MeasureThe second measure assesses the necessity forperforming attention distillation on each attentionhead. We hypothesize that there exist some at-tention heads whose attention weights remain rel-atively stable during continual training on newtasks, suggesting that they are less sensitive totask-specific details and focus more on general orshared knowledge. This hypothesis aligns withprior research (Zhao et al., 2023), which revealedthat only a few modules change drastically duringcontinual learning, while others stay relatively sta-ble and may be shared across tasks as commonknowledge. Based on this, we propose that stableattention heads may encode general knowledge thatis less prone to forgetting, and thus distillation ofsuch heads should be minimized. To this end, weleverage the variability of the attention weights dur-ing continual learning to measure the forgettabilityof the attention head:",
  "hIl,h(11)": "where arg topkz denotes the set of z that achievesthe k largest values. k is BH for H and BL for L.Additionally, to reduce the O(n2) cost of distill-ing the entire attention map, we introduce a querybudget BT and randomly select the queries T fordistillation. After determining H and T, we canrewrite Equation 5 as follows:",
  "k=1E(x,y)RkLad(A, Ak)": "(12)Overall, SEEKR sets three types of budgets toallow flexible control over training costs. First,the layer budget adjusts the number of layers forattention-accelerating algorithms or our distillationstrategy. Second, the head budget filters out lessessential heads and reduces training costs. Lastly,the query budget specifically targets at reducing thecosts associated with distilling long texts.",
  "Experimental Setup": "4.1.1DatasetsCL Benchmark for LLMs. We evaluate ourmethod on TRACE (Wang et al., 2023c), a contin-ual learning benchmark for LLMs that includeseight datasets covering domain-specific knowl-edge, multilingual capabilities, code generation,and mathematical reasoning. We use the reasoning-augmented version of datasets and conduct experi-ments under two task orders following the originalpaper. After continual learning, we assess the per-formance of the continually learned tasks and thechanges in the general ability of LLMs.CL on Traditional NLP Tasks. SuperNI (Wanget al., 2022a) contains a variety of traditional NLPtasks and can serve as a practical benchmark for continual learning of large language models. Simi-lar to Zhao et al., 2024, we select three datasets foreach of the four types of tasks, i.e. information ex-traction, question answering, summarization, andsentiment analysis, to examine the effectiveness ofcontinual learning methods. For each dataset, 1000samples and 100 samples are randomly sampledfor training and testing, respectively.",
  "Baselines": "We compare SEEKR with nine baseline methods:(1) SeqFT sequentially finetunes the model with-out continual learning strategies. (2) EWC (Kirk-patrick et al., 2017) regularizes parameter varia-tions based on parameter importance scores. (3)LwF (Li and Hoiem, 2017) distills the model ofthe last task using the current task data. (4) Re-play finetunes the model with the current taskdata and a small number of replay samples. (5)DER++ (Buzzega et al., 2020) saves the logits ofthe replay samples from the old models for dis-tillation, and combines distillation and replay toreduce forgetting. (6) LFPT5 (Qin and Joty, 2021)learns a soft prompt to generate pseudo samples ofprevious tasks for replaying. (7) O-LoRA (Wanget al., 2023b) imposes orthogonal constraints onthe LoRA matrices for all tasks. (8) L2P (Wanget al., 2022b) instantiates a prompt pool for adap-tive prompt selection and prompt tuning for indi-vidual samples. (9) PP (Razdaibiedina et al., 2023)tunes a set of prompts for each task and concate-nates them together. In addition, the results of themulti-task trained models are reported as MTL andserve as the upper-bound reference.",
  "Implementation Details": "SEEKR is a versatile continual learning methodcompatible with any transformer-based model. Fol-lowing Wang et al., 2023c, we conduct our mainexperiments on two popular LLMs, i.e. LLaMA-2-7B-chat (Touvron et al., 2023) and Vicuna-7B-v1.5 (Zheng et al., 2024).We also scale to alarger model Vicuna-13B-v1.5 to validate the ef-fectiveness of SEEKR. All models are trained on 8NVIDIA Tesla A800 using the DeepSpeed library.The training batch size is 128. For methods not",
  ": Comparison with the state-of-the-art methodson SuperNI benchmark. The experiments are conductedon LLaMA-2-7B": "involving parameter-efficient tuning modules, thelearning rate is 1e-5. For replay-based methods,the default replay ratio is 1%. For SEEKR, 1 inEquation 13 is set to 0.5. 2 is 1e3 for a replayratio of 1% and 1e2 for 10%. The head budget BHis 128, and the layer budget BL is 24 by defaultand 8 for 13B models or a replay ratio of 10%. Thequery budget BT is 100. All experimental resultswere averaged over 3 runs. More implementationdetails can be found in Appendix B.",
  "Main Results": "compares the overall continual learningperformance of SEEKR with other baselines onTRACE benchmark. Following Wang et al., 2023c,we also report the changes in the general abilityof LLMs after continual learning in . Sim-ilar experiments on the SuperNI benchmark aredisplayed in .SEEKR effectively mitigates catastrophic for-getting of continually learned tasks. Comparedto traditional and state-of-the-art continual learn-ing approaches, SEEKR consistently achieves the",
  ": Results of SEEKR across different distillation budgets and different replay data ratios": "highest OP and the lowest magnitude of BWT inall settings. Note that the BWT metric specificallycaptures the resistance of methods to catastrophicforgetting, thus the results demonstrate SEEKRssuperiority in maintaining performance on newlylearned tasks. Additionally, on the SuperNI bench-mark, we achieve the best performance using only asmall proportion of replay samples, likely becausethe benchmark consists of traditional NLP tasks,which are less challenging. SEEKR fully exploits the small amount of re-play data and exhibits excellent data efficiency.Among all replay-based methods, SEEKR standsout with a distinct advantage. On the TRACEbenchmark, both Replay and DER++ show lim-ited benefits with a lower ratio of replay data. Incontrast, SEEKR demonstrates remarkable per-formance with just 1% of the samples replayed,achieving comparable or even better results thanother methods that replay 10% of the samples. Thisunderscores the ability of SEEKR to maximize theuse of a small number of old samples and the inher-ent knowledge in the old models. SEEKR is effective in maintaining the generalability of the original LLM. exhibits thechanges in LLMs general ability after continuallearning. LLMs that are continually trained on newtasks show a decline in general task performance,demonstrating the catastrophic forgetting of theiroriginal capabilities. Results validated that SEEKR,which elaborately distills multiple finetuned LLMswith a variety of data, helps to maintain the generalcapabilities of the model. This could benefit fromthe fact that our approach preserves the knowledgeof the intricate internal functions in LLMs at theattention head level.",
  "Ablation Studies": "Effect of distillation budget. (a) ex-hibits the performance of our method under dif-ferent budgets. With a fixed layer budget of 24, alarger head budget can lead to better results, butthis improvement tends to plateau at a budget of128. Similarly, the performance improves with anincreasing layer budget and reaches its optimumat 24. These results further emphasize the signif-icance of distilling the right attention heads. Dis-tilling less essential attention heads may lead toineffective work.Effect of more replay samples. To further ex-plore the potential of SEEKR, we experiment withan increased ratio of replay samples. Meanwhile,we compare SEEKR with Replay to demonstrate itsdata efficiency. As shown in (b), SEEKRsteadily improves performance as the number ofreplay samples grows. At a replay ratio of 10%, the 0.00.10.20.30.40.50.60.70.80.91.0 Cumulative Variation in Attention weights (min-max normalized) Number of heads",
  ": Visualization of the importance scores of allheads in the model": "BWT score exceeds 0, indicating no forgetting oreven a positive transfer has been achieved, and theoverall performance approximates the upper boundof multi-task training. Moreover, compared withReplay, SEEKR is very data efficient by utilizingonly 1% of the old data to achieve the performanceof replaying ten times that amount.Effectiveness of our head importance mea-sure. We present the results of the ablation studyon the proposed head importance measure in Ta-ble 4 . The results show that the random selectionof distilled attention heads noticeably resulted ina higher forgetting indicator, while using eithersensitivity-based or variation-based measures helpsidentify important heads for knowledge retention.Finally, combining both of the above measures pro-duces the best results.",
  "Scale to larger models. To validate the general-izability of SEEKR across different model scales,we conducted additional experiments on a largermodel, Vicuna-13B-v1.5. shows that our": "approach still effectively preserves both the per-formance of newly learned tasks and the generalcapabilities of the original model.Variation in attention weights. To further con-firm our hypothesis in .2.2, we examinethe cumulative changes in attention weights of eachattention head during sequential finetuning. Theresults in reveal that most attention headsremain stable throughout the process, while a smallproportion undergo significant changes. This ob-servation is similar to prior findings (Zhao et al.,2023) and supports our hypothesis that these sta-ble attention heads do exist, making it reasonableto identify them and avoid unnecessary attentiondistillation.Analysis of selected important heads. illustrates that important attention heads are mainlydistributed in the middle and deep layers of themodel, while almost none are observed in the shal-low layers. This aligns with the idea that the shal-low layers encode more generalized knowledgeand are less susceptible to forgetting. A closerlook at further reveals that the importancescores for the deeper layers are concentrated in afew heads, while those for the middle layers aremore evenly spread over a larger number of heads.This may be because the heads in the deeper layersare more thoroughly function-specialized.",
  "Continual Learning for LLMs": "Existing continual learning methods are typ-ically classified into three broad categories:regularization-based methods, replay-based meth-ods,and architectural-based methods.(1)Regularization-based methods restrict modelvariations to alleviate forgetting. Some works pe-nalize changes to important parameters for previ-ously learned tasks (Kirkpatrick et al., 2017; Wanget al., 2023b; He et al., 2023), while others resort toknowledge distillation to maintain the old modelspredictions (Li and Hoiem, 2017; Buzzega et al.,2020; Kang et al., 2022). (2) Replay-based meth-ods replay data from the old tasks during trainingon the new task. Experience replay methods (Re-buffi et al., 2017; Wang et al., 2024) design dataselection strategies of previous samples, and gener-ative replay (Shin et al., 2017; Qin and Joty, 2021)uses generative models to produce synthetic datafrom previous tasks. Other methods (Yang et al.,2023) retain old tasks by storing statistical infor- mation of the old tasks instead of the original data.(3) Architecture-based methods alter the modelstructure to accommodate different tasks. Recently,this type of methods on LLMs (Wang et al., 2022b;Razdaibiedina et al., 2023) often add parameter-efficient tuning modules for new tasks. SEEKR falls into the category of replay-baseddistillation methods and focuses on the preservationof important attention mechanisms in LLMs. Un-like existing output or parameter importance mea-sures (Kirkpatrick et al., 2017; Kang et al., 2022),which focus solely on task loss sensitivity, our headimportance measure includes a forgettability aspect.This reflects the susceptibility to forgetting and thegenerality of knowledge in different heads, therebydetermining the necessity for distillation.",
  "Knowledge Distillation": "Knowledge distillation aims to leverage the teachermodels performance and generalize it to the stu-dent model (Hinton et al., 2015; Park et al., 2019;Guo et al., 2023). For language models, Sanh et al.,2019 uses the teacher models generation distri-bution for each token as a supervision signal forthe student model, and some other works (Wanget al., 2020b,a) distill the attention scores of onelayer to transfer the knowledge of larger LMs intosmaller models. Unlike their objectives of transfer-ring knowledge between models of different sizes,we use attention distillation for knowledge reten-tion. Both our teacher and student models share asimilar architecture and are derived from the samepre-trained LLM, which enables head-by-head andlayer-by-layer distillation.",
  "Conclusion": "In this paper, we propose SEEKR, an efficientreplay-based distillation method for continual learn-ing in LLMs. SEEKR resorts to attention distilla-tion of important heads for finer-grained knowledgeretention, which identifies valuable heads throughthe proposed knowledge-retention-oriented impor-tance measures. Combined with a hierarchical bud-get allocation mechanism, SEEKR can ensure itsutility across various resource levels. Extensive ex-periments consistently validated the effectivenessof our method in preserving the performance ofnewly learned tasks and the original ability of theinitial LLMs.",
  "Limitations": "Despite the potential benefits of SEEKR, severallimitations need to be considered. First, SEEKR isinherently a replay-based approach, which may notbe applicable in scenarios where historical data in-volves privacy concerns. A potential solution is touse SEEKR with pseudo-samples generated by thetrained LLM, but this approach requires further ex-ploration. Second, due to computational resourcelimitations, we did not experiment with larger-scaleLLMs like LLaMA-2-70B. Additionally, the appli-cation of SEEKR to continual learning with mul-timodal large language models remains to be ex-plored in the future.",
  "Acknowledgements": "ThisworkwassupportedbyNationalKey R&D Program of China under GrantNo.2021ZD0110400, also partly supported byBeijing Natural Science Foundation under Grant4244099, National Natural Science Foundationof China under Grant No.62276260, Postdoc-toral Fellowship Program of CPSF under GrantGZC20232996,ChinaPostdoctoralScienceFoundation under Gant 2024M753498. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,et al. 2020. Piqa: Reasoning about physical com-monsense in natural language. In Proceedings of theAAAI conference on artificial intelligence, volume 34,pages 74327439. Pietro Buzzega, Matteo Boschini, Angelo Porrello, Da-vide Abati, and Simone Calderara. 2020. Dark expe-rience for general continual learning: a strong, simplebaseline. Advances in neural information processingsystems, 33:1592015930. Arslan Chaudhry, Puneet K Dokania, ThalaiyasingamAjanthan, and Philip HS Torr. 2018. Riemannianwalk for incremental learning: Understanding forget-ting and intransigence. In Proceedings of the Euro-pean conference on computer vision (ECCV), pages532547. Christopher Clark, Kenton Lee, Ming-Wei Chang,Tom Kwiatkowski, Michael Collins, and KristinaToutanova. 2019. Boolq: Exploring the surprisingdifficulty of natural yes/no questions. arXiv preprintarXiv:1905.10044. Jonathan H Clark, Eunsol Choi, Michael Collins, DanGarrette, Tom Kwiatkowski, Vitaly Nikolaev, andJennimaria Palomaki. 2020. Tydi qa: A benchmarkfor information-seeking question answering in ty po-logically di verse languages. Transactions of the As-sociation for Computational Linguistics, 8:454470. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, ReiichiroNakano, et al. 2021. Training verifiers to solve mathword problems. arXiv preprint arXiv:2110.14168. Ahmad Ghazal, Tilmann Rabl, Minqing Hu, FrancoisRaab, Meikel Poess, Alain Crolotte, and Hans-ArnoJacobsen. 2013. Bigbench: Towards an industry stan-dard benchmark for big data analytics. In Proceed-ings of the 2013 ACM SIGMOD international confer-ence on Management of data, pages 11971208. Qiao Gu, Dongsub Shim, and Florian Shkurti. 2023.Preserving linear separability in continual learningby backward feature projection. In Proceedings ofthe IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 2428624295.",
  "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.Distilling the knowledge in a neural network. arXivpreprint arXiv:1503.02531": "Jae-young Jo and Sung-Hyon Myaeng. 2020. Roles andutilization of attention heads in transformer-basedneural language models. In Proceedings of the 58thAnnual Meeting of the Association for ComputationalLinguistics, pages 34043417. Minsoo Kang, Jaeyoo Park, and Bohyung Han. 2022.Class-incremental learning by knowledge distillationwith adaptive feature consolidation. In Proceedingsof the IEEE/CVF conference on computer vision andpattern recognition, pages 1607116080.",
  "Zixuan Ke and Bing Liu. 2022. Continual learning ofnatural language processing tasks: A survey. arXivpreprint arXiv:2211.12701": "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,Joel Veness, Guillaume Desjardins, Andrei A Rusu,Kieran Milan, John Quan, Tiago Ramalho, Ag-nieszka Grabska-Barwinska, et al. 2017.Over-coming catastrophic forgetting in neural networks.Proceedings of the national academy of sciences,114(13):35213526. Chong Li, Shaonan Wang, Yunhao Zhang, Jiajun Zhang,and Chengqing Zong. 2023. Interpreting and ex-ploiting functional specialization in multi-head at-tention under multi-task learning. arXiv preprintarXiv:2310.10318.",
  "Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong,and Furu Wei. 2020a.Minilmv2:Multi-headself-attention relation distillation for compress-ing pretrained transformers.arXiv preprintarXiv:2012.15828": "Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, NanYang, and Ming Zhou. 2020b. Minilm: Deep self-attention distillation for task-agnostic compressionof pre-trained transformers. Advances in Neural In-formation Processing Systems, 33:57765788. Xiao Wang, Tianze Chen, Qiming Ge, Han Xia, RongBao, Rui Zheng, Qi Zhang, Tao Gui, and XuanjingHuang. 2023b. Orthogonal subspace learning forlanguage model continual learning. arXiv preprintarXiv:2310.14152. Xiao Wang, Yuansen Zhang, Tianze Chen, SongyangGao, Senjie Jin, Xianjun Yang, Zhiheng Xi, RuiZheng, Yicheng Zou, Tao Gui, et al. 2023c. Trace:A comprehensive benchmark for continual learn-ing in large language models.arXiv preprintarXiv:2310.06762. Yifan Wang, Yafei Liu, Chufan Shi, Haoling Li, ChenChen, Haonan Lu, and Yujiu Yang. 2024.In-scl: A data-efficient continual learning paradigm forfine-tuning large language models with instructions.arXiv preprint arXiv:2403.11435. Yizhong Wang, Swaroop Mishra, Pegah Alipoor-molabashi,Yeganeh Kordi,Amirreza Mirzaei,Anjana Arunkumar, Arjun Ashok, Arut SelvanDhanasekaran, Atharva Naik, David Stap, et al.2022a. Super-naturalinstructions: Generalization viadeclarative instructions on 1600+ nlp tasks. arXivpreprint arXiv:2204.07705. Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang,Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot,Jennifer Dy, and Tomas Pfister. 2022b. Learning toprompt for continual learning. In Proceedings ofthe IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 139149. Yang Yang, Zhiying Cui, Junjie Xu, Changhong Zhong,Wei-Shi Zheng, and Ruixuan Wang. 2023. Continuallearning with bayesian model based on a fixed pre-trained feature extractor. Visual Intelligence, 1(1):5. Haiyan Zhao, Tianyi Zhou, Guodong Long, Jing Jiang,and Chengqi Zhang. 2023. Does continual learningequally forget all parameters? In International Con-ference on Machine Learning, pages 4228042303.PMLR. Weixiang Zhao, Shilong Wang, Yulin Hu, Yanyan Zhao,Bing Qin, Xuanyu Zhang, Qing Yang, DongliangXu, and Wanxiang Che. 2024.Dapt: A dual at-tention framework for parameter-efficient continuallearning of large language models. arXiv preprintarXiv:2401.08295.",
  "FortheTRACEbenchmark(Wangetal.,": "2023c), we conduct experiments on the reasoning-augmented datasets as such high-quality train-ing data is more suitable for the LLM learningparadigm. The task order is consistent with thetwo orders provided by the benchmark, which arealso displayed in . For evaluation on thechanges in the general ability, we test the LLMs onthe datasets (Hendrycks et al., 2020; Ghazal et al.,2013; Clark et al., 2020; Bisk et al., 2020; Clarket al., 2019; Cobbe et al., 2021) included in thisbenchmark.For the SuperNI benchmark (Wang et al., 2022a),we choose four types of tasks and three dataseteach for continual learning, containing a total of 12traditional NLP tasks similar to Zhao et al., 2024.The two task orders can be found in .",
  "BImplementation Details": "For methods not involving parameter-efficient tun-ing (PET) modules, we finetuning the LLMs on thetask sequence in order1 for 5, 5, 5, 5, 5, 5, 10, 5epochs, order2 for 10, 10, 10, 5, 5, 5, 5, 5 epochs,and order3 and order4 for 10 epochs each. For thecompared baseline methods involving PET mod-ules, the training epochs vary from 5 to 15 epochsfor better performance. The hyperparameters ofthe compared baseline methods were kept the sameas in the original repositories. If they did not per-form well, we conducted additional searches forthe optimal learning rate.For all the replay-based methods, we randomlyselected the indicated proportion of replay sam-ples from the full training set and kept the replaysamples utilized by each method consistent for fair-ness. For the replay-based distillation methods, thedistillation signals, i.e. output logits and attentionweights, of each old teacher model are saved in thememory buffer along with the original replay sam-ples and loaded from the buffer during training onthe new task. When replaying the old data, samplesfrom the memory buffer and the current task aresampled in an evenly interleaved manner accordingto the ratio of their volumes."
}