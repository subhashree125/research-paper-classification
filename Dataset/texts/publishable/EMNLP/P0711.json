{
  "Abstract": "Retrieving accurate domain knowledge and pro-viding helpful information are crucial in devel-oping an effective end-to-end task-oriented dia-logue system (E2ETOD). Existing approachesto this field follow a retrieve-then-generateparadigm and train their systems on one spe-cific domain. However, existing approachesstill suffer from the Distractive AttributesProblem (DAP): struggling to deal with falsebut similar knowledge (a.k.a hard negative en-tities), which is even more intractable whencountless pieces of knowledge from differentdomains are blended in a real-world scenario.To alleviate DAP, we propose the Relevance-aware Adaptive Learning (ReAL), a novel two-stage training framework that eliminates hardnegatives step-by-step and aligns retrieval withgeneration. In the first stage, we introducea top-k adaptive contrastive loss and utilizethe divergence-driven feedback from the frozengenerator to pre-train the retriever. In the sec-ond stage, we propose using the metric scoredistribution as an anchor to align retrieval withgeneration. Thorough experiments on threebenchmark datasets demonstrate ReALs supe-riority over existing methods, with extensiveanalysis validating its strong capabilities ofovercoming in- and cross-domain distractions.",
  "Introduction": "The advent of an intelligent era has seen rapidadvancement of task-oriented dialogue systems(TOD), demonstrating their potential and flexibil-ity across various practical applications like healthconsulting, financial services, and home automa-tion. Though conventional pipeline TOD systemsmodel the task with interrelated modules and haveachieved remarkable results, several limitations ex-ist, such as error propagation (Qin et al., 2023),increased complexity (Goyal et al., 2022; Xin et al.,2021), and maintenance challenges (Qin et al.,",
  "Top-K Retrieved Entities": ": An example from MultiWOZ 2.1 (Eric et al.,2019). The top-k retrieved entities are severely dis-tracted by in- and cross-domain similar but false entities,leading to inaccurate retrieval and retrieval-generationmisalignment. 2023), making the development and deployment ofsuch systems resource-intensive and less scalable.Hence, end-to-end task-oriented dialogue systems(E2ETOD) have aroused increasing research at-tention for their ability to retrieve from externalknowledge bases (KB) to generate the responseend-to-end without any intermediate annotations.Retrieving domain knowledge precisely is a cru-cial part of E2ETOD (Wan et al., 2023; Shi et al.,2023a; Shen et al., 2023; Qin et al., 2019). Sinceknowledge bases are growing daily, old-fashionedmethods combining knowledge retrieval and re-sponse generation into one single model (Madottoet al., 2018; Qin et al., 2020) are facing the issue ofKB scalability. Drawing inspiration from retrieval-augmented generation (RAG) (Singh et al., 2021;Lewis et al., 2020; Guu et al., 2020), Q-TOD (Tianet al., 2022) employs a held-out retriever and decou-ples knowledge retrieval from response generation,which gets rid of the unscalable problem. Follow-ing the technical route, MAKER (Wan et al., 2023) introduces a multi-grained knowledge retriever forlarge-scale cross-domain KBs. Shi et al. (2023a)propose a dual-feedback mechanism to provide theretriever with stronger supervision signals. Bene-fiting from the dual-encoder retriever architecture,these methods can train the retriever end-to-endand match semantics efficiently.Despite these encouraging developments, theparadigm of decoupling knowledge retrieval fromresponse generation can lead to Distractive At-tribute Problem (DAP) in task-oriented dialoguesystems: (I) Inaccurate retrieval: E2ETOD sys-tems suffer from in- and cross-domain distrac-tions while retrieving top-k entities from large-scale knowledge bases (Wan et al., 2023; Shi et al.,2023a). (II) Retrieval-generation misalignment:the top-k retrieved knowledge entities are similarin attributes and easily lead to retrieval-generationmisalignment (Shen et al., 2023). (III) Ambigu-ous retriever pre-training: when determiningthe weakly labeled data, the common pre-trainingmethod (Qin et al., 2019) selects the entities withthe most attribute value occurrences in the dialoguecontext and system response as pseudo-positive ex-amples. However, these entities are possibly notthe ground truth and lead to ambiguous retrieverpre-training. As illustrated in , the groundtruth entity is distracted by in- and cross-domainhard negatives during retrieval and generation.In this paper, we propose a two-stage Relevance-aware Adaptive Learning (ReAL) framework to ad-dress three aspects of DAP jointly. In the first stage,we introduce a top-k adaptive contrastive loss andutilize the divergence-driven feedback from thefrozen generator to mitigate ambiguity when pre-training the retriever. Instead of picking the entitywith the most attribute value occurrences as the solepseudo-positive, we construct top-k context-entitypairs and estimate the matching degree betweenthe pairs. These relevance weights are assignedto different context-entity pairs to promote adap-tive contrastive learning. In addition, to furtheravoid retrieving hard negatives, we measure therelevance between the entity and context with thegeneration probability of the frozen generator. In-tuitively, the more relevant the entity and contextare, the greater the generation probability. Thus,we utilize the KL divergence between the retrievaland relevance likelihood as the divergence-drivenfeedback to facilitate more precise retrieval. In thesecond stage, we propose using the metric scoredistribution as an anchor to align retrieval with gen- eration. We significantly reduce the gap betweenretrieval and generation by minimizing the KL di-vergence between the metric score distribution andthe likelihood of retrieving entities.Extensive experiments on multiple KB-attachedtask-oriented dialogue datasets demonstrate thegreat performance of ReAL in achieving accurateretrieval and fluent generation (Eric et al., 2019;Eric and Manning, 2017; Wen et al., 2016). Tofurther illustrate the effectiveness of our adaptiveretriever pre-training method, we replace the re-trievers in previous methods with ours and showuniversal effectiveness.In a nutshell, the main contributions of this pa-per are three-fold: (I) We propose a two-stagerelevance-aware adaptive learning framework forE2ETOD to achieve accurate retrieval and gen-eration.(II) We refine the pre-training of theretriever with an adaptive learning method, pre-venting mismatches caused by hard negative pairs.(III) Extensive experiments on three benchmarkdatasets demonstrate the effectiveness of the pro-posed ReAL.",
  "End-to-end Task-oriented Dialogue": "From the perspective of knowledge embedding, re-cent years have witnessed remarkable developmentin E2ETOD systems. Drawing inspiration frompointer networks (Vinyals et al., 2015), Madottoet al. (2018), Qin et al. (2020), and Raghu et al.(2021) use memory networks to store knowledgeexplicitly and combine multi-hop attention overthe memories to acquire relevant information. Incontrast, Huang et al. (2022) and Ding et al. (2024)propose autoregressive entity generation to retrieveknowledge implicitly.Ever since pre-trained language models (PLMs)take the lead in NLP tasks, the knowledge enti-ties are linearized to be encoded by the PLM en-coder (Xie et al., 2022; Tian et al., 2022), whichare subsequent input for response generation. Di-alogKG (Rony et al., 2022) uses a graph neu-ral network to select entities from the flattenedrecords. MAKER (Wan et al., 2023) introducesmulti-grained retrieval, involving both entity andattribute selection. As mentioned earlier, althoughthe retrieve-then-generate framework has been suc-cessful, its paradigm of decoupling knowledge re-trieval from response generation can lead to DAPin task-oriented dialogue systems. While previous researches (Shi et al., 2023a; Wan et al., 2023; Shenet al., 2023) only consider one part of DAP (eitherthe retrieval distractions or retrieval-generation mis-alignment), we solve the problem jointly with ourproposed framework.",
  "Knowledge Retriever": "Enhancing language models with pertinent infor-mation from diverse knowledge sources has proveneffective in improving performance across vari-ous NLP tasks (Khandelwal et al., 2019; Borgeaudet al., 2022; Lewis et al., 2020; Xie et al., 2024). Asone of the most successful retrieval structures, thedual-encoder architecture (Yih et al., 2011) encodesqueries and passages separately. The relevance be-tween a query-passage pair is computed throughinner product or Euclidean distance. Based on this,DPR (Karpukhin et al., 2020) trains the retrieverwith in-batch documents and samples negative ex-amples for contrastive learning, enabling the pre-trained retriever to perform well in open-domainquestion answering. REALM (Guu et al., 2020)and RAG (Lewis et al., 2020) consider the retrievedpassages as latent variables and train the retriever-generator system jointly. Unlike prior retrieval-augmented language models, REPLUG (Shi et al.,2023b) simply treats the frozen LM as a black-boxmodel and augments it with a tuneable retriever.Lei et al. (2023) reduce the false positive problemby pre-training the dense retriever with contrastivelearning. Cheng et al. (2024) utilize an unboundedmemory pool and employ a memory selector tochoose a single output as the memory for the nextgeneration round.",
  "Preliminaries": "Given a dialog D = {u1, r1, , uT , rT } consist-ing of T turns, we denote the dialogue contextof the t-th turn as Ct, which encompasses all pre-ceding user utterances and system responses upto that turn, i.e., Ct = {u1, r1, , ut1, rt1, ut}.ut and rt denote the user utterance and systemresponse of t-th turn, respectively. To adapt todifferent domain-specific problems, an externalknowledge base K = {e1, e2, , eB} is providedas a set of entities.Here, each entity ei com-prises N attribute-value pairs, denoted as ei ={a1, vi,1, , aN, vi,N}. End-to-end task-orienteddialogue systems leverage the dialogue context Cand knowledge base K as input to generate re-",
  "Dual-encoder Knowledge Retriever": "From the dialogue context Ct and knowledge baseK, the retriever aims to retrieve a small set ofknowledge entities from K, which are relevant toCt. To ensure simplicity and effectiveness, we fol-low the dual-encoder architecture widely used inopen-domain QA and E2ETOD (Singh et al., 2021;Wan et al., 2023; Shen et al., 2023). By concatenat-ing the user utterances and system responses, thedialogue context is encoded as the query, while theattribute-value pair of the i-th knowledge entity isconcatenated and encoded as the external knowl-edge. The similarity between the encoded Ct andei is calculated as follows,",
  "Adaptive Retriever Pre-training": "To learn the representations of the dialogue con-text and the knowledge entities, we employ a pre-trained language model (PLM) to extract the fi-nal [CLS] token to represent them. However, thePLMs initialized with their pre-trained weightsshow a bad retrieval performance, which may re-sult in the \"collapsed representations\" mentionedin previous studies (Shi et al., 2023a; Wan et al.,2023). Consequently, we propose a top-K adaptivecontrastive loss and utilize the divergence-drivenfeedback from the frozen PLM generator () topre-train the retriever. Top-K Adaptive Contrastive LearningSincethe knowledge base does not annotate the gold re-trieved entities, Qin et al. (2019) utilize distantsupervision and design a set of heuristics to extracttraining data for the retriever. When determiningthe weakly labeled data, the vanilla method selectsthe entity with the most attribute value occurrencesin the dialogue context and system response aspseudo-positive examples. For a positive context-entity pair (Ct, e+) of the t-th turn, the vanilla con-trastive loss is computed by:",
  "Divergence": ": The framework of our proposed ReAL. During the adaptive pre-training stage, the retriever is guided bythe top-k contrastive loss and divergence-driven supervised feedback to eliminate distractions step-by-step. Thesubsequent end-to-end fine-tuning stage aligns retrieval with generation through the metric-driven KL divergence.The generator parameters are frozen during the former stage and tuned during the latter one. good retriever can identify the close relationshipbetween the corresponding dialogue context andthe labeled entity in the representation space. How-ever, the entity with the most occurrences of itsattributes is not necessarily the ground truth en-tity. The vanilla contrastive learning method maymislead the model to pull similar but unrelated en-tities toward the context in the embedding spaceand further harm the validity of representations.Therefore, we propose a simple yet effectiveTop-K adaptive contrastive loss. We construct top-K context-entity pairs and estimate the matchingdegree between the pairs, conducted using the inter-mediate trained retriever . According to the esti-mated relevance, we assign the weights to differentpairs adaptively and improve the vanilla contrastiveloss as follows,",
  "q(ei | K, Ct; ) =exp (s (Ct, ei) /)Kj=1 exp (s (Ct, ej) /), (4)": "where is a temperature hyperparameter. Since itis impractical to marginalize over all the entitiesin the knowledge base, we only consider the re-trieved top-K entities for estimation. In addition,the hard negative entities always show up with thetrue positives in the top-K candidates. Therefore,the estimation over the retrieved K would lay moreemphasis on this specific problem. Subsequently,we utilize the generation probability of the frozengenerator to measure the relevance between theentity and the dialogue context, which has a clearintuition that the more relevant the entity and con-text are, the greater the generation probability. Therelevance score is computed as follows,",
  "Lnll = log p(rt | Ht; ).(10)": "However, as stated in Sec. 1, training the generatoronly with the NLL loss still results in a misalign-ment between generation and retrieval. Drawinginspiration from Cheng et al. (2024), we proposeutilizing the BLEU score distribution as an anchorto align the retrieval and generation. To be specific,we define the BLEU score distribution as follows,",
  "Datasets and Evaluation Metrics": "We conduct experiments on three KB-attachedtask-oriented dialogue datasets: MultiWOZ 2.1(MultiWOZ) (Eric et al., 2019), Stanford Multi-Domain (SMD) (Eric and Manning, 2017), andCamRest (Wen et al., 2016). The knowledge basesare condensed with all the entities that meet theuser goal of the current dialogue. The statistics ofthe benchmark datasets are listed in . Eachdialogue in these datasets is linked to a condensedknowledge base containing all entities that meet theusers goal for that dialogue. For MultiWOZ, eachcondensed knowledge base includes 7 entities. ForSMD and CamRest, the size of these knowledgebases varies: from 0 to 8 entities with an averageof 5.95 for SMD, and from 0 to 57 entities with anaverage of 1.93 for CamRest.Following previous work (Tian et al., 2022; Xieet al., 2022; Wu et al., 2022), we adopt BLEU (Pap-ineni et al., 2002) and Entity F1 (Eric and Manning,2017) as our primary evaluation metrics. BLEU as-sesses the fluency of a generated response by mea-suring its n-gram overlap with a reference response,while Entity F1 evaluates the accuracy of embed-ded knowledge by micro-averaging precision andrecall scores of attribute values in the generatedresponse.",
  "Implementation Details": "We employ BERT (Devlin et al., 2018) as the en-coder of our entity selector and attribute selectorand employ T5 (Raffel et al., 2020) to implementthe response generator. Our model is trained usingAdamW optimizer (Loshchilov and Hutter, 2017)with a batch size of 64. We conduct all experimentson 4 24G NVIDIA RTX 3090 GPUs and selectthe best checkpoint based on model performance",
  "Ours18.6456.8427.6575.53": ": Performance on two large-scale benchmarkdatasets. The best scores are in bold and the second-bestones are underlined. denotes our model significantlyoutperforms baselines with p < 0.05 under t-test. on the validation set. All experiments results areobtained by averaging the scores over five runswith different random seeds. Following Wan et al.(2023), we pre-train the retriever on the MultiWOZand CamRest datasets. Due to the SMD datasetsknowledge base being specific to each dialogue, itis not possible to compile a global knowledge basefrom the dialogues. Consequently, pre-training is",
  "Comparative Baselines": "We perform a comprehensive comparative studyagainst ReAL by considering the baselines withdifferent retrieval strategies. FG2Seq (He et al.,2020), CDNET (Raghu et al., 2021), GraphMem-Dialog (Wu et al., 2022), ECO (Huang et al., 2022),and Uni-TOD (Ding et al., 2024) integrate knowl-edge retrieval and response generation in one singlemodel. In contrast, DigloKG (Rony et al., 2022),UnifiedSKG (Xie et al., 2022), Q-TOD (Tianet al., 2022), DF-TOD (Shi et al., 2023a), MK-TOD (Shen et al., 2023), and MAKER (Wan et al.,2023) decouple the retrieval and generation pro-cess.",
  "BLEUEntity F1BLEUEntity F1": "ReAL18.1155.1317.8354.43w/o Ladapt17.97 ( 0.14)54.04 ( 1.09)17.72 ( 0.11)52.89 ( 1.54)w/o Ldiv17.24 ( 0.87)54.34 ( 0.79)17.16 ( 0.67)53.23 ( 1.20)w/o Lpre16.22 ( 1.89)53.17 ( 1.96)16.59 ( 1.24)51.38 ( 3.05)w/o Lalign17.04 ( 1.07)54.10 ( 1.03)16.87 ( 0.96)53.29 ( 1.14) : Results of ablation study on MultiWOZ under the condensed and large-scale setting with T5-base, where\"w/o\" means without. When ablating Ladapt and Lpre, we substitute Ladapt with the vanilla contrastive loss Linfo.",
  "Ours17.5153.2692.44MAKER17.1849.0586.47Vanilla Linfo16.6748.7782.71Frequency16.6048.0075.94BM2516.2145.5626.32": ": Comparison of different retrievers under thelarge-scale setting of MultiWOZ. Oracle means directlyusing the condensed knowledge base. Vanilla Linfo de-notes using the vanilla contrastive pre-trained retriever. indicates the results cited from Wan et al. (2023). 2.54 points in Entity F1. On SMD, the improve-ments are 0.42 in BLEU and 1.09 in Entity F1.However, the best Entity F1 result on CamRestis kept by the strong baseline Uni-TOD, thoughits results on MultiWOZ are comparatively muchweaker. It is worth noting that each condensedknowledge base for CamRest only contains 1.93entities on average (Wan et al., 2023), which isbeneficial to autoregressive models like DialoKGand Uni-TOD. When the number comes to 7 forMultiWOZ, our model takes the lead naturally.Through our proposed effective relevance-awareadaptive learning method, our model with the T5-Base backbone even exceeds the ones with theT5-Large generator. On MultiWOZ and CamRest,ReAL shows superiority in Entity F1 and leads bya big margin, which is attributed to the refined top-k adaptive contrastive loss and divergence-drivenfeedback. Large-scale Knowledge BaseThough the re-sults in are outstanding, the real-world task-oriented dialogue systems face a more intractablesituation in which countless pieces of knowledgefrom different domains are blended without orders.Thus, we follow previous researches (Wan et al.,2023; Shi et al., 2023a) to conduct experiments un-der a large-scale setting. As shown in , we gather the entities of all dialogs in MultiWOZ andCamRest respectively for evaluation. On one hand,the existing approaches experience significant per-formance degradation without exception when uti-lizing large-scale knowledge bases. The Entity F1score of MAKER and DF-TOD drop 2.81/2.6 and0.91/0.21 points on MultiWOZ, respectively. Incontrast, our method shows much greater stabilitywith only a decline of 0.7/0.42 points while keep-ing a competitive performance. On the other hand,our ReAL system demonstrates superior perfor-mance compared to all baselines, despite a minordeficit in BLEU on CamRest. Notably, our sys-tem, even with large-scale knowledge bases, con-sistently outperforms other systems that rely oncondensed knowledge bases, which are easier toretrieve. These findings highlight the exceptionalcapability of our system in handling large-scaleknowledge bases and its practicality for real-worldapplications.",
  "Ablation Study": "To better understand how each part of ReAL affectsits performance, we conduct an ablation study onMultiWOZ under the condensed and large-scalesetting with T5-base backbone, and the results areshown in . When substituting the top-kadaptive contrastive loss Ladapt with the vanillaLinfo, the performance of our model drops notice-ably. This drop is more severe under the large-scalesetting, with a reduction of 0.11 in BLEU and 1.54in Entity F1 scores, compared to the condensedsetting, where the reductions are 0.14 in BLEU and1.09 in Entity F1. The larger drop in the large-scalesetting suggests that the poorly-trained retrieversuffers significantly from DAP without adaptivepre-training. Removing the divergence-driven su-pervised feedback also causes a performance reduc-tion, though it is less severe than removing Ladapt.However, the ablation of Lpre demonstrates thesignificant coordination between the two losses.",
  ": Performance of applying our adaptive pre-trained retriever to advanced models under the large-scalebenchmark setting. denotes our re-implementation": "Abandoning Lpre results in the largest performancereduction, with decreases of 1.89 in BLEU and1.96 in Entity F1 in the condensed setting, and 1.24in BLEU and 3.05 in Entity F1 in the large-scalesetting. This substantial drop certifies the effective-ness of eliminating hard negatives step-by-step andhighlights the critical role of the proposed adaptivepre-training in maintaining high performance. Inaddition, the ablation results of the metric-drivenloss Lalign show its validity in aligning retrievaland generation. Therefore, the proposed methodReAL is effective as each of its components con-tributes to its overall performance, and removingany of these components leads to a noticeable de-cline in both metrics.",
  "Comparison of Retrievers": "To further validate the effectiveness of our ReALretriever, we compare the performance of differentretrievers under the large-scale setting of Multi-WOZ. To ensure a fair comparison, we follow theprocedure of Wan et al. (2023) and employ thegenerator with the same backbone. Since the de-tails are not mentioned, we utilize T5-Base as thebackbone for response generation due to its similarperformance. The results in demonstratethat our method achieves the highest BLEU andEntity F1 scores among all methods, indicatingsuperior precision in text generation and entity re-trieval. Though directly acquiring all entities, theoracle method fails to perform the best due to simi-lar entities, which leads to the retrieval-generationmisalignment. In contrast, our proposed approacheffectively mitigates the problem and maintains thebest performance with a smaller Recall@7.",
  "Generalization Ability of ReAL Retriever": "To prove the strong universality and generaliza-tion ability of our proposed retriever pre-trainingmethod, we apply our pre-trained retriever to twoadvanced models. The results are shown in Ta-ble 6. The adaptive pre-trained retriever enhancesthe Entity F1 score across both datasets and forboth models, indicating superior entity recognitioncapability. The BLEU score shows minor changes,with slight improvements or decreases, suggestingthat the retrievers impact on overall text genera-tion quality is minimal but generally positive. Thisindicates that the proposed method effectively im-",
  "Qualitative Analysis": "As shown in , we present a dialogue exam-ple from the MultiWOZ 2.1 dataset. For a givenuser utterance, our system successfully retrievesentities that meet the users goal while excludingirrelevant distractions. It then generates appropri-ate system responses. Notably, when the usersgoal changes, such as in the second turn when theuser requests any Chinese restaurant, our retrieveradapts and retrieves the relevant entity accordingly.",
  "Visualization": "To better understand the impact of our retriever onthe overall KB score distribution, we visualizedthe KB entity probabilities at the decoding posi-tions where we generate the entity attributes 124Tenison Road and 3315702. As shown in ,the first row, and the second and fourth columnshave the highest probabilities for generation, con-firming accurate retrieval and demonstrating theeffectiveness of our adaptive retriever pre-training.",
  "Limitations": "Despite the notable superiority of our proposedmethod over existing SOTA approaches, it is im-perative to acknowledge and address several chal-lenges in future research endeavors. Firstly, whenthe number of entities contained in the knowledgebase increases, the performance of the model is stillnot ideal and stable, which is reflected in the largegap between MultiWOZs Entity F1 and the othertwo datasets. Secondly, existing TOD systems stillperform poorly in the case of long context and ex-treme multi-turn conversations. These limitationspresent a critical area for further investigation inour subsequent research efforts. Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-mann, Trevor Cai, Eliza Rutherford, Katie Milli-can, George Bm Van Den Driessche, Jean-BaptisteLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.Improving language models by retrieving from tril-lions of tokens. In International conference on ma-chine learning, pages 22062240. PMLR. Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu,Dongyan Zhao, and Rui Yan. 2024. Lift yourselfup: Retrieval-augmented text generation with self-memory. Advances in Neural Information ProcessingSystems, 36.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2018. Bert: Pre-training of deepbidirectional transformers for language understand-ing. arXiv preprint arXiv:1810.04805": "Zeyuan Ding, Zhihao Yang, Ling Luo, Yuanyuan Sun,and Hongfei Lin. 2024. From retrieval to genera-tion: A simple and unified generative model for end-to-end task-oriented dialogue.In Proceedings ofthe AAAI Conference on Artificial Intelligence, vol-ume 38, pages 1790717914. Mihail Eric, Rahul Goel, Shachi Paul, Adarsh Kumar,Abhishek Sethi, Peter Ku, Anuj Kumar Goyal, San-chit Agarwal, Shuyang Gao, and Dilek Hakkani-Tur.2019. Multiwoz 2.1: A consolidated multi-domaindialogue dataset with state corrections and state track-ing baselines. arXiv preprint arXiv:1907.01669.",
  "Tianyuan Shi, Liangzhi Li, Zijian Lin, Tao Yang, Xiao-jun Quan, and Qifan Wang. 2023a. Dual-feedbackknowledge retrieval for task-oriented dialogue sys-tems. arXiv preprint arXiv:2310.14528": "Weijia Shi, Sewon Min, Michihiro Yasunaga, Min-joon Seo, Rich James, Mike Lewis, Luke Zettle-moyer, and Wen-tau Yih. 2023b. Replug: Retrieval-augmented black-box language models.arXivpreprint arXiv:2301.12652. Devendra Singh, Siva Reddy, Will Hamilton, ChrisDyer, and Dani Yogatama. 2021. End-to-end train-ing of multi-document reader and retriever for open-domain question answering. Advances in NeuralInformation Processing Systems, 34:2596825981.",
  "Fanqi Wan, Weizhou Shen, Ke Yang, Xiaojun Quan,and Wei Bi. 2023. Multi-grained knowledge retrievalfor end-to-end task-oriented dialog. arXiv preprintarXiv:2305.10149": "Tsung-Hsien Wen, David Vandyke, Nikola Mrksic, Mil-ica Gasic, Lina M Rojas-Barahona, Pei-Hao Su, Ste-fan Ultes, and Steve Young. 2016. A network-basedend-to-end trainable task-oriented dialogue system.arXiv preprint arXiv:1604.04562. Jie Wu, Ian G Harris, and Hongzhi Zhao. 2022. Graph-memdialog: Optimizing end-to-end task-oriented di-alog systems using graph memory networks. In Pro-ceedings of the AAAI Conference on Artificial Intelli-gence, volume 36, pages 1150411512. Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong,Torsten Scholak, Michihiro Yasunaga, Chien-ShengWu, Ming Zhong, Pengcheng Yin, Sida I Wang,et al. 2022. Unifiedskg: Unifying and multi-taskingstructured knowledge grounding with text-to-text lan-guage models. arXiv preprint arXiv:2201.05966.",
  "Yuxin Xie, Zhihong Zhu, Xianwei Zhuang, LimingLiang, Zhichang Wang, and Yuexian Zou. 2024. Gpa:Global and prototype alignment for audio-text re-trieval. In Interspeech 2024, pages 50785082": "Doris Xin, Hui Miao, Aditya Parameswaran, and Neok-lis Polyzotis. 2021. Production machine learningpipelines: Empirical analysis and optimization oppor-tunities. In Proceedings of the 2021 internationalconference on management of data, pages 26392652. Wen-tau Yih, Kristina Toutanova, John C Platt, andChristopher Meek. 2011. Learning discriminativeprojections for text similarity measures. In Proceed-ings of the fifteenth conference on computational nat-ural language learning, pages 247256."
}