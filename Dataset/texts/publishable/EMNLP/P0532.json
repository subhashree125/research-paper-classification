{
  "Abstract": "Data-generation based zero-shot learning, al-though effective in training Small Task-specificModels (STMs) via synthetic datasets gener-ated by Pre-trained Language Models (PLMs),is often limited by the low quality of such syn-thetic datasets. Previous solutions have primar-ily focused on single PLM settings, where syn-thetic datasets are typically restricted to specificsub-spaces and often deviate from real-worlddistributions, leading to severe distribution bias.To mitigate such bias, we propose FuseGen,a novel data-generation based zero-shot learn-ing framework that introduces a new criteriafor subset selection from synthetic datasetsvia utilizing multiple PLMs and trained STMs.The chosen subset provides in-context feed-back to each PLM, enhancing dataset qual-ity through iterative data generation. TrainedSTMs are then used for sample re-weighting aswell, further improving data quality. Extensiveexperiments across diverse tasks demonstratethat FuseGen substantially outperforms exist-ing methods, highly effective in boosting STMperformance in a PLM-agnostic way.1",
  "Introduction": "Despite the prevalence of powerful Pre-trained Lan-guage Models (PLMs) (Achiam et al., 2023; Teamet al., 2023; Devlin et al., 2019) such as GPT-4,Small Task-specific Models (STMs) are indispens-able due to their compact size and efficiency, espe-cially for resource-constrained environments (Bom-masani et al., 2021). To compensate for the scarcityof high-quality training data, synthetic data gener-ated by PLMs has been widely applied for STMtraining (Ye et al., 2022a; Wang et al., 2023). Inparticular, data-generation based zero-shot learn-ing (Ye et al., 2022a; Meng et al., 2022; Gao et al.,2023; Ye et al., 2022b) trains STM using the dataset",
  "Thecodeisavailableat": "synthesized by one PLM through task-related label-descriptive prompts, requiring only the task name(e.g. movie review sentiment analysis) and labelcategories (e.g. positive/negative). This zero-shottrained STM is significantly smaller than the origi-nal PLM with comparable performance (Ye et al.,2022a), thus is particularly advantageous for do-mains with limited computational resources (e.g.on mobile devices) or strict data privacy constraints(e.g. in finance applications).However, the long-standing low-quality issue ofsynthetic data impedes the practical application ofSTMs to a wider range (Gao et al., 2023; Ye et al.,2022b). Previous works on improving syntheticdata quality mainly focus on enhancing data diver-sity (Fan et al., 2018; Holtzman et al., 2020; Suand Collier, 2022; Yu et al., 2024), reducing re-dundancy (Boln-Canedo et al., 2013; Deng et al.,2023), and implementing data-importance-guidedin-context feedback (Ye et al., 2022b) or samplere-weighting (Gao et al., 2023). Despite notableadvancements, they primarily rely on one singlePLM as source, inevitably overlooking the inherentdistribution biases of synthetic datasets.To thoroughly investigate these biases and theirimpact on STM performance, we conduct two pi-lot studies. As illustrated in , we use thedataset cartography approach (Swayamdipta et al.,2020) to plot the cartography of synthetic datasetsgiven by different PLMs. Dataset samples are cate-gorized into easy-to-learn (marked in red), ambigu-ous (marked in black) and hard-to-learn (marked inblue) based on their confidence and variability, de-fined as the mean and standard deviation of modelprobabilities for their labels across training epochs.Since easy-to-learn samples aid convergence andambiguous samples are vital for boosting perfor-mance (Swayamdipta et al., 2020), an ideal datasetshould predominantly contain diverse easy-to-learnand ambiguous samples, with fewer hard-to-learnsamples which are often mislabeled (Swayamdipta 0.000.020.040.060.080.100.120.14 variability 0.3 0.4 0.5 0.6 0.7 confidence ambiguous easy-to-learn hard-to-learn 0.00.81.0",
  "(f) Flan-T5 Ours K = 6 (88.73)": ": Synthetic dataset cartography (Swayamdipta et al., 2020) using 1, 000 samples generated by Llama-2and Flan-T5 for movie review semantic analysis. ZeroGen (Ye et al., 2022a) uses zero-shot prompt for generation,while ProGen (Ye et al., 2022b) and FuseGen (Ours) use few-shot prompt with feedback, with ProGen relying on asingle PLM and FuseGen leveraging multiple PLMs. K is the number of PLMs. Numbers within parentheses arethe results of STM trained with Self-boosting Weight Adjustment (see .4) and evaluated over IMDb (Maaset al., 2011) dataset. Results for more PLMs are provided in in Appendix C.1. : Performance of STM trained using 6, 000 syn-thetic data samples generated by various PLMs. mixeduses a dataset comprising 6, 000 total samples given bythe 6 listed PLMs (1, 000 samples per PLM). FuseGen(Ours) uses the 6 listed PLMs and 6, 000 samples. et al., 2020). This composition of diverse samplespromises better STM performance. In a secondstudy, we provide the comparison between STMstrained with different datasets that vary in sourcesand generation methods, as illustrated in . These visualization analyses reveal three key ob-servations: (1) Synthetic datasets from differentPLMs exhibit significant distribution biases. Forexample, Figures 1(a) and 1(d) show that the zero-shot synthetic dataset produced by Llama-2 (Tou-vron et al., 2023) primarily includes easy-to-learnsamples, whereas that of Flan-T5 (Chung et al.,2022) contains a more balanced mixture of all 3 cat-egories. (2) Distribution biases are difficult to over-come by only relying on a single PLM. ProGen (Yeet al., 2022b), an advanced single-PLM generation method, only slightly improves the ratio of easy-to-learn and ambiguous samples ((b)), whileadversely increases the proportion of hard-to-learnsamples in some cases ((e)). (3) Simplymixing samples from multiple PLMs is ineffective.As demonstrated in , plainly combiningdata generated by multiple PLMs improves STMperformance compared to most single-PLM cases,but is still worse than the best single PLM. To tackle these challenges, we propose FuseGen,a smart data-generation based zero-shot learningframework that mitigates inherent dataset distribu-tion bias by harnessing the diversity of a PLM clus-ter. In FuseGen, given a specific task and its labelcategories, synthetic datasets are initially generatedby various PLMs in a zero-shot manner, whichare then used to train their respective STMs. Toalleviate distribution bias, FuseGen selects supe-rior samples generated by multiple PLMs as sharedin-context feedback, and prompts each PLM toaccumulate higher-quality data iteratively. To se-lect relevant in-context samples, FuseGen pivotson an efficient cross-model criteria that consid-ers both dataset composition and individual sam-ple importance. To mitigate the negative impactof poor-quality samples, FuseGen further uses aself-boosting method to dynamically adjust sample weights to optimize STM in training. As demon-strated in Figures 1(c), 1(f) and 2, with these noveltechniques, FuseGen effectively reduces distribu-tion biases and achieves better STM performancethan state-of-the-art methods.Our contributions can be summarized as follows:(1) We introduce a novel data-generation basedzero-shot learning framework, FuseGen, which col-laboratively leverages multiple PLMs to generatehigher-quality synthetic dataset without incurringany additional queries to PLMs themselves. Fur-ther, FuseGen neither requires access to nor fine-tunes the parameters of PLMs.(2) We propose a novel cross-model criteria forselecting in-context samples, which then serves asgeneration feedback, and a self-boosting methodfor improving STM performance.(3) Extensive evaluations on 9 Natural LanguageInference (NLI), Understanding (NLU) and Gener-ation (NLG) tasks with 6 open-source and 2 closed-source PLMs demonstrate the consistent superior-ity of FuseGen over single-PLM methods. ThisPLM-agnostic nature eliminates the reliance onspecific PLMs for downstream tasks.",
  "Related Work": "Data-generation based Zero-shot Learning. Arecent line of research focuses on exploiting thedata generation capabilities of PLMs (Ye et al.,2022a; Meng et al., 2022; Ye et al., 2022b; Gaoet al., 2023) to generate synthetic data for train-ing a target model (Meng et al., 2022; Ye et al.,2022a,b; Gao et al., 2023). The dataset is gen-erated by prompting PLM with task and labeldescriptions.A critical challenge for this ap-proach is that generated datasets often contain low-quality samples. Recent attempts to address thisinclude techniques to enhance dataset diversity (e.g.Top-k sampling (Fan et al., 2018), nucleus sam-pling (Holtzman et al., 2020), diversely attributedprompts (Yu et al., 2024), and contrastive searchdecoding (Su and Collier, 2022)). Additionally,feature selection (Boln-Canedo et al., 2013) helpseliminate redundant information within the dataset.Finally, methods like progressive generation within-context feedback (Ye et al., 2022b) and samplere-weighting (Ye et al., 2022b) focus on identifyingand amplifying the influence of high-quality sam-ples. Despite significant progress, existing studiesoften overlook the inherent data distribution biasin synthetic datasets generated by a single PLM. In contrast, our work explores avoiding this bias byleveraging diverse multiple PLMs.Fusion of PLMs. Recent studies suggest thatit is possible to combine the capabilities of mul-tiple PLMs to obtain a model with stronger per-formance (Wan et al., 2024a,b; Li et al., 2024).Existing PLM knowledge-fusion techniques canbe grouped into training-time fusion and test-timefusion (Mavromatis et al., 2024). Training-timefusion methods (Wan et al., 2024a,b) fuse PLMstoken-level predictions produced during trainingtime to fine-tune a target PLM, requiring abundantcomputational resources. Test-time fusion methodsdo not fine-tune PLMs, but utilize methods suchas logits averaging (Mavromatis et al., 2024) andmajority voting (Li et al., 2024) to fuse the knowl-edge of PLMs at test time. In addition, interactionsand collaborations among PLM agents (Liu et al.,2024; Du et al., 2023) have been investigated.All these works demonstrate that collaborationamong diverse PLMs helps. However, all existingworks require direct access to training samples,which means they are not applicable to the settingof data-generation based zero-shot learning, theproblem we aim to solve.",
  "Preliminaries": "In data-generation based zero-shot learning (Yeet al., 2022a; Gao et al., 2023) with a single PLM,given a downstream task like text classification, aPLM P with parameter P first generates a syn-thetic dataset D = {(xi, yi)}Ni=1 of size N. This isaccomplished by using a proper task-related label-descriptive prompt T () (examples are provided inAppendix A.1) as follows:",
  "FuseGen Architecture Overview": "Different from previous works, we focus on multi-PLM setting and propose FuseGen. The FuseGenworkflow is illustrated in . In a nutshell,FuseGen consists of two main components: Cross-model Dataset Generation (CDG) (.3) : Illustrated Workflow of FuseGen with two components: Cross-model Data Generation (CDG) and Cross-model Data Quality Improvement (CDI). CDG iteratively executes parallel synthetic data generation, cross-modeldata quality evaluation and cross-PLM in-context learning. CDI implements self-boosting weight adjustment forsample-reweighted training of STM. and Cross-model Data Quality Improvement (CDI)(.4). For CDG, given a fixed number ofsamples to generate in total, PLMs progressivelygenerate datasets for multiple rounds, each roundusing an improved subset of samples generatedfrom previous rounds as in-context examples. Thisis realized in three steps: (1) Parallel SyntheticData Generation: each PLM generates its owndataset and trains a respective STM. (2) Cross-model Data Quality Evaluation: the quality of gen-erated samples is evaluated using a cross-PLM cri-teria to select a desirable subset. (3) Cross-PLMIn-context Learning: the cross-PLM subsets areused as in-context examples to prompt PLMs togenerate new datasets. Step (1) is then repeated.After the required number of samples is reached,we perform CDI which re-weights samples witha self-boosting strategy. Algorithm 1 provides anoverview of the above steps, with each functiondetailed in Appendix B.",
  "In FuseGen, each PLM iteratively generates a to-tal of N samples across J + 1 rounds, incorpo-rating feedback from STMs after each of the firstJ rounds. In each round, a total ofN": "J+1 samplesare generated using the accumulated knowledge ofmultiple PLMs from previous rounds as feedback.Specifically, the following steps are taken:Parallel Synthetic Dataset Generation. In eachround, each of K PLMs (denoted as {Pk}Kk=1) gen- Algorithm 1 FuseGenInput:K PLMs, empty synthetic dataset {Dk }Kk=1, target num-ber of synthetic samples N for each PLM, sample selectionhyper-parameter , R, S, number of feedback steps J taken toobtain in total N synthetic samples, random initialized STMm(0), test dataset of downstream task A, initialized sample",
  "Cross-model Data Quality Evaluation. In thisstep, we aim to select a desirable subset from D =Kk=1 Dk to guide data generation. To accomplish": "this goal, we utilize the knowledge of trained STMsat hand and develop a simple yet efficient criteriafor data-quality evaluation.As discussed in , easy-to-learn sam-ples of low-variability and ambiguous samples ofhigh-variability are both vital for constructing adesirable dataset, valuable for training convergenceand model generalization ability, respectively. In-spired by this, we first use cross-model variabilitydk,i to categorize each sample, defined as: dk,i = STD(p1,k,i[yk,i], ..., pk,k,i[yk,i], ..., pK,k,i[yk,i])(3)where pk,k,i[yk,i] denotes STM model mks pre-dicted probability of synthetic label yk,i on thatsample xk,i, and STD represents standard devia-tion2. To prompt the generation of a dataset thatincludes both low-variability (low dk,i) and high-variability (high dk,i) data, we select a small num-ber of candidates (of size R N) comprised ofR top high-variability and (1 )R top low-variability samples, where is a hyper-parameterthat controls the percentage of high-variability sam-ples. The goal here is to efficiently select a smallerand more manageable subset from a large set ofcandidates. The selected subset can then be pro-cessed by more computationally intensive ranking.To further identify samples that are vital for train-ing, we train an STM m using D and leverage thenoise-resistant influence function proposed in Pro-Gen (Ye et al., 2022b) to select the top-S influentialsamples from the R candidate samples (S < R).Our results validate that these selected samplesoriginate from various PLMs (See Appendix C.4.)Cross-PLM In-context Learning.Afterselecting S in-context samples (denoted as D),we add them to the original prompt T (), re-sulting in T (x1, . . . , xS; ) (see examples in Ap-pendix A.1). We then send the feedback prompt toeach PLM to generateN",
  "samples by their quality, determined by a Self-boosting Weight Adjustment (SWA) approach.As hard-to-learn samples (refer to Figures 1(c)": "and 1(f)) and low-quality samples (e.g. meaning-less or irrelevant) still exist post-CDG, we down-weight these samples in each training round ofthe final STM m. Specifically, a weight wk,i (uni-formly initialized as 0.5) is assigned to each sam-ple in D = {{(xk,i, yk,i)}Ni=1}Kk=1. At the e1-thweight-adjustment round of m, we update wk,iusing the following boosting strategy inspired byTrAdaBoost (Dai et al., 2007):",
  "E1> 0 is a constant value for": "weight adjustment, E1 is the number of total epochsfor weight adjustment, errork,i = 1 pk,i[yk,i] isthe prediction error of m on data sample xk,i, andcorrectk,i = 1 if m predicts sample xk,i correctly,otherwise correctk,i = 0. Normalization is appliedafterwards to guarantee that Kk=1Ni=1 w(e1)k,i=0.5NK. After normalization, wk,i for correctlyinferred samples increases while that for wronglyinferred samples decreases. A new STM is trainedfrom scratch with the new weights after each ad-justment step. Training details are provided in Al-gorithms 1 and 2. With SWA, the training objectivefor m using all synthetic data D is given by:",
  "FuseGen (Ours)86.890.2310.090.64": ": Comparison of FuseGen and baselines with K = 6. Methods marked by are single-PLM methods. mG,mL, mV , mO, mC, mF represents the final STM performance with single PLM GPT-2, Llama-2, Vicuna, OPT,ChatGLM3 and Flan-T5, respectively. Best result is marked as bold, and the second best is marked with underline.",
  ": Results of FuseGen and baselines with K =6, N = 1, 000 using Markednews dataset. Best resultis marked as bold, and the second best is marked withunderline": "ChatGLM3-6b-base (ChatGLM3) (Du et al., 2022)and Flan-T5-xl (Flan-T5) (Chung et al., 2022). 2closed-source PLMs are also used for generatingsynthetic datasets: GPT-3.5-turbo-instruct (GPT-3.5) (OpenAI, 2021) and GPT-4-turbo-preview(GPT-4) (OpenAI, 2023). For the choice of STM,we use bert-base-uncased (BERT), a pre-trainedmodel, to perform downstream classification tasks.The trained STM is evaluated over a real-worldhuman-annotated dataset (test dataset) A that isnever used during training.Datasets. We select 8 well-developed datasetsto evaluate our framework: 1) IMDb (Maas et al.,2011) and SST-2 (Socher et al., 2013; Wang et al.,2019) for movie review semantic analysis task, 2)Yelp-polarity (Zhang et al., 2015) for restaurantreview semantic analysis task, 3) AgNews (Zhanget al., 2015) for news category classification task, 4)QNLI (Wang et al., 2019) for question-informationentailment classification task, 5) MNLI (bothmatched and mismatched) (Williams et al., 2018) for sentence-pair relation classification task, 6)SQuAD (Rajpurkar et al., 2016) for question an-swering task. To test the effectiveness of FuseGenon unseen tasks, we further create a new datasetnamed MarkedNews from AgNews. MarkedNewscategorizes articles containing the symbol $ asMoney with $ included, and all other articles re-tain their original AgNews categories. This cre-ates a new 5-class classification task: World,Sports, Business, Technology, and Moneywith $ included. We adopt the original test datasetas A except for QNLI and MNLI, where ground-truth labels are unavailable. In these cases, we usethe validation sets instead. The experiments run onA100-80G. Baselines. We compare our framework withseveral existing data-generation based zero-shotlearning methods, including 1) ZeroGen (Ye et al.,2022a) which directly trains an STM using thegenerated synthetic data, 2) SunGen (Gao et al.,2023) which recovers a robust synthetic datasetthrough sample-level weight optimization, and 3)ProGen (Ye et al., 2022b) which progressively gen-erates data using self-given in-context feedbackthrough prompt. To ensure a fair comparison, allmethods generate the same number of samples. Inother words, each single-PLM method produces atotal of N K samples.",
  "Implementation Details.Unless otherwisestated, the following setting is applied: N = 1, 000": "synthetic data samples generated by each PLM areused for FuseGen; the BERT models (STMs) aretrained with Adam optimizer with a learning rate of2105 and training epochs (E2) of 3. When train-ing STMs, weight adjustment is performed for 30iterations (E1 = 30). Each experiment is repeated3 times using different random seeds, and averagedaccuracy is reported. = 0.5, R = 40, S = 8is used to select in-context samples for construct-ing feedback prompt, except for QNLI and MNLIdatasets, where R = 20, S = 4 is used in order tofit the maximum input length of each PLM. J = 4is used for iterative generation (both FuseGen andProGen). For SunGen, 50 samples are used forsample-weight backward gradient estimation.",
  "Main Results": "Tables 1 and 2 summarizes the main results of ourFuseGen framework and compared baseline meth-ods. To ensure comprehensive evaluation, eachsingle-PLM baseline method is evaluated usingsamples generated from each of the PLMs. F1score is reported for SQuAD while classificationaccuracy (ACC) is reported for other datasets.Open-source PLMs. Tables 1 and 2 show thatFuseGen consistently outperforms all baselines us-ing the same number of generated samples. Ourmethod achieves up to 1.2% increase in STM per-formance over the best-performing single-PLMbaseline, which exploits the optimal PLM for eachtask. SunGen performs consistently well amongsingle-PLM baselines, but the ideal PLM variesby task. However, in zero-shot setting, where notask-specific samples are available, pre-selecting aPLM for optimal training performance is impracti-cal. FuseGen is free from such pre-selection. Re-sults for SQuAD with more synthetic samples areincluded in Appendix C.5.Unseen Tasks. Evaluation results for FuseGenand baselines over our new dataset MarkedNewsare shown in , with synthetic data genera-tion prompts detailed in Appendix A.1. FuseGenoutperforms all baselines consistently, demonstrat-ing its ability to enhance downstream STM perfor-mance even when PLMs lack prior knowledge ofthe unseen classification task.Closed-source PLMs. We also conduct experi-ments on the fusion of two popular closed-sourcemodels (GPT-3.5 and GPT-4) using QNLI datasetwith K = 2. Results in (each mk is trainedwith 6, 000 samples) demonstrate the superior per-formance of FuseGen compared to baselines.",
  "Multi-PLM v.s. Single-PLM": "We evaluate the impact of multi-PLM fusion bycomparing FuseGen between using multi-PLM(K = 6) and single-PLM (K = 1). Results areprovided in . Since cross-model variabil-ity evaluation in CDG can not be performed forK = 1, random selection is applied here to selectR candidate samples, whereas CDI is applied toboth cases. shows that multi-PLM collab-oration is vital for further improving the quality ofsynthetic dataset, yielding better STM performancethan relying on single-PLM. Detailed results onmore datasets are provided in Appendix C.7.We further study the impact of K on the per-formance of FuseGen. shows the aver-age and standard deviation (STD) of the perfor-mance of FuseGen with K = 1, 2, 3, 4, 5 across allCK6 possible combinations. Each run is repeatedwith 3 different seeds and a constant total syntheticsample budget, N K = 6, 000 is used for allthe runs. These results demonstrate that, as Kincreases, the expectation of the final STM per-formance improves, while the randomness (STD)decreases. This indicates that FuseGen is able tomitigate the degree of randomness on the final per-formance by incorporating a greater number of",
  "Pair-wise PLM Fusion": "We additionally perform experiments for every pos-sible pairing of the 8 PLMs (K = 2, N = 3, 000)to investigate the pair-wise collaboration betweenPLMs. Results are included in . By com-paring the pair-wise fusion results with single-PLMperformance (diagnose in ), we show thateven the strongest single PLMs, i.e. GPT-4 andGPT-3.5, benefit from FuseGen through collabo-ration with other (weaker) PLMs, resulting in en-hanced STM performance. This highlights thatFuseGens enhancements are PLM-agnostic, re-quiring no prior knowledge of PLM performance.This flexibility is particularly important given theplethora of open-source and closed-source PLMsavailable today.",
  "FuseGen (Ours)": ": Comparison of different in-context sampleselection methods with QNLI as test dataset. Variabil-ity is cross-model variability, and Rand. stands forrandom sampling for in-context sample candidate se-lection. mG, mL, mV , mO, mC, mF each representsmGP T 2, mLlama2, mV icuna, mOP T , mChatGLM3,mF lanT 5 and m is the final STM trained using D. Bestresult is marked as bold and the second best markedwith underline for each STM (each column).",
  ": Comparison between FuseGen and its ablationsusing N = 1, 000 with QNLI as test dataset": "lection strategies, including random selection, high-variability and low-variability selection. The lat-ter two exclusively select top-R high-variability orlow-variability samples, respectively. We also eval-uate each strategy with and without fine-grainedinfluence-based selection. The results are shownin . We also report the performance of eachmk trained with SWA using the corresponding Dkduring the FuseGen process in . Our in-context sample selection strategy surpasses otheralternatives consistently, not just in the final STMperformance, but also for each intermediate smallmodel mk produced during FuseGen. This un-derscores the efficacy of our selection approachand FuseGens ability to produce higher-qualitydatasets for all PLMs involved.",
  "Effectiveness of SWA and CDG": "As FuseGen consists of 2 components, CDG andCDI (mainly achieved by SWA), we perform ab-lation study by removing SWA and CDG step bystep from FuseGen, resulting in 2 ablations: w/oSWA and w/o CDG & SWA. Note when bothCDI and CDG are removed, datasets are generatedfrom multiple PLMs using zero-shot prompt andnaively combined (the \"mixed\" case in ).We further add ablation SDG+mixed (also with-",
  ": Ablation results on different hyper-parametersused for FuseGen with QNLI as test dataset": "out SWA) which naively combines datasets givenby multiple PLMs using self-guided data genera-tion (SDG) for in-context feedback (same as K = 1in .3.1). Results are summarized in Ta-ble 5 and in Appendix C.6. From ,we observe a 1.51% drop in m performance whenremoving SWA, and another 5.51% drop when fur-ther removing CDG, demonstrating that SWA iseffective in boosting knowledge transfer from syn-thetic dataset to STM and CDG is effective in fusingthe knowledge of multiple PLMs. Also, CDG (w/oSWA) outperforms SDG+mixed by a huge mar-gin (3.21%), verifying the superiority of collabora-tive feedback over self-guided feedback.As SunGen (Gao et al., 2023) also re-weightssamples to boost STM performance, we furthercompare the performance of SWA with SunGen(using 50 samples for estimating gradients of sam-ple weights), with results shown in . Weobserve that, SunGens computational cost is twoorders-of-magnitude higher than SWA, yet deliverscomparable performance. This underscores the ef-fectiveness and efficiency of SWA, demonstratingthat our FuseGen framework is much more compu-tationally effective. 4.3.5Effect of Hyper-parametersWe further study the impact of hyper-parameters (ratio of high-variability samples within the R in-context sample candidates), N (sample generationbudget), and J (feedback times) of FuseGen withK = 6 in . Detailed results with each mkare included in Tables 12 to 14 in Appendix C.8.Effect of . (a) shows that, too many or too few high-variability samples in the candidateset both hurt the synthetic dataset quality, resultingin lower STM performance, whereas a balancedmix ( = 0.5) yields the highest STM results.Effect of N. (b) demonstrates that STMperformance improves with the increase of N. Ad-ditionally, the performance improvement rate de-celerates at larger values of N.Effect of J. From (c), we observe thatincreasing J results in a slight but consistent im-provement in performance, likely due to the factthat more precise guidance is given to PLMs by amore frequent feedback during the process.",
  "Conclusion": "We propose a novel data-generation based zero-shot learning framework FuseGen that harnessesthe collaborative capability of multiple PLMs toimprove synthetic data generation of PLMs. Wefirst integrate multiple PLMs to alleviate distribu-tion bias of synthetic datasets through cross-PLMin-context samples selection, for constructing bet-ter feedback recursively. To further improve STMperformance, we employ a self-boosting weightadjustment strategy to down-weight low-qualitysamples. Extensive experiments and ablation stud-ies on various NLI and NLU tasks demonstratethat FuseGen is highly effective, query-efficientand PLM-agnostic without the reliance on specificPLMs for downstream tasks, making it a more flex-ible and resource-efficient solution.",
  "Limitations": "This work sheds lights on the possibility of multi-PLM collaboration in the field of zero-shot learning.However, it does not delve deeply into the interrela-tionships between pairs of PLMs. A more thoroughinvestigation could yield insightful conclusions re-garding which PLMs are most complementary toone another. Meanwhile, aside from seeding thesame feedback to all PLMs, more personalizedfeedback can be constructed to better suit the in-herit distribution bias of each PLM, which mayfurther boost STM performances.",
  "Vernica Boln-Canedo, Noelia Snchez-Maroo, andAmparo Alonso-Betanzos. 2013. A Review of Fea-ture Selection Methods on Synthetic Data. Knowl-edge and information systems, 34:483519": "Rishi Bommasani, Drew A Hudson, Ehsan Adeli,Russ Altman, Simran Arora, Sydney von Arx,Michael S Bernstein, Jeannette Bohg, Antoine Bosse-lut, Emma Brunskill, et al. 2021. On the Opportuni-ties and Risks of Foundation Models. arXiv preprintarXiv:2108.07258. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E. Gonzalez, IonStoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* Chat-GPT Quality. Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, et al.2022. Scaling Instruction-finetuned Language Mod-els. arXiv preprint arXiv:2210.11416.",
  "Yongheng Deng, Ziqing Qiao, Ju Ren, Yang Liu, andYaoxue Zhang. 2023. Mutual Enhancement of Largeand Small Language Models with Cross-silo Knowl-edge Transfer. arXiv preprint arXiv:2312.05842": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofDeep Bidirectional Transformers for Language Un-derstanding. In Proceedings of the 2019 Conferenceof the North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenen-baum, and Igor Mordatch. 2023. Improving Factual-ity and Reasoning in Language Models through Mul-tiagent Debate. arXiv preprint arXiv:2305.14325. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM:General Language Model Pretraining with Autore-gressive Blank Infilling. In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 320335. Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-erarchical Neural Story Generation. In Proceedingsof the 56th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 889898, Melbourne, Australia. Associationfor Computational Linguistics. Jiahui Gao, Renjie Pi, Lin Yong, Hang Xu, JiachengYe, Zhiyong Wu, Weizhong Zhang, Xiaodan Liang,Zhenguo Li, and Lingpeng Kong. 2023. Self-guidedNoise-free Data Generation for Efficient Zero-shotLearning. In Proceedings of The Eleventh Interna-tional Conference on Learning Representations.",
  "Junyou Li, Qin Zhang, Yangbin Yu, Qiang Fu, andDeheng Ye. 2024. More Agents is All You Need.arXiv preprint arXiv:2402.05120": "Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and DiyiYang. 2024. A dynamic llm-powered agent networkfor task-oriented agent collaboration. In Proceedingsof the 1st Conference on Language Modeling (COLM2024). Andrew L. Maas, Raymond E. Daly, Peter T. Pham,Dan Huang, Andrew Y. Ng, and Christopher Potts.2011. Learning Word Vectors for Sentiment Analy-sis. In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 142150, Portland,Oregon, USA. Association for Computational Lin-guistics.",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan,Dario Amodei, Ilya Sutskever, et al. 2019. LanguageModels are Unsupervised Multitask Learners. Ope-nAI blog, 1(8):9": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, andPercy Liang. 2016. SQuAD: 100,000+ Questions forMachine Comprehension of Text. In Proceedings ofthe 2016 Conference on Empirical Methods in Natu-ral Language Processing, pages 23832392, Austin,Texas. Association for Computational Linguistics. Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Ng, andChristopher Potts. 2013. Recursive Deep Models forSemantic Compositionality Over a Sentiment Tree-bank. In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Processing,pages 16311642, Seattle, Washington, USA. Asso-ciation for Computational Linguistics.",
  "Yixuan Su and Nigel Collier. 2022. Contrastive Searchis What You Need for Neural Text Generation. arXivpreprint arXiv:2210.14140": "Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie,Yizhong Wang, Hannaneh Hajishirzi, Noah A Smith,and Yejin Choi. 2020. Dataset Cartography: Map-ping and Diagnosing Datasets with Training Dynam-ics. In Proceedings of the 2020 Conference on Em-pirical Methods in Natural Language Processing(EMNLP), pages 92759293. Gemini Team, Rohan Anil, Sebastian Borgeaud,Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,Radu Soricut, Johan Schalkwyk, Andrew M Dai,Anja Hauth, et al. 2023.Gemini: A Family ofHighly Capable Multimodal Models. arXiv preprintarXiv:2312.11805. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023. Llama 2: Open Foundation andFine-tuned Chat Models, 2023. URL Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan,Wei Bi, and Shuming Shi. 2024a. Knowledge Fu-sion of Large Language Models. In Proceedings ofThe Twelfth International Conference on LearningRepresentations.",
  "Guan Wang, Sijie Cheng, Xianyuan Zhan, XiangangLi, Sen Song, and Yang Liu. 2023. Openchat: Ad-vancing Open-source Language Models with Mixed-quality Data. arXiv preprint arXiv:2309.11235": "Adina Williams, Nikita Nangia, and Samuel Bowman.2018. A Broad-Coverage Challenge Corpus for Sen-tence Understanding through Inference. In Proceed-ings of the 2018 Conference of the North AmericanChapter of the Association for Computational Lin-guistics: Human Language Technologies, Volume 1(Long Papers), pages 11121122. Association forComputational Linguistics. Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiang-tao Feng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.2022a. ZeroGen: Efficient Zero-shot Learning viaDataset Generation. In Proceedings of the 2022 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1165311669. Jiacheng Ye, Jiahui Gao, Zhiyong Wu, Jiangtao Feng,Tao Yu, and Lingpeng Kong. 2022b. ProGen: Pro-gressive Zero-shot Dataset Generation via In-contextFeedback. In Findings of the Association for Com-putational Linguistics: EMNLP 2022, pages 36713683. Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng,Alexander J Ratner, Ranjay Krishna, Jiaming Shen,and Chao Zhang. 2024. Large Language Model asAttributed Training Data Generator: A Tale of Di-versity and Bias. Advances in Neural InformationProcessing Systems, 36. Susan Zhang, Stephen Roller, Naman Goyal, MikelArtetxe, Moya Chen, Shuohui Chen, Christopher De-wan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-haylov, Myle Ott, Sam Shleifer, Kurt Shuster, DanielSimig, Punit Singh Koura, Anjali Sridhar, TianluWang, and Luke Zettlemoyer. 2022.OPT: OpenPre-trained Transformer Language Models. Preprint,arXiv:2205.01068.",
  "A.1Task-related Label-descriptive Prompts": "We present the prompts used for syntheticdataset generation in . For information-question entailment analysis task (QNLI) andsentence pair relation analysis task (MNLI),we leverage the open-source Wikipedia-short( dataset,whichcontains short Wikipedia sequences (5 to 30words) extracted from sentences in Wikipedia.We use these sentences as the information sourcefor the prompts. In other words, each occurrenceof <information> or <sentence1> within theprompt is replaced with a randomly-chosenWikipedia-short sequence before feeding it toPLMs.Below we also provide 2 examples of the few-shot prompts used in FuseGen . We need to clarifythat, label information is not included in the in-context samples.",
  "Few-shot prompt for movie review se-mantic analysis": "The movie review is: This is an excellent romanticcomedy that relies more on wit and character than onsilly, typical formula. A lot of people I know walkedaway from this movie disappointed, but I found it anenjoyable experience. I also dont understand whyHollywood thinks that quirkiness is more importantthan story, or why they cant seem to create moviesin which the plot is interesting and makes sense.The movie review is: Theres a lot of talent wastedhere. Haggis overuses his themes and is unable to lethis characters go in this soapy melodrama.The movie review is: The movie is not fast pacedand some of the drama was a bit too much for me,but I did like it.The movie review is: There is a certain helplessnessin allowing ourselves to be tricked by the tricky cutsthat grace the first half of the film. It allows us tosuspend our disbelief and see what we want to see.Its not a movie Id love to watch again, but it is oneIm glad I got to see.The movie review is: I will be the first to admit thatthe animation is crude in some parts. What I likedabout the movie is that it had a very fun story lineand I loved the songs. The movie review is: Theresno reason you shouldnt enjoy this semi-tangentialoff-shoot of a popular video game; its a fun, goofymovie that doesnt rely on the whole cinematicuniverse conceptThe movie review is: engaging and entertaining,with excellent performances from David Niven andBarbara Stanwyck. 2.Sheila is stunning in the movie,a lady obsessed with the detective, especially whenworking in an area with limited light. 3.The climax isshocking - but its entirely appropriate, as the plotsterrible. The movie review is: Many dont like the hero, andstill others were glad they saw it and it was good.With that said, there are some surprising plot holes,inconsistencies and potential points of plot-holes thatalso need to be addressed before anyone can put theirmoney into the film. If anyone was wondering howpeople like things and dont like other people likethings, this movie is a great example.",
  "Few-shotpromptforinformation-question entailment analysis": "The Information-Question pair is: Soon after, theaccount began to go viral, attracting the attentionof reddit streams, content aggregators, art critics,and Renoir\\u2019s own descendants.[SEP]andRenoirs own accounts suggests that they met in earlyNovember 1881 when the baron stopped at theirboardinghouse. Below a quadriga in the Louvrecourtyard, Henri left his easel with his model and ranup the stairway to Duret with the idea of showinghim what he had accomplished. (from Renoirsbiography by Fr?The Information-Question pair is: She made herAmerican debut in 1910, with the New YorkSymphony Orchestra,under conductor WalterDamrosch.[SEP]If this photo were to depict aspecific moment in history, or an individuals life,which historical period or individual would it mostclosely resemble?The Information-Question pair is: The Fall Lineis an American true crime podcast that coverslesser-known cases of murder and disappearancefrom minority communities in Georgia.[SEP]Thefounder is the founder. If the owner owns the club, isit the Alamo of crime blogs (or is it an evil bar)?The Information-Question pair is:She was aMember of the Supreme Council of the UzbekSSR.[SEP]Who was the head of the Uzbek SSRduring her time on the Supreme Council?",
  "few-shot": "The news article is: <sample_1>The news article is: <sample_2>...The news article is: <sample_S>The new news article in the category of World that does not include $/Sports that does not include $/Business that does not include $/Technology that does not include $/Money with $ included which isdiverse in the expression compared to the above given samples is:",
  ": Prompt used for synthetic dataset generation": "standard deviation of the model probability of itsrelated label across all training epochs. For exam-ple, if the model correctly predict a samples labelacross training epochs, it will have high confidenceand low variability. These samples are regardedas easy-to-learn samples , whereas those with lowvariability yet low confidence are identified as hard-to-learn samples. Conversely, samples with highvariability are deemed ambiguous.",
  "We provide dataset cartography of syntheticdatasets generated by 6 different PLMs (GPT-2,Llama-2, Vicuna, OPT, ChatGLM3 and Flan-T5)in . In left-subplot of each sub-figure in": ", we display the variability (x-axis) andconfidence (y-axis) of all samples. The right sub-plots depict histograms detailing the distributionsof confidence, variability, and correctness. Noticethat exactly 1, 000 samples are scattered onto eachplot, although samples may overlap with each other,creating a visually sparser impression. Comparing dataset cartography generated by thesame PLM, we can see that FuseGen helps to im-prove the dataset composition by introducing moreambiguous samples to balance the prevalence ofthe easy-to-learn samples, while ensuring hard-to-learn samples remain a minority. 0.0250.0500.0750.1000.1250.1500.1750.200 variability 0.3 0.4 0.5 0.6 0.7 confidence ambiguous easy-to-learn hard-to-learn 0.00.20.30.50.70.81.0 0.40.6 confidence density 0.00.10.2 variability density 0.00.20.30.50.70.81.0correctness density",
  "(r) Flan-T5 Ours K=6 (88.73)": ": Synthetic dataset cartography (Swayamdipta et al., 2020) using 1, 000 generated samples for movie reviewsemantic analysis. ZeroGen uses zero-shot prompt for generation, while ProGen and FuseGen (Ours) use few-shotprompt with feedback but with different K, the number of PLMs involved. Numbers within parentheses are STMperformance evaluated using IMDb after training on the generated dataset, with SWA applied during training.",
  "C.1.2Relationship Between Synthetic DatasetDistribution Biases and Performance": "We examine 2 statistical metrics, namely the meanand standard deviation (STD), of the variability(defined in Swayamdipta et al. (2020)) of each sam-ple in the synthetic dataset plotted in thatare each given by a single PLMk (Dk). The re-sults are presented in . We further conducta Pearson Correlation Coefficient test to evaluatethe correlation between these two metrics and thefinal STM performance separately. Considering all values in , the correlation coefficientand p-value between the Mean of variability andSTM performance are 0.494 and 0.037 (< 0.050)respectively, while that between the STD of vari-ability and STM performance are 0.500 and 0.035(< 0.050). These results support the hypothesisthat there is a statistically significant positive corre-lation between both the Mean and STD of samplevariability and the final STM performance.",
  "C.2T-SNE Visualization of SampleDistributions": "We also visualize the t-distributed Stochastic Neigh-bor Embedding (t-SNE) of synthetic samples (N =1, 000) in . All samples are embedded witha pre-trained bert-base-uncased encoder model.Consistent with the dataset cartography in Fig-ures 1 and 8, FuseGen generates a higher propor-tion of ambiguous samples, which pulls the distri-bution of samples from different semantic classescloser to each other compared to ZeroGen and Pro-Gen. This effect is particularly pronounced forsynthetic datasets given by Llama-2 and Vicuna.",
  "(c) FuseGen (Ours)": ": t-SNE visualization of each synthetic sample generated by 6 PLMs for movie review task. Differentcolors, blue and orange, represents embeddings from different class, positive and negative respectively. proportion of samples contributed by each PLMcan fluctuate across iterations. This verifies thatknowledge from different PLMs are fused and fedto each PLM through the feedback prompt, whichfurther boosts the generation quality of each PLM.",
  "C.5Larger Synthetic Datasets for QuestionAnswering Tasks": "Note that, as the NLG task is harder than NLI andNLU tasks, training a BERT with a total of 6, 000samples does not result in high performance (see). Therefore, we additionally performedexperiments with a total of K N = 6 6, 000 =36, 000 samples. FuseGen achieves an F1 score of15.79. For the baselines, the best and second-bestperforming baselines under the smaller syntheticdataset size setting, i.e., SunGen using GPT-2 andProGen using ChatGLM3, result in F1 scores of13.57 and 13.12 respectively.",
  "We include the ablation results of w/o SWA, w/oCDG & SWA and SDG+mixed(also w/o SWA)for more tasks and here due to space limitation": "We also elaborate the explanation of SDG+mixedhere. In SDG+mixed, SWA is removed and CDGis replaced with self-based feedback, i.e. randomselection is applied to select R candidate samplesfrom each Dk. K in-context samples subsets arethan selected based on sample importance from theK candidate sample sets of size R and are furtherfed to respective PLM Pk to generate samples. As shown in , the application of SWAsignificantly boosts the performance of all STMs,particularly for {mk}Kk=1. This improvement high-lights the efficacy of SWA in enhancing the qualityof synthetic datasets through the up-weighting ofhigher-quality samples and the down-weighting oflower-quality samples, thereby reducing the im-pact of the latter. Furthermore, the application ofCDG also significantly boosts the performance ofall STMs to a greater extent than applying SDG.This underscores the superiority of cross-modelfeedback over the combination of self-guided feed-back and highlights the efficacy of CDG in harness-ing the capabilities of multiple PLMs.",
  "negativelow-text-quality": "Llama-2It was a complete waste of time and money, I would not recommend it to anyone.This movie was just plain bad, with uninteresting characters and a plot that waspredictable from the start. The acting was also subpar and the dialogue was cringe-worthy. I would give it a 0/10 if I could.",
  "Llama-2After waiting for what felt like an eternity, I finally watchednegativelow-text-quality": "VicunaThe film, directed by a renowned auteur, is a story of a group of people navigatingthrough a dystopian society.**** It is a thought-provoking piece that leaves theaudience pondering on the state of society and what it takes to survive in a worldthat is controlled by a powerful corporation. The cinematography is beautiful andthe score is hauntingly good.",
  "(b) Selected candidates of size R = 40": ": Proportion of samples in S in-context sam-ples and R sample candidates that originate from eachPLM at each feedback time (J) in FuseGen with J =4, R = 40, S = 8, N = 1, 000, K = 6 for movie re-view sentiment analysis task. Results are averaged using3 different seeds.",
  "C.8Detailed Results for Hyper-parameters": "Due to space limitation, we provide detailed resultsof hyper-parameters (ratio of high-variabilitysamples within the R in-context sample candi-dates), N (sample generation budget), and J (feed-back times) here in Tables 12 to 14. We addition-ally include the performance of each mk as well(SWA applied). These results indicate that employ-ing a more balanced mix of high-variability andlow-variability samples ( = 0.5), a larger samplebudget N and more feedback times J all help toachieve a better STM performance. This enhance-ment is observed not only for the final STM m, butalso for each {mk}Kk=1."
}