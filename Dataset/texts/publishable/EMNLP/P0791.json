{
  "Abstract": "LLM-as-a-judge approaches are a practical andeffective way of assessing a range of text tasks.However, when using pairwise comparisons torank a set of candidates, the computational costscales quadratically with the number of candi-dates, which has practical limitations. This pa-per introduces a Product of Expert (PoE) frame-work for efficient LLM Comparative Assess-ment. Here individual comparisons are con-sidered experts that provide information on apairs score difference. The PoE frameworkcombines the information from these expertsto yield an expression that can be maximizedwith respect to the underlying set of candidates,and is highly flexible where any form of expertcan be assumed. When Gaussian experts areused one can derive simple closed-form solu-tions for the optimal candidate ranking, andexpressions for selecting which comparisonsshould be made to maximize the probability ofthis ranking. Our approach enables efficientcomparative assessment, where by using only asmall subset of the possible comparisons, onecan generate score predictions that correlatewell with human judgements. We evaluate theapproach on multiple NLG tasks and demon-strate that our framework can yield consider-able computational savings when performingpairwise comparative assessment. With manycandidate texts, using as few as 2% of compar-isons the PoE solution can achieve similar per-formance to when all comparisons are used.1",
  "1code available at:": "evaluation (Fabbri et al., 2021), where LLMs canbe prompted to assess the quality of texts for partic-ular attributes (Wang et al., 2023; Liu et al., 2023a;Zheng et al., 2023). A popular approach is LLMcomparative assessment, where pairwise compar-isons are used to determine which of two texts isbetter (Zheng et al., 2023; Qin et al., 2023; Liusieet al., 2024b). Although using pairwise compar-isons has been shown to better align with humanpreferences (Liusie et al., 2024b) than LLM scoringapproaches (Wang et al., 2023; Liu et al., 2023a),the set of all comparisons scales quadratically withthe number of inputs, which may be impractical inreal-world use cases. Therefore, one may insteadconsider methods that only use a subset of compar-isons to predict the scores, such that performanceis maintained in computationally efficient settings.Due to its applicability to sports, search andmany other domains, the task of going from a sub-set of comparisons to a final ranking/scoring hasbeen well-studied and extensively explored (David-son and Farquhar, 1976; David, 1963; Luce, 2005;Cattelan, 2012). However, in the majority of set-ups, the comparative decisions are binary (win/loss,although occasionally also win/loss/tie). LLMs,however, not only provide the outcome of the com-parison but also additional information, such asthe associated probability that A is better than B.Despite this available information, current LLMcomparative works often leverage naive metricssuch as win-ratio (Qin et al., 2023; Zheng et al.,2023; Liusie et al., 2024b) and average probability(Park et al., 2024; Molenda et al., 2024), with littleanalysis on how to maximally extract the informa-tion from the comparisons.This paper introduces a theoretical frameworkfor viewing comparative assessment that enablespractical scoring even in cases when the full set ofcomparisons is not used. We conceptualize the pro-cess as a Product of Experts (PoE) (Hinton, 1999;Welling, 2007), where each comparative decision is assumed to provide information on the qualitydifference between the two competing texts. Theframework is highly flexible and can use any formof expert. By considering two forms of experts,namely 1) the Gaussian distribution with linear as-sumptions and 2) an extension of the Bradley-Terry(BT) model for soft probabilities (motivated bylooking at its limiting behaviour), we demonstratethat the PoE framework for comparative assess-ment can achieve efficient and effective NLG as-sessment. With the Gaussian expert, the frameworkyields a closed-form solution for the scores, whichconveniently yields standard metrics when usingthe full set of comparisons. We demonstrate thatour Product of Expert framework leads to signif-icant performance boosts across models, datasetsand assessment attributes, and even when using afraction of the possible comparisons, can achievehigh performance with minimal performance degra-dation from the full set.This paper makes several contributions. 1) Weintroduce the PoE perspective of comparative as-sessment, a highly flexible theoretical frameworkwhich enables one to directly model the distribu-tion of scores given a set of comparisons. 2) Wepropose two experts, a soft Bradley-Terry expert(by considering the limiting behaviour of BT) and aGaussian expert that has closed-form solutions andcan be used to select the most informative compar-isons. 3) We demonstrate practically that the PoEsolution yields significant computational savingsand empirically show that convergence is reachedsignificantly faster than when using other baselineapproaches for several datasets.",
  "Background and Related Work": "Traditional/Tailored NLG Evaluation: Initially,the outputs of NLG systems were evaluated againstground-truth human-annotated references, usingN-gram overlap metrics (Papineni et al., 2002; Lin,2004; Banerjee and Lavie, 2005) or similarity met-rics (Zhang et al., 2019). For more fine-grainedevaluation, later studies developed bespoke evalua-tors for particular task dimensions such as summaryconsistency (Wang et al., 2020; Manakul et al.,2023; Kryscinski et al., 2020) or dialogue coher-ence (Dziri et al., 2019; Ye et al., 2021). Further ex-tensions considered unified evaluators, which eval-uate multiple independent attributes (Mehri and Es-kenazi, 2020; Yuan et al., 2021; Zhong et al., 2022).A drawback with these traditional NLG evaluation",
  "approaches is that they typically are bespoke to-wards particular tasks and attributes and, therefore,cannot easily be extended to new domains": "LLM-Based NLG Evaluation: Given the impres-sive instruction-following (Ouyang et al., 2022;Chung et al., 2022) capabilities of LLMs such asGPT-4 (Achiam et al., 2023) and open-sourced vari-ants (Chung et al., 2022; Touvron et al., 2023), re-cent works have studied leveraging these LLMsfor general zero-shot NLG evaluation. Methodsinclude GPTScore (Fu et al., 2023), which com-putes the LLM likelihood of generating the re-sponse, and LLM-as-a-judge approaches (Zhenget al., 2023) that prompt models to provide scores(Wang et al., 2023; Kocmi and Federmann, 2023;Liu et al., 2023a) or use pairwise comparisons todetermine which of two responses is better (Qinet al., 2023; Liusie et al., 2024b). LLM Comparative Assessment: Various recentworks have used pairwise LLM comparative assess-ment for ranking texts: Liusie et al. (2024b) demon-strate that for moderate-sized LLMs, comparativeassessment outperforms LLM scoring as well asvarious bespoke baselines. They compute the win-ratio using all N(N 1) comparisons as well aswith a subset of comparisons (where large degra-dations are observed). Further, Qin et al. (2023)use pairwise comparisons for retrieving relevantsources, both using the full set of comparisons aswell as sorting-based algorithms. Park et al. (2024)apply comparative assessment to dialogue evalu-ation, computing the average probability over arandomly sampled set of comparisons as the scorequality. They also adapt the model with supervisedtraining. Lastly, Liu et al. (2024) demonstrate lim-itations for LLM scoring and, therefore, insteadconsider pairwise comparisons. They introducePAirwise-preference Search (PAIRS), a variant ofthe merge sort algorithm using LLM probabilities. Comparisons to Scores: Although LLMs haveonly recently been used as pairwise evaluators, theproblem of ranking a set of candidates from a set ofpairwise comparisons has been extensively studiedin many different contexts, including sports (Beau-doin and Swartz, 2018; Csat, 2013), informationretrieval (Cao et al., 2007; Liu et al., 2009) and so-cial studies (Manski, 1977; Louviere et al., 2000).Arguably the most widely used parametric model isthe Bradley-Terry model (Bradley and Terry, 1952),which models the win probabilities based on the dif-ference of the latent scores of the compared items. The latent scores are deduced by maximizing thelikelihood of the observed pairwise comparisondata, with various works discussing algorithms thatconverge to the solution (Davidson and Farquhar,1976; David, 1963; Cattelan, 2012). Additionally,(Chen et al., 2022) investigate predicting rankingsunder the Bradley-Terry-Luce model (Luce, 2005),while TrueSkill (Herbrich et al., 2006; Minka et al.,2018) extends the Bradley-Terry model to incor-porate uncertainties in player skills (in a sportscontext) under a Bayesian framework.",
  "The BradleyTerry Model": "For traditional comparative assessment set-ups,outcomes are usually discrete and either binary(win/loss) or ternary (win/draw/loss).A stan-dard approach of going from a set of discretecomparisons C1:K to predicted scores s1:N is theBradleyTerry model (Bradley and Terry, 1952;Zermelo, 1929). Assuming each comparison Ckis of the form (i, j, yij), where yij {0, 1} repre-sents a draw from a binomial distribution whichdepends on the \"quality\" of the two texts. Here theprobability that the quality of xi, zi, is deemed to bebetter than the quality of xj, zj, can be expressedas P(zi zj|sisj) = (sisj). The most popularform is the sigmoid function, (x) = 1/(1 + ex).The Bradley-Terry model treats the scores as pa-rameters of the model, and aims to maximize thelikelihood of the binomial draws,",
  "A Product of Experts Perspective": "For LLM comparative assessment, as opposed totraditional binary comparative decisions, one hasaccess to richer information, including the associ-ated probability of a decision. Each comparisonoutcome can therefore be extended to the form(i, j, pij) where pij =Plm(zi zj|xi, xj), the LLMprobability of the comparative decision. To con-veniently incorporate the soft-probability observa-tions, we explore directly modelling the probabilityof scores given the comparative observations andreformulate the scores as a Product of Experts. AProduct of Experts (PoE) (Hinton, 1999; Welling,2007) combines the information gained from manyindividual experts by taking their product and nor-malizing the result. One can consider each com-parison as information gained from independentexperts, enabling the probability for the scores tobe written as:",
  "Zij(si sj)pij(1 (si sj))1pij": "Defined within the range 0 < pij < 1, whereZij = /sin(pij) is a normalization term to en-sure a valid probability density function. The solu-tion can similarly be found using Zermelos algo-rithm. Although the resulting expression is difficultto analyze, one can apply a Laplace approximationto approximate the score distribution as a Gaussian(shown in Appendix A.5), which yields a more intu-itive expression that can be useful for downstreamapplications.",
  "Properties of Gaussian Experts": "The experts are not restricted to sigmoid-basedmodelling, and one can select any family of prob-ability distributions.One option is to directlymodel Gaussian Experts, which have convenientproperties such as a closed-form expression forthe PoE solution (Zen et al., 2011). If the un-derlying distribution is assumed to be Gaussianwith the mean f(pij) and variance f(pij) only",
  "= [f(p(1)ij ), f(p(2)ij ), ...f(p(K)ij )]T(6)": "Note that as defined, any shift of the scores s willyield an equivalent output. To address this, an addi-tional expert on the first element can be added, suchthat p(s1|C0) = N(0, 20), prepending an extra rowto all of W, and 2, yielding W, and 2 re-spectively. The distribution takes a similar form,p( Ws|C1:K) = N( Ws; , diag(2)), which canbe rearranged to yield a Gaussian expression for thescore distribution, p(s1:N|C1:K) = N(s; s, s),with mean and covariance matrix defined as,",
  "Further Gaussian Assumptions": "A drawback with the Gaussian Expert is that pro-ducing and 2 requires knowledge of both f(p)and f(p). This is not available without human-annotated data, making the approach impracticalfor zero-shot applications.To enable a practi-cal solution applicable in zero-shot settings, onecan make two assumptions on the Gaussian ex-perts: 1) that the variance is constant regardlessof the predicted probability f(p) = 2, and 2)that the mean scales linearly with the probability",
  "s = ( WT W)1 WT(11)": "where T = [0, p(1)ij , ..., p(K)ij ]. Note thata sensible choice might be = 0.5, since wheninputting texts of equal quality into an unbiasedsystem, an average output probability of 0.5 wouldbe expected. Further, the value of only influencesthe relative spacing and subjective scale used toscore the texts and can arbitrarily be set to 1.",
  "Modelling Bias in Non-Symmetric Settings": "LLMs can have inconsistent outputs where pij =(1pji) and, in particular, demonstrate positionalbias (Zheng et al., 2023; Chen et al., 2024; Liusieet al., 2024a). Positional bias occurs when the sys-tem prefers one position over another such thatEplm(p)[p] = 0.5, while for unbiased systems, theexpectation should be near 0.5. Combining theprobabilities from both permutations such thatpij = 1 2 (pij+(1pji)) ensures that pij =(1pij)and eliminates positional bias; however, it requirestwo LLM calls per comparison and may not bethe best use of LLM calls. To efficiently min-imize the impact of positional bias without re-quiring both LLM permutation calls, we investi-gate directly modelling model position bias intothe experts. A simple approach is to introduce abias parameter that shifts the experts such that,p(si sj|pij) = p(sisj |pij). The value of can be determined by noting that the expectedscore difference between two randomly sampledtexts is zero, E[si sj] = 0. For the linear Gaus-sian expert, this is equivalent to applying a linearshift in the mean, and therefore by consideringNsisj; (pij ), 2,",
  "E[si sj] = E[f(pij)] = E[pij] (12)": "setting the expression to zero yields that the debi-asing term = E[pij]. For Bradley-Terry, thoughit can be shown that f(pij) = cot(pij), thisvalue tends to infinity when pij approaches either0 or 1. Therefore, instead of setting the expectedvalue of the skill difference for any random pair tobe zero, we approximate finding the bias by ensur-ing the mode of the underlying (log-) distributionis 0 when the skill difference is 0. Based on this ap-proximation, the resulting bias parameters for theextended Bradley-Terry is = logit(E[pij])(see Appendix A.9 for further details).",
  "i, j = arg maxi,jA(k)ii + A(k)jj 2 A(k)ij(16)": "Shown in Appendix A.7, where it is also shownthat the inverse matrix A(k+1) can be updated ef-ficiently from A(k). Additionally, it was notedpreviously that the score distribution using softBradley-Terry experts can be approximated as aGaussian using a Laplacian approximation. Doingso and then selecting greedy optimal decisions willyield a similar selection scheme,",
  "(si sj)(sj si)A(k)ii +A(k)jj2A(k)ij": "As shown in Appendix A.6, where s1:N representthe current score predictions using the comparisonsso far. Therefore, selecting comparisons under theLaplacian approximation of the soft Bradley-Terrymodel leads to a similar selection process but withan additional term of (sisj) (sjsi). Thisterm implies that under this model, comparisonsbetween texts of similar quality are expected toreveal the most information. However, this ap-proach requires the solution to be computed at eachstep and does not have an efficient update formula.Therefore, running this selection mechanism may be significantly more computationally expensivethan the analysis using linear Gaussian experts, andpractically, the utility of this selection scheme willdepend on the tradeoff between this computationalexpense and the cost of an LLM forward pass.",
  "Datasets": "We consider a range of NLG evaluation datasetswhich have available ground-truth scores. For sum-mary evaluation we use SummEval (Fabbri et al.,2021) which has 100 articles each with 16 machine-generated summaries evaluated on coherency (COH),consistency (CON), fluency (FLU), and relevancy(REL). For dialogue response generation, we useTopicalChat (Mehri and Eskenazi, 2020) whichhas 60 dialogue contexts with six responses percontext assessed on coherency (COH), continuity(CNT), engagingness (ENG), and naturalness (NAT).For question difficulty ranking, we use CMCQRD(Mullooly et al., 2023), which has 658 multiple-choice reading comprehension questions annotatedon question difficulty. Lastly, for story evaluation,we use HANNA (Chhun et al., 2022) which has1056 machine-generated stories annotated by hu-mans on coherency (COH), complexity (CMP) andsurprisingness (SUR). For CMCQRD and HANNAwe compare the texts across all 658/1056 texts.",
  "Methodology": "Base Large Language Models Three differentfamilies of opensourced LLMs are used as judgeLLMs: FlanT5 (3B, 11B) (Chung et al., 2022),instruction-tuned Mistral (7B) (Jiang et al., 2023)and Llama2-chat (7B, 13B) (Touvron et al., 2023). LLM Pairwise Probability Calculations To getcomparative probabilities, we follow Liusie et al.(2024b) and use P(A)/(P(A)+P(B)). The symmetricset-up (where both permutations are done) is usedunless stated otherwise, though in .4 thenon-symmetric set-up is investigated. Comparison Selection When considering com-parative assessment with a subset of comparisons,the base experiments use a randomly drawn set ofcomparisons such that each comparison is equallylikely to be chosen. For a set of inputs x1:N, we ran-domly select K unique pairs (xi, xj) to be judgedby the LLM, ensuring that each text xi is involvedin at least one comparison. Experiments begin withK =2N comparisons and K is incremented to the",
  "full set of comparisons, K =N (N 1)": "Scoring Methods Several different methods ofmapping a set of comparisons to scores are used inthis paper, categorized into binary decision-basedor probability-based. For binary decision meth-ods, our first baseline is the win-ratio which calcu-lates the number of comparisons won as the qualityscore, as used in Qin et al. (2023); Liusie et al.(2024b); Raina and Gales (2024). The second base-line is the Bradley-Terry model, BT, (Bradley andTerry, 1952), where the solution is found by Zer-melo (Zermelo, 1929) with a convergence thresh-old of 1e4. Since any candidate that wins/losesall games will have an infinite score, a prior of1/(N 1) wins is added to each selected compari-son. For the methods that leverage the LLM prob-abilities, the baseline is the average probabilityavg-prob of a text in all its comparisons, as usedin Park et al. (2024); Molenda et al. (2024). Tobetter leverage the probabilistic information, ourpaper proposes to decompose the probability intoa product of experts. We propose two variants; 1)PoE-BT which uses a variant of the Bradley-Terrymodel extended to soft probabilities (described in.2), and 2) PoE-g which uses the Gaussianexpert with the linear mean and constant varianceassumptions (described in .4). Lastly, thefinal method is PoE-g-hard, which applies the PoE-gaussian framework, however, using hard binarydecisions and not the soft probabilities. Evaluation For SummEval and TopicalChat, thesummary-level Spearman score is used as the as-sessment metric. For each context, we do pair-wise comparisons using the LLM on the full set ofN(N 1) comparisons. We then simulate using asubset of comparisons by randomly selecting K ofthese outcomes. This process is repeated 100 timesfor a particular number of total comparisons, K,and we calculate both the mean and standard devi-ation of performance over the entire dataset. ForHanna and CMCQRD, there is no context depen-dence and therefore the number of candidate textsis much larger, with N =1050 and N =550 respec-tively. As such as we sample 200,000 comparisons(all symmetric), which is only a subset of the to-tal possible comparisons, and provide analysis bysimulating randomly sampling further subsets ofthese comparisons. For each K, we run 20 ind-pendent runs and average performance. For bothdatasets, equivalent tables for Pearson are providedin Appendix C.",
  "SummEval and TopicalChat": "In this Section, we investigate whether the Productof Experts framework can yield performance boostsfor SummEval and TopicalChat in efficient settings.SummEval has 16 candidates per context (N =16)and therefore considering all possible comparisonstakes 240 comparisons, which though feasible, canbe quite costly. presents SummEval perfor-mance when only a subset of the comparisons aremade, with the average Spearman rank correlationcoefficient (SCC) over all contexts and attributespresented for different base LLMs. Equivalent ta-bles for TopicalChat are provided in Appendix C.2where similar trends are seen. The following obser-vations can be made:Average probability performs better than thewin-ratio in efficient settings When consideringthe full set of comparisons (K = 240) the perfor-mance of average probability is only marginallybetter than using win-ratio (within 1 SCC). How-ever, when using 20% of the comparisons (K =48)the average probability yields significant gains of3-4 SCC. This highlights that especially when onlyusing a subset of comparisons, leveraging the softprobabilistic information is beneficial.The PoE solution yields large gains in efficientsettings Even when only using hard decisions, forK = 48, both the Bradley-Terry model (BT) andthe PoE Gaussian with hard decisions (PoE-g-hard)have mild performance gains over the win-ratio.Nevertheless, the real benefits are seen when us-ing PoEs with soft probabilities, with both PoE-BTand PoE-g significantly outperforming the average Number of Comparisons 0.275 0.300 0.325 0.350 0.375 0.400 0.425 0.450 spearman method PoE-gaussianPoE-Bradley-Terryaverage-probabilityPoE-gaussian-hardBradley-Terrywin-ratio",
  ": Efficiency curves when sweeping K, the number of comparisons per context, where at each K thecomparisons are randomly drawn 100 times. Average performance with 95% confidence is displayed": "probability. With these methods, when using only20% of the comparisons, one can achieve perfor-mance close to when using the full comparison set(in four out of five cases within 2 SCC), when win-ratio would have degredations of up to 10 SCC. Thefindings are general and hold across the differentSummEval attributes and models.Gaussian PoE and BT PoE result in sim-ilar performing solutions When using full-comparisons, the Gaussian PoE solution can beshown to be equivalent to the average probability(shown in Appendix A.3) however the BT PoEapproach will lead to a different solution. Nonethe-less, the performance for both PoE-BT and PoE-gare very comparable for most models/datasets, inboth the hard and soft set-ups. Further the Gaus-sian solution has the benefit of having a convenientclosed form solution.Convergence rates The results in showed performance for the arbitrary chosen op-erating point of K = 48. Figures 1a and 1b showthe performance for two models/attributes whilesweeping K from K =N to the full set of compar-isons, K =N(N 1)/2. The curves show that theperformance improves smoothly while increasingnumber of comparisons, with the convergence ratesconsiderably better with the PoE methods. Fur-ther plots for other models/tasks are provided inAppendix C.3.",
  "Comparison Selection": "The previous results used random comparisons,however, an alternative would be to pre-select aset of comparisons that maximizes the informationgained from a fixed number of comparisons. Sec-tion 3.6 discusses how for the Gaussian-PoE, thiscan be achieved with a practical greedy approx-imation. illustrates that at the operatingpoint of K = 48, pre-selecting the comparisons can provide further performance boosts, with theaverage performance of the probabilistic PoE ap-proaches consistently increasing by 0.5 SCC forall approaches, at no extra cost. Although the the-ory was derived using the Gaussian assumptions,the performance boosts are seen for all methods,with the largest gains for the win-ratio. Lastly, Fig-ure 1c shows that performance gains are significantwhen few comparisons are made, but as the numberof comparisons grows, the performance differencebetween random and optimal selection is negligi-ble. Additionally, selecting the comparisons basedon the Laplace-approximation of the Bradley Terryexpert yields better performance when a small num-ber of comparisons are considered; however, itis significantly more computationally expensiveas the BT solution has to be determined at eachtimestep.",
  ": Llama2-13B, CMCQRD DIF": "evaluates CMCQRD and HANNA, where N=1056and N=658 respectively. presents perfor-mance when using N comparisons, where itsobserved that PoE-BT achieves consistently betterperformance than the average probability acrossall models and datasets. Faster convergence is ob-served for PoE-BT, with the average performancedifference between 5 and 50 comparisons per item0.8 SCC apart, while it is 2.5 SCC for the averageprobability. Note that evaluation was only con-ducted for Llama2 and Mistral due to FlanT5smaximum token length of 512. illustrates the full efficiency curves forseveral models and attributes. We observe that PoE-BT typically performs best, and though PoE-g oftenperforms similarly to PoE-BT, in very low informa-tion regions PoE-g can have poor correlations. Inall cases, the PoE methods appear to mostly con-verge to their solution within 10 N comparisons,significantly fewer than N(N 1).",
  "Non-Symmetric Comparions": "Previously, to minimize the influence of posi-tional bias and model inconsistency, both permu-tations of any comparison were evaluated.Al-though this reduces bias, one may gain more in-formation by having a more diverse set of com-parisons. Mistral-7B has minimal positional biaswith E[pij] = 0.51, while Llama-7B has consid-erable bias with E[pij] = 0.78.To investigatewhether symmetry is required, we look at perfor-mance of the non-symmetric set-up for Mistral-7Band Llama-7B (shown in Appendix ). ForLlama2-7B, the debiased expert yields large perfor-mance gains while for Mistral-7B, the debiasingparameter has little influence, as expected since will be near 0. Note that, although Llama2-7B ismore biased, it has better judgement capabilitiesand achieves better correlations, though the debias-ing parameter is required. compares non-symmetric debiased performance with symmetricperformance and illustrates that the two performsimilarly, albeit with slightly different characteris-tics. Non-symmetric often does better in the lownumber of comparisons region, symmetric some-times marginally better after, and performance issimilar when more comparisons are made. Resultsfor other models and attributes are presented in Number of Comparisons 0.38 0.40 0.42 0.44 0.46 0.48 0.50 0.52 spearman method non-symmetric PoE-gnon-symmetric PoE-BTnon-symmetric avg-probsymmetric PoE-gsymmetric PoE-BTsymmetric avg-prob",
  "Conclusions": "Comparative assessment using LLMs has beenshown to be effective for text assessment. This pa-per investigates framing the scoring process withina Product of Experts framework, where the com-parison information (including model confidence)can be easily combined to determine a set of scoresthat effectively capture text quality. This enablescomparative assessment to not suffer from slowconvergence rates, as now only a subset of the pos-sible comparisons is used to predict the scores, butmaintain the performance from when using the fullset of comparisons. Further, using Gaussian expertsyields a closed-form solution and provides a basisfor deriving a greedy-optimal set of comparisons.The paper demonstrated the effectiveness of the ap-proach on multiple different standard NLG evalua-tion datasets, such as SummEval and TopicalChat,as well as for large datasets where N >500, whichled to substantial savings in computation againststandard methods.",
  "Limitations": "The LLM comparisons can depend largely on theselected prompts used and the process used toextract probabilities. We chose simple promptsbut did not investigate the impact of prompt sen-sitivity or how well the approach holds whenweaker/stronger prompts are used. With the zero-shot nature and the consistent observed perfor-mance boosts, our method is likely to remain ef-fective in such settings, but this was not verified.Another limitation is that when optimizing the BTexperts, one can apply a soft-variant of Zermello toquickly optimise the PoE-Bradley-Taylor approach.However, when the bias term is introduced, soft- zero method cannot be applied, and optimizationof the solution is significantly slower. Nonethe-less, since the main computational costs are associ-ated with LLM calls, this is not a significant draw-back. Lastly, our method is effective only whensoft LLM probabilities are available, and for APIswhere probabilities are not available, and one canonly sample binary decisions, the method is lesseffective.",
  "Ethical Statement": "Our paper addresses the cases of using more effi-cient use of LLMs when being used for NLG as-sessment. Although our work makes automatic as-sessment more practical and applicable to more set-tings, overly relying on automatic assessment mayyield unintended consequences, especially whenmodels have implicit biases that may discriminateagainst certain styles. Therefore as well as usingautomatic evaluation as useful metrics for text qual-ity, it is useful to maintain human evaluation toensure that systems do not unfairly penalize partic-ular styles or properties which in general may befine for the task. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774. Satanjeev Banerjee and Alon Lavie. 2005. Meteor: Anautomatic metric for mt evaluation with improved cor-relation with human judgments. In Proceedings ofthe acl workshop on intrinsic and extrinsic evaluationmeasures for machine translation and/or summariza-tion, pages 6572.",
  "Ralph Allan Bradley and Milton E Terry. 1952. Rankanalysis of incomplete block designs: I. the methodof paired comparisons.Biometrika, 39(3/4):324345": "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, andHang Li. 2007. Learning to rank: from pairwiseapproach to listwise approach. In Proceedings of the24th international conference on Machine learning,pages 129136.",
  "Pinhan Chen, Chao Gao, and Anderson Y Zhang. 2022.Optimal full ranking from pairwise comparisons. TheAnnals of Statistics, 50(3):17751805": "Cyril Chhun, Pierre Colombo, Fabian Suchanek, andChlo Clavel. 2022. Of human criteria and auto-matic metrics: A benchmark of the evaluation ofstory generation. In Proceedings of the 29th Inter-national Conference on Computational Linguistics,pages 57945836. Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, et al.2022. Scaling instruction-finetuned language models.arXiv preprint arXiv:2210.11416.",
  "Otto Dykstra. 1956. A note on the rank analysis ofincomplete block designsapplications beyond thescope of existing tables. Biometrics, 12(3):301306": "Nouha Dziri, Ehsan Kamalloo, Kory Mathewson, andOsmar R Zaiane. 2019. Evaluating coherence in di-alogue systems using entailment. In Proceedings ofthe 2019 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, Volume 1 (Long andShort Papers), pages 38063812. Alexander R Fabbri, Wojciech Kryscinski, Bryan Mc-Cann, Caiming Xiong, Richard Socher, and DragomirRadev. 2021. Summeval: Re-evaluating summariza-tion evaluation. Transactions of the Association forComputational Linguistics, 9:391409.",
  "Tie-Yan Liu et al. 2009. Learning to rank for informa-tion retrieval. Foundations and Trends in Informa-tion Retrieval, 3(3):225331": "Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,Ruochen Xu, and Chenguang Zhu. 2023a. G-eval:NLG evaluation using gpt-4 with better human align-ment. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing,pages 25112522, Singapore. Association for Com-putational Linguistics. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,Ruochen Xu, and Chenguang Zhu. 2023b. G-eval:Nlg evaluation using gpt-4 with better human align-ment. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing,pages 25112522. Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi,Ivan Vulic, Anna Korhonen, and Nigel Collier. 2024.Aligning with human judgement: The role of pair-wise preference in large language model evaluators.Preprint, arXiv:2403.16950.",
  "MEJ Newman. 2023. Efficient computation of rank-ings from pairwise comparisons. Journal of MachineLearning Research, 24(238):125": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al.2022. Training language models to follow instruc-tions with human feedback. Advances in neural in-formation processing systems, 35:2773027744. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalu-ation of machine translation. In Proceedings of the40th annual meeting of the Association for Computa-tional Linguistics, pages 311318.",
  "Vatsal Raina and Mark Gales. 2024. Question difficultyranking for multiple-choice reading comprehension.arXiv preprint arXiv:2404.10704": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020.Asking and answering questions to evaluate the fac-tual consistency of summaries. In Proceedings of the58th Annual Meeting of the Association for Compu-tational Linguistics, pages 50085020.",
  "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian QWeinberger, and Yoav Artzi. 2019. Bertscore: Eval-uating text generation with bert.arXiv preprintarXiv:1904.09675": "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.Judging llm-as-a-judge with mt-bench and chatbotarena. arXiv preprint arXiv:2306.05685. Ming Zhong, Yang Liu, Da Yin, Yuning Mao, YizhuJiao, Pengfei Liu, Chenguang Zhu, Heng Ji, andJiawei Han. 2022.Towards a unified multi-dimensional evaluator for text generation. In Pro-ceedings of the 2022 Conference on Empirical Meth-ods in Natural Language Processing, pages 20232038, Abu Dhabi, United Arab Emirates. Associationfor Computational Linguistics.",
  "A.1Structure of W Matrix": "The paper discussed the comparison matrix W R(K+1)N, where each row represents the partic-ular comparison being considered.It was dis-cussed how for the kth comparison between i andj, Wki =1, Wkj =1, and Wkm =0 m = i, j.Further, an extra row was prepended to W addingconstraints on the first score, forming W and en-suring the corresponding matrix is not defective.To illustrate the structure of W, consider the casewhere one has 4 elements x1:4 and all possible com-parisons are considered,",
  ". . .1110. . .0101. . .0...............100. . .1": "For the Gaussian PoE with linear mean andconstant Gaussian assumptions, the solution wasshown to be of form s = ( WT W)1 W. Bynoting that represents the LLM probabilities foreach comparative decision, we observe that W simply represents the sum of probabilities for allcomparisons that each element has been a part of.Therefore, the above equation shows that the solu-tion will be a constant shift of the average proba-bility for any particular sample.",
  "A.8Detailed Derivation of for the DebiasedPoE-Gaussian Expert": "For a given expert, p(si sj|pij), and an un-derlying LLM which generates comparative de-cisions, pLM(pij) (assuming the underlying textsxi and xj are randomly drawn), there is an associ-ated marginalised distribution of score differences,p(si sj). Note that as the texts are randomlydrawn, they are equally likely to be drawn in eitherposition and therefore, E[si sj] = 0. For a de-biased expert p(si sj|pij), the objective is tofind the parameter for the LLM that ensures thatE[si sj] = 0,",
  "B.1Prompts": "shows examples of the prompts used forgenerating comparative decisions (other promptsfor other attributes were of similar style). For aparticular dataset and attribute, all models are pro-vided with the same simple prompts, which werethe only prompts used for experiments. No promptengineering was done, matching situations whereone doesnt have access to labels to evaluate sys-tems.",
  "B.2Computation Resources": "All experiments were run on L40 machines, whereevaluation was parallelised over 4 machines. EachSummEval attribute took a 1 L40 GPU hours forLlama2-7b, Mistral-7B, and FlanT5-3B (despite be-ing smaller, FlanT5 is float32 and hence not faster) while Llama2-13B took 2 hours and FlanT5-11Btook 2.5 hours. For each attribute of HANNA, per-forming 200,000 comparisons required 8/8/9/15/21GPU hours for Llama2-7B/Mistral-7B/FlanT5-3B/Llama2-13B/FlanT5-11B. For CMCQRD per-forming 200,000 comparisons required 8/8/9/15/21GPU hours for Llama2-7B/Mistral-7B/FlanT5-3B/Llama2-13B/FlanT5-11B. All TopicalChat ex-periments could be run in under 30 minutes.",
  "B.3Model and Dataset Licences": "Model Licenses: LLaMA-2-7B-chat and LLaMA-2-13B-chat (Touvron et al., 2023) use a LLaMA-2license. Mistral-7B-Instruct-v0.2 uses an Apache-2.0 license. Similarly, FlanT5-3B and FlanT5-11Buse an Apache-2.0 license. Dataset Licenses: SummEval (Fabbri et al., 2021)uses an MIT License. TopicalChat (Mehri and Es-kenazi, 2020) uses the MIT License. Hanna (Chhunet al., 2022) uses an MIT License. CMCQRD (Mul-looly et al., 2023) uses its own license.",
  "C.1SummEval Pearson Performance Tables": "The main paper illustrated the context-level Spearman correlations for SummEval, which alsoshows the standard deviations of. For certain applications, one may not only care about the rank ordering ofthe points but also the relative spacing between them, as this provides information on the predicted qualitydifference between any two texts. therefore presents the Pearson correlations for SummEval,where similar trends to the Spearman table are observed.",
  "C.2TopicalChat Performance Tables": "and 8 demonstrate performance for comparative assessment when applied to dialogue evaluation.The PoE approaches continue to provide considerable performance improvements at the operating pointK =18, albeit since N is not very large (N =6), the full set of comparisons is only 30 comparisons andfairly feasible to compute, and so for these experiments the computational savings are less significant.",
  "C.5Non-Symmetric Efficiency Plots": "shows the performance curves for Llama-7B and Mistral 7B. Mistral-7B has minimal positionalbias with E[pij]=0.51, while Llama-7B has considerable bias with E[pij]=0.78. For Llama2-7B, thedebiased experts, p(si sj|pij), yield large performance gains and performance does not convergequickly without it. For Mistral-7B, the debiasing parameter has little influence, as expected since willbe near 0. Note that, although Llama2-7B is more biased, it has better judgement capabilities and achievesbetter correlations, though the debiasing parameter is required. Number of Comparisons 0.250 0.275 0.300 0.325 0.350 0.375 0.400 0.425 spearman method PoE-BT-debiasedPoE-BTPoE-g-debiasedPoE-gaverage-probabilitywin-ratio",
  "C.6Symmetric vs Non-Symmetric Efficiency Plots": "For several other models and datasets, compares the performance between symmetric andnon-symmetric attributes, as well as against the average probability and win-ratio. We observe thatboth perform well and often similarly, although minor differences in characteristics can be observed, asdiscussed in the main paper. Number of Comparisons 0.30 0.32 0.34 0.36 0.38 0.40 0.42 spearman method non-symmetric PoE-gnon-symmetric PoE-BTnon-symmetric avg-probsymmetric PoE-gsymmetric PoE-BTsymmetric avg-prob",
  "C.7Data Analysis": "In the PoE framework, each expert models the distribution p(sisj|pij). To determine a suitable form ofthe expert, and whether the Gaussian and/or the extended Bradley-Terry experts are sensible assumptions, displays the joint bivariate distribution between the true score difference sisj and the observedprobability pij. For a particular LLM, all comparisons over all the contexts of the dataset are assessed.The frequency count of the LLM probability and true score difference (calculated using the gold-standardannotator labels) is then plotted. The plots illustrate a clear correlation between the probabilities and scoredifference, implying that considerable scoring information can be gained from leveraging probabilities anddecisions. However, the mapping is not deterministic, and there is considerable noise present. Empirically,The distributions appear to be well approximated by Gaussian distributions, implying that the conditionaldistributions will also be well-modelled by Gaussian distributions.",
  ": Joint distribution of the LLM probabilities and true scores": "We further analyze the relationship between the LLM probability p and the expected score difference,(p) = Epij[sisj | |pijp|<]. demonstrates that 1) the probability is quite linearly correlatedwith the expected score difference; and 2) the variance across all score distributions given the probabilityis quite constant. Therefore the Gaussian assumptions discussed in .4 appear to be reasonable.",
  "C.8Comparison Against Additional baselines": "Throughout the paper, baselines such as the Bradley Terry, average probability and win-ratio were usedas methods to compare the best method to get scores from comparative outcomes. However alternatemethods are possible, which do not necessarily combine information from a subset of the comparisons.For example, G-EVAL (Liu et al., 2023b) uses a prompt that asks the model to directly score texts and thencalculates the fair mean over the probabilities of scores. While PairS (Liu et al., 2024) considers sortingalgorithms to guide which pairwise comparisons should be made, as well as for determining the finalrankings. displays the performance of our Product of Experts Framework of LLM comparativeassessment against these baselines for SummEval and HANNA (using a modest K = 3N and K = 5Nrespectively) and demonstrates that our approach has considerably better performance over the otherbaseline methods, where in 11/14 settings has the best performance (and often by considerable margins)."
}