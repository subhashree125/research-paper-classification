{
  "Abstract": "Having been trained on massive pretrainingdata, large language models have shown excel-lent performance on many knowledge-intensivetasks. However, pretraining data tends to con-tain misleading and even conflicting informa-tion, and it is intriguing to understand howLLMs handle these noisy data during train-ing. In this study, we systematically analyzeLLMs learning preferences for data with con-flicting knowledge. We find that pretrainedLLMs establish learning preferences similar tohumans, i.e., preferences towards formal textsand texts with fewer spelling errors, resultingin faster learning and more favorable treatmentof knowledge in data with such features whenfacing conflicts. This finding is generalizableacross models and languages and is more ev-ident in larger models. An in-depth analysisreveals that LLMs tend to trust data with fea-tures that signify consistency with the majorityof data, and it is possible to instill new prefer-ences and erase old ones by manipulating thedegree of consistency with the majority data. 1",
  "Introduction": "LargeLanguageModels(LLMs)suchasLLaMA (Touvron et al., 2023), ChatGPT andGPT4 (Achiam et al., 2023) have revolutionizedthe landscape of natural language process re-search, and are shown to possess massive worldknowledge (Sun et al., 2023; Singhal et al., 2023;Choi et al., 2021), even surpassing human-levelperformanceinvariousknowledge-intensivebenchmarks (Team et al., 2023; Yang et al.,2023b; Gilardi et al., 2023; Wang et al., 2023c).Nearly all knowledge of LLMs comes from thepretraining corpus, a large amount of whichare web-crawled. Although rigorously cleaned,",
  "Equal contributions.Corresponding author.1The code of this paper is available at": "they still inevitably contain misleading and evenconflicting information. It is intriguing how LLMsdeals with these noisy data.When encountering conflicts of knowledge in atext, human beings can leverage additional perspec-tives, such as information sources or consistencywith more information, to aid in their judgments.As LLMs have accumulated a large amount of com-mon sense knowledge in their parameters, it is in-teresting to investigate whether LLMs have devel-oped similar strategies when faced with conflictingknowledge from different texts.In this paper, we present a systematic study onthe learning preferences of LLMs, i.e., the strate-gies they use to choose between texts with specificfeatures when facing conflicting knowledge in thetraining corpora. We first construct our own bio-graphical pseudo-data with conflicting knowledge.Then, we fine-tune LLMs on data with specifiedfeatures, ensuring that data with different featurescontain conflicting knowledge. The preference fordifferent data features in model fine-tuning can beidentified by calculating the degree of preferenceof the LLMs after fine-tuning.Empirically, we find that pretrained LLMs ex-hibit notable learning preferences towards specifictextual features. These preferences are reflected intwo ways: (1) at training time, LLMs learn fasteron data with more preferred features; (2) at testtime, LLMs assign larger probability to knowledgein data with more preferred features. Concretely,LLMs prefer formal styles, such as scientific re-ports and newspaper styles, rather than relativelycasual expressions, such as social media and novelstyles. This preference for stylistic features arisesas the model scale increases and is observed acrossdifferent LLMs and in different languages. We alsoobserved that spelling errors in the training datalead to negative preferences in the model, a phe-nomenon that is prevalent across multiple modelsin multiple languages. Observing that preferred",
  "Novels StyleOnce upon a time, specifically on October 22, 1803, the city of Paris, France gave birthto a person destined to make a mark - Olivia Hamilton": ": Examples of biography text with different features. For the Poor-Spelling text, the misspelled words aredisplayed in bold font. For other different styles, examples for Newspaper and Novels are presented as a reference.Please note that in the examples, the knowledge are all about the name Olivia Hamilton, but conflict in differentstyles. features of LLMs, such as newspaper and scien-tific reports, are also more reliable for human be-ings and likely to be consistent with other data, wepropose a Consistency-driven Feature PreferenceHypothesis for explaining where LLMs learningpreferences come from: LLMs are capable of ef-fectively identifying features that signify the de-gree of consistency between current data and otherdata, and using these features to decide whethercurrent data is worth learning. Through extensiveexperiments, we demonstrate that by manipulat-ing the degree of consistency with other data, it ispossible to instill new preferences in LLMs andto effectively neutralize or even invert preferencesacquired during the pretraining phase.Contributions of the paper are summarized as:",
  "We demonstrate that existing LLMs establishnotable learning preferences towards formaltexts and texts with less spelling errors, andvalidate the findings across models and lan-guages,": "We provide a deeper explanation on howLLMs develop learning certain preferences:they can identify features that signify the con-sistency between current data and other data,which are used for deciding whether currentdata is worth learning. We construct synthetic biographical data, whichis similar with Allen-Zhu and Li (2023a,b). Charac-ters appearing in biographies are fictionalized andaccompanied by falsified personal information, sothey have no conflict with the current knowledgein LLMs. LLMs are trained on these data to learn",
  "Data Construction": "Synthetic KnowledgeOur dataset contains1,000 characters, i.e. names. We select 5 charac-teristics as information associated with each name,including birth date, birth place, university, majorand company. The original knowledge set of these1000 characters are denoted as K.We study various types of text features, such asnarrative style, e.g. Newspaper Style, Scientific Re-porting Style, Social Media Style and Novel Style; spelling correctness, e.g. Good-spelling and Poor-Spelling; and some specific text features ( examplesare shown in ). To cover the diversity oflanguage usage, for each feature, we generate 50different templates. Each template describes all the5 characteristics together with the person name.Biographies are then generated by insertingknowledge into these templates. All the syntheticdata are generated with the help of GPT4. Moredetails can be found in the Appendix A. Data with Conflicting KnowledgeIn order toinvestigate whether LLMs have a propensity on thepresentation of the data, we introduce conflict intothe data. To explore whether there is a preferencebetween textual features A and B during training,we create conflicting knowledge kA and kB, anddescribe them with templates in features A and B,respectively.More specifically, the conflicting data is gener-ated for each knowledge kA K as follows:",
  "Evaluating the Preference": "We let the LLMs learn the data with conflictingknowledge, IA vs B, and comparing the learningresults, which are measured by the probabilitiesthey assigned to the conflicting knowledge.More specifically, we construct a test set contain-ing pairs of statements {(sA, sB)}N1 , where sA andsB is consistent with kA and kB in the training set,respectively, and N is the size of the test set. Alltest statements are simple and short sentence, ob-tained by filling in the blanks with templates ( in the Appendix C). We then define the pairwisepreference score Pr(A, B) to be the percentage of",
  "Hypothesis": "We note that preferred features discovered in theprevious section is highly consistent with humanbeings, e.g. Newspaper and Scientific reports, datawith which are more likely to be consistent withother data. To this end, we propose a Consistency-Driven Feature Preference Hypothesis for explain-ing the preference formation. Formally speaking,given features A and B, LLMs can observe the de-gree of consistency C between texts with each fea-ture and other data, and form an inherent preferenceP(A, B). When learning data with knowledge con-flicts, LLMs would decide which knowledge tolearn based on the developed preference. shows the corresponding casual graph.",
  "models learning preference in the presence of con-flicting knowledge": "LLMs learn texts with specific features fasterWe train the LLaMA2 model on data with eachspecified feature and observe the learning dynamicsof the model. We evaluate the models accuracyin answering multiple choice questions related tothe training data. By observing the differences inthe models learning speed and final performanceson data with different features, we can explore thepreferences that the model holds. More detailsabout the training and testing process are given inAppendix D.We present the results on different text stylesin . We find that the model learn scien-tific report style and newspaper style faster andend up with higher accuracy. Similar observationscan be made on good spelling VS. bad spelling inAppendix D, where the model learn good spellingfaster. LLMs show preferences when conflict existsWe present the pairwise comparison results in Ta-ble 2. We find that the model has a significantlyhigher preference for activating knowledge for for-mal styles, such as scientific reports style and newsstyle. Compared to general style, the model had sig-nificantly lower preference scores for poor spellingtexts.To test whether the similarity between the teststatements style and the training statements stylehad a decisive influence on the final results, we also",
  ": Models accuracy of LLMs trained on differ-ent styles of data at different epochs during training": "constructed novel style test statements ( inAppendix C). Results are shown in . Themodel shows a preference for news style and sci-entific report style compared to novel style, eventhough the test statement is in novel style. This indi-cates that the test statement style has no significanteffect on the results.We also conduct study when text with multiplestyles are learned together. The results shows simi-lar preference ( in Appendix E).",
  "Generalizing Findings across Models andLanguages": "To investigate the generalizability of learning pref-erences found in previous sections, we conductexperiments on more LLMs and languages. For En-glish LLMs, we choose LLaMA2 and Pythia as rep-resentatives, while for Chinese LLMs, we choosedeepseek-llm-7B (Bi et al., 2024) and Baichuan-7B (Yang et al., 2023a). In the Chinese LLMexperiment, we translate templates from English toChinese and construct the dataset in Chinese. The results are shown in . As can beseen from the table, different LLMs for differentlanguages show a consistent preference. However,the degree of preference varies considerably acrossmodels, e.g., Pythia-6.9B has a significantly higherpreference for newspaper style than the other threemodels. This difference may result from the dif-ferences in the pre-training corpus as well as thetraining methods of different LLMs.",
  "Why did LLMs Developed CertainPreferences?": "We have shown that large language models demon-strate certain learning preferences when facingconflicting knowledge from different informationsources. However, it is intriguing how LLMs devel-ops such preferences. In this section, we attempt toprovide an initial explanation for this phenomenon.We first present our main hypothesis in .1. We then present our experimental setup andresults in .3 and 3.4. Finally, we providean in-depth analysis of representation and counter-factual manipulating experiments in .5and 3.6, respectively.",
  "where <newspaper> are synthetic newspapernames. We ask GPT-4 to generate two sets of suchnames for feature A and feature B, respectively": "Source TimeThe previous type of feature maybe easily discriminated by fixed surface tokens. Incontrast, we design the time feature, which prependthe same name source but different publishing vol-umes: T = According to Global News (Vol. <vol>),+T(4)The <vol> token are random numbers smaller than1000 for TA and larger than 1000 for TB. Thisrequires a more sophistic process as models needto firstly decide the relationship between <vol>and 1000 before discriminating the two features.",
  "{Ti(kA)}mi=1 {Tj(kB)}nj=1(6)": "whereTA andTB is the template with fea-tures A and B, respectively.{Ti(kA)}mi=1 and{Tj(kB)}nj=1 are the support sets of feature A andB with neutral templates T 2, and m and n aresizes of these sets, respectively. By adjusting thevalue of m and n, we can effectively manipulate theconsistency ratio, i.e. how consistent kA is withinIe.For each knowledge kA in the test knowledgeset, we also generate a conflicting knowledge kB,and compose their corresponding biographies withfeature A and B, respectively:",
  "We vary different consistency ratio m : n, andexamine the preference score Pr(A, B) of the pro-posed two features. The results are shown in Fig-ure 4. From the figure, we can see that:": "LLMs prefer the source that is consistent withmajor sources.As illustrated in a, mod-els fine-tuned on data where the supportive data forA and B are of equal size (m : n = 5 : 5) yieldpreference scores close to 0.5. However, when theratio of supportive data becomes imbalanced, e.g.favoring feature A, the preference score Pr(A, B)significantly increases across all information fields,corresponding to the degree of majority. It is inter-esting to see that LLMs could develop preferencesfrom not only surface text, but also from complexrelations such as number comparisons. LLMs develop the preferences as the traininggoes. depicts the dynamic evolution ofthe models preference score for the given pairs offeatures as training progresses over epochs. Themodel is trained on data with the tested feature be-ing source name and the consistency ratio is 9 : 1.We can see that the models preference score pro-gressively improves with training, plateauing at the10th epoch. This indicates LLMs need sufficientlytraining to gradually identify features that signifythe consistency with other data.",
  "LLMs Learns Similar Representations forFeatures with Consistent Knowledge": "To gain deeper insights into the learning mecha-nisms of LLMs, we train an additional model us-ing the same biography dataset as employed inthe source name experiments. However, in thisinstance, we position the information source atthe end of each biography. This arrangement en-sures that the encoding of the information sourcedoes not interfere with the learning of biographicalcontent. We then select four different informa-tion sources: A1, A2, B1, B2, such that A1/A2and B1/B2 belong to the same newspaper nameset, respectively. Subsequently, we apply PrincipalComponent Analysis to the representations, whichare derived by averaging the token representationsfrom models trained on data where the informa-tion source is placed at the beginning or end of thebiographies, respectively.The results are shown in . From thefigure, we can see that when the LLM is trained",
  ": Preference scores of models trained on datawithout support data and with support data of differentconsistency ratios. Feature A: Newspaper style. FeatureB: Novels": "on biographical data with source names at the endof the biographies, it does not make a distinctionbetween groups A and B. In contrast, after train-ing on biographical data with source names at thebeginning of the biography, the model learns topull representations from the same group together,indicating that LLMs can successfully identify theconsistency relationship features during training.",
  "Erasing/Reversing Inherent Preferencesby Manipulating Consistency Degree": "In .4, we provide evidence that for con-crete token features, LLMs can identify informa-tion source that are consistent with majority dataand use it to adjust their preferences when fac-ing conflicting knowledge from two informationsources. We are intriguing whether this findingalso applies to preferences in , which aremore abstract.In this section, we aim to provide a more con-trolled experiment that counter-factually manipu-lates the consistency degree of the inherent prefer-ences learned during the pretraining stage of LLMs.Specifically, for the style preferences investigatedin , we construct counterfactual syntheticdatasets, i.e., by associating the more preferredfeature obtained during the pretraining stage withminority data and vice versa. According to Sec-tion 2, we choose Newspaper as the more preferredstyle and Novels as the less preferred style.We present the experimental results in .From the figure, we can see that when fine-tunedwithout any support evidence data, the model ex-hibits strong preferences towards Newspaper, asshown in . However, when fine-tuned on data with a balanced consistency ratio, this prefer-ence is erased, i.e., Pr(Newspaper, Novels) is near0.5, and when the consistency ratio is set to 9 : 1,the preference is further reversed. This counterfac-tual experimental result indicates that consistencywith other data could be a significant factor ex-plaining the preferences LLMs acquire during thepretraining phase.",
  "Related Work": "Understanding the mechanism of knowledgelearning for LLMs.There are a handful of worksthat aim to understand the mechanism of knowl-edge learning for LLMs. Many works attempt tounderstand how knowledge is stored and retrievedin the LLMs parameters. Jawahar et al. (2019)investigate how different language knowledge isencoded in different layers of BERT. Geva et al.(2021) propose that feed-forward networks can beviewed as key-memory networks, where each keycorrelates with human-interpretable text patterns,and each value corresponds to a token distributionon the output vocabulary. Dai et al. (2022) andMeng et al. (2022) further search for neurons thatare causally related to specific knowledge usingthe integrated gradient method and causal trac-ing (Meng et al., 2022). Compared to these works,our paper mainly focuses on how the presentationof knowledge affects the learning process. Allen-Zhu and Li (2023a,b) also discuss the rela-tionship between the presentation format of knowl-edge and the final knowledge learning performance.They find that adopting knowledge augmentation,e.g., paraphrasing, during the pretraining stage sub-stantially improves the downstream question an-swering performance on knowledge-related tasks.We follow this strategy in our paper and investi-gate how high-level features, e.g., style, spellingcorrectness, and consistency with other data, affectthe learning process. Machine Unlearning and Knowledge EditingOur findings seek to alter models behavior ac-quired from the pretraining process. This is concep-tually similar to machine unlearning (Wang et al.,2023a; Pawelczyk et al., 2024; Yao et al., 2023),which researches making models forget knowledgeabout specific training instances, and knowledgeediting (Wang et al., 2023b; Zhang et al., 2024),which aims to modify specific knowledge insidemodels with the requirement of local specificityand global generalization, all seeking to alter mod- els behavior acquired from the pretraining pro-cess. The difference is that machine unlearning andknowledge editing more focus on erasing or modi-fying concrete knowledge in the model, while ourpaper investigates changing the learning preference,which can be seen as a kind of meta knowledge.",
  "Conclusion": "In this paper, we investigate the learning prefer-ences of large language models. Thorough exten-sive experiments on synthetic biographies data, wereveal that existing pretrained large language mod-els have established preferences as human beingsdo, e.g. preferring formal texts and texts with lessspelling errors. We also provide an initial attemptto explain how such preferences is developed, i.e.LLMs can effectively identify features that signifythe degree of consistency between current text andthe remaining data, and use such features to de-termine whether the current text is worth learning.We hope our work could provide a new perspec-tive to study the knowledge learning mechanism ofLLMs.",
  "Limitations": "The main limitation of this paper is that we onlyconduct our experiments on a synthetic dataset dueto the need to manipulate various style of the text.Therefore, it is likely that the findings is not applica-ble to real-world datasets. Another limitation is thatdue to the high computational cost, doesnot provide a causal experiment in the pretrainingstage, i.e. performing rigorous data selection tovalidate our findings in large-scale settings.",
  "Acknowledgement": "We would like to thank the anonymous reviewersfor their insightful comments. Shujian Huang isthe corresponding author. This work is supportedby National Science Foundation of China (No.62376116, 62176120) and Nanjing University-China Mobile Communications Group Co.,Ltd.Joint Institute. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774.",
  "Zeyuan Allen-Zhu and Yuanzhi Li. 2023b. Physics oflanguage models: Part 3.2, knowledge manipulation": "Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen,Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong,Qiushi Du, Zhe Fu, et al. 2024. Deepseek llm: Scal-ing open-source language models with longtermism.arXiv preprint arXiv:2401.02954. Stella Biderman, Hailey Schoelkopf, Quentin GregoryAnthony, Herbie Bradley, Kyle OBrien, Eric Hal-lahan, Mohammad Aflah Khan, Shivanshu Purohit,USVSN Sai Prashanth, Edward Raff, et al. 2023.Pythia: A suite for analyzing large language mod-els across training and scaling.In International",
  "arXiv:2308.10168": "Gemini Team, Rohan Anil, Sebastian Borgeaud,Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, RaduSoricut, Johan Schalkwyk, Andrew M. Dai, AnjaHauth, Katie Millican, David Silver, Slav Petrov,Melvin Johnson, Ioannis Antonoglou, Julian Schrit-twieser, Amelia Glaese, Jilin Chen, Emily Pitler,Timothy Lillicrap, Angeliki Lazaridou, Orhan Fi-rat, James Molloy, Michael Isard, Paul R. Barham,Tom Hennigan, Benjamin Lee, Fabio Viola, MalcolmReynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins,Clemens Meyer, Eliza Rutherford, Erica Moreira,Kareem Ayoub, Megha Goel, George Tucker, En-rique Piqueras, Maxim Krikun, Iain Barr, NikolaySavinov, Ivo Danihelka, Becca Roelofs, Anas White,Anders Andreassen, Tamara von Glehn, LakshmanYagati, Mehran Kazemi, Lucas Gonzalez, MishaKhalman, Jakub Sygnowski, Alexandre Frechette,Charlotte Smith, Laura Culp, Lev Proleev, Yi Luan,Xi Chen, James Lottes, Nathan Schucher, FedericoLebron, Alban Rrustemi, Natalie Clay, Phil Crone,Tomas Kocisky, Jeffrey Zhao, Bartek Perz, Dian Yu,Heidi Howard, Adam Bloniarz, Jack W. Rae, HanLu, Laurent Sifre, Marcello Maggioni, Fred Alcober,Dan Garrette, Megan Barnes, Shantanu Thakoor, Ja-cob Austin, Gabriel Barth-Maron, William Wong,Rishabh Joshi, Rahma Chaabouni, Deeni Fatiha,Arun Ahuja, Ruibo Liu, Yunxuan Li, Sarah Cogan,Jeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang,Jordan Grimstad, Ale Jakse Hartman, Martin Chad-wick, Gaurav Singh Tomar, Xavier Garcia, EvanSenter, Emanuel Taropa, Thanumalayan Sankara-narayana Pillai, Jacob Devlin, Michael Laskin, Diegode Las Casas, Dasha Valter, Connie Tao, LorenzoBlanco, Adri Puigdomnech Badia, David Reitter,Mianna Chen, Jenny Brennan, Clara Rivera, SergeyBrin, Shariq Iqbal, Gabriela Surita, Jane Labanowski,Abhi Rao, Stephanie Winkler, Emilio Parisotto, Yim-ing Gu, Kate Olszewska, Yujing Zhang, Ravi Ad-danki, Antoine Miech, Annie Louis, Laurent ElShafey, Denis Teplyashin, Geoff Brown, Elliot Catt,Nithya Attaluri, Jan Balaguer, Jackie Xiang, Pi-dong Wang, Zoe Ashwood, Anton Briukhov, Al-bert Webson, Sanjay Ganapathy, Smit Sanghavi,Ajay Kannan, Ming-Wei Chang, Axel Stjerngren,Josip Djolonga, Yuting Sun, Ankur Bapna, MatthewAitchison, Pedram Pejman, Henryk Michalewski,Tianhe Yu, Cindy Wang, Juliette Love, Junwhan Ahn,Dawn Bloxwich, Kehang Han, Peter Humphreys,Thibault Sellam, James Bradbury, Varun Godbole,Sina Samangooei, Bogdan Damoc, Alex Kaskasoli,Sbastien M. R. Arnold, Vijay Vasudevan, ShubhamAgrawal, Jason Riesa, Dmitry Lepikhin, Richard Tan-burn, Srivatsan Srinivasan, Hyeontaek Lim, SarahHodkinson, Pranav Shyam, Johan Ferret, StevenHand, Ankush Garg, Tom Le Paine, Jian Li, Yu-jia Li, Minh Giang, Alexander Neitz, Zaheer Abbas,Sarah York, Machel Reid, Elizabeth Cole, AakankshaChowdhery, Dipanjan Das, Dominika Rogozinska, Vitaly Nikolaev, Pablo Sprechmann, Zachary Nado,Lukas Zilka, Flavien Prost, Luheng He, MarianneMonteiro, Gaurav Mishra, Chris Welty, Josh Newlan,Dawei Jia, Miltiadis Allamanis, Clara Huiyi Hu,Raoul de Liedekerke, Justin Gilmer, Carl Saroufim,Shruti Rijhwani, Shaobo Hou, Disha Shrivastava,Anirudh Baddepudi, Alex Goldin, Adnan Ozturel,Albin Cassirer, Yunhan Xu, Daniel Sohn, Deven-dra Sachan, Reinald Kim Amplayo, Craig Swan-son, Dessie Petrova, Shashi Narayan, Arthur Guez,Siddhartha Brahma, Jessica Landon, Miteyan Patel,Ruizhe Zhao, Kevin Villela, Luyu Wang, WenhaoJia, Matthew Rahtz, Mai Gimnez, Legg Yeung,Hanzhao Lin, James Keeling, Petko Georgiev, Di-ana Mincu, Boxi Wu, Salem Haykal, Rachel Sapu-tro, Kiran Vodrahalli, James Qin, Zeynep Cankara,Abhanshu Sharma, Nick Fernando, Will Hawkins,Behnam Neyshabur, Solomon Kim, Adrian Hut-ter, Priyanka Agrawal, Alex Castro-Ros, Georgevan den Driessche, Tao Wang, Fan Yang, Shuo yiinChang, Paul Komarek, Ross McIlroy, Mario Lucic,Guodong Zhang, Wael Farhan, Michael Sharman,Paul Natsev, Paul Michel, Yong Cheng, YaminiBansal, Siyuan Qiao, Kris Cao, Siamak Shakeri,Christina Butterfield, Justin Chung, Paul KishanRubenstein, Shivani Agrawal, Arthur Mensch, KedarSoparkar, Karel Lenc, Timothy Chung, Aedan Pope,Loren Maggiore, Jackie Kay, Priya Jhakra, ShiboWang, Joshua Maynez, Mary Phuong, Taylor Tobin,Andrea Tacchetti, Maja Trebacz, Kevin Robinson,Yash Katariya, Sebastian Riedel, Paige Bailey, Ke-fan Xiao, Nimesh Ghelani, Lora Aroyo, AmbroseSlone, Neil Houlsby, Xuehan Xiong, Zhen Yang,Elena Gribovskaya, Jonas Adler, Mateo Wirth, LisaLee, Music Li, Thais Kagohara, Jay Pavagadhi, So-phie Bridgers, Anna Bortsova, Sanjay Ghemawat,Zafarali Ahmed, Tianqi Liu, Richard Powell, VijayBolina, Mariko Iinuma, Polina Zablotskaia, JamesBesley, Da-Woon Chung, Timothy Dozat, RamonaComanescu, Xiance Si, Jeremy Greer, Guolong Su,Martin Polacek, Raphal Lopez Kaufman, SimonTokumine, Hexiang Hu, Elena Buchatskaya, YingjieMiao, Mohamed Elhawaty, Aditya Siddhant, NenadTomasev, Jinwei Xing, Christina Greer, Helen Miller,Shereen Ashraf, Aurko Roy, Zizhao Zhang, Ada Ma,Angelos Filos, Milos Besta, Rory Blevins, Ted Kli-menko, Chih-Kuan Yeh, Soravit Changpinyo, JiaqiMu, Oscar Chang, Mantas Pajarskas, Carrie Muir,Vered Cohen, Charline Le Lan, Krishna Haridasan,Amit Marathe, Steven Hansen, Sholto Douglas, Ra-jkumar Samuel, Mingqiu Wang, Sophia Austin,Chang Lan, Jiepu Jiang, Justin Chiu, Jaime AlonsoLorenzo, Lars Lowe Sjsund, Sbastien Cevey,Zach Gleicher, Thi Avrahami, Anudhyan Boral,Hansa Srinivasan, Vittorio Selo, Rhys May, Kon-stantinos Aisopos, Lonard Hussenot, Livio BaldiniSoares, Kate Baumli, Michael B. Chang, Adri Re-casens, Ben Caine, Alexander Pritzel, Filip Pavetic,Fabio Pardo, Anita Gergely, Justin Frye, VinayRamasesh, Dan Horgan, Kartikeya Badola, NoraKassner, Subhrajit Roy, Ethan Dyer, Vctor Cam-pos, Alex Tomala, Yunhao Tang, Dalia El Badawy,Elspeth White, Basil Mustafa, Oran Lang, Ab-hishek Jindal, Sharad Vikram, Zhitao Gong, Sergi Caelles, Ross Hemsley, Gregory Thornton, Fangxi-aoyu Feng, Wojciech Stokowiec, Ce Zheng, PhoebeThacker, aglar nl, Zhishuai Zhang, Moham-mad Saleh, James Svensson, Max Bileschi, PiyushPatil, Ankesh Anand, Roman Ring, Katerina Tsihlas,Arpi Vezer, Marco Selvi, Toby Shevlane, Mikel Ro-driguez, Tom Kwiatkowski, Samira Daruki, KeranRong, Allan Dafoe, Nicholas FitzGerald, KerenGu-Lemberg, Mina Khan, Lisa Anne Hendricks,Marie Pellat, Vladimir Feinberg, James Cobon-Kerr, Tara Sainath, Maribeth Rauh, Sayed HadiHashemi, Richard Ives, Yana Hasson, YaGuangLi, Eric Noland, Yuan Cao, Nathan Byrd, Le Hou,Qingze Wang, Thibault Sottiaux, Michela Paganini,Jean-Baptiste Lespiau, Alexandre Moufarek, SamerHassan, Kaushik Shivakumar, Joost van Amers-foort, Amol Mandhane, Pratik Joshi, AnirudhGoyal, Matthew Tung, Andrew Brock, Hannah Shea-han, Vedant Misra, Cheng Li, Nemanja Rakicevic,Mostafa Dehghani, Fangyu Liu, Sid Mittal, JunhyukOh, Seb Noury, Eren Sezener, Fantine Huot, MatthewLamm, Nicola De Cao, Charlie Chen, GamaleldinElsayed, Ed Chi, Mahdis Mahdieh, Ian Tenney, NanHua, Ivan Petrychenko, Patrick Kane, Dylan Scand-inaro, Rishub Jain, Jonathan Uesato, Romina Datta,Adam Sadovsky, Oskar Bunyan, Dominik Rabiej,Shimu Wu, John Zhang, Gautam Vasudevan, EdouardLeurent, Mahmoud Alnahlawi, Ionut Georgescu, NanWei, Ivy Zheng, Betty Chan, Pam G Rabinovitch,Piotr Stanczyk, Ye Zhang, David Steiner, SubhajitNaskar, Michael Azzam, Matthew Johnson, AdamPaszke, Chung-Cheng Chiu, Jaume Sanchez Elias,Afroz Mohiuddin, Faizan Muhammad, Jin Miao,Andrew Lee, Nino Vieillard, Sahitya Potluri, JanePark, Elnaz Davoodi, Jiageng Zhang, Jeff Stanway,Drew Garmon, Abhijit Karmarkar, Zhe Dong, JongLee, Aviral Kumar, Luowei Zhou, Jonathan Evens,William Isaac, Zhe Chen, Johnson Jia, AnselmLevskaya, Zhenkai Zhu, Chris Gorgolewski, PeterGrabowski, Yu Mao, Alberto Magni, Kaisheng Yao,Javier Snaider, Norman Casagrande, Paul Sugan-than, Evan Palmer, Geoffrey Irving, Edward Loper,Manaal Faruqui, Isha Arkatkar, Nanxin Chen, IzhakShafran, Michael Fink, Alfonso Castao, Irene Gian-noumis, Wooyeol Kim, Mikoaj Rybinski, AshwinSreevatsa, Jennifer Prendki, David Soergel, AdrianGoedeckemeyer, Willi Gierke, Mohsen Jafari, MeenuGaba, Jeremy Wiesner, Diana Gage Wright, YawenWei, Harsha Vashisht, Yana Kulizhskaya, Jay Hoover,Maigo Le, Lu Li, Chimezie Iwuanyanwu, Lu Liu,Kevin Ramirez, Andrey Khorlin, Albert Cui, TianLIN, Marin Georgiev, Marcus Wu, Ricardo Aguilar,Keith Pallo, Abhishek Chakladar, Alena Repina, Xi-hui Wu, Tom van der Weide, Priya Ponnapalli, Car-oline Kaplan, Jiri Simsa, Shuangfeng Li, OlivierDousse, Fan Yang, Jeff Piper, Nathan Ie, MinnieLui, Rama Pasumarthi, Nathan Lintz, Anitha Vi-jayakumar, Lam Nguyen Thiet, Daniel Andor, PedroValenzuela, Cosmin Paduraru, Daiyi Peng, Kather-ine Lee, Shuyuan Zhang, Somer Greene, Duc DungNguyen, Paula Kurylowicz, Sarmishta Velury, Se-bastian Krause, Cassidy Hardin, Lucas Dixon, LiliJanzer, Kiam Choo, Ziqiang Feng, Biao Zhang,Achintya Singhal, Tejasi Latkar, Mingyang Zhang, Quoc Le, Elena Allica Abellan, Dayou Du, Dan McK-innon, Natasha Antropova, Tolga Bolukbasi, OrgadKeller, David Reid, Daniel Finchelstein, Maria AbiRaad, Remi Crocker, Peter Hawkins, Robert Dadashi,Colin Gaffney, Sid Lall, Ken Franko, Egor Filonov,Anna Bulanova, Rmi Leblond, Vikas Yadav, ShirleyChung, Harry Askham, Luis C. Cobo, Kelvin Xu,Felix Fischer, Jun Xu, Christina Sorokin, Chris Al-berti, Chu-Cheng Lin, Colin Evans, Hao Zhou, AlekDimitriev, Hannah Forbes, Dylan Banarse, ZoraTung, Jeremiah Liu, Mark Omernick, Colton Bishop,Chintu Kumar, Rachel Sterneck, Ryan Foley, RohanJain, Swaroop Mishra, Jiawei Xia, Taylor Bos, Ge-offrey Cideron, Ehsan Amid, Francesco Piccinno,Xingyu Wang, Praseem Banzal, Petru Gurita, HilaNoga, Premal Shah, Daniel J. Mankowitz, AlexPolozov, Nate Kushman, Victoria Krakovna, SashaBrown, MohammadHossein Bateni, Dennis Duan,Vlad Firoiu, Meghana Thotakuri, Tom Natan, An-had Mohananey, Matthieu Geist, Sidharth Mudgal,Sertan Girgin, Hui Li, Jiayu Ye, Ofir Roval, ReikoTojo, Michael Kwong, James Lee-Thorp, Christo-pher Yew, Quan Yuan, Sumit Bagri, Danila Sinopal-nikov, Sabela Ramos, John Mellor, Abhishek Sharma,Aliaksei Severyn, Jonathan Lai, Kathy Wu, Heng-Tze Cheng, David Miller, Nicolas Sonnerat, DenisVnukov, Rory Greig, Jennifer Beattie, Emily Cave-ness, Libin Bai, Julian Eisenschlos, Alex Korchem-niy, Tomy Tsai, Mimi Jasarevic, Weize Kong, PhuongDao, Zeyu Zheng, Frederick Liu, Fan Yang, RuiZhu, Mark Geller, Tian Huey Teh, Jason Sanmiya,Evgeny Gladchenko, Nejc Trdin, Andrei Sozanschi,Daniel Toyama, Evan Rosen, Sasan Tavakkol, Lint-ing Xue, Chen Elkind, Oliver Woodman, John Car-penter, George Papamakarios, Rupert Kemp, SushantKafle, Tanya Grunina, Rishika Sinha, Alice Tal-bert, Abhimanyu Goyal, Diane Wu, Denese Owusu-Afriyie, Cosmo Du, Chloe Thornton, Jordi Pont-Tuset, Pradyumna Narayana, Jing Li, Sabaer Fatehi,John Wieting, Omar Ajmeri, Benigno Uria, Tao Zhu,Yeongil Ko, Laura Knight, Amlie Hliou, NingNiu, Shane Gu, Chenxi Pang, Dustin Tran, YeqingLi, Nir Levine, Ariel Stolovich, Norbert Kalb, Re-beca Santamaria-Fernandez, Sonam Goenka, WennyYustalim, Robin Strudel, Ali Elqursh, Balaji Laksh-minarayanan, Charlie Deck, Shyam Upadhyay, HyoLee, Mike Dusenberry, Zonglin Li, Xuezhi Wang,Kyle Levin, Raphael Hoffmann, Dan Holtmann-Rice, Olivier Bachem, Summer Yue, Sho Arora,Eric Malmi, Daniil Mirylenka, Qijun Tan, ChristyKoh, Soheil Hassas Yeganeh, Siim Pder, StevenZheng, Francesco Pongetti, Mukarram Tariq, Yan-hua Sun, Lucian Ionita, Mojtaba Seyedhosseini,Pouya Tafti, Ragha Kotikalapudi, Zhiyu Liu, An-mol Gulati, Jasmine Liu, Xinyu Ye, Bart Chrzaszcz,Lily Wang, Nikhil Sethi, Tianrun Li, Ben Brown,Shreya Singh, Wei Fan, Aaron Parisi, Joe Stanton,Chenkai Kuang, Vinod Koverkathu, Christopher A.Choquette-Choo, Yunjie Li, TJ Lu, Abe Ittycheriah,Prakash Shroff, Pei Sun, Mani Varadarajan, Sanaz Ba-hargam, Rob Willoughby, David Gaddy, Ishita Das-gupta, Guillaume Desjardins, Marco Cornero, BronaRobenek, Bhavishya Mittal, Ben Albrecht, AshishShenoy, Fedor Moiseev, Henrik Jacobsson, Alireza Ghaffarkhah, Morgane Rivire, Alanna Walton, Cl-ment Crepy, Alicia Parrish, Yuan Liu, ZongweiZhou, Clement Farabet, Carey Radebaugh, PraveenSrinivasan, Claudia van der Salm, Andreas Fidje-land, Salvatore Scellato, Eri Latorre-Chimoto, HannaKlimczak-Plucinska, David Bridson, Dario de Ce-sare, Tom Hudson, Piermaria Mendolicchio, LexiWalker, Alex Morris, Ivo Penchev, Matthew Mauger,Alexey Guseynov, Alison Reid, Seth Odoom, LuciaLoher, Victor Cotruta, Madhavi Yenugula, DominikGrewe, Anastasia Petrushkina, Tom Duerig, AntonioSanchez, Steve Yadlowsky, Amy Shen, Amir Glober-son, Adam Kurzrok, Lynette Webb, Sahil Dua, DongLi, Preethi Lahoti, Surya Bhupatiraju, Dan Hurt, Ha-roon Qureshi, Ananth Agarwal, Tomer Shani, MatanEyal, Anuj Khare, Shreyas Rammohan Belle, LeiWang, Chetan Tekur, Mihir Sanjay Kale, JinliangWei, Ruoxin Sang, Brennan Saeta, Tyler Liechty,Yi Sun, Yao Zhao, Stephan Lee, Pandu Nayak, DougFritz, Manish Reddy Vuyyuru, John Aslanides, NidhiVyas, Martin Wicke, Xiao Ma, Taylan Bilal, Ev-genii Eltyshev, Daniel Balle, Nina Martin, HardieCate, James Manyika, Keyvan Amiri, Yelin Kim,Xi Xiong, Kai Kang, Florian Luisier, Nilesh Tripu-raneni, David Madras, Mandy Guo, Austin Waters,Oliver Wang, Joshua Ainslie, Jason Baldridge, HanZhang, Garima Pruthi, Jakob Bauer, Feng Yang, Ri-ham Mansour, Jason Gelman, Yang Xu, GeorgePolovets, Ji Liu, Honglong Cai, Warren Chen, Xi-angHai Sheng, Emily Xue, Sherjil Ozair, Adams Yu,Christof Angermueller, Xiaowei Li, Weiren Wang, Ju-lia Wiesinger, Emmanouil Koukoumidis, Yuan Tian,Anand Iyer, Madhu Gurumurthy, Mark Goldenson,Parashar Shah, MK Blake, Hongkun Yu, AnthonyUrbanowicz, Jennimaria Palomaki, Chrisantha Fer-nando, Kevin Brooks, Ken Durden, Harsh Mehta,Nikola Momchev, Elahe Rahimtoroghi, Maria Geor-gaki, Amit Raul, Sebastian Ruder, Morgan Red-shaw, Jinhyuk Lee, Komal Jalan, Dinghua Li, GingerPerng, Blake Hechtman, Parker Schuh, Milad Nasr,Mia Chen, Kieran Milan, Vladimir Mikulik, TrevorStrohman, Juliana Franco, Tim Green, Demis Has-sabis, Koray Kavukcuoglu, Jeffrey Dean, and OriolVinyals. 2023. Gemini: A family of highly capablemultimodal models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprint",
  "AData Construction Details": "The details of each biographical data entry are sam-pled independently and randomly from a uniformdistribution. Birthday information has 2001228choices, while all other features have 100 choices.The names of these characters do not overlapwith celebrities to ensure that knowledge in thebase dataset does not conflict with the models ex-isting knowledge. Moreover, there is some correla-tion between graduation school and major, as wellas work company and work city, to prevent the in-troduction of counterfactual knowledge. All of theabove characterization information was generatedby GPT4.",
  "CTest Data Construction": "We used the same set of templates to constructtest statements in almost all experiments and in allsettings in our paper. The test templates we usedare shown in .In order to verify whether the similarity betweenthe style of the test statements and the style of thetraining statements has a decisive influence on thefinal results, this work also constructed novel styletest statements. The novel style test statements areshown in .",
  "D.1Data Construction": "In the training data testing experiments, we do notintroduce conflicts, but instead directly allow themodel to be trained on data with a single text fea-ture. Thus, the dataset in this section can be simplyrepresented by IA = T iA(k)5i=1, where TA denotesthe template with the current text feature A to be",
  "D.3Evaluation": "We measure the effectiveness of the model in learn-ing the training data by the accuracy with whichthe model completes multiple choice questions re-lated to the training data. Specifically, we constructa test set {(s, sa, sb, sc)}N1 , where each piece ofdata in the test set contains four statements. s is thestatement that is consistent with the training datarepresentation, whereas sa, sb, sc are the incorrectchoices constructed with random data, and N isthe size of the test set. We then used perplexity toexamine the proportion of models that preferred s.",
  "EResults of multiple-style comparison": "In real training scenarios, the LLMs may face farmore sources of conflict than the two styles. Inorder to investigate whether the models aforemen-tioned preferences exist when multiple styles allconflict on the same knowledge, we conduct ex-periments on 10 different styles simultaneously.All styles describe the same characters, but thecharacter attributes are all different. We evaluatethe percentage of attributes corresponding to eachstyle as having the highest probability of output,as shown in . As can be seen from thefigure, the model preference remains, i.e. the moreformal styles such as textbooks style, newspapers"
}