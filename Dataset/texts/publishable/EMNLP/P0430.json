{
  "Abstract": "With contributions from the open-source com-munity, a vast amount of instruction tuning(IT) data has emerged. Given the significantresource allocation required for training andevaluating models, it is advantageous to havean efficient method for selecting high-qualityIT data. However, existing methods for instruc-tion data selection have limitations such as re-lying on fragile external APIs, being affectedby biases in GPT models, or reducing the di-versity of the selected instruction dataset. Inthis paper, we propose an industrial-friendly,expert-aligned and diversity-preserved instruc-tion data selection method: Clustering andRanking (CaR). CaR employs a two-step pro-cess: first, it ranks instruction pairs using ahigh-accuracy (84.25%) scoring model alignedwith expert preferences; second, it preservesdataset diversity through clustering.In ourexperiment, CaR efficiently selected a mere1.96% of Alpacas IT data, yet the resulting Al-paCaR model surpassed Alpacas performanceby an average of 32.1% in GPT-4 evaluations.Moreover, we find that data selecting is a con-sistent paradigm whether the pre-trained modelis more capable or the model parameters scal-ing up. Our approach employs compact modelswith 550M parameters and incurs just 11.2%of the financial outlay of current methods, en-hancing its industrial deployability.",
  "Introduction": "Language Models (LMs) acquire the capability tofollow instructions through Instruction Tuning (IT)(Radford et al., 2019; Brown et al., 2020; Zhanget al., 2023), which aligns Large Language Mod-els (LLMs) with critical human standards suchas security, privacy, and legal compliance. Self-instruct proposes a novel methodology that utilizesLMs to construct IT datasets (Wang et al., 2022),",
  "Instruction Tuning Dataset Size": "52k9k70k1k 7B13B30B::: AlpaCaR 30B AlpaCaR 13B AlpaCaR 7B Alpaca 30B Alpaca 13B Alpaca 7B Pre-trained LLaMA size Vicuna 7BAlpaca Cleaned 7B Alpaca PandaLM 7B Alpagasus 30B Alpagasus 13B Alpagasus 7B : Alpaca 52k select by CaR: Alpaca 52k select by GPT-3.5: Alpaca 52k : Alpaca cleaned : SharedGPT : Alpaca 52k select by PandaLM Instruction tuning dataset : Compares the performance of the proposedAlpaCaR model to established baseline models overfour test sets. Our AlpaCaR achieves the best modelperformance with the smallest amount of instructiontuning data. greatly improving the efficiency of instruction gen-eration. Alpaca leveraged a similar strategy (Taoriet al., 2023), utilizing text-davinci-003 to con-struct the Alpaca_52k dataset, and subsequent ITon LLaMA-7B model (Touvron et al., 2023) led tothe creation of Alpaca.Despite these advancements, the quality of in-structions remains paramount over their quantity.Zhou et al. (2023) carefully curated 1,000 instruc-tions, ensuring data quality and diversity by hu-man being, resulting in LIMA model significantlyoutperforming the Alpaca. Nevertheless, creatinghigh-quality instruction sets through manual anno-tation is both time-consuming and labor-intensive(Chiang et al., 2023). A promising approach to mit-igate this challenge involves filtering a small subsetof high-quality and diverse instructions from thevast amounts of existing instruction data.Alpagasus (Chen et al., 2023) introduced a",
  "%72.44%63.19%57.48%78.12%45.00%65.00%56.25%": ": Accuracy of the IQS, CometInstruct and GPTmodels on test sets. Reflecting the alignment of themodel with human preferences in the task of InstructionPairs Quality Estimation. The second row presents re-sults for instruction pairs sourced from the IQE test set,while the third row shows acc on instruction pairs fromVicuna_80, demonstrating the models generalization toother distributions, see more details in Appendix C.1.The IQS and CometInstruct model were fine-tuned asdescribed in Appendix C.2, while the GPT model usedprompts referenced in the Appendix B.2. straightforward yet effective method that utilizesGPT-3.5-Turbo to filter roughly 9k instructions,surpassing Alpacas performance. However, thisapproach overlooks data diversity, and GPTs evalu-ations rated 17.3% instruction pairs generated bytext-davinci-003 above 4.5 and 74.9% above4.0, demonstrating GPTs self-enhancement biasZheng et al. (2023), rendering it unsuitable for as-sessing instructions generated by models withinthe same series. Therefore, more authentic humanpreferences should be used to filter instruction sets.Moreover, relying on fragile and expensive exter-nal GPT APIs limits Alpagasus in industrial de-ployment, especially in low-computation resourcescenarios.In this work, we propose an effective and ef-ficient method for selecting instruction pairs Clustering and Ranking (CaR). CaR consists oftwo steps. The first is ranking through qualityestimation on instruction pairs, where an expert-aligned scoring model (with 550M parametersonly) achieves an accuracy of 84.25% with expertpreferences. Then, a clustering step ensures theoverall diversity of the dataset, minimizing poten-tial capability gaps. Our contributions are summa-rized as follows: We introduce Instruction Pair Quality Esti-mation (IQE), a new stage before IT processwhich aims to use the assessment results ofinstruction datasets as an aid for the actualfine-tuning of language models and evaluationon benchmarks, reducing the time and com-putational expenses for model performancevalidation in IT process by over 90%.",
  "We propose a novel quality evaluationparadigm for IT dataset that is independent": "of external APIs and aligns well with humanexperts preferences. As shown in ,our small Instruction pair Quality Scoring(IQS) model, compared to GPT-4, achievesa 21.05% improvement in aligning with hu-man preferences for data quality. We propose CaR, an instruction selectionmethod that aligns with expert insights andpreserves diversity, showcasing significant en-hancements in model performance and train-ing efficiency. As shown in , CaR usesa small model to filter high-quality instructiondata, achieving an average performance ex-ceeding Alpaca by about 13.3% to 32.8% onthe Alpaca_52k dataset using only a 1.96%subset of instructions. This implies a reduc-tion of 98% in training time and resources. In section 5, experiments found that the dataselecting paradigm is effective even with moreadequate pre-training (LLaMA 1LLaMA 3)or model parameter scaling (7B30B). How-ever, data selecting methods at higher dataquality, such as Alpaca-GPT4 (Peng et al.,2023), are still challenging.",
  "Our work is motivated by the challenges of dataquality in instruction tuning and the limitations ofexisting approaches": "From Quality Estimation to Instruction PairQuality Estimation.Quality estimation is a cru-cial task in machine translation (MT), enabling theassessment of MT models effectiveness and theselection of high-quality translations for specificpurposes, such as manual post-editing. Similarly,LLMs IT process faces the challenge of rapidlyshifting from rare to abundant instruction pairs withinconsistent quality. Ensuring the quality of ITdatasets presents a significant challenge, necessitat-ing adjustments to the pre-trained model, executinginference on test datasets, and undergoing evalua-tion by LLM or human annotators. These processesare not only time-intensive but also demand con-siderable computational resources. To address this, we propose a paradigm shift from evaluating modelperformance to assessing IT datasets via IQE. Ourgoal is to perform a coarse screening of a largenumber of instructions using IQE, followed by re-fining and selecting the optimal LLM with minimaldatasets to reduce the overall computational costassociated with instruction filtering and verifica-tion. GPT as a Judge Exhibits Systematic Bias.Re-searchers often use GPT preferences as a proxyfor human preferences in scenarios requiring hu-man feedback, due to time and cost considerations(Zhou et al., 2023; Rafailov et al., 2023; Duboiset al., 2023; Lee et al., 2023). However, GPT-4 hasbeen shown to exhibit systemic biases in its eval-uations, including positional bias, verbosity bias,and self-enhancement bias (Zheng et al., 2024a;Wang et al., 2023a). While researchers generallyview Alpaca 52k as needing improvement (Alpaca-DataCleaned 2 ; Liu et al., 2023b), GPTs evalua-tions rated 9k instruction pairs above 4.5 and 39kabove 4.0. Introducing more realistic human prefer-ences for instruction filtering could further enhancemodel performance. Instruction Diversity Inspires LLMs Multi-tasks Capability.Recent studies have high-lighted the importance of data diversity in im-proving the performance of LLMs (Zhou et al.,2023; Chen et al., 2023). Dong et al. (2023) foundthat combining training data from various tasksboosts LLMs performance in low-resource scenar-ios. Inspired by these findings, we posit that inte-grating instructions from different tasks enhancesLLMs capabilities in low-resource settings. Con-sequently, ensuring the diversity of the IT datasetis paramount, particularly when dealing with large-scale models and limited high-quality data for eachtask.",
  "Clustering and Ranking Method": "Considering the aforementioned motivations, wepropose a straightforward yet effective data selec-tion framework, Cluster and Ranking, which in-tegrates the dimensions of quality and diversity.Inspired by Zhou et al. (2023)s work, we first se-lect a subset that ensures the retention of a largenumber of high-quality instructions, then supple-ment a small number of high-quality instructionsfrom each cluster to enhance data diversity while preserving instruction quality. As illustrated in , the framework begins by evaluating the entiredataset using the IQS model, assigning a scoreito each instruction pairi. Subsequently, the clus-ter model is employed to partition all candidateinstruction pairs into k clusters. Finally, all instruc-tion pairs are sorted based on their scores, and thetop n1 pairs are selected; Within each cluster, thetop n2 pairs are chosen based on their scores. Theresulting high-quality sub-dataset with preserveddiversity is curated by deduplicating n1 + k n2pairs of instructions and is intended for the trainingof AlpaCaR.Sections 2.3 and 2.4 provide a comprehensivediscussion of the ranking and clustering method-ologies implemented in CaR.",
  "Single Instruction Pair Quality Estimation": "To explore the IQE task, we adapt the Comet frame-work (Rei et al., 2020) and develop a suitable frame-work for leveraging expert preference. Our trainingdata is derived from expert-revised dataset (Liuet al., 2023b), consisting of 3,751 instruction pairsfrom Alpaca_52k that were refined by linguisticexperts to enhance fluency, accuracy, and seman-tic coherence between questions and responses.We categorize unedited instructions and responsesfrom text-davinci-003 as GPT Preference, andexpert-revised instructions as Expert Preference.To enable the model to discern features across thesecategories, we curated 2,541 markedly distinct in-structions from the expert-revised dataset, ensuringan edit distance above a small threshold. Theseinstruction pairs are then randomly allocated theminto training, validation, and test sets following an8:1:1 distribution.Initially, we experimented with the translationranking model architecture from the Comet frame-work to leverage the paired annotations in expert-revised better. In (left), Cometinstruct op-timizes the model using instruction and input asanchors, minimizing semantic distance to human-preferred responses while maximizing distance toGPT-generated outputs. This approach achieves72.44% accuracy on the test set but fails to fullyleverage the improvements about Input made byexperts. To address this, as illustrated in (right), we retained the pre-trained XLM-RoBERTalarge in Cometinstruct and directly concatenatedthe instruction pair components to train the IQSmodel. As shown in , our IQS model out-performs GPT-3.5 (version: GPT-3.5-Turbo) and",
  "Training Alpaca": "Top n2 *k instructions Training AlpaCaR Alpaca 52k Top n1 instructions : An overview of Cluster and Ranking (CaR) method. Unlike directly training Alpaca with the entireAlpaca_52k dataset, CaR first uses the IQS model to score all instructions (brown arrow). Then it selects the topn1 instructions ranked by quality. Next, a clustering model (violet arrow) groups all instructions into k clusters,selecting n2 from each. These are concatenated and deduplicated to form a diverse, high-quality sub-dataset fortraining AlpaCaR. GPT-4 (version: GPT-4-1106-preview). Furtheranalysis reveals that GPT-4 favors original instruc-tions in 62.2% of incorrect cases, showing that evenadvanced GPT models often prefer GPT-aligned in-structions. Additionally, GPT-4 struggles to recog-nize nuanced semantic changes made by experts in37.8% of incorrect cases, revealing its difficulty inrecognizing expert and nuanced semantic changeswith minimal adjustments. Despite GPT-4s strongalignment with human preferences in most generaltasks, its subpar performance on the expert-reviseddataset highlights a subtle gap between expert pref-erences and GPT preferences.",
  "Diversity": "Within the instruction filtering framework, it isimperative to filter out a minimal subset of datafrom a vast array of instructions, resulting in a lim-ited number of instructions per task. In such low-resource scenarios, Dong et al. (2023) has demon-strated that blending training data from varioustasks enhances the LLMs proficiency across differ-ent abilities. Intuitively, by assigning a task labelto each instruction pair, we can preserve instruc-tion pairs associated with a broader range of tasks,thereby facilitating cross-task instruction synergyand enhancing model performance. To determinetask labels for instruction pairs, we evaluated man-ual labeling, classification models, and clusteringmodels, selecting clustering for our study. Manuallabeling, though more accurate, is labor-intensive and less adaptable to various datasets. We hypothe-size that instruction pairs within the same task aresemantically close, allowing their distribution tobe learned via classification models. Nonetheless,such models may struggle with flexibility whenfaced with out-of-domain data.To enhance the methods versatility, we optedfor an unsupervised clustering-based approach topreserve data diversity. A clustering algorithm canidentify semantically close instruction pairs andform clusters for different tasks. Moreover, thischoice allows for efficient adaptation to differentdatasets without retraining from scratch by form-ing new clusters when encountering out-of-domaininstruction pairs.Regarding the clustering methodology, we em-ploy the k-Means algorithm. Initially, a sentence-transformers model is used to map sentences to a384-dimensional dense vector space. Subsequently,semantic features are PCA-reduced to retain 95%of dimensions. Finally, by setting the number ofclusters as k =",
  "Test Datasets": "To avoid confusion arising from the similarityin naming between models and datasets, we usethe format ModelName_DatasetSize to repre-sent datasets.Following previous methodolo-gies, we assess four datasets: Self-instruct_252(Li et al., 2023b), Vicuna_80 (Chiang et al.,2023), PandaLM_170 (Wang et al., 2023b), andCoachLM_150 (Liu et al., 2023b). This approachcovers a broader range of instructions, minimizingevaluation bias.",
  "#all , where #all is thenumber of test set samples; (3) QS, a quality scorethat measures the ratio of responses reaching thereference level, formulated as QS= #win+#tie": "#all.Evaluation Approach: (1) GPT-4 Turbo, cur-rently the most powerful LLM widely used to re-place manual responses quality assessments, withprompts designed by Chiang et al. (2023). How-ever, this method faces limitations due to APIdependency and inherent biases. (2) PandaLM,an open-source evaluation model that can be de-ployed locally, providing efficient LLM assess-ments (Wang et al., 2023b). Trained on 300k sam-ples using GPT-3.5, it effectively mitigates biasesand achieves 88.3% of GPT-4s evaluation capabil-ity. (3) Human, three experts with an average of12.57 years of experience independently conductedcomparisons based on the criteria in Appendix EAfter comprehensive consideration, we use the eval-uation results of PandaLM to measure the modelsinstruction-following ability in most experiments,while some key principal experiments utilize GPT-4 and human for assessment. The prompt for GPT-4s evaluation is designed by Chiang et al. (2023),as detailed in the Appendix B.1.",
  "Comparison with Baselines": "We conduct a comparative analysis of two estab-lished baseline LLMs, Alpaca and Vicuna, whichwere fine-tuned using 52,000 text instructionsthrough text-davinci-003 and 70,000 ChatGPT di-alogues, respectively. Furthermore, we explorethree models that advance upon Alpaca: Alpaca-PandaLM and Alpaca-cleaned, which employ in-structional enhancement methods, and Alpagasus,which incorporates an instruction filtering method.All models were trained with identical hyperparam-eter settings. As delineated in , AlpaCaR,at the 7B scale, outperforms not only the foun-dational models of Alpaca and Vicuna but alsoAlpaca-PandaLM, Alpaca-cleaned, and Alpaga-sus. Overall, AlpaCaR achieves significant per-formance improvements over Alpaca across the 7B,13B, and 30B scales, validating the efficacy of theCaR method. The notable performance gains ofAlpaCaR, accomplished with reduced data usagecompared to Alpagasus, underscore the importanceof leveraging high-quality human preferences anddata diversity in enhancing model performance.",
  "Reliability of IQE Results": "To verify whether the IQE results genuinely reflectthe performance of LLMs after IT, we examined thecorrelation between scores given by the IQS modeland the performance of fine-tuned LLMs on testsets. Given that Alpagasus obtained 9k instructionsrated above 4.5 using GPT-3.5-Turbo, we simi-larly selected the top 9k instructions ranked by IQSmodel and Comet model. We then calculated theaverage score for the three IT sub-datasets using theIQS model, fine-tuned LLaMA-7B, and tested itsperformance by averaging models winning scoreson four datasets against reference. As illustratedin , the average IQS score and the fine-tunedmodels performance are generally consistent, in-dicating that IQE results can approximately reflect",
  "Ablation Study": "Quality Dimension.To illustrate the significanceof data quality, we employed the IQS models scoreto rank 52,000 instructions. Subsequently, we ex-tracted subsets of the top 1,000, 2,000 and up to42,000 instructions to train LLaMA-7B. In ,the horizontal axis represents the size of instruc-tion dataset, where a higher count signifies moreinstructions of relatively lower quality, while thevertical axis shows the winning score relative toAlpaca. The results indicate that models trainedwith selected data generally surpass the one trainedwith the entire dataset. As more instructions of rel-atively lower quality are included, the performanceof the LLM generally declines. Remarkably, themodel approaches its optimal performance with amere 1,000 high-quality IT data. Therefore, in theCaR method, we select n1 = 1000 instructions toensure the chosen IT sub-dataset is of high quality.",
  ": Compare AlpaCaR with baselines, includingAlpaca and randomly selected 1k instructions": "demonstrates that, compared to using only1k high-quality data selected by IQS model, theCaR method enhances performance when a smallnumber of samples (up to 5) are selected from eachcluster. Selecting too many samples can negativelyimpact the overall quality of the IT sub-dataset andthe performance of the LLMs. Moreover, the CaRmethod achieves nearly optimal performance byselecting n2 = 1 sample from each cluster, thusenhancing the diversity of the IT sub-dataset. Importance of Diversity.An ideal IT datasetshould encompass a rich variety of data, but deter-mining the optimal number of instructions per clus-ter required for the model to effectively correspondto the task remains a challenge. We designed exper-iments to demonstrate the importance of diversityand explore values of n2, the trade-off between thenumber and quality of samples per cluster.Designing strict ablation experiments in this con-text is challenging due to the difficulty in ensuringconsistent instruction set quality while maintainingthe same number of instructions. To explore this,we established three experimental groups with in-creasing diversity (baseline: reference response).In , the winning rates on the Self-Instructand Vicuna test sets show that models with morediverse instruction sets perform better.",
  "Compare with Random & GPT-4 Result": "presents the results of ablation experiments,revealing that randomly selecting 1,017 instructionpairs from 52k dataset leads to a decrease in modelperformance compared to Alpaca.In contrast,the instruction pairs selected by the CaR methodshow significant improvements at 7B (29.8%), 13B(32.7%), and 30B (33.1%) scales.Furthermore, to address cost considerations, weemployed GPT-4s evaluation framework exclu-sively on four datasets to compare AlpaCaR againstAlpaca. As depicted in and elaborated uponin Appendix D, GPT-4 exhibited similar evaluativeoutcomes: AlpaCaR outperformed baseline in themajority of instances, thereby substantiating the ef-ficacy of the CaR method. Employing CaR, whichinvolves selecting 1.96% of the dataset, has provento yield superior preferences across a variety ofparameter scales.",
  "Human Evaluation": "We have formulated detailed evaluation criteria,covering seven aspects: fluency, relevance, correct-ness, consistency, satisfaction, informativeness andsecurity, which are further categorized into 27 pri-mary and 58 secondary classifications. Additionaldetails are provided in Appendix E.We compared AlpaCaR 30B vs. Alpaca 30B onVicuna_80 test set. The human evaluation resultsdemonstrated that AlpaCaR performed at least aswell as Alpaca across all categories and was pre-ferred by language experts in the vast majority ofcases. The specific results are shown in . in Appendix F displays case study fromthe math category. We found that under strict eval-uation criteria, experts believed that neither modelprovided the correct final answer, resulting in atie. However, a more detailed analysis reveals thatAlpaCaR utilized CoT to explore the correct rea-soning steps, although errors occurred after certainsteps. In contrast, Alpaca simply provided a con-",
  "Larger Instruction Tuning Datasets": "To further explore the performance of CaR in moremassive and complex datasets, we conducted ad-ditional experiments on even larger instructiondatasets. Following recent work (Du et al., 2023;Liu et al., 2023a), we combined five instruction tun-ing datasets, including Alpaca, Dolly_v2 (Conoveret al., 2023), Alpaca-evol-instruct (Xu et al., 2023),HC3 (Guo et al., 2023), and LIMA (Zhou et al.,2023), to obtain a large-mixed-dataset containing181,253 instructions. Then we used CaR to fil-ter the large-mixed dataset and obtained CaR_50kcontaining 50k instructions. shows that the model fine-tuned on 50kinstructions selected by CaR outperforms Alpacaat the same number of instructions using LLaMA 27B as the base pre-trained model. In addition, themodel fine-tuned using CaR_50k outperforms theone using mixed-181k instruction tuning dataset.This illustrates that the bottleneck of Alpacais not that pre-trained LLaMA cannot learn moreknowledge from more instructions, but rather that",
  "Cost Comparison": "Here, we compare the computational costs of Al-paCaR, Alpaca, and Alpagasus, focusing on in-struction evaluation and full parameter fine-tuningat the 30B scale, as detailed in . For in-struction evaluation using an API-based method,we refer to the official pricing 3, while for modeltraining or inference, we consider the rental costsof GPUs 4. In summary, training AlpaCaR sig-nificantly saves both time and costs, compared toAlpaca or Alpagasus.",
  "Is the Benefit Derived from DataSelecting Universally Applicable?": "Filtering a high quality instruction sub-dataset tosupervised fine-tuning LLaMA 1 significantly re-duces computational cost and effectively improvesLLM performances. More crucially, it is essentialto ascertain whether data screening constitutes aconsistent paradigm for performance enhancement,particularly as pre-trained model become increas-ingly powerful and model parameters scaling up.In this section, we used the average WS on Vi-cuna_80 and Self-instruct_252 test set to explorethe generalization of data selection. A consistent paradigm when pre-training ismore adequate?Base pre-trained LLMs ac-quire knowledge through pre-training. LLaMA1, LLaMA 2, and LLaMA 3 were pre-trained us-ing 1T, 2.4T, and 15T tokens, respectively. Whenpre-trained models exhibit strong capabilities, canthey discern the quality of fine-tuning instructions,rendering instruction selecting redundant? To in-vestigate this, we employed LLaMA 1 7B, LLaMA",
  ": Impact of data selection as models parametersor instruction quality increase": "2 7B, and LLaMA 3 8B pre-trained models, com-paring fine-tuning using the full dataset or subsetsfiltered by GPT-3.5 Turbo or CaR. shows theresults on Alpaca_52k and Dolly_15k IT datasets.The findings suggest that even as base pre-trainedLLMs become more powerful, models fine-tunedon filtered data surpass those trained on full in-structions. LLaMA 3 8B is more susceptible tolow-quality instructions, impeding its ability to fol-low instructions in downstream tasks. A consistent paradigm when model size scal-ing up?Many new capabilities and phenomenaemerge as the model parameters scaling up. Thusanother question is whether instruction tuning dataselection is still important as the parameters in-crease. We experimented the performance of themodel fine-tuned by full versus selected instruc-tions at the 7B-30B scale, due to limited computa-tional conditions. As shown on the left side of (left), The horizontal direction showed no sig-nificant improvement in model performance evenas the model size increased. However, the verticaldirection showed that the model performs betterusing instructions selected by GPT-3.5 or CaR atall scales.",
  "A consistent paradigm when instructions qual-ity improves?Alpaca-GPT4 (Peng et al., 2023)contains instruction generated by GPT-4 using Al-paca prompts, which quality significantly improved": "compared to Alpaca. Distinguishing high-qualityinstructions remains a challenge when instructionquality generally improves. As depicted in (right), models trained by CaR-selected instruc-tions are inferior to full instructions. We argue thatthe IQS model cannot significantly discriminateinstruction quality in such a high-quality data dis-tribution, so randomly filtering instructions causedperformance degradation similar to . A simi-lar phenomenon occurs when using LLMs to selectinstructions. Qwen1.5-110B-chat and Qwen-maxscored more than 1,800 of the 2,000 instructionsin the Alpaca-GPT4 dataset as perfect score, in-dicating that the quality of the evaluated instruc-tions in this situation approaching the boundariesof the LLMs capabilities. So data selecting meth-ods at higher data quality are still challenging,and maybe gradient-based (Xia et al., 2024) or in-context learning-based (Li et al., 2023c) methodsdemonstrate greater potential.",
  "Conclusion": "In this paper, we focus on exploring and resolv-ing the issue of instruction selection during su-pervised fine-tuning stage. We introduce the CaRmethod and examine two perspectives that are war-rant considered: (1) Evaluating instruction qualityusing more authentic human preferences: modelstrained with data annotated by linguistic expertsshow higher agreement rates and the selected in-structions lead to better-performing models. (2)Instruction diversity inspires LLMs stronger capa-bility: Under our selection framework, preservinga small number of instructions for different tasksthrough cluster improves model performance. Ex-perimental results show that fine-tuning LLaMA(ranging from 7B to 30B parameters) with a 1.96%subset of instructions selected by CaR outperformsmodels trained on full datasets or data selected byGPT. Moreover, data selecting methods using GPT-family or CaR is a consistent paradigm whetherthe pre-trained model is more capable or the modelparameters scaling up, while those at higher dataquality are still challenging. Additionally, our ap-proach can be deployed locally without relying onAPIs, thereby enabling a more efficient instructionselection approach in low-computation resourceenvironments.",
  "Limitation": "Despite the outstanding performance of CaR acrossmultiple test sets, its experiments were confinedto filtering on only several datasets. The diverseformats of different open-source instruction setspose challenges for the academic community inter-ested in instruction filtering tasks. In the future, weplan to validate the effectiveness of CaR on moredatasets such as WizardLM_evol_instruct_70k (Xuet al., 2023). Moreover, while CaR is primarilyused for single-turn dialogue instruction filtering,exploring its application in multi-turn dialogue in-struction filtering presents an attractive directionfor future research.",
  "We reveal the following potential risks of our re-search based on ethical considerations:": "1. Quality of instruction data: While the pro-posed method aims to select high-quality in-struction data, there is still a risk that the se-lected subset may not fully represent the diver-sity and complexity of the entire dataset. Thiscould potentially lead to biased or incompletetraining of models and cause adverse socialimpact. 2. Bias and fairness: As with any AI research,there is a need to ensure fairness and miti-gate biases. The selection process and scoringmodel used in CaR should be carefully moni-tored to prevent any unintentional biases, suchas favoring certain types of instructions or ex-cluding underrepresented groups. 3. Industrial deployment and responsible use: Asthe method is designed for industrial scenar-ios, it is important to consider the responsi-ble use of the developed models. Ensuringthat the models are not used for unethicalpurposes or harmful applications is crucial.Additionally, monitoring and addressing anyunintended consequences or biases that mayemerge during deployment should be a prior-ity.",
  "This work was supported in part by the NationalScience Foundation of China (No.62276056), theNatural Science Foundation of Liaoning Province": "of China (2022-KF-16-01), the Fundamental Re-search Funds for the Central Universities (Nos.N2216016 and N2316002), the Yunnan Fundamen-tal Research Projects (No. 202401BC070021), andthe Program of Introducing Talents of Disciplineto Universities, Plan 111 (No.B16009). Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, KalpaGunaratna, Vikas Yadav, Zheng Tang, Vijay Srini-vasan, Tianyi Zhou, Heng Huang, et al. 2023. Al-pagasus: Training a better alpaca with fewer data.arXiv preprint arXiv:2307.08701. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E. Gonzalez, IonStoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgptquality. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anasta-sios Nikolas Angelopoulos, Tianle Li, Dacheng Li,Hao Zhang, Banghua Zhu, Michael Jordan, Joseph EGonzalez, et al. 2024. Chatbot arena: An open plat-form for evaluating llms by human preference. arXivpreprint arXiv:2403.04132. Xu Chu, Ihab F Ilyas, Sanjay Krishnan, and JiannanWang. 2016. Data cleaning: Overview and emerg-ing challenges. In Proceedings of the 2016 inter-national conference on management of data, pages22012206. Mike Conover, Matt Hayes, Ankit Mathur, XiangruiMeng, Jianwei Xie, Jun Wan, Sam Shah, Ali Gh-odsi, Patrick Wendell, Matei Zaharia, et al. 2023.Free dolly: Introducing the worlds first truly openinstruction-tuned llm. Guanting Dong, Hongyi Yuan, Keming Lu, Cheng-peng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang,Zheng Yuan, Chang Zhou, and Jingren Zhou. 2023.How abilities in large language models are affectedby supervised fine-tuning data composition. arXivpreprint arXiv:2310.05492.",
  "Xian Li, Ping Yu, Chunting Zhou, Timo Schick, LukeZettlemoyer, Omer Levy, Jason Weston, and MikeLewis. 2023b. Self-alignment with instruction back-translation. arXiv preprint arXiv:2308.06259": "Yunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang,Min Yang, Lei Zhang, Shuzheng Si, Junhao Liu,Tongliang Liu, Fei Huang, et al. 2023c. One shotlearning as instruction data prospector for large lan-guage models. arXiv preprint arXiv:2312.10302. Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, andJunxian He. 2023a.What makes good data foralignment?a comprehensive study of automaticdata selection in instruction tuning. arXiv preprintarXiv:2312.15685. Xiaoyong Liu and W Bruce Croft. 2004. Cluster-basedretrieval using language models. In Proceedings ofthe 27th annual international ACM SIGIR confer-ence on Research and development in informationretrieval, pages 186193. Yilun Liu, Shimin Tao, Xiaofeng Zhao, Ming Zhu, Wen-bing Ma, Junhao Zhu, Chang Su, Yutai Hou, MiaoZhang, Min Zhang, et al. 2023b. Automatic instruc-tion optimization for open-source llm instruction tun-ing. arXiv preprint arXiv:2311.13246.",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan,Dario Amodei, Ilya Sutskever, et al. 2019. Languagemodels are unsupervised multitask learners. OpenAIblog, 1(8):9": "Rafael Rafailov, Archit Sharma, Eric Mitchell, StefanoErmon, Christopher D Manning, and Chelsea Finn.2023. Direct preference optimization: Your languagemodel is secretly a reward model. arXiv preprintarXiv:2305.18290. Ricardo Rei, Craig Stewart, Ana C Farinha, and AlonLavie. 2020. COMET: A neural framework for MTevaluation. In Proceedings of the 2020 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP), pages 26852702, Online. Associationfor Computational Linguistics. Yizhou Sun, Jiawei Han, Peixiang Zhao, Zhijun Yin,Hong Cheng, and Tianyi Wu. 2009. Rankclus: in-tegrating clustering with ranking for heterogeneousinformation network analysis. In Proceedings of the12th international conference on extending databasetechnology: advances in database technology, pages565576. Hongyin Tang, Xingwu Sun, Beihong Jin, JingangWang, Fuzheng Zhang, and Wei Wu. 2021. Improv-ing document representations by generating pseudoquery embeddings for dense retrieval. arXiv preprintarXiv:2105.03599.",
  "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, YannDubois, Xuechen Li, Carlos Guestrin, Percy Liang,and Tatsunori B. Hashimoto. 2023. Stanford alpaca:An instruction-following llama model": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro,Faisal Azhar, et al. 2023. Llama: Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processingsystems, 30.",
  "Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, BinghuaiLin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.2023a. Large language models are not fair evaluators.arXiv preprint arXiv:2305.17926": "Yidong Wang, Zhuohao Yu, Zhengran Zeng, LinyiYang, Cunxiang Wang, Hao Chen, Chaoya Jiang,Rui Xie, Jindong Wang, Xing Xie, et al. 2023b.Pandalm: An automatic evaluation benchmark forllm instruction tuning optimization. arXiv preprintarXiv:2306.05087. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-isa Liu, Noah A Smith, Daniel Khashabi, and Han-naneh Hajishirzi. 2022. Self-instruct: Aligning lan-guage model with self generated instructions. arXivpreprint arXiv:2212.10560.",
  "Mengzhou Xia, Sadhika Malladi, Suchin Gururangan,Sanjeev Arora, and Danqi Chen. 2024. Less: Se-lecting influential data for targeted instruction tuning.arXiv preprint arXiv:2402.04333": "Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,Pu Zhao, Jiazhan Feng, Chongyang Tao, and DaxinJiang. 2023.Wizardlm: Empowering large lan-guage models to follow complex instructions. arXivpreprint arXiv:2304.12244. Zhiqiang Yuan, Junwei Liu, Qiancheng Zi, Ming-wei Liu, Xin Peng, and Yiling Lou. 2023.Eval-uating instruction-tuned large language models oncode comprehension and generation. arXiv preprintarXiv:2308.01240.",
  "Daochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, FanYang, Zhimeng Jiang, Shaochen Zhong, and Xia Hu.2023. Data-centric artificial intelligence: A survey.arXiv preprint arXiv:2303.10158": "Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tian-wei Zhang, Fei Wu, et al. 2023. Instruction tuningfor large language models: A survey. arXiv preprintarXiv:2308.10792. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.Judging llm-as-a-judge with mt-bench and chatbotarena. arXiv preprint arXiv:2306.05685. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024a.Judging llm-as-a-judge with mt-bench and chatbotarena. Advances in Neural Information ProcessingSystems, 36. Yaowei Zheng, Richong Zhang, Junhao Zhang, YeYan-han YeYanhan, and Zheyan Luo. 2024b. LlamaFac-tory: Unified efficient fine-tuning of 100+ languagemodels. In Proceedings of the 62nd Annual Meet-ing of the Association for Computational Linguistics(Volume 3: System Demonstrations), pages 400410,Bangkok, Thailand. Association for ComputationalLinguistics.",
  "ARelated work": "Quality Estimation and Comet framework.Quality estimation is a pivotal task in machinetranslation, involving scoring or ranking transla-tion results to select higher-quality data. Comet(Rei et al., 2020) leverages input and referencetranslations to accurately assess translation quality,employing two architectures: the Estimator modeland the Translation Ranking model. The Estima-tor model directly predicts quality scores for eachevaluation instance, while the Translation Rankingmodel learns parameters from paired evaluationdata to predict reasonable quality scores. Algorithm - Data Lifecycle.In the modern eraof deep learning, high-quality data has becomethe cornerstone for training robust and effectivemodels. Over the past decade, there has been agrowing emphasis on the collection and curationof superior data (Chu et al., 2016; Motamedi et al.,2021). The emergence of data-centric AI has un-derscored the belief that data quality is as crucial asalgorithmic advancements within the AI/ML life-cycle (Hajij et al., 2021; Zha et al., 2023). Thisparadigm shift has been particularly evident sincethe introduction of the Transformer architecture(Vaswani et al., 2017), which has revolutionizedthe field of language modeling. Rather than focus-ing on disruptive innovations in model structure,researchers have concentrated on leveraging theeffectiveness of the Transformer architecture bystacking transformer blocks to create more potentmodels. Additionally, significant improvements inmodel performance have been achieved throughthe construction of task-specific datasets and theenhancement of data quality (Zhou et al., 2023;Chen et al., 2023; Li et al., 2023c). Futher perspective of clustering and ranking.Many domains have employed methods similar toclustering and ranking. In information retrieval,Google extensively utilizes the PageRank algo-rithm (Page et al., 1999) to calculate the importanceof hyperlinks between webpages. Liu et al. devel-oped a cluster-based retrieval model by construct-ing language models for clusters (Liu and Croft,2004), combining documents within the same clus-ter and searching/ranking clusters based on querygeneration likelihood. Tang et al. enhanced theBi-encoders performance in dense information re-trieval tasks by using clustering algorithms to gener-ate \"pseudo-query embeddings\" (Tang et al., 2021). Selecting suitable data for LLM inference is cru-cial in the RAG field, as discussed by Yuan et al.(2023) and Mu et al. (2023), who explore methodsfor finding appropriate demonstrations to improveLLM performance. In the network domain, Sunet al. introduced the RankClus framework (Sunet al., 2009), which integrates clustering and rank-ing methods to strengthen heterogeneous informa-tion network analysis. Evaluation of LLMs.Evaluating the open-domain instruction-following capabilities of LLMspresents a significant challenge. Currently, the pre-vailing approach involves employing human evalu-ators or GPT-4 to compare the inference responseof different models. Consequently, recent studies,including PandaLM (Wang et al., 2023b), Vicuna(Chiang et al., 2023), CoachLM (Liu et al., 2023b),and Self-Instruct (Wang et al., 2022), have curatedand provided their own instruction sets to evaluateinstruction-finetuned LLMs. Additionally, leader-boards such as MT-Bench (Zheng et al., 2024a),Alpaca-Eval (Dubois et al., 2023), and ChatbotArena (Chiang et al., 2024) have been establishedto measure the instruction-following abilities ofthese models. PandaLM (Wang et al., 2023b) andAuto-J (Li et al., 2023a) efforts focus on trainingLLMs to provide more impartial and accurate eval-uations. By leveraging these latest advancements,we aim to evaluate our models performance us-ing human-generated instruction sets, ensuring acomprehensive and rigorous assessment of its ca-pabilities in following open-ended instructions.",
  "B.1IQE Prompt": "[The Start of Assistant As Instruction and Answer]{Instruction pair 1}[The End of Assistant As Instruction and Answer][The Start of Assistant Bs Instruction and Answer]{Instruction pair 2}[The End of Assistant Bs Instruction and Answer][System]We would like to request your feedback on the per-formance of two AI assistants in response to the userquestion displayed above. Please rate the helpfulness,relevance, accuracy, level of details of their responses.Each assistant receives an overall score on a scale of1 to 10, where a higher score indicates better overallperformance. Please first output a single line containingonly two values indicating the scores for Assistant 1 and2, respectively. The two scores are separated by a space.In the subsequent line, please provide a comprehensiveexplanation of your evaluation, avoiding any potentialbias and ensuring that the order in which the responseswere presented does not affect your judgment.",
  "B.2Response Comparison Prompt": "[Question]{Instruction}[The Start of Assistant 1s Answer]{Response 1}[The End of Assistant 1s Answer][The Start of Assistant 2s Answer]{Response 2}[The End of Assistant 2s Answer][System]Please act as an impartial judge and evaluate the qual-ity of the responses provided by two AI assistants tothe user question displayed below. You should choosethe assistant that follows the users instructions and an-swers the users question better. Your evaluation shouldconsider factors such as the helpfulness, relevance, ac-curacy, depth, creativity, and level of detail of theirresponses. Begin your evaluation by comparing thetwo responses and provide a short explanation. Avoidany positional biases and ensure that the order in whichthe responses were presented does not influence yourdecision. Do not allow the length of the responses toinfluence your evaluation. Do not favor certain namesof the assistants. Be as objective as possible. Afterproviding your explanation, output your final verdict bystrictly following this format: [[A]] if assistant A isbetter, [[B]] if assistant B is better, and [[C]] for atie.",
  "C.1Evaluation Metric of IQE": "The second row of presents results for in-struction pairs sourced from the IQE test set, whichare instructions revised by language expert. Thethird row shows accuracy on instruction pairs fromVicuna_80, demonstrating the models generaliza-tion to other distributions. The instructions areprovided by the dataset, while language expertsevaluates the quality of two responses generated bydifferent models, establishing the ground truth la-bels. In the calculation of accuracy, if the absolutedifference between the scores of two responses isless than 0.01 assigned by IQS or CometInstruct,the outcome is considered a Tie.",
  "C.2Model Architecture of IQS andCometinstruct": "In the IQE task, the IQS model and Comet modelcorrespond to the Estimator model architectureand Translation Ranking model architecture in theComet framework, respectively. As shown in , The Cometinstruction model concatenates in-structions with input to form anchors. It then feedspairs of better and worse responses into the model.Finally, the model is trained using a triplet marginloss function to distinguish between the superior",
  "\\": "The length of the line segment can be foundusing the formula for the distance betweentwo points:d = (x2 - x1)2 + (y2 - y1)2In this case, x2 = 10, x1 = 2, y2 = 4, and y1 = -2.Plugging these numbers into the formula, we getd = (10 - 2)2 + (4 - (-2))2Similarly, we can calculate the length of the linesegment using the Pythagorean-Pythagorean-Circles Theorem:d = (x2 + y2)In this case, x2 = 10, y2 = 4, sod = (10 + 4) = 5.Therefore, the length of the line segment is 5.",
  "GProfile of Involved Language Experts": "To ensure a comprehensive and rigorous humanevaluation of LLM abilities, we established a col-laboration with the language service center of aprominent international corporation. We recruiteda team of highly educated, multilingual languageexperts with diverse skills in translation, localiza-tion, writing, and testing, who dedicated their full-time efforts to this task. Specifically, three expertspossessing an average experience of 12.57 years,are responsible for conducting a human evaluationof AlpaCaR and other LLMs.",
  "HDiscussion of CaR framework": "Selecting top-n ranked samples for each clusteris indeed an intuitive and interesting idea thatintegrates the two steps of clustering and rank-ing.We have also experimented with this set-ting in our early research. However, a challengearises when the predefined number of clusters k =Numberinstructions/2 = 161 is used. When top-n is small, the resulting dataset size is insufficientfor the model to achieve good instruction-followingcapacity. Conversely, when top-n is large, it intro-duces more low-quality instruction pairs, whichnegatively impacts the performance of LLMs. An",
  ": Discussion of CaR framework: k top-n v.s.n1 + k n2": "early version of our experimental results (baseline:Alpaca 52k) is shown in .The experimental results indicate that this com-binatorial approach performs less effectively thantreating the two components separately. Our ideais to additionally and separately extract the top n1instructions using only the ranking step to ensurethat most high-quality instructions are included (asindicated in section 2.2) while using a smaller topn2 to prevent the inclusion of a large number oflow-quality instruction pairs. Experimenting withdifferent values of k might alleviate this problem,but we aim to propose a more automated processand avoid involving additional hyperparameter tun-ing."
}