{
  "Abstract": "Emotion significantly influences human behav-ior and decision-making processes. We proposea labeling methodology grounded in PlutchiksWheel of Emotions theory for emotion classifi-cation. Furthermore, we employ a Mixture ofExperts (MoE) architecture to evaluate the effi-cacy of this labeling approach, by identifyingthe specific emotions that each expert learnsto classify. Experimental results reveal thatour methodology improves the performance ofemotion classification.",
  "Introduction": "Emotion is essential in human life, having influ-ence on our thoughts, behaviors, and communica-tion. Recognizing the paramount importance ofemotions, researchers have made significant effortsto analyze and understand them (Picard, 1997). Aparticularly important area of this research is emo-tion recognition in text, as it forms a substantialpart of our daily interactions, including email andSocial Network Service (SNS).While sentiment analysis, categorizing text aspositive, negative, or neutral, has advanced signifi-cantly, recognizing the full spectrum of emotions intextsuch as joy, anger, sadness, and fearremainsa challenging task. Mao et al. (2023) report thatRoBERTa large with HG-F24 achieved 84.7% ac-curacy on sentiment analysis of Amazon productreviews but only 40.9% accuracy in emotion detec-tion using a Twitter (X) dataset.Previous research utilizing deep learning tech-nology has demonstrated significant promise in ex-tracting emotions from text (Yu et al., 2018; Bazio-tis et al., 2018; Ying et al., 2019; Li and Xiao, 2023;Alhuzali and Ananiadou, 2021). Recently, Chenet al. (2023) conducted a study analyzing the role ofemotions in controversial Reddit comments usinglanguage models. He et al. (2024) systematically",
  "*Corresponding author": "measured the affective alignment of language mod-els (LMs) by comparing LM-generated responsesto SNSs on two socio-political issues. However,these studies face challenges like sampling biasand subjective annotation. For instance, Chai et al.(2024) note that existing multi-label text classifica-tion models lack the ability to generalize complexconcepts. Ahanin et al. (2023) argue that currentmethods overlook the sentiment polarity of words.To tackle the problems in emotion annotation,we introduce a new labeling approach. Our pri-mary objective is to enhance the expressiveness ofemotion labels by applying Plutchiks Wheel ofEmotions and Diagram of Emotion Dyads. Fur-thermore, we employ a Mixture of Experts (MoE)framework for emotion classification, which iden-tifies the specific emotion that each expert in themodel is best at classifying. This approach seeksto validate the improved classification performanceand specialization of experts in distinct emotionalcategories.The key contributions of this research are listedas follows:",
  "Affective Computing": "Emotions are physical and mental states inducedby neurophysiological changes, often associatedwith specific thoughts, feelings, behavioral re-sponses, and varying degrees of pleasure or dis-pleasure (Damasio, 1998; Ekman and Davidson,1994; Panksepp, 2004). They intertwine with mood,temperament, personality, disposition, and creativ-ity (Averill, 1999). Recent research across psychol-ogy, medicine, history, sociology, and computerscience highlights the complexity and importanceof understanding emotions.Despite extensive research, there is no univer- sally accepted definition of emotion (Cabanac,2002; Clore and Ortony, 2008). Emotions are cate-gorized into various affects corresponding to spe-cific situations (Barrett, 2006), and numerous theo-ries have been proposed, each offering distinct per-spectives on emotional experiences (James, 1884;Candland, 2003).Ekman has significantly advanced our under-standing of basic emotions through his researchon facial expressions (Ekman, 1984). He identifiedsix fundamental emotions: anger, disgust, fear,happiness, sadness, and surprise (Ekman, 1992a,b;Miller, 2016). Later, he expanded this list to in-clude amusement, contempt, contentment, embar-rassment, excitement, guilt, pride in achievement,relief, satisfaction, sensory pleasure, and shame,recognizing emotions not expressed solely throughfacial muscles (Ekman, 1999).Our labeling method relies on Plutchiks emo-tion theories (Plutchik, 2000, 1988), which defineeight basic emotions, grouped as joy versus sad-ness; anger versus fear; trust versus disgust; andsurprise versus anticipation. These basic emo-tions can combine to form complex emotions, asdepicted in ; for instance, the complexemotion love is formed by joy and trust, while re-morse is a mix of disgust and sadness. These com-plex emotions may arise from cultural conditioningor associations combined with the basic emotions.He further introduced twenty-four Primary, Sec-ondary, and Tertiary dyads, representing differ-ent emotion combinations, and noted that emotionscan vary in intensity from mild to intense (Plutchik,1991; Turner, 2000). As illustrated in , forinstance, annoyance, anger, and rage fall withinthe same category with different intensities.",
  "Mixture of Expert": "The Mixture of Experts (MoE) method divides com-plex problems into multiple sub-problems, usingspecialized models (i.e., experts) to address eachsub-problem. MoE utilizes a gating network tocombine the outputs of each expert model, select-ing the most suitable expert for a given input. Thisapproach is particularly useful for datasets withdiverse characteristics, enhancing model perfor-mance and computational efficiency. Eigen et al. (2013) introduced the idea of us-ing multiple MoEs, each with its own gating net-work, as part of a deep model. This approach ismore powerful since complex problems may con-tain many sub-problems, each requiring different experts. They also suggest that introducing sparsitycould transform MoE into a tool for computationalefficiency. Shazeer et al. (2017) proposed a newtype of general-purpose neural network component:a Sparsely-Gated Mixture-of-Experts Layer (MoE).This method uses Noisy top-k gating, which addssparsity and noise to the Softmax Gate used in theMoE architecture (Jordan and Jacobs, 1994), select-ing the top k values among the experts to producethe output. There are numerous other attempts toimprove the gate network (Clark et al., 2022; Haz-imeh et al., 2021; Zhou et al., 2022). Lepikhin et al. (2020) replaced the TransformerEncoders FFN layer with MoE, distributing ex-perts across devices. This had the drawback ofslower speeds when computations concentrated ona single expert. Fedus et al. (2022) improved this bylimiting each token to one expert (k=1) and restrict-ing the number of tokens per expert. Jiang et al.(2024) used an MoE structure with Top-k Gatingand SwiGLU as experts within the Mistral modelsTransformer block, improving performance acrosstasks and showing each expert specialized in spe-cific tasks.",
  "Mixture of Emotion Expert": "We aim to apply Mixture of Experts (MoE) toeach model to determine whether each expert canbe trained as a specialist in individual emotions.As previously mentioned, there are several gatingmethods that connect inputs to specific experts. Fol-lowing the approach in Jiang et al. (2024), we se-lected the k most relevant experts for each token.The reason for experimenting with multiple valuesof k instead of fixing it is to account for complexemotions such as love and optimism, which aredescribed as mixtures of several basic emotionsaccording to Plutchik (2000, 1988). This consid-eration is crucial when tokens contain complexemotions.For the implementation of MoE, we referred toMixtral (Jiang et al., 2024). The MoE structureused in Mixtral determines the output for a giveninput x by taking a weighted sum of the expertnetworks outputs, with weights provided by thegating network. This is efficiently implemented us-ing a softmax over the Top-K logits of a linear layer.A brief overview of the MoE Layer is provided in.To compare how well the model understandsemotions when MoE is applied, we used the FFNnetwork of the base model as experts. To observethe performance changes with minimal parametermodifications, we replaced the FFN network in thelast transformer block of each model with an MoEstructure.",
  "Experimental Setup": "Our experiments utilize two transformer-basedmodels, Llama-2(Touvron et al., 2023) and Mis-tral(Jiang et al., 2023), each with 7 billion parame-ters, chosen for their effectiveness across variousdomains (Hou et al., 2024; Yu et al., 2024; Gruveret al., 2023). The unmodified versions of thesemodels served as baselines for comparison. Weaccessed these models via the Hugging Face APIand fine-tuned them using Q-LoRA(Dettmers et al.,2024). For all experiments, we used the same hy-perparameters except for the k value. Performancewas evaluated by averaging the results over fiveruns for each setting. Detailed hyperparameter con-figurations are provided in Section A.1.",
  ": Emotion distribution across train, validation,and test sets for GoEmotions with Plutchik labeling": "Secondary, and Tertiary dyads, which, whendecomposed, satisfy criterion 1. As a result, weselected SemEval-2018 (Mohammad et al., 2018)and GoEmotions (Demszky et al., 2020).SemEval-2018 contains tweets, each labeledwith one or more of 11 emotions, or marked as Neu-tral. GoEmotions consists of 58K Reddit commentsfrom 2005 to 2019, each labeled with one or moreof 27 emotions, or marked as Neutral. The rulesfor applying Plutchik labeling to these datasets aredetailed in Tables 1 and 2.For a fair comparison, we excluded data for emo-tions not covered by Plutchiks 8 basic emotionsor their dyads, as well as Neutral, in all experi-ments. The final datasets are detailed in Tables 3,4, 5, and 6. We fine-tuned the classification modelsusing the training sets and evaluated their perfor-mance on the test sets.",
  "Main Results": "Tables 7 and 8 present the F1-scores of our pro-posed methods on two datasets. showsthe performance for different k values when ap-plying MoE in Normal Labeling. For SemEval-2018, the macro-F1 indicates the model exceedsbaseline performance at k=2, achieving the highestperformance. In GoEmotions, the Mistral modelsurpasses the baseline across all k values, peakingat k=4, while the Llama2 model underperformsat all k values. The micro-F1 shows the highestperformance at k=4 in all cases.Overall, SemEval-2018 shows a consistent trendin macro-F1 changes with varying k values, un-like GoEmotions. This inconsistency, as shownin , is due to significant label imbalancein GoEmotions. Elbayad et al. (2023) and Feduset al. (2022) explain that MoE models tend to over-",
  ": F1 scores of the models with Normal Labeling.Upper: Llama2, Lower: Mistral": "fit on low-resource data, suggesting that the expertsin the MoE model failed to learn effectively forcertain emotions due to extreme imbalance. Addi-tionally, grief and pride have significantly fewertest samples, leading to high variance in perfor-mance metrics. Thus, performance comparisonsusing macro-F1 in GoEmotions may not be accu-rate. presents the performance of MoE withPlutchik Labeling varying the k values . WithSemEval-2018, the highest macro-F1 was obtainedat k=3, outperforming the baseline model. In GoE-motions, the Mistral model achieved the highestscore at k=4, while the Llama2 model exceededthe baseline at k=1. The highest micro-F1 scorewas generally obtained at k=3, except for the Mis-tral model on GoEmotions, which showed differentpatterns.Plutchik Labeling resulted in more stable andsuperior performance than Normal Labeling, espe-cially in GoEmotions, mitigating the effects of se-vere label imbalance. The MoE-trained model con-sistently outperformed the baseline model acrossvarious k values. depicts the changes in macro-F1 perfor-mance across both datasets with varying k values.When applying Plutchik Labeling, there is a sig-nificant improvement in performance compared toNormal Labeling, both in the baseline and all MoEconfigurations. Notably, in SemEval-2018, whenk is set to 1, the performance improvement withPlutchik Labeling is less pronounced compared tothe baseline and other k values. This suggests thatselecting two or more experts in SemEval-2018allows for better interpretation of emotions.",
  "Underperforming Emotions": "To assess the effectiveness of Plutchik Labeling,we tested whether it could enhance the classifica-tion of underperforming emotions, defined as thosewith F1-scores below 0.6 in the Normal Labelingdataset. presents the F1-scores for underper-forming Emotions in SemEval-2018. When apply-ing Plutchik Labeling, pessimism is decomposedinto anticipation and sadness, resulting in the re-moval of the pessimism label. For basic Emotions, 1AN: Anger, ANO: Annoyance, ANT: Anticipation, CO:Confusion, CUR: Curiosity, DIS: Disappointment, DAP: Dis-approval, DIG: Disgust, EXC: Excitement, GRF: Grief, LO:Love, OPT: Optimism, PES: Pessimism, PRI: Pride, REM:Remorse, SUR: Surprise, TRU: Trust",
  ": F1-scores of underperforming emotions inSemEval-2018": "both anticipation and trust showed significant im-provement in classification performance due to dataaugmentation. However, in the case of surprise, thetransition from Normal Labeling to Plutchik Label-ing did not benefit from data augmentation. presents the F1-scores for the un-derperforming emotions in GoEmotions. Basicemotions such as anger, disgust, and surpriseidentified as underperforming emotions demon-strated substantial improvement with Plutchik La-beling. However, many of the other underperform-ing emotions in GoEmotions are either complexemotions or represent varying intensities (mild orintense), making direct comparisons with PlutchikLabeling more difficult. By comparing the macro-F1 scores of underper-forming emotions between Normal Labeling andPlutchik Labeling in Tables 9 and 10, we observea significant overall improvement in classificationperformance across both datasets. This enhance-ment suggests that our proposed method effectivelyimproves the classification of emotions that are typ-ically harder to classify accurately. We believe thatthis demonstrates the potential of Plutchik Labelingto enhance the robustness and accuracy of emotionclassification systems.",
  "Complex Emotions": "To assess whether our MoE approach improves theclassification of complex emotions, we comparedthe F1-scores of complex emotions between thebaseline and MoE models under Normal Labeling.As similar trends were observed across various kvalues, we focused on the specific k values thatshowed the most significant improvement in macro-F1 scores for each dataset, relative to the baseline. presents the classification performanceof complex emotions in SemEval-2018, compar-ing the baseline with the Top-2 MoE models. TheMoE approach yielded a substantial improvementin macro-F1, significantly increasing the perfor-mance for pessimism, which was previously cate-gorized as an underperforming emotion. presents the complex emotion clas-sification performance of the baseline and Top-4MoE models in GoEmotions. Based on macro-F1,Llama2 showed a slight improvement, while Mis-tral had a slight decline. Llama2s performancedropped for confusion and pride, whereas Mistraldeclined for confusion, curiosity, disappointment,disapproval, and pride.Pride, with limited data samples, poses a chal-",
  ": F1-scores of complex emotions in GoEmo-tions": "lenge for performance improvement due to signif-icant data imbalance. Other complex emotions,particularly those sharing elements with surprise,also face classification difficulties. According toPlutchik (1991), confusion, curiosity, disappoint-ment, and disapproval overlap with surprise. How-ever, Clore and Ortony (2013) argue that surpriseis a cognitive state, not an emotion, as it lacksintrinsic valence and can manifest in both posi-tive and negative contexts, depending on subse-quent evaluations. This difference in perspectiveadds complexity to distinguishing surprise fromrelated emotions that involve both cognitive andaffective components. As a result, our study facedchallenges applying the MoE model, which likelystruggled to classify surprise and other complexemotions that range from neutral to evaluative.",
  "Analysis": "We investigated the relationships between emotionsby analyzing the predominant expert selections foreach. By tracking the output values of the GateLayer in a Mixture of Experts (MoE) model, weidentified which Experts were primarily selectedfor each emotion.Our approach involved selecting Experts foreach token and aggregating the selection propor-tions of the Top-k Experts per token for each input.The value of k corresponds to the Top-k used inthe MoE, with the selection proportions for eachtoken summing to 1. Inputs were grouped by theiremotions labels, and the aggregate Expert selec-tion proportions for each label were computed andstandardized. Using these frequencies of Expertselections for each emotion, we plotted emotion-",
  "emotion correlations to examine the relationshipsbetween emotions": "a shows that joy, love, and optimismexhibit strong correlations, indicating that posi-tive emotions are closely interconnected in theSemEval-2018 dataset. In contrast, anger, sad-ness, and disgust show strong positive correlationswith each other, as well as with fear and pessimism,forming a cluster of negative emotions. Addition-ally, optimism and pessimism, as well as love andsadness, show strong negative correlations witheach other, indicating that these emotions have op-posite characteristics. Furthermore, love tends tohave high correlations with joy and trust, optimismwith joy, and pessimism with anticipation and sad-ness. These patterns also allow us to understand thesimilarities between complex emotions and theircomponent basic emotions. In GoEmotions, as shown in b, joy, love,optimism, and admiration exhibit strong positivecorrelations, indicating their close interrelation aspositive emotions. Conversely, anger, annoyance,excitement, fear, grief, and pride form a groupof negative emotions, with admiration and angershowing a strong negative correlation, highlight-ing their opposing nature. Additionally, the com-plex emotions disappointment and curiosity corre-late highly with sadness and surprise, respectively,while anger correlates strongly with annoyanceand sadness with grief. These patterns reveal thesimilarities between complex emotions and theircomponent emotions, as well as the relationshipsbetween basic emotions and their milder or moreintense counterparts. Overall, while the selection of Experts for eachemotion does not perfectly align with Plutchiksemotion theory, the results show a significant de-gree of similarity. This suggests that our approachis effective for emotion analysis. These findingscontribute to a deeper understanding of emotionalinterrelations and can aid in improving emotionprediction models.",
  "Conclusion": "Our approach, grounded in Plutchiks emotion the-ory and utilizing the MoE architecture, significantlyenhances the performance of multi-label emotionclassification tasks. The proposed methodologieswere evaluated against baseline models, demon-strating significant improvements in classificationaccuracy. Notably, our approach excelled in iden-tifying emotions that are traditionally difficult toclassify and showed superior performance in rec-ognizing complex emotions.Moreover, the analysis of expert selection ten-dencies, based on emotion correlations, revealedthat our models behavior closely aligns withPlutchiks emotion theory. This alignment not onlyenhances classification accuracy but also providesa theoretically grounded insight into emotional in-teractions.Thus, we believe that our research presents arobust framework for multi-label emotion classifi-cation, integrating psychological theories and ad-vanced machine learning techniques in emotionrecognition tasks. Future research could focus onrefining the classification of mild and intense varia-tions of emotions.",
  "Limitations": "This study acknowledges several limitations. First,utilizing Plutchiks emotion theory requires thedataset to include all eight basic emotions definedby the theory, posing a challenge for datasets lack-ing these emotions. Furthermore, excluding emo-tions not covered by Plutchiks emotion theory canbe inefficient, making careful selection of datasetscrucial. Future research could improve the label-ing method by incorporating additional emotionmodels, such as the OCC model (Clore and Ortony,2013).Second, during the application of MoE, we en-countered a known issue where tokens clusteredaround specific experts. This imbalance suggeststhe model may not fully leverage all experts. Weplan to design a more sophisticated MoE structureto address this in the near future. This work was partly supported by the NationalResearch Foundation of Korea grant funded bythe Korean government(MSIT) (No.RS-2024-00357849), Institute of Information & communi-cations Technology Planning & Evaluation(IITP)grant funded by the Korea government(MSIT) (RS-2019-II190421, AI Graduate School Support Pro-gram(Sungkyunkwan University)), the Korea Plan-ning & Evaluation Institute of Industrial Technol-ogy (KEIT) grant funded by the Korea government(MOTIE) (No.RS-2024-00413839). Zahra Ahanin, Maizatul Akmar Ismail, NarinderjitSingh Sawaran Singh, and Ammar AL-Ashmori.2023. Hybrid feature extraction for multi-label emo-tion classification in english text messages. Sustain-ability, 15(16).",
  "D. Candland. 2003. Emotion. Core books in psychol-ogy. Authors Choice Press": "Yuyang Chai, Zhuang Li, Jiahui Liu, Lei Chen, FeiLi, Donghong Ji, and Chong Teng. 2024. Compo-sitional generalization for multi-label text classifica-tion: A data-augmentation approach. Proceedingsof the AAAI Conference on Artificial Intelligence,38(16):1772717735. Kai Chen, Zihao He, Rong-Ching Chang, Jonathan May,and Kristina Lerman. 2023. Anger breeds contro-versy: Analyzing controversy and emotions on reddit.In Social, Cultural, and Behavioral Modeling, pages4453, Cham. Springer Nature Switzerland. Aidan Clark, Diego de Las Casas, Aurelia Guy, ArthurMensch, Michela Paganini, Jordan Hoffmann, Bog-dan Damoc, Blake Hechtman, Trevor Cai, SebastianBorgeaud, et al. 2022. Unified scaling laws for routedlanguage models. In International conference on ma-chine learning, pages 40574086. PMLR.",
  "William Fedus, Barret Zoph, and Noam Shazeer. 2022.Switch transformers: Scaling to trillion parametermodels with simple and efficient sparsity. Journal ofMachine Learning Research, 23(120):139": "Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew GWilson. 2023. Large language models are zero-shottime series forecasters. In Advances in Neural Infor-mation Processing Systems, volume 36, pages 1962219635. Curran Associates, Inc. Hussein Hazimeh, Zhe Zhao, Aakanksha Chowdh-ery, Maheswaran Sathiamoorthy, Yihua Chen, RahulMazumder, Lichan Hong, and Ed Chi. 2021. Dselect-k: Differentiable selection in the mixture of expertswith applications to multi-task learning. Advances inNeural Information Processing Systems, 34:2933529347.",
  "Mind, os-IX(34):188205": "Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023. Mistral7b. arXiv preprint arXiv:2310.06825. Albert Q Jiang, Alexandre Sablayrolles, AntoineRoux, Arthur Mensch, Blanche Savary, Chris Bam-ford, Devendra Singh Chaplot, Diego de las Casas,Emma Bou Hanna, Florian Bressand, et al. 2024.Mixtral of experts. arXiv preprint arXiv:2401.04088.",
  "Robert Plutchik. 2000. Emotions in the practice of psy-chotherapy: Clinical implications of affect theories": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,Andy Davis, Quoc Le, Geoffrey Hinton, and JeffDean. 2017. Outrageously large neural networks:The sparsely-gated mixture-of-experts layer. arXivpreprint arXiv:1701.06538. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288.",
  "J. Turner. 2000. On the Origins of Human Emotions:A Sociological Inquiry into the Evolution of HumanAffect. Stanford University Press": "Wenhao Ying, Rong Xiang, and Qin Lu. 2019. Im-proving multi-label emotion classification by inte-grating both general and domain-specific knowledge.In Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019), pages 316321. Jianfei Yu, Lus Marujo, Jing Jiang, Pradeep Karuturi,and William Brendel. 2018. Improving multi-labelemotion classification via sentiment classificationwith dual attention transfer network. In Proceed-ings of the 2018 Conference on Empirical Methodsin Natural Language Processing, pages 10971102,Brussels, Belgium. Association for ComputationalLinguistics. Longhui Yu, Weisen Jiang, Han Shi, YU Jincheng,Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li,Adrian Weller, and Weiyang Liu. 2024. Metamath:Bootstrap your own mathematical questions for largelanguage models. In The Twelfth International Con-ference on Learning Representations. Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, YanpingHuang, Vincent Zhao, Andrew M Dai, Quoc V Le,James Laudon, et al. 2022. Mixture-of-experts withexpert choice routing. Advances in Neural Informa-tion Processing Systems, 35:71037114."
}