{
  "Abstract": "Referring Expression Comprehension (REC),which aims to ground a local visual region vianatural language, is a task that heavily relies onmultimodal alignment. Most existing methodsutilize powerful pre-trained models to transfervisual/linguistic knowledge by full fine-tuning.However, full fine-tuning the entire backbonenot only breaks the rich prior knowledge em-bedded in the pre-training, but also incurs sig-nificant computational costs. Motivated by therecent emergence of Parameter-Efficient Trans-fer Learning (PETL) methods, we aim to solvethe REC task in an effective and efficient man-ner. Directly applying these PETL methodsto the REC task is inappropriate, as they lackthe specific-domain abilities for precise localvisual perception and visual-language align-ment. Therefore, we propose a novel frame-work of Multimodal Prior-guided Parameter Ef-ficient Tuning, namely MaPPER. Specifically,MaPPER comprises Dynamic Prior Adaptersguided by an aligned prior, and Local Con-volution Adapters to extract precise local se-mantics for better visual perception. More-over, the Prior-Guided Text module is pro-posed to further utilize the prior for facilitat-ing the cross-modal alignment. Experimen-tal results on three widely-used benchmarksdemonstrate that MaPPER achieves the bestaccuracy compared to the full fine-tuning andother PETL methods with only 1.41% tunablebackbone parameters. Our code is available at",
  ": Comparision to others PETL methods": "for visual language understanding, with broad ap-plications in fields such as visual-language naviga-tion (Liu et al., 2024a) and human-machine inter-action (Chen et al., 2023). Different from vanillaobject detection task, REC needs to extract not onlyglobal and local spatial information from images,but also relies on the alignment of multimodal fea-tures. Existing approaches (Deng et al., 2021; Kamathet al., 2021; Deng et al., 2023; Shi et al., 2022)transfer the language and vision knowledge frompre-trained models by fully fine-tuning. However,such a fine-tuning strategy is sub-optimal for REC,as reflected in the following aspects: 1) Fine-tuningthe entire backbone might suffer catastrophic for-getting and undermine the extensive prior knowl-edge learned from pre-training. 2) The computa-tional cost requirements surge dramatically, par-ticularly for larger foundational models, leadingto a significant increase in GPU memory usage.This limits the accessibility of large models forresearchers with limited hardware resources.To address these issues, we shift our fo-cus to Parameter-Efficient Transfer Learning(PETL) (Chowdhury et al., 2023; Wang et al., 2023a). PETL methods like Adapter tuning andPrompt tuning provide efficient ways to utilize pre-trained models by adjusting a small set of parame-ters instead of fine-tuning the entire network (Xinet al., 2024c). This approach saves computationalresources while still competitive performance im-provements. By integrating PETL techniques, wecan enhance our models flexibility and efficiencyin adapting to REC. However, we empirically findthat directly using these PETL methods cannotachieve satisfactory results in REC (see ).We argue the main reasons are twofold: 1) the tar-get objects that require attention in REC often oc-cupy local regions of uncertain size in images, andmost existing PETL methods lack the crucial abil-ity to extract multi-scale local semantics for visualperception. 2) REC is a task that strongly relieson multimodal alignment, and language-orientedadapters are obviously deficient in aligning withvisual information. Recently, PETL methods havealso been introduced into REC tasks (Xiao et al.,2024; Liu et al., 2024c). HiVG (Xiao et al., 2024)adopts LoRA to fine-tune the frozen CLIP model,but it is not an efficient enough approach due tothe heavy alignment design using cross-attentionmodule. In contrast, DARA (Liu et al., 2024c) is alightweight method in PETL paradigm. However,DARA does not fully address the need for localvisual adaptation in the referring expression com-prehension, potentially compromising the modelsability to capture fine-grained visual details.Considering the aforementioned issues, inthis paper, we propose a novel framework ofMultimodal Prior-guided Parameter Efficient Tun-ing for REC (MaPPER) that improves text under-standing with the aligned prior and enhances visionperception by combining local visual semanticswith global perception. As shown in , weintroduce the vision-aligned text module to gener-ate the aligned prior, which works for the align-ment of vision and language feature. Moreover,we insert the Local Convolution Adapter (LoCA)into vision blocks for enhancing visual percep-tion. Specifically, we propose the Dynamic PriorAdapter (DyPA) presented in , DyPA candynamically adjust each token by considering thesignificance score guided by the aligned prior. Inorder to promote the interaction of text and visionfeatures, we further propose the Prior-guided Textmodule (PGT) for fusing the prior and text feature.For the visual branch, most pre-trained visual mod-els are powerful transformer architectures. Unfor- tunately, vision transformers are observed ignoringlocal feature details (Peng et al., 2021), which de-creases the discriminability between backgroundsand foregrounds. Motivated by this, we introducethe Local Convolution Adapter (LoCA), which in-tegrates multi-scale local knowledge, thereby en-hancing the representational power for pre-trainedvision transformers. Extensive experiments on Re-fCOCO (Yu et al., 2016), RefCOCO+ (Yu et al.,2016), and RefCOCOg (Mao et al., 2016; Nagarajaet al., 2016) demonstrate the effectiveness and effi-ciency of our framework. Our main contributionsare as follows: We perform an in-depth exploration of parameter-efficient transfer learning (PETL) methods forREC tasks. We introduce MaPPER aimed atimproving both the effectiveness and efficiencyof visual-text alignment, as well as enhancingvisual perception by incorporating local visualsemantics. We propose the novel Dynamic Prior Adapter(DyPA) and Local Convolution Adapter (LoCA).The former employs aligned prior to dynamicallyadjust the language encoder, while the latter in-troduces local visual features for enhancing thevisual encoder.",
  "Referring Expression Comprehension": "Referring expression comprehension (REC) (Yuet al., 2018; Yang et al., 2019; Deng et al., 2021;Xiao et al., 2023; Liu et al., 2024e; Xiao et al.,2024) aims to locate a local visual region in imagesby textual descriptions. Early propose-and-rankmethods (Hong et al., 2019; Chen et al., 2019)follow a two-stage pipeline which first utilizes pre-trained object detectors to obtain a set of regionproposals, which are then ranked based on theirsimilarity scores with the given textual description.However, these two-stage methods face challengesin terms of the performance of the proposal gener-ators and the additional ranking mechanisms. Af-ter the introduction of ViT, the Transformer-basedmethods (Deng et al., 2021; Du et al., 2022; Yanget al., 2022; Zhu et al., 2022; Liu et al., 2024c;Zhu et al., 2023) have recently emerged that signif-icantly improve the grounding performance. Most recently, grounding multimodal large languagemodels (Li et al., 2023; Wang et al., 2023b) havepropelled the state-of-the-art (SOTA) performance,these works require a large amount of in-domainand other domain datasets. As REC models con-tinue to scale up in size and complexity, fully fine-tuning becomes extremely high training cost.",
  "Parameter-efficient Transfer Learning": "The continuous expansion of pre-trained models de-mands significant computational resources and con-sumes considerable storage during fine-tuning (Liuet al., 2024d). To address these challenges, re-searchers in the NLP and CV domain have exploredPETL methods (Hu et al., 2022; Chen et al., 2022;Yuan et al., 2023; Liu et al., 2024b). By focus-ing on updating only a small subset of parame-ters, PETL achieves a balance between maintaininghigh performance and ensuring computational effi-ciency. This method is particularly advantageousfor deploying large-scale models, addressing thechallenges posed by increasing model sizes whilestreamlining the adaptation process to new tasks.The main PETL methods can be classified intothree categories: (i) selectively updating a tiny num-ber of existing model parameters (Guo et al., 2020;Zaken et al., 2021); (ii) adjusting newly added pa-rameters to the model or its input (Li and Liang,2021; Zhou et al., 2022; Xin et al., 2024b); (iii)applying low-rank factorization techniques to theparameters that require updates (Hu et al., 2022;Karimi Mahabadi et al., 2021; Hao et al., 2023; Liuet al., 2024f; Xin et al., 2024a). Some pioneeringworks like ETRIS (Xu et al., 2023) and DARA (Liuet al., 2024c) sought to utilize adapters to adapt pre-trained models to referring image segmentation andreferring expression comprehension, respectively.However, their proposed modules like Bridger (Xuet al., 2023) and RA (Liu et al., 2024c) are insuf-ficient for capturing the complexity of multi-scalelocal visual features.",
  "Framework Overview": "The overall framework of the proposed MaPPERis illustrated in . Our approach freezesthe pre-trained backbone, ensuring parameter ef-ficiency. This framework consists of two distinctefficient tuning modules. The first module, knownas the Dynamic Prior Adapter, utilizes aligned priorgenerated from the Vision-aligned Prior Module to enable efficient modal alignment and adapta-tion. The second module, referred to as the LocalConvolution Adapter module, integrates local vi-sual features into global prior (pre-trained visualknowledge) from the visual encoder, thereby reg-ularizing the whole visual perception. Finally, thecomplete textual features, alongside aligned prior,are inputted into the Prior-guided Text module forpromoting the multimodal alignment.",
  "Text & Image Feature Extraction": "Text Encoder. The REC task relies heavily onword-level understanding due to its concise linguis-tic expression format, such as \"front middle yellowguy\", to convey referring information. Owing toits bi-directional encoder representations and themasked language modeling, BERT (Devlin et al.,2018) excels in word-level understanding, makingit suitable for text encoding in REC domain. Giventhe input referring expression T, the text expressionis firstly converted into a one-hot vector. Subse-quently, each one-hot vector is tokenized into aseries of linguistic tokens. A special [CLS] tokenis prefixed to the sequence, and the sequence oftokens is then fed into a stack of 12 transformerencoder layers to progressively capture and modelthe intricate language tokens.Visual Encoder. Our work adopts the transformer-based DINOv2-B/14 (Oquab et al., 2023) as thevisual backbone. The model involves training theVision Transformer (ViT) model (Dosovitskiy et al.,2020) on the extensive LVD-142M dataset, utiliz-ing a self-supervised learning strategy. This ap-proach equips the model with the ability to extractpowerful visual features, which in turn delivers im-pressive performance across various downstreamtasks. Given an input image I0 RH0W03, theimage is initially divided into N non-overlappingpatches, which are then linearly projected into D-dim patch embeddings Ip RND. Meanwhile, alearnable [CLS] token is prepended to Ip, produc-ing I R(N+1)D.Considering the substantial number of param-eters, we opt to freeze visual and text encodersduring the fine-tuning process. This strategy allowsfor a more efficient allocation of computational re-sources and focuses the learning on the adjustmentsof other modules.",
  "Vision Feature": ": Overall architecture of MaPPER. MaPPER freezes the pre-trained vision encoder and language encoder.For the language branch, Dynamic Prior Adapters (DyPA) utilize aligned priors generated from the Vision-alignedPrior Module to enable efficient modal alignment and adaptation. For the language branch, Local ConvolutionAdapters (LoCA) integrate local visual features the global prior (pre-trained visual knowledge) from the visualencoder. Moreover, the Prior-guided Text module for promoting the multimodal alignment. which has a relatively high word-level understand-ing. However, BERT lacks alignment with vision inthe pre-training process, and we introduce a Vision-aligned Prior Module to generate a vision-alignedprior. The prior serves for better adjusting BERTencoder, and promoting the interaction of text andvision features.Vision-aligned Prior Module (VAP). The coreof VAP to a produce vision-aligned prior for theREC domain. Considering that CLIP (Radfordet al., 2021) model inherently has the ability toalign visual with text feature, we used the frozenCLIP followed by a mapping layer M as the VAPmodule. Given the text input t, the vision-alignedprior p can be formulated as follows:",
  "p = M(CLIPf(t)).(1)": "where the CLIPf denotes the frozen CLIP back-bone.Dynamic Prior Adapter (DyPA). To dynami-cally bridge the gap between the pre-trained BERTmodel and the complex REC task, we introduce theDynamic Prior Adapter, which operates in parallelwith the text encoder, as shown in . DyPAcomprising four module: a dynamic scale mod-ule (DS), a downward projection with parametersW tdown Rrd, a ReLU activation layer, and anupward projection with parameters W tup Rdr.Specifically, we adopt the DS module for in-tegrating the vision-aligned prior p to different layers in the BERT encoder. The module gener-ates scale factors Sf using a scoring weight matrixWs R1d, eliminating manual hyper-parametertuning. Given the prior p, the dynamic scalingfactor can be formulated as follows:",
  "xt = Sf ReLUxtW tdownW tup.(3)": "DyPA utilizes the vision-aligned prior p to dynam-ically regularize the feed-forward during adaptertuning. To mitigate the influence of Adapter out-puts during the initial stages of model training, weinitialize W tup to zero.Prior-guided Text Module (PGT). Through thedesign of the DyPA module, we efficiently fine-tune the BERT model to produce fine-grainedaligned text features for the REC tasks. In or-der to promote the interaction of text and visionfeatures for the Multimodal Interactive Module inSec.3.5, we propose a Prior-Guided Text Module,fusing the prior p RNtCp into the text features",
  ": The structure of the Dynamic Prior Adapter": "t RNtCt generated by the BERT encoder. Toachieve this, we employ a projection layer, denotedby Proj RCpCt, to map the prior p onto atransformed representation p. This projection isspecifically designed to align the dimensions of theprior with the text features. In order to streamlinethe process, we concatenate t with the transformedpriors p to get the final text feature ft integratedwith the vision-aligned prior.",
  "Global & Local Visual Perception": "For visual perception in the REC task, local fea-tures and global representations are important coun-terparts. Although pre-trained DINOv2 can pro-vide powerful and robust visual features to achievepromising performance, the task-specific visual at-tention in the REC task often focuses on localizedareas of uncertain size in images, which have beenvisualized in .Local Convolution Adapter (LoCA). To furtherfacilitate the visual perception ability of DINOv2for the REC task, we propose a Local ConvolutionAdapter (LoCA) module to adjust the visual foun-dation models. LoCA introduces the multi-scalelocal information to further enhance visual percep-tion. The local convolution adapter consists of adown-projection layer W vdown, a multi-scale con-volution module, a ReLU activation layer, and theup-projection layer W vup. Specifically, in one visual encoder layer, thedownward projection layer receives processed vi-sual tokens xv from the Multi-head Attention(MHA) layer as input and produces adapted. Themulti-scale convolution module consists of two par-allel convolutional paths of multi-scale (11, 33).The 11 convolution is strategically placed beforethe 33 convolutions to reduce channel dimension.This design and the bottleneck structure make thelocal convolution adapter still lightweight. Theoutputs of the multi-scale convolutional paths areconcatenated to form the local feature floc.",
  "floc = (flocWvup).(6)": "Global and Local Visual Integration. To augmentthe DINOv2 backbone with multi-scale local visualperception on the REC task, we integrate the LocalConvolution Adapter (LoCA) in parallel with theMLP layer within the transformer block. By theconcise design, LoCA module adds multi-scalelocal prior into the DINOv2 model for the RECtask. The output of each adapted transformer blockcan be described as:",
  "Multimodal Interactive Module": "We have implemented a transformer (Vaswani et al.,2017) architecture that seamlessly integrates mul-timodal embeddings to forecast the bounding boxof the referenced object. Specifically, the adaptedvision embeddings fv RNvCv and languageembeddings fl RNlCl are first projected into acommon space of joint embeddings f v RNvCpand f l RNlCp, both with a unified channelsize. Followed by TransVG (Deng et al., 2021) andDARA (Liu et al., 2024c), these joint embeddings,",
  "Full Fine-tuning": "MAttNet (Yu et al., 2018)CVPR18100%76.65 81.14 69.99 65.33 71.62 56.02-66.58 67.27RvG-Tree (Hong et al., 2019)TPAMI19100%75.06 78.61 69.85 63.51 67.45 56.66-66.95 66.51NMTree (Liu et al., 2019)ICCV19100%76.41 81.21 70.09 66.46 72.02 57.52 64.62 65.87 66.44FAOA (Yang et al., 2019)ICCV19100%72.54 74.35 68.50 56.81 60.23 49.60 56.12 61.33 60.26ReSC-Large (Yang et al., 2020)ECCV20100%77.63 80.45 72.30 63.59 68.36 56.81 63.12 67.30 67.20TransVG (Deng et al., 2021)ICCV21100%80.32 82.67 78.12 63.50 68.15 55.63 66.56 67.66 67.44QRNet (Ye et al., 2022)CVPR22100%84.01 85.85 82.34 72.94 76.17 63.81 71.89 73.03 72.52Dynamic-MDETR (Shi et al., 2022)TPAMI23100%85.97 88.82 80.12 74.83 81.70 63.44 72.21 74.14 74.49PFOS (Sun et al., 2022)TMM22100%77.37 80.43 72.87 63.74 68.54 55.84 61.46 67.08 66.35SeqTR (Zhu et al., 2022)ECCV22100%81.23 85.00 76.08 68.82 75.37 58.78-71.35 71.58Word2Pix (Zhao et al., 2022)TNNLS22100%81.20 84.39 78.12 69.46 76.81 61.57-70.81 71.34YORO (Ho et al., 2023)ECCV22100%82.90 85.60 77.40 73.50 78.60 64.90-73.40 74.30CLIP-VG (Xiao et al., 2023)TMM23100%84.29 87.76 78.43 69.55 77.33 57.62 72.64 73.18 72.54JMRI (Zhu et al., 2023)TIM23100%82.97 87.30 74.62 71.17 79.82 57.01 69.32 71.96 72.04MGCross (Miao et al., 2023)TIP24100%85.10 88.23 80.08 74.44 79.48 65.21 74.50 77.25 75.78",
  "DARA (Liu et al., 2024c)Arxiv241.63%81.16 82.76 76.72 65.58 69.83 57.22 67.21 69.22 67.67MaPPER (Ours)-1.41%86.03 88.90 81.19 74.92 81.12 65.68 74.60 76.32 75.81": ": Comparison with latest SOTA methods on RefCOCO/+/g for visual grounding. indicates that allof the RefCOCO/+/g training data has been used during pre-training. \"Tuned/Total param.\" is the averagepercentage of tuned parameters in backbone. We highlight the best and the second-best results. along with a learnable [REG] token, are processedthrough a series of six transformer encoder layers,to fuse the cross-modality embeddings. Finally, aprediction head, implemented as a Multi-layer Per-ceptron with two 256-dimensional hidden layersand a linear output layer, takes the [REG] token asinput and projects it onto the 4-dimensional coordi-nates for defining the bounding box.",
  "Experimental Setup": "Datasets and Evaluation Metrics. We validateour method on three widely-used REC benchmarks:RefCOCO (Yu et al., 2016), RefCOCO+ (Yu et al.,2016), and RefCOCOg (Mao et al., 2016; Nagarajaet al., 2016). We follow the previous research thatemploys top-1 accuracy (%) as the evaluation met-ric. Specifically, a prediction is deemed accurateonly when its IoU exceeds or equals 0.5. In addi-tion to , we also report the numberof tunable parameters in the pre-trained encoders tocompare the fine-tuning efficiency with traditionalfull fine-tuning and other PETL methods.Implementation Details. The vision encoder isinitialized with DINOv2-B/14 (Oquab et al., 2023),while the language encoder uses BERT-base (De-vlin et al., 2018). The resolution of the input imageis 518518. Both the DINOv2-B/14 model andthe BERT-base model process tokens with a fea- ture dimension of 768. The Multimodal InteractiveModule uses Xavier initialization. DyPA are ini-tialized with Kaiming normal initialization and in-serted into the transformer layers for the languageencoder. The bottleneck dimension Cd for DyPA is32. For LoCA, the 11 convolution before the 33convolution reduces the channel to 24. The outputdimensions of the two convolutional paths are 192and 96, so the input dimension of the these convo-lutional paths is 288. For fair comparisons, PETLmethods in use the same base architecture,and keeping the vision and language encoder fixed.",
  "Main Results": "We conducted a comprehensive comparison be-tween our proposed MaPPER model and a seriesof previous referring expression comprehension(REC) methods. The main experimental results arepresented in , from which we can observethat: MaPPER achieves the best accuracy whileensuring parameter efficiency among all methods,thus validating its effectiveness and efficiency.Effectiveness. As shown, on the three com-monly challenging benchmarks, MaPPER outper-forms all traditional full fine-tuning methods. Com-pared to DARA (Liu et al., 2024c), a parameter-efficient transfer learning method, we achieves bestresults on the three benchmarks. Notably, evencompared to some methods that are pre-trained on",
  "(b)1.5884.28 86.02 79.38": ": Effectiveness of Local Convolution Adapter(LoCA) for the visual branch. Note the ablation studywithout adding any component in the text branch, andwe freeze the text encoder. (a) represents freezing boththe text and visual branche. the the RefCOCO/+/g (indicated by in ),our MaPPER model achieves the highest scoresacross all evaluation tasks, with particularly strongperformance on the RefCOCO+, which presentgreater challenges compared to RefCOCO.Efficiency. clearly illustrates that MaP-PER not only achieves the best performance, butalso highlights its huge advantages in parameterefficiency. MaPPER reduced the tunable backboneparameters by 98.59% compared to the traditionalfull fine tuning method. Compared to the PETLmethod DARA (Liu et al., 2024c), MaPPER hasalso lower tunable parameters.",
  "Comparison with Other PETL Methods": "We conduct experiments comparing our MaPPERwith other parameter-efficient tuning methods us-ing DINOv2-Base as the backbone. To ensurefairness, we retain the original parameter settingsfrom previous methods and adjust the bottleneck toachieve comparable parameter counts. illus-trates that MaPPER outperforms other PETL meth-ods on all three benchmarks, and even performsbetter than fully fine-tuning. This highlights theeffectiveness of MaPPER in adapting pre-trainedknowledge for the REC domain. Through intro-ducing vision-aligned prior, MaPPER enhance themodeling of the vision-text alignment capability.Furthermore, inserting Local Convolution Adapters",
  "Ablation Study": "Effectiveness of Local Convolution Adapter. Weassess the impact of the Local Convolution Adapter(LoCA) by performing an ablation study and re-porting the results on RefCOCO validation andtest datasets. From , it is evident that in-troducing the LoCA yields a great improvement,increasing the average performance to 1.87%. Thisindicates that the LoCA enhances the visual per-ception of DINOv2 with local visual feature.Effect of Multi-scale Size for Visual Branch. Tofurther verify the effect of local visual information,we perform the attempts of using only a single-sizeconvolution kernel (11), and three scales (1133 55). indicates that it is difficult for anadapter with a single-size convolution kernel (a) toperform well for the REC. Local Features are toofine-grained (c) are also not optimal. In contrast,appropriate multi-scale (b) provide proper localinformation, thus achieving the best performance.Effect of the Vision-aligned Prior for TextBranch.From , we can see that: (1)Freezing the text encoder while only tuning localconvolution adapter can also brings great perfor-",
  "(f)2.77(+1.19) 86.03 88.90 81.19": ": Effectiveness of the Vision-Prior for the textbranch. Note (a) represents freezing the text encoderwhile tuning the LoCA in the visual encoder, and theLoCA included in Params.. (b) represents using thevanilla adapter without p. (d) represents only using thePGT without any adapters. mance ( (a)); (2) it is crucial to obtain adynamic scale with the vision-prior, the DynamicPrior Adapter (DyPA) brings better performancecompared to vanilla adapter fixing the scale to 1.0( (b,c)); (3) by the design of Prior-guidedText Module (PGT), we further promote the inter-action of text and vision features ( (f)); (4)Incorporating the DyPA and PGT results in an aver-age improvement of 1.02% compared to only usingDyPA.",
  "To investigate the impact of vision-aligned prior,we visualize the attention maps from the Multi-": "modal Interactive Module under two strategies:with and without the vision-aligned prior. In theabsence of the prior represents the text adapter with-out dynamic scale, and the prior-guided text mod-ule is not introduced. As shown in , referringexpressions contain object appearance attributions,human actions, and spatial relationships. It is ob-servable that the model can focus well on the localtarget region of the whole image with the vision-aligned prior. This indicates that vision-alignedprior enhancing the alignment ability of MaPPER.",
  "Conclusion": "In this study, we present an innovative Parameter-Efficient Transfer Learning (PETL) approach de-signed for multi-modal language grounding tasks,especially in referring expression comprehension.MaPPER enhances the adapters with multi-modalprior through the implementation of a simple yeteffective fine-tuning strategy. We aims at improv-ing both the effectiveness and efficiency of visual-text alignment, as well as enhancing visual percep-tion by incorporating local visual semantics. TheDynamic Prior Adapter (DyPA) employs alignedprior to dynamically adjust the language encoder,while the Local Convolution Adapter (LoCA) in-troduces local visual features for enhancing thevisual encoder. MaPPER not only surpasses theperformance of fully fine-tuned models but also",
  "Limitation": "While our proposed method has shown enhancedefficiency, scalability, and parameter optimizationin the realm of REC tasks, surpassing conventionalfully fine-tuned models, our empirical inquirieshave been confined to this specific domain. It isimperative for future research to broaden the valida-tion scope encompass include variety range of othermulti-modal tasks. Moreover, while our approachcan effectively decrease the quantity of parametersnecessitating training, thus conserving computa-tional and storage resources, it still mandates atraining process. As the frontier of multi-modallarge-scale models progresses, there is a signifi-cant opportunity for future exploration into open-vocabulary zero-shot referring expression compre-hension. This area of research could unveil inno-vative pathways and contribute to the evolution ofmodels capable of comprehending and generatingexpressions without the constraint of prior training. Yuqi Bu, Xin Wu, Liuwu Li, Yi Cai, Qiong Liu, andQingbao Huang. 2023. Segment-level and category-oriented network for knowledge-based referring ex-pression comprehension. In Findings of the Associa-tion for Computational Linguistics: ACL 2023.",
  "Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,Feng Zhu, and Rui Zhao. 2023. Shikra: Unleashingmultimodal llms referential dialogue magic. arXivpreprint arXiv:2306.15195": "Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang,Yibing Song, Jue Wang, and Ping Luo. 2022. Adapt-former: Adapting vision transformers for scalablevisual recognition. In Proceedings of the Advancesin Neural Information Processing Systems. Yi Wen Chen, Yi Hsuan Tsai, Tiantian Wang, Yen YuLin, and Ming Hsuan Yang. 2019. Referring expres-sion object segmentation with caption-aware consis-tency. In Proceedings of the British Machine VisionConference. Sanjoy Chowdhury, Sayan Nag, and Dinesh Manocha.2023. APoLLo: Unified adapter and prompt learningfor vision language models. In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing. Jiajun Deng, Zhengyuan Yang, Tianlang Chen, Wen-gang Zhou, and Houqiang Li. 2021. Transvg: End-to-end visual grounding with transformers. In Pro-ceedings of the IEEE/CVF International Conferenceon Computer Vision. Jiajun Deng, Zhengyuan Yang, Daqing Liu, TianlangChen, Wengang Zhou, Yanyong Zhang, Houqiang Li,and Wanli Ouyang. 2023. Transvg++: End-to-endvisual grounding with language conditioned visiontransformer. IEEE Transactions on Pattern Analysisand Machine Intelligence.",
  "Tianxiang Hao, Hui Chen, Yuchen Guo, and GuiguangDing. 2023. Consolidator: Mergable adapter withgroup connections for vision transformer. In Interna-tional Conference on Learning Representations": "Chih-Hui Ho, Srikar Appalaraju, Bhavan Jasani,R Manmatha, and Nuno Vasconcelos. 2023. Yoro-lightweight end to end visual grounding. In Com-puter VisionECCV 2022 Workshops: Tel Aviv, Israel,October 2327, 2022, Proceedings, Part VIII, pages323. Springer. Richang Hong, Daqing Liu, Xiaoyu Mo, Xiangnan He,and Hanwang Zhang. 2019. Learning to composeand reason with language tree structures for visualgrounding. IEEE Transactions on Pattern Analysisand Machine Intelligence. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,Bruna Morrone, Quentin De Laroussilhe, AndreaGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.Parameter-efficient transfer learning for nlp. In Pro-ceedings of the International Conference on MachineLearning. Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,et al. 2022. LoRA: Low-rank adaptation of large lan-guage models. In Proceedings of the InternationalConference on Learning Representations.",
  "Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:Optimizing continuous prompts for generation. arXivpreprint arXiv:2101.00190": "Daqing Liu, Hanwang Zhang, Feng Wu, and Zheng-JunZha. 2019. Learning to assemble neural module treenetworks for visual grounding. In Proceedings of theIEEE/CVF International Conference on ComputerVision. Shilong Liu, Shijia Huang, Feng Li, Hao Zhang,Yaoyuan Liang, Hang Su, Jun Zhu, and Lei Zhang.2023. Dq-detr: Dual query detection transformer forphrase extraction and grounding. In Proceedings ofthe AAAI Conference on Artificial Intelligence, vol-ume 37, pages 17281736. Ting Liu, Yue Hu, Wansen Wu, Youkai Wang, KaiXu, and Quanjun Yin. 2024a. Dap: Domain-awareprompt learning for vision-and-language navigation.In Proceedings of the IEEE International Conferenceon Acoustics, Speech and Signal Processing. Ting Liu, Yue Hu, Wansen Wu, Youkai Wang, KaiXu, and Quanjun Yin. 2024b. Panda: Prompt-basedcontext-and indoor-aware pretraining for vision andlanguage navigation. In MultiMedia Modeling: 30thInternational Conference. Ting Liu, Xuyang Liu, Siteng Huang, Honggang Chen,Quanjun Yin, Long Qin, Donglin Wang, and Yue Hu.2024c. DARA: Domain- and relation-aware adaptersmake parameter-efficient tuning for visual grounding.In Proceedings of the IEEE International Conferenceon Multimedia and Expo. Ting Liu, Xuyang Liu, Liangtao Shi, Zunnan Xu,Siteng Huang, Yi Xin, and Quanjun Yin. 2024d.Sparse-Tuning: Adapting vision transformers withefficient fine-tuning and inference. arXiv preprintarXiv:2405.14700. Xuyang Liu, Siteng Huang, Yachen Kang, HonggangChen, and Donglin Wang. 2024e. VGDiffZero: Text-to-image diffusion models can be zero-shot visualgrounders. In Proceedings of the IEEE InternationalConference on Acoustics, Speech and Signal Process-ing. Xuyang Liu, Ting Liu, Siteng Huang, Yue Hu, Quan-jun Yin, Donglin Wang, and Honggang Chen.2024f. M2ist: Multi-modal interactive side-tuningfor memory-efficient referring expression compre-hension. arXiv preprint arXiv:2407.01131. Junhua Mao, Jonathan Huang, Alexander Toshev, OanaCamburu, Alan L Yuille, and Kevin Murphy. 2016.Generation and comprehension of unambiguous ob-ject descriptions. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recog-nition.",
  "Varun K Nagaraja, Vlad I Morariu, and Larry S Davis.2016. Modeling context between objects for refer-ring expression understanding. In Proceedings of theEuropean Conference on Computer Vision": "Maxime Oquab, Timothe Darcet, Tho Moutakanni,Huy V Vo, Marc Szafraniec, Vasil Khalidov, PierreFernandez, Daniel HAZIZA, Francisco Massa,Alaaeldin El-Nouby, et al. 2023. Dinov2: Learningrobust visual features without supervision. Transac-tions on Machine Learning Research. Zhiliang Peng, Wei Huang, Shanzhi Gu, Lingxi Xie,Yaowei Wang, Jianbin Jiao, and Qixiang Ye. 2021.Conformer: Local features coupling global repre-sentations for visual recognition. In Proceedings ofthe IEEE/CVF international conference on computervision, pages 367376. Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,et al. 2021.Learning transferable visual modelsfrom natural language supervision. In InternationalConference on Machine Learning, pages 87488763.PMLR. Fengyuan Shi, Ruopeng Gao, Weilin Huang, and LiminWang. 2022.Dynamic mdetr: A dynamic mul-timodal transformer decoder for visual grounding.IEEE Transactions on Pattern Analysis and MachineIntelligence. Wei Su, Peihan Miao, Huanzhang Dou, and Xi Li.2024. Scanformer: Referring expression compre-hension by iteratively scanning. In Proceedings ofthe IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1344913458. Mengyang Sun, Wei Suo, Peng Wang, Yanning Zhang,and Qi Wu. 2022. A proposal-free one-stage frame-work for referring expression comprehension andgeneration via dense cross-attention. IEEE Transac-tions on Multimedia.",
  "Transactions on Pattern Analysis and Machine Intel-ligence": "Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. In Proceedings of the Advances in NeuralInformation Processing Systems. Qifan Wang, Yuning Mao, Jingang Wang, Hanchao Yu,Shaoliang Nie, Sinong Wang, Fuli Feng, Lifu Huang,Xiaojun Quan, Zenglin Xu, et al. 2023a. APrompt:Attention prompt tuning for efficient adaptation ofpre-trained language models. In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing. Weihan Wang, Qingsong Lv, Wenmeng Yu, WenyiHong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang,Lei Zhao, Xixuan Song, et al. 2023b. Cogvlm: Vi-sual expert for pretrained language models. arXivpreprint arXiv:2311.03079. Cantao Wu, Yi Cai, Liuwu Li, and Jiexin Wang. 2023.Scene graph enhanced pseudo-labeling for referringexpression comprehension. In Findings of the As-sociation for Computational Linguistics: EMNLP2023.",
  "Linhui Xiao, Xiaoshan Yang, Fang Peng, Ming Yan,Yaowei Wang, and Changsheng Xu. 2023.Clip-vg: Self-paced curriculum adapting of clip for visualgrounding. IEEE Transactions on Multimedia": "Yi Xin, Junlong Du, Qiang Wang, Zhiwen Lin, andKe Yan. 2024a. Vmt-adapter: Parameter-efficienttransfer learning for multi-task dense scene under-standing. In Proceedings of the AAAI Conferenceon Artificial Intelligence, volume 38, pages 1608516093. Yi Xin, Junlong Du, Qiang Wang, Ke Yan, andShouhong Ding. 2024b. Mmap: Multi-modal align-ment prompt for cross-domain multi-task learning.In Proceedings of the AAAI Conference on ArtificialIntelligence, volume 38, pages 1607616084. Yi Xin, Siqi Luo, Haodi Zhou, Junlong Du, Xiao-hong Liu, Yue Fan, Qing Li, and Yuntao Du. 2024c.Parameter-efficient fine-tuning for pre-trained visionmodels: A survey. arXiv preprint arXiv:2402.02242. Zunnan Xu, Zhihong Chen, Yong Zhang, Yibing Song,Xiang Wan, and Guanbin Li. 2023. Bridging visionand language encoders: Parameter-efficient tuningfor referring image segmentation. In Proceedingsof the IEEE/CVF International Conference on Com-puter Vision. Li Yang, Yan Xu, Chunfeng Yuan, Wei Liu, Bing Li,and Weiming Hu. 2022. Improving visual groundingwith visual-linguistic verification and iterative rea-soning. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition. Zhengyuan Yang, Tianlang Chen, Liwei Wang, andJiebo Luo. 2020. Improving one-stage visual ground-ing by recursive sub-query construction. In Proceed-ings of the European Conference on Computer Vision. Zhengyuan Yang, Boqing Gong, Liwei Wang, WenbingHuang, Dong Yu, and Jiebo Luo. 2019. A fast andaccurate one-stage approach to visual grounding. InProceedings of the IEEE/CVF International Confer-ence on Computer Vision. Jiabo Ye, Junfeng Tian, Ming Yan, Xiaoshan Yang,Xuwu Wang, Ji Zhang, Liang He, and Xin Lin. 2022.Shifting more attention to visual backbone: Query-modulated refinement networks for end-to-end visualgrounding. In Proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition,pages 1550215512. Licheng Yu, Zhe Lin, Xiaohui Shen, et al. 2018. MAt-tNet: Modular attention network for referring ex-pression comprehension.In Proceedings of theIEEE/CVF Conference on Computer Vision and Pat-tern Recognition.",
  "Kaiyang Zhou, Jingkang Yang, Chen Change Loy, andZiwei Liu. 2022.Learning to prompt for vision-language models. International Journal of ComputerVision, 130(9):23372348": "Chaoyang Zhu, Yiyi Zhou, Yunhang Shen, Gen Luo,Xingjia Pan, Mingbao Lin, Chao Chen, Liujuan Cao,Xiaoshuai Sun, and Rongrong Ji. 2022. SeqTR: Asimple yet universal network for visual grounding.In Proceedings of the European Conference on Com-puter Vision. Hong Zhu, Qingyang Lu, Lei Xue, Mogen Xue, Guan-glin Yuan, and Bineng Zhong. 2023. Visual ground-ing with joint multi-modal representation and inter-action. IEEE Transactions on Instrumentation andMeasurement."
}