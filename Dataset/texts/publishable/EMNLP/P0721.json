{
  "Abstract": "The scaling of large language models (LLMs)is a critical research area for the efficiency andeffectiveness of model training and deployment.Our work investigates the transferability anddiscrepancies of scaling laws between DenseModels and Mixture of Experts (MoE) models.Through a combination of theoretical analysisand extensive experiments, including consistentloss scaling, optimal batch size and learningrate scaling, and resource allocation strategiesscaling, our findings reveal that the power-lawscaling framework also applies to MoE Models,indicating that the fundamental principles gov-erning the scaling behavior of these models arepreserved, even though the architecture differs.Additionally, MoE Models demonstrate supe-rior generalization, resulting in lower testinglosses with the same training compute budgetcompared to Dense Models. These findingsindicate the scaling consistency and transfergeneralization capabilities of MoE Models, pro-viding new insights for optimizing MoE Modeltraining and deployment strategies.",
  "Introduction": "The advent and scaling of large language models(LLMs), such as GPT (Brown et al., 2020; Achiamet al., 2023), Llama (Touvron et al., 2023a,b), Gem-ini (Team et al., 2023), Gopher (Rae et al., 2021),Chinchilla (Hoffmann et al., 2022), and Mistral(Jiang et al., 2023), have marked a transformativeera in artificial intelligence and natural languageprocessing. Characterized by their vast parametercounts and extensive training datasets, these mod-els have significantly advanced capabilities acrossvarious domains, including machine translation(Brown et al., 2020; Hendy et al., 2023; Garcia andFirat, 2022), logical reasoning (Huang and Chang,2022; Wei et al., 2022; Chen et al., 2021a, 2024a),and medical applications (Thirunavukarasu et al.,",
  "*Corresponding authors": "2023; Xiao et al., 2022). However, their increasingcomplexity and parameter scale posits an urgentneed for innovative scaling strategies that optimizecomputational efficiency without compromisingperformance.Historically, Dense Transformer Models havedominated due to their simplicity and scalability.And the scaling laws (Kaplan et al., 2020; Hoff-mann et al., 2022) for dense models have been thor-oughly investigated across different circumstances,such as over-training (Gadre et al., 2024) and data-limiting (Muennighoff et al., 2024; Chen et al.,2022). Despite their efficacy, the huge computa-tional demands of these models necessitate explo-ration of alternative architectures like Mixture ofExperts (MoE) (Yuksel et al., 2012; Shazeer et al.,2017; Du et al., 2022; Shen et al., 2024; Chen et al.,2021b; Xiao et al., 2021), which offer a promis-ing reduction in computational load through sparseactivations and dynamic expert routing.This paper delves into the analysis of scalinglaws for Dense and MoE Models within the con-text of LLMs. We extend foundational research onhyperparameters, such as compute budget, batchsize, and learning rate (Kaplan et al., 2020; Hoff-mann et al., 2022; McCandlish et al., 2018; Li et al.,2024; Chen et al., 2024b), to explore their transfer-ability and applicability across these architectures.Our experiments involve models up to 7 billionparameters and datasets exceeding 100 billion to-kens, aiming to uncover universal scaling behaviorspotentially applicable to both model types.Our results verify the hypothesis that certainscaling laws, particularly those related to loss andhyperparameters, may indeed be universal, bridg-ing architectural gaps between Dense and MoEModels. This universality suggests a simplificationin hyperparameter tuning across different scalesand architectures, which could significantly stream-line the training processes for various LLMs. Fur-thermore, we provide detailed analyses of the dif- ferential impacts of coefficient changes betweenMoE and Dense Models, offering both empiricaland theoretical insights into the superior data effi-ciency of MoE Models. Concretely, MoE Modelscan achieve comparable performance with fewertraining tokens than Dense Models, alleviating dataconstraints in LLM training. Our findings can besummarized as follows: Consistent Scaling Law Framework: BothMoE and Dense Models demonstrate a consis-tent and transferable scaling law framework,encompassing loss scaling as well as optimalbatch size and learning rate scaling.Thisalignment implies that the established prac-tices and insights for optimizing Dense Mod-els can be readily applied to MoE Models,potentially streamlining the process of identi-fying optimal hyperparameters and reducingexperimental complexity. Enhanced Data Efficiency in MoE Models:MoE Models demonstrate an approximate16.37% improvement in data utilization overDense Models under similar computationalbudgets. Theoretical and empirical analysessuggest that during training process, MoEModels, particularly when utilizing the AdamOptimizer, experience lower gradient noisescales. These results show that MoE Mod-els could achieve stable training with smallerbatch sizes and larger learning rates, poten-tially speeding up the training process andimproving training convergence.",
  "Related Work": "Large Language ModelsLarge language mod-els (LLMs) such as GPT (Brown et al., 2020),Llama (Touvron et al., 2023a,b), Chinchilla (Hoff-mann et al., 2022), Gopher (Rae et al., 2021),Mixtral 8x7B (Jiang et al., 2024), Switch Trans-former (Fedus et al., 2022), GLaM (Du et al., 2022),and DeepSpeed-MoE (Rajbhandari et al., 2022)have advanced significantly, categorized into DenseModels and Mixture of Experts (MoE) Models.Dense Models activate all parameters per forwardpass, while MoE Models activate only a subset,allowing for larger model scales without propor-tional increases in computational costs. Despitetheir complexity, MoE Models have shown poten-tial for superior performance and efficiency. Scaling Laws for LLMsDue to the significantcosts associated with training process, understand-ing the scaling laws of large language models(LLMs) is crucial. Studies (Bahri et al., 2021; Ka-plan et al., 2020; Bi et al., 2024) have establisheda power-law relationship between model loss andfactors like training tokens and compute budget.Recent work Yun et al. (2024) has explored theserelationships further in MoE Models, indicatingcost-effective scaling benefits but also highlightingchallenges such as expert selection and load balanc-ing. However, a systematic investigation into thescaling laws of MoE Models hyperparameters andthe transferability of scaling laws between DenseModels and MoE Models remains lacking, whichis the focus of our work. Hyperparameters EstimationAs model sizesincrease, precise optimal hyperparameter estima-tion becomes critical (Chen and Wang, 2021). Re-search McCandlish et al. (2018) has focused onoptimizing batch size and learning rates to balancetraining speed and efficiency. Novel approaches(Yang et al., 2022, 2023) like Maximal UpdateParametrization suggest that optimal hyperparame-ters for smaller models might scale to larger mod-els effectively. Our study extends these insightsto explore hyperparameter transferability betweenDense and MoE Models, focusing on resource allo-cation, learning rate, batch size, and their transferrules for Dense Models and MoE Models.",
  "Preliminary": "The scaling law of the training loss for Dense Mod-els with respect to the number of training tokensand model size has been extensively studied (Ka-plan et al., 2020; Hoffmann et al., 2022). Previouswork (Hoffmann et al., 2022) has proposed thefollowing scaling law (shown in Equation 1). Tointroduce the concept of model scale (the FLOPsdivided by the number of training tokens) as N inour work, we denote model size (number of param-eters) as P to avoid confusion.",
  ": The extrapolated scaling curves for 1.5BMixture of Experts (MoE) models. This demonstratesthat the proposed Loss Scaling Curve L(N, D, E) =A": "N E +BD + (E < 100), fits well for MoE Models(eight experts). Specifically, D is the number of tokensand N is the model scale, which is compute budget (C)divided by D, instead of the model size. E is the num-ber of experts and represents the random noise scaleof dataset. A, B, , and are all coefficients. C is the compute budget with an approximationC = 6PD. , , A and B are all coefficients.For MoE (Mixture of Experts) Models, previousresearch (Clark et al., 2022) has proposed a sep-arable scaling law (Equation 2) for loss betweenmodel size and the number of experts.",
  "P aEb(3)": "When using Equation 2 to fit the training loss curveof MoE Models, Clark et al. (2022) claim that a de-crease in performance has been observed given byexpert scaling. Specifically, the value of b increaseswith model size (in Equation 2) when model sizeis large. This suggests that as the model size in-creases, the benefit from increasing the number ofexperts E will finally decrease. To enhance thefitting ability of the scaling laws for MoE Models,a quadratic interaction term is added, resulting inEquation 4.",
  "Scaling Laws for Training Loss": "Kaplan et al. (2020) and Hoffmann et al. (2022)originally proposed a scaling law for Dense Mod-els, while Clark et al. (2022) extended this law toscenarios involving multiple experts (MoE Mod-els). Upon closer examination of Equation 3 andEquation 4, we observed that when the numberof experts (E) remains fixed, these equations canbe simplified to the first term in Equation 1. Mo-tivated by these insights, we introduce a unifiedscaling law for both Dense Models and MoE Mod-els, represented by Equation 5. Specifically, sincethe decrease in performance is observed only whenthe number of experts (E) is large, we adopt thesimplified one (Equation 2) for small E value (be-low 100). Besides, previous work (Bi et al., 2024)suggests replacing the model size P (number of pa-rameters) with model scale N, which is the resultof the FLOPs divided by the number of tokens inorder to fit Equation 1 more accurately. Finally, weget Equation 5:",
  "s.t.FLOPs(N, D) = C": "where L is the training loss, D is the number oftraining tokens, and N is the model scale, which isthe non-embedding FLOPs (C) divided by D. E isthe number of experts and we suggest E is smallerthan 100. roughly estimates the natural noise ofthe dataset, representing the minimum achievabletraining loss. A, B, , and are coefficients.In order to validate Equation 5, in our experi-ment, we fitted the training data for both 200M and700M MoE Models (both with Eight Experts) to acurve. We then used this curve to predict the train-ing loss scaling behavior of a 1.5B MoE model.The results, shown in , demonstrate thatthe formula we proposed based on previous workis applicable to MoE Models when the number ofexperts is not large. It could be observed that thereduction in benefits from increasing E is minimaland the scaling equation stands within our experi-ment scope. Therefore, we adopted the simplifiedversion. This formula suggests that the Equation 5could equal to Equation 1, given a fixed number ofexperts, for MoE Models. This consistency couldhelp us to compare the computing resource allo- Batch Size (Sequences)",
  "(b)": ": We plot the optimal learning rate values together with the corresponding training loss values acrossdifferent model sizes for both Dense Models and MoE Models. In log scale diagrams, (a) demonstrates the log-logrelationship of training loss vs. optimal learning rate for Dense Models. (b) demonstrates the log-log relationshipof training loss vs. optimal learning rate for MoE Models. This indicates that the power-law relationship remainsconsistent not only across model sizes but also across model architectures. The total overlap of the comparativeperformance interval is about 76.2%. In order to verify this, in this experiment (shownin ), we plot contour lines that representconfigurations with an equivalent number of train-ing tokens. For each contour line, we identify thepoints that achieved the minimum training loss,recording their corresponding learning rate. Then illustrates the relationship between train-ing loss and optimal learning rate in Equation 15.",
  "Estimating Optimal Resource AllocationStrategy Scaling": "After fitting the training loss (L) as a function ofthe number of tokens (D), the model scale (N), andthe number of experts (E), we proceed to derivethe optimal computing resource allocation strategyfor model scale and the number of training tokensgiven a fixed compute budget.The objective can be defined as follows: given afixed number of compute budget, how to estimatethe optimal resource allocation strategy for modelscale and the number of training tokens to minimizethe training loss.",
  "MoE Model0.4100.590": "From the data shown in , we observe thatthe exponent for the optimal model scale (N) inMixture of Experts (MoE) models is larger, whilethe exponent for the optimal number of trainingtokens (D) is smaller, compared to their DenseModel counterparts. This suggests that MoE Mod-els benefit more from increasing model size relativeto the number of training tokens.This finding helps explain why MoE Modelscan outperform larger Dense Models. The higherutilization of training data by MoE Models allowsthem to leverage their diverse sub-networks moreeffectively, capturing a broader range of featuresand patterns. Moreover, this suggests that it is moreadvantageous to allocate a larger computing budgetto MoE Models compared to Dense Models. 1 1062 1063 106 4 1066 106 Optimal Batch Size (Tokens) 1.5 2.0 2.5 3.0 3.5 4.0",
  "Estimating Optimal Batch Size": "Batch size is a crucial hyperparameter for the train-ing process. Previous works (McCandlish et al.,2018; Kaplan et al., 2020) have investigated thescaling law between optimal batch size and loss forDense Models. In this experiment, we conduct theanalysis of both models.To start, according to previous work (McCan-dlish et al., 2018), let Lopt(B) denote the optimalimprovement in the loss function when using abatch size B with the optimal step size opt(B). Ittakes into account the noise introduced by the gra-dient estimation process. Then Lopt(B) could beshown in Equation 9.",
  "(9)": "where Lmax is the maximum possible improve-ment in the loss function when the true gradientis used without noise. Here, the noise scale Bnoisemeasures the scale of the noise in the gradient esti-mates relative to the true gradient. It helps quantifyhow much the noise affects the gradient estimationprocess. The noise scale could be defined as:",
  "Emin 1) = 1(11)": "where Smin and Emin are the minimum trainingsteps and training examples needed to achieve aspecific performance. Finally, from the empiri-cal and theoretical verification (McCandlish et al.,2018; Kaplan et al., 2020; Hu et al., 2024), theoptimal batch size at a specific training loss couldbe approximated using Bopt Bnoise, then we getEquation 12.",
  "LB(12)": "where B and B are both coefficients. Bopt is theoptimal batch size given a noisy gradient and L isthe loss value.Equation 12 indicates that Bopt serves as thebalance point between training speed and data effi-ciency. It represents the optimal trade-off betweentraining speed and data efficiency. Furthermore, itindicates that as training progresses and the lossdecreases, Bopt gradually becomes larger, indicat-ing that larger batch size is required to maintain thebalance between training speed and data efficiencyas the model converges.",
  "(13)": "where opt(B) represents the optimal step size thatminimizes the expected loss from the noisy gradi-ent. And max represents the optimal step size thatminimizes the loss function when using the noise-less true gradient G to update the parameters. It isdefined as max =|G|2 GT HG. This relationship (Equa-tion 13) shows that when B is relatively small, thisEquation could be reduced as a nearly linear scaling(Granziol et al., 2022; Goyal et al., 2017). Whenthe batch size is fixed, with optimal learning ratedecreases with the increasing of noise scale.",
  "Bnoise": "Recent research by Li et al. (2024) and Granziolet al. (2022) reveals an interesting trend in the op-timal learning rate for the Adam Optimizer con-cerning the optimal batch size Bopt or noise scaleBnoise. It shows a non-monotonic behavior, indicat-ing that as the noise scale Bnoise increases, the opti-mal learning rate initially follows a nearly squareroot scaling pattern, dominated by the second term,before decreasing as the first term gains dominance.The transition point occurs when these two termsreach a balance, a critical juncture determined bythe specific values of Bnoise and B.",
  "where and are both coefficients. opt is theoptimal learning rate given a noisy gradient and Lis the loss value": "From the observation, MoE Models are likelyto have a larger optimal learning rate comparedto Dense Models when assuming the same lossvalue. Firstly, MoE Models tend to have smallernoise scale for the same loss value compared toDense Models. A smaller noise scale indicatesthat the gradient estimates are less noisy, allowingfor more accurate updates with smaller batch sizes.This efficiency with smaller batches also translatesinto requiring larger learning rate for effective op-timization. Secondly, the optimal learning rate isroughly inversely proportional to the noise scalewhen the first term dominates for Equation 14. Inconclusion, MoE Models are generally more effi-cient with smaller batch sizes and larger learningrates. It is very likely the model architecture thatmakes MoE models more efficient and more ableto handle complex data.",
  "Generalization of Scaling Law": "We predict that MoE Models could have better gen-eralization ability compared to their Dense coun-terparts. Firstly, MoE Models consist of multipleexpert sub-networks that specialize in different as-pects of the data, which enables the model to cap-ture a broader range of features and patterns andthen leads to better generalization across varioustasks and datasets. Additionally, the ensemble na-ture of MoE Models, where each expert contributesto the final prediction, reduces overfitting and im-proves robustness by combining predictions frommultiple models. Finally, the gating mechanismsin MoE Models control the contribution of eachexpert to the final prediction, acting as a form of",
  "MoE-200M14.360.9614.4014.602.77": "regularization. This regularization helps preventoverfitting by focusing on the most relevant expertsfor each input, resulting in better generalization.To investigate the generalization performance ofthe scaling law for both MoE Models (Eight Ex-perts) and Dense Models, we explore and comparethe relationship of training loss with testing lossand compute budget for both Dense Models andMoE Models (shown in ). We highlightthe stable power-law relationship interval acrossdifferent model sizes, illustrating the correlationbetween testing loss and compute budget. Notably,MoE Models consistently exhibit a smaller testingloss for a given compute budget, indicating theirsuperior generalization performance.We also explore the performance of the Denseand MoE Models on different testing sets in Ta-ble 2. It clearly shows that MoE Models could out-perform a Dense Model with comparative modelsize. The consistent trend in performance acrossdifferent datasets underscores the transferabilityand reliability of the scaling law observed in ourstudy. This finding suggests that the performanceimprovements achieved by MoE Models are notlimited to specific datasets or conditions but holdtrue across diverse testing sets, indicating robust-ness and generalizability in real-world applications.",
  "Conclusion": "In this paper, we investigate the transfer of tradi-tional scaling laws from Dense Models to Mixtureof Experts (MoE) Models. Our investigation con-firms that the power-law relationship extends toMoE Models regarding the consistency and trans- ferability of scaling strategies, including resourceallocation strategy, optimal batch size / learningrate scaling, and so on. This observation indicatesthat the fundamental principles of training dynam-ics and the behavior of scaling rules are similar forboth Models. This means existing knowledge andpractices for optimizing Dense Models can be eas-ily adapted to MoE Models, potentially reducingthe experimental burden of finding the best hyper-parameters. Besides, we find that MoE Modelsdemonstrate approximately a 16.37% improvementin data utilization efficiency compared to DenseModels with a fixed compute budget. Thus we sug-gest prioritizing increasing model scale over otherfactors when training MoE Models, highlightingtheir greater data efficiency. Finally, we use boththeoretical and empirical analysis to reveal that dur-ing the training process, MoE Models exhibit alower gradient noise scale when using the AdamOptimizer. At the same training loss value, MoEModels can achieve stable training with smallerbatch sizes and larger learning rates, potentiallyspeeding up the training process and improvingconvergence. It shows that MoE Models can makemore efficient use of training data and computa-tional resources, extracting more information pertraining token, leading to faster training times andbetter utilization of available data. These resultsoffer valuable insights for refining training and de-ployment strategies for MoE Models.",
  "Limitation": "The experiments were constrained by the availablecomputational resources. Firstly, we have not yetexplored scenarios with more than 100 experts inour experiments. It has been observed that as themodel size increases, the marginal benefit from in-creasing the number of experts tends to decrease.Moreover, although we tested our model on severalbenchmark datasets, they dont cover all domains.To ensure robust evaluation, we plan to validatethe model on more tasks and domains and inves-tigate the scaling relationships between differentmetrics. Additionally, as the number of trainingtokens exceeded 100 billion, we observed signsof overtraining for both models, indicating dimin-ishing returns in performance improvements withadditional training data. These areas remain openfor future research.",
  "Yasaman Bahri, Ethan Dyer, Jared Kaplan, JaehoonLee, and Utkarsh Sharma. 2021. Explaining neuralscaling laws. arXiv preprint arXiv:2102.06701": "Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen,Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong,Qiushi Du, Zhe Fu, et al. 2024. Deepseek llm: Scal-ing open-source language models with longtermism.arXiv preprint arXiv:2401.02954. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Zhengyu Chen, Jixie Ge, Heshen Zhan, Siteng Huang,and Donglin Wang. 2021a. Pareto self-supervisedtraining for few-shot learning. In Proceedings ofthe IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1366313672. Zhengyu Chen and Donglin Wang. 2021.Multi-initialization meta-learning with domain adaptation.In ICASSP 2021-2021 IEEE International Confer-ence on Acoustics, Speech and Signal Processing(ICASSP), pages 13901394. IEEE.",
  "Zhengyu Chen, Teng Xiao, and Kun Kuang. 2022. Ba-gnn: On learning bias-aware graph neural network.In 2022 IEEE 38th International Conference on DataEngineering (ICDE), pages 30123024. IEEE": "Zhengyu Chen, Teng Xiao, Kun Kuang, Zheqi Lv, MinZhang, Jinluan Yang, Chengqiang Lu, Hongxia Yang,and Fei Wu. 2024a. Learning to reweight for gen-eralizable graph neural network. In Proceedings ofthe AAAI Conference on Artificial Intelligence, vol-ume 38, pages 83208328. Zhengyu Chen, Teng Xiao, Donglin Wang, and MinZhang. 2024b. Pareto graph self-supervised learning.In ICASSP 2024-2024 IEEE International Confer-ence on Acoustics, Speech and Signal Processing(ICASSP), pages 66306634. IEEE. Zhengyu Chen, Ziqing Xu, and Donglin Wang. 2021b.Deep transfer tensor decomposition with orthogonalconstraint for recommender systems. In Proceedingsof the AAAI Conference on Artificial Intelligence,volume 35, pages 40104018. Aidan Clark, Diego De Las Casas, Aurelia Guy, ArthurMensch, Michela Paganini, Jordan Hoffmann, Bog-dan Damoc, Blake Hechtman, Trevor Cai, SebastianBorgeaud, George Bm Van Den Driessche, ElizaRutherford, Tom Hennigan, Matthew J Johnson,Albin Cassirer, Chris Jones, Elena Buchatskaya,David Budden, Laurent Sifre, Simon Osindero, OriolVinyals, MarcAurelio Ranzato, Jack Rae, ErichElsen, Koray Kavukcuoglu, and Karen Simonyan.2022. Unified scaling laws for routed language mod-els. In Proceedings of the 39th International Con-ference on Machine Learning, volume 162 of Pro-ceedings of Machine Learning Research, pages 40574086. PMLR. Nan Du, Yanping Huang, Andrew M Dai, Simon Tong,Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022.Glam: Efficient scaling of language models withmixture-of-experts. In International Conference onMachine Learning, pages 55475569. PMLR.",
  "William Fedus, Barret Zoph, and Noam Shazeer. 2022.Switch transformers: Scaling to trillion parametermodels with simple and efficient sparsity. Journal ofMachine Learning Research, 23(120):139": "Samir Yitzhak Gadre, Georgios Smyrnis, VaishaalShankar, Suchin Gururangan, Mitchell Wortsman,Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li,Sedrick Keh, et al. 2024. Language models scalereliably with over-training and on downstream tasks.arXiv preprint arXiv:2403.08540. Leo Gao, Stella Biderman, Sid Black, Laurence Gold-ing, Travis Hoppe, Charles Foster, Jason Phang,Horace He, Anish Thite, Noa Nabeshima, ShawnPresser, and Connor Leahy. 2020. The Pile: An800GB Dataset of Diverse Text for Language Model-ing. arXiv e-prints, arXiv:2101.00027.",
  "Xavier Garcia and Orhan Firat. 2022. Using naturallanguage prompts for machine translation. arXivpreprint arXiv:2202.11822": "Priya Goyal, Piotr Dollr, Ross Girshick, Pieter No-ordhuis, Lukasz Wesolowski, Aapo Kyrola, AndrewTulloch, Yangqing Jia, and Kaiming He. 2017. Ac-curate, large minibatch sgd: Training imagenet in 1hour. arXiv preprint arXiv:1706.02677. Diego Granziol, Stefan Zohren, and Stephen Roberts.2022. Learning rates as a function of batch size: Arandom matrix theory approach to neural networktraining. Journal of Machine Learning Research,23(173):165. Amr Hendy, Mohamed Gomaa Abdelrehim, AmrSharaf, Vikas Raunak, Mohamed Gabr, HitokazuMatsushita, Young Jin Kim, Mohamed Afify, andHany Hassan Awadalla. 2023. How good are gptmodels at machine translation? a comprehensiveevaluation. ArXiv, abs/2302.09210.",
  "Jie Huang and Kevin Chen-Chuan Chang. 2022. To-wards reasoning in large language models: A survey.arXiv preprint arXiv:2212.10403": "Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023. Mistral7b. arXiv preprint arXiv:2310.06825. Albert Q Jiang, Alexandre Sablayrolles, AntoineRoux, Arthur Mensch, Blanche Savary, Chris Bam-ford, Devendra Singh Chaplot, Diego de las Casas,Emma Bou Hanna, Florian Bressand, et al. 2024.Mixtral of experts. arXiv preprint arXiv:2401.04088. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.Brown, Benjamin Chess, Rewon Child, Scott Gray,Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.Scaling laws for neural language models. Preprint,arXiv:2001.08361. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,Dehao Chen, Orhan Firat, Yanping Huang, MaximKrikun, Noam Shazeer, and Zhifeng Chen. 2020.Gshard: Scaling giant models with conditional com-putation and automatic sharding.arXiv preprintarXiv:2006.16668. Shuaipeng Li, Penghao Zhao, Hailin Zhang, XingwuSun, Hao Wu, Dian Jiao, Weiyan Wang, ChengjunLiu, Zheng Fang, Jinbao Xue, et al. 2024. Surgephenomenon in optimal learning rate and batch sizescaling. arXiv preprint arXiv:2405.14578.",
  "Alec Radford, Jeff Wu, Rewon Child, David Luan,Dario Amodei, and Ilya Sutskever. 2019. Languagemodels are unsupervised multitask learners": "Jack W Rae, Sebastian Borgeaud, Trevor Cai, KatieMillican, Jordan Hoffmann, Francis Song, JohnAslanides, Sarah Henderson, Roman Ring, Susan-nah Young, et al. 2021. Scaling language models:Methods, analysis & insights from training gopher.arXiv preprint arXiv:2112.11446. Samyam Rajbhandari, Conglong Li, Zhewei Yao, Min-jia Zhang, Reza Yazdani Aminabadi, Ammar Ah-mad Awan, Jeff Rasley, and Yuxiong He. 2022.Deepspeed-moe: Advancing mixture-of-experts in-ference and training to power next-generation ai scale.In International conference on machine learning,pages 1833218346. PMLR. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,Andy Davis, Quoc Le, Geoffrey Hinton, and JeffDean. 2017. Outrageously large neural networks:The sparsely-gated mixture-of-experts layer. arXivpreprint arXiv:1701.06538.",
  "Arun James Thirunavukarasu, Darren Shu Jeng Ting,Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan,and Daniel Shu Wei Ting. 2023. Large languagemodels in medicine. Nature medicine, 29(8):19301940": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, et al. 2023a.Llama:Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023b.Llama 2: Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in neuralinformation processing systems, 35:2482424837.",
  "Teng Xiao, Zhengyu Chen, Zhimeng Guo, ZeyangZhuang, and Suhang Wang. 2022. Decoupled self-supervised learning for graphs. Advances in NeuralInformation Processing Systems, 35:620634": "Teng Xiao, Zhengyu Chen, Donglin Wang, and SuhangWang. 2021. Learning how to propagate messages ingraph neural networks. In Proceedings of the 27thACM SIGKDD Conference on Knowledge Discovery& Data Mining, pages 18941903. Greg Yang, Edward J Hu, Igor Babuschkin, SzymonSidor, Xiaodong Liu, David Farhi, Nick Ryder, JakubPachocki, Weizhu Chen, and Jianfeng Gao. 2022.Tensor programs v: Tuning large neural networksvia zero-shot hyperparameter transfer. arXiv preprintarXiv:2203.03466.",
  "AExperimental Settings": "The architecture of all models (including DenseModels and MoE Models) are shown in table 3 andtable 4, respectively. The number of layers, atten-tion heads, hidden dimensions, and other relevantdetails are listed in the tables.During training, we employed the AdamW op-timizer with parameters 1 = 0.9 and 2 = 0.95for all models. Following the Chinchilla law (Hoff-mann et al., 2022), we established a maximumlearning rate of 1.5 103 for smaller modelsand 2 104 for larger ones. A cosine schedulerwith a 10x learning rate decay was implementedthroughout the training process. We applied Gaus-sian smoothing with a 10-step window length toenhance the training curve.Specifically, we identified the best performancevalues within our hyperparameter range. The rangeof hyperparameter settings, including batch sizeand learning rate, was carefully selected for eachmodel size to ensure optimal performance withinthe designated FLOP budget. Our observationsindicate that performance tends to converge to op-timal values around the neighborhood of the bestsettings, as illustrated in . 2e-31.5e-31e-36e-45e-4",
  "Batch Size": "2.8782.8862.9122.8752.889 2.8222.8292.8532.8252.831 2.7472.7422.7142.7382.751 2.6892.6772.6842.6982.701 2.6612.6592.6692.6702.680 2.6962.6882.6842.6842.690 50M 2.70 2.75 2.80 2.85 2.90 : We observed that the optimal settings area Learning Rate of 1.5e-3 and a Batch Size of 256.Additionally, in the neighborhood of these settings, thetraining loss values have an error within 2%, indicatingthat the model will converge to the optimal value aroundthe best hyperparameter settings.",
  "BDiscussion of related works": "Large language models (LLMs) have obtained sig-nificant attention and undergone substantial devel-opment in recent years. They can be categorizedinto two main classes based on their parameter uti-lization during the forward pass: Dense Models and MoE (Model of Experts) Models. Dense Mod-els, such as GPT-2 (Radford et al., 2019), GPT-3 (Brown et al., 2020), Llama (Touvron et al.,2023a,b), Chinchilla (Hoffmann et al., 2022), andGopher (Rae et al., 2021), maintain a total param-eter count equal to the active parameter count perforward pass. On the other hand, MoE Models(Shazeer et al., 2017) activate only a subset of totalparameters during training, as seen in models suchas Mixtral 8x7B (Jiang et al., 2024), Switch Trans-former (Fedus et al., 2022), GShard (Lepikhin et al.,2020), GLaM (Du et al., 2022), and DeepSpeed-MoE (Rajbhandari et al., 2022). Dense Models areknown for their simplicity in implementation andtraining. However, MoE Models can scale to signif-icantly larger total parameter counts without a pro-portional increase in computational cost. Despitechallenges like load balancing and expert selectionfaced by MoE Models, prior research indicates thatthey offer superior performance due to increasedmodel capacity and enhanced data efficiency.Due to the significant costs associated with train-ing process, understanding the scaling laws of largelanguage models (LLMs) is crucial. Some pre-vious studies (Kaplan et al., 2020; Bahri et al.,2021) have proposed a power-law relationship be-tween loss and various factors like the number ofnon-embedding parameters, training tokens, andcompute budget across different magnitudes. No-tably, Kaplan et al. (2020) found that increasing themodel size by 8 times only requires a roughly 5xincrease in data to avoid penalties. In contrast toearlier findings, Hoffmann et al. (2022) implementsoptimized training configurations, which includethe use of training tokens and learning rate sched-ules, and recommends scaling training tokens inproportion to model size. Additionally, researchby Bi et al. (2024) explores the scaling laws ofbatch size and learning rate in relation to modelscale (non-embedding FLOPs per token), offeringa more precise estimation. They propose an alloca-tion strategy for scaling up models and data basedon dataset quality. While some studies (Li et al.,2024; McCandlish et al., 2018) link the optimalbatch size to gradient noise scale and optimizertype, recent attention has shifted towards Mixtureof Expert (MoE) models due to their potential costsavings. Fedus et al. (2022) investigates the scalingproperties of MoE Models and suggest that havingmore parameters (experts) with a fixed computa-tional budget accelerates training and surpassesdense Transformer baselines. On the other hand, Yun et al. (2024) incorporate the hyperparameterE (number of experts) into existing scaling laws,revealing diminishing returns with increasing ex-pert numbers. However, a systematic investigationinto the scaling laws of MoE Models hyperparame-ters and the transferability of scaling laws betweenDense Models and MoE Models remains lacking,which will be the focus of our work.With the increasing size of models in deep learn-ing, hyperparameter estimation has gained signif-icant attention due to the huge costs associatedwith hyperparameter tuning. In training large lan-guage models, several key hyperparameters requirecareful consideration and selection. Previous re-search has offered valuable insights that inspireour approach.For instance, McCandlish et al. (2018) highlights the trade-off between trainingspeed and efficiency, focusing on the critical batchsize, which can be measured by the gradient noisescale. Other studies (Goyal et al., 2017; Granziolet al., 2022; Li et al., 2024) propose a linear scalingrule for adjusting learning rates as a function ofminibatch size for SGD optimizers, while a squareroot scaling rule is suggested for adaptive optimiz-ers. Some works focus on optimal hyperparametertransfer. For instance, unlike traditional BayesianOptimization (BO)-based methods (Horvth et al.,2021; Perrone et al., 2018), recent studies (Yanget al., 2022, 2023) emphasize transferring hyper-parameters across model scales. They introduce anovel zero-shot strategy known as Maximal UpdateParametrization (P), demonstrating that optimalhyperparameter choices for smaller models remaineffective for larger ones. However, there is a lackof research on hyperparameter transfer across MoEModels and Dense Models. Our work specificallyemphasizes critical hyperparameters transfer suchas resource allocation, learning rate, and batch size,and their transfer rules for Dense Models and MoEModels."
}