{
  "Abstract": "Program of Thoughts (PoT) is an approachcharacterized by its executable intermediatesteps, which ensure the accuracy of the logi-cal calculations in the reasoning process. Cur-rently, PoT primarily uses Python. However,relying solely on a single language may resultin suboptimal solutions and overlook the poten-tial benefits of other programming languages.In this paper, we conduct comprehensive ex-periments on the programming languages usedin PoT and find that no single language con-sistently delivers optimal performance acrossall tasks and models. The effectiveness of eachlanguage varies depending on the specific sce-narios. Inspired by this, we propose a taskand model agnostic approach called MultiPoT,which harnesses strength and diversity fromvarious languages. Experimental results revealthat it significantly outperforms Python Self-Consistency. Furthermore, it achieves compa-rable or superior performance compared to thebest monolingual PoT in almost all tasks acrossall models. In particular, MultiPoT achievesmore than 4.6% improvement on average onChatGPT (gpt-3.5-turbo-0701)1.",
  "Introduction": "Program of Thoughts (PoT) aims to prompt CodeLarge Language Models (Code LLMs) to de-compose complex problems into successive exe-cutable codes (Gao et al., 2023; Chen et al., 2022).Through execution by an external interpreter, thefinal results are accurately obtained, decoupling thecomputational process from the LLMs. PoT signif-icantly reduces computation errors and improvesreasoning performance (Wang et al., 2023a). Sub-sequently, benefiting from its flexibility and scala-bility, it is gradually applied to a broader spectrumof fields like image reasoning (Surs et al., 2023;",
  "Answer:03/31/2007": ":Comparison of PoT with different PLs.Pythons timedelta lacks support for year compu-tation, leading to a leap year (2008 has 366 days) errorby subtracting 365 days. R and JavaScript directly com-pute the year and get the correct answer. Gupta and Kembhavi, 2023), financial QA (Koncel-Kedziorski et al., 2023) and robotic control (Liet al., 2023a). Nowadays, PoT has become a keymethod for enabling intelligence in agents (Yanget al., 2024; Wang et al., 2024). The widespreadapplicability highlights its significance.Despite significant progress, PoT has a notablelimitation: to the best of our knowledge, all re-search on PoT focuses on Python. However, sinceCode LLMs are capable of multilingual genera-tion,2 and most of the reasoning tasks are language-independent, many other programming languages(PLs) can also be applied to PoT, especially whenconsidering their unique strength and diversity.From the perspective of tasks, different PLs repre-",
  "In this paper, our multilingual represents multiple pro-gramming languages, not natural languages": "sent PoT in different forms. As shown in ,the representation and calculation of dates in R ismore concise than that in Python, which can reducethe complexity when LLMs generate PoTs. Fromthe perspective of models, their multilingual abil-ity is inconsistent. For instance, C++ of DeepseekCoder outperforms Python on the code generationtask (Guo et al., 2024). It is natural to wonderwhether this phenomenon also occurs on reasoningtasks. Therefore, a crucial question is raised withthese perspectives: Is Python truly the optimal lan-guage for all tasks and models for PoT? Relying onPython may lead to a local optimum. In ,Pythons timedelta does not support year, re-sulting in a miscalculation for the leap year. Incontrast, R and JavaScript yield the correct answer.Motivated by this, we conduct comprehensiveexperiments for multilingual PoTs. Beyond Python,we select four PLs: three widely used general lan-guages (JavaScript, Java, and C++) and a nichebut comprehensive language (R). For a comprehen-sive comparison, we identify five distinct sub-taskswithin reasoning tasks: math applications (Cobbeet al., 2021; Patel et al., 2021; Miao et al., 2020),math (Hendrycks et al., 2021), tabular, date, andspatial (Suzgun et al., 2022). We select four back-bone LLMs: ChatGPT (gpt-3.5-turbo-0701) andthree strongest Code LLMs (Starcoder (Li et al.,2023b), Code Llama (Roziere et al., 2023), andDeepseek Coder (Guo et al., 2024)). Under bothgreedy decoding and Self-Consistency (Wang et al.,2022) settings, we answer that Python is not al-ways the optimal choice, as the best language de-pends on the specific task and model being used.In addition to the analysis contribution, to lever-age the strength of multiple PLs, we further intro-duce a simple yet effective approach, called Mul-tiPoT (Multilingual Program of Thoughts). It isa task and model agnostic approach, which usesLLMs to synchronously generate PoTs with vari-ous PLs and subsequently integrates their resultsvia a voting mechanism. The use of multiple PLsalso provides greater diversity and reduces theprobability of repeating the same errors comparedto single-language sampling. Experimental resultsdemonstrate that MultiPoT outperforms PythonSelf-Consistency significantly. Furthermore, Mul-tiPoT effectively matches or even surpasses thetop-performing languages across nearly all tasksand models, and outperforms on averages. Espe-cially on both ChatGPT and Starcoder, MultiPoTperforms the best on four out of five tasks, with",
  "Program of Thoughts": "CoT is a specific form of in-context learning (Weiet al., 2022; Brown et al., 2020; Chowdhery et al.,2023). Its demonstrations consist of intermedi-ate steps imitating the human thought process.It significantly enhances models reasoning capa-bilities (Yang et al., 2023) but suffers from er-rors associated with calculations (Madaan andYazdanbakhsh, 2022).CoT always uses Self-Consistency (Wang et al., 2023c) to improve an-swer accuracy through sampling and voting.PoT (Chen et al., 2022; Gao et al., 2023) is anextension of CoT to avoid incorrect calculation.It represents intermediate steps as comments andcode and executes the entire program with an inter-preter to obtain answers. PoT not only excels in rea-soning tasks but has rapidly extended to practicalapplications, including chart understanding, imagereasoning, financial QA and robotic control (Zhanget al., 2024; Surs et al., 2023; Gupta and Kem-bhavi, 2023; Koncel-Kedziorski et al., 2023; Liet al., 2023a). It has become a key method foragents to perform complex reasoning and tool invo-cation (Yang et al., 2024; Wang et al., 2024). It isimportant to note that all previous PoT work onlyuse Python. For the first time, we are exploringPoTs that use multiple PLs.",
  "Python PoT in Demostration": "penguins = [ {\"name\": \"Louis,...}, {\"name\": \"James\",...},...]penguins_less_than_8_years = sum(1 for penguin in penguins if penguin[\"age\"] < 8)sorted_penguins =sorted(penguins, key=lambda p: p[\"name\"]) C++ PoT in Demonstration #include <algorithm>#include <vector>...struct Penguin {string name;int age;...};... for (const auto& penguin : penguins) if (penguin.age < 8) count++; sort(penguins.begin(), penguins.end(), [](const Penguin &a, const Penguin &b) return a.name < b.name;",
  "Python PoTQuestion": ": An overview of MultiPoT and Self-Consistency. MultiPoT first constructs prompts for each PL, ensuringa consistent reasoning process while also considering the distinct coding styles. It then integrates these PLs:generating multilingual PoTs based on the prompts, executing them to gather results, and finally voting for theanswer. In contrast to Self-Consistencys single-language focus, MultiPoT leverages multiple PLs. handle multilingual programming (Kocetkov et al.,2022; Nguyen et al., 2023; Gao et al., 2020; Ni-jkamp et al., 2023; Chen et al., 2021). This capabil-ity extends code tasks like generation, optimization,translation, and repair to other languages beyondPython (Gimeno et al., 2023; Shypula et al., 2023;Zhang et al., 2023; Wu et al., 2023). Despite theprogress, current multilingual research (Jin et al.,2023; Joshi et al., 2023; Khare et al., 2023) mainlyfocuses on code-related tasks, neglecting the po-tential of PLs as tools to assist in other tasks. Ad-ditionally, these studies often treat each languageseparately without interaction. Our study pioneersthe use of multiple PLs in reasoning tasks and in-troduces a novel integrated approach, leveragingthe collective strength and diversity of various PLsto enhance overall performance.",
  "Methodology": "provides an overview of MultiPoT andSelf-Consistency to highlight their differences.Concretely, MultiPoT consists of two main steps.First, a dedicated prompt is designed for each PLto sufficiently leverage the capability of the modelwith regard to the PL (.1). Second, PoTsin various PL are respectively generated by prompt-ing the LLM with the prompts. The final answeris obtained by executing the PoTs and integratingtheir results via a voting mechanism (.2).Distinct from Self-Consistency, which relies on",
  "Multilingual Prompts Construction": "To instruct a LLM to generate PoT for a given ques-tion, a demonstration is included in the prompt.The demonstration consists of an example ques-tion and PoT. To ensure fairness, demonstrationsof various PLs share the same example questions.Based on that, to efficiently leverage the capabilityof a LLM with regard to a PL, each PL is providedwith a dedicated example PoT, taking into accountits language-specific characteristics (Wang et al.,2023b). Note that language-agnostic features, suchas algorithms and data structures, remain the samefor example PoTs of all PLs, ensuring an identicalreasoning process.Concretely, the language-specific characteris-tics of each PL for constructing its dedicated ex-ample PoT includes Built-in Content, SpecialSyntax, Type Definition, and Varibale Naming. provides some examples of the charac-teristics. (1) while Python can directly employthe sort function, C++ has to load it from thealgorithm library. Regarding variables, Pythonslist is more similar to C++s vector than itsarray. (2) List comprehension like sum(1 forpenguin in penguins if penguin[\"age\"] <8) is a standard syntax in Python. However, astraightforward for-loop is the common practice in other PLs. (3) Static PLs such as C++ requireto define the variable type. We carefully defineint and double variables to ensure computa-tional accuracy and enhance flexibility by definingstruct. (4) We keep the naming styles of eachPL. For instance, Python uses Snake Case, whereasJava favors Camel Case (secondPenguin). Ap-pendix A.5 shows the demonstrations. The aboveexamples present the variations in example PoTsacross different PLs.To accurately assess themodels capability in a specific PL, it is crucialto carefully consider its characteristics during theprocess of constructing.Based on identical reasoning process, we suc-cessfully craft demonstrations of each PL exhibit-ing its characteristics. By adding the question afterthe demonstration, we get the prompt for each PL.",
  "Integration": "While Self-Consistency enhances performance bysampling to explore more reasoning paths, it canlead to repeated errors across different samples. Incontrast, MultiPoT constructs multilingual promptsand generates PoTs in multiple PLs, significantlyincreasing the diversity of results.Specifically, after constructing prompts for eachPL, models generate corresponding PoTs, whiletracking cumulative probabilities. These probabili-ties indicate the models confidence in each answer,with higher probabilities denoting greater confi-dence. PoTs are then executed and results are col-lected. The final answer is determined by voting onthese results. In cases of tied votes, answers withhigher cumulative probabilities are favored. Theintegration of multiple PLs introduces more poten-tial correct answers and reduces the probability ofthe same errors in candidate results.",
  "Programming Languages": "When selecting PLs to compare with Python, wefocus on diversity. JavaScript is the most popu-lar language on GitHub (GitHub, 2023) and hasless overlap in application with Python, particu-larly in the ML/AI domains. R is a flexible andpowerful language like Python but has much lessdata in pre-training data. The three PLs above aredynamic languages that do not require explicit vari-able type definitions. To incorporate the diversityof language types, we select the two most com-mon static languages, Java and C++. The latter is closer to low-level programming and has fewerextension packages. We do not include C due toits high similarity with C++. These five languagesoffer a diverse range of application scenarios, datavolumes, and language types compared to Python.",
  "Tasks": "We select representative and discriminating tasks.We initially select four tasks from Gao et al. (2023):Math Application (Appl.), Date, Tabular andSpatial, and add the task Math. Appl. containselementary-level mathematical application prob-lems (GSM8K, SVAMP, Asdiv (Cobbe et al., 2021;Patel et al., 2021; Miao et al., 2020)). Date, Tabu-lar, and Spatial are extracted from BBH-Hard (Suz-gun et al., 2022) (Date Understanding, Penguinsin a Table, Reasoning about Coloured Objects).These tasks assess understanding and reasoningabout temporal sequences, structured text, and spa-tial positioning respectively. Math, consisting ofthe transformed MATH (Hendrycks et al., 2021)dataset. The difference between Math and Appl.lies in the level of difficulty. Math is more chal-lenging and directly describes the math questionwithout scenarios. The five tasks are distinct andrepresentative of the evaluation of reasoning ca-pabilities. They are language-agnostic, meaningthat they can be performed in any PL, effectivelydemonstrating the models reasoning ability acrossdifferent languages. The additional details of thetasks are in the Appendix A.1.",
  "Metric": "Remaining consistent with previous work (Chenet al., 2022; Gao et al., 2023), the metric is accu-racy. For tasks whose ground truth are real numbers(Appl./Math), the answer is considered correct if itsdifference from the ground truth is less than 1e-3;for tasks with string-type ground truth (Date/Tabu-lar/Spatial), the answer is considered correct onlyif it is exactly the same as the ground truth.",
  "Backbone LLMs": "As the previously used code-davinci family is nolonger accessible, we select four backbone LLMs,including the three strongest Code LLMs: Star-coder (15B), Code Llama (34B), and DeepseekCoder (33B). We select the base versions. Theexperiments of the Python version are discussedin .2, and the results are consistent withour conclusions and methodology. ChatGPT isalso utilized as a representative of code-capable (a) Starcoder(b) Code Llama(c) Deepseek Coder PythonRC++JavaJavaScript",
  "AVGAVGAVG": ": The greedy decoding performance of three models across five tasks in five different PLs. AVG denotes theaverage performance of a PL across all tasks. Each language performance is expressed as a ratio to the highest-performing language for that specific task. The center of the circle represents 50%. Detailed numerical data areprovided in the in Appendix A.2.",
  "Inference Details": "We combine Chen et al. (2022) and Gao et al.(2023)s prompt templates for few-shot inference.We fix the questions from the previous work andwrite code in the respective PLs. The number ofquestions in each task is shown in Appendix A.1.When sampling for Self-Consistency, we followChen et al. (2022) and set t = 0.4, top_p = 1. Fora fair comparison with MultiPoT which integratesfive languages, we set k = 5.",
  "Results": "In this section, we first discover that Python is notthe best language for all tasks and all models fromthe results of greedy decoding. There is no suchperfect language. The performance of each PLvaries greatly depending on the task and model(.1). After Self-Consistency, the perfor- mance discrepancy still exists. Finally, by inte-grating multiple languages, MultiPoT significantlyoutperforms Python. Furthermore, its performancematches or exceeds the best monolingual PoTs inalmost all scenarios and achieves improvement ontask and model averages (.2).",
  "Comparison among PLs": "Python is not the optimal language choice. Fig-ure 3 shows the performance gap between each lan-guage and the best-performing language on eachtask of the three Code LLMs. It illustrates thatPython does not achieve the best performance onany of the tasks for any of the Code LLMs. OnDeepseek Coder, Python is even the worst on av-erage. shows the greedy decoding resultsof ChatGPT. Although Python performs best onTabular, it falls short by 2.9% and 8.4% comparedto the best PL on Math and Date respectively. Thepreference for Python among humans may be dueto its simple syntax and high readability, but it is asubjective bias that PoT only needs it. Relying onPython leads to a suboptimal outcome.",
  "However, it is important to note that there is noone-size-fits-all language. The gap between PLs issignificant when considering each task and model": "TheperformanceofeachPListask-dependent. AVG performance does not fully cap-ture the disparity among languages.Java andJavaScript performances of Starcoder differ by only0.41% on AVG, but by 6.71% on Tabular. Whilethe difference between the best and worst PLs ofChatGPT on AVG is less than 2% in , thereare four tasks whose gap among languages exceeds6%. Different languages are suitable for differenttasks. indicates that, except for C++, allPLs excel in at least one task on ChatGPT. More-over, on ChatGPT, except for JavaScript, each lan-guage also ranks as the least effective in at leastone task. A language that performs exceptionallywell in one task might underperform in another.For instance, R demonstrates superior performanceon Date for both Code LLMs and ChatGPT, yet itis the least effective on Appl. and Math. The performance of each PL is model-dependent. Code LLMs and ChatGPT differ sig-nificantly. The results of three Code LLMs are av-eraged and compared with ChatGPT in . Itshows that, on Appl., C++ performs best on CodeLLMs but ranks second-to-last on ChatGPT; onMath, JavaScript excels on Code LLMs but sim-ilarly ranks second-to-last on ChatGPT; and onSpatial, Java ranks second-highest on Code LLMs(with only a 0.05% less than C++) but is second-to-last on ChatGPT. Even within Code LLMs, dis-parities between models are evident.",
  "shows that Code Llama has a clear preference for": "Java, which keeps the top two ranks across all tasks,yet is not observed on the remaining models. OnDeepseek Coder, C++ leads on average, whereasit ranks last on the other models. R ranks secondon Spatial on Deepseek Coder, but the worst on theother two Code LLMs.These variations demonstrate that different PLsexhibit unique strengths and diversity due tocomplex factors such as task suitability and modelpreference. A further error analysis of the experi-mental results is shown in Appendix A.3.",
  "Comparision between Self-Consistencyand MultiPoT": "Self-Consistency does not eliminate perfor-mance disparities between PLs, despite it signifi-cantly improving the performance. presentsthe Self-Consistency results. The inherent strengthof different languages persist. The optimal PL oneach scenario is generally consistent with greedydecoding results, except Python emerges as thesuperior language on Math on all model. The weak-nesses of each language is further amplified. Forexample, on Date of Deepseek Coder, C++ alreadyhad the lowest performance in greedy decoding,and Self-Consistency increases this gap even more.As a result, C++ shifts from the highest average per-formance in greedy decoding on Deepseek Coderto the lowest in Self-Consistency, despite remain-ing the best on Appl., Tabular, and Spatial. A singlelanguage offers limited diversity. When faced withtasks outside its strength, monolingual samples of-ten make the same mistakes repeatedly, resultingin incorrect answers being chosen through voting.Different from Self-Consistency relying on a RC++ Python JavaJS (a) Starcoder Accuracy(%) ReasoningGenerationData Amount RPythonJSJavaC++ (b) Deepseek Coder RC++JSPython Java (c) Code Llama Accuracy(%) RC++Java PythonJS (d) ChatGPT Data Amount(%) : The reasoning ability, code generation ability,and percentage in pre-training data for different lan-guages. Generation lacks data for R. The horizontalcoordinates of each model are ranked according to therise in reasoning performance (excluding R).",
  "single PL, MutliPoT integrates multiple PLs. Itnot only leverages the distinct strength of eachPL, but also utilizes their greater diversity toreduce the probability of repeating the same errors": "MultiPoT significantly outperforms Pythonon almost all scenarios. It enhances performancein tasks or models where Python is weak. Acrossthe four models, MultiPoT improves upon Pythonsperformance on Date by at least 15%, and in aver-age (AVG) performance by 4.33% to 7.32%. Fur-thermore, MultiPoT also capitalizes on Pythonsstrength. On Math, where Python excels, MultiPoTalso achieves the best results, except in DeepseekCoder, where it slightly trails Python but remainssignificantly ahead of other languages. MultiPoT achieves comparable or superiorperformance to the best monolingual resultsacross all tasks and models. It is task-agnostic.It surpasses Self-Consistency on four tasks, rank-ing second on the remaining task, regardless ofwhether on Code LLMs average () or Chat-GPT. MultiPoT is also model-agnostic. It is the topperformer across all LLMs on Tabular. On AVG,MultiPoT outperforms the best monolingual resulton all four models. Particularly on ChatGPT andStarcoder, it exhibits an improvement of over 4.6%. The performance of PLs depends on the taskand model. Analyzing the interplay of PL, task,and model in practical applications is challenging.Therefore, MultiPoT is a great choice which hasconsistently high performance across scenarios.",
  "Reasoning Ability of Different Languages": "In .1, we note that the ranking of the av-erage performance of PL varies on each model.The language distribution in the pre-training dataof Starcoder and Deepseek Coder offers insightsinto whether data amount, defined as the percent-age of each language in the pre-training corpus,impacts reasoning capabilities. Moreover, we areinterested in examining whether code generationand reasoning of multilingual ability are aligned.The difference between the two tasks is elucidatedin Appendix A.4. To assess code generation ability,we utilize the results of each model on the Multilin-gual HumanEval benchmark, focusing on the fouravailable languages, excluding R due to a lack ofevaluation dataset.Data distribution influences but does not com-pletely determine reasoning ability. shows the relative relationships among reasoningperformance of C++, Python, and Java are consis-tent with data distribution on Starcoder. However,R demonstrates unexpectedly strong performance,which has an extremely low percentage in bothmodels. C++ has less data amount than Java onDeepseek Coder, but better reasoning performance.This suggests that there are other factors affectingperformance besides data distribution.Code generation abilities do not always alignwith reasoning abilities. We compare the fourlanguages excluding R in . On ChatGPT,the reasoning and code generation abilities of C++,Java, and Python align perfectly. However, an oppo-site trend is observed in Deepseek Coders Python,JavaScript, and Java, where the two abilities di-verge significantly. It highlights the necessity oftesting the reasoning abilities of different PLs.Zero-shot reasoning ability shows consider-able inconsistency when compared to 3-shot rea-soning ability. presents the results of zero-shot and 3-shot experiments using Code Llama 34Bon Appl., where AC denotes accuracy, and incor-rect outcomes are further classified into RuntimeErrors (RE) and Wrong Answers (WA). The resultsreveal particularly steep declines in R, C++, andJavaScript, largely driven by a significant increasein RE. This suggests that different PLs exhibit vary-ing levels of sensitivity to shot settings. Two promi-nent error patterns emerge from the zero-shot out-puts: (1) LLM frequently generates repetitive com-ments until reaching the maximum sequence length,",
  "MultiPoT Analysis": "MutliPoT has the highest coverage rate. Unlikethe voting mechanism which requires a majorityfor the correct answer, the coverage rate is the per-centage of questions that have at least one correctanswer in five candidate answers in the dataset. Forexample, if the candidate answers to a question are(6 5 5 5 7) and the ground truth is 7, the ques-tion is covered. Coverage rate can be consideredas an upper bound because this metric representsthe proportion of all potentially solvable problems, (a) Starcoder Performances(%) (b) Deepseek Coder Data Amount Little -> LargeData Amount Large -> Little The Number of Languages",
  ": The impact of the number of integrating PLs.We test the different order of adding languages": "assuming there exists a selection mechanism bet-ter than the current voting mechanism. demonstrates coverage rates on all four models andMultiPoT achieves the highest. The monolingualsampling covers less than the multilingual attempts,highlighting that the strength of different PLs ex-ists. MultiPoT effectively utilizes the strength ofdifferent PLs and has the highest upper bound.MutliPoT has stable performance. When re-sults are tied, the top-ranked result is selected. Dif-ferent sorting methods reflect the stability. shows the performance fluctuation. MultiPoT isless than 1% across various sorting criteria, in-cluding PoT length, randomness, or data amountfrom pre-training, compared to the default cumula-tive probability sorting. This indicates that Multi-PoT consistently selects the correct answer directly,with few instances of ties with incorrect answers.This also suggests a lower probability of differentPoTs making the same errors.More PLs are better. We investigate the im-pact of the number of PLs on MultiPoT. On bothStarcoder and Deepseek Coder, we incrementallyadd languages in both ascending and descendingorder of data amount in . The results showthat MultiPoTs performance improves with morePLs, regardless of the order. This suggests that",
  "All Dynamic50.4174.92Dynamic + Static51.8775.77": ": The impact of different language type combina-tions on MultiPoT. All Dynamic indicates that the threelanguages are all dynamic, and Dynamic+Static indi-cates a combination of dynamic and static languages. MultiPoT is highly scalable and performance canbe further enhanced by incorporating more PLs.More language types are better. Python, R,and JavaScript are dynamic languages, while C++and Java are static. To investigate whether a di-verse set of language types enhances MultiPoTsperformance, we focus on three PLs. On Starcoderand ChatGPT, JavaScript emerges as the highest-performing dynamic language, surpassing Java,which leads between the static languages. Con-sequently, we integrate JavaScript, Python, andR as All Dynamic and combine Java, Python,and R to represent Dynamic + Static.The re-sults in indicate that replacing the higher-performing JavaScript with the lower-performingJava improves performance. This suggests thatmore language types can provide more diversity toMultiPoT, thereby further enhancing performance.MultiPoT also works on Python model. Ourprior experiments with Code LLMs utilize the Baseversion. However, Code LLMs also have a Python-specific version trained with additional Python cor-pora. Evaluating MultiPoT on this Python ver-sion, as shown in , we find that PythonSelf-Consistency improves on Appl. and Mathbut declines on the other tasks compared to theBase model. Moreover, MultiPoT still outperformsPython Self-Consistency on all tasks except Math,highlighting the adaptability of MultiPoT. Notably,MultiPoTs performance on the Python model islower across all tasks than on the Base model. Thissuggests that extensive training on monolingual cor-pora might diminish the Base models multilingualabilities on reasoning tasks.MultiPoT is better than CoT Self-Consistency.",
  ": Comparison between Self-Consistency of CoT,PoT (Python), and MultiPoT. CoT results are based onDeepseek LLM v2, while PoT and MultiPoT are basedon Deepseek Coder v2": "To compare the performance of CoT and PoT inscenarios where precise mathematical calculationsare not required, we conduct experiments on Date,Table, and Spatialusing Deepseek LLM v2 (a405B MoE LLM) for CoT and Deepseek Coderv2 (continually pretrained from Deepseek LLMv2) for PoT. The results, shown in , indicatethat PoT achieves better Self-Consistency than CoTon Table and Spatial, with MultiPoT further im-proving performance. On Table, the improvementdemonstrates the advantage of PoT in understand-ing and reasoning over structured data. On Date,however, PoT slightly underperforms CoT, whichis primarily due to PoT interpreting the differencebetween two dates as exclusive, while natural lan-guage typically considers it inclusive. Nevertheless,the results suggest that PoT remains valuable in sce-narios where precise calculations are unnecessary,and MultiPoT continues to be effective.",
  "Conclusion": "Regarding the reliance on Python in PoT, we con-ducted extensive experiments across various mod-els and tasks using multiple PLs. Our findingsshow that Python is not always the best choice; theoptimal language depends on the specific task andmodel. Building on this insight, we introduce Mul-tiPoT, a simple yet effective multilingual integratedmethod that leverages the strengths and diversity ofdifferent PLs. MultiPoT significantly outperformsPython and achieves matches or exceeds perfor-mance to the best monolingual outcomes in nearlyall scenarios. With its high stability, MultiPoT of-fers a promising avenue for future research.",
  "Limitations": "Our study is comprehensive, but has certain limita-tions that we plan to address in future research. Dueto computational resource constraints, we confineour experiments to a select number of commonlyused programming languages (PLs). While thesePLs are representative, they do not encompass theentire spectrum of languages used in programming.Future research could investigate the advantagesof incorporating a broader range of programminglanguages. This may reveal further insights andimprove the relevance of our findings.",
  "Acknowledge": "We gratefully acknowledge the support of the Na-tional Natural Science Foundation of China (NSFC)via grant 62236004, 62206078, 62441603 and62476073 and the support of Du Xiaoman (Bei-jing) Science Technology Co., Ltd. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020.Language models are few-shot learners.In Ad-vances in Neural Information Processing Systems,volume 33, pages 18771901. Curran Associates,Inc. Mark Chen, Jerry Tworek, Heewoo Jun, QimingYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-plan, Harri Edwards, Yuri Burda, Nicholas Joseph,Greg Brockman, Alex Ray, Raul Puri, GretchenKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-try, Pamela Mishkin, Brooke Chan, Scott Gray,Nick Ryder, Mikhail Pavlov, Alethea Power, LukaszKaiser, Mohammad Bavarian, Clemens Winter,Philippe Tillet, Felipe Petroski Such, Dave Cum-mings, Matthias Plappert, Fotios Chantzis, Eliza-beth Barnes, Ariel Herbert-Voss, William HebgenGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,William Saunders, Christopher Hesse, Andrew N.Carr, Jan Leike, Josh Achiam, Vedant Misra, EvanMorikawa, Alec Radford, Matthew Knight, MilesBrundage, Mira Murati, Katie Mayer, Peter Welinder,Bob McGrew, Dario Amodei, Sam McCandlish, IlyaSutskever, and Wojciech Zaremba. 2021. Evaluatinglarge language models trained on code.",
  "Wenhu Chen, Xueguang Ma, Xinyi Wang, andWilliam W. Cohen. 2022.Program of thoughtsprompting: Disentangling computation from rea-soning for numerical reasoning tasks.CoRR,abs/2211.12588": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts,Paul Barham, Hyung Won Chung, Charles Sutton,Sebastian Gehrmann, Parker Schuh, Kensen Shi,Sasha Tsvyashchenko, Joshua Maynez, AbhishekRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-odkumar Prabhakaran, Emily Reif, Nan Du, BenHutchinson, Reiner Pope, James Bradbury, JacobAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,Toju Duke, Anselm Levskaya, Sanjay Ghemawat,Sunipa Dev, Henryk Michalewski, Xavier Garcia,Vedant Misra, Kevin Robinson, Liam Fedus, DennyZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,Barret Zoph, Alexander Spiridonov, Ryan Sepassi,David Dohan, Shivani Agrawal, Mark Omernick, An-drew M. Dai, Thanumalayan Sankaranarayana Pil-lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,Rewon Child, Oleksandr Polozov, Katherine Lee,Zongwei Zhou, Xuezhi Wang, Brennan Saeta, MarkDiaz, Orhan Firat, Michele Catasta, Jason Wei, KathyMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,and Noah Fiedel. 2023. Palm: Scaling language mod-eling with pathways. Journal of Machine LearningResearch, 24(240):1113. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, ReiichiroNakano, et al. 2021. Training verifiers to solve mathword problems. arXiv preprint arXiv:2110.14168. Leo Gao, Stella Biderman, Sid Black, Laurence Gold-ing, Travis Hoppe, Charles Foster, Jason Phang,Horace He, Anish Thite, Noa Nabeshima, ShawnPresser, and Connor Leahy. 2020.The pile: An800gb dataset of diverse text for language modeling. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,Pengfei Liu, Yiming Yang, Jamie Callan, and Gra-ham Neubig. 2023. Pal: Program-aided languagemodels. In International Conference on MachineLearning, pages 1076410799. PMLR.",
  "Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wen-feng Liang. 2024. Deepseek-coder: When the largelanguage model meets programming the rise ofcode intelligence": "Tanmay Gupta and Aniruddha Kembhavi. 2023. Vi-sual programming: Compositional visual reasoningwithout training. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recog-nition, pages 1495314962. Dan Hendrycks, Collin Burns, Saurav Kadavath, AkulArora, Steven Basart, Eric Tang, Dawn Song, andJacob Steinhardt. 2021. Measuring mathematicalproblem solving with the math dataset. In Thirty-fifth Conference on Neural Information ProcessingSystems Datasets and Benchmarks Track (Round 2).",
  "Avishree Khare, Saikat Dutta, Ziyang Li, Alaia Solko-Breslin, Rajeev Alur, and Mayur Naik. 2023. Under-standing the effectiveness of large language modelsin detecting security vulnerabilities": "Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li,Chenghao Mou, Carlos Muoz Ferrandis, Yacine Jer-nite, Margaret Mitchell, Sean Hughes, Thomas Wolf,et al. 2022. The stack: 3 tb of permissively licensedsource code. arXiv preprint arXiv:2211.15533. Rik Koncel-Kedziorski, Michael Krumdick, Viet Lai,Varshini Reddy, Charles Lovering, and Chris Tan-ner. 2023.Bizbench: A quantitative reasoningbenchmark for business and finance. arXiv preprintarXiv:2311.06602. Chengshu Li, Jacky Liang, Fei Xia, Andy Zeng, SergeyLevine, Dorsa Sadigh, Karol Hausman, Xinyun Chen,Li Fei-Fei, and brian ichter. 2023a. Chain of code:Reasoning with a language model-augmented codeinterpreter. In NeurIPS 2023 Foundation Models forDecision Making Workshop. Raymond Li, Loubna Ben Allal, Yangtian Zi, NiklasMuennighoff, Denis Kocetkov, Chenghao Mou, MarcMarone, Christopher Akiki, Jia Li, Jenny Chim, et al.2023b.Starcoder: may the source be with you!arXiv preprint arXiv:2305.06161.",
  "Aman Madaan and Amir Yazdanbakhsh. 2022. Textand patterns: For effective chain of thought, it takestwo to tango. arXiv preprint arXiv:2209.07686": "Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su.2020. A diverse corpus for evaluating and developingenglish math word problem solvers. In Proceedingsof the 58th Annual Meeting of the Association forComputational Linguistics, pages 975984. Dung Nguyen, Le Nam, Anh Dau, Anh Nguyen, KhanhNghiem, Jin Guo, and Nghi Bui. 2023. The vault:A comprehensive multilingual dataset for advanc-ing code understanding and generation. In Findingsof the Association for Computational Linguistics:EMNLP 2023, pages 47634788, Singapore. Associ-ation for Computational Linguistics.",
  "Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, HuanWang, Yingbo Zhou, Silvio Savarese, and CaimingXiong. 2023. Codegen: An open large languagemodel for code with multi-turn program synthesis": "Arkil Patel, Satwik Bhattamishra, and Navin Goyal.2021. Are nlp models really able to solve simplemath word problems? In Proceedings of the 2021Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 20802094. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, StenSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,Jingyu Liu, Tal Remez, Jrmy Rapin, et al. 2023.Code llama: Open foundation models for code. arXivpreprint arXiv:2308.12950. Alexander Shypula, Aman Madaan, Yimeng Zeng, UriAlon, Jacob Gardner, Milad Hashemi, Graham Neu-big, Parthasarathy Ranganathan, Osbert Bastani, andAmir Yazdanbakhsh. 2023. Learning performance-improving code edits.",
  "Xingyao Wang,Sha Li,and Heng Ji. 2023b": "Code4Struct: Code generation for few-shot eventstructure prediction. In Proceedings of the 61st An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 36403663, Toronto, Canada. Association for Computa-tional Linguistics. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,Ed H Chi, Sharan Narang, Aakanksha Chowdhery,and Denny Zhou. 2022. Self-consistency improveschain of thought reasoning in language models. InThe Eleventh International Conference on LearningRepresentations. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,Ed H. Chi, Sharan Narang, Aakanksha Chowdhery,and Denny Zhou. 2023c. Self-consistency improveschain of thought reasoning in language models. InThe Eleventh International Conference on LearningRepresentations. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in NeuralInformation Processing Systems, 35:2482424837. Yi Wu, Nan Jiang, Hung Viet Pham, Thibaud Lutellier,Jordan Davis, Lin Tan, Petr Babkin, and SameenaShah. 2023. How effective are neural networks forfixing security vulnerabilities. In Proceedings of the32nd ACM SIGSOFT International Symposium onSoftware Testing and Analysis, ISSTA 23. ACM.",
  "Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu,Quoc V. Le, Denny Zhou, and Xinyun Chen. 2023.Large language models as optimizers": "Ke Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi R.Fung, Sha Li, Zixuan Huang, Xu Cao, XingyaoWang, Yiquan Wang, Heng Ji, and Chengxiang Zhai.2024. If llm is the wizard, then code is the wand: Asurvey on how code empowers large language modelsto serve as intelligent agents. Jiyang Zhang, Pengyu Nie, Junyi Jessy Li, and MilosGligoric. 2023. Multilingual code co-evolution usinglarge language models. In Proceedings of the 31stACM Joint European Software Engineering Confer-ence and Symposium on the Foundations of SoftwareEngineering, ESEC/FSE 2023, page 695707, NewYork, NY, USA. Association for Computing Machin-ery. Liang Zhang, Anwen Hu, Haiyang Xu, Ming Yan,Yichen Xu, Qin Jin, Ji Zhang, and Fei Huang. 2024.Tinychart: Efficient chart understanding with visualtoken merging and program-of-thoughts learning.arXiv preprint arXiv:2404.16635.",
  "Appl.comprises the GSM8K (Cobbe et al.,": "2021), SVAMP (Patel et al., 2021), and As-div (Miao et al., 2020) datasets. These datasetscontain elementary-level math problems set in spe-cific application scenarios, focusing on mathemati-cal abstraction and modeling skills, with relativelylow difficulty. Since they are the same type ofquestions, we merge them into one task. Math, con-sisting of the transformed MATH (Hendrycks et al.,2021) dataset, whose answers to the problems areexpressed using LaTeX. Its too hard to constructprompts in other languages that meet all the require-ments, we select those that can be calculated to asingle number, excluding problems with intervalor formula-based answers. The filtered results areshown in .",
  "A.3Error Analysis": "We further classify incorrect results into Wrong An-swer (WA) and Runtime Error (RE), representingcases where the program runs but produces incor-rect answers and where the program encounterserrors during execution, respectively. Tables 13 to show the results for the four models.It is evident that there are significant differencesin the proportion of runtime errors (RE) across dif-ferent languages and models for each task. Evenlanguages with similar performance exhibit differ-ent distributions of errors. For instance, on Appl.of Deepseek Coder, the accuracy difference be-tween Java and JavaScript is less than 0.1%, yetJavaScript has an RE rate of 2.06%, while Javasis only 0.63%. It indicates that the types of errorsvary significantly among languages.A further categorization of the types of RE isconducted. We classify all REs into eight errortypes. Redeclaration represents duplicate namingof variables. Division by Zero represents the de-nominator in the division is zero. Illegal Outputrepresents the answer can not be parsed or con-verted correctly. Time Limit Error represents theprogram runs out of time and sometimes it is due tostack space overflow. Compile Error often meansthere are some syntax error in the program. Unde-fined Identifier includes Undefined Variables andUndefined Functions, which means the variablesor functions are not defined before they are used.Variable Type Error indicates that the types ofvariables are mismatched when they are involvedin some operations, for example addition or divi-sion. shows the proportion of differentRE types for Deepseek Coder across five tasks andfive languages. presents the proportion ofvarious RE types for four LLMs on Appl. across alllanguages. Deepseek Coder and the Appl. task areselected because the languages have the most simi-lar performance on them. The results demonstratethat even in scenarios where languages exhibit sim-ilar performance, the proportions of RE differ sig-nificantly among languages. For instance, the RErate on ChatGPTs Appl. of R and C++ differs byonly 0.02%, yet Illegal Output account for 82.46%of C++ errors, in comparison to only 24.71% forR. Given that each prompt is accurate, the differingerror distributions are attributable to the intrinsiccharacteristics of the languages, thereby demon-strating their diversity and the non-repetitive natureof their errors.",
  "Upon further analysis of generated contents, sev-eral common failure patterns emerge:": "Date Calculation: PoTs often misinterpretthe difference between two dates as exclu-sive, contrary to natural language conventions,where the interval is typically inclusive. Nev-ertheless, R demonstrates comparable perfor-mance to CoT on Date, indicating its potentialfor temporal reasoning tasks. Output Content: PoTs frequently respondto yes/no questions by outputting attributesinstead of directly answering the question. Forexample, when asked, On the desk, there is ateal pen and a yellow textbook. Is the textbookyellow?, the correct answer is yes, but PoTmight respond with yellow. Demonstration Constraint: Demonstrationsare less restrictive for JavaScript, as it tendsto output extra information beyond what isrequired, including descriptive sentences andvariables, even when only the final answer isneeded.",
  "A.4Difference Between Code Generation andPoT": "illustrates that performance in code gen-eration does not fully align with that in reasoningtasks.Although both tasks involve generating code tosolve problems, their objectives differ. The codegeneration task assesses the LLMs ability to assistdevelopment in an engineering environment, cov-ering real-world engineering issues. For example,consider the following problems: Given a positivefloating point number, return its decimal part andGiven a list of integers, return a tuple containingthe sum and product of all the integers in the list.Although these problems require some reasoning,",
  ": The execution result of programs generated from Starcoder": "the focus is primarily on language comprehensionand engineering skills.In contrast, reasoning tasks aim to test the LLMslogical reasoning abilities. The generated code actsas a carrier of logic and facilitates the use of tools,such as more precise calculations, dictionaries forstoring and retrieving attribute information, or cal-endars to aid in date reasoning. Reasoning tasksfocus on a subset of a programming languagescapabilities, rather than its entire spectrum in engi-neering practice.Therefore, although there is some overlap be-tween code generation and reasoning tasks, theyare not entirely the same. This is why there isonly partial consistency between the two tasks in and highlights the necessity of testing dif-ferent programming languages in reasoning tasks..",
  "Here are our multilingual prompts.We showprompts of Tabular(3-shots) as an example andprompts for other tasks are in the released code": "Question: Here is a table where the first line is a header and each subsequent line is a penguin:name, age, height (cm), weight (kg)Louis, 7, 50, 11Bernard, 5, 80, 13Vincent, 9, 60, 11Gwen, 8, 70, 15For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.We now add a penguin to the table:James, 12, 90, 12How many penguins are less than 8 years old?",
  ": Python Prompt of the first question": "Question: Here is a table where the first line is a header and each subsequent line is a penguin:name, age, height (cm), weight (kg)Louis, 7, 50, 11Bernard, 5, 80, 13Vincent, 9, 60, 11Gwen, 8, 70, 15For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.Which is the youngest penguin?",
  ": Python Prompt of the second question": "Question: Here is a table where the first line is a header and each subsequent line is a penguin:name, age, height (cm), weight (kg)Louis, 7, 50, 11Bernard, 5, 80, 13Vincent, 9, 60, 11Gwen, 8, 70, 15For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.What is the name of the second penguin sorted by alphabetic order?",
  ": Python Prompt of the third question": "Question: Here is a table where the first line is a header and each subsequent line is a penguin:name, age, height (cm), weight (kg)Louis, 7, 50, 11Bernard, 5, 80, 13Vincent, 9, 60, 11Gwen, 8, 70, 15For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.We now add a penguin to the table:James, 12, 90, 12How many penguins are less than 8 years old?",
  ": C++ Prompt of the first question": "Question: Here is a table where the first line is a header and each subsequent line is a penguin:name, age, height (cm), weight (kg)Louis, 7, 50, 11Bernard, 5, 80, 13Vincent, 9, 60, 11Gwen, 8, 70, 15For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.Which is the youngest penguin?",
  ": C++ Prompt of the second question": "Question: Here is a table where the first line is a header and each subsequent line is a penguin:name, age, height (cm), weight (kg)Louis, 7, 50, 11Bernard, 5, 80, 13Vincent, 9, 60, 11Gwen, 8, 70, 15For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.What is the name of the second penguin sorted by alphabetic order?",
  ": C++ Prompt of the third question": "Question: Here is a table where the first line is a header and each subsequent line is a penguin:name, age, height (cm), weight (kg)Louis, 7, 50, 11Bernard, 5, 80, 13Vincent, 9, 60, 11Gwen, 8, 70, 15For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.We now add a penguin to the table:James, 12, 90, 12How many penguins are less than 8 years old?",
  ": Java Prompt of the first question": "Question: Here is a table where the first line is a header and each subsequent line is a penguin:name, age, height (cm), weight (kg)Louis, 7, 50, 11Bernard, 5, 80, 13Vincent, 9, 60, 11Gwen, 8, 70, 15For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.Which is the youngest penguin?",
  ": Java Prompt of the second question": "Question: Here is a table where the first line is a header and each subsequent line is a penguin:name, age, height (cm), weight (kg)Louis, 7, 50, 11Bernard, 5, 80, 13Vincent, 9, 60, 11Gwen, 8, 70, 15For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.What is the name of the second penguin sorted by alphabetic order?",
  ": Java Prompt of the third question": "Question: Here is a table where the first line is a header and each subsequent line is a penguin:name, age, height (cm), weight (kg)Louis, 7, 50, 11Bernard, 5, 80, 13Vincent, 9, 60, 11Gwen, 8, 70, 15For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.We now add a penguin to the table:James, 12, 90, 12How many penguins are less than 8 years old?",
  ": JavaScript Prompt of the first question": "Question: Here is a table where the first line is a header and each subsequent line is a penguin:name, age, height (cm), weight (kg)Louis, 7, 50, 11Bernard, 5, 80, 13Vincent, 9, 60, 11Gwen, 8, 70, 15For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.Which is the youngest penguin?",
  ": JavaScript Prompt of the second question": "Question: Here is a table where the first line is a header and each subsequent line is a penguin:name, age, height (cm), weight (kg)Louis, 7, 50, 11Bernard, 5, 80, 13Vincent, 9, 60, 11Gwen, 8, 70, 15For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.What is the name of the second penguin sorted by alphabetic order?",
  ": JavaScript Prompt of the third question": "Question: Here is a table where the first line is a header and each subsequent line is a penguin:name, age, height (cm), weight (kg)Louis, 7, 50, 11Bernard, 5, 80, 13Vincent, 9, 60, 11Gwen, 8, 70, 15For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.We now add a penguin to the table:James, 12, 90, 12How many penguins are less than 8 years old?",
  ": R Prompt of the first question": "Question: Here is a table where the first line is a header and each subsequent line is a penguin:name, age, height (cm), weight (kg)Louis, 7, 50, 11Bernard, 5, 80, 13Vincent, 9, 60, 11Gwen, 8, 70, 15For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.Which is the youngest penguin?",
  ": R Prompt of the second question": "Question: Here is a table where the first line is a header and each subsequent line is a penguin:name, age, height (cm), weight (kg)Louis, 7, 50, 11Bernard, 5, 80, 13Vincent, 9, 60, 11Gwen, 8, 70, 15For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.What is the name of the second penguin sorted by alphabetic order?"
}