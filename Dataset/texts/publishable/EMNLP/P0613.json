{
  "Abstract": "Despite tremendous advancements, currentstate-of-the-artVision-LanguageModels(VLMs) are still far from perfect. They tend tohallucinate and may generate biased responses.In such circumstances, having a way to assessthe reliability of a given response generatedby a VLM is quite useful. Existing methods,such as estimating uncertainty using answerlikelihoods or prompt-based confidence gener-ation, often suffer from overconfidence. Othermethods use self-consistency comparisonbut are affected by confirmation biases. Toalleviate these, we propose Decompose andCompare Consistency (DeCC) for reliabilitymeasurement. By comparing the consistencybetween the direct answer generated using theVLMs internal reasoning process, and theindirect answers obtained by decomposing thequestion into sub-questions and reasoning overthe sub-answers produced by the VLM, DeCCmeasures the reliability of VLMs direct an-swer. Experiments across six vision-languagetasks with three VLMs show DeCCs reliabilityestimation achieves better correlation with taskaccuracy compared to the existing methods.The code is publicly available at",
  "Introduction": "Automatic measurement of reliability of responsesgenerated by AI systems such as vision-languagemodels (VLMs) is useful for deciding whether totrust a response or not, which in turn is neces-sary to build secure systems and enable furtherimprovements (Varshney and Baral, 2023). Exist-ing reliability estimation methods often estimatethe models uncertainty using answer likelihoodsor prompt the model to generate a confidencevalue (Xiong et al., 2024; Tian et al., 2023; Mielkeet al., 2022). These methods often fail to correlatewell with task accuracy because models are notwell-calibrated and tend to be overconfident (Chen",
  "Correct AnswerIncorrect Answer": ": DeCC begins by decomposing the questioninto multiple sub-questions. The candidate VLM an-swers these sub-questions, creating sub-QA pairs. Boththe candidate VLM and an LLM independently reasonover these pairs to derive reasoned answers. We thencompare the direct answer with the reasoned answers toassess reliability. We also explore how different consis-tency comparison settings impact DeCCs effectiveness. et al., 2023b). Other methods attempt to incorpo-rate calibrated confidence generation as a traininggoal (Lin et al., 2022; Ye and Durrett, 2022; Ohet al., 2024), but retraining the model is inefficientand even impractical for measuring the reliabilityof multiple VLMs or closed-source models. Someworks use self-consistency to measure reliability bycomparing the consistency among multiple gener-ated answers (Wang et al., 2022; Chen et al., 2024a,2023a), but self-consistency might suffer from con-firmation biases (Feng et al., 2024).To better measure VLMs answer reliability, wepropose a method called Decompose and CompareConsistency (DeCC). As shown in Fig 1, we firstdecompose the original question into several sub-questions. The candidate VLM then answers thesesub-questions, generating a sequence of sub-QApairs.We use both the candidate VLM and aseparate LLM, acting as two independent agents, to reason over the sub-QA pairs and obtain theirrespective reasoned answers. We then comparethe consistency between these reasoned answersand the answer generated directly by the VLMto measure the reliability of the VLMs direct an-swer. Using the candidate VLM to reason oversub-QA pairs provides insights into how robustlythe VLM understands the question. However, suchself-consistency can sometimes introduce confir-mation biases (Feng et al., 2024). Thus, we alsoemploy an LLM to reason over the sub-QA pairsseparately. We test both single-agent and multi-agent settings. For the single-agent setting, we usethe consistency between the direct answer and oneof the agents reasoned answers to determine re-liability. For the multi-agent setting, we combinethe consistency check results from both agents todetermine if the answer is reliable, unreliable, or re-quires further information for measurement. We as-sume that if the VLM understands the question welland conducts reliable reasoning, a conflict is lesslikely to occur between its direct answer, derivedfrom its internal reasoning process, and the decom-posed answer, derived from an external reasoningprocess. We evaluate DeCC on six vision-languagetasks using three different state-of-the-art VLMs.Experimental results demonstrate that DeCC, whichis both model-agnostic and task-agnostic, exhibitsa higher correlation with the VLMs task accuracycompared to the existing methods. Additionally,we observe that the effectiveness of different con-sistency comparison settings is correlated with thecandidate VLMs capabilities.",
  "Related Work": "Existing methods use uncertainty-based metrics forreliability measurement, such as setting a reliabil-ity threshold on answer likelihoods (Pereyra et al.,2017; Geifman and El-Yaniv, 2017; Whiteheadet al., 2022), or prompting the model to generatea confidence value (Xiong et al., 2024; Tian et al.,2023; Li et al., 2024; Mielke et al., 2022). However,uncertainty-based metrics often lead to overconfi-dence since confidence calibration is not a traininggoal (Chen et al., 2023b). But retraining modelsto generate calibrated confidence (Oh et al., 2024;Lin et al., 2022; Zhang et al., 2023) is impracticalfor evaluating multiple VLMs. Self-consistencymethods generate multiple responses to assess re-liability (Wang et al., 2022; Chen et al., 2024a,2023a) but suffer from confirmation biases (Huang et al., 2024; Xie et al., 2024). Multi-agent collab-oration can mitigate this. Feng et al. (2024) usemultiple LLMs to interact in cooperative and com-petitive settings to evaluate reliability. Srinivasanet al. (2024) use LLMs to generate related questionsabout the image and use high-confidence QA pairsas premises, with the original QA as the hypothesis,to determine reliability. Our approach differs by de-composing the question into simpler sub-questions.We also conduct extensive experiments to explorethe effectiveness of different consistency compari-son settings on reliability measurement.",
  "Task Decomposition": "First, the decomposer, which could be any VLM,decomposes the question Q into a sequence of sub-questions conditioned on I. The candidate VLMthen answers these sub-questions, resulting in asequence of sub-QA pairs. Next, the candidateVLM and a separate LLM, acting as two indepen-dent agents, reason over the sub-QA pairs and Q,yielding VLMs reasoned answer AV and LLMsreasoned answer AL. To enhance robustness, wealso experiment with a two-iteration decompositionprocess. In the second iteration, sub-QA pairs fromthe first iteration, along with Q and I, are used toguide the decomposer in generating additional sub-questions. The candidate VLM answers these newsub-questions, conditioned on I and previous sub-QA pairs, resulting in new sub-QA pairs containingmore information. Finally, both agents reason overall sub-QA pairs from both iterations to providetheir updated reasoned answers, AV and AL",
  ",if A is consistent with A0,otherwise": "We check if A = A to determine the consistency.For two-iteration decomposition, we compare Awith AV and AL to obtain R in a similar way.Multi-Agent As shown in Fig 2, we first con-duct consistency checks of A with AV and ALand obtain ConsV (consistency between A andAV ) and ConsL (consistency between A and AL).If ConsV = ConsL, we assign R = ConsV .If ConsV = ConsL, we proceed to the second-iteration consistency checks, where we compareupdated reasoned answers AV and AL with A, ob-taining ConsV and ConsL. We assign R as:",
  "ConsV ,if ConsV = ConsLConsL,if ConsV = ConsV andConsL = ConsLConsV ,if ConsV = ConsV andConsL = ConsL": "(1) The first scenario indicates that the consistencycheck outcome for one of the agents has changedfrom the first iteration, leading to the same con-sistency check outcomes between the two agents.(2) The second scenario indicates that both agentsshow strong confidence in their respective consis-tencies with respect to the direct answer. We trustthe LLMs consistency check, as it provides a moreobjective assessment, Relying solely on textual de-composition information, whereas the VLM mightsuffer from its inherent biases towards certain re-sponses. (3) The third scenario indicates that thesecond-iteration decomposition provides additionalinformation, influencing both agents reasoningand changing their consistency with respect to thedirect answer. We trust the VLMs consistencycheck outcome, as VLM is less likely to changeits response due to its inherent biases, whereas theLLMs response is more likely to change since itis operating under incomplete information (lack ofimage). So a change in VLMs response indicatesit potentially overcame its biases with additionalsub-QA pairs. See Appendix for Algorithm 1.",
  "We use the Brier Score (BS)(Brier, 1950) to mea-sure the correlation between reliability and taskaccuracy: BS = 1": "NNi=1(Ri Acci)2, where Nis the evaluation dataset size, Ri is the reliabilityscore for the i-th answer, and Acci is the accuracyfor the i-th answer. BS ranges between 0 and 1,with lower values indicating better correlation be-tween R and Acc. We also apply DeCC for theselective prediction task where the model abstainsfrom answering when its response is estimated tobe unreliable. To measure DeCC effectiveness atselective prediction we use the Effective Reliability(ER) metric proposed in (Whitehead et al., 2022).ER captures the trade-off between risk (task accu-racy across all answered questions) and coverage(number of questions answered). Both low risk butlow coverage and high coverage but high risk leadto low ER. ER for the i-th answer is computed as:",
  "BSERBSERBSERBSERBSERBSERBSER": "LLaVA1.5-7B as Candidate VLMAcc: 55.0Acc: 59.2Acc: 67.3Acc: 59.6Acc: 34.3Acc: 24.5Acc: 49.9Perplexity of Direct Answer55.70.738.220.422.855.039.324.342.4-8.225.9-1.437.415.1Generated Numerical Confidence66.5-32.540.818.322.155.628.044.067.3-35.375.8-51.650.1-0.2Generated Linguistic Confidence67.5-35.040.219.622.654.827.644.869.6-39.177.2-54.450.8-1.5Self-Consistency based on Paraphrase38.517.532.825.719.059.240.523.939.1-5.635.6-11.534.318.2 DeCCVLM Agent Consistency31.924.536.422.218.259.635.328.352.3-18.146.3-21.836.715.8VLM Agent Consistency (2 iterations)32.523.934.524.118.359.536.127.449.1-14.945.6-21.136.016.5LLM Agent Consistency32.024.435.922.724.553.337.526.034.10.130.7-6.232.420.1LLM Agent Consistency (2 iterations)30.625.832.626.022.355.534.628.936.8-2.631.0-6.531.321.2Multi-Agent Consistency (2 iterations)31.524.933.525.120.157.734.628.936.4-2.232.2-7.731.421.1 Idefics2-8B as Candidate VLMAcc: 39.3Acc: 78.6Acc: 83.1Acc: 70.0Acc: 39.9Acc: 48.0Acc: 59.8Perplexity of Direct Answer59.7-20.034.128.219.963.229.843.540.6-1.030.015.135.621.5Generated Numerical Confidence40.8-0.537.725.336.346.725.349.167.7-43.649.3-1.642.812.6Generated Linguistic Confidence35.0-3.140.222.125.256.626.845.660.4-36.342.43.538.314.7Self-Consistency based on Paraphrase59.1-19.331.630.416.366.528.943.841.6-2.040.84.836.420.7",
  "DeCCVLM Agent Consistency33.237.028.342.811.976.618.961.344.9-1.223.831.426.841.3VLM Agent Consistency (2 iterations)33.936.329.142.011.377.218.661.544.8-1.124.330.927.041.1": "LLM Agent Consistency36.333.937.633.522.266.329.450.840.33.337.118.133.834.3LLM Agent Consistency (2 iterations)34.535.734.936.218.869.727.053.136.96.833.321.930.937.2Multi-Agent Consistency (2 iterations)34.335.932.638.515.473.123.856.437.46.231.124.129.139.0 : Measuring Brier Score (BS) and Effective Reliability (ER) for various reliability measurement methods.Best results are in bold. Second-best results are underlined. Acc represents the task accuracy of the candidate VLM.All scores are in percentage. DeCC surpasses all baselines in average Brier Score and Effective Reliability. X%. A threshold determines reliability. Gener-ated Linguistic Confidence: Prompt the VLM tostate I am confident/not confident in this answer.Self-Consistency based on Paraphrase: Prompt aVLM to paraphrase the original question into fourvariations. If n or more paraphrased answers differfrom the direct answer, R is 0 otherwise 1. 2",
  "Main Results": "We conduct experiments on six vision-languagetasks3, covering commonsense reasoning, fine-grained compositional reasoning, and science un-derstanding (see Appendix A.1 for dataset descrip-tions). We evaluate three state-of-the-art VLMs:LLaVA1.5-7B (Liu et al., 2023), Idefics2-8B (Lau-renon et al., 2024), and InternVL1.5-25.5B (Chenet al., 2024b) (see Appendix A.2 for implementa- 2We select the best threshold and n for each VLM basedon the Brier Score (results in Tables 4 and 5).3All datasets are multiple-choice except for MMMU andMathVista, whose answers are very short. We use stringmatching for consistency comparison. tion details). The overall results are shown in Ta-ble 1. DeCC achieves the best and second-best meanperformance (mean across datasets) on Brier Scoreand Effective Reliability. DeCC reduces the relativemean Brier Score by 8.7% on LLaVA, 14.3% onIdefics2, and 2.9% on InternVL compared to thebest existing methods. DeCC also increases relativemean Effective Reliability by 16.5% on LLaVA,25.6% on Idefics2, and 1.7% on InternVL. We ob-serve that with increasing VLM size, the perfor-mance of most methods improves, suggesting thatreliability measurement is correlated with VLMscapabilities. For the effectiveness of DeCCs differ-ent consistency comparison settings, we observean interesting trend: (1) For weaker VLMs, i.e.,LLaVA, LLM Agent Consistency achieves the bestperformance, likely because VLMs struggle to rea-son over the sub-QA pairs and suffer from confir-mation biases. (2) For stronger VLMs, i.e. Idefics2,Multi-Agent Consistency performs the best sug-gesting that the VLM and LLM reasoners comple-",
  "Further Analysis": "Decoding StrategyThe decoding strategy ofdecomposer can influence the generated sub-questions, which in turn impacts DeCCs overallperformance. We conducted experiments usingsampling-based decoding with two different tem-perature settings, 0.8 and 0.9, while keeping thenucleus sampling (Holtzman et al., 2020) probabil-ity fixed at 0.9 for both. The results are presentedin , using Idefics2-8B as the candidate VLM.Across all decoding strategies and temperature set-tings, DeCC consistently outperforms the baselines,demonstrating its effectiveness.",
  "Question Type AnalysisPrevious studies haveshown that VLMs can exhibit biases toward certain": "question types. For instance, they respond yesto over 80% of queries about non-existent objects,such as in prompts like Is there an object in thisimage? (Wang et al., 2023). If the decomposedsub-questions retain the same type as the originalquestion, they may inherit the same bias, affectingevaluation reliability. To investigate this, we con-duct the question type analysis to assess whetherDeCC is prone to this issue 4. shows thenumber of questions and question types per sampleacross benchmarks. DeCC generates at least 2.1 dis-tinct question types per sample for all benchmarks,reducing the impact of VLM biases. Additional AnalysisTo better understand theworkflow of DeCC, we provide some qualitativeexamples in Appendix A.4. We also provide thecomputational costs analysis in Appendix A.5 to ad-dress concerns about the additional computationalrequirements of DeCC and demonstrate its practicalapplicability.",
  "Limitations": "Our experiments demonstrate that consistency com-parison based on task decomposition can bettermeasure the reliability of VLM answers. However,there are several limitations to our current study:Decomposition Performance: The effectiveness ofour framework is influenced by the performance ofthe decomposition process. Currently, we have notfully explored the optimization and impact of dif-ferent decomposition strategies for reliability mea-surement. Multi-Agent Consistency Comparison:We tested decomposition with only one LLM forthe multi-agent part. Conducting more experimentswith various LLMs will help assess the generaliza-tion and robustness of our framework. Future workwill address these limitations to validate and en-hance the generalization of our proposed method.",
  "Glenn W Brier. 1950.Verification of forecasts ex-pressed in terms of probability. Monthly weatherreview, 78(1):13": "Angelica Chen, Jason Phang, Alicia Parrish, VishakhPadmakumar, Chen Zhao, Samuel R. Bowman, andKyunghyun Cho. 2024a.Two failures of self-consistency in the multi-step reasoning of LLMs.Transactions on Machine Learning Research. Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu,Mingyuan Tao, Zhihang Fu, and Jieping Ye. 2023a.Inside: Llms internal states retain the power of hal-lucination detection. In The Twelfth InternationalConference on Learning Representations. Yangyi Chen, Lifan Yuan, Ganqu Cui, Zhiyuan Liu,and Heng Ji. 2023b. A close look into the calibra-tion of pre-trained language models. In Proceedingsof the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 13431367, Toronto, Canada. Association forComputational Linguistics. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye,Zhangwei Gao, Erfei Cui, Wenwen Tong, KongzhiHu, Jiapeng Luo, Zheng Ma, et al. 2024b. How farare we to gpt-4v? closing the gap to commercialmultimodal models with open-source suites. arXivpreprint arXiv:2404.16821. Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding,Vidhisha Balachandran, and Yulia Tsvetkov. 2024.Dont hallucinate, abstain: Identifying llm knowl-edge gaps via multi-llm collaboration.Preprint,arXiv:2402.00367.",
  "Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, andYejin Choi. 2020. The curious case of neural text de-generation. In International Conference on LearningRepresentations": "JieHuang,XinyunChen,SwaroopMishra,Huaixiu Steven Zheng, Adams Wei Yu, Xiny-ing Song, and Denny Zhou. 2024. Large languagemodels cannot self-correct reasoning yet. In TheTwelfth International Conference on LearningRepresentations. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, YingSheng, Lianmin Zheng, Cody Hao Yu, Joseph E.Gonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-cient memory management for large language modelserving with pagedattention. In Proceedings of theACM SIGOPS 29th Symposium on Operating SystemsPrinciples.",
  "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong JaeLee. 2023. Visual instruction tuning": "Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun-yuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. 2024.Mathvista: Evaluating mathematical reasoning offoundation models in visual contexts.In Inter-national Conference on Learning Representations(ICLR). Kenneth Marino, Mohammad Rastegari, Ali Farhadi,and Roozbeh Mottaghi. 2019. Ok-vqa: A visual ques-tion answering benchmark requiring external knowl-edge. In Proceedings of the IEEE/cvf conferenceon computer vision and pattern recognition, pages31953204. Sabrina J. Mielke, Arthur Szlam, Emily Dinan, and Y-Lan Boureau. 2022. Reducing conversational agentsoverconfidence through linguistic calibration. Trans-actions of the Association for Computational Linguis-tics, 10:857872. Changdae Oh, Mijoo Kim, Hyesu Lim, Junhyeok Park,Euiseog Jeong, Zhi-Qi Cheng, and Kyungwoo Song.2024. Towards calibrated robust fine-tuning of vision-language models. In NeurIPS 2023 Workshop onDistribution Shifts: New Frontiers with FoundationModels.",
  "Gabriel Pereyra, George Tucker, Jan Chorowski, LukaszKaiser, and Geoffrey Hinton. 2017. Regularizingneural networks by penalizing confident output dis-tributions": "Dustin Schwenk, Apoorv Khandelwal, ChristopherClark, Kenneth Marino, and Roozbeh Mottaghi. 2022.A-okvqa: A benchmark for visual question answer-ing using world knowledge. In European Conferenceon Computer Vision, pages 146162. Springer. TejasSrinivasan,JackHessel,TanmayGupta,Bill Yuchen Lin, Yejin Choi, Jesse Thomason, andKhyathi Raghavi Chandu. 2024.Selective\" se-lective prediction\": Reducing unnecessary absten-tion in vision-language reasoning. arXiv preprintarXiv:2402.15610. Tristan Thrush, Ryan Jiang, Max Bartolo, AmanpreetSingh, Adina Williams, Douwe Kiela, and CandaceRoss. 2022. Winoground: Probing vision and lan-guage models for visio-linguistic compositionality.In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 52385248. Katherine Tian, Eric Mitchell, Allan Zhou, ArchitSharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn,and Christopher Manning. 2023. Just ask for cali-bration: Strategies for eliciting calibrated confidencescores from language models fine-tuned with humanfeedback. In Proceedings of the 2023 Conferenceon Empirical Methods in Natural Language Process-ing, pages 54335442, Singapore. Association forComputational Linguistics. Neeraj Varshney and Chitta Baral. 2023.Post-abstention: Towards reliably re-attempting the ab-stained instances in qa. In Proceedings of the 61stAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 967982. Junyang Wang, Yiyang Zhou, Guohai Xu, PengchengShi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, MingYan, Ji Zhang, Jihua Zhu, et al. 2023. Evaluationand analysis of hallucination in large vision-languagemodels. arXiv preprint arXiv:2308.15126. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,Ed H Chi, Sharan Narang, Aakanksha Chowdhery,and Denny Zhou. 2022. Self-consistency improveschain of thought reasoning in language models. InThe Eleventh International Conference on LearningRepresentations. Spencer Whitehead, Suzanne Petryk, Vedaad Shakib,Joseph Gonzalez, Trevor Darrell, Anna Rohrbach,and Marcus Rohrbach. 2022. Reliable visual ques-tion answering: Abstain rather than answer incor-rectly. In European Conference on Computer Vision,pages 148166. Springer. Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, andYu Su. 2024. Adaptive chameleon or stubborn sloth:Revealing the behavior of large language models inknowledge conflicts. In The Twelfth InternationalConference on Learning Representations.",
  "Ning Xie, Farley Lai, Derek Doran, and Asim Ka-dav. 2019.Visual entailment: A novel task forfine-grained image understanding. arXiv preprintarXiv:1901.06706": "Miao Xiong, Zhiyuan Hu, Xinyang Lu, YIFEI LI, JieFu, Junxian He, and Bryan Hooi. 2024. Can LLMsexpress their uncertainty? an empirical evaluation ofconfidence elicitation in LLMs. In The Twelfth Inter-national Conference on Learning Representations. Xi Ye and Greg Durrett. 2022. Can explanations be use-ful for calibrating black box models? In Proceedingsof the 60th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 61996212, Dublin, Ireland. Association forComputational Linguistics. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng,Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang,Weiming Ren, Yuxuan Sun, et al. 2023. Mmmu:A massive multi-discipline multimodal understand-ing and reasoning benchmark for expert agi. arXivpreprint arXiv:2311.16502. Rowan Zellers, Yonatan Bisk, Ali Farhadi, and YejinChoi. 2019.From recognition to cognition: Vi-sual commonsense reasoning. In Proceedings of theIEEE/CVF conference on computer vision and pat-tern recognition, pages 67206731. Hanning Zhang, Shizhe Diao, Yong Lin, Yi R Fung,Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji,and Tong Zhang. 2023. R-tuning: Teaching largelanguage models to refuse unknown questions. arXivpreprint arXiv:2311.09677.",
  "A.1Datasets": "SNLI-VE requires VLMs to identify whether therelationship between the given image premise andtext hypothesis is entailment, neutral, or contra-diction. Visual Commonsense Reasoning (VCR)requires higher-order cognition and commonsensereasoning of VLMs. It provides an image and aquestion about certain objects in the image, alongwith four candidate answers, where the VLMsneed to choose the correct answer. We add rect-angles of different colors to the image and indi-cate the corresponding objects index in the up-per right corner of each rectangle to distinguishthe objects.A-OKVQA is an augmented suc-cessor of OK-VQA (Marino et al., 2019) and re-quires a broad base of commonsense and worldknowledge to answer questions. Four candidateanswers are provided along with each question.Winoground (Wino.) is proposed for measuringvision-linguistic compositional reasoning. It con-tains two images and two captions. The modelneeds to correctly match the captions to the images,but crucially, both captions contain an identicalset of words, only in a different order. MMMUis designed to evaluate VLMs on massive multi-discipline tasks demanding college-level subjectknowledge and deliberate reasoning. Several can-didate answers are provided along with each ques-tion. MathVista focuses on mathematical reason-ing in visual contexts. We treat all datasets exceptfor MMMU and MathVista as multiple-choice QAtasks. For evaluation:",
  "We use InternVL-1.5 (Chen et al., 2024b) as thedecomposer for decomposition and question para-": "phrasing.For decomposition, we employ few-shot prompting by randomly selecting four sam-ples from SNLI-VE and ScienceQA, with manu-ally written decomposition processes as guidance.The few-shot prompt for decomposition is pro-vided in . Only text is used in the few-shotprompt, without images. The decomposer deter-mines the number of sub-questions needed. Thefew-shot prompt for the second-iteration decom-position is shown in For paraphrasing, weuse the same samples with manually written para-phrased questions. The few-shot prompt for para-phrasing is provided in . The remainingdatasets are approached with a zero-shot strategy.We use OpenHermes-2.5-Mistral-7B5 as the LLMAgent for reasoning. We evaluate three VLMs:LLaVA1.5-7B (Liu et al., 2023), Idefics2-8B (Lau-renon et al., 2024), and InternVL (Chen et al.,2024b), all operating under a zero-shot settingacross all datasets. Since all datasets are multiple-choice QA tasks or short answers, we use stringmatching for answer consistency. For baselinethreshold settings:",
  "A.3Evaluation Metric Selection": "In our settings, we obtain binary reliability scoresfor each answer. We use the Brier Score (Brier,1950) and Effective Reliability (Whitehead et al.,2022) to evaluate the reliability measurement. Wedo not use Expected Calibration Error (ECE) (Guoet al., 2017) because ECE is suitable for evaluat-ing scores over a range of values. ECE relies onhaving a range of predicted probabilities to com-pare against actual accuracy. With only two reli-ability levels (0 or 1), there are no intermediate",
  "MetricSNLIVCRA - OKVQAWino.MMMUMathVistaMean": "LLaVAPerplexity Threshold - 1.056.458.677.863.534.224.552.5Perplexity Threshold - 1.0556.447.436.058.131.524.642.3Perplexity Threshold - 1.1056.443.328.748.432.124.738.9Perplexity Threshold - 1.1556.241.925.141.335.225.137.5Perplexity Threshold - 1.2056.339.723.441.039.525.337.5Perplexity Threshold - 1.2555.738.222.839.342.425.937.4 Idefics2Perplexity Threshold - 1.039.762.383.173.340.045.157.2Perplexity Threshold - 1.0559.133.322.632.636.631.635.9Perplexity Threshold - 1.1059.734.119.929.840.630.035.6Perplexity Threshold - 1.1560.136.518.527.943.931.036.3Perplexity Threshold - 1.2060.337.517.027.049.032.437.2Perplexity Threshold - 1.2560.238.016.626.653.035.038.2 InternVLPerplexity Threshold - 1.070.271.188.580.243.655.268.1Perplexity Threshold - 1.0544.944.623.144.641.444.540.5Perplexity Threshold - 1.1038.838.017.937.139.240.835.3Perplexity Threshold - 1.1534.334.915.634.338.638.732.7Perplexity Threshold - 1.2031.832.514.131.338.935.430.7Perplexity Threshold - 1.2529.630.213.529.437.736.329.4Perplexity Threshold - 1.3028.329.112.727.536.636.128.4Perplexity Threshold - 1.3527.828.312.926.836.436.228.1Perplexity Threshold - 1.4028.027.512.124.037.336.527.6",
  ": Brier Score using different numbers of inconsistent paraphrased-direct answer pairs out of a total of 4 pairs.Best results are in bold. All scores are in percentage": "probabilities to assess the correlation. We also findCoverage at Risk (C@R) (Whitehead et al., 2022)not applicable to our settings. C@R measures theCoverage proportion of correctly answered ques-tions if we tolerate an R% of wrong answers bysorting predictions in descending order of score listand calculating coverage until the risk threshold isreached. C@R is not suitable for binary reliabilityscores because it relies on a range of reliability lev-els to sort and progressively evaluate predictions.With only binary scores, there is no meaningfulway to sort the predictions by reliability. Conse-quently, C@R cannot provide a useful measure ofperformance in our setting.",
  "A.4Case Study": "Fig 3 shows an example from A-OKVQA whereall answers are consistent, and we assign the di-rect answer as reliable. Fig 4 shows an examplefrom A-OKVQA where there is a contradiction be-tween the consistency check results of the agentsreasoned answers and the direct answer. In thiscase, for the first sub-QA pair, the candidate VLMcorrectly identifies the birds as geese but fails toconduct correct reasoning over the decompositionprocess, deriving the same answer as the direct an-swer. Meanwhile, the LLM effectively utilizes theinformation from the decomposition. Both agentsdo not change their consistency check results. Asillustrated in .2, we trust the LLMs con-",
  "A.5Computational Costs Analysis": "DeCC requires multiple steps question decomposi-tion, answering the decomposed questions, compar-ing consistency between the answers. This raisesconcerns about DeCC being computationally moreexpensive than other approaches such as perplex-ity, generated numerical confidence scores, or self-consistency. To address this concern, we providedetailed computational costs for DeCC and com-pare it with that of baselines. Since the compu-tational cost in DeCC is influenced by the num-ber of sub-questions generated, we conduct thisanalysis on the VCR and AOKVQA, as these twodatasets represent the median number of generatedsub-questions among all benchmarks. All statis-tics are computed using Idefics2-8B (Laurenonet al., 2024) as the candidate VLM over a ran-domly chosen sample of 1,000 instances with amean of three runs. For LLM Agent reasoning,we use vLLM (Kwon et al., 2023) for speedingup inference speed. Others are implemented us-ing the Transformers toolkit6 from Hugging Facewithout any speeding-up strategies. The consis-tency comparison stage requires only string com-parisons, which is very fast (1 to 2 seconds for1000 samples), so we do not list the time. We re-",
  "LLMs Reasoned answer is inconsistent, the answer is unreliable": ": Example for the inconsistent situation. TheVLMs reasoned answer is consistent with the directanswer, while the LLMs reasoned answer is inconsis-tent. Both agents do not change their consistency checkresults. We trust the LLMs consistency check resultsand assign the direct answer as unreliable. Question: Why is person 0 standing over person 1 ?Options:A: person 0 was taking the measurements of person 1 .B: person 0 is working on person 1 ' s drink orderC: person 0 is preparing person 1 for executionD: person 0 is preparing to give a speech to person 1 Sub-Q 1: What is the setting of the image?Sub-A 1: Dining room.Sub-Q 2: What is the relationship between person 0 and person 1?Sub-A 2: MarriedSub-Q 3: What is person 0 doing in relation to person 1? Sub-A 3: ServingSub-Q 4: What is the context of interaction between person 0 and person 1?Sub-A 4: Dinner",
  "All answers are inconsistent, the answer is unreliable": ": Example for the inconsistent situation. Allanswers are inconsistent, while none of these answersare correct, indicating the VLMs do not understand thequestion well. We assign the direct answer as unreliable. port the computational costs analysis in .For baselines such as perplexity of direct answerand generated confidence, the time cost is equal todirect answering, i.e., 0.22 seconds per sample forVCR and 0.16 seconds per sample for AOKVQA.For the baseline Self-Consistency based on Para-phrase, the consumed time is similar to the firstiteration of DeCC, 4.8 seconds per sample for VCRand 3.9 seconds per sample for AOKVQA. For thefirst iteration of DeCC, the most time-consumingstage is the Question Pre-Decomposition stage forgenerating the sub-questions, taking 3.96 secondsper sample for VCR and 3.36 seconds per sam-ple for AOKVQA. However, this process needs tobe performed only once per sample before evalu-",
  "VCRBaselinesPerplexity of Direct Answer, Generated ConfidenceN/a10000.22Self-Consistency based on Paraphrase410004.80": "DeCC Multi-Agent Consistency StagesQuestion Pre-Decomposition3.910003.96Sub-Question Answering and VLM Agent Reasoning3.910000.84LLM Agent Reasoning3.910000.18Second Question Decomposition4.13664.09Second Sub-Question Answering and VLM Agent Reasoning4.13660.93Second LLM Agent Reasoning4.13660.20 DeCC Expected Value (Multi-Agent Consistency)[1000 3.9 + 366 4.1]/1000 = 5.41000[1000 (3.96 + 0.84 + 0.18)+366 (4.09 + 0.93 + 0.20)]/1000 = 6.89DeCC Expected Value (VLM-Agent Consistency (2 Iterations))8.010009.82DeCC Expected Value (LLM-Agent Consistency (2 Iterations))8.0100010.20",
  "AOKVQABaselinesPerplexity of Direct Answer, Generated ConfidenceN/a10000.16Self-Consistency based on Paraphrase410003.90": "DeCC Multi-Agent Consistency StagesQuestion Pre-Decomposition3.210003.36Sub-Question Answering and VLM Agent Reasoning3.210000.54LLM Agent Reasoning3.210000.10Second Question Decomposition3.92533.79Second Sub-Question Answering and VLM Agent Reasoning3.92530.66Second LLM Agent Reasoning3.92530.12 DeCC Expected Value (Multi-Agent Consistency)[1000 3.2 + 253 3.9]/1000 = 4.191000[1000 (3.36 + 0.54 + 0.10)+253 (3.79 + 0.66 + 0.12)]/1000 = 5.16DeCC Expected Value (VLM-Agent Consistency (2 Iterations))7.110008.35DeCC Expected Value (LLM-Agent Consistency (2 Iterations))7.110008.57",
  ": Computational costs analysis on VCR and AOKVQA": "ation and can then be reused to evaluate multipleVLMs, thus mitigating the impact on evaluationspeed. The Sub-Question Answering and VLMAgent Reasoning + LLM Agent Reasoning takes1.02 seconds per sample for VCR and 0.64 secondsper sample for AOKVQA. The second iterationof DeCC takes 5.22 seconds per sample for VCRand 4.57 seconds per sample for AOKVQA. How-ever, the second decomposition is required onlyfor samples where there is a disagreement betweenVLM Agent Consistency and LLM Agent Consis-tency from the first iteration, rather than for allsamples. For example, only 25.3% of samples inAOKVQA and 36.6% of samples in VCR requirethe second iteration. Thus, the total time for thesecond iteration is significantly less than the firstiteration. Overall the expected total time of DeCCMulti-Agent Setting taking into account both firstand second round of decompositions is 6.89 sec-onds per sample for VCR and 5.16 seconds persample for AOKVQA. Note that for VLM-AgentConsistency (2 Iterations) and LLM-Agent Consis-tency (2 Iterations), the expected total time is largerthan the multi-agent setting because we need to dothe second round of decomposition for all samples.With engineering optimizations and parallel pro-cessing, the computing time of DeCC can be furtherreduced. Note that DeCC does not require any train-ing or tuning of thresholds while other baselines such as perplexity, generated numerical confidencescores, or self-consistency require tuning certainthresholds, necessitating annotated datasets. Forexample, we used 1,000 samples for threshold tun-ing in our experiments. Data annotation is usuallytime-consuming and costly, averaging 10 secondsper sample7 with further filtering requests, and forspecialized domains, this time can increase signifi-cantly, thus increasing the hidden costs. Comparedwith the data annotation, the time cost of DeCC issignificantly lower. In conclusion, despite the addi-tional computation introduced by DeCC comparedto baselines, our comprehensive experimental eval-uation validates the effectiveness and practicalityof DeCC across various benchmarks and VLMs.",
  "Few-Shot Prompt for Decomposition": "Given an image and an associated main question, design pre-questions that focus on important contextualinformation in the image useful for answering the main question. Pre-questions should provide cluesto answer the main question. Each pre-question should be short and easy to understand. Pre-questionsshould focus on context visual clues of the image. Pre-questions should provide clues to answer the mainquestion. Example scenario to illustrate the expected interaction pattern:Main Question: Is this statement entailment, neutral or contradiction based on the image? Statement: Aprofessor is late to class Options: A: entailment, B: neutral, C: contradiction.Pre-question 1: Is there a person in the image wearing clothing typically associated with a professor?Pre-question 2: Is the person in the image displaying any behavior that could be interpreted as being lateto class, such as being out of breath or looking at a clock?Pre-question 3: Is there a classroom setting in the image, such as desks or a blackboard? Example scenario to illustrate the expected interaction pattern:Context: Below is a food web from a tundra ecosystem in Nunavut, a territory in Northern Canada. Afood web models how the matter eaten by organisms moves through an ecosystem. The arrows in a foodweb represent how matter moves between organisms in an ecosystem. Main Question: Based on thearrows, which of the following organisms is a decomposer? Choices: A: mushroom, B: lichenPre-question 1: Does the mushroom eat any other organisms in the food web?Pre-question 2: Does the lichen eat any other organisms in the food web?Pre-question 3: Does the lichen produce any material that other organisms can use?Pre-question 4: Does the mushroom produce any material that other organisms can use?Pre-question 5: Does a decomposer produce any material that other organisms can use? Example scenario to illustrate the expected interaction pattern:Main Question: Is this statement entailment, neutral or contradiction based on the image? Statement:Two children play in the park. Options: A: entailment, B: neutral, C: contradiction.Pre-question 1: Are there any children in the image?Pre-question 2: Are the two children playing in the park? Example scenario to illustrate the expected interaction pattern:User: Context: Use the graph to answer the question below. Main Question: Which month has thehighest average precipitation in Santiago? Choices: A: March, B: October, C: JunePre-question 1: What kind of graph is shown?Pre-question 2: Does the graph show the average precipitation for each month in Santiago?Pre-question 3: For which month is the bar highest in the graph?",
  "Few-Shot Prompt for Second-Iteration Decomposition": "You will be given an image and an associated main question, and some sub-question-answer pairs.However, these sub-questions might not be sufficient to answer the main question due to lack of detailor conflicting answers. You need to design additional sub-questions that focus on important contextualinformation in the image useful for answering the main question. Each pre-question should be short,easy to understand, and provide clues to answer the main question. Example scenario to illustrate the expected interaction pattern:Main Question: Is this statement entailment, neutral, or contradiction based on the image? Statement: Aprofessor is late to class Options: A: entailment, B: neutral, C: contradiction.Sub-questions and answers:Sub-question 1: Is there a person in the image wearing clothing typically associated with a professor?Sub-answer 1: Yes.Sub-question 2: Is the person in the image displaying any behavior that could be interpreted as being lateto class, such as being out of breath or looking at a clock?Sub-answer 2: No.Sub-question 3: Is there a classroom setting in the image, such as desks or a blackboard?Sub-answer 3: Yes.Your return:Additional Sub-question 1: What is the persons age in the image?Additional Sub-question 2: Is the person more likely to be a student or a professor?Additional Sub-question 3: Is the person holding any books or papers? Example scenario to illustrate the expected interaction pattern:Context: Below is a food web from a tundra ecosystem in Nunavut, a territory in Northern Canada. Afood web models how the matter eaten by organisms moves through an ecosystem. The arrows in a foodweb represent how matter moves between organisms in an ecosystem. Main Question: Based on thearrows, which of the following organisms is a decomposer? Choices: A: mushroom, B: lichen.Sub-questions and answers:Sub-question 1: Does the mushroom eat any other organisms in the food web?Sub-answer 1: Yes.Sub-question 2: Does the lichen eat any other organisms in the food web?Sub-answer 2: No.Sub-question 3: Does the lichen produce any material that other organisms can use?Sub-answer 3: Yes.Sub-question 4: Does the mushroom produce any material that other organisms can use?Sub-answer 4: No.Sub-question 5: Does a decomposer produce any material that other organisms can use?Sub-answer 5: Yes.Your return:Additional Sub-question 1: Is there any arrow pointing towards the mushroom?Additional Sub-question 2: Is there any arrow pointing towards the lichen?Additional Sub-question 3: What is the mushrooms role in the food web?Additional Sub-question 4: What is the lichens role in the food web?",
  "Few-Shot Prompt for Paraphrase": "Your goal is to paraphrase the given question into 4 questions. Each question should only change thewording of the original question slightly or just replace a few words. The questions should be easy tounderstand and should not change the meaning of the original question. If the questions come with somechoices, you should not change these choices. Example scenario to illustrate the expected interaction pattern:Main Question: Is this statement entailment, neutral, or contradiction based on the image? Statement: Aprofessor is late to class Options: A: entailment, B: neutral, C: contradiction.Paraphrased question 1: Is this statement entailment, neutral, or contradiction based on the image?Statement: A teacher is late to class Options: A: entailment, B: neutral, C: contradiction.Paraphrased question 2: Is this statement entailment, neutral, or contradiction based on the image?Statement: A professor is tardy to class Options: A: entailment, B: neutral, C: contradiction.Paraphrased question 3: Is this statement entailment, neutral, or contradiction based on the image?Statement: A professor is not on time for class Options: A: entailment, B: neutral, C: contradiction.Paraphrased question 4: Is this statement entailment, neutral, or contradiction based on the image?Statement: A teacher is not punctual for class Options: A: entailment, B: neutral, C: contradiction. Example scenario to illustrate the expected interaction pattern:Context: Below is a food web from a tundra ecosystem in Nunavut, a territory in Northern Canada. Afood web models how the matter eaten by organisms moves through an ecosystem. The arrows in a foodweb represent how matter moves between organisms in an ecosystem. Main Question: Based on thearrows, which of the following organisms is a decomposer? Choices: A: mushroom, B: lichenParaphrased question 1: Based on the arrows, which of these choices is a decomposer? Choices: A:mushroom, B: lichenParaphrased question 2: Based on the arrows, which of the following is a decomposer? Choices: A:mushroom, B: lichenParaphrased question 3: Which of the following is a decomposer based on the arrows? Choices: A:mushroom, B: lichenParaphrased question 4: Which is a decomposer based on the figure? Choices: A: mushroom, B: lichen Example scenario to illustrate the expected interaction pattern:Main Question: Is this statement entailment, neutral, or contradiction based on the image? Statement:Two children play in the park. Options: A: entailment, B: neutral, C: contradiction.Paraphrased question 1: Is this statement entailment, neutral, or contradiction based on the image?Statement: Two kids play in the park. Options: A: entailment, B: neutral, C: contradiction.Paraphrased question 2: Is this statement entailment, neutral, or contradiction based on the image?Statement: Two children are playing in the park. Options: A: entailment, B: neutral, C: contradiction.Paraphrased question 3: Is this statement entailment, neutral, or contradiction based on the image?Statement: Two kids are playing in the park. Options: A: entailment, B: neutral, C: contradiction.Paraphrased question 4: Is this statement entailment, neutral, or contradiction based on the image? State-ment: There are two children playing in the park. Options: A: entailment, B: neutral, C: contradiction. Example scenario to illustrate the expected interaction pattern:User: Context: Use the graph to answer the question below. Main Question: Which month has thehighest average precipitation in Santiago? Choices: A: March, B: October, C: JuneParaphrased question 1: Which month has the highest average rainfall in Santiago? Choices: A: March,B: October, C: JuneParaphrased question 2: Which months precipitation is the highest in Santiago? Choices: A: March, B:October, C: JuneParaphrased question 3: Which month has the most precipitation in Santiago? Choices: A: March, B:October, C: JuneParaphrased question 4: Which month has the most rainfall in Santiago? Choices: A: March, B: October,C: June"
}