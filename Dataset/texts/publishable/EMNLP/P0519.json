{
  "Abstract": "Zero-shot singing voice synthesis (SVS) withstyle transfer and style control aims to gen-erate high-quality singing voices with unseentimbres and styles (including singing method,emotion, rhythm, technique, and pronuncia-tion) from audio and text prompts. However,the multifaceted nature of singing styles posesa significant challenge for effective modeling,transfer, and control. Furthermore, current SVSmodels often fail to generate singing voices richin stylistic nuances for unseen singers. To ad-dress these challenges, we introduce TCSinger,the first zero-shot SVS model for style transferacross cross-lingual speech and singing styles,along with multi-level style control. Specifi-cally, TCSinger proposes three primary mod-ules: 1) the clustering style encoder employs aclustering vector quantization model to stablycondense style information into a compact la-tent space; 2) the Style and Duration LanguageModel (S&D-LM) concurrently predicts styleinformation and phoneme duration, which ben-efits both; 3) the style adaptive decoder uses anovel mel-style adaptive normalization methodto generate singing voices with enhanced de-tails. Experimental results show that TCSingeroutperforms all baseline models in synthesisquality, singer similarity, and style controlla-bility across various tasks, including zero-shotstyle transfer, multi-level style control, cross-lingual style transfer, and speech-to-singingstyle transfer. Singing voice samples can beaccessed at",
  "*Corresponding Author": "spectrograms, which are subsequently synthesizedinto the target singing voice using a vocoder.Recent years have seen significant advancementsin SVS technology (Shi et al., 2022; Cho et al.,2022; Zhang et al., 2023; Kim et al., 2024; Liuet al., 2022a; Zhang et al., 2024a). However, thegrowing demand for personalized and controllablesinging experiences presents challenges for cur-rent SVS models. Unlike traditional SVS tasks,zero-shot SVS with style transfer and style con-trol seeks to generate high-quality singing voiceswith unseen timbres and styles from audio and textprompts. This approach can be extended to morepersonalized and controllable applications, such asdubbing for entertainment short videos or profes-sional music composition. Personal singing stylesmainly include singing method (like bel canto),emotion (happy and sad), rhythm (including thestylistic handling of individual notes and transi-tions between them), techniques (such as falsetto),and pronunciation (like articulation). Despite this,traditional SVS methods lack the necessary mech-anisms to effectively model, transfer, and controlthese personal styles. Their performance tends todecline for unseen singers, as these methods gen-erally assume that target singers are identifiableduring the training phase (Zhang et al., 2024a).Presently, zero-shot SVS with style transferand style control primarily faces two major chal-lenges: 1) The multifaceted nature of singing stylespresents a substantial challenge for comprehensivemodeling, as well as effective transfer and con-trol. Previous approaches use pre-trained modelsto capture styles (Cooper et al., 2020). StyleSinger(Zhang et al., 2024a) uses a Residual Quantization(RQ) model to capture styles. However, these mod-els focus on limited aspects of styles, neglectingstyles like singing methods. Moreover, they fail toconduct multi-level style control. 2) Existing SVSmodels often fail to generate singing voices richin stylistic nuances for unseen singers. VISinger 2 (Zhang et al., 2022b) uses digital signal processingtechniques to enhance synthesis quality. Diffsinger(Liu et al., 2022a) employs a diffusion decoder tocapture the intricacies of singing voices. However,these methods do not adequately incorporate styleinformation into synthesis, leading to results thatlack style variations in zero-shot tasks.To address these challenges, we introduce TC-Singer, the first zero-shot SVS model for style trans-fer across cross-lingual speech and singing styles,along with multi-level style control. TCSingertransfers and controls styles (like singing methods,emotion, rhythm, techniques, and pronunciation)from audio and text prompts to synthesize high-quality singing voices. To model diverse styles(like singing methods, emotion, rhythm, technique,and pronunciation), we propose the clustering styleencoder, which uses a clustering vector quantiza-tion (CVQ) model to condense style informationinto a compact latent space, thus facilitating sub-sequent predictions, as well as enhance both train-ing stability and reconstruction quality. For styletransfer and control, we introduce the Style andDuration Language Model (S&D-LM). The S&D-LM incorporates a multi-task language module us-ing audio and text prompts to concurrently predictstyle information and phoneme duration, therebyenhancing both. To generate singing voices rich instylistic nuances, we introduce the style adaptivedecoder, which employs a novel mel-style adaptivenormalization method to refine mel-spectrogramswith decoupled style information. Our experimen-tal results show that TCSinger outperforms othercurrent best-performing baseline models in metricsincluding synthesis quality, singer similarity, andstyle controllability across various tasks, includingzero-shot style transfer, multi-level style control,cross-lingual style transfer, and speech-to-singing(STS) style transfer. Overall, our main contribu-tions can be summarized as follows: We present TCSinger, the first zero-shot SVSmodel for style transfer across cross-lingualspeech and singing styles, along with multi-level style control. TCSinger excels in person-alized and controllable SVS tasks. We introduce the clustering style encoder toextract styles, and the Style and Duration Lan-guage Model (S&D-LM) to predict both styleinformation and phoneme duration, address-ing style modeling, transfer, and control.",
  "Singing Voice Synthesis": "Singing Voice Synthesis (SVS) has emerged as adynamic field focused on generating high-qualitysinging voices from provided lyrics and musi-cal scores.DiffSinger (Liu et al., 2022a) usesa diffusion-based decoder (Ho et al., 2020) forhigh-quality generation. VISinger 2 (Zhang et al.,2022b) uses digital signal processing techniquesto enhance synthesis quality. Kim et al. (2024)disentangles timbre and pitch using adversarialmulti-task learning and improves the naturalnessof generated singing voices. Choi and Nam (2022)presents a melody-unsupervised model that onlyrequires pairs of audio and lyrics, thus eliminat-ing the need for temporal alignment. MuSE-SVS(Kim et al., 2023) introduces a multi-singer emo-tional singing voice synthesizer. RMSSinger (Heet al., 2023) proposes a pitch diffusion predictor toforecast F0 and UV, and a diffusion-based post-netto improve synthesis quality. Nonetheless, thesemethods are based on the assumption that targetsingers are visible during the training phase, lead-ing to a decline in synthesis quality in zero-shotscenarios. For singing datasets, GTSinger (Zhanget al., 2024b) makes substantial contributions byreleasing a multi-lingual and multi-technique anno-tated singing dataset. Recently, StyleSinger (Zhanget al., 2024a) has designed a normalization methodto enhance the model generalization. Furthermore,these methods do not adequately incorporate di-verse style information into the synthesis of singingvoices, resulting in limited style variations in gen-erated audio for zero-shot SVS tasks.",
  "Style Modeling, Transfer and Control": "Modeling, transferring, and controlling styles re-main pivotal areas of audio research, with past mod-els predominantly leveraging pre-trained modelsto capture a limited array of styles (Kumar et al., 2021). Atmaja and Sasou (2022) evaluates the per-formance of wav2vec 2.0 (Baevski et al., 2020), Hu-BERT (Hsu et al., 2021), and WavLM (Chen et al.,2022) in speech emotion recognition tasks. Gen-erspeech (Huang et al., 2022a) integrates globaland local style adaptors to capture speech styles.Styler (Lee et al., 2021) separates styles into var-ious levels of supervision. YourTTS (Casanovaet al., 2022) conditions the affine coupling layersof the flow-based decoder to handle zero-shot tasks.Mega-TTS (Jiang et al., 2023) decomposes speechinto multiple attributes and models prosody usinga language model. Recently, StyleSinger (Zhanget al., 2024a) has employed a residual quantizationmodel to capture detailed styles in singing voices.Although these approaches have made strides incapturing some aspects of style, there remains anotable gap in fully modeling styles (like singingmethods and techniques), and extending these ca-pabilities to cross-lingual speech and singing styles,as well as in multi-level style control.",
  "Overview": "The architecture of TCSinger is depicted in (a). We disentangle singing voices into separaterepresentations for content, style (including singingmethod, emotion, rhythm, technique, and pronun-ciation), and timbre. For content representation,lyrics are encoded through a phoneme encoder,while a note encoder captures musical notes. Forstyle representation, we use clustering vector quan-tization (CVQ) in the clustering style encoder tostably condense style information into a compact la-tent space, thus facilitating subsequent predictions.For timbre representation, we feed a prompt mel-spectrogram, sampled from different audio of thesame singer, into the timbre encoder to obtain a one-dimensional timbre vector, disentangling timbrefrom other information. Then, we utilize the Styleand Duration Language Model (S&D-LM) to pre-dict style information and phoneme duration. Sincestyles and duration of singing voices are closelyrelated, a composite module benefits both. More- over, the S&D-LM achieves both style transfer andstyle control with audio and text prompts. Next, weemploy the pitch diffusion predictor for F0 predic-tion and the style adaptive decoder to generate thetarget mel-spectrogram. The style adaptive decodergenerates intricately detailed singing voices usinga novel mel-style adaptive normalization method.During training, we train the clustering style en-coder for reconstruction in the first phase and S&D-LM for style prediction in the second phase. Dur-ing inference, we can input audio prompts or textprompts to S&D-LM for style transfer or control.Please refer to Appendix A for more details.",
  "Clustering Style Encoder": "To comprehensively capture styles (such as singingmethods, emotion, rhythm, technique, and pro-nunciation) from mel-spectrograms, we introducethe clustering style encoder. As shown in (d), the input mel-spectrogram is initially refinedthrough WaveNet blocks before being condensedinto phoneme-level hidden states by a pooling layerbased on the phoneme boundary. Subsequently, theconvolution stacks capture phoneme-level correla-tions. Next, we use a linear projection to map theoutput into a low-dimensional latent variable spacefor code index lookup, which can significantly in-crease the codebooks usage (Yu et al., 2021). TheCVQ layer (Zheng and Vedaldi, 2023) then usesthese inputs x to generate phoneme-level style rep-resentations, establishing an information bottleneckthat effectively eliminates non-style information.Through the dimensionality reduction of the linearprojection and the bottleneck of CVQ, we achievea decoupling of styles from timbre and contentinformation. Compared to traditional VQ (VanDen Oord et al., 2017), CVQ adopts a dynamicinitialization strategy during training, ensuring thatless-used or unused code vectors are modified morethan frequently used ones, thus solving the code-book collapse issue (Zheng and Vedaldi, 2023). Toenhance training stability and improve reconstruc-tion quality, we apply 2 normalization to the en-coded latent variables and all latent variables in thecodebook. 2 normalization has been proven effec-tive for VQ in the image domain (Yu et al., 2021).Notably, we are the first to use CVQ in the singingfield, ensuring stable and high-quality extraction ofstyle information. We input ground truth (GT) au-dio during training for learning diverse styles andprompt audio during inference for style transfer.For more details, please refer to Appendix A.2.",
  "Text Prompts": ": The architecture of TCSinger. In Figure (a), S&D-LM represents the Style and Duration Language Model,and LR stands for length regulator. In Figure (b), the S&D-LM autoregressively predicts style information andphoneme duration. In Figure (c), intermediate mel-spectrograms are refined with style information in the styleadaptive decoder. In Figure (d), the clustering style encoder extracts style information from mel-spectrograms.",
  "Style Adaptive Decoder": "The dynamic nature of singing voices poses asubstantial challenge to traditional mel-decoders,which often fail to effectively capture the intricaciesof mel-spectrograms. Furthermore, using VQ to ex-tract style information is inherently lossy (Razaviet al., 2019), and closely related styles can easilybe encoded into identical codebook indices. Con-sequently, if we employ traditional mel-decodershere, our synthesized singing voices may becomerigid and lacking in stylistic variation. To addressthese challenges, we introduce the style adaptivedecoder, which utilizes a novel mel-style adaptivenormalization method. While the adaptive instancenormalization method has been widely used in im-age tasks (Zheng et al., 2022), our work is the firstto refine overall mel-spectrograms using decou-pled style information. Our approach can infusestylistic variations into mel-spectrograms, therebygenerating more natural and diverse audio results,even when the same style quantization is used forclosely related styles in decoder inputs. As depicted in (c), our style adaptivedecoder is based on an 8-step diffusion-based de-coder (Huang et al., 2022b). We utilize FFT as thedenoiser and enhance it with multiple layers of ourmel-style adaptive normalization method. We de-note the intermediate mel-spectrogram of the i-thlayer in the diffusion decoder denoiser as mi. Ini-th layer, mi1 is initially normalized using a nor-malization method and then adapted by the scaleand bias that are computed from the style embed-ding s. Denote the mean and standard deviationcalculation as () and (). We employ Layer Nor-",
  "(mi1)+ (s).(1)": "() and () are two learned affine transforma-tions for converting s to the scaling and bias values.As () and () inject the stylistic variant in-formation, it encourages similar decoder inputs togenerate natural and diverse mel-spectrograms. Totrain the decoder, we use both the Mean AbsoluteError (MAE) loss and the Structural Similarity In-dex (SSIM) loss (Wang et al., 2004). For moredetails, please refer to Appendix A.6.",
  "S&D-LM": "Singing styles (like singing methods, emotion,rhythm, technique, and pronunciation) usually ex-hibit both local and long-term dependencies, andthey change rapidly over time with a weak corre-lation to content. This makes the conditional lan-guage model inherently ideal for predicting styles.Meanwhile, phoneme duration is rich in variationsand closely related to singing styles. Therefore, wepropose the Style and Duration Language Model(S&D-LM). Through S&D-LM, we can achieveboth zero-shot style transfer and multi-level stylecontrol using audio and text prompts.Style Transfer: Given the lyrics l, notes nof the target, along with lyrics l, notes n, mel-spectrogram m of the audio prompt, our goal is tosynthesize the high-quality target singing voicesmel-spectrogram m with unseen timbre and stylesof the audio prompt. Initially, we use different en-coders to extract the timbre information t, content",
  "c = Econtent(l, n), c = Econtent(l, n),(2)": "where E denotes encoders for each attribute. Giventhat the target timbre t is anticipated to mirror theaudio prompt, we also require the target styles s.Utilizing the powerful in-context learning capabili-ties of language models, we design the S&D-LM topredict s. Concurrently, we also use the S&D-LMto predict the target phoneme duration d, leveragingthe strong correlation between phoneme durationand styles in singing voices to enhance both pre-dictions. Our S&D-LM is based on a decoder-onlytransformer-based architecture (Brown et al., 2020).We concatenate the prompt phoneme duration d,prompt styles s, prompt content c, target content c,and target timbre t to form the input. The autore-gressive prediction process will be:",
  "m = D(s, d, t, c, F0).(4)": "Style Control: With alternative text prompts,we do not need to extract s and d from the audioprompt. Instead, we use the text encoder to en-code the global (singing method and emotion) andphoneme-level (technique for each phoneme) textprompts tp to concatenate with c, c, and t to formthe input. For more details about the text encoder,please refer to Appendix A.7. Subsequently, theprediction process of S&D-LM changes to:",
  "Training and Inference Procedures": "Training Procedures The final loss terms of TC-Singer in the training phase consist of the follow-ing parts: 1) CVQ loss LCV Q: the CVQ loss forthe clustering style encoder; 2) Pitch reconstruc-tion loss Lgdiff, Lmdiff: the Gaussian diffusionloss and the multinomial diffusion loss betweenthe predicted and the GT pitch spectrogram forthe pitch diffusion predictor; 3) Mel reconstructionloss Lmae, Lssim: the MAE loss and the SSIM lossbetween the predicted and the GT mel-spectrogramfor the style adaptive decoder. 4) Duration predic-tion loss Ldur: the MSE loss between the predictedand the GT phoneme-level duration in log scalefor S&D-LM in the teacher-forcing mode; 5) Styleprediction loss Lstyle: the cross-entropy loss be-tween the predicted and the GT style informationfor S&D-LM in the teacher-forcing mode.Inference with Style Transfer Refer to (a) and Equation 3, during inference of zero-shotstyle transfer, we use c, t, s, d extracted from theaudio prompt, and the target content c as inputsfor the S&D-LM, and obtain s, u. Then, since thetargets timbre and prompt remain unchanged, ac-cording to Equation 4, we concatenate the contentc, timbre t, style information s, and phoneme du-ration d of the target to generate F0 by the pitchdiffusion predictor, and final mel-spectrogram mby the style adaptive decoder. Therefore, the gener- ated target singing voice can effectively transfer thetimbre and styles of the audio prompt. Moreover,we can transfer cross-lingual speech and singingstyles. For cross-lingual experiments, the languageof the lyrics in the prompt and the target differ(such as English and Chinese), but the process re-mains the same. For STS experiments, speech datais used as the audio prompt, allowing the targetsinging voice to transfer the timbre and styles ofthe speech, with the remaining steps consistent.Inference with Style Control Refer to (b) and Equation 5. During inference for multi-level style control, the audio prompt provides onlythe timbre, eliminating the need to extract promptstyles using the clustering style encoder. Bothglobal and phoneme-level text prompts are encodedusing the text encoder to replace s and d, synthesiz-ing the target s and d, with the rest of the processconsistent with style transfer tasks. The globaltext prompt encompasses singing methods (e.g.,bel canto, pop) and emotions (e.g., happy, sad),while phoneme-level text prompts control tech-niques (e.g., mixed voice, falsetto, breathy, vibrato,glissando, pharyngeal) for each phoneme. Throughthese text prompts, we can generate singing voiceswith personalized timbre and independently con-trollable styles on both global and phoneme levels.",
  "Experimental Setup": "In this section, we present the datasets utilized byTCSinger, delve into the implementation and train-ing details, discuss the evaluation methodologies,and introduce the baseline models.Dataset We use the open-source singing datasetwith style annotations, GTSinger (Zhang et al.,2024b), specifically its Chinese and English sub-set (5 singers, 36 hours of Chinese and Englishsinging and speech). We also enrich our data withM4Singer (Zhang et al., 2022a) (20 singers, 30hours of Chinese singing), OpenSinger (Huanget al., 2021) (93 singers, 85 hours of Chinesesinging), AISHELL-3 (Shi et al., 2021) (218singers, 85 hours of Chinese speech), and a subsetof PopBuTFy (Liu et al., 2022b) (20 singers, 18hours of English speech and singing). Then, wemanually annotate these singing data with globalstyle labels (singing method and emotion) and sixphoneme-level singing techniques. Subsequently,we randomly chose 40 singers as the unseen testset to evaluate TCSinger in the zero-shot scenario for all tasks. Notably, our dataset partitioning care-fully ensures that both training and test sets for alltasks include cross-lingual singing and speech data.Please refer to Appendix B for more details.Implementation Details We set the sample rate to48000Hz, the window size to 1024, the hop size to256, and the number of mel bins to 80 to derive mel-spectrograms from raw waveforms. The defaultsize of the codebook for CVQ is 512. The S&D-LM model is a decoder-only architecture with 8Transformer layers and 512 embedding dimensions.Please refer to Appendix A.1 for more details.Training Details We train our model using fourNVIDIA 3090 Ti GPUs. The Adam optimizer isused with 1 = 0.9 and 2 = 0.98. The main SVSmodel takes 300k steps and the S&D-LM takes100k steps to train until convergence. Output mel-spectrograms are transformed into singing voicesby a pre-trained HiFi-GAN (Kong et al., 2020).Evaluation Details We use both objective and sub-jective evaluation metrics to validate the perfor-mance of TCSinger. For subjective metrics, weconduct the MOS (mean opinion score) and CMOS(comparative mean opinion score) evaluation. Weemploy the MOS-Q to judge synthesis quality(including clarity, naturalness, and rich stylisticdetails), MOS-S to assess singer similarity (interms of timbre and styles) between the result andprompt, and MOS-C to evaluate style controllabil-ity (accuracy and expressiveness of style control).Both these metrics are rated from 1 to 5 and re-ported with 95% confidence intervals. In the abla-tion study, we employ CMOS-Q to gauge synthesisquality, and CMOS-S to evaluate singer similarity.For objective metrics, we use Singer Cosine Sim-ilarity (Cos) to judge singer similarity, and MeanCepstral Distortion (MCD) along with F0 FrameError (FFE) to quantify synthesis quality. Pleaserefer to Appendix C for more details.Baseline Models We conduct a comprehensivecomparative analysis of synthesis quality, stylecontrollability, and singer similarity for TCSingeragainst several baseline models. Initially, we eval-uate our model against the ground truth (GT) andthe audio generated by HiFi-GAN (GT (vocoder)).Additionally, we examine TCSinger with two high-performing speech models that conduct style trans-fer: YourTTS (Casanova et al., 2022) and Mega-TTS (Jiang et al., 2023). To ensure a fair com-parison for singing tasks, we enhance these mod-els with a note encoder to process music scoresand train them on speech and singing data. Subse-",
  "Main Results": "Zero-Shot Style Transfer To assess the perfor-mance of TCSinger and baseline models in thezero-shot style transfer task, we randomly selectsamples with unseen singers from the test set as tar-gets and different utterances from the same singersto form prompts. As shown in , we have thefollowing findings: 1) TCSinger exhibits outstand-ing synthesis quality, as indicated by the highestMOS-Q and the lowest FFE and MCD. This under-scores the models impressive adaptability in han-dling zero-shot SVS scenarios. 2) TCSinger alsoexcels in singer similarity, as denoted by the highestMOS-S and Cos. This highlights our models supe-rior ability to model and transfer different singingstyles precisely, thanks to the innovative designof our components. Our style adaptive decodereffectively improves the rich stylistic details of syn-thesis quality, rendering the singing voices morenatural and of superior quality. Meanwhile, ourclustering style encoder shows an excellent capa-bility for modeling styles across a wide range ofcategories. Finally, the S&D-LM delivers excel-lent prediction results for style information andphoneme duration, significantly contributing to syn-thesis quality and singer similarity. As shown in, our TCSinger not only displays greaterdetails in the mel-spectrogram, but also effectivelylearns the technique, pronunciation, and rhythmof the audio prompt. In contrast, other baselinemodels lack details in mel-spectrograms, and theirpitch curves remain flat, failing to transfer diversesinging styles. Upon listening to demos, it can be found that our model effectively transfers timbre,singing methods, emotion, rhythm, technique, andpronunciation of audio prompts.Multi-Level Style Control We add global andphoneme-level text embedding to each baselinemodel to enable style control. Then, we compareTCSinger using multi-level text prompts. We con-duct both parallel and non-parallel experiments ac-cording to the target styles. In the parallel experi-ments, we randomly select unseen audio from thetest set, using the GT global style and phoneme-level techniques as the target. In the non-parallelexperiments, global styles and six techniques arerandomly yet appropriately assigned. For globalstyles, we specify singing methods (bel canto andpop) and emotions (happy and sad) for each test tar-get. For phoneme-level styles, we select none, oneor more specific techniques (mixed voice, falsetto,breathy, vibrato, glissando, and pharyngeal) foreach phoneme of target content. As shown in Ta-ble 2, we can find that TCSinger surpasses otherbaseline models in both the highest synthesis qual-ity (MOS-Q) and style controllability (MOS-C) inboth parallel and non-parallel experiments. Thisindicates that, in addition to excelling in styletransfer, our model also performs well in multi-level style control, and we are the first methodfor multi-level singing style control. This successis attributed to our clustering style encoders ex-ceptional style modeling capabilities, the S&D-LMs effective style control, and the style adap-tive decoders capacity to generate stylistically richsinging voices. Upon listening to demos, it is obvi-ous that our model effectively controls the globalsinging method and emotion, as well as phoneme-level techniques. For more detailed results withobjective evaluations, please refer to Appendix E.2.Cross-Lingual Style Transfer To test the zero-shot cross-lingual style transfer performance of var-ious models, we use unseen test data with different",
  ": Synthesis quality and singer similarity compar-isons for zero-shot cross-lingual style transfer. We useMOS-Q and MOS-S for comparison": "lyrics languages as prompts and targets for infer-ence (like English and Chinese), using MOS-Q andMOS-S as evaluation. As shown in , our TC-Singer outperforms other baseline models regard-ing both synthesis quality (MOS-Q) and singer sim-ilarity (MOS-S). Benefiting from our models forcomprehensively modeling and effectively transfer-ring diverse styles, TCSinger performs well in across-lingual environment.Speech-to-Singing Style Transfer We conductedexperiments on both parallel and cross-lingual STSstyle transfer. In parallel experiments, we randomlyselect samples with unseen singers from the test set as targets and different speech from the samesingers to form prompts. In cross-lingual experi-ments, we select the speech prompt in a differentlyric language from the target (such as Chinese andEnglish). As shown in , we can find thatboth synthesis quality (MOS-Q) and singer similar-ity (MOS-S) of TCSinger are superior to those ofbaseline models in both parallel and cross-lingualSTS experiments. This demonstrates the excellentability of our model in cross-lingual speech andsinging style modeling and transfer.",
  "Ablation Study": "As depicted in , we conduct ablation studiesto showcase the efficacy of various designs withinTCSinger. We use CMOS-Q to test the variationin synthesis quality, and CMOS-S to measure thechanges in singer similarity. 1) Using VQ insteadof CVQ in the clustering style encoder resulted indecreased synthesis quality and singer similarity,indicating the importance of CVQ for stable andhigh-quality style extraction. 2) Eliminating thestyle adaptive decoder and using an 8-step diffu-sion decoder (Huang et al., 2022b) led to declines",
  ": Synthesis quality and singer similarity com-parisons for ablation study. SAD denotes style adaptivedecoder and DM means duration model of S&D-LM.We use CMOS-Q and CMOS-S for comparison": "in both synthesis quality and singer similarity, un-derscoring the role of our method in enhancingstyle diversity in singing voices. 3) Predicting onlystyles in the S&D-LM while using a simple du-ration predictor (Ren et al., 2020) for phonemeduration also resulted in decreased synthesis qual-ity and singer similarity. This demonstrates themutual benefits of our model in predicting bothphoneme duration and style information. Pleaserefer to Appendix E.1 for more results.",
  "Conclusion": "In this paper, we introduce TCSinger, the first zero-shot SVS model for style transfer across cross-lingual speech and singing styles, along with multi-level style control. TCSinger transfers and con-trols styles (like singing methods, emotion, rhythm,technique, and pronunciation) from audio and textprompts to synthesize high-quality singing voices.The performance of our model is primarily en-hanced through three key components: 1) the clus-tering style encoder that stably condenses styleinformation into a compact latent space using aCVQ model, thus facilitating subsequent predic-tions; 2) the Style and Duration Language Model(S&D-LM), which predicts style information andphoneme duration simultaneously, which benefitsboth; and 3) the style adaptive decoder that em- ploys a novel mel-style adaptive normalizationmethod to generate enhanced details in singingvoices. Experimental results demonstrate that TC-Singer surpasses baseline models in synthesis qual-ity, singer similarity, and style controllability acrosszero-shot style transfer, multi-level style control,cross-lingual style transfer, and STS style transfer.",
  "Limitations": "Our method has two primary limitations. First, itcurrently supports control over only six singingtechniques, which does not encompass the fullrange of commonly used singing techniques. Fu-ture work will focus on broadening the range ofcontrollable techniques to enhance the versatility ofstyle control tasks. Second, our multilingual datacurrently only facilitates cross-lingual style trans-fer between Chinese and English. In the future,we plan to gather more diverse language data forconducting multilingual style transfer experiments.",
  "Ethics Statement": "TCSinger, with its capability to transfer and controldiverse styles of singing voices, could potentiallybe misused for dubbing in entertainment videos,raising concerns about the infringement of singerscopyrights.Additionally, its ability to transfercross-lingual speech and singing styles poses risksof unfair competition and potential unemploymentfor professionals in related singing occupations. Tomitigate these risks, we will implement stringentrestrictions on the use of our model to prevent unau-thorized and unethical applications. We will alsoexplore methods such as vocal watermarking toprotect individual privacy.",
  "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-ton. 2016.Layer normalization.arXiv preprintarXiv:1607.06450": "Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,and Michael Auli. 2020. wav2vec 2.0: A frameworkfor self-supervised learning of speech representations.Advances in neural information processing systems,33:1244912460. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Edresson Casanova, Julian Weber, Christopher DShulby, Arnaldo Candido Junior, Eren Glge, andMoacir A Ponti. 2022. Yourtts: Towards zero-shotmulti-speaker tts and zero-shot voice conversion foreveryone. In International Conference on MachineLearning, pages 27092720. PMLR. Sanyuan Chen, Chengyi Wang, Zhengyang Chen,Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, NaoyukiKanda, Takuya Yoshioka, Xiong Xiao, et al. 2022.Wavlm: Large-scale self-supervised pre-training forfull stack speech processing. IEEE Journal of Se-lected Topics in Signal Processing, 16(6):15051518.",
  "Yin-Ping Cho, Yu Tsao, Hsin-Min Wang, and Yi-WenLiu. 2022. Mandarin singing voice synthesis withdenoising diffusion probabilistic wasserstein gan": "Soonbeom Choi and Juhan Nam. 2022.A melody-unsupervision model for singing voice synthesis.In ICASSP 2022-2022 IEEE International Confer-ence on Acoustics, Speech and Signal Processing(ICASSP), pages 72427246. IEEE. Erica Cooper, Cheng-I Lai, Yusuke Yasuda, FumingFang, Xin Wang, Nanxin Chen, and Junichi Ya-magishi. 2020.Zero-shot multi-speaker text-to-speech with state-of-the-art neural speaker embed-dings. In ICASSP 2020-2020 IEEE InternationalConference on Acoustics, Speech and Signal Process-ing (ICASSP), pages 61846188. IEEE.",
  "Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. De-noising diffusion probabilistic models. Advancesin neural information processing systems, 33:68406851": "Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,Kushal Lakhotia, Ruslan Salakhutdinov, and Abdel-rahman Mohamed. 2021. Hubert: Self-supervisedspeech representation learning by masked predictionof hidden units. IEEE/ACM Transactions on Audio,Speech, and Language Processing, 29:34513460. Rongjie Huang, Feiyang Chen, Yi Ren, Jinglin Liu,Chenye Cui, and Zhou Zhao. 2021. Multi-singer:Fast multi-singer singing voice vocoder with a large-scale corpus. In Proceedings of the 29th ACM In-ternational Conference on Multimedia, pages 39453954.",
  "Rongjie Huang, Yi Ren, Jinglin Liu, Chenye Cui,and Zhou Zhao. 2022a.Generspeech: Towardsstyle transfer for generalizable out-of-domain text-to-speech synthesis. arXiv preprint arXiv:2205.07211": "Rongjie Huang, Zhou Zhao, Huadai Liu, Jinglin Liu,Chenye Cui, and Yi Ren. 2022b. Prodiff: Progressivefast diffusion model for high-quality text-to-speech.In Proceedings of the 30th ACM International Con-ference on Multimedia, pages 25952605. Ziyue Jiang, Yi Ren, Zhenhui Ye, Jinglin Liu, ChenZhang, Qian Yang, Shengpeng Ji, Rongjie Huang,Chunfeng Wang, Xiang Yin, et al. 2023. Mega-tts:Zero-shot text-to-speech at scale with intrinsic induc-tive bias. arXiv preprint arXiv:2306.03509. Sungjae Kim, Yewon Kim, Jewoo Jun, and Injung Kim.2023. Muse-svs: Multi-singer emotional singingvoice synthesizer that controls emotional intensity.IEEE/ACM Transactions on Audio, Speech, and Lan-guage Processing.",
  "Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero PSimoncelli. 2004. Image quality assessment: fromerror visibility to structural similarity. IEEE transac-tions on image processing, 13(4):600612": "Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruom-ing Pang, James Qin, Alexander Ku, Yuanzhong Xu,Jason Baldridge, and Yonghui Wu. 2021. Vector-quantized image modeling with improved vqgan.arXiv preprint arXiv:2110.04627. Lichao Zhang, Ruiqi Li, Shoutong Wang, Liqun Deng,Jinglin Liu, Yi Ren, Jinzheng He, Rongjie Huang,Jieming Zhu, Xiao Chen, et al. 2022a. M4singer:A multi-style, multi-singer and musical score pro-vided mandarin singing corpus. Advances in NeuralInformation Processing Systems, 35:69146926. Yongmao Zhang, Heyang Xue, Hanzhao Li, Lei Xie,Tingwei Guo, Ruixiong Zhang, and Caixia Gong.2022b. Visinger 2: High-fidelity end-to-end singingvoice synthesis enhanced by digital signal processingsynthesizer. Yu Zhang, Rongjie Huang, Ruiqi Li, JinZheng He, YanXia, Feiyang Chen, Xinyu Duan, Baoxing Huai, andZhou Zhao. 2024a. Stylesinger: Style transfer for out-of-domain singing voice synthesis. In Proceedingsof the AAAI Conference on Artificial Intelligence,volume 38, pages 1959719605. Yu Zhang, Changhao Pan, Wenxiang Guo, Ruiqi Li,Zhiyuan Zhu, Jialei Wang, Wenhao Xu, JingyuLu, Zhiqing Hong, Chuxin Wang, LiChao Zhang,Jinzheng He, Ziyue Jiang, Yuxin Chen, Chen Yang,Jiecheng Zhou, Xinyu Cheng, and Zhou Zhao. 2024b.Gtsinger: A global multi-technique singing corpuswith realistic music scores for all singing tasks.",
  "A.2Clustering Style Encoder": "In the first phase, we train the clustering vectorquantization (CVQ) codebook. Here, the cluster-ing style encoder extracts style information directlyfrom the ground truth (GT) audio. During the sec-ond phase of training, we train the Style and Du-ration Language Model (S&D-LM) by extractingstyle information from the GT audio and inputtingit into the S&D-LM, facilitating training in theteacher-forcing mode. During style transfer infer-ence, we use audio prompts to extract style infor-mation and then input it into the S&D-LM. CVQ selects encoded features as anchors to up-date the unused or less-used code vectors. Thisstrategy brings unused code vectors closer in dis-tribution to the encoded features, increasing thelikelihood of being chosen and optimized. To trainthe clustering style encoder, we use the CVQ losswith 2 normalization and the contrastive loss:",
  "Ni=1 esim(ek,zi )/ In particular, for": "each code vector ek, we directly select the clos-est feature z+i as the positive pair and sample otherfarther features zi as negative pairs using the dis-tance computations with 2 normalization. Whencomputing the distance, we also use 2 normaliza-tion to map all features and latent variables in thecodebook onto a sphere. The Euclidean distance of2-normalized latent variables 2(ek) 2(zi)22is transformed into the cosine similarity betweenthe code vectors ek and the feature zi. The con-trastive loss effectively encourages sparsity in thecodebook (Zheng and Vedaldi, 2023).",
  "A.3Content Encoder": "Our content encoder is composed of a phonemeencoder and a note encoder. The phoneme en-coder processes a sequence of phonemes througha phoneme embedding layer and four FFT blocks,culminating in the production of phoneme features.On the other hand, the note encoder is responsiblefor handling musical score information. It pro-cesses note pitches, note types (including rest, slur,grace, etc.), and note duration. Note pitches, types,and duration undergo processing through two em-bedding layers and a linear projection layer respec-tively, thereby generating note features.",
  "q(yt|yt1) = C(yt|(1 t)yt1 + t/K),(7)": "where C denotes a categorical distribution withprobability parameters, xt {0, 1}K, and t isthe probability of uniformly resampling a category.In the reverse process, we train a neural networkto approximate the noise from the noisy input xtand y0 from the noisy sample yt at timestep t. Theequations of the reverse process are as follows:",
  "A.7Text Encoder": "Our text encoder serves as a modular componentwithin our framework, with a remarkably straight-forward structure, akin to the type embeddingmodel used in the note encoder.Our text en-coder includes a global style embedding for pro-cessing global text prompts and a phoneme-levelstyle embedding for handling phoneme-level textprompts. Notably, the entire target should use thesame singing method and emotion for naturalness,while techniques can vary between phonemes. Ourdivision into global and phoneme-level styles re-flects this necessity. For global style embedding,our labeling encompasses two categories of infor-mation: two emotions (happy and sad) and twosinging methods (bel canto and pop). We can spec-ify these two categories, and our text encoder willprocess them into embedding. For phoneme-levelstyle embedding, each phoneme can be specifiedwith up to six techniques. The techniques we usedinclude mixed voice, falsetto, breathy, vibrato, glis-sando, and pharyngeal. We process the techniquelist into six technique lists with phoneme lengthsand embed each separately. Finally, we concatenateall these embeddings to form the text embedding.During both training and inference, multi-level textprompts are thus embedded, transforming into vec-tors of 512 embedding size. This size is maintainedconsistent with the hidden size of the S&D-LM, en-suring seamless integration and processing withinour model architecture.",
  "Currently, most open-source singing datasetslack music scores and multi-level style annota-": "tions. We use the only open-source singing datasetwith style annotations GTSinger (Zhang et al.,2024b), specifically its Chinese and English sub-set (5 singers, 36 hours of Chinese and Englishsinging and speech). Additionally, we incorporateM4Singer (Zhang et al., 2022a) (20 singers and 30hours of Chinese singing) to expand the diversityof singers and styles. Subsequently, we also addOpenSinger (Huang et al., 2021) (93 singers and85 hours of Chinese singing), AISHELL-3 (Shiet al., 2021) (218 singers and 85 hours of Chinesespeech), and a subset of PopBuTFy (Liu et al.,2022b) (20 singers, 10 hours of English speech,and 8 hours of English singing) to further expandthe dataset. None of these three datasets has mu-sic scores and alignments, so we use ROSVOT (Liet al., 2024) for coarse music score annotations andthe Montreal Forced Aligner (MFA) (McAuliffeet al., 2017) for the coarse alignment between lyricsand audio. The time distribution of our datasets forcross-lingual speech and singing data are listed in. We use all these datasets under licenseCC BY-NC-SA 4.0. Moreover, with the assistanceof music experts, we manually annotate part ofsinging data with distinct global style class labels.We categorize songs into happy and sad based onemotion. In singing methods, we classify songsas bel canto and pop. These classifications arecombined into the final style class labels, whichwill be the global text prompts. We also annotatephoneme-level techniques for these singing data.We annotate phoneme-level techniques includingmixed voice, falsetto, breathy, vibrato, glissando,and pharyngeal. These phoneme-level techniquelabels form the phoneme-level text prompts. Wehire all music experts and annotators with musi-cal backgrounds at a rate of $300 per hour. Theyhave agreed to make their contributions used forresearch purposes.For phonetic content, Chinese phonemes wereextracted using pypinyin 1, English phonemes fol-lowed the ARPA standard 2. We selected thesestandards because Chinese uses pinyin for pronun-ciation and ARPA includes English stress patterns,making them the most suitable phoneme standardsfor each language. We then add all phonemes ina unified phoneme set. This strategy allows ourmodel to embed phonemes for all languages duringtraining for all tasks. Subsequently, we randomly chose 40 singers as the unseen test set to evaluateTCSinger in the zero-shot scenario for all tasks.Notably, our dataset partitioning carefully ensuresthat both training and test sets for all tasks includecross-lingual singing and speech data.",
  "C.1Subjective Evaluation": "For each task, we randomly select 20 pairs of sen-tences from our test set for subjective evaluation.Each pair consists of an audio prompt that pro-vides timbre and styles, and a synthesized singingvoice, each of which is listened to by at least 15professional listeners. In the context of MOS-Qand CMOS-Q evaluations, these listeners are in-structed to concentrate on synthesis quality (includ-ing clarity, naturalness, and rich stylistic details),irrespective of singer similarity (in terms of timbreand styles). Conversely, during MOS-S and CMOS-S evaluations, the listeners are directed to assesssinger similarity (singer similarity in terms of tim-bre and styles) to the audio prompt, disregardingany differences in content or synthesis quality (in-cluding quality, clarity, naturalness, and rich stylis-tic details). For MOS-C, the listeners are informedto evaluate style controllability (accuracy and ex-pressiveness of style control), disregarding any dif-ferences in content, timbre, or synthesis quality(including quality, clarity, naturalness, and richstylistic details). In MOS-Q, MOS-S, and MOS-Cevaluations, listeners are requested to grade varioussinging voice samples on a Likert scale rangingfrom 1 to 5. For CMOS-Q and CMOS-S evalu-ations, listeners are guided to compare pairs ofsinging voice samples generated by different sys-tems and express their preferences. The preferencescale is as follows: 0 for no difference, 1 for aslight difference, and 2 for a significant difference.It is important to note that all participants are fairlycompensated for their time and effort. We compen-sated participants at a rate of $12 per hour, with atotal expenditure of approximately $300 for partic-ipant compensation. Participants are informed thatthe results will be used for scientific research.",
  "C.2Objective Evaluation": "To objectively evaluate the timbre similarity andsynthesis quality of the test set, we employ threemetrics: Cosine Similarity (Cos), F0 Frame Er-ror (FFE), and Mean Cepstral Distortion (MCD).Cosine Similarity is used to measure the resem- blance in the singers identity between the synthe-sized singing voice and the audio prompt. This isdone by computing the average cosine similaritybetween the embeddings extracted from the synthe-sized voices and the audio prompt, thus providingan objective indication of the performance in singersimilarity. To be more specific, we use the WavLM(Chen et al., 2022) fine-tuned for speaker verifica-tion 3 to extract singer embedding. Subsequently,we use FFE, which amalgamates metrics for voic-ing decision error and F0 error. FFE effectively cap-tures essential synthesis quality information. Next,we employ MCD for measuring audio quality:",
  "DDetails of Baseline Models": "In the related works section, we have describedthe characteristics of each baseline model and dis-cussed their weaknesses. YourTTS, primarily ap-plied to English speech, conditions the affine cou-pling layers of the flow-based decoder to handlezero-shot tasks. However, it does not model variousstyles in detail (e.g., rhythm, pronunciation) and islimited to speech, as well as lacking controllability.Mega-TTS, which can be applied to both Englishand Chinese speech, decomposes speech into mul-tiple attributes. However, it does not model variousstyles in detail (e.g., emotion) and is also limitedto speech, lacking controllability. RMSSinger pri-marily focuses on Chinese singing voices and usesa diffusion-based pitch predictor to model F0 andimprove generation quality. However, it cannotperform style transfer or zero-shot SVS and lackscontrollability. StyleSinger, which primarily ap-plies to Chinese singing, employs a residual quan-tization model to capture detailed styles in singingvoices. However, it does not consider the stylesof singing methods and techniques and also lackscontrollability.",
  ": The singer similarity results from using thetimbre of Singer A and the style information of SingerB to synthesize the target singing voice": "ditional experiments. In these tests, we utilizedthe timbre of singer A and the style information ofsinger B. The results are shown in . Ob-jective metrics Cosine similarity and subjectivemetrics MOS-T (where more than 15 professionallisteners focus solely on the timbre similarity, dis-regarding quality and styles, ranging from 1 to 5, 5means very similar), indicate that the synthesizedresults match the timbre of singer A while differingfrom that of singer B. This outcome shows thatour clustering style encoder successfully decouplestimbre and style in the mel-spectrogram.",
  "E.2Multi-Level Style Control": "Currently, there are no open-source classifiers forsinging emotions or techniques to use for objectiveevaluation. Moreover, we are the first to conductmulti-level style control for singing, making theuse of objective metrics quite challenging, and theaccuracy of classifiers may not fully reflect the ef-fectiveness. For instance, emotion in singing is rel-atively difficult for the model to judge, and detailedtechnique variations also result in low accuracy.Here, we provide the results of our tested emo-tion classifier. We fine-tune it based on WavLM(Chen et al., 2022), achieving an accuracy of 85.1%for binary emotion classification. Using this, weprovided the objective metric for emotion control(represented as acc_emo, in %), averaged over thetest sets emotion accuracy:",
  ": Singing technique classification accuracy(acc_meth) across different methods": "Technique recognition is relatively more com-plex. We design a technique recognition modelbased on ROSVOT (Li et al., 2024) and use cross-entropy loss for technique labels. The inputs ofthe technique recognition model include the mel-spectrogram, pitch, and phoneme boundaries, withthe output being the predicted probabilities of sixtechniques. We first provide the technique recogni-tion models performance on our dataset:"
}