{
  "Abstract": "Best practices for high conflict conversationslike counseling or customer support almost al-ways include recommendations to paraphrasethe previous speaker. Although paraphrase clas-sification has received widespread attention inNLP, paraphrases are usually considered inde-pendent from context, and common modelsand datasets are not applicable to dialog set-tings. In this work, we investigate paraphrasesacross turns in dialog (e.g., Speaker 1: Thatbook is mine. becomes Speaker 2: That bookis yours.). We provide an operationalizationof context-dependent paraphrases, and developa training for crowd-workers to classify para-phrases in dialog. We introduce ContextDeP,a dataset with utterance pairs from NPR andCNN news interviews annotated for context-dependent paraphrases. To enable analysis onlabel variation, the dataset contains 5,581 an-notations on 600 utterance pairs. We presentpromising results with in-context learning andwith token classification models for automaticparaphrase detection in dialog.",
  "Introduction": "Repeating or paraphrasing what the previousspeaker said has time and time again been foundto be important in human-to-human or human-to-computer dialogs: It encourages elaboration and in-trospection in counseling (Rogers, 1951; Miller andRollnick, 2012; Hill, 1992; Shah et al., 2022), canhelp deescalate conflicts in crisis negotiations (Vec-chi et al., 2005; Voss and Raz, 2016; Vecchi et al.,2019), can have a positive impact on relationships(Weger Jr et al., 2010; Roos, 2022), can increasethe perceived response quality of dialog systems(Weizenbaum, 1966; Dieter et al., 2019) and gen-erally provides tangible understanding-checks toground what both speakers agree on (Clark, 1996;Jurafsky and Martin, 2019).Fortunately, in NLP, paraphrases have receivedwide-spread attention: Researchers have created",
  "Guest:And people always prefer, of course, to see the pope": "as the principal celebrant of the mass. So thats good. Thatllbe tonight. And it will be his 26th mass and it will be the 40thor, rather, the 30th time that this is offered in round the worldtransmission. And it will be my 20th time in doing it as atelevision commentator from Rome so.Host: Yes, youve been doing this for a while now. : Context-Dependent Paraphrase in a NewsInterview. The interview host paraphrases part of theguests utterance. It is only a paraphrase in the currentcontext (e.g., doing something 20 times and doing some-thing for a while are not generally synonymous). Ourannotators provide word-level highlighting. The colorsintensity shows the share of annotators that selectedthe word. Here, most annotators selected the same textspans, some included from Rome as part of what isparaphrased by the host. We underline the paraphraseidentified by our fine-tuned DeBERTa token classifier. numerous paraphrase datasets (Dolan and Brock-ett, 2005; Zhang et al., 2019; Dong et al., 2021;Kanerva et al., 2023), developed methods to auto-matically identify paraphrases (Zhang et al., 2019;Wei et al., 2022a; Zhou et al., 2022), and used para-phrase datasets to train semantic sentence represen-tations (Reimers and Gurevych, 2019; Gao et al.,2021) and benchmark LLMs (Wang et al., 2018;bench authors, 2023). However, most previouswork (1) has focused on context-independent para-phrases, i.e., texts that are semantically equivalentindependent from the given context, and has notinvestigated the automatic detection of paraphrasesacross turns in dialog, (2) has classified paraphrasesat the level of full texts even though paraphrasesoften only occur in portions of larger texts (see also), (3) uses a small number of 13 anno-tations per paraphrase pair (Dolan and Brockett,2005; Kanerva et al., 2023), (4) only annotate textpairs that are likely to include paraphrases us-ing heuristics such as lexical similarity (Dolan andBrockett, 2005), although, especially for the dialogsetting, we can not expect lexical similarity to be",
  "RANDOM0.720.23": "G: So both parties agree that we need to stop horrific acts of violence againstanimals. But everyone is standing behind this. It is time to stop horrific acts ofbrutality on animals.H: Britains Queen Elizabeths senior dresser writes \"If her majesty is due to attendan engagement in particularly cold weather from 2019 onwards fake fur will be usedto make sure she stays warm.\" its a very stark example of a monarch followingpublic opinion in the U.K. which is moving away from fur and it very muchembraces prevention of cruelty to the animals.",
  "PARA0.650.19": "G: [...] it could be programmed in. But again, youd have to set that up as part ofyour flight plan.H: So youd have to say Im going to drop to 5,000 feet, then go back up to35,000 feet, and you would have had to have done that at the beginning.",
  "/15": ": Agreement Scores as an Indicator of Plausible Variation. For each dataset, we display the accuracywith the majority vote (Acc.) which is the mean overlap of a raters classification with the majority vote classificationexcluding the current rater and Krippendorff (1980)s alpha () for the binary classifications by all raters over allpairs. The relatively low Ks scores can be explained by pairs where either label is plausible. We display such anexample for each dataset with the share of annotators classifying it it as a paraphrase (Vote). high for all or even most paraphrase pairs (e.g., thepair in only overlaps in two words) and (5)either use short annotation instructions (Dolan andBrockett, 2005) that rely on annotator intuitionsor long and complex instructions (Kanerva et al.,2023) that limit the total number of annotators.We address all five limitations with this work.First, we are, to the best of our knowledge, thefirst to focus on operationalizing, annotating andautomatically detecting context-dependent para-phrases across turns in dialog. Dialog is a settingthat is uniquely sensitive to context (Grice, 1957,1975; Davis, 2002), e.g., doing this for a whilenow and 20th time [...] as a television commen-tator in are not generally semanticallyequivalent. Second, instead of classifying whethertwo complete texts A and B are paraphrases of eachother, we focus on classifying whether there existsa selection of a text B that paraphrases a selectionof a text A, and identifying the text spans thatconstitute the paraphrase pair (e.g., ).Third, we collect a larger number of annotationsof up to 21 per item in line with typical effortsto address plausible human label variation (Nieet al., 2020; Sap et al., 2022). Even though context-dependent paraphrase identification in dialog mightat first seem straight forward with a clear groundtruth, similar to other objective tasks in NLP(Uma et al., 2021), human annotators (plausibly)disagree on labels (Dolan and Brockett, 2005; Kan-erva et al., 2023). For example, consider the first text pair in . [The money] cant hurt canbe interpreted in at least two different ways: as astatement with approximately the same meaning asthe money will help or as an opposing statementmeaning the money actually wont help but at leastIt cant hurt either. Fourth, instead of using heuris-tics to select text pairs for annotations, we choosea dialog setting where paraphrases are relativelylikely to occur: transcripts of NPR and CNN newsinterviews (Zhu et al., 2021) since in (news) inter-views paraphrasing or more generally active listen-ing is encouraged (Clayman and Heritage, 2002;Hight and Smyth, 2002; Sedorkin et al., 2023).While the interview domain shows some uniquecharacteristics limiting generalizability (e.g., hostsusing paraphrases to simplify the guests statementsfor the audience), the interview domain is is suit-able to demonstrate our new task and includes adiverse set of topics and guests. Fifth, we developan annotation procedure that goes beyond relyingon intuitions and is scalable to a large number ofannotators: an accessible example-centric, hands-on, 15-minute training before annotation.In short, we operationalize context-dependentparaphrases in dialog with a definition and aniteratively developed hands-on training for an-notators. Then, annotators classify paraphrasesand identify the spans of text that constitute theparaphrase.We release ContextDeP (Context-",
  "G: Im like, \"Fortnite\", what is that? Idont even know what it is H: So, you werent even familiar?G: My wife is going through the samething herself.H: Shes also looking for work": ": Contextual Paraphrases (CP). We includetext spans ( CP) that range from clear to approximateequivalence for the given context. Few examples arevery clear. Deciding between approximate equivalenceand non-equivalence turns out to be a difficult task. Inour dataset, annotator agreement scores can be used asa proxy for the ambiguity of an item. pairs from NPR and CNN news interviews. We usein-context learning (ICL) with generative modelslike Llama 2 or GPT-4 and fine-tune a DeBERTatoken classifier to detect paraphrases in dialog. Wereach promising results of F1 scores from 0.73 to0.81. Generative models perform better at clas-sification, while the token classifier provides textspans without parsing errors. We hope to advancedialog based evaluations of LLMs and the reliabledetection of paraphrases in dialog. Code1, anno-tated data2,3 and the trained model4 are publiclyavailable for research purposes.",
  "Related Work": "Paraphrases have most successfully been classi-fied by encoder architectures with fine-tuned clas-sification heads (Zhang et al., 2019; Wahle et al.,2023) and more recently using in-context learningwith generative models like GPT-3.5 and Llama 2(Wei et al., 2022a; Wang et al., 2022c; Wahle et al.,2023). To the best of our knowledge, only Wanget al. (2022a) go beyond classifying paraphrases atthe complete sentence level. They use a DeBERTatoken classifier to highlight text spans that are notpart of a paraphrase, i.e., the reverse of our task.",
  "G: There are militant groups out there fir-ing against the military.H: Why did the army decide today to movein and clear out the camp?": ": Non-Paraphrases in Dialog. We do not in-clude text pairs ( CP) that are semantically related butwhere the second speaker does not actually rephrasea point the first speaker makes. Frequent cases aretext spans that might only be considered approximatelyequivalent when taken out of context (underlined) andpairs that have too distant meanings, for example, whenthe interviewer continues with the same or a relatedtopic but adds further-reaching conclusions or new facts. Paraphrase taxonomies commonly go beyondbinary classifications to make more fine-graineddistinctions between paraphrase types, often in-cluding considerations w.r.t. the context of the textpairs. Bhagat and Hovy (2013) and Kovatchev et al.(2018) describe substitutions and other lexical oper-ations that result in paraphrases in a given sententialcontext. Shwartz and Dagan (2016) show that con-text information can reverse semantic relations be-tween phrases. Vila et al. (2014) discuss text pairsthat are equivalent when one presupposes encyclo-pedic or situational knowledge (e.g., referents orintentions5), but exclude them as non-paraphrases.Further, to the best of our knowledge, most pre-vious work annotate sentence pairs without con-sidering the document context, with Kanerva et al.(2023) being the only exception, and no previouswork looking at detecting paraphrases in dialog.Dialog act taxonomies aim to classify thecommunicative function of an utterance indialog and commonly include acts such asSummarize/Reformulate (Stolcke et al., 2000;Core and Allen, 1997). However, generally, com-municative function can be orthogonal to meaningequivalence. For example, the paraphrase from Ta-ble 2 So you werent even familiar? would prob-ably be a Declarative Yes-No-Question dialogact (Stolcke et al., 2000), while the non-paraphraseSo you dont have a problem with ... ? in would also be a Declarative Yes-No-Question.We see paraphrase detection in dialog as more ele-",
  "Context-Dependent Paraphrases inDialog": "In NLP, paraphrases typically are pairs of text thatare approximately equivalent in meaning (Bhagatand Hovy, 2013), since full equivalence usuallyonly applies for practically identical strings (Bha-gat and Hovy, 2013; Dolan and Brockett, 2005) with some scholars even claiming that differentsentences can never be fully equivalent in meaning(Hirst, 2003; Clark, 1992; Bolinger, 1974). Thefield of NLP has mostly focused on paraphrasesthat are context-independent, i.e., approximatelyequivalent without considering a given context(Dolan and Brockett, 2005; Wang et al., 2018;Zhang et al., 2019). Some studies have opera-tionalized paraphrases using more fine-grained tax-onomies, where context is sometimes considered(Bhagat and Hovy, 2013; Vila et al., 2014; Ko-vatchev et al., 2018). However, only a few datasetsinclude such paraphrases (Kovatchev et al., 2018;Kanerva et al., 2023) and to the best of our knowl-edge none that focus on context-dependent para-phrases or dialog data.We define a context-dependent paraphrase astwo text excerpts that are at least approximatelyequivalent in meaning in a given situation but notnecessarily in all non-absurd situations.6 For ex-ample, consider the first exchange in . Inthis situation, I uttered by the first speaker andYou uttered by the second speaker are clearlysignifying the same person. However, if utteredby the same speaker I and you probably donot signify the same person. The text pair in Ta-ble 2 is thus equivalent in at least one but not in allnon-absurd situations. The text excerpts formingcontext-dependent paraphrases do not have to becomplete utterances. In many cases they are por-tions of utterances, see highlights in . Notethat in dialog, the second speaker should rephrasepart of the first speakers point in the given situation(context condition) and not just talk about some-thing semantically related (equivalence condition).Context-dependent paraphrases range from clear(first example in ) to approximate contex-tual equivalence (last example in ). Whenthe guest says My wife is going through the same",
  "6definition combines elements from Kanerva et al. (2021)and Bhagat and Hovy (2013)": "thing, it seems reasonable to assume that the hostis using contextual knowledge to infer that thesame thing and looking for a job are equivalentfor the given exchange. Even though in this lastexample the meaning of the two utterances couldalso be subject to different interpretations, we stillconsider such cases to be context-dependent para-phrases for two reasons: (1) similar to findings incontext-independent paraphrase detection, limitingourselves to very clear cases would mostly resultin uninteresting, practically identical strings and(2) we ultimately want to identify paraphrases inhuman dialog, which is full of implicit contextualmeaning (Grice, 1957, 1975; Davis, 2002).We specifically exclude common cases of dis-agreements between annotators7 that we considernot to be context-dependent paraphrases in dialog,see . First, we exclude text spans that mightbe considered approximately equivalent when theyare looked at in isolation but do not represent aparaphrase of the guests point in the given situa-tion (e.g., the military and the army in ).Second, we exclude text pairs that diverge too muchfrom the original meaning when the second speakeradds conclusions, inferences or new facts. In aninterview setting, journalists make use of differentquestion types and communication strategies relat-ing to their agenda (Clayman and Heritage, 2002)that can sometimes seem like paraphrases. Forexample in , the hosts question So, you...? could be read as a paraphrase with the goal ofchecking understanding with the guest. However,it is more likely to be a declarative conclusion thatgoes beyond what the guest said.",
  "Dataset": "Generally, people do not paraphrase each other inevery conversation. We focus on the news interviewsetting, because paraphrasing, or more generally ac-tive listening, is a common practice for journalists(Clayman and Heritage, 2002; Hight and Smyth,2002; Sedorkin et al., 2023). We therefore also onlyconsider whether the journalist (the interview host)paraphrases the interview guest and not the otherway around. We use Zhu et al. (2021)s MediaSumcorpus which consists of over 450K news interviewtranscripts and their summaries from 19992019NPR and 20002020 CNN interviews.8",
  "Preprocessing": "We only include two-person interviews, i.e., a con-versation between an interview host and a guest.We remove interviews with fewer than four turns,utterances that only consist of two words or of morethan 200 words, and the first and last turns of inter-views (often welcoming addresses and goodbyes).Overall, this leaves 34,419 interviews with 148,522(guest, host)-pairs. See App. B.1 for details.",
  "Data Samples for Annotation": "Even though paraphrases are relatively likely inthe news interview setting, most randomly sam-pled text pairs still do not include paraphrases. Todistribute annotation resources to text pairs thatare likely to be paraphrase, previous work usuallyselects pairs based on heuristics like textual sim-ilarity features, e.g., word overlap, edit distance,or semantic similarity (Dolan and Brockett, 2005;Su and Yan, 2017; Dong et al., 2021). However,these approaches are systematically biased towardsselecting more obvious, often lexically similar textpairs, possibly excluding many context-dependentparaphrases. For example, the guest and host utter-ance in have varying lengths, only overlapin three words and have a semantic similarity scoreof only 0.139. Similar to Kanerva et al. (2023), weinstead use a manual selection of promising textpairs for annotation: We (1) randomly sample aset of text pairs and (2) manually classify at eachof them to (3) select three sets of text pairs thatvary in their paraphrase distribution for the moreresource-intensive crowd-sourced annotations: theRANDOM, BALANCED and PARA set.Lead Author Annotation. We shuffle and uni-formly sample 1,304 interviews. For each inter-view, we sample a maximum of 5 consecutive(guest, host)-pairs. To select promising paraphrasecandidates, the lead author then manually classi-",
  ": Split of Dataset. For each set, we show thenumber of text pairs and the total number of annotations": "fies all 4,450 text pairs as paraphrases vs. non-paraphrases (see App. B.2 for details).10 In total,about 14.9% of the sampled text pairs are classifiedas paraphrases by the lead author. On a randomset of 100 (guest, host)-pairs (RANDOM), we latercompare the lead authors classifications with thecrowd-sourced paraphrase classifications (see App.B.2). 89% of the lead authors classifications arethe same as the crowd majority. Note that the leadauthors classifications do not affect the qualityof the annotations released with the dataset butonly the text pairs that are selected for annotation.However, using lead author annotations instead oflexical level heuristics should increase paraphrasediversity in the released dataset beyond high lexicalsimilarity pairs.Paraphrase Candidate Selection. We samplethree datasets for annotation that differ in their esti-mated paraphrase distributions (based on the leadauthor annotations): BALANCED is a set 100text pairs sampled for equal representation of para-phrases and non-paraphrases. We annotate thisdataset first with a high number of annotators per(guest, host)-pair, to decide on a crowd-worker al-location strategy that performs well for paraphrasesas well as non-paraphrases. RANDOM is a uni-form random sample of 100 text pairs. One mainuse of the dataset is to evaluate the quality of crowd-worker annotations on a random sample. PARAis a set of 400 text pairs with an estimated 84%of paraphrases designed to increase the variety ofparaphrases in our dataset. Details on the samplingof the three datasets can be found in App. B.3.",
  "Annotation Task": "Given a (guest, host) utterance pair, annotators (1)classify whether the host is paraphrasing any partof the guests utterance and, if so, (2) highlight theparaphrase in the guest and host utterance. Thisresults in data points like the one in . Notethat our setup differs from prior work, which usu-ally involves classifying whether an entire text Bis a paraphrase of an entire text A (e.g., Dolan andBrockett, 2005). Instead, given texts A and B, ourtask is to determine whether there exists a selectionof words from text B and text A, where the selec-tion of text B is a paraphrase of the selection of textA. Our annotators are not only performing binaryclassification, but they also highlight the position ofthe paraphrase. To the best of our knowledge, weare the first to approach paraphrase detection in thisway. Moreover, in contrast to previous work, theconsidered text pairs are usually longer than justone sentence and are contextualized dialog turns.",
  "Plausible Label Variation": "The task of annotating context-independent para-phrases is already difficult. Disagreements betweenhuman annotators are common (Dolan and Brock-ett, 2005; Krishna et al., 2020; Kanerva et al., 2023) even with extensive manuals for annotators (Kan-erva et al., 2023). In related semantic tasks like tex-tual entailment,11 disagreements have been linkedto plausible label variations inherent to the task",
  ": Low Quality Annotations. We show humanhighlights that can be considered wrong or noisy. Whenabsent, we underline the correct highlights": "(Pavlick and Kwiatkowski, 2019; Nie et al., 2020;Jiang and de Marneffe, 2022).Our task setup adds further challenges: First,instead of classifying full sentence pairs, annota-tors have to read relatively long texts and decidewhether any portion of the text pair is a paraphrase.Second, while in previous work annotators usuallyhad to decide if two texts are generally approxi-mately equivalent, they now need to identify para-phrases in a highly contextual setting with oftenincomplete information.As a result, similar to the task of textual en-tailment, we expect classifying context-dependentparaphrases in dialog to not always have a clearground truth. We display examples of plausiblelabel variation in . To handle label variation,common strategies are performing quality checkswith annotators (Jiang and de Marneffe, 2022) andrecruiting a larger number of annotators for a sin-gle item (Nie et al., 2020; Sap et al., 2022). We doboth, see our approach in 5.3 and 5.4.",
  "Annotator Training": "When annotating paraphrases, the instructions forannotators are often short, do not explain chal-lenges and rely on annotator intuitions (Dolan andBrockett, 2005; Lan et al., 2017).12 In contrast,Kanerva et al. (2023) recently used an elaborate17-page manual. However, they relied on only 6 ex-pert annotators that might not be able to representthe full complexity of the task (5.2). We aim fora trade-off between short intuition-based and longcomplex instructions that facilitates recruitmentof a larger number of annotators: an accessibleexample-centric, hands-on 15-minute training ofannotators that teaches our operationalization ofcontext-dependent paraphrases (3). We provide",
  "DeBERTa v3 large AGGREGATED-0.730.670.81-0.520.66DeBERTa v3 large ALL-0.660.820.56-0.450.64": ": Modeling Results. We boldface the best and underline the second best performance. We display theextraction error of predictions from generative models and, for classification, the F1, precision and recall score aswell as, for highlights, the Jaccard Index for the guest and host utterances. Higher values are better () except forextraction errors (). GPT-4 is the best classification model, while, overall, DeBERTa is the best highlight model as itdoes not lead to any extraction errors. (1) a short paraphrase definition, (2) examples ofcontext-dependent paraphrases showing clear andapproximate equivalence (c.f. ), (3) exam-ples of common difficulties with paraphrase clas-sification in dialog (c.f. and 3), and use(4) a hands-on approach where annotators haveto already classify and highlight paraphrases afterreceiving instructions. Only once they make theright choice on what is () and is not a para-phrase () and highlight the correct spansthey are shown the next set of instructions. Onlyannotators that undergo the full training and passtwo comprehension and two attention checks arepart of our released dataset. Overall, 49% of theannotators who finished the training passed it. SeeApp. C for the instructions and further details.",
  "Annotator Allocation": "To the best of our knowledge, text pairs in para-phrase datasets receive a fixed number of 1, upto a maximum of 5 annotations (Kanerva et al.,2023; Zhang et al., 2019; Lan et al., 2017; Dolanand Brockett, 2005). However, this might not beenough to represent the inherent plausible variationto the task (5.2). We have each pair in BAL-ANCED annotated by 2021 trained annotators tosimulate different annotator allocation strategies(App. C.4). Then, for RANDOM and PARA, weuse a dynamic allocation strategy: Each pair re-ceives at least 3 annotations. We dynamically col-lect more annotations, up to 15, on pairs with highdisagreement (i.e., entropy > 0.8). Overall, thisresults in an average of 9 annotations per text pairacross our released dataset.",
  "Results": "We discuss annotations results (tables 1, 4, 6) onour datasets BALANCED, RANDOM and PARA.Classification agreement as an indicator ofvariation. Agreement for classification is relativelylow (). We inspect a sample of 100 anno-tations on the RANDOM set and manually assessannotation quality. 90% of the annotations can besaid to be at least plausible (see for lowquality and for plausible variation exam-ples), which is in line with the fact that we only usehigh quality annotators (5.3). Further, we man-ually analyze the 42 annotations of ten randomlysampled annotators: Nine annotators consistentlyprovide high quality annotations, while the otherannotator chooses not a paraphrase a few timestoo often (see Appendix C.7 for details). As a re-sult, we assume that most disagreements are dueto the inherent plausible label variation of the task(5.2).Higher agreement on paraphrase position.Krippendorffs unitizing on the highlights ishigher than in other areas13 (see ). We alsocalculate the Intersection-over-union between thehighlighted words (i.e., Jaccard Index), a commonand interpretable evaluation measure for annotatorhighlights (Herrewijnen et al., 2024; Mendez Guz-man et al., 2022; Mathew et al., 2021; Malik et al.,2021). It seems that while annotations vary onwhether there is a paraphrase or not, they agree fre-quently on the position of the possible paraphrase.On average, at least 50% of the highlighted words",
  "G: A lot of them were the Bay Area influxthat came up and bought homes to flip. Youknow what flipping is, right?H: Mm-hmm. Buying a house, improvingit, selling it out of profit": ": Model Errors. We show examples of predic-tion errors made by DeBERTa (D) and GPT-4 (G). Wedisplay model predictions (D/G) for paraphrases ()and non-paraphrases () and compare it to the crowd-majority (T). If one model predicted a paraphrase thecorresponding text spans are underlined. For compari-son, we also display the crowd majority highlights. are the same between annotations.14 Agreement ishigher on the host utterance, because on averagethe host utterance is shorter than the guest utterance(33 < 85 words).Label variation is highest for paraphrases.Between the datasets, classification agreement islowest for PARA. This is what we expected sinceit has the largest portion of hard non-repetitionparaphrases (see App. B.3). Krippendorffs islower for the RANDOM than the BALANCEDset, even though we expected the RANDOM setto include easier decisions for annotators (RAN-DOM includes more unrelated non-paraphrases,see App. B.3). As the other agreement heuristic isrelatively high on RANDOM, the lower valuescould be a result of Krippendorffs measure beingsensitive to imbalanced label distributions (Riezlerand Hagmann, 2022), see also displayingthe imbalanced distribution for RANDOM.",
  "Modeling": "In , we do a random 70, 15, 15 split of our5,581 annotations, along the 600 unique pairs.Token Classifier. Similar to Wang et al. (2022a),we fine-tune a large DeBERTa model15 (He et al.,2020) on token classification to highlight theparaphrase positions (for hyperparameters, seeApp. D.2). We train two models: using all 3,896training annotations (ALL in ) and usingthe majority aggregated training annotations over",
  "G:...thenhegoesonandreferences and": "makes mention of Rudy Giuliani three times in thisconversationH: And Rudy Giuliani was a private lawyer not a gov-ernment official, so why is he coming up so much inthis conversation between two world leaders? : Highlighting Differences. We show exam-ples of highlights made by DeBERTa, GPT-4 and humanhighlights. Lower intensity means less human anno-tators selected the word. While GPT-4 struggles withproviding highlights at all (c.f. extraction error in Ta-ble 8), DeBERTa highlights tend to be too sparse (justRudy Giuliani, coming and conversation in thehost utterance). Here, we highlight words, when thesoftmax probability is > 0.4417 instead of 0.5. Onthe complete test set, this also increases the mean Jac-card Index (by 0.06/0.01 for guest/host compared to). the 420 unique (guest, host) training pairs (AG-GREGATED in ). We consider a modelto have predicted a paraphrase for a pair if at leastone token is highlighted with softmax probability 0.5 in both texts. For each model, we averageperformances over three seeds.In-Context Learning. We further prompt thefollowing generative models (see URLs in App.D.1) to both classify and highlight the positionof paraphrases: Llama 2 7B and 70B (Touvronet al., 2023), Vicuna7B (Zheng et al., 2023),Mistral 7B Instruct v0.2 (Jiang et al., 2023),Openchat 3.5 (Wang et al., 2023), Gemma 7B(Team et al., 2024), Mixtral 8x7B Instructv0.1 (Jiang et al., 2024) and GPT-416 (Achiamet al., 2023). We design the prompt to be as close aspossible to the annotator training using a few-shotsetup (Brown et al., 2020; Zhao et al., 2021) withall 8 examples shown during annotator training.We also provide explanations in the prompt (Weiet al., 2022b; Ye and Durrett, 2022) and use self-consistency by prompting the models 10 (GPT-4and Llama 70B: 3) times (Wang et al., 2022b). Forthe prompt and further hyperparameter settings seeApp. D.1.Results. For evaluation, we consider a pair tocontain a paraphrase if it has been classified bya majority of crowd-workers and a word to bepart of the paraphrase if it has been highlighted",
  "API calls where performed using the gpt-4 model idin March 2024.17We tried a few different thresholds > 0.40 with 0.44getting the biggest gain in the Jaccard Index on the test set": "by a majority of crowd-workers. We leave soft-evaluation approaches to future work (Uma et al.,2021), among others because of challenges in ex-tracting label distributions for in-context learningin a straight-forward way (Hu and Levy, 2023;Lee et al., 2023). See for test set perfor-mances. Performances for the token classifier arethe mean over three seeds. Performances for thegenerative models is the majority vote for the 310self-consistency calls. We display the F1 score forclassification and, as before (5.5), Intersection-Over-Union of the highlighted words for guest andhost utterance highlights (Jaccard Indices), see, forexample, DeYoung et al. (2020). For in-contextlearning, we also display how often we could notextract the highlights or classifications from modelresponses. Note that the test set contains 93 ele-ments, so differences between models might appearbigger than they are.Overall, GPT-4 and Mixtral 8x7B achieve thebest results in paraphrase classification. In high-lighting, our DeBERTa token classifiers and GPT-4achieve the best overlap with human annotations.However, due to problems with extracting high-lights from model responses (e.g., hallucinations,see App. D.3), our fine-tuned DeBERTa token clas-sifiers are probably the best choice to extractthe position of paraphrases. While the DeBERTaAGGREGATED model achieves higher F1 scores, theDeBERTa ALL model has the highest precision outof all models. We provide our best-performingDeBERTa AGGREGATED model (model with seed 202and F1 score of 0.76) on the Hugging Face Hub18 and use it in the following error analysis.Error Analysis.We consider the best-performing classification and highlighting mod-els for error analysis, i.e., GPT-4 and DeBERTaAGGREGATED. We manually analyze a sample ofmisclassifications, for examples see . Over-all, the classification quality is better for GPT-4.The DeBERTa classifier finds more paraphrases(note that DeBERTa AGGREGATED for seed 202 hasa recall of 0.86) but also predicts more false posi-tives than GPT-4. For both models, the items withincorrect predictions also show higher human dis-agreement. The average entropy for human classi-fications is lower for the correct (0.45 for DeBERTa,0.45 for GPT-4) than for the incorrect model predic-tions (0.59 for DeBERTa, 0.67 for GPT-4). DeBERTa highlights shorter spans of text (on average 6.6/6.2,compared to 16.7/10.9 for GPT-4 for guest/hostrespectively), while GPT-4 usually highlights com-plete (sub-)sentences. GPT-4 highlights are largelyof good quality, however they often can not be ex-tracted (see App. D.3). The DeBERTa highlightscan seem chopped up and missing key informa-tion (e.g., the original host highlights in are just Rudy Giuliani, coming and conversa-tion). We recommend performing a classificationof an utterance pairs as a paraphrase when thereexist softmax probabilities 0.5 for both guestand host utterance, but then selecting the highlightsalso based on softmax probabilities lower than 0.5.Alternatively, the best DeBERTa ALL model19 pro-vides fewer but seemingly more consistent high-lights (see Appendix D.3). One possible reasonfor this could be that DeBERTa ALL was trained onindividual highlights provided by single annotators,rather than on aggregated highlights.",
  "Conclusion": "A majority of work on paraphrases in NLP haslooked at the semantic equivalence of sentencepairs in context-independent settings. However,the human dialog setting is highly contextual andtypical methods fall short. We provide an opera-tionalization of context-dependent paraphrases andan up-scalable hands-on training for annotators.We demonstrate the annotation approach by pro-viding 5,581 annotations on a set of 600 turn pairsfrom news interviews. Next to paraphrase classifi-cations, we also provide annotations for paraphrasepositions in utterances. In-context learning and to-ken classification both show promising results onour dataset. With this work, we contribute to theautomatic detection of paraphrases in dialog. Wehope that this will benefit both NLP researchers inthe creation of LLMs and social science researchersin analyzing paraphrasing in human-to-human orhuman-to-computer dialogues on a larger scale.",
  "Limitations": "Even though the number of our unique text pairs isrelatively small, we release a high number of highquality annotations per text pair (5,581 annotationson 600 text pairs). Releasing more annotations onfewer items (here: text pairs), has increasinglybeen more common in NLP (Nie et al., 2020; Sap et al., 2022). Further, big datasets become lessnecessary with better generative models: Usingonly eight paraphrases pairs in our prompt alreadyled to promising results. We further use the full3,896 annotations from the training set to train atoken classifier showing competitive results withthe open generative models. However, the tokenclassifier and other potential fine-tuning approacheswould probably profit from a bigger dataset.Even though our dataset of news interviewsshowed frequent, different and diverse occurrencesof paraphrasing, it might not be representative ofparaphrasing behavior in conversations across dif-ferent contexts and social groups. In the future,we aim to expand our dataset with further out-of-domain items.Our data creation process was not aimed at scal-ability. While our developed annotator trainingprocedure can easily be scaled to a larger groupof crowd-workers, we manually selected text pairsfor annotation. Future work could scale this byskipping manual selection and accepting a moreimbalanced dataset or using our trained classifiersas a heuristic to identify likely paraphrases.Even though we carefully prepared the annota-tor training and took several steps to ensure high-quality annotations, there remain several choicesthat were out of our scope to experiment with, butmight have improved quality even more. For ex-ample, experimenting with different visualizationsof paraphrase highlighting, text fonts, giving an-notators an option to add confidence scores forclassifications and so on.We only use one prompt that is as close as pos-sible to the instructions the human annotators re-ceive. We use the same prompt with the exact sameformatting for all different generative LLMs. How-ever, experimenting with different prompts mightimprove performance (Weng, 2023) and some mod-els might benefit from certain formatting or phras-ing. We leave in-depth testing of prompts to futurework. Further, it might be possible to improve theperformance of our DeBERTa model, through pro-viding contextual information (like speaker namesand interview summary). Currently, these are onlyprovided to the generative models.In this work we collect a high number of humanannotations per item and highlight the plausible la-bel variation in our dataset. However, we use hardinstead of soft-evaluation approaches (Uma et al.,2021) for the computational models. We do this be-cause, among others, extracting label distributions for in-context learning is challenging (Hu and Levy,2023; Lee et al., 2023). We leave the developmentof a soft evaluation approach to future work butwant to highlight the potential of our dataset here:The high number of annotations per item enablesthe modeling of classifications and text highlightsas distributions, similar to Zhang and de Marneffe(2021). Further, our dataset provides anonymizedunique ids for all annotators and enables modelingof different perspectives, e.g., with similar methodsto Sachdeva et al. (2022) and Deng et al. (2023).We do not differentiate between different com-municative functions, intentions or strategies thataffect the presence of paraphrases in a dialog. Thisis relevant as paraphrases might, for example, bea more conscious choice by interviewers (Clay-man and Heritage, 2002) or a more unconsciousoccurrence similar to the linguistic alignment ofthe references for discussed objects (Xu and Reit-ter, 2015; Garrod and Anderson, 1987). With thiswork, we hope to provide an outline of the generalclass of context-dependent paraphrases in dialogthat lays the groundwork for further, fine-graineddistinctions.",
  "Ethical Considerations": "We hope that the ethical concerns of reusing a pub-lic dataset (Zhu et al., 2021) are minimal. Espe-cially, since the CNN and NPR interviews are be-tween public figures and were broadcast publicly,with consent, on national radio and TV.Our dataset might not be representative of En-glish paraphrasing behavior in dialogs across dif-ferent social groups and contexts as it is taken fromU.S. news interviews with public figures from twobroadcasters. We caution against using our modelswithout validation on out-of-domain data.We performed several studies with U.S.-basedcrowd-workers as part of this work. We payed par-ticipants a median of 11.41$/h which is abovefederal minimum wage. Crowd-workers consentedto the release of their annotations. We do not re-lease identifying ids of crowd-workers.We confirm to have read and that we abide by theACL Code of Ethics. Beside the mentioned ethicalconsiderations, we do not foresee immediate risksof our work.",
  "Acknowledgements": "We thank the anonymous ARR reviewers for theirconstructive comments. Further, we thank the NLPGroup at Utrecht University and, specifically, ElizeHerrewijnen, Massimo Poesio, Kees van Deemter,Yupei Du, Qixiang Fang, Melody Sepahpour-Fard,Shane Kaszefski Yaschuk, Pablo Mosteiro, andAlbert Gatt, for, among others, feedback on writ-ing and presentation, discussions on annotator dis-agreement and testing multiple iterations of ourannotation scheme. We thank Charlotte Vaaen,Martin Wegmann and Hella Winkler for feedbackon our annotation scheme. We thank Barbara Bziukfor feedback on presentation. This research wassupported by the Digital Society - The InformedCitizen research programme, which is (partly) fi-nanced by the Dutch Research Council (NWO),project 410.19.007. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Alt-man, Shyamal Anadkat, et al. 2023. GPT-4 tech-nical report.Computing Research Repository,arXiv:2303.08774.",
  "Dwight Bolinger. 1974. Meaning and form. Trans-actions of the New York Academy of Sciences, 36(2Series II):218233": "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020.Language models are few-shot learners.In Ad-vances in Neural Information Processing Systems,volume 33, pages 18771901. Curran Associates,Inc.",
  "Wayne A Davis. 2002.Meaning, expression andthought. Cambridge University Press": "Naihao Deng, Xinliang Zhang, Siyang Liu, Winston Wu,Lu Wang, and Rada Mihalcea. 2023. You are whatyou annotate: Towards better models through anno-tator representations. In Findings of the Associationfor Computational Linguistics: EMNLP 2023, pages1247512498, Singapore. Association for Computa-tional Linguistics. Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani,Eric Lehman, Caiming Xiong, Richard Socher, andByron C. Wallace. 2020. ERASER: A benchmark toevaluate rationalized NLP models. In Proceedingsof the 58th Annual Meeting of the Association forComputational Linguistics, pages 44434458, Online.Association for Computational Linguistics. Justin Dieter, Tian Wang, Arun Tejasvi Chaganty, Ga-bor Angeli, and Angel X. Chang. 2019. Mimic andrephrase: Reflective listening in open-ended dialogue.In Proceedings of the 23rd Conference on Computa-tional Natural Language Learning (CoNLL), pages393403, Hong Kong, China. Association for Com-putational Linguistics.",
  "Guillaume Lample, Lucile Saulnier, et al. 2023.Mistral 7B.Computing Research Repository,arXiv:2310.06825": "Albert Q Jiang, Alexandre Sablayrolles, AntoineRoux, Arthur Mensch, Blanche Savary, Chris Bam-ford, Devendra Singh Chaplot, Diego de las Casas,Emma Bou Hanna, Florian Bressand, et al. 2024.Mixtral of experts. Computing Research Repository,arXiv:2401.04088. Nan-Jiang Jiang and Marie-Catherine de Marneffe.2022. Investigating reasons for disagreement in natu-ral language inference. Transactions of the Associa-tion for Computational Linguistics, 10:13571374.",
  "Dan Jurafsky and James H Martin. 2019. Speech andlanguage processing (3rd ed. draft)": "Jenna Kanerva, Filip Ginter, Li-Hsin Chang, Iiro Ras-tas, Valtteri Skantsi, Jemina Kilpelinen, Hanna-MariKupari, Aurora Piirto, Jenna Saarni, Maija Sevn,et al. 2021. Annotation guidelines for the Turkuparaphrase corpus. Computing Research Repository,arXiv:2108.07499. Jenna Kanerva, Filip Ginter, Li-Hsin Chang, Iiro Rastas,Valtteri Skantsi, Jemina Kilpelinen, Hanna-Mari Ku-pari, Aurora Piirto, Jenna Saarni, Maija Sevn, andet al. 2023. Towards diverse and contextually an-chored paraphrase modeling: A dataset and baselinesfor finnish. Natural Language Engineering, page135. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-guage models are zero-shot reasoners. Advances inneural information processing systems, 35:2219922213. Venelin Kovatchev, M. Antnia Mart, and MariaSalam. 2018. ETPC - a paraphrase identificationcorpus annotated with extended paraphrase typologyand negation. In Proceedings of the Eleventh In-ternational Conference on Language Resources andEvaluation (LREC 2018), Miyazaki, Japan. EuropeanLanguage Resources Association (ELRA).",
  "pages 12241234, Copenhagen, Denmark. Associa-tion for Computational Linguistics": "Noah Lee, Na Min An, and James Thorne. 2023. Canlarge language models capture dissenting humanvoices?In Proceedings of the 2023 Conferenceon Empirical Methods in Natural Language Process-ing, pages 45694585, Singapore. Association forComputational Linguistics. Vijit Malik, Rishabh Sanjay, Shubham Kumar Nigam,Kripabandhu Ghosh, Shouvik Kumar Guha, ArnabBhattacharya, and Ashutosh Modi. 2021. ILDC forCJPE: Indian legal documents corpus for court judg-ment prediction and explanation. In Proceedingsof the 59th Annual Meeting of the Association forComputational Linguistics and the 11th InternationalJoint Conference on Natural Language Processing(Volume 1: Long Papers), pages 40464062, Online.Association for Computational Linguistics. Binny Mathew, Punyajoy Saha, Seid Muhie Yimam,Chris Biemann, Pawan Goyal, and Animesh Mukher-jee. 2021. HateXplain: A benchmark dataset forexplainable hate speech detection.Proceedingsof the AAAI Conference on Artificial Intelligence,35(17):1486714875. Erick Mendez Guzman, Viktor Schlegel, and RizaBatista-Navarro. 2022.RaFoLa:A rationale-annotated corpus for detecting indicators of forcedlabour. In Proceedings of the Thirteenth LanguageResources and Evaluation Conference, pages 36103625, Marseille, France. European Language Re-sources Association.",
  "Carla Roos. 2022. Everyday Diplomacy: dealing withcontroversy online and face-to-face. Ph.D. thesis,University of Groningen": "Pratik Sachdeva, Renata Barreto, Geoff Bacon, Alexan-der Sahn, Claudia von Vacano, and Chris Kennedy.2022. The measuring hate speech corpus: Leverag-ing rasch measurement theory for data perspectivism.In Proceedings of the 1st Workshop on PerspectivistApproaches to NLP @LREC2022, pages 8394, Mar-seille, France. European Language Resources Asso-ciation. Maarten Sap, Swabha Swayamdipta, Laura Vianna,Xuhui Zhou, Yejin Choi, and Noah A. Smith. 2022.Annotators with attitudes: How annotator beliefsand identities bias toxic language detection. In Pro-ceedings of the 2022 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages58845906, Seattle, United States. Association forComputational Linguistics.",
  "Gail Sedorkin, Amy Forbes, Ralph Begleiter, TravisParry, and Lisa Svanetti. 2023. Interviewing: A guidefor journalists and writers. Routledge": "Raj Sanjay Shah, Faye Holt, Shirley Anugrah Hayati,Aastha Agarwal, Yi-Chia Wang, Robert E Kraut,and Diyi Yang. 2022. Modeling motivational inter-viewing strategies on an online peer-to-peer counsel-ing platform. Proceedings of the ACM on Human-Computer Interaction, 6(CSCW2):124. Vered Shwartz and Ido Dagan. 2016. Adding context tosemantic data-driven paraphrasing. In Proceedingsof the Fifth Joint Conference on Lexical and Compu-tational Semantics, pages 108113, Berlin, Germany.Association for Computational Linguistics. Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-beth Shriberg, Rebecca Bates, Daniel Jurafsky, PaulTaylor, Rachel Martin, Carol Van Ess-Dykema, andMarie Meteer. 2000. Dialogue act modeling for au-tomatic tagging and recognition of conversationalspeech. Computational Linguistics, 26(3):339374. Yu Su and Xifeng Yan. 2017. Cross-domain semanticparsing via paraphrasing. In Proceedings of the 2017Conference on Empirical Methods in Natural Lan-guage Processing, pages 12351246, Copenhagen,Denmark. Association for Computational Linguis-tics. Jamar Sullivan Jr., Will Brackenbury, Andrew McNutt,Kevin Bryson, Kwam Byll, Yuxin Chen, MichaelLittman, Chenhao Tan, and Blase Ur. 2022. Explain-ing why: How instructions and user interfaces im-pact annotator rationales when labeling text data. InProceedings of the 2022 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 521531, Seattle, United States. Associationfor Computational Linguistics. Gemma Team, Thomas Mesnard, Cassidy Hardin,Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,Laurent Sifre, Morgane Rivire, Mihir Sanjay Kale,Juliette Love, et al. 2024. Gemma: Open modelsbased on gemini research and technology. Comput-ing Research Repository, arXiv:2403.08295. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023. Llama 2: Open foundationand fine-tuned chat models. Computing ResearchRepository, arXiv:2307.09288.",
  "Alexandra N Uma, Tommaso Fornaciari, Dirk Hovy, Sil-viu Paun, Barbara Plank, and Massimo Poesio. 2021.Learning from disagreement: A survey. Journal ofArtificial Intelligence Research, 72:13851470": "Gregory M Vecchi, Vincent B Van Hasselt, andStephen J Romano. 2005. Crisis (hostage) negoti-ation: current strategies and issues in high-risk con-flict resolution. Aggression and Violent Behavior,10(5):533551. Gregory M Vecchi, Gilbert KH Wong, Paul WC Wong,and Mary Ann Markey. 2019. Negotiating in theskies of hong kong: The efficacy of the behavioralinfluence stairway model (BISM) in suicidal crisissituations. Aggression and violent behavior, 48:230239.",
  "Jan Philip Wahle, Bela Gipp, and Terry Ruas. 2023": "Paraphrase types for generation and detection. In Pro-ceedings of the 2023 Conference on Empirical Meth-ods in Natural Language Processing, pages 1214812164, Singapore. Association for ComputationalLinguistics. Alex Wang, Amanpreet Singh, Julian Michael, FelixHill, Omer Levy, and Samuel Bowman. 2018. GLUE:A multi-task benchmark and analysis platform for nat-ural language understanding. In Proceedings of the2018 EMNLP Workshop BlackboxNLP: Analyzingand Interpreting Neural Networks for NLP, pages353355, Brussels, Belgium. Association for Com-putational Linguistics. Guan Wang, Sijie Cheng, Xianyuan Zhan, XiangangLi, Sen Song, and Yang Liu. 2023. OpenChat: Ad-vancing open-source language models with mixed-quality data.Computing Research Repository,arXiv:2309.11235. Shuohang Wang, Ruochen Xu, Yang Liu, ChenguangZhu, and Michael Zeng. 2022a. ParaTag: A datasetof paraphrase tagging for fine-grained labels, NLGevaluation, and data augmentation. In Proceedingsof the 2022 Conference on Empirical Methods in Nat-ural Language Processing, pages 71117122, AbuDhabi, United Arab Emirates. Association for Com-putational Linguistics. Xuezhi Wang, Jason Wei, Dale Schuurmans, QuocLe, Ed Chi, Sharan Narang, Aakanksha Chowdhery,and Denny Zhou. 2022b. Self-consistency improveschain of thought reasoning in language models. Com-puting Research Repository, arXiv:2203.11171. Yizhong Wang, Swaroop Mishra, Pegah Alipoormo-labashi, Yeganeh Kordi, Amirreza Mirzaei, AtharvaNaik, Arjun Ashok, Arut Selvan Dhanasekaran,Anjana Arunkumar, David Stap, Eshaan Pathak,Giannis Karamanolakis, Haizhi Lai, Ishan Puro-hit, Ishani Mondal, Jacob Anderson, Kirby Kuznia,Krima Doshi, Kuntal Kumar Pal, Maitreya Patel,Mehrad Moradshahi, Mihir Parmar, Mirali Purohit,Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma,Ravsehaj Singh Puri, Rushang Karia, Savan Doshi,Shailaja Keyur Sampat, Siddhartha Mishra, SujanReddy A, Sumanta Patro, Tanay Dixit, and XudongShen. 2022c. Super-NaturalInstructions: General-ization via declarative instructions on 1600+ NLPtasks. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Processing,pages 50855109, Abu Dhabi, United Arab Emirates.Association for Computational Linguistics. Harry Weger Jr, Gina R Castle, and Melissa C Emmett.2010. Active listening in peer interviews: The in-fluence of message paraphrasing on perceptions oflistening skill. International Journal of Listening,24(1):3449. Jason Wei, Maarten Bosma, Vincent Y Zhao, KelvinGuu, Adams Wei Yu, Brian Lester, Nan Du, An-drew M Dai, and Quoc V Le. 2022a. Finetuned lan-guage models are zero-shot learners. InternationalConference on Learning Representations.",
  "Lilian Weng. 2023.Prompt engineering.lilian-weng.github.io": "Ka Wong and Praveen Paritosh. 2022. k-Rater Relia-bility: The correct unit of reliability for aggregatedhuman annotations. In Proceedings of the 60th An-nual Meeting of the Association for ComputationalLinguistics (Volume 2: Short Papers), pages 378384, Dublin, Ireland. Association for ComputationalLinguistics. Yang Xu and David Reitter. 2015. An evaluation andcomparison of linguistic alignment measures.InProceedings of the 6th Workshop on Cognitive Mod-eling and Computational Linguistics, pages 5867,Denver, Colorado. Association for ComputationalLinguistics. Xi Ye and Greg Durrett. 2022. The unreliability ofexplanations in few-shot prompting for textual rea-soning. In Advances in Neural Information Process-ing Systems, volume 35, pages 3037830392. CurranAssociates, Inc. XinliangFrederickZhangandMarie-Catherinede Marneffe. 2021. Identifying inherent disagree-ment in natural language inference. In Proceedingsof the 2021 Conference of the North AmericanChapter of the Association for ComputationalLinguistics: Human Language Technologies, pages49084915, Online. Association for ComputationalLinguistics.",
  "Yuan Zhang, Jason Baldridge, and Luheng He. 2019": "PAWS: Paraphrase adversaries from word scrambling.In Proceedings of the 2019 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,Volume 1 (Long and Short Papers), pages 12981308,Minneapolis, Minnesota. Association for Computa-tional Linguistics. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, andSameer Singh. 2021. Calibrate before use: Improv-ing few-shot performance of language models. InProceedings of the 38th International Conferenceon Machine Learning, volume 139 of Proceedingsof Machine Learning Research, pages 1269712706.PMLR. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,Joseph E Gonzalez, and Ion Stoica. 2023. JudgingLLM-as-a-judge with MT-bench and Chatbot Arena.In Advances in Neural Information Processing Sys-tems, volume 36, pages 4659546623. Curran Asso-ciates, Inc.",
  "AContext-Dependent Paraphrases inDialog": "Should one include repetitions?Repetitionshave been typically included in paraphrase tax-onomies (Bhagat and Hovy, 2013; Zhou et al.,2022) even though, e.g., Kanerva et al. (2023)asked annotators to exclude such pairs as they con-sidered them uninteresting paraphrases. However,distinguishing repetitions from paraphrases turnsout to be especially hard in dialog: speakers tendto leave words out when they repeat and adapt thepronouns to match their perspective (e.g., I -> you).We therefore include repetitions in our definitionof context-dependent paraphrases. In fact, thosemainly make up the Clear Contextual EquivalenceParaphrases (see ).",
  "BDataset": "Topic of the Dataset.The topics of the CNN andNPR news interviews (Zhu et al., 2021) are mostlycentered around U.S. politics (e.g., presidential orlocal elections, 9/11, foreign policy in the middleeast), sports (e.g., baseball, football), domestic nat-ural disasters or crimes and popular culture (e.g.,interviews with book authors). Utterance Pair IDs.We use unique IDs for ut-terance pairs. For example, for NPR-4-2, NPR-4is the ID used for interviews20 as done in Zhu et al.(2021), 2 is the position of the start of the guestutterance in the utterance list as separated into turnsby Zhu et al. (2021), in this case Thank you..",
  "In this case referring to": "This can be challenging: In the speaker list, au-thors sometimes have non-unique identifiers (e.g.,STEVE PROFFITT, PROFFITT or S. PROF-FITT refer to the same speaker). If one authoridentifier string is contained in the other we assumethem to be the same speaker.21 We generally as-sume the first speaker to be the host. We remove538 NPR and 1,917 CNN interviews because theidentifier of the second speaker includes the key-words host or anchor thus contradicting ourassumption. This leaves 14,000 NPR and 50,301CNN 2-person interviews.2. Removing first and last turns of an inter-view. The first turns in our 2-person interviewsare usually (reactions to) welcoming addresses andacknowledgments by host and guest22, while thelast often contain goodbyes or acknowledgments23.We remove the first two and the last two (guest,host)-pairs. This step removes 2,409 NPR and26,419 CNN interviews because they are fewerthan 5-turns long. For the remaining interviews,this removes 34,773 NPR and 71,646 CNN (guest,host)-pairs.3. Removing short and long utterances. Wefurther remove short guest utterances of 12 wordsas they leave not much to paraphrase.24 3,540 NPRand 12,675 CNN pairs are removed like this. Wealso remove pairs where the host utterance consistsof only 12 words.25. 2,940 NPR and 11,389 CNNpairs are removed like this. We also remove pairs 21There might be other cases where different string identi-fiers in the dataset refer to the same speaker although they arenot substrings of the other (e.g., S. PROFFITT and STEVEPROFFITT). For a randomly sampled selection of 44 inter-views that were identified as more than 2 person interviews,12 contained errors in the matching. 2/12 were the result oftypos and 10/12 were the result of additions to the name like(voice-over) or (on camera).22For example, Im Farai Chideya. Welcome. Thankyou.23For example the last 3 turns in the considered NPR-4interview: Well, Dr. Hader. Thanks for the information.,Well, thank you for helping share that information [...], Well,thanks again. Dr. Shannon Hader [...]24We manually looked at a random sample of 0.3% 48such pairs. The 1-2 token guest utterances are mostly (40/48)assertions of reception by the guest (e.g., Yes., Exactly.Exactly., Thats right). Some are signals of protest (4/48)(e.g., Hey, man., Yes, but..., Hold on.). None of themwere reproduced by the host in the next turn.25We manually looked at a random sample of 0.3% 37such pairs. The 12 tokens host utterances are mostly (28/37)assertions of reception by the host (e.g., Yeah., Yes.,Sure., Right., Right. Right., Ah, okay.). Some arerequests for elaboration (5/37) (e.g., How so?, Like?,Four?) or reactions (3/37) (e.g., Wow!, Oh, interesting.).Only one example Four? was reproducing content in theform of a repetition. : Label distribution after first author anno-tations performed in two batches. First author labelclassification was performed in two batches. The firstbatch consists of 750 text pairs, the second of 3,700.",
  "B.2First Author Annotations": "We provide more details on the first author annota-tions for selecting paraphrase candidates (4.2).Deciding on first author annotations. Sincethe share of paraphrases in randomly sampled(guest, host)-pairs was only at around 5-15% ininitial pilots with lab members, similar to previ-ous work, we opted to do a pre-selection of textpairs before proceeding with the more resource-intensive paraphrase annotation (c.f. 5.5 and App.C). However, commonly used automatic heuris-tics were not suitable for the highly contextualdiscourse setting (c.f. 4.2). Instead, we experi-mented with discarding obvious non-paraphrasesthrough crowd-sourced annotations and comparedit to manual annotations by the lead author, ul-timately deciding on using lead author annota-tions. One of the reasons was that discarding ob-vious non-paraphrases was more resource inten-sive and difficult for crowd-workers than expected,making the resources needed for discarding non-paraphrases too close to annotating paraphrasesthemselves which defeats the purpose of doing apre-selection in the first place.Changing lead author annotations from dis-carding obvious non-paraphrases to keeping in-teresting paraphrases. On an initial set of 750random (guest, host)-pairs, we remained with theinitial idea of discarding obvious non-paraphrasepairs. However, due to a resulting high share ofuninteresting or improbable paraphrase pairs, we 26200 is the practical limit for the number of words forthe chosen type of question (i.e., Highlight Question) in theused survey hosting platform (i.e., Qualtrics). It also limitsannotation time per question.",
  "Missing Context125": ": Statistics Labels First Batch. For 750 manu-ally reviewed pairs, we also labeled several other cate-gories. We found 88 paraphrases, 519 non-paraphrases,18 ambiguous cases and 125 where the missing contextimpeded a definite decision. Note that we tried to not as-sign ambiguous if we were leaning to one category overanother. Other categorizations include: perspective-shift (the perspective shifts between guest and host,e.g., you -> I), directional (guest or host utter-ance is entailed from or subsumed in the other), partial(a subsection could be understood as a paraphrase, butthe overall larger section is clearly not a paraphrase),related (two utterances are closely related but no para-phrases), conclusion (host draws a conclusion or addsan interpretation that goes beyond a paraphrase). Somelabels were only added in the last 200 annotations andtherefore include the > indication.",
  "BALANCED0.72RANDOM0.89PARA0.72": ": Lead vs. Crowd Classifications. We displaythe average overlap between the lead authors classifica-tions and the majority vote of the crowd. The overlap isthe highest on the RANDOM set. Probably because wekeep all obvious non-paraphrases for classification andthe annotators face less ambiguous (guest, host)-pairsto classify. opted to classify paraphrases vs. non-paraphrasesinstead of possible paraphrases vs. obvious non-paraphrases. The lead author re-annotated the ini-tial set of 750 paraphrase candidates and annotated4450 additional (guest, host)-pairs for paraphrasevs. non-paraphrase. In the first batch, the leadauthor additionally labeled a variety of differentparaphrase types/difficulties (e.g., high lexical sim-ilarity, missing context, unrelated), see also , in the second batch this was restricted to repe-",
  "Unrelated utterances131.00More Difficult410.76Topically related240.67High Lexical Similarity110.64Partial100.80Conclusion110.55": ": Selection of 100 Paraphrase Candidatesfor detailed Annotation. The sample was selectedbased on assigned categories during paraphrase can-didate annotation. Categories within Paraphrase andNon-Paraphrase can overlap. We display accuracyw.r.t. first author annotations. tition paraphrase, paraphrase and non-paraphrase.The distribution of these three categories is dis-played in .Relation to with Crowd Majority Annotations.We display the overlap between the lead authorsparaphrase classifications and the released classifi-cations of the crowd majority in Table B.2.",
  "B.3Paraphrase Candidate Selection": "Based on the lead author classifications into para-phrase, non-paraphrase and repetition, we buildthree datasets for annotation (main paper 4.2). Wedisplay the first author classification distributionfor the three datasets in .BALANCED. The BALANCED set is a sampleof 100 (guest, host)-pairs that were randomly sam-pled based on the first batch of lead author annota-tions (B.2). We had additional lead author labelsavailable for this set, see for the distribu-tion of these on the BALANCED set. Constraintswere 50 paraphrases and 50 non-paraphrases. Inorder to include more complex cases, we sam-pled more difficult than unrelated non paraphrasepairs and we limited the number of repetition para-phrases (51% of paraphrases are repetitions in thefull batch, but only 33% of paraphrases in BAL-ANCED are repetitions). Due to a sampling error,we ended up with a 46/56 split. Later, we calcu-late the majority vote of the 2021 annotations per(guest, host)-pair on this set, and then evaluate it by : Distribution of Labels by Lead Author. Wedisplay the estimated number of (non-)paraphrases fromthe lead author annotations for the random subsample(RANDOM), the BALANCED sample and the widerparaphrase variety sample (PARA). Note, RANDOMconsists of 100 elements, however only 98 are includedin this statistic here (leading to numbers like 6.1). 2 pairswere not classified by the lead author because they weretoo ambiguous or were missing context information toreach a decision. We exclude such pairs in all othersamples. comparing it against the lead author classification,see acc. column.RANDOM. The random set is a sample of 100(guest, host)-pairs that was uniformly sampledfrom the second batch of lead author annotations(B.2).PARA. After selecting the RANDOM set, thePARA set of 400 (guest, host)-pairs was sampledto reach a specified total 350 paraphrases and 150non-paraphrases together with the RANDOM set.27 The PARA set was selected to make the total num-ber of non-repetition paraphrases together withRANDOM reach 300, while limiting the amountof repetition paraphrases to 50. Conversely, non-paraphrases were sampled to add up to 150. Thisled to 66 non-paraphrases and 334 paraphrases be-ing sampled for the PARA set. 27RANDOM and PARA were undergoing annotation to-gether in a second annotation round, after BALANCED hadalready been annotated. The aim was to reach a higher distribu-tion of paraphrases in our released dataset. The 350/150 splitwas somewhat arbitrary. It could have easily been 400/100 or300/200 as well.",
  "(1) The lead author repeatedly annotated thesame set of (guest, host)-pairs with a time differ-ence of one week. See an example of early self-disagreement in": "(2) With insights from (1) and our definitionof context-dependent paraphrases, we created an-notator instructions. We iteratively improved in-structions while testing them with volunteers, labmembers and Prolific crowd-workers. See exam-ples of disagreements that led to changes in . (3) Based on insights from (2), we introduced anintermediate annotator training that explains para-phrase annotation in a hands-on way: Annotatorshave to correctly annotate a teaching example toget to the next page instead of just reading an in-struction. As soon as the correct selection is made,an explanation is show (e.g., Figures 6 and 10).After some testing rounds, we also require annota-tors to pass 2 attention (see ) as well as 2comprehension checks (see Figures 5 and 11). (4) We test the developed training on a selectionof 20 (guest, host)-pairs out of which 10 were clas-sified as clearly containing a paraphrase, and 10 ascontaining no paraphrase by the lead author, half ofall examples we considered to be more difficult toclassify (e.g., paraphrase with a low lexical overlap,non-paraphrase with a high lexical overlap). Twolab members reached pairwise Cohen of 0.51 afterreceiving training. Two newly recruited Prolificannotators reached average pairwise Cohen of 0.42after going through training. Due to the inherentdifficulty of the task and the good annotation qual-ity when manually inspecting the 20 examples foreach annotator, we carry on with this training setup.",
  "We train participants to recognize paraphrases (see for the instructions they received). Wepresented (guest, host)-pairs with their MediaSumsummaries, the date of the interview and the in-": "terviewer names for context.28 Participants wereonly admitted to the paraphrase annotation if theypassed two attention checks (see ) andtwo comprehension checks (see and 11).Comprehension Checks. Similar to examplesin , they are presented with a clear para-phrase pair (App. ) and a less obviouscontext-dependent paraphrase pair (App. ) that they have to classify as a paraphrase. Addi-tionally, they are only allowed to highlight the textspans that are a part of the paraphrase.Training Stats. Of the initial 347 Prolific an-notators who started the training, 95 aborted thestudy without giving a reason29 and 126 were ex-cluded from further studies because they failed atleast one comprehension (29%) or attention check(24%) during training. Since annotators can per-form annotations after training over a span of sev-eral days, we further exclude single annotation ses-sions, where the annotator fails any of two attentionchecks. 28The additional information of summary, date and speakernames increased reported understanding of context and easeddifficulty of the task in pilot studies among lab members.29Usually quickly, we assume that they did not want totake part in a multi-part study or did not like the task itself.",
  "Self-Disa-gree-ment": "Guest: [..] So there was a consensus organization last year that people from genetics andethics law got together and said, in theory, it should be acceptable to try this in humanbeings. The question will be, how much safety and evidence do we have to have fromanimal models before we say its acceptable.Host: When it comes to this issue, lets face it, while there are the concerns here in theUnited States, its happening in other countries.",
  "LabMem-bers": "Guest: Hey, its going to be a long and a long week, and were going to use every singleminute of it to make sure that Americans know that Al Gore and Joe Lieberman are fightingfor working families, right here in Los Angeles and across America.Host: And are you guys ready to go?",
  "Guest: [...] There are militant groups out there firing against the military. And we just - wereally dont know who is whom.Host: Why did the army decide today to move in and clear out the camp?": "Guest: Police have indicated that they have been getting cooperation from the peopleinvolved, of course, they are looking at all of her personal relationships to see if there wereany problems there. [...]Host: Well what have family members told you? I know youve talked to various membersof her family. I understand she never missed her shifts at the restaurant where she worked.[...]Guest: Yes, it is, all $640,000.Host: Thats a lot of dough.",
  "similar to (C)": "Guest: [...] Would I ever thought that this would be happening, no, it is, its crazy? Justenjoy the moment.Host: [...] , Magic Johnson was saying that when he first started taking meetings withinvestors or with business people, they didnt take him seriously, but he thought maybe theyjust wanted his autograph. [...]",
  "(AT) throw out an-notators that donot select obviouspairs": ": Examples of Disagreements in Paraphrase Annotation Pilots. All of the presented examples werehighlighted by at least one annotator and selected as not showing any paraphrases at all by at least one otherannotator. We show examples from three different conditions: Self-disagreement for the lead author, disagreementsbetween volunteers/lab members and disagreements between Prolific annotators. These disagreements informedlater training instructions: For (C), see ; for (P), see ; for (CD), see ; for (H), see ;for (AT), we chose the separate training setup with attention and comprehension checks, see Figures 5, 11 and 12.Early on, we chose to include repetitions in our paraphrase definition since it turned out to be conceptually difficultto separate the two especially in a context-dependent setting (e.g., is You dont know. a repetition of I do notknow it. or not?), see .",
  "C.4Annotator Allocation Strategy": "To the best of our knowledge, what constitutes agood number of annotators per item has not beeninvestigated for paraphrase classification.Summary. Based on the 2021 annotations peritem for the BALANCED set, we simulate fixedand dynamic strategies to recruit up to 20 annota-tions per item. We evaluate the different strategiesw.r.t. closeness to the annotations of all 2021 anno-tators. When considering resource cost and perfor-mance trade-offs, dynamic recruitment strategiesperformed better than allocating a fixed number ofannotators for each item.Details. We consider three different strategiesfor allocating annotators to an item: (1) using afixed number for all items, (2) for each item, dy-namically allocate annotators until n of them agreeand (3) similar to Engelson and Dagan (1996), foreach item, dynamically allocate annotators untilthe entropy is below a given threshold t or a maxi-mum number of annotators has been allocated. Wesimulate each of these strategies using the annota-tions on BALANCED. We evaluate the strategieson (a) cost, i.e., the average number of annotatorsper item and (b) performance via (i) the overlap be-tween the full 20 annotator majority vote (i.e., weassume this is the best possible result) and the pre-dicted majority vote for the considered strategy and(ii) k-rater-reliability (Wong and Paritosh, 2022) a measure to compare the agreement between ag-gregated votes. Note, for the dynamic setup wechange the original calculation of kRR (Wong andParitosh, 2022) by dynamically recruiting more orless annotators per item and thus aggregating thevotes of a varying instead of a fixed number ofannotators.Results. See for the results. We se-lected a practical resource limit of an average 8annotators per items and the requirement of at least90% accuracy with the majority vote and 0.7 kRR(dotted lines). We decide on strategy (3) dynam-ically recruiting annotators (minimally 3, maxi-",
  "C.5Annotator Payment": "Via Prolifics internal screening system, we re-cruited native speakers located in the US. Paymentfor a survey was only withheld if annotators failedtwo attention checks within the same survey orwhen a comprehension check at the very beginningof the study was failed30 in line with Prolific guide-lines.31 Across all Prolific studies performedfor this work (including pilots), we payed partici-pants a median of 8.98/h 11.41$/h32 whichis above federal minimum wage in the US.33 30Technically, in line with Prolific guidelines, we do notwithhold payment but ask annotators to return their studyin this case. Practically this is the same, as all annotators didreturn such a study when asked.31Prolific Attention and Comprehension Check Policy32on March 20th 202433Federal minimum wage in the US is $7.25/h5.71/h according to on March 20th 2024",
  "(b) kRR": ": Annotator Recruitment Strategies. To decide the number of annotators for a specific item, we testthree different strategies: (1) using a fixed number of annotators across all items (ALL), (2) increasing the numberof annotators until at least n annotators agree for each item (absolute) and (3) increasing the number of annotatorsfrom 3 until the entropy is smaller than a given threshold (entropy) or a maximum of 10, 15 or 20 annotators isreached. We display the accuracy of the methods compared to using all 20 annotations in (15a) and the reliabilitymeasure kRR depending on the average number of annotators used (Wong and Paritosh, 2022) in (15b). We set amaximum average cost of 8 annotators per item and require a minimum accuracy of 90% as well as a minimumkRR of 0.70. When a strategy fulfills these requirements (i.e., falls in the upper left quadrants for (a) and (b)), wedisplay the entropy thresholds for (3) and absolute number of annotators for (2).",
  "(b) Quality Checks Passed": ": On BALANCED, later training sessions take longer and pass fewer quality checks. In 16a, wedisplay the seconds the nth annotator needs to go through the training session. The annotators are ordered accordingto the dates they completed training. Annotations were distributed across 6 different days in June 2023. The greenline represents the median duration time of the first n participants. The red line displays the initially estimatedcompletion time of 900 seconds according to pilot studies. The blue line is a linear regression estimate of theduration and its 95% confidence interval. On average, participants participating on a later date need more timeto finish. In 16b, we display the summed number of the first n participants that passed the quality checks duringtraining. The grey line represents the angle bisector, i.e., if every participant would pass all quality checks. Laterparticipants are less likely to pass the quality checks.",
  "C.6Varying Annotator Behavior over Time": "For the BALANCED set, we performed separatetraining and annotation rounds. See forthe completion times and share of passed qual-ity checks of Prolific annotators in the trainingsession. Participants that were recruited later per-formed worse: they pass less quality checks and need more time. This effect was noticeable butit is not quite clear to us why this happens. Werecruit all participants at once for later studies andnot iteratively as for the BALANCED set, to avoideffects that have to do with study age. The effecton the quality of the released annotations shouldbe minimal as we discard annotators that do not",
  "C.7Intra-Annotator Annotations Quality": "We manually randomly sample ten annotators (withanonymized PROLIFIC ids 60, 6, 86, 84, 47, 31, 68,88, 41, 92) and analyze 42 of their annotatations.Nine annotators consistently provide plausible an-notations, while the other annotator chooses not aparaphrase a few times too often. We also noticedsome other annotator-specific tendencies, for ex-ample, one annotator might tend to highlight fewerwords, more words or prefer exact lexical matches.",
  "Models.We provide the Huggingface URLsto our used models.Vicuna 7B: Mis-tral 7B Instruct v0.2: Open-chat: Mix-tral 8x7B Instruct v0.1:": "Prompt.We use a few-shot prompt that is closeto the original annotator training and instructions,see . We use chain-of-thought like ex-planations, i.e., always starting with Lets thinkstep by step. and ending with Therefore, theanswer is, (Kojima et al., 2022) and a few-shotsetup showing all 8 examples showed to annota-tors during training (Figures 412). For GPT-4, weuse a temperature of 1, self-consistency throughprompting the model 3 times (Wang et al., 2022b)and the default top_p nucleus sampling value of1, a maximum of new tokens to 512. For all thehuggingface models, we use a temperature of 1,self-consistency through prompting the model 10times (only 3 times for Lllama 70B due to resourcelimits) and a top_k sampling of the top 10 tokens, amaximum of new tokens of 400 for all other models.Note, there are many more prompts and choices wecould have tried that are out-of-the scope of thiswork. Further steps could have included separatingthe classification and highlighting task, experiment-ing with further phrasings and so on. We leave thisto future work.",
  "D.2Token Classification": "We use settings very close to Wang et al. (2022a)and test different learning rates and number ofepochs with 3 different seeds each. We use the\"save best model\" option to save the model afterthe epoch which yielded the best result on the devset. For the results, see . We use a learn-ing rate of 3e-3 and 12 epochs for further modeling.",
  "3e-340.65 0.073e-3120.65 0.003e-3160.60 0.10": ": Hyperparameter tuning on the DEV set.We train a token classifier for learning rates 1e-3, 3e-3,5e-3 and epochs 4, 8, 12 and 16 for 3 seeds. We keeplearning rate fixed at 3e-3 when varying the numberof epochs and epoch fixed at 8 when varyig the learn-ing rates. Best options of learning rate and epoch areunderlined. Best F1 score is boldfaced. A Paraphrasei sarewordingorr e p e t i t i o nofc o n t e n tintheguest ' ss t a t e m e n t .I tr e p h r a s e swhattheguests a i d .Given ani n t e r v i e won withthesummary :FreshPrinceS t a rAlfonsoR ibe ir oSues Over Dance Moves ;Rapper 2 MillyAllegesHis Dance Moves were Copied .Guest and Hostsaythef o l l o w i n g :Guest(TERRENCE FERGUSON, RAPPER) :Iguessi twas season 5 when theypremieredi tinthe game . A bunchof DMs,a bunchofT w i t t e rr e q u e s t s ,emails ,e v e r y t h i n gwasli ke ,you ,your gamei sinthedance ,you needtosue ,\" F o r t n i t e \"s t o l ei t .Evenl i k ebiga r t i s t s ,majora r t i s t sl i k eJoeButtonsands t u f f ,theyhavet h e i rown l i k eshow ,d a i l ys t r u g g l e ,theysay ,you ,you mustsue\" F o r t n i t e \" ,and I 'm l ike ,\" F o r t n i t e \" ,whati st h a t ?I don ' teven know whati ti sHost(QUEST) :So you weren ' tevenf a m i l i a r ?Inthereply ,doesthehostp a r a p h r a s esomethings p e c i f i ctheguestsays ? Explanation :Let ' st h i n ks t e pbys t e p .TerrenceFergusonsaysa ttheendofh i st u r nt h a the didn ' t knowF o r t n i t e .Quest ,thehostoftheinterview ,r e p e a t st h a ttheguestdoesn ' t knowF o r t n i t e .So theybothsayt h a ttheguestdidn ' t knowF o r t n i t e .Therefore ,theansweri syes ,thehosti sp a r a p h r a s i n gtheguest .VerbatimQuoteGuest :\" I 'm li ke ,\" F o r t n i t e \" ,whati st h a t ?I don ' teven know whati ti s \"VerbatimQuote Host :\" you weren ' tevenf a m i l i a r ?\"C l a s s i f i c a t i o n :Yes . Given ani n t e r v i e won 2013101 withthesummary :. . .Guest and Hostsaythef o l l o w i n g :Guest(REP . RAUL LABRADOR (R) , IDAHO) :. . .Host(BLITZER) :. . .Inthereply ,doesthehostp a r a p h r a s esomethings p e c i f i ctheguestsays ? Explanation :Let ' st h i n ks t e pbys t e p . EXPLANATION Therefore ,theansweri syes ,hosti sp a r a p h r a s i n gtheguest .VerbatimQuoteGuest :\"We wouldl i k ethes e n a t o r stoa c t u a l l ycome andn e g o t i a t ewithus . \"VerbatimQuote Host :\" you wantton e g o t i a t e \"C l a s s i f i c a t i o n :Yes .",
  "ITEM": "Explanation :. . .VerbatimQuoteGuest :\" s hi pp in ghim hereto me\"VerbatimQuote Host :\" comingto New J e r s e yandbeingunderthea u s p i c e s \" \" of De Lacy Davis . \"C l a s s i f i c a t i o n :Yes .",
  "D.3Highlighting Analysis": "We compare the highlights provided by DeBERTaAGGREGATED34 and DeBERTa ALL35 on 10 text pairsfrom the test set that were classified as paraphrasesby both models. We provide examples in .DeBERTa ALL highlights are shorter, often more onpoint and arguably more consistent than DeBERTaAGGREGATED highlights. We also manually ana-lyzed 10 text pairs from the test set that GPT-4classified as paraphrases. We provide examplesof GPT-4 highlights in . Generally, theyseem of good quality, but have the tendency to spancomplete sub-sentences, even if not all is relevant.Hallucinations. One of the biggest problemsfor in-context learning are the extractions of thehighlighting from the model responses which haserrors in up to 71% of the cases in . Mostof these errors can be split into two categories: (1)inconsistent highlighting, where the model classi-fies a paraphrase but does not highlight text spansin both, the guest and host utterance and (2) hal-lucinations, where the model highlights spans thatdo not exist in that form in the guest or host utter-ance. Hallucination is more prevalent than incon-sistent highlighting for GPT-4, where in most casesit leaves out words (e.g., coming back to a nor-mal winter vs. coming back daryn to a normalwinter), in some other cases it adds or replaceswords (e.g.,hes a counterpuncher vs. hes coun-terpuncher), uses morphological variation (e.g.,youve vs. you have) or quotes from the wrongsource (e.g., from the host when considering theguest utterance). Most of these extraction errorsseem to be resolvable by humans when looking atthem manually, so it might be possible to addressthem in future work with a more advanced match-ing algorithm or by querying GPT-4 until one getsa parsable response. When looking at the classifi-cations by GPT-4 they often seem plausible, evenwhen counted as incorrect with the F1 score.",
  "AGGALLCShortened Examples": "G: There are people that are in that age range where we know theyre high risk, why are theygoing to thesupermarket tobuy their own groceries? Get the community, the neighborhoodto go and help them.H: if youre going to help somebody by helping them maybe get their groceries, how longdoes the coronavirus live on surfaces?",
  "G:And people always prefer, of course, to see the pope as the principal celebrant of the": "mass. So thats good. Thatll be tonight. And it will be his 26th mass and it will be the 40thor, rather, the 30th time that this is offered in round the world transmission. And it will bemy 20th time in doing it as a television commentator from Rome so.H: Yes, youve been doing this for a while now. G: Well, what happened was we finally waved down a Coast Guard helicopter. And whatthey were looking for were people with disabilities and medical conditions, which none of usreally had. They didnt lift any of us into the helicopter or anything. What they told us was tobasically walkout of our house, up the street,trying to fight against the current that wasgoing theopposite way of where we needed to go.H: So you walked through that current to get to the higher ground or get to a drier spot?",
  "G: Theyve now spent $6 million on this Benghazi investigation. They keep coming up withmore and more interviews.H: On Benghazi, Trey Gowdy now saysyour committee has interviewed 75 witnesses": ": DeBERTa ALL vs DeBERTa AGGREGATED highlights. Paraphrase highlights predicted by the best DeBERTaALL (i.e., seed 201 with F1 score of 0.72) and the best DeBERTa AGG model (i.e., seed 202 F1 score of 0.76, same asin the main paper). Even though DeBERTa AGG gets better F1 scores on classification, the DeBERTa ALL highlightsare arguably more on point. For comparison, we also display the human highlights if they exist. Note, highlightscan exist even if the crowd majority vote did not predict a paraphrase.",
  "GPT-4CShortened Examples": "G: We also want to see what connections exist between pardons and potential gifts to the ClintonLibrary.H: Congressman, short of, though, having a thank-you note attached to a check that went to the ClintonLibrary, what is it exactly that is going to prove that there was a quid pro quo, that these pardons wereactually bought?"
}