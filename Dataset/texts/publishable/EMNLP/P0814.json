{
  "Abstract": "Large language models such as ChatGPT ex-hibit striking political biases. If users querythem about political information, they oftentake a normative stance. To overcome this, wealign LLMs with diverse political viewpointsfrom 100,000 comments written by candidatesrunning for national parliament in Switzerland.Models aligned with this data can generatemore accurate political viewpoints from Swissparties, compared to commercial models suchas ChatGPT. We also propose a procedure togenerate balanced overviews summarizing mul-tiple viewpoints using such models. The repli-cation package contains all code and data.",
  "Introduction": "Large language models (LLMs) have become verypopular, with chat applications like ChatGPT andGemini having hundreds of millions of active userscombined.1 One of the intended use cases is theretrieval of factual information (e.g., Mehdi, 2023).Interacting with chatbots can influence users views(Jakesch et al., 2023) and potentially influence be-havior (e.g., Stieger et al., 2021; Sharma et al.,2024). Because of this, LLMs if used as decisionaids in high-stakes contexts such as shaping politi-cal views or votes must return factually correctand unbiased statements.Political bias is present in all first-generationLLMs (Feng et al., 2023). Also, ChatGPT is notimpartial, as several recent papers have shown:ChatGPT exhibits progressive, liberal, and pro-environmental biases (Rozado, 2023; Hartmannet al., 2023; Motoki et al., 2024; Rutinowski et al.,2024). Given these findings, Hartmann et al. (2023)ask: \"What if ChatGPT exhibits a political ideology system: You are a helpful Swiss policy advisor.You are asked a policy issue or question. You arein the political party P, and you reply in L.user: Whats your opinion on the following issueor question: Q",
  ": Prompt for conditional generation. Varyingattributes are party P, language L, and political issue Q": "that may pervade its synthetic responses and subtlyinfluence its millions of unsuspecting users?\"To address such concerns, we propose to alignLLMs such that they can generate more diverse po-litical viewpoints. Inspired by early work with GPTmodels and conditional generation based on meta-data (Zellers et al., 2019), we align LLMs towardaccurately reflecting diverse viewpoints from Swissparties. Throughout the paper, we use prompts ofthe form shown in : We ask an LLM aboutits opinion on political issues while being in partyP and speaking language L.2 We align LLMs with data obtained from theSwiss voting advice application smartvote: Wehave ca. 100,000 comments written by candidatesrunning for national parliament in Switzerland, cou-pled with meta-data, comprising 33 attributes suchas party, language, canton, age, profession, favoritemovies, and others. Smartvote lets both user andcandidates fill out a survey about stances towardvarious political issues. Then, it matches candi-dates with the highest overlap to a user. This helpsvoters decide which candidates they could vote for.We took that survey with ChatGPT. We find thatChatGPT is most closely aligned with candidatesfrom the Green Liberal Party of Switzerland, fol-lowed by other liberal parties. These results con-firm its progressive and pro-environmental bias inthe Swiss case. A more exhaustive analysis is pre-sented in Appendix A, and we show anecdotal evi-dence of such biases in .",
  "PartyChatGPT generationORPO-aligned Llama 3 model": "SPYes, the state should do more to promote equal educational opportunities in order to promoteequal opportunities and reduce social inequalities. Tuition vouchers for pupils from low-income families can be a useful instrument in this regard. overlap answer and party stance:high Equal opportunities are thefoundation of our state. over-lap answer and party stance:highDie MitteYes, Die Mitte supports the promotion of equal educational opportunities for all pupils.Measures such as tuition vouchers for pupils from low-income families can help to promoteequal opportunities in the education system. overlap answer and party stance: medium Funding is important, but notonly for low-income families.overlap answer and partystance: highFDPYes, the state should do more to promote equal educational opportunities. Tuition vouchersfor pupils from low-income families could be a useful instrument in this regard. overlapanswer and party stance: low The cantons are responsible forproviding schools. overlap an-swer and party stance: highSVPYes, the state should do more to promote equal educational opportunities in order to promoteequal opportunities and reduce social inequalities. Tuition vouchers for pupils from low-income families can be a useful instrument in this regard. overlap answer and party stance:low",
  "Promotinglow-achievingpupils is not the solution.overlap answer and partystance: high": ": Political stances generated with ChatGPT and an aligned model for the policy issue Should the state domore to promote equal educational opportunities? for all major Swiss parties represented in the Federal Council ofSwitzerland. Political leanings (taken from Wikipedia): SP = center-left, Die Mitte = centerist, FDP = center-right,SVP = right-wing. Text in bold (overlap) inserted by authors. In this work,we align Llama 3 models(AI@Meta, 2024) with smartvote data, combiningconditional generation (e.g., Zellers et al., 2019)and monolithic preference optimization (Honget al., 2024) alignment. We find that the resultingaligned models generate more diverse and moreaccurate political viewpoints compared to commer-cial or non-aligned models and these alignedviewpoints are preferred in human evaluation.Such models can be used to create accurate po-litical views of all Swiss parties toward an issue,which then could be summarized by other capableLLMs (e.g. OpenAI et al., 2024) to give balancedoverviews. Such approaches potentially facilitatefinding political compromises or learning moreabout political issues. However, we urge moreresearch to better understand the promises and dan-gers of AI in providing political information orvoting advice. In any case, we strongly believe thatif LLMs were used in such circumstances, theydbetter be accurate and impartial.",
  "Related Work": "After LLMs became popular, it did not take longfor social scientists to start investigating the polit-ical leaning of LLMs, specifically ChatGPT (e.g.,Rozado, 2023; Hartmann et al., 2023; Motoki et al.,2024; Rutinowski et al., 2024). All of them re-ported that ChatGPT has political biases and a cer-tain political leaning.Relatedly, the NLP community also noticed po-litical leanings of LLMs (Feng et al., 2023; Banget al., 2024), and started to develop appropriatecounter-measures. One approach is to explicitlyalign them with specific leanings: Jiang et al.(2022) train \"CommunityLM\", an LM specifically aligned with a certain political leaning on a ded-icated corpus and the authors investigate theworldviews of such communities by probing the re-sulting LLM. Concurrent work fine-tunes an ensem-ble of such CommunityLMs, and shows that thesecan be used as well to produce balanced overviews(Feng et al., 2024). Lastly, future work to generatebalanced overviews using our LLMs would profitfrom literature on summarizing subjective opinions(Suhara et al., 2020; Amplayo and Lapata, 2021) orexplicitly generating consensus statements (Bakkeret al., 2022).",
  "Data": "We use the same data source as (Vamvas and Sen-nrich, 2020): Comments written by candidates run-ning for national parliament in Switzerland. Thecomments were written for and submitted to the vot-ing advice application smartvote. This applicationhelps voters determine which candidates or par-ties have political preferences similar to their own.Prior to an election, candidates can report theirstance on a short (30 questions) or long (75 ques-tions) survey across various political issues. Voterscan take the same survey and are matched withcandidates having the highest overlap (smartvotereturns an ordered list of all candidates a voter canelect, ordered by survey overlap). The questions aredrafted by a team of political scientists (for moredetails, see Thurman and Gasser, 2009). Thesequestions are yes-no questions, and smartvote al-lows candidates to submit comments which furtherexplain their stance on an issue (these responsescan be queried by users). We have 100,000 suchcomments for ca. 200 political questions acrossthe last three national elections in Switzerland. We use these comments and metadata to align modelsusing conditional generation.Smartvote is a popular service in Switzerland:85% of candidates running for elections in Switzer-land have a smartvote profile, and one in five votersconsults smartvote before elections. Thus, our datalikely offers a rich overview of possible politicalstances in Switzerland. We show more detaileddataset statistics in Appendix B.",
  "Methods": "In conditional generation, we want to generate textbased on constraints or metadata (e.g., Zellers et al.,2019; Zhou et al., 2023). For example, previouswork generated news articles based on the attributesdomain, date, authors and headline (Zellers et al.,2019). Alignment datasets usually contain triplesof the form instruction, preferred choice and re-jected choice.3 We interpret conditional generationfor alignment as sampling a comment toward a po-litical issue q drafted by somebody from party pspeaking language l as the preferred choice. Forthe rejected choice, we sample a comment for thesame issue q in the same language l, but from acandidate who is in a different political party p.We use reference-free monolithic preference op-timization (ORPO; Hong et al., 2024) as our align-ment objective. We optimize the following jointloss taken directly from the ORPO paper:",
  "(2)": "where the first part of equation 1 is just super-vised fine-tuning. The second part LOR from equa-tion 2 increases the likelihood of the preferredchoice yw and decreases the likelihood of the re-jected choice yl. For the exact details, we refer to(Hong et al., 2024). We believe this loss is wellsuited for conditional generation, as it pushes apartcomments with different metadata, although theymight only differ in subtle nuances. We will com-pare ORPO-aligned models to direct supervisedfine-tuning (dSFT; Taori et al., 2023) in the resultssection.We also experimented with direct policy opti-mization DPO (Rafailov et al., 2023) following the",
  "That is for reference-free methods such as DPO (Tunstallet al., 2023) or ORPO (Hong et al., 2024)": "recipe outlined in (Tunstall et al., 2023) and RLHF(Stiennon et al., 2020). However, initial qualitativeexploration has shown that the trained models didnot generate satisfactory output. We use LORA forall experiments but do not tune hyperparameters.We believe that it should be possible to get simi-lar results with other alignment algorithms, but wehave not explored that any further.In all our experiments, we used the transformerTRL library (von Werra et al., 2020) and the 4bitquantized unsloth version of Llama 3 8B models4 (AI@Meta, 2024) and fine-tuned all models usingLoRA (Hu et al., 2022). For the supervised fine-tuning, we used the hyper-parameters outlined in(Tunstall et al., 2023), and for ORPO alignment,we proceeded with the hyper-parameters outlinedin (Hong et al., 2024).",
  "Results": "We present four sets of results on our datasetsdevelopment and test split: Qualitative evidence,diversity of generations, similarity between gener-ated text and human references, and human eval-uation. All our models use the prompt templateshown in .We present results for zero-shot settings (Chat-GPT 3.5, GPT-4o, and Llama 3), few-shot settings(ChatGPT 3.5), Llama 3 fine-tuned using direct su-pervision (Taori et al., 2023), and Llama 3 alignedusing ORPO.",
  "Diversity of Generations": "We show qualitative evidence of political bias inChatGPT generations and a lack of variety in re-sponses in , where ChatGPT generates al-most identical, progressive responses for all par-ties, although actual party stances toward the issueof whether the state should promote equal educa-tional opportunities vary substantially. The ORPO-aligned Llama 3 model, on the other hand, accu-rately captures these different stances.We present additional quantitative evidence ofthis phenomena. For each political issue and model,we compute Jaccard similarities between the gen-erations for different parties. We plot the averageoverlap between responses measured in Jaccardsimilarities5 in .",
  ": Average diversity of replies within a politicalissue, measured with Jaccard similarities (lower Jaccardsimilarity means higher diversity)": "We find that all zero-shot generations are strik-ingly similar and have a high average similarity.Both few-shot learning and supervised fine-tuningresults in more diverse generations, reducing thenumber of overlapping words by 30% comparedto ChatGPT zero-shot. The ORPO-aligned modelsfurther reduce overlapping generations and resultin an average similarity of 0.24, roughly half of theoverlap measured for ChatGPT 3.5 in a zero-shotsetting.",
  "Quantitative Evaluation": "As another set of results, we compute MAUVEscores (Pillutla et al., 2021). MAUVE is an auto-mated metric that measures the gap between neuraltext and human references using LLM representa-tions. The higher the MAUVE score, the closer thegenerated text and the human references are. Be-cause our generations are either in German, French,or Italian, we use a multi-lingual RoBERTa modelas a featurizer (Conneau et al., 2020). shows the resulting scores over different datasetsplits. We show average results over five runs (with95% confidence intervals), sampling different ref-erence comments in each run.",
  ": Automated metrics measuring overlap betweenmodel-generated replies and actual replies in the devel-opment and testset": "The overall picture is similar to the diversity re-sults. Zero-shot experiments result in the lowestoverall MAUVE scores. Few-shot and supervisedfine-tuning again lead to comparable MAUVEscores. Lastly, the ORPO-aligned generations ob-tain the by far highest MAUVE scores, indicatingthat they are closest to the actual reference com-ments.These results are robust across runs, and the 95%confidence intervals remain small. Furthermore,we computed MAUVE scores with an MBERTencoder (Devlin et al., 2019) which produces verysimilar results and the same ranking (Devlin et al.,2019).",
  ": Win rates by different models": "Due to the high costs of manual evaluation, weonly select four models for human validation. Over-all, the generations produced by the ORPO-alignedmodels are preferred in around 60% of the cases,whereas generations from the other models havesimilar win rates. The author team, with the helpof the mayor of a Swiss city, manually annotated40 comments in a deliberative setting. All datapoints during that annotation round were discussedat length. Before seeing the model generations, theteam discussed what an optimal generation for aquestion given the party would look like. We thendiscussed which of the two generations is closer to that. We treat this set as the gold standard. In-ter annotator agreement of the annotator and thisgold standard is 0.55 (Cohens kappa), indicatingmoderate agreement. If we discard evaluationswhere the annotator or the team settles for a tie,this agreement rises to 0.84, indicating almost per-fect agreement. Detailed instructions, annotatordemographics, and further robustness validationscan be found in Appendix C.In sum, all evaluations suggest that aligningLLMs with ORPO seems to work best to gener-ate diverse political viewpoints. But what are thereasons why annotators prefer the generations ofthe ORPO-aligned model?We analyze all human evaluations where theORPO-aligned model is involved and the human an-notation is not a tie. This error analysis reveals thatwe can categorize these data points as follows: Thelosing models generation is (a) inaccurate (n=35),(b) less nuanced (n=27), or (c) otherwise inferior(n=6).6 We find that the ORPO-aligned modelsresponse was often preferred because it is moreaccurate (60% winrate in these cases). At the sametime, the generation of the ORPO-aligned model isoften slightly less nuanced (44% winrate in caseswhere the decision was explained by nuance). For(c), ORPO-aligned models are neither doing worseor better than other models (50%).",
  "Discussion and Conclusion": "If explicitly asked to produce political viewpointsfrom a certain party perspective, LLM-generatedtext should accurately reflect the viewpoints of theparty. We show that current LLMs fail to do so ina zero-shot setting in the Swiss context.Combining alignment and conditional genera-tion substantially improves such generations, aswe have shown qualitatively and quantitativelythroughout this work. Supervised fine-tuning ofLlama models and few-shot experiments result insimilar performance, both beating the zero-shotsetting. ORPO-aligned models work best for thistype of conditional generation, and we confirm thisfinding with a diverse set of results.Such aligned models have practical use casesbeyond accurately presenting specific party pref-erences. We can also use such models to generatea balanced overview of viewpoints toward a spe-cific issue. We show a simple algorithm to do so in",
  ": Pseudocode for generating and synthesizinganswers": "We first generate a stance to a policy issue forall parties, and then let GPT-4o provide a summaryof these stances. For illustration, we run this pro-cedure for the issue Should the state do more topromote equal educational opportunities? shows the overview synthesized from the ORPO-aligned Llama 3 model is more balanced and accu-rate than the one synthesized from the responsesobtained in a zero-shot setting.Generating text that contains political views hasmajor implications. LLMs have the potential toshift attitudes and behavior. If they do that in thepolitical domain, this might influence elections, oneof the most important decision-making processesin democracies (Berger et al., 2008). Our recom-mendations are two-fold: First, further research isnecessary to explore the promises and pitfalls ofLLMs delivering political information or offeringexplicit voting advice. Second, there is a need forboth societal and scientific debates on the role ofLLMs and AI in democratic processes. Hartmann et al. (2023) asked what if ChatGPTexhibited a political ideology that may pervade itssynthetic responses and subtly influence its unsus-pecting users? Our work speaks to this question.We see a number of strategies going forward:(1) LLMs would always refuse to answer anythingrelated to shaping political beliefs and take sin-cere political impartiality as an alignment goal. (2)LLMs would always produce broad overviews ofpolitical issues (as produced by our methods in Fig-ure 9). Our work might facilitate creating appro-priate datasets for this. (3) LLMs would explicitlyproduce text that is aligned with a certain politicalleaning. In this case, however, the provider of anLLM would be fully transparent about this, and/orthe user should be able to fully control what ide-ology LLM-generated text should be aligned with.Aligning models explicitly with party preferencesusing conditional generation, as presented in thiswork, is one way toward such LLMs.",
  "We acknowledge several limitations and outline arange of possibilities for future work": "Choice of models and Alignment algorithms.We have mainly experimented with Llama 3 mod-els and ChatGPT 3.5 zero-shot. There are, by now,other capable open-source models (Mistral, Mix-tral, Llama 2) or model sizes (70B) that we couldhave fine-tuned with the method proposed in thiswork. Also, there exists a range of different align-ment algorithms (DPO, RLHF), which we haveexperimented with, but the resulting models didnot pass initial vibes tests. We plan to investigateall of this more thoroughly in future work. Choice of metadata for conditional generation.In preliminary experiments, we experimented withthe (Vamvas and Sennrich, 2020) dataset and gen-erated comments based on stance (pro/contra) andnot party affiliation. Eyeballing these results indi-cates that ORPO-aligned models in this setting alsoreturn more diverse answers than zero-shot models,and ORPO-aligned model generations seem morecreative than SFT models. We take this as evidencefor the robustness of conditional alignment, butwe have not exhaustively evaluated this. Next, wethink there are exciting opportunities in alignmentwith more metadata, such as canton, age, gender,and any other attribute potentially influencing po-litical viewpoints. We tried this in preliminary ex-periments with Mistral models and DPO. However,this didnt work. We plan to revisit this with Llama3 and ORPO.",
  "Data availability.All data used in this study andthe replication package are publicly available ongithub: dominiksinsaarland/swiss_alignment": "Further data.We believe it should be possible,in principle, to find more diverse data sources withparty affiliation (e.g., newspaper or TV interviews,party website content). It should be possible tocollect such, and this would make for a datasetincluding more diverse political questions, whichmight lead to more creative models. This approachcan be used across countries and parties, and thusallow for replication studies in contexts not relatedto smartvote data or Switzerland.",
  "We also acknowledge ethical implications of ourwork": "Contested topic.We believe the combination ofLLMs and democracy is a very delicate topic, andthroughout the manuscript tried to do justice tosuch challenging circumstances. On one hand, weall exhibit political biases, which we tried to re-move from the paper as good as possible, but wealso acknowledge that we probably have not writ-ten a completeley impartial piece. The same holdsfor LLMs. Another goal of this paper is to increasethe awareness about these points. Intended use case.We believe it is importantthat LLMs, its users, developers testers and otherstakeholders are aware of political bias in machine-generated text. In this paper, we do not arguefor creating chatbots which act as echochambersand/or reinforce existing biases present in suchmodels or change political views or actions ofusers. We want to argue for the opposite, that LLMsshould exactly not do that. We tried to do justice tothis goal. Biases in LLMs.Political bias is one sort of biaspresent in LLMs. There are others (see e.g., Abidet al., 2021; Lucy and Bamman, 2021), which arenot addressed in this work. Our resulting modelsmay potentially perpetuate these biases. Accuracy, hallucinations, and outdated informa-tion.Our aligned models, as well as ChatGPT,are not 100% accurate in producing political in-formation: They produce hallucinations or otherpotentially harmful text, hence we do not advocateto use them in a commercial context, but proposea method to potentially mitigate political biases inLLMs. Further, we align our models on smartvotecomments from 2015 - 2023. Parties might changetheir stance in the meantime. It remains an openresearch question how to incorporate such changesin stances. Non-constitutional parties.We included view-points of political parties that operate within thelimits of the constitution. Whether LLMs shouldreproduce the content of extremist parties withoutdisclaimers is not within the scope of our research.",
  "AI@Meta. 2024. Llama 3 Model Card": "Reinald Kim Amplayo and Mirella Lapata. 2021. In-formative and Controllable Opinion Summarization.In Proceedings of the 16th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics: Main Volume, pages 26622672, Online.Association for Computational Linguistics. Michiel Bakker, Martin Chadwick, Hannah Sheahan,Michael Tessler, Lucy Campbell-Gillingham, JanBalaguer, Nat McAleese, Amelia Glaese, JohnAslanides, Matt Botvinick, and Christopher Sum-merfield. 2022. Fine-tuning Language Models toFind Agreement among Humans with Diverse Prefer-ences. In Advances in Neural Information Process-ing Systems, volume 35, pages 3817638189. CurranAssociates, Inc. Yejin Bang, Delong Chen, Nayeon Lee, and PascaleFung. 2024. Measuring Political Bias in Large Lan-guage Models: What Is Said and How It Is Said. InProceedings of the 62nd Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 1114211159, Bangkok, Thai-land. Association for Computational Linguistics.",
  "Jonah Berger, Marc Meredith, and S. Christian Wheeler.2008. Contextual Priming: Where People Vote Af-fects how they Vote. Proceedings of the NationalAcademy of Sciences, 105(26):88468849": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal,Vishrav Chaudhary, Guillaume Wenzek, FranciscoGuzmn, Edouard Grave, Myle Ott, Luke Zettle-moyer, and Veselin Stoyanov. 2020. UnsupervisedCross-lingual Representation Learning at Scale. InProceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 84408451, Online. Association for Computational Lin-guistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofDeep Bidirectional Transformers for Language Un-derstanding. In Proceedings of the 2019 Conferenceof the North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics. Shangbin Feng, Chan Young Park, Yuhan Liu, and YuliaTsvetkov. 2023. From Pretraining Data to LanguageModels to Downstream Tasks: Tracking the Trailsof Political Biases Leading to Unfair NLP Models.In Proceedings of the 61st Annual Meeting of theAssociation for Computational Linguistics (Volume 1:Long Papers), pages 1173711762, Toronto, Canada.Association for Computational Linguistics. Shangbin Feng, Taylor Sorensen, Yuhan Liu, JillianFisher, Chan Young Park, Yejin Choi, and YuliaTsvetkov. 2024.Modular Pluralism: PluralisticAlignment via Multi-LLM Collaboration. Preprint,arXiv:2406.15951. Jochen Hartmann, Jasper Schwenzow, and MaximilianWitte. 2023. The Political Ideology of Conversa-tional AI: Converging Evidence on ChatGPTs Pro-environmental, Left-libertarian Orientation. Preprint,arXiv:2301.01768.",
  "ORPO: Monolithic Preference Optimization withoutReference Model. Preprint, arXiv:2403.07691": "Edward J Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. 2022. LORA: Low-Rank Adaptationof Large Language Models. In International Confer-ence on Learning Representations. Maurice Jakesch, Advait Bhat, Daniel Buschek, LiorZalmanson, and Mor Naaman. 2023. Co-Writingwith Opinionated Language Models Affects UsersViews. In Proceedings of the 2023 CHI Conferenceon Human Factors in Computing Systems, CHI 23,New York, NY, USA. Association for ComputingMachinery. Hang Jiang, Doug Beeferman, Brandon Roy, and DebRoy. 2022. CommunityLM: Probing Partisan World-views from Language Models. In Proceedings ofthe 29th International Conference on ComputationalLinguistics, pages 68186826, Gyeongju, Republicof Korea. International Committee on ComputationalLinguistics.",
  "OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,Lama Ahmad, and Ilge Akkaya et al. 2024. GPT-4Technical Report. Preprint, arXiv:2303.08774": "Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers,John Thickstun, Sean Welleck, Yejin Choi, and ZaidHarchaoui. 2021. MAUVE: Measuring the Gap Be-tween Neural Text and Human Text using DivergenceFrontiers. In NeurIPS. Rafael Rafailov, Archit Sharma, Eric Mitchell, StefanoErmon, Christopher D. Manning, and Chelsea Finn.2023. Direct Preference Optimization: Your Lan-guage Model is Secretly a Reward Model. Preprint,arXiv:2305.18290.",
  "Generative Echo Chamber? Effects of LLM-PoweredSearch Systems on Diverse Information Seeking.Preprint, arXiv:2402.05880": "Mirjam Stieger, Christoph Flckiger, Dominik Regger,Tobias Kowatsch, Brent W. Roberts, and MathiasAllemand. 2021. Changing Personality Traits withthe help of a Digital Personality Change Intervention.Proceedings of the National Academy of Sciences,118(8):e2017548118. Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M.Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,Dario Amodei, and Paul Christiano. 2020. Learningto Summarize from Human Feedback. In Proceed-ings of the 34th International Conference on Neu-ral Information Processing Systems, NIPS 20, RedHook, NY, USA. Curran Associates Inc. Yoshihiko Suhara, Xiaolan Wang, Stefanos Angelidis,and Wang-Chiew Tan. 2020. OpinionDigest: A Sim-ple Framework for Opinion Summarization. In Pro-ceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 57895798, Online. Association for Computational Lin-guistics. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, YannDubois, Xuechen Li, Carlos Guestrin, Percy Liang,and Tatsunori B Hashimoto. 2023. Alpaca: A Strong,Replicable Instruction-following Model.Stan-ford Center for Research on Foundation Models. stanford. edu/2023/03/13/alpaca. html,3(6):7.",
  "AChatGPT and smartvote": "We have taken the smartVote survey with Chat-GPT for the national elections in 2023 in Switzer-land. We have taken the short survey (30 ques-tions) using a temperature of 0 and the followingsystem prompt: You are a helpful research assis-tant in Switzerland. You are given a few questionswhich you need to answer. You MUST reply withonly one of \"yes\", \"rather yes\", \"rather no\", \"no\".The output categories are the options users can en-ter while taking the smartvote survey. The userprompt equals to the actual smart vote questions,e.g., \"Should the state do more to promote equaleducational opportunities\"?In , we show the output a user receivesafter taking the smartvote survey: In the top panel,we show political preferences across different di-mensions. The categories correspond to: LiberaleGesellschaft = Liberal society; Offene Aussenpoli-tik = Open foreign policy; Liberale Wirtschaft-spolitik = Liberal economic policy; Restriktive Fi-nanzpolitik =Restrictive financial policy; Restrik-tive Migrationspolitk = Restrictive migration pol-icy; Ausgebauter Umweltschutz = Expanded envi-ronmental protection; Ausgebauter Sozialstaat =Expanded welfare state.In the bottom panel, we show the candidateswho were identified as having the highest politicaloverlap with ChatGPT. 7 out of 12 (58%) of themost aligned candidates would be from the (Young)Liberal Party of Switzerland (GLP or JGLP).",
  "BDataset Statistics": "In , we show 5 randomly sampled comments(and their English translation) from our dataset.In , we show dataset statistics, the numberof examples in each split, the number of politicalissues and the share of different languages in thedifferent splits of the dataset. In , we showa histogram of the sequence lengths in the dataset(across all splits). We excluded comments shorterthan five words.We have access to smartvote data for the nationalparliament elections in 2015, 2019, and 2023. Wesplit the data into a training set, a development,and a test set. Both the development and test setconsist of 10% of the political issues from the 2023election that were not present in the 2015 or 2019survey.We show the 10 most often occuring parties andtheir associated number of comments in .",
  "CAnnotation Guidelines": "We recruited an annotator from Switzerland with auniversity degree in political science and a strongself-declared interest in Swiss politics. The anno-tator read a random sample of 200 messages. Theannotator was instructed as follows: Here is some information on the project: LargeLanguage Models (LLMs, such as ChatGPT) of-ten have biases. For a research project, we havefine-tuned an open-source LLM to make it more rep-resentative of the political values of different Swisspeople. We used Smartvote data for this alignment.Now, an important question is whether our fine-"
}