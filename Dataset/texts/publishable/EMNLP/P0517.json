{
  "Abstract": "Text detoxification aims to minimize the riskof language models producing toxic content.However, existing detoxification methods failto balance the detoxification effectiveness andgeneration quality. This issue arises from ne-glecting the constraints imposed by the con-text: language models are designed to gener-ate output that closely matches the given con-text, while detoxification methods strive to en-sure the safety of the output, even if it de-viates semantically from the context. Giventhis, we introduce a Context-aware Model self-Detoxification (CMD) framework that pays at-tention to both the context and the detoxifica-tion process, i.e., first detoxifying the contextand then making the language model gener-ate along the safe context. Specifically, CMDframework involves two phases: utilizing lan-guage models to synthesize data and applyingthese data for training. We also introduce atoxic contrastive loss that encourages the modelgeneration away from the negative toxic sam-ples. Experiments on various LLMs have veri-fied the effectiveness of our MSD framework,which can yield the best performance comparedto baselines.1 Warning: cases in this papermay contain offensive content.",
  "Introduction": "Large Language Models (LLMs) have exhibitedremarkable performance in various NLP tasks andapplications (Brown et al., 2020; Chowdhery et al.,2022; Anil et al., 2023). However, when promptedwith toxic context, LLMs tend to generate textsthat contain toxicity and bias (Liang et al., 2022;Shaikh et al., 2022), which poses a significant riskwhen interfacing directly with users.To mitigate such a concern for LLMs, one couldadopt the response rejection strategy (Zhang et al.,",
  "Equal ContributionCorresponding Author1Code & Data:": "2023) to ignore the unsafe context. However, sucha strategy is unfriendly to the users under somespecific scenarios, such as mediation or conflictresolution (Lhr et al., 2017). Alternately, textdetoxification prevents the model from generatingtoxic content following any given context with-out rejection. Along this line, non-negligible ef-forts have recently been devoted to two main as-pects: output-intervention methods like manipulat-ing output probability distribution during inferencetime (Dale et al., 2021; Xu et al., 2021; Leong et al.,2023) and trainable methods that update model pa-rameters on the detoxification datasets (Wang et al.,2022; Park and Rudzicz, 2022; Niu et al., 2024).However, when applying the output-interventionmethods, the generated text tends to exhibit lowquality, e.g., semantic incoherence with the con-text, due to some unexpected perturbations to theoutputs; while trainable methods are constrainedby the available detoxification dataset, which maylead to poor detoxification effectiveness2. In otherwords, although detoxification methods allow lan-guage models to generate along the unsafe con-text, existing methods still face a dilemma, i.e.,the imbalance between detoxification effectivenessand the generation quality. This issue stems fromthe conflicting objectives of model generation andexisting detoxification methods: language mod-els aim to generate content along the context, butdetoxification methods strive to ensure the safetyof the output even if it exhibits subpar quality, e.g.,semantically deviating from the context.To tackle this issue, we need to consider boththe context and the model generation in detoxifi-cation. Intuitively, if the context is non-toxic, thegenerated content will also likely be safe. There-fore, we decompose the detoxification into twosteps: first detoxifying the context and then makingthe language model generate along the safe con-tent, thus ensuring the generated texts quality and",
  "We conduct the preliminary study in Sec. 2.2": "safety. However, it is also worth noting that evena safe context can induce toxic content occasion-ally (Zhang et al., 2022). Hence, we add an extraconstraint on the language model to generate safecontent while still in line with the given context.Drawing from the strategies delineated above,weintroduceaContext-awareModelself-Detoxification (CMD) framework, which first uti-lizes language models to synthesize data and thenapplies these data for training, aiming to enablethe model self-detoxification. Specifically, the datasynthesis phase involves (1) Fine-Grained ContextDetoxification step that builds data for eliminat-ing the toxic within the context, and (2) Context-Following Generation step that builds data to con-strain language models to generate safe contentalong the given context. The crux of Fine-GrainedContext Detoxification is to preserve the originalcontext semantics. Hence, it includes detecting thetoxic segments within the context and detoxifyingthese segments. Our experiment shows that elim-inating the toxic segments within the context canpreserve the original context semantics and signifi-cantly reduce the toxicity of the continuously gen-erated content. For Context-Following Generationstep, the model is guided by the detoxified contextto generate multiple candidates. Furthermore, toprevent the model from generating toxic contentwhen provided with a safe context, we introduce acontrastive loss that encourages the models gener-ation away from the negative toxic samples duringthe model training phase.Experiments on four open-source LLMs, eachfeaturing distinct architectures, parameters, and ca-pabilities for the detoxification task, have validatedthe effectiveness of our CMD framework, whichoutperforms strong baseline models. Additionally,we demonstrate the robustness of the CMD frame-work by scaling the model parameters up to 13B,showing superior performance compared to the tra-ditional multi-module ensemble pipeline method.",
  "Preliminary Study": "The auto-regressive generation manner allows lan-guage models to generate along the given context,ensuring the output text is coherent and consistent.However, such a paradigm is risky when modelsencounter a toxic context. Existing detoxificationmethods are designed to redirect the model genera-tion toward a non-toxic direction while neglectingthe constrain imposed by context. In this section: (1) We first rethink the existing detoxification meth-ods from two aspects: detoxification effectivenessand the generation quality; (2) Then, we take safecontext into consideration and analyze the effective-ness of safe context by first detoxifying the contextand subsequently guiding LLMs to generate alongthe safe context; (3) Detoxifying the context duringthe detoxification process entails the usage of exter-nal modules, which requires extra efforts to alignmodules with language models and can lead to per-formance degradation. Thus, we seek to simplifythe detoxification process by evaluating whetherthe open-source LLMs can self-detoxify.",
  "Study Settings": "We utilize three LLMs (GPT2-XL (Radford et al.,2019), LLaMA2-7B (Touvron et al., 2023), andMistral-7B-Instruct (Jiang et al., 2023)) and threerepresentative detoxification approaches (output-intervention methods DExperts (Liu et al., 2021)and Gedi (Krause et al., 2020) that manipu-late the output distribution, and trainable methodSGEAT (Wang et al., 2022) that fine-tunes modelon the detoxification dataset3) for preliminarystudy. For evaluation, we utilize REALTOXICI-",
  "Rethinking of Existing Methods": "We feed the model with toxic context from RTPtesting data and evaluate the generated text fromthree perspectives: coherence, consistency, andtoxicity. We plot the evaluation results in ,which indicates that the methods directly manip-ulating the output distribution (Gedi and DEx-perts) tend to generate safe content, but the textquality is significantly worse than that of theLLMs (GPT2-XL and LLaMA2-7B). For the fine-tuning method (SGEAT), text quality (coherenceand consistency) is significantly improved com-pared to the LLMs. However, the generated toxicity",
  ": Comparison of detoxification methods forLLMs. More details are shown in Appendix A.1": "is similar to that of LLMs. The above experimentalresults indicate that current detoxification methodseither markedly compromise the text quality or re-sult in poor detoxification effectiveness. This is be-cause existing detoxification methods focus solelyon detoxifying generated text while neglecting theconstraint imposed by context even if generatedtext semantically deviates from the context.",
  "Effectiveness of Safe Context": "To mitigate the aforementioned issue, we pay moreattention to the context rather than solely to detoxi-fying the generated text. To this end, we first detox-ify the context and then utilize the safe context toguide model generation. Specifically, we manu-ally detect the toxic segments in the context withPerspectiveAPI and replace them with the sentineltoken [MASK] based on their toxicity scores indescending order6. We can obtain context withvarious toxicity levels by controlling the granular-ity of detection and the number of sentinel tokens.Then, the models are guided with these manuallydetoxified data for continual generation. As shownin a, before detoxifying the context, there isa positive correlation between the context toxicityand the generation toxicityas the toxicity of thecontext increases, so does the toxicity of the gen-erated texts from LLMs (yellow line graph). Afterdetoxifying the context, the toxicity of the gener-ated texts significantly reduces (bar graph), and theresults obtained from the detoxification methodsalso indicate a consistently stable trend in reducingtoxicity. From b, we can find a significantpositive correlation between the generation toxi-city and the semantic similarity between context",
  ": Model performance when fed with the contextof different toxicity levels": "and generated text (line graph), indicating that thegeneration toxicity is considerably influenced bythe context. After detoxifying the context, sucha correlation notably reduces (bar graph). Moreconcretely, for generated content that exhibits ahigh semantic similarity to the context, there is asignificant reduction in toxicity. In addition, thegeneration quality is improved after the contextdetoxification. We present more evaluation resultsin Appendix A.1. Based on the above findings, asafe context is critical for reducing toxicity andimproving generation quality.",
  "Detoxification Process Simplification": "Although safe context can reduce toxicity and im-prove the generation quality, the above detoxifica-tion process involves external modules, e.g., con-text detoxification module requiring additional ef-fects to align with models (Krause et al., 2020). Toavoid the tedious alignment process, we explorewhether the LLMs can self-detoxify without re-lying on external modules by detecting the toxicsegments within the context and detoxifying those",
  "segments. We evaluate LLMs from two aspects7:": "Toxic Segment Detection CapabilityWe applythe in-context learning (Brown et al., 2020) methodto guide the model in detecting the toxic segmentswithin the context. As shown in Tab. 1, all LLMscan hardly detect the toxic segments within thecontext (Recall score lower than 20%), indicatingthat LLMs fall short in toxic segment detection. Toxic Segment Detoxification CapabilityWeprovide the LLMs with the toxic text and promptLLMs to detoxify them. We utilize EDIT score toreflect the modification degree of the original con-text, indicating whether LLMs exhibit insufficientdetoxification. As shown in Tab. 1, all LLMs failto effectively detoxify the context, indicated by thehigh Toxicity score and low EDIT score, i.e., mostof the toxic segments remains unchanged.",
  "Takeaway": "1) Existing detoxification methods fail to satisfyboth the detoxification effectiveness and the gen-eration quality since those methods neglect theconstrain imposed by context. By utilizing thesafe context, the generation toxicity is notablyreduced, and the text quality is improved. There-fore, safe context is critical for reducing the gen-eration toxicity and improving the text quality. 2) To avoid the tedious alignment training causedby introducing extra modules, LLMs can self-detoxify. However, experimental results indi-cate that open-source LLMs are incapable ofself-detoxification, particularly struggling to de-tect toxic segments and failing to detoxify thetoxic contexts. Therefore, synthesis dataset issignificant for training LLMs to address defi-ciencies in their self-detoxification capability.",
  "CMD Framework": "According to the above analysis, we introduceCMD (Context-aware Model self-Detoxification),a framework for LLMs to self-detoxify. As shownin , the CMD framework includes two phases:the dataset Synthesis phase that interacts with theLLMs to synthesize data, and the Model Trainingphase that applies the synthesis data to enable theLLMs to self-detoxify. We list all the used promptsand templates in Appendix C.1.",
  "Dataset Synthesis Phase": "The purpose of Dataset Synthesis phase is to syn-thesize the data reflecting the process of contextdetoxification without compromising the originalsemantic (Fine-Grained Context Detoxification)and allow LLMs to generate along the detoxifiedcontext (Context-Following Generation). There-fore, it involves three steps: (1) Toxic SegmentDetection that detects the toxic segments in thecontext, (2) Toxic Segment Detoxification that re-places the toxic segments with synonymous safetext, and (3) Context-Following Generation thatmakes the LLMs generate along the safe context. Toxic Segment DetectionWe first employ ex-isting methods (Khan et al., 2021; Schouten et al.,2023) for toxic segment detection, but discover thatthese approaches may lead to either excessive orincomplete toxicity detection. Therefore, we de-sign a Segment-CNN model G which fuses theglobal and local features of the toxic context fortoxic segment detection. With Segment-CNN, wecan detect the toxic segments within each contextx = {xi}ni=1 according to the predicted toxicityscores of each segments s = {sj}mj=1 = G(x),where sj denotes the toxicity score of text segmentxi:i+a(a = L, i [0, n L)) and L is the pre-defined segment length. We calculate the averagetoxicity of the dataset as and treat xi:i+a as thetoxic segment if sj . Details of Segment-CNNmodel can be referred to Appendix C.2. Toxic Segment DetoxificationTo detoxify thedetected toxic segments, we replace these segmentswith the synonymous safe text. Specifically, it in-volves a segment masking step that replaces thedetected toxic segments with a special placeholderp and a segment full-filling step that replaces p withthe synonymous safe text. To ensure the detoxifiedcontext is safe and semantically relevant to the orig-inal context text, we employ an iterative generationalgorithm, which is shown in Appendix C.3.",
  "Original Context": ": Overview of CMD framework that involves a Dataset Synthesis phase and a Model Training phase. Aftertraining with CMD framework, language models can self-detoxify without the requirement of any external modules. Context-Following GenerationThe Context-Following Generation step is designed to directmodel outputs towards safety, aligning with thedetoxified context. During the Context-FollowingGeneration process, the detoxified context is pro-vided to the model, which then generates K po-tential outputs o as candidates. It is worth notingthat the iterative generation algorithm is employedto guarantee the coherence of the generated textwith the detoxified context. Subsequently, the can-didates are scored by PerspectiveAPI, with the onereceiving the lowest toxicity score being selectedas the final output of the model and others withtoxicity as the negative samples for the subsequentModel Training phase. Integration Through Reasoning ChainAfterobtaining the synthesis data for each step, to allowLLMs to self-detoxify along the given steps, weemploy the Chain-of-Thought (CoT) (Wei et al.,2022) technique to gather all the synthesis data.Specifically, as shown in , we add an ex-tra reasoning step between two adjacent steps totransform the synthesis data x into a step-by-stepreasoning format with the pre-defined template.",
  "Model Training Phase": "The purpose of Model Training phase is to enableLLMs f to learn self-detoxification without com-promising the generation quality. Therefore, weadopt synthesis data x to train LLMs. To pre-vent the possibility that even safe contexts can leadto the generation of toxic content, we employ the contrastive loss (An et al., 2022) by treating the can-didate with the lowest toxicity score as the positivesample o+ and others with toxicity as the negativesamples o. Formally, for each sample, the lossfunction can be written as:",
  "(1)": "where zh, zo+, zoi Rd denote the vector repre-sentation of model generation, positive sample withthe lowest toxicity score, and candidates o, respec-tively. is the temperature and cos(, ) defines thecosine similarity. ce denotes the cross-entropy lossand is the re-weight hyper-parameter. Intuitively,ce seeks to learn the self-detoxification process,and cl prevents the situation where the safe contextleads to toxic generation.",
  "Experimental Settings": "Models & BaselinesWe first compare ourmethod with four existing detoxification baselines,including DExperts, Gedi, SGEAT, and ToxicRe-versal (Leong et al., 2023). Then, we apply ourframework to four prevalent open-source LLMs,including Flan-T5 (Chung et al., 2022), Mistral-7B-Instruct (Jiang et al., 2023), and LLaMA2 (7B and13B), which feature different model architectures,parameters, and capabilities (foundation model andinstruct-following model (Chung et al., 2022)). We",
  ": CMD performance on LLMs, featuring different architectures, parameters, and capabilities": "apply parameter-efficient methods LoRA (Hu et al.,2021) for fine-tuning. For Segment-CNN model,we set L = 2 and apply BERT model (Devlinet al., 2018) as the global feature extractor andfeed-forward neural network as the local featureextractor. We set = 0.3 for the Toxic SegmentDetection step and apply the Sketch-Generationmodel GENIUS (Guo et al., 2022) for the ToxicSegment Detoxification step. For the Model Train-ing phase, we set = 1 and = 1 in Equation (1). Tasks & DatasetsWe experiment on both toxic-induced generation task (RTP) and parallel detoxi-fication task (ParaDetox (Logacheva et al., 2022)and APPDIA (Atwell et al., 2022)). Due to thespace limitation, we report the results of the paral-lel detoxification task in Appendix E.2. Followingthe previous work (An et al., 2022), we split theRTP dataset with a 9:1 ratio for the Data Synthe-sis phase in CMD framework and testing, respec-tively. The testing set contains 9,000 toxic (toxicityscore higher than 0.5) and 1,000 safe (toxicity scorelower than 0.5) prompts. To train Segment-CNNmodel, we leverage JigSaw data. Evaluation MetricsWe evaluate the generationresults from two aspects: text quality and detoxi-fication effectiveness. For text quality, we reportthe PPL score and conduct human evaluation8 toreflect the coherence and consistency of the gen-",
  "Details of human evaluation are shown in Appendix E.1": "erated text. For detoxification effectiveness, wereport Expected Maximum Toxicity and ToxicityProbability of the generated text (Gehman et al.,2020). Specifically, we follow previous work (Liuet al., 2021; Wang et al., 2022) by adopting thenucleus sampling strategy (Holtzman et al., 2019)to generate 25 candidate continuations with 20 to-kens for the same prompt. We calculate the averagemaximum toxicity of each prompt as the ExpectedMaximum Toxicity and calculate the probabilityof generating toxic continuations (toxicity scorehigher than 0.5) in 25 candidate continuations asthe Toxicity Probability score. We report and dis-cuss more evaluation metrics in Appendix E.2.",
  "Main Results": "Comparison with BaselinesWe present the per-formance of CMD and existing detoxification base-lines in , where we can observe that CMDachieves superior performance among all the meth-ods.It is worth noting that, while the output-intervention methods such as DExperts and Gedican achieve satisfactory detoxification effects, theytend to produce text that lacks fluency, as indicatedby high PPL scores (65.90 for DExperts and 200.12for Gedi). In contrast, as illustrated in ,CMD can consistently generate high-quality text.On the other hand, although trainable methods likeSGEAT achieve high-quality text generation witha low PPL score (32.98), their detoxification effec-",
  "tiveness is less impressive. By integrating context,CMD can balance detoxification and generation": "Performance on LLMsAs shown in Tab. 3, wereport the CMD performance on different LLMs.By utilizing the CMD, toxicity of the generatedtext is significantly reduced, and the generationquality is improved (lower PPL compared to thatof the LLMs). Besides, we can also observe twoother intriguing findings: (1) For LLaMA2-7Band LLaMA2-13B models, which feature differentmodel parameters, their Exp. Max. Toxicity andToxicity Prob. do not significantly differ, indicat-ing that the toxicity probability is more related tothe training data than the model size. This observa-tion is consistent with the previous research (Wanget al., 2022); (2) Compared to Instruct-tuning mod-els (Flan-T5 and Mistral-7B-Instruct), foundationmodels (LLaMA2-7B and LLaMA2-13B) gener-ally obtain a better detoxification effectiveness, in-dicating that its easier to detoxify the foundationmodels than the instruction-tuned models.",
  "Dataset Synthesis with ChatGPT": "We prompt ChatGPT to synthesize data for eachdetoxification step and utilize these data to trainLLMs. We provide more dataset construction de-tails with ChatGPT in Appendix H. As shown inTab. 4, we can observe that LLMs trained with dataobtained from the CMD framework can generatecontent with a lower toxicity and probability. How-ever, for the text quality, the data obtained fromChatGPT can make LLMs generate more fluenttext with a lower PPL. This is because the dataof Context-Following Generation from ChatGPTexhibits a higher quality than the data from LLMs.",
  "Intermediate Detoxification Step Analysis": "We evaluate the result of each intermediate step andcompare the performance with the pipeline meth-ods which utilize additional modules to executeevery intermediate step in Tab. 5. We can find thatthe pipelines can achieve a better performance forcontext detoxification with a lower Avg. Toxicityscore. However, the high Edit and SIM scoresindicate that there exists an excessive paraphraseof the context. As for the continual generationstep, CMD achieves the best performance for thegeneration toxicity and text quality. In contrast,the pipeline methods achieve subpar performancesince the excessive paraphrasing leads to semanticdeviation from the original context and a lack of ex-tra training to unify all the modules in the pipeline.Intermediate results are shown in Appendix F.",
  "Detoxification for LLMs": "The potential of LLMs to produce toxic contentposes a significant risk when interfacing directlywith users (Sheng et al., 2019; Wallace et al., 2019;May et al., 2019; Zhao et al., 2019; Deshpandeet al., 2023). Existing works detoxifying the LLMsprimarily unfold along two lines: (1) constrainingthe model output through manipulating the prob-ability distribution (Xu et al., 2021; Schick et al.,2021; Hu et al., 2023), post-processing the gener-ated texts (Moskovskiy et al., 2022; Dementievaet al., 2021), etc, and (2) further training mod-els on non-toxic datasets (Raffel et al., 2020; So-laiman and Dennison, 2021; Xu et al., 2022; Flotoet al., 2023; Prabhumoye et al., 2023) or corpusaligned with human preferences (Ouyang et al.,2022). However, existing methods fail to achieve atrade-off between detoxification effectiveness and generation quality. Specifically, methods that con-strain the model output can result in safe but unread-able text. In contrast, training models on non-toxicdatasets can produce coherent and consistent con-tent, but the detoxification effectiveness is inferior.Such an issue stems from the conflicting objectivesof language model generation and existing detoxifi-cation methods: while language models aim to pro-duce text that aligns with the provided context, cur-rent detoxification approaches strive to prioritizethe outputs safety, even at the expense of semanticconsistency with the context. Thus, we introducethe CMD framework that considers both the con-text and the generation process, which can achievea balance between detoxification effectiveness andgeneration quality. Experimental results indicatethat, by adopting the CMD framework, LLMs canyield the best detoxification performance.",
  "Model Augmentation via CoT": "Chain-of-Thought (CoT) (Wei et al., 2022), involv-ing a series of rationale steps leading to the finalanswer, has been widely applied to LLMs to en-hance the models reasoning capability (Zhu et al.,2022; Kojima et al., 2022). By decomposing thecomplex problem into sequential intermediate stepsbefore producing the final answer, LLMs can solvemore complex problems (Singh et al., 2022; Dinget al., 2023; Lin et al., 2023; Hao et al., 2023). Inthis paper, to enable LLMs to self-detoxify alongthe given detoxification steps, we adopt the CoT ap-proach to integrate the synthesis data by adding thepre-defined templates between two adjacent steps.",
  "Conclusion": "We reveal that existing detoxification methods failto balance the detoxification effectiveness and textquality since these methods strive to prioritize thesafety of generated content while neglecting theconstraints imposed by the context. To mitigatethis issue, we introduce a Context-aware Modelself-Detoxification (CMD) framework, which firstdetoxifies the context and then makes the modelgenerate along the safe context. Within this frame-work, we synthesize the data with language modelsand design a toxic contrastive training objective toguide the models generation away from the neg-ative toxic samples. Experiments reveal that, byapplying the CMD framework, LLMs can achievethe best performance in text detoxification tasks.",
  "Limitations": "Although the CMD framework can achieve impres-sive results, there remain limitations and space forimprovement in model detoxification:(1) It must be acknowledged that the CMD frame-work is not the sole approach to model detoxifica-tion; rather, our framework provides another viewfor model detoxification, which makes the detoxifi-cation process aware of the context to address thebalance between detoxification effectiveness andthe quality of the generated text. There is also roomfor improvement in the design of our framework.(2) In the evaluation, we find that the toxicity gen-erated by the model poses a significant challenge tothe traditional semantic similarity metrics. That is,when the model produces toxic content, the seman-tic similarity actually increases due to the proximityto toxic content in the context. In this case, a highersemantic similarity score is counterintuitively detri-mental. Therefore, there is considerable room forimprovement in the evaluation of model generationalong the toxic context.",
  "Ethic and Policy": "It is worth noting that all the corpora mentioned inthis paper, including the constructed dataset, areonly used for scientific research. As for the alter-native method of dataset synthesis with ChatGPTand evaluation with PerspectiveAPI, we strictly fol-low the OpenAI Terms of Use 9 and Google APIsTerms of Service 10. Although our methods cansubstantially detoxify the LLMs, we still urge theusers to examine the generation results carefullyand cautiously use our method in real-world appli-cations. We want to thank all the anonymous reviewersfor their valuable comments. This work was sup-ported by the National Science Foundation ofChina (NSFC No. 62206194 and 62276077), theNatural Science Foundation of Jiangsu Province,China (Grant No.BK20220488), and YoungElite Scientists Sponsorship Program by CAST(2023QNRC001).",
  "Katherine Atwell, Sabit Hassan, and Malihe Alikhani.2022. Appdia: A discourse-aware transformer-basedstyle transfer model for offensive social media con-versations. arXiv preprint arXiv:2209.08207": "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts,Paul Barham, Hyung Won Chung, Charles Sutton,Sebastian Gehrmann, et al. 2022. Palm: Scalinglanguage modeling with pathways. arXiv preprintarXiv:2204.02311. Hyung Won Chung, Le Hou, Shayne Longpre, Bar-ret Zoph, Yi Tay, William Fedus, Eric Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, et al.2022. Scaling instruction-finetuned language models.arXiv preprint arXiv:2210.11416. David Dale, Anton Voronov, Daryna Dementieva, Var-vara Logacheva, Olga Kozlova, Nikita Semenov, andAlexander Panchenko. 2021. Text detoxification us-ing large pre-trained neural models. arXiv preprintarXiv:2109.08914. DarynaDementieva,SergeyUstyantsev,DavidDale, Olga Kozlova, Nikita Semenov, AlexanderPanchenko, and Varvara Logacheva. 2021. Crowd-sourcing of parallel corpora: the case of style transferfor detoxification. In Proceedings of the 2nd CrowdScience Workshop: Trust, Ethics, and Excellence inCrowdsourced Data Management at Scale co-locatedwith 47th International Conference on Very LargeData Bases (VLDB 2021 ( org/2021/)),pages 3549.",
  "Edward J Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,and Weizhu Chen. 2021.Lora: Low-rank adap-tation of large language models.arXiv preprintarXiv:2106.09685": "Xinshuo Hu, Dongfang Li, Zihao Zheng, Zhenyu Liu,Baotian Hu, and Min Zhang. 2023. Separate thewheat from the chaff: Model deficiency unlearn-ing via parameter-efficient module operation. arXivpreprint arXiv:2308.08090. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023. Mistral7b. arXiv preprint arXiv:2310.06825.",
  "Kevin Lin, Christopher Agia, Toki Migimatsu, MarcoPavone, and Jeannette Bohg. 2023. Text2motion:From natural language instructions to feasible plans.arXiv preprint arXiv:2303.12153": "Alisa Liu,Maarten Sap,Ximing Lu,SwabhaSwayamdipta, Chandra Bhagavatula, Noah A Smith,and Yejin Choi. 2021. Dexperts: Decoding-time con-trolled text generation with experts and anti-experts.arXiv preprint arXiv:2105.03023. Varvara Logacheva,Daryna Dementieva,SergeyUstyantsev, Daniil Moskovskiy, David Dale, IrinaKrotova, Nikita Semenov, and Alexander Panchenko.2022. Paradetox: Detoxification with parallel data.In Proceedings of the 60th Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 68046818. Katharina Lhr, Frieder Graef, Michelle Bonatti,Henry F Mahoo, Jane Wambura, and Stefan Sieber.2017. Conflict management systems for large sci-entific research projects. International Journal ofConflict Management, 28(3):322345.",
  "Yoon A Park and Frank Rudzicz. 2022. Detoxifyinglanguage models with a toxic corpus. LTEDI 2022,page 41": "Mohammad Mahdi Abdollah Pour, Parsa Farinneya,Manasa Bharadwaj, Nikhil Verma, Ali Pesarang-hader, and Scott Sanner. 2023. COUNT: COntrastiveUNlikelihood text style transfer for text detoxifica-tion. In Findings of the Association for Computa-tional Linguistics: EMNLP 2023, pages 86588666,Singapore. Association for Computational Linguis-tics. Shrimai Prabhumoye, Mostofa Patwary, MohammadShoeybi, and Bryan Catanzaro. 2023. Adding in-structions during pretraining: Effective way of con-trolling toxicity in language models. arXiv preprintarXiv:2302.07388.",
  "Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:Sentence embeddings using siamese bert-networks.arXiv preprint arXiv:1908.10084": "Timo Schick, Sahana Udupa, and Hinrich Schtze. 2021.Self-diagnosis and self-debiasing: A proposal for re-ducing corpus-based bias in nlp. Transactions of theAssociation for Computational Linguistics, 9:14081424. Stefan F Schouten, Baran Barbarestani, Wondimagegn-hue Tufa, Piek Vossen, and Ilia Markov. 2023. Cross-domain toxic spans detection. In International Con-ference on Applications of Natural Language to In-formation Systems, pages 533545. Springer. Omar Shaikh, Hongxin Zhang, William Held, MichaelBernstein, and Diyi Yang. 2022. On second thought,lets not think step by step! bias and toxicity in zero-shot reasoning. arXiv preprint arXiv:2212.08061.",
  "Albert Xu, Eshaan Pathak, Eric Wallace, Suchin Guru-rangan, Maarten Sap, and Dan Klein. 2021. Detoxi-fying language models risks marginalizing minorityvoices. arXiv preprint arXiv:2104.06390": "Canwen Xu, Zexue He, Zhankui He, and JulianMcAuley. 2022. Leashing the inner demons: Self-detoxification for language models. In Proceedingsof the AAAI Conference on Artificial Intelligence,volume 36, pages 1153011537. Mian Zhang, Lifeng Jin, Linfeng Song, Haitao Mi, Wen-liang Chen, and Dong Yu. 2023. Safeconv: Explain-ing and correcting conversational unsafe behavior. InProceedings of the 61st Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 2235.",
  "A.2Illustration of PerspectiveAPI": "PerspectiveAPI can score the toxicity of the toxicsegments in the given text.We leverage Per-spectiveAPI to pinpoint specific segments of theinput that are flagged as potentially toxic.Inthe model training phase, these segments, whichhave a toxicity score greater than a predefinedthreshold (i.e., 0.5), are considered toxic partsthat need to be masked by the sentinel token\"[MASK]\". For example, for the toxic contextWhat are you motherfckers going to do aboutthe..., the PerspectiveAPI can detect the toxic seg-ment motherfckers in the context. Then, sincethe toxic score of motherfckers is greater than0.5, we replace this segment with [MASK].",
  "In-Context Templates for Toxic Detection": "Demonstration 1# Input: None of this would be possible if wedidnt have a fucked up# Output: fucked upDemonstration 2# Input: Thus when shit hits the fan insects willprove to be a# Output: shitDemonstration 3# Input: It should be noted, since idiots alwaysmisunderstand this point, that freedom# Output: idiotsDemonstration 4# Input: For five years she was on and off witha sexist pig who would make her# Output: sexist pig",
  "C.2Design of Segment-CNN Model": "We apply the Segment-CNN model only for detect-ing the toxic segments within the context duringthe Dataset Synthesis phase. After training withthe CMD framework, LLMs can self-detoxifywithout the Segment-CNN model.The architecture of Segment-CNN model isshown in , where the sentence-level toxicityscore Sglobal is obtained from a global feature ex-tractor G and segment-level toxicity score Sspanis obtained from a 1-D CNN model C (Krizhevskyet al., 2017) followed by a local feature extractorF. As for training, given one context containingn segments x = {x1, , xn} and the stride ofthe convolutional kernel k, the loss function can bewritten as:",
  "i=1iCE(F(Ck(G(xi))), S(label)spani ),": "where i is the re-weighting hyper-parameter foreach segment xi, CE denotes the cross-entropyloss.Specifically, we set i = 1 for non-toxic spansand i = 2 for toxic segments. We calculate thetoxicity scores of S(label)global and S(label)spani with Perspec-tiveAPI and employ the data augmentation by ran-domly inserting toxic segments into each trainingsample to improve the classification accuracy for",
  "In-Context Templates for Toxic Detoxification": "Demonstration 1# Input: None of this would be possible if wedidnt have a fucked up# Output: None of this would be possible if wedidnt have theDemonstration 2# Input: Thus when shit hits the fan insects willprove to be a# Output: Thus the fan insects will prove to beaDemonstration 3# Input: It should be noted, since idiots alwaysmisunderstand this point, that freedom# Output: It should be noted, since people al-ways misunderstand this point, that freedomDemonstration 4# Input: How the fuck is this guy a psychiatristwhen# Output: How can you call this guy a psychia-trist when",
  "D.2Data Processing Details": "We list statistics of all the training and testing datain Tab. 8. Specifically, to evaluate the toxicity clas-sification capability of LLMs, we sample 3,000toxic entries and 3,000 non-toxic entries from theJigSaw dataset and combine them as the toxicityclassification testing data. To construct the CMDsynthesis data, we first sample 15,000 toxic entries",
  "(f) Toxicity of detoxification methodscondition on toxicity distribution": ": Full evaluation results when feeding models with different contexts (toxic and safe), where (a) showsthe SIM score between the context and output texts and (d) illustrates the performance of different detoxificationmethods and LLMs. As for the other four figures, we utilize line charts and histograms to represent the performanceof models fed with original context and corresponding safe context respectively. from the REALTOXICPROMPT dataset according tothe semantic similarity. Subsequently, we filter out10,000 of these data based on their perplexity as thetoxic portion. In addition, we incorporate 5,000 en-tries into the data as the non-toxic portion. For thetesting set, we randomly selected 10,000 entrieswith a toxic to non-toxic ratio of 9:1, consistentwith the original datasets distribution.",
  "E.1Human Evaluation": "We show the human evaluation interface in a,which is built with the open-source Python web li-brary Django 12. To ensure consistency amongnine annotators, we report the Fleiss kappascore (Fleiss, 1971) in Tab. 9, and we can observe that all the inter-annotator agreements are substan-tially consistent ( [0.6, 1]). As shown in Fig-ure 11b, during the evaluation, each comparisonpair contains one prompt and two correspondingoutputs generated from two different models. Theannotator is allowed to choose \"Tie\" if it is hard todistinguish two generation cases. We can ensurethat each annotator is independent during their an-notation process and the total annotation process isfair. We paid each annotator $ 0.05 for comparingeach pair. The payment is reasonable, consideringthat it would take an average of 30 seconds for anannotator to finish a comparison.",
  "E.2More Experiments & Evaluation Metrics": "Expand CMD to Parallel Detoxification TaskIn addition to conducting the experiments on thetext detoxification task, we also expand the CMDframework to parallel detoxification task and com-pare CMD with Paradetox (Logacheva et al., 2022)and COUNT (Pour et al., 2023) methods. Specifi-cally, we select Para-detox (Logacheva et al., 2022)and APPDIA (Atwell et al., 2022) datasets for train-ing and evaluation. Following (Logacheva et al.,2022; Pour et al., 2023), we report the BLUE,Style, SIM (Wieting et al., 2019) and Fluencyscore (Warstadt et al., 2019) in Tab. 11. We canobserve that our CMD method can still achieve thebest performance. Discussion of Evaluation Metrics on Detoxifica-tion TaskAs shown in Tab. 10, apart from thePerplexity (PPL) score reported in Tab. 2, Tab. 3, we also evaluate the text quality with Fluencyscore (Warstadt et al., 2019), where CMD frame-work still achieves the best performance.It is worth noting that we also consider otherevaluation metrics to reflect the text quality fromtwo aspects:",
  ": Fluency score among different detoxificationmethods": "We observe that Diversity metrics can some-times correlate with unreadable or chaotic textgeneration, which is counterproductive to ourgoal of producing coherent and safe content(shown in Fig 12 and 13). This observationis particularly evident in previous detoxifica-tion works such as DExperts and Gedi, whichprioritize detoxification effectiveness over thequality of the generated text. Semantic Similarity that reflect the semanticsimilarity between generation and prompt: wefind there is a tendency for higher semanticsimilarity between the generated text and thetoxic context to result in lower quality andhigher toxicity (as illustrated in . Asfor evaluation metric like BERTScore (Zhanget al., 2019), which measures the semanticoverlap between the generated text and theoriginal text, it may not be ideal in this sce-nario since it could inadvertently reward se-mantic similarities that are detrimental to thedetoxification process. Given these findings, we believe that there issignificant room for improvement in the selectionand development of evaluation metrics for detox-ification tasks. We acknowledge the challenge offinding metrics that accurately reflect the balancebetween detoxification and text quality, especiallywhen dealing with toxic contexts that are not idealreferences. We have also discussed these points inthe limitations section of our paper, emphasizingthe need for more nuanced and task-specific evalua-tion methods that can better capture the essence ofdetoxification effectiveness without compromisingthe quality of the generated content.",
  "GModel Detoxification Pipeline": "We follow the CMD framework to divide the ModelDetoxification Pipeline into three steps: toxic seg-ment detection, toxic segment detoxification, andcontinual generation. We first use PerspectiveAPIto detect the toxic segments of context, which is thereason that it can achieve 100% detection accuracy.Furthermore, we paraphrase the toxic context to en-sure safety. Specifically, we design two pipelines,where pipeline1 detects the toxic segment withSegment-CNN model and paraphrases the detectedtoxic segment with GENIUS; pipeline2 employsParaGedi (Dale et al., 2021) to detoxify the con-text. After context detoxification, we feed the safecontext to LLMs to continually generate 20 tokens.We show the generated results in .",
  "HData Synthesis with ChatGPT": "Specifically, we adopt gpt-3.5-turbo-instructmodel13 (Ouyang et al., 2022) and carefully de-sign the prompts to synthesize data for each sub-step. We list all the designed prompts for eachsub-step in and show the comparison be-tween the data from CMD and ChatGPT in .After obtaining all the data produced from thegpt-3.5-turbo-instruct model, we filter outtexts that are too long (more than 64 tokens) ortoo short (less than 20 tokens). We can observe thatthe data produced from ChatGPT exhibits a higherquality than the CMD framework due to the stronggeneration capability of ChatGPT. In addition, asshown in , we sample the case generatedfrom the model trained with CMD and ChatGPT.",
  "CMD framework:": "# He told me it was a piece of shit and that he was disgusted # Lets detoxify step by step. # Thecontext is toxic. # Detect the toxic segment within the context: he told me it was a piece <MASK>that he was disgusted # Detoxify the toxic segments: he told me it was a piece of item that he wasdisgusted # Generation: with and that he thought it was inappropriate.",
  "Comparison between CMD and Pipelines": "CMD Framework:# Text: All the bad shit that any person with a Wikipedia page has ever done even if they have not yetbeen proven # Lets detoxify step by step. # The context is toxic. # Detect the toxic segment withinthe context: all <MASK> any person with a wiki page has ever done even if they have not yet beenproven # Detoxify the toxic segments: all the bad things any person with a wiki page has ever doneeven if they have not yet been proven # Generation: guilty of any crimes or wrongdoing. For example,the Wiki article \"The FBIs Operation Pipeline 1 (Segment-CNN+GENIUS):Context: All the bad shit that any person with a Wikipedia page has ever done even if they have notyet been provenParaphrase: All the things any person with a Wikipedia page has ever done. even if they have not yetbeen provenOutput: to have done something that could be considered a violation of the code of conduct. but whatthe fuck Pipeline 2 (ParaGedi):Context: All the bad shit that any person with a Wikipedia page has ever done even if they have notyet been provenParaphrase: All the bad work that any Wikipedia page had ever done even though they have notOutput: done anything. It makes the rest of Wikipedia look like a bunch of idiots , and even worse,",
  "Prompts for Data Synthesis with ChatGPT": "Toxic Segment Detection:Judge if the given context contains toxicity. If toxic, find the toxic portions of the given context andthose places with [MASK] token; finally, return Toxic following with the masked context. Ifnon-toxic, return Safe following with the original context. Just return the (masked) context withoutany extra context or explanation. {Original Context}",
  "Synthesis Data from Different Models": "CMD Framework# What are you motherf*ckers going to do about the # Lets detoxify step by step. # The context istoxic. # Detect the toxic segment within the context: What are you <MASK> going to do about the #Detoxify the toxic segments: What are you going to do about the # Generation: people who are onwelfare? \"One man, who asked not to be named, said he ChatGPT# Text: What are you motherf*ckers going to do about the # Lets detoxify step by step. # The contextis toxic. # Detect the toxic segment within the context: What are you <MASK> going to do about the# Detoxify the toxic segments: What are you folks going to do about the # Generation: mysteriousdevice found at the heart of the ancient ruins? Discovered by a group of intrepid explorers, this device,made of an unknown metal and covered in glowing runes",
  "Generation Results": "Generation from CMD Framework# Im getting really tired of this bullshit to the point where I dont care if you # Lets detoxify step bystep. # The context is toxic. # Detect the toxic segment within the content: Im getting really tired of<MASK> the point where i dont care if you # Detoxify the toxic segments: Im getting really tired ofit, the point where i dont care if you # Generation: know it. Its been a while since I posted on thisblog. Ive Generation from ChatGPT# Im getting really tired of this bullshit to the point where I dont care if you # Lets detoxify step bystep. # The context is toxic. # Detect the toxic segment within the content: Im getting really tired ofthis <MASK> to the point where I dont care if you # Detoxify the toxic segments: Im getting reallytired of this situation to the point where I dont care if you # Generation: leave me. Its gotten to thepoint where I cant even be bothered to find a way out. Youre"
}