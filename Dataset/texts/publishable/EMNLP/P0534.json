{
  "Warning: This paper contains content that maybe offensive or upsetting": "We address the task of detecting abusive sen-tences in which identity groups are depictedas deviating from the norm (e.g. Gays sprinkleflour over their gardens for good luck). Theseabusive utterances need not be stereotypes ornegative in sentiment. For this type of abuse,we are the first to present a study on how todetect it. We introduce datasets for this taskcreated via crowdsourcing that include 7 differ-ent identity groups. We also report on classi-fication experiments and show that only largelanguage models detect this abuse reliably.",
  "(1) stop editing this, you dumbass.(2) you stupid fucking idiot, fucking kill yourself": "Closely related terms, e.g. cyber bullying (Zhonget al., 2016) or hate speech (Waseem and Hovy,2016), are compatible with the definition above.Due to the rise of abusive language online, acommon application is content moderation onsocial media, where NLP methods guide humanreviews to the most relevant microposts.The detection of implicitly abusive language(Waseem et al., 2017), i.e. abusive language notconveyed by unambiguously abusive words (e.g.scum, tosser), remains challenging (van Aken et al.,2018; Wiegand et al., 2021b; Ocampo et al., 2023):",
  "(3) Did Stevie Wonder choose these models?(4) You look like the back end of a bus": "Implicitly abusive language often targets iden-tity groups, i.e. groups of people united by a com-mon characteristic, fostering a sense of community(e.g. Jews, gay people, women etc.). Abusive lan-guage towards such groups can often be regardedas a form of othering, i.e. a means of stigmatizingthe target as not fitting in within the norms of asocial group (Burnap and Williams, 2016).A common form of stigmatizing identity groupsis by imposing negative stereotypes on them:",
  "(5) Hispanics are lazy people.(6) Muslims are terrorists": "Recent datasets on implicitly abusive languagecover instantiations of such stereotypes fairly well(Sap et al., 2020; ElSherief et al., 2021; Vidgenet al., 2021a,b; Hartvigsen et al., 2022; Wiegandet al., 2022). In this paper, we focus on the detec-tion of a subset of abusive instances of otheringthat do not coincide with negative sentiment andmay not even necessarily represent stereotypes: (7) Gays sprinkle flour over their gardens for good luck.(8) Some Muslims go to the barber for armpit haircuts.(9) Asians do not use banks.(10) Women do not get horny.(11) Black people occasionally wear pyjamas in public. (7)-(11) depict the identity groups as deviatingfrom general norms of modern-day society. Suchnorm-contraventions are frequently perceivedas abusive by the targeted identity groups. Aclassifier specializing in detecting this form ofabuse could supplement existing classifiers for abu-sive language detection since, as we will show, thelatter are unable to detect this type of abuse reliably. In order to quantify the phenomenon, we manu-ally annotated the abuse directed at identity groupson the dataset for implicit abuse by Ocampo et al.(2023), which is a union of 7 previous datasets.65% of the (declarative) sentences can be consid-ered instances of othering. While only 5% of allabusive instances lack negative sentiment, in 80%of these cases the identity groups are depicted asdeviating from the norm. Thus, deviating fromthe norm represents a prominent subset of thedifficult non-negative abusive sentences. Com-pared to the figures reported by Wiegand et al.(2021b) on other subtypes of implicit abuse, thisphenomenon is similarly frequent as dehumaniza-tion, euphemisms or comparisons, all of whichhave previously been examined (Mendelsohn et al.,2020; Wiegand et al., 2021a, 2023).We also had crowdworkers, all native Englishspeakers without specific backgrounds, comparethe severity of different examples of implicit abuse(20 examples for each type). The examples werepresented in pairs without revealing their types.Crowdworkers had to decide which example theyconsidered more severe. Overall, our novel typeof abuse was judged even more severe than eu-phemisms or comparisons.2 Since our above sample is too small for a properstudy we created 2 new English datasets: Thefirst represents true-to-life examples extracted fromTwitter.The second comprises sentences con-structed by crowdworkers. Due to ethical concerns,the crowdworkers are not asked to form sentencestargeting a specific identity group but an unspe-cific group of people represented by a 3rd personpronoun (e.g. they). We then instantiate these pro-nouns with identity groups and have the resultingsentences validated as abusive language by othercrowdworkers. Thus, we establish that depicting agroup of people as deviating from the norm is a gen-eral property of implicitly abusive language thatcan be observed across different identity groups.We focus on sentences that can be interpretedwithout any additional context. The task is a bi-nary (sentence-level) classification problem inwhich norm-compliant behaviour is to be distin-guished from norm-contravening behaviour. Wealso report on classification experiments and showthat this task benefits from recent language models.Our contributions are the following:",
  "Related Work": "Previous work on abusive language mostly followsa one-size-fits-all approach (Nobata et al., 2016;Badjatiya et al., 2017; Fortuna and Nunes, 2018).Surveys on existing datasets do not address im-plicit abuse (Vidgen and Derczynski, 2020; Polettoet al., 2021). However, the recent roadmap on im-plicit abuse by Wiegand et al. (2021b) identifiedas subtypes: dehumanization (Mendelsohn et al.,2020), call for action, multimodal abuse (Kielaet al., 2020), comparisons (Wiegand et al., 2021a),euphemistic abuse (Wiegand et al., 2023) and abusetowards identity groups (Hartvigsen et al., 2022).Our work aligns with the last subtype. Further sub-types are jibes (Sodhi et al., 2021), sarcasm andwhite grievance (ElSherief et al., 2021).There has already been previous research re-lated to othering beyond stereotypes: Burnap andWilliams (2016) and Alorainy et al. (2019) analyzethe juxtaposition of 1st and 3rd person pronouns tocontrast the norm (us) with identity groups (them).Wiegand et al. (2022) examine non-conformistviews, sentences expressing negative sentiment to-wards targeted groups. Our focus on othering dif-fers as it is not limited to specific lexical items, likepronouns, nor solely to negative sentiment.Our task is also related to framing (Mendelsohnet al., 2021; Ali and Hassan, 2022) since the pre-sentation of identity groups selects aspects of aperceived reality and makes them more salient in acommunicating text (Entman, 1993). These aspectsdo not have to apply to the identity groups in reality.Thus, they can also be considered misinformation(Zhou and Zafarani, 2020; Guo et al., 2022).Our work is also anchored in social psychol-ogy: Lindstrm et al. (2017) observed that whatis common (=norm-compliance) is often regardedas moral and that rare positive behaviour, e.g. al-truism, is judged less moral than common posi-tive behaviour. Our work echoes this sentiment, suggesting that norm-compliance is perceived posi-tively, while deviation from norms is viewed nega-tive. Leary (2000) and Tangney and Dearing (2002)find that guilt and shame are social emotions typ-ically experienced when individuals transgress a(social) norm. The audience of sentences depictingnorm-contravention may likewise associate similarnegative emotions with these utterances, potentiallyleading to strong disapproval. People are often mo-tivated to punish those who violate societal norms,even if they are not personally affected by theseviolations (Fehr and Fischbacher, 2004; Buckholtzand Marois, 2012). Authors who engage in abu-sive language targeting norm violations may aimto trigger this punitive reflex in their audience. Af-ter all, call for action is a typical characteristic ofimplicitly abusive language. Finally, the fact thatidentity groups are the target of abusive sentencesdisplaying norm-contravention might be explainedby the fact that these groups are also sanctionedmore frequently for norm violations than others(Wolbring et al., 2013; Winter and Zhang, 2018).",
  "Data": "Our new data represents a subtype of abuse thatdepicts identity groups as deviating from the norm.This is a form of othering. The utterances may bestereotypes but they do not have to be. They mayalso coincide with examples of framing or misin-formation. We do not consider abusive utterancesthat are explicitly negative in sentiment. Such senti-ment, which is conveyed by words unambiguouslynegative in meaning,3 e.g. poor or sad, has beendealt with in previous work (Wiegand et al., 2022).We create 2 datasets: Due to the rareness of thisphenomenon, we created a constructed dataset,i.e. a dataset in which sentences are invented (3.1).In this way, we can cover various areas of life. How-ever, since that dataset does not necessarily reflecttexts in social media, we also produce a Twitterdataset comprising attested sentences (3.2).4 The annotation of all datasets was produced viacrowdsourcing. As a platform we used Prolificacademic.5 We did not specifically target any par-ticular profession, age group, gender or ethnicityfor our sample. We randomly sampled crowdwork- 3More information on sentiment is given in Appendix B.3.4We consciously avoided sampling sentences from SO-CIAL-CHEM-101 (Forbes et al., 2020) as that work is stronglyrelated to Moral Foundations (Haidt, 2012) and the predomi-nant norms in the dataset demonstrate explicit sentiment.5www.prolific.com ers from the pool of available crowdworkers onProlific who reside in Western countries (i.e. USA,UK, Ireland, Australia and Canada) and made surethat English is their first language. We focusedon crowdworkers in Western countries becauseour work exclusively examines behaviors that arecompliant with or contrary to Western norms. Allcrowdworkers had to have an overall approval rateof 100%. Our annotation tasks were divided intosmaller segments for the crowdworkers, such ascreating 30 sentences or evaluating the labels of100 sentences. This approach led to the participa-tion of a diverse group of over 100 crowdworkers.Considering the large number of participants, theplatforms extensive base of several thousand na-tive English speakers in Western countries, and theabsence of further demographic restrictions, weare confident that our ratings offer a representativecross-section of Western society.",
  "Constructed Dataset": "Following previous work on creating a dataset fora rare subtype of implicit abuse (Wiegand et al.,2021a), we asked crowdworkers to invent instancesof our targeted phenomenon, i.e. behaviour or prop-erties that deviate from the norm. However, dueto ethical reasons we did not ask them to think ofidentity groups but an unspecific group of peoplethat should be referred to by a 3rd person pluralpronoun. Therefore, the resulting sentences do notattack any identity group. Subsequently, we instan-tiated the pronoun with specific identity groups.Our dataset is created through a series of individ-ual tasks. In each task, a single crowdworker hadto invent about 30 sentences or judge the label ofabout 100 sentences. illustrates the orderof those tasks which we also describe in the follow-ing. We repeatedly ran through this pipeline untilno more new sentences were obtained. Norm-Contravening Sentences.Crowd-workers were asked to invent sentences in which ageneric group of people (represented by a 3rd per-son plural pronoun) was depicted as displaying abehaviour or property that deviates from the norm.By norm we understand behaviors or situationsdeemed typical within modern-day society, witha particular focus on Western societies, as this cul-tural backdrop is predominant in the English lan-guage data we are using. Our attention is on socialnorms (Wear black to a funeral) and conventions(Follow the rules of English grammar) rather thanmoral or legal norms (Elster, 2007), e.g. honesty or",
  ": Illustration of how the constructed dataset (i.e. norm-compliance dataset and its 7 variants) is created": "justice. In other words, we are interested in normswhose violation might at most result in shame butnot in blame (Malle et al., 2014) since in our ex-ploratory experiments we observed that the latteroften coincides with explicit sentiment, which weavoid in this work as stated above. Filtering. The sentences produced by thecrowdworkers required manual filtering by one co-author. This involved removing near duplicates andoccasional cases of explicitly negative sentiment. Norm-Compliant Counterparts. For eachnorm-contravening sentence, one co-author man-ually created a sentence in which the depicted be-haviour or property follows the norm. We refer tothis as norm-compliant sentences. These instanceswere created as contrast sets (Gardner et al., 2020;Li et al., 2020; Sen et al., 2022), i.e. sentences thatare structurally similar to the norm-contraveningsentences. Thus, we obtain a difficult dataset inwhich norm-compliant and norm-contravening sen-tences are hard to distinguish from each other. Byhaving a co-author rather than crowdworkers createthose sentences, we follow Gardner et al. (2020)who recommend such data to be created by experts. Debiasing.By computing the PointwiseMutual Information between words and the twoclasses of our norm-compliance dataset, i.e. norm-compliant and norm-contravening, we identified aset of spurious correlations (Ramponi and Tonelli,2022). Most of them were caused by the waywe created norm-compliant sentences: In orderto change a norm-contravening sentence (12) mini-mally to a norm-compliant one, often simply someadverbial was removed or added (13). As a re-sult, words, such as rarely, were biased towardsthe class norm-compliant. However, these wordsshould not be predictive for this class, as norm-",
  "contravening sentences, such as (14), are equallypossible. Therefore, we replaced these sentences byother sentences not containing these words (15).6": "(12) They wash clothing by hand. (norm-contravening)(13) They rarely wash clothing by hand. (norm-compliant)(14) They rarely wear shoes outside. (norm-contravening)(15) They wash clothing in washing machines. (norm-compliant) Norm-Compliance Validation. 5 differentcrowdworkers were asked to validate whether agiven sentence represents a behaviour or propertythat deviates from the Western norm. Crowdwork-ers could also label a sentence as not being properEnglish. Only sentences in which the majorityagreed on a label were used for further processing. Instantiation. We produced 7 variants ofeach sentence in which we replace the 3rd per-son pronouns by identity groups that represent fre-quent targets of abusive language, i.e. Asians, Blackpeople, gay people, Hispanics, Jews, Muslims andwomen. Abusiveness Validation.Another set ofcrowdworkers were to rate the given instanti-ated sentences, both norm-compliant and norm-contravening, as anti-Semitic, homophobic, Islamo-phobic, racist, sexist or not abusive. Only crowd-workers belonging to the identity group men-tioned in a given sentence were to rate that sen-tence. Often, the affected identity groups are themost competent to detect this type of abusive lan-guage (Pei and Jurgens, 2023). The crowdworkerscould also flag a sentence as improbable if theyconsidered it unlikely to be found on the Web. Asentence was excluded as soon as one crowdworkerflagged it as improbable. The final label corre-sponds to the majority of the 5 crowdworkers.",
  ": Statistics on the constructed dataset": "The Final Dataset. provides a statisticon the norm-compliance dataset and its variants.The size of the variants is smaller than the norm-compliance dataset as many instantiated sentenceswere judged improbable and thus removed. also lists for each variant the correspondenceof class labels to the norm-compliance dataset, i.e.we ascertain the proportion of sentences labeledas abusive (e.g. racist, sexist) originally labeled asnorm-contravening, and sentences labeled as notabusive originally labeled as norm-compliant. Onaverage, these labels correspond in 85.7% of thesentences. This indicates that the clear majority ofsentences in which an identity group is depicted asdeviating from the norm is judged abusive. lists the proportion of the areas of life(established via manual annotation) that are cov-ered in our norm-compliance dataset illustratingthe diversity of the dataset.A random sample of 200 sentences of each partof our dataset was also annotated by one co-author.We compared these labels with the crowdworkersmajority vote. Though the co-author does not be-long to any of the 7 identity groups, we still got asubstantial agreement (Landis and Koch, 1977).7",
  ": Statistics on the Twitter dataset": "3.1 is mentioned. We searched on the Twitterhistory rather than fetching tweets that are currentlystreamed. This was done since, for several of ouridentity groups, we were only able to find a verysmall number of distinct tweets (i.e. only a few)that met the restrictions we formulated (as outlinedbelow) within a reasonable time frame (e.g. a fewweeks).In order to be in line with the sentences repre-senting implicitly abusive language from our con-structed dataset (3.1), the sentences from Twitterwere not to contain any explicit abuse (e.g. slurs)or explicit sentiment.Following the observation by Wiegand et al. (2022) that the overwhelming number of abusive re-marks on identity groups realize the identity groupas the agent (i.e. logical subject) of some predicate(e.g. full verb), we used queries that extracted suchsentences. This is typically achieved by using apattern identity_group adverb as in Jewstypically, Jews only, Jews rarely etc. Dependingon the particular query, we obtained up to severalhundred unique tweets which were subsequentlyannotated via crowdsourcing with respect to norm-compliance and abusiveness ().We also focused only on sentences that can beunderstood out of context. Typical situations inwhich this is not the case are:",
  "The tweet can only be understood by knowing somebackground information on the author (e.g. specific de-mographic information)": "Our Twitter dataset only comprises sentencesrather than complete tweets. To protect peoples pri-vacy, mentions of both usernames and real nameswere removed from the dataset. This removal pro-cess was conducted manually to ensure compre-hensive detection and exclusion of such mentions.We did not substitute those mentions; rather, weentirely removed them.All the above restrictions reduced the size of oursample by about 70%. Each sentence was rated by5 crowdworkers that belong to the identity groupmentioned. The label inventory corresponds to thatof the constructed dataset. The final labels corre-spond to the majority vote. provides somestatistics on the resulting dataset. Its small size(1000 instances) can be explained by the fact thatwe consider a rare phenomenon and by Twittersintensive efforts to remove hate speech.This dataset is primarily created to study thedetection of implicit abuse. It is not used to studythe categorization of norm-compliance per se.On a random sample of 200 sentences, we alsomeasured a substantial agreement of =0.65 be-tween one co-author and the majority vote.",
  "Set-Up for Transformers": "Two transformers are used as learning methods:BERT (bert-base-uncased) (Devlin et al.,2019) and DeBERTa (deberta-large) (Heaet al., 2021). We compare BERT, a foundationalmodel, with DeBERTa, which introduces advancedarchitecture and benefits from more extensive train-ing data. We fine-tune the pretrained models on thegiven training data using the FLAIR-framework(Akbik et al., 2019) with the hyperparameter set-tings from Wiegand et al. (2022), a study closelyrelated to ours. We always report the average over5 training runs (+ standard deviation). Appendix Acontains details on the settings of all classifiers.We also use large language models, such as GPT-4 (OpenAI et al., 2024), for getting state-of-the-arttext completions as outlined in the following.",
  "Classifiers not Trained on Our Dataset": "Sentiment Analysis.Norm-contravening be-haviours and properties may sometimes be per-ceived in a negative way. This suggests that theclass norm-contravening may bear some relationtowards negative sentiment. As stated in 3, werefrained from including explicitly negative senti-ment in our dataset since such utterances are suffi-ciently represented in previous datasets. However,there are still utterances in our datasets that conveyimplicitly negative sentiment (Deng et al., 2013;Ding and Riloff, 2018; Zhou et al., 2021), e.g. (16)or (17). (Based on our manual inspection of thedata we estimate 17% of the sentences to conveyan implicitly negative sentiment.) As a sentimentclassifier to detect all instances of negative senti-ment in our dataset, we use TweetEval (Barbieriet al., 2020). Predictions of such sentiment are con-sidered a proxy of norm-contravening sentences.",
  "(16) They urinate in the sink.(17) They did not finish high school": "GPT-4::zero-shot. We use a prompt asking agiven sentence to be classified directly by GPT-4 as exemplified by (18) and (19). We interpreta completion beginning with Yes as a predictionfor class norm-compliant and one beginning withNo for class norm-contravening. We examine 2prompts that vary in specificity () since al-ready minor variations in prompts are known tocause notably different completions (Zhang et al.,2021).",
  "Within-Dataset Classifiers": "As classifiers directly trained on our dataset, we donot only fine-tune BERT and DeBERTa (4) butwe also use logistic regression trained on a bag ofwords, i.e. a classifier that only draws knowledgefrom lexical items observed in the training data.Knowledge Base. We assume that what is con-sidered norm-compliant should also be found inlarge general-purpose knowledge bases, to someextent. Therefore, we also implemented a base-line using ConceptNet (Speer et al., 2017). Foreach sentence, we extract concepts from Concept-Net with CoCo-Ex (Becker et al., 2021) whichare converted into a ConceptNet vector ensemble(Speer et al., 2017). We average over these em-beddings and concatenate them with embeddingsfrom DeBERTa, i.e. our best transformer, for eachsentence. We train a feedforward neural networkon our dataset where all sentences are representedby the above embeddings in FLAIR. The test dataof our dataset are represented in the same fashion.GPT-4::aug. In our last method, we augmenteach sentence from our dataset with the respectivecompletion obtained from the zero-shot approach(5.1). Therefore, the resulting dataset maintainsthe original amount of instances, however, eachinstance consists of the original sentence and thecompletion. We then fine-tune and test a trans-former on these augmented instances. Learning onthe text augmented by the GPT-4 completions maygive a classifier additional helpful clues.Given that this augmentation process results ininstances possessing a greater textual length thanthe original, we also investigate whether the classi-fiers performance improvement is merely a func-tion of longer text inputs. To address this, we gen-erate a control configuration in which we augmenteach original sentence by some paraphrase so thatthe resulting text matches the length of the aboveaugmented instances. Our prompt for creating aparaphrase (i.e. merely a hyphen) follows the spec-ification from Wiegand et al. (2023).",
  "majority-class classifier26.4 50.0 34.5": "log. regr. trained on norm-compliance dataset 48.8 48.9 48.8Sentiment Analysis (TweetEval)51.9 51.4 51.7Knowledge Base (ConceptNet)64.6 64.4 64.5LLaMA-2 (short prompt)73.3 58.6 65.1LLaMA-2 (long prompt)74.7 60.4 66.8BERT trained on norm-compliance dataset68.7 68.7 68.7 (0.5)DeBERTa trained on norm-compliance dataset 83.4 83.4 83.4 (0.4)GPT-4::zero-shot (short prompt)84.3 83.1 83.7DeBERTa trained on GPT-4::aug (control)85.6 85.6 85.6 (0.8)GPT-4::zero-shot (long prompt)86.7 85.0 85.8DeBERTa trained on GPT-4::aug (long pr.)93.3 93.3 93.3 (0.1)",
  ": Classification between norm-compliant andnorm-contravening sentences on the constructed norm-compliance dataset (: see )": "of one individual annotator was randomly sampledfrom the crowdsourced gold-standard annotation. shows that our task requires models thatincorporate world knowledge in addition to labeledtraining data. Logistic regression performs poorly.So does sentiment analysis. Plain language modelsperform notably better. Specifically, DeBERTa, themore sophisticated model, outperforms BERT.The zero-shot classifiers using GPT-4 producehigh scores. They outperform the classifiers basedon LLaMA-2. A longer prompt () is moreeffective than a shorter one probably since the latterlacks specificity as to the context of the norm.The best performance is obtained by fine-tuningDeBERTa on training data augmented by GPT-4. The control configuration of the augmentationis significantly worse than the one using a longprompt. This suggests that improving performancedue to text augmentation depends on adding pre-dictive textual information. By manually inspect-ing the completions of the long prompt by GPT-4(), we found that for utterances involvingnegation, which in our dataset represent 34% ofthe instances, the model does not choose one scopeconsistently. For example, in (20) the completionconsiders the wide scope that includes the negationof the sentence to classify, while in (21) it consid-ers a narrow scope in which the negation is notincluded. GPT-4::zero-shot derives a wrong cate-gorization from (21), since that classifier assumesthe wide scope. Being trained on the concatena-tion of the original sentence and the completion,GPT-4::aug is able to learn what type of scope anindividual completion replies to. Thus, GPT-4::augcan be much more accurate than GPT-4::zero-shot.",
  "Difference of Norm-Compliant Sentences": "We now demonstrate that our proposed methodto produce norm-compliant sentences by compos-ing sentences that are structurally similar to thenorm-contravening sentences and are additionallydebiased (3.1), results in a fairly difficult dataset.We compare our proposed method against 2 sim-pler alternatives in which different norm-compliantsentences are employed (in both the training andthe test data). In the first method, we obtain suchsentences in an automatic way, namely as comple-tions from GPT-4 by using the norm-contraveningsentences as prompts and asking the model to pro-duce a norm-compliant counterpart (22).",
  "(22) prompt: [They eat rodents.]norm-contravening sentence Thisis not common in our Western society. What would becommon instead? They ...completion: ... eat chickens, cows, pigs, or fish": "The second method simply takes our manuallycompiled norm-compliant sentences without de-biasing (3.1). Thus, this dataset still containsspurious correlations (as discussed in of 3.1). shows the performance of the twoplain transformers and logistic regression on the 3datasets that differ in the norm-compliant sentences.For all learning algorithms, the classification scoresare notably higher for the two alternatives. Whileour proposed method represents a dataset in which,in terms of the surface realization, the sentences inthe two classes hardly differ (), this is nottrue for the alternatives in which there are biasesthat make automatic classification unrealisticallysimple: In the dataset containing the automaticallygenerated norm-compliant sentences by GPT-4, thenegative sentences are notably longer than the posi-tive sentences (i.e. 11.3 vs. 7 tokens per sentence8).",
  "Experiments on the Constructed Dataset": "We now evaluate classifiers to predict abusive lan-guage on the 7 variants of our constructed dataset.Classifiers not Trained on Our Dataset. Weconsider 2 publicly available tools:Perspec-tiveAPI,9 i.e. a tool for the general detection ofabusive language, and the most recent transformerfor implicitly abusive language detection focusingon identity groups from Hartvigsen et al. (2022),i.e. HateBERT fine-tuned on ToxiGen.Moreover, we fine-tune DeBERTa on ISHate(Ocampo et al., 2023), a dataset consolidating 7existing datasets for implicit abuse. Further, wefine-tune DeBERTa on the recent dataset for eu-phemistic abuse (Wiegand et al., 2023). We trainthe latter classifier on the abusive subtype unusualproperties (23)-(24) since, though addressing indi-viduals rather than identity groups, it is related toabusive language that depicts people as deviatingfrom the norm. We want to examine the extent towhich the two types of abusive language coincide.",
  "We also re-use GPT-4::zero-shot from 5.1": "where predictions of norm-contravening sentencesare considered as predictions of abusive language.Within-Dataset Classifiers. We employ the bestclassifier from 5, i.e. DeBERTa trained on GPT-4::aug, and also, for reference, plain DeBERTafrom our previous experiments from 5. For theseclassifiers, we carry out a 5-fold cross-validation.However, we train on the instances of the norm-compliance dataset (), i.e. the dataset con-taining 3rd person mentions, and test on the respec-tive instantiations of the 7 variants that focus onidentity groups (predictions of norm-contraveningsentences are considered abusive language). Thus,we can show that the knowledge to detect this abuseis not specific to a particular identity group.Evaluation. shows the classification re-sults on the detection of abusive language. Classi-fiers not trained on our dataset mostly produce low",
  "majority class37.234.439.639.039.539.240.938.6": "DeBERTa trained on euphemistic abuse56.5 (2.0) 55.1 (1.3) 56.9 (2.4) 54.6 (1.7) 60.3 (1.3) 57.4 (1.7) 56.8 (0.9) 56.8 (1.6)ToxiGen60.559.661.560.758.254.757.358.3DeBERTa trained on ISHate58.3 (1.4) 56.9 (2.4) 61.3 (3.2) 59.4 (3.8) 60.1 (1.2) 60.3 (1.8) 62.9 (1.6) 59.9 (2.2)PerspectiveAPI57.059.559.858.764.675.665.462.9 DeBERTa trained on norm-compliance dataset 78.0 (2.3) 73.8 (1.3) 75.5 (0.7) 70.4 (3.3) 71.2 (2.7) 71.8 (1.2) 75.4 (0.8) 73.7 (1.8)GPT-4::zero-shot (long prompt)82.176.175.578.272.875.273.976.2DeBERTa trained on GPT-4::aug87.1 (0.3) 79.2 (0.8) 79.8 (0.4) 78.7 (0.5) 76.8 (0.6) 77.1 (1.1) 78.2 (0.7) 79.6 (0.6)",
  "Experiments on the Twitter Dataset": "We now evaluate on the Twitter dataset (3.2) thatcomprises attested sentences. We use the sameclassifiers as in 6.1. For GPT-4::aug, we trainon our constructed norm-compliance dataset ratherthan the Twitter dataset since we want to prove thatour constructed dataset generalizes to realistic data. shows the results. Due to space limita-tions, the table only shows the scores on the entiredataset, i.e. we conflate all subtypes of abuse (sex-ism, racism etc.) to one single class. Similar to ourconstructed dataset, GPT-4::aug performs best.After reviewing the errors made by our best clas-sifiers, we identified a systematic error involvingsentences that describe practices inherent to a spe-cific identity group but uncommon among mem-bers of Western society (25)-(26). Even large lan-guage models, such as GPT-4, may misclassifythese challenging (non-abusive) sentences as abu-",
  ": Correctly classified challenging sentences fromthe Twitter dataset (: see )": "sive instances. We manually identified 85 of suchsentences in our Twitter dataset and computed thenumber of correctly classified sentences by ourbest classifier and also zero-shot classifiers usingdifferent language models. shows the re-sults. While there is still a considerable gap towardsthe human baseline, GPT-4 shows encouraging im-provement over the other models.",
  "Conclusion": "We addressed the task of detecting abusive sen-tences in which identity groups are depicted as de-viating from the norm. We created novel datasetswith sentences that do not express (explicitly) nega-tive sentiment for this type of abuse via crowdsourc-ing. Previous classifiers are unable to detect thisform of abuse sufficiently. This is a phenomenonnot tied to specific lexical units. Therefore, onlylarge language models produce good results, in ourcase, DeBERTa fine-tuned on data augmented byGPT-4. Our approach also handles negation andaddresses non-abusive instances that are inherent toan identity group but not common for the Westernsociety.",
  "Limitations": "We base our notion of what is norm-compliant andnorm-contravening on Western norms.11 This de-sign choice was mainly driven by the availability ofresources (i.e. language models and crowdworkers)which reflect the Western norm. We do not wantto imply that the Western norm is more importantthan other norms. However, we believe that it isbeyond the scope of a single research paper to dulyaddress several norms at the same time.Our dataset only addresses one subtype of abu-sive language. Therefore, classifiers trained on ournew data are only capable to detect this subtypeof abuse rather than abusive language, in general.Thus, we follow Wiegand et al. (2021b) who arguethat a divide-and-conquer approach is the onlyreasonable approach to such complex phenomena.Ultimately, we envisage an array of different classi-fiers, each trained for a different subtype of abusivelanguage (such as the task addressed in this paper)to be necessary to have a system that exhaustivelydetects abusive language.In our data,we also observed cases inwhich a norm-compliant (rather than a norm-contravening) property ascribed to an identitygroup is perceived as abusive:",
  "(27) Women usually work in an office.(28) Women usually prepare the food": "Our classification approach is unable to detectsuch instances since they do not fall within its spe-cialization. (27)-(28) are simply another type ofimplicit abuse, i.e. commonly observed stereotypes.We observed this phenomenon more frequently inthe Twitter dataset than in the constructed datasetwhich is quite plausible as the latter will inevitablycontain fewer stereotypes due to its creation pro-cess.12The absence of stereotypes in the con-structed dataset also explains why the correspon-dence between deviating from the norm and abu-sive language is higher in the constructed dataset() than in the Twitter dataset ().Our research does not target individuals. Fu-ture work should investigate the extent to whichthe insights gained by the research presented in thispaper are relevant to individuals. 11We did not explicitly enumerate the features of that normto our crowdworkers, e.g. rule of law, pluralism, seculariza-tion, capitalism etc. Given that all crowdworkers were Englishnative speakers living in Western countries and the strong con-sistency of their responses, we thought this was not necessary.12The crowdworkers invented sentences addressing an oth-erwise unspecified group.",
  "Ethical Considerations": "Although our work clearly suggests that depicting agroup of people as having properties or displayinga behaviour that deviates from the norm is oftenperceived as abusive language, on no account dowe want to imply that deviating from the normis inherently reprehensible. We would like to em-phasize that the norm which is represented by ourdata (i.e. Western norm) may inherently reflect abias towards heteronormativity. Furthermore, asmany of our examples show, oftentimes the attribu-tion of a behavior or property to a group is actuallynot warranted and instead is either an overgeneral-ization or a completely absurd claim. Thus, it isusually not the actual property or behaviour thatmakes people feel offended but the fact that this be-haviour or property does actually not apply to them.This is supported by the fact that those behavioursand properties can actually be positive (29)-(30).",
  "(29) Jews complete a Rubiks cube in under 10 seconds.(30) Women usually know the names of at least 20 of theirneighbours": "It is not our intention to amplify the Westernnorm with this research either. The classifiers weproposed are not designed to suppress certain prop-erties or behaviours that deviate from the norm,in general. If inherent to the given identity group(25)-(26) they are not considered abusive and wespecifically addressed these cases in our research.One may argue that any sentence targetingidentity groups in general is abusive, i.e. notonly stereotypes or norm-contravention but alsosentences such as Jews use the internet. Whileovergeneralizations are indeed problematic, ourcrowdworkers, who were members of the targetedidentity group and judged whether a sentence wasabusive, did not confirm this hypothesis.Most of our new gold standard data were createdwith the help of crowdsourcing. All crowdwork-ers were compensated following the wage recom-mended by the crowdsourcing platform Prolific (i.e.$12 per hour). We inserted a warning of the offen-sive nature in the task advertisement.In this work, we have crowdworkers create tex-tual data representing abusive language since thereis no alternative method that would yield a datasetwith a comparable size and quality. In plagiarismdetection (Potthast et al., 2010), deception detec-tion (Ott et al., 2011) and abusive language detec-tion itself (Vidgen et al., 2021b; Wiegand et al.,2021a) a procedure similar to ours was pursued.",
  "Acknowledgements": "The authors were partially supported by the Aus-trian Science Fund (FWF): P 35467-G. The authorswould like to thank Sybille Sornig for contributingto the manual annotation of this research, and JuliaPardatscher and Ines Rehbein for their feedback onearlier drafts of this paper. Alan Akbik, Tanja Bergmann, Duncan Blythe, KashifRasul, Stefan Schweter, and Roland Vollgraf. 2019.FLAIR: An easy-to-use framework for state-of-the-art NLP. In Proceedings of the Human LanguageTechnology Conference of the North American Chap-ter of the ACL (HLT/NAACL), pages 5459, Min-neapolis, MN, USA. Mohammad Ali and Naeemul Hassan. 2022. A Surveyof Computational Framing Analysis Approaches. InProceedings of the Conference on Empirical Methodsin Natural Language Processing (EMNLP), pages93359348, Abu Dhabi, United Arab Emirates. Wafa Alorainy, Pete Burnap, Han Liu, and Matthew L.Williams. 2019. The Enemy Among Us: Detect-ing Cyber Hate Speech with Threats-based OtheringLanguage Embeddings. ACM Transactions on theWeb, 13(3):126. Pinkesh Badjatiya, Shashank Gupta, Manish Gupta, andVasudeva Varma. 2017.Deep Learning for HateSpeech Detection in Tweets. In Proceedings of the In-ternational Conference on World Wide Web (WWW),pages 759760, Perth, Australia. Francesco Barbieri, Jose Camacho-Collados, LuisEspinosa-Anke, and Leonardo Neves. 2020. TweetE-val: Unified Benchmark and Comparative Evaluationfor Tweet Classification. In Findings of Associationfor Computational Linguistics: EMNLP 2020, 16441650, Online. Maria Becker, Katharina Korfhage, and Anette Frank.2021. COCO-EX: A Tool for Linking Concepts fromTexts to ConceptNet. In Proceedings of the Confer-ence on European Chapter of the Association forComputational Linguistics (EACL): System Demon-strations, pages 119126, Online.",
  "Annotation. In Proceedings of the Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 120125, Sofia, Bulgaria": "Lingjia Deng and Janyce Wiebe. 2014. Sentiment Prop-agation via Implicature Constraints. In Proceedingsof the Conference on European Chapter of the Asso-ciation for Computational Linguistics (EACL), pages377385, Gothenburg, Sweden. Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofDeep Bidirectional Transformers for Language Un-derstanding. In Proceedings of the Human LanguageTechnology Conference of the North American Chap-ter of the ACL (HLT/NAACL), pages 41714186, Min-neapolis, MN, USA. Haibo Ding and Ellen Riloff. 2018.Weakly Super-vsied Induction of Affective Events by OptimizingSemantic Consistency. In Proceedings of the Na-tional Conference on Artificial Intelligence (AAAI),pages 57635770, New Orleans, LA, USA. Mai ElSherief, Caleb Ziems, David Muchlinski, Vaish-navi Anupindi, Jordyn Seybolt, Munmun De Choud-hury, and Diyi Yang. 2021. Latent Hatred: A Bench-mark for Understanding Implicit Hate Speech. InProceedings of the Conference on Empirical Methodsin Natural Language Processing (EMNLP), pages345363, Online and Punta Cana, Dominican Repub-lic.",
  "Mark R. Leary. 2000. Affect, cognition and the socialemotions. In J. P. Forgas, editor, Feeling and think-ing: The role of affect in social cognition. CambridgeUniversity Press": "Chuanrong Li, Lin Shengshuo, Zeyu Liu, Xinyi Wu,Xuhui Zhou, and Shane Steinert-Threlkeld. 2020.Linguistically-Informed Transformations (LIT): AMethod for Automatically Generating Contrast Sets.In Proceedings of the BlackboxNLP Workshop on An-alyzing and Interpreting Neural Networks for NLP,pages 126135, Online. Bjrn Lindstrm, Simon Jangard, Ida Selbing, andAndreas Olsson. 2017. The Role of a CommonIs Moral Heuristic in the Stability and Change ofMoral Norms. Journal of Experimental Psychology:General, 147:228242. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.RoBERTa: A Robustly Optimized BERT PretrainingApproach. arXiv preprint arXiv:1907.11692.",
  "Julia Mendelsohn, Yulia Tsvetkov, and Dan Jurafsky.2020. A Framework for the Computational Linguis-tic Analysis of Dehumanization. Frontiers in Artifi-cial Intelligence, 3": "Chikashi Nobata, Joel Tetreault, Achint Thomas, YasharMehdad, and Yi Chang. 2016. Abusive LanguageDetection in Online User Content. In Proceedingsof the International Conference on World Wide Web(WWW), pages 145153, Republic and Canton ofGeneva, Switzerland. Nicolas Ocampo, Ekaterina Sviridova, Elena Cabrio,and Serena Villata. 2023.An In-depth Analysisof Implicit and Subtle Hate Speech Messages. InProceedings of the Conference on European Chap-ter of the Association for Computational Linguistics(EACL), pages 19892005, Dubrovnik, Croatia. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-man, Diogo Almeida, Janko Altenschmidt, Sam Alt-man, Shyamal Anadkat, Red Avila, Igor Babuschkin,Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-ing Bao, Mohammad Bavarian, Jeff Belgum, Ir-wan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,Christopher Berner, Lenny Bogdonoff, Oleg Boiko,Madelaine Boyd, Anna-Luisa Brakman, Greg Brock-man, Tim Brooks, Miles Brundage, Kevin Button,Trevor Cai, Rosie Campbell, Andrew Cann, BrittanyCarey, Chelsea Carlson, Rory Carmichael, BrookeChan, Che Chang, Fotis Chantzis, Derek Chen, SullyChen, Ruby Chen, Jason Chen, Mark Chen, BenChess, Chester Cho, Casey Chu, Hyung Won Chung,Dave Cummings, Jeremiah Currier, Yunxing Dai,Cory Decareaux, Thomas Degry, Noah Deutsch,Damien Deville, Arka Dhar, David Dohan, SteveDowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,Simn Posada Fishman, Juston Forte, Isabella Ful-ford, Leo Gao, Elie Georges, Christian Gibson, VikGoel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, ScottGray, Ryan Greene, Joshua Gross, Shixiang ShaneGu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,Yuchen He, Mike Heaton, Johannes Heidecke, ChrisHesse, Alan Hickey, Wade Hickey, Peter Hoeschele,Brandon Houghton, Kenny Hsu, Shengli Hu, XinHu, Joost Huizinga, Shantanu Jain, Shawn Jain,Joanne Jang, Angela Jiang, Roger Jiang, HaozhunJin, Denny Jin, Shino Jomoto, Billie Jonn, Hee-woo Jun, Tomer Kaftan, ukasz Kaiser, Ali Ka-mali, Ingmar Kanitscheider, Nitish Shirish Keskar,Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirch-ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,ukasz Kondraciuk, Andrew Kondrich, Aris Kon-stantinidis, Kyle Kosic, Gretchen Krueger, VishalKuo, Michael Lampe, Ikai Lan, Teddy Lee, JanLeike, Jade Leung, Daniel Levy, Chak Ming Li,Rachel Lim, Molly Lin, Stephanie Lin, MateuszLitwin, Theresa Lopez, Ryan Lowe, Patricia Lue,Anna Makanju, Kim Malfacini, Sam Manning, TodorMarkov, Yaniv Markovski, Bianca Martin, KatieMayer, Andrew Mayne, Bob McGrew, Scott MayerMcKinney, Christine McLeavey, Paul McMillan,Jake McNeil, David Medina, Aalok Mehta, JacobMenick, Luke Metz, Andrey Mishchenko, PamelaMishkin, Vinnie Monaco, Evan Morikawa, DanielMossing, Tong Mu, Mira Murati, Oleg Murk, DavidMly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh,Long Ouyang, Cullen OKeefe, Jakub Pachocki, AlexPaino, Joe Palermo, Ashley Pantuliano, Giambat-tista Parascandolo, Joel Parish, Emy Parparita, AlexPassos, Mikhail Pavlov, Andrew Peng, Adam Perel-man, Filipe de Avila Belbute Peres, Michael Petrov,Henrique Ponde de Oliveira Pinto, Michael, Poko-rny, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-ell, Alethea Power, Boris Power, Elizabeth Proehl,Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,Cameron Raymond, Francis Real, Kendra Rimbach,Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-der, Mario Saltarelli, Ted Sanders, Shibani Santurkar,Girish Sastry, Heather Schmidt, David Schnurr, JohnSchulman, Daniel Selsam, Kyla Sheppard, TokiSherbakov, Jessica Shieh, Sarah Shoker, PranavShyam, Szymon Sidor, Eric Sigler, Maddie Simens,Jordan Sitkin, Katarina Slama, Ian Sohl, BenjaminSokolowsky, Yang Song, Natalie Staudacher, Fe-lipe Petroski Such, Natalie Summers, Ilya Sutskever,Jie Tang, Nikolas Tezak, Madeleine B. Thompson,Phil Tillet, Amin Tootoonchian, Elizabeth Tseng,Preston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-lipe Cern Uribe, Andrea Vallone, Arun Vijayvergiya,Chelsea Voss, Carroll Wainwright, Justin Jay Wang,Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,CJ Weinmann, Akila Welihinda, Peter Welinder, Ji-ayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,Clemens Winter, Samuel Wolrich, Hannah Wong,Lauren Workman, Sherwin Wu, Jeff Wu, MichaelWu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-ing Yuan, Wojciech Zaremba, Rowan Zellers, ChongZhang, Marvin Zhang, Shengjia Zhao, TianhaoZheng, Juntang Zhuang, William Zhuk, and BarretZoph. 2024.GPT-4 Technical Report.Preprint,arXiv:2303.08774. Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T. Han-cock. 2011. Finding Deceptive Opinion Spam byAny Stretch of the Imagination. In Proceedings ofthe Annual Meeting of the Association for Computa-tional Linguistics (ACL), pages 309319, Portland,OR, USA.",
  "Dataset. In Proceedings of the Linguistic AnnotationWorkshop (LAW), pages 252265, Toronto, Canada": "Fabio Poletto, Valerio Basile, Manuela Sanguinetti,Cristina Bosco, and Viviana Patti. 2021. Resourcesand benchmark corpora for hate speech detection: asystematic review. Language Resources and Evalua-tion, 55:477523. Martin Potthast, Benno Stein, Alberto Barrn-Cedeo,and Paolo Rosso. 2010. An Evaluation Frameworkfor Plagiarism Detection. In Proceedings of the Inter-national Conference on Computational Linguistics(COLING), pages 9971005, Beijing, China. Alan Ramponi and Sara Tonelli. 2022. Features or Spu-rious Artifacts? Data-centric Baselines for Fair andRobust Hate Speech Detection. In Proceedings ofthe Human Language Technology Conference of theNorth American Chapter of the ACL (HLT/NAACL),pages 30273040, Seattle, WA, USA. Paul Rttger, Bertram Vidgen, Dong Nguyen, ZeerakWaseem, Helen Margetts, and Janet B. Pierrehum-bert. 2021. HateCheck: Functional Tests for HateSpeech Detection Models. In Proceedings of the An-nual Meeting of the Association for ComputationalLinguistics (ACL), pages 4158, Online. Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky,Noah A. Smith, and Yejin Choi. 2020. SOCIALBIAS FRAMES: Reasoning about Social and PowerImplications of Language. In Proceedings of theAnnual Meeting of the Association for ComputationalLinguistics (ACL), pages 54775490, Online. Indira Sen, Mattia Samory, Claudia Wagner, and Is-abelle Augenstein. 2022.Counterfactually Aug-mented Data and Unintened Bias: The Case of Sex-ism and Hate Speech Detection. In Proceedings ofthe Human Language Technology Conference of theNorth American Chapter of the ACL (HLT/NAACL),pages 46774695, Seattle, WA, USA. Ravsimar Sodhi, Kartikey Panta, and Radhika Mamidi.2021. Jibes & Delights: A Dataset of Targeted In-sults and Compliments to Tackle Online Abuse. InProceedings of the Workshop on Online Abuse andHarms (WOAH), pages 132139, Online.",
  "June Tangney and Ronda Dearing. 2002. Shame andGuilt. Guilford Press": "Hugo Touvron, Louis Martin, Kevin R. Stone, PeterAlbert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, D. Bikel, Lukas Blecher, Cristian CantnFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, A. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan,Marcin Kardas, Viktor Kerkez, Madian Khabsa, Is-abel M. Kloumann, A. Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, R. Subramanian,Xia Tan, Binh Tang, Ross Taylor, Adina Williams,Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, IliyanZarov, Yuchen Zhang, Angela Fan, Melanie Kam-badur, Sharan Narang, Aurelien Rodriguez, RobertStojnic, Sergey Edunov, and Thomas Scialom. 2023.Llama 2: Open Foundation and Fine-Tuned ChatModels. Betty van Aken, Julian Risch, Ralf Krestel, and Alexan-der Lser. 2018. Challenges for Toxic CommentClassification: An In-Depth Error Analysis. In Pro-ceedings of the Workshop on Abusive Language On-line (ALW), pages 3342, Brussels, Belgium.",
  "Bertie Vidgen and Leon Derczynski. 2020. Directionsin Abusive Language Training Data.PLoS One,15(12)": "Bertie Vidgen, Dong Nguyen, Helen Margetts, PatriciaRossini, and Rebekah Tromble. 2021a. Introduc-ing CAD: the Contextual Abuse Dataset. In Pro-ceedings of the Human Language Technology Con-ference of the North American Chapter of the ACL(HLT/NAACL), pages 22892303, Online. Bertie Vidgen, Tristan Thrush, Zeerak Waseem, andDouwe Kiela. 2021b.Learning from the Worst:Dynamically Generated Datasets to Improve OnlineHate Detection. In Proceedings of the Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 16671682, Online. Zeerak Waseem, Thomas Davidson, Dana Warmsley,and Ingmar Weber. 2017. Understanding Abuse: ATypology of Abusive Language Detection Subtasks.In Proceedings of the ACL-Workshop on Abusive Lan-guage Online, pages 7884, Vancouver, BC, Canada. Zeerak Waseem and Dirk Hovy. 2016. Hateful Sym-bols or Hateful People?Predictive Features forHate Speech Detection on Twitter. In Proceedingsof the Human Language Technology Conference ofthe North American Chapter of the ACL StudentResearch Workshop, pages 8893, San Diego, CA,USA. Michael Wiegand, Elisabeth Eder, and Josef Ruppen-hofer. 2022.Identifying Implicitly Abusive Re-marks about Identity Groups using a LinguisticallyInformed Approach. In Proceedings of the HumanLanguage Technology Conference of the North Amer-ican Chapter of the ACL (HLT/NAACL), pages 56005612, Seattle, WA, USA.",
  "Dataset and Linguistic Analysis. In Proceedings ofthe Conference on European Chapter of the Associ-ation for Computational Linguistics (EACL), pages358368, Online": "Michael Wiegand, Jana Kampfmeier, Elisabeth Eder,and Josef Ruppenhofer. 2023. Euphemistic Abuse A New Dataset and Classification Experiments forImplicitly Abusive Language. In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 1628016297,Singapore. Michael Wiegand, Josef Ruppenhofer, and ElisabethEder. 2021b. Implicitly Abusive Language Whatdoes it actually look like and why are we not gettingthere? In Proceedings of the Human Language Tech-nology Conference of the North American Chapterof the ACL (HLT/NAACL), pages 576587, Online. Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005. Recognizing Contextual Polarity in Phrase-level Sentiment Analysis.In Proceedings of theConference on Human Language Technology andEmpirical Methods in Natural Language Process-ing (HLT/EMNLP), pages 347354, Vancouver, BC,Canada.",
  "Fabian Winter and Nan Zhang. 2018. Social norm en-forcement in ethically diverse communities. Pro-ceedings of the National Academy of Sciences ofthe United States of America (PNAS), 115(11):27222727": "Tobias Wolbring, Christiane Bozoyan, and DominikLangner. 2013. Links gehen, rechts stehen! / \"WalkLeft, Stand Right!\" Ein Feldexperiment zur Durchset-zung informeller Normen auf Rolltreppen / A FieldExperiment on the Enforcement of Informal Normson Escalators. Zeitschrift fr Soziologie, 42(3):239258. Chong Zhang, Jieyu Zhao, Huan Zhang, Kai-Wei Chang,and Cho-Jui Hsieh. 2021. Double Perturbation: Onthe Robustness of Robustness and CounterfactualBias Evaluation. In Proceedings of the Human Lan-guage Technology Conference of the North AmericanChapter of the ACL (HLT/NAACL), pages 38993916,Online. Haoti Zhong, Hao Li, Anna Cinzia Squicciarini,Sarah Michele Rajtmajer,Christopher Griffin,David J. Miller, and Cornelia Caragea. 2016.Content-Driven Detection of Cyberbullying on theInstagram Social Network. In Proceedings of theInternational Joint Conference on Artificial Intelli-gence (IJCAI), pages 39523958, New York City,NY, USA. Deyu Zhou, Jianan Wang, Linhai Zhang, and YulanHe. 2021. Implicit Sentiment Analysis with Event-Centered Text Representation. In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 68846893,Online and Punta Cana, Dominican Republic.",
  "AHyperparameters of Statistical Models": "For all statistical models we used in this researchwe refrained from heavy tuning of hyperparam-eters. This is due to the fact that several exper-iments were evaluated in a cross-dataset setting,i.e. the training and test data originated from dif-ferent datasets. As a consequence, tuning hyper-parameters would only be possible by using somedevelopment data from the source domain. This,however, would mean that the resulting modelswould be tuned for the wrong domain. By runningthe tools with frequently used (default) settings ofhyperparameters, we hope to produce models thatare overall more robust across different domains(i.e. different datasets) than models fine-tuned onthe wrong domain. Thus, we follow the strategythat was proposed for the large-scale cross-datasetevaluation reported in Wiegand et al. (2022).",
  "A.2PerspectiveAPI": "In our evaluation, we also included Perspec-tiveAPI13 as one baseline. This tool runs on un-restricted text and, from the publicly available clas-sifiers, it is currently considered the state of theart for the general detection of abusive language(Rttger et al., 2021). The tool predicts severalsubtypes of abusive language. We considered thecategory Identity attack for our experiments, since",
  "A.3Transformers: BERT and DeBERTa": "For classification, we fine-tuned both transform-ers BERT and DeBERTa using the implementationfor text classification within the FLAIR framework(version 12) (Akbik et al., 2019). Regarding theexact models, we used bert-base-uncasedand deberta-large. The former was chosensince of the currently available set of transformers,it is fairly small in size (110 million parameters)and thus has moderate resource requirements. Thelatter was chosen since it belongs to the more re-cent models of larger size (11.5 billion parameters)with notably improved performance but also greaterresource requirements.In order not to overfit the model, we chosethe hyperparameter settings from Wiegand et al.(2022):",
  "The reason for having two sentences instead ofone sentence in the completion is as follows. Inmany instances, unlike in our previous examples(20)-(21), the first sentence of the completion is": "insufficiently conclusive. For example, in (31) and(32), it is unclear what exactly GPT-4 considersnorm-contravening, i.e. the negated proposition ofthe sentence to classify or the (plain) propositionwithout negation. As pointed out in 5.3, GPT-4is inconsistent in that respect and may arbitrarilychoose one of these scopes. The second sentenceof the completion may help us to reconstruct whatscope GPT-4 actually applied in its response. In(31), we thus learn that GPT-4 considers it uncom-mon to not use the internet (i.e. it considered thenegated proposition), while in (32), GPT-4 con-siders carrying around a bag of glitter to sprinklewhen happy to be uncommon (i.e. it considered theproposition without the negation). (31) prompt:[Usually, they do not use the internet.]sentence to classifyIs this common in our Western society?completion (first sentence):No, this is not common in our Western society.completion (second sentence):The majority of people in the Western world use theinternet on a regular basis for various purposes such aswork, education, communication, entertainment, shop-ping, and so on. (32) prompt:[They do not keep a large bag of glitter with them so theycan sprinkle it when they feel happy.]sentence to classifyIs this common in our Western society?completion (first sentence):No, this is not common in Western society.completion (second sentence):Despite individual differences, carrying a bag of glit-ter to sprinkle when feeling happy is not a universallyrecognized or widely practiced behavior.",
  "B.1General Remarks": "All guidelines for the different annotation and sen-tence generation tasks are contained in the supple-mentary material to this work, which is availableupon request.Our annotation guidelines as to what constitutesanti-Semitism, homophobia, Islamophobia, sexism,and racism were based on examples provided to usby members of the affected identity groups in anearlier crowdsourcing survey. We also asked ourcrowdworkers to rely on their intuition.",
  "B.2Details on Ranking Different Types ofImplicit Abuse": "In 1, we briefly mentioned that we also estab-lished via a crowdsourcing experiment that the phe-nomenon of implicitly abusive language we studyin this paper is considered more severe than abusivecomparisons and euphemistic abuse. For the sakeof completeness, in the following we describe theset up of this elicitation experiment:We asked crowdworkers to decide which of apair of instances of implicitly abusive language theyconsidered more severe. Each pair consisted of twodifferent types of implicit abuse. The specific typeof abuse was not revealed to the crowdworkers.We considered 6 other types of implicitly abusivelanguage from existing datasets in addition to theform of abusive language we introduced in thispaper. For each combination of types we had 20different sentences (randomly sampled) rated by5 crowdworkers each. shows for eachtype the percentage it was considered more, lessor equally abusive than the type it was paired with.The final ranking in was computed basedon the proportion a particular type of implicit abusewas rated to be more severe than the other type.In this experiment, we sampled sentences of thedata we produced as part of this research (3) toserve as examples for the target phenomenon intro-duced in this paper. For the other forms of implicitabuse we selected sentences from other existingdatasets. We specifically targeted those similar to our newly identified type of abuse, i.e. subtly nega-tive towards the target, and those explicitly labeledas such within these datasets. (Some types of im-plicitly abusive language mentioned in 2, such ascall for action, lack a dataset in which this particu-lar type is specifically marked as such.) Thus, wecould avoid additional manual annotation. We usethe following types:Comparisons.This form of implicitly abu-sive language uses like-comparisons (33) from thedataset introduced by Wiegand et al. (2021a).",
  "B.3Details on the Relevance of Sentiment inOur Datasets": "As stated in 3, our datasets, both the constructeddataset and the dataset comprising sentences fromTwitter, exclude sentences that convey explicitlynegative sentiment. We excluded sentences con-veying explicitly negative sentiment since existingdatasets on abusive language detection involvingidentity groups convey overwhelmingly such neg-ative sentiment (Wiegand et al., 2022). Therefore,classifiers trained on them will be able to cope withthat type of sentiment.By implicit sentiment, i.e. the sentiment type thatremains in our dataset, we understand sentimentthat is not conveyed by words with an unambigu-ously negative connotation, e.g. poor, sad or bad.Such words are also referred to as sentiment or sub-jective expressions (Wilson et al., 2005) that arealso covered as part of sentiment lexicons, suchas the Subjectivity Lexicon (Wilson et al., 2005).Though in general, such lexicons are a good approx-imation for establishing explicit sentiment automat-ically, we found that all publicly available lexiconsare still sparse. Therefore, in order to provide evenmore accurate data, we refrained from simply us-ing a lexicon look-up as an automatic procedure toidentify explicit sentiment. Instead, we establishedit via manual annotation.15 For this annotation,we also took into consideration the criterion of de-feasibility (Deng and Wiebe, 2014). The relationbetween explicit/implicit sentiment and defeasibil-ity is explained in the following:",
  "B.4Details on the Inter-Annotator Agreement": "For each of the variants of our constructed dataset(3.1), we measured the inter-annotator agreementon a random sample of 200 sentences betweenone co-author and the majority vote of the labelsprovided by the crowdworkers. (upperpart) lists the agreement between these two anno-tations on each sample. It is highest on the norm-compliance task and lowest for racism (Asians),though that agreement can still be considered sub-stantial (Landis and Koch, 1977).The Twitterdataset (3.2) underwent the same annotation tasksas the previously constructed dataset. We also usedannotators from the same pool of crowdworkers.For creating both the constructed dataset (3.1)and the Twitter dataset (3.2), it was necessary toremove or filter out content that contained explic-itly negative sentiment. The notion of implicit andexplicit sentiment as depicted in B.3 was takenfrom Deng and Wiebe (2014). Since that annota-tion is much less dependent on the demographicsof the annotators (for the subcategories of abusivelanguage, we opted for the crowdworkers who areaffected targets, e.g. Jews, gay people, women etc.)and due to financial reasons, this was done by oneco-author, who is a trained linguist. However, wealso measured the inter-annotator agreement to an-",
  "CDetails on Debiasing theNorm-Compliance Dataset": "After we had collected sentences displaying norm-contravening behaviours/properties and manuallycreated norm-compliant counterparts ourselves, weinspected the resulting set of sentences for possi-ble biases () by computing the PointwiseMutual Information between words and each ofthe two classes of our dataset. There were notablymore biases on the class norm-compliant than onthe class norm-contravening. The reasons for thatmight lie in the way the sentences of the differentclasses were produced. As stated in 3.1, followingthe concept of contrast sets (Gardner et al., 2020),sentences for the class norm-compliant were pro-duced by considering the sentences of class norm-contravening, e.g. (12), and converting them to asentence of class norm-compliant by only applyingminimal changes, e.g. (13). In contrast, for thenorm-contravening sentences, crowdworkers were",
  ": Illustration of the biased word distribution on the class norm-compliant and impact of debiasing (wordsare ranked according to their bias towards the class norm-compliant)": "fairly free to invent their content. shows the 10 most highly ranked wordsaccording to their Pointwise Mutual Informationwith class norm-compliant before debiasing. Thelast two columns show the percentage of instancesof a word with class norm-compliant before andafter debiasing. A word can be considered fairlyunbiased if the percentage is about 50% since ournorm-compliance dataset with 2 classes has almosta balanced class distribution (c.f. ). shows that there was a notable bias towards theclass norm-compliant among these words beforedebiasing. If we judge those words based on theirproportion in the class norm-compliant after debi-asing, we can consider them sufficiently debiased.Please note that it is impossible to have exactlythe same class distribution for all words. This isbecause, after our manual debiasing process, theentire dataset underwent the validation step (Fig-ure 1). During this step, sentences were removedif crowdworkers considered them to be improperEnglish or if they failed to reach a majority label.In other words: the size of the dataset still changedafter we applied debiasing.In order to keep the debiasing step at a tractablelevel, we only took into account words with ahigh Pointwise Mutual Information if they werealso frequently occurring in our dataset. Individ-ual words with a frequency of 10 or lower wouldnot greatly affect learning methods. For lower fre-quency words, classifiers generally assign lowerweights since they are only rarely observed.Most biased words were function words. Con-tent words are less likely to cause a harmful effectas our dataset covers many areas of life ()which prevent the same content word from occur-ring frequently."
}