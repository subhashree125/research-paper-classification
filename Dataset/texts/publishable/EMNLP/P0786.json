{
  "Abstract": "To explain social phenomena and identify sys-tematic biases, much research in computationalsocial science focuses on comparative text anal-yses. These studies often rely on coarse corpus-level statistics or local word-level analyses,mainly in English. We introduce the INFOGAPmethodan efficient and reliable approach tolocating information gaps and inconsistenciesin articles at the fact level, across languages.We evaluate INFOGAP by analyzing LGBT peo-ples portrayals, across 2.7K biography pageson English, Russian, and French Wikipedias.We find large discrepancies in factual coverageacross the languages. Moreover, our analysisreveals that biographical facts carrying negativeconnotations are more likely to be highlightedin Russian Wikipedia. Crucially, INFOGAPboth facilitates large scale analyses, and pin-points local document- and fact-level informa-tion gaps, laying a new foundation for targetedand nuanced comparative language analysis atscale.1",
  "Introduction": "Wikipedia has several hundred language editions,a sizeable number of which have more than 100Karticles. Despite its neutral point of view policy,abundant evidence of content discrepancies acrosslanguage editions has been well-documented onthe platform (e.g., Hecht and Gergle, 2010; Calla-han and Herring, 2011; Eom et al., 2015; Wagneret al., 2015; Park et al., 2021). There are numer-ous motivations for identifying and studying thesevariations, e.g., identifying content variations andgaps can aid editors in removing social and cul-tural biases (Field et al., 2022). Alternatively, froma social science perspective, comparative analy-ses of prominent topics across Wikipedia languageeditions provides a window into studying cross-",
  "English": ": We propose a method, INFOGAP, to locatefact (mis)alignments in Wikipedia biographies in differ-ent language versions. INFOGAP identifies facts that arecommon to a pair of articles (Griner was born on Octo-ber 18, 1990), and facts unique to one language version(Griner had recorded the sixth triple-double; En only)enabling further analysis of information gaps, editorsselective preferences within articles, and analyses atscale across languages, cultures, and demographics.",
  "cultural differences at scale (Callahan and Herring,2011)": "Existing methods for examining cross-languagedifferences and gaps across Wikipedias rely on ag-gregate statistics, such as the number of languagesan article is available in (Wagner et al., 2015), sum-mary metrics of positive and negative connotations(Park et al., 2021), text complexity measures (Kimet al., 2016; Field et al., 2022), or differences in hy-perlink graph structures (Hecht and Gergle, 2010;Laufer et al., 2015). While these metrics are use-ful for understanding broad trends, they do notfacilitate nuanced comparative analysis, failing toinform readers and editors how the content theyengage with varies across language versions. Atthe same time, manual fine-grained comparativeanalyses (e.g., Callahan and Herring, 2011) do not scale and run the risk of incorporating researchersbiases.In this work, we propose INFOGAP, a highlyreliable method for identifying overlaps and gapsacross different language articles on the same topic.Our method is composed of two steps: an align-ment step aimed at aligning facts across differentlanguage versions, followed by a validation stepaimed at determining fact equivalence. INFOGAPallows us to automatically identify exact contentdifferences, as illustrated in , enabling bothaggregate and fine-grained comparative analyses(2).After verifying the accuracy of INFOGAP againstour manual annotations, we demonstrate its useful-ness through a comparative analysis on thousandsof multilingual Wikipedia biographies from theLGBTBIOCORPUS (Park et al., 2021). We findthat the coverage of public figures differs substan-tially across languages (3). For example, whencomparing Russian and English biographies, wefind that on average 34% of the content in Russianbiographies is not present in their English counter-parts.Critically, as suggested in manual analyses by Park et al. (2021), our automatic analyses at scaleidentify that many of the bios carry a significantlydifferent implied sentiment towards the figure, de-pending on the language version that is accessed.2 Aggregating these sentiment imbalances across2.7K biographies, we contribute the insight thatRussian LGBT biographies share disproportion-ately more negative sentiment facts with Englishbiographies than positive ones. Overall, INFOGAPenables the pinpointing of fine-grained factual andframing distinctions between narratives across lan-guages, aggregates these insights across thousandsof articles, and offers tools to identify the specificdocuments that most clearly highlight these nu-ances.",
  "INFOGAP: Identifying InformationAsymmetry in Wikipedia Articles": "Consider a pair of articles on a topic written indifferent languages. We call one article E and theother in the pair F. Moreover, we represent Eby a series of facts e1, . . . , en and F similarly asf1, . . . , fm. Our method determines for a givenfact ei E whether it appears in F (F ei) or",
  ": Schematic of the INFOGAP procedure. We de-scribe the Fact Decomposition and Multilingual Align-ment steps in 2.1, and the Alignment Verification stepin 2.2": "not (F ei). The pipeline is directional, so wecan compute both F ei and E fi. Withoutloss of generality, we will describe the procedurefor obtaining the labels F ei, for all ei. We referto this as the E F direction. presents an overview of INFOGAP. Weprimarily focus on two steps. First, following Minet al. (2023), we narrow the search space of equiv-alent facts by aligning a fact in E to facts in Fthat may convey the same information (Sec 2.1).This allows us to efficiently assess the equivalencebetween aligned facts (Sec 2.2). We determine thereliability of INFOGAP in .3.",
  "X-FACTALIGN:Cross-Lingual Fact Alignment": "Fact Decomposition.As a first step, we need torepresent an article (e.g. E) as a series of facts(e1, . . . , en). Sentences are suboptimal for this pur-pose, as they can be overly complex. Instead, fol-lowing Kamoi et al. (2023), we use GPT-4 (Achiamet al., 2023) for fact decomposition. Differentlyfrom Kamoi et al. (2023), who decompose sen-tences, we decompose entire paragraphs, to providemore context to the model and allow it to resolveco-references. See Appendix A for the prompt. Fact Representation.In order to determinewhether ei is also conveyed in F, we embed eachfact in E and in F using multilingual LaBSE em-beddings (Feng et al., 2020). The straightforwardway to align facts is by computing the cosine sim-ilarity between ei and each fact fj F, aligningei to the most similar fact: arg minj d(ei, fj). Wefind this approach can further be improved by con-sidering the context of the surrounding facts. In the following paragraphs, we describe two improve-ments we made in X-FACTALIGN. First, we re-strict the pool of paragraphs in F from which fjcan be retrieved. Second, we apply an adjustmentto the computation of d(, fi), accounting for thehubness of fj (Lazaridou et al., 2015). Paragraph Alignment.We can partition thefacts in E into their paragraphs: P 1E, ..., P NE . Simi-larly for F: P 1F , ..., P MF . We represent each para-graph by the set of its facts embeddings. We thenconstruct a bipartite graph between paragraphs inE and paragraphs in F, adding a directed edgefrom each paragraph in E, P iE, to a paragraph inF, P jF such that j = MaxSim j d(P iE, P jF ) (Khattaband Zaharia, 2020). We do the same in the otherdirection, going from F to E. Removing the direc-tion from the edges, we obtain an adjacency matrixA between the paragraphs so that each paragraph inE is connected to at least one paragraph in F. Fora given fact ei, we can now limit the pool of align-ment candidates in F to fjs where the paragraphsof ei and fj are adjacent in the graph. Correcting for Hubness.Given that we are com-paring facts from articles on the same topic, directlycomputing d(ei, fj) can lead to aligning unrelatedfacts that discuss the same common named entities.In particular, some facts fj are similar to manyother facts ei, causing a hubness problem (Lazari-dou et al., 2015; Conneau et al., 2017). To mitigatethis, we follow Artetxe and Schwenk (2019) andnormalize d(, fj) so that it is a function of the se-mantic density of fj. The density-normalized dis-tance D(ei, fi) = d(ei, fj)hubness(fj). We com-pute the hubness of fj by computing the averagenearest neighbor distance (kNN = 5) between fjand 50 other facts drawn from paragraphs that arenot in the adjacency list of the paragraph contain-ing ei. Overall, this process enables us to retrievek = 2 facts from F that may convey the sameinformation as ei.",
  "X-FACTMATCH:Cross-Lingual Fact Matching": "With ei and its aligned facts fj, we can now answerthe question whether a given fact ei E appearsin F (F ei) or not (F ei). We assume thatif F ei, there exist facts in F that entail ei. Inparticular, we can expect these facts to be alignedwith ei. We thus relax the problem of judgingwhether F ei to whether any of the facts fj",
  "retrieved by X-FACTALIGN entail ei, i.e. whetherany({ fj ei | j [k] }).3": "We use entailment as a shorthand for conveyingthe same information as despite a minor deviationfrom the definition of entailment in linguistics as astrict logical entailment (Heim and Kratzer, 1998),and in NLP as a human [reading the premise]would typically think that the hypothesis is likelytrue (Dagan et al., 2005; Bowman et al., 2015).Our definition is a bit more relaxed and we alsoconsider partial entailment (Levy et al., 2013), i.e.,when the most important information in ei is con-veyed by F, allowing the omission of peripheralinformation. To that end, we dont use existing NLImodels. Furthermore, research on cross-languageentailment detection is limited (Negri et al., 2012;Rodriguez et al., 2023), and to our knowledge thereare no publicly available models that can determinethe entailment between a premise and a hypothesisin different languages.Inspired by Min et al. (2023) and Shafayat et al. (2024) who used GPT-4 to assess the truthfulnessof a model-generated fact against a trusted knowl-edge base, we prompt GPT-4 to compare an En-glish fact to its aligned facts in F. Concretely,we prompt the model with the hypothesis fact eiand the two immediately preceding facts for con-text (ei1 and ei2), along with all of the premisefacts fj and their contexts (fj1 and fj2). Weinstruct the model to determine whether ei can beinferred from any of the fj (j [k]). Appendix Bpresents the prompt that we use for all languagepairs. The models prediction serves as the finallabel for whether F ei.",
  "French of 10 people, and in English and Russian for12 people, comprising nearly 10K facts altogether.We draw on biographies from the LGBTBIOCOR-": "PUS (Park et al., 2021), a corpus we analyze in 3at a larger scale. See for a breakdown ofthe number of facts and Appendix C for the biogra-phies.We annotated a subset of the facts in each lan-guage pair and direction. Given a hypothesis ei, theretrieved candidate facts fj from X-FACTALIGN,and their contexts, we ask the annotator to choosebetween three options: (1) the hypothesis fact eican be inferred from one of the retrieved fj; (2) thehypothesis fact can be inferred from the article F,but not from the fj (indicating that X-FACTALIGNfailed to retrieve the correct fact); (3) ei cannot beinferred from F. We also provide relaxed versionsof options (1) and (2), where ei can be partiallyinferred from one of the retrieved fj or partiallyinferred from F. Concretely, our annotation taskclosely resembles the X-FACTMATCH step, withtwo key differences. First, we provide the annota-tors with English translations of non-English factsand their contexts, using the NLLB model (Costa-juss et al., 2022). Second, if a hypothesis fact eicannot be inferred from the k facts retrieved byX-FACTALIGN, we ask the annotator to read thefull Wikipedia article F to determine whether eican be inferred from it.One author annotated 80 facts in each languagepair and for both directions within each languagepair. Another author annotated 40 of those 80 foreach language pair. We obtained substantial inter-annotator agreements, with Cohens = 0.71 forEn/Fr and = 0.78 for En/Ru. We thus concludethat the task is relatively unambiguous.In order to determine the reliability of INFOGAP,we compute the predictions against the annotated80 facts for each language pair. presentsthe F1 scores that range from 0.78 to 0.9, indicat-ing that the INFOGAP pipeline is highly reliable in identifying whether a fact in E is present in F (andvice-versa). Substituting X-FACTMATCH with aRoBERTa NLI baseline (Liu, 2019) performs sig-nificantly worse.4 The RoBERTa NLI model rarelypredicts an entailment label on our dataset, result-ing in its poor performance. With the exceptionof En Ru, the NLI baseline is outperformed bya classifier that randomly predicts whether the tar-get fact is entailed. INFOGAP significantly outper-forms both (p < 0.05, with a bootstrap percentiletest; Efron and Tibshirani, 1994).",
  "Using INFOGAP to AnalyzeAsymmetries in LGBT Wikipedia Bios": "Having validated the effectiveness of INFOGAP, wemove onto applying it to answer questions aboutinformation gaps in Wikipedia. We focus on iden-tifying content differences between language ver-sions articles on LGBT public figures. Prior workby Park et al. (2021) identified that English articleson average portrayed these figures with more posi-tive sentiment, as well as greater power and agency(Sap et al., 2017), relative to articles in Russian andSpanish.To gain further insight into cross-linguistic varia-tion towards LGBT people portrayals, we draw onthe LGBTBIOCORPUS corpus (Park et al., 2021).The corpus comprises 1, 350 biographies of LGBTpeople, each paired with biographies of non-LGBTpeople matched on most social attributes exceptsexual orientation using the matching method in-troduced in Field et al. (2022). Given that INFO-GAP enables us to directly compare the contentbetween different language versions of a biography,we contend that our analysis can provide a moredirect characterization of differences in LGBT bios.Specifically, we look at En, Fr, and Ru Wikipedias.We consider the following research questions:RQ1: To what extent does factual knowledge differacross language versions of the same bio (Sec 3.2)?RQ2: Does a persons affiliation with the LGBTcommunity have an effect on the information gapin their bios (Sec 3.3)?RQ3: Can we use INFOGAP to identify sections toremediate (Sec 3.4)?These questions are intentionally ordered fromhigh-level (language-level) to low-level (individual-and fact-level) to demonstrate that INFOGAP en-ables both high-level quantitative analyses and ef-",
  "ThebaselineisavailableonHuggingFaceascross-encoder/nli-roberta-base": ": Distribution of information overlaps forLGBTBioCorpus. Top: Distribution over the percent-age of facts in En biographies also found in their Frand Ru counterparts. Bottom: Distribution over the per-centage of facts in Fr and Ru biographies also foundin their English counterparts. N = 2, 700 biographies.In general, En biographies contain more facts that areexclusive to En.",
  "Implementation Details": "The LGBTBIOCORPUS is significantly larger thanthe small set of 22 biographies from ,leading to high cost and runtime when applyingINFOGAP. Parsing Alan Turings biography withINFOGAP alone, for example, can require morethan 100K GPT-4 tokens.5 We thus use the GPT-4predictions to finetune smaller models that are moreefficient.Specifically, we use flan-t5-large(Chung et al., 2024) for both directions of the En/Frpair and mt5-large (Xue et al., 2020) for both di-rections of the En/Ru pair. We find that the T5variants perform well at modeling the annotationsfrom 2, obtaining macro-averaged F1 scores of0.90 (En Ru, Ru En) and 0.87 (En Fr, Fr En). We provide fine-tuning hyperparametersand validation set performances in Appendix D.",
  "We used gpt-4-1106-preview. At the time of writing,this API cost $30.00 USD/1M tokens": "in information overlap between language versionsof Wikipedia biographies at scale. Specifically, weconsider the INFOGAP predictions for the entirecorpus (LGBT and non-LGBT bios). visualizes the distribution of the numberof facts that can be found in both language versionsof the same bio. In the top-left subfigure, we showa histogram of the amount of information in theEn article that can also be found in the Fr article(En Fr). The median of the distribution is 0.35,indicating that for half of the biographies, only 35%of the information in the En article can be found inthe Fr article. By comparison, the median of the Fr En distribution is 0.55, much higher than themedian of the En Fr distribution, indicating thatEn biographies contain more unique informationthan their Fr counterparts.Considering En/Ru, we find that En articles con-tain significantly more unique information than Rucounterparts, with the median En Ru overlapbeing 0.23. Much of the information in the Ru ar-ticles meanwhile can be found in the En articles,with a median overlap of 0.66 for Ru En.We also note that the INFOGAP ratios reflect thewell known local heros effect, where biographiesof individuals whose nationality matches the lan-guage of the article tend to have greater coverage,length, and visibility (Callahan and Herring, 2011;Field et al., 2022; Hecht and Gergle, 2010; Oeberstand Ridderbecks, 2024). When the nationality ofthe person is Russian (66 people), the median En Ru overlap increases to 0.29 (+5%) while theRu En overlap decreases to 0.44 (22%). Simi-larly for French (148 people), the median En Froverlap increases to 0.52 (+17%), while the Fr En overlap decreases to 0.29 (26%). Overall, thisresult indicates that there are large scale disparitiesin information overlap ratios across language ver-sions, building on Callahan and Herrings (2011)early analysis.",
  "RQ2: Effect of LGBT Affiliation onInformation Gaps": "Given the large scale differences in content be-tween language versions, we turn to the questionof whether LGBT people biographies exhibit dif-ferent patterns of information overlap compared tonon-LGBT people. For example, do Russian bi-ographies tend to include or exclude certain typesof information depending on whether the biogra-phy is about an LGBT person? To investigate thisquestion, we fit a binomial regression model to de-",
  "termine which factors contribute to the inclusionof En facts in the corresponding Fr or Ru bios, andvice versa": "Features. displays the features we use,along with their coeffcient estimates.6 Naturally,we include a binary feature is_lgbt indicatingwhether the bio is of an LGBT person. Crucially,we also need to consider the connotation of facts inthe English article. Park et al. (2021) found that En-glish LGBT bios were portrayed with greater senti-ment, power, and agency than Russian bios. How-ever, this prior work cannot shed light on whetherthe difference in sentiment is due to Russian biosincluding negative sentiment facts that are not inthe English bios, excluding positive sentiment factsfrom the English bios, or both. We directly addressthis question using INFOGAP.To determine the connotation of a fact ei, wehave to consider the context in its original sen-tence, which requires mapping between a fact anda sentence. To map facts to their original sentences(e.g., Cook is on the board of directors of Nike Cook is also on the boards of directors of Nike,Inc. and the National Football Foundation), weuse forced alignment; see Appendix E.We obtain connotation predictions at the sen-",
  ": Distribution of implied sentiment about biog-raphy subjects for En, Fr, and Ru articles": "tence level by prompting a language model to de-termine whether a given sentence (in the context ofthe two prior sentences) portrays the subject of thebiography in a positive, negative, or neutral light.Similar to the distillation of the INFOGAP processto a smaller model (3.1), we first obtain connota-tion labels using GPT-4 for a smaller set of bios,and use those labels to finetune a smaller modelfor scaling to the full LGBTBIOCORPUS (see Ap-pendix E for details, including human annotation ofthe connotation labels). We use the sentence-levelconnotation label as the label for its constituentfacts. presents this label distribution. Regression Model.Without loss of generality,consider modeling the amount of information inthe En bios that is also present in the Fr bios, i.e.,the En Fr direction. To perform our binomialregression, we first partition each bio into threesets positive, negative, and neutral facts. Eachpartition represents one datapoint for fitting theregression model, so each bio contributes threedatapoints. Within each of these three partitions,some facts will also be present in F, while otherswill be exclusive to E. We model this using abayesian binomial regression model (McElreath,2018):",
  "overlap | Np 1 + conn + is_lgbt + is_lgbt:conn": "where conn gets the value of either conn_pos,conn_neg, or conn_neutral, depending on the in-put partition, Np is the number of facts in the cur-rent partition of E, and overlap is the number offacts that are also in F (at most Np). is_lgbt:connis an interaction between the two categorical vari-ables. See Appendix F for model-fitting details. Connotation is a predictive factor.Listed in, our results indicate that connotation is apredictive factor in nearly all language pairs and di-rections considered, except conn_neg in En Fr.Further, the polarity of the conn_pos andconn_neg factors is always negative, suggestingthat polarized facts tend to be included in lowerrates than neutral facts, which are more agreeableacross language versions. To ground the effect size of the coefficients, we can simulate predictionsfrom the regression model. For example, a valueof -0.07 for conn_pos in the En Fr model in-dicates that 34.4% of the positive facts in En areincluded in the Fr bios, compared to 36.6% of theneutral facts. Negative connotation facts are disproportion-ately included in Russian LGBT bios.Consid-ering Russian biographies, we draw from the largecoefficient value of the is_lgbt feature that factsfrom the English article are more likely to be ref-erenced when the article is about an LGBT publicfigure. Moreover, from the is_lgbt:conn_neg in-teraction, we find that negative facts are more likelyto be referenced than positive ones. To quantify thesize of this effect, we simulate posterior predictionsfrom the binomial regression model. We find an av-erage 50.87% of negative Russian facts are sharedwith the English biographies when they describean LGBT public figure, whereas only 38.53% ofnegative facts are shared with English bios whenthey are non-LGBT.",
  "RQ3: Identifying Sections to Remediate": "Our analysis in Sec 3.3 revealed that facts carry-ing a more polarizing (non-neutral) connotationsare less likely to be shared across language ver-sions. This suggests that many biographies maycarry a significantly different overall connotation,depending on the language version in which theyare read. Unlike the manual analysis performedin prior works (e.g., Park et al., 2021; Callahanand Herring, 2011, among others) to identify suchlanguage-version imbalanced content, INFOGAPcan automatically locate imbalanced content. Parket al. (2021) in particular focused on bios wherethe subject was portrayed with a more negativeimplied sentiment. Here, we focus on a differentaspect of sentiment differences: the omission ofcontent with positive implied sentiment from onelanguage version.Specifically, we follow these steps to identify im-balanced content: First, we identify bios from theLGBTBIOCORPUS where a high rate of positivefacts are excluded from one language version com-pared to another.7 Next, we introduce a method toidentify positive life events that are missing in thatlanguage version. We provide a formal argument",
  "demonstrating that our INFOGAP based methodfor identifying missing events is highly accurate.Finally, we conclude with examples of findings": "Step 1. Identifying biographies with imbalancedimplied sentiment.Consider a pair of articles Eand F written in different languages, and supposewe wanted to find bios where F omitted positivecontent at a high rate. We conduct a hypothesis testto determine whether the number of positive factsincluded in both languages is significantly lowerthan expected based on the overlap rate of neutralfacts. Concretely, we perform a bayesian hypoth-esis test based on the BetaBinomial distribution;we provide complete details in Appendix G.Our test identifies 274 imbalanced LGBT biogra-phies when considering En Ru and 236 whenconsidering En Fr. We can follow the same pro-cedure for finding English biographies that compar-atively lack positive information, when comparedto their French and Russian counterparts. We find105 and 199 biographies in the Ru En and Fr En direction, respectively. Step 2. Identifying events that are unique to alanguage version.Having identified biographiesthat could benefit from remediation, we next fo-cus on finding the positive-connotation carryingcontent that is missing from one language version.By comparison to Park et al. (2021), who couldonly analyze 10 biographies for identifying imbal-anced content, we can leverage INFOGAP to iden-tify imbalanced content at scale within the subsetof biographies we identified. We focus on findingpositive connotation events longer collections offacts that are thematically related rather than indi-vidual isolated facts since the omission of a wholeevent is more egregious. Practically speaking, wesearch for paragraphs V = e1, . . . , eNV where allfacts in the paragraph are missing from F:",
  "We then select a subset of M: paragraphs contain-ing at least one positive connotation fact": "INFOGAP is highly effective at identifying miss-ing events.Consider an event V = e1, . . . , eNVthat is described in article E. Suppose that INFO-GAP predicted that V is not covered by F, thatis that F does not entail any of the events in V:F ei, i [NV ]. For INFOGAP to be wrong,i.e., V is actually present in F, there needs toexist a subset of facts ei(1), . . . , ei(k) V, for",
  "Proposition 1 (Error Bound of Event Identificationthrough InfoGap). The probability of INFOGAPmaking k errors is exp(2(1 )2k), where is the error rate of the classifier when it predictsF ei": "Proof. Given that the error rate of the classifier is, the expected number of errors for k predictionsis k. However, the classifier made k mistakes, sowe have made k + (1 ) k errors, an additivefactor of t = (1)k more mistakes than expected.By Hoeffdings inequality (Appendix H), wherewe supply our expected value = k and thedeviation from the expected value of t = (1)k,we obtain an upper bound of: exp2(1 )2k2/k= exp(2(1 )2k). The significance of this claim is that it is rare forthe INFOGAP classifier to make a large number (k)of mistakes when the error rate is (where << 1).Moreover, the probability of mistakes decreasesvery quickly in the accuracy of the classifier andthe number of facts in V that were predicted to notbe entailed by F. As we showed empirically in, the INFOGAP classifier is reliable (low) and thus it has a strong capacity to find eventsthat are only described in one language version.8 Findings.In , we demonstrate positiveevents that are unique to one language versionwhen compared to another. We find that ChelseaMannings Fr page describes praise for her whistle-blowing during the Afghanistan war.The Frpage also discusses her whistleblowing on the AbuGhraib prison conditions (Hersh, 2004). Conspic-uously, both events are omitted from the En page,despite the En page being otherwise longer. Amer-ican perception of this instance of whistleblow-ing skewed negative (Pew Research Center, 2010),which may have played a role in the disparitiesbetween the En and Fr pages.We also find Tim Cooks Ru page but not his Enpage makes note of his fundraising initiative todefend Ukraine in the current Russo-Ukranian war.It is unsurprising that it appears in the Ru page, as it 8One shortcoming of this argument is if F discusses a com-pletely different aspect of the event V than E. We conjecturethat this is unlikely since both articles should at least containthe central propositions about the event. directly pertains to Russia. However, the omissionof this fact from the En page is remarkable, since ithad received some media attention from Americanoutlets (Clark and Schiffer, 2022). One reason forthis omission may be that there is a partisan divideon US involvement in the war (Pew Research Cen-ter, 2024). This fact may not have been included inEn to maintain a veneer of neutrality. It is important to note however that WikipediasNeutral Point of View policy advocates for a bal-anced representation of views (Matei and Dobrescu,2011), rather than outright filtering or censorship.Our findings raise questions about the degree towhich a cross-linguistically consistent NeutralPoint of View is realizable. INFOGAP enablesstudying these cross-linguistic differences in por-trayals of public figures at scale.",
  "Related Work": "AutomatedcomparisonofmultilingualWikipedia articles.We contribute to a largebody of work on understanding differencesbetween language versions of Wikipedia. Hechtand Gergle (2010) also compare Wikipedialanguage versions and consider their informationgaps, and later develop a web tool to bridge thesemultilingual gaps (Bao et al., 2012). However,their evaluation is at a higher level of abstraction:they look at whether or not two language versionshave on a topic.By comparison, we comparecontent differences between two language versionson the same topic. Duh et al. (2013) considereda pipeline similar to INFOGAP for the task ofkeeping multilingual Wikipedia documents con-sistent. However, their pipeline used embeddingsimilarity; in early experiments, we found thatusing embedding similarity for identifying poten-tial entailments performed very poorly (relativeto X-FACTMATCH). Massa and Scrinzi (2012)created a web tool that permits visual comparisonof Wikipedia articles in two different languages.Rodriguez et al. (2023) also perform comparativeanalyses across language versions in Wikipedia.However, they consider more fine-grained contentdifferences between pairs of the most closelyrelated paragraphs between different languageversions article on a topic. Their method wasnot designed for computing the overall articlelevel overlaps and differences of the form wedemonstrate in .",
  "PairPersonEvents": "En , Ru Tim CookIn 2022, following Russias invasion of Ukraine, Tim Cook called on the companysemployees to donate to help Ukraine. Apples CEO announced the decision to suspendsales of equipment in Russia and also said that the company would triple the amount ofdonations made by employees to support Ukraine, and this would be retroactive to February25, 2022. En , Fr Chelsea ManningRon Paul, a leader of the libertarian movement within the Republican Party, endorsedManning on April 12, 2013, stating that Manning had done more for peace thanObamareferring to Obamas 2009 Nobel Peace Prize win: While President Obamawas initiating and expanding unconstitutional wars abroad, Manning, whose actions causedexactly zero deaths, was shining a light on the truth behind those wars. Which of the twohas done more for peace is clear. En , Fr Caster SemenyaIn 2010, the British magazine New Statesman included Semenya in its annual list of50 People That Matter for unintentionally instigating an international and often ill-tempered debate on gender politics, feminism, and race, becoming an inspiration to gendercampaigners around the world En , Ru Ada ColauDuring her period as mayor of Barcelona, Colau has maintained a political stance againstactivities that are susceptible of contributing to greenhouse gas emissions and air pollution.She has repeatedly opposed the expansion of El Prat airport and the use of private cars inthe city, and has pushed regional authorities to restrict the number of cruise ships arrivalsin Barcelona. In 2020 she declared a climate emergency, advocating limiting the con-sumption of meat at schools and forbidding councillors from using the Barcelona-Madridair shuttle. : Examples of events from biographies that contain a large number of positive facts that are only containedin one language version of the article relative to another. We provide translations (Google Translate) for the first tworows, rather than the original French and Russian content. Case studies on cultural differences in multilin-gual Wikipedia.We highlight two studies thatwere not mentioned elsewhere in this work. Hick-man et al. (2021) analyze how a boundary disputeover Kashmir between India and Pakistan is rep-resented in English, Hindi, and Urdu Wikipedia,analyzing how the Neutral Point of View principleis upheld. They find there is a sizeable number ofcross-language editors between Urdu and English,as well as Hindi and English, but not Urdu andHindi, attributing this to the popularity of EnglishWikipedia. Kharazian et al. (2024) studied howthe Croatian language version of Wikipedia wasusurped by a small group of editors who aimed topromote far-right bias and disinformation about var-ious Croatian political figures, groups, and events.This bias was apparent when comparing the Croat-ian articles to Serbian and English ones.",
  "Conclusion": "We presented INFOGAP, a reliable method for ef-ficient comparative analysis between two narra-tives on the same topic written in different lan-guages. We deployed the method to discover dif-ferences in LGBT peoples portrayals, locatingshared facts, as well as information gaps and in-consistencies across 2.7K English, Russian, andFrench Wikipedia biography pages. INFOGAP can be directly applied beyond analyzing differencesin multilingual Wikipedia biographies. Analyzingvariation in topic coverage is at the heart of muchresearch in the social sciences, from understandingmedia manipulation strategies (Field et al., 2018),to analyzing differences in argumentation from dif-ferent stances in a contentious debate (Luo et al.,2020), to analyzing quotation patterns in partisanmedia (Niculae et al., 2015). Overall, our researchlays the foundation for enabling targeted, nuancedtextual comparative analyses at scale.",
  "Limitations": "Applicability to specialized domains.Ourmethod relies on the language understanding abil-ities of the underlying language model (GPT-4 inour case). While we were able to achieve highaccuracy on the LGBTBIOCORPUS, it is not guar-anteed that similarly high-accuracy can be achievedif we were to apply INFOGAP to more specializeddomains, where domain expertise may be requiredto assess the equivalence of two facts in differentlanguages, such as comparing Wikipedia articlesconcerning scientific topics.",
  "Connotation is subjective.In we in-vestigated the effect of connotation on the inclu-sion of facts in different language versions. We": "acknowledge that connotation is fairly subjective,and may depend on a readers stance towards thetopic and their cultural background. To ensure ahigh degree of replicability of our results, we havereleased our all of the finetuned models we appliedin 3, including the connotation models. Ablations of INFOGAP components.We didnot perform ablations of the components of the X-FACTALIGN step in the INFOGAP pipeline (2).Our aim was to demonstrate that high-quality auto-matic cross-lingual comparative analysis is not onlypossible (2.3) but provides considerable benefitsin downstream analyses (3). We will perform thor-ough ablations with a larger number of annotatedsamples in future work.",
  "Data.The dataset used in this study, LGBTBIO-CORPUS, is publicly available": "Models.We used language models to make clas-sification predictions, limiting their ability to gen-erate offensive content. We used a closed-sourcemodel, GPT-4, which entails high costs, and maynot be suitable for applying our method to differentdatasets, especially those containing private infor-mation. The distilled version of INFOGAP, whichuses open-source models, addresses both concerns.",
  "Acknowledgements": "We thank Shreya Prakash for advice on our regres-sion analyses and hypothesis testing. We thankMiikka Silfverberg and Jai Aggarwal for helpfulfeedback on the manuscript. FS is supported by anNSERC PGS-D scholarship. VS is supported bythe Vector Institute for AI, the CIFAR AI Chair pro-gram, and NSERC. We gratefully acknowledge sup-port from the National Science Foundation underCAREER Grant No. IIS2142739, and NSF grantsNo. IIS2125201 and IIS2203097. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774.",
  "Linguistics, pages 31973203, Florence, Italy. Asso-ciation for Computational Linguistics": "Patti Bao, Brent Hecht, Samuel Carton, MahmoodQuaderi, Michael Horn, and Darren Gergle. 2012.Omnipedia: bridging the wikipedia language gap. InProceedings of the SIGCHI Conference on HumanFactors in Computing Systems, pages 10751084. Samuel R. Bowman, Gabor Angeli, Christopher Potts,and Christopher D. Manning. 2015. A large anno-tated corpus for learning natural language inference.In Proceedings of the 2015 Conference on Empiri-cal Methods in Natural Language Processing, pages632642, Lisbon, Portugal. Association for Compu-tational Linguistics.",
  "Matthew D Hoffman, Andrew Gelman, et al. 2014. Theno-u-turn sampler: adaptively setting path lengthsin hamiltonian monte carlo. J. Mach. Learn. Res.,15(1):15931623": "Ryo Kamoi, Tanya Goyal, Juan Diego Rodriguez, andGreg Durrett. 2023. WiCE: Real-world entailmentfor claims in Wikipedia. In Proceedings of the 2023Conference on Empirical Methods in Natural Lan-guage Processing, pages 75617583, Singapore. As-sociation for Computational Linguistics. Zarine Kharazian, Kate Starbird, and Benjamin MakoHill. 2024. Governance capture in a self-governingcommunity: A qualitative comparison of the croat-ian, serbian, bosnian, and serbo-croatian wikipedias.Proceedings of the ACM on Human-Computer Inter-action, 8(CSCW1):126. Omar Khattab and Matei Zaharia. 2020. Colbert: Effi-cient and effective passage search via contextualizedlate interaction over bert. In Proceedings of the 43rdInternational ACM SIGIR conference on researchand development in Information Retrieval, pages 3948.",
  "Michael Mitzenmacher and Eli Upfal. 2017. Probabil-ity and computing: Randomization and probabilistictechniques in algorithms and data analysis. Cam-bridge university press": "Matteo Negri, Alessandro Marchetti, Yashar Mehdad,Luisa Bentivogli, and Danilo Giampiccolo. 2012.Semeval-2012 task 8: Cross-lingual textual entail-ment for content synchronization. In *SEM 2012:The First Joint Conference on Lexical and Compu-tational Semantics Volume 1: Proceedings of themain conference and the shared task, and Volume2: Proceedings of the Sixth International Workshopon Semantic Evaluation (SemEval 2012), pages 399407, Montral, Canada. Association for Computa-tional Linguistics. Vlad Niculae, Caroline Suen, Justine Zhang, CristianDanescu-Niculescu-Mizil, and Jure Leskovec. 2015.Quotus: The structure of political media coverage asrevealed by quoting patterns. In Proceedings of the24th International Conference on World Wide Web,pages 798808.",
  "CSeed biographies": "In and , we list the seed set of bi-ographies, that were used for obtaining INFOGAPlabels. We performed our human annotation ex-periment 2.3 for INFOGAP on these labels. Wethen used these labels to distill flan-t5-largeand mt5-large for our analyses in 3.We also used these seed biographies for obtain-ing connotation labels from GPT-4 for our anal-ysis in 3, which we then also used to distillinto flan-t5-large and mt5-large for predict-ing connotation labels at a larger scale.",
  ": Parameters provided to the HugggingFace trainer for the flan-t5-large and mt5-large models": "split of 0.9/0.1. As for evaluation metrics, weused Rouge-1 for fact decomposition (2.1), andMicro-F1 for X-FACTMATCH (2.2) and connota-tion prediction (3.3). For the En Fr and En Fr directions, we apply the flan-t5 models. Weobtained strong validation set performance (0.85Rouge-1; 0.85 and 0.88 F1s for the connotationprediction and X-FACTMATCH tasks, respectively)using the same hyperparameter settings across allthree tasks. We found that flan-t5-large did not general-ize well to Ru, obtaining poor performance in factdecomposition and often predicting nonsensicalRussian strings. We thus resorted to mt5-large in-stead (Xue et al., 2020), since Russian is one of thelargest languages in terms of its pre-training datasizes. After a hyperparameter sweep over learn-ing rates, gradient accumulation sizes, and weightdecay values, we found much better performancewith mT5, obtaining validation set performances of0.89, 0.79, and 0.86 for fact decomposition, con-notation prediction, and X-FACTMATCH tasks, re-spectively.",
  "EConnotation modeling": "Forced alignment procedure.As mentioned in3.3, we applied forced alignment to assign de-composed facts back into their original full sen-tences. Forced alignment is a constrained versionof Dynamic Time Warping, where the alignment ismonotonic. Forced alignment requires a distancefunction, we used hubness-corrected distance (Sec-tion 2.1). Connotation prompts. We provide the promptsused for obtaining connotation labels in .The content variable contains up to 3 sentences,si2, si1, si. While were interested in the conno-tation towards person_name conveyed in the lastsentence si, we provide the prior two sentences formore context. We prompted for both connotationlabels and rationales for the labels, after findingthat prompting for a rationale prevented the mod-els from vastly overextending the neutral label.This aligns with prior research on text classifica-tion, where generating rationales improved accu-racy (Wiegreffe et al., 2021). EnThe pronoun for {person_name} is{pronoun}.Does the following textabout {person_name} imply a positive,neutral, or negative sentiment towards{person_name}? Explain why in one sen-tence. Write your response in JSON for-mat with two keys: label and explanation).\\n {content} (pos/neutral/neg) FrLepronomdu{person_name}est{pronoun}. Est-ce que le texte suivantau sujet de {person_name} implique unsentiment positif, neutre ou ngatif envers{person_name}?Expliquez pourquoien une phrase.crivez votre rponseen format JSON avec deux cls:ti-quette et explication).\\n{content}(pos/neutral/neg)",
  "E.1Validation of connotation labelpredictions": "To validate the connotation labels predicted in 3.3,we sampled 10 positive, 10 negative, and 10 neu-tral connotation label predictions from Appendix Cfor each of the 3 languages, thus obtaining 90 data-points in total. One co-author then annotated eachdatapoint manually, and compared the annotationsagainst the labels predicted by GPT-4 from the con-notation prompt in Appendix E.We provide the results of this classification in. We find that the connotation predictionsare generally reliable, with all errors stemmingfrom confusion between neutral and positive,or neutral and positive, rather than the moresevere error of confusing positive and negativelabels. This aligns with observations in previousresearch on computational modeling of connotation(Park et al., 2021; Rashkin et al., 2015; Sap et al.,2017, among others). Distillation.Having validated the quality of theGPT-4 connotation predictions, we use the pre-dicted labels to finetune more scalable, lightweightmodels for predicting the connotation labels. Weprovide hyperparameter details in Appendix D.",
  "GIdentifying biographies with a positiveconnotation imbalance across languageversions": "Plan.We consider the En Fr direction foran arbitrary bio, without loss of generality. Wewill use the amount of neutral facts shared by botharticles to parameterize a BetaBinomial distribu-tion. After fitting this distribution, we will simulatedraws from it to predict how much positive infor-mation should be shared by both articles. When theactual amount of shared positive connotation factsis much lower than the amount predicted by thefitted BetaBinomial distribution, we can considerthis an imbalanced biography for the En Frdirection.",
  "Implementation.We first set the prior forthe neutral fact distribution to uniform (prior": "to observing the actual neutral overlap ratio):Beta(1, 1). We leverage the useful fact that the pos-terior distribution after observing x neutral factsei(1), . . . , ei(x) in both En and Fr out of n totalfacts in En is Beta(1 + x, 1 + n x) (MacKay,2003).We can then simulate draws from theBetaBinomial distribution, first drawing a samplefrom Beta(1+x, 1+nx), followed by predictingamount of En facts that should also be found in Fr.The number of trials is fixed to the total number ofpositive facts in the En article.Thus, this binomial distribution tells us the num-ber of positive facts we would expect to see inboth articles, if positive facts were not omitted ata higher rate than neutral facts. We can then drawS = 1000 samples, counting the number of timesK the expected amount of shared positive connota-tion facts is higher than the actual amount. WhenK/S is close to 1.0, there is a large amount ofpositive information being omitted the Fr article,compared to the En one. We use 1 K/S as ap-value, with an = 0.05.We emphasize further that this method can beapplied in either direction (e.g., En Fr, orFr En), as well as for finding negatively im-balanced biographies, where one language versionincludes negative content at a rate much higher thanexpected under the neutral rate."
}