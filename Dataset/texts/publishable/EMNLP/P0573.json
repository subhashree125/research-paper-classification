{
  "Abstract": "Retrieval models are often evaluated onpartially-annotated datasets.Each query ismapped to a few relevant texts and the remain-ing corpus is assumed to be irrelevant. As aresult, models that successfully retrieve falselylabeled negatives are punished in evaluation.Unfortunately, completely annotating all textsfor every query is not resource efficient. In thiswork, we show that using partially-annotateddatasets in evaluation can paint a distortedpicture. We curate D-MERIT, a passage re-trieval evaluation set from Wikipedia, aspir-ing to contain all relevant passages for eachquery. Queries describe a group (e.g., journalsabout linguistics) and relevant passages areevidence that entities belong to the group (e.g.,a passage indicating that Language is a jour-nal about linguistics). We show that evaluatingon a dataset containing annotations for only asubset of the relevant passages might result inmisleading ranking of the retrieval systems andthat as more relevant texts are included in theevaluation set, the rankings converge. We pro-pose our dataset as a resource for evaluationand our study as a recommendation for balancebetween resource-efficiency and reliable evalu-ation when annotating evaluation sets for textretrieval. Our dataset can be downloaded from",
  "Introduction": "Passage retrieval, the task of retrieving relevantpassages for a given query from a large corpus, isa traditional IR task (Kaszkiel and Zobel, 1997;Callan, 1994; Zobel et al., 1995). Within NLP,it has many applications, such as Open-DomainQuestion-Answering (ODQA) (Karpukhin et al.,2020; Zhu et al., 2021; Mavi et al., 2022; Rogerset al., 2023) and fact verification (Bekoulis et al.,2021; Murayama, 2021; Vallayil et al., 2023).",
  "QUERY": ": Demonstrating the evidence retrieval taskdescribed in .2. The query is Names of firstworld war camoufleurs. Highlighted text correspondsto the query requirements: names (green), First WorldWar (red), and camouflage (orange). A passage mustmatch all requirements to be considered as evidence. Recently, the task has experienced a renaissancedue to the modern retrieval-augmented-generationsetup leveraging LLMs (aka RAG) (Lewis et al.,2021; Cai et al., 2022; Li et al., 2022). In all ofthose cases, retrieval makes for a crucial compo-nent of the system (Cai et al., 2022; Ram et al.,2023).It is common practice, and often essential toevaluate the retriever component separately fromthe full system. This is done by using large-scaledata resources that map queries to relevant pas-sages.1 The vast majority of available datasets areonly partially-annotated; a query is mapped to asingle (or a few) relevant passages and all otherpassages are assumed to be irrelevant (Bajaj et al.,2018; Kwiatkowski et al., 2019), leading to manypassages falsely labeled as negatives in the dataset.This practice has long been contested (Zobel, 1998;Buckley and Voorhees, 2004; Craswell et al., 2020;",
  "Relevancy is defined according to the task in hand. In thiswork, we adopt the definition of TREC (Craswell et al., 2020),a popular retrieval research challenge": "Gupta and MacAvaney, 2022), yet due to the mas-sive size of modern corpora, exhaustively annotat-ing all passages for every query is highly imprac-tical. As an example, MS-MARCO (Bajaj et al.,2018) consists of ~1M queries and ~8.8M passages,which amounts to ~8.8 trillion annotations.Evaluating retrieval solutions using a partially-annotated dataset is obviously not ideal. A sys-tem retrieving a non-annotated relevant passagerather than an annotated one is unjustly penalized.Some work has been done on metrics and meth-ods attempting to deal with this issue (Buckley andVoorhees, 2004; Yilmaz and Aslam, 2006; MacA-vaney and Soldaini, 2023). However, the commonpractice is still using vanilla metrics (e.g. MRR,Recall), and the impact of partial annotation dur-ing evaluation using these metrics is still unclear.Does the ranking of systems change? Do the in-accurate scores falsely crown the wrong systemsas the SOTAs? Moreover, we wonder how manyrelevant passages are needed in order to sufficientlyreduce the error and correctly rank systems.In this work, we propose D-MERIT; Dataset forMulti-Evidence Retrieval Testing, an evaluation setfor retrieval systems, striving to pair each query toall of its relevant passages. In our setting, relevantpassages are evidence that some entity belongs toa group described in the query. While we use it toexplore the consequences of having an evaluationdataset with only a few relevant passages annotated,D-MERIT is also highly suitable for use in high-recall settings, where the task is to retrieve as manyrelevant texts as possible for a given query, as itcontains almost all relevant passages available inthe corpus for each query.We first show that evaluation of systems withthe common single-relevant setup (for each query,annotate passages until a single relevant passageis found) is sensitive to the way in which passageswere selected during annotation. As a result, dif-ferent selections lead to different rankings of sys-tems. However, we observe that when a systemvery significantly outperforms another, represent-ing a seminal improvement or breakthrough, thesingle-relevant setup is likely to provide accuraterankings. Then, we mimic partially-annotated se-tups, gradually adding annotated relevant passagesto queries, hence reducing the number of falsely la-beled negatives in the data. Our findings reveal thatin order to reliably evaluate retrieval systems thatare reasonably close in performance, a significantportion of relevant passages must be found. This is substantial because it implies that when evaluat-ing using partially-annotated datasets, some systemmight seem better-performing than another, whilein fact, the opposite is true. To summarize, ourcontributions are as follows:",
  "Desiderata": "To observe the impact of having falsely labeled neg-atives in an evaluation set, we need to have a datasetwhere the falsely labeled negatives are marked assuch. This calls for a completely-annotated dataset,that will allow us to reliably evaluate systems per-formance, as well as examine the effects of partial-annotation. To accentuate the gap between partialand full annotation, queries in the dataset should bemapped to many relevant passages. We are set totry to identify all relevant passages for each query,but annotating all passages for each query is un-realistic. Therefore, we desire a framework thatoffers inherent mappings between queries and highquality candidate passages. To push our methodtowards exhaustiveness, our automatic approach tocandidate collection needs to lean towards recall,followed by an automatic filtering stage.",
  "Task Definition": "Evidence Retrieval.We choose evidence re-trieval as our task as it naturally complements ourneed to collect queries with numerous relevant pas-sages. In this task, passages are considered relevantif they contain text that can be seen as evidence thatsome answer satisfies the query. Previous work con-sidering this task did not collect more than a singleevidence (Malaviya et al., 2023; Amouyal et al.,2023) or did not aspire to be completely-annotated(Zhong et al., 2022). Instead, they map queries toanswers, and collect evidence for each answer froma single document. Our goal is to map a query toall evidence in the corpus, without the limitationof a single document. Our setup.In our setup, that can be seen as an ex-tension of the single-evidence setup in (Malaviyaet al., 2023) to an all-evidence one, a query de-scribes a group of entities and relevant passages areevidence that an entity is a member of the group.The task is then, given a query representing somegroup, to retrieve all texts stating that some entityis a part of this group. For instance, showsevidence for the query names of first World Warcamoufleurs. The first passage confirms FredrickJudd Waugh is an entity that belongs to the groupof World War 1 camoufleurs. More concretely,each query lists constraints, and an evidence wouldassociate an entity with all of them.2 In the exam-ple above, a query describes the group of all WorldWar 1 camoufleurs, an evidence would then needto indicate an entity (1) took part in World War 1;(2) was a camoufleur. For example, the second pas-sage in states Abbot Thayer advocated forcoloration and countershading camouflage duringWorld War 1, which satisfies these requirements.",
  "Dataset Curation": "We adopt the Wikipedia framework3, which al-lows us to take advantage of the Wikidata struc-ture (Vrandecic and Krtzsch, 2014) to extractgroups and their corresponding members. We usethe Wikipedia link network to obtain mappings be-tween an article and all other articles referencingit. Our curation process involves three stages: (1)collecting queries and candidates all passageswith high likelihood of containing evidence (Sec-tion 2.3.2); (2) automatic annotation of candidatepassages (.3.3); (3) generating natural lan-guage queries (.5).",
  "Corpus": "Our corpus is limited to the introduction sectionof Wikipedia articles. Without limiting our collec-tion process to a specific section, the number ofannotations per article would have multiplied by~5, which would have made the annotation processsignificantly more expensive. We opted to focus onthe introduction section, because it is a section thatis consistent across most articles, and it is intuitivethat many evidence lie there. In total, our corpus iscomprised of 6, 477, 139 passages.",
  "The queries in our setup are somewhat reminiscent to theintersection queries in (Malaviya et al., 2023), where a querymakes for a list of requirements.3The Wikidump is from July 1st, 2023": "2.3.2Query and Candidate CollectionExtracting list members.The collection processbegins by scanning articles prefixed with list offor tables using the Wikidata format. We extractcolumns with name in their title, as these aremost likely to describe entities. Each such columnis extracted separately and makes for a set of mem-bers. Columns containing empty values or valueswithout a dedicated Wiki article are discarded. Collecting candidatesWe employ the \"WhatLinks Here\" feature from Wikidata. This tool pro-vides a list of all articles that reference a specificarticle (and its aliases). The reference count of anarticle can vary significantly, even for members ofthe same list. For example, Shogi has over 600references, while Machi Koro only has 9. Bothappear in the group Japanese board games. Tomanage this disparity and keep the candidate countfeasible, we discard columns containing an articlewith more than 10K references. 2.3.3Evidence IdentificationTo complete the dataset construction, we need tosift through the collected candidates. Human evalu-ation would have been the most reliable route, how-ever, it does not scale. We thus turn to the currentstate-of-the-art large language model for automaticfiltering, and show it nears human judgement. Automatic identification.We use GPT-44 to fil-ter 250K passages across 2.5K queries. Eachprompt consists of a passage paired with a queryembedded in our definition of relevance, askingthe model to judge for relevance. To ensure eachquery is meaningful in number of evidence, querieswith less than five evidence were discarded. Fortechnical details, see Appendix C.",
  "Evaluation of Construction Process": "In order for D-MERIT to contain a significant por-tion of the positives for each query, some assump-tions need to hold. First, Wikipedia list pages needto be exhaustive.5 This is a common assumptionalso taken by (Amouyal et al., 2023) and (Malaviyaet al., 2023). Our dataset construction methodalso relies on the accuracy of Wikipedias linkingnetwork. This is a limitation of the method (and 4We used GPT-4-1106-preview.Future references toGPT-4 refer to this version.5Note that we only need the list to be exhaustive withrespect to the corpus, i.e. if some set member is not in the listbut is also not mentioned in Wikipedia introductions, it willnot hinder the exhaustiveness of our collection method.",
  "Mill CreekIsland": "Mill Creek Island is a bar island on the Ohio River in TylerCounty, West Virginia. The island lies upstream from GrandviewIsland and the towns of New Matamoras, Ohio and Friendly, WestVirginia. It takes its name from Mill Creek, which empties intothe Ohio River from the Ohio side in its vicinity. Mill CreekIsland is protected as part of the Ohio River Islands NationalWildlife Refuge.",
  "DaveTretowicz": "Dave Tretowicz (born March 15, 1969) is an American formerprofessional ice hockey player. In 1988, he was drafted in theNHL by the Calgary Flames. He competed in the menstournament at the 1992 Winter Olympics. : Examples of records in our dataset. Query is the generated natural-language query describing a group.Member is an entity that belongs to the group described by the query. Candidate is the Wikipedia article fromwhich the evidence is taken from. Evidence is a passage indicating the members association with the group. is therefore mentioned in the limitations section).Herein, we want to show these assumptions do notmeaningfully degrade the quality of the dataset. Tothis end, we approximate D-MERITs complete-ness and soundness by evaluating the candidatecollection process if we have missed a meaning-ful number of evidence during candidate collection.To complete the evaluation of D-MERITs qual-ity, we also evaluate our automatic identificationmodel, GPT-4, to confirm it reliably identifies thevast majority of evidence without adding muchfalse positives. Evaluation tasks.We turn to Amazon Mechani-cal Turk (AMT) for sourcing human raters. For thecandidate collection evaluation, a human rater isprovided with a passage and a prompt containingthe query, and is requested to mark whether thepassage is evidence or not. In the task designed togauge the quality of the automatic identification,in addition to the passage and prompt, the annota-tion of GPT-4 is also provided. The rater is thenrequested to judge the correctness of the annota-tion. Since judging relevance can be subtle6, wemake a decision to judge the correctness of annota-tions, instead of to annotate and compare results toGPT-4. This encourages the rater to consider the an-notations perspective and allows tolerance towardborderline cases. The selection and conditioningprocess of human raters is detailed in Appendix C. 6Consider row 2 in , where the passage does not ex-plicitly say that Ohio River Islands National Wildlife Refugeis in West Virginia. Instead, it says that Mill Creek Island,which is in West Virginia, is part of the Ohio River IslandsNational Wildlife Refuge. Exhaustiveness of candidate collection.To en-sure our collection process is nearly exhaustive, weneed another evidence collection process, indepen-dent of ours. We thus adopt the popular TREC ap-proach (Craswell et al., 2020), where a number ofsystems retrieve the top-k passages given a query,and are then unified to a single set of passages tobe judged for relevancy. We use 12 different sys-tems, described in .1. As for the pooldepth, we select k = 20 to match our experimentalstudy. Several works researched the relation be-tween pool depth and the completeness of TRECevaluations (Buckley et al., 2007; Keenan et al.,2001; Lu et al., 2016) raising concerns regardingreliability of the shallow pool depth commonlyused (the typical TREC setup uses a k = 10 depth),hence we also extrapolate the results of this evalua-tion to a k = 100 pool depth. We select 23 random queries from D-MERIT,and use the TREC approach to retrieve 2, 329unique passages. Since we are looking for rele-vant passages that we missed, we discard uniquepassages that were already annotated by our pro-cess (311 such cases, all relevant) and are left with2, 018 passages. We ask human raters to mark theremaining passages for relevance and find only 35new evidence. In total, the TREC process finds346 relevant passages, 311 of which were foundby our process too. To put this in context, for thesame 23 queries, our process finds 990 relevantpassages. We note that while our method retrievesmany more evidence, it is tailor-made to the Wiki-data format, while the method from TREC can beapplied to any corpus. To further attest to the ex- haustiveness of our approach, we extrapolate theanalysis to k = 100, and estimate the number ofidentified evidence to increase to 638, with only60 new evidence. A more profound discussion ofTRECs coverage, including details on the extrapo-lation process, can be viewed in Appendix E.To summarize, the TREC process, with a pooldepth of k = 20, finds 346 positives and requires2, 329 annotations ( 14.9% positives in the pool).Our method finds 990 positives, requiring 3, 206annotations ( 30% positives in the pool). TheTREC process adds only 3.5% new positivesto our method. When TREC is extrapolated toa pool depth of k = 100, D-MERIT still has ahigh (estimated) coverage of 94.5% of identifiedevidence. Comparing automatic to manual identification.To verify GPT-4 is comparable to manual identifica-tion, we collect a random sample of 1, 300 (query,passage) pairs, consisting of 650 evidence. Out ofall the samples, the rater agrees with GPT-4 84.7%of the time.7 Specifically, they disagreed with themodel on 141 cases of relevant and only 57 casesof not relevant.",
  "D-MERIT Overview": "The final dataset comprises 1, 196 queries, encom-passing 60, 333 evidence in total. There are 50.44evidence per query on average, and a median of22, ranging from a minimum of 5 to a maximumof 682 evidence. On average, each group membercontributes about 2 evidence to a query, with 61.8%of the evidence coming from articles other than themembers own articles. The average number ofmembers per query stands at 23.71. We note thatit is possible for some members to not contributeany evidence to a query, for example, when theevidence is not in the introduction. In weshow the members and evidence distributions, andthe relation between the number of members andnumber of evidence mapped to a query. 7To further validate this number, we check agreement be-tween two expert annotators. On 400 examples, a 94% agree-ment is reached. This indicates that the task is less subjectivethan general relevance tasks which tend to have a lower agree-ment, explaining the relatively high human-GPT agreement.",
  "Setup": "Systems.To ensure our analysis is unbiasedtowards a specific retrieval paradigm, we uti-lize the Pyserini information retrieval toolkit (Linet al., 2021a) to experiment across twelve di-verse, out-of-the-box systems: five sparse, fourdense, and three hybrid systems. (1) In the sparsecategory; BM25 (Robertson and Walker, 1994),QLD (Zhai and Lafferty, 2001), UniCoil (Lin andMa, 2021), SPLADEv2 (Formal et al., 2021) andSPLADE++ (Formal et al., 2022). (2) For the densemethods; DPR (Karpukhin et al., 2020), coCon-denser (Gao and Callan, 2022), RetroMAE-distill(Xiao et al., 2022), and TCT-Colbert-V2 (Lin et al.,2021b). (3) In the hybrid category; TCT-Colbert-V2-Hybrid (Lin et al., 2021b), coCondenser-Hybrid, and RetroMAE-Hybrid. Further detailsregarding the systems can be found in Appendix B. Evaluation metrics.Needing a metric to quan-tify the ability of systems to retrieve multiple evi-dence, we opt to use recall@k as this is a simple,common metric for this task. For brevity, we re-port recall@20 in the main paper, and show resultson recall@5, recall@50, and recall@100 in Ap-pendix F. We note that other k values show similar trends to k=20, and conclusions drawn in this pa-per generalize to other k values reported as well.Other suitable metrics (NDCG, MAP, R-precision)are discussed and reported in Appendix A. Afterevaluating the performance of each system, weare interested in comparing the recall-based rank-ing of systems to quantify the gap between thepartially- and fully-annotated settings. We utilizeKendall- (Kendall, 1938), which can intuitively beunderstood as a measure of similarity between tworanking orders. This metric evaluates the numberof pairwise agreements (concordant pairs) versusdisagreements (discordant pairs) in the ranking or-der of systems between the two settings. A highKendall- score (close to 1) indicates a strong cor-relation, signifying that the rankings in the partially-and fully-annotated settings are similar, whereas alow score (close to 1) suggests major differences.Specifically, if we have n systems, and C is thenumber of concordant pairs while D is the numberof discordant pairs, then Kendall- is given by theformula = CD",
  "Is the single-relevant setup reliable?": "To assess the single-relevant setup, we start by ran-domly sampling an evidence for each query. Weevaluate each system on the formed single-relevantevaluation set and compare the resulting systemranking to the ground-truth ranking formed us-ing the fully-annotated dataset. To mitigate therandomness, we run this experiment 1, 000 times,and find that the mean ( std) Kendall- valueis 0.936 (0.038), translating to an error-rate of3.2%. These numbers suggest that sampling a ran-dom evidence for each query leads to reliable re-sults. Unfortunately, in order to properly randomlysample an evidence, one would need to annotate anon-feasible amount of passages in most datasets.8",
  "For example, in the 2020 TREC challenge (Craswell et al.,": "2021), operating on the MS-MARCO (Bajaj et al., 2018)dataset, 11, 386 relevant passages were found for 54 queries,an average of 210 per query. In Appendix E we estimate theseare only 50% of the actual relevant passages leading toroughly 500 per query. Given the corpus size, 8M pas-sages, one would need 16K annotations on average to finda single relevant passage randomly for a single query.",
  ": Kendall- similarities and error-rate for thedifferent biases in a single-annotation setup": ": Selection techniques for a single-relevant set-ting. The x-axis denotes systems used to select passagesfor annotation. Each tick represents the performance ofsystems on the same dataset with different annotations.An intersection demonstrates a swap in rankings. In practice, some method is used to select thepassages sent for annotation. This method is usu-ally biased9. To determine whether selecting anevidence in a biased manner is problematic or not,we explore 3 biases: most popular selects the mostpopular10 evidence for each query. We also con-sider a length-selection approach, which considersthe number of words in a given passage, by select-ing the longest and shortest evidence available foreach query. Results are presented in . Itcan be seen that as opposed to random selection, inthe more likely scenario of a biased selection theerror-rate is much higher, suggesting that the single-relevant setting is unreliable. A popular techniquefor sampling passages for annotation is using an ex-isting retrieval system, and annotating passages inthe order they are retrieved until a relevant passage 9For example, it has been shown that models tend to sufferfrom popularity bias (Gupta and MacAvaney, 2022) and thatsparse methods tend to prefer longer texts over shorter oneswhile a human annotator is likely to prefer shorter texts.10We define popularity as the number of times an article isreferenced, which can be derived using the What Links Herefeature from .3.2. is found. We simulate this by considering each ofour 12 considered retrievers as the base system. Wethen evaluate all of the systems on the 12 formedevaluation sets. Results are plotted in . Thegraph shows that the selection technique, used topick which passages are annotated, has a majoreffect on the systems measured performance andon the ranking of the different systems. For exam-ple, when choosing evidence using BM-25, QLD isranked as the best system (excluding BM-25 itself),while when choosing evidence using either coCon-denser, coCondenser-Hybrid, DPR or TCT-Colbert,QLD is the worst performing system. For other sys-tems selecting evidence, it is ranked somewhere inbetween. When comparing the 12 rankings formedusing these evaluation sets to the ranking formedby the completely annotated dataset, the averageKendall- score computed is 0.616, translating toan average error-rate of 19.2%.11 indicatesthat system-based selection is indeed closer to bi-ased selection than it is to random selection. Insummary, the experiments presented in this sectionshow that while random selection of evidence canlead to reliable results in the single-relevant sce-nario, the more realistic case (where the annotatedevidence is not randomly selected) is prone to gen-erating misleading results and ranking of systems.",
  "Is the single-relevant scenario enoughwhen systems are significantly separated?": "After establishing that there are cases where thesingle-relevant scenario is not reliable, we ask inwhat cases it can be sufficient. To explore this,we first define buckets of pairs of systems as fol-lows. A pair of systems (A, B) is in a [pmin, pmax)bucket if A is better performing than B, and thestatistical significance computation for the differ-ence between these two systems leads to a p-valueof at least pmin and at most pmax, using a rel-ative t-test, as computed on the fully annotatedevaluation set. We then repeat the final experi-ment described in .2, but when calculat-ing Kendall- and its error-rate we only considerpairs of systems that fall in some bucket. We de-note this measure as partial-Kendall-.12 We con- 11We eliminate the system used to select the evidence fromthe computation, as it generates artificial swaps. For examplewhen computing the Kendall- for the ranking formed bychoosing the first evidence as ranked by BM-25, Kendall- iscomputed on the ranking of all except BM-25.12We opt to use Kendall- due to its simplicity, yet it doesnot accurately capture all the intricacies of ranking systemperformance. More details on this and an involved metric, sider 3 buckets: [0, 0.01) represents systems withvery low p-values, meaning they are very far apartin performance, hence should be easier to ordercorrectly. [0.01, 0.05) represents systems with asignificant, yet not extreme difference. The finalbucket, [0.05, 1), contains pairs of systems that donot differentiate in a statistically significant way.Results are shown in . We observe that,as expected, the error-rate drops when a bucketrepresents a smaller p-value, indicating higher sig-nificance that the systems are ordered correctly.",
  "Do rankings stabilize as falsely labelednegatives decrease?": "Taking the evidence chosen using the different sys-tems as discussed in .2, we gradually adda fraction of annotated evidence for each query inthe evaluation set. We then evaluate the systems oneach partially annotated dataset by comparing theranking achieved to the fully annotated evaluationset. We divide pairs of systems into buckets basedon their p-values, as described in .3, andfor each percentile we average results across thedifferent system pairs falling within each bucket.Results are presented in . Depending on thesignificance of the difference between systems, re-sults show a different portion of evidence needs tobe annotated in order to achieve the correct order.For example, if we are aiming at a 0.8 Kendall-score, representing a 10% error-rate, for verysignificant pairs of systems acquiring 20% ofthe positives should suffice, while for systems witha non-significant difference between them, almostall positives are needed.",
  ": Partial-Kendall- between rankings of sys-tems with k percent annotations and ranking with allevidence, using recall@20. System pairs are dividedinto 3 buckets as described in .3": "Multi-answer retrieval.QAMParI (Amouyalet al., 2023) introduce a benchmark of ques-tions with multiple answers extracted from listsin Wikipedia, and Quest (Malaviya et al., 2023) isa dataset with queries containing implicit set oper-ations based on Wikipedia category names. Bothlimit evidence collection to the Wikipedia articleof the answer. In contrast, our goal is to identify allrelevant evidence for each answer, including otherWikipedia articles. RomQA (Zhong et al., 2022)curates a large multi-evidence and multi-answerbenchmark derived from the Wikidata knowledgegraph with the goal of challenging the retriever andQA model. Although RomQA provides a largenumber of evidence, they do not aim for completeannotation nor to understand the negative effectof evaluation with partial annotations. Our pathsdiverge in that they seek to evaluate QA modelsand we aim to understand the effects of partial an-notations on retriever evaluation, and to collect allevidence for each answer. Exhaustive annotation.TREC Deep Learning(Craswell et al., 2020, 2021, 2022, 2023, 2024)is a yearly effort to completely-annotate queriesfor passage retrieval from the MS-Marco bench-mark (Bajaj et al., 2018). Since annotating the en-tirety of MS-MARCO is unrealistic (~1M queriesand ~8.8M passages), they conduct a competitionwhere participants submit the results of their re-trievers. Then, the results are pooled and theirrelevancy is evaluated. However, manual evalua-tion is a non-scalable approach, and over a spanof five years (20192023) only 312 queries were annotated. In addition, exhaustiveness is unlikelyas previously observed in (Zobel, 1998) and fur-ther corroborated in Appendix E. NERetrieve (Katzet al., 2023) shares our aspiration for a completely-annotated dataset. It proposes a retrieval-basedNER task that creates a Wikipedia-based datasetwhere entity types function as queries and relevantpassages contain a span that mentions instancesof the entities (e.g., Dinosaurs is an entity typeand Velociraptor is an instance of it). With somesimilarity to our process, they collect candidatesby relaxed matching of mentions of entities in doc-uments that reference them (on DBPedias link-graph (Lehmann et al., 2015)), and then use a clas-sifier to filter out cases that do not match their query.However, our work annotates evidence and not sim-ply mentions of entities in a passage. Moreover,in addition to creating an exhaustively annotateddataset, we study the effects of partial annotation.",
  "Conclusions": "In this work we question whether the lack of rigor-ous annotation in modern retrieval datasets resultsin false conclusions. To answer this, we createD-MERIT from Wikipedia. D-MERIT aspires tocollect all relevant passages in the corpus for eachquery, a property made possible due to Wikipediasunique structure. We use D-MERIT to explore theimpact of evaluating systems on datasets riddledwith falsely labeled negatives; We demonstrate thatevaluation based on queries with a single annotatedrelevant passage is highly dependent on the pas-sages selected for annotation, unless one system issignificantly superior to all others. We also showthat the number of annotations required to stabi-lize the rankings is a factor of the difference inperformance between systems. We conclude thatthere is a clear efficiency-reliability curve whenit comes to the amount of annotations investedin a retrieval evaluation set, and that when pick-ing the correct spot on this curve considerationsshould include the estimated difference betweenthe systems in question and the method used tochoose the passages sent to annotation. We showthat the commonly used TREC-style evaluationmethod fails to find a significant portion of the rel-evant passages in D-MERIT, suggesting that usingthis annotation approach on D-MERIT would leadto a non-negligible error rate. If its possible, ourrecommendation for other datasets would be to es-timate the coverage of the TREC method before using it for evaluation. Otherwise, its results shouldbe taken with a grain-of-salt. Finally, our datasetopens a new avenue for research, both as a test-bedfor evaluation studies, as well as evaluation in ahigh-recall setting.",
  "Limitations": "Generalization of conclusions. We (and manybefore us) believe that in order to properly evalu-ate retrieval systems, the community should striveto collect all (or most) relevant passages. We be-lieve this is true for many different datasets andscenarios. Having said that, showing this explic-itly requires to completely annotate datasets, whichis hard and expensive. Furthermore, our datasetcollection method does not generalize to other cor-pora as it highly relies on the Wikipedia structure(specifically, on the \"list of\" pages). Therefore,while we do believe that most of our conclusionscan generalize to many other datasets, technicallywe could show them only on the dataset we used. Exhaustiveness. Our evidence identificationprocess is automated by GPT-4, the current state-of-the-art for text analysis. Despite achieving highagreement with human annotators, it is not perfect.Furthermore, even with a flawless model, comput-ing the relevance of all passages in Wikipedia foreach member in each query would have resultedin millions of inferences, which would have madethe creation of this dataset unfathomably expen-sive. We thus make the (sensible) assumption thata passage with evidence must contain a link to thearticle of the entity. It is possible some evidencewere never collected, as analyzed in .4. Data evaluation compatibility. Our dataset ismade of set-queries with multiple members (trans-lating to multiple answers in the QA setting). Insuch cases, systems are usually evaluated usingdatasets containing a single relevant per answer.In .2 we evaluate and draw conclusionsusing a single positive per query. We do so in orderto draw conclusions regarding cases where singlepositives per query are used, but in practice thesedatasets usually contain single-answer queries (e.g.MS-MARCO). While we do believe our conclu-sions generalize to this case, it would have beenmore accurate to use such a single-answer-per-query dataset. Unfortunately, collecting such afully annotated dataset is not trivial.",
  "Ethics Statement": "Automatic annotation.Since our annotation isautomatic, it is model-dependent. This means itis vulnerable to the models biases. As a result, itmay fail to attribute evidence to a query if a can-didate is under-represented in the models trainingdata. This might cause D-MERIT to miss out onevidence that belongs to some under-representedgroup. Rater details.To collect annotations on ourdataset, we used Amazon Mechanical Turk (AMT).All raters had the following qualifications: (1) over5,000 completed HITs; (2) 99% approval rate orhigher; (3) Native English speakers from England,New Zealand, Canada, Australia, or United States.Raters were paid $0.07 per HIT, and on average,$20 an hour. In addition, raters that performed thetask well were given bonuses that reached doublepay. Annotation collection and usage policy.Raterswere notified that their annotations are intendedfor research use in the field of Natural LanguageProcessing and Information Retrieval, and will ulti-mately be shared publicly. The task and collectedannotations were objective and excluded personalinformation. Moreover, all data sources for thestudy were publicly accessible. Computing resources.We used only modestcomputing resources. For both, the dataset cre-ation and the experimentation, we used a singleAmazon-EC2-g5.4xlarge instance for 200 hours,which costs $1.6 per hour. For the annotation ofthe passages, and creation of the natural-languagequeries, we utilized GPT-4-1106-preview, whichat the time of writing, is priced at $0.01 for 1Kinput tokens, and $0.03 for 1K output tokens. Intotal, we paid ~$3,000 for our use of the model.",
  "This project received funding from the Euro-pean Research Council (ERC) under the EuropeanUnions Horizon 2020 research and innovationprogramme, grant agreement No. 802774 (iEX-TRACT)": "Samuel Amouyal, Tomer Wolfson, Ohad Rubin, OriYoran, Jonathan Herzig, and Jonathan Berant. 2023.QAMPARI: A benchmark for open-domain questionswith many answers. In Proceedings of the ThirdWorkshop on Natural Language Generation, Evalua-tion, and Metrics (GEM), pages 97110, Singapore.Association for Computational Linguistics. Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng,Jianfeng Gao, Xiaodong Liu, Rangan Majumder, An-drew McNamara, Bhaskar Mitra, Tri Nguyen, MirRosenberg, Xia Song, Alina Stoica, Saurabh Tiwary,and Tong Wang. 2018. Ms marco: A human gener-ated machine reading comprehension dataset.",
  "C Buckley, Darrin Dimmick, Ian Soboroff, and EllenVoorhees. 2007. Bias and the limits of pooling forlarge collections": "Chris Buckley and Ellen M. Voorhees. 2004. Retrievalevaluation with incomplete information. In Proceed-ings of the 27th Annual International ACM SIGIRConference on Research and Development in Infor-mation Retrieval, SIGIR 04, page 2532, New York,NY, USA. Association for Computing Machinery. Deng Cai, Yan Wang, Lemao Liu, and Shuming Shi.2022. Recent advances in retrieval-augmented textgeneration. In Proceedings of the 45th InternationalACM SIGIR Conference on Research and Devel-opment in Information Retrieval, SIGIR 22, page34173419, New York, NY, USA. Association forComputing Machinery. James P. Callan. 1994. Passage-level evidence in docu-ment retrieval. In Proceedings of the 17th Annual In-ternational ACM SIGIR Conference on Research andDevelopment in Information Retrieval, SIGIR 94,page 302310, Berlin, Heidelberg. Springer-Verlag.",
  "Thibault Formal, Carlos Lassance, Benjamin Pi-wowarski, and Stphane Clinchant. 2021. Spladev2: Sparse lexical and expansion model for informa-tion retrieval": "Luyu Gao and Jamie Callan. 2022. Unsupervised cor-pus aware language model pre-training for dense pas-sage retrieval. In Proceedings of the 60th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 28432853,Dublin, Ireland. Association for Computational Lin-guistics. Prashansa Gupta and Sean MacAvaney. 2022. On sur-vivorship bias in ms marco. In Proceedings of the45th International ACM SIGIR Conference on Re-search and Development in Information Retrieval,SIGIR 22. ACM. Vladimir Karpukhin, Barlas Oguz, Sewon Min, PatrickLewis, Ledell Wu, Sergey Edunov, Danqi Chen, andWen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the2020 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 67696781,Online. Association for Computational Linguistics.",
  "Maurice G Kendall. 1945. The treatment of ties inranking problems. Biometrika, 33(3):239251": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-field, Michael Collins, Ankur Parikh, Chris Alberti,Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-ton Lee, Kristina Toutanova, Llion Jones, MatthewKelcey, Ming-Wei Chang, Andrew M. Dai, JakobUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-ral questions: A benchmark for question answeringresearch. Transactions of the Association for Compu-tational Linguistics, 7:452466. Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,Dimitris Kontokostas, Pablo N. Mendes, SebastianHellmann, Mohamed Morsey, Patrick van Kleef,S. Auer, and Christian Bizer. 2015. Dbpedia - alarge-scale, multilingual knowledge base extractedfrom wikipedia. Semantic Web, 6:167195. Patrick Lewis, Ethan Perez, Aleksandra Piktus, FabioPetroni, Vladimir Karpukhin, Naman Goyal, Hein-rich Kttler, Mike Lewis, Wen tau Yih, Tim Rock-tschel, Sebastian Riedel, and Douwe Kiela. 2021.Retrieval-augmented generation for knowledge-intensive nlp tasks.",
  "Jimmy Lin and Xueguang Ma. 2021. A few brief noteson deepimpact, coil, and a conceptual frameworkfor information retrieval techniques. arXiv preprintarXiv:2106.14807": "Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, and Rodrigo Nogueira.2021a. Pyserini: A python toolkit for reproducibleinformation retrieval research with sparse and denserepresentations. In Proceedings of the 44th Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval, SIGIR 21,page 23562362, New York, NY, USA. Associationfor Computing Machinery. Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin.2021b. In-batch negatives for knowledge distillationwith tightly-coupled teachers for dense retrieval. InProceedings of the 6th Workshop on RepresentationLearning for NLP (RepL4NLP-2021), pages 163173,Online. Association for Computational Linguistics.",
  "Taichi Murayama. 2021. Dataset of fake news detec-tion and fact verification: a survey. arXiv preprintarXiv:2111.03299": "Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,Amnon Shashua, Kevin Leyton-Brown, and YoavShoham. 2023. In-context retrieval-augmented lan-guage models. Transactions of the Association forComputational Linguistics, 11:13161331. S. E. Robertson and S. Walker. 1994. Some simpleeffective approximations to the 2-poisson model forprobabilistic weighted retrieval. In Proceedings ofthe 17th Annual International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval, SIGIR 94, page 232241, Berlin, Heidel-berg. Springer-Verlag.",
  "Denny Vrandecic and Markus Krtzsch. 2014. Wiki-data: A free collaborative knowledge base. Commu-nications of the ACM, 57:7885": "Shitao Xiao, Zheng Liu, Yingxia Shao, and Zhao Cao.2022. RetroMAE: Pre-training retrieval-oriented lan-guage models via masked auto-encoder. In Proceed-ings of the 2022 Conference on Empirical Methods inNatural Language Processing, pages 538548, AbuDhabi, United Arab Emirates. Association for Com-putational Linguistics. Emine Yilmaz and Javed A. Aslam. 2006. Estimat-ing average precision with incomplete and imperfectjudgments. In Proceedings of the 15th ACM Inter-national Conference on Information and KnowledgeManagement, CIKM 06, page 102111, New York,NY, USA. Association for Computing Machinery. Chengxiang Zhai and John Lafferty. 2001. A study ofsmoothing methods for language models applied toad hoc information retrieval. In Proceedings of the24th Annual International ACM SIGIR Conferenceon Research and Development in Information Re-trieval, SIGIR 01, page 334342, New York, NY,USA. Association for Computing Machinery.",
  "ABenchmarking D-MERIT": "While tangential to this paper, the D-MERITdataset allows us to benchmark the ability of exist-ing retrieval models to perform on the full-recallretrieval setup, as its coverage is very high as re-ported in .4. This section describes thisbenchmark process. Benchmark metrics.We select Recall, Normal-ized Discounted Cumulative Gain (NDCG) andMean Average Precision (MAP). In addition, giventhat we possess complete evidence for every query,we can calculate R-precision a form of recallwhere k varies for each query, determined by thespecific total evidence count to that query. Forinstance, if a query corresponds to 40 pieces ofevidence, then k is set at 40. Achieving a perfectscore means that the top 40 results are all evidenceassociated with the query. Results.Performance of all systems is shownin , with SPLADE++ and SPLADEv2 per-forming best across all metrics. The scores sug-gest there is substantial room for improvement onour evidence retrieval task. For example, the re-call@100 score indicates no system successfullyretrieves even half of the evidence on average.",
  "BFurther Details: Experimental Study": "To allow reproduction of our results, we detailthe hyper-parameters used in our work.Weutilize the Pyserini information retrieval toolkit(Lin et al., 2021a) with the following settingsfor each system: BM25 is employed using thestandard Lucene index for indexing and retriev-ing results.Similarly, QLD is used but withthe QLD reweighing option to refine the pro-cess. UniCoil embeddings are generated with thecastorini/unicoil-noexp-msmarco-passage encoder,and retrieval is conducted using Lucene search withthe impact option to incorporate unicoil weights.SPLADEv2 and SPLADE++ follow a similar ap-proach, where passages and queries are embeddedusing their respective official code repositories, andretrieval is performed using Lucene with the im-pact option. DPR involves embedding passagesand queries with the facebook/dpr-ctx_encoder-multiset-base and facebook/dpr-question_encoder-multiset-base encoders, respectively, with retrievalvia FAISS (Douze et al., 2024).RetroMAE-distill adopts a similar strategy, utilizing theShitao/RetroMAE_MSMARCO_distill encoder for both queries and passages.TCT-Colbert-V2 also mirrors this approach but uses thecastorini/tct_colbert-v2-msmarco encoder.co-Condenserinvolvestrainingdocumentandquery encoders on the Natural Questions dataset(Kwiatkowski et al., 2019) using the CoCondenserofficial code repository. Hybrid models such asTCT-Colbert-V2-Hybrid, coCondenser-Hybrid,and RetroMAE-Hybrid combine the strengthsof BM25 with TCT-Colbert-V2, coCondenser,and RetroMAE-distill respectively, using a fusionscore with = 0.1.",
  "License.D-MERITbuildsondatafromWikipedia, which carries a Creative CommonsAttribution-ShareAlike 4.0 International License.This license requires that any derivative works alsocarry the same license": "Conditioning human raters.Before the evalu-ation process begins, we need to assure the raterswe use understand the task and can perform it ad-equately. We thus begin a conditioning process.First, we run a qualification exam, and the ratersthat get all the questions right, are invited to aniterative training process. The process includessmall batches, of up to 100 (passage, prompt) pairs,where the rater submits their response and we pro-vide personal feedback. Moreover, all tasks in-cluded an option to mark the example as difficultor provide textual feedback about it, to encouragecommunication from the raters as they work. Aftereach batch raters are filtered out, until we remainwith a single rater with a success rate of over 95%on a single batch. The task is visualized in . Automatic identification details.To automati-cally identify evidence, GPT-4 is provided with apassage and a structured query. In this context, astructured query begins with the article name, fol-lowed by its section names arranged hierarchically(separated by ), corresponding to the structureof the article, and ultimately culminating in thecolumn value. For instance, a typical structuredquery could be Cities and Towns in Cambodia(article name) Cities (section name) Name(column name). The task for GPT-4 is to determinewhether the passage provides evidence supportingthe query. The evaluation involves analyzing thetext to ascertain whether the passage directly orindirectly confirms the entity in question is part of",
  "SystemRecall@kNDCG@kMAP@kR-precision520501005205010052050100": "SPLADE++9.4324.1136.0245.1638.1736.5438.0540.567.1115.019.3521.7228.16SPLADEv27.8221.2133.2943.3432.0931.4333.7837.005.7412.2016.0318.2724.82TCT-Colbert-Hybrid7.8519.6229.7137.9734.8631.6032.2334.335.8011.4814.7816.5622.75bm256.6517.4627.5435.7628.9327.2028.6231.134.769.7612.8314.6120.86RetroMAE-Hybrid7.3017.4825.9532.8533.9529.2129.1930.825.7110.6313.1414.4820.12RetroMAE7.0316.6224.7831.6132.7127.9827.9429.615.4710.0512.3813.6619.29TCT-Colbert6.2715.4423.5930.9529.3125.7326.0827.954.588.6411.0212.3918.02CoCondenser-Hybrid5.2814.8124.2532.8822.1321.8723.9626.893.416.829.1010.6316.78QLD5.4913.9623.5631.9624.5421.7123.6326.553.777.079.5111.1316.56CoCondenser4.8713.7523.0231.5220.7120.4222.6425.543.146.208.359.7715.69Unicoil4.4710.9517.2723.2820.8617.9618.7020.493.256.057.728.8313.19DPR3.909.6215.9921.7218.5115.9016.6418.412.634.485.676.3710.89 : Performance of a variety of baselines on D-MERIT. Recall, NDCG, and MAP are evaluated over four kvalues: 5, 20, 50, and 100. The k value in R-precision is the total number of evidence of a query, which changesfrom query to query. the group defined by the query. For example, in aquery aimed at identifying names of Cambodiancities, the passage must either explicitly state orstrongly suggest that a particular city belongs inCambodia to be considered relevant. Our promptsfollow our definition of relevance from .2:Ifyouwerewritingareportonmemberbeingpartofarticle-name,andwouldliketogather*all*thedocuments that directly confirm memberis part of article-name, in the categoryhierarchy article-name section-name column-name, will you add the followingdocument to the collection? Answer withyes or no. Natural-language query generation prompt.To translate a structured query to its natural-language variant, we prompt GPT-4 using thetemplate below. Examples of input and output canbe viewed in .Please pretend you are a typical GoogleSearch user, show me what you would writein the search bar. For example: culturalpropertyofnationalsignificanceinSwitzerland:Zurich Richterswil Name,where indicates a hierarchy, a typicalsearchwouldbe:namesofculturalproperties of national significance inRichterswil, Zurich, Switzerland.",
  ": Examples of structured queries and their corre-sponding natural-language form": "and dis-concordant elements between two ranksover a set of elements. More general variants ofKendall- (Kendall, 1945; Stuart, 1953) addresscases where ties exist (i.e., in one ranking two ele-ments received an identical score).The simplicity of Kendall- makes it temptingto utilize it to compare the ranking of retrieval sys-tems. However, it fails to capture some of theintricacies of this comparison due to several rea-sons. First, simply comparing system scores isinsufficient, as an additional verification using asignificance test is necessary. Ties can be defined(i.e., system A is tied with system B if p > 0.05),but the relation is not transitive (A tied with B andB tied with C does not imply that A is tied with C),as required by variants of Kendall- that supportties. Second, some ranking errors are more trou-blesome than others. Finding that a new system istied with the baseline system when in fact it isworse might be undesirable. However, incorrectlyreporting that it is better is improper.Even though Kendall- suffers from the short-comings above, we hypothesize that it is still a goodmetric for comparing performance rankings. To val-idate this we propose a new metric, concordance, : Concordance between rankings of systems with varying percentages of evidence and ranking with allevidence, using recall@5, recall@20, recall@50, and recall@100. System pairs are divided into 3 buckets asdescribed in .3. that addresses these shortcomings of Kendall- andits variants. This is done by considering the rela-tions A > B and A < B for a pair of systems Aand B. This way if in the ground truth A is signifi-cantly better than B and in the compared rankingA is tied with B, the two rankings will agree on therelation A < B (will be false in both) and disagreeon the relation A > B. In a more troublesomeerror, where A < B in the compared ranking, thetwo rankings will disagree on both relations. For-mally, let 1 and 2 be two rankings of a set ofretrieval systems S. For each pair of systems s1, s2and ranking we define",
  "s2=s11(s1, s2) 2(s1, s2),": "where P(n, r) is the number of permutations ofsize r from a set of size n, and is the XNORoperator (equals to 1 if both inputs equal).Using concordance, we validate the results foundin .3 and .4 using Kendall-.This is done by repeating the experiment and cal-culating the mean concordance of system rankingsgiven evidence found by different systems withthe ground truth ranking (in which all evidenceare annotated). We run this experiment for a sin-gle annotated evidence and different percentiles ofannotated evidence.In and we see that pairs of sys-tems with a very significant difference betweenthem (i.e., p < 0.01) are evaluated with higher ac-curacy than systems falling in the other two buckets.This validates the results found in .3 and.4 and shows that Kendall- is a goodproxy for evaluating the rankings of IR systems.",
  ": Concordance computed only on pairs of sys-tems that fall within the [pmin, pmax) bucket. k is therecall@k used": "datasets. In this section we compare our approachfor collecting multiple evidence for queries withtheir approach. This is done by applying TRECsapproach to our dataset and testing its coverage.This will reveal, even though anecdotally, the abil-ity of TRECs approach to find numerous evidence.The approach in TREC does not utilize a struc-tured data source for the creation of the judgementset. Instead, they create a pool of candidates fromthe set of passages retrieved by a large set of sys-tems. Specifically, TREC runs a competition andpublishes a query set and a corpus. Any partic-ipant team executes their system and submits aretrieved list. Then, TREC pools top-k passagesfrom each participant and sends them for humanannotation, annotating for relevancy. Before ap-plying the approach used by TREC to our datasetwe first formally define this process. Let Q be theset of queries and Eq the evidence set of queryq Q. In addition, let S be the set of systems andEq,s be the evidence set found in the top-10 pas-sages retrieved by system s S for query q Q.Then, the judgement set of query q is defined asJq(S) = sSEq,s. We denote the coverage of Son Q as:",
  "|Eq|": "When fixing the number of passages retrievedby each system to k = 10, as done in TREC, andgiven the 12 systems considered in this paper (see.1), we can compute their coverage onD-MERIT which is equal to 31.7%. While thismay be low, we only consider a small number ofsystems, as it is typical to use around 100 systems.Also, increasing k is expected to increase the cov-",
  "E.1Extrapolating Number of Systems": "Due to time and compute constraints using 100 sys-tems, as typically done in the TREC competition,is unrealistic. This leads us to approximate the cov-erage instead. In order to approximate the coverageof a larger number of systems we first fix k = 10,and compute the expected coverage of a randomsubset of systems of size t uniformly sampled fromS. That is,",
  "CQ(S, t) =ESU(S), |S|=t[CQ(S)]": "Given the values of CQ(S, t) for t = 1, . . . , 12, wefit a logarithmic curve (as coverage is both con-cave and monotonically-increasing) to these ob-servations and observe a root mean-squared-error(RMSE) of 0.16% and a maximum error of 0.31%.Finally we extrapolate to predict the coverage fort = 13, . . . , 100. The results of the experiment ispresented in . As can be seen, we predict thatbroadening the judgement sets by retrieving with asmany as 100 systems only increases the coveragefrom 31.7% to 47.1%. This result further corrob-orates the finding by (Zobel, 1998), which statesthat the pooling approach used in TREC finds, atbest, 50-70% of the evidence. We conclude thatour approach is able to achieve a much higher cov-erage. This is expected to improve the correctnessof our evaluation. Note that our approach dependson structured data in Wikipedia. On the other hand,the approach utilized in TREC is universal as it canbe applied to any corpus and query.",
  "Increasing the pool size can uncover additionalpositive results, but will result in a significantly": "larger annotation pool size. We adopt a similarmethod to extrapolating the coverage by increasingthe number of systems, and but focus instead onthe size of the pool.We use the coverage evaluation dataset describedin section 2.4 which takes a the top-20 pool from12 systems and uses human annotators to label therelevancy of each entry in the pool. Next, we assigneach relevant entry in the pool its minimum rankfrom all systems and construct pools for each depthsize. For example, for k=10, we take all documentsthat were ranked at the top-10 by at least a singlesystem.Finally, we extrapolate to predict for the num-ber of newly identified evidence ( ) and theoverall documents found by the pooling approach( ) for t = 21, . . . , 100. The results showthat even for a pool-depth of k = 100, we esti-mate that only 60 new evidences will be identified.This means that the coverage of our method is esti-mated to be 94.5% out of all identified evidence.In addition, we see that the pooling approach fork = 100 is estimated to retrieve 638 evidence (578already found by our method) covering only 60.8%with a significant increase of annotation overhead."
}