{
  "Abstract": "The potential effectiveness of counterspeech asa hate speech mitigation strategy is attractingincreasing interest in the NLG research commu-nity, particularly towards the task of automati-cally producing it. However, automatically gen-erated responses often lack the argumentativerichness which characterises expert-producedcounterspeech. In this work, we focus on twoaspects of counterspeech generation to producemore cogent responses. First, by investigatingthe tension between helpfulness and harmless-ness of LLMs, we test whether the presence ofsafety guardrails hinders the quality of the gen-erations. Secondly, we assess whether attack-ing a specific component of the hate speech re-sults in a more effective argumentative strategyto fight online hate. By conducting an exten-sive human and automatic evaluation, we showhow the presence of safety guardrails can bedetrimental also to a task that inherently aimsat fostering positive social interactions. More-over, our results show that attacking a specificcomponent of the hate speech, and in particularits implicit negative stereotype and its hatefulparts, leads to higher-quality generations.",
  "Introduction": "With the ever-increasing spread of social mediaplatforms, online hate speech (HS) has become acrucial concern. A promising strategy to fight on-line hate is counterspeech (CS), which is defined asnon-aggressive textual feedback that uses credibleevidence, factual arguments, and alternative view-points (Schieb and Preuss, 2016; Benesch, 2014).The potential effectiveness of counterspeech as ahate speech mitigation strategy has motivated anincreasing interest in Natural Language Genera-tion (NLG) research towards the automation of itsproduction (Bonaldi et al., 2024).",
  "Prompt": ": The annotation and generation process: first,the premises and conclusion of a hateful message areidentified, and their weakness/hatefulness is annotated.Then, we generate counterspeech attacking these ele-ments, with and without guardrails. However, despite the technological advance-ments in NLG, counterspeech generation is stillsubject to some limits. In particular, while humanexperts are able to produce counterspeech rich inarguments, language models often tend to generategeneric replies, e.g. simply denouncing the hatefulmessage (Mun et al., 2023; Tekiroglu et al., 2022,2020). Being hate speech countering a communi-cation exchange, it is subject to rhetorical rules:our goal is to investigate how it is possible to pro-duce cogent and convincing counterspeech, whichis, therefore, more similar to what experts produce.To do so, we will focus on two different aspects of counterspeech generation.Firstly, following a recent line in NLP research,we will focus on the existing tension between help-fulness and harmlessness of LLMs (Rttger et al.,2023; Bai et al., 2022a). In particular, we hypoth-esise that an exaggerated safety (Rttger et al.,2023) can have a negative impact on models per-formance even when doing a task that by definitionshould follow safety principles, i.e., hate counter-ing, by making its generations vaguer and less ar-gumentatively effective. Therefore, we formulateResearch Question 1 (RQ1) as follows: do safetyguardrails affect the quality of generated counter-speech, and in particular its perceived cogency?Secondly, we investigate different argumentativestrategies to produce counterspeech and comparetheir effectiveness. So far, the automatic generationof counterspeech has mainly focused on generallyattacking the hateful message (Halim et al., 2023;Tekiroglu et al., 2022; Qian et al., 2019). How-ever, we hypothesise that focusing on a specificpart of the hate speech results in a more effectivecounterspeech, potentially hindering the strength ofthe interlocutors convictions. This leads us to Re-search Question 2 (RQ2): is focusing on a specificcomponent of the hate speech better than gener-ally attacking the entire message? In particular,following existing work in counterargument andcounterspeech generation, we will test four rhetori-cal attacking strategies: attacking the hate speechas a whole, attacking its implied statement (Munet al., 2023), its hateful premises/conclusion, andfocusing on the weakest premise/conclusion of itsargumentation (Alshomary et al., 2021).To answer our research questions, we rely onthe White Supremacy Forum dataset (WSF, de Gib-ert et al., 2018). We first identify and annotatethe hate speech examples with an argumentativestructure1(3). Then, we use Mistral (Jiang et al.,2023) to generate counterspeech in reply to thesemessages (4), with and without safety guardrails(RQ1), and attacking different parts of the message(RQ2). Finally, we conduct an extensive humanand automatic evaluation to assess the quality ofthe generated counterspeech (5). An example ofthe annotation and generation process is providedin , while the entire workflow includingthe evaluation is depicted in . The results(6) show how safety guardrails have a detrimen-",
  "The annotations are available at:": "tal effect on the amount and logical correctness ofthe supporting arguments provided by the counter-speech, while, at the same time, their absence doesnot have an impact on the perceived safety of thegenerations. Moreover, focusing on the impliedstatement or on the hateful components of the hatespeech results in better counterspeech generationthan generally attacking the message as a whole.",
  "LLM safety and performance": "Limiting the potential misuse of Large LanguageModels has become a goal of primary importancein NLG research. In particular, an established re-search line is to develop helpful, honest and harm-less language models (Askell et al., 2021). Possibleways to achieve harmlessness include red teaming(Ganguli et al., 2022) and aligning the model withspecific safety principles at training time (Bai et al.,2022b,a). However, a tension exists between help-fulness and harmlessness (Rttger et al., 2023; Baiet al., 2022a): in particular, exaggerated safety canlead to poor model performance. In this regard,previous work has mainly focused on analysingcases where the models fail to answer totally saferequests because of exaggerated safety. In thiswork, we hypothesise that safety guardrails canalso interfere with tasks that, by definition, needto comply with high safety standards, i.e., counter-speech generation, by making the generations lessargumentatively effective.",
  "Counterargument generation": "Counterargument generation has been tackled withrule-based systems (Sato et al., 2015; Wachsmuthet al., 2018) and as a neural generation task (Huaand Wang, 2018; Hua et al., 2019). Regarding thelatter approach, Alshomary et al. (2021) studied ar-gument undermining, i.e., attacking an argument byarguing against the validity of its premises. In par-ticular, they first identify the weakest premises ofan argument using a BERT model, and then they at-tack them with a counter-argument generated withGPT. Similarly, in one of the attacking strategiespresented in this paper, we will first identify theweakest premise/conclusion of a hate speech exam-ple and then generate counterspeech attacking it.Alshomary and Wachsmuth (2023) instead, focused : Our workflow comprises three steps: first, hateful messages from the WSF dataset are annotated combininghuman and machine effort. Second, counterspeech is generated with and without safety guardrails (CSw/ andCSw/o, respectively), and using different attacking strategies (CSbase, CSweak, CShate, CSIS). Finally, both humanand automatic evaluations are performed. on rebutting an arguments conclusion by jointlylearning how to generate the conclusion and thecounter-argument. Finally, Lin et al. (2023) feedLlama with Chain-of-Thought instructions to guideit in identifying common reasoning errors in de-bate and generating a candidate counter-argumentcorresponding to each possible error.",
  "Counterspeech and argumentation": "Furman et al. (2023b) are the first to focus on iden-tifying argumentative aspects (i.e., the conclusionand justification) in hateful tweets, creating theASOHMO corpus. Following this work, Furmanet al. (2023a) associated each HS in the ASOHMOcorpus with manually written counterspeech usingdifferent strategies. They show that when argumen-tative information is provided, better counterspeechis obtained.Even if the ASOHMO corpus represents a validresource, its characteristics do not fit our require-ments. First, in line with other counter argumen-tation studies (Alshomary et al., 2021; Alshomaryand Wachsmuth, 2023), we are interested in de-composing the hate speech into premises and con-clusion, in contrast with the justification macro-element. Moreover, we want the premises and con-clusions to be stand-alone sentences, while this isnot always the case in ASOHMO, where justifica-tions/conclusions can consist of only hashtags (e.g.#buildthedamnwall as conclusion). For these rea-sons, we choose to create new data for our study.Finally, even if it can not be strictly consideredas an argumentative strategy, Mun et al. (2023)employ six psychologically inspired strategies tocounter the implicit stereotype of hate speech. Theyshow the importance of accounting for the stereo-types implied by hate speech when generating coun-terspeech. In this line, we also design one of our",
  "Dataset": "We focus on the White Supremacy Forum dataset(WSF, de Gibert et al., 2018), which contains in-stances of real hate speech in English scraped fromStormfront, the most influential white supremacyforum on the web. The dataset comprises a totalof 1119 hate speech examples, with an averagelength of 24 tokens. WSF primarily targets eth-nicity (42%), gender (36%), social class (7%), andnationality (7%). WSF is the only dataset includingexamples meeting all the following criteria at once:the data come from a social media platform, arehateful, and have a sufficient length to allow forthe identification of an argumentative structure. Infact, as shown in other existing datasets, the hatefulcontent coming from widely used platforms suchas Twitter has a too simple argumentative struc-ture. For example, in the ASOHMO corpus see(see Furman et al., 2023b), conclusions consist ofonly hashtags, rather than stand-alone sentences.2",
  "Annotation procedure": "We are interested in identifying the argumentativemessages present in the WSF dataset, i.e., thosecontaining at least one premise and one conclusion.Therefore, we employ a human-machine collabo-ration approach for the identification of premises,conclusion, and implied statement (Fanton et al.,2021; Bonaldi et al., 2022, see Appendix A.1 formore technical details). In particular, we followa three-phase strategy: (i) we automatically ex-tract these elements, (ii) a manual validation iscarried out by two annotators, (iii) disagreementsare solved via discussion or by a third annotator. Premises and conclusionAs a first step,premises and conclusions were automatically ex-tracted using gpt-3.5-turbo-instruct. Then,by comparing the original HS message with the ex-tracted arguments, two human annotators manuallyvalidated their correctness, following this proce-dure: if the HS had no premise or conclusion, themessage was discarded as non-argumentative. Ifit contained at least one premise and a conclusion,but they were imperfectly extracted by the model,they were manually modified with the least pos-sible effort. If they needed to be rewritten fromscratch, they were discarded. Then, they also anno-tated whether each premise and conclusion, takenin isolation, was hateful and whether it representedthe weakest point of the hate speech argument. Dis-agreements that could not be solved via discus-sion were solved by a third annotator. We considerweak the easiest element to attack, i.e., the onefor which the annotator can come up with manypossible counterarguments. In this way, only oneelement per example can be identified as weak (ei-ther one premise or conclusion). An annotatedexample, with premise and conclusion is shownbelow:",
  "Here, no premises are supporting the HS claim": "Implied statementThe implied statement (IS) isthe implicit negative stereotype present in a hate-ful message. We automatically extract the IS fromthe WSF data by using a fine-tuned BART model(Akazawa et al., 2023). All the extracted impliedstatements have a predefined structure: subject -predicate - object, e.g. Muslims are terror-ists. After the extraction, two annotators validatedthe IS correctness as follows: if the HS has anexplicit target, but the negative stereotype is notcorrect, the IS is modified. Otherwise, if the HShas no explicit target but it can be easily derived,the HS is modified to make the target explicit, andthe IS is annotated accordingly. If no target of hatecan be easily identified, the HS is discarded. Forexample, the IS concerning the HS shown in is: Immigrants are inferior to whites.",
  "Annotation statistics": "From the 350 longest examples, 200 were identi-fied as argumentative (i.e., containing at least onepremise and one conclusion). We also add 27 par-tially modified examples, in order to obtain a morebalanced distribution (e.g. more examples wherethe weak part is not identical to the hateful part).In total, we collect 227 annotated HS, with an aver-age length of 37.9 tokens. Overall, 408 premiseswere identified, i.e. 1.8 premises on average perHS. The annotated dataset is sufficiently variedalso in terms of covered targets of hate, i.e. severaldifferent ethnicities (59.9%), nationality (17.6%),religion (17.1%), sexual orientation (4.8%), gender(2.2%), and others (1.3%)3. As shown in ,in 59.2% of the examples the weakest point wasidentified in the conclusion. As regards hateful-ness instead, the most common case is that both thepremise(s) and the conclusion are hateful. In a mi-nority of cases (15 examples) neither the premisenor the conclusion, when taken in isolation, are con-sidered hateful by the annotators. In these cases,the hateful part is most likely the inferential stepconnecting different argumentative components, orthe IS, as in the example shown below, annotatedwith premise 1, premise 2 and conclusion:",
  "Counterspeech generation": "Our goal is two-folded. Firstly, we aim to comparethe counterspeech generation of a model wheresafety guardrails are present (CSw/) to a modelwithout guardrails (CSw/o). Secondly, we want tocompare the effect on CS generation of four differ-ent rhetorical strategies, attacking different parts ofthe hate speech: the hateful premises/conclusion(CShate), the weakest point of its argumentation(CSweak), its implied statement (CSIS), or the en-tire message as a baseline (CSbase). Controlling safetyIn order to have the highestpossible control on the safety aspect in our exper-imental design, we choose Mistral Instruct (Jianget al., 2023), the only instruct model which, tothe best of our knowledge, is not provided withany safety tuning at training time. To obtain thegenerations with guardrails (CSw/), we use thesystem prompt provided by Jiang et al. (2023)4,whereas for generations without guardrails (CSw/o)this prefix is simply absent. The implementationof guardrails at the prompt level allows us to com-pare the safe and unsafe configurations whilekeeping all other confounding variables constant.By reading preliminary generations obtained withMistral Instruct v0.1 and v0.2 7B, with and withoutguardrails, we hypothesise that the v0.2 model wassubject to some safety tuning/alignment, despitethe absence of any reference to this in its modelcard5. To prove our hypothesis, we use SimpleSafe- 4i.e., Alwaysassistwithcare,respect,andtruth. Respond with utmost utility yet securely.Avoid harmful, unethical, prejudiced, or negativecontent.Ensurerepliespromotefairnessandpositivity.5From HuggingFace: The Mistral 7B Instruct model [...]does not have any moderation mechanisms. tyTests (Vidgen et al., 2023), a set of 100 promptsto test for several dimensions of safety risks inLLMs, and we prompt both the Mistral Instructv0.1 and v0.2 models, in their without guardrailsconfiguration. Additionally, we also manually cre-ate 13 prompts that are more related to the taskof our interest, i.e., Tell me a joke about{minority} and Provide an offensive replyto the following statement: {statement},where we substitute {statement} with sentenceslike I am gay or I will have an abortion6.Then, one annotator manually evaluated whetherthe response given by each model was safe, poten-tially unsafe or unsafe. Over 113 unsafe prompts,the v0.1 model generates unsafe replies in 64 cases(56,6%) and v0.2 in 20 cases (17,7%). For thisreason, we decided to use Mistral Instruct v0.1 7B,since it gives us more control over the presenceor absence of guardrails than v0.2 version. Thegenerations are performed in a zero-shot setting(max_new_tokens=156, do_sample=True), sincepreliminary comparisons with few-shots genera-tions showed almost identical results. Attacking strategiesWe use a different promptaccording to the attacking strategy of interest. Inparticular, for attacking the IS, the weak and thehateful part (CSIS, CSweak, CShate, respectively),we use the following prompt:Generate counterspeech for the followingargument: {message} in no more than twosentences, focusing only on the followingpart {part to attack}.For the baseline strategy that is not attacking anypart of the HS (CSbase), we remove focusingonly on the following part from the prompt.The prompts are kept as simple as possible to avoidany additional noise. We restrict the length of thegenerated CS to no more than two sentences to ob-tain a similar length to that of messages that canbe usually found on social media platforms. More-over, as underlined in previous studies concerningmisinformation countering, verbose explanationsare generally not appreciated by readers (Russoet al., 2023).The {message} we provide in the prompt isnot the original one, but the concatenation of thepremises and conclusion that we extracted, con-nected by the word therefore. For example, inthe case of the HS represented in , the pro-",
  "A complete list of the 13 additional prompts we createdcan be found in Appendix A.2": "vided {message} is: The US and Ireland are the1st world. They are educated unlike the beaners.Therefore, Americans and Irish understand eachother better than Spaniards and the beaners.. Wedecided to provide the LLM with the concatenationof extracted premises and conclusion instead of theoriginal hate speech for having a more controlledexperimental setting. In fact, a preliminary experi-ment comparing the use of the original hate speechwith the concatenation of premises and conclusionshowed no perceivable differences in the outputaccording to the annotators. At the same time,using as input the concatenation of premises andconclusion allowed us to have more comparableprompts in the different tested attacking strategies.In particular, we could perform a controlled exper-iment using exactly the same prompt wording forall configurations and isolating the effect of attack-ing various parts of the input. Instead, using asinput the original hate speech, and attacking oneof its premises/conclusion (which might be slightlyrephrased with respect to the original hate speech)might have introduced additional noise. Addition-ally, during the annotation process, the annotatorsnoticed that some hate speech examples in the WSFdataset are difficult to comprehend, as they can begrammatically incorrect, use a specific vocabulary,and refer to conspiracy theories without context. Inthese cases, the extracted premises and conclusionhelped the annotators better understand the originalmeaning of the messages.",
  "Human evaluation": "We recruited 17 graduate-level volunteer annota-tors for one hour, where they were given a briefpresentation introducing the task, annotation guide-lines, and annotated examples (see Appendix A.3for more details). We opted for a within-subject de-sign: each annotator was presented with 17 HS-CSpairs randomly selected from the tested configu-rations, and each example was annotated indepen-dently. In this way, we collected a total of 285annotated HS-CS pairs, of which 235 are uniqueand 50 are overlapping (see Appendix A.4 for moredetails). We rely on the following four dimensionsdefined on a 1-5 Likert scale:Cogency (COG) measures the amount and logical correctness of the supporting arguments providedby the CS: we use an adapted version of this metricfrom Stapleton and Wu (2015).Suitableness (SUI) measures how much a CS has asuitable style, meaning that it is polite, not hateful,it attacks the message and not the hater (Chunget al., 2021; Tekiroglu et al., 2022; Bonaldi et al.,2023). Moreover, the CS should defend the minor-ity attacked by the HS.Relevance (REL) measures how relevant the CS isto the content of the corresponding HS in terms oftopic and target of hate (Bengoetxea et al., 2024).Informativeness (INF) measures how informativethe CS is as a response to its HS in terms of thequantity and factual correctness of included facts(Chung et al., 2021).Considering that annotators scored each CSacross these four dimensions, the total number ofevaluations we collected is 1140.We also compute an Overall Score (Ov. Sc.),obtained by averaging the normalised scores of thefour dimensions. While Cogency and Informative-ness might look very similar, as they both dependon the number of arguments/information and theirlogical/factual correctness, they measure two dif-ferent dimensions. For instance, a CS can be highlyinformative but poorly cogent:",
  "HS: We should go back to when slavery was legal: theywere better times when that n****s scum was our property": "CS: The Civil War represented the end for slavery in mostplaces in the US, due to measures such as the ConfiscationActs and the Emancipation Proclamation. The abolishmentof slavery was ratified on December 6, 1865, with theThirteenth Amendment to the United States Constitution. In this example, the CS provides factually cor-rect information not mentioned in the HS regardingthe abolishment of slavery, but without providingsupporting reasons to counter the HS. For thesereasons, this CS would have a score of 5 for infor-mativeness and 1 for cogency.",
  ": The results grouped by safety configuration": "et al., 2014; Bertoldi et al., 2013)7.OpenAIs content moderation API8 (SAF)measures the potential harm caused by a text,according to 11 dimensions (e.g., hate, sexual,violence). For each text, we select the highestobtained score, to reflect the unsafety of the text.We report the result of 1 score, so that thehigher, the safer.ArgJudge (ArgJ) is a BERT model trained onhuman scores on counter-arguments quality, fromLin et al. (2023). It reflects how much a counterar-gument forms a strong rebuttal relationship to agiven argument.",
  "Results and discussion": "In this section, we show the results9, grouped bysafety configuration (CSw/ and CSw/o), and attack-ing strategy (CShate, CSweak, CSIS, CSbase). Inthe following tables, the * symbol represents a sta-tistically significant difference10, the best scoresare in bold and the second best are underlined. Safety configurations shows the resultsgrouped by safety configuration. CSw/o reacheshigher scores than CSw/ for almost all dimensions,except for suitableness, where the two systemshave almost the same score.In fact, by quali-tatively analysing the examples, it was possibleto notice how the absence of guardrails does notimpact the perceived safety of the counterspeech:CSw/o rarely contains toxic content but is rathercharacterised by a more direct style. The pres-ence of guardrails does not have an evident impact",
  "For all the subsets of data of our interest, we show theaverage obtained on 5 different shuffles of the dataset.8": "9We report the macro averages since we consider multipleannotations for the same example as equally contributing tothe final score of a generated CS. We also computed the microaverages, and the results were coherent.10The statistical significance was calculated using the Mann-Whitney U test. For Tables 4 and 16, we show only the sig-nificant differences on the same dimension, between eitherdifferent strategies and the same safety configuration or be-tween different safety configurations and the same strategy. on informativeness either: even in this case, thetwo systems have very similar scores, with a dif-ference of only 0.05. At the same time, CSw/ois significantly more cogent than CSw/ (p-value =0.017), supporting our assumption that the presenceof guardrails has a detrimental impact on the argu-mentative strength of the model. The similar safetyof the two models and the higher cogency of CSw/ois also reflected in the SAF and ArgJ scores, respec-tively. CSw/ is also more repetitive than CSw/o:we hypothesize that repetitiveness and cogency areclosely related. In particular, CSw/ is characterisedby a high rate of overly safe calls to action, thatsound empathetic but lack a proper argumentation.This is confirmed by the higher frequency of pe-riphrases such as lets embrace, we should striveto/focus on, it is crucial/essential/important to(58.6% of these periphrases occur in CSw/). Anexample of this behavior is shown below:",
  "HS: Let mother nature evolve these people naturally be-cause no amount of money or aid is going to drag thesethings into the civilized world where they serve absolutelyno purpose": "CSw/: The statement is harmful and unethical as it de-humanizes Black people and suggests that they have noinherent value. It is also a simplistic and incomplete viewof the world that fails to recognize the complex social,economic, and historical factors that influence a personsplace in the world. The statement promotes a dangerousmisconception that leads to prejudice, discrimination, andviolence. It is important to challenge such thinking andstrive towards creating a more just and equitable society. CSw/o: The argument that black people are uncivilizedis not supported by scientific evidence and is a commonmisconception. Human civilization is not determined byphysical abilities, but by the development of culture andsociety through the use of reason and cooperation. In this example, CSw/ is mainly denouncing theHS, recurring to the periphrases described above(highlighted in bold), whereas CSw/o directly coun-ters the argument presented in the HS. Therefore,answering RQ1, we find that the absence of safetyguardrails has a positive effect on the cogency ofthe CS, without hindering its perceived safety.",
  ": The results grouped by attacking strategy": "Attacking strategiesTurning to the attackingstrategies (), CSIS reaches the highest scorefor all the human metrics, except for relevance,where it reaches the second best score. As re-gards cogency, since the IS makes explicit the tar-get of hate, attacking it ensures that the CS doesnot produce counterarguments against minor pointsbrought up by the HS, but that it more directly fo-cuses on defending the targeted minority, whichis one of the aspects considered for cogency (seeAppendix A.3 for more details). At the same time,CSIS is also the most repetitive strategy. This ap-parent contradiction can be explained by the factthat human annotators were served with few exam-ples at once, generated with different strategies: therepetitiveness of CSIS instead, becomes apparentonly when considering many examples from thesame strategy. Also CShate obtains good resultsoverall: it has the highest relevance and the secondbest informativeness. Moreover, it is also the sec-ond least repetitive strategy, and the one with thehighest ArgJ score. Finally, CSbase is the strategywith the lowest relevance, which is also signifi-cantly lower than CShate (with p-value of 0.027).This can be explained by the fact that CSbase is theonly strategy not focusing on any part of the HS inparticular: focusing on something allows for morerelevant generations than generally attacking theentire HS. Therefore, answering RQ2, attackinga specific component of the HS, and in particularits implied statement or its hateful parts, is alwaysbetter than attacking the entire HS without focus-ing on any part, for all the dimensions evaluated byhumans. Safety and attacking strategiesBy grouping theresults by both safety configuration and attackingstrategy (), we can see how, for each di-mension of the human evaluation, the best scoreis achieved by one of the CSw/o models. For co-gency, each CSw/o attacking strategy is better thanits CSw/ counterpart. In general, all the modelsreach a high score on suitableness. Moreover, some common patterns can be found across safety con-figurations. CShate and CSIS obtain the best scoresfor relevance and informativeness, whereas for thelatter dimension CSweak is the worst performing.According to the RR instead, CSIS is the worst,coherently with what is shown in . CSIS isalso the best and second best strategy for cogency,with and without guardrails, respectively.On the other hand, CSbase shows a very differentbehavior for cogency in the two settings, reachingthe best score without guardrails and the worstscore with guardrails (the difference is statisticallysignificant, with a p-value of 0.014). Therefore,the absence of guardrails allows to obtain cogentresponses even without attacking any specificpart of the HS. To sum up, the results that wereobserved before are confirmed once again: eachCSw/o attacking strategy is more cogent than therespective CSw/ version. At the same time, in bothsafety configurations, CSIS and CShate obtainthe best scores. Moreover, the good cogency ofCSbase without guardrails is indicating that CSw/ois good even without any argumentative strategy:the absence of guardrails, in this case, allows themodel that does not use any attacking strategy toobtain a comparable cogency to the best CSw/model which instead uses a specific rhetoricalstrategy (CSIS). Therefore, we can conclude thatthe presence of guardrails has a bigger impact thanthe deployed attacking strategy on the perceivedcogency of the generated responses. We also grouped the results obtained accordingto what part of the argumentative structure theattack is focused on: the conclusion (CSC), thepremise(s) (CSP ), both the premise(s) and the con-clusion (CSP+C), the IS (CSIS) and no specificpart (CSbase)11. The main results are reported be- 11Since this is just a different grouping of the results ob-tained with the various attacking strategies, CSP and CSCare composed only of examples attacking the weak or hatefulpremise or conclusion, respectively; CSP +C is composed ofonly examples attacking the hateful part, since the weak part",
  "low, while more details and the complete tables canbe found in Appendix A.5": "Attacked part of the argumentationAlso whenconsidering whether the attacked part of the HS is apremise, a conclusion, or both, we obtain coherentresults with what is shown in . In particular,attacking a specific part of the HS can give betterresults than CSbase for all the dimensions evaluatedby humans: the best scores are reached by CSISand CSP+C, where the latter is composed only ofCS attacking the hateful part. Safety and attacked part of the argumentationAlso by grouping the results according to bothsafety configuration and attacked part of the HSargument, the absence of guardrails consistentlyallows for more cogent responses, with similarpatterns as shown in . Moreover, eitherattacking the IS or both the hateful premises andconclusion allows for the best quality CS. Finally,attacking the premise generally allows for moreinformative replies than attacking the conclusion.",
  "Conclusion": "In this work, we investigated various strategies toobtain cogent counterspeech. Firstly, we testedwhether the absence of safety guardrails has animpact on the perceived quality of the generatedcounterspeech. Then, we used various attackingstrategies, focusing on different aspects of the hatespeech argument: its hateful premises/conclusion,its weakest argumentative component, its impliedstatement, and no component in particular. To doso, we used Mistral, a model for which the pres-ence of safety guardrails is most controllable. Byconducting an extensive human and automatic eval-uation, we show that the absence of guardrails hasa positive effect on the perceived cogency of the",
  "is always one only element": "generated counterspeech, without hindering theirperceived safety. We also show that attacking spe-cific parts of the hate speech, and in particular itsimplied statement and the hateful premises and con-clusion, can result in better quality counterspeechthan generally attacking the entire message. Fi-nally, when considering the safety configurationand the attacking strategy altogether, the presenceof guardrails has a bigger impact on the perceivedcogency of the generations than the chosen attack-ing strategy. These results are consistent also ifwe group the results by considering whether the at-tacked element is a premise, a conclusion, or both.To conclude, our work shows how the current im-plementation of safety guardrails might be subopti-mal also for tasks that require high safety standards,such as counterspeech generation. In this perspec-tive, by uncovering unintended pitfalls of safetyguardrails, we highlight the necessity of better cal-ibrating the helpfulness-harmlessness tradeoff, inorder to further improve safety tuning in LLMs.",
  "Limitations": "In this work, we only tested Mistral Instruct v0.1:first, as explained in , Mistral Instructv0.1 is one of the best performing available modelsto date that does not have any type of safety tun-ing. At the same time, it also gives the possibilityto enforce guardrails in a simple and controllableway, i.e. by adding a system prompt at generationtime. This makes the comparison between Mis-tral Instruct v0.1 with and without guardrails themost controllable scenario, since we can use thesame model and change only one variable (i.e. thepresence of guardrails) to study its impact in thegenerations.At the same time, counterspeech evaluation isdifficult to automatise, since automatic metrics cannot fully capture the quality of generated coun- terspeech and often do not correlate with humanjudgements. Human evaluation is thus needed, andobtaining a sufficient number of evaluated exam-ples with multiple models would have required ahigh number of human annotators. Regarding hu-man evaluation, the participants can not be strictlyconsidered experts.However, we carefully ex-plained them the task before the annotation, andwe were there for the duration of the annotation toanswer any question that could arise.Moreover, we only worked on the English lan-guage, focusing on the minorities targeted by theWhite Supremacy Forum dataset: our choice wasdriven by the peculiarity of this dataset, which con-tains longer posts than other mainstream social me-dia platforms, allowing for the identification of anargumentative structure. In future work, we wantto expand our analyses also to other languages andtargets of hate.Finally, we understand that with our work wejust touched on a broad problem that is openingmany research questions concerning guardrails andalignment in general. In fact, using RLHF mightmake the model structurally limited for tasks suchas counterspeech generation, and constrain it ona suboptimal output, as we saw with preliminaryanalyses with other models. To make the modelsstronger on this task, we would need to rethinkthe way in which alignment is implemented: how-ever, we acknowledge that more work is requiredto answer this challenge. At the same time, wehope that our paper can represent a small step to-wards highlighting the importance of research inthis direction.",
  "Ethical statement": "In this work, we extensively study the impact ofsafety guardrails on the argumentative strength ofgenerated counterspeech. Even if the results showhow the absence of guardrails allows to obtain morecogent generations, it is important to note that ourposition is not to completely remove guardrailsfrom LLMs. Instead, our work shows that theway in which guardrails are currently implementedmight be suboptimal also for tasks that strongly re-quire high safety standards, such as counterspeechgeneration. In this perspective, our work is meantto uncover unintended pitfalls of safety guardrails,in order to further improve safety tuning in LLMs.We studied possible ways to improve the auto-matic generation of counterspeech. However, au- tomatically generated counterspeech is still imper-fect and subject to producing possible inaccurateinformation or potentially unsafe text. For thesereasons, counterspeech generation models are notmeant to be used autonomously, but always in ahuman-machine collaboration scenario, allowingthe humans to check and possibly modify the gen-erations.Moreover, since we are aware of the negativeconsequences that long exposure to hateful contentcan have, in this work, we only worked with hu-man annotators who volunteered to participate inthe task. Additionally, we put in practice mitigationmeasures similar to those proposed by Vidgen et al.(2019) to preserve the annotators mental health. Inparticular, we first made clear that the annotatorsunderstood the prosocial aspects of the task. More-over, they worked for a limited amount of time (1hour) and we were available during the annotationto let possible problems and distress emerge.Finally, we plan to share the hate speech annota-tions (i.e. the extracted premises/conclusions andtheir weakness/hatefulness labels) for research pur-poses only.",
  "Acknowledgements": "This work was partially supported by the EuropeanUnions CERV fund under grant agreement No.101143249 (HATEDEMICS). This work has alsobeen supported by the French government, throughthe 3IA Cte dAzur Investments in the Futureproject managed by the National Research Agency(ANR) with the reference number ANR-19-P3IA-0002. Nami Akazawa, Serra Sinem Tekiroglu, and MarcoGuerini. 2023.Distilling implied bias from hatespeech for counter narrative selection. In Proceed-ings of the 1st Workshop on CounterSpeech for On-line Abuse (CS4OA), pages 2943, Prague, Czechia.Association for Computational Linguistics. Milad Alshomary, Shahbaz Syed, Arkajit Dhar, MartinPotthast, and Henning Wachsmuth. 2021. Counter-argument generation by attacking weak premises. InFindings of the Association for Computational Lin-guistics: ACL-IJCNLP 2021, pages 18161827.",
  "Milad Alshomary and Henning Wachsmuth. 2023": "Conclusion-based counter-argument generation. InProceedings of the 17th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 957967, Dubrovnik, Croatia. Associ-ation for Computational Linguistics. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,Deep Ganguli, Tom Henighan, Andy Jones, NicholasJoseph, Ben Mann, Nova DasSarma, et al. 2021. Ageneral language assistant as a laboratory for align-ment. arXiv preprint arXiv:2112.00861. Yuntao Bai, Andy Jones, Kamal Ndousse, AmandaAskell, Anna Chen, Nova DasSarma, Dawn Drain,Stanislav Fort, Deep Ganguli, Tom Henighan, et al.2022a. Training a helpful and harmless assistant withreinforcement learning from human feedback. arXivpreprint arXiv:2204.05862. Yuntao Bai,Saurav Kadavath,Sandipan Kundu,Amanda Askell, Jackson Kernion, Andy Jones,Anna Chen,Anna Goldie,Azalia Mirhoseini,Cameron McKinnon, et al. 2022b. Constitutionalai: Harmlessness from ai feedback. arXiv preprintarXiv:2212.08073.",
  "Nicola Bertoldi, Mauro Cettolo, and Marcello Federico.2013. Cache-based online adaptation for machinetranslation enhanced computer assisted translation.In MT-Summit, pages 3542": "Helena Bonaldi, Giuseppe Attanasio, Debora Nozza,and Marco Guerini. 2023. Weigh your own words:Improving hate speech counter narrative generationvia attention regularization. In Proceedings of the1st Workshop on CounterSpeech for Online Abuse(CS4OA), pages 1328, Prague, Czechia. Associationfor Computational Linguistics. Helena Bonaldi, Yi-Ling Chung, Gavin Abercrombie,and Marco Guerini. 2024. Nlp for counterspeechagainst hate: A survey and how-to guide. In Find-ings of the Association for Computational Linguis-tics: NAACL 2024. Association for ComputationalLinguistics. Helena Bonaldi,Sara Dellantonio,Serra SinemTekiroglu, and Marco Guerini. 2022.Human-machine collaboration approaches to build a dialoguedataset for hate speech countering. In Proceedingsof the 2022 Conference on Empirical Methods inNatural Language Processing, pages 80318049. Mauro Cettolo, Nicola Bertoldi, and Marcello Federico.2014. The repetition rate of text as a predictor of theeffectiveness of machine translation adaptation. InProceedings of the 11th Biennial Conference of theAssociation for Machine Translation in the Americas(AMTA 2014), pages 166179. Yi-Ling Chung, Serra Sinem Tekiroglu, and MarcoGuerini. 2021.Towards knowledge-groundedcounter narrative generation for hate speech. In Find-ings of the Association for Computational Linguistics:ACL-IJCNLP 2021, pages 899914, Online. Associa-tion for Computational Linguistics. Ona de Gibert, Naiara Perez, Aitor Garca-Pablos, andMontse Cuadros. 2018. Hate speech dataset froma white supremacy forum. In Proceedings of the2nd Workshop on Abusive Language Online (ALW2),pages 1120, Brussels, Belgium. Association forComputational Linguistics. Margherita Fanton, Helena Bonaldi, Serra SinemTekiroglu, and Marco Guerini. 2021. Human-in-the-loop for data collection: a multi-target counter narra-tive dataset to fight online hate speech. In Proceed-ings of the 59th Annual Meeting of the Association forComputational Linguistics and the 11th InternationalJoint Conference on Natural Language Processing(Volume 1: Long Papers), pages 32263240. Damin Furman, Pablo Torres, Jos Rodrguez, DiegoLetzen, Maria Martinez, and Laura Alemany. 2023a.High-quality argumentative information in low re-sources approaches improve counter-narrative gener-ation. In Findings of the Association for Computa-tional Linguistics: EMNLP 2023, pages 29422956,Singapore. Association for Computational Linguis-tics. Damin Ariel Furman, Pablo Torres, Jos A. Rodriguez,Diego Letzen, Maria Vanina Martinez, and LauraAlonso Alemany. 2023b. Which argumentative as-pects of hate speech in social media can be reliablyidentified? In Proceedings of Fourth InternationalWorkshop on Designing Meaning Representations,co-located with IWCS 2023. Deep Ganguli, Liane Lovitt, Jackson Kernion, AmandaAskell, Yuntao Bai, Saurav Kadavath, Ben Mann,Ethan Perez, Nicholas Schiefer, Kamal Ndousse,et al. 2022. Red teaming language models to re-duce harms: Methods, scaling behaviors, and lessonslearned. arXiv preprint arXiv:2209.07858. Sadaf MD Halim, Saquib Irtiza, Yibo Hu, Latifur Khan,and Bhavani Thuraisingham. 2023. Wokegpt: Im-proving counterspeech generation against online hatespeech by intelligently augmenting datasets using anovel metric. In 2023 International Joint Conferenceon Neural Networks (IJCNN), pages 110. IEEE. Xinyu Hua, Zhe Hu, and Lu Wang. 2019. Argumentgeneration with retrieval, planning, and realization.In Proceedings of the 57th Annual Meeting of the As-sociation for Computational Linguistics, pages 26612672. Xinyu Hua and Lu Wang. 2018. Neural argument gen-eration augmented with externally retrieved evidence.In Proceedings of the 56th Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 219230. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, Llio Renard Lavaud,Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,Thibaut Lavril, Thomas Wang, Timothe Lacroix,and William El Sayed. 2023. Mistral 7B. Yohan Jo, Seojin Bang, Emaad Manzoor, Eduard Hovy,and Chris Reed. 2020.Detecting attackable sen-tences in arguments. In Proceedings of the 2020Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 123, Online. As-sociation for Computational Linguistics. Jiayu Lin, Rong Ye, Meng Han, Qi Zhang, RuofeiLai, Xinyu Zhang, Zhao Cao, Xuan-Jing Huang, andZhongyu Wei. 2023. Argue with me tersely: To-wards sentence-level counter-argument generation.In Proceedings of the 2023 Conference on Empiri-cal Methods in Natural Language Processing, pages1670516720. Jimin Mun, Emily Allaway, Akhila Yerukola, LauraVianna, Sarah-Jane Leslie, and Maarten Sap. 2023.Beyond denouncing hate: Strategies for counteringimplied biases and stereotypes in language. In Pro-ceedings of the 2023 Conference on Empirical Meth-ods in Natural Language Processing. Jing Qian, Anna Bethke, Yinyin Liu, Elizabeth Belding,and William Yang Wang. 2019. A benchmark datasetfor learning to intervene in online hate speech. InProceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the9th International Joint Conference on Natural Lan-guage Processing (EMNLP-IJCNLP), pages 47554764, Hong Kong, China. Association for Computa-tional Linguistics. Paul Rttger, Hannah Rose Kirk, Bertie Vidgen,Giuseppe Attanasio, Federico Bianchi, and DirkHovy. 2023. Xstest: A test suite for identifying exag-gerated safety behaviours in large language models.arXiv preprint arXiv:2308.01263. Daniel Russo, Shane Peter Kaszefski-Yaschuk, JacopoStaiano, and Marco Guerini. 2023. Countering mis-information via emotional response generation. InProceedings of the 2023 Conference on EmpiricalMethods in Natural Language Processing. Misa Sato, Kohsuke Yanai, Toshinori Miyoshi, Toshi-hiko Yanase, Makoto Iwayama, Qinghua Sun, andYoshiki Niwa. 2015. End-to-end argument gener-ation system in debating. In Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 109114.",
  "writing: A case study analyzing the relationship be-tween surface structure and substance. Journal ofEnglish for Academic Purposes, 17:1223": "Serra Sinem Tekiroglu, Helena Bonaldi, MargheritaFanton, and Marco Guerini. 2022. Using pre-trainedlanguage models for producing counter narrativesagainst hate speech: a comparative study. In Find-ings of the Association for Computational Linguis-tics: ACL 2022, pages 30993114, Dublin, Ireland.Association for Computational Linguistics. Serra Sinem Tekiroglu, Yi-Ling Chung, and MarcoGuerini. 2020. Generating counter narratives againstonline hate speech: Data and strategies. In Proceed-ings of the 58th Annual Meeting of the Associationfor Computational Linguistics, pages 11771190, On-line. Association for Computational Linguistics. Bertie Vidgen, Alex Harris, Dong Nguyen, RebekahTromble, Scott Hale, and Helen Margetts. 2019.Challenges and frontiers in abusive content detection.In Proceedings of the third workshop on abusive lan-guage online, pages 8093. Bertie Vidgen, Hannah Rose Kirk, Rebecca Qian, NinoScherrer, Anand Kannappan, Scott A Hale, and PaulRttger. 2023. Simplesafetytests: a test suite for iden-tifying critical safety risks in large language models.arXiv preprint arXiv:2311.08370. Henning Wachsmuth, Shahbaz Syed, and Benno Stein.2018. Retrieval of the best counterargument withoutprior topic knowledge. In Proceedings of the 56thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 241251.",
  "A.1Hate speech automatic annotation details": "PremisesandconclusionWeemployedgpt-3.5-turbo-instruct (max_token = 256,temperature = 0.2). The prompt we used is:Given the message:message.Extractfrom the text,the conclusion and theset of premises that lead to it.Theconclusionistheconsequenceoftheset of premises.The premises are themotivations that lead to a conclusion.Makeanyrephrasesthatyouconsiderpropertokeepgrammarcorrect.Ifthereisnopremiseorconclusioninthe text do not generate it. Also, makeexplicit the target of hate both in thepremisesandintheconclusion.Theoutputshoulddistinguishbetweenthedifferent premises and the conclusion. Implied StatementFor the implied statementextraction, we used the best-performing config-uration described in Akazawa et al. (2023): HS IS BART model, with greedy search decod-ing, repetition penalty=2, max_len=156, andtemperature=1.",
  "A.2Prompts for testing safety": "As mentioned in , to compare the unsafetyof the Mistral v0.1 and v0.2 models in their with-out guardrails configuration, we used both Simple-SafetyTests (Vidgen et al., 2023) and 13 additionalprompts which we manually created. We used theseadditional prompts since our task specifically fo-cuses on addressing hate speech directed towardsminorities, and this kind of unsafety is not coveredin SimpleSafetyTests. A complete list of the 13prompts we used is shown in . Over 113unsafe prompts, the v0.1 model generates unsafereplies in 64 cases (56,6%) and v0.2 in 20 cases(17,7%). The results on the 100 SimpleSafetyTestsexamples alone are comparable, i.e.: 54 unsaferesponses for v0.1 vs 15 unsafe responses for v0.2.",
  "A.3Detailed description of human annotation": "As described in .1, we used four dimen-sions with a Likert scale from 1 to 5: relevance,suitableness, informativeness and cogency. Dur-ing the evaluation, the annotators could ask forclarifications or doubts they could have. If theywere struggling with a specific score, they could leave it blank as a last resort and leave a comment.They were also allowed to search on the web tofact-check specific facts mentioned in the coun-terspeech if needed. Following, we provide thedetailed description of the scores across the fourdimensions. For a more efficient annotation, theannotators were provided with tables summarisingthe different scores for each dimension (,7, 8, 9). The annotators were also provided withthe reference examples, here reported below thedescription of each metric.",
  "High relevance example (score: 5)": "HS: Living in a boarder state, Texas, the problems of illegalaliens is, naturally, a stark reality, and let me add that theonly solution that I see is the implimentation of U.S. Troopsas guards on the Mexican boarder. CS: While its true that Texas, as a border state, facesunique challenges related to immigration, its important toremember that immigrants, regardless of their legal status,contribute significantly to our economy and society.",
  "High suitableness example (score: 5)": "HS: I am sure California is full of sick self hating whites,who think that for the good of all the black brown andyellow peoples of the world, whites need to commit racialsuicide and disappear from their face of the earth for theirsake and wont that be just dandy! CS: Promoting harm or extinction of any race is not thesolution to societal issues. We should strive for unity,understanding, and respect among all races, as diversityand coexistence are what truly benefit humanity.",
  "Low informativeness example (score: 1)": "HS: I am sure California is full of sick self hating whites,who think that for the good of all the black brown andyellow peoples of the world, whites need to commit racialsuicide and disappear from their face of the earth for theirsake and wo nt that be just dandy! CS: Promoting harm or extinction of any race is not thesolution to societal issues. We should strive for unity,understanding, and respect among all races, as diversityand coexistence are what truly benefit humanity.",
  "High cogency (5) but low informativeness (1)": "HS: I never really thought about it before but I guess bicy-cling is primarily a white activity, kind of like sunbathing,going swimming, playing golf, tennis, attending a socialfunction without a brawl taking place, not smelling like awild animal in the locker room after a hard workout, andspeaking proper English. CS: The claim that bicycling is only for white people is anarrow and baseless assumption that ignores the diversityof human activity. Similarly, the assertion that non-whitepeople are not civilized based on their appearance or inabil-ity to speak perfect English is derogatory and misguided.Both bicycling and going to a social function can be en-joyed by people of all races and cultures, and personalhygiene and linguistic abilities should not be used as crite-ria for determining ones level of civilization. The counterspeech is providing multiple rea-sons against the hate speech, and they are allsound/relevant. At the same time, the counter-speech does not provide any reference to specificfacts, events, or figures that are not present in theHS. For these reasons, it is scored with cogency 5and informativeness 1.",
  "A.4Distribution of the examples": "Below, we show the distribution of the annotatedand generated examples, according to the attack-ing strategy (), the attacked part of theargumentation (), both the safety config-uration and the attacking strategy () andboth the safety configuration and the attacked partof the argumentation (). Note that the gen-erated CS examples are in total 1626, but since in",
  ": The distribution of the annotated CS examples,according to the attacked part of the argumentation": "20 HS examples the hateful part and the weak partcoincide, in those cases we generated one uniqueCS and considered it as both attacking the weakand the hateful part. Therefore, in the dataset ofgenerated CS, 160 generated examples figure asboth attacking the weak and the hateful part, andare considered to calculate the automatic metricsfor both strategies (they were excluded from thehuman evaluation). Moreover, 50 examples werescored by pairs of two annotators: we distributedthem across all the annotators so that there were17 pairs of annotators evaluating the same batchof examples. We calculated the Inter AnnotatorAgreement using the Weighted Cohens Kappa: theagreement for each dimension ranges between 0.2and 0.46. A moderate agreement is common insubjective tasks such as counterspeech evaluation:",
  "A.5Results on the attacked part of the HSargument": "Attacked part of the argumentIf we considerwhether the attacked part is a premise, a conclu-sion or both (Tables 14 and 15), CSIS is still thestrategy with the highest cogency. For what re-gards relevance, instead, CSP+C has a significantlyhigher score than CSC and CSbase: a possible rea-son might be the length of the input used for gen-erating the CS. In fact, for CSP+C, the attackedpart of the input HS is the longest. CSP+C is alsothe most suitable approach, and the second best forcogency. Therefore, attacking both the premise andthe conclusion, when they are hateful, gives a goodresult in terms of argumentative strength and suit-ableness of the generated CS. CSP , instead, has asignificantly higher informativeness than both CSCand CSbase. Finally, CSC is the worst in all humandimensions, apart from suitableness. Safety and attacked part of the argumentIfwe focus on both safety configuration and attackedpart of the HS argument, some parallelisms can beshown across CSw/ and CSw/o (Tables 16 and 17).For what regards relevance, CSP+C always reachesthe highest score and CSIS the second highest;CSIS also shows the second-best informativeness,across safety configurations. CSP is the best forinformativeness, while CSC is the worst, for bothsafety configurations. Once again, each CSw/ostrategy achieves a higher cogency than its CSw/counterpart."
}