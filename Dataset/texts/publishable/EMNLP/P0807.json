{
  "Abstract": "Acoustic foundation models, fine-tuned for Au-tomatic Speech Recognition (ASR), suffer fromperformance degradation in wild acoustic testsettings when deployed in real-world scenar-ios. Stabilizing online Test-Time Adaptation(TTA) under these conditions remains an openand unexplored question.Existing wild vi-sion TTA methods often fail to handle speechdata effectively due to the unique character-istics of high-entropy speech frames, whichare unreliably filtered out even when contain-ing crucial semantic content. Furthermore, un-like static vision data, speech signals followshort-term consistency, requiring specializedadaptation strategies. In this work, we pro-pose a novel wild acoustic TTA method tailoredfor ASR fine-tuned acoustic foundation mod-els. Our method, Confidence-Enhanced Adap-tation, performs frame-level adaptation usinga confidence-aware weight scheme to avoid fil-tering out essential information in high-entropyframes. Additionally, we apply consistencyregularization during test-time optimization toleverage the inherent short-term consistency ofspeech signals. Our experiments on both syn-thetic and real-world datasets demonstrate thatour approach outperforms existing baselinesunder various wild acoustic test settings, in-cluding Gaussian noise, environmental sounds,accent variations, and sung speech 1.",
  "Corruptions": "WER Wav2vec2 Large : Robustness analysis of Wav2vec2 Base andLarge under wild acoustic test settings including 1)Noise (N): additive noises on LibriSpeech test-otherset, 2) Accent (A): accents of L2 learners on L2-Arcticsubset 3) Singing (S): sung speech on DSing test set. In-Domain (ID) indicates the performance on LibriSpeechtest-other set without additive noises. WER is short forWord Error Rate. et al., 2019), and timbre variations due to accentor pronunciation changes (Yang et al., 2023b).While recent acoustic foundation models, such asWav2vec2 (Baevski et al., 2020), fine-tuned onAutomatic Speech Recognition (ASR) achieve ex-cellent performances, they exhibit notable perfor-mance degradation when confronted with the test-time speech in the wild, as depicted in .Consequently, there exists an emergent demandto adapt these acoustic foundation models in wildacoustic test settings when deployed in the realworld.Prior methods for mitigating domain shifts re-quire access to domain-specific source data underthe unsupervised domain adaptation setting (Bellet al., 2020), limiting the application to online sce-narios where speech data come from the wild worldwith mixed distribution shifts. Test-Time Adap-tation (TTA) emerges as a critical paradigm foraddressing distribution shifts at inference time, en-abling online updates of models on test data in asource-free way. Recent work, SUTA (Lin et al.,2022), presents a pilot study on TTA for ASR mod-els by applying entropy minimization to speech frame adaptation, demonstrating impressive perfor-mance on single-utterance TTA. However, SUTAfocuses on mild test settings, e.g., testing on speechwith synthetic and real noises. In the dynamicwild world, acoustic foundation models may facearbitrary test speech data with severe distributionshifts, such as sung speech. As such, stabilizing on-line TTA under wild acoustic test settings remainsan open and unexplored question. Recent work,SAR (Niu et al., 2023), proposes an efficient op-timization scheme for stabling online TTA in thewild vision test settings. However, direct adoptionof SAR to speech data is challenging because SARcharacterizes high-entropy noisy speech samples asunreliable and potentially harmful for model adap-tation and proposes to filter them out for stablingunder wild vision test settings.In this work, we empirically identify a sub-stantial proportion of noisy frames within non-silent speech segments under wild acoustic testsettings. We observe that these frames containvital semantic information crucial for accuraterecognition and merely discarding these noisyframes may adversely affect model performance.Consequently, rather than excluding these noisynon-silent frames, we propose Confidence En-hanced Adaptation (CEA), which performs frame-level adaptation using a confidence-aware weightscheme. CEA prioritizes uncertain frames and en-courages models to focus more on these uncertainframes by denoising their intermediate represen-tations. Additionally, we emphasize that frameswithin a short speech segment are temporally coher-ent, largely due to the consistent nature of phone-mic content within such windows, thus proposingshort-term consistency regularization to stabilizewild acoustic TTA. This contrasts with image sam-ples in a batch, which are frequently treated asindependent entities. We conduct a wide rangeof experiments for ASR fine-tuned acoustic foun-dation models on both synthetic and real-worlddatasets, systematically assessing the models ro-bustness against Gaussian noises, environmentalsounds, accents of second language (L2) learners,and singing (a.k.a. sung speech). The experimentalresults demonstrate the effectiveness of our methodunder wild acoustic test settings.In summary, our contributions are summarizedas follows:",
  "Test-Time Adaptation": "Test-time adaption plays an essential role in ad-dressing distribution shifts encountered in test sam-ples, enabling online updates of models during thetest phase using unsupervised objectives. Mostprior TTA methods in the computer vision do-main rely on Batch Normalization layers (Ioffeand Szegedy, 2015; Lim et al., 2023; Niu et al.,2022) and assume sample independence within thesame batch (Wang et al., 2022; Gong et al., 2022)despite addressing non-i.i.d data streams in fluctu-ating environments, rendering them less applica-ble to speech data. Additionally, real-world datashifts, including both covariate and label shifts,pose significant challenges for deployment (Kohet al., 2021; Niu et al., 2023; Zhou et al., 2023).From another line of research, (Huang et al., 2022)introduced a training-free TTA framework that han-dles non-stationary covariate shifts by leveraging alatent continuous-time dynamical system to infermodel parameters. Recent work provides a pilotstudy on TTA for ASR models under mild testsettings (Lin et al., 2022), and improves TTA forgeneral ASR models via sequence-level general-ized entropy minimization (Lin et al., 2022). Ourwork focuses more on stabilizing online TTA forASR models under wild acoustic settings. We em-pirically analyze frame-level entropy distributionand underscore the short-term consistency natureof speech signals.",
  "Robustness for ASR": "There is a long history of developing robust speechrecognition methods (Li et al., 2014). For exam-ple, Huang and Mak (2017, 2018) enhances therobustness of acoustic models by incorporatinghigher-order features, while Huang et al. (2019,2021) improves the noise robustness by guidingthe model to focus on inferred informative latentacoustic events. Different from improving modelrobustness by training with large-scale augmenteddata (Radford et al., 2023), there are various adap-tation approaches for acoustic distribution shifts.Recent works explore input reprogramming (Yanget al., 2021, 2023a) with supervised optimizationtargets. Unsupervised domain adaptation (UDA)approaches investigate the feature alignment (Houet al., 2021), data augmentation (Hsu et al., 2017),domain adversarial training (Sun et al., 2017, 2018),knowledge distillation (Li et al., 2017), and self-training (Li et al., 2017). However, these methodsrequire access to the source data with severe la-tency and heavy computation, and tackle distinctacoustic shifts, such as speaker (Deng et al., 2022)and accent adaptation (Yang et al., 2023b) in iso-lation, limiting their applications to online scenar-ios. Early test-time method for traditional acousticmodels, LUHC, with parameterized activation func-tions (Swietojanski and Renals, 2014; Swietojanskiet al., 2016) also deals with specific acoustic shifts,lacking the generalization ability under wild acous-tic test settings. Despite the success of prior adap-tation methods, the development of online TTA formodern ASR-fined acoustic foundation models un-der wild acoustic test settings remains an open andunexplored question.",
  "Preliminary": "We center our focus on the fully Test-Time Adapta-tion framework, characterized by episodic modeladaptation, where the model is reset after process-ing each utterance. We denote the ASR fine-tunedacoustic foundation model as f(y|x). We investi-gate the popular acoustic foundation models suchas Wav2vec2 (Baevski et al., 2020), HuBERT (Hsuet al., 2021), WavLM (Chen et al., 2022), which canbe typically decomposed into two constituent com-ponents: a feature extractor g(z|x), parameterizedby , and a transformer encoder h(y|z), param-eterized by . This decomposition is expressedas:f(y|x) = h(g(x))(1) where = {, } represents the collective set ofmodel parameters. The feature extractor g takes asinput waveform audio or log-mel spectrogram. Thetransformer encoder h serves as an audio encoderand outputs acoustic representations. Consideringa test-time speech sequence x1:n of variable lengthn in the wild, typically with arbitrary domain shifts,the primary objective entails adapting the acousticfoundation model f to enhance its performancefor x1:n.",
  "Wild Acoustic Test Settings": "Wild acoustic distribution shifts encountered withinthe speech domain may originate from severalsources, including:Speaker Changes. Timbre variations in speechstemming from changes in the speakers identity.Environmental Noises.Perturbations intro-duced by ambient noises in the recording environ-ments.Pronunciation Changes. Alteration in pronun-ciation characteristics such as accent or singing.Text-Domain Changes. Shifts in the linguisticcontent or context of the speech data.It is noteworthy that speaker changes, environ-mental noises, and pronunciation changes are typi-cally categorized as covariate shift, as they pertainto variations in the input data distribution. In con-trast, text-domain changes are categorized as labelshift, as they involve alterations in the output dis-tribution. Furthermore, it is important to acknowl-edge that real-world speech data often exhibit shiftsstemming from multiple sources simultaneously,rendering the adaptation under wild acoustic testsettings complex and challenging.",
  "Layer Norm": ": The overall framework of the proposed method. The figure takes a Connectionist Temporal Classification(CTC) based acoustic foundation model as an example. This framework involves two steps. The confidence enhancedadaptation is first performed to boost the reliability of noisy frames. The temporal consistency regularization isemployed across the entire input sequence and jointly optimized with entropy minimization.",
  "sil-hsil-l": ": Frame-Level Entropy Distribution in ASRfine-tuned Acoustic Foundation Models: the entropydistributions are computed for Wav2vec2 Base modelson the LibriSpeech noise-corrupted test-other and DS-ing test datasets across adaptation steps. We employ athreshold of 0.4 ln C, as recommended in Niu et al.(2022), where C represents the number of task classes.Frames with entropy values exceeding this threshold arehighlighted in red, indicating high-entropy (h) frames,while low-entropy (l) frames are marked in blue. Weuse to denote non-silent (non-sil) frames and forsilent (sil) frames and take the blank symbol as an ap-proximate indicator. The training steps range from 0 to9, and the results presented in each subfigure are basedon the average of 100 random samples. the sung speech dataset, DSing-test. These exper-iments were performed with the ASR fine-tunedWav2vec2 Base model. We subsequently evaluatedthe percentages of high-entropy and low-entropyframes for both non-silent and silent speech seg-ments. The classification of frames as silent ornon-silent was determined based on pseudo labelsderived from model predictions.As illustrated in , our findings reveal that, prior to any adaptation (Step=0), within the non-silent frames category, there exists a prevalence ofhigh-entropy frames compared to low-entropy onesfor Base models. Conversely, the opposite trendis observed within the silent frames category. Itis worth noting that existing literature (Niu et al.,2023) provides heuristic insights suggesting thathigh-entropy samples may be unreliable and couldpotentially have a detrimental impact on modeladaptation. However, it is crucial to recognize thatthese noisy frames contain essential content infor-mation that is critical for speech recognition. Whileprior research suggests that filtering out such un-reliable samples may aid in stabilizing adaptationunder wild vision test settings and improving per-formance, this approach proves infeasible in ourspecific case.In response, rather than dropping these high-entropy noisy frames, we propose a learning-based approach, Confidence Enhanced Adaptation(CEA), which performs frame-level adaptation us-ing a confidence-aware weight scheme. CEA pri-oritizes uncertain frames and encourages modelsto focus more on these uncertain frames by de-noising their intermediate representations. Denot-ing yci = f(c|x1:n) as the predicted probabilityof class c for i-th frame, we quantify uncertaintythrough entropy, defined as:",
  "+ exp(E(xi))Iyi=c0(xi)(4)": "where c0 signifies the index corresponding tosilent frames, and I is an indicator function. Suchdesign empowers the model to assign greater impor-tance to frames where it exhibits lower confidence.The increased weight encourages the model to fo-cus more on these uncertain frames during adapta-tion, potentially leading to heightened model con-fidence on such frames. Note that this adaptationprocess entails an update of the feature extractorg. This empowers models with the capability toadapt to wild acoustic shifts, even in the presenceof substantial covariate shifts. As evidenced in Fig-ure 3, the count of high-entropy frames diminisheswhile low-entropy frame counts increase with eachadaptation step, underscoring the effectiveness ofCEA.",
  "Short-Term Consistency Regularization": "In the domain of speech signal processing, a salientcharacteristic is the short-term stability, wheresuccessive speech frames often convey the samephoneme or speech unit. This intrinsic temporalcorrelation is a defining attribute of speech data,making it essential for stabilizing online TTA un-der wild acoustic test settings. Nevertheless, priorTTA methods largely overlook this inherent tempo-ral correlation within individual speech sequences.To address this limitation, we propose a feature-wise short-term consistency regularization tech-nique. We perform this regularization step afterthe confidence enhanced adaptation process. Thissequencing is deliberate as introducing temporal regularization over representations of noisy framescan potentially confuse models and yield undesir-able optimization outcomes. Concretely, the reg-ularization is jointly optimized alongside entropyminimization, as represented by the following equa-tion:",
  "(5)": "where denotes the weight assigned to the reg-ularization loss, and LN represents the affine pa-rameters associated with layer normalization acrossthe entire acoustic foundation model. Here, zi sig-nifies the feature representation of i-th frame ob-tained from the fine-tuned feature extractor, andzi represents the modified feature representationachieved through a parameter-free self-attentionoperation. The parameter k denotes the size of thewindow considered as the neighborhood of framexi. This regularization technique effectively cap-tures the inherent temporal consistency found inspeech data by compelling the representation of xito closely resemble that of its neighboring frameswithin a predefined window. Despite the possiblepeaky behavior of CTC, the proposed temporal con-sistency can be treated as introducing the inductivebias of \"short-term stability\" in the adaptation (Ra-biner et al., 2007).",
  "Experiments": "In this section, we undertake an evaluation of therobustness of ASR fine-tuned acoustic foundationmodels under wild acoustic test settings. We dis-cuss the robustness against synthetic noises includ-ing Gaussian noises and real-world environmentalsounds in .2, real-world data shifts includ-ing L2 accents and singing voice (sung speech) in.3, and decoding strategy pertaining tolanguage models in .4. We provide moreevaluation results using various acoustic models inAppendix B.4.",
  ": WER (%) results on LS-C over five severity levels of Gaussian noises using Wav2vec2 Base with greedydecoding. = 0 represents the uncorrupted case. The best results are bold": "of severity to simulate various degrees of corrup-tion as per (Hendrycks and Dietterich, 2019) forevaluating the trend of model robustness. Higherlevels indicate more severe corruption althoughheavily corrupted speech data may not be commoncases in the real world. Subsequently, the secondsynthetic dataset, named LS-P, is the LibriSpeechtest-other set Perturbed by real-world environmen-tal sounds. This dataset encompasses eight diversetypes of environmental sound, including Air Condi-tioner, Babble, Munching, Shutting Door, VacuumCleaner, Airport Announcements, Copy Machine,and Typing. These environmental sounds are fromthe MS-SNSD noise test set (Reddy et al., 2019).Each type is added to the original audio with fivedistinctive signal-to-noise ratios (SNRs) represent-ing five levels of severity. Our study further extendsto two real-world datasets. The L2-Arctic (Zhaoet al., 2018) dataset comprises speech data fromsecond language (L2) learners originating from sixcountries with different first languages (L1): Ara-bic, Mandarin, Hindi, Korean, Spanish, and Viet-namese. Furthermore, we broaden our investiga-tion to encompass music datasets, DSing (Dabikeand Barker, 2019) and Hansen (Hansen and Fraun-hofer, 2012), featuring singing voice (sung speech).More details of dataset statistics can be found inAppendix A.1 and details of implementation canbe found in Appendix A.2. Baselines.To assess the adaptation performanceof our proposed method, we consider the follow-ing TTA baselines. Tent (Wang et al., 2020) adapttransformation layers with the objective of entropyminimization. Despite it being initially proposedfor batch normalization, we refer to updating theaffine parameters of layer normalization as Tentin our work. In addition, we involve the base-line TeCo (Yi et al., 2023), originally proposed for video classification with temporal coherenceregularization, due to its applicability to sequentialdata. Our comparison also includes the SAR (Niuet al., 2023), specifically designed to address datashifts in the dynamic wild world. Furthermore, wealso introduce comparisons with SUTA (Lin et al.,2022) using entropy minimization and minimumclass confusion, and SGEM (Kim et al., 2023) us-ing sequential-level generalized entropy minimiza-tion in conjunction with beam search employinglanguage models.",
  "Robustness to Synthetic Noises": "Gaussian Noises.In the initial phase of our ex-periments, we focus on synthetic data and assessthe robustness in the presence of various levels ofGaussian noise injected into the test speech audio.The outcomes are reported in . It is observedthat our proposed method consistently outperformsexisting baseline approaches across five levels ofnoise. Notably, our approach achieves a relative im-provement of 21.5% on average in terms of WER,when compared to using the source model withoutadaptation.Furthermore, it is imperative to note that SAR,designed for addressing wild vision data shifts,demonstrates comparatively less improvementcompared with the Tent method. This observationunderscores the limitations of filtering noisy framesfor speech recognition. Instead, the learning-basedadaptation adopted in our method shows superi-ority. Moreover, we discover that TeCo providesmarginal improvement compared to Tent, indicat-ing that coherence regularization is limited in thecontext of noisy frames. In contrast, our confi-dence enhanced adaptation yields further benefitsfor temporal consistency regularization. Environmental Sounds.We further evaluatethe robustness on LS-P, which introduces eightcommon environmental sounds in the test audioat five levels of severity. The results of adding AirConditioner sound and Typing sound are reportedin and respectively (Full experi-mental results can be found in Appendix B.8). It is noticeable that our method can yield over 30%relative improvements in low-SNR scenarios. No-tably, for the case with 5 dB SNR in , ourmethod demonstrates a substantial 41.7% relativeimprovement, suggesting its efficacy in mitigatingthe impact of real-world environmental sound cor-ruption.",
  "Robustness to Real-World Data Shifts": "L2 Accents.Data shifts resulting from accentvariations are a common occurrence in real-worldscenarios, arising from differences in dialects ornon-native speech patterns. Another pertinent in-stance of such shifts is encountered in childrensspeech, which is also a common pronunciationchange and one type of accent in the real world. Inorder to assess the robustness to such pronunciationvariations, we undertake the test-time adaptationto accents exhibited by L2 learners using the L2-Arctic dataset. To comprehensively evaluate theperformance, we evaluate all speakers for each L1and present the speaker-level results for each L1 inAppendix B.9. The experimental findings consis-tently underscore the superiority of our proposedmethod across different L1 categories. Singing Voice.In this session, we discuss therobustness of ASR fine-tuned acoustic foundationmodels to singing voice for the first time. Singing,also referred to as sung speech, is characterizedby a distinctive pronunciation pattern. Notably,it encompasses various frequency fluctuations, in-cluding the apparent pitch variations along with",
  ":WER (%) results on DSing-test usingConformer-CTC and Conformer-Transducer": "the melody. This constitutes a tremendous covari-ate shift, rendering the adaptation from speech tosinging more challenging than that from speech tospeech. Moreover, the existence of professionalsinging techniques further compounds the chal-lenges associated with adaptation. For instance,the elongation of word pronunciation, a commonoccurrence in singing, is a departure from typicalspeech patterns.To evaluate the adaptation performance undershifts from singing voice, we conduct experimentson three datasets, utilizing both Wav2vec2 Baseand Wav2vec2 Large models. The outcomes arepresented in . The results indicate that ourproposed method consistently attains the best per-formances for both Base and Large models. Inaddition, the Wav2vec2 Large model exhibits supe-rior robustness than the Base model. Nevertheless,it still experiences a noticeable performance degra-dation when compared with adaptation in noiseand accent robustness evaluations, suggesting thelimited ability of acoustic foundation models underwild acoustic test settings.",
  "Decoding Strategies": "We discuss the decoding strategies employed inexperiments in this session. In our preceding exper-iments, we mainly utilize greedy decoding, whichdoes not explicitly tackle the text-domain changes.In the subsequent analysis, we compare our pro-posed method with SGEM, which leverages beamsearch for decoding. The results are presented in. Notably, our findings reveal that even inthe absence of explicit adaptation for the languagemodel, our approach still consistently outperformsSGEM. We also observe that the results achievedby our method using greedy search can, on aver-age, surpass those of SGEM. We conjecture thatour proposed short-term consistency regularizationaddresses the label shift implicitly by fostering la-bel coherency among neighbor frames. Moreover,it is discovered that the enhancements facilitated",
  "Generalization on Different ASR Models": "We examine the robustness of CTC-based acous-tic foundation models in our main experimentsand Appendix B.4. To verify the efficacy of ourmethod on other end-to-end ASR models such asConformer and Transducer, we conducted experi-ments on Conformer-CTC (Gulati et al., 2020) andConformer-Transducer (Burchi and Vielzeuf, 2021)as per Kim et al. (2023). For consistent setting andfair comparison, we experimented with DSing-testand reported the results in . The empiricalresults illustrate that our proposed method can begeneralized to different end-to-end ASR modelsand outperform SUTA and SGEM baselines.",
  "Ablation Study": "We conduct the ablation study on Noise, Accent,Singing shifts respectively using Wav2vec2 Basewith greedy search to dissect the individual impactof two core components proposed in our methods.The results presented in illustrate that theremoval of short-term consistency regularization(STCR) leads to a relatively modest decline in per-formance, in contrast to the more substantial dete-rioration observed upon the removal of confidenceenhanced adaptation (CEA). This observation un-derscores the significance of our proposed CEA.Furthermore, the introduction of STCR yields ad-ditional performance gains when employed in con-junction with CEA. These experimental findingsalso indicate a pronounced efficacy of our methodin mitigating noise shifts as opposed to accent andsinging shifts. We conjecture the reason could bethat the shift caused by Gaussian noises for eachframe is consistent while other shifts such as accentshift could be different within frames.",
  "Latency Analysis": "We did the adaptation with a single coming utter-ance and counted the difference between the timewhen the utterance has ended and the time whenthe adaptation process has ended. We calculatethe average latency over all samples of Librispeechtest-other set on Wav2vec2 Base and obtain thelatency of 1.07 seconds. The average recognitionrun-time on A5000 is 1.20 seconds. We believe thiscould be an acceptable delay due to large parametersizes for acoustic foundation models. We provideadditional comparisons in terms of computing inAppendix B.2.",
  "Comparison with Whisper": "State-of-the-art models such as Whisper (Radfordet al., 2023) improve noise robustness by leverag-ing a large training corpus with data augmentationby adding noise. To gain an insight on how our TTAmethod for improving noise robustness compareswith Whisper, we conduct additional experimentson LS-C using Whisper and report the performancein . We choose Whisper-Base due to its com-parable parameter size to Wav2vec2 Base. Notethat it is impossible to make a fair comparison sinceboth Whisper Base (74M) and Small (244M) havedifferent parameter sizes to the Wav2vec2 Base( 90M). It is interesting to observe that the per-formances of adapted Wav2vec2 Base in can surpass those of unadapted Whisper-Base forseverity levels 1 to 4, and the unadapted Whisper-Base.en for severity levels 1 and 2, demonstrat-ing the effectiveness of the proposed TTA method.This also indicates that training with augmenteddata like Whisper brings more robustness to moresevere corruption. However, whether these resultsgeneralize to wilder acoustic test settings, whichare beyond the scope of Whispers objective butcentral to ours, remains an open question for futureinvestigation.",
  "In this paper, we study the Test-Time Adaptationof ASR fine-tuned acoustic foundation models": "under wild acoustic test settings. By investigat-ing the role of high-entropy noisy frames withinnon-silent speech segments, we introduce Con-fidence Enhanced Adaptation with a confidence-aware weight optimization scheme to prioritizethese noisy frames for efficient adaptation via de-noising their intermediate representations ratherthan discarding them. Moreover, our emphasis onshort-term stability of speech signals leads us to ap-ply consistency regularization, yielding further im-provement for stable online TTA. Our experimentalfindings suggest a consistent improvement for dif-ferent types of acoustic shifts and different degreesof corruption on synthetic and real-world datasets,demonstrating the efficacy of our approach underwild acoustic test settings.",
  "Limitations": "Our work is subject to several limitations. Firstly,further research endeavors could encompass abroader exploration of adaptation techniques forthe decoder model, particularly for text-domainadaptation. It remains challenging to adapt lan-guage models to address text-domain shifts dueto the unavailability of target domain texts in theTTA setting. Additionally, we mainly experimentwith ASR fine-tuned acoustic foundation models.The broader applicability of our method to diversespeech tasks, including but not limited to multi-speaker-related scenarios, spoken language under-standing, and general audio classification tasks re-mains unexplored. Therefore, we consider adaptingour approach to these tasks under wild acoustic testsettings as the future work. Finally, our work isnot specifically tailored for online streaming ap-plications and TTA under streaming scenarios forlatency reduction is definitely essential in futurework.",
  "The authors would like to thank anonymous review-ers for their valuable suggestions. This project isfunded by a research grant MOE-MOESOL2021-0005 from the Ministry of Education in Singapore": "Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,and Michael Auli. 2020. wav2vec 2.0: A frameworkfor self-supervised learning of speech representations.Advances in neural information processing systems,33:1244912460. Peter Bell, Joachim Fainberg, Ondrej Klejch, Jinyu Li,Steve Renals, and Pawel Swietojanski. 2020. Adap-tation algorithms for neural network-based speechrecognition: An overview. IEEE Open Journal ofSignal Processing, 2:3366. Maxime Burchi and Valentin Vielzeuf. 2021. Efficientconformer: Progressive downsampling and groupedattention for automatic speech recognition. In 2021IEEE Automatic Speech Recognition and Understand-ing Workshop (ASRU), pages 815. IEEE. Sanyuan Chen, Chengyi Wang, Zhengyang Chen,Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, NaoyukiKanda, Takuya Yoshioka, Xiong Xiao, et al. 2022.Wavlm: Large-scale self-supervised pre-training forfull stack speech processing. IEEE Journal of Se-lected Topics in Signal Processing, 16(6):15051518.",
  "Gerardo Roa Dabike and Jon Barker. 2019. Automaticlyric transcription from karaoke vocal tracks: Re-sources and a baseline system. In Interspeech, pages579583": "Jiajun Deng, Xurong Xie, Tianzi Wang, Mingyu Cui,Boyang Xue, Zengrui Jin, Mengzhe Geng, GuinanLi, Xunying Liu, and Helen Meng. 2022. Confidencescore based conformer speaker adaptation for speechrecognition. arXiv preprint arXiv:2206.12045. Taesik Gong, Jongheon Jeong, Taewon Kim, YewonKim, Jinwoo Shin, and Sung-Ju Lee. 2022. Note:Robust continual test-time adaptation against tem-poral correlation. Advances in Neural InformationProcessing Systems, 35:2725327266. Anmol Gulati, James Qin, Chung-Cheng Chiu, NikiParmar, Yu Zhang, Jiahui Yu, Wei Han, ShiboWang, Zhengdong Zhang, Yonghui Wu, et al.2020. Conformer: Convolution-augmented trans-former for speech recognition.arXiv preprintarXiv:2005.08100. Jens Kofod Hansen and IDMT Fraunhofer. 2012.Recognition of phonemes in a-cappella recordingsusing temporal patterns and mel frequency cepstralcoefficients. In 9th Sound and Music ComputingConference (SMC), pages 494499.",
  "Wenxin Hou, Jindong Wang, Xu Tan, Tao Qin, andTakahiro Shinozaki. 2021.Cross-domain speechrecognition with unsupervised character-level distri-bution matching. arXiv preprint arXiv:2104.07491": "Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,Kushal Lakhotia, Ruslan Salakhutdinov, and Abdel-rahman Mohamed. 2021. Hubert: Self-supervisedspeech representation learning by masked predictionof hidden units. IEEE/ACM Transactions on Audio,Speech, and Language Processing, 29:34513460. Wei-Ning Hsu, Yu Zhang, and James Glass. 2017. Unsu-pervised domain adaptation for robust speech recog-nition via variational autoencoder-based data aug-mentation. In 2017 IEEE automatic speech recog-nition and understanding workshop (ASRU), pages1623. IEEE. Hengguan Huang, Xiangming Gu, Hao Wang, ChangXiao, Hongfu Liu, and Ye Wang. 2022. Extrapola-tive continuous-time bayesian neural network forfast training-free test-time adaptation. Advances inNeural Information Processing Systems, 35:3600036013.",
  "Hengguan Huang, Hao Wang, and Brian Mak. 2019.Recurrent poisson process unit for speech recognition.In Proceedings of the AAAI Conference on ArtificialIntelligence, volume 33, pages 65386545": "Sergey Ioffe and Christian Szegedy. 2015. Batch nor-malization: Accelerating deep network training by re-ducing internal covariate shift. In International con-ference on machine learning, pages 448456. pmlr. Changhun Kim, Joonhyung Park, Hajin Shim, andEunho Yang. 2023.Sgem: Test-time adaptationfor automatic speech recognition via sequential-levelgeneralized entropy minimization. arXiv preprintarXiv:2306.01981. PangWeiKoh,ShioriSagawa,HenrikMark-lund, Sang Michael Xie, Marvin Zhang, AkshayBalsubramani, Weihua Hu, Michihiro Yasunaga,Richard Lanas Phillips, Irena Gao, et al. 2021. Wilds:A benchmark of in-the-wild distribution shifts. In In-ternational Conference on Machine Learning, pages56375664. PMLR. Ludwig Krzinger, Dominik Winkelbauer, Lujun Li,Tobias Watzel, and Gerhard Rigoll. 2020.Ctc-segmentation of large corpora for German end-to-endspeech recognition. In International Conference onSpeech and Computer, pages 267278. Springer.",
  "Lawrence R Rabiner, Ronald W Schafer, et al. 2007. In-troduction to digital speech processing. Foundationsand Trends in Signal Processing, 1(12):1194": "Alec Radford, Jong Wook Kim, Tao Xu, Greg Brock-man, Christine McLeavey, and Ilya Sutskever. 2023.Robust speech recognition via large-scale weak su-pervision. In International Conference on MachineLearning, pages 2849228518. PMLR. Chandan KA Reddy, Ebrahim Beyrami, Jamie Pool,Ross Cutler, Sriram Srinivasan, and Johannes Gehrke.2019. A scalable noisy speech dataset and onlinesubjective test framework. Proc. Interspeech 2019,pages 18161820. Sining Sun, Ching-Feng Yeh, Mei-Yuh Hwang, MariOstendorf, and Lei Xie. 2018. Domain adversarialtraining for accented speech recognition. In 2018IEEE international conference on acoustics, speechand signal processing (ICASSP), pages 48544858.IEEE.",
  "Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai.2022. Continual test-time domain adaptation. In Pro-ceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 72017211": "Wei Wei, Hengguan Huang, Xiangming Gu, Hao Wang,and Ye Wang. 2022. Unsupervised mismatch local-ization in cross-modal sequential data with applica-tion to mispronunciations localization. Transactionson Machine Learning Research. Chao-Han Huck Yang, Bo Li, Yu Zhang, Nanxin Chen,Rohit Prabhavalkar, Tara N Sainath, and TrevorStrohman. 2023a. From english to more languages:Parameter-efficient model reprogramming for cross-lingual speech recognition. In ICASSP 2023-2023IEEE International Conference on Acoustics, Speechand Signal Processing (ICASSP), pages 15. IEEE. Chao-Han Huck Yang, Yun-Yun Tsai, and Pin-Yu Chen.2021. Voice2series: Reprogramming acoustic mod-els for time series classification. In Internationalconference on machine learning, pages 1180811819.PMLR.",
  "Elad Ben Zaken, Shauli Ravfogel, and Yoav Gold-berg. 2021.Bitfit:Simple parameter-efficientfine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199": "Guanlong Zhao, Sinem Sonsaat, Alif Silpachai, IvanaLucic, Evgeny Chukharev-Hudilainen, John Levis,and Ricardo Gutierrez-Osuna. 2018. L2-arctic: Anon-native english speech corpus. In Interspeech,pages 27832787. Zhi Zhou, Lan-Zhe Guo, Lin-Han Jia, Dingchu Zhang,and Yu-Feng Li. 2023. ODS: Test-time adaptation inthe presence of open-world data shift. In Proceedingsof the 40th International Conference on MachineLearning, volume 202 of Proceedings of MachineLearning Research, pages 4257442588. PMLR.",
  "A.1Dataset Details": "We show the statistics of datasets used in our workin where # Utt. indicates the total numberof utterances. We build our synthetic datasets onLibriSpeech test-other set. For LS-C, we add theGaussian noises when preparing the data loaderand use the amplitudes {0.005, 0.01, 0.015, 0.02,0.03} as level 1-5 severity. For LS-P, we use theAirConditioner_6, Typing_2, Babble_4, Munch-ing_3, ShuttingDoor_6, VacuumCleaner_1, Airpor-tAnnouncements_2, CopyMachine_2 wave filesfrom MS-SNSD 2 as the environmental sounds andsynthesize audios with signal-to-noise ratios {10,5, 0, -5, -10} seperately. For L2-Arctic, we usethe default splits of 24 non-native speakers witha balanced gender and L1 distribution. For musicdatasets, we use the default DSing dev and test setsand the full Hansen set (no split).",
  "In our experimental evaluations, we mainly em-ploy the acoustic foundation model, Wav2vec2": "Specifically, we utilize its Connectionist TemporalClassification (CTC) variants with different modelsizes, Wav2vec2 Base and Wav2vec2 Large. Weinvolve the usage of publicly available Wav2vev2Base 3 and Wav2vec2 Large 4 models fine-tunedon speech recognition tasks. The detailed struc-ture of the CTC model is a single fully-connectedlayer and softmax on top of the foundation model.Given that CTC-based models do not explicitlymodel silences, we take those with the pseudo la-bel <BLANK> as silent frames and the rest as non-silent frames as per (Krzinger et al., 2020; Weiet al., 2022; Yang et al., 2023c). We are inter-ested in those frames carrying important semanticinformation so we take the blank indicator as anapproximation. The advantage is to directly utilizethe test-time inference output without additionalcomputation such as a VAD module. Moreover,we found taking the blank symbol as an indicatorhas already achieved good performance in existingwork (Yoshimura et al., 2020) which serves as agood support. We mainly conduct experiments onthese two models despite the applicability of ourmethod to other transformer-based architectures ofacoustic foundation models. To make a fair compar-ison with methods employing beam search, we uti-lize the same 4-gram language model 5 as SGEM.Since our test-time setting requires no access to thetarget text, we use the language model trained onthe speech dataset despite the text-domain shift.For the Conformer and Transducer, we employConformer-CTC 6 and Conformer-Transducer 7.All speech inputs are sampled or resampled at16Khz.We use Pytorch and Huggingface Transformersin our implementation. All experiments are run ona single NVIDIA A5000 GPU (24G). We evaluatethe performance of all baselines after adaptation forten steps. We use the AdamW optimizer as defaultfor all experiments. The weight of consistencyregularization is set to be 0.3. We consider thelearning rate in {2e-4, 5e-4, 8e-4} for tuning affineparameters of layer normalization and consider thelearning rate in {2e-5, 5e-5} for tuning feature ex- tractor. Since the TTA setting has no validationset, we follow SUTA and use the hyperparametersobtained from Librispeech test-other set with noiselevel = 0.01 as the default for the experiments.For singing data experiments, we use the hyperpa-rameters obtained from DSing-dev as the defaultfor experiments on DSing-test and Hansen.",
  "B.1More Ablation Study": "Strategies for Frame Selection We proceed toanalyze strategies utilized for the selection ofspeech frames optimized within the CEA frame-work.We investigate three pseudo-label-basedstrategies, namely a) selection of non-silent frames(as used in our method), b) selection of silentframes, and c) selection of all frames. The resultsare detailed in . The empirical findings re-veal that the optimization of silent frames or allframes within CEA yields inferior performancecompared to the optimization of non-silent frames.Moreover, it is observed that the degradation is notso substantial, as optimizing silent or all framesmay also contribute to enhancing the reliability ofnoisy frames.",
  ": Ablation study of strategies for frame selection.WER (%) results are reported": "Efficacy of STCR on SUTA To further validatethe efficacy of short-term consistency regulariza-tion, we did one more ablation study using SUTA+ STCR on the DSing-test set, and observed thatthe proposed SCTR can enhance SUTA with WERdecreasing from 51.3 to 50.9. However, the per-formance of SUTA + STCR still lags behind ourmethod CEA + STCR with WER 50.1, whichdemonstrates that our proposed CEA also con-tributes to the final improvement.",
  "B.4Results on More Acoustic FoundationModels": "In an extension of the main experiments, we delvedinto the adaptation performance across diverseacoustic foundation models. Specifically, our addi-tional experiments utilize various models including,Hubert-Base 8, Hubert-Large 9, WavLM-Base 10,and WavLM-Large 11 from Huggingface. Theseexperiments are conducted to assess the adaptationperformance ain relation to different model sizes,and training data sources. The outcomes on theLS-C and DSing-test datasets are reported in Ta-ble 12 and respectively. We employ theword error rate reduction (WERR) to measure therelative improvement brought by our adaptationmethod. We summarize the findings as follows:",
  ": WER (%) results on DSing-test using both base and large models of Wav2vec2, Hubert, WavLM withgreedy decoding. WERR stands for word error rate reduction": "Model Sizes. A comparative analysis is con-ducted between the base and large versions of eachmodel. The findings reveal that large models con-sistently surpass base models. Furthermore, ourproposed approach uniformly improves both baseand large models. A notable observation is that ourmethod elicits a greater average improvement inbase models compared to large models within theLS-C dataset. This trend is particularly pronouncedunder lower noise levels ranging from 1 to 3. Incontrast, within the DSing-test set, the enhance-ment for large models is more significant than forbase models. The phenomenon may be attributed tothe fact that large models already exhibit commend-able performance under minor corruptions, evenwithout adaptation, thus providing limited scopefor further improvement. However, in scenariosinvolving significant shifts, the expansive parame-terization of large models facilitates more effectiveadaptation, whereas base models face challenges.Training Data Sources. A comparative eval-uation of models trained with different datasets,including Wav2vec2-Large trained with 960h Lib-riSpeech set, Hubert-Large trained with 960h Lib-riSpeech set, and WavLM-Large trained with 100hLibriSpeech clean set, indicates that the larger-sizedata set establish a stronger foundation for test-timeadaptation. A similar inference can be drawn whencomparing Wav2vec2-Base trained with 960h Lib-riSpeech set, Hubert-Base trained with 100h Lib-riSpeech clean set, and WavLM-Base trained with100h LibriSpeech clean set.In summary, our proposed unsupervised TTAmethod demonstrates a considerable benefit acrossdiverse acoustic foundation models, reflecting sub-stantial improvements for different model sizes andtraining data sources.",
  "B.5Analysis on Large Vocabulary Size": "Our proposed method can be generalizable to mod-els with large vocabulary sizes. Theoretically, themaximum entropy for non-silent frames is expectedto increase due to the larger number of classes.Practically, this might also depend on the test inputand models. To analyze the entropy distribution fornon-silent and silent frames, we conduct an addi-tional experiment using the Conformer-CTC modelwith BPE tokenization, which has a larger vocab-ulary size than the one of the Wav2vec2 model.We observed an increase in entropy for non-silentframes from 59.4% to 70.0%, as illustrated in .",
  "Full51.289.7M31.9307M": ": Results with different parameterizations onDSing-test using Wav2vec2 Base and Large models. Weconsider (1) Bias-Only: all bias terms, (2) LNs: all scaleand shift terms of Layer Normalization, 3) FE+LNs:parameters of the feature extractor and all scale andshift terms of Layer Normalization, and (4) Full: allparameters. Word Error Rate (%) and the number ofparameters (Params) are reported.",
  "B.6Connection with Existing Frozen ModelAdaptation": "Our TTA-based method also exhibits parameterefficiency. It is essential to emphasize that ourapproach does not introduce additional layers ofnormalization. Instead, we adapt the affine param-eters (the scale and the shift ) of the existinglayer normalization from the pre-training phase,which means no new trainable parameters are intro-duced. It is noteworthy to highlight the differencebetween our method and existing frozen modeladaptation methods, such as P-tuning, LoRA, andAdapter. Unlike these techniques, our method con-ducts source-free unsupervised adaptation using asingle utterance. Furthermore, our primary objec-tive of adaptation is to address open-world acousticdata shifts, rather than task adaptation.",
  "B.7Results on Different Parameterizations": "In order to further evaluate the effectiveness of ourproposed method across diverse parameterizations,we conduct additional experiments on the DSing-test set using Wav2vec2 Base and Large models.Specifically, we explore four distinct parameteri- zation schemes and compute their correspondingnumber of parameters: (1) Bias-Only refers to fine-tuning only bias terms as per Zaken et al. (2021).(2) LNs encompasses the adjustment of all scaleand shift terms associated with layer normalization.(3) FE+LNs involves the parameters of the featureextractor in addition to all scale and shift terms oflayer normalization. (4) Full entails the fine-tuningof all parameters within the model. It is importantto note that all other experimental settings exceptfor parameterization have remained consistent. Theexperimental results are presented in . Ourfindings reveal that our method exhibits compat-ibility with different parameterizations, yieldingcomparable performances. Among these parame-terizations, LNs demonstrate the smallest numberof parameters adjusted, thereby illustrating the pa-rameter efficiency of our method."
}