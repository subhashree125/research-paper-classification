{
  "Abstract": "Prompt optimization aims to find the bestprompt to a large language model (LLM) fora given task. LLMs have been successfullyused to help find and improve prompt candi-dates for single-step tasks. However, realis-tic tasks for agents are multi-step and intro-duce new challenges: (1) Prompt content islikely to be more extensive and complex, mak-ing it more difficult for LLMs to analyze er-rors, (2) the impact of an individual step isdifficult to evaluate, and (3) different peoplemay have varied preferences about task exe-cution. While humans struggle to optimizeprompts, they are good at providing feedbackabout LLM outputs; we therefore introduce anew LLM-driven discrete prompt optimizationframework PRompt Optimization in Multi-StepTasks (PROMST) that incorporates human-designed feedback rules to automatically offerdirect suggestions for improvement. We alsouse an extra learned heuristic model that pre-dicts prompt performance to efficiently samplefrom prompt candidates. This approach sig-nificantly outperforms both human-engineeredprompts and several other prompt optimizationmethods across 11 representative multi-steptasks (an average 10.6%-29.3% improvementto current best methods on five LLMs respec-tively). We believe our work can serve as abenchmark for automatic prompt optimizationfor LLM-driven multi-step tasks. Datasets andCodes are available at Project Pageisavailableat",
  "Introduction": "The performance of large language models (LLMs)on a given task is sensitive to the prompt, so promptengineering aims to create prompts that fully lever-age the capabilities of LLMs. Due to the lack ofaccess to model parameters for black-box LLMs, techniques for automatic prompt optimization haveprimarily focused on searching over the vast dis-crete space of tokenized language inputs (Chenget al., 2023). Recent studies have shown that LLMs,combined with evolutionary algorithms, can helpwith this search by reasoning over errors made us-ing existing prompts to suggest edits or generatenew candidate prompts (Pryzant et al., 2023; Wanget al., 2023; Yang et al., 2023). These approacheshave been evaluated on relatively simple one-steptasks, such as mathematical calculations (Roy andRoth, 2016; Cobbe et al., 2021), instruction in-duction (Honovich et al., 2022), and factual analy-sis (Wu et al., 2023b). The associated prompts arealso relatively short, usually one to three sentences.In this work, we aim to optimize prompts forLLM-driven agents solving multi-step tasks andpropose a method called PRompt Optimization inMulti-Step Tasks (PROMST). In these tasks, anLLM is used to decide a systems actions (e.g., vir-tual software Wu et al. 2023a; Zhou et al. 2023a orreal robots Chen et al. 2023a; Firoozi et al. 2023)as it interacts with an environment over multiplesteps (Abdulhai et al., 2023). Engineering goodprompts is hard due to the typical prompt length(300+ tokens) and individual task constraints andrules. The prompts needed for multi-step tasksare more complex to judge the long-horizon cor-rectness of a single action. This difficulty hindersLLMs from automatically reasoning over errorsand producing better prompts, which in turn re-duces the effectiveness of current methods for auto-mated prompt optimization. Prompt optimizationin multi-step tasks is still an open challenge.Considering that humans excel in analyzing er-rors and incorporating relevant domain knowledgeinto feedback, we formalize PROMST as a frame-work involving human input, as shown in .Here, during the multi-step agent-environment in-teractions, the agent (indicated by TaskLLM in) sometimes makes errors and fails the task. : The PROMST framework. Given an initial human-designed prompt and the state of theenvironment for the current task, the TaskLLM iteratively generates an action and executes it until eitheran error occurs or the task is complete. Human-designed feedback rules automatically generate feedbackabout errors that is then provided as context to the PromptLLM when generating new prompt candidates.The task performance is scored according to a human-designed score function; this score can be used withthe prompt to train a score prediction model online. Given new prompt candidates, this score predictionmodel is used to select a subset of candidates to evaluate for the next generation. While some work has used LLMs to evaluate errors,we instead use human-designed feedback rules con-structed a priori that address different types of er-rors. Depending on the error, feedback is automati-cally generated and passed as additional context toan LLM that is responsible for producing a new setof candidate prompts (indicated by PromptLLMin ). A score is assigned to each promptindicating the agents task performance given thatprompt. Since the evaluation of many candidateprompts for multi-step tasks in environments canbe expensive, we fine-tune a score prediction modelonline using prompt-score pairs which can be usedas a heuristic to select a subset of the candidateprompts to evaluate. Our experiments in 11 tasks show that the in-tegration of human feedback and the score modelgreatly improves the prompt optimization process(10.6%-29.3% relative improvements over all base-line methods across different LLMs). PROMSTachieves the best performance on most tasks.PROMST has also been shown to perform bet-ter in multi-trial settings when combined with dy-namic approaches. We further show that the human-designed evaluation rules can be used to help aligntask performance with human preferences. Ex-tensive experiments are conducted to validate theframework and investigate the underlying reasonswhy some prompts are more effective than others.",
  "In summary, our contributions are : (1) To our": "best knowledge, PROMST is the first to exploreautomatic prompt optimization in multi-step agenttasks. We release all codes and prompts for 11multi-step environments, which may serve as abenchmark for future research. (2) We show thatthe integration of human feedback and a fine-tunedscore model outperforms existing methods acrossvarious tasks and LLMs. (3) Our research indicatesthat PROMST is orthogonal and integrates wellwith established dynamic approaches. (4) We findthat human-designed rules for task evaluation helpalign optimized prompts with human preferences.",
  "Related Work": "Prompt OptimizationTo improve performanceof black-box API models, it is useful to engineerthe discrete prompts for downstream tasks. Variousbest practices have emerged for human-designedtask prompts, such as including examples (Brownet al., 2020) or promoting reasoning chains (Ko-jima et al., 2022; Wei et al., 2022).However,manually designing prompts requires extensive hu-man trial-and-error and is sub-optimal; thus, manyrecent works focus on automating this process.Some methods approximate the gradients (Diaoet al., 2022) or emulate them via natural language(Pryzant et al., 2023). Others use edit operatorsto modify an initial prompt, driven either by rein-forcement learning (Zhang et al., 2023) or score-guided search (Prasad et al., 2023). To help bal- ance exploration and exploitation of prompts, sev-eral approaches have used LLM-driven evolution(Guo et al., 2023a; Fernando et al., 2023; Ma et al.,2023; Ye et al., 2023). In several works, LLMsare directly used to generate prompt candidates(Zhou et al., 2023b) often with feedback about par-ent prompts (Wang et al., 2023; Ma et al., 2023).Our work focuses on domains that include complexmulti-step tasks, in which the evaluation and reflec-tion processes are more challenging so that scoreprediction models and human feedback rules areintroduced in order to mitigate this problem.LLM Based Agents for Multi-Step TasksThere are many recent works that use LLMs formulti-step planning. LLMs are used to interactwith softwares and websites (Ma et al., 2024; Wuet al., 2023a; Zhou et al., 2023a), plan robot ac-tions (Chen et al., 2023b; Ahn et al., 2022; Huanget al., 2022a; Ma et al., 2023; Aghzal et al., 2023;Chen et al., 2023c), and connect to external tools(Liu et al., 2023; Chen et al., 2023a; Qin et al.,2023). Instead of careful design of lengthy promptsto capture all the constraints, our approach usesprompt optimization to transition from a simple ini-tial human-provided prompt to a high-performingprompt.LLM Self-reflection from FeedbackIn plan-ning domains, it is useful to provide feedback aboutsyntactic errors (Silver et al., 2023; Skreta et al.,2023), potential infinite loops (Silver et al., 2023),failed action execution (Huang et al., 2022b), andgenerated trajectories (Chen et al., 2023a). Otherrecent work has shown that LLM-generated feed-back via self-evaluation can improve performanceon a variety of tasks (Yang et al., 2022; Wellecket al., 2022; Madaan et al., 2023), including promptengineering (Wang et al., 2023) and reinforcementlearning (Shinn et al., 2023; Ma et al., 2023). Com-pared to above works, our work combines LLMself-reflection with human-provided feedback tem-plates to help improve performance on the morechallenging multi-step tasks.Another type of methods is to utilize self-reflection for online dynamic feedback during taskexecution, such as Reflexion (Shinn et al., 2024).While Reflexion needs multiple trials for onlineoptimization of action memory for each singletest, our method PROMST only needs one trialfor task execution and the prompt is optimized of-fline across multiple tests. In .4, we findthat PROMST outperforms the offline variation ofReflexion and can perform better when combining",
  "Problem Formulation": "Given a base LLM B and a target task T, the goalof prompt optimization is to craft an optimizednatural language prompt P that maximizes the per-formance of B on T. Here the prompt P consistsof multiple components, such as a task description,scoring rules, and safety constraints. In multi-steptasks, the state information of the environment ateach step will be transformed into a text string Sand provided to the LLM B to make decisions. Thehistory of state (S), action (a), and environmentfeedback (e) will also be reported to LLM. For theith testing trial on a particular task, the probabilityof an action sequence [ai,1, ai,2, ..., ai,j] is:",
  "PROMST Framework": "illustrates the general framework ofPROMST. The goal is to more efficiently andstrategically search over the vast space of possibleprompts while integrating human-designed feed-back of candidate prompt performance. LLMsare used in two key steps of PROMST: (1) theexecution of the task via the current candidateprompt (TaskLLM) and (2) the generation of newcandidate prompts given any available feedbackabout the current prompts performance on the task(PromptLLM). We refer to a single execution ina testing case of a task as a trial. In each trial, theTaskLLM executes the task over multiple rounds of interaction with the environment; for each round,the TaskLLM is provided both the current candi-date prompt and the current trials execution historyand generates the next action for the agent to take.Task execution terminates when an error is detectedor the task is complete. The candidate prompt Pis assigned a score for that trial via the human-designed score function. Each candidate prompt isevaluated over multiple trials in which the initial en-vironment state (e.g. number of objects, number ofagents) is varied, resulting in a final average scorecalculated over all the trials. Once the candidateprompts have all been evaluated and assigned auto-matic feedback, the top performers are selected asparents for a new generation of candidate prompts.The PromptLLM uses each parent prompt and itsfeedback to generate new candidate prompts. Thisprocess is also described in Algorithm 1, 2, 3 inAppendix B.Score Prediction ModelIn general, produc-ing more candidate prompts per generation allowsfor more exploration over the space of possibleprompts; however, there is a trade-off between thenumber of candidates per generation and the cost ofevaluation, and multi-step tasks can be much moreexpensive to evaluate (we query the TaskLLM foreach next action). To help mitigate the evalua-tion cost for a generation, we learn a score predic-tion model online that functions as a heuristic withwhich to choose a subset of the generated candidateprompts for actual evaluation.Algorithm 2 in Appendix B shows the processof implementing the score prediction model as aheuristic for filtering candidate prompts. We fine-tune a task-specific bidirectional Longformer-base(148M) (Beltagy et al., 2020) model. The prompt-score pairs on which we fine-tune are collectedonline during early iterations of PROMST; there-fore, the score prediction model is not applied untilsdth generation, where sd is a hyperparameter. Wecontinue to update the learned model at each gen-eration with the new prompt-score pairs. To miti-gate variance, we fine-tune multiple models on fiverounds with the collected data following a random4:1 train/test split. The generated prompt candidatep will be selected for task evaluation if:",
  "where E[Mk(p)] and Var[Mk(p) are the mean andvariance of predicted scores for p from five models": "The E[errork] is the average testing error of fivescore models. The max(D.score()) is the highestscore of existing prompts. To balance efficient ex-ploration and conservative filtering, we only filterprompt candidates when the score prediction modelis sufficiently confident. When the variance of theprediction model is high, we want to be conserva-tive in its application in order to reduce the chancethat we filter out a good candidate. Similarly, wechoose to be conservative in our filtering when theerror is high. Equation 3 is therefore one formu-lation that incorporates these general ideas. Thehyperparameter hyper_M allows this conservative-ness to be tuned by users.Human-Designed Feedback RulesDuringtask execution, the TaskLLM may encounter anerror, resulting in the task being terminated. It isuseful for the PromptLLM to have context aboutthis error when generating new prompt candidates.Since automatic error analysis via LLMs is diffi-cult for multi-step tasks (e.g. an agent stuck in anaction loop), we instead use human-designed rulesto automatically synthesize feedback, as shown in. Some types of errors can be commonacross all tasks (e.g. syntactic errors), while othersare task-specific. For types of human feedback ineach task, see Appendix A.In .4, we do ablation experiments toshow that variability over the wording of feed-back has little impact on PROMST performance.Hence, PROMST does not require humans to iter-ate over possible versions of feedback templatesvia trial-and-error efforts. Additionally, for eachtask, humans just need to design 2 to 4 new errortypes since some of the error types are commonacross tasks. Even for the task of multi-hop ques-tion answering, error types like syntactic errorsand stuck in the loop still exist and can be sharedwith other tasks.Candidate Prompt GeneratorWe producenew candidate prompts from a parent prompt intwo steps: (1) summarizing feedback via an LLM(SumLLM) and (2) generating new prompt candi-dates via an LLM provided with the summarizedfeedback as context (GenLLM). In order to en-courage exploration over more diverse candidateprompts, we randomly choose 10 instances of feed-back. This random selection also likely promotesmore frequent errors. Given the selected feedback,SumLLM produces a summary that is included ascontext to GenLLM for generating new candidates.See Algorithm 3 in Appendix B for another de-",
  "Environments": "As shown in , we test on 11 multi-steptasks requiring strong logical, geometrical, sci-entific, and commonsense reasoning capabilities(Zhou et al., 2023a; Shridhar et al., 2020; Wanget al., 2022; Chen et al., 2023b; Aghzal et al., 2023;Valmeekam et al., 2023). Each environment re-quires the LLM agent to determine the next actionin the large discrete action space. Please refer toAppendix D for a complete description of all tasks.",
  "Baselines": "We compare PROMST with six recent representa-tive methods: Automatic Prompt Engineer (APE)(Zhou et al., 2023b), Automatic Prompt Optimiza-tion (APO) (Pryzant et al., 2023), PromptAgent(Wang et al., 2023), LLM-As-Optimizer (Yanget al., 2023), PromptBreeder (Fernando et al.,2023), and Evolutionary Prompt Optimizer (Guoet al., 2023b). All the methods under comparisoninvolve iterative optimization of prompts. Somemethods require error feedback through LLM self-reflection, while others do not.For methodsthat need error feedback, we randomly select 10instances of feedback, similar to the PROMSTmethod but without the rules of human feedback.",
  "Experimental Setups": "For a fair comparison, all methods start the op-timization from initial human-designed prompts;where possible, we use the provided publicly avail-able prompts for each method. In all cases, weset the LLM sampling temperature to 0. For eachmethod, we report the score of the best perform-ing prompt on each task; in this case, the score iscomputed as: S = num(sub-goalsuccess)/num(sub-goalall),(4)where the score S is the ratio of the numberof successfully completed sub-goals/sub-steps tothe total number of sub-goals/sub-steps, i.e., thetask progress score. Note that the task completionscore can also serve as the metric, while it may besparse in some situations. Both types of scores arepositively correlated as shown in Appendix .In .4, we also preliminarily test the impactof changing the scoring function S.Model Types One interesting feature of thesemethods is that the LLM used to execute the task(TaskLLM) and the LLM used to generate newcandidate prompts (PromptLLM) do not need tobe the same model. We mainly test two combi-nations of models. The first uses GPT-3.5 (gpt-3.5-turbo-16k-0613) as the TaskLLM and GPT-4 (gpt-4-turbo-preview) (Achiam et al., 2023) as the PromptLLM, and the second uses GPT-4 forboth the TaskLLM and PromptLLM. To verify theeffectiveness of PROMST in varied models, wealso evaluate it using Claude 3 Opus (Anthropic,2024), Mixtral-8x7B, and Mixtral-Large (Jianget al., 2024) as both TaskLLM and PromptLLM.Mixtral-8x7B is an open model, while all the othersare closed. We also explore whether the optimizedprompts specialized for one type of LLM can gen-eralize better performance to other types of LLMs.Context Window Limit The constraint of themodels context window is an issue for LLM-basedagents, especially for longer multi-step tasks. Rely-ing on the intuition that recency is important, in allthe tested methods we use a sliding window of thehistory of state-action-feedback tuples, truncatingthe history by pruning older parts of the historythat extend beyond the window length, which is acommon technique used in LLM agent researches.Hyperparameters For a fair comparison, westandardize all hyperparameters across methods, al-lowing each to explore the same number of promptcandidates at an equivalent level. The expansionnumber n controls the number of kid prompts gen-erated based on each parent prompt, which is setto 20 in the first level and 8 for all additional lev-els. In each level, top k = 5 current prompts areselected as the parent prompts for further optimiza-tion. Search terminates once the recent three levelsdo not have any score improvements. In PROMST,we set hyper_M = 0.8 in Equation 3 to filter outprompts with low scores. We set sd = 4 so that thescore model is not applied until 4th generation.",
  "Results and Analysis": "Overall Better Performance and Ta-ble 2 show the main experimental results. Notethat BoxLift task is not included in sincewe find GPT-4 can already achieve a full score withthe initial human prompt. shows the ex-perimental results on other three types of LLMs.Due to limited computational resources, we onlyselect four representative tasks and two strongestbaseline methods (APO, PromptAgent) when eval-uating other LLMs. and in Ap-pendix E test the performance of the optimizedprompts trained from one LLM with other types ofTaskLLMs.The main takeaways are: 1) PROMST performsthe best in most tasks.On average, PROMSToutperforms strongest baseline PromptAgent withGPT-3.5-0613 (0.27 vs 0.32), GPT-4 (0.61 vs 0.69), Claude-3-opus (0.36 vs 0.46), Open-Mixtral-8x7B(0.12 vs 0.15), and Mixtral-large (0.30 vs 0.34). 2)When testing the best prompts trained from GPT-3.5-0613 and GPT-4 with a different TaskLLM, wefind that they still outperform human prompts. 3)However, each LLM does best with the promptsoptimized on it. For example, the best prompts ac-quired when using GPT-3.5-0613 as the TaskLLMdo not further improve performance when appliedto GPT-4, and vice versa. 4) PROMST performswell when the TaskLLM and PromptLLM are thesame LLM, showing that it does not rely on astronger PromptLLM to pass extra knowledge intoprompts, which can be regarded as cheating. : Several results inspecting the learnedscore prediction model. (a) The distribution/ratioof prompt scores with/without the score predictionmodel. (b) The prediction error of the model on thetraining data and heldout test data as the amount oftraining data increases. (c) A plot of the predictedscore vs the actual score for various prompts; blueare the prompts that were chosen as parents for newcandidates. (d) The trend of the best performingprompt during optimization for increasing itera-tions both with and without using the learned scoreprediction model. Effects of Score ModelTo analyze the effectsof the score model, we use BoxLift as a represen-tative example, as shown in . ashows the distribution of prompt scores exploredin all the levels (1-8) with and without the scoreprediction model implemented, respectively. Theimplementation of the score prediction model trulymakes the exploration more efficient since less low-scored prompts are explored. b shows thetraining and testing errors of the score model versusdifferent amounts of collected training data. The : Scores for initial (human) and optimized prompts on various multi-step tasks for different methods.P.Agent, LLMOP, P.Breeder, and P.Evolution refer to PromptAgent, LLM-As-Optimizer, PromptBreeder,Evolutionary Prompt Optimizer, respectively. GPT-3.5-0613 for TaskLLM and GPT-4 for PromptLLM.",
  "AVERAGE0.510.560.590.610.580.540.560.69": ": (a) Comparison of score prediction errorsfor few-shot GPT-4 vs finetuning Longformer forincreasing amount of few-shot examples or trainingdata, respectively. (b) An ablation study of theimpact of the human-designed feedback rules ontask performance for four multi-step tasks. overfitting effect decreases with increasing datanumber. c tests the fine-tuned score mod-els on levels 5-8.We also evaluate the prompts that were filteredout by the score model and plot the predicted andactual scores. We find that nearly all chosen prompt candidates achieve scores higher than 0.4, and thefiltered prompts have reliably low scores. We com-pare the evolution curves for PROMST with/with-out the score model, as shown in d. Theresults show that both the training and testing pathsconverge faster and achieve better scores using thescore model. The ablation experiments in otherenvironments also have the same trend (shown inAppendix G). Overall, we find the score predictionmodel improves the efficiency and effectivenessof prompt search. For each prompt optimizationprocess of PROMST, usually the best prompt ap-pears at the iteration number 5 to 7, as visualizedin d and Appendix G.Ablation on Methods of Score ModelIn-stead of fine-tuning a pre-trained Longformer-basemodel, another way to acquire score predictionmodels is few-shot learning via GPT-4. acompares these two methods under varied train-ing/example data number. GPT-4 is given randomly",
  "ALFWORLD0.300.23BOXNET20.220.18BOXLIFT0.900.85": "selected prompt-score pairs as examples duringthe study. We find that the performance of GPT-4few-shot learning cannot improve with the increas-ing number of examples. The fine-tuning methodsurpasses GPT-4 few-shot learning once the datanumber increases over 40.Ablation on SumLLM ComponentSinceTaskLLM and GenLLM are necessary in the wholeframework, we compare PROMST with/withoutSumLLM component in to verify its effec-tiveness. The integration of SumLLM improves theperformance on all the three representative tasks.Ablation on Human FeedbackWe comparethe method with/without human feedback, bothwithout the learned score model. As seen in Fig-ure 5b, human feedback contributes to much higherscores across four tasks. In our work, the originalhuman feedback templates did not require itera-tions via trial-and-error over possible versions. Todemonstrate that designing human feedback rulesis straightforward and requires minimal efforts, wetest other four feedback templates in . Theresults show that variability over the wording of thetemplates has little impact on the performance ofPROMST. Thus, including the response in the feed- back template is a useful task- and error-agnosticguiding principle. Appendix H articulates morespecifically on the designing of four compared hu-man feedback templates and the reasons why de-signing feedback rules is effortless.Preference Alignment via Score FunctionThe choice of score functions impacts prompt op-timization, in which humans may have differentpreferences for the same task. In Appendix J, weexplore the impacts of varied score functions andfind that PROMST can well align with human pref-erences by modifying score function formats.Explanability for Better PromptsWe alsotry to dig out some mechanisms why the optimizedprompts are better. In , we plot promptscore vs. token length and perplexity, which im-plies some clues that longer prompts may be better.Meanwhile, when viewing through the discoveredbest prompts in Appendix M, we find some cluesabout better component emergence, i.e., the bestprompts tend to list all the careful points one byone clearly. We conduct an ablation study by sum-marizing detailed careful points into varying tokenlengths using GPT-4 and evaluating their perfor-mance. The results indicate that task scores consis-tently decline as token lengths decrease, underscor-ing the importance of clearly listing detailed points.More specific discussion is shown in Appendix K.Comparison and Combination with ReflexionIn Appendix L we find that PROMST outperformsthe dynamic approach, Reflexion, in prompt opti-mization and achieves enhanced performance whenReflexion is integrated in a multi-trial setting.",
  "BOXNET20.220.200.230.170.18BOXLIFT0.900.930.970.730.87": "and human preference alignment, we propose theintegration of human feedback, a learned score pre-diction model, and the modification of task scorefunctions. Our approach generally outperforms sixrepresentative baselines on 11 different task envi-ronments over all the five LLMs. PROMST is or-thogonal and combinatorial to existing dynamic ap-proaches. The discovered best prompts have someinspiring characteristics for better performance.",
  "The limitations and potential societal risks of thiswork are as follows:": "Huge resource consumption of API callsAutomatic prompt optimization requires significantcomputing resources and LLM API queries due toits search-based nature, which is a common issuein this research track. Though the introduction ofscore model makes the searching more efficient,the around 100 prompt candidate exploration isstill a large burden. Score model increases computing demandsof local devicesThe fine-tuned score predictionmodel trades-off the number of API queries foron-device computation by selecting good candidateprompts. Still, the training of extra score modelsincreases the computing demands on local devices. Extra burden of designing human feedbackrulesIntroducing human feedback into promptoptimization is a natural way because the currentLLM cannot well summarize and reflect errorsin multi-step tasks, as shown in ablation studies.Integrating human feedback is inspired by thephenomenon that humans are good at providingfeedback about LLM errors but struggle tooptimize prompts. We admit that requiring humanpre-defined feedback will somewhat increase theburden of users and that introducing automaticfeedback is one important work to be explored inthe future. Fine-tuning score model requires enoughdata pointsThe fine-tuning process of scoremodels typically requires around 100 prompt-scorepairs, which is suitable for black box promptsearching since over 100 data points are trulyneeded for satisfying performance. However, thescore model may not be suitable if in the future amore efficient searching method appears so thatdata points are not such much.",
  "This work was supported by ONR under AwardN00014-22-1-2478 and MIT-IBM Watson AI Lab.However, this article solely reflects the opinionsand conclusions of its authors": "Marwa Abdulhai, Isadora White, Charlie Snell, CharlesSun, Joey Hong, Yuexiang Zhai, Kelvin Xu, andSergey Levine. 2023.Lmrl gym: Benchmarksfor multi-turn reinforcement learning with languagemodels. arXiv preprint arXiv:2311.18232. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774.",
  "Yongchao Chen, Rujul Gandhi, Yang Zhang, andChuchu Fan. 2023c. Nl2tl: Transforming naturallanguages to temporal logics using large languagemodels. arXiv preprint arXiv:2305.07766": "Jiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, HongningWang, Yuxiao Dong, Jie Tang, and Minlie Huang.2023.Black-box prompt optimization: Aligninglarge language models without model training. arXivpreprint arXiv:2311.04155. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, ReiichiroNakano, et al. 2021.Training verifiers to solvemath word problems, 2021.URL",
  "ChrisanthaFernando,DylanBanarse,HenrykMichalewski, Simon Osindero, and Tim Rock-tschel. 2023.Promptbreeder:Self-referentialself-improvement via prompt evolution.arXivpreprint arXiv:2309.16797": "Roya Firoozi,Johnathan Tucker,Stephen Tian,Anirudha Majumdar, Jiankai Sun, Weiyu Liu, YukeZhu, Shuran Song, Ashish Kapoor, Karol Hausman,et al. 2023. Foundation models in robotics: Appli-cations, challenges, and the future. arXiv preprintarXiv:2312.07843. Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, KaitaoSong, Xu Tan, Guoqing Liu, Jiang Bian, and YujiuYang. 2023a.Connecting large language modelswith evolutionary algorithms yields powerful promptoptimizers. arXiv preprint arXiv:2309.08532. Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, KaitaoSong, Xu Tan, Guoqing Liu, Jiang Bian, and YujiuYang. 2023b. Connecting large language modelswith evolutionary algorithms yields powerful promptoptimizers. Preprint, arXiv:2309.08532.",
  "Or Honovich, Uri Shaham, Samuel R Bowman, andOmer Levy. 2022. Instruction induction: From fewexamples to natural language task descriptions. arXivpreprint arXiv:2205.10782": "Wenlong Huang, Pieter Abbeel, Deepak Pathak, andIgor Mordatch. 2022a. Language models as zero-shot planners: Extracting actionable knowledge forembodied agents. In International Conference onMachine Learning, pages 91189147. PMLR. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan,Jacky Liang, Pete Florence, Andy Zeng, JonathanTompson, Igor Mordatch, Yevgen Chebotar, et al.2022b.Inner monologue: Embodied reasoningthrough planning with language models.arXivpreprint arXiv:2207.05608. Albert Q Jiang, Alexandre Sablayrolles, AntoineRoux, Arthur Mensch, Blanche Savary, Chris Bam-ford, Devendra Singh Chaplot, Diego de las Casas,Emma Bou Hanna, Florian Bressand, et al. 2024.Mixtral of experts. arXiv preprint arXiv:2401.04088. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-guage models are zero-shot reasoners. Advances inneural information processing systems, 35:2219922213. Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu,Shiqi Zhang, Joydeep Biswas, and Peter Stone.2023. Llm+ p: Empowering large language mod-els with optimal planning proficiency. arXiv preprintarXiv:2304.11477. Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang,Yujiu Yang, Yaohui Jin, Zhenzhong Lan, LingpengKong, and Junxian He. 2024. Agentboard: An analyt-ical evaluation board of multi-turn llm agents. arXivpreprint arXiv:2401.13178. Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, YukeZhu, Linxi Fan, and Anima Anandkumar. 2023. Eu-reka: Human-level reward design via coding largelanguage models. arXiv preprint arXiv:2310.12931. Aman Madaan, Niket Tandon, Prakhar Gupta, SkylerHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,et al. 2023. Self-refine: Iterative refinement withself-feedback. arXiv preprint arXiv:2303.17651. Archiki Prasad, Peter Hase, Xiang Zhou, and MohitBansal. 2023. Grips: Gradient-free, edit-based in-struction search for prompting large language models.In Proceedings of the 17th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics, pages 38273846. Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chen-guang Zhu, and Michael Zeng. 2023. Automaticprompt optimization with gradient descent andbeam search. In The 2023 Conference on Empiri-cal Methods in Natural Language Processing. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, LanYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,Bill Qian, et al. 2023. Toolllm: Facilitating largelanguage models to master 16000+ real-world apis.arXiv preprint arXiv:2307.16789.",
  "Subhro Roy and Dan Roth. 2016.Solving gen-eral arithmetic word problems.arXiv preprintarXiv:1608.01413": "Noah Shinn, Federico Cassano, Ashwin Gopinath,Karthik Narasimhan, and Shunyu Yao. 2024. Re-flexion: Language agents with verbal reinforcementlearning. Advances in Neural Information Process-ing Systems, 36. Noah Shinn, Federico Cassano, Ashwin Gopinath,Karthik R Narasimhan, and Shunyu Yao. 2023. Re-flexion: Language agents with verbal reinforcementlearning. In Thirty-seventh Conference on NeuralInformation Processing Systems. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Ct,Yonatan Bisk,Adam Trischler,and MatthewHausknecht. 2020. Alfworld: Aligning text and em-bodied environments for interactive learning. arXivpreprint arXiv:2010.03768. Tom Silver, Soham Dan, Kavitha Srinivas, Joshua BTenenbaum, Leslie Pack Kaelbling, and MichaelKatz. 2023.Generalized planning in pddl do-mains with pretrained large language models. arXivpreprint arXiv:2305.11014. Marta Skreta, Naruki Yoshikawa, Sebastian Arellano-Rubach, Zhi Ji, Lasse Bjrn Kristensen, KouroshDarvish,Aln Aspuru-Guzik,Florian Shkurti,and Animesh Garg. 2023.Errors are usefulprompts: Instruction guided task programming withverifier-assisted iterative prompting. arXiv preprintarXiv:2303.14100. Karthik Valmeekam, Matthew Marquez, Alberto Olmo,Sarath Sreedharan, and Subbarao Kambhampati.2023. Planbench: An extensible benchmark for eval-uating large language models on planning and rea-soning about change. In Thirty-seventh Conferenceon Neural Information Processing Systems Datasetsand Benchmarks Track.",
  "Ruoyao Wang, Peter Jansen, Marc-Alexandre Ct, andPrithviraj Ammanabrolu. 2022. Scienceworld: Isyour agent smarter than a 5th grader? arXiv preprintarXiv:2203.07540": "Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai,Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric PXing,and Zhiting Hu. 2023.Promptagent:Strategic planning with language models enablesexpert-level prompt optimization.arXiv preprintarXiv:2310.16427. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in NeuralInformation Processing Systems, 35:2482424837. Sean Welleck, Ximing Lu, Peter West, Faeze Brah-man, Tianxiao Shen, Daniel Khashabi, and YejinChoi. 2022. Generating sequences by learning toself-correct. In The Eleventh International Confer-ence on Learning Representations. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu,Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang,Xiaoyun Zhang, and Chi Wang. 2023a.Auto-gen: Enabling next-gen llm applications via multi-agent conversation framework.arXiv preprintarXiv:2308.08155. Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyrek,Boyuan Chen, Bailin Wang, Najoung Kim, Jacob An-dreas, and Yoon Kim. 2023b. Reasoning or reciting?exploring the capabilities and limitations of languagemodels through counterfactual tasks. arXiv preprintarXiv:2307.02477.",
  "Qinyuan Ye, Maxamed Axmed, Reid Pryzant, andFereshte Khani. 2023. Prompt engineering a promptengineer. arXiv preprint arXiv:2311.05661": "Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schu-urmans, and Joseph E. Gonzalez. 2023. TEMPERA:Test-time prompt editing via reinforcement learning.In The Eleventh International Conference on Learn-ing Representations. Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou,Robert Lo, Abishek Sridhar, Xianyi Cheng, YonatanBisk, Daniel Fried, Uri Alon, et al. 2023a. Webarena:A realistic web environment for building autonomousagents. arXiv preprint arXiv:2307.13854. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,Keiran Paster, Silviu Pitis, Harris Chan, and JimmyBa. 2023b. Large language models are human-levelprompt engineers.In The Eleventh InternationalConference on Learning Representations.",
  "Here we describe the 11 environments for multi-step tasks on which the various methods were tested.They require strong logical, geometrical, scientific, and commonsense reasoning capabilities": "WebarenaWebarena (a) is a real web environment containing four applications: onlineshopping, discussion forums, collaborative development, and business content management. It supports11 different web browsing actions. The observation space consists of structured web content. WebArenaoffers multi-round and continuous web browsing interaction simulation. AlfworldAlfworld (b) are Household tasks that require models to explore rooms and usecommonsense reasoning to perform tasks, such as AIJput a pencil on the desk AI. The execution scoresare calculated by pre-defined subgoals based on necessary observations to finish a task and the successflag provided by environments. ScienceworldScienceworld (c) is a complex interactive text environment that poses asignificant challenge to agents AZ scientific commonsense. This environment requires agents to navigatethrough eight distinct functional rooms (e.g., workshop, kitchen) and utilize the tools to complete taskssuch as AIJmeasure the melting point of the orange juice AI. BoxNet1BoxNet1 (d) consists of robot arms, colored boxes (squares), and colored goallocations (circles). Each robot arm is assigned to a cell indicated by the dotted lines and can only movewithin this cell. The goal is to move all boxes into the goal locations of corresponding colors in the fewesttime steps. Each arm has two possible actions: (1) move a box within its cell to a neighboring cell, and (2)move a box within its cell to a goal location within its cell. BoxNet2BoxNet2 (e) is similar to BoxNet1 but has an additional constraint. In BoxNet2,boxes can only be moved between cells by being placed at the corners of cells (indicated by the small redcircles), and each cell corner can only hold one box at a time. Each arm has two possible actions: (1)move a box from a corner to a different corner of its cell, and (2) move a box from a corner to a goallocation within its cell. BoxLiftBoxLift (f) consists of robots of different types and boxes of different sizes andweights. The robots are able to lift different amounts of weight and can cooperate with each other to liftone box. A box will be lifted only if the total lifting capability of robots is greater than the box AZsweight. The goal is to lift all boxes in fewest time steps. Further, the LLM agent can only observe the sizeof each box, not its actual weight. The weight of a box is roughly proportional to its size (with somerandomness), so the LLM agent should benefit from incorporating prior state/action feedback whenplanning. WarehouseWarehouse (g) consists of robots that need to move all boxes to a target deliveryregion in the fewest time steps. The free space for the robots to move is discretized into cells, and a robotcan only move to an adjacent cell in a single time step. Each cell can only contain one robot at eachtimestep. A robot is able to pick up a box if it is in the cell adjacent to that box. Each robot has five possibleactions: (1) & (2) move left or right if the adjacent cell exists, (3) pick up an adjacent box, (4) placethe box to the target delivery region, (5) move from target delivery region to any adjacent cell of free space. Gridworld1Gridworld1 (h) consists of obstacles (black) and goals (red). The robot needs tovisit all goals, and any attempt to move into obstacles or move out of the grid will result in failure. Therobot has five possible actions: (1) move up, (2) move down, (3) move left, (4) move right, (5) visit goal.",
  "order. The robot action are the same as in Gridworld1, but visit goal can be performed only when thecorresponding goal is in the correct order": "BlocksworldIn Blocksworld (i), the goal is to stack a set of blocks (brown) according to aspecific order. A robot can pick up, unstack, or stack a block only when the block is clear. A block is clearif the block has no other blocks on top of it and if the block is not picked up. The robot has four possibleactions: (1) pick up a block, (2) unstack a block from the top of another block, (3) put down a block, (4)stack a block on top of another block. LogisticsLogistics (j) consists of objects, locations, and cities. The objects can be packages,trucks, or airplanes. The locations can be generic locations or airports, and each location is associatedwith a single city. Trucks can travel to different locations within a city but not to a different city; airplanescan travel to any airports, including those in other cities. The goal is to transport packages to their goallocations via the trucks (such as for intra-city travel) and the airplanes (such as for inter-city travel). Theavailable actions are: (1) load a package into a truck, (2) load a package into an airplane, (3) unloada package from a truck, (4) unload a package from an airplane, (5) drive a truck from one location toanother location within a city, (6) fly an airplane from one airport to another airport. : Scores for initial and optimized prompts using different types of LLMs as TaskLLM. The opti-mized prompts are the best discovered prompts by PROMST for GPT-3.5-0613. The optimized promptsare further tested with GPT-3.5-0301 and GPT-4 to study whether they can keep better performances thanthe initial prompts.",
  "EGeneralization to different models for optimized prompts": ": Corresponding values of task progress rates (score format used in our study) and task completionrates (another possible score format). The task completion score is positively correlated with the taskprogress score. However, the task completion score has lower value and sensitivity since it is more sparse,which is the reason why we did not use it as the metric in our study.",
  "HEfforts for designing human feedback rules": "Regarding the expected human efforts when designing the feedback rules, we note that several of thetemplates are common across all tasks (e.g. syntax errors). Thus, for a novel task, a human user is onlyexpected to need to design a few (3-5) templates, and they are typically intuitive as they relate to thespecific task. While this is a low-effort requirement, our primary experimental results show that this effortcan significantly improve task performance.In order to better understand the generalizability of the human-designed feedback templates, we performan additional ablation study as shown in . In our work, the original feedback templates did notrequire iterations via trial-and-error over possible versions. In the ablation study, we compare using theoriginal templates with four other variations of the templates:(1) Paraphrased We use GPT-4 to generate semantically-consistent paraphrased versions of the originalfeedback templates to replace the original templates. This both simulates variation across human usersand tests the sensitivity of the wording.(2) Random We use GPT-4 to generate 10 different versions of a template for each type of error. Duringoptimization, we randomly sample from these 10 possible templates per error type, introducing morefine-grained variation than in (1).(3) WO response component In the original feedback templates, we included the output of theTaskLLM that was incorrect (see ) for better reasoning of PromptLLM. In this ablation, we testthe impact of removing this component.(4) WO stuck in the loop We exclude the error type of being stuck in a loop to test the impact of ahuman user choosing to include different types of error feedback. The results for two tasks from our largerexperiments are provided below.The results in show that variability over the wording of the templates has little impact on theperformance of PROMST. Interestingly, randomly choosing paraphrased templates ((2) from the abovedescription) generally improves performance; we suspect this may be due to increased diversity overgenerated prompt candidates and is worth further investigation. This ablation also shows that removingthe TaskLLM response ((3) from above) and removing the stuck in a loop error type both reduce theperformance. Based on the above discussion, we can conclude that including the response in the feedbacktemplate is a useful task- and error-agnostic guiding principle.",
  "IComponent changes in each environment": "In this section we display the evolution of task executing characteristics (task success number, executingstep number, syntactic error number, query limit error number, stuck in loop number, collision errornumber) vs. testing scores for all the prompts explored during prompt optimization process. The threeshown tasks (BoxLift, BoxNet1, BoxNet2) share the same trends such as the rising executing step numberand decreasing syntactic error number with the increasing testing score, but also have different trendssuch as stuck in loop number.",
  "JThe influence of score functions": "The choice of score function impacts prompt optimization. The initial score functions in Equation 4 aresimple and intuitive, only caring about the number of goals/sub-steps accomplished. However, humansmay have different preferences for the same task. For instance, a user may also care about efficiency (thenumber of action steps taken) or safety (collision avoidance). We observe a general trend that the stepnumber increases as the prompt score increases in all the three shown tasks (see Appendix I). However,in BoxNet2 () the collision error number gradually increases with the increasing prompt scores.These two general trends are not aligned with the user preference.Then how to design the score function to balance user preferences remains an issue. In Appendix J, wetried the two forms of modified scores:",
  "SM = SO/(1 + ratio factor_value),(6)": "where SM and SO are the modified score and the original score (defined in Equation 4), respectively. Thefactor_value is a factor that the user cares about, e.g., step number or collision error number. We find thatthe general SM vs. SO trend can be tuned quite disparately by adjusting the hyperparameter ratio (see and ).We choose two modified score functions that trend similarly to the original score function. Then weoptimize the prompts with PROMST but using modified score function. To save computing resources, weinitialize the prompt optimization with the best prompts found with the original score function. shows the optimization results. Compared to the original prompts acquired with the original score function(red), the newly discovered prompts (green) generally have higher modified scores, though the values oforiginal scores slightly decrease. This suggests that we can align with human preferences by changing theform of the score functions, which can be captured and revealed by the selection framework in PROMST.",
  "KExplanability for better prompts": "We are interested in whether there are features of the prompts that correlate with high scores.Prompt score vs. token length and perplexity As plotted in , we found that there is a roughtrend across different tasks that longer prompts corresponded with higher scores. We also investigatedprompt perplexity (using GPT-2 to get prompt token log probabilities) but found no clear correlation. Allthe initial and discovered best prompts are listed in Appendix M.Listing careful points one by one clearly We also find some clues about better component emergence.The best prompts tend to list all the careful points of the task one by one clearly, which is consistent tohuman intuitions. To study whether this characteristic can effectively improve the performance, we carryout the ablation study by compressing these careful points into shortened context summarized by GPT-4and then testing their performance. We obtain contexts of varying token lengths by querying GPT-4 tosummarize with different levels of detail. In the following two texts, we display the original best promptof BoxLift with token length of 326 and one corresponding compressed prompt with token length of 145as an example. As shown in , the task score of both BoxLift and GridWorld2 decline with thedecreasing length of careful points part. This reveals that listing the careful points of the task one by oneclearly is an effective method to enhance task executing performance, which is automatically emergedfrom prompt optimization process. This characteristic also inspires the designing of prompts in complextasks with multiple constraints.",
  "BoxLift Original Best prompt for GPT-3.5-turbo-16k-0613Token Length of Careful Points Part = 326, Score = 0.90": "- Each agent can only lift one box per step and must not be assigned to multiple boxes within thesame step.- Agents can collaborate to lift a box, but each agent can only be assigned to one box in each step.- The combined lifting capacity of the agents assigned to a box must meet or exceed the boxsestimated weight, which is roughly proportional to its volume. Verify that the total capacity ofassigned agents is sufficient before including them in the plan.- Integrate feedback from each step to avoid ineffective actions and adapt your strategy dynamically.Do not repeat agent combinations that have failed in previous attempts.- Utilize agents efficiently by exploring different combinations and managing resources to maximizethe number of boxes lifted per step. Ensure that agents are not duplicated within the same actionplan.- Prioritize boxes based on the number of previous attempts, the volume of the box, and the capacities of available agents. Attempt untried boxes first, followed by those that have been attempted fewertimes.- Consider complex combinations of agents for heavier boxes and be prepared to incrementally addmore agents if simpler combinations fail. Provide examples of how to form these combinations.- In situations where no available agents can lift a box due to insufficient capacity, adjust your planto include additional agents or explore alternative strategies, such as reevaluating the order of boxlifting or temporarily setting aside boxes that cannot be lifted until more agents are available.- Correct the example action plans to reflect the proper JSON format and constraints. Show how toadjust the action plan based on the feedback received, including how to add additional agents orchange agent assignments.",
  "BoxLift Compressed prompt for GPT-3.5-turbo-16k-0613Token Length of Careful Points Part = 145, Score = 0.73": "Agents can lift one box per step and must not handle multiple boxes in the same step. Collaborationis allowed, but each agent is limited to one box per step. The combined lifting capacity ofagents must meet or exceed a boxs estimated weight. Verify agent capacity before planning.Avoid repeating failed combinations and adapt strategies dynamically. Optimize agent efficiencyby exploring different combinations and managing resources to maximize lifted boxes per step.Prioritize untried boxes and those attempted fewer times. Use complex agent combinations forheavier boxes and adjust plans if no agents can lift a box, considering alternative strategies orreordering tasks. Correct action plans to reflect constraints and JSON format, showing adjustmentsbased on feedback, including adding agents or changing assignments. : Evolution of Task Score vs. Token Length of Careful Points Part in BoxLift and GridWorld2.The contexts of careful points part with different token lengths are summarized by GPT-4. The topmostdata point is the original context from the best prompt in each task.",
  "LComparison and combination with Reflexion": "Instead of utilizing the execution feedback by optimizing offline prompts such as PROMST, dynamicapproaches like Reflexion (Shinn et al., 2024) directly optimizing the action plan during multiple onlinetrials. These methods assume the agent has chances of trying multiple trials and the new trial depends onthe reflection from previous failed trials. The contexts summarized from the reflection are saved in thememory module, which is unique for each testing case. Though the original approach can not be directlyapplied into the task of prompt optimization, here we modify the original online Reflexion into the offlineReflexion for the comparison with PROMST on prompt optimization. That is, we add the memory moduleinto the initial prompt to explore whether it can help improve the prompt performance.Since the memory module is varied in each testing case, we tried two methods to add onto the initialprompt: 1) Offline Reflexion 1: Directly concatenating several numbers/cases of memory module withthe initial prompt; 2) Offline Reflexion 2: Querying LLM to summarize the memory from the multiplecases and then concatenate with the initial prompt. The prompts used for acquiring memory modules arefrom the Reflexion paper, while the summarization prompt is designed by ourselves. The feedback ispurely from the environment without the human factors. shows the testing results. We find that the Offline Reflexion 1 is not very effective in promptoptimization. The reason is that the distilled memory is always too case-specific so that the context cannot well generalize to the diversified testing data. Meanwhile, the memory in each case is lengthy so thatthe final prompt can only contain 3-5 cases, which can not act as a well-rounded guidance during testing.The summarization over multiple case memories is a good way to mitigate this issue, as shown in OfflineReflexion 2. However, Offline Reflexion 2 is still not as effective as PROMST, which is reasonable sinceOffline Reflexion 2 does not have following modules compared to PROMST: optimization on classifyingthe error types, human feedback, genetic algorithms, and score prediction model.We further test whether PROMST can perform better when integrating with Online Reflexion. Thatmeans we directly test the best prompts discovered in PROMST under the multi-trial setting and useReflexion as the online feedback module. The results in reveal that the online feedback trulyfurther enhances the agent performance, which is consistent with the intuition since previous failedtrials help a better decision making in the current trial. The above results show that PROMST performsbetter than Reflexion in offline prompt optimization and can well combine with Reflexion in online taskexecution, resulting into a better solution.",
  "MHuman prompts and discovered best prompts for GPT-3.5-0613 and GPT-4 in all the11 multi-step tasks": "Our work can serve as a benchmark for prompt optimization, particularly on multi-step agent tasks. Hence,we list all the initial human prompts and discovered best prompts for GPT-3.5-0613 and GPT-4 modelsacross the 11 tasks. Note that we do not list the best prompt of GPT-4 in BoxLift since the optimizedprompt can easily achieve the full score 1.0. We also do not list the best prompt of GPT-3.5-0613 inWareHouse since all the discovered prompts achieve scores near 0.0.",
  "Webarena Human promptScore = 0.22 (GPT-3.5-turbo-16k-0613 as the testing LLM)Score = 0.57 (GPT-4 as the testing LLM)": "Heres the information youll have:The users objective: This is the task youre trying to complete.The current web pages accessibility tree: This is a simplified representation of the windowedwebpage, providing key information.The current web pages URL: This is the page youre currently navigating.The open tabs: These are the tabs you have open.The useful websites and corresponding URL you can navigate:",
  "reddit: online shop: e-commerce platform: gitlab: wikipedia: map:": "Your role is to decide on an action based on the observation and current valid actions. Ensure thatthe planned action in the current step is within the current valid actions.The actions you can perform fall into several categories:Page Operation Actions: click [id]: This action clicks on an element with a specific id on the webpage. type [id] [content] [press_enter_after=0|1]: Use this to type the contentinto the field with id. By default, the Enter key is pressed after typing unless press_enter_afteris set to 0. Ensure the content syntax is correct for the context (e.g., search queries should usethe proper format for the website).",
  "Alfworld Human promptScore = 0.075 (GPT-3.5-turbo-16k-0613 as the testing LLM)Score = 0.45 (GPT-4 as the testing LLM)": "Your task is to interact with a virtual household simulator to accomplish a specific task. Witheach interaction, you will receive an observation and current valid actions. Your role is to de-cide on an action based on the observation and current valid actions. Please ensure that anyobjects ({obj}) and receptacles ({recep}) you mention in your response are present in theobservation provided. Ensure that the planned action in the current step is within the currentvalid actions. Example objects are like a cellphone 3, a newspaper 2, a statue 1, and a televi-sion 1. Example receptacles are like a coffeetable 1, a diningtable 1, a drawer 4, a drawer 3,a drawer 2, a drawer 1, a dresser 1, a garbagecan 1, a sidetable 2, a sidetable 1, and a sofa1. Example actions are like [go to dresser 1, take statue 1 from dresser 1,heat apple 1 with microwave 1, open cabinet 2] Do not repeat the actions allthe time! Learn from the previous action/observation history.Here are the available actions you can take:",
  "Alfworld Best prompt for GPT-3.5-turbo-16k-0613Score = 0.30 (GPT-3.5-turbo-16k-0613 as the testing LLM)": "Your task is to interact with a virtual household simulator to achieve a clearly defined goal. You willreceive observations and a list of current valid actions after each interaction. Your role is to selectan appropriate action based on the observation, the goal, and the valid actions available. Ensurethat any objects ({obj}) and receptacles ({recep}) you mention in your response are present inthe observation provided. Your planned action must be one of the current valid actions.To successfully complete the task, please adhere to the following optimized guidelines:",
  ". Valid Action Selection: Strictly choose your actions from the provided list of current validactions. Do not attempt any actions that are not listed as valid for the current situation": "4. State Tracking and Changes: Keep a mental model of the environments state and updateit with each actions outcome. Recognize that actions can alter the state of the environment,necessitating a reassessment of valid actions. 5. Feedback Utilization and Error Handling: Use feedback from the simulator to learn fromunsuccessful actions. If an action fails, select a different valid action, avoiding repetition ofineffective choices.",
  "Alfworld Best prompt for GPT-4Score = 0.57 (GPT-4 as the testing LLM)": "Your task is to interact with a virtual household simulator to achieve a specific goal. Each interactionprovides you with an observation and a dynamic list of valid actions. Your role is to select an actionthat aligns with the goal, using the observation and the valid actions as your guide.Before selecting an action, confirm that any objects ({obj}) and receptacles ({recep}) youintend to interact with are mentioned in the observation. Only choose an action that is currentlyvalid.Here are the refined guidelines to ensure effective decision-making:",
  "Scienceworld Human promptScore = 0.18 (GPT-3.5-turbo-16k-0613 as the testing LLM)Score = 0.70 (GPT-4 as the testing LLM)": "You are an agent in a virtual science school environment, tasked to interact with various elements.Your role is to decide on an action based on the observation and current valid actions. Please ensurethat any objects ({OBJ}) and locations ({LOC}) you mention in your response are present inthe observation provided. Ensure that the planned action in the current step is within the currentvalid actions. Example objects are like a picture, a substance called air, a thermometer, and astopwatch. Example locations are like a coffeetable 1, a diningtable 1, a drawer 4, a drawer 3, adrawer 2, a drawer 1, a dresser 1, a garbagecan 1, a sidetable 2, a sidetable 1, and a sofa 1. Exampleactions are like [go to dresser 1, take statue 1 from dresser 1, heat apple 1 with microwave 1, opencabinet 2]. Do not repeat the actions all the time! Learn from the previous action/observation history.",
  "Scienceworld Best prompt for GPT-3.5-turbo-16k-0613Score = 0.21 (GPT-3.5-turbo-16k-0613 as the testing LLM)": "You are an intelligent agent in a virtual science school environment, with the mission to interactwith various elements to complete specific tasks. Your success depends on making informeddecisions based on accurate observations and a list of valid actions. Before you act, always perform a look around to confirm your current location and the objectswithin it.This ensures you are aware of your environment and prevents interactions withnon-existent items. Additionally, regularly check your inventory to be aware of the items youpossess before attempting to use them. As you plan your actions, refer to the provided list of commands and adhere strictly to the correctformat. Learn from past interactions and do not repeat actions that have been marked as invalid orunsuccessful. Instead, adapt your strategy to navigate the environment effectively.",
  "Scienceworld Best prompt for GPT-4Score = 0.81 (GPT-4 as the testing LLM)": "You are an agent in a virtual science school environment, with the objective of interacting withvarious elements to complete tasks. Your actions must be based on the observations providedand align with the current valid actions list. It is imperative to use only the objects ({OBJ})and locations ({LOC}) mentioned in the observation. Your planned action should be checkedagainst the valid actions list to ensure it is permissible. Adapt your actions based on previous feedback, avoiding repetition of invalid actions. Your actionsshould be goal-oriented, contributing directly to the tasks objective. Use objects and locationsprecisely as they appear in the observations and valid actions list, and ensure that your commandsare specific and accurate.",
  "task inventory": "Before suggesting an action, confirm it is listed as a valid action. If feedback indicates an action isinvalid, do not repeat it; instead, reassess and choose a different valid action. Regularly use theinventory command to manage items youre carrying and the task command to keep theobjective in focus. If you encounter an error, recognize it, and correct your approach. Prioritizeefficiency by performing actions in a sequence that is most likely to achieve the goal, avoidingunnecessary steps. Maintain consistency in the terminology used for objects and actions, as per the observations andvalid actions list. If the environment feedback suggests a misunderstanding of the environment orthe structure, take time to look around and reassess your strategy.",
  "BoxNet1 Human promptScore = 0.076 (GPT-3.5-turbo-16k-0613 as the testing LLM)Score = 0.65 (GPT-4 as the testing LLM)": "You are a central planner directing agents in a grid-like field to move colored boxes. Each agentis assigned to a 1x1 square and can only interact with objects in its area. Agents can move abox to a neighboring square or a same-color target. Each square can contain many targets and boxes.",
  "BoxNet1 Best prompt for GPT-3.5-turbo-16k-0613Score = 0.25 (GPT-3.5-turbo-16k-0613 as the testing LLM)": "As a central planner, your primary objective is to coordinate the actions of agents on a grid field toalign colored boxes with their corresponding color-coded targets. Each agent occupies a unique 1x1square and can interact with only one object at a time within that space. Agents can move a box toan adjacent square or place it directly onto a target of the same color located within their square.",
  "Keep in mind that a single square may contain multiple boxes and targets of different colors, butagents can only interact with one at a time": "The grid is composed of squares, each identified by the coordinates of its center (e.g., square[0.5,0.5]).Commands to agents must be issued using the precise structure: move(box_color,destination), where box_color is the color of the box to be moved, and destination is either thecoordinate of an adjacent square in the format square[x.y, z.w] or a target within the same square,indicated by target_color. Your task is to issue precise, valid, and executable instructions to the agents in JSON format, withthe goal of matching all boxes with their designated color-coded targets. Agents will providefeedback on the execution of each action, which you must use to adapt and refine your instructions.Strategic planning and coordination of the agents actions are essential for the efficient and effectivecompletion of the task. Here is the JSON format for your action plan, which should only include agents that have a validand executable task for the upcoming step. Each agents action must be clearly stated in quotes andseparated by commas:json{Agent[x.y, z.w]: move(box_color, destination),// Additional agents actions formatted similarly, separated by commas}In your plan, each agent must be mentioned only once, and all coordinates and targets specifiedmust be accurate and feasible.Use the term move consistently and avoid including anyunnecessary details or instructions that are not action commands. Strictly maintain the correctJSON format, with proper use of braces, quotes, and colons. Before proposing a move, confirm that it is a viable action for that agent, given the current state ofthe grid, the positions of agents, boxes, and targets. Update your strategy based on feedback fromthe agents and avoid suggesting moves that have been previously identified as invalid. Prioritizeactions that contribute to the most efficient completion of the task, and refrain from assigningactions to agents that have no available tasks or have already been given a task in the current step.Your instructions must demonstrate a thorough understanding of the tasks objective and integratelessons learned from past errors to prevent the repetition of unsuccessful actions.",
  "BoxNet1 Best prompt for GPT-4Score = 0.79 (GPT-4 as the testing LLM)": "You are a central planner tasked with directing agents in a grid-like field to move colored boxes totheir corresponding color-coded targets. Each agent occupies a 1x1 square and can only interactwith objects within its square. Agents can move a box to an adjacent square or directly to a targetsquare of the same color. A square may contain multiple boxes and targets. The squares are identified by their center coordinates (e.g., square[0.5, 0.5]). Actions are formattedas: move(box_color, destination), where box_color is the color of the box and destination is eithera target of the same color or an adjacent square. Your objective is to create an action plan that instructs each agent to match all boxes to theircolor-coded targets in the most efficient manner. After an agent performs an action, it will providefeedback for the next sequence of actions. You must coordinate the agents based on the updatedgrid state.",
  "BoxNet2 Human promptScore = 0.044 (GPT-3.5-turbo-16k-0613 as the testing LLM)Score = 0.34 (GPT-4 as the testing LLM)": "You are a central planner directing agents in a grid-like field to move colored boxes. Each agent isassigned to a 1x1 square and can only interact with objects located on the corners of its square.Agents can move a box to other three corners or a same-color target in its square. Each square cancontain many targets.",
  "The squares are identified by their center coordinates, e.g., square[0.5, 0.5]. Actions are like:move(box_red, target_red) or move(box_red, position[1.0, 0.0])": "Do remember that each corner can only contain at most one box! Hence, you need to avoid thecollision of boxes. Actions like move two boxes into the same corner at the same time or move onebox into the corner that already has one box are not allowed! Your task is to instruct each agent to match all boxes to their color-coded targets. After each move,agents provide updates for the next sequence of actions. Your job is to coordinate the agentsoptimally.",
  "BoxNet2 Best prompt for GPT-3.5-turbo-16k-0613Score = 0.22 (GPT-3.5-turbo-16k-0613 as the testing LLM)": "As a central planner, your objective is to strategically direct agents to relocate colored boxes withina grid-like field, ensuring each box is matched with its corresponding color-coded target. Agentsoccupy individual 1x1 squares and can interact with objects at the corners of their square. They canmove a box to any of the three other corners within their square or directly to a target of the samecolor if it is within their square. A single square may contain multiple targets, but each corner canonly hold one box at a time. Your instructions must be formatted as precise, executable actions in a dictionary format, whereeach key-value pair represents an agent and its assigned action. The format for the action plan is asfollows:{Agent[coordinate]: move(object, location),// Additional agents and actions as necessary}For example: {Agent[0.5, 0.5]: move(box_blue, target_blue),Agent[1.5, 1.5]: move(box_red, position[1.0, 1.0])}To optimize the relocation process and prevent any collisions or inefficiencies, your action plansmust adhere to these refined guidelines:1. Ensure no corner is assigned more than one box at any time to avoid overlaps.2. Include only agents with a viable task for the next action in your plan; exclude idle agents.3. Learn from the outcomes of previous actions to refine your strategy, avoiding ineffective movesand preventing action loops.4. Give priority to actions that move boxes directly to their color-coded targets when such movesare possible.5. Coordinate agents to prevent collisions, ensuring no two agents move boxes to the same positionsimultaneously.6. Aim for the most efficient sequence of moves to match all boxes with their targets in the feweststeps possible.7. Strictly maintain the specified dictionary format for action plans for clarity and consistency incommunication. 8. Continuously adjust your planning based on the outcomes of previous actions to enhanceefficiency and avoid repeating mistakes.9. Consider the entire grid and strategically plan actions for optimal coordination among all agents.10. Ensure that each action is unambiguous and clearly defined, allowing agents to execute the planwithout confusion. Remember to correct any errors from previous steps in your new plan. Your ultimate goal isthe successful matching of all boxes to their targets in the most efficient manner possible, whileadhering to the rules of the environment and the capabilities of the agents.",
  "BoxNet2 Best prompt for GPT-4Score = 0.42 (GPT-4 as the testing LLM)": "You are a central planner tasked with directing agents to move colored boxes to their correspondingcolor-coded targets within a grid-like environment. Each agent controls a 1x1 square and caninteract with objects at the corners of its square. The objective is to match all boxes to their targetswith optimal efficiency and no collisions. To achieve this, follow these refined rules and guidelines:",
  ". **Unique Square Identification**: Identify each square by its center coordinates, for example,square[0.5, 0.5]": "2. **Valid Actions**: Agents can move a box within their square to a different corner or directly toa target of the same color. Use the format move(box_color, target_color) for moving to a targetwithin the same square, and move(box_color, position[x, y]) for moving to a corner within thesame square, where x and y are relative corner coordinates.",
  ". **Unique Identification**: Ensure each agent and box is uniquely identified to avoid assigningmultiple actions to the same entity within a single planning step": "Your action plan should resemble the following example, with modifications based on the currentstate of the grid and the rules outlined above:json{Agent[0.5, 0.5]: move(box_blue, target_blue),Agent[1.5, 1.5]: move(box_red, position[1.0, 1.0])}The goal is to match all boxes to their color-coded targets with optimal efficiency and no collisions.Ensure that each action is valid, efficient, and adheres to the rules, avoiding any form of collision orinvalid move.",
  "BoxLift Human promptScore = 0.31 (GPT-3.5-turbo-16k-0613 as the testing LLM)Score = 0.92 (GPT-4 as the testing LLM)": "You are a central planner directing lifting agents in a warehouse to lift boxes. Each agent hasdifferent lifting capability and can cooperate with each other to lift one box. In summation oflifting capability, the agents can lift all boxes. The boxes are identified by their volume, e.g., box[1.4V]. The agents are identified by theirlifting weight capability, e.g., agent[1.5W]. Actions are like:box[1.7V]:agent[2.5W],box[6.0V]:agent[1.5W], agent[2.5W]. Your task is to divide the group of each agent to lift all the boxes. After each step, environmentsprovide updates for the left boxes. Your job is to coordinate the agents optimally to minimize thestep number. Note that the agents can only lift one box at a time. Each lifting agent can be used only oncein each step! You can combine multiple agents to lift one box like box[3.0V]:agent[1.5W],agent[2.5W]! Try to combine many agents to lift one box together once you find it can not belifted. [The volume of the box is roughly proportional to the weight of the box, but with some randomness.Thus, the planner should guess the box weight based on the box volume and previous state/actionfeedback.]",
  "BoxLift Best prompt for GPT-3.5-turbo-16k-0613Score = 0.90 (GPT-3.5-turbo-16k-0613 as the testing LLM)": "As the central planner in our warehouse, your primary goal is to efficiently coordinate the lifting ofboxes by assigning agents with specific lifting capacities. Each box is marked by its volume (e.g.,box[1.4V]), and each agent by their lifting capacity (e.g., agent[1.5W]). Your task is to create anaction plan that minimizes the number of steps required to lift all boxes, adhering to the followingupdated constraints and guidelines: - Each agent can only lift one box per step and must not be assigned to multiple boxes within thesame step.- Agents can collaborate to lift a box, but each agent can only be assigned to one box in each step.- The combined lifting capacity of the agents assigned to a box must meet or exceed the boxsestimated weight, which is roughly proportional to its volume. Verify that the total capacity ofassigned agents is sufficient before including them in the plan. Your action plan must be provided in strict JSON format, with agent assignments within the JSONobject in an array format, even if there is only one agent lifting a box. Ensure that the JSON keysand values are properly quoted with double quotes, and that arrays use square brackets. Here is anexample of how to structure your plan correctly: json{box[1.7V]: [agent[1.5W]],box[3.0V]: [agent[1.5W], agent[2.5W]]}After each lifting step, you will receive feedback on the remaining boxes. It is imperative toincorporate this feedback to refine your strategy. Avoid repeating combinations of agents that havepreviously failed to lift a box. Instead, explore alternative combinations and incrementally addmore agents if necessary. Prioritize boxes based on a clear set of criteria, including the number of previous attempts, thevolume of the box, and the capacities of available agents. Attempt untried boxes first, followedby those that have been attempted fewer times. If a box cannot be lifted due to insufficient agentcapacity, adjust your plan in the subsequent step to include additional agents.",
  "To ensure the effectiveness of your strategy, please adhere to these updated guidelines:": "- Integrate feedback from each step to avoid ineffective actions and adapt your strategy dynamically.Do not repeat agent combinations that have failed in previous attempts.- Utilize agents efficiently by exploring different combinations and managing resources to maximizethe number of boxes lifted per step. Ensure that agents are not duplicated within the same actionplan.- Prioritize boxes based on the number of previous attempts, the volume of the box, and thecapacities of available agents. Attempt untried boxes first, followed by those that have beenattempted fewer times.- Consider complex combinations of agents for heavier boxes and be prepared to incrementally addmore agents if simpler combinations fail. Provide examples of how to form these combinations.- In situations where no available agents can lift a box due to insufficient capacity, adjust your planto include additional agents or explore alternative strategies, such as reevaluating the order of boxlifting or temporarily setting aside boxes that cannot be lifted until more agents are available.- Correct the example action plans to reflect the proper JSON format and constraints. Show how toadjust the action plan based on the feedback received, including how to add additional agents orchange agent assignments.",
  "Agent can only walk on horizontal tracks and enter specific regions for picking up boxes. Eachagent can only hold one box each time. Each agent can do the actions:": "1) When the robot is on the track, it can pick up one box whose location is 0.5 away from the robot(either location difference in x or y.). For example, pick box_1.5_1.0Note that the agent canonly pick the box near its location, their row locations should have difference of 0.5, and columndifference should be 0.0, e.g., agent0 is in track_1 and column_3 and can do pick box_1.5_3.0 orpick box_0.5_3.0.",
  ") When the robot is on the target, it can move its position to the track to get onto the track andcarry the boxes. For example, move to track_1 AI": "4) When the robot is on the track, it can move its position to the target to pour the box into thetarget. For example, move to targetNote that robots without box on it can also move to target toavoid being obstacle of other robots. All robots moving to the target will pour their boxes. Hence,the final goal is to pour all the boxes into the target. Multiple robots can locate in target in the sametime, but cannot be in the same track position in the same time.The warehouse playground has left side column 0 and right side, if the agent column is at these twosides, they can only move right or move left but not both directions.If the agent in the target, it can move to the left side of all the tracksIf the agent is in the left side of the track, it can move to the target and drop the box. Your task is to assign each agent the task in the next step. After each step, environments provideupdates for each agent and the state of left boxes. Your job is to coordinate the agents optimally tominimize the step number. [Do remember that each position(track and column locations) can only accommodate one agenteach step! Hence, you need to avoid the collision with other agents. Actions like move two agentsinto the same position at the same time or move one agent into the position that already has oneagent are not allowed!] Specify your action plan in this format: {agent0:move left, agent1:move to track_1,agent2:pick box_1.5_1.0, agent3:move to target, agent4:move right, agent5:pickbox_1.5_3.0}. Include an agent only if it has actions in the next step.",
  "WareHouse Best prompt for GPT-4Score = 0.512 (GPT-4 as the testing LLM)": "You are a central planner tasked with the strategic coordination of autonomous mobile agentswithin a warehouse environment. Your primary goal is to orchestrate the movement of these agentsto efficiently transport boxes from their initial locations to a designated target area. Each agent cancarry only one box at a time. To successfully accomplish this task, agents must adhere to a set ofrules and constraints that govern their actions.",
  "The agents can perform the following actions, under specific conditions:": "1) Pick Up Box: An agent can pick up a box if it is directly adjacent to it on the track, specifically0.5 units away either in the x or y direction. For instance, an agent positioned at track_1, column_3,can execute pick box_1.5_3.0 or pick box_0.5_3.0 if the box is present and the agent is not",
  "already carrying a box": "2) Move Horizontally: An agent on the track can move horizontally by one unit either to the left orto the right, unless it is at the extremities of the tracks (column 0 or the last column), where it canonly move away from the extremity. Use the commands move left or move right to direct thisaction.",
  "The following constraints must be observed:": "- An agent not carrying a box may move to the target area to prevent obstructing the path of otheragents.- Multiple agents can occupy the target area simultaneously, but they must not be positioned on thesame track and column at the same time.- Agents at the extremities of the tracks are restricted to moving in one direction only (to the rightfrom column 0 and to the left from the last column).- Collision avoidance is mandatory: no two agents are allowed to occupy the same track and columnposition at the same time. Your responsibility is to devise a plan for the next move of each agent with the aim of minimizingthe total number of steps required. After each move, you will receive updated information aboutthe positions of each agent and the locations of the remaining boxes. Use this information to refineyour strategy and prevent collisions. Action plans must be formatted as follows: {agent0:move left, agent1:move to track_1,agent2:pick box_1.5_1.0, agent3:move to target, agent4:move right, agent5:pickbox_1.5_3.0}. Include an agent in your action plan only if it needs to take action in the next step. The overarching objective is to transport all boxes to the target area with maximum efficiency, incompliance with the established rules and constraints. Your planning must be reflective of thecurrent warehouse conditions, including the agents positions, whether they are carrying a box, andthe box locations, to ensure seamless operations. Use feedback from the environment to adjustfuture actions, avoiding repetition of actions that were previously indicated as not doable, andensure that the action plan is precise and includes only necessary agent movements.",
  "If the robot is in the same square with a goal, you can pick up the goal and the square becomes empty": "[(1) Note that the coordinate system is different from the Cartesian coordinate system. The originis at the top left corner. The coordinate representation is [row_number, column_number].For example, if you are in the square , Move up leads to , Move down leads to ,Move left leads to , and Move right leads to .(2) In your response, you can only use {} to specify your action. For example, {Move up}. Do notadd any other words or symbols in your response. Also use {} only once in your whole response sothat we know what is next action without ambiguity.]",
  "Gridworld1 Best prompt for GPT-3.5-turbo-16k-0613Score = 0.38 (GPT-3.5-turbo-16k-0613 as the testing LLM)": "You (the robot) are tasked with navigating a grid-like field to sequentially collect all goals whileavoiding obstacles. Each goal and obstacle occupies a distinct 1x1 square on the grid. Your currentposition is known, and you must use this information to make strategic decisions that adhere to thefollowing optimized, clarified, and refined rules:",
  ". **Immediate Goal Collection**: If a goal is located on your current square, immediately collectit with the action {Pick goal} before considering any movement": "2. **Enhanced Obstacle and Boundary Avoidance**: Before planning a move, confirm that theintended path is free of obstacles and within the grid limits. The grids origin is at the top leftcorner, with coordinates [row_number, column_number]. Do not attempt to move into a squarewith an obstacle or beyond the grid boundaries. 3. **Strategic Goal Pursuit**: Identify the location of the nearest goal using the most efficientpath calculation and plan a path towards it, circumventing any obstacles as necessary. Your movesshould be calculated to reduce the distance to the nearest goal unless an obstacle dictates a detour. 4. **Dynamic Strategy Adaptation**: Reflect on the outcomes of previous actions to enhanceyour decision-making process. Avoid actions that have previously led to collisions or have notprogressed you towards a goal. Adjust your strategy to be more effective.",
  ". **Prioritization of Actions**: The collection of goals is your primary mission. Move only if it isstrategic for goal acquisition or essential for obstacle circumvention": "6. **Continuous State Assessment and Adjustment**: Consistently verify and update your currentstate after each action. This includes your position, the positions of goals, and the locations ofobstacles to ensure your next action is based on the most current information. 7. **Feedback-Driven Action Refinement**: Integrate feedback from the environment and yourprevious actions to refine your approach. If an action was ineffective or incorrect, adopt a differentstrategy that complies with the established rules.",
  ". **Effective Feedback Application**: Utilize feedback from the environment to continuouslyimprove your actions, particularly after an unsuccessful or ineffective move": "13. **Nearest Goal Prioritization**: Always determine the nearest goals location from yourcurrent position before planning your next move. This ensures that your actions are optimized forgoal collection efficiency. 14. **State Verification Before Action**: Before planning your next move, verify your currentstate, including the presence of goals and obstacles, to ensure that your next action is appropriateand strategic. 15. **Avoidance of Ineffective Repetition**: Use feedback from the environment to avoidrepeating actions that have been proven ineffective or incorrect. Learn from past outcomes to makebetter decisions. 16. **Clear Movement Decision Criteria**: When multiple movement options are available,choose the direction that brings you closest to the nearest goal without violating obstacle andboundary rules. If equidistant, prioritize moves in the following order: up, left, down, right. 17. **Loop Prevention and Progress Assessment**: If you find yourself oscillating between two ormore squares without making progress, reassess the situation and choose a different path to breakthe loop. After each move, assess whether you are closer to the nearest goal to ensure progress isbeing made.",
  "collection and efficient navigation": "20. **Feedback Mechanism Accuracy**: Ensure that the feedback mechanism is correctlyinterpreting the robots actions, particularly when collecting goals. If the feedback indicates anerror in goal collection when the action was correct, the mechanism should be adjusted to recognizethe successful collection. 21. **Boundary and Obstacle Confirmation**: Before each move, perform a boundary andobstacle check to confirm that the intended path is valid. This check must be accurate to preventinvalid moves that violate the rules. 22. **Goal Collection Confirmation**: When on a square with a goal, confirm the collection of thegoal before any movement is considered. This action must be prioritized over all others to alignwith the missions primary objective. 23. **Error Recognition and Recovery**: The robot must be capable of recognizing when an errorhas occurred, such as attempting to move into an obstacle or outside the grid, and take immediatecorrective action. 24. **Comprehensive State Verification**: Continuously verify the robots current state, includingits position, the positions of goals, and the locations of obstacles, before planning and executing thenext move.",
  "If the goal in the current square is not the next goal, you can not pick it up. You should move toother squares to find the next goal": "[(1) Note that the coordinate system is different from the Cartesian coordinate system. The originis at the top left corner. The coordinate representation is [row_number, column_number].For example, if you are in the square , Move up leads to , Move down leads to ,Move left leads to , and Move right leads to .(2) The robot should pick up all the goals in order, index from 0 to larger. For example, if there are3 goals, the robot should pick up the goal_0 first, then the goal 1, and finally the goal 2.(3) In your response, you can only use {} to specify your action. For example, Move up. Do notadd any other words or symbols in your response. Also use {} only once in your whole responseso that we know what is next action without ambiguity.]",
  "Movement Rules:- You may move one square at a time in one of four directions: up, down, left, or right.- You must not move into squares with obstacles or beyond the grid boundaries": "Goal Collection Rules:- You must pick up a goal only if it is the next in sequence and you are on the same square as thatgoal.- Once a goal is picked up, the square it occupied becomes traversable.- If you encounter a goal that is not the next in sequence, you cannot pick it up and must navigate tofind the correct goal. Coordinate System:- The grids origin is at the top left corner, with coordinates given as [row_number, column_number].- Moving up decreases the row number, moving down increases the row number, moving leftdecreases the column number, and moving right increases the column number. Action Specification:- Specify your action using only one of the following commands within curly braces: {Move up},{Move down}, {Move left}, {Move right}, {Pick goal}.- Do not include any additional words, symbols, or multiple actions within the braces. Adaptive Learning and Error Correction:- Learn from the outcome of each action to avoid ineffective or rule-violating moves.- Continuously update your strategy based on your current position, the positions of remaininggoals, and the locations of obstacles.- Avoid repeating a sequence of moves that does not change your state or bring you closer to thenext goal.- If an action does not progress towards the goal or violates the rules, reassess and choose a differentaction. Action Planning and Efficiency:- Before each move, verify your current position and assess the most efficient path to the next goal,avoiding obstacles and grid edges.- If you are on the same square as the next goal, the only valid action is {Pick goal}.- If the next goal is not directly accessible, plan an alternative route that brings you closer to thegoal without violating movement rules.- Prioritize picking up the goal over moving if you are on the goal square.",
  "Adhere to these optimized rules for successful navigation and goal collection:": "1. **Sequential Goal Collection**: Before suggesting {Pick goal}, explicitly state the number ofthe goal you are attempting to collect and confirm it is the next in the sequence. Do not attempt tocollect a goal if it is not the correct one in the order. 2. **State and Position Awareness**: Continuously update your current position on the grid andthe location of the next goal. Plan your moves to efficiently reach the next goal, avoiding obstaclesand grid boundaries. 3. **Action Preconditions**: Only suggest Pick goal when you have verified that you are on thecorrect goal square and that the goal is the next in the sequence. Provide a clear justification foryour action by stating your current position and the goals position. 4. **Learning from Errors**: If an action is ineffective, analyze the outcome, learn from themistake, and adjust your strategy to avoid repeating the error. State the reason for the error and theadjustment you will make.",
  ". **Clear Movement Rules**: Adhere to the rules of movement and goal collection withoutambiguity, ensuring that each action is deliberate and aligns with the goal sequence": "Before suggesting an action, confirm your current position, the location of the next goal, and theabsence of obstacles in your path. Justify your action choice by referencing the goal sequence andyour current position relative to the next goal. If an error occurs, analyze why it happened andadjust your strategy accordingly. The coordinate system for the grid has its origin at the top left corner, with coordinates representedas [row_number, column_number]. For example, from , {Move up} results in , {Movedown} in , {Move left} in , and {Move right} in . Your possible actions are: {Move up}, {Move down}, {Move left}, {Move right}, {Pick goal}.Respond with only one of these actions, formatted as shown, at the end of each turn. Before takingan action, ensure it aligns with the goal sequence and the rules provided.",
  "Blocksworld Human promptScore = 0.19 (GPT-3.5-turbo-16k-0613 as the testing LLM)Score = 0.71 (GPT-4 as the testing LLM)": "I am playing with a set of blocks where I need to arrange the blocks into stacks. Here are the actionsI can doPick up a blockUnstack a block from on top of another blockPut down a blockStack a block on top of another block I have the following restrictions on my actions:I can only pick up or unstack one block at a time.I can only pick up or unstack a block if my hand is empty.I can only pick up a block if the block is on the table and the block is clear. A block is clear if theblock has no other blocks on top of it and if the block is not picked up.I can only unstack a block from on top of another block if the block I am unstacking was really ontop of the other block.I can only unstack a block from on top of another block if the block I am unstacking is clear.Once I pick up or unstack a block, I am holding the block.I can only put down a block that I am holding.I can only stack a block on top of another block if I am holding the block being stacked.I can only stack a block on top of another block if the block onto which I am stacking the block isclear.Once I put down or stack a block, my hand becomes empty.Once you stack a block on top of a second block, the second block is no longer clear. Please learn from previous steps. Not purely repeat the actions but learn why the state changes orremains in a dead loop. Avoid being stuck in action loops. Specify your action in this format at theend of your answer: pick up the {}, put down the {}, stack the {} on top of the {},unstack the {}",
  "Blocksworld Best prompt for GPT-3.5-turbo-16k-0613Score = 0.6 (GPT-3.5-turbo-16k-0613 as the testing LLM)": "I am tasked with arranging a set of blocks into specific configurations through a block-stackingactivity. My available actions are:- Pick up a block that is clear and on the table.- Unstack a clear block from the top of another block.- Put down a block onto the table, ensuring my hand is empty afterward.- Stack a block onto another clear block, ensuring my hand is empty afterward. To ensure successful completion of these actions, I must follow these rules:1. I can only manipulate one block at a time.2. My hand must be empty before I can pick up or unstack a block.3. A block is considered clear and eligible to be picked up if it has no blocks on top of it, is on thetable, and is not being held.4. I can unstack a block only if it is the topmost block on another and there are no blocks above it.5. When I pick up or unstack a block, I will be holding it.6. I can only put down or stack a block that I am currently holding.7. A block can be stacked onto another only if the bottom block is clear.8. My hand must be empty before and after I place or stack a block.9. Stacking a block on top of another makes the bottom block non-clear. To optimize task execution and avoid errors, I will adhere to the following strategies:- Conduct a comprehensive state verification before each action to ensure all preconditions are met:my hand is empty before picking up or unstacking; the block is clear, on the table, and not beingheld for picking up; and I am holding a block before putting down or stacking.- Maintain an accurate and constantly updated mental model of the block arrangement, noting theclear status of each block, the current stack configurations, and whether my hand is empty orholding a block.- Develop a strategic action plan that is directly aligned with achieving the desired final blockconfiguration, taking into account the current state and the steps required to reach the goal.- Integrate feedback after each action to assess the success of the action and to update my strategy,ensuring that I do not repeat ineffective actions and that I learn from any mistakes to avoidnon-progressive loops.- Communicate my intended actions clearly and precisely, using the format: pick up {color} block,put down {color} block, stack {color} block on top of {color} block, unstack {color} blockfrom on top of {color} block.- Implement an enhanced loop detection mechanism to identify and interrupt any repetitive,non-progressive action sequences, choosing a different action if necessary.- Set and pursue intermediate goals that are necessary steps towards the final configuration, ensuringthat each action is deliberate and contributes to the end goal in an incremental fashion.- Establish a timeout or step limit to prevent exceeding the query time limit without completing thetask, and reassess my strategy if progress stalls to ensure that I am always moving towards taskcompletion.- Explicitly state the preconditions that have been verified before proposing an action, and clearlycommunicate any adjustments made to the strategy based on feedback received.- Introduce a robust error handling strategy that allows for backtracking or reassessment of the plan",
  "To effectively arrange a set of blocks into the desired stacks, adhere to the following structuredapproach, which has been refined based on previous feedback and identified errors:": "1. **Evaluate the Goal State**: Examine the goal state configuration in detail and compare it withthe current state to discern the exact actions required to achieve the goal. Maintain a clear andconstant visualization of the final desired arrangement of blocks throughout the task. 2. **Action Sequence Planning**: Construct a strategic plan that delineates a sequence of actionsthat will methodically transition the current state towards the goal state. Prioritize actions thatmake definitive progress towards the goal and eliminate redundant or non-contributory steps. 3.**Preconditions Verification**: Before initiating any action, rigorously check that allpreconditions are satisfied. Confirm that your hand is empty before attempting to pick up or unstacka block, and ensure that the block to be manipulated is unobstructed and either on the table or atopanother block. 4. **Execute Actions**: Implement the necessary actions, strictly following the prescribed formatand constraints:- To pick up a block: pick up the {color} block.- To unstack a block: unstack the {color} block from on top of the {color} block.- To put down a block: put down the {color} block.- To stack a block: stack the {color} block on top of the {color} block. 5. **Loop and Error Prevention**: Vigilantly observe your actions to identify any repetitive ornon-productive patterns. Upon detecting a loop, promptly reassess and revise the action plan.Document past errors to prevent their recurrence. 6. **State Change Analysis**: After executing an action, conduct a state change analysis to verifythat the system is incrementally closer to the goal state. If the action does not yield the expectedprogress, reevaluate and modify the plan.",
  "Logistics Human promptScore = 0.083 (GPT-3.5-turbo-16k-0613 as the testing LLM)Score = 0.50 (GPT-4 as the testing LLM)": "You have to plan logistics to transport packages within cities via trucks and between cities viaairplanes. Locations within a city are directly connected (trucks can move between any two suchlocations), and so are the cities. In each city there is exactly one truck and each city has onelocation that serves as an airport. Here are the actions that can be performed:Load a package into a truck at a location.Load a package into an airplane at a location.Unload a package from a truck at a location.Unload a package from an airplane at a location.Drive a truck from one location to another location within a city.Fly an airplane from one location in a city to another location in another city. The following are the restrictions on the actions:A package can be loaded into a truck only if the package and the truck are in the same location.Once a package is loaded into a truck, the package is not at the location and is in the truck.A package can be loaded into an airplane only if the package and the airplane are in the samelocation.Once a package is loaded into an airplane, the package is not at the location and is in the airplane.A package can be unloaded from a truck only if the package is in the truck.Once a package is unloaded from a truck, the package is not in the truck and is at the location ofthe truck.A package can be unloaded from an airplane only if the package in the airplane. Once a package isunloaded from an airplane, the package is not in the airplane and is at the location of the airplane. A truck can be driven from one location to another if the truck is at the from-location and bothfrom-location and to-location are locations in the same city. Once a truck is driven from onelocation to another, it is not at the from-location and is at the to-location.An airplane can be flown from one city to another if the from-location and the to-location areairports and the airplane is at the from-location.Once an airplane is flown from one city to another the airplane is not at the from-location and is atthe to-location. Please learn from previous steps. Not purely repeat the actions but learn why the state changes orremains in a dead loop. Avoid being stuck in action loops. Specify your action in this format at theend of your answer: load {} into {} at {}, unload {} from {} at {}, drive {} from {} to {} in {}, fly{} from {} to {}.",
  "To optimize the logistics of transporting packages within cities using trucks and between citiesusing airplanes, follow these enhanced and precise guidelines:": "1. **Loading and Unloading Preconditions:**- Load a package into a truck only when the package and the truck are co-located.- Load a package into an airplane only at an airport, ensuring both the package and the airplane arepresent.- Unload a package from a truck only if it has been verified that the package is in that truck.- Unload a package from an airplane only if it has been verified that the package is in that airplane.",
  ". **State Changes:**- Reflect the packages new location as inside the vehicle upon loading and at the vehicles locationupon unloading": "4. **Action Format:**- Actions must be articulated as follows:- For loading/unloading: load {package} into {vehicle} at {location} or unload {package} from{vehicle} at {location}- For driving: drive {truck} from {from-location} to {to-location} in {city}- For flying: fly {airplane} from {from-airport} to {to-airport} 5. **Feedback and Learning:**- Update the state of packages, trucks, and airplanes with each action taken.- Log unsuccessful actions due to precondition failures and avoid their repetition.- Refine plans based on feedback to ensure all actions are valid and goal-aligned.",
  "To enhance logistics operations and avoid errors, follow these optimized steps:": "1. **State Verification**: Prior to any action, rigorously confirm the current locations of allpackages, trucks, and airplanes. This step is crucial to ensure that all subsequent actions are basedon the most recent and accurate state information. 2. **Action Execution**: Execute actions strictly adhering to these preconditions:- Load a package into a truck at a location only if the package and the truck are confirmed to be atthat location.- Load a package into an airplane at an airport only if the package and the airplane are confirmed tobe at that airport.- Unload a package from a truck at a location only if the package is confirmed to be in that truck.- Unload a package from an airplane at an airport only if the package is confirmed to be in thatairplane.- Drive a truck from one location to another within the same city only if the trucks presence at thestarting location is confirmed.- Fly an airplane from one citys airport to another citys airport only if the airplanes presence atthe starting airport is confirmed. 3. **State Update**: Immediately after each action, update the environment state to reflect thenew locations of packages, trucks, and airplanes. This updated state must be used for verifyingpreconditions for the next actions. 4. **Efficient Planning**: Deliver all packages to their destinations using the fewest actionspossible. Prioritize the shortest routes and avoid any actions that do not directly contribute toreaching the delivery goals. 5. **Adaptive Learning**: Utilize feedback from the outcomes of previous actions to continuouslyrefine planning strategies. Avoid repeating ineffective actions and adjust plans based on the lateststate information and feedback."
}