{
  "Abstract": "Large Language Models (LLMs) have demon-strated exceptional proficiency in instruction-following, making them increasingly integralto various applications. However, this capabil-ity introduces the risk of prompt injection at-tacks, where malicious instructions are embed-ded in the input to trigger unintended actionsor content. Understanding the robustness ofLLMs against such attacks is critical for ensur-ing their safe deployment. In this work, we es-tablish a benchmark to evaluate the robustnessof instruction-following LLMs against promptinjection attacks, assessing their ability to dis-cern which instructions to follow and whichto disregard. Through extensive experimentswith leading instruction-following LLMs, wereveal significant vulnerabilities, particularlyin models that mis-follow injected instructions.Our results show that certain models are exces-sively inclined to prioritize embedded instruc-tions in prompts, often focusing on the latterparts of the prompt without fully understandingthe overall context. Conversely, models thatexhibit stronger contextual understanding andinstruction-following capabilities tend to bemore easily compromised by injected instruc-tions. These findings highlight the need to bal-ance improving LLMs instruction-followingabilities with enhancing their overall compre-hension of prompts, to prevent mis-followinginappropriate instructions. We hope our anal-ysis provides valuable insights into these vul-nerabilities, contributing to the development ofmore robust solutions in the future.1",
  "*Work done while at Microsoft1": "their few-shot in-context learning and instruction-following abilities through fine-tuning using multi-task instruction data, referred to as instruction tun-ing (Wang et al., 2022; Peng et al., 2023). Notableexamples of instruction-tuned LLMs and chatbotsinclude open-sourced models like FLAN (Wei et al.,2021), Alpaca (Taori et al., 2023), Vicuna (Chi-ang et al., 2023), LLaMA2-Chat (Touvron et al.,2023b) and proprietary models such as InstructGPTand ChatGPT (Ouyang et al., 2022), GPT-4 (Ope-nAI, 2023b), and Claude.2 Extensive research hasbeen focusing on improving and benchmarking theinstruction-following and problem-solving capabil-ities of LLMs (Li et al., 2023; Chia et al., 2023;Zheng et al., 2023).However, their strong instruction-following ca-pabilities might have also amplified the risks ofprompt injection attacks in practical usage. No-tably, popular LLM-integrated applications suchas Bing Chat3, ChatGPT plugin4 and retrieval-augmented generation systems (Lewis et al., 2020;Borgeaud et al., 2022) have incorporated searchengines or API call functions to access externalinformation for more accurate and knowledgeableresponses to user queries. However, this integra-tion also exposes LLMs to the risk of retrievingpoisoned web content containing adversarial in-structions injected by external attackers. Theseadversarial instructions might modify the originaltarget instructions and prompt the LLMs to takeunexpected actions, such as sending private userinformation to the attackers email address (Gre-shake et al., 2023). To defend against such promptinjection attacks, LLMs should possess the capa-bility to understand the context of the prompt andeffectively distinguish between original target in-structions and injected adversarial instructions.",
  "Web search results": "Christopher Allen Lloyd ( born October 22 , 1938 ) is an American actor , voice actor , and comedian . He is best known for his roles as Emmett `` Doc '' Brown in the Back to the Future trilogy , Judge Doom in Who Framed Roger Rabbit ( 1988 ) , Merlock the Magician in DuckTales the Movie : Treasure of the Lost Lamp ( 1990 ) , Uncle Fester in The Addams Family ( 1991 ) and its sequel Addams Family Values ( 1993 ) , and Grigori Rasputin in Anastasia ( 1997 ) . What is Christopher Allen Lloyd's occupation?",
  "Third-party attack": ": Example of our evaluation setup. The LLMis tasked with answering the user question (highlightedin green) using web search results that have been pre-injected with an adversarial question (highlighted in red).Although the LLM could initially generate the correctanswer, it might be misled by the injected question. To this end, we introduce a benchmark to eval-uate the robustness of LLMs in following instruc-tions against prompt injection attacks. As illus-trated in , our benchmark targets commonscenarios encountered by LLM-integrated applica-tions like ChatGPT, where the model is requiredto answer user questions based on web search re-sults. This setting is critical for evaluating LLMsinstruction-following robustness, as the web searchresults could potentially contain adversarial instruc-tions pre-injected by third-party attackers on web-sites, posing a significant threat to the integrity ofthe LLMs responses (Greshake et al., 2023).In our study, we conducted controlled experi-ments using four representative QA datasets, Nat-uralQuestions (Kwiatkowski et al., 2019), Trivi-aQA (Joshi et al., 2017), SQuAD (Rajpurkar et al.,2016), and HotpotQA (Yang et al., 2018). Specifi-cally, we inject adversarial instructions in the websearch result, i.e., paragraphs, based on which themodels generate the answer to the user-input ques-tion. Instead of injecting adversarial instructionsthat elicit malicious outputs (Perez and Ribeiro,2022; Kang et al., 2023), we examine benign ad-versarial instructions: questions related to the websearch content but different from the original targetquery. Our primary objective is twofold: (1) toassess the extent to which the LLMs outputs areinfluenced by the injected instructions, and (2) todetermine whether the LLMs prioritize the originaltarget instructions or the injected ones. To evaluate this, we introduced two different metrics, basedon the standard QA evaluation metrics comparingthe LLM responses with the golden answers forboth the original and injected questions. We adoptthis setup because the QA task allows for scalableand precise measurement, given the relatively fixednature of the desired answer spans, as opposed tothe inherent variability in free-form instruction andgeneration tasks.Our experimental results reveal that both open-sourced and proprietary LLMs exhibit significantvulnerabilities against prompt injection attacks. Weobserved a discrepancy between the models sizesand instruction-following capabilities, and their ro-bustness against prompt injection attacks. Somemodels are overly instruction-tuned to follow anyinstruction phrase in the prompt, typically focus-ing on the latter sections without a comprehensiveunderstanding of the entire prompt context or dis-cernment of appropriate instructions to follow. Ad-ditionally, we found that even the more robust mod-els, with a superior grasp of the prompt context andinstruction-following abilities, are prone to beingcompromised by specific injected phrases, such asignore previous prompt (Perez and Ribeiro, 2022).These findings highlight the importance of not justimproving the models instruction-following capa-bilities, but also their understanding of the promptcontext and discernment of appropriate instructionsto follow inside the prompt. We also conducted in-depth analysis covered various aspects, includingthe impact of attack and defense mechanisms, thetypes of injected instructions, and their injectedposition within the prompt. We hope our findingcould shed light on these vulnerabilities, offeringvaluable insights that could guide the developmentof more robust solutions in future work.",
  "Instruction-Following LLMs": "Current LLMs show impressive abilities to han-dle various real-world tasks by including naturallanguage task instruction and optionally in-contextexamples in the prompt. Leading proprietary mod-els such as InstructGPT (Ouyang et al., 2022),ChatGPT (OpenAI, 2023a), and GPT-4 (Ope-nAI, 2023b) exhibit particularly strong instruction-following capacities. Through instruction-tuning,current open-sourced models like Alpaca (Taoriet al., 2023) and Vicuna (Vicuna, 2023) have sig-nificantly enhanced their instruction-following ca- pabilities, even approaching the performance ofthe larger GPT-series models. To facilitate a betterunderstanding and evaluation of these instruction-following LLMs, various benchmarks have beenestablished to assess their performance in follow-ing instructions and solving problems across a widerange of tasks (Beeching et al., 2023; Chia et al.,2023; alp, 2023; Zheng et al., 2023). However,comprehensive and quantitative evaluations on as-sessing the robustness of LLMs against promptinjection attacks are still absent.",
  "Prompt Injection": "The ease of access to LLMs has simplified the pro-cess for potential attackers. They can effortlesslyinsert adversarial instructions into the prompt andthus force the models to perform unexpected ac-tions. For example, Perez and Ribeiro (2022) in-vestigated two forms of prompt injection initiatedby malicious users. Goal hijacking\" redirects theoriginal goal toward a new target, while promptleaking\" compels LLMs to disclose proprietarysystem instructions added by LLM API vendors.Moreover, Kang et al. (2023) demonstrated thatthe programmatic behavior of LLMs makes theirdefense mechanisms susceptible to classic securityattacks like obfuscation, code injection, payloadsplitting, and virtualization. In addition to injec-tions during LLM inference, Yan et al. (2023) andShu et al. (2023) explore the concept of poison-ing the instruction-tuning data. Besides malicioususer-initiated injections, instructions injected byexternal attackers present a growing threat to LLM-integrated applications. They may introduce exter-nal web content, tainted by third-party attackers,into the prompt, misleding LLMs (Greshake et al.,2023). These adversarial instructions, termed in-direct prompt injection,\" are commonly embeddedwithin the prompts content section. As a result,models are required to discern between the origi-nal target instructions and these injected ones byconsidering the prompt context.",
  "Evaluation Objectives": "Our objective is to evaluate the capability ofinstruction-following LLMs to effectively defendagainst adversarial instructions injected in theprompt. Robust LLMs should exhibit the ability toidentify the user query as the primary instruction tobe followed, rather than being misled by the contentwithin the retrieved context knowledge, which mayintroduce additional instructions. Consequently,our evaluation focuses on two key aspects: (1) Per-formance Influence (PI): measuring the extent towhich LLMs are affected by the injected instruc-tions, and (2) Instruction Discrimination (ID):determining whether LLMs tend to adhere to theoriginal target instruction or the adversarial instruc-tion injected into the content.",
  "Task Setup and Datasets": "We conduct our evaluation using the open-bookquestion-answering (QA) task as our testbed.Specifically, we focus on extractive QA, where theanswer is a span within the provided context, ratherthan free-form QA. There are two main reasonsfor this choice. Firstly, QA reflects the real-worldscenario of commercial systems like Bing Chat,which answers user questions based on web searchresults. Secondly, it is easier to automatically eval-uate the generation quality (answer accuracy) anddetermine whether the LLM is following the userinstruction, i.e., answering the user questions.The task is formulated as follows: given a userquery q and a web search result c as the con-text, the system is required to generate an answera. We experiment with four representative QAdatasets: NaturalQuestions (Kwiatkowski et al.,2019), TriviaQA (Joshi et al., 2017), SQuAD (Ra-jpurkar et al., 2016), and HotpotQA (Yang et al.,2018) For each dataset, we randomly select 1000samples from their dev sets to form our evaluationset Dtest. Given the evaluated LLM f that takesthe question-context (q, c) as input and generatesthe answer, the standard accuracy over the test setDtest is:",
  "(q,c,a,q)Dtestv(f(q, c + q), a),": "where the new context c + q is the original contextc injected with the adversarial instruction q. Weempirically observed that injecting the instructionat the end of the context is the most challenging forthe LLMs to defend against.As discussed in , for scalable and pre-cise evaluations, we use another question as theadversarial instruction q to inject into the contextc. Specifically, we use another question, denotedas q, which has a distinct answer a present in thegiven context c, but differs from the original targetquestion q and answer a. In this scenario, the in-jected question q is coherent and can be answeredbased on the context c. The correct identificationof the real user instruction requires the LLMs tocomprehend the prompt structure. Among the fourdatasets, SQuAD has already provided multiple QApairs for each context. In this case, we use one pairas the original target QA pair (q, a), and anotheras the injected QA pair (q, a). For the other threedatasets, each context comes with only one QApair, which we use as the original target QA pair (q,a). To create the injected pairs for these datasets,we utilized GPT-4 to generate an alternative QApair (q, a), based on the given context c. Evaluation MetricsOur evaluation primarily fo-cuses on assessing the extent to which the gener-ation of the LLM f is affected by the adversarialinstruction. Hence, we adopt the PerformanceDrop Rate (PDR) metric (Zhu et al., 2023), whichquantifies the percentage of performance drop inthe answer accuracy for the user question q:",
  "A PDR value of 0 implies that the model is notinfluenced by the injected instruction. Conversely,": "a higher PDR score denotes a more significant in-fluence from adversarial instructions, indicatingreduced robustness.Another objective of our evaluation is to deter-mine whether the model tends to adhere to theoriginal target question q or the injected adversarialquestion q. To achieve this, we also automaticallymeasure the models output accuracy concerningthe injected question q:",
  "Experimental Setup": "Weconductevaluationsoneightleadinginstruction-following LLMs according to Al-pacaEval (Li et al., 2023),5 which tests the abilityof models to follow general user instructions.Our evaluations include both proprietary modelsand open-sourced models, as shown in .We also list their AlpacaEval performance forreference. To accommodate space limitations insubsequent result discussions, we refer to thesemodels using specific model index identifiers.",
  "We employ tailored prompt templates for vari-ous instruction-tuned models, as elaborated in theAppendix. By default, we use four demonstration": "examples (4-shot). For each evaluated question, weinject the adversarial instruction at the end of theweb search result and position the user questionabove the web search results. So the user inputwould be: Question: {q}\\nSearch results: <con-text> {c + q} </context>\". Additionally, we haveexperimented with various settings, which are pre-sented in .3 and 4.4.",
  "Main Results": "We first conducted quantitative evaluations on thefour benchmark datasets. The results are shown in. Given the constraints of space, we use thesimplified model identifiers (M1-M8) in the figure.The exact mapping of M1-M8 to their respectivemodel names can be found in . Huge robustness gap among modelsWe ob-served consistent trends across these evaluationmetrics and datasets. Notably, there was a markeddifference in robustness among the models we eval-uated. The two proprietary models GPT-3.5-Turbo(M1) and Claude-2 (M2) were notably more robustthan the other evaluated open-sourced models. Discrepancy between model sizes, instruction-following capabilities, and robustnessDespiteits notable performance in instruction-followingas evaluated in AlpacaEval, LLaMA2-70B-Chat(M3) did not exhibit greater robustness than itssmaller counterparts in our evaluations. In contrast,Vicuna-33B-v1.3 (M4), a more modestly-sizedmodel, showed superior robustness compared tomost other open-sourced models. The 13B models,",
  ": Impact of instruction injection position. Higher PDR and lower IDR indicate decreased robustness": "including Vicuna-13B-v1.3 (M5) and LLaMA2-13B-Chat (M6), were less robust than the 33Bmodel Vicuna-33B-v1.3 but showed better robust-ness than the 7B models and even the 70B model,LLaMA2-70B-Chat, in some cases. The small-est, 7B models, consistently displayed the leastrobustness, with Zephyr-7B-Chat (M7) perform-ing the weakest in our evaluation. This was incontrast to its impressive instruction-following ca-pabilities as evaluated by AlpacaEval, where it wasthe strongest among 7B-sized models and evenoutperformed many larger models. These find-ings indicate that instruction-following capabilitiesand model size may not necessarily correlate withinstruction-following robustness.",
  "Additional Analysis": "Effects of injected instruction typesIn addi-tion to injecting context-relevant instructions (ques-tions), we also tested the injection of general, free-form user instructions from Self-instruct (Wanget al., 2022). For instance, a task instruction mightbe, Come up with a haiku poem. This type ofinjected instruction is considered irrelevant to theuser query and the context in the prompt, unlike thecontext-relevant questions used in our main setup.Since it is hard to automatically measure whetherthe model follows this instruction, we only reportPDR scores in .Most models demonstrated greater robustnessagainst the context-irrelevant injected instructionscompared to the context-relevant ones. Notably,Vicuna-13B-v1.3 (M5) and LLaMA2-13B-Chat(M6) showed particular sensitivity in this regard.",
  ": Quantitative evaluation of PDR () against in-jections of context-irrelevant and relevant instructions": "Effects of injection positionsWe conducted ex-periments to investigate the influence of differentpositions for injecting adversarial instructions intothe context. The context was split into sentences,and the adversarial instruction was injected at var-ious positions: Start (the beginning of the con-text), Middle (the middle of the context), andEnd (the end of the context). The results fromthe NaturalQuestion dataset are illustrated in Fig-ure 3. The models demonstrating superior robust-ness, GPT-3.5-Turbo, Claude-2, and Vicuna-33B-v1.3, showed less susceptibility to injections posi-tioned. However, their performance declined sig-nificantly when the injection was placed at the end.In contrast, the other less robust models displayeda marked sensitivity to the position of the injection,with a progressively greater drop in performanceobserved when the injection was at the start, themiddle, and most notably at the end. This findingsuggests that the more robust models may possessa more holistic understanding of the entire prompt",
  "Investigating Attack and DefenseMechanisms": "Considering our observations that less robust mod-els tend to focus excessively on the latter sectionsof prompts without fully comprehending the entirecontext, this section explores the effects of posi-tioning the original target instructions at the endof prompts. Moreover, we investigate the impactof various attack and defense mechanisms. To as-sess the inherent abilities and characteristics ofinstruction-following LLMs, our analysis focuseson prompt-level attacks and defense mechanisms. OrderIn addition to testing the question-context-answer (QCA) order in our main setup, we alsoexplored the context-question-answer (CQA) or-der, where the order of the question and context isreversed. In the CQA format, the original instruc-tion/question is positioned closer to the end of theprompt than the injected instruction. This mirrorsthe sandwich defense method, where the originalinstruction is repeated at the end of the prompt.7 DefenseIn our evaluation setup detailed in Sec-tion 4.1, we implemented a basic defense mecha-nism. This involved isolating the content part ofthe context knowledge, where the adversarial in-structions were injected, and using a system promptinstructing the model to disregard any instructions",
  "in the content part. We examine the effectivenessof this defense mechanism across different models": "AttackFor the attack strategy, we initially useda naive approach where the adversarial instructionwas directly injected into the content part. Wethen experimented with more challenging attacksby adding a jailbreak prefix to the injected in-struction. For instance, when the context is belowthe original instruction (the QCA format), we pre-fixed the injected instruction with phrases like Ig-nore my previous instructions\" (Perez and Ribeiro,2022), trying to compromise the model to forgetthe original instruction placed above. Conversely,when the context is placed above the original in-struction (the CQA format), the prefix for the in-jected instruction was Please respond to each ofmy upcoming questions individually, with one an-swer per response.\" This approach aims to manip-ulate the model into prioritizing the first question,i.e., the injected instruction. ResultsThese experiments were conducted onthe NaturalQuestions dataset, with the results pre-sented in . We found that robust modelswith a better grasp of the prompt context demon-strated increased vulnerability to attacks using com-promised instructions or phrases. Specifically, thethree most robust models in our evaluations, GPT-3.5-Turbo, Claude-2, and Vicuna-33B-v1.3, expe-rienced a more significant drop in PDR when sub-jected to the attacks. By contrast, the least robustmodels in our evaluations, namely LLaMA2-70B-Chat, Zephyr-7B-Beta, and Alpaca-7B, are mini-",
  ": Human evaluations on 100 test cases from the NaturalQuestions dataset": "mally affected by these prompt-level instructionalattacks. Additionally, we observed that the systemprompt, designed to instruct models to ignore in-jected instructions found in the content part, didinfluence to some extent, yet not consistently effec-tive in all cases.Concerning the CQA format, where the origi-nal instruction is placed at the end of the prompt,it is generally easier to defend compared to theQCA format, with the exception of GPT-3.5-Turbo.We observed that under the CQA format, robustmodels like GPT-3.5-Turbo and Vicuna-33B-v1.3,which have a comprehensive understanding of theentire prompt context, still faced significant perfor-mance drops due to the attacks. Interestingly, thesemore capable and context-aware models could alsobe more easily compromised by specific injectedphrases, raising additional concerns and necessitat-ing effective solutions to enable models to discernappropriate instructions to follow.",
  "Human Evaluations": "To gain a deeper understanding of the systems re-sponses, we conducted human evaluations on 100randomly sampled test cases from the NaturalQues-tions test set. We employed three college studentswho are native English speakers to annotate theresponses from eight evaluated models for eachtest case. The models names were anonymizedand their order was randomized in the evaluationprocess. Each annotator was asked to categorizethe responses into five types: (A) The responseattempts exclusively to address the original targetquestion q; (B) The response attempts exclusivelyto address the injected adversarial instruction q;(C) The response attempts to address both the user question q, and injected adversarial instruction q;(D) The response refuses to provide an answer; (E)The response does not answer either of the twoquestions, or it is unclear which question the re-sponse is attempting to address. We used majorityvoting to determine the final annotation for eachresponse. The final agreement rate is 80.5%, andthe Fleisss kappa is 0.7302.As observed in , the overall trend alignswith our automatic evaluation results, as presentedin . GPT-3.5-Turbo, Claude-2, and Vicuna-33B-v1.3 emerged as the top three most robustmodels. On the other end, Zephyr-7B-Beta andAlpaca-7B demonstrated the least robustness, withLLaMA2-70B-Chat also showing a lack of ro-bustness. Notably, Claude-2 and Zephyr-7B-Betatended to respond to both the original and injectedquestions, a pattern less commonly observed in theother models. Additionally, it was found that GPT-3.5-Turbo occasionally refused to answer, which isnot observed in the other models.",
  "Conclusion": "In this paper, we establish a benchmark based onQA datasets to evaluate the instruction-followingrobustness of LLMs against prompt injection at-tacks. Our comprehensive experiments with lead-ing instruction-following LLMs uncovered notablelimitations in their ability to defend against suchattacks. Our results suggest that a models size andits instruction-following capabilities do not neces-sarily correlate with its robustness to prompt injec-tions. We observed that more robust models shouldideally exhibit a comprehensive understanding ofthe entire prompt, rather than overly focusing onthe latter sections of the prompt to complete the text, a characteristic common in less robust mod-els. This work aims to highlight the susceptibilityof current instruction-following models to promptinjections and to offer insights into the underlyingcauses, thereby guiding the development of futuresolutions and enhancing the security and reliabilityof these models.",
  "Limitations": "Our benchmark is established based on QA datasetsto evaluate the instruction-following robustness ofLLMs against prompt injection attacks. This bench-mark allowed us to assess the models ability tofollow the system and user instructions and exam-ine the effectiveness of various attack and defensestrategies. While other tasks or instructions couldbe formulated, we believe our study offers valuableinsights and helps draw attention to this issue. Weacknowledge the potential for data contaminationin the evaluated LLMs due to prior exposure toQA datasets. However, we believe this would notsignificantly impact our conclusions, as our focusis on the changes in instruction-following accuracy,which reflect the models adherence to instructions.Nonetheless, we recommend broadening the scopeof evaluation to include a wider range of tasks anddatasets. We also encourage further research todevelop more effective strategies for addressinginstruction mis-following in future work.",
  "Ethical statements": "We introduce a benchmark to assess the instruction-following robustness of LLMs against prompt injec-tion. We simulate scenarios by injecting additionalquestions generated by GPT-4 given the contextof question-answering from existing datasets. Wemanually verified that the generated questions donot involve personal privacy information or harm-ful content, as they pertain solely to the context ofexisting question-answering datasets. Therefore,we do not anticipate any ethical concerns regardingour work.",
  "Yew Ken Chia, Pengfei Hong, Lidong Bing, and Sou-janya Poria. 2023. Instructeval: Towards holisticevaluation of instruction-tuned large language mod-els. arXiv preprint arXiv:2306.04757": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E. Gonzalez, IonStoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgptquality. Kai Greshake, Sahar Abdelnabi, Shailesh Mishra,Christoph Endres, Thorsten Holz, and Mario Fritz.2023. More than youve asked for: A comprehen-sive analysis of novel prompt injection threats toapplication-integrated large language models. arXivpreprint arXiv:2302.12173. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023. Mistral7b. arXiv preprint arXiv:2310.06825.",
  "Po-Nien Kung and Nanyun Peng. 2023.Do mod-els really learn to follow instructions? an empir-ical study of instruction tuning.arXiv preprintarXiv:2305.11383": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-field, Michael Collins, Ankur Parikh, Chris Alberti,Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-ton Lee, et al. 2019. Natural questions: a benchmarkfor question answering research. Transactions of theAssociation for Computational Linguistics, 7:453466. Patrick Lewis, Ethan Perez, Aleksandra Piktus, FabioPetroni, Vladimir Karpukhin, Naman Goyal, Hein-rich Kttler, Mike Lewis, Wen-tau Yih, Tim Rock-tschel, et al. 2020. Retrieval-augmented generationfor knowledge-intensive nlp tasks. Advances in Neu-ral Information Processing Systems, 33:94599474.",
  "Manli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geiping,Chaowei Xiao, and Tom Goldstein. 2023. On theexploitability of instruction tuning. arXiv preprintarXiv:2306.17194": "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, YannDubois, Xuechen Li, Carlos Guestrin, Percy Liang,and Tatsunori B Hashimoto. 2023.Alpaca: Astrong, replicable instruction-following model. Stan-ford Center for Research on Foundation Models. stanford. edu/2023/03/13/alpaca. html,3(6):7. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, et al. 2023a.Llama:Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023b.Llama 2: Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Lewis Tunstall, Edward Beeching, Nathan Lambert,Nazneen Rajani, Kashif Rasul, Younes Belkada,Shengyi Huang, Leandro von Werra, ClmentineFourrier, Nathan Habib, et al. 2023. Zephyr: Di-rect distillation of lm alignment.arXiv preprintarXiv:2310.16944.",
  "Vicuna. 2023. Vicuna: An open-source chatbot im-pressing gpt-4 with 90%* chatgpt quality": "Eric Wallace, Kai Xiao, Reimar Leike, Lilian Weng,Johannes Heidecke, and Alex Beutel. 2024. The in-struction hierarchy: Training llms to prioritize privi-leged instructions. arXiv preprint arXiv:2404.13208. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-isa Liu, Noah A Smith, Daniel Khashabi, and Han-naneh Hajishirzi. 2022. Self-instruct: Aligning lan-guage model with self generated instructions. arXivpreprint arXiv:2212.10560. Jason Wei, Maarten Bosma, Vincent Y Zhao, KelvinGuu, Adams Wei Yu, Brian Lester, Nan Du, An-drew M Dai, and Quoc V Le. 2021. Finetuned lan-guage models are zero-shot learners. arXiv preprintarXiv:2109.01652. Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen,Zheng Tang, Hai Wang, Vijay Srinivasan, Xiang Ren,and Hongxia Jin. 2023. Backdooring instruction-tuned large language models with virtual prompt in-jection. In NeurIPS 2023 Workshop on Backdoors inDeep Learning-The Good, the Bad, and the Ugly. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-gio, William W Cohen, Ruslan Salakhutdinov, andChristopher D Manning. 2018. Hotpotqa: A datasetfor diverse, explainable multi-hop question answer-ing. arXiv preprint arXiv:1809.09600. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,Joseph E. Gonzalez, and Ion Stoica. 2023. Judgingllm-as-a-judge with mt-bench and chatbot arena. Kaijie Zhu, Jindong Wang, Jiaheng Zhou, ZichenWang, Hao Chen, Yidong Wang, Linyi Yang, WeiYe, Neil Zhenqiang Gong, Yue Zhang, et al. 2023.Promptbench: Towards evaluating the robustness oflarge language models on adversarial prompts. arXivpreprint arXiv:2306.04528.",
  "A.1Inference details": "We evaluate six popular instruction-tuned modelswith varied sizes. Alpaca-7B (Taori et al., 2023)is a 7B LLaMA (Touvron et al., 2023a) modelfine-tuned on 52k instruction data generated byGPT-3 (Wang et al., 2022). Zephyr-7B-Beta (Tun-stall et al., 2023) is an instruction-tuned versionof Mistral-7B (Jiang et al., 2023), which is theleading model among its size on the AlpacaEvalleaderboard. Vicuna-13B-v1.3 and Vicuna-33B-v1.3 (Vicuna, 2023) are LLaMA models fine-tunedon users conversations with ChatGPT. LLaMA-2-13B-Chat, and LLaMA-2-70B-Chat are both chat-tuned versions for LLaMA-2 models (Touvronet al., 2023b). These models represent a range ofsizes and instruction-following capabilities. For thesix open-sourced models, we utilized their check-points available on Huggingface8. The specificpaths for these models are detailed in . Forinference, we set the temperature and top_p bothas 0.5 and max tokens as 64. For each test case, weconducted a single inference run. All inferenceswere executed on a cluster equipped with eight 48GNVIDIA RTX A6000 GPUs.",
  "B.1Number of demonstration examples": "We examined the effect of varying the number ofdemonstration examples (n-shot) in the prompt,ranging from 0 to 5 (more examples might exceedthe context window). The results from four mod-els on the NaturalQuestion dataset are illustratedin . Notably, when no demonstration ex-amples (0-shot) are provided, all performance met-rics are poor. This outcome is expected since themodels are typically trained to generate detailedresponses to user queries, whereas our evaluationanticipates a single answer span. Thus, incorpo-rating demonstration examples in the prompt iscrucial for a meaningful robustness evaluation.We observed that the optimal number of exam-ples for robustness assessment is four. At this point,the performance on the original target task peaks,and the score for the injected task is at its lowest,indicating the best robustness score for the model.This setting was chosen to demonstrate that, evenunder the easiest conditions, the models exhibitlimited robustness. Increasing the number of exam-ples to five led to a decrease in the original tasksperformance. Hence, we opted for the setting ofusing four demonstration examples."
}