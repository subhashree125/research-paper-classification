{
  "Abstract": "The tool-use Large Language Models (LLMs)that integrate with external Python interpretershave signicantly enhanced mathematical rea-soning capabilities for open-source LLMs,while tool-free methods chose another track:augmenting math reasoning data. However,a great method to integrate the above two re-search paths and combine their advantages re-mains to be explored. In this work, we rstly in-clude new math questions via multi-perspectivedata augmenting methods and then synthesizecode-nested solutions to them. The open LLMs(e.g., Llama-2) are netuned on the augmenteddataset to get the resulting models, MuMath-Code (-Math-Code). During the inferencephase, our MuMath-Code generates code andinteracts with the external python interpreter toget the execution results. Therefore, MuMath-Code leverages the advantages of both the ex-ternal tool and data augmentation. To fullyleverage the advantages of our augmented data,we propose a two-stage training strategy: InStage-1, we netune Llama-2 on pure CoTdata to get an intermediate model, which thenis trained on the code-nested data in Stage-2 to get the resulting MuMath-Code.OurMuMath-Code-7B achieves 83.8% on GSM8Kand 52.4% on MATH, while MuMath-Code-70B model achieves new state-of-the-art per-formance among open methodsachieving90.7% on GSM8K and 55.1% on MATH. Ex-tensive experiments validate the combinationof tool use and data augmentation, as well asour two-stage training strategy. We releasethe proposed dataset along with the associatedcode for public use:",
  "MAmmoTHMathCoderToRAMuMath-Code": ": The comparison between our MuMath-Codeand other state-of-the-art tool-use LLMs. MuMath-Code exhibits a substantial improvement in perfor-mance on both GSM8K (Cobbe et al., 2021) andMATH (Hendrycks et al., 2021b), relative to the previ-ous approaches. Brown et al., 2020; Raffel et al., 2023) espe-cially the proprietary ones such as GPT-4 (Ope-nAI, 2023a) and Claude-3 (Anthropic, 2024) havedemonstrated superiority in a variety of tasks, e.g.,text classication (Wang et al., 2018; Devlin et al.,2019; Min et al., 2022; Jiang et al., 2023b), auto-mated coding (Chen et al., 2021; Luo et al., 2023b),instructions following (Longpre et al., 2023), andmath problem solving (Chowdhery et al., 2022;Lewkowycz et al., 2022; Anil et al., 2023; Fu et al.,2023a). Among these tasks, the capability to han- dle math problems stands as a typical and criti-cal criterion for the evaluation of different LLMs.However, a signicant performance disparity is ob-served between open-source LLMs, for instance,LLaMA (Touvron et al., 2023a,b), and their propri-etary counterparts, when it comes to mathematicalreasoning ability.In recent years, many scholarly publicationshave been directed towards improving the mathe-matical prociency of LLMs, which can be catego-rized into two distinct research trajectories: thosethat purely rely on natural language reasoning andthose that incorporate external tools. The formermethods are tool-free, mainly depends on data aug-mentation to enhance the models mathematicalreasoning capability, while the second trajectory(namely tool-use LLMs) are often coupled withexternal Python interpreters. From the perspectiveof knowledge distillation (Huang et al., 2022; Liet al., 2022; Magister et al., 2023; Ho et al., 2023;Fu et al., 2023b; Shridhar et al., 2023), both main-stream approaches transfer math reasoning abilitiesfrom the powerful teacher models (for instance,GPT-4) to the inferior open foundation models.The tool-free methods synthesize a large numberof new math problems and corresponding solutions,taking the original training math QA pairs as the ini-tial data seeds. Scaling law theoretically providesthe basis for the ongoing improvement of LLMsperformance by constantly incorporating new train-ing data. Representative approaches are RFT (Yuanet al., 2023), MetaMath (Yu et al., 2023), Wizard-Math (Luo et al., 2023a), MuggleMath (Li et al.,2023), MuMath (You et al., 2024), etc. As forthe second trajectory, code executors substantiallyenhance LLMs in particularly challenging com-putational and logical tasks, thereby alleviatingthe problem-solving burden on them. This tool-use category is exemplied by PAL (Gao et al.,2023), PoT (Chen et al., 2023a), MAmmoTH (Yueet al., 2023), ToRA (Gou et al., 2023) and Math-Coder (Wang et al., 2023).Although the aforementioned research pathshave been individually successful, to date, fewmethods have been developed that amalgamatetheir respective advantages. In this paper, we pro-pose a novel method that integrates tool usage withdata augmentation to synthesize a large amountof multi-perspective mathematical questions andsolutions (we employ the augmenting methods in-troduced in a previous work MuMath (You et al.,2024)). Specically, we utilize proprietary LLMs (like GPT-4) to generate Python code while synthe-sizing new solutions to math problems, and thenne-tune the open-source models (e.g., LLaMA)on the augmented dataset. The resulting model,MuMath-Code, is thus equipped with the abilityto write code for math problem solving. Duringthe inference phase, our MuMath-Code can gen-erates both CoT (Wei et al., 2022) reasoning textsand Python code blocks. These code blocks arethen extracted and executed by an external Pythoninterpreter, and the execution results are returnedto MuMath-Code for subsequent rounds of CoTreasoning or code generation until the nal resultis obtained or the maximum number of executionrounds is reached.The multi-perspective mathematical questionset comprises questions augmented via rephras-ing (Yu et al., 2023), alteration (Li et al., 2023;You et al., 2024), FOBAR (Jiang et al., 2023a),BF-Trans (You et al., 2024), besides those fromthe original training sets. Regarding the solutionsnested with Python code, we leverage a generalpattern like the ones used in ToRA (Gou et al.,2023) and MathCoder (Wang et al., 2023): CoT-PoT interleaving. However, we propose prex CoT,code debugging and pseudo-answer guidance lter-ing to improve the consistency and quality of ouraugmented solutions. The prex CoT is a thought-ful analysis in pure natural language before codegeneration, making the LLMs consider this anal-ysis while generating all the subsequent content,which thus are helpful for the models to learn thewhole solution. Besides, we prompt GPT-4 to de-bug and correct the inexecutable code when re-questing the solutions, and we keep the faulty codesince this process of verication and correction canhelp boost the models coding prociency. Further-more, for those synthesized questions via alteration,which lack ground truth answers as ltering guid-ance, we choose the majority-voting answers asthe pseudo-answers. This process can increase thecorrectness of the generated solutions and thus im-prove the data quality generally. We name the pro-posed dataset as MuMath-Code-Data and denoteit as D-code.Moreover, previous tool-use LLMs for math arederived by directly netuning on code-nested data,which thus fail to fully harness the intrinsic nat-ural language reasoning capability of the LLMsthemselves. Different from the other tool-use meth-ods, we design a two-stage training strategy tobetter combine the advantages of data augmenta- tion and external code execution. The rst stage isto enhance the models pure language mathemat-ical reasoning, where the largest (751K) datasetproposed in MuMath (here called MuMath-Dataand denoted as D) is utilized to netune LLaMA,and get an intermediate model, MuMath. In thesecond stage, we continue netuning MuMath onMuMath-Code-Data to equip the model with theability to write code for solving math problems.The resulting model, MuMath-Code, is thus canbe prompted to leverage the Python interpreter toexecute its generated code for securing the desir-able outputs at inference time.Our contributions are summarized as follows: We construct a multi-perspective augmenta-tion dataset with code-nested solutions formath problem solving, called MuMath-Code-Data.",
  "Tool-Free LLMs for Math": "Rejection Sampling-based Fine-Tuning (RFT, Yuanet al., 2023) only augments the solutions via re-jection sampling to collect a variety of differentreasoning paths. Since RFT does not introducenew math questions, the diversity of the augmenteddataset is quite low, which limits the performanceimprovement of the netuned models. With the aimof incorporating a broader spectrum of questions,MetaMath (Yu et al., 2023) employs rephrasing,Self-Verication (SV, Weng et al., 2023) and FO-BAR (Jiang et al., 2023a) to generate new questions.Ideally speaking, like the original questions, thereare also ground truth answers for ltering solutionsto these augmented questions. To bring in more di-verse data, WizardMath (Xu et al., 2023; Luo et al.,2023a) and MuggleMath (Li et al., 2023) chooseto create totally new questions via evolution or di-rectional modication (changing numbers, adding conditions, increasing complexity, etc.) based onthe seed questions. These altered questions haveno ground truth answers, thus lacking a criterion tolter their corresponding synthesized solutions.Furthermore, MuMath (You et al., 2024) lever-ages some of the aforementioned methods, andadditionally proposes BF-Trans and expression re-placement (etc.) to perform comprehensive aug-mentation, thus constructing a multi-perspectivemath question set with much greater diversity. Forimproving data quality, majority sampling servesas the ltering rule for the synthesized solutionsto those new questions without deterministicallyknown answers. Instead of solution ltering, a con-temporary work, Xwin-Math (Li et al., 2024), em-ploys verication with solution requesting duringquestion synthesis, thereby improving the solvabil-ity of the questions and the correctness of the an-swers. Since there is no restriction on the directionof question modication, Xwin-Math theoreticallyoffers a wider variety of diverse synthesized data.Balancing the efcacy and the ease of replication,in this paper the proposed MuMath-Code opts toemploy the question augmentation from MuMath,although it is orthogonal to any other augmentationmethods.Nevertheless, as probabilistic models, LLMs in-herently have limitations in logical reasoning andnumerical computation. Thus, to improve the accu-racy of mathematical problem-solving while rely-ing solely on the capabilities of LLMs necessitatesthe utilization of a substantially larger dataset com-pared to tool-use methods.",
  "Tool-Use LLMs for Math": "Another research trajectory highlights the synergybetween LLMs and external tools. Pioneering ef-forts along this include the Program-aided Lan-guage model (PAL, Gao et al., 2023) and Pro-gram of Thought (PoT, Chen et al., 2023a). More-over, MAmmoTH (Yue et al., 2023) integrates bothCoT and PoT in a coarse-grained fashion (eachsample corresponds to only one of these two pos-sible solution types), enabling exible inferencewhere the netuned models may adopt differentmethods for different questions. Different fromMAmmoTH, ToRA (Gou et al., 2023) interleavespython code blocks and natural language reason-ing parts over multiple turns for a same solution,which offers a more exible combination of CoTand PoT. However, neither MAmmoTH nor ToRAemploys query augmentation, thereby narrowing the range of math questions, which in effect, limitsthe problem-solving capabilities that can be ac-quired. Wang et al. propose a contemporaneouswork with ToRA, MathCoder (Wang et al., 2023),where each solution is also organized in an inter-leaved manner. Besides, they introduce interpola-tion problems to mitigate the disparity in difcultylevel between GSM8K (Cobbe et al., 2021) andMATH (Hendrycks et al., 2021c). Hence, like ourMuMath-Code, MathCoder is also an amalgama-tion of tool usage and math question augmentation,although the new questions it introduces are com-paratively narrow in scope and limited in diversity.Similar to ToRA and MathCoder, we also con-struct such solutions that intertwine Python codewith pure language reasoning text to adaptablycombine LLMs with external code executing tools.However, we propose prex CoT, code debug-ging, and pseudo-answer guidance ltering to fur-ther enrich the solutions and improve their cor-rectness. Additionally, different from MathCoder,the question augmentation we utilize are multi-perspective, thus offering greater diversity and ex-posing the model to a broader scope of novel ques-tions, thereby signicantly enhancing the modelsgeneralization capabilities.",
  "Methodology": "We employ the augmented questions from Mu-Math (You et al., 2024), detailed in Appendix A,and synthesize code-nested solutions to them. Tohelp the models better learn such solutions withmulti-turn code generation, code execution andpure natural language reasoning, we propose prexCoT, code debugging, and pseudo-answer guidanceltering to augment the quality of the synthetic data,as well as a two-stage training strategy. delineates the overall pipeline.",
  "MuMath-Code-Data": "To facilitate the interaction with the python inter-preter, we synthesize the code-nested solutions forthe models to learn, each consisting of multi-turncode generation, code execution and pure naturallanguage reasoning.Specically, for each question from Q, weprompt proprietary LLMs to request solutions eachwith at least one block of code, which is then ex-tracted and passed to the external interpreter forexecution. Every execution result is appended tothe preceding content, right after the corresponding code block. If the code execution fails, we appenda prompt to actively debug, using all the previouscontent as a whole new prompt to request the cor-rected code, which we then extract and executeagain. By iterating this process multiple times, weobtain a reasoning path comprising code, executionoutcomes and natural language analysis. This rea-soning path is similar to that of MathCoder (Wanget al., 2023) and ToRA (Gou et al., 2023), but thedifferences lie in the use of our proposed prexCoT, code debugging, and pseudo-answer guidanceltering, which will be elaborated on in this section.We marked MuMath-Data-Code as D-code. Prex CoTWe have observed that before gen-erating code, a thorough pure natural languageanalysis is helpful for the models performance.Therefore, we deliberately add a thoughtful CoTreasoning before code writing. The request promptused is Analyze the question; list some knowledgepoints related to the question and benecial forproblem solving. Code DebuggingSeveral research studies (Gouet al., 2023; An et al., 2024; Liao et al., 2024)have shown that the use of error correction andverication data can improve the mathematical rea-soning capabilities of LLMs. Therefore, we intro-duce an error correction process for our augmenteddataset. Specically, while constructing a solution,if the generated code fails to execute, we append aprompt The code above has encountered a prob-lem. Now point out its mistakes and then correctthem. for GPT-4 to debug the code and write newcode until the executable code is obtained, or themaximum number of requests is reached. The fail-ing code and error information are kept to equip thenetuned models with self-debugging ability (Chenet al., 2023b), and thus enhance their coding pro-ciency for solving math problems. Pseudo-AnswerGuidanceFilteringInMuMath-Data, we employ majority samplingto lter solutions.This provides us withpseudo-answers for the augmented questionscorresponding no reference answers, which canalso be employed for MuMath-Code-Data to selectsolutions. This approach improves the correctnessof the synthesized solutions, thereby leadingto an enhancement in the overall quality of theaugmented data.To sum up, we mark the i-th CoT (pure natu-ral language reasoning) part as ci; the i-th python",
  "[Output]": ": Illustration of our proposed method. The foundation model is rst trained through an initial stage, resultingin an intermediary model that possesses more powerful math reasoning capability. This intermediary model is thenfurther trained on the proposed dataset to learn code generation and tool interaction, leading to the nal model,MuMath-Code. code part is marked as pi, which always beginswith ```python and ends with ```; the i-th codeexecution output is denoted as oi, beginning with```output and ending with ```. To formalize, oneresulting solution s is dened as follows:",
  "t=1log Pxt|q, x<t; , (2)": "where the solution s = (x1, x2, ..., xl) contains ltokens, and is the model parameter.This training stage endows the models with afairly strong mathematical reasoning capability,which can be seen as an preliminary task for thesecond stage learning. Stage-2The second stage training is on MuMath-Code-Data, where the models concentrate on PoT-CoT interleaved data to learn how to interact withan external tool (i.e., the Python interpreter). Wemask the loss of the outputs from the code execu-tion, which should not be learned by the models.The learning target is:",
  "(3)": "where pn = . The training process at Stage-2 isconsistent with the inference, so we do not needto consider the issue of catastrophic forgetting (re-garding the natural language reasoning in Stage-1).At inference time, after being given a mathematicalproblem, the netuned model needs to generatecode for problem solving, and then an external in-terpreter executes the code and returns the resultfor the model to continue generating. Therefore,Stage-2 training simulates the above inference pro-cess by masking out the losses of the executionoutputs.",
  "DatasetsOur seed datasets for synthesis arethe training sets of two popular math reasoning": "benchmarks: GSM8K (Cobbe et al., 2021) andMATH (Hendrycks et al., 2021b). GSM8K con-tains elementary school math problems, comprising7,473 training instances and 1,319 test instances;while MATH encompasses math competition prob-lems at the high school level with 7,500 trainingsamples and 5,000 for test.We take the MuMath (You et al., 2024) dataset(750K) as our D for Stage-1 training, and theMuMath augmented question set Q are utilized toconstruct D-code for Stage-2; in Q, we request15 solutions for each question that originates fromGSM8K and 30 for MATH-related ones, and thenperform ltering to get 30K samples for each ques-tion subset, making 600K in total.For evaluation, we select GSM8K and MATHtest sets as the in-domain benchmarks, while GSM-Hard (Gao et al., 2023), SVAMP (Patel et al., 2021),TabMWP (Lu et al., 2023), ASDIV (Miao et al.,2020) and SingleEQ, SingleOP, AddSub, and Mul-tiArith (Koncel-Kedziorski et al., 2016) as the out-of-domain ones (to see the generalization ability ofour models). Implementation DetailsOur study utilizesLLaMA-2 (7B, 13B and 70B) (Touvron et al.,2023b) and CodeLlama (7B, 13B, 34B, and70B) (Rozire et al., 2023) as the foundation mod-els for full-parameter netuning, correspondingto MuMath-Code-L and MuMath-Code-CL as theresulting models. We employ AdamW as the op-timizer and a cosine learning rate scheduler witha 0.03 warmup ratio. Across all the models andboth stages, we train 3 epochs with a 128 globalbatch size. All the models except for LLaMA-70Band CodeLlama-70B are trained using the Deep-speed framework, while those two 70B models aretrained using Megatron for the sake of speed. Thehardware we use are NVIDIA H800 GPUs.",
  "Comparison Results": "As shown in , the comparison experiment ofour models with the current state-of-the-art demon-strates that our approach consistently achieves su-perior performance across all scales of open-sourcemodels on all the datasets. Notably, our MuMath-Code-L 7B model has attained a test accuracy of83.8 on the GSM8K, and MuMath-Code-CL 7Bhas reached a score of 52.4 on MATH. These out-comes surpass many 70B open-source baselinesand even some proprietary LLMs. Additionally,our MuMath-Code-CL 34B and 70B achieve 55.0+ on MATH, two impressive results considering thatthey are accomplished by leveraging data augmen-tation techniques based on the original training setwithout the incorporation of extensive additionalmathematical corpora for pre-training.There are some noteworthy ndings from the ex-perimental statistics presented in the table, suchas the performance of MuMath-Code-CL 13Bon MATH, registering at 53.1, which is onlymarginally higher than that of MuMath-Code-CL7B, which stands at 52.4. Moreover, the MuMath-Code-CL 34Bs performance on MATH, scoring at55.0, is very close to that of the MuMath-Code-CL70B, which records a score of 55.1. We specu-late that this may be attributed to the phenomenonwhere, beyond a certain threshold of data volume,the advantages conferred by increased model sizemay be diminished or even offset by the benetsderived from the expanded dataset. Additionally,variations in the training frameworks may also con-tribute to the observed discrepancy between theperformances of MuMath-Code-CL 34B and 70B.Moreover, despite without netuning on GSM-Hard, SVAMP, TabMWP, ASDiv and MAWPS(MAWPS results are averaged over Singleeq, Sin-gleop, Addsub, and MultArith, following ToRA),MuMath-Code still signicantly outperforms theother sate-of-the-art open-source methods, whichdemonstrates the strong generalizability of ourmodels.",
  "Effectiveness of the Two-Stage TrainingStrategy": "MuMath-Code is derived from a two-stage train-ing process that enhances the models pure naturallanguage reasoning capabilities and the ability togenerate code and interact with external tools. Inthis section, we validate the efcacy of this bifur-cated training strategy. Unless otherwise specied,all ablation experiments presented in this paper areconducted on 7B models, for the sake of time ef-ciency. We have designed a comparative evaluationof model performances for two-stage and one-stagetraining strategies. The two-stage training referredto here is as described in .2, which in-volves continuing training from the checkpointsof the rst stage (the MuMath models). The one-stage training, directly applies the second stage oftraining on the base models. illustrates theperformance comparison of models derived fromboth strategies across different data volumes, re-vealing that training solely on D-code is worse than",
  "tool-free open LLMs": "7BLLaMA-2 (Touvron et al., 2023b)13.34.17.838.031.150.760.9LLaMA-2 SFT (Touvron et al., 2023b)41.37.216.131.927.847.460.0WizardMath (Luo et al., 2023a)54.910.720.657.338.159.173.7MetaMath (Yu et al., 2023)66.519.8-----MuggleMath (Li et al., 2023)68.4------MuMath (You et al., 2024)70.922.0-76.8-93.687.3 13BLLaMA-2 (Touvron et al., 2023b)24.36.313.643.139.556.370.4LLaMA-2 SFT (Touvron et al., 2023b)51.19.222.346.335.858.675.0WizardMath (Luo et al., 2023a)63.914.028.464.346.765.879.7MetaMath (Yu et al., 2023)72.322.4-----MuggleMath (Li et al., 2023)74------MuMath (You et al., 2024)76.425.3----- 70BLLaMA-2 (Touvron et al., 2023b)57.814.436.073.657.576.092.4LLaMA-2 SFT (Touvron et al., 2023b)69.314.939.064.053.071.384.8WizardMath (Luo et al., 2023a)81.622.750.380.049.876.286.2MetaMath(Yu et al., 2023)82.326.6-----MuggleMath (Li et al., 2023)82.3------MuMath (You et al., 2024)84.532.2-87.6-96.692.0",
  ": Ablation studies show both prex CoT andcode debugging have played a role in enhancing theperformance of our MuMath-Code models": "the two-stage training. Furthermore, by mergingthe training data from both stages into a singledataset for one-stage training, we observe that theoutcomes are still not as favorable as those obtainedfrom two separate training stages.To further validate the effectiveness of our two-stage training strategy, we select MetaMath (Yuet al., 2023) and Xwin-Math (Team, 2023) 7Bmodels as the initial checkpoints for Stage-2 train-ing, emulating the scenario where relevant datasetswere employed during the rst stage (Consideringthe unavailability of the most recent models anddataset proposed in (Li et al., 2024), we opt toutilize Xwin-Math-7B-V1.0 detailed in the corre-sponding GitHub repository). illustratesthat models ne-tuned from MetaMath and Xwin-Math checkpoints on D-code (two-stage) outper-form the one directly trained from Llama (single-stage), verifying the efcacy and transferability ofa two-stage training strategy as well as the compat-ibility of our D-code with different rst-stage CoTdatasets.",
  ": Pseudo-answer guidance ltering consistentlyimproves the performances of the models netuned ondatasets of various sizes": "D-code via two distinct approaches: the rst ap-proach involves the removal of the prex CoT,thereby eliminating the detailed preliminary anal-ysis and directly begining with code writing; thesecond approach consists of retaining only the naland successfully executed code and omitting allthe other inexecutable code before as well as thecorresponding debugging process. The results ofthis ablation study are presented in , whichdemonstrates that the exclusion of either the prexCoT or code debugging leads to a decline in themodels test accuracy. This emphatically under-scores the signicance of a thorough analysis priorto code writing and the code mistake correctionprocess for the models learning.Moreover, we conduct another ablation experi-ment on pseudo-answer guidance ltering. In Sec-tion 3.1, we note that pseudo-answers are suitablefor synthetic questions that lack a denitive cor-rect answer, namely those in Qalter and Qreplace.In MuMath, majority voting is utilized to assignpseudo-answers to these questions. These pseudo-answers are then also employed to lter the data forD-code in the second training stage. As illustratedin , ne-tuning the model with data lteredthrough this pseudo-answer technique proves tobe more benecial than solutions obtained throughdirectly random sampling. This trend holds acrossdata volumes ranging from 30K to 180K.",
  "MuMath-Code-CL39.232.8": ": Comparison results on MMLU-Math andAQuA-RAT test sets. The results indicate our MuMath-Code shows good transferability to other math-relatedtasks (note that we did not specically netune on suchdata). RAT (Ling et al., 2017), two math-related multi-choice datasets. illustrates that althoughwithout specically being netuned for the multi-choice math task, MuMath-Code still shows goodperformance on such data, demonstrating the trans-ferability of our model to other math-related tasksfor its innerly improved math reasoning capability.",
  "Conclusion": "In this paper, we propose a multi-perspectiveand code integrated math reasoning dataset calledMuMath-Code-Data, where each solution containsmulti-turn code generation, code execution andpure natural language analysis (CoT). Througha two-stage training strategy, our MuMath-Codemodels outperforms the state-of-the-art open meth-ods and even some powerful proprietary onesacross different scales on the in-domain reason-ing datasets (i.e., GSM8K and MATH) as well asthose out-of-domain ones. Additionally, ablationstudies demonstrates the effectiveness of our threenovel methods for the data synthesis: prex CoT,code debugging and pseudo-answer guidance lter-ing. Our work represents a new attempt at integrat-ing mathematical question augmentation (tool-free)with code generation and execution (tool-use) toenhance the mathematical reasoning capabilitiesof LLMs, and we hope it can inspire subsequentresearch endeavors.",
  "Limitations": "Our work is limited by the capabilities of the LLMsused to synthesize new math reasoning data. More-over, currently our method is not capable of solvingproof problems, as we have not trained on relevantdata. We plan to study this area in the next step tofurther enhance the capability of our models. This work was supported by National KeyR&DProgramofChinaunderGrantNo.2020AAA0104500, the National Natural ScienceFoundation of China under Grant No. 42476194,the Natural Science Foundation of ShandongProvince under Grants No. ZR2020MF131 and No.ZR2021ZD19, and Project of Associative Train-ing of Ocean University of China under Grant No.202265007.",
  "Anthropic. 2024. The claude 3 model family: Opus, son-net, haiku": "Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, T. J. Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, ClemensWinter, Christopher Hesse, Mark Chen, Eric Sigler,Mateusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020.Language models are few-shot learners.ArXiv,abs/2005.14165. Mark Chen, Jerry Tworek, Heewoo Jun, QimingYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-plan, Harri Edwards, Yuri Burda, Nicholas Joseph,Greg Brockman, Alex Ray, Raul Puri, GretchenKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-try, Pamela Mishkin, Brooke Chan, Scott Gray,Nick Ryder, Mikhail Pavlov, Alethea Power, LukaszKaiser, Mohammad Bavarian, Clemens Winter,Philippe Tillet, Felipe Petroski Such, Dave Cum-mings, Matthias Plappert, Fotios Chantzis, Eliza-beth Barnes, Ariel Herbert-Voss, William HebgenGuss, Alex Nichol, Alex Paino, Nikolas Tezak, JieTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,William Saunders, Christopher Hesse, Andrew N.Carr, Jan Leike, Josh Achiam, Vedant Misra, EvanMorikawa, Alec Radford, Matthew Knight, MilesBrundage, Mira Murati, Katie Mayer, Peter Welinder,Bob McGrew, Dario Amodei, Sam McCandlish, IlyaSutskever, and Wojciech Zaremba. 2021. Evaluatinglarge language models trained on code.",
  "Xinyun Chen, Maxwell Lin, Nathanael Schrli, andDenny Zhou. 2023b. Teaching large language mod-els to self-debug": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts,Paul Barham, Hyung Won Chung, Charles Sutton,Sebastian Gehrmann, Parker Schuh, Kensen Shi,Sasha Tsvyashchenko, Joshua Maynez, AbhishekRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-odkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, JacobAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,Toju Duke, Anselm Levskaya, Sanjay Ghemawat,Sunipa Dev, Henryk Michalewski, Xavier Garcia,Vedant Misra, Kevin Robinson, Liam Fedus, DennyZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,Barret Zoph, Alexander Spiridonov, Ryan Sepassi,David Dohan, Shivani Agrawal, Mark Omernick, An-drew M. Dai, Thanumalayan Sankaranarayana Pil-lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,Rewon Child, Oleksandr Polozov, Katherine Lee,Zongwei Zhou, Xuezhi Wang, Brennan Saeta, MarkDiaz, Orhan Firat, Michele Catasta, Jason Wei, KathyMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,and Noah Fiedel. 2022. Palm: Scaling language mod-eling with pathways. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, ReiichiroNakano, et al. 2021. Training veriers to solve mathword problems. arXiv preprint arXiv:2110.14168. Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics.",
  "Weisen Jiang, Han Shi, Longhui Yu, Zhengying Liu,Yu Zhang, Zhenguo Li, and James T. Kwok. 2023a.Forward-backward reasoning in large language mod-els for mathematical verication": "Weisen Jiang, Yu Zhang, and James Kwok. 2023b. Ef-fective structured prompting by meta-learning andrepresentative verbalizer. In Proceedings of the 40thInternational Conference on Machine Learning, vol-ume 202 of Proceedings of Machine Learning Re-search, pages 1518615199. PMLR. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, NateKushman, and Hannaneh Hajishirzi. 2016. MAWPS:A math word problem repository. In Proceedings ofthe 2016 Conference of the North American Chapterof the Association for Computational Linguistics: Hu-man Language Technologies, pages 11521157, SanDiego, California. Association for ComputationalLinguistics. Aitor Lewkowycz, Anders Andreassen, David Dohan,Ethan Dyer, Henryk Michalewski, Vinay Ramasesh,Ambrose Slone, Cem Anil, Imanol Schlag, TheoGutman-Solo, et al. 2022. Solving quantitative rea-soning problems with language models. Advancesin Neural Information Processing Systems, 35:38433857. Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nan-ning Zheng, Han Hu, Zheng Zhang, and HouwenPeng. 2024. Common 7b language models alreadypossess strong math capabilities.arXiv preprintarXiv:2403.04706. Chengpeng Li, Zheng Yuan, Hongyi Yuan, GuantingDong, Keming Lu, Jiancan Wu, Chuanqi Tan, XiangWang, and Chang Zhou. 2023. Query and responseaugmentation cannot help out-of-domain math rea-soning generalization. Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen,Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian,Baolin Peng, Yi Mao, Wenhu Chen, and XifengYan. 2022. Explanations from large language modelsmake small reasoners better.",
  "Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-som. 2017. Program induction by rationale genera-tion : Learning to solve and explain algebraic wordproblems": "Shayne Longpre, Le Hou, Tu Vu, Albert Webson,Hyung Won Chung, Yi Tay, Denny Zhou, Quoc VLe, Barret Zoph, Jason Wei, et al. 2023. The ancollection: Designing data and methods for effectiveinstruction tuning. arXiv preprint arXiv:2301.13688. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu,Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark,and Ashwin Kalyan. 2023. Dynamic prompt learningvia policy gradient for semi-structured mathematicalreasoning. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-guang Lou, Chongyang Tao, Xiubo Geng, QingweiLin, Shifeng Chen, and Dongmei Zhang. 2023a. Wiz-ardmath: Empowering mathematical reasoning forlarge language models via reinforced evol-instruct. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, XiuboGeng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qing-wei Lin, and Daxin Jiang. 2023b.Wizardcoder:Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568.",
  "Lucie Charlotte Magister, Jonathan Mallinson, JakubAdamek, Eric Malmi, and Aliaksei Severyn. 2023.Teaching small language models to reason": "Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.2020. A diverse corpus for evaluating and developingEnglish math word problem solvers. In Proceedingsof the 58th Annual Meeting of the Association forComputational Linguistics, pages 975984, Online.Association for Computational Linguistics. Sewon Min, Mike Lewis, Luke Zettlemoyer, and Han-naneh Hajishirzi. 2022. MetaICL: Learning to learnin context. In Proceedings of the 2022 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 27912809, Seattle, United States.Association for Computational Linguistics.",
  "A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, , andI. Sutskever. 2019. Language models are unsuper-vised multitask learners. Technical Report": "Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J. Liu. 2023. Exploring the limitsof transfer learning with a unied text-to-text trans-former. Baptiste Rozire, Jonas Gehring, Fabian Gloeckle,Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, YossiAdi, Jingyu Liu, Tal Remez, Jrmy Rapin, ArtyomKozhevnikov, Ivan Evtimov, Joanna Bitton, ManishBhatt, Cristian Canton Ferrer, Aaron Grattaori, Wen-han Xiong, Alexandre Dfossez, Jade Copet, FaisalAzhar, Hugo Touvron, Louis Martin, Nicolas Usunier,Thomas Scialom, and Gabriel Synnaeve. 2023. Codellama: Open foundation models for code. Kumar Shridhar, Alessandro Stolfo, and MrinmayaSachan. 2023. Distilling reasoning capabilities intosmaller language models. In Findings of the Asso-ciation for Computational Linguistics: ACL 2023,pages 70597073, Toronto, Canada. Association forComputational Linguistics.",
  "Xwin-Math Team. 2023. Xwin-math": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Aurelien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. 2023a. Llama: Openand efcient foundation language models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023b. Llama 2: Open foundation andne-tuned chat models. Alex Wang, Amanpreet Singh, Julian Michael, FelixHill, Omer Levy, and Samuel R Bowman. 2018.Glue: A multi-task benchmark and analysis platformfor natural language understanding. arXiv preprintarXiv:1804.07461. Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, SichunLuo, Weikang Shi, Renrui Zhang, Linqi Song,Mingjie Zhan, and Hongsheng Li. 2023. Mathcoder:Seamless code integration in llms for enhanced math-ematical reasoning.",
  "Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,Pu Zhao, Jiazhan Feng, Chongyang Tao, and DaxinJiang. 2023. Wizardlm: Empowering large languagemodels to follow complex instructions": "Weihao You, Shuo Yin, Xudong Zhao, Zhilong Ji, Guo-qiang Zhong, and Jinfeng Bai. 2024. MuMath: Multi-perspective data augmentation for mathematical rea-soning in large language models. In Findings of theAssociation for Computational Linguistics: NAACL2024, pages 29322958, Mexico City, Mexico. Asso-ciation for Computational Linguistics. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu,Zhengying Liu, Yu Zhang, James T. Kwok, ZhenguoLi, Adrian Weller, and Weiyang Liu. 2023. Meta-math: Bootstrap your own mathematical questionsfor large language models.",
  "A.1MuMath Augmented Questions": "Theoriginalquestionsfromthetrainingsets of GSM8K (Cobbe et al., 2021) andMATH (Hendrycks et al., 2021b) are taken asthe seed question set Qoriginal.The questionaugmenting methods employed in MuMath areconducted on this seed set, which are brieyconcluded as follows: (1) RephrasingRewrite a text while keeping theoriginal meaning unchanged. Based on the fact thata rephrased question holds the same meaning asthe original one, the nal answer of it should alsobe the same. We denote the rephrased question setas Qrephrase. (2) Question AlterationThere are ve mannersto alter the original questions, like changing num-bers and adding more conditions, concluded inMuggleMath (Li et al., 2023). The resultant ques-tion set created via alteration is referred to asQalter = Qalter1 Qalter2 Qalter3 Qalter4 Qalter5. Besides, Expression Replacement, pro-posed in MuMath, rstly get the expressions of thesolution to an original question, then change thecalculation operators within them. Based on thechanged expressions, a new question is asked togenerate. Qreplace represents the question set pro-duced by this augmentation technique. Note thatQalter and Qreplace correspond no denitely cor-rect answers due to modications in the questionsintrinsic meanings. (3) FOBARFollowing Jiang et al. (2023a), wemask a certain condition in an initial question bysubstituting it with X\", and meanwhile give theanswer to the original question as a new condition,thereby creating a reverse question that seeks todetermine the value of the unknown X. Qfobar isutilized to mark the FOBAR question set. (4) BF-TransBackward-Forward Transforma-tion (BF-Trans), proposed in MuMath, aims toconstruct such backward questions that can be an-swered through direct reasoning, bypassing the ne-cessity of solving equations to nd the unknownvariables (thus resemble the data sampled fromthe original distribution). For a certain question-answer pair, BF-Trans rstly utilize FOBAR totransform the original question into a backwardone; secondly, we rephrase the FOBAR question into a new form where the masked value is re-quested directly instead of employing an unknownvariable X, resulting in a secondary forward ques-tion which we called BF-Trans question. The setof these BF-Trans questions is marked as Qbf.To sum up, all the 10 aforementioned subsets(5 in Qalter) constitute the resulting question setQ = Qoriginal Qrephrase Qalter Qreplace Qfobar Qbf. Based on Q, we generate 2 datasetscalled MuMath-Data and MuMath-Code-Data,emphasizing pure natural language mathematicalreasoning and tool interaction via code generation,respectively.",
  "MuMath-Data (denoted as D) is just the largestdataset from MuMath, which contains about 750Ksamples with pure CoT reasoning solutions to ques-tions in Q": "Majority SamplingAs is introduced in the paperof MuMath, for Qalter and Qreplace whose eachquestion has no reference answer, majority sam-pling is utilized to lter all the randomly gener-ated solutions and only those solutions with themajority answers are kept. In other words, eachmajority answer serves as a pseudo-answer to thecorresponding question.",
  "CMuMath-Code-Data": "In D-code, the libraries such as sympy, numpy,scipy and cvxpy are frequently used, and the num-ber of code lines for each problem ranges from 9to 119 (max rounds).As for those synthesized questions withoutground truth answers, there are inevitably wrongsolutions, though we perform majority voting toenhance the data quality. We have conducted qual-ity evaluation on the synthesized dataset, wherewe randomly sample 100 QA pairs for GSM8K-augmented dataset (hard to review MATHs) andafter manually reviewing we nd 95 of them arecorrect (quite good).",
  "from sympy.ntheory.modular import solve_congruencedef smallest_number():congruences = [(1, 4), (1, 3), (2, 5)]result = solve_congruence(*congruences)return result": "result = smallest_number()print(result)``````outputValueError: 1 is not an integer```The code above has encountered a problem. Now point out its mistakes and then correct them.Mistake:The error is in the format of the input to the function solve_congruence. The function is expectingtuples of the format (remainder, modulus), but the inputs need to be integers, not strings.",
  "(b) Test on MATH": ": Scaling all the subsets of MuMath-Code-Data. Initially, the model has already been netuned on MuMath-Data (thus two-stage training results). It is observable that the curves show very similar trends to those in . number of samples. The base model is LLaMA 7B and it is directly trained on the subsets of D-code(single-stage training). It is evident that with the increase in data volume, all subsets continuouslycontribute to the enhancement of the models performance, and the curves still do not show saturation.This indicates that employing our methodology allows for the continued addition of data to further improvethe LLMs mathematical reasoning capabilities. For the two-stage scenario where the initial model is anintermediate checkpoint from Stage-1, please see ."
}