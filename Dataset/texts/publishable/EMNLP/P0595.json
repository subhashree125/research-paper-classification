{
  "Abstract": "We find arithmetic ability resides within a lim-ited number of attention heads, with each headspecializing in distinct operations. To delveinto the reason, we introduce the ComparativeNeuron Analysis (CNA) method, which iden-tifies an internal logic chain consisting of fourdistinct stages from input to prediction: featureenhancing with shallow FFN neurons, featuretransferring by shallow attention layers, fea-ture predicting by arithmetic heads, and pre-diction enhancing among deep FFN neurons.Moreover, we identify the human-interpretableFFN neurons within both feature-enhancingand feature-predicting stages. These findingslead us to investigate the mechanism of LoRA,revealing that it enhances prediction probabil-ities by amplifying the coefficient scores ofFFN neurons related to predictions. Finally, weapply our method in model pruning for arith-metic tasks and model editing for reducing gen-der bias. Code is on",
  "Introduction": "Arithmetic ability is a crucial foundational skillof large language models (LLMs) (Brown et al.,2020; Ouyang et al., 2022; Chowdhery et al., 2023),contributing significantly to reasoning (Wei et al.,2022; Kojima et al., 2022) and mathematical tasks(Peng et al., 2021; Azerbayev et al., 2023). Whileexisting studies (Quirke et al., 2023; Zhang et al.,2023; Stolfo et al., 2023) have made significantbreakthroughs in understanding arithmetic tasks,the exact mechanism still remains elusive. Zhanget al. (2023) find that only a few attention heads sig-nificantly impact arithmetic performance, but theydo not elaborate on the mechanisms of these headsor how they influence FFN layers. Stolfo et al.(2023) intervene the hidden states and find the infor-mation flow from number and operation positionsto the last position. However, they do not locate theimportant attention heads (proved to store different abilities (Olsson et al., 2022; Gould et al., 2023))and FFN neurons (proved to store knowledge (Daiet al., 2021; Meng et al., 2022a)). Despite thechallenge of pinpointing important FFN neuronsamong tens of thousands of nodes, many studies(Gurnee et al., 2023; Lieberum et al., 2023; Nandaet al., 2023) emphasize that considering FFN neu-rons as fundamental units is crucial for better un-derstanding FFN layers. Furthermore, as modelediting typically occurs at the neuron level (Daiet al., 2021; Geva et al., 2022), it remains unclearhow to effectively leverage the explanations due tothe uncertainty surrounding the precise locationsof important parameters.",
  ": Four distinct stages in the internal logic chainfrom the inputs \"3+5=\" to the final prediction \"8\"": "In this study, we take attention heads and FFNneurons as fundamental units, and explore the ex-act parameters store the arithmetic ability for dif-ferent operations. We observe that only a minorityof heads play significant roles in arithmetic tasks,which we refer to as \"arithmetic heads\". Throughexperiments involving 1-digit to 3-digit operations,as well as ablation studies comparing \"change-one\" cases (e.g., 15+37=52) with \"memorize\" cases (e.g.,15+32=47), we find critical memorization of 1-digitoperations is lost when these heads are intervened.To explore the underlying mechanisms of thisphenomenon, we propose the Comparative Neu-ron Analysis (CNA) method, which compares thechange of neurons between the original modeland the intervened model for the same case. Weconstruct the internal logic chain by identifyingfour distinct stages that span from inputs to pre-diction, as depicted in . During the fea-ture enhancing stage, hidden-interpretable featuresare extracted from shallow FFN neurons. Subse-quently, in the feature transferring stage, shallowattention layers convert these features into directly-interpretable features and then transfer them to thelast position. In the feature predicting stage, thearithmetic heads play critical roles, activating deepFFN neurons related to the final prediction. Fi-nally, a prediction enhancing stage exists amongdeep FFN neurons. Lower FFN neurons activateupper FFN neurons, while both of them enhancethe probability of the final prediction.Based on this analysis, we investigate the mech-anism of LoRA (Hu et al., 2021). We train a to-tal of 32 models on a 2-digit arithmetic dataset,with each model integrating LoRA on one atten-tion layer (0th to 31th). Starting from the 10thmodel, the accuracy of the model exhibits a notice-able downward trend, with varying rates of declineobserved in the feature enhancing and predictionenhancing stages. Employing our CNA methodto compare the original model with the fine-tunedmodel, we note a significant increase in the coeffi-cient scores of crucial deep FFN neurons. Hence,we conclude that LoRA enhances the final predic-tion by amplifying the coefficient scores of impor-tant FFN neurons. Finally, using our findings, wedevelop methods on model pruning for arithmetictasks, and model editing for reducing gender bias.To summarize, our contributions are as follows:1. We find the reason why only a few headscan influence arithmetic ability is that these headsstore crucial parameters for memorizing 1D op-erations. We identify human-interpretable FFNneurons across both shallow and deep layers.2. We propose the CNA method and constructthe internal logic chain from inputs to predictionwith four stages: feature enhancing, feature trans-ferring, feature predicting, prediction enhancing.3. We use the CNA method to explore the mech- anism of LoRA and find LoRA increases the prob-ability of final predictions by amplifying the impor-tant FFN neurons coefficient scores. We design amodel pruning method for arithmetic tasks, and amodel editing method for reducing gender bias.",
  "Mechanistic Interpretability": "Mechanistic interpretability aims to reverse engi-neer the intricate computations executed by trans-formers. The analysis of transformer circuits standsas a key approach within this domain. Elhageet al. (2021) and Olsson et al. (2022) investigatethe mechanism using a two-layer attention-onlytransformer and discover that induction heads canmake predictions similar to [A][B] ... [A] -> [B].Yu and Ananiadou (2024a) explore the details ofin-context learning in a mechanistic view. Wanget al. (2022) present an explanation on an indirectobject identification case in GPT2.Causal mediation analysis (Pearl, 2001; Viget al., 2020) is also widely used for locating im-portant modules. Meng et al. (2022a,b) intervenethe hidden states of GPT2 (Radford et al., 2019)and ascertain that the medium FFN layers play asignificant role in processing subject names. Wanget al. (2023) intervene the attention layers to ex-plore the mechanism of in-context learning andobserve an information flow from demonstrationsto corresponding labels. Geva et al. (2023) find twocritical points on relation and subjection positionsthrough interventions on attention edges.Since causal mediation analysis methods requireexpensive forward pass over multiple input, severalstudies try to design static methods for interpret-ing language models. Geva et al. (2022) utilizethe product of norm and coefficient score to lo-cate important FFN neurons and find many FFNneurons have human-interpretable concepts whenprojecting into vocabulary space. Dar et al. (2022)find most matrices in attention and FFN layers areinterpretable in vocabulary space.",
  "Understanding Arithmetic in LLM": "Hanna et al. (2023) investigate how GPT2-smallcomputes greater-than. Gould et al. (2023) demon-strate that successor heads can aid in predicting thesubsequent order, such as predicting \"3\" after \"2\".Zhang et al. (2023) investigate the attention headsfor addition operation, and find only a few headsplay significant roles. Zhong et al. (2024) inves- tigate the clock and pizza algorithms for modularaddition. Quirke et al. (2023) studies n-digit inte-ger addition on an one-layer transformer, and findindividual digits are computed in parallel. Throughinterventions on hidden states, Stolfo et al. (2023)find that attention layers transform the informationto the last token, and FFN layers capture result-related information.",
  "Background": "We start by introducing the inference pass indecoder-only language models. Following previ-ous studies (Geva et al., 2023), we omit the biasterm and layer normalization (Ba et al., 2016). Themodel aims to generate a probability distribution Ybased on an input sequence X = [t1, t2, ..., tT ] con-sisting of T tokens. Y is a B-dimension vector con-taining probabilities for each token in vocabularyV . Each token ti in X is embedded into a vectorx0i Rd using an embedding matrix E RBd.Then the vectors undergo transformation throughL + 1 transformer layers (0th-Lth). Vector xli onthe ith position at layer l is computed by:",
  "xli = xl1i+ Ali + F li(1)": "where Ali Rd and F li Rd are the outputs ofthe lth attention and FFN layers, referred to as theattention output and FFN output, respectively. xl1irepresents the layer output at layer l 1, whichalso serves as the layer input at layer l. The termxl1i+ Ali is denoted as the residual output. Theattention layer captures information from differentpositions through H multiple heads ATTN lj, andthe FFN layer transforms the residual output bymatrices Wfc1 and Wfc2 with non-linearity :",
  "Y = softmax(Eu xLT )(4)": "Geva et al. (2020) demonstrate that the FFN layercan be conceptualized as key-value memories, withmatrices W lfc1 RdN and W lfc2 RNd storingkeys and values for N neurons. The FFN output isobtained by adding N subvalues, where each sub-value is the result of multiplying a coefficient scoremlk with a fc2 vector fc2lk Rd (also referredto as the FFN value). These coefficient scoresare calculated as the inner product between theresidual output and the corresponding fc1 vectorfc1lk Rd (also referred to as the FFN key):",
  "Interventions on Attention Heads": "We make a 2-digit arithmetic dataset, includingaddition (2D+), subtraction (2D-), multiplication(2D*) and division (2D/). Similar to Stolfo et al.(2023), we design four prompts for each operationincluding both numbers (e.g. 3) and number words(e.g. three), reported in Appendix A. The evalu-ation dataset has 1,600 sentences. We intervenethe attention heads by setting all the heads param-eters into zero, and we take accuracy as metric.Llama-7B consists of 32 layers with 32 heads perlayer. Consequently, we execute the model 1,024times (intervening on one head each time for 1,600cases) and compute the average accuracy on theevaluation dataset.",
  "Results of Different Heads": "The accuracy of the original model is 74.8%. Inter-ventions on the majority of heads (976 in total) leadto only a minor decrease in accuracy (0.01%-2%).Only three heads result in a decrease of 10% ormore. The top5 heads are shown in .Interventions on head 1722, 159 and 1419 cause12.7% or more decrease. Specifically, 1722 reduces",
  "Reasons Causing Accuracy Decrease": "Since the accuracy of more complicated operationsare low, we analyze the most important head foreach operation in 1-digit (1D), 2-digit (2D) and 3-digit (3D) operations, shown in . The mostimportant heads in 1D, 2D and 3D operations arethe same. We report the details of top5 heads inAppendix E. In comparison to addition, subtraction,and division, the top head for multiplication doesnot significantly impact accuracy. We leave furtherinvestigation of this phenomenon for future work.",
  "D46.562.26.854.92D58.452.611.271.83D52.556.98.153.2": ": Accuracy decrease (%) in 1D, 2D and 3D.In , the decreases of 1D, 2D and 3D op-erations are similar. Therefore, we hypothesizethat the heads store important parameters about 1Doperations. Since 2D and 3D also rely on the mem-orization of 1D operations, the 2D/3D accuracydecrease when the 1D memorization is lost.",
  ": Accuracy decrease (%) on memorize andchange-one cases": "We also analyze two types of cases for each op-eration, which are named \"change-one\" (similar tothe definition of \"carry\" in Opedal et al. (2024))and \"memorize\". \"Memorize\" cases only requirememorization. For example, \"15+32=47\" requiresmemorization about \"5+2=7\" and \"1+3=4\", thus \"15+32= -> 4\" and \"15+32=4 -> 7\" are two \"mem-orize\" cases.\"Change-one\" cases require thechange-one ability. For example, \"15+37= -> 5\"is a \"change-one\" case, as the output is based on\"5=1+3+1\". For multiplication and division cases,we take the last token as \"memorize\" cases, andothers as \"change-one\" cases. We compute the ac-curacy decrease between the original model andthe intervened model for each operation. The re-sults are shown in . If the heads only storechange-one abilities, the decrease of \"memorize\"cases should be much smaller than \"change-one\"cases. However, the accuracy decrease of \"mem-orize\" cases and \"change-one\" cases are similar.Hence, we hypothesize the heads store parametersfor memorizing 1D operations.",
  "Methodology": "The core idea of our proposed CNA method is com-paring the same neuron across different modelsgiven the same input, or comparing the same neu-ron across different inputs within the same model.Due to the computational intensity of the forwardpass, employing causal mediation analysis meth-ods on every neuron is impractical. Therefore, wetake the increase of log probability (Yu and Anani-adou, 2024b) as importance score for each neuron.The importance score of a FFN neuron mlkfc2lk islog(p(w|xl1T+AlT +mlkfc2lk))log(p(w|xl1T+AlT )), where w is the final predicted token and theprobability is computed by the softmax functionwhen multiplying the vectors with the unembed-ding matrix (Eq.4). Then we compute the change ofeach neurons importance score between the orig-inal model and the intervened model (interveninghead 1722), and sort the change score to locate themost important neurons causing the final predictionprobability decrease. We only intervene one headbecause this head can result very much decrease inaccuracy. In later sections, we introduce the analy-sis process focusing on a specific case \"3+5=\", anddevise various methods to prove these findings areapplicable to all 1D+ and 1D- cases.",
  "Feature Predicting via Arithmetic Head": "For case \"3+5=\" with prediction \"8\", we computethe importance score change for each neuron, andfind the most important neurons are in FFN layers.We project these neurons in vocabulary space (Gevaet al., 2022) by multiplying the FFN neurons vand unembedding matrix: Pv = softmax(Eu v).The top tokens when projecting into the unembed-ding space are shown in . 283696 means the3696th neuron in the 28th FFN layer. \"ori\" and\"inv\" denote the original and the intervened model(\"mdl\"). \"imp\" and \"coef\" represent the importancescore and coefficient score of each neuron.",
  ": Importance scores and coefficient scores oflocated important FFN neurons for input \"3+5=\"": "All these neurons contain concepts about \"eight\"and \"8\" in top tokens. The importance scores andcoefficient scores drop a lot in the intervened model.From the interpretable results, we hypothesize thatthe reason why the accuracy decreases a lot in theintervened model is that head 1722 stores impor-tant parameters for activating the important FFNneurons related to the final prediction. To verifythis hypothesis, we conduct two experiments onall 1D+ and 1D- cases. For each case, we employthe CNA method to identify the important FFNneurons. Then in the original model we only in-tervene the most important FFN neurons (\"mask\")or intervene all the other FFN neurons within the17th31th layers (\"keep\"). The accuracy decreaseon all 1D+ and 1D- cases is presented in .",
  "When intervening the top99 FFN neurons, the": "accuracy decreases 100%. When intervening allthe other neurons in deep FFN layers, the accuracyonly decreases 3.9%. This suggests that almost allimportant information for predicting the final tokenis contained within the FFN neurons identified byour CNA method. We also report the decrease ofthe top neurons coefficient scores (\"coef\") betweenthe intervened model and the original model in. In all situations, the coefficient scores dropmuch. Therefore, our hypothesis is verified: head1722 stores important parameters for activating theimportant FFN neurons related to final predictions.When head 1722 is intervened, coefficient scoresof important FFN neurons drop a lot, thus finalpredictions probabilities drop much.",
  "Prediction Enhancing among Deep FFNNeurons": "In case \"3+5=\", we observe that there is a predic-tion enhancing stage among the most importantFFN neurons 283696, 257164 and 195769. The innerproduct scores between the FFN value of 195769and the FFN keys of 257164 and 283696 are large.Additionally, the inner product between the FFNvalue of 257164 and the FFN key of 283696 is alsolarge. Therefore, a prediction enhancing directedacyclic graph (PE-DAG) exists among the threeneurons, where 195769 is the root. Activation of thelower FFN neuron recursively triggers activationsof upper semantic-related FFN neurons.To explore whether the prediction enhancingstage also exists in other 1D+ and 1D- cases, wecompute the coefficient score change of importantFFN neurons when intervening the lowest neuronamong the most important neurons. If there aremany neurons in the lowest layer, we intervenethe neuron with the largest importance score in thelowest layer. Decrease of coefficient score whenintervening the lowest important neuron in the orig-inal model are shown in .",
  ": Decrease (%) of coefficient score when inter-vening the lowest neuron among important neurons": "In , the top FFN neurons also play alarge role in GPT-J. When intervening the top99neurons, the accuracy decreases 58.4%. Comparedwith Llama, the degrees of coefficient decreaseand accuracy change are both smaller. In , when intervening the lowest neuron among theimportant neurons identified by our CNA method,the deep neurons coefficient scores decrease.",
  "Feature Enhancing with Hidden-Interpretable Shallow FFN Neurons": "Stolfo et al. (2023) utilize causal mediation anal-ysis and find the model processes numbers andoperators on early FFN layers and transfer into lastposition via attention layers. In this section, ourobjective is to locate the specific neurons fulfillingthis function and to analyze the roles of shallowFFN layers and attention layers in this process. Toidentify the important shallow FFN neurons forcase \"3+5=\"->\"8\", we sort the neurons by com-puting the inner products between the PE-DAGroot 195769 and the attention transformation of eachFFN neuron. We find that the neurons (on residualstreams of \"3\" and \"5\") with highest inner prod-ucts are hidden-interpretable. When projecting theoriginal neurons into vocabulary space, they donot contain human-interpretable concepts in toptokens. However, after the transformation of at-tention layers, these neurons become interpretable.Moreover, we find that the word embeddings of\"3\" and \"5\" are also hidden-interpretable. The topvocabulary tokens of original and 15th attentionlayer transformation are shown in .",
  "We hypothesize that these hidden-interpretableFFN neurons are crucial for enhancing input fea-tures. We develop a zero-shot method to identify": "these hidden-interpretable shallow FFN neurons.For each FFN neuron on 0th15th layer, we com-pute the transformation by 0th 16th attentionlayers value-output matrices, and project thesevectors into vocabulary space. If the top50 tokenscontain M or more concepts related to numbersor operations, we add this neuron into a hidden-interpretable neuron set. Then we intervene all theneurons in this neuron set in the original model,and compute the accuracy decrease on all 1D+ and1D- cases. The number of neurons and accuracyunder different M are shown in .",
  ": Decrease (%) of accuracy on 1D+ and 1D-cases when intervening hidden-interpretable neurons": "There are 176,128 neurons in 0th 15th FFNlayers. Intervening with only 1,953 neurons (M=2)results in a decrease of 53.9%. This strongly sug-gests that these hidden-interpretable neurons play asignificant role in enhancing features and are valu-able for final predictions. Further supporting thisnotion is the observation that randomly intervening1,953 neurons on the 0th 15th FFN layers onlyresults in an accuracy decrease of 2.6%. Comparedto directly interpretable neurons in deep FFN lay-ers, hidden-interpretable neurons in shallow FFNlayers are more widely distributed. When interven-ing 10,426 neurons (about 6% of all neurons in0th 15th layers), the accuracy decreases 68.4%.",
  "Constructing the Internal Logic Chainfrom Inputs to Prediction": "In .2-4.4, we apply our CNA method toidentify the important neurons for the case \"3+5\",and also design experiments to verify the generalityacross other 1D+ and 1D- cases. In this section,we conclude the internal logic chain from inputs toprediction for case \"3+5=\" -> \"8\":First, in feature enhancing stage, shallow FFNneurons containing hidden-interpretable features(e.g.112258, 124072) are extracted.In featuretransferring stage, the hidden-interpretable features(word embeddings and shallow FFN neurons) aretransformed into directly-interpretable features byattention layers and then transferred to the last po-sition. In feature predicting stage, head 1722 acti-vates deep FFN neurons associated with the con-cept of \"8\" (e.g. 283696, 257164, 195769) based on the enhanced features. Finally, in the prediction en-hancing stage, lower FFN neurons activate higherFFN neurons, which collectively contribute to theprobability of \"8\" in the final prediction.Through our CNA method, we precisely iden-tify crucial parameters (attention heads and FFNneurons) for predicting final tokens. Comparedto prior studies, our approach enables the discov-ery of more detailed locations and offers a clearerexplanation of the information flow. Given ourmethods ability to pinpoint precise parameters, itcan be effectively leveraged for downstream taskssuch as model pruning and model editing, whichwe discuss in .",
  "Understanding the Mechanism of LoRA": "LoRA (Hu et al., 2021) is a commonly usedparameter-efficient fine-tuning method (Houlsbyet al., 2019; Li and Liang, 2021; Lester et al., 2021).By adding trainable low-rank matrices into atten-tion layers, models are fine-tuned with only 0.5%additional parameters, yielding favorable outcomes.Intuitively, LoRA is similar to a head. Inspired bythe analysis on arithmetic heads, we apply the CNAmethod to understand the mechanism of LoRA.We first investigate whether LoRA plays distinctroles when added into various layers. We fine-tune32 models on the 2-digit arithmetic dataset, witheach model incorporating a low-rank matrix intoa distinct attention layer. Notably, we introducenegative numbers in 2D cases such as \"3-5=-2\",as the original Llama model does not learn thisconcept well. The training and testing set consistof 18,000 and 2,000 sentences, respectively. Wedetermine the optimal learning rate from choices of0.001, 0.0005, and 0.0001. The maximum epoch isset to 4. The results are depicted in .",
  "All the fine-tuned models exhibit superior ac-": "curacy compared to the original model (62.96%).The 0th and the 31th layer may have special use,since the accuracy of the 0th and 31th models dif-fers much from their neighboring models. Theaccuracy of the 1st 9th models is around 90%.Starting from the 10th model, the accuracy keepsdecreasing. The average slope during the 10th to16th models differs from that of the 17th to 30thmodels. Motivated by LoRAs accuracy curve andthe analysis of arithmetic heads, we hypothesizethat LoRA enhances the correct predictions proba-bilities by amplifying the deep FFN neurons relatedto final predictions. We apply our CNA method onthe original model and five LoRA models analyzingthe case \"3+5=\", detailed in .",
  ": Important neurons coefficient scores on theoriginal model and five fine-tuned models for \"3+5=\"": "Across all five fine-tuned models, the coefficientscores of 257164 and 195769 surpass those of theoriginal model. The scores are higher in shallow-layer models compared to deep-layer models. Thesignificant decrease in the coefficient score ob-served in 257164 in the 20th model can be attributedto its failure to leverage the features of 195769.",
  ": Coefficient score increase (%) of differentfine-tuned models compared with the original model": "For all cases, we compute the average coeffi-cient score increase of 1st9th, 10th16th, and17th30th models on the most important neurons,detailed in . Across all scenarios, the co-efficient scores of significant FFN neurons surpassthose of the original model. Notably, fine-tuningLoRA in shallow layers yields a greater amplifica-tion of FFN neurons coefficient scores comparedto deep layers. This observation validates our hy-pothesis: LoRA enhances the probabilities of finalpredictions by amplifying the coefficient scores ofdeep FFN neurons relevant to final predictions.",
  "Model Pruning for Arithmetic Tasks": "As recent powerful models boast tens of billionsof parameters, the extraction of sub-networks fromthese large models for various downstream taskshas become crucial. This approach is based onthe assumption that only a small subset of parame-ters in an over-parameterized model are pertinentto a specific task and similar tasks share similarsub-networks (Pfeiffer et al., 2023). Recent works(Stanczak et al., 2022; Foroutan et al., 2022) inmultilingual models can support these hypotheses. In this section, we apply our findings on modelpruning for arithmetic tasks. As discussed in Sec-tion 4, important information for final predictionsis concentrated in only a few deep FFN neurons.Therefore, we design a simple method to pruneuseless neurons in deep FFN layers. We apply ourCNA method between the original model and the9th LoRA model on all the 1D+, 1D-, 1D* and1D/ cases, to find the important top500 neurons foreach case. Then we prune all the other FFN neu-rons among 17th31th layers, thus only 5% deepFFN neurons are saved in the pruned model. Fi-nally, we add LoRA on the 9th layer of the prunedmodel, and fine-tune on the training set. The pa-rameters on deep FFN layers are reduced to 5%,and only 0.015% LoRA parameters are added.",
  ": Accuracy on 2-digit datasets": "The results are shown in .The ac-curacy of the fine-tuned pruned model (LoRA9-p) is 82.3%, better than original Llama (62.9%).While our method do not reach the performanceof the fine-tuned model without pruning (LoRA9),it still offers a promising avenue for model prun-ing. Furthermore, although 2-digit arithmetic is aneasy task, fine-tuning LoRA on a randomly-prunedmodel (LoRA9-r) with the same number of neuronsfails to yield satisfactory results (only 17.1%). Thisfurther underscores the significance of our method.",
  "Model Editing for Reducing Gender Bias": "Even though LLMs have achieved great success,they can learn, perpetuate, and amplify harmfulsocial biases (Gallegos et al., 2023). In this section,we focus on gender bias, which is observed in dif-ferent models (de Vassimon Manela et al., 2021;Kotek et al., 2023). We apply our CNA method an-alyzing similar cases with different genders in thesame model. For example, we identify the impor-tant neurons for predicting \"nurse\" by calculatingthe change of importance scores between sentences\"A woman works as a\" and \"A man works as a\".Since the other words are the same except \"woman\"and \"man\", these neurons contain much genderbias causing p(nurse|woman) > p(nurse|man).The neurons top tokens of are shown in .For example, the top tokens of FFN neuron 198436are all professions. Under the input \"A womanworks as a\", this neurons coefficient score is 3.39.While the neurons coefficient score is only 0.14activated by \"A man works as a\", proving that thisneuron contains much gender bias.",
  ": FFN neurons contain gender bias. \"F\":woman": "We then apply our CNA method on 32 com-mon professions contain gender bias (detailed inAppendix D). Designing four prompts, we iden-tify top18 important FFN neurons and edit themby setting their parameters to zero.The aver-age perplexity difference log(p(prof|gend1)) log(p(prof|gend2)) is shown in , reducedby 35.7% when only 18 neurons are edited.",
  "Discussion and Conclusion": "We aim to discuss the mechanisms behind causalmediation analysis and static interpretation meth-ods. Causal mediation analysis methods can findthe \"root cause\" (head 1722) of the probabilitychange, which are usually not interpretable. Staticmethods can locate the interpretable \"direct cause\"(FFN neurons), but many elements can activatethese neurons. Our CNA method can locate both\"root cause\" and \"direct cause\", and reconstruct thewhole logic chain from inputs to prediction.Overall, we identify the important attentionheads and FFN neurons for arithmetic operations.We propose the comparative neuron analysis (CNA)method and construct the internal logic chain frominputs to prediction, including the feature enhanc-ing stage, feature transferring stage, feature predict-ing stage, and prediction enhancing stage. Basedon these findings, we find LoRA increases the finalpredictions probabilities by enlarging the impor-tant FFN neurons coefficient scores. Finally, weapply our method and findings on model pruningfor arithmetic tasks, and model editing for reduc-ing gender bias. Our method and analysis offer acomprehensive insight for understanding LLM.",
  "Limitations": "The case studies rely on projecting vectors in vo-cabulary space, which is widely used in previousstudies (Elhage et al., 2021; Ram et al., 2022; Gevaet al., 2022; Dar et al., 2022). While the resultsare empirically interpretable, the theories of thismethod are incomplete. Therefore, we utilize thismethod in our case studies and supplement ourfindings with additional methods to strengthen ourconclusions, thus enhancing their persuasiveness.Another limitation lies in the lack of standard-ization across various studies regarding attributionmethods. Different intervention methods (zero in-tervention, noise intervention, replace intervention,etc.) may get different results. Apart from causalmediation analysis methods and static interpreta-tion methods, gradient-based methods (Sundarara-jan et al., 2017) and SHAP values (Lundberg andLee, 2017) are also widely utilized for attributingimportant modules. However, these methods of-ten demand substantial computational resources,rendering them unsuitable for our work.A potential risk of our work is that attackers canidentify the important neurons and edit these neu-rons to change the output probability distribution. For instance, instead of reducing the gender biasby setting the neurons parameters to zero, theycan amplify the gender bias professions probabili-ties by enlarging the identified neurons in .2. Hence, it is important to distinguish whethera model is edited, and we leave this exploration infuture work.",
  "Acknowledgements": "This work is supported by the project JPNP20006from New Energy and Industrial Technology De-velopment Organization (NEDO). This work is sup-ported by the computational shared facility and thestudentship from the Department of Computer Sci-ence at the University of Manchester. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster,Marco Dos Santos, Stephen McAleer, Albert Q Jiang,Jia Deng, Stella Biderman, and Sean Welleck. 2023.Llemma: An open language model for mathematics.arXiv preprint arXiv:2310.10631.",
  "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-ton. 2016.Layer normalization.arXiv preprintarXiv:1607.06450": "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts, PaulBarham, Hyung Won Chung, Charles Sutton, Sebas-tian Gehrmann, et al. 2023. Palm: Scaling languagemodeling with pathways. Journal of Machine Learn-ing Research, 24(240):1113.",
  "Guy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant.2022. Analyzing transformers in embedding space.arXiv preprint arXiv:2209.02535": "Daniel de Vassimon Manela, David Errington, ThomasFisher, Boris van Breugel, and Pasquale Minervini.2021. Stereotype and skew: Quantifying gender biasin pre-trained and fine-tuned language models. InProceedings of the 16th Conference of the EuropeanChapter of the Association for Computational Lin-guistics: Main Volume, pages 22322242. Nelson Elhage, Neel Nanda, Catherine Olsson, TomHenighan, Nicholas Joseph, Ben Mann, AmandaAskell, Yuntao Bai, Anna Chen, Tom Conerly, et al.2021. A mathematical framework for transformercircuits. Transformer Circuits Thread, 1:1.",
  "Rhys Gould, Euan Ong, George Ogden, and ArthurConmy. 2023. Successor heads: Recurring, inter-pretable attention heads in the wild. arXiv preprintarXiv:2312.09230": "Wes Gurnee, Neel Nanda, Matthew Pauly, Kather-ine Harvey, Dmitrii Troitskii, and Dimitris Bert-simas. 2023.Finding neurons in a haystack:Case studies with sparse probing. arXiv preprintarXiv:2305.01610. Michael Hanna, Ollie Liu, and Alexandre Variengien.2023. How does gpt-2 compute greater-than?: In-terpreting mathematical abilities in a pre-trained lan-guage model. In Thirty-seventh Conference on Neu-ral Information Processing Systems. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,Bruna Morrone, Quentin De Laroussilhe, AndreaGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.Parameter-efficient transfer learning for nlp. In In-ternational conference on machine learning, pages27902799. PMLR.",
  "Neel Nanda, Senthooran Rajamanoharan, Janos Kramar,and Rohin Shah. 2023. Fact finding: Attempting toreverse-engineer factual recall on the neuron level.In Alignment Forum, page 6": "Catherine Olsson, Nelson Elhage, Neel Nanda, NicholasJoseph, Nova DasSarma, Tom Henighan, Ben Mann,Amanda Askell, Yuntao Bai, Anna Chen, et al. 2022.In-context learning and induction heads.arXivpreprint arXiv:2209.11895. Andreas Opedal, Alessandro Stolfo, Haruki Shirakami,Ying Jiao, Ryan Cotterell, Bernhard Schlkopf, Ab-ulhair Saparov, and Mrinmaya Sachan. 2024. Dolanguage models exhibit the same cognitive biases inproblem solving as human learners? arXiv preprintarXiv:2401.18070. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al.2022. Training language models to follow instruc-tions with human feedback. Advances in neural in-formation processing systems, 35:2773027744.",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan,Dario Amodei, Ilya Sutskever, et al. 2019. Languagemodels are unsupervised multitask learners. OpenAIblog, 1(8):9": "Ori Ram, Liat Bezalel, Adi Zicher, Yonatan Be-linkov, Jonathan Berant, and Amir Globerson. 2022.What are you token about?dense retrieval asdistributions over the vocabulary.arXiv preprintarXiv:2212.10380. Karolina Stanczak, Edoardo Ponti, Lucas Torroba Hen-nigen, Ryan Cotterell, and Isabelle Augenstein. 2022.Same neurons, different languages: Probing mor-phosyntax in multilingual pre-trained models. arXivpreprint arXiv:2205.02023. Alessandro Stolfo, Yonatan Belinkov, and MrinmayaSachan. 2023. A mechanistic interpretation of arith-metic reasoning in language models using causalmediation analysis. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 70357052.",
  "Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.Axiomatic attribution for deep networks. In Interna-tional conference on machine learning, pages 33193328. PMLR": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro,Faisal Azhar, et al. 2023. Llama: Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971. Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov,Sharon Qian, Daniel Nevo, Yaron Singer, and StuartShieber. 2020. Investigating gender bias in languagemodels using causal mediation analysis. Advancesin neural information processing systems, 33:1238812401.",
  "Ben Wang and Aran Komatsuzaki. 2021. Gpt-j-6b: A 6billion parameter autoregressive language model": "Kevin Wang, Alexandre Variengien, Arthur Conmy,Buck Shlegeris, and Jacob Steinhardt. 2022.In-terpretability in the wild: a circuit for indirect ob-ject identification in gpt-2 small.arXiv preprintarXiv:2211.00593. Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou,Fandong Meng, Jie Zhou, and Xu Sun. 2023. Labelwords are anchors: An information flow perspectivefor understanding in-context learning. arXiv preprintarXiv:2305.14160. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in neuralinformation processing systems, 35:2482424837.",
  ": Decrease (%) of accuracy on 1D/ cases whenintervening hidden-interpretable neurons": "In , The hidden-interpretable neuronsin shallow FFN layers are important for 1D/ cases(e.g. \"72/8=\"). When intervening 10,426 hidden-interpretable shallow FFN neurons, the accuracyreduces 82.1%. For comparison, we randomly in-tervene 10,426 FFN neurons in shallow FFN lay-ers, and the interventions only cause a decrease of5.1%.Overall, head 1419 shares the same mechanismwith head 1722. Head 1419 stores important param-eters for division operations, while head 1722 isresponsible for addition and subtraction.",
  ": Accuracy (%) when intervening differentheads in GPT-J": "In GPT-J, we also observe that different headsstore important parameters for various operations.For instance, the accuracy of 2D- decreases signifi-cantly when intervening in head 139, whereas head1414 holds significant parameters for 2D/.Then we apply the CNA method between theoriginal model and the intervened model on head139 on 2D- cases. The results are shown in (corresponding to ), (correspond-ing to ), and (corresponding to).",
  "DDetails for Evaluating Gender Bias": "We design eight prompts to find the most commonprofessions causing the gender bias. The promptsare shown in , where <gend> is \"man\" or\"woman\".We compute the top100 predictions of eachprompt for different genders, and compare the dif-ferent professions, which are shown in .These professions contain much gender bias. Wethen apply our CNA method between cases withdifferent genders under the first prompt, and iden-tify the top18 important neurons causing the differ-ence. Finally, we edit the top18 neurons by settingtheir parameters to zero, and then compute the per-"
}