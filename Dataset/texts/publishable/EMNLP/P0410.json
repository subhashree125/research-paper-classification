{
  "Abstract": "Depression is a critical concern in global men-tal health, prompting extensive research into AI-based detection methods. Among various AItechnologies, Large Language Models (LLMs)stand out for their versatility in mental health-care applications. However, their primary lim-itation arises from their exclusive dependenceon textual input, which constrains their over-all capabilities. Furthermore, the utilization ofLLMs in identifying and analyzing depressivestates is still relatively untapped. In this paper,we present an innovative approach to integrat-ing acoustic speech information into the LLMsframework for multimodal depression detec-tion. We investigate an efficient method for de-pression detection by integrating speech signalsinto LLMs utilizing Acoustic Landmarks. Byincorporating acoustic landmarks, which arespecific to the pronunciation of spoken words,our method adds critical dimensions to text tran-scripts. This integration also provides insightsinto the unique speech patterns of individuals,revealing the potential mental states of individ-uals. Evaluations of the proposed approach onthe DAIC-WOZ dataset reveal state-of-the-artresults when compared with existing Audio-Text baselines. In addition, this approach isnot only valuable for the detection of depres-sion but also represents a new perspective inenhancing the ability of LLMs to comprehendand process speech signals.",
  "Introduction": "Depression, a common mental disorder affect-ing 10-15% of the global population, is charac-terized by persistent low mood, loss of interest,and lack of energy, making it a prevalent andcostly illness (Walker et al., 2018). Given the time-consuming, expensive, and sometimes ineffectivenature of traditional depression treatment methods,a growing number of researchers are turning theirattention to developing automated depression detec- : Example of Acoustic Landmark (2-gram con-cat landmark (g+p-), (s+p+), (p+,p-), ..., (g-b-)), Land-marks are extracted from abrupt changes in the speechsignal. They can discretize speech into a series of tokensthat possess linguistic significance. tion systems. Concurrently, Large language mod-els (LLMs) have recently demonstrated remark-able success across a variety of tasks (Chowdheryet al., 2023; Touvron et al., 2023). These large lan-guage models have been applied to various health-care issues, including general surgery (Oh et al.,2023), dementia diagnosis (Wang et al., 2023), andgastroenterology (Lahat et al., 2023) and achievedexcellent results. However, their main limitationstems from their sole reliance on textual input,which limits their full potential. Simultaneously,the use of Large Language Models (LLMs) in de-pression detection remains largely unexplored. Inparticular, there has been no effort to integratespeechdespite growing evidence that speech sig-nals can reveal indicators of depression (Wu et al.,2023; Huang et al., 2019a)into these LLMs, anadvancement that could greatly improve their ef-fectiveness in identifying depression (Zheng et al.,2023). One of the key approaches to incorporatingspeech signals into LLMs is through the discretiza-tion of speech. However, the current landscape ofspeech discretization, heavily reliant on deep learn-ing techniques (Zeghidour et al., 2021; Dfossezet al., 2022), faces significant challenges due toits considerable GPU memory requirements. Thisis particularly problematic in the field of depres- : Overview of LLM-Landmark Depression Detection Pipeline, broadly categorized into three stages:landmark detection (on the left), cross-modal instruction fine-tuning (in the middle), and P-tuning for depressiondetection (on the right). sion detection, where data often consists of lengthyconversations (DeVault et al., 2014). The needfor completed conversations is vital for accuratedepression detection (Wu et al., 2023; Sun et al.,2022), rendering the existing deep learning-basedmethods impractical for such applications. For thispurpose, it is necessary to find an efficient approachthat allows for the discretization of speech with re-duced GPU memory usage.Acoustic landmarks represent event markers in-tricately linked with the articulation of speech,forming a concise alternative framework for speechprocessing (Liu, 1996; Stevens, 2002). This ap-proach emphasizes the analysis of abrupt acousticchanges at the subsegmental level, thereby pro-viding a succinct and precise phonetic descriptionof language. These landmarks, characterized bytheir binary values, establish a minimal yet effec-tive set for differentiating each language segmentfrom others. They maintain a direct and signifi-cant relationship with acoustic properties and ar-ticulation (including individual pronunciation), en-suring discernibility despite unwanted variabilityintroduced by diverse hardware and environmentalbackgrounds (Huang et al., 2018, 2019b). Theirdiscrete nature not only allows for efficient integra-tion into large language models but also offers aviable alternative for understanding speech signalsin depression detection, bypassing the limitationsof current deep learning-based techniques. Thisinnovative approach promises a more feasible andresource-efficient pathway for analyzing complexspeech patterns in mental health diagnostics.In this paper, we introduce a novel multimodalapproach to depression detection, utilizing a com-bination of acoustic landmarks and large languagemodels.We investigate the properties of largelanguage models at various stages and under dif- ferent conditions after integrating landmark-basedspeech information. We investigate how LLMslearn speech landmarks and assess the impact ofconversational fine-tuning on the performance ofLLMs in tasks related to depression detection.In summary, our contributions include the fol-lowing: To the best of our knowledge, this is the firststudy to apply LLMs to multimodal depres-sion detection and the inaugural effort to inte-grate speech information into LLMs for thispurpose. We proposed a new baseline for theapplication of LLMs in the field of automaticdepression detection. Compared with prior baseline audio-text meth-ods (Wu et al., 2023), our approach not onlyachieved SOTA performance but also involveda comprehensive analysis of the properties ofLLMs post the integration of landmarks. Unlike previous deep learning-based methodsfor aiding LLMs in understanding speech, weexplored a new, more efficient approach toenable LLMs to process speech signals. Thisnovel method opens up a potentially ground-breaking direction for enhancing LLMs com-prehension of speech.",
  "Large Language Models": "Large language models have achieved success innatural language processing and have been ex-tended to encompass computer vision and speechsignal processing (Brown et al., 2020; Touvronet al., 2023; Li et al., 2023b; Liu et al., 2024). How-ever, there is a significant gap in research aimed atenabling LLMs to comprehend speech efficiently.",
  ": Landmark Detection Filter": "Parameter-efficient fine-tuning refers to selec-tively updating a small subset of the models pa-rameters or adding lightweight trainable layers, tocustomize the model for specific tasks or domainswith reduced computational overhead. Existingworks employed low-rank adaptation (LoRA) tofine-tune LLM efficiently. LoRA reduces computa-tional complexity by freezing the pre-trained LLMand injecting trainable rank decomposition matri-ces A and B into its transformer-based layers (Huet al., 2022). The forward pass is subsequentlydefined as the linear combination of those fromthe pre-trained model and from the trained decom-posed matrices A and B.",
  "Acoustic Landmarks": "The concept of acoustic landmarks originally stemsfrom research on distinctive features (Garvin, 1953;Zhang et al., 2024a). Some researchers posit thatfor certain phonetic contrasts, a listener relies onacoustic landmarks to gather the necessary acousticcues for deciphering the underlying distinctive fea-tures (Liu, 1996). This perspective highlights theimportance of these landmarks in the auditory pro-cessing and interpretation of speech. Subsequentresearch has utilized acoustic landmarks for appli-cations in speech recognition (Liu, 1996; He et al.,2019) as well as in addressing mental health-relatedproblems (Huang et al., 2018, 2019a). Althoughdifferent scholars have slightly varied definitionsof acoustic landmarks, Joel and colleagues (Boyceet al., 2012) expanded upon Lius paper (Liu, 1996)by releasing a MATLAB version of a landmark de-tection toolkit, which has become the most widely",
  "Automatic Depression Detection": "The use of AI technology for depression detec-tion has been developing for many years. Someresearchers (Cummins et al., 2011; Huang et al.,2018, 2019a) have utilized traditional methods suchas Support Vector Machines (SVMs) (Noble, 2006)for depression detection. With the advancementof deep learning technologies (Gulati et al., 2020;Zhang et al., 2024c), an increasing number of re-searchers have been experimenting with deep learn-ing approaches for depression detection. Zhao andothers have explored the use of transformer modelsfor processing speech inputs in depression detec-tion (Zhao et al., 2020). Shen and colleagues haveemployed BI-LSTM architectures, combining textand speech for this purpose (Shen et al., 2022).Further extending these techniques, Wu (Wu et al.,2023) utilized speech self-supervised models (Chenet al., 2022; Hsu et al., 2021; Liu et al., 2022) andintegrated them with RoBERTa (Liu et al., 2019)for a more comprehensive text-audio multimodalapproach to depression detection.",
  "Overview": "Our methodology, detailed in , encom-passes a three-step training process. The first phaseinvolves extracting acoustic landmarks from speechand conducting an array of data processing oper-ations. Subsequently, in the Cross-modal Instruc-tion Fine-Tuning phase, we engage the LLM inlearning the nuances and characteristics of acousticlandmarks. The culminating phase is the P-Tuningprocess, wherein the LLM is meticulously trainedto apply its understanding to diagnose depression.",
  "Landmarks Extraction and DataPreprocessing": "3.2.1Landmarks Extraction illustrates an example of acoustic land-marks, where speech signals are discretized intoa series of symbols that carry linguistic relevance. details the specific acoustic landmarks uti-lized in our study. Diverging from Lius paper (Liu,1996), our research also pays attention to frication,voice frication, and periodicity.Our method primarily draws inspiration fromJoels (Boyce et al., 2012) and Lius (Liu, 1996)work. However, since they have not open-sourced",
  ": Description of the six landmarks investigated": "their code, many of their approachs details remainunknown. In the following section, We introduceour Python-based landmark detection algorithm,developed to address these gaps and to adapt theconceptual framework to our specific requirements.Initially, the spectrogram is divided into six fre-quency bands. Landmarks are identified throughenergy changes within these six bands, using a two-pass strategy. Different landmarks are determinedby either a single band or a combination of multi-ple bands (Liu, 1996). This approach is visuallyrepresented by the two parallel branches emanatingfrom the spectrogram block in .The detection algorithm for Glottal (g), Burst(b), and Syllabic (s) landmarks is fundamentallyaligned with Lius approach (Liu, 1996). How-ever, diverging from Lius method, we employ 5dBand 8dB as threshold values because of differentsmoothing methods between Python and Matlab.Additionally, considering that the opening and clos-ing of the glottis occur in pairs, We implementeddynamic programming to ensure that g landmarksappear in pairs, thus enhancing the physiologicalaccuracy of our detection.Our methodology for identifying f+ and v+ land-marks involves detecting a 6 dB power increase inat least three high-frequency bands (bands 4-6), anda power decrease in low-frequency bands (bands2 and 3). For f- and v-, the criteria are reversed: a6 dB power decrease in the same high-frequencybands and a power increase in the low-frequencybands. The distinguishing factor here is that frica-tion landmarks are detected within unvoiced seg-ments (b landmark), while voiced frication land-marks are sought in voiced segments (s landmark).Regarding the detection of the periodicity (p)landmarks, we perform autocorrelation calcula-",
  "tions on the audio frame to identify repetitive orperiodic patterns in the data. For a detailed descrip-tion of our landmark detection algorithm, pleaserefer to Appendix A": "3.2.2Data Augmentation and ProcessingDepression assessments are commonly conductedthrough clinical interviews, with each session re-ceiving a singular label. This labeling method,when applied to a given dataset size, leads to fewersamples in datasets compared with the much largernumber of utterances and frames typically encoun-tered in other speech-related tasks. As a result, thespeech depression detection task faces a notablechallenge of data scarcity. Moreover, the issue ofdata imbalance is particularly acute in the dataset,as instances of healthy (positive cases) are signifi-cantly outnumbered by depression (negative) cases.We adopted Wus approach (Wu et al., 2023) of aug-menting the training set through sub-dialogue shuf-fling. Sub-dialogue shuffling involves samplinga sub-dialogue xs:e from each complete dialoguex1:T , where s and e represent the randomly selectedstart and end utterance indexes, respectively.This technique allowed us to balance the numberof positive and negative samples effectively, whilesubstantially increasing the dataset size. Differingfrom Wus method, our use of landmarks in speechprocessing enables the use of longer sub-dialoguesfor training purposes. To ensure a fair compari-son, we maintained the same data size (same sub-dialogue sampling number M=1000) as Wus ap-proach. For a detailed description of the algorithm,please refer to Appendix B.Previous research has indicated that the patternsin which landmarks appear are more valuable thanthe individual landmarks themselves (Huang et al.,2019a). Therefore, as shown in , we com-bined landmarks, treating every two consecutivelandmarks as a single unit. This approach not onlybetter represents the patterns of landmarks but alsoeffectively reduces the length of the landmark se-quence in each sample.",
  ": F1 scores for the different LLM models, We test all Llama2 models for 7B and 13B, also test on GPT": "task involves providing an LLM with instructionsto predict potential acoustic landmarks based ontext. This method serves a dual purpose: it enablesthe LLM to learn about acoustic landmarks, andit also aligns speech (landmarks) and text modali-ties using paired data. We adopt LoRA (Hu et al.,2022) by incorporating low-rank matrices into theQuery and Key matrices of the self-attention layer,facilitating efficient adaptation and fine-tuning. Ad-ditionally, we resize the embedding layer of theLLMs to add the merged landmarks to the vocabu-lary. During the training process, both the embed-ding layer, linear head and the LoRA matricesare actively trained to integrate these new elementseffectively. The training objective is to minimizethe negative log-likelihood, and the loss calculationapplies to all samples (including the prefix), whichcan be formulated as:",
  "i=1log P(si,j|s<i,j, M), (1)": "where x is the number of samples in dataset C, yjis the text and corresponding landmarks in sampleS, and M denotes the large language model thatwe have fine-tuned.Additionally, during dataset construction, we in-corporate hints for the LLM. For example, whendata are sourced from a patient with depression,we include a hint indicating their origin from adepressed patient. Experimentally, we found thismethod of data construction to be crucial, whichalso supports our hypothesis that the acoustic land-marks from individuals with depression differfrom those of healthy individuals. For detailedtemplate construction, please refer to Appendix C.",
  "P-Tuning for Depression Detection": "In the previous stage, we trained the LLMs to un-derstand what landmarks are. Following this, weemploy P-tuning (Liu et al., 2023) to enable theLLMs to integrate text and landmarks for depres-sion detection. We replace the lm head layer withthe classification layer. The training objective isto minimize cross-entropy for classification, which",
  "c=1yo,c log(po,c),(2)": "where C is the number of classes. yo,c is an indi-cator variable that is 1 if the observation o belongsto class c and 0 otherwise. po,c is the predictedprobability of observation o belonging to class c.We also compared instruction tuning using LoRAwith P-tuning and discovered that manually con-structed templates are not well-suited for de-pression classification tasks. Furthermore, weobserved a performance improvement when apply-ing LoRA matrices across all layers of Llama2.",
  "Decision Making": "In the previous study by (Wu et al., 2023), theyachieved state-of-the-art (SOTA) results through anensemble approach, combining WavLM, WavLMpre-trained on emotional recognition tasks, and thecombined result of RoBERTa and WavLM. Adopt-ing a similar strategy, we fine-tune three distinctLlaMA2 (Text + Landmark) models, each withdifferent data volumes (different numbers of sub-dialogue M(900, 1000, 1100)), and used them forensemble voting.",
  "Experimental Setup": "Dataset. The DAIC-WOZ dataset (DeVault et al.,2014), recognized as a standard for depression de-tection, includes 189 clinical interview recordingsbetween interviewers and patients. In its trainingsubset, 30 of the total 107 interviews are labelled asdepressed, while the development subset contains12 depressed instances out of 35 interviews. Consis-tently with previous studies (Gong and Poellabauer,2017; Shen et al., 2022; Wu et al., 2022, 2023), wereport our results on the development subset.Model Configurations.Our research utilizesLlama2-7B, Llama-7B Chat, Llama2-13B, andLlama2-13B Chat, conducted on a system equippedwith 8 NVIDIA A100 80GB GPUs. Llama 2-Chatwas optimized for engaging in two-way conver-sations. In the cross-modal instruction fine-tuning",
  "Llama2 (M =1100)0.719": ": A comparison of our proposed system withprevious state-of-the-art (SOTA), where all ensembleoutcomes(F1 Score) are derived from a majority vote.In the table, M denotes the number of augmented sub-dialogues per dialogue in our data augmentation al-gorithm, while the previous SOTA used M=1000 sub-dialogues. stage, We fine-tuned the model with 10 epochs with128 batch sizes, 8 Lora ranks, 100 warmup steps,and a 1e-6 learning rate. In the depression detec-tion stage, we fine-tuned the model with 8 epochswith 256 batch sizes, 30 virtual tokens, 256 encoderhidden sizes, and a 1e-6 learning rate. In both ex-periments, we used AdamW as an optimizer withthe model parallel to fine-tune our model. In theablation study stage, we used hyperparameter tun-ing following the Tree-structured Parzen Estimator(TPE) paradigm (Bergstra et al., 2011).",
  "Main Result: Performance of differentLLMs in Depression Detection task": "Depression Detection in Llama2. displaysthe F1 scores obtained by Llama2 in depression de-tection across different scenarios. Additionally, weconducted a comparison of our findings with theresults obtained from GPT-3.5 and GPT-4, focusingsolely on their performance in the text modality. Itis crucial to highlight that we did not fine-tune GPT-3 or GPT-4 for our purposes. Rather, we employedcarefully crafted prompts(see appendix D), allow-ing the GPT models to assess whether a particularsample was from a patient with depression.For the landmark only and landmark + textresults, the process involved first undergoing hintcross-modal instruction fine-tuning and then em-ploying P-tuning for depression detection. Theobjective was to equip the LLMs with a prelimi-nary understanding of landmarks before advancingto the diagnostic stage for depression.The experimental results reveal that when LLMssolely use the text modality for depression de-tection, the performance of all models, includingnotably powerful ones like GPT-3.5 and GPT-4,which excel in many tasks, is not particularly im-pressive and remains somewhat unsatisfactory. We attribute the subpar performance to two main fac-tors. First is the inherent limitation of the textmodality in conveying emotional information.For instance, consider the sentence, \"Its raining to-day.\" While some may find this statement positive,others might feel the opposite. Its challenging todiscern the emotional nuances from the text alone,but with audio information, we could accuratelycapture the emotional context of the statement. Sec-ondly, the issue lies with the data itself. Labelsare only available at the document level, and dataare scarce (currently, there are no larger publicdatasets available for multimodal depression de-tection). This limitation in data granularity andvolume significantly hinders the models ability toaccurately detect depression.The introduction of landmarks led to enhancedperformance across all models, affirming the effec-tiveness of our method in integrating landmarks.Landmarks can represent some of the acoustic in-formation due to affective variation, providing ad-ditional information that assists LLMs in detectingdepression. Nonetheless, the efficacy of using land-marks in isolation for depression detection wasfound to be suboptimal. Drawing on past research,we believe this is due to the fact that even aftercross-modal instruction fine-tuning, relying solelyon information from other modalities (such as au-dio or visual) could potentially impair the stabilityof LLMs (Zhang et al., 2023; Li et al., 2023c).When we combined multiple Llama2 models thathad integrated both text and landmark informationfor depression detection, we achieved SOTA resultsas shown in table 3. Furthermore, as indicated in, there is a gradual improvement in Llama2sperformance in depression detection tasks as thenumber of sub-dialogues per dialogue increases.This observation further emphasizes the crucialrole that data quantity plays in the effectiveness ofdepression detection tasks.5Ablation Study and DiscussionIn this chapter, we conduct an empirical study tometiculously analyze and elucidate the character-istics of LLMs that we identified in the context ofdepression detection during our experiments.",
  ": Evaluation loss for different configurations up to 4000 steps": "patient with depression significantly impacts thetraining outcome. As evident from , with-out a hint, the loss converged to around 1.76 (asshown in a). In contrast, with a hint, theloss consistently converged to near 1.1 (as depictedin Figures 4b and 4c). d offers a morevivid illustration of the substantial difference thatthe presence or absence of a hint makes to themodels performance in our empirical study. Thisphenomenon supports our previous conjecture thatindividuals with depression and those who arehealthy differ in their vocal expressions and thatlandmarks are capable of reflecting this charac-teristic. Although the differences between Llama2and Llama2 Chat are not substantial, it is still ob-servable that, in this phase, Llama2 outperformsits Chat version. We will provide a more detaileddiscussion in the subsequent section.",
  "How LLMs Learn from AcousticLandmarks": "To further investigate how LLMs learn acousticlandmarks, we extended the application of LoRAbeyond just the attention layers, applying it acrossall layers for comprehensive analysis (Pu et al.,2023; Sun et al., 2023; Li et al., 2023a; Zhanget al., 2024b). To find the matrix with the greatestcontribution, we first need to define the method forcalculating the contribution of a matrix. We can ap-proximately consider the changes in the LoRA ma-trix as indicative of its contribution to the task (Heet al., 2021). Therefore, we assess that the contri-bution of a matrix is calculated by summing the ab-solute values of all its elements, normalized by thetotal number of elements in the matrix. Suppose wehave a set of LoRA matrices L1, L2, . . . , Ln, eachmatrix Li being an a b matrix. Then, the contri-bution Ci of matrix Li can be calculated using the",
  "k=1|Li(j, k)|.(3)": "Here, |Li(j, k)| represents the absolute value ofthe element in the jth row and kth column of ma-trix Li. After calculating the contribution value(C), we rank and select the ten matrices with thehighest and the lowest contributions for furtheranalysis. separately illustrates the fourmatrices with the greatest contributions and thefour with the least. To validate the effectiveness ofthis method, we deactivated the five matrices withthe smallest contributions and observed that thishad no significant impact on our results.Our analysis of the matrices revealed that LLMsprimarily learn landmarks through the feedfor-ward network, while the contribution of the LoRAmatrices in the attention layers is quite minimal.This phenomenon is also observed when trainingLLMs to learn speech codecs (Hao et al., 2023),suggesting that even though landmarks have in-herent linguistic significance, LLMs tend to treatlandmarks as abstract tensors, similar to speechcodecs, during the learning process. Additionally,we observed that layers closer to the beginning ofthe LLMs have a greater contribution to learninglandmarks. This could be because LLMs treat land-marks as new vocabulary items, leading to moreupdates in layers nearer to the embedding layer.",
  "(h) Bottom Layer 4": ": The top four images represent the LoRA matrices of the layers that contribute most significantly to thelarge language models learning of landmarks. The bottom four images depict the LoRA matrices of the layers withthe least contribution. As can be inferred from the graphs title, the feedforward layer is the primary contributor. we tested LlaMA2 Chat and found that its perfor-mance, both during the Cross-modal InstructionFine-Tuning stage and the depression detectionphase, was inferior to that of LlaMA2. We hy-pothesize two potential reasons for this. The firstis that the Chat version might not be suitable forclassification tasks. The second, and our preferredexplanation, is that the Chat version, having beenadjusted, tends to avoid answering questions to mit-igate ethical risks. To validate our hypothesis, wefirst reimagined the classification task as a gener-ative task, where the LLMs diagnoses depressionthrough dialogue responses. We tested this zero-shot scenario on GPT-3.5 and GPT-4. Addition-ally, we applied LoRA for instruction fine-tuningin various scenarios presented in , to ob-serve how the models perform post-tuning. Weobserved that when treating depression detectionas a generative task, neither LlaMA2 nor GPT mod-els performed particularly well, with the dialogue-enhanced LlaMA Chat still underperforming com-pared with LlaMA. This suggests that LLMs in thefield of depression detection are subject to certainartificial limitations, impacting their effectivenessin this specific application. The details of the tem-plate can be seen on Appendix D.",
  "From our previous ablation experiments, we foundthat the conventional method of incorporating": "LoRA matrices into attention layers might not bewell-suited for depression detection tasks. After ex-perimenting with applying LoRA matrices acrossall layers and conducting a hyperparameter search,we observed that LoRA, in this context, achievedresults similar to those of P-tuning. Furthermore, inour use of LoRA for classification tasks, we testeda variety of manually crafted templates. However,none were as effective as using no task-specificprompt template. We believe this occurs becausewhen we explicitly inform the LLMs that the taskinvolves depression detection, the model tends toavoid responses that could pose ethical risks. 6ConclusionThis paper introduces an efficient approach for de-pression detection using acoustic landmarks andLLMs. This approach is not only valuable for thedetection of depression but also represents a newperspective in enhancing the ability of LLMs tocomprehend speech signals. Furthermore, we arethe first to research multimodal depression de-tection using LLMs. We establish a new bench-mark with a SOTA F1-score of 0.84 through ensem-ble learning. Additionally, we evaluated variousPEFT methods and discovered that applying Loraacross all layers yields identical outcomes for bothP-tuning and Lora in depression detection. Ouranalysis further reveals how LLMs process speechlandmarks, guiding future research in this domain.",
  "Limitations": "In addition, The study is confined to the DAIC-WOZ dataset, which is currently the most com-monly used and only publicly available dataset inthe field of multimodal depression recognition, par-ticularly in the area of speech. The difficulty inacquiring data due to numerous privacy concernssurrounding depression datasets is acknowledged.Despite the limitations of focusing on this singledataset, it aligns with traditional research method-ologies in this domain, as previous studies havepredominantly relied on it.",
  "Ethics Statement": "The DAIC-WOZ datasets are publicly availablebenchmarks and have been automatically de-identifed to protect patient privacy. Although ourmodel improves the factual accuracy of generatedreports, its performance still lags behind the needsof practical deployment. The outputs of our modelmay contain false observations and diagnoses dueto systematic biases. In this regard, we stronglyurge the users to examine the generated output inreal-world applications cautiously.",
  "Suzanne Boyce, Harriet Fell, and Joel MacAuslan. 2012.Speechmark: Landmark detection tool for speechanalysis. In Thirteenth Annual Conference of theInternational Speech Communication Association": "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. In Proc. Adv. Neural Inf. Process. Syst.,volume 33, pages 18771901. Sanyuan Chen, Chengyi Wang, Zhengyang Chen,Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, NaoyukiKanda, Takuya Yoshioka, Xiong Xiao, et al. 2022.Wavlm: Large-scale self-supervised pre-training forfull stack speech processing. IEEE Journal of Se-lected Topics in Signal Processing, 16(6):15051518. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts, PaulBarham, Hyung Won Chung, Charles Sutton, Sebas-tian Gehrmann, et al. 2023. Palm: Scaling languagemodeling with pathways. Journal of Machine Learn-ing Research, 24(240):1113. Nicholas Cummins, Julien Epps, Michael Breakspear,and Roland Goecke. 2011. An investigation of de-pressed speech detection: Features and normalization.In Twelfth Annual Conference of the InternationalSpeech Communication Association.",
  "Hongkun Hao, Long Zhou, Shujie Liu, Jinyu Li, ShujieHu, Rui Wang, and Furu Wei. 2023. Boosting largelanguage model for speech synthesis: An empiricalstudy. arXiv preprint arXiv:2401.00246": "Di He, Xuesong Yang, Boon Pang Lim, Yi Liang, MarkHasegawa-Johnson, and Deming Chen. 2019. Whenctc training meets acoustic landmarks. In ICASSP2019-2019 IEEE International Conference on Acous-tics, Speech and Signal Processing (ICASSP), pages59966000. IEEE. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. 2021. Towards aunified view of parameter-efficient transfer learning.In International Conference on Learning Representa-tions. Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,Kushal Lakhotia, Ruslan Salakhutdinov, and Abdel-rahman Mohamed. 2021. Hubert: Self-supervisedspeech representation learning by masked predictionof hidden units. IEEE/ACM Transactions on Audio,Speech, and Language Processing, 29:34513460. Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and WeizhuChen. 2022. LoRA: Low-rank adaptation of largelanguage models. In Proc. Int. Conf. Learn. Repre-sentations.",
  "Zhaocheng Huang, Julien Epps, Dale Joachim, andMichael Chen. 2018.Depression detection fromshort utterances via diverse smartphones in naturalenvironmental conditions. In INTERSPEECH, pages33933397": "Adi Lahat, Eyal Shachar, Benjamin Avidan, Zina Shatz,Benjamin S Glicksberg, and Eyal Klang. 2023. Eval-uating the use of large language model in identifyingtop research questions in gastroenterology. Scientificreports, 13(1):4164. Shuyue Stella Li, Beining Xu, Xiangyu Zhang, HexinLiu, Wenhan Chao, and Paola Garcia. 2023a. A quan-titative approach to understand self-supervised mod-els as cross-lingual feature extracters. In Proceedingsof the 6th International Conference on Natural Lan-guage and Speech Processing (ICNLSP 2023), pages200211. Shuyue Stella Li, Xiangyu Zhang, Shu Zhou, HongchaoShu, Ruixing Liang, Hexin Liu, and Leibny PaolaGarcia. 2023b.Pqlm-multilingual decentralizedportable quantum language model. In ICASSP 2023-2023 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP), pages 15.IEEE.",
  "Yuang Li, Yu Wu, Jinyu Li, and Shujie Liu. 2023c.Prompting large language models for zero-shotdomain adaptation in speech recognition.arXivpreprint arXiv:2306.16007": "Hexin Liu, Leibny Paola Garcia Perera, Andy W. H.Khong, Eng Siong Chng, Suzy J. Styles, and SanjeevKhudanpur. 2022. Efficient self-supervised learn-ing representations for spoken language identifica-tion. IEEE J. Sel. Topics Signal Process., 16(6):12961307. Hexin Liu, Xiangyu Zhang, Leibny Paola Garcia,Andy WH Khong, Eng Siong Chng, and ShinjiWatanabe. 2024. Aligning speech to languages toenhance code-switching speech recognition. arXivpreprint arXiv:2403.05887.",
  "William S Noble. 2006. What is a support vector ma-chine? Nature biotechnology, 24(12):15651567": "Namkee Oh, Gyu-Seong Choi, and Woo Yong Lee.2023. Chatgpt goes to the operating room: evalu-ating gpt-4 performance and its potential in surgicaleducation and training in the era of large languagemodels. Annals of Surgical Treatment and Research,104(5):269. George Pu, Anirudh Jain, Jihan Yin, and Russell Kaplan.2023. Empirical analysis of the strengths and weak-nesses of peft techniques for llms. In ICLR 2023Workshop on Mathematical and Empirical Under-standing of Foundation Models. Ying Shen, Huiyu Yang, and Lin Lin. 2022. Automaticdepression detection: An emotional audio-textualcorpus and a gru/bilstm-based model. In ICASSP2022-2022 IEEE International Conference on Acous-tics, Speech and Signal Processing (ICASSP), pages62476251. IEEE.",
  "Hao Sun, Yen-Wei Chen, and Lanfen Lin. 2022. Ten-sorformer: A tensor-based multimodal transformerfor multimodal sentiment analysis and depression de-tection. IEEE Transactions on Affective Computing": "Xianghui Sun, Yunjie Ji, Baochang Ma, and Xian-gang Li. 2023. A comparative study between full-parameter and lora-based fine-tuning on chinese in-struction data for instruction following large languagemodel. arXiv preprint arXiv:2304.08109. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Jane Walker, Katy Burke, Marta Wanat, Rebecca Fisher,Josephine Fielding, Amy Mulick, Stephen Puntis,Joseph Sharpe, Michelle Degli Esposti, Eli Harriss,et al. 2018. The prevalence of depression in general",
  "hospital inpatients: a systematic review and meta-analysis of interview-based studies. Psychologicalmedicine, 48(14):22852298": "Zhuo Wang, Rongzhen Li, Bowen Dong, Jie Wang,Xiuxing Li, Ning Liu, Chenhui Mao, Wei Zhang,Liling Dong, Jing Gao, et al. 2023. Can llms likegpt-4 outperform traditional ai tools in dementiadiagnosis? maybe, but not today. arXiv preprintarXiv:2306.01499. Wen Wu, Mengyue Wu, and Kai Yu. 2022. Climate andweather: Inspecting depression detection via emotionrecognition. In ICASSP 2022-2022 IEEE Interna-tional Conference on Acoustics, Speech and SignalProcessing (ICASSP), pages 62626266. IEEE. Wen Wu, Chao Zhang, and Philip C Woodland. 2023.Self-supervised representations in speech-based de-pression detection. In ICASSP 2023-2023 IEEE In-ternational Conference on Acoustics, Speech andSignal Processing (ICASSP), pages 15. IEEE. Neil Zeghidour, Alejandro Luebs, Ahmed Omran,Jan Skoglund,and Marco Tagliasacchi. 2021.Soundstream: An end-to-end neural audio codec.IEEE/ACM Transactions on Audio, Speech, and Lan-guage Processing, 30:495507. Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, andYu Qiao. 2023. Llama-adapter: Efficient fine-tuningof language models with zero-init attention. arXivpreprint arXiv:2303.16199. Xiangyu Zhang, Daijiao Liu, Tianyi Xiao, Cihan Xiao,Tuende Szalay, Mostafa Shahin, Beena Ahmed, andJulien Epps. 2024a. Auto-landmark: Acoustic land-mark dataset and open-source toolkit for landmarkextraction. arXiv preprint arXiv:2409.07969.",
  "Xiangyu Zhang, Jianbo Ma, Mostafa Shahin, BeenaAhmed, and Julien Epps. 2024b. Rethinking mambain speech processing by self-supervised models.arXiv preprint arXiv:2409.07273": "Xiangyu Zhang, Qiquan Zhang, Hexin Liu, TianyiXiao, Xinyuan Qian, Beena Ahmed, EliathambyAmbikairajah, Haizhou Li, and Julien Epps. 2024c.Mamba in speech: Towards an alternative to self-attention. arXiv preprint arXiv:2405.12609. Ziping Zhao, Zhongtian Bao, Zixing Zhang, NicholasCummins, Haishuai Wang, and Bjrn Schuller. 2020.Hierarchical attention transfer networks for depres-sion assessment from speech. In ICASSP 2020-2020IEEE international conference on acoustics, speechand signal processing (ICASSP), pages 71597163.IEEE. Wenbo Zheng, Lan Yan, and Fei-Yue Wang. 2023. Twobirds with one stone: Knowledge-embedded tempo-ral convolutional transformer for depression detec-tion and emotion recognition. IEEE Transactions onAffective Computing.",
  "P = x[i] max(vl, vr),(12)": "where vl and vr are the lowest points on either sideof x[i], before reaching a higher point. A peak isconsidered significant if its prominence exceeds apredefined threshold.The width W of a peak is measured at a verticaldistance P from its highest point. Points x[l] andx[r], where l < i < r, are the positions at whichthe signal drops below the threshold defined by theprominence:",
  "A.2Details of Specific Landmark Detection": "g landmark When both the coarse and fine filtersexhibit a peak in band 1, it is identified as a glandmark.b landmark In an unvoiced segment (not be-tween +g and the next -g), if at least three outof five frequency bands demonstrate simultaneouspower increases of no less than 6 dB in both coarseand fine filters, a specific condition or criterion ismet.s landmark In an unvoiced segment (between+g and the next -g), if at least three out of fivefrequency bands demonstrate simultaneous powerincreases of no less than 6 dB in both coarse andfine filters, a specific condition or criterion is met.f+ and v+ landmarks involves detecting a 6dB power increase in at least three high-frequencybands (4, 5, 6), and a power decrease in low-frequency bands (2, 3). For f- and v-, the criteriaare reversed: a 6 dB power decrease in the samehigh-frequency bands and a power increase in thelow-frequency bands.The distinguishing factor hereis that frication landmarks are detected within un-voiced segments (b landmark), while voiced frica-tion landmarks are sought in voiced segments (slandmark).p landmark, p landmark extraction can bedivided into several steps.1. Frame Segmentation:Let the audio signal be Y (t).Define the frame length N and frame shift .For the i-th frame, we consider the segmentY [i : i + N].2. Autocorrelation Calculation:For each frame Yi, calculate the autocorrelationfunction Rxx(k):",
  ": end for": "Apply smoothing(As defined in the previous sec-tion) to the upsampled energy function.6. Binarization:Define a threshold , and convert the smoothed en-ergy function into a binary signal.7. Jump Detection:Detect positive and negative jumps in the binarysignal.8. P Landmark Index and Time Determination:Record the positions of jumps, which are the in-dices of P landmarks.Convert these indices into time points to determinethe P landmarks.",
  "BDetails of Data Augmentation": "The training set was expanded by shuffling sub-dialogues, selecting portions xs:e from each fulldialogue x1:T , with s and e as random start andend indices. The algorithm outlines this process.Initially, it counts the positive and negative samples,setting M+ as the target number of sub-dialoguesfor each positive dialogue (Algorithm 1, lines 1-3). To balance augmentation, M is calculatedusing N+, N, and M+ (line 4). For both pos-itive and negative dialogues, corresponding M+ and M sub-dialogues are generated (lines 8-12).The sub-dialogue length, d, is set within the rangedefined by l and h, chosen randomly (lines 14-15). The start index s is randomly selected withinits range, and the end index e is determined accord-ingly (lines 16-18) (Wu et al., 2023)."
}