{
  "Abstract": "Top-view perspective denotes a typical way inwhich humans read and reason over differenttypes of maps, and it is vital for localization andnavigation of humans as well as of non-humanagents, such as the ones backed by large Vision-Language Models (VLMs). Nonetheless, spa-tial reasoning capabilities of modern VLMs inthis setup remain unattested and underexplored.In this work, we study their capability to under-stand and reason over spatial relations from thetop view. The focus on top view also enablescontrolled evaluations at different granularityof spatial reasoning; we clearly disentangle dif-ferent abilities (e.g., recognizing particular ob-jects versus understanding their relative posi-tions). We introduce the TOPVIEWRS (Top-View Reasoning in Space) dataset, consistingof 11,384 multiple-choice questions with ei-ther realistic or semantic top-view map as vi-sual input. We then use it to study and evalu-ate VLMs across 4 perception and reasoningtasks with different levels of complexity. Eval-uation of 10 representative open- and closed-source VLMs reveals the gap of more than50% compared to average human performance,and it is even lower than the random baselinein some cases.Although additional experi-ments show that Chain-of-Thought reasoningcan boost model capabilities by 5.82% on aver-age, the overall performance of VLMs remainslimited. Our findings underscore the criticalneed for enhanced model capability in top-viewspatial reasoning and set a foundation for fur-ther research towards human-level proficiencyof VLMs in real-world multimodal tasks.",
  "* Equal contribution": "answering, language generation, and arithmeticreasoning (Qin et al., 2023a; Zhao et al., 2023).Building on these text-only LLMs, the so-calledVision Language Models (VLMs), equipped withthe capability to process and reason over multi-modal vision-language information, have enabledmulti-modal processing (Yin et al., 2023; Wu et al.,2023). They ground language reasoning abilityof LLMs into the information of different modal-ities (Chandu et al., 2021). Prominent examplesof VLMs such as LLaVA (Liu et al., 2023b), GPT-4V (OpenAI, 2023), and Gemini (Google, 2024),have demonstrated strong performance across ap-plications such as visual question answering (Liet al., 2023d), image captioning (Diesendruck et al.,2024), and object grounding (Zheng et al., 2024).Spatial reasoning, one of the fundamental desir-able properties of and requirements for VLMs, hasalso gained increased attention recently (Rajabi andKosecka, 2023; Liu et al., 2023a; Chen et al., 2024).It requires grounding the models reasoning abilitywith natural language into its visual perception ofthe surrounding environment (Freksa, 1991). Inparticular, it involves two critical steps: (i) inter-preting the environment visually, and (ii) reasoningover spatial relations. As a fundamental ability forthe model to recognize, understand, and navigatethrough the physical world, it plays a crucial role invarious downstream tasks such as vision-languagegeneration (Li et al., 2024a) and embodied AI (Choet al., 2024). However, previous research has fo-cused on exploring spatial reasoning abilities ofVLMs only from a conventional first-person per-spective view (Liu et al., 2023a). In this work, weaim to study and evaluate spatial understanding andreasoning capability of VLMs from the top-viewperspective, also referred to as the birds-eye view(Li et al., 2024b).When compared to the conventional perspectiveview, top view offers better natural alignment: itis the typical perspective used for reading maps or : Illustration of the four evaluation tasks with an incremental level of complexity on the two types oftop-view maps (photo-realistic versus semantic maps), covering top-view perception and spatial reasoning abilities,with 9 sub-tasks in total (red font), focusing on different, well-defined VLM abilities. The radar graphs (top right)compare the representative models performance on all sub-tasks, indicating a large gap with human performance. presenting floor plans. Moreover, it is inherentlymore complex: top-view maps encapsulate a wealthof information about different scenes, locations,objects and their relationships in the environmentbased on a single image. In addition to the photo-realistic top-view maps, semantic top-view maps(Nanwani et al., 2023; Li et al., 2024a) use differentcolors to represent different types of objects; werun experiments with both map types, see .One advantage of top-view maps is that theydefine a controlled and interpretable experimentalframework. Indoor scenes, which are the focusof this work, typically feature a relatively stableset of objects and layouts, making them ideal forcontrolled studies. This allows us to disentangleand investigate various aspects of spatial reasoningand VLMs capabilities in a controlled manner.1",
  "For instance, we can apply different interventions (e.g.,": "In this work, we thus investigate the basic top-view spatial understanding and reasoning abilitiesof current state-of-the-art VLMs across four tasksof gradually increasing complexity, and their finer-grained sub-tasks. The tasks are as follows. 1)Top-View Recognition assesses whether the modelcan recognize concrete objects and scenes in top-view maps. 2) Top-View Localization evaluatesthe ability to localize objects or regions on a mapbased on textual descriptions. (3) Static SpatialReasoning investigates whether the model can rea-son about spatial relationships among localizedobjects and regions within the map. (4) DynamicSpatial Reasoning evaluates reasoning about spatialrelations along the points of a dynamic navigationpath. illustrates all the tasks with concrete",
  "drawing a navigation trajectory in a realistic map, or changingthe color-object mapping in a semantic top-view map)": "examples. As one key finding of this study, con-ducted evaluations reveal that current VLMs lacksufficient capability to effectively tackle top-viewspatial reasoning challenges, indicating substantialroom for improvement in future research. Contributions. 1) We define the top-view spa-tial reasoning challenge for VLMs via 4 care-fully designed tasks of increasing complexity, alsoencompassing 9 distinct fine-grained sub-taskswith a structured design of the questions focus-ing on different model abilities. 2) We collect theTOPVIEWRS dataset, comprising 11,384 multiple-choice questions with either photo-realistic or se-mantic top-view maps of real-world scenariosthrough a pipeline of automatic collection followedby human alignment. 3) We use TOPVIEWRS toevaluate and study 10 VLMs from different modelfamilies and sizes, highlighting the substantial per-formance gap compared to humans.2",
  "Related Work": "Top-View Map Understanding. There are onlylimited studies in NLP focused on the use of top-view maps, though considerable research has beenconducted within the broader AI community on theso-called birds-eye view, which is an instance oftop view. This body of work has explored applica-tions in autonomous driving (Unger et al., 2023; Liet al., 2024c), with contributions on fusing differenttypes of views (Qin et al., 2023b) and working witharbitrary camera setups (Peng et al., 2023). In otherapplication scenarios, Yan et al. (2021) introduce abirds-eye view person re-identification task.Efforts to bridge top-view images with naturallanguage in applications beyond the above are lessdiverse. The WAY dataset, proposed by Hahn et al.(2020), contains 6,154 dialogs aimed at localizingan observers position on a top-view map throughconversations between an observer and a locator.This dataset has inspired follow-up research fo-cusing on merging vision with dialog information(Zhang et al., 2024a) and leveraging pretrainingstrategies to enhance performance (Hahn and Rehg,2022). In general, prior research does not assessVLMs basic spatial reasoning abilities with top-view images and lacks fine-grained and control-lable analyses of these fundamental abilities.",
  "We publicly release (the part of) the dataset and codeonline at": "reasoning with the advancement of LLMs (Yamadaet al., 2024), within the context of relative spatialrelation recognition (Mirzaee et al., 2021; Shi et al.,2022), natural language navigation (Yamada et al.,2024), and planning (Momennejad et al., 2023)(see Appendix A for a more complete overview).Cross-modal spatial reasoning puts forwardhigher requirements for the models in terms oflanguage grounding (Rozanova et al., 2021; Rajabiand Kosecka, 2023). Liu et al. (2023a) investigatespatial reasoning with 2D natural realistic front-view images and Chen et al. (2024) extend theanalysis to 3D point clouds. The environmentalcontexts become more diverse compared to syn-thetic symbols in text-only spatial reasoning, rang-ing from indoor environments (Koch et al., 2024) tooutdoor street views (Chen et al., 2019). Regardingtypical tasks, visual QA (VQA) is the mainstreamtask for benchmarking spatial reasoning abilities(Dong et al., 2021; Banerjee et al., 2021; Liu et al.,2023a; Li et al., 2023a,b; Kamath et al., 2023),while other tasks include vision-language naviga-tion (Chen et al., 2019; Li et al., 2024a) and userinterface grounding (Rozanova et al., 2021).3 We stress that none of the prior research effortsallows for disentangled evaluation of models spa-tial reasoning abilities. Prior work typically con-flates object recognition with spatial reasoning. Wethus design a dataset and conduct a study that notonly offers insight into fundamental abilities butalso allows for easier interpretation of results (4).",
  "Task Definition": "Following prior work (Li et al., 2023a), we frameall tasks as multiple-choice QA tasks. Given atop-view (realistic or semantic) map of a room M,the model must choose the correct option oi fromthe four options provided O = {o0, o1, o2, o3} thatanswers the question.4 This format simplifies theevaluation and interpretation of the results. Top-View Maps. We provide two different typesof top-view maps to the models: realistic mapsMReal and semantic maps MSem. Realistic mapsare constructed by placing a simulated orthographiccamera above the scene to capture a photo-realistic 3Research on multi-modal spatial reasoning also intersectswith efforts from the computer vision community on sceneunderstanding (Teney et al., 2017), simultaneous localizationand mapping (Cadena et al., 2016), and combining LLMs withrepresentations of the 3D physical world (Hong et al., 2023).4For simplicity, for each question, there is always a singlecorrect answer. top-view image. Semantic maps represent objectsin the scene with colored bounding boxes. Eachobject is assigned a specific color and labeled atthe same relative coordinates on the map to pre-serve the objects semantic information and spa-tial allocation. In comparison to realistic maps,semantic maps simplify the initial step of spatialreasoning (i.e., environment interpretation) by la-beling the object types with corresponding colorsand excluding irrelevant additional details such asshape and texture found in realistic top-view maps.Given the customizable and flexible nature of color-object mapping, the semantic map can also serveas an ideal testbed for evaluating models out-of-distribution (OOD) performance, thereby encour-aging further exploration beyond the scope of thiswork. Example maps are in . Tasks and Sub-Tasks. We define 4 different taskswhich cover a total of 9 finer-grained sub-tasks,with concrete examples shown in . Thetasks are designed to have an increasing level ofcomplexity, where each subsequent task dependson the abilities measured in the preceding one(s).(1) Top-View Recognition evaluates the fundamen-tal ability to interpret the input map, and covers twosub-tasks: Object Recognition and Scene Recog-nition. It does not require the model to identifyspecific locations of objects and rooms.(2) Top-View Localization investigates whether themodel can localize objects or rooms in the top-view map based on textual descriptions, includingObject Localization and Scene Localization as twosub-tasks. Beyond understanding the top-view mapas a whole, it requires the model to ground entitiesin the map, representing the models ability to alignspatial descriptions with corresponding locations.(3) Static Spatial Reasoning aims to evaluate themodels spatial reasoning ability with more com-plex questions. It includes two sub-tasks: reason-ing over Scene Counting and Relative Spatial Rela-tions between different objects and rooms. Thesequestions require the model to perform multi-stepreasoning based on the recognition and localizationof entities in the top-view map.(4) Dynamic Spatial Reasoning. Finally, we in-troduce a novel task that involves dynamic spatialreasoning over top-view maps in the context ofagent navigation. It requires the model to under-stand the sequential relations along the points of thenavigation path (sub-task Dynamic Action Count-ing) and answer spatial questions with regard to",
  "TOPVIEWRS Dataset": "In order to study and evaluate the abilities of state-of-the-art VLMs on the 4 tasks spanning 9 sub-tasks from 3, we now introduce a novel evaluationdataset, TOPVIEWRS, which focuses on top-viewmaps of indoor scenes (i.e., houses and rooms),discussed in what follows. Dataset Features. It introduces several advance-ments and innovative features that distinguish itfrom all prior visual spatial reasoning datasets.1) Multi-Scale Top-View Maps: The selected top-view maps of indoor scenes (see ) pro-vide a more natural representation of spatial en-vironments that aligns with human cognitive map(Epstein et al., 2017). This makes benchmarkingspatial awareness more straightforward and mean-while mitigates spurious correlations in the posi-tions between objects commonly found in realisticfront-view images. Compared to the front view, themulti-scale top-view maps of single rooms and fullhouses add more divergence in the granularity ofthe entities (objects or rooms) in spatial reasoning.Meanwhile, we provide both realistic maps andsemantic maps for more comprehensive evaluation.2) Realistic Environmental Scenarios with RichObject Sets: We provide real-world environmentsfrom indoor scenes, with 80 objects per scene onaverage, ensuring a natural distribution and com-plexity of object locations. This also sets it apartfrom existing front-view spatial reasoning datasets,which typically contain only a handful of objects.3) Structured Question Framework: Unlike previ-ous datasets (Li et al., 2023a; Liu et al., 2023a; Ka-math et al., 2023), which conflate spatial reasoningwith object recognition, our dataset clearly defines4 tasks including 9 sub-tasks in total using diversequestion templates. This structured approach al-lows for a fine-grained evaluation and analysis ofmodels capabilities from various perspectives andlevels of granularity. Dataset Collection. We employ a two-stage datacollection strategy that includes automatic collec-tion from a simulator and alignment through humanjudgment. First, to approximate real-life scenar-ios, we use the Matterport3D dataset (Chang et al.,2017), which includes 90 building-scale sceneswith instance-level semantic and room-level region TVR TVL SSR DSR",
  "Relative Spatial Description": "top left 11% top center 8% top right 11% middle left 14% middle center11% middle right 12% bottom left 11% bottom center 14% bottom right8% up9% down 9% left 10% right 11%overlap 12% up left 11% up right 14% down left 13%down right 12% : TOPVIEWRS data statistics, showing distribution of task sizes, objects, regions, spatial and relative spatialdescriptions in realistic and semantic map settings, where the tasks are described with their initials for visualization. annotations in 3D meshes. We filter these to ex-clude multi-floor and low-quality scenes, select-ing 7 scenes with an average of 80 objects and 12rooms each. Realistic top-view maps are extractedusing orthographic cameras, and semantic top-viewmaps are constructed using the Habitat (ManolisSavva* et al., 2019; Szot et al., 2021) simulationenvironment. We then design a structured questionframework with 15 templates to minimize humanlabor and standardize the data collection process.To ensure quality, a second stage of manual hu-man judgment aligns and verifies the data, ensuringquestions are natural and correct. Participants areencouraged to discard or modify data points to im-prove quality, maintaining alignment with humanjudgments. We refer readers to Appendix B for fur-ther details regarding the data collection process. Dataset Statistics. TOPVIEWRS comprises a totalof 11,384 multiple-choice questions after humanverification, with 5,539 questions associated withrealistic top-view maps, and 5,845 with semantictop-view maps. Human verification keeps 587/784questions from the automatic collection phase forTop-View Recognition, 1,077/1,384 for Top-ViewLocalization, 2,340/3,080 for Static Spatial Rea-soning. The choices are uniformly distributed overchoices A (25.5%), B (24.6%), C (24.5%) and D(25.4%). shows the distribution of differ-ent tasks, objects, regions and spatial descriptions.The size of each task aligns with its correspondingdifficulty level, where the easier task comprisesfewer examples. We provide further insights andtechnical details in Appendix B.4.",
  "Experiments and Results": "Models and Implementation. We test a repre-sentative selection of both open-sourced and close-sourced models which achieve state-of-the-art per-formance on a range of multimodal benchmarks(Liu et al., 2023c; Li et al., 2023a) in a zero-shot in-ference setup. Regarding open-sourced models, westudy and evaluate Idefics (9B & 80B) (Laurenonet al., 2023), LLaVA-Next (7B, 13B & 34B) (Liuet al., 2024), InternLM-XComposer2 (7B) (Donget al., 2024), Qwen-VL (7B) (Bai et al., 2023). Thechosen close-sourced models are GPT-4V (Ope-nAI, 2023) and Gemini (Google, 2024).5 All themodels are implemented within the VLMEvalKitframework (OpenCompass Contributors, 2023). Prompts. For realistic maps, we provide the VLMswith the task description along with the multiple-choice question. For semantic maps, in addition tothe information above, we also introduce the con-cept of a semantic map to the model and providethe color-object mapping in the prompt in order tofacilitate its understanding of the abstract map. Weonly provide the color-object mappings of the col-ors that are presented in the semantic map as a pre-processing strategy in order to exclude irrelevantinformation. For the specific prompting templatesused in this paper, we refer to Appendix C.2. Evaluation Measures.We measure multiple-choice QA accuracy via Exact Match (EM) andPartial Match (PM). EM measures whether the pre-dicted option indices are exactly the same as thelabel indices. However, there may be cases where",
  "We use GPT-4-turbo-2024-04-09 of GPT-4V and lateststable gemini-pro-vision 1.0 of Gemini": "the correct answer to the question can be consid-ered partially correct, e.g., the answer is top rightwhile the prediction is top left. PM then calculatesthe proportion of overlapping words between thepredicted answer and the gold answer. It is calcu-lated based on the correctness of the text spans (orwords) of predicted options, as given by:",
  "Results and Discussion": "We first discuss the models performance acrossour four tasks, with results summarized in ,and fine-grained sub-task performance illustratedin . We find that the performance of currentstate-of-the-art VLMs is unsatisfactory on the pro-posed TOPVIEWRS benchmark with model-wiseaverage EM and PM over all tasks below 50%.Gemini is the best-performing model for realisticmaps, while GPT-4V excels in semantic maps. Forsome models, such as Qwen-VL, the results aresometimes much worse than the random baseline.This issue primarily arises from the models diffi-culty in following the instructions to choose fromthe four provided options. Models perform better on recognition and lo-calization tasks compared to reasoning tasks.Top-View Recognition consistently demonstratesthe highest performance across all models. Geminishows human-comparable performance with theEM score over 90%. Top-View Localization ex-hibits lower performance compared to Top-ViewRecognition, followed by Static Spatial Reasoning.This highlights that the decline in model perfor-mance is primarily due to the lack of spatial reason-ing, rather than the domain shift introduced by thetop-view map. The performance difference of vari-ous tasks with different levels of complexity alsounderscores the advantage of our benchmark tocapture well-defined and disentangled phenomena,which allows for controlled studies in controlledenvironments.Regarding Dynamic Spatial Reasoning, modelsperform better on this task than on the previoustasks. Fine-grained performance in in-dicates that the improved performance primarilystems from high accuracy in dynamic action count-ing and spatial localization, which constitute 18%and 66% of the data respectively for this task. Weattribute the high accuracy in these areas to theequivalence between navigation path symbols andvisual prompting (Shtedritski et al., 2023). Despite",
  "these advancements, the overall EM accuracy re-mains below 40%, and models still struggle withreasoning over dynamic relative spatial relations": "Larger models do not always show better spa-tial awareness. Surprisingly, our results revealthat larger model sizes do not consistently trans-late to better performance. In Top-View Recogni-tion, closed-source models outperform open-sourcemodels by 31.10% EM with realistic maps and29.33% EM with semantic maps. However, theperformance gap narrows as the task complexityincreases. Using realistic maps as the visual in-put, Gemini stands out by achieving a minimum of5.53% higher EM accuracy in Static Spatial Rea-soning compared to other models, while GPT-4Vperforms worse than Idefics-9B on both Static andDynamic Spatial Reasoning tasks. This indicatesa lack of significant difference in spatial aware-ness between closed-source and open-source mod-els for tasks with higher complexity, despite thedisparity in their model sizes. This trend holdstrue within open-sourced models as well. BothIdefics and LLaVANext model families in somecases show comparable or worse performance withlarger model variants than with smaller ones. Simi-lar observations have been made by previous stud-ies (Zhong et al., 2021; Shi et al., 2024). We con-jecture that this might be caused by inadequateevidence of the scaling law (Kaplan et al., 2020) inthe computer vision community (Tian et al., 2024).The results on TOPVIEWRS thus advocate for fur-ther investigation and analysis in this area. Models perform better in easier tasks with se-mantic maps. In simple tasks such as Top-ViewRecognition, models generally perform better withsemantic maps than with realistic maps, except forQwen-VL, showing an improvement of 20.35%.However, this advantage decreases in more com-plex tasks. For Top-View Localization and StaticSpatial Reasoning, models struggle to utilize se-mantic top-view maps, yielding performances akinto random baselines in both EM and PM accuracy.One possible explanation is that the semantic top-view image and the input prompt with color-objectmapping deviate too much from the models train-ing data distribution. This is further evidenced bythe predictions from open-sourced models such asQwen-VL, which fail to respond to instructions andanswer with numbers or RGB values 91.25% of thetime for Top-View Localization and 47.65% of thetime for Static Spatial Reasoning.",
  "(b) Performance with semantic top-view maps": ": Visualization of fine-grained comparison with 10 models and humans on 9 sub-tasks using realistic andsemantic top-view maps, demonstrating that most current models perform on par with random baseline in spatialreasoning and has a large gap with human performance. Exact numbers are reported in in the Appendix. Fine-Grained Insights with Sub-Tasks. Modelsusing realistic maps excel more in the sub-task ofScene Recognition, which involves larger entities,compared to Object Recognition. This gap is alsoevident in a 12.66% and 19.73% performance dif-ference between object-level and scene-level local-ization with both map types. Conversely, with se-mantic maps, the model struggles more with scene-level recognition than with realistic maps, showingan 11.09% lower performance than object-levelrecognition among closed-source models. Mostmodels perform similarly to a random baseline inreasoning over spatial relations but show higheraccuracy in scene counting. This likely occurs be- cause 95% of the correct room counts are withina narrow range (1 or 2), reflecting real-life dis-tributions. Thus, models leverage commonsenseknowledge as the shortcut for counting, as seen inthe 54.73% performance gap (with GPT-4V) be-tween counting scenes and actions. However, thespatial localization and reasoning abilities of bothopen-source and closed-source models still remainunsatisfactory, even at the level of sub-tasks.",
  ": Performance (EM) between human and GPT-4V on all the sub-tasks, demonstrating a huge gap be-tween GPT-4V and human": "this end, we recruited 4 human participants whowere not involved in dataset creation for humanevaluation. A total of 60 data points with realistictop-view maps are randomly selected from the sub-tasks, covering all fine-grained question types.6 Weuse Fleiss Kappa as the measure of inter-annotatoragreement. The kappa score is 0.747, indicatingsubstantial agreement shared by the human partic-ipants according to Landis and Koch (1977). Theaverage performance of the human participants isshown in . The experimental results showthat there is still a large gap with human perfor-mance by over 50% across all the sub-tasks thatinvolve spatial awareness. We also observe thatwith GPT-4V, human performs 47.8% higher thanthe model on average. The gap between human andmodel performance is larger on complex reasoningtasks compared to the recognition tasks, indicatingplenty of room for improvement. Chain-of-Thought Helps Elicit Spatial Reason-ing. Due to the compositionality of Static SpatialReasoning based on Top-View Recognition and Lo-calization in task design, the model is supposed toanswer the question based on the locations of theentities in the top-view map. Inspired by this re-quirement, we explored whether Chain-of-Thought(CoT) reasoning (Wei et al., 2022) could facilitatespatial reasoning by initially prompting the modelto localize entities before producing the final an-swer to the question. To implement this, we mod-ified the instruction to include: You should firstlocalize the entity and then answer the questionbased on the locations, thereby encouraging themodel to process information and think step by step. 6We did not run human evaluation on semantic maps be-cause they are inherently easier to reason over; they skip theprocess of recognizing the objects before reasoning, whichmakes the task simpler but with more sufficient and accurateinformation for reasoning.",
  ": Comparison of model performance (EM) w/and w/o Chain of Thought (CoT) on Static Spatial Rea-soning, showing that CoT helps elicit spatial reasoning": "Considering that CoT has shown effectiveness inlarger models (Wei et al., 2022; Li et al., 2023c),we conducted experiments with GPT-4V and Gem-ini to evaluate this hypothesis. As shown in , incorporating CoT into the reasoning processnotably enhances performance. Specifically, themodels accuracy improved by 4.58% when usingrealistic maps and 6.34% with semantic maps forGPT-4V. This improvement underscores the po-tential of step-by-step reasoning in enhancing theefficacy of spatial reasoning tasks, but there is stilla substantial performance gap to the human ceiling.",
  "Conclusion": "In this study, we designed four tasks to examine thecapabilities of VLMs as top-view spatial reason-ers, progressing from basic top-view map compre-hension to dynamic spatial reasoning along nav-igation paths. To enable investigation into top-view spatial reasoning abilities of VLMs, we col-lected a novel dataset, TOPVIEWRS, which in-cludes 11,384 multiple-choice questions, featuringphoto-realistic and semantic top-view maps as thevisual input. Our extensive experiments involvedevaluating 10 VLMs across various model familiesand sizes on TOPVIEWRS. The results highlight acritical observation: particularly in complex reason-ing tasks, VLMs frequently perform only as wellas a random baseline, with even more pronounceddeficits when handling tasks with semantic maps.Moreover, there is a noticeable performance gapcompared to human annotators, underscoring thesignificant potential for further improvements inthis area. In response to these findings, we dis-covered that employing chain-of-thought reasoningenhances model performance in spatial reasoningby 5.82%. Despite this progress, the overall perfor-mance of VLMs on spatial reasoning remains lessthan satisfactory. We hope that our study can setthe stage for future research in multimodal spatialreasoning and encourage further investigations into",
  "Limitations": "The TOPVIEWRS dataset primarily evaluatesmodel performance in entity recognition, localiza-tion, and spatial reasoning over 2D top-view maps.However, it does not yet include task-oriented plan-ning with spatial awareness, which involves morecomplex sequential decision-making and dynamicinteractions.Further, our dataset assumes one correct answerper question, but exploring scenarios with multiplecorrect answers or no correct answers could furtherchallenge systems and provide valuable insights.We also advocate for further research to explorehow spatial awareness in models impacts down-stream tasks such as navigation instruction genera-tion (Li et al., 2024a) and task completion by lan-guage agents in real-world environments (Parasharet al., 2023).Moreover, our study is currently limited to 2Dtop-view maps, whereas spatial reasoning can en-compass a variety of modalities and perspectives,such as 3D point clouds.From the perspective of the models, the rapidprogress in VLMs makes it hard to include allnew releases such as Idefics 2 (Laurenon et al.,2024). Additionally, multimodal in-context learn-ing (MICL) remains underexplored and is only sup-ported by VLMs trained with interleaved image-text data (Baldassini et al., 2024). Although notuniversal across all VLMs, MICL has been effec-tive in handling out-of-distribution tasks (Zhanget al., 2024b), which could also be interesting inTOPVIEWRS, especially with semantic maps asvisual inputs. In future work, we aim to extendour analysis to include more modalities, evaluate abroader range of models and their capabilities, andinvestigate additional downstream tasks involvingspatial awareness.",
  "tion above the local minimum average. They haveconsented to the use of their annotations in ourresearch. We do not see any potential risk of ourproject": "The work was partly supported by a Royal Soci-ety University Research Fellowship Inclusive andSustainable Language Technology for a Truly Mul-tilingual World (no 221137) awarded to Ivan Vulic.We thank Fangyu Liu and Ye Mao for constructivefeedback on the draft of this paper. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,and Jingren Zhou. 2023. Qwen-vl: A versatile vision-language model for understanding, localization, textreading, and beyond. Preprint, arXiv:2308.12966.",
  "Folco Bertini Baldassini, Mustafa Shukor, MatthieuCord, Laure Soulier, and Benjamin Piwowarski. 2024.What makes multimodal in-context learning work?Preprint, arXiv:2404.15736": "Pratyay Banerjee, Tejas Gokhale, Yezhou Yang, andChitta Baral. 2021. Weakly supervised relative spa-tial reasoning for visual question answering. In Pro-ceedings of the IEEE/CVF International Conferenceon Computer Vision (ICCV), pages 19081918. Cesar Cadena, Luca Carlone, Henry Carrillo, YasirLatif, Davide Scaramuzza, Jose Neira, Ian Reid, andJohn J. Leonard. 2016.Past, present, and futureof simultaneous localization and mapping: Towardthe robust-perception age. IEEE Transactions onRobotics, 32(6):13091332. Khyathi Raghavi Chandu, Yonatan Bisk, and Alan WBlack. 2021. Grounding grounding in NLP. InFindings of the Association for Computational Lin-guistics: ACL-IJCNLP 2021, pages 42834305, On-line. Association for Computational Linguistics. Angel Chang, Angela Dai, Thomas Funkhouser, MaciejHalber, Matthias Niessner, Manolis Savva, ShuranSong, Andy Zeng, and Yinda Zhang. 2017. Mat-terport3d: Learning from rgb-d data in indoor envi-ronments. International Conference on 3D Vision(3DV). Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter,Danny Driess, Pete Florence, Dorsa Sadigh, LeonidasGuibas, and Fei Xia. 2024. Spatialvlm: Endowingvision-language models with spatial reasoning capa-bilities. Preprint, arXiv:2401.12168.",
  "Spatially-aware transformers for embodied agents.In The Twelfth International Conference on LearningRepresentations": "Maurice Diesendruck, Jianzhe Lin, Shima Imani, Gay-athri Mahalingam, Mingyang Xu, and Jie Zhao. 2024.Learning how to ask: Cycle-consistency refinesprompts in multimodal foundation models. Preprint,arXiv:2402.08756. Tianai Dong, Alberto Testoni, Luciana Benotti, and Raf-faella Bernardi. 2021. Visually grounded follow-upquestions: a dataset of spatial questions which re-quire dialogue history. In Proceedings of SecondInternational Combined Workshop on Spatial Lan-guage Understanding and Grounded Communicationfor Robotics, pages 2231, Online. Association forComputational Linguistics. Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao,Bin Wang, Linke Ouyang, Xilin Wei, SongyangZhang, Haodong Duan, Maosong Cao, WenweiZhang, Yining Li, Hang Yan, Yang Gao, XinyueZhang, Wei Li, Jingwen Li, Kai Chen, ConghuiHe, Xingcheng Zhang, Yu Qiao, Dahua Lin, andJiaqi Wang. 2024. Internlm-xcomposer2: Master-ing free-form text-image composition and compre-hension in vision-language large model. Preprint,arXiv:2401.16420.",
  "Gemini Team Google. 2024.Gemini:A familyof highly capable multimodal models.Preprint,arXiv:2312.11805": "Meera Hahn, Jacob Krantz, Dhruv Batra, Devi Parikh,James Rehg, Stefan Lee, and Peter Anderson. 2020.Where are you? localization from embodied dialog.In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 806822, Online. Association for Computa-tional Linguistics. Meera Hahn and James M. Rehg. 2022. Transformer-based localization from embodied dialog with large-scale pre-training. In Proceedings of the 2nd Confer-ence of the Asia-Pacific Chapter of the Associationfor Computational Linguistics and the 12th Interna-tional Joint Conference on Natural Language Pro-cessing (Volume 2: Short Papers), pages 295301,Online only. Association for Computational Linguis-tics.",
  "Amita Kamath, Jack Hessel, and Kai-Wei Chang. 2023": "Whats up with vision-language models? investi-gating their struggle with spatial reasoning. In Pro-ceedings of the 2023 Conference on Empirical Meth-ods in Natural Language Processing, pages 91619175, Singapore. Association for Computational Lin-guistics. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.Brown, Benjamin Chess, Rewon Child, Scott Gray,Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.Scaling laws for neural language models. Preprint,arXiv:2001.08361.",
  "Chengzu Li, Chao Zhang, Simone Teufel, Rama SanandDoddipatla, and Svetlana Stoyanchev. 2024a. Seman-tic map-based generation of navigation instructions.Preprint, arXiv:2403.19603": "Hao Li, Jinfa Huang, Peng Jin, Guoli Song, Qi Wu,and Jie Chen. 2023b. Weakly-supervised 3d spa-tial reasoning for text-based visual question an-swering. IEEE Transactions on Image Processing,32:33673382. Hongyang Li, Chonghao Sima, Jifeng Dai, WenhaiWang, Lewei Lu, Huijie Wang, Jia Zeng, Zhiqi Li,Jiazhi Yang, Hanming Deng, Hao Tian, Enze Xie,Jiangwei Xie, Li Chen, Tianyu Li, Yang Li, YuluGao, Xiaosong Jia, Si Liu, Jianping Shi, Dahua Lin,and Yu Qiao. 2024b. Delving into the devils of birds-eye-view perception: A review, evaluation and recipe.IEEE Transactions on Pattern Analysis and MachineIntelligence, 46(4):21512170. Hongyang Li, Chonghao Sima, Jifeng Dai, WenhaiWang, Lewei Lu, Huijie Wang, Jia Zeng, Zhiqi Li,Jiazhi Yang, Hanming Deng, Hao Tian, Enze Xie,Jiangwei Xie, Li Chen, Tianyu Li, Yang Li, YuluGao, Xiaosong Jia, Si Liu, Jianping Shi, Dahua Lin,and Yu Qiao. 2024c. Delving into the devils of birds-eye-view perception: A review, evaluation and recipe.IEEE Transactions on Pattern Analysis and MachineIntelligence, 46(4):21512170. Liunian Harold Li, Jack Hessel, Youngjae Yu, XiangRen, Kai-Wei Chang, and Yejin Choi. 2023c. Sym-bolic chain-of-thought distillation: Small models canalso think step-by-step. In Proceedings of the 61stAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 26652679, Toronto, Canada. Association for Computa-tional Linguistics. Yunxin Li, Longyue Wang, Baotian Hu, Xinyu Chen,Wanqi Zhong, Chenyang Lyu, Wei Wang, and MinZhang. 2023d. A comprehensive evaluation of gpt-4von knowledge-intensive visual question answering.Preprint, arXiv:2311.07536.",
  "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong JaeLee. 2023b. Visual instruction tuning": "Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,Songyang Zhang, Wangbo Zhao, Yike Yuan, JiaqiWang, Conghui He, Ziwei Liu, Kai Chen, and DahuaLin. 2023c. Mmbench: Is your multi-modal modelan all-around player? Preprint, arXiv:2307.06281. ManolisSavva*,AbhishekKadian*,OleksandrMaksymets*, Yili Zhao, Erik Wijmans, Bhavana Jain,Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Ma-lik, Devi Parikh, and Dhruv Batra. 2019. Habitat:A Platform for Embodied AI Research. In Proceed-ings of the IEEE/CVF International Conference onComputer Vision (ICCV).",
  "Dhabi, United Arab Emirates. Association for Com-putational Linguistics": "Roshanak Mirzaee, Hossein Rajaby Faghihi, QiangNing, and Parisa Kordjamshidi. 2021. SPARTQA:A textual question answering benchmark for spatialreasoning. In Proceedings of the 2021 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 45824598, Online. Association forComputational Linguistics. Ida Momennejad, Hosein Hasanbeig, Felipe Vieira Fru-jeri, Hiteshi Sharma, Nebojsa Jojic, Hamid Palangi,Robert Ness, and Jonathan Larson. 2023. Evaluatingcognitive maps and planning in large language mod-els with cogeval. In Advances in Neural InformationProcessing Systems, volume 36, pages 6973669751.Curran Associates, Inc. Laksh Nanwani, Anmol Agarwal, Kanishk Jain, RaghavPrabhakar,Aaron Monis,Aditya Mathur,Kr-ishna Murthy Jatavallabhula, A. H. Abdul Hafez,Vineet Gandhi, and K. Madhava Krishna. 2023.Instance-level semantic maps for vision languagenavigation. In 2023 32nd IEEE International Confer-ence on Robot and Human Interactive Communica-tion (RO-MAN). IEEE.",
  "OpenCompass Contributors. 2023.Opencompass:A universal evaluation platform for foundationmodels": "Priyam Parashar, Vidhi Jain, Xiaohan Zhang, Jay Vakil,Sam Powers, Yonatan Bisk, and Chris Paxton. 2023.Slap: Spatial-language attention policies. In Pro-ceedings of The 7th Conference on Robot Learning,volume 229 of Proceedings of Machine LearningResearch, pages 35713596. PMLR. Lang Peng, Zhirong Chen, Zhangjie Fu, PengpengLiang, and Erkang Cheng. 2023.Bevsegformer:Birds eye view semantic segmentation from arbitrarycamera rigs. In 2023 IEEE/CVF Winter Conferenceon Applications of Computer Vision (WACV). IEEE. Chengwei Qin, Aston Zhang, Zhuosheng Zhang, JiaaoChen, Michihiro Yasunaga, and Diyi Yang. 2023a.Is ChatGPT a general-purpose natural language pro-cessing task solver? In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 13391384, Singapore. Associa-tion for Computational Linguistics. Zequn Qin, Jingyu Chen, Chao Chen, Xiaozhi Chen,and Xi Li. 2023b. Unifusion: Unified multi-viewfusion transformer for spatial-temporal represen-tation in birds-eye-view.In Proceedings of theIEEE/CVF International Conference on ComputerVision (ICCV), pages 86908699.",
  "Stepgame: A new benchmark for robust multi-hopspatial reasoning in texts. In Proceedings of the AAAIConference on Artificial Intelligence, volume 36,pages 1132111329": "Aleksandar Shtedritski, Christian Rupprecht, and An-drea Vedaldi. 2023. What does clip know about a redcircle? visual prompt engineering for vlms. In 2023IEEE/CVF International Conference on ComputerVision (ICCV). IEEE. Andrew Szot, Alex Clegg, Eric Undersander, ErikWijmans, Yili Zhao, John Turner, Noah Maestre,Mustafa Mukadam, Devendra Chaplot, OleksandrMaksymets, Aaron Gokaslan, Vladimir Vondrus,Sameer Dharur, Franziska Meier, Wojciech Galuba,Angel Chang, Zsolt Kira, Vladlen Koltun, JitendraMalik, Manolis Savva, and Dhruv Batra. 2021. Habi-tat 2.0: Training home assistants to rearrange theirhabitat. In Advances in Neural Information Process-ing Systems (NeurIPS). Damien Teney, Lingqiao Liu, and Anton Van Den Hen-gel. 2017. Graph-structured representations for vi-sual question answering. In 2017 IEEE Conferenceon Computer Vision and Pattern Recognition (CVPR),pages 32333241.",
  "Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, andLiwei Wang. 2024. Visual autoregressive modeling:Scalable image generation via next-scale prediction.Preprint, arXiv:2404.02905": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288.",
  "David Unger, Nikhil Gosala, Varun Ravi Kumar,Shubhankar Borse, Abhinav Valada, and SenthilYogamani. 2023.Multi-camera birds eye viewperception for autonomous driving.Preprint,arXiv:2309.09080": "Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,and Denny Zhou. 2022. Chain-of-thought prompt-ing elicits reasoning in large language models. InAdvances in Neural Information Processing Systems,volume 35, pages 2482424837. Curran Associates,Inc. Jason Weston, Antoine Bordes, Sumit Chopra, andToms Mikolov. 2016. Towards ai-complete questionanswering: A set of prerequisite toy tasks. In 4th In-ternational Conference on Learning Representations,ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,Conference Track Proceedings.",
  "Yutaro Yamada, Yihan Bao, Andrew Kyle Lampinen,Jungo Kasai, and Ilker Yildirim. 2024.Evaluat-ing spatial understanding of large language models.Transactions on Machine Learning Research": "Cheng Yan, Guansong Pang, Lei Wang, Jile Jiao, Xue-tao Feng, Chunhua Shen, and Jingjing Li. 2021. Bv-person: A large-scale dataset for bird-view personre-identification. In Proceedings of the IEEE/CVF In-ternational Conference on Computer Vision (ICCV),pages 1094310952. Zhun Yang, Adam Ishay, and Joohyung Lee. 2023. Cou-pling large language models with logic programmingfor robust and general reasoning from text. In Find-ings of the Association for Computational Linguis-tics: ACL 2023, pages 51865219, Toronto, Canada.Association for Computational Linguistics.",
  "Chao Zhang,Mohan Li,Ignas Budvytis,andStephan Liwicki. 2024a. Dialoc: An iterative ap-proach to embodied dialog localization. Preprint,arXiv:2403.06846": "Xingxuan Zhang, Jiansheng Li, Wenjing Chu, JunjiaHai, Renzhe Xu, Yuqing Yang, Shikai Guan, Ji-azheng Xu, and Peng Cui. 2024b. On the out-of-distribution generalization of multimodal large lan-guage models. Preprint, arXiv:2402.06599. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,Xiaolei Wang, Yupeng Hou, Yingqian Min, Be-ichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,Chen Yang, Yushuo Chen, Zhipeng Chen, JinhaoJiang, Ruiyang Ren, Yifan Li, Xinyu Tang, ZikangLiu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.2023. A survey of large language models. Preprint,arXiv:2303.18223.",
  "In addition to which provides a briefoverview of previous work most relevant to ourwork, for completeness we also provide additionalrelated work focused on unimodal spatial reasoningfrom text only": "Spatial Reasoning on Text. Spatial reasoninghas been investigated with the advancement ofLLMs (Yamada et al., 2024). Various benchmarkshave been proposed to evaluate models spatial rea-soning abilities, including relative spatial relationrecognition (Weston et al., 2016; Mirzaee et al.,2021; Shi et al., 2022), natural language navigation(Yamada et al., 2024), and planning (Momennejadet al., 2023). Mirzaee and Kordjamshidi (2022)suggest that introducing synthetic data of spatialreasoning when pre-training helps to improve thespatial awareness of the model. Yang et al. (2023)justify the feasibility of using a logical form asan intermediate representation to improve the spa-tial reasoning ability in easy scenarios. Insteadof describing the spatial relations with natural lan-guage, Wu et al. (2024) feed the model with a 2Dsquare grid similar to ASCII-art format and provethat visualising the reasoning procedure explicitlyhelps to improve the models ability in multi-hopspatial reasoning. Constrained by language de-scriptions, most datasets focus on reasoning oversymbols within simple scenarios (e.g. grid-basednavigation) and are synthetically generated. How-ever, real-life scenarios are often more complexand rich in physical semantics. This raises con-cerns about the models actual spatial reasoningabilities compared to their proficiency in under-standing linguistic patterns.",
  "BFurther Details on DatasetConstruction": "The TOPVIEWRS is derived from Matterport3D(Chang et al., 2017) and is supposed to be used fornon-commercial academic use only, under the Termof Use (Matterport End User Licence AgreementFor Academic Use of Model Data).In addition to the main content in , weprovide further details with regard to TOPVIEWRSdataset construction in what follows.",
  "Photo-Realistic Top-View Map. We extract real-istic top-view maps using MeshLab by placing anorthographic camera on the top of the 3D scenesand taking a camera shot": "Semantic Top-View Map. We construct themusing the Habitat simulation environment (Mano-lis Savva* et al., 2019; Szot et al., 2021). Foreach building floor, Matterport3D contains the 2Dand 3D semantic segmentation human annotations,which can be retrieved to identify the type of ob-jects as well as the rooms. The 3D coordinates ofthe entitys (object and room) center (xi, yi, hi) andthe size of the entitys bounding box (wx, wy, wh)can also be retrieved as part of the circumstantialinformation. This information is then used for theconstruction of the semantic top-view map.When we obtain the object information forthe purpose of constructing a top-view seman-tic map, we design certain rules to exclude spe-cific types of objects from all 40 object anno-tation categories of Matterport3D. We believethese objects could either 1) be less meaningfulin terms of semantics or 2) take up a large areain the semantic map, which obstructs other ob-jects beneath. The filtered objects include:misc,ceiling,objects,floor,wall,void,curtain,column,beam,board panel.We also filter out the objects based on theirheights hobj and sizes wobj compared to the roomsheights hroom and sizes wroom. We only keep theobjects if they satisfy the following relations:",
  "2wobj": "After having all the object annotations, we usethe get_topdown_map API of the Habitat simula-tor to get the top-down map of the scene, whichdescribes the navigable area and the overall shapeof the environment, but without any object annota-tions. Based on this map, we then draw the bound-ing boxes with different colors to represent the objects in the environments. Considering that theobjects on the top may obstruct the bottom ob-jects in the top-view map, to mimic this characteris-tic, we create the semantic top-view map based onthe heights of the objects, where lower objects aredrawn first. shows the mapping betweenthe RGB values and object types used for the cre-ation of a semantic top-view map in our work.After having the top-view maps of the wholefloor, we crop them into smaller rooms accordingto the region boundaries obtained from the Habitatsimulator.",
  "B.2Structured Question Framework Design": "In order to minimize human labor and standard-ize the collection pipeline, we adopt the template-based question generation method following thepractice of Liu et al. (2023a); we design 15 differ-ent templates in total to construct the sub-tasks foreach task. In particular, we consider benchmarkingdifferent perspectives of the models ability withineach task in a fine-grained manner when design-ing the templates. The question templates are alsomulti-scale in terms of objects or rooms with full orpartial top-view maps for Top-View Recognition,Top-View Localization and Static Spatial Reason-ing. For Dynamic Spatial Reasoning, the designedquestions evaluate the recognition and reasoningfrom the scale of single navigation points (DynamicAction Counting and Spatial Localization) to thewhole path (Dynamic Relative Spatial Reasoning).Below we provide the designed templates for all9 sub-tasks, with some examples shown in .In what follows, we also introduce the logic for se-lecting the correct answer and other wrong choiceswhen constructing the multiple-choice questions.",
  "living room, stairs": "B.2.3Static Spatial Reasoning lists the templates for the Static SpatialReasoning task.For rooms, we restrict the re-gions within the same range as in Top-View Lo-calization. Concerning objects, we focus on theobjects that are common and large enough to rec-ognize in daily life, which includes: chair,table,cushion,sofa,bed,chest_of_drawers,sink,toilet,bathtub,stool,plant,stairs,shower,fireplace,gym_equipment,seating. B.2.4Dynamic Spatial ReasoningFor Dynamic Action Counting, we define that avalid turn should involve more than a 30-degreerotation. For Dynamic Relative Spatial Reasoning,the direction is also defined by the relative spatialrelation between the starting point and ending point,where the spatial description is determined by 30-degree intervals.",
  ": Templates for Dynamic Action Counting, Dy-namic Relative Spatial Reasoning, and Dynamic SpatialLocalization sub-tasks": "the answer to the questions, because we have allthe spatial information and semantic annotation ofthe objects in the scene, we write a set of ruleswith code for each type of question in order toautomatically obtain the golden answer accordingto the simulation environments. For all the wrongchoices in the multiple-choice settings, they arerandomly chosen from other possible candidates ofthe same kind (e.g. objects, rooms, numbers, etc.).After having all the options for multiple-choicequestions, we randomize the order of the options tomake the correct choices evenly distributed amongpossible options A, B, C, and D.",
  "In our preliminary quality control, we realized thatsemantic annotations of environments may some-": "times be inaccurate. Moreover, even though weexclude some unreasonable objects, the top viewof certain objects can sometimes be challengingto recognize, even for humans. To address theseissues, we have implemented a second stage in ourdataset creation process: alignment and verificationbased on human judgments.When validating the automatically collecteddata, the human participants are supposed to checkthe correctness of the question-answer pair andchoose one of the following four actions accordingto their own judgments: 1) skip the instance if itcannot be repaired and/or looks strange, 2) modifythe pair by replacing the options or the entities inthe question in order to make it answerable by hu-mans, 3) correct the answer if it is wrong, 4) keepthe data if it is answerable by humans and correct.In order to ensure the quality of the dataset, wecommunicated to the human participants that theyare supposed to be cautious when accepting a datapoint/instance. On a practical level, the participantsmay either discard this data point or modify the op-tions of this data to make the correct choice moredistinguishable by humans. This helps to excludethe data points where different human judges maydiverge and thus ensure the alignment between thedataset and general human judgments. We assurethat the alignment process does not include anyinformation with regard to personal identificationor offensive content.In our experiments, we also provide the corre-sponding rules of how we obtain the answer for themodel with textual description in the prompt (seeAppendix C.2).",
  "B.4Dataset Statistics": "We provide further insight into different portions ofthe TOPVIEWRS dataset with regard to the objectand room distribution in , whereas statisticsover different sub-tasks are provided in .The visualization demonstrates that the objectsor regions that are hard to recognize (e.g. gymequipment, utility room, etc.) have fewer occur-rences in the dataset compared to those which areeasier to identify and typically more common (e.g.bed, table, bedroom, etc.). Bed, chair and tableare the top-3 most frequently mentioned objectsand bedroom, dining room and living room are themost common regions in the dataset. Among all thespatial descriptions, the diagonal spatial relations(e.g. top right, up left) are more frequently referredto as the correct choice as relative spatial descrip- tions in Static Spatial Reasoning while being lessfrequently used as absolute spatial descriptions inTop-View Localization.Regarding the dataset size per each sub-task,object-level recognition and localization take alarge portion of data in the Top-View Recognitionand Localization tasks. For Static Spatial Reason-ing, reasoning over relative spatial relations takesthe main part of the data. Dynamic Spatial Local-ization has the largest number of data instancesoverall. The numbers are different with realisticmaps and semantic maps for each task. The dispar-ity stems from the second stage of dataset creation,where the human annotators have excluded moredata points associated with more complex, photo-realistic maps due to various possible reasons.",
  "C.2Prompts": "and 12 show the prompt templates of eachtask used in the main experiments () with re-alistic and semantic top-view maps as visual input,respectively. and 14 show the prompt tem-plates used for Chain-of-Thought reasoning usingrealistic and semantic top-view maps ().Within the prompt templates, <QUESTION> and<OPTIONS> are replaced with the question and op-tion list O = {o0, o1, o2, o3} (e.g. A. bed; B.chair; C. table; D. cushion). For semantic top-view maps, <MAPPING> is replaced with the RGB-object mapping, as shown below.",
  "Suppose you are a navigation agent tracingthe path. Your job is to assess whetherthere's a turn at each intermediate point": "bed chair table cushion sofa seating sink stairs toilet stool chest_of_drawers shower cabinet bathtub fireplace plant gym_equipment 600Object Distribution of Correct Answer (Realistic map) bed chair table cushion sofa seating sink toilet stairs stool chest_of_drawers fireplace shower bathtub cabinet plant gym_equipment Object Distribution of Correct Answer (Semantic map) bedroom dining room living room bathroom kitchen stairs toilet office bar lounge familyroom/lounge laundryroom/mudroom rec/game closet porch/terrace/deck utilityroom/toolroom Room Distribution of Correct Answer (Realistic Map) bedroom dining room living room bathroom kitchen toilet stairs office bar lounge familyroom/lounge laundryroom/mudroom rec/game closet porch/terrace/deck utilityroom/toolroom Room Distribution of Correct Answer (Semantic Map)",
  "Dynamic Spatial Reasoning": "This is a semantic top-view map of a room with the navigation path. In the semantic map, variousobjects are depicted by colored bounding boxes, each with its corresponding color, and there maybe instances of overlap between them. The navigation path starts from the green triangle (RGB ) and ends at the red star (RGB ). The direction of the path is denoted by a seriesof yellow arrows (RGB ), with intermediate points highlighted in RGB .Below are the RGB color codes associated with each object and symbol, presented in the formatRGB -> Object:<MAPPING><TASK-SPECIFIC INSTRUCTION> Please respond to the question below by selecting one choicefrom a list of available options provided. Your response should only include the letter of the chosenoption (A, B, C, or D) with no additional explanation.Question: <QUESTION>Options: <OPTIONS>;Answer:",
  "Top-View Recognition, Top-View Localization and Static Spatial Reasoning": "This is a semantic top-view map of a room. Various objects are depicted by colored boundingboxes, each with its corresponding color, and there may be instances of overlap between them.Below are the RGB color codes associated with each object, presented in the format RGB ->Object:<MAPPING>Please respond to the question below by selecting one choice from a list of available optionsprovided. Your response should only include the letter of the chosen option (A, B, C, or D) withno additional explanation.Question: <QUESTION>Options: <OPTIONS>;Answer:",
  "Static Spatial Reasoning": "This is a semantic top-view map of a room. Various objects are depicted by colored boundingboxes, each with its corresponding color, and there may be instances of overlap between them.Below are the RGB color codes associated with each object, presented in the format RGB ->Object:<MAPPING>Please respond to the question below by selecting one choice from a list of available optionsprovided. You should explain your reasoning step-by-step by first localizing the entities and thenreasoning over the question based on the locations. You should conclude your chosen option (A, B,C, or D) starting with The answer is .Question: <QUESTION>Options: <OPTIONS>;Answer: Lets think step by step."
}