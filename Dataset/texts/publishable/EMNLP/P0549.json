{
  "Abstract": "State-of-the-artLargeLanguageModels(LLMs) are accredited with an increasingnumber of different capabilities, ranging fromreading comprehension over advanced math-ematical and reasoning skills to possessingscientific knowledge. In this paper we focuson multi-hop reasoningthe ability to identifyand integrate information from multiple textualsources. Given the concerns with the presenceof simplifying cues in existing multi-hopreasoning benchmarks, which allow modelsto circumvent the reasoning requirement, weset out to investigate whether LLMs are proneto exploiting such simplifying cues. We findevidence that they indeed circumvent therequirement to perform multi-hop reasoning,but they do so in more subtle ways than whatwas reported about their fine-tuned pre-trainedlanguage model (PLM) predecessors.Wepropose a challenging multi-hop reasoningbenchmark by generating seemingly plausiblemulti-hop reasoning chains that ultimately leadto incorrect answers. We evaluate multipleopen and proprietary state-of-the-art LLMsand show that their multi-hop reasoning perfor-mance is affected, as indicated by up to 45%relative decrease in F1 score when presentedwith such seemingly plausible alternatives. Wealso find thatwhile LLMs tend to ignoremisleading lexical cuesmisleading reasoningpaths indeed present a significant challenge.The code and data are made available at",
  "*Work done while author was at AICS": "Original Q: Who created the 2003 remake of the 1983 over-head view, vehicular combat game developed by Bally Mid-way?HotpotQA Paragraph 1: Highway Pursuit is a computergame remake of Spy Hunter created by Adam Dawes [.. .].HotpotQA Paragraph 2: Spy Hunter is an overhead view,vehicular combat game developed by Bally Midway andreleased in arcades in 1983.GPT-4 Answer: Adam Dawes Fake paragraph 1: Road Blaster, developed by Atari Cor-poration in 1983, stands out as a seminal entry in the vehicularcombat game genre [...].Fake paragraph 2: The 2003 remake of Road Blaster wasmasterfully recreated by Jonathan Fields [.. .].GPT-4 Answer: Jonathan Fields : Our proposed method evaluates the multi-hop reasoning capabilities of Large Language Modelsby adding seemingly plausible, yet ultimately wrongalternate reasoning paths, impacting the reasoning per-formance of state-of-the-art LLMs such as GPT-4. ous understanding and reasoning capabilities, rang-ing from arithmetic (Cobbe et al., 2021), deduc-tive (Saparov et al., 2023) and formal (Schlegelet al., 2022b; Madusanka et al., 2023) reason-ing and possessing general (AlKhamissi et al.,2022), and domain-specific (He et al., 2023) knowl-edge. Due to their size and generalisation capa-bilities (Brown et al., 2020), their evaluation onbenchmarks requiring such types of reasoning istypically performed in zero- or few-shot settings onmany NLP tasks, without the need for fine-tuningdatasets.These zero- and few-shot capabilities seem toalleviate one of the weaknesses identified with theprevious generation of fine-tuning based NLP archi-tectures such as transformer-based (Vaswani et al.,2017), and pre-trained language models (Devlinet al., 2019)the reliance on data-set specific arte-facts (Gururangan et al., 2018; Schlegel et al.,2022a) and, as a consequence, lack of generalisa-tion beyond specific datasets. For example, in oneof the popular reading comprehension and reason-ing benchmarks (Dua et al., 2019), the majority of questions starting with How many can be an-swered correctly with 2. Following standard fine-tuning practice and splitting data in train and testrandomly, such a simple heuristic will be presentin both training and evaluation data, so a fine-tunedmodel will learn it and obtain high scores, withoutnecessarily performing reasoning. LLMs seem-ingly circumvent this issue, as they are not fine-tuned on benchmark data. As such, they are notexposed to simplifying dataset artefacts by design,and it is reasonable to assume that they do not learnto exploit them.However, while there is a growing body ofwork investigating the strengths and limitationsof LLMs (Huang et al., 2023b), little research hasbeen carried out to validate this assumption, andto investigate whether and to what extent LLMsinherit the dataset artefact weaknesses of theirfine-tuned predecessors. This is an important re-search question to pursue, motivated by recent find-ings on benchmark leakage into pre-training orinstruction-tuning data (Deng et al., 2024), whichinvalidate the zero-shot setting and potentially al-low LLMs to learn such dataset artefacts. Anotherline of research suggests that LLMs tend to over-reason (Chiang and Lee, 2024), perhaps due tosycophancy (Perez et al., 2023), i.e., the tendencyto generate the presumably preferred answer overthe correct one, leading to complicated reasoningwhere none is required.In this paper, we turn our attention to the well-studied capability to perform multi-hop reasoningand reading comprehensionthat is, to integratetextual information from multiple different sourcedocuments. Typically, this capability is evaluatedby asking questions where the necessary infor-mation to arrive at the correct answer is spreadacross multiple documents (Yang et al., 2018a;Welbl et al., 2018; Inoue et al., 2020). It is im-portant to understand to what extent NLP methodspossess this capability, as it is required for manyreal-world tasks, such as retrieval-augmented gen-eration (Lewis et al., 2020) when summarising re-trieved documents, and because it is a necessaryprerequisite to human-level reading comprehen-sion (Kintsch, 1988).Previous work has shown that NLP architecturesmight possess inadequate capabilities to performmulti-hop reasoning (Min et al., 2019a). However,these findings were established before the advent oflarge language models. To have a clear understand-ing of the limitations of the capabilities of state-of- the-art research, it is crucial to re-investigate theseclaims with the current generation of LLM-basedapproaches (Bowman, 2022). While there is vividresearch on (open-book) multi-hop reasoning capa-bilities of LLMs (Sakarvadia et al., 2023; Liu et al.,2023; Yang et al., 2024), how well they performwhen presented with multiple, seemingly plausiblemulti-hop reasoning paths remains unclear.To address this gap, we focus on the capability ofLLMs to perform multi-hop reasoning when multi-ple seemingly plausible answers are present, whereonly minor details invalidate the alternative. Weshow that existing methodscalibrated to evaluatepre-LLM architecturesare inadequate to evaluateLLMs, and that LLM reasoning failures are indeeddistinct from their fine-tuned PLM predecessors.We present a methodology to generate challengingexamples with plausible distractors to evaluateLLMs capabilities to perform multi-hop reasoningwhen presented with seemingly correct, but ulti-mately wrong and thus distracting evidence. Ourresults show that the reasoning capabilities of arange of open and proprietary LLMs, includingGPT-4, are affected by these plausible distrac-tors.",
  "Related Work": "It has been shown that basic pattern match-ing (Schlegel et al., 2020) and one-hop (Min et al.,2019a) models can solve a large proportion of ques-tions in multi-hop question answering datasets, pre-sumably because the answer sentence often con-tains keywords common with the question, thusnegating the need to follow a reasoning path andattend to multiple documents. Particularly Hot-potQA (Yang et al., 2018b), due to its multi-hopquestion design, was the subject of multiple studies.Approaches architecturally incapable of multi-hopreasoning still achieved close to state-of-the-art per-formance (Min et al., 2019a; Trivedi et al., 2020),suggesting questions answerable in such a way donot necessitate multi-hop reasoning.In light of these results, several adversarial at-tacks have been proposed to check whether thedataset evaluates multi-hop reasoning without ex-hibiting shortcuts, by ensuring that the correctanswer can only be procured if the evaluated modelcan retrieve and combine information from distinctreasoning hops. Jiang and Bansal (2019) eliciteddistracting paragraphs by using the titles of the goldparagraphs and the answer, which are subjected to phrase-level perturbations and word replacement,thus creating a distracting paragraph. Others de-composed the multi-hop questions in multiple sin-gle questions (Min et al., 2019b; Perez et al., 2020;Ding et al., 2021) (e.g. DecompRC in )showed that thetypically BERT- or other PLM-basedfine-tuned SOTA models struggled to an-swer both sub-questions correctly when answeringthe complete question, or were distracted by theiralterations, suggesting the presence of reasoningshortcuts (Tang et al., 2021).By design, these methods bear only negative pre-dictive power (Gardner et al., 2020): failing to seea performance drop does not imply that the modelperforms the evaluated capability well, but ratherthat the methodology might have limited suitabilityto evaluate the investigated phenomenon, i.e., multi-hop reasoning. As the methodologies presentedabove focus on fine-tuned models, they assumethat multi-hop reasoning is circumvented throughsimple, lexical similarity-based methods like wordmatching. For example, Jiang and Bansal (2019)do not consider that their generated paragraphs areisolated, as they contain no explicit reference toother paragraphs in the context, such as a sharednamed entity. Meanwhile, Ding et al. (2021)* onlyadd a single distracting sentence. Thus, simpleword matching, which ensures that the final an-swer is of the same entity type as in the question,can often lead to the correct answer. This mightnot be sufficient for LLMs, as theydue to theirsize and emergent capabilitiesmight circumventmulti-hop reasoning by exploiting more subtle tex-tual cues. Indeed, in our empirical study, we showthat existing methods, due to these limitations, donot adequately test an LLMs reasoning capabili-ties.Therefore, to analyse an LLMs ability to reasonmore adequately, we go beyond the state of theart and introduce a novel method to more effec-tively evaluate the multi-hop reasoning capabilitiesof LLMs. Specifically, we ensure the continuityof seemingly plausible alternative reasoning paths,which lead to answers that are ultimately wrong.To succeed, the model is required to pay close atten-tion to small yet important details in the questionsand paragraphs.This ability is important practically, for examplewhen an LLM is prompted to evaluate/summarisethe outcome of a debate, where both sides will",
  "*No public code/dataset was made available": "present plausible arguments with only one beingultimately correct (Sun et al., 2023; Li et al., 2024).With LLMs increasingly used to judge and improve(other) LLMs potentially similar outputs on thesame topic (Huang et al., 2023a), it is importantto establish, if they possess the necessary prerequi-sites to do so. More broadly, similar to other worksin this line of research, we look at linguistic com-petence rather than performance (Chomsky, 1965):if we accredit multi-hop reasoning capabilities toLLMs, then, similar to humans, we expect them toexhibit these capacities not only in the majority ofcases but in edge case scenarios as well, such aswhen presented with seemingly plausible alternatereasoning paths.",
  "Methodology": "In this section, we describe our approach to evalu-ating the multi-hop reasoning capabilities of LLMs.We do so by creating distractor paragraphs thatpresent seemingly plausible yet incorrect alterna-tive paths in the reasoning chain while ensuringthat this process doesnt affect the final solution.First, the question is treated as a two-hop ques-tion and converted into two sub-questions. Thisis done to be able to branch out alternative rea-soning paths from each of the sub-questions. Thesub-questions are analyzed to identify modifiableportions, which are then manipulated to create dis-tractor sub-questions that lead to a different an-swer and thus a different reasoning chain, which isultimately wrong, as the models are presented withthe original, unmodified question. The distractorsub-questions are finally used to generate dis-tractor paragraphs containing distractor answersutilizing an LLM.The method comprises three main steps: I. Ac-quiring the main entity, II. Extracting its modifiabledetails, and III. Creating the distractor paragraphs. Question: What year did Guns N Roses perform a promo fora movie starring Arnold Schwarzenegger as a former NewYork Detective?Sub question 1: Which movie stars Arnold Schwarzeneggeras a former New York Police detective?Sub question 2: What year did Guns N Roses perform apromo for End of Days (answer of the previous question)?",
  "I. Acquiring the main entityWe use the human-annotated sub-questions from Tang et al. (2021),as exemplified in . We define main en-": "tities as those that are the focus of the question.For example, in , the main entities for thesub-questions would be movie stars and yearrespectively. We choose the main entity in eachsub-question, using a few dependency parse-basedrules. Intuitively, we exploit the relations betweenthe wh-word and other noun phrases to extractthe main entity. Specifically:",
  "(i) If the wh question word WH is the root, andthere exists a word A with a dependency nsubjor nsubj:pass with WH as the head, A is themain entity": "(ii) Alternatively, if there exists a word A with adependency of type det, nsubj, or nsubj:passwith a wh-word WH:(a) If A is a noun, A is the main entity.(b) Otherwise, if A is a verb, the word Bhaving a relation acl:recl with B beingthe head, we mark B as the main entity.",
  "(ii) If the above rule is satisfied, we check if C orD has a dependency appos with any namedentity": "(iii) If there is no such relation, modifiers of D ofthe form nummod, amod, nmod, compound,or flat are used to get modifiable parts if themodifier isnt the main entity identified in theprevious steps. We extract the modifiers and not the object theymodify for two reasons: First, changing the objectoften causes the overall question to become nonsen-sical. Secondly, changing the modifier ensures aminimal yet semantically meaningful modificationof the question (Schlegel et al., 2021).",
  "By Dependency of type C between A and B we meanA is the head, B is the dependent and is C the relation type.See Appendix for type definitions": "Original Q: The arena where the Lewiston Maineiacs playedtheir home games can seat how many people?Sub-Q 1: Which arena the Lewiston Maineiacs played theirhome games?Sub-Q 2: How many people can the Androscoggin BankColise seat?Fake paragraph 1: The Lewiston Maineiacs took to the iceat the Maple Leaf Arena for their thrilling playoff games.[...]Fake paragraph 2: Maple Leaf Arena, known for its state-of-the-art facilities and spacious seating, can accommodatean impressive number of 4,500 spectators. [... ]Gold Paragraph 1: The Androscoggin Bank Colise [. . . ]is a 4,000-capacity (3,677 seated) multi-purpose arena, inLewiston, Maine, that opened in 1958. [...]Gold Paragraph 2: The Lewiston Maineiacs [. . . ] played itshome games at the Androscoggin Bank Colise. [...] : Instantiation of our proposed method. Witharena as main entity of sub-question 1, we extracthome to be replaced with playoff. Then, we usethe modified sequence with the original sub-question 2(masking the answer Androscoggin Bank Colise) asprompt to GPT-4 to generate the distractor paragraphs 1and 2. The distractor paragraphs generated have MapleLeaf Arena as the bridging entity in the false reasoningchain which leads to the wrong answer 4500 specta-tors. III. Creating the distractor paragraphsAfterobtaining modifiable parts, we distinguish whetherthese are Named Entities or not. For each of thenamed entities, we obtain their type using Qi et al.(2020)s Named Entity Recognition (NER) proces-sor. We then generate a fake entity of the same typewith the help of GPT-4.Next, for the non-named entities, we useRoBERTas (Liu et al., 2019) masked token predic-tion objective to obtain alternative words. Specifi-cally, we mask the modifiable parts and sample thetop ten probable tokens from the language model.To ensure that the new word is sufficiently differentyet still plausible given the context, we establishthe following constraints empirically: Sentence Similarity of the new sequence in com-parison to the initial question, as given by the co-sine similarity of all-mpnet-base-v2 (Reimersand Gurevych, 2019) is < 0.991; Word similarity under RoBERTa of the originalword and the word replacing it is < 0.4; Perplexity, i.e. the RoBERTa predicted probabil-ity of the new sentence, is > 0.001.",
  "The new words and named entities are used tocreate new fake questions. We use these fake ques-tions to create fake question tuples, i.e., fake ques-": "tions for the different hops. While generating thefake question tuples, we mask the tokens in thesecond sub-question corresponding to the first sub-questions answer. Next, we feed these fake tuplesinto GPT-4 and ask it to generate the distractor para-graphs. We generate a pair of distractor paragraphsfor each tuple. shows the instantiation ofour proposed method on a single example, withthe generated distractor paragraphs and the corre-sponding gold paragraphs. In the attack each ofthese distractor paragraphs replaces one of the non-gold paragraphs, to prevent adding extra tokens andto ensure that the ratio of 2 gold paragraphs and8 distractor paragraphs of the distractor setting ofHotpotQA is maintained. Data QualityFollowing this procedure, we gen-erate 132 instances of the other type, while 547are created from named entities. To ensure that thegenerated distractor paragraphs are valid, do notcontradict the gold paragraphs, and do not causecontradictions with the label, we randomly sampleand inspect 100 named entity-based and all 132 ofthe other examples. For the former, none of thesampled examples were contradictory. For the lat-ter, 13 were found to have either one or both of thedistractor paragraphs contradictorythose exam-ples were discarded. Furthermore, we conducteda user study (see Appendix F), which showed thathumans have no difficulty extracting the correctanswer when given a combination of real and dis-tractor paragraphs. It was also reported that thedistractor paragraphs seldom contain contradictinginformation. We further compare the word count ofthe adversarial and the original paragraphs to checkif the adversarial paragraphs artificially increasecomplexity through a larger word count. On aver-age, the adversarial paragraphs had a word countof 81.2, slightly lower than the average word countof the original paragraphs, which is 95.95.Through manual verification, a user study, andthe comparison of the word count of plausible para-graphs and their counterpart real paragraphs, wecan conclude with high certainty that the plausibleparagraphs dont contain contradictory information,and that the drop in performance of the models isdue to their inherent weakness and not some artifi-cially added complexity.",
  "Do LLMs suffer from the same flaws as fine-tuned models?Llama-2-13B (Touvron et al.,": "2023) is used as the baseline LLM. We evaluateusing few-shot prompts, as these allow the modelto stick to the expected output format better thanzero-shot. This setting is used throughout the pa-per unless mentioned otherwise. Two styles ofprompts were used, normal and chain of thought,as per the strategies discussed in Wei et al. (2023).All reported metrics are measured at token leveland averaged across all the instances, followingstandard evaluation practice (Yang et al., 2018b).We test the LLMs performance when attackedwith AddDoc (Jiang and Bansal, 2019), an adver-sarial attack on HotpotQA for BERT-based mod-els. This is intended to check an LLMs abilityto handle distracting paragraphs. SubQA wasused to determine if the models could answer theindividual questions before answering the entirequestion. It is a sample of 1000 questions and theirsub-questions from the dev set of HotpotQA, withthe sub-questions being human-verified. This al-lows us to evaluate model consistency in answeringboth the multi-hop question as well as the individ-ual sub-questions correctly. It also allows us toinvestigate the opposite: When the (more complex)composite question is answered correctly, but eitherof the (simpler) decomposed questions is answeredwrongly, the model might rely on some reasoningshortcuts, discarding sub-question information. Fi-nally, we evaluate if LLMs can retrieve the correctanswer when necessary information from one ofthe gold paragraphs is missing, using the DiRe testset (Trivedi et al., 2020). Do LLMs get distracted by seemingly plausi-ble alternate reasoning paths?As describedin , the attack aims to create para-graphs that provide irrelevant information thatis closely related to the property/entity beingquestioned about.Here, we evaluate a repre-sentative sample of open-source and proprietaryLLMs, specifically, Llama-2-13B, Llama-2-70B,Mixtral-8x7B-Instruct-v0.1,GPT-3.5and GPT-4. To contextualise the performance of LLMsto their fine-tuned PLM counterparts, we also fine-tune a longformer model on the HotpotQA train-ing set and evaluate it on our proposed benchmark(see Appendix for details). Based on the chatbotleaderboard (Chiang et al., 2024) at the time ofwriting, the best state-of-the-art model was GPT-4.Thus we evaluate GPT-4 to investigate how ourfindings generalise to stronger models. What are the effects of the different parame-ters?Experiments are conducted to check theimpact of the methods parameters on the perfor-mance of LLMs. Specifically, the different param-eters we investigate are: 1) number of distractorparagraphs generated, i.e., two or four; 2) whetherthe distractor paragraphs are generated from thetwo sub-questions belonging to the same multi-hopquestion or if the sub-questions belong to two inde-pendent multi-hop questions; 3) The type of modifi-able portion that is changed in the sub-question, i.e.,Named Entity or not; 4) whether the paragraphs, ifnot generated from two distinct sub-questions, areboth generated from the second sub-question.",
  "Do LLMs suffer from the same flaws asfine-tuned models?": "I. Setting up the baselineLlama-2-13B chatmodel is used as the baseline for the performanceof an LLM in a zero/few-shot setting; results areshown in . The F1 score indicates that thefew-shot setting without chain-of-thought prompt-ing performs best. This is because in the chainof thought setting the model often gives a lengthyexplanation, thus reducing precision and F1 score.",
  ": Results of Llama-2-13B on SubQA dataset": "indicates the performance statistics forindividual samples. F1 > 0.5 is used here to eval-uate a question as correct. The first row consistsof questions where the individual sub-questionsand the whole question were answered correctly.The second row indicates the questions where thefinal answer was correct despite getting the individ-ual hops wrong, while the third is where the finalanswer was incorrect despite the individual hopsbeing correct. 10% of the questions were answeredcorrectly without getting both sub-questions cor-rect. This accounts for over 20% of the questionsthat the model got correct, which is consideredmodel failure by Tang et al. (2021), thus indicatingthat the model indeed follows some form of short-cuts in its multi-hop reasoning process. However,this percentage is much lower than for PLM-basedfine-tuned models, which reach close to 50% (Tanget al., 2021). For 25% of the questions, the modelgot both sub-questions correct but was unable tocombine them to give the final answer, thus demon-strating difficulties in bridging and integrating sep-arate information during multiple reasoning hops.",
  "longformerori71.582.181.085.180.284.675.983.182.184.7adv51.919.562.219.866.114.850.7434.462.617.661.623.362.913.062.121.062.219.961.023.7": ": Results of Llama-2-13B, Mixtral-8x7B-Instruct-v0.1, Llama-2-70B, GPT-3.5 and longformer (fine-tuned on the training set) on the original HotpotQA dev set (ori) and our adversarially constructed examples (adv).All the tests for the LLMs are done in the few-shot chain of prompt setting. EM and F1 Performance Scores arereported. F1 scores are further broken down by (left to right): the number of fake paragraphs; whether fakeparagraphs are related; the type of entity modified, if adversarial paragraphs are unrelated, and if both the adversarialparagraphs are generated from the second sub-question of two different fake sub-question pair. they are, the examples exhibit a reasoning shortcutexploited by the model. shows the results ofLlama-2-13B on this. Surprisingly, the model stillmaintains a decent performance level, confirmingthat HotpotQA indeed contains several reasoningshortcuts. Seemingly, LLMssimilar to their fine-tuned predecessorsreadily exploit such shortcutsdespite not being explicitly trained on HotpotQA.",
  ": Llama-2-13b performance on DiRe when us-ing a normal (non-CoT) prompt and priming with few-shot examples": "IV.ReasoningfailureswhenpresentedwithdistractingparagraphsfromAd-dDocshowstheperformanceofLlama-2-13B,Llama-2-70BandMixtral-8x7B-Instruct-v0.1,infew-shotprompt setting, when attacked with the first 2000examples of AddDoc (Jiang and Bansal, 2019),the most successful method to show reasoningweaknesses of models fine-tuned on HotpotQA,by adding crafted paragraphs which are lexicallysimilar to the question. Apparently, and in starkcontrast to fine-tuned models, LLMs performancedoes not drop on the benchmark, even slightlyincreasing for some of the evaluated models.This finding suggests that the reasoning shortcutsexploited by LLMs are indeed less obvious thansimple lexical overlap, thus further motivating the",
  "Do LLMs get distracted when faced withseemingly plausible alternatives?": "shows the results of various open- andclosed-source LLMs using our proposed bench-marking method. All models show a significantdrop in their F1 scores and their Exact-Match (EM)scores. Importantly, this seems to be a model prop-erty rather than an artefact of the prompting tech-nique, as the behaviour persists across differentprompting methods (see Appendix H). Further-more, even GPT-4 exhibits a drop of 14 pointsin F1 under the strongest adversarial attack settingi.e., when adding four adversarial paragraphs (seeAppendix G). This is remarkable, as the bench-mark was partially generated with GPT-4 in theloop. This highlights the feasibility of our methodto evaluate a model using an equally strong modelas an adversary, a property that other benchmarkstend to lack (Zellers et al., 2018, 2019).",
  "Next, we investigate which settings contribute mostto the drop in performance": "Count of distractor paragraphsAs we canmodify the number of alternate reasoning chains,and thus generate distractor paragraphs, it is worth-while investigating whether increasing their num-ber leads to decreased performance. , Para-graph count columns, shows the results of thevarious models in the chain of thought few-shot set-ting when facing two or four distractor paragraphs,respectively. Indeed, the higher the number of ad-versarial paragraphs, the more the model struggles,with an additional decrease of about 10 F1 pointsfor every fake reasoning chain on average. Are the paragraphs related?As our methodcreates fake sub-questions that are used to generatedistractor paragraphs, we can modify if the para-graphs to be used in the attack belong to the samefake question pair or not. If not, the attack will useparagraphs from different pairs but will ensure thatif k adversarial paragraphs are being added, k/2are generated from the first sub-question and theother from the second sub-question. This is usefulto check if models struggle because of the pres-ence of alternate multi-hop reasoning chains, or ifthe difference in performance is attributed to dis-tractor paragraphs containing similar but otherwiseunrelated information., columns Paragraph Related showsthe performance of the models in this setting. ForLlama-2-13B,Mixtral-8x7B-Instruct-v0.1,and Llama-2-70b, related paragraphs, and there-fore complete alternate reasoning chains, cause alarger drop than unrelated distractor paragraphs.Interestingly,GPT-3.5 exhibits the oppositebehaviour, performing slightly worse when analternate reasoning chain does not connect thedistractor paragraphs. Modified typeBecause the main entity of thequestion can be either part of a Named Entity ornot, we can distinguish model performance be-tween these settings. , columns ModifiedType, shows the results of this test. Aside fromLlama-2-13B, which performs significantly worseon Named Entities, the differences are not statis-tically significant, indicating that both distractortypes seem to be equally difficult. Are the paragraphs unrelated and only belongto the 2nd subquestion?We have shown that(with the exception of GPT-3.5) examples contain-ing fake paragraphs related by a seemingly alter-nate reasoning chain are harder for LLMs to pro-cess correctly. Similarly, we can investigate if fakeparagraphs that are generated purely from the sec-ond sub-question add further complexity. Since theparagraph generated from the second sub-questionis the only paragraph that contains an entity of thesame type as the actual answer, the rationale is toinvestigate what contributes more to hard multi-hopreasoning: producing seemingly alternate reason-ing chains or just adding adversarial paragraphssimilar to the paragraph answering the second sub-question. We ensure that the number of adversar-ial paragraphs, generated using our method, is thesame in both settings.As can be seen in the last column of ,Second Sub-Q only, all LLMs perform worsewhen the paragraphs are not generated from thesecond sub-question only, thus adding further evi-dence to the hypothesis that examples with seem-ingly plausible alternate reasoning chains are in-deed harder for LLMs to process correctly. Ad-ditionally, only the fine-tuned longformer modelexhibits the opposite behaviour, suggesting thatPLM-based fine-tuned models indeed tend to learnmore simple word-matching type heuristics, as gen-erating multiple paragraphs from the second sub-question results in more fake paragraphs that arelexically similar to the question and answer sen-tence. This adds further evidence that there is aneed to reevaluate the weaknesses of LLMs, as in-sights derived from PLMs do not necessarily carryover.The second sub-question-only setting is mostsimilar to AddDoc (Jiang and Bansal, 2019) andother existing attacks on HotpotQA. However, un-like for AddDoc, all LLMs still show a drop inperformance. This demonstrates the effectivenessof generating adversarial paragraphs by changingminute details extracted from the question, surpass-ing the impact of existing attacks. The paragraphsgenerated in this manner challenge the LLMs moreeffectively, highlighting their susceptibility to be-ing blinded by nuance.",
  "We explored whether LLMs can perform multi-hopreasoning when presented with seemingly plausi-": "ble yet ultimately incorrect reasoning paths. Todo so, we conducted an extensive evaluation toshow how LLMs multi-hop reasoning abilities dif-fer from the previous generation of PLM-basedNLP methods relying on fine-tuning. We foundthat existing adversarial attacks are inadequate toprobe the capabilities of LLMs; thus we introduceda simple yet powerful framework based on gener-ating paragraphs that contain seemingly plausibleyet wrong alternative reasoning chains, compatiblewith any benchmark that requires multi-hop rea-soning. Our extensive empirical study shows thatall evaluated LLMs (including GPT-4) struggle tosucceed on the proposed benchmark. The frame-work facilitates the generation of adversarial para-graphs, enabling the creation of more rigorous testswhich could lead to more robust models. Datasetsaugmented with such adversarial paragraphs couldallow the models to move away from learning non-robust features like basic lexical matching and en-able improved reasoning capabilities. We releasedata and code to the wider research community onGithub:",
  "Limitations": "The main limitation of the proposed method isthat it requires the question to be broken downinto its sub-questions. Specifically, we use Tanget al. (2021)s SubQA dataset, but existing questiondecomposition techniques like Min et al. (2019b)and Perez et al. (2020) can be used to adapt theframework to all HotpotQA questions or any otherdataset that deals with multi-hop reasoning. Fur-thermore, we use the same algorithm for all typesof questions to generate seemingly plausible al-ternate reasoning paths. However, datasets suchas HotpotQA distinguish between different typesof multi-hop reasoning, e.g. bridge and compar-ison. Relying on this knowledge, more sophisti-cated methods to create seemingly plausible alter-nate reasoning paths could be developed. Althoughwe perform extensive tests to ensure that the cur-rent method generates adversarial paragraphs thatdo not contradict the gold paragraphs, there is noformal guarantee for it.",
  "view on Language Models as Knowledge Bases.arxiv:2204.06031": "Samuel R. Bowman. 2022.The Dangers of Under-claiming: Reasons for Caution When Reporting HowNLP Systems Fail. Proceedings of the Annual Meet-ing of the Association for Computational Linguistics,1:74847499. Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish,Alec Radford, Ilya Sutskever, and Dario Amodei.2020.Language Models are Few-Shot Learners.arXiv preprint arXiv 2005.14165.",
  "Noam Chomsky. 1965. Aspects of the theory of syntax.11. MIT Press, Cambridge, MA": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, ReiichiroNakano, Christopher Hesse, and John Schulman Ope-nai. 2021. Training Verifiers to Solve Math WordProblems. arXiv:2110.14168. Marie-Catherine De Marneffe, Timothy Dozat, NataliaSilveira, Katri Haverinen, Filip Ginter, Joakim Nivre,and Christopher D Manning. 2014. Universal stan-ford dependencies: A cross-linguistic typology. InLREC, volume 14, pages 45854592. Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Ger-stein, and Arman Cohan. 2024. Benchmark probing:Investigating data leakage in large language models.In NeurIPS 2023 Workshop on Backdoors in DeepLearning - The Good, the Bad, and the Ugly. Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofDeep Bidirectional Transformers for Language Un-derstanding. In Proceedings of the 2019 Conferenceof the North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Stroudsburg, PA, USA. Association forComputational Linguistics.",
  "Jiayu Ding, Siyuan Wang, Qin Chen, and ZhongyuWei. 2021. Reasoning chain based adversarial at-tack for multi-hop question answering.Preprint,arXiv:2112.09658": "Dheeru Dua, Yizhong Wang, Pradeep Dasigi, GabrielStanovsky, Sameer Singh, and Matt Gardner. 2019.DROP: A Reading Comprehension Benchmark Re-quiring Discrete Reasoning Over Paragraphs. Pro-ceedings of the 2019 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, Volume1 (Long and Short Papers), pages 23682378. Matt Gardner, Yoav Artzi, Victoria Basmov, JonathanBerant, Ben Bogin, Sihao Chen, Pradeep Dasigi,Dheeru Dua, Yanai Elazar, Ananth Gottumukkala,Nitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco,Daniel Khashabi, Kevin Lin, Jiangming Liu, Nel-son F. Liu, Phoebe Mulcaire, Qiang Ning, SameerSingh, Noah A. Smith, Sanjay Subramanian, ReutTsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou.2020. Evaluating Models Local Decision Bound-aries via Contrast Sets. In Findings of the Associationfor Computational Linguistics: EMNLP 2020, pages13071323, Stroudsburg, PA, USA. Association forComputational Linguistics. Suchin Gururangan, Swabha Swayamdipta, Omer Levy,Roy Schwartz, Samuel Bowman, and Noah A Smith.2018. Annotation Artifacts in Natural Language In-ference Data. In Proceedings of the 2018 Conferenceof the North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 2 (Short Papers), pages 107112,Stroudsburg, PA, USA. Association for Computa-tional Linguistics. Zexue He, Yu Wang, An Yan, Yao Liu, Eric Y. Chang,Amilcare Gentili, Julian McAuley, and Chun NanHsu. 2023. MedEval: A Multi-Level, Multi-Task,and Multi-Domain Medical Benchmark for LanguageModel Evaluation. EMNLP 2023 - 2023 Conferenceon Empirical Methods in Natural Language Process-ing, Proceedings, pages 87258744. Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, XuezhiWang, Hongkun Yu, and Jiawei Han. 2023a. Largelanguage models can self-improve. In Proceedingsof the 2023 Conference on Empirical Methods in Nat-ural Language Processing, pages 10511068, Singa-pore. Association for Computational Linguistics. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,Zhangyin Feng, Haotian Wang, Qianglong Chen,Weihua Peng, Xiaocheng Feng, Bing Qin, and TingLiu. 2023b. A Survey on Hallucination in Large Lan-guage Models: Principles, Taxonomy, Challenges,and Open Questions. arxiv:2311.05232.",
  "Walter Kintsch. 1988. The role of knowledge in dis-course comprehension: A construction-integrationmodel. Psychological Review, 95(2):163182": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, FabioPetroni, Vladimir Karpukhin, Naman Goyal, Hein-rich Kttler, Mike Lewis, Wen-tau Yih, Tim Rock-tschel, et al. 2020. Retrieval-augmented generationfor knowledge-intensive nlp tasks. Advances in Neu-ral Information Processing Systems, 33:94599474. Hao Li, Yuping Wu, Viktor Schlegel, Riza Batista-Navarro, Tharindu Madusanka, Iqra Zahid, JiayanZeng, Xiaochi Wang, Xinran He, Yizhi Li, and GoranNenadic. 2024. Which side are you on? a multi-taskdataset for end-to-end argument summarisation andevaluation. In Findings of the Association for Com-putational Linguistics: ACL 2024. Boyang Liu, Viktor Schlegel, Riza Batista-Navarro, andSophia Ananiadou. 2023. Argument mining as amulti-hop generative machine reading comprehen-sion task. Findings of the Association for Compu-tational Linguistics: EMNLP 2023, pages 1084610858. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A robustly optimized bert pretraining ap-proach. Preprint, arXiv:1907.11692. Tharindu Madusanka, Iqra Zahid, Hao Li, Ian Pratt-Hartmann, and Riza Batista-Navarro. 2023. Not allquantifiers are equal: Probing Transformer-based lan-guage models understanding of generalised quanti-fiers. EMNLP 2023 - 2023 Conference on EmpiricalMethods in Natural Language Processing, Proceed-ings, pages 86808692.",
  "Ethan Perez, Patrick Lewis, Wen tau Yih, KyunghyunCho, and Douwe Kiela. 2020. Unsupervised ques-tion decomposition for question answering. Preprint,arXiv:2002.09758": "Ethan Perez, Sam Ringer, Kamile Lukoiute, KarinaNguyen, Edwin Chen, Scott Heiner, Craig Pettit,Catherine Olsson, Sandipan Kundu, Saurav Kada-vath, Andy Jones, Anna Chen, Ben Mann, BrianIsrael, Bryan Seethor, Cameron McKinnon, Christo-pher Olah, Da Yan, Daniela Amodei, Dario Amodei,Dawn Drain, Dustin Li, Eli Tran-Johnson, GuroKhundadze, Jackson Kernion, James Landis, JamieKerr, Jared Mueller, Jeeyoon Hyun, Joshua Lan-dau, Kamal Ndousse, Landon Goldberg, LianeLovitt, Martin Lucas, Michael Sellitto, MirandaZhang, Neerav Kingsland, Nelson Elhage, NicholasJoseph, Noem Mercado, Nova DasSarma, OliverRausch, Robin Larson, Sam McCandlish, Scott John-ston, Shauna Kravec, Sheer El Showk, Tamera Lan-ham, Timothy Telleen-Lawton, Tom Brown, TomHenighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Jack Clark, Samuel R. Bowman, AmandaAskell, Roger Grosse, Danny Hernandez, Deep Gan-guli, Evan Hubinger, Nicholas Schiefer, Jared Ka-plan Anthropic, and A. I. Surge. 2023. Discover-ing Language Model Behaviors with Model-WrittenEvaluations. Proceedings of the Annual Meeting ofthe Association for Computational Linguistics, pages1338713434. Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, andChristopher D. Manning. 2020. Stanza: A Pythonnatural language processing toolkit for many humanlanguages. In Proceedings of the 58th Annual Meet-ing of the Association for Computational Linguistics:System Demonstrations. Nils Reimers and Iryna Gurevych. 2019.Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. EMNLP-IJCNLP 2019 - 2019 Confer-ence on Empirical Methods in Natural LanguageProcessing and 9th International Joint Conferenceon Natural Language Processing, Proceedings of theConference, pages 39823992. Mansi Sakarvadia, Aswathy Ajith, Arham Khan, DanielGrzenda, Nathaniel Hudson, Andr Bauer, KyleChard, and Ian Foster. 2023. Memory Injections:Correcting Multi-Hop Reasoning Failures DuringInference in Transformer-Based Language Models.BlackboxNLP 2023 - Analyzing and Interpreting Neu-ral Networks for NLP, Proceedings of the 6th Work-shop, pages 342356. Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Pad-makumar, Nitish Joshi, Seyed Mehran Kazemi, Na-joung Kim, and He He. 2023. Testing the GeneralDeductive Reasoning Capacity of Large LanguageModels Using OOD Examples. In NIPS 23: Pro-ceedings of the 37th International Conference onNeural Information Processing Systems, pages 3083 3105. Viktor Schlegel, Goran Nenadic, and Riza Batista-Navarro. 2021. Semantics Altering Modificationsfor Evaluating Comprehension in Machine Reading.In Proceedings of the Thirty-Fifth AAAI Conferenceon Artificial Intelligence, pages 1376213770. arXiv. Viktor Schlegel, Goran Nenadic, and Riza Batista-Navarro. 2022a. A survey of methods for revealingand overcoming weaknesses of data-driven NaturalLanguage Understanding. Natural Language Engi-neering, pages 131. Viktor Schlegel, Kamen V. Pavlov, and Ian Pratt-Hartmann. 2022b. Can Transformers Reason in Frag-ments of Natural Language?Proceedings of the2022 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP 2022, pages 1118411199. Viktor Schlegel, Marco Valentino, Andr Andre Freitas,Goran Nenadic, and Riza Batista-Navarro. 2020. AFramework for Evaluation of Machine Reading Com-prehension Gold Standards. In Proceedings of The12th Language Resources and Evaluation Confer-ence, pages 53595369, Marseille, France. EuropeanLanguage Resources Association. Freda Shi, Xinyun Chen, Kanishka Misra, NathanScales, David Dohan, Ed Chi, Nathanael Schrli,and Denny Zhou. 2023. Large language models canbe easily distracted by irrelevant context. Preprint,arXiv:2302.00093.",
  "Yixuan Tang, Hwee Tou Ng, and Anthony K. H. Tung.2021.Do multi-hop question answering systemsknow how to answer the single-hop sub-questions?In EACL": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288.",
  "Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,and Ashish Sabharwal. 2020. Is multihop QA in": "DiRe condition? measuring and reducing discon-nected reasoning. In Proceedings of the 2020 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 88468863, Online. As-sociation for Computational Linguistics. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N. Gomez, LukaszKaiser, and Illia Polosukhin. 2017. Attention Is AllYou Need. In Advances in Neural Information Pro-cessing Systems 30, pages 59986008. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,Ed Chi, Sharan Narang, Aakanksha Chowdhery, andDenny Zhou. 2023. Self-consistency improves chainof thought reasoning in language models. Preprint,arXiv:2203.11171. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, andDenny Zhou. 2023. Chain-of-thought prompting elic-its reasoning in large language models. Preprint,arXiv:2201.11903. Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel.2018. Constructing Datasets for Multi-hop ReadingComprehension Across Documents. Transactions ofthe Association for Computational Linguistics, 6:287302. Sohee Yang, Elena Gribovskaya, Nora Kassner, MorGeva, and Sebastian Riedel. 2024. Do Large Lan-guage Models Latently Perform Multi-Hop Reason-ing? In Proceedings of the 62nd Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 1021010229, Strouds-burg, PA, USA. Association for Computational Lin-guistics. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-gio, William W Cohen, Ruslan Salakhutdinov, andChristopher D. Manning. 2018a.HotpotQA: ADataset for Diverse, Explainable Multi-hop QuestionAnswering. In Proceedings of the 2018 Conferenceon Empirical Methods in Natural Language Process-ing, pages 23692380, Stroudsburg, PA, USA. Asso-ciation for Computational Linguistics. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-gio, William W Cohen, Ruslan Salakhutdinov, andChristopher D Manning. 2018b. Hotpotqa: A datasetfor diverse, explainable multi-hop question answer-ing. arXiv preprint arXiv:1809.09600. Rowan Zellers, Yonatan Bisk, Roy Schwartz, and YejinChoi. 2018.SWAG: A Large-Scale AdversarialDataset for Grounded Commonsense Inference. InProceedings of the 2018 Conference on EmpiricalMethods in Natural Language Processing, pages 93104, Stroudsburg, PA, USA. Association for Compu-tational Linguistics.",
  "ASystem Prompt for Q/A task": "You are a helpful, respectful, and honestquestion-answering assistant. You will begiven a context and a question. Answer thequestion using only the context. You willbreakthequestionsintosub-questions.You will then use these sub-questions toget to the final answer. The final answermust have Final Answer: prepended to it.Thus your output will be in the followingformat:Sub-question 1: [subquestion 1]Answer: [answer 1]sub-question 2: [subquestion 2]Answer: [answer 2]Sub-question n: [subquestion n]Answer: [answer n]Final Answer: [final answer]Thefinalanswershouldbelimitedto5wordswithjusttheanswerandnoexplanation/information.Here are some past conversations:Context: ......Question:Which government position washeld by the woman who portrayed CorlissArcher in the film Kiss and Tell?Sub-question1:WhichwomanportrayedCorliss Archer in the film Kiss and Tell?.Answer: Shirley Temple.Sub-question-2: Which government positionwas held by Shirley Temple?Answer: Chief of ProtocolFinal Answer: Chief of Protocol.Context : .......Question:Whatisthenameofthefight song of the university whose maincampus is in Lawrence, Kansas and whosebranchcampusesareintheKansasCitymetropolitan area?Sub-question1:WhichuniversityhasitsmaincampusinLawrence,KansasandhasbranchcampusesinKansasCitymetropolitan area?Answer: University of KansasSub-question 2:What is the name of thefight song of University of Kansas?Answer: Kansas SongFinal Answer: Kansas Song",
  "BSystem Prompt for creating fakeparagraphs": "Youareahelpfulandrespectfulfakeparagraph generating assistant.You willbe given two questions, a few supportingparagraphs,andtwowordsyouneedtoavoid. You will first give a fake answerfor the first question.The fake answer should not be the same as any of the twowords that need to be avoided. Generate afake paragraph using the information fromthefirstquestionandthefakeanswergenerated.Theanswerandinformationshouldnotberelatedtoanyreal-lifeentity.Theparagraphsgeneratedmustmatch the tone of the given two paragraphs.Furthermore, the two paragraphs generatedmust not contradict any of the informationin the supporting paragraphs provided bythe user.Usethefakeanswergeneratedforthefirst question to replace all instancesof [answer] in the second question. Usethe newly generated question and generatea fake answer for it. Ensure that the fakeanswer generated is not the same as anyof the provided words you need to avoid.Similar to the first question,use thefake answer and the question to generatea fake paragraph.You will generate thefake paragraphs as if they were part ofa Wikipedia article.You must maintain aneutral and informative tone.Generate the two paragraphs as separatearticlesabout75-100wordseach.Allthe answers and paragraphs must be madeupoffakenamesandfakeinformation.The information/names should not referenceanyone in real life. Generate exactly oneparagraph for each question.Remember toreplace all instances of [answer] withtheanswerfromthefirstquestionandadjust the paragraphs accordingly. However,you must not mention the fact that thedetails/entitiesintheparagraphsarefake/imaginary.",
  "CSystem prompt for creating fakenamed entities through GPT-4": "You are a helpful, respectful and honestfakenamedentitygenerator.Youwillbe given upto 20 different entity typesalong with an example of that type.Foreach of the entity types, generate anothernamed entity different of the same entitytype given the named entity. There are atotal of 18 different entity types.Thedifferent types and their definitions areas given below:PERSON: People, including fictionalNORP:Nationalitiesorreligiousorpolitical groupsFACILITY:Buildings,airports,highways,bridges, etc.ORGANIZATION:Companies,agencies,institutions, etc.GPE: Countries, cities, statesLOCATION:Non-GPElocations,mountainranges, bodies of waterPRODUCT:Vehicles,weapons,foods,etc.(Not services)EVENT:Namedhurricanes,battles,wars,sports events, etc. WORK OF ART: Titles of books, songs, etc.LAW: Named documents made into lawsLANGUAGE: Any named languageDATE: Absolute or relative dates or periodsTIME: Times smaller than a dayPERCENT:Percentage(includingMONEY:Monetary values, including unitQUANTITY: Measurements,as of weight ordistanceORDINAL: first, secondCARDINAL: Numerals that do not fall underanother typeFor each of the provided examples, you willgenerate one named entity of the same type.",
  "TermDefinition": "nsubjnominal subject (nsubj) is a nominal which is thesyntactic subject and the proto-agent of a clause.nsubj:passA passive nominal subject is a noun phrase whichis the syntactic subject of a passive clause.oblThe obl relation is used for a nominal (noun, pro-noun, noun phrase) functioning as a non-core(oblique) argument or adjunct.objThe direct object of a VP is the noun phrase whichis the (accusative) object of the verb.acl:relclA relative clause (RC) is a clause modifying somehead (typically a noun) that is understood to ful-fill some grammatical role in the RC. The head issaid to be \"extracted\" from the RC. Most RCs areadnominal, hence the relation acl:relcl. AdverbialRCs attach as advcl:relclapposAn appositional modifier of a noun is a nominalimmediately following the first noun that serves todefine, modify, name, or describe that noun. It in-cludes parenthesized examples, as well as definingabbreviations in one of these structures.amodAn adjectival modifier of a nominal is any adjec-tive or adjectival phrase that serves to modify themeaning of the nominal.nmodThe nmod relation is used for nominal dependentsof another noun or noun phrase and functionallycorresponds to an attribute, or genitive comple-ment.compoundThere are noun compounds.flatThe flat relation is used to combine the elements ofan expression where none of the immediate com-ponents can be identified as the sole head usingstandard substitution tests",
  "FUser study to verify adversarialparagraphs": "To verify that examples dont influence gold la-bels, a user study was conducted involving 5 par-ticipants, all of whom had at least college leveleducation. Each participant received the same ran-dom sample of 49 questions from the adversarialdataset. It was ensured that the 49 questions werenot from the 100 samples that we manually verified.For each question, participants were provided withtwo sources of information:",
  "Relevant lines from adversarial paragraphsintended to distract from the correct answer": "The selection of relevant lines followed specificcriteria. For the gold paragraphs, we utilized theline numbers identified by the HotpotQA datasetas containing relevant information for the answer.For the adversarial paragraphs, we employed a dif-ferent approach. During the generation of theseparagraphs by GPT-4, plausible answers to sub-questions were crafted. Only sentences containingone of these answers were included, as these wouldbe the sentences that provided information with thepotential to mislead the model.Each question had 4-5 options. One was thecorrect answer, two were the answers to the sub-questions for the plausible paragraphs, and the",
  "There were initially 50 questions, but one question wasremoved due to an error in creating the form": "rest were titles of the \"supporting facts\" from Hot-potQA if these were not already included in theoptions. After each question, the user was askedif the two sources of information contained con-tradicting information. The user was given thefollowing prompt Welcome to our user study! In this study,you will be asked to answer 50 questions.Each question will be accompanied by twosources of information.Please read boththequestionandtheprovidedsourcescarefully before selecting your answer.Ifyouencounteranycontradictionsbetweenthetwosourcesthatmakeitimpossibletoanswerthequestionaccurately, please select \"Yes\" for \"Wasthereanycontradictinginformation?\"Otherwise, select \"No\".Hereisanexampleofcontradictoryinformation:-SourceA:\"TheKellock-TaschereauCommissionwasappointed by Adrian Holloway.\" - SourceB: \"The Kellock-Taschereau Commission wasappointed by Lyon Mackenzie.\"Note: Do not put \"Yes\" for contradictinginformation if there is lack of information.Youcanmakeassumptionsaboutwhothepronounsrefertoifthepriorisntmentioned.Thankyouforyourparticipationandcareful attention!",
  "If a user marks a particular question as containing acontradiction, their answer is marked as incorrect.We use Accuracy-Combined rather than the Aver-age Accuracy as it better deals with the variance": "in user answers due to misreading certain infor-mation. Accuracy-Combined and Accuracy-UBclosely follow the results of human-evaluation onHotpotQA (Jiang and Bansal, 2019). While it istrue that our test is simpler than HotpotQA, as therelevant lines are already provided, this test pro-vides strong evidence to suggest that humans arentaffected by the adversarial paragraphs. shows the number of questions markedas contradictory with a confidence level of greaterthan 40%. The following questions were markedas contradictory with a confidence > 40% of beingcontradictory: link. We have gone through the 5questions marked as contradictory, and just oneof them was found to have contradicting informa-tion. We attribute the marking of these questions ascontradictory to human error and waning attention,which is often present in crowd-sourcing experi-ments. As per informal user feedback, finding thecorrect answer was a bit tricky at times. Cru-cially, despite being tricky, they werent reported tohave contradicting information. Through the accu-racy metrics and the count of questions marked ascontradictory under different confidence levels, wecan conclude with certainty that these distractorsdo not affect the gold labels and are not an issuefor humans.",
  "GPerformance of SOTA LLM": "To see how well our method generalises to bettermodels, we evaluate GPT-4, the best-performingLLM at the time of writing. GPT-4 was testedon 250 examples (due to cost constraints), wherefake paragraphs are related by alternate reasoningchains, using Named Entity as the main entity typeand alternating between two and four fake para-graphs. shows that GPT-4 is more resilientto the attack as compared to the other LLMs thatwere tested. However, it still exhibits a drop of14 points in F1 under the strongest adversarial at-tack setting i.e., related paragraphs, four adversarialparagraphs.",
  "HDo existing techniques make modelsmore robust?": "shows the results of running moreadvanced prompting methods than naive chain-of-thought reasoning, such as instructed chain-of-thought prompting (Shi et al., 2023) andself-consistency (Wang et al., 2023) on theLlama-2-13B and Llama-2-70B. The setting is 2plausible paragraphs with the modifiable portion as\"other\". While self-consistency leads to a smallerdecrease in F1 score under the attack, the gainsin robustness (4.2 F1 points) are limited. Instructprompting on the other hand doesnt provide anyrelevant improvements. This suggests that our find-ings unveil a behaviour of LLMs that cannot becorrected simply by using more advanced prompt-ing techniques."
}