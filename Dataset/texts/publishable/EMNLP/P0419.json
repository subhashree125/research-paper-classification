{
  "Abstract": "Ensuring that the benefits of sign language tech-nologies are distributed equitably among allcommunity members is crucial. Thus, it isimportant to address potential biases and in-equities that may arise from the design or use ofthese resources. Crowd-sourced sign languagedatasets, such as the ASL Citizen dataset, aregreat resources for improving accessibility andpreserving linguistic diversity, but they must beused thoughtfully to avoid reinforcing existingbiases. In this work, we utilize the rich informationabout participant demographics and lexical fea-tures present in the ASL Citizen dataset to studyand document the biases that may result frommodels trained on crowd-sourced sign datasets.Further, we apply several bias mitigation tech-niques during model training, and find thatthese techniques reduce performance dispar-ities without decreasing accuracy. With thepublication of this work, we release the demo-graphic information about the participants inthe ASL Citizen dataset to encourage futurebias mitigation work in this space.",
  "Introduction": "Within the field of natural language processing,sign languages are under-resourced compared tospoken languages, compounded by the fact thatmost accessible information (e.g. online resourcesand social media) is written in a spoken language(Desai et al., 2024). Datasets like the ASL Citizendataset offer significant potential for improvingaccessibility and preserving the linguistic richnessof sign languages, yet their use requires carefulconsideration to avoid reinforcing existing biases.In this context, our research aims to explore thefactors that might influence the performance ofmodels trained on these datasets, particularly whenused for dictionary retrieval tasks.Because sign languages have comparativelyfewer resources than spoken languages, identify- : Accuracy and gender parity (calculated bydividing accuracy on female participants by accuracyon male participants) of the baseline pose-based ISLRmodel released with the ASL Citizen dataset (left) andour best-performing feature-based debiasing technique(right), in which we resample videos with lower videoquality scores at a higher rate. Our approach improvesboth overall model accuracy and the gender parity. ing biases in existing sign language resources iscritical. But biases can manifest differently in signlanguages than in spoken languages. For instance,ASL pronouns, unlike English pronouns, are notassigned a gender, so the common method of study-ing bias in English text through the lens of genderedpronoun use does not apply. Temporal elements,such as signing speed, also come into play, un-like in written language. Signing speed may beimpacted by a signers fluency, age, etc.In this work, we analyze how signer demograph-ics and more latent sources of bias may impactmodels trained on the ASL Citizen dataset for thetask of Isolated Sign Language Recognition (ISLR).We first examine the demographic distributionsin the ASL Citizen dataset, and present a linguis-tic analysis of the dataset based on the ASL-Lex(Caselli et al., 2017) annotations for each sign. Wethen report the prevalence of various linguistic andvideo-level features among demographics. We ex-amine how demographic features, in conjunction with lexical and video-level features, may impactmodel results. Finally, we experiment with mul-tiple debiasing techniques to reduce performancegaps between genders, and find that we are ableto reduce these gaps and improve overall modelaccuracy ().In summary, we present an analysis of demo-graphics, sign-level features, and video-level fea-tures in the ASL Citizen dataset and address thefollowing research questions:",
  ". Which demographic and linguistic factors im-pact dictionary retrieval results for modelstrained on the ASL Citizen dataset?": "2. Can we use debiasing strategies to mitigatedisparate impacts while maintaining high per-formance for dictionary retrieval models?With this work, we also release the demographicdata for the ASL Citizen dataset1, so future re-searchers can continue to study and mitigate bias insign language processing systems. Further, we re-lease the code for our experiments and analyses2.",
  "Related Work": "Most readily-available information (i.e. online re-sources and social media) is written, which maylimit accessibility for signers. Sign language pro-cessing tasks, such as dictionary retrieval, are de-signed to improve the accessibility of existing sys-tems and resources for Deaf and Hard-of-Hearing(DHH) people. Desai et al. (2024) created the ASLCitizen dataset to supplement existing dictionaryretrieval resources with crowd-sourced videos fromsigners.The ASL Citizen dataset was released to 1) ad-dress the resource gap between sign and spokenlanguages, and 2) improve video-based dictionaryretrieval for sign language, where signers demon-strate a particular sign and the system returns a listof similar signs, ranked from most to least simi-lar. Video-based dictionary retrieval systems canhelp language learners understand the meaning ofa sign, and allow signers to access dictionary re-sources using sign languages (Desai et al., 2024).As a crowd-sourced dataset with videos of individ-ual signs, the ASL Citizen dataset also serves toimprove documentation of sign languages. Thisdataset is the first crowd-sourced dataset of videos",
  "Demographics available through the ASL Citizen projectpage:": "for isolated signs, and members of deaf commu-nities participated in, and were compensated for,this effort. When supplemented with the Sem-Lexbenchmark (Kezar et al., 2023a), a crowdsourcedISLR dataset released shortly after, 174k videos intotal can be used for ISLR. The ASL Citizen datasetis licensed by Microsoft Research and is bound bythe Microsoft Research Licensing Terms3.The ASL Citizen dataset is composed of videosof individual signs for isolated sign language recog-nition (ISLR). Other ISLR datasets with videos ofindividual signs have been released, including WL-ASL (Li et al., 2020), Purdue RVL-SLL (Wilburand Kak, 2006), BOSTON-ASLLVD (Athitsoset al., 2008), and RWTH BOSTON-50 (Zahediet al., 2005). The above datasets, however, arenot crowd-sourced. The closest dataset to the ASLCitizen dataset is the Sem-Lex Benchmark (Kezaret al., 2023a), a crowdsourced ISLR dataset withover 91k videos. Because the Sem-Lex Benchmarkdoes not release demographic information aboutthe participants, we are not able to include it in ourbias studies.The ASL Citizen dataset is made up of crowd-sourced videos from ASL signers, where eachvideo corresponds to a particular sign. The cor-pus is composed of videos for 2731 unique signs,all of which are contained in the ASL-Lex datasetCaselli et al. (2017), a lexical database of signswith annotations including the relative frequency,iconicity, grammatical class, English translations,and phonological properties of the sign. Thus, re-searchers studying this dataset can also take advan-tage of the ASL-Lex annotations. As part of theoriginal data collection effort, demographic infor-mation about each participant was collected, but itwas not released. With the publication of this work,we release the demographic data in this set, andprovide a detailed analysis of this data.Our analyses of demographics and bias are moti-vated by evidence in the literature that a signers de-mographics may impact their signing. For instance,characteristics of particular spoken languages ordialects have been shown to influence gestures, andin turn sign production (Cormier et al., 2010). Oneexample of an ASL dialect is Black ASL, whichscholarly evidence has shown to be its own dialect(Toliver-Smith and Gentry, 2017), and for whichdocumentation of dialectical differences dates back",
  "Terms of use at are using this dataset in accordance with its intended use": "to 1965 (Stokoe et al., 1965). Whether an indi-vidual speaks Black ASL is likely heavily influ-enced by their race or ethnicity. An example ofgeographic differences is Marthas Vineyard, anisland off the coast of the United States, where anentire sign language emerged due to the high preva-lence of DHH individuals in this community. Hear-ing and DHH people alike used this language tocommunicate until the mid-1900s (Kusters, 2010).There is also a distinct Canadian ASL dialect usedby signers in English-speaking areas of Canada(Padden, 2010), which is documented in a dictio-nary (Bailey et al., 2002). Age of language ac-quisition also impacts ASL production; delayedfirst-language acquisition affects syntactic knowl-edge for ASL signers (Boudreault and Mayberry,2006) and late acquisition (compared to native ac-quisition) was found to impact sensitivity to verbagreement (Emmorey et al., 1995).Previous work also indicates the impact of cer-tain visual and linguistic features on sign languagemodeling. Training an ISLR model to predict a signand its phonological characteristics was found toimprove model performance by almost 9% (Kezaret al., 2023b). (Sarhan et al., 2023) find improvedperformance when using attention to focus on handmovements in sign videos.To our knowledge, there are no existing worksthat extensively study various sources of model biason a crowd-sourced dataset of sign videos with la-beled participant demographics. With this work,we aim to address this gap with a systematic analy-sis of the impact of various participant-level, sign-level, and video-level features, and experiment withdebiasing techniques to reduce disparities in modelperformance.",
  "Data": "The ASL Citizen dataset is a crowd-sourced datasetcontaining 83,399 videos of individual signs inASL from 52 different participants. The datasetcontains 2731 unique signs that are included in theASL-Lex (Caselli et al., 2017) dataset, a datasetwith detailed lexical annotations for each sign. Theauthors of the original work report some demo-graphic statistics, but the demographics of indi-vidual (de-identified) participants have not beenreleased. Here, we provide a detailed report thatincludes demographic breakdowns and analyses ofvarious linguistic and video features in the dataset,including the breakdown of these features by gen-",
  "Demographic Distributions": "In total, the ASL Citizen dataset is comprised of 32(61.5%) women and 20 (38.5%) men. 21 womenare represented in the training set (60%), 5 in thevalidation set (83%), and 6 in the test set (55%).The vast majority of participants report an ASLlevel of 6 or 7, as we show in in AppendixA. The participants also list their U.S. states. Usingthis information, we divide them into four regionsbased on the U.S. Census definitions 4: Northeast,Midwest, South, and West. More participants inthe dataset are from the Northeast than any other re-gion, as shown in in Appendix A. We alsofind that the age range of participants is skewed;participants in their 20s and 30s make up 32 of the52 participants (see in Appendix A).Participants did not note their ethnicity or racefor this dataset. As such, to uncover potential biasesrelated to the participants perceived skin tone intheir videos, we run the skin-tone-classifierPython package from Rejn Pina and Ma on theframe with the first detected face in each video. Wefind that when we do not specify that the videoswere in color, the classifier most often detects themas black and white. When we specify that thevideos are in color, the most common skin tonedetected (out of the default color palette used inRejn Pina and Ma) is #81654f. Because the clas-sifier most commonly detects as black and white,we also try specifying the video frames as beingblack and white. In this setting, the most commonskin tone detected is #b0b0b0, and the distributiondiffers from when the images are specified colorimages. We plot these results in .",
  "Video LengthWe study the distribution of videolengths in order to better understand how videolength may vary in this dataset. We find that the": "distribution of video lengths (s) is skewed left, witha longer tail on the right, as shown in .We also study whether video lengths vary, onaverage, for participants of different ages and gen-ders. To account for differences between the signsdepicted by participants (since participants did notall record the same signs), for each video, we cal-culate the number of standard deviations (SDs) thevideo length is away from the mean for all videosof that sign - in other words, we calculate the z-score at the sign level. We show this calculationin the equation below, where vi(s) represents thelength of video i depicting sign s.",
  "s(1)": "We find that, while men on average record videosover .3 SDs longer than the mean, women on av-erage record videos over .2 SDs shorter than themean. Thus, compared to other videos with thesame sign, women record shorter videos than menon average. We show these results in .Older participants, particularly those in their 70s,record longer videos on average (again, relative toother videos of the same sign) than younger par-ticipants. During manual inspection, we find olderparticipants are more likely to have longer pausesbefore or after signing than younger participants,which may explain this gap. We also show theseresults in . Sign FrequencyThe ASL Citizen dataset is com-prised of 2731 signs from the ASL-Lex datasetCaselli et al. (2017), a dataset with expert annota-tions about properties of each sign including fre-quency of use, iconicity, and varying phonologicalproperties. To collect sign frequency labels, deafsigners who use ASL were asked to rate signs from1 to 7 in terms of how often they appear in everydayconversations, where 1 was very infrequently\" and7 was very frequently\". We plot and compare thesign frequency distributions for the ASL Citizendataset and the ASL-Lex dataset in , andfind that they are very similar.We also find that there is little variation in aver-age sign frequency for different genders. For maleparticipants, the average sign frequency is 4.1592,while the average sign frequency for female partic-ipants is 4.1395, indicating that female participantschose slightly less frequently-occurring signs thanmen overall.",
  "Sign IconicityThe ASL-Lex dataset also con-tains crowd-sourced annotations for sign iconicity,": "where non-signing hearing annotators watch videosof a sign and evaluated how much they look likethe signs meaning from 1 (not iconic) to 7 (veryiconic). We calculate an average iconicity of 3.378in the ASL-Lex dataset, and 3.379 in the ASL Citi-zen dataset. We plot these distributions in , and again find that they are very similar.We find average iconicity is 3.378 for womenand 3.381 for men. This indicates that, as with fre-quency, there is only a slight difference, on average,between the iconicity of signs chosen by male andfemale participants.",
  "For our experiments, we use the baseline I3Dand ST-GCN models which were trained on theASL Citizen dataset and released along with thedataset.5. We describe the details of these modelsbelow": "I3DThe I3D model is a 3D convolutional net-work trained on the video frames themselves (Car-reira and Zisserman, 2017). As with the originalASL Citizen baselines, we train our I3D model onpreprocessed video frames from the sign videos inthe ASL Citizen training set. These videos are eachstandardized to 64 frames by skipping or paddingframes depending on video length. Videos are thenrandomly flipped horizontally to imitate right- andleft-handed signers.",
  "ST-GCN": "The ST-GCN model is a temporal graph convolu-tional network trained on pose information (Yanet al., 2018). As with the original ASL Citizen base-line, we obtain pose representations for each frameusing Mediapipe holistic (Lugaresi et al., 2019),with a set of 27 keypoints established by Open-Hands (Selvaraj et al., 2022). These keypoints arecenter scaled and normalized using the distancebetween the shoulder keypoints. The frames arecapped at a maximum of 128, and random shear-ing and rotation transformations are applied duringtraining for data augmentation. : I3D (top) and ST-GCN (bottom) top 1 accu-racy scores by detected skin tone. We find that, despitebeing less represented in the dataset, videos with lighterdetected skin tones have higher accuracy scores on aver-age for both models. The ST-GCN model, in particular,exhibits this behavior.",
  "Participant-level differences": "Baseline models perform over 10 percentagepoints better for male vs. female participantsWe run the baseline I3D and ST-GCN modelstrained on the ASL Citizen dataset (Desai et al.,2024), and, for both models, find an accuracydisparity between male and female participants.For the I3D model, the overall Top-1 accuracy is0.6306, while for females it is 0.5914 and for malesit is 0.6776; in other words, a gap of over 10 points",
  "in favor of male participants is observed. An evenbigger gap is observed for the ST-GCN model; theoverall Top-1 accuracy is 0.5944, while the Top-1accuracy is 0.6838 for males and 0.52 for females": "Average model accuracy varies greatly betweenparticipantsOne possible contributor to theabove performance disparities for male and femaleparticipants is variation in participant-level modelaccuracy. There are 11 participants whose videosare in the test set for the ASL Citizen dataset. Ofthese 11 participants, 6 are female and 5 are male.When calculating accuracy scores for each partici-pant, we find high variation for both models, withover 15-point differences between the highest andlowest accuracy scores (see . This variationmay contribute to the gender performance gap, asthere are only a few participants of each gender inthe test set.While performing manual inspection, we findseveral characteristics of user videos that appear tovary between participants. Different participantshave different background or lighting quality, andsome participants mouth the word being signedwhile other participants do not. We also find in-stances of repetition, where the sign is repeated inthe video, from P15, a female participant. There arealso some instances of fingerspelling, where partic-ipants fingerspell the sign before signing it. Theseand other individual differences may contribute tothe observed performance disparities. The models perform better on lighter skin tonesthan darker skin tones on averageDespitedarker skin tones making up most of the detectedskin tones for videos in this dataset (see ),we find that models average higher performancewhen the detected skin tone is lighter. We illus-trate this phenomenon for both models in . As this figure shows, I3D follows similar trendsto ST-GCN in terms of comparative performancefor different skin colors, performing the best forlighter skin tones #BEA07E and #E5C8A6. Thatbeing said, ST-GCN performs comparatively morepoorly on the three darkest skin tones (#373028,#422811, and #513B2E) and the lightest skin tone(#E7C1B8) than I3D, when compared to the higher-performing skin tones. This indicates that, thoughboth models show similar patterns regarding theskin tones with higher/lower performances, theRGB-based I3D model appears to perform bet-ter overall on darker skintones than the ST-GCNmodel. Although we find variations in accuracy be-",
  "n > 20.38460.4668": ": Top-1 accuracy scores for videos within a cer-tain number of SDs away from the mean for videos ofthe same sign. For both models, videos with lengthscloser to the mean yield better model performance. tween participants in the previous section, the skintones are categorized at the video level. Thus, it ispossible to see variation in predicted skin tone fordifferent videos recorded by the same individual.The lighting quality of individual videos may be aconfounder for these results. Trained models exhibit the highest average per-formance on participants in their 20s and 60sThe ASL Citizen test set is made up of 11 individ-uals in their 20s, 30s, 50s, and 60s. We find that,as with gender, model accuracy varies for differ-ent age ranges; the highest accuracy scores wereachieved for participants in their 20s and 60s. Thiscould be influenced by the proportion of partici-pants in their 20s in the training set.",
  "Video-level differences": "Performance decreases as the video length di-verges from the averageFor each sign video inthe ASL Citizen dataset, we calculate the z-scoreof its video length compared to other videos of thesame sign. We then place these values into buckets:less than -2, -2 to -1, -1 to 0, 0 to 1, 1 to 2, andmore than 2 SDs from the mean. We find that, onaverage, the videos farther away from the meansee decreased model performance compared to thevideos closest to the mean. The results in full arein . Performance decreases when video quality de-gradesIn addition to video length, we studythe impact of video quality on model accuracy.Given that we are studying the quality of indi-vidual video frames without a reference image,we use the BRISQUE score (Mittal et al., 2012)to measure image quality of individual frames.Higher BRISQUE scores indicate lower quality,while lower BRISQUE scores indicate higher qual-ity. We find that higher BRISQUE scores corre-late negatively with Top-1 model performance for : Association between BRISQUE image qualityscores and accuracy. Higher BRISQUE scores indicatelower image quality, and vice versa. Thus, higher im-age quality appears to be associated with better modelperformance.",
  "the I3D model, with a Spearman correlation of = 0.0367 and a p-value of p = 1.53x108.We show a scatterplot of these results in ,along with a linear regression line": "Dissimilarity between participant and seedsigner signs negatively impacts model accuracyfor the ST-GCN pose modelThe Frecht dis-tance is often used as an evaluation metric for signlanguage generation, to study the similarity be-tween generated signs and references (Hwang et al.,2024; Dong et al., 2024) (see D for more details).In the ASL Citizen dataset, one of the participantsis a paid ASL model who records videos for everysign, referred to as the seed signer\".We study whether dissimilarity between the par-ticipant and seed signer may have a negative im-pact on model accuracy. To do so, we use the posemodels used as input to the ST-GCN model. Ev-ery .25 seconds, we measure the distance betweenthe model pose and the participants pose at thatframe, studying the distance between left handsand right hands separately. We find no significantrelationship between right hand or left hand dis-tance from the seed signer for the I3D model, andfor the ST-GCN model we find a significant nega-tive Spearman correlation between distance fromthe seed signer and accuracy for the right hand( = .0289, p = 0.001). We plot these results,along with lines of best fit, in . When the average signing speed\" is closer tothe sign-level average, performance is betterInaddition to video length, we are interested in study-ing the average distance between poses over con-sistent time intervals. We want to study how muchmovement on average occurs within these incre-ments, i.e. the speed\" of sign production. Westudy this by calculating the pairwise Frechet dis-tance between poses at each 0.25 second interval,",
  "n > 2.57110.4739.5619.5107": ": Number of SDs away from the mean of the sign(in buckets) for the speed\" of signing, i.e. the averageFrechet distance between poses every 0.25 seconds, forright hand and left hand. We find that, for both righthand and left hand, the performance degrades as theaverage speed\" of the sign production in a sign videodeviates from the average for that particular sign. with distance calculated between a pose and thepose .25s after, starting from the first frame. Weagain take this distance for the participants righthand and left hand. We find that, on average, thefarther away a participants average signing speedis from that signs mean, the worse performance is,with especially high performance degradations 2SDs or more from the mean. We show these resultsin .",
  "Sign-level lexical features": "Here, we present results for four sign-level fea-tures annotated in the ASL-Lex dataset: sign fre-quency, iconicity, phonological complexity, andneighborhood density. We find that several of thesefeatures are significantly correlated with model per-formance, which we discuss below. Sign frequency, phonological complexity, andneighborhood density are negatively correlatedwith model accuracyAs mentioned in 3.2,sign frequency annotations in the ASL-Lex datasetwere collected from ASL signers. The ASL-Lex2.0 dataset (Sehyr et al., 2021) also contains a newphonological complexity metric. Using 7 differentcategories of complexity, scores were calculatedby assigning a 0 or 1 to each category (dependingon whether that category was present) and addingthem together, for a maximum possible scores of 7(most complex) and a minimum possible score of 0.The highest complexity score in the dataset was a6. Neighborhood density was calculated based onthe number of signs that shared all, or all but one,phonological features with the sign.Intuitively, we expect negative associations withphonological complexity and accuracy as wellas neighborhood density and accuracy, and in- deed find significant negative correlations ( =0.0618, p = 0.005 for phonological complex-ity and rho = 0.0584, p = 0.01 for neighbor-hood density). However, we also find a significantnegative association between sign frequency andmodel accuracy ( = 0.057, p = 0.011). Ex-isting work indicates that higher-frequency wordsare produced more quickly than low-frequencywords (Jescheniak and Levelt, 1994; Emmoreyet al., 2013; Gimeno-Martnez and Baus, 2022);thus, it is possible that this association could berelated to video length. There is no significant correlation betweeniconicity and model accuracyAs mentioned in 3.2, sign iconicity ratings were also collected forthe ASL-Lex dataset. We find a very slight posi-tive correlation between sign iconicity and modelaccuracy ( = 0.044), which is not significant(p = 0.8424). Thus, we conclude that visual simi-larity to the English word appears not to affect themodels ability to recognize a sign.",
  "Which features are the best predictors ofmodel accuracy?": "After looking at the impacts of lexical, demo-graphic, and video features on model accuracy,we are interested in studying which features are(by themselves) the best predictors of model accu-racy. As such, we study the mutual informationbetween each feature and the Top-1 accuracy forthe I3D and ST-GCN models. We study 19 fea-tures in total, where some relate to participant de-mographics (e.g. age and gender), others relateto the sign lexical features (e.g. sign iconicity),and the rest are characteristics of individual videos(e.g. BRISQUE score and Frechet distances). Wefind that the 5 most impactful features are charac-teristics of individual videos (BRISQUE, Frechetfrom seed signer, and absolute z-score of sign-ing speed\"), with BRISQUE video quality scoresshowing the highest mutual information with Top-1accuracy. Out of the lexical features, sign iconicityhas the highest mutual information, and out of thedemographic features, the participants ASL levelhas the highest mutual information with the modelperformance. The results are in . : The relationships between sign frequency (left), sign iconicity (center left), phonological complexity(center right), and neighborhood density (right) and top 1 accuracy for the ST-GCN model. We find that signfrequency, phonological complexity, and neighborhood density are all significantly negatively correlated with modelaccuracy (p < 0.05) when calculating the Spearmans rank correlation. However, despite a slight positive correlationbetween iconicity and accuracy, the p-value is not significant.",
  "OverallFemale participantsMale participantsParityModelTop-1Top-5Top-10Top-1Top-5Top-10Top-1Top-5Top-10(Top-1)": "ST-GCN.5238.7665.8295.4406.6886.7665.6236.8601.9374.7065ST-GCN (VL).5488.7923.8515.4666.7200.7941.6476.8791.9205.7205ST-GCN (VL, fem.).5395.7926.8538.4621.7202.7974.63.8795.9216.7334ST-GCN (brisque, HP).4723.7344.8046.3949.6551.73540.5653.8296.8877.6986ST-GCN (brisque, LP).5580.7960.8545.4801.7279.8011.6516.8779.9187.7368 : Performance of ST-GCN baseline against models that use the resampling strategies discussed in 6.3. Wefind that all resampling strategies improve accuracy and gender parity over the baseline (for every metric but Top-10Male), and resampling lower quality videos at a higher rate improves gender parity the most, followed closely byresampling based on video length from only female participants.",
  "Training on single-gender subsets": "We first address the gender performance gap bytraining on participants of each gender in isolation.When doing this, we find a slight difference be-tween the performance gaps for models trained onmale-only and female-only subsets. For the modeltrained on the male-only subset, the Top-1 accuracyfor male subjects is .292, and the Top-1 accuracy is.168. For the model trained on the female-only sub-set, the Top-1 accuracy for male subjects is .291,and the Top-1 accuracy for female subjects is .206.Thus, the model trained only on female subjects hasa smaller gap, and higher accuracy parity, betweenmale and female subjects than the model trainedon only male subjects. However, both models havelow performance overall, so the Top-1 accuracyparity for subjects of different genders (calculatedby dividing the female accuracy by the male accu-racy) is .7571 for the model trained on all subjectscompared to .7079 for the model trained on onlyfemale subjects. The model trained on only malesubjects has the lowest accuracy parity, at .5746.We show these results in in Appendix I.",
  "Training label shift": "In addition to training on single-gender subsets, weexperiment with a label-shift approach to debias-ing. Because ISLR is a multiclass problem, weexperiment with the reduction-to-binary approachfor debiasing multi-class classification tasks pro-posed by Alabdulmohsin et al. (2022). We run thelabel-shift algorithm and train the ST-GCN modelon the debiased labels for 25 epochs, and comparethe performance of the debiased model to the ST-GCN model without debiasing, which we also trainfor 25 epochs. We find that the model trained onregular labels actually has a higher ratio for femaleto male accuracy than the debiased model: .7476for the baseline model, and .7052 for the debiasedmodel. We show these results in full in .",
  "Weighted resampling": "Although there is a large gender performance gapobserved (5.1), based on the results from ,other features are much more heavily tied to modelaccuracy. Thus, it is likely that these features (inparticular, features at the video level) may influ-ence results. But what happens if the impact ofvideos with potentially-noisy features is reducedduring training? We experiment with weighted re-sampling, where samples with certain features are",
  "more likely to be resampled. We explain how wecalculate the resampling probability, and present re-sults, for each variable we study in the paragraphsbelow": "Video lengthWe first experiment with calcu-lating the resampling probability based on videolength. Given that videos closer to the mean pro-duced higher accuracy scores, we wanted to resam-ple these videos at a higher rate to reduce trainingnoise. We calculate the probability of resamplingas follows, where vi(s) refers to the length of videoi for sign s, s refers to the mean video length ofvideos depicting sign s, and s refers to the SD forvideo lengths of videos depicting sign s:",
  "We show the results for this approach in Table": "3, represented by the ST-GCN (VL) model. Wefind that this approach improves upon the baselineST-GCN model by at least 2 percentage points forall accuracy metrics, and improves gender parityfor Top-1 accuracy by 1.4%. Video length for female participantsWe thenexperiment with the exact same resampling processdescribed above, based on number of SDs from themean for video length, but only resample videosfrom female participants. Because training on anall-female subset yielded a higher test accuracy forfemale subjects than an all-male subset (),we want to investigate whether restricting our re-sampled data to female participants improves thegender performance gap. We show these resultsin , under the baseline ST-GCN (VL, fem.).We find that this approach exceeds calculating theresampling probability using video length for par-ticipants of all genders for Top-5 and Top-10 accu-racy. We also find that this baseline achieves thesecond-highest gender parity of all of the baselines,at 2.69% higher than the baseline. Thus, we find ev-idence that resampling based on video length SDs,but only videos from female participants (the groupwith the lower model accuracy scores), greatly im-proves gender parity over the baseline model. BRISQUE scoreBecause the BRISQUE scoreshows the highest mutual information with Top-1accuracy, we experiment with resampling based onthe video quality. We experiment with two differentresampling strategies: resampling higher-qualityvideos at a higher rate (resampling high quality) and resampling lower-quality videos at a higherrate (resampling low quality). We discuss thesestrategies below.Resample high quality: We first experimentwith resampling more high-quality videos (lowerBRISQUE scores) at a higher rate by setting the re-sampling probability as a function of the BRISQUEscore, with higher BRISQUE scores reducing theresampling probability. We calculate the probabil-ity of resampling as follows, where Bi(s) refers tothe BRISQUE score of video i:",
  "Conclusion": "In this work, we address a gap in sign languageprocessing research by exploring biases in sign lan-guage resources, and experimenting with strategiesto mitigate these biases. We focus on the ASL Citi-zen dataset in particular, and release demographicinformation for this dataset to aid future work. Wefind performance gaps related to skin tone, partic-ipant age, and gender. Still, we find that videolevel features, such as the video quality, signingspeed\", and video length, appear to be the bestpredictors of model accuracy. We find that selec-tively resampling data with video lengths closer tothe mean improves overall performance. We alsofind that doing this resampling strategy for onlythe group with lower model performance (female,when comparing genders) improves the gender par-ity for model performance. We find that resamplinglower-quality videos at a higher rate achieves thehighest Top-1 accuracy and gender parity.",
  "While in this work we find and document perfor-mance gaps between participants of different de-mographics such as age and gender, because of": "the differences between individual participants thatwe detail above (see ), and the number ofparticipants in the test set (11), it is unclear howmuch of these differences are due to age or to otherunderlying factors.Another limitation is that we focus on a singledataset. This is due in part to the fact that this is theonly large-scale crowdsourced dataset for isolatedsign language recognition with demographic labels.However, as more crowdsourced sign language re-sources become available, it is critical that theseanalyses are repeated on these datasets to assessthe generalizability of our results.",
  "Ethical Implications": "In our analysis of participant demographics, and ac-companying features, for the ASL Citizen dataset,we present some characteristics of the dataset thatvary between demographics. For instance, we dis-cuss our findings that male participants and olderparticipants typically record longer videos. It isimportant to emphasize that these findings shouldnot be generalized to all ASL signers, and that theyshould instead be used to study the characteristicsof this dataset in particular.Further, this work is not exhaustive; there aremany sources of bias unexplored by this work, in-cluding differences in participant culture or ethnic-ity. There may be many more sources or dimen-sions of bias not covered in this paper that shouldbe explored by future work.We also note that participants who chose to de-note their demographic information (which was op-tional) consented for this information to be anony-mously released as part of the dataset. No iden-tifiable information about the participants will bereleased with the publication of this paper; rather,anonymous participant IDs will be accompaniedwith their demographics.",
  "Carole Sue Bailey, Kathy Dolby, and Hilda MarianCampbell. 2002. The Canadian dictionary of ASL.University of Alberta": "Patrick Boudreault and Rachel I Mayberry. 2006. Gram-matical processing in american sign language: Ageof first-language acquisition effects in relation to syn-tactic structure. Language and cognitive processes,21(5):608635. Joao Carreira and Andrew Zisserman. 2017. Quo vadis,action recognition? a new model and the kineticsdataset.In proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages62996308.",
  "Autoregressive sign language production: A gloss-free approach with discrete representations. Preprint,arXiv:2309.12179": "Jrg D Jescheniak and Willem JM Levelt. 1994. Wordfrequency effects in speech production: Retrievalof syntactic information and of phonological form.Journal of experimental psychology: learning, Mem-ory, and cognition, 20(4):824. Lee Kezar, Jesse Thomason, Naomi Caselli, Zed Sehyr,and Elana Pontecorvo. 2023a. The sem-lex bench-mark: Modeling asl signs and their phonemes. InProceedings of the 25th International ACM SIGAC-CESS Conference on Computers and Accessibility,ASSETS 23, New York, NY, USA. Association forComputing Machinery.",
  "Annelies Kusters. 2010. Deaf utopias? reviewing thesociocultural literature on the worlds marthas vine-yard situations. Journal of deaf studies and deafeducation, 15(1):316": "Dongxu Li, Cristian Rodriguez, Xin Yu, and HongdongLi. 2020. Word-level deep sign language recognitionfrom video: A new large-scale dataset and methodscomparison. In Proceedings of the IEEE/CVF winterconference on applications of computer vision, pages14591469. Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, ChrisMcClanahan, Esha Uboweja, Michael Hays, FanZhang, Chuo-Ling Chang, Ming Guang Yong,Juhyun Lee, et al. 2019. Mediapipe: A frameworkfor building perception pipelines.arXiv preprintarXiv:1906.08172.",
  "Ronnie Wilbur and Avinash C Kak. 2006. Purdue rvl-slll american sign language database": "Sijie Yan, Yuanjun Xiong, and Dahua Lin. 2018. Spatialtemporal graph convolutional networks for skeleton-based action recognition. In Proceedings of the AAAIconference on artificial intelligence, volume 32. Morteza Zahedi, Daniel Keysers, Thomas Deselaers,and Hermann Ney. 2005. Combination of tangent dis-tance and an image distortion model for appearance-based sign language recognition. In Pattern Recogni-tion: 27th DAGM Symposium, Vienna, Austria, Au-gust 31-September 2, 2005. Proceedings 27, pages401408. Springer.",
  "AParticipant Demographics": "Here, we plot the demographic information dis-cussed in 3.1. Note that providing demographicinformation was optional, so these numbers willnot always add up to the total number of partici-pants (52).In , we plot the distribution of ASL lev-els and regions associated with the participants inthe ASL Citizen dataset. We find that most par-ticipants are at an ASL level of 6 of 7, with onlyone participant each at level 3 or 4. A pluralityof participants are from the Northeast, almost half.The West contains the fewest participants.In , we plot the distribution of partici-pants ages. We find that participants are mostlyskewed towards younger adults (20s and 30s) butthat there is also a slight skew towards contestantsin their 60s. Contestants in their 20s, 30s, 40s, 50s,60s, and 70s are represented in the dataset, but con-testants in their 40s and 70s are not represented inthe test set.In , we plot the distribution of skin tonesin the dataset when frames are set as color imagesand black-and-white images. We include black-and-white images because we found that, whenan image type was not set, the model detected the",
  ": Frequency of detected skin tones of partici-pants in videos when the video frames were set manuallyto color images (left) and black and white images (right)": "images as black-and-white images in the majorityof cases. One notable finding is that the skin colormodel detected lighter skin tones more frequentlywhen the images were set to black-and-white thanwhen they were set to color images. This indicatespossible unreliability of the skin color detection; itis possible, for instance, that when the images areset to color, the system classifies the skin colors asdarker than they actually are.",
  "BVideo Length Distributions": "In , we find that video lengths havea skewed distribution, where the average videolength is higher than the median. In other words,video lengths lower than the mean are more com-mon and vice versa, and there is a long tail to theright. After watching participants videos, we sus-pect that this difference in video length is a resultof some participants having a tendency to pause formultiple seconds at the beginning of end of theirrecording. This happens especially often with thefirst couple of videos that people record.We also find that female participants have, onaverage, shorter videos related to their signs thanmale participants. For each sign video, we calcu-lated the mean and standard deviation for all videoswith that sign. We then calculated how many stan-dard deviations those movies were away from themean.",
  ": Distribution of video lengths for all signvideos in the ASL Citizen dataset. The distributionis skewed towards the right, with a long tail on the right": ": Average number of standard deviations awayfrom the mean at the sign level for male and femaleparticipants (top) and participants in their 20s, 30s, 40s,50s, 60s, and 70s (bottom). Relative to other videos ofthe same sign, women tend to record shorter videos, andolder participants tend to record longer videos. : Distributions of labeled sign frequencies foreach of the 2731 signs from the ASL-Lex dataset (top)and all of the sign videos in the ASL Citizen dataset(bottom). The distributions are very similar, indicatingthat users chosen signs of certain frequencies at a similarrate to how they are distributed in the ASL-Lex dataset.",
  "CLexical Feature Distribution": "In addition to getting demographic and video fea-tures, we used the ASL-Lex (Caselli et al., 2017)annotations to analyze lexical features in the ASLCitizen dataset. We found that, for sign frequencyand iconicity, the distributions are very similar tothose in the ASL-Lex dataset. The distributions ofboth datasets are plotted side-by-side for frequencyand iconicity, respectively, in Figures 10 and 11.",
  "EAccuracies for different age ranges": "In , we show the Top-1 accuracy scoresfor the I3D and ST-GCN model for participants ofdifferent ages. We find the highest scores occurfor participants in their 20s and 30s, with the thirdhighest scores occuring for participants in their60s. Participants in their 40s and 70s were notrepresented in the test set.",
  "FModel accuracies for each participantin the test set": "In , we report the accuracy scores for thebaseline ST-GCN model on the participants in thetest set of the ASL Citizen dataset. We find differ-ences of over 20 points between participant aver-ages for both models. P6, P9, P15, P17, P18, andP22 disclosed that they are female, while the otherparticipants disclosed that they are male. : The Frechet distance from the seed (model)signer vs. top-1 accuracy for the I3D model (top) andST-GCN model (bottom), with the distance between lefthands on the left and the distance between right handson the right.",
  "GFrechet distance from seed signer": "In , we plot the Top-1 accuracies forthe I3D and ST-GCN model as a function of theFrechet distance from the seed signer for each signvideo (where the seed signer is a recruited ASLmodel for the ASL Citizen dataset). We find asignificant negative correlation between Frechetdistance from the seed signer and Top-1 accuracyfor the ST-GCN pose model, but no significant cor-relations for the I3D model.",
  "HMutual Information Results": "In , we present the mutual information re-sults in full for each studied variable. We study19 variables total, spanning demographics, signlexical features, and video-level features, and cal-culate the mutual information between each featureand the Top-1 accuracy. We find the highest lev-els of mutual information to occur for video-levelfeatures, suggesting features of individual videosare more impactful for model accuracy than demo-graphic characteristics of the participants. Out ofthe demographic characteristics, the ASL level ofthe participant appears to be the most influentialwith respect to accuracy.",
  "Bounding Box Area (LH)00": ": Mutual information for each of the featuresabove and the Top-1 accuracy for the ST-GCN and I3Dmodels, respectively. For both models, the BRISQUEscore, average Frechet distance from the model (righthand and left hand) and the absolute value of the numberof SDs of the average Frechet distance between framesare the top three features, with the other features far be-hind. This seemingly indicates that video-level featuresare the biggest indicator of model accuracy.",
  "JResults for model trained on debiasedlabels": "We report the results for a model trained for 25epochs on training labels that were debiased usingthe reduction-to-binary techniques proposed by Al-abdulmohsin et al. (2022). We find that the modeltrained on regular labels actually had a higher accu-racy parity score (ratio of female accuracy to maleaccuracy) than the model trained on debiased la-bels. We show the Top-1, Top-5, and Top-10 resultsfor each model in .",
  "All.244.479.581.224.434.527.594.828.881Male.291.548.653.292.538.639.684.902.939Female.206.421.521.168.347.433.520.767.833": ": Performances for ST-GCN model trained on only male subjects, only female subjects, and all subjects,respectively. We find that the model trained on only female subjects has the lowest performance gap between maleand female subjects in the test set, but the ratio of female accuracy to male accuracy is highest for the model trainedon all subjects."
}