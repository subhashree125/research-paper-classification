{
  "Abstract": "Recent advances in Large Language Models(LLM) have led to substantial interest in theirapplication to commonsense reasoning tasks.Despite their potential, LLMs are susceptible toreasoning errors and hallucinations that may beharmful in use cases where accurate reasoningis critical. This challenge underscores the needfor verifiable, debuggable, and repairable LLMreasoning. Recent works have made progresstoward verifiable reasoning with LLMs by us-ing them as either (i) a reasoner over an ax-iomatic knowledge base, or (ii) a semanticparser for use in existing logical inference sys-tems. However, both settings are unable toextract commonsense axioms from the LLMthat are not already formalized in the knowl-edge base, and also lack a reliable method torepair missed commonsense inferences. In thiswork, we present LLM-TRes, a logical reason-ing framework based on the notion of theoryresolution that allows for seamless integrationof the commonsense knowledge from LLMswith a verifiable logical reasoning frameworkthat mitigates hallucinations and facilitates de-bugging of the reasoning procedure as well asrepair. We crucially prove that repaired axiomsare theoretically guaranteed to be given prece-dence over flawed ones in our theory resolutioninference process. We conclude by evaluat-ing on three diverse language-based reasoningtasks preference reasoning, deductive reason-ing, and causal commonsense reasoning anddemonstrate the superior performance of LLM-TRes vs. state-of-the-art LLM-based reasoningmethods in terms of both accuracy and reason-ing correctness.",
  "Introduction": "The rise of Large Language Models (LLMs) hasmarked a pivotal moment in the real-world de-ployment of AI, particularly due to the excep-tional ability of LLMs to handle complex reason-ing tasks (Chang et al., 2024; Huang and Chang, 2023). Research has shown that LLMs have ac-quired significant commonsense knowledge (Zhaoet al., 2024; Bian et al., 2023), which is crucialfor engaging with real-world users in tasks such asquestion answering (Singhal et al., 2023) and rec-ommendation (Sanner et al., 2023). Unfortunately,LLMs are prone to a variety of reasoning errors; forexample, they commonly incorporate superficiallyplausible but factually incorrect information intotheir reasoning in a phenomenon known as hallu-cination (Zhang et al., 2023b; Ji et al., 2023; Guoet al., 2024). Furthermore, since the underlyingreasoning process of the LLM is latent and hencelargely opaque, validating reasoning soundness andidentifying errors remains an open research prob-lem. Such issues present a significant challenge tothe reliability of using LLMs as reasoning systems,which impedes their practical utility (Mallen et al.,2023).In light of these obstacles, recent research hasproposed methodologies for extracting verifiablereasoning from LLMs by leveraging formal reason-ing procedures. Such works fall under two maincategories: (i) Using the LLM as a reasoner acrossan axiomatic knowledge base, while organizing thereasoning process into simpler subgoals to facili-tate soundness of the overall reasoning (Kazemiet al., 2023). (ii) Leveraging the LLM as a se-mantic parser that translates natural language state-ments into logical axioms, followed by the use ofan off-the-shelf theorem prover to perform logicalreasoning (Pan et al., 2023; Olausson et al., 2023).While these seminal works have made progresstowards verifiable LLM reasoning, their applica-tion in real-world tasks requiring commonsensereasoning is limited since they all suffer from theinability to extract verifiable commonsense axiomsfrom the LLM that are not already formalized inthe provided knowledge base axioms. Hence, theseexisting methodologies critically lack the abilityto leverage the LLM as a verifiable commonsense \"\"\"\"\"\"\"\"",
  "Repair Axiom": "\"\"\"\" \"\"\"\" Priority Queue for Recipe 2 Clauses Priority Queue for Recipe 2 Clauses Priority Queue for Recipe 2 Clauses Priority Queue for Recipe 2 Clauses Priority Queue for Recipe 2 Clauses \"\"\"\" Recipe 2: \"Cajun Catfish Stew\" \"\"\"\"\"\" Recipe 2: \"Cajun Catfish Stew\" \"\"\"\"\"\" Recipe 1: \"Garlic Shrimp\" Recipe 1 is the moreprobable recipe before repair. After repair, Recipe 2 is now the more probable recipe. I'd like a seafood recipe with a kick to it. Reasoning with Repair Axioms",
  "Query": ": Preference reasoning is used as an illustrative example to show the LLM-TRes workflow.Top:LLM-based theory resolution is performed to calculate proof scores of two candidate Recipes entailing theQuery.The proof begins from the negated query, and for each resolvent clause, a priority score tuple:(proof plausibility score, proof length) is calculated and pushed to a priority queue (only Recipe 2 clauses areshown here). At each step, the clause with the highest priority in the queue becomes the active clause. Here, dueto a flawed low probability assigned to catfish entailing seafood, the proof score of Recipe 2 is mistakenlycalculated lower than it should be. Bottom: After insertion of the Repair Axiom, the erroneous reasoning is repaired,leading to a higher score for the correct Recipe 2.",
  "reasoner to fill-in inevitable knowledge base gaps.Furthermore, these methods lack any mechanismfor repairing reasoning mistakes after detection": "To address these challenges, we propose LLM-TRes, a formal reasoning framework using LLMs.LLM-TRes satisfies three key desiderata that weillustrate through the worked example in :(i) Verifiability: allowing for verification of everystep in the reasoning process (i.e., from each suc-cessful refutation resolution , one can backtracethe entire proof of the Query). (ii) Debuggability:being able to identify the incorrect inferences thatled to a reasoning mistake (i.e., we observe an in-correct low probability LLM inference that catfishentails seafood for Recipe 2). (iii) Repairability:enabling a deterministic and reliable mechanismfor rectifying the identified errors to produce cor-rect inference (e.g., once we add the the explicit Re-pair Axiom y catfish(y) seafood(y), wearrive at a much higher proof plausibility for thecorrect preference match of the query to Recipe 2). Formally, LLM-TRes is based on the conceptof theory resolution (Stickel, 1985; Baumgartner,1992), drawn from classical logical reasoning thatenables the integration of specialized reasonersinto the resolution theorem-proving inference rule.Leveraging theory resolution, LLM-TRes seam-lessly incorporates LLMs as specialized reasonersequipped with commonsense knowledge into veri-fiable logical reasoning. This integration enablesextraction of relevant commonsense axioms fromthe LLM that cannot otherwise be obtained fromthe knowledge base. Finally, by capitalizing on aspecially defined selection rule in our resolutionframework, we formally prove that repairing flawedreasoning by the LLM is possible by providing cor-rect axioms that are theoretically guaranteed tooverride the LLMs flawed reasoning.In summary, we contribute the following:",
  "Related Works": "Reasoning with LLMsWhile their primary de-sign was for text generation, LLMs exhibit re-markable performance in many other NLP tasksthat require a variety of reasoning skills (Changet al., 2024; Xu et al., 2023). Despite such im-pressive capabilities, errors and hallucinations thatcan commonly occur in LLM reasoning have moti-vated research on obtaining dependable reasoningfrom LLMs while leveraging their intrinsic knowl-edge (Toroghi et al., 2024). In this regard, severalapproaches have been proposed to elicit strongerreasoning performance from LLMs such as Chain-of-Thought prompting (Wei et al., 2022; Kojimaet al., 2022), Self-Consistency (Wang et al., 2022),Least-to-Most prompting (Zhou et al., 2022), andSelection-Inference (Creswell et al., 2022). De-spite being effective in improving reasoning perfor-mance, all these methods follow an informal rea-soning procedure in which the LLM is in charge ofperforming reasoning and thus does not guaranteethe faithfulness of the reasoning process (Shanahan,2024; Pan et al., 2023). For instance, the reasoningability of these methods may be unreliable for tasksrequiring out-of-domain reasoning (Saparov et al.,2024; Liang et al., 2022), tasks involving nega-tion (Anil et al., 2022), and often degrades withan increase in the length of reasoning steps (Dziriet al., 2024).",
  "Formal Reasoning with LLMsTo obtain reli-able and verifiable reasoning from LLMs, a number": "of works have proposed the idea of using LLMsin a formal reasoning framework a systematicand logical process governed by a set of rules andprinciples (Galotti, 1989). Two main approacheshave been proposed in this regard. In the first ap-proach, the LLM is utilized to perform differentsub-tasks of a formal logical inference rule to rea-son over an axiomatic knowledge base. For ex-ample, LAMBADA (Kazemi et al., 2023) uses theLLM to perform goal decomposition, rule selection,and fact-checking in a backward chaining process.In a related vein, SymBa (Lee and Hwang, 2024)introduces a top-down solver to control the proofprocess and uses the LLM as an aide to the solver.In the second approach, LLMs are used as a se-mantic parser to translate natural language axiomsand facts to a specific logical format; here the re-sponsibility of inference is delegated to a symbolictheorem prover. LogicLM (Pan et al., 2023) usesthis idea with a self-refinement mechanism to al-low the LLM to refine its symbolic conversions.Since LLMs commonly make syntactic and seman-tic errors in the parsing process, LINC (Olaussonet al., 2023) performs majority voting over multiplesolutions to obtain the final result. These works have made significant progress inincreasing the reliability and verifiability of LLMreasoning. However, they only utilize axioms thatare explicitly provided in the knowledge base, andlack the ability to leverage the intrinsic common-sense knowledge of the LLM by extracting com-monsense axioms. This prevents existing methodsfrom incorporating verifiable LLM-derived com-monsense knowledge in their reasoning, which isoften critical in practical deployed usage. More-over, these existing methods do not support a reli-able mechanism for rectifying incorrect reasoningsteps. We aim to address all of these limitationswith our contribution of the LLM-TRes framework.",
  "Methodology: LLM-based TheoryResolution (LLM-TRes)": "We first introduce the resolution rule and the con-cept of theory resolution and then explain our LLM-based Theory Resolution (LLM-TRes) methodol-ogy. For the logical knowledge representation inthis work, we assume a function- and equality-freefirst-order logical (FOL) syntax (Chang and Lee,2014) with all FOL sentences translated to clausalnormal form as demonstrated in . Resolution RuleResolution is a sound and com-plete inference rule that performs inference by de-riving a resolvent clause from two premise clausescontaining complementary literals. Given two FOLsentences in clausal form, a new clause can be de-rived via resolution of their complementary literals,e.g.,",
  "A(x) C(x),(1)": "under the unification = {x/y}. Following thisprocedure, new clauses are derived until eithera contradiction is found (e.g., deriving bothclauses A(x) and A(x) that resolve to ), or nofurther resolutions are possible. Finding a contra-diction implies that the original set of clauses isinconsistent. Therefore, given the knowledge baseK and a query q, to prove that K q, one can applythe resolution inference rule to show that K qleads to a contradiction .",
  "Theory ResolutionTheory resolution (Stickel,": "1985) is a methodology that enables the integra-tion of special purpose reasoning theories into res-olution theorem proving. Based on theory reso-lution, given two clauses c1 = A(x) B(x) andc2 = C(x)D(x), if a theorem prover T identifiesB(x) and C(y) under unification = {x/y} tobe unsatisfiable (i.e., xB(x) C(x) T ), de-spite lacking complementary literals with identicalpredicates, the two clauses can still be resolved:",
  "A(x) D(x).(2)": "Theory resolution considerably broadens the appli-cability of the resolution inference rule by liftingthe condition of resolving only complementary lit-erals. In this work, we use an LLM as the theorythat identifies the unsatisfiable natural languagepredicates to do reasoning via theory resolution. Natural Language LogicNatural language en-compasses a significant amount of information thatcannot be easily represented using symbolic logic.Although one can represent functions and predi-cates in symbolic logic, it may be hard to fullyaxiomatize their real-world meaning, which is asubstantial limitation of the semantic parsing ap-proaches. For instance, being spicy and havinga kick to it are assigned completely different predi-cates, and pure symbolic reasoning cannot identifythe intuitive entailment relationship between them without specific axioms. Moreover, representingcommonsense knowledge in symbolic logic is verychallenging (Davis, 2014). However, LLMs arecapable of understanding the semantic relationshipbetween such predicates and also encompass sub-stantial commonsense knowledge, which can beused for reasoning in real-world applications.As mentioned earlier, theory resolution offers thecapability to resolve non-complementary literals ifthey are deemed unsatisfiable by a theorem prover.By employing an LLM as the theorem prover, wecan leverage the theory resolution framework toconduct resolution within an extended version ofFirst-Order Logic, where predicates and functionsare no longer symbols but natural language texts, asystem we call Natural Language (NL) Logic.Using the LLM theorem prover in the NLlogic, the unsatisfiability condition of the the-ory resolution reduces to natural language en-tailment. In other words, if an LLM identifiesa natural language predicate B to entail predi-cate D, i.e., B(x) LLM D(x), and therefore,B(x) D(x) LLM , then literals B(x) andD(x) can be resolved. For instance, given clausesc1 =kick to it(x) and c2 = spicy (x)Q(x),in which Q(x) is another literal with a natural lan-guage predicate, since the LLM identifies the natu-ral language entailment kick to it LLM spicy,a theory resolution step can be performed as",
  "LLM-TRes Algorithm": "This section presents LLM-TRes, an algorithm forefficient logical commonsense reasoning based ontheory resolution using LLMs. The workflow ofLLM-TRes is shown through a worked example in, and formalized in Algorithm 1. Problem DefinitionConsider a set of queries Qand a knowledge base (KB) denoted by K whichcomprises a set of axioms A and a set of facts F,all represented in natural language logic in clausalform. In this work, we aim to propose an inferencerule i that for each q Q, finds a set of proofsdenoted by proofs, such that each proof f proofsconsists of a subset of clauses in K, and is assigneda priority score reflecting the priority of the proof.",
  "AlgorithmTo prove that K entails the query q viaresolution, we must demonstrate that iteratively ap-plying resolution to derive new clauses from Kq": "leads to a contradiction, thereby proving its unsat-isfiability. The first question that arises is whichclause should be chosen to begin the resolutionproof. Two major paradigms are used in perform-ing resolution: (i) starting from the clauses in Kto derive q from them and resolve it with q, anapproach known as forward chaining, or (ii) start-ing from q and resolving it with clauses in K toreach a contradiction, known as backward chaining.Since backward chaining employs a goal-drivenapproach, which is shown to improve efficiencyin reasoning over natural language (Kazemi et al.,2023), we begin the resolution process from q.Therefore, q becomes our first active clause thatwe need to resolve with a clause from K.The potentially enormous size of K poses a ma-jor challenge. Also, as the resolution process pro-gresses, new clauses are created, leading to a fur-ther expansion in the size of the search space. Toperform resolution efficiently in this combinatorialsearch space, LLM-TRes employs two strategies:(i) prioritizing the resolvent clauses to continue theresolution process, and (ii) restricting the theoryresolution search space using semantic similarity. Resolution Priority Definition and Ordering: Thefirst mechanism employed in LLM-TRes to enableefficient resolution is prioritizing candidate clauses.Using this prioritization scheme, resolvent clausesthat have a higher potential for being part of a plau-sible proof will be given precedence over clausesgenerated from less plausible resolution steps. Theplausibility of a theory resolution step, in whichan active clause c is resolved with a clause ctargetto generate the resolvent clause cres, denoted byentailcres , is determined by calculating the probabilitythat the LLM assigns to ctarget entailing c.",
  "entailcres= p(ctarget LLM c).(4)": "These plausibility scores can help us prioritizethe resolvent clauses. For instance, in the exam-ple provided in , since resolving shrimpwith seafood yields a higher entailment scorethan resolving garlic with seafood, it is in-tuitive to prioritize the first resolvent as it is morelikely to be part of the final proof. Since we areinterested in identifying the most plausible proofs,i.e., the sequences of theory resolution steps withthe highest entailment scores, we define the firstentry of our priority score for each resolvent clausecres as the overall entailment score of all resolutionsteps beginning from the original negated query",
  "entailcres .(5)": "When choosing between equally plausibleproofs, we are interested in shorter proofs thatavoid redundant steps. We assign a second priorityscore to reflect this preference which is consideredonly when the entailment proof scores are equal.As for the proof entailment score, we can obtainthe proof length score of cres inductively from themaximum proof length of its parent clauses as",
  "Restricting Theory Resolution with Embeddings:The knowledge base may contain various axioms": "and facts, many of which are irrelevant to the ac-tive clause. To enhance efficiency and maintain thegrowth of the resolution space tractable, we restrictthe size of our resolution search space by a branch-ing factor b and select candidate target clauses forperforming resolution based on their semantic rele-vance to the current active clause. Concretely, weuse the similarity scores between zc, the word em-bedding vector of the active clause c, and zc, wordembedding vectors of each candidate clause c tofind c, the set of b most relevant clauses to c as",
  "c = {c|(c K)(c = c)(zTc zc )}, (8)": "in which is set to the bth highest inner productscore between embeddings of c and other clauses,thus resulting in top-b theory resolution candidates.Next, theory resolution can be performed betweenc and each clause in c as in Equation 2.These two mechanisms together enable an effi-cient inference via LLM-based theory resolution.At the beginning of each iteration of LLM-TRes,the clause holding the foremost position in PQbecomes the active clause. Once a resolution stepleads to contradiction, the proof and its respec-tive priority score are added to the set of foundproofs by backtracking the ancestor clauses up tothe negated query.This algorithm continues until either a certainnumber of proofs are found or the maximum num-ber of iterations is exceeded. Notably, LLM-TResis not limited to proving a single query; instead, itfinds a set of proofs with each assigned a strengthscore. This functionality allows it to assess thelikelihood of each query being entailed, which isbeneficial for applications requiring ranking, suchas answering multiple-choice questions. In appli-cations where a binary truth value is consideredfor the query, the proof scores of q and q arecompared. Our experiments cover both cases.",
  "Repairability of Erroneous Resolution": "Since LLM-TRes provides access to atomic infer-ence steps in the resolution process, it facilitatesverifiability and debuggability. Although the entail-ment probabilities assigned by the LLMs may beerroneous, the exact resolution step at which thefailure occurs is discernible. Furthermore, it canbe easily corrected by introducing a rectifying ruleinto the knowledge base.An example of such a case is presented in Figure",
  "Tasks and Datasets": "Preference Reasoning Understanding user prefer-ences from natural language statements is a com-plex but essential task in applications such as rec-ommendation (Austin et al., 2024; Toroghi et al.,2023). For this task, we use Recipe-MPR (Zhanget al., 2023a), a dataset consisting of 500 userqueries stating their preference toward recipes, e.g.,I would like meat lasagna but Im watching myweight with five-way recipe options. Deductive Reasoning We use ProntoQA (Saparovand He, 2022), a widely used dataset for evaluat-ing the deductive reasoning ability of LLMs. Thisdataset consists of natural language queries aboutKBs including facts and axioms generated from on-tologies. We use 500 queries of the true ontology asthey are consistent with the real world and are use-ful to evaluate commonsense reasoning. We selectthe most challenging 5-hop subset of the dataset. Causal Commonsense Reasoning We use COPA-SSE (Brassard et al., 2022), a dataset for reason-ing about event causes and effects using a semi-structured KB. In the effect split of this dataset,an event is provided such as The pen ran out ofink., together with semi-structured explanationswith assigned quality scores, and the task is to de-termine the more plausible candidate effect, e.g.,I used a pencil. or I signed my name.. : Reasoning performance of methods across the three datasets. Gemma fails to provide explanations forRecipe-MPR, so reasoning scores cannot be calculated for it (Fail). LAMBADA requires a rule set that is notprovided in Recipe-MPR, and cannot rank proofs which is necessary for COPA-SSE. Pure entailment does notgenerate proofs, so the reasoning scores do not apply to it (NA).",
  "Baselines and Evaluation": "We use LAMBADA2, a seminal work in formalreasoning with LLMs, and zero-shot Chain-of-Thought (CoT) prompting (Kojima et al., 2022) asour comparison baselines. Semantic parsing meth-ods are inherently unable to perform commonsensereasoning and do not apply to our tasks. We useGPT-3.5 Turbo as the LLM for LAMBADA and forconverting the natural language axioms and queriesto the clausal form in our method, and obtain theentailment probabilities for theory resolution usingBART large (Lewis et al., 2020) model3 trainedon MNLI (Williams et al., 2018) dataset. We com-pare against CoT prompting with GPT-3.5 Turbo,Llama3 8B, Mistral 7B, and Gemma 7B. To en-sure that the difference in the performance of ourmodel and the baselines is not due to using differ-ent LLMs, we also use pure BART-large entailmentscores between facts and query as a baseline.We evaluate the reasoning performance of themodels considering the correctness of the final an-swers, measured by the accuracy, and correctnessof the reasoning process measured by the reasoningscore (RS) which we manually assess for the first20 queries the models answer correctly. RS is com-monly evaluated as a binary judgment on whetherthe predicted proof is supported by the ground truthproof (Kazemi et al., 2023; Lee and Hwang, 2024).However, RS does not assess the number of er-rors. Therefore, in addition to this metric whichwe call macro RS, to provide a more fine-grainedevaluation of the provided proofs, based on the ideaprovided in Min et al. (2023), we use a new metricwhich we name micro RS. Given a provided proof",
  "Results": "RQ1: Comparison of Reasoning PerformanceResults of the reasoning performance are providedin . On deductive and causal commonsensereasoning tasks, LLM-TRes achieves higher ac-curacies than the baselines although the languagemodels they use are multiple times larger. On pref-erence reasoning, LLM-TRes achieves the second-highest accuracy after CoT prompting with GPT3.5Turbo with a rather small margin. On ProntoQA,since the high-quality conversion of the query andthe knowledge base to the clausal format is straight-forward, LLM-TRes can prioritize complementaryliterals to perform exact resolution, resulting ina near-ideal performance. The failure cases ofLLM-TRes are due to the LLMs limitation inunderstanding contraposition as noted in previouswork (Zhang et al., 2024). Nonetheless, LLM-TResmaintains consistently high performance, unlikeother baselines which vary across tasks. For in-stance, while CoT with GPT-3.5 excels on Recipe-MPR and COPA-SSE, it is outperformed by Mistralon ProntoQA, which in turn performs rather poorlyon Recipe-MPR and COPA-SSE.On Reasoning Score, LLM-TRes outperformsall baselines across the three datasets at both themacro and micro level, showcasing its capability toprovide proofs that are consistent with the groundtruth proof. LAMBADA is unable to reason onRecipe-MPR as it performs backward chaining on Number of Masked Rules 0.40 0.60 0.80 1.00",
  ": Reasoning performance of different models on ProntoQA with larger KB. We sample random axiomsfrom other queries to increase the size of KB": "explicit rule sets, which Recipe-MPR does not pro-vide. Also, since LAMBADA can only prove orrefute a query based on a KB and cannot score andrank the plausibility of proofs, it cannot choose themore plausible effects on the COPA-SSE dataset.Since CoT using Gemma refrained from providingany proof for preference reasoning despite beingprompted to do so, the reasoning score could notbe calculated for it. Finally, pure entailment doesnot provide proofs so RS cannot be evaluated. RQ2: Robustness to Incompleteness of the KBAssuming access to a complete KB in which allrequired axioms are provided is often unrealisticin practical applications. Therefore, a common-sense reasoning methodology must be able to ex-tract the intrinsic commonsense knowledge of theLLMs to overcome the incompleteness of the KB.We assess this capability by repeatedly runningexperiments on ProntoQA, each time removing anumber of randomly chosen rules from the KB.We chose ProntoQA for this study as it is the onlydataset with large rule sets that enables experimentswith various ablated rules. Results of this experi-ment are provided in . Although ablatingrules from the KB decreases the accuracies of bothLLM-TRes and the best baseline, CoT with Mis-",
  "tral, LLM-TRes often maintains higher accuracy.Moreover, the consistently higher reasoning scoreof LLM-TRes proves its superior ability to generatevalid proofs": "RQ3: Robustness to Increase in Size of the KBIn this experiment, we evaluate the robustness ofLLM-TRes and other baselines to increases in theKB size. We form a large KB consisting of 75distinct rules across different ProntoQA queriesand each time add a fraction of this KB to theoriginal rule set of the query while randomly mask-ing 2 rules of the original KB. This experimentmainly aims to determine if the restricting resolu-tion search space of LLM-TRes using semantic sim-ilarity can identify the relevant clauses to the activeclause. In all tests, LLM-TRes uses the similaritybetween GPT-3 embeddings of the clauses with abranching factor of 15. Meanwhile, other baselinesinclude the entire KB in the prompt which is costlyand inefficient. Results of this test, shown in Fig-ure 3, depict that LLM-TRes and the best baseline,CoT with Mistral, sustain their performance, butLLM-TRes consistently obtains higher reasoningscores while using a more efficient methodologyfor pruning the reasoning search space.",
  "Conclusion": "We presented LLM-TRes, a novel framework forformal reasoning with LLMs based on theory reso-lution that allowed us to integrate LLMs into reso-lution logical reasoning seamlessly. By providingaccess to every atomic reasoning step, LLM-TResenabled verifiability and debuggability of the pro-cess. It also offered a reliable repairing mecha-nism for correcting flaws in the LLM reasoning byasserting the particular missed axiom which wastheoretically guaranteed to override the mistakenlylow-probability resolution step. The promising per-formance of LLM-TRes on preference reasoning,deductive reasoning, and causal commonsense rea-soning tasks demonstrates its efficacy in providingaccurate answers and correct proofs. These capabil-ities make LLM-TRes a robust foundation for coun-teracting hallucination and pave the way for moretrustworthy deployment of LLM-based common-sense reasoners in applications where correctness,verifiability, and repairability are paramount.",
  "Limitations": "While we believe this work has made substantialprogress in verifiable, debuggable, and repairablecommonsense reasoning, it naturally has limita-tions that we hope will encourage further inves-tigation and future work. As mentioned in thepaper, we provided a reliable mechanism for er-roneous reasoning processes; however, determin-ing a flawed step requires expert judgment. In ourwork, we do not focus on evaluating the reasoningsteps and how the repair axioms are introduced.Proposing an automated mechanism for evaluat-ing the reasoning steps can be a direction of futureresearch. Furthermore, as in all LLM-based reason-ing methodologies, obtaining high reasoning per-formances requires an apt LLM. As we discussedin .3, limitations of the utilized LLM suchas their shortcomings in understanding contraposi-tion can pose challenges to the overall performanceof the method. Finally, as we mentioned in thepaper, LLM-TRes focuses on the natural languageextension of First Order Logic (FOL) which weintroduced, and extending it to Higher-Order Logic(HOL) could be considered as a future researchdirection given the prior uses of HOL in formaliz-ing natural language semantics and complex modalconstructs (van Eijck and Unger, 2010).",
  "Ethics Statement": "Our contribution of LLM-TRes aims to enabletransparent reasoning with LLMs such that the cor-rectness of every reasoning step can be verified andpotentially repaired if incorrect. However, it is im-portant for us to note that a correct proof or line ofargument from premises neither presupposes thatthe premises are ethical nor that the conclusion de-rived from the premises and line of reasoning isethical. In this sense, practical use of LLM-TResstill requires ethical oversight to monitor ethicaland bias considerations for any axioms entered bythe user as well as to verify that unintended reason-ing hallucinations by the underlying LLM have notled to incorrect, biased, or unethical conclusions.",
  "This work was supported by LG Electronics,Toronto AI Lab Grant Ref No. 2024-0565": "Cem Anil, Yuhuai Wu, Anders Andreassen, AitorLewkowycz, Vedant Misra, Vinay Ramasesh, Am-brose Slone, Guy Gur-Ari, Ethan Dyer, and BehnamNeyshabur. 2022. Exploring length generalization inlarge language models. Advances in Neural Informa-tion Processing Systems, 35:3854638556. David Eric Austin, Anton Korikov, Armin Toroghi, andScott Sanner. 2024. Bayesian optimization with llm-based acquisition functions for natural language pref-erence elicitation. arXiv preprint arXiv:2405.00981. Peter Baumgartner. 1992. An ordered theory resolu-tion calculus. In Logic Programming and AutomatedReasoning: International Conference LPAR92 St.Petersburg, Russia, July 1520, 1992 Proceedings 3,pages 119130. Springer. Ning Bian, Xianpei Han, Le Sun, Hongyu Lin, YaojieLu, Ben He, Shanshan Jiang, and Bin Dong. 2023.Chatgpt is a knowledgeable but inexperienced solver:An investigation of commonsense problem in largelanguage models. arXiv preprint arXiv:2303.16421. Ana Brassard, Benjamin Heinzerling, Pride Kavumba,and Kentaro Inui. 2022. Copa-sse: Semi-structuredexplanations for commonsense reasoning. In Pro-ceedings of the Thirteenth Language Resources andEvaluation Conference, pages 39944000.",
  "Jie Huang and Kevin Chen-Chuan Chang. 2023. To-wards reasoning in large language models: A survey.In Findings of the Association for ComputationalLinguistics: ACL 2023, pages 10491065": "Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, DanSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, AndreaMadotto, and Pascale Fung. 2023. Survey of halluci-nation in natural language generation. ACM Comput-ing Surveys, 55(12):138. Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu,and Deepak Ramachandran. 2023. Lambada: Back-ward chaining for automated reasoning in naturallanguage. In Proceedings of the 61st Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 65476568. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-guage models are zero-shot reasoners. Advances inneural information processing systems, 35:2219922213.",
  "Jinu Lee and Wonseok Hwang. 2024. Symba: Symbolicbackward chaining for multi-step natural languagereasoning. arXiv preprint arXiv:2402.12806": "Mike Lewis, Yinhan Liu, Naman Goyal, MarjanGhazvininejad, Abdelrahman Mohamed, Omer Levy,Veselin Stoyanov, and Luke Zettlemoyer. 2020. Bart:Denoising sequence-to-sequence pre-training for nat-ural language generation, translation, and comprehen-sion. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics, pages78717880. Percy Liang, Rishi Bommasani, Tony Lee, DimitrisTsipras, Dilara Soylu, Michihiro Yasunaga, YianZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-mar, et al. 2022. Holistic evaluation of languagemodels. arXiv preprint arXiv:2211.09110. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,Daniel Khashabi, and Hannaneh Hajishirzi. 2023.When not to trust language models: Investigatingeffectiveness of parametric and non-parametric mem-ories. In Proceedings of the 61st Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 98029822. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-moyer, and Hannaneh Hajishirzi. 2023. Factscore:Fine-grained atomic evaluation of factual precisionin long form text generation. In Proceedings of the2023 Conference on Empirical Methods in NaturalLanguage Processing, pages 1207612100. Theo Olausson, Alex Gu, Ben Lipkin, Cedegao Zhang,Armando Solar-Lezama, Joshua Tenenbaum, andRoger Levy. 2023. Linc: A neurosymbolic approachfor logical reasoning by combining language modelswith first-order logic provers. In Proceedings of the2023 Conference on Empirical Methods in NaturalLanguage Processing, pages 51535176. Liangming Pan, Alon Albalak, Xinyi Wang, andWilliam Wang. 2023. Logic-lm: Empowering largelanguage models with symbolic solvers for faithfullogical reasoning. In Findings of the Associationfor Computational Linguistics: EMNLP 2023, pages38063824. Scott Sanner, Krisztian Balog, Filip Radlinski, BenWedin, and Lucas Dixon. 2023. Large language mod-els are competitive near cold-start recommenders forlanguage-and item-based preferences. In Proceed-ings of the 17th ACM conference on recommendersystems, pages 890896.",
  "Mark E Stickel. 1985. Automated deduction by the-ory resolution. Journal of Automated Reasoning,1(4):333355": "Armin Toroghi, Griffin Floto, Zhenwei Tang, and ScottSanner. 2023. Bayesian knowledge-driven critiquingwith indirect evidence. In Proceedings of the 46th In-ternational ACM SIGIR Conference on Research andDevelopment in Information Retrieval, SIGIR 23,page 18381842, New York, NY, USA. Associationfor Computing Machinery. Armin Toroghi, Willis Guo, Mohammad Mahdi Abdol-lah Pour, and Scott Sanner. 2024. Right for right rea-sons: Large language models for verifiable common-sense knowledge graph question answering. arXivpreprint arXiv:2403.01390.",
  "Jan van Eijck and Christina Unger. 2010.Compu-tational Semantics with Functional Programming.Cambridge University Press": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,Ed H Chi, Sharan Narang, Aakanksha Chowdhery,and Denny Zhou. 2022. Self-consistency improveschain of thought reasoning in language models. InThe Eleventh International Conference on LearningRepresentations. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in neuralinformation processing systems, 35:2482424837. Adina Williams, Nikita Nangia, and Samuel Bowman.2018. A broad-coverage challenge corpus for sen-tence understanding through inference. In Proceed-ings of the 2018 Conference of the North AmericanChapter of the Association for Computational Lin-guistics: Human Language Technologies, Volume 1(Long Papers), pages 11121122. Yudong Xu, Wenhao Li, Pashootan Vaezipoor, ScottSanner, and Elias B Khalil. 2023.Llms and theabstraction and reasoning corpus: Successes, failures,and the importance of object-based representations.arXiv preprint arXiv:2305.18354. Haochen Zhang, Anton Korikov, Parsa Farinneya, Mo-hammad Mahdi Abdollah Pour, Manasa Bharadwaj,Ali Pesaranghader, Xi Yu Huang, Yi Xin Lok, ZhaoqiWang, Nathan Jones, et al. 2023a. Recipe-mpr: Atest collection for evaluating multi-aspect preference-based natural language retrieval. In Proceedings ofthe 46th International ACM SIGIR Conference onResearch and Development in Information Retrieval,pages 27442753. Yanfang Zhang, Yiliu Sun, Yibing Zhan, Dapeng Tao,Dacheng Tao, and Chen Gong. 2024.Large lan-guage models as an indirect reasoner: Contrapositiveand contradiction for automated reasoning. arXivpreprint arXiv:2402.03667. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,Yulong Chen, et al. 2023b. Sirens song in the aiocean: a survey on hallucination in large languagemodels. arXiv preprint arXiv:2309.01219.",
  "Proposition 2. Consider proof P c using axiom that derives clause c. For any incorrect LLM rea-soning axiom , a Repair Axiom can be insertedsuch that P cwill be produced before P c": "Proof. A proof P c= Pc {} can be viewedas the combined set of clauses Pc and thatderive clause c. We can define the proof scoree(P c ) of clause c by inductively unrolling Equa-tion 5 for e(c) over all ancestor clauses P c thatderive it.This yields a simple product form:e(P c ) = entail cPc entailc. Now, compar-",
  "LAMBADA": "We apply these models to three distinct tasks,offering a comparative analysis of their responses.Detailed anecdotal examples are provided to elu-cidate the models capabilities and limitations. Ineach anecdotal example, we present the query, therule set (on datasets that contain it), and the groundtruth proof, followed by the response that eachmodel provides. We also indicate correct proof steps with green color and highlight incorrect onesin red.An example of the Deductive reasoning task, asevaluated on the ProntoQA dataset, is detailed inSection B.1. Additionally, we explore the modelsperformance on causal commonsense reasoningusing the COPA-SSE dataset and on preferencereasoning using the Recipe-MPR dataset, with ex-amples discussed in Sections B.2 and B.3, respec-tively.",
  "B.1Deductive Reasoning": "Query: is x imaginary?Rule set: Real numbers are numbers. Every real number is not imaginary. Each integer is a real number.Natural numbers are integers. Each natural number is not negative. Every prime number is a naturalnumber. Each prime number is prime. Mersenne primes are prime numbers. Every Mersenne prime isnot composite. Complex numbers are imaginary. x is a Mersenne prime.Ground Truth Proof: x is a Mersenne prime. Mersenne primes are prime numbers. Every prime numberis a natural number. Natural numbers are integers. Each integer is a real number. Every real number isnot imaginary. Answer: False.LLM-TRes:",
  "Since equery > equery, the answer is False": "CoT with Mistral: 1. Rule 1: Every real number is not imaginary.2. Rule 11: Complex numbers are imaginary.3. Given:x is a Mersenne prime.4. Rule 1: Every Mersenne prime is a real number (since it is a prime number and a prime number is areal number).5. Since x is a Mersenne prime and a real number, it is not imaginary by definition.Therefore, the answer to the query is False.CoT with Llama3: From the Knowledge Base, we know that x is a Mersenne prime (Rule 9).From Rule 8, we know that Every Mersenne prime is not composite.From Rule 7, we know that Each prime number is prime.From Rule 6, we know that Every prime number is a natural number.From Rule 5, we know that Each natural number is not negative.From Rule 4, we know that Natural numbers are integers.From Rule 3, we know that Each integer is a real number.From Rule 2, we know that Every real number is not imaginary.From Rule 1, we know that Real numbers are numbers.Since x is a Mersenne prime, and Mersenne primes are not composite, and prime numbers are naturalnumbers, and natural numbers are integers, and integers are real numbers, and real numbers are notimaginary, we can conclude that x is not imaginary. Therefore, the answer to the query is False.CoT with Gemma: I used the following rules from the Knowledge Base to answer the query:Real numbers are numbersEvery real number is not imaginaryMersenne primes are prime numbersEvery Mersenne prime is not compositeAnswer: Therefore, the answer to the query is False.CoT with GPT3.5: To answer this query, we will use the following rules from the Knowledge Base:1. Each prime number is prime. 2. Each prime number is not composite. 3. x is a Mersenne prime. 4.Every Mersenne prime is not composite. 5. Complex numbers are imaginary.Proof: From rule 3 and rule 4, we can conclude that x is not a composite number. Since x is a Mersenneprime, and every Mersenne prime is not composite (rule 4), we can infer that x is a prime number. Fromrule 1, Each prime number is prime. Finally, from rule 5, Complex numbers are imaginary, we canconclude that if x is a prime number and all prime numbers are prime, then x is not imaginary.Therefore, the answer to the query is False.LAMBADA: Begin proof from x is imaginary.Fact selection: No fact proves this rule. Goal decomposition by selecting the next rule: Complex numbers are imaginary.Goal decomposed to: x is a complex number.Fact selection: found fact: Every Mersenne prime is not composite.Fact verification failed. (the fact does not prove the subgoal.)Goal decomposition by selecting the next rule: Complex numbers are imaginary.Goal decomposed to: x is an imaginary number.Fact selection: found fact: x is a Mersenne prime.Fact verification failed. (the fact does not prove the subgoal.)Goal decomposition by selecting the next rule: Complex numbers are imaginary.Goal decomposition failed.Since the query cannot be proven, it is False.",
  "B.2Causal Commonsense Reasoning": "Query: The crowd intensified.Options: 1- The father handed his son some money., 2- The father grabbed his sons hand.Answer: The father grabbed his sons hand.Rules: 1- (an intense crowd, capable of, making your son hard to find), 2- (making your son hard tofind, causes desire, keep an eye on son), 3- (keeping an eye on son, results in, grabbing sons hand), 4-(handing money to son, causes, son having money), 5- (father, capable of, handing money to son)Ground Truth Proof: 1, 2, 3.LLM-TRes:",
  "Since eoption2 > eoption1, the answer is Option2": "CoT with Mistral: Based on the given knowledge base, the consequence The father grabbed his sonshand is the more probable one.Proof: According to the knowledge base, an intense crowd can make your son hard to find. This situationwould likely cause a desire in the father to keep a close eye on his son (rule: [making your son hard tofind, causes desire, keep an eye on son]). The rule [keeping an eye on son, results in, grabbingsons hand] states that keeping an eye on his son results in the father grabbing his sons hand. Therefore,the father grabbing his sons hand is a more probable consequence given the knowledge base.Additionally, the rule [father, capable of, handing money to son] states that a father is capableof handing money to his son. However, there is no rule directly linking an intense crowd to the fatherhanding money to his son. Therefore, the probability of the father handing his son some money is lowerthan the father grabbing his sons hand.CoT with Llama3: Consequence: The father handed his son some money.Proof: The rules used from the Knowledge Base are:1. [father, CapableOf, handing money to son] 2. [handing money to son, Causes,son havingmoney]From the event The crowd intensified, we can infer that the crowd was intense, which is related to thefirst rule. This rule states that a father is capable of handing money to his son. The second rule states thathanding money to son causes son having money. Therefore, we can conclude that the father handed hisson some money.CoT with Gemma: The father handed his son some money.(Gemma did not provide any proof for this query.)CoT with GPT3.5: The father handed his son some money.Proof: 1. handing money to son, causes, son having money (KB)2. father, capable of, handing money to son (KB)3. Therefore, the father handing money to his son can be inferred from the KB.",
  "B.3Preference Reasoning": "Query: Id like a seafood recipe with a kick to it.Options: 1- Garlic shrimp., 2- Cajun catfish stew.Answer: 2- Cajun catfish stew.LLM-TRes: Please refer to .CoT with Mistral: The query asks for a seafood recipe with a kick to it. Option 1, Garlic shrimp, is aclassic seafood recipe that is known for its strong garlic flavor, but it doesnt necessarily have a \"kick\" toit. Option 2, Cajun catfish stew, on the other hand, is a spicy seafood dish that is known for its bold andfiery flavors, which would provide the desired kick.Therefore, the selected option is: 2. Cajun catfish stew.CoT with Llama3: I think the most suitable option for a seafood recipe with a kick is the Cajun catfishstew because it combines the bold flavors of Cajun cuisine with the richness of seafood, which is likelyto provide a spicy and savory experience.Therefore, the selected option is: 2- Cajun catfish stew.CoT with Gemma: The query calls for a seafood recipe with a kick. Garlic shrimp is a popular seafoodrecipe that is known for its bold flavor and spicy kick.Therefore, the selected option is: 1- Garlic shrimp.CoT with GPT3.5: The query asks for a seafood recipe with a kick, which implies a dish with bold andspicy flavors. While garlic shrimp is delicious, Cajun cuisine is known for its spicy and flavorful dishes,making Cajun catfish stew a more suitable option for someone looking for a seafood recipe with a kick.,the selected option is: 2. Cajun catfish stew.",
  "Prompt for Deductive Reasoning with Baselines": "Task: You will be given a query about anobject x. You are a helpful and smartassistant trying to answer this query.In order to do this, a fact about xand a set of rules are provided to youin a Knowledge Base. Using theserules, you must both provide an answerto the query (the answer has to be\"True\" or \"False\") and give a proof ofyour answer by using the rules fromKnowledge Base. Think step-by-step andtry to use the rules one-by-one toanswer the query. Begin your responseby providing the proof and stating therules you used from the knowledge baseto give the answer. Then, give yourfinal answer to the query by sayingeither \"Therefore, the answer to thequery is True\" or \"Therefore, theanswer to the query is False\" and notsaying anything else.Query: {{ QUERY }}KB: {{ KB }}",
  "Prompt for Causal Commonsense Reasoningwith Baseline LLMs": "Task: You will be given a sentenceabout an event. Also, a number ofrules in the form of a KnowledgeBase are presented to you. Forthis event, two possibleconsequences are given. You needto determine which of theseconsequences can be inferred fromthe event and the rules in theKnowledge Base. You must provide aproof for your answer by using therules from the Knowledge Base.First, copy the consequence youthink can be inferred. Then, inthe next line, provide your proofby stating the rules you used fromthe Knowledge Base. Let's thinkstep by step.",
  "Prompt for Conversion of Natural Language KBto Clausal Form": "Task: you are a First-Order Logic expert.A sentence written in NaturalLanguage will be presented to you.Convert that sentence to First-OrderLogic. In this conversion, followthese syntactic rules:1- Instead of universal quantifier, writeFOR_ALL.2- Write all predicates for the variable(x) , even if the sentence refers toa specific object. For example, \"127is an integer\" must be converted to\"integer(x)\" or \"Bob is a cat\" mustbe converted to \"cat(x)\".3- If the predicate name has multipleparts, use _ instead ofin thename.4- Instead of the implication symbol, use=> .5- Use~as the symbol of negation.6- Only use lowercase letters forpredicate names.7- Even if the sentence is incorrect inyour opinion, convert it to FOLgiven the stated rules without anyfurther explanation.8- If the sentence is not in the formatof a universal statement, just stateit as a predicate. For example, \"Bobis a cat\" must be converted to\"cat(x)\".[few-shot examples]"
}