{
  "Abstract": "End-to-end Task-Oriented Dialog (TOD) sys-tems typically require extensive trainingdatasets to perform well. In contrast, largelanguage model (LLM) based TOD systemscan excel even with limited data due to theirability to learn tasks through in-context exem-plars. However, these models lack alignmentwith the style of responses in training data andoften generate comprehensive responses, mak-ing it difficult for users to grasp the informationquickly. In response, we propose SyncTOD thatsynergizes LLMs with task-specific hints toimprove alignment in low-data settings. Sync-TOD employs small auxiliary models to pro-vide hints and select exemplars for in-contextprompts. With ChatGPT, SyncTOD achievessuperior performance compared to LLM-basedbaselines and SoTA models in low-data set-tings, while retaining competitive performancein full-data settings.",
  "Introduction": "The rise of large-language models (LLMs) hasprogressed the field of NLP by leaps and bounds(Google, 2023; Touvron et al., 2023). Pre-trainedover massive data, LLMs work remarkably wellwith just in-context learning for many NLP taskslike natural language inference, summarization,and dialogs (Kavumba et al., 2023; Hu et al., 2022;Zheng et al., 2023).One specific domain within dialogs where LLMsshow promise is in building Task-Oriented Di-alogs (TOD) systems, where they generate agentresponses based on the dialog history and task-specific knowledge. TOD systems, in general, canbe divided into two types: modular (Young et al.,2013) and end-to-end (Madotto et al., 2018). Modu-lar systems require domain experts to define dialogstates and annotate each train dialog with state an-notations. Unlike modular, end-to-end systems do",
  ": GPT-4 lists many potential options and extra-neous details instead of seeking user input and lacksalignment with the gold": "not require such expensive state annotations. Inthis work, we focus on end-to-end TOD systems.Existing end-to-end task-oriented dialogue(TOD) systems perform well when a reasonablenumber of training dialogues are available. How-ever, in many practical scenarios, only a limitednumber of expert-curated training dialogues areavailable. showcases the performanceof MAKER, a state-of-the-art (SoTA) end-to-endTOD model, on varying training data scales. Whentraining data is limited, existing TOD approachesfail to learn the underlying task effectively, leadingto a performance drop.In contrast, large language models (LLMs) within-context learning perform better than supervisedmodels when the training dataset is limited. Theinherent reasoning capabilities of LLMs help themlearn the associated task with just a few examples. Unfortunately, LLM-based TOD systems do notalign well with the language and style in the train-ing dialogs, often generating overly comprehensiveresponses. This alignment is crucial, particularlyin scenarios like in-car voice assistants, where re-sponses must be concise and easily consumablewithout causing distraction.As an illustrative example, see the responses gen-erated by various models in . We see thatGPT-4 is good at reasoning but lacks alignmentin presenting information. When the gold seeksadditional user input when posed with excessiveoptions, GPT-4 tends to be overly comprehensive,listing many potential options and extraneous de-tails. This verbosity, while informative, can hinderusers from easily grasping the information. On theother hand, MAKER, a SoTA supervised approach,is well aligned with agent utterances in training butmakes many mistakes in reasoning. Contributions: We propose Synergizing in-contextlearning with hints for TOD (SyncTOD), that alignsLLMs with the stylings of the available trainingdata. In particular, it trains auxiliary models toprovide LLMs (accessed via an API) with hints(such as expected entity types in the response andresponse length) on how to phrase the response;selecting exemplars conditioned on these hints fur-ther improves the alignment of the responses. Onthree publicly available datasets, SyncTOD con-sistently outperforms both vanilla prompting andSoTA supervised models in low-data settings whilemaintaining competitive performance compared tosupervised models in full-data settings. Our codeis available at",
  "Related Work": "Conventional TOD systems follow a modular de-sign (Young et al., 2013; Rojas-Barahona et al.,2016; Hosseini-Asl et al., 2020; Qin et al., 2023)and require annotations for DST, PL and NLG. Thiswork, however, focuses on end-to-end TOD sys-tems (Eric et al., 2017; Madotto et al., 2018; Raghuet al., 2019; Wu et al.; Qin et al., 2023) that allevi-ate the need for annotations by directly predictingthe response given dialog history and knowledgebase (KB).Though LLMs have been explored for TODtasks (Hu et al., 2022; Hudecek and Duek, 2023;Bang et al., 2023; Li et al., 2024), to the best of ourknowledge, we are the first to explore them in an end-to-end setting. Directional Stimulus Prompting(DSP), an approach closer to ours, uses keywordsand dialog acts as hints for summarization andresponse generation tasks, respectively (Li et al.,2024). However, unlike DSP, SyncTOD uses mul-tiple hints entity types, response length, and di-alog closure relevant to the TOD task. Further,SyncTOD also uses these hints to improve the in-context exemplars quality using a retrieve-rerankapproach.A natural approach for combining training datawith in-context learning is via retrieval-augmentedgeneration (RAG) (Lewis et al., 2020; Guu et al.,2020). Here, a retriever model infuses LLM in-put with exemplars from the training that are sim-ilar to the test sample (Lewis et al., 2020; Meadeet al., 2023; Shi et al., 2024; Ram et al., 2023). Al-though out-of-box retrievers work reasonably well(Ram et al., 2023), many recent works strive toimprove the retriever model further. (Zhang et al.,2018; Wang et al., 2024) employ reward-based andcontrastive learning to improve retrieval quality.Specifically, they use LLMs to obtain soft rewardsto fine-tune the retriever model. Recently, Patidaret al. (2024) fused multiple retriever models learnedfrom training data with LLMs for knowledge-basedquestion-answering tasks.What sets SyncTODapart from RAG is its use of hints not only forselecting the informative exemplars but also forsteering LLM generation from within the prompt.",
  "SyncTOD": "Let c = [u1, a1, u2, a2, ..., uj] be a user-agent di-alog history with u and a being user and agentutterances respectively. Let y = aj be the nextsystem response. The task of a TOD system is topredict the next system response y given the dialoghistory c and a knowledge base (KB) K associatedwith the users task. Let D = {(hi, Ki, yi)}ni=1denote the train dialogs.In the in-context learning setup, an LLM isqueried (via API) with an input prompt contain-ing task instructions, a few exemplars, and (c, K)to generate y. A popular technique for leveragingtrain dialogs in the in-context learning setup is re-trieval augmented generation (RAG) (Zhang et al.,2023; Guu et al., 2020). In RAG, the exemplarsthat are most similar to c are retrieved from D andare used for generating y.Our proposed approach, SyncTOD, synergizes in-context learning of LLMs with hints to better align : SyncTOD predicts useful hints H about the expected response. The hints improve exemplar quality viare-ranking and steer the LLM (accessed via API) toward the expected response from within the prompt. with agent utterances in the training data D. shows the overall architecture. SyncTOD has twomain components: hint predictors and exemplarselector. The hint predictors output a set of hints Hgiven the dialog history c. These hints are domain-agnostic clues, such as the entity types that shouldbe included in the response and the length of theresponse, that can guide the generation to followthe same style as the train dialogs. The second com-ponent, exemplar selector, first retrieves relevantexemplars from D based on c, and then re-ranksthe retrieved exemplars based on H. Both thesecomponents are aimed at aligning the languageand style of LLM responses to agent responses inthe train dialogs D. As the gold responses y areavailable for the exemplars, we simply infer thecorresponding hints from y and add the hints tothe exemplars. The predictors are only used to in-fer hints for the given input dialog with history c.Please refer to appendix H for the exact prompt.",
  "SyncTOD uses three types of hints: entity types (inresponse), response length, and dialog closure": "Entity Types (ET): Entities are the information-rich elements in the agents response. For example,the hotel name \"Lovell Lodge\" is the crucial ele-ment in the agent response How does the LovellLodge sound?\". We posit that for a given dialogcontext and KB, the set of entity types in the agentresponse (e.g., {hotel name}) captures the crux ofthe response. Hence using expected entity types inthe response as hints would align the LLM genera-tion to D.Specifically, for given (c, K), SyncTOD predictsa list of entity types et present in the expected sys-tem response. Then, SyncTOD amends the promptwith the rule The response must only include en-",
  "tities of type: et. To predict et, SyncTOD learnsan ET predictor model P(et|c, K) on the dataset{(ci, Ki, eti)}ni=1, where gold etis are the types ofentities in gold response": "Dialog Closure (DC): The style of the dialog clo-sures varies depending on the task at hand, andeach dataset has a different way of closing the dia-log. But ChatGPT generates similar, verbose andopen-ended responses to the users closing salu-tations. To alleviate this, SyncTOD uses dialogclosure prediction dc for a given dialog (c, K) asa hint to steer LLM towards a successful closureof the dialog. Specifically, SyncTOD amends theinput prompt with a rule: The response must closethe dialog., when dc is true. For a training dialog(ci, Ki, yi), we define dc = True if and only if yiis the last utterance in the dialog. Response size (RS): For a (ci, Ki, yi) D, re-sponse size rs equals the number of words in theresponse yi.SyncTOD learns an RS predictorP(rs|c, K) on the dataset {(ci, Ki, rsi)}ni=1 andamends the input with rule: The response must bers words or shorter.For a test dialog (c, K), SyncTOD predicts thehints H = ( et, rs, dc) using ET, RS, and DC hintpredictors, respectively.",
  "Exemplar Selector": "Retrieval: SyncTOD has a retrieve-rerank mecha-nism to select in-context exemplars (Nogueira andCho, 2019). Following Liu et al. (2021), SyncTODselects points from D semantically closer to thegiven test dialog (c, K). Specifically, it encodesthe dialog history c using a pre-trained encoder andperforms a maximum inner-product search over Dto retrieve the top-k points. All our experimentsuse BAAI/bge-large-en-v1.5 encoder model (Xiao",
  "et al., 2023)": "Re-ranking: Intuitively, an example with the samedialog state as the input is an ideal choice for anexemplar. However, end-to-end TOD datasets donot include dialog state annotations. Instead, weposit that dialog history and hints are reasonableproxies for the dialog state. SyncTOD thus re-ranksthe retrieved datapoints based on hints.Let (ci, Ki, yi) be a retrieved datapoint and Hisbe its associated hints. SyncTOD computes similar-ity score between hints H and Hi as follows",
  "Experimental Setup": "Datasets For our evaluation, we use the Multi-WOZ2.1 (Budzianowski et al., 2018), StanfordMulti-domain (SMD) (Eric et al., 2017), andBiTOD (English) (Lin et al.) multi-domain datasets.Appendix A provides additional details about thedatasets. Baselines: We compare SyncTOD against the re-cent baselines - GLMP (Wu et al.), FG2Seq (Heet al., 2020a), CDNet (Raghu et al., 2021), Uni-fiedSKG (Xie et al., 2022), and MAKER (Wanet al., 2023). We also compare against RAG withBAAI/bge-large-en-v1.5 model for exemplar re-triever. Further, we report the performance of Chat-GPT (gpt-3.5-turbo) and GPT-4(gpt-4-0613) in astandard few-shot setting with fixed exemplars1.Training details for hint predictors and retrieval ofSyncTOD are in Appendix D. Evaluation Metric: For evaluating model perfor-mance, we use the Entity F1 (Wu et al.) and BLEU(Papineni et al., 2002) metrics prevalent in the end-to-end TOD paradigm (Wu et al.; He et al., 2020a;Raghu et al., 2021; Xie et al., 2022; Wan et al.,2023, inter alia).",
  ": Performance of SyncTOD and baselines onMultiWOZ, SMD and BiTOD datasets": "competitive Entity F1 scores, with SyncTOD (GPT-4) outperforming all the supervised baseline mod-els. Further, ChatGPT and GPT-4 enjoy consistentperformance gains when coupled with SyncTOD.Interestingly, RAG LLMs display a stronger En-tity F1 performance on SMD than other datasets.In SMD, users express preferences differently thanthe other two datasets. In MultiWOZ and BiTOD,users give detailed preferences for area, price, rat-ing, etc., and can change these during the conver-sation. In SMD, preferences are simpler, like thenearest parking, city weather, or meeting times.Thus, MultiWOZ and BiTOD present a more chal-lenging problem for LLMs than SMD.Unlike Entity F1, SyncTOD variants performpoorly in BLEU. Entity F1 measures whether thesystem response includes relevant entities from theKB and dialog history. Whereas BLEU computesn-gram precision between the system response andthe gold response. Notably, a system response thatincludes all relevant entities, can still receive a lowBLEU score due to differences in phrasing. Wefind that SyncTOD responses are meaningful andinclude relevant entities, resulting in good Entity F1scores. However, they use different phrasing andhave less lexical overlap with gold responses, lead-ing to lower BLEU scores. We verify the quality ofSyncTOD responses via human evaluations. Human Evaluations: We had two annotators eval-uate responses from Gold, MAKER, 2, and Sync-TOD (GPT-4) models. They assessed the responsesfor a) appropriateness to the dialog history and KB,b) fluency and c) consistency on a 1-5 Likert Scale(Likert, 1932). The results in demonstratethat SyncTOD surpasses MAKER in appropriate-ness and fluency across datasets, indicating higher",
  ": SyncTOD is better aligned with Gold thanRAG": "response quality. Consistency evaluation show-cases SyncTOD is truthfulness to the dialog historyand the KB. Appendix F details our evaluation pro-tocol.Low Data Setting: shows the evalua-tion with varying training data sizes. SyncTOD(ChatGPT) consistently enhances ChatGPT per-formance and outperforms MAKER with limiteddata. In MultiWOZ, SyncTOD (ChatGPT) leadsuntil MAKER catches up at around 1000 dialogs.In SMD, SyncTOD (ChatGPT) achieves EntityF1 similar to MAKER with less than 20 exam-ples, while MAKER needs 16x more data.InBiTOD, SyncTOD (ChatGPT) significantly sur-passes MAKER across training data scales.Alignment Study:SyncTOD aligns LLM re-sponses with the dataset style. We validate thisby comparing the average response length (AvgLen) and average entity count (Avg Ent) of goldand SyncTOD responses from the test set (). SyncTOD stats are closer to gold than RAG,indicating better alignment.Ablations: We perform ablations on SyncTOD(ChatGPT), with results in . Hints andexemplar retrieval are critical for SyncTODs per-formance across datasets.Dropping exemplarre-ranking significantly impacts MultiWOZ and",
  "Conclusion": "We propose SyncTOD that leverages LLMs for end-to-end TOD. Given a dialog history and KB, Sync-TOD obtains hints about the expected response us-ing auxiliary models. It then uses predicted hintsto retrieve quality exemplars and guide LLMs to-ward the desired response. With automatic/humanevaluation, we showed that SyncTOD outperformsthe SoTA baseline models.Further, SyncTODshowcases a strong performance in the low-datasetting. We release code for future research at",
  "Limitations": "It would be interesting to see how SyncTOD bene-fits from advanced prompting techniques like chain-of-thought and self-consistency. Further, SyncTODis only tested on English datasets, though the modelcan easily be extended to different languages by itsdesign. Additionally, SyncTOD performance canfurther be improved by designing much more so-phisticated hints. Finally, SyncTOD involves bothtraining the hint prediction modules and promptingan LLM, resulting in the cost of using LLMs andtraining the model.",
  "Ethics Statement": "In this work, we use OpenAIs ChatGPT and GPT-4 which are commercial LLMs whose training de-tails are not publicly available. Thus, it is unclearwhether these models have seen the datasets usedin this work during their training. In our experi-ments, we benchmark Zero-shot (ChatGPT) on allthe datasets and report the performance in table 2.As zero-shot (ChatGPT) performs poorly, we be-lieve that our datasets were not part of ChatGPTstraining set.",
  "Acknowledgements": "This work is supported by IBM AI Horizons Net-work grant, grants by Google, Verisk, and Mi-crosoft, an IBM SUR award and the Jai Guptachair fellowship by IIT Delhi. Vishal is supportedby a Google Fellowship. We thank the IIT DelhiHPC facility for its computational resources. Weare grateful to Microsoft AFMR for supporting thiswork. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-liang Dai, Dan Su, Bryan Wilie, Holy Lovenia, ZiweiJi, Tiezheng Yu, Willy Chung, et al. 2023. A multi-task, multilingual, multimodal evaluation of chatgpton reasoning, hallucination, and interactivity. In Pro-ceedings of the 13th International Joint Conferenceon Natural Language Processing and the 3rd Confer-ence of the Asia-Pacific Chapter of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 675718. Pawe Budzianowski, Tsung-Hsien Wen, Bo-HsiangTseng, Iigo Casanueva, Stefan Ultes, Osman Ra-madan, and Milica Gasic. 2018. Multiwoz - a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling. In Conference on Em-pirical Methods in Natural Language Processing. Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, et al.2024. Scaling instruction-finetuned language models.Journal of Machine Learning Research, 25(70):153. Mihail Eric, Lakshmi Krishnan, Francois Charette, andChristopher D Manning. 2017. Key-value retrievalnetworks for task-oriented dialogue. In Proceedingsof the 18th Annual SIGdial Meeting on Discourseand Dialogue, pages 3749.",
  "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-pat, and Mingwei Chang. 2020. Retrieval augmentedlanguage model pre-training. In International confer-ence on machine learning, pages 39293938. PMLR": "Pengcheng He, Jianfeng Gao, and Weizhu Chen. De-bertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding shar-ing. In The Eleventh International Conference onLearning Representations. Zhenhao He, Yuhong He, Qingyao Wu, and Jian Chen.2020a. Fg2seq: Effectively encoding knowledge forend-to-end task-oriented dialog. In ICASSP 2020-2020 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP), pages 80298033. IEEE.",
  "Zhenhao He, Jiachun Wang, and Jian Chen. 2020b.Task-oriented dialog generation with enhanced en-tity representation. In INTERSPEECH, pages 39053909": "Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu,Semih Yavuz, and Richard Socher. 2020. A simplelanguage model for task-oriented dialogue. Advancesin Neural Information Processing Systems, 33:2017920191. Yushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu,Noah A. Smith, and Mari Ostendorf. 2022.In-context learning for few-shot dialogue state track-ing. In Conference on Empirical Methods in NaturalLanguage Processing. Guanhuan Huang, Xiaojun Quan, and Qifan Wang.2022. Autoregressive entity generation for end-to-end task-oriented dialog. In Proceedings of the 29thInternational Conference on Computational Linguis-tics, pages 323332. Vojtech Hudecek and Ondrej Duek. 2023. Are largelanguage models all you need for task-oriented dia-logue? In Proceedings of the 24th Annual Meetingof the Special Interest Group on Discourse and Dia-logue, pages 216228. Pride Kavumba, Ana Brassard, Benjamin Heinzerling,and Kentaro Inui. 2023.Prompting for explana-tions improves adversarial nli. is this true?{Yes} itis {true} because {it weakens superficial cues}. InFindings of the Association for Computational Lin-guistics: EACL 2023, pages 21652180. Patrick Lewis, Ethan Perez, Aleksandra Piktus, FabioPetroni, Vladimir Karpukhin, Naman Goyal, Hein-rich Kttler, Mike Lewis, Wen-tau Yih, Tim Rock-tschel, et al. 2020. Retrieval-augmented generationfor knowledge-intensive nlp tasks. Advances in Neu-ral Information Processing Systems, 33:94599474. Zekun Li, Baolin Peng, Pengcheng He, Michel Galley,Jianfeng Gao, and Xifeng Yan. 2024. Guiding largelanguage models via directional stimulus prompting.Advances in Neural Information Processing Systems,36.",
  "Rensis Likert. 1932. A technique for the measurementof attitude scales": "Zhaojiang Lin, Andrea Madotto, Genta Indra Winata,Peng Xu, Feijun Jiang, Yuxiang Hu, Chen Shi, andPascale Fung.Bitod: A bilingual multi-domaindataset for task-oriented dialogue modeling.InThirty-fifth Conference on Neural Information Pro-cessing Systems Datasets and Benchmarks Track(Round 1). Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,Lawrence Carin, and Weizhu Chen. 2021.Whatmakes good in-context examples for gpt-3? In Work-shop on Knowledge Extraction and Integration forDeep Learning Architectures; Deep Learning InsideOut.",
  "Ilya Loshchilov and Frank Hutter. 2017. Decoupledweight decay regularization. In International Confer-ence on Learning Representations": "Andrea Madotto, Samuel Cahyawijaya, Genta IndraWinata, Yan Xu, Zihan Liu, Zhaojiang Lin, and Pas-cale Fung. 2020. Learning knowledge bases withparameters for task-oriented dialogue systems. InFindings of the Association for Computational Lin-guistics: EMNLP 2020, pages 23722394. Andrea Madotto, Chien-Sheng Wu, and Pascale Fung.2018. Mem2seq: Effectively incorporating knowl-edge bases into end-to-end task-oriented dialog sys-tems. In Proceedings of the 56th Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 14681478. Nicholas Meade, Spandana Gella, Devamanyu Hazarika,Prakhar Gupta, Di Jin, Siva Reddy, Yang Liu, andDilek Hakkani-Tur. 2023. Using in-context learningto improve dialogue safety. In Findings of the Associ-ation for Computational Linguistics: EMNLP 2023,pages 1188211910.",
  "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalu-ation of machine translation. In Annual Meeting ofthe Association for Computational Linguistics": "Mayur Patidar, Riya Sawhney, Avinash Singh, BiswajitChatterjee, Indrajit Bhattacharya, et al. 2024. Few-shot transfer learning for knowledge base questionanswering: Fusing supervised models with in-contextlearning. In Proceedings of the 62nd Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 91479165. Libo Qin, Yijia Liu, Wanxiang Che, Haoyang Wen,Yangming Li, and Ting Liu. 2019. Entity-consistentend-to-end task-oriented dialogue system with kb re-triever. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processingand the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), pages133142. Libo Qin, Wenbo Pan, Qiguang Chen, Lizi Liao, ZhouYu, Yue Zhang, Wanxiang Che, and Min Li. 2023.End-to-end task-oriented dialogue: A survey of tasks,methods, and future directions. In Proceedings of the2023 Conference on Empirical Methods in NaturalLanguage Processing, pages 59255941. Libo Qin, Xiao Xu, Wanxiang Che, Yue Zhang, andTing Liu. 2020. Dynamic fusion network for multi-domain end-to-end task-oriented dialog. In AnnualMeeting of the Association for Computational Lin-guistics. Dinesh Raghu, Nikhil Gupta, et al. 2019. Disentanglinglanguage and knowledge in task-oriented dialogs. InProceedings of the 2019 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,Volume 1 (Long and Short Papers), pages 12391255. Dinesh Raghu, Atishya Jain, Sachindra Joshi, et al. 2021.Constraint based knowledge base distillation in end-to-end task oriented dialogs. In Findings of the Asso-ciation for Computational Linguistics: ACL-IJCNLP2021, pages 50515061. Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,Amnon Shashua, Kevin Leyton-Brown, and YoavShoham. 2023. In-context retrieval-augmented lan-guage models. Transactions of the Association forComputational Linguistics, 11:13161331. Lina Maria Rojas-Barahona, Milica Gaic, Nikola Mrk-sic, Pei hao Su, Stefan Ultes, Tsung-Hsien Wen,Steve J. Young, and David Vandyke. 2016.Anetwork-based end-to-end trainable task-oriented di-alogue system. In Conference of the European Chap-ter of the Association for Computational Linguistics. Md Rashad Al Hasan Rony, Ricardo Usbeck, and JensLehmann. 2022.Dialokg: Knowledge-structureaware task-oriented dialogue generation. In Find-ings of the Association for Computational Linguis-tics: NAACL 2022, pages 25572571. Weijia Shi, Sewon Min, Michihiro Yasunaga, Min-joon Seo, Richard James, Mike Lewis, Luke Zettle-moyer, and Wen-tau Yih. 2024. Replug: Retrieval-augmented black-box language models. In Proceed-ings of the 2024 Conference of the North AmericanChapter of the Association for Computational Lin-guistics: Human Language Technologies (Volume 1:Long Papers), pages 83648377. Xin Tian, Yingzhan Lin, Mengfei Song, Siqi Bao, FanWang, H. He, Shuqi Sun, and Hua Wu. 2022. Q-tod:A query-driven task-oriented dialogue system. InConference on Empirical Methods in Natural Lan-guage Processing. Hugo Touvron, Louis Martin, Kevin R. Stone, PeterAlbert, Amjad Almahairi, Yasmine Babaei, Niko-lay Bashlykov, Soumya Batra, Prajjwal Bhargava,Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cris-tian Cantn Ferrer, Moya Chen, Guillem Cucurull,David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,Naman Goyal, Anthony S. Hartshorn, Saghar Hos-seini, Rui Hou, Hakan Inan, Marcin Kardas, ViktorKerkez, Madian Khabsa, Isabel M. Kloumann, A. V.Korenev, Punit Singh Koura, Marie-Anne Lachaux,Thibaut Lavril, Jenya Lee, Diana Liskovich, YinghaiLu, Yuning Mao, Xavier Martinet, Todor Mihaylov,Pushkar Mishra, Igor Molybog, Yixin Nie, AndrewPoulton, Jeremy Reizenstein, Rashi Rungta, KalyanSaladi, Alan Schelten, Ruan Silva, Eric MichaelSmith, R. Subramanian, Xia Tan, Binh Tang, RossTaylor, Adina Williams, Jian Xiang Kuan, PuxinXu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, An-gela Fan, Melanie Kambadur, Sharan Narang, Aure-lien Rodriguez, Robert Stojnic, Sergey Edunov, andThomas Scialom. 2023. Llama 2: Open foundationand fine-tuned chat models. ArXiv, abs/2307.09288. Fanqi Wan, Weizhou Shen, Ke Yang, Xiaojun Quan, andWei Bi. 2023. Multi-grained knowledge retrieval forend-to-end task-oriented dialog. In Annual Meetingof the Association for Computational Linguistics. Liang Wang, Nan Yang, and Furu Wei. 2024. Learningto retrieve in-context examples for large languagemodels. In Proceedings of the 18th Conference ofthe European Chapter of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages17521767. Haoyang Wen, Yijia Liu, Wanxiang Che, Libo Qin, andTing Liu. 2018. Sequence-to-sequence learning fortask-oriented dialogue with dialogue state representa-tion. In International Conference on ComputationalLinguistics.",
  "CAdditional Baselines": "We compared our model against the following end-to-end TOD baselines - We compare SyncTODagainst the following baselines - DSR (Wen et al.,2018), KB-Retriever (Qin et al., 2019), GLMP(Wu et al.), DF-Net (Qin et al., 2020), GPT-2+KE(Madotto et al., 2020), EER (He et al., 2020b),FG2Seq (He et al., 2020a), CDNet (Raghu et al.,",
  "We use Nvidia V100 GPUs to train all our models": "ET Predictors: We model all the ET predictors asflan-t5-large (Chung et al., 2024) sequence predic-tors and train them for 8 epochs with a learning rate(LR) of 1e 4 and batch size (BS) of 32. We usea linear decay LR scheduler with a warm-up ratioof 0.1. We use AdamW optimizer (Loshchilov andHutter, 2017). Training time was around 10 hours. DC Predictors: We model all the DC predictors asdeberta-V3-base (He et al.) binary classifiers andtrain them for 5 epochs with an LR of 3e5, BS of16, and linear decay LR scheduler with a warm-upratio of 0.1. We use AdamW optimizer. Trainingtime was around 1 hour.",
  "RS Predictors: During our experiments, we foundthat the training RS predictor is unstable. Thus, weuse a constant RS predictor with a value equal tothe mean response size in training data": "Exemplar Retrieval: For the MultiWOZ dataset,we use the last user utterance in the dialog contextto dense retrieve k = 30 samples from the trainingdata. We then re-rank them based on the hints andpick the top two.For the SMD dataset, we found that retrievalusing the entire dialog context works the best. Weattribute it to shorter dialog context and utterancesin the SMD dataset. Further, we use k = 2 asexemplars are already of high quality.",
  "FHuman Evaluation Details": "A snapshot of our human evaluation portal is givenin figure 3. Detailed evaluation guidelines are givenat the end of this section.We human-evaluate responses from three TODsystems - Gold, MAKER, and SyncTOD (GPT-4).From MultiWOZ and SMD datasets, we sample 80context-response pairs to evaluate appropriatenessand fluency. Two annotators, undergraduate andgraduate student volunteers, then independentlyscore TOD system responses for these sampleson a Likert scale (Likert, 1932). Here, the inter-annotator agreement was Kendalls Tau = 0.47at (p < 0.0001).To evaluate consistency, we randomly sample60 context-response pairs from the two datasets.Two student volunteers rated responses from theGold, MAKER, and SyncTOD systems on a 1-5 point Likert scale.One volunteer is a PhDscholar, while the other is a graduate student witha background in machine learning and NLP. Here,the inter-annotator agreement was Kendalls Tau = 0.45 at (p < 0.0001).The detailed evaluation guidelines are given be-low.",
  "How to judge fluency?": "Evaluate the linguistic quality of the response, in-cluding grammar, coherence, and readability. Thefluency of the response is independent of the dialogcontext or ground truth. A system output can bemarked strongly disagree for appropriateness andstill be marked strongly agree for fluency. You canmake your own rules about what each rating in thescale means for fluency, but please be consistentwith the rules you come up with.",
  "How to judge Consistency?": "Consistency of system response is the degree towhich the systems response accurately reflects andlogically aligns with the dialogue history and theknowledge base. Please rate each system responseon the following scale. Strongly Disagree (SD): The response is com-pletely inconsistent with the dialogue historyand the knowledge base. It provides incorrectinformation, contradicts previous dialog, anddoes not align with known facts.",
  "Disagree (D): The response has significant in-consistencies with the dialogue history andknowledge base. It may provide some cor-rect information but contains major errors orcontradictions": "Neutral (N): The response is generally consis-tent with the dialogue history and knowledgebase but may include minor errors or inconsis-tencies. The response mostly aligns with theprevious context but might have inaccuraciesor ambiguities. Agree (A): The response is consistent withthe dialogue history and knowledge base. Itcorrectly addresses the context and facts, withonly minor issues that do not significantly im-pact the overall coherence.",
  "dialog history - User and system utterances inthe dialog context c": "follow-up response - For exemplars, we suc-cinctly re-iterate the task definition and the entitytypes expected in the response, followed by gold en-tities and the response. For the test sample, we onlyprovide task definition and entity types expectedin the response and prompt the LLM to generateentities and the final response in order.",
  "H.2Creating rules from hints": "We transform hints H = (et, dc, rs) to rules in theprompt as follows. For response size, We add a ruleThe response must be rs words or shorter.For dialog closure dc = True(False), we add a ruleThe response must (not) close the dialog..For entity types et = [t1, t2, t3], we add a rule Theresponse must only include entities oftype - t1, t2, t3.. We also introduce a rule Theresponse must not include any entitiesof type - t1, t2, .. where t are entity types notpresent in et. We find that explicitly presenting neg-ative entity types demotivates LLM from includingextraneous entities in the response.",
  "MultiWOZ": "Henceforth, assume that you are a customer support expert. I will give you an incomplete dialog between a user and a customer service representative. As an expert, you must suggest the most appropriate follow-upresponse to the dialog. Ensure you also include correct information (entities) from the given database. Entities can be of the following types -1. name - name of a place (restaurant, hotel or attraction)2. address - address of the place3. phone - phone number of the place4. food - the type of food a restaurant serves5. area - a region of the city, e.g. centre, north, south, east, west6. postcode - postcode of the place7. price range - price range of the place, e.g. cheap, moderate, expensive8. type - the type of a place, e.g. restaurant, hotel, gusesthouse, attraction9. reference number - reference code for booking, e.g. 542j9wog10. stars - star rating of the hotel, e.g. 3 stars11. choice - number of available choices that match users requirements, e.g. many, few, several, 10",
  "[dialog history 1]user: hello i am looking for a place to go , can you help me ?": "[follow-up response 1]Lets think step-by-step.As an expert, I must understand the users requirements from [dialog history 1], identify the relevant information from the [database 1], follow all the [rules 1] and write the response.I will include entities of type [choice] in my response.I will include these entities - [(choice, 79)]assistant: i have found 79 place for you . do you have any specific ideas in mind ? [example 2][database 2]{corn guest house\":{\"address\":\"154 chesterton road\",\"phone\":\"01223353888\",\"area\":\"north\",\"postcode\":\"cb41da\",\"price range\":\"moderate\",\"type\":\"guesthouse\",\"stars\":\"4 star\",\"choice\":\"24\"},....} [rules 2]The response must be 10 words or shorter.The response must not close the dialog.The response must only include entities of type - choice.The response must not include any entities of type - name, address, phone, food, area, postcode, price range, type, reference number, stars.",
  "[dialog history 2]user: i d like to find a guesthouse to stay": "[follow-up response 2]Lets think step-by-step.As an expert, I must understand the users requirements from [dialog history 2], identify the relevant information from the [database 2], follow all the [rules 2] and write the response.I will include entities of type [choice] in my response.I will include these entities - [(choice, 24)]assistant: no problem . we have 24 to choose from . any specifics ? [example 3][database 3]{\"great saint mary s church\":{\"address\":\"market square\",\"phone\":\"01223350914\",\"area\":\"centre\",\"postcode\":\"cb23pq\",\"price range\":\"cheap\",\"type\":\"architecture\",\"choice\":\"a lot\"},....} [rules 3]The response must be 15 words or shorter.The response must not close the dialog.The response must only include entities of type - choice.The response must not include any entities of type - name, address, phone, food, area, postcode, price range, type, reference number, stars.",
  "SMD": "Henceforth, assume that you are an expert in in-car infotainment. I will give you an incomplete dialog between a user and an in-car infotainment system. As an expert, you mustsuggest the most appropriate follow-up response to the dialog. Ensure you also include correct information (entities) from the given database. Entities can be of the following types -1. poi - name of a point of interest, e.g., home, starbucks, pizza chicago, etc.2. address - address of a poi, e.g, 783 arcadia pl.3. poi type - the type of a poi, e.g., tea or coffee place, hospital, shopping center, etc.4. traffic info - traffic status on the way to a poi, e.g., heavy traffic, no traffic, road block nearby, etc.5. distance - distance of a poi from the users current location, e.g., 2 miles, 4 miles, etc.6. event - an event in the users calendar7. date - date in a month like the 1st or the 4th or day of a week like monday, wednesday.8. time - the time on which an event is scheduled9. party - party attending an event, e.g., tom, boss, brother, executive team, etc.10. agenda - agenda associated with an event, e.g., discuss dress code, go over budget, etc.11. room - meeting place of an event, e.g., conference room 100, etc.12. location - a location for which the user may request the weather information, e.g, boston, los angeles, etc.13. weather attribute - weather description in a location, e.g., cloudy, warm, hot, overcast etc.14. temperature - the in a location, e.g., 60f, 100f, etc.15. weekly time - temporal indicators like today, tomorrow, next week etc.",
  "[example 1][database 1]{\"trader joes\":{\"address\":\"408 university ave\",\"poi type\":\"grocery store\",\"traffic info\":\"no traffic\",\"distance\":\"5 miles\"},....}": "[rules 1]The response must be 11 words or shorter.The response must not close the dialog.The response must only include entities of type - poi, poi type.The response must not include any entities of type - address, traffic info, distance, event, date, time, party, agenda, room, location, weather attribute, temperature, weekly time.",
  "[dialog history 1]user: give me directions to the nearest grocery store": "[follow-up response 1]Lets think step-by-step.As an expert, I must understand the users requirements from [dialog history 1], identify the relevant information from the [database 1], follow all the [rules 1] and write the response.I will include entities of type [poi, poi type] in my response.I will include these entities - [(poi, trader joes), (poi type, grocery store)]system: the nearest grocery store is trader joes , would you like directions ?",
  "[example 2][database 2]{\"safeway\":{\"address\":\"452 arcadia pl\",\"poi type\":\"grocery store\",\"traffic info\":\"heavy traffic\",\"distance\":\"4 miles\"},....}": "[rules 2]The response must be 23 words or shorter.The response must not close the dialog.The response must only include entities of type - distance, poi, traffic info.The response must not include any entities of type - address, poi type, event, date, time, party, agenda, room, location, weather attribute, temperature, weekly time.",
  "[dialog history 2]user: give me directions to the closest grocery store": "[follow-up response 2]Lets think step-by-step.As an expert, I must understand the users requirements from [dialog history 2], identify the relevant information from the [database 2], follow all the [rules 2] and write the response.I will include entities of type [distance, poi, traffic info] in my response.I will include these entities - [(distance, 4 miles), (poi, safeway), (traffic info, heavy traffic)]system: we re 4 miles away from safeway but there is heavy traffic in this moment : do i set the gps to go there ?",
  "[example 3][database 3]{\"sigona farmers market\":{\"address\":\"638 amherst st\",\"poi type\":\"grocery store\",\"traffic info\":\"no traffic\",\"distance\":\"4 miles\"},....}": "[rules 3]The response must be 10 words or shorter.The response must not close the dialog.The response must only include entities of type - distance, poi, poi type.The response must not include any entities of type - address, traffic info, event, date, time, party, agenda, room, location, weather attribute, temperature, weekly time.",
  "[dialog history 3]user: give me directions to the closest grocery store": "[follow-up response 3]Lets think step-by-step.As an expert, I must understand the users requirements from [dialog history 3], identify the relevant information from the [database 3], follow all the [rules 3] and write the response.I will include entities of type [distance, poi, poi type] in my response.I will include these entities -",
  "BiTOD": "Henceforth, assume that you are a customer support expert. I will give you an incomplete dialog between a user and a customer service representative. As an expert, you must suggest the mostappropriate follow-up response to the dialog. Ensure you also include correct information (entities) from the given database. Entities can be of the following types -1. name - name of a place (restaurant, hotel or attraction)2. address - address of the place3. phone number - phone number of the place4. location - a part of the city e.g. canal road, central district5. rating - user rating of the place out of 10 e.g. 8, 96. price level - price range of the place, e.g. cheap, moderate, expensive7. reference number - reference code for booking, e.g. 542j9wog8. stars - star rating of the hotel, e.g. 3 stars9. price per night - hotel charges per night e.g. 512, 600, etc.10. number of rooms - number of rooms to book for the customer e.g. 1, 211. number of nights - number of nights the customer wants to book the hotel e.g. 2, 312. user name - name of the user e.g. Jack, Henry13. start month - starting month of the booking e.g. July, May, etc.14. start day - starting day of the booking e.g. 12, 30 etc.15. cuisine - the cuisine of a restaurant, e.g. thai, chinese, etc.16. dietary restrictions - dietary restrictions that the restaurant facilitates e.g. vegan, gluten free17. number of people - number of people to reserve a restaurant for e.g. 2, 1018. month - a month of the year e.g. january, february, etc.19. day - a day of the month/week e.g. 12, 17, monday, etc.20. time - a time of the daay e.g. 1200, 1330, 930, etc.21. type - type of an attraction e.g. zoos and aquariums, shopping, etc.",
  "[example 1][database 1]": "\"house 1881\":\"phone number\":\"852 0071 5353\",\"location\":\"tsim sha tsui\",\"rating\":\"8\",\"price level\":\"expensive\",\"reference number\":\"swm2n2uu\",\"stars\":\"5\",\"price per night\":\"1895\",\"number of rooms\":\"2\",... [rules 1]The response must be 20 words or shorter.The response must not close the dialog.The response must only include entities of type - name, rating.The response must not include any entities of type - address, phone number, location, price level, reference number, stars, price per night, number of rooms, number of nights, user name, startmonth, start day, cuisine, dietary restrictions, number of people, month, day, time, type. [dialog history 1]user: hi , im looking for recommendations for hotels .assistant: sure , what hotel rating are you looking for ?user: i want at least a rating of 1 .assistant: ok , what about price level or location ?user: i want a expensive hotel . i dont care about the location .assistant: sounds good , what about the number of stars of the hotel ?user: should be at least 2 stars . [follow-up response 1]Lets think step-by-step.As an expert, I must understand the users requirements from [dialog history 1], identify the relevant information from the [database 1], follow all the [rules 1] and write the response.I will include entities of type [name, rating] in my response.I will include these entities - [[name, house 1881], [rating, 8]]assistant: i found #16 hotels that are available . the house 1881 looks good . it has a rating of 8 .",
  "[example 2][database 2]": "\"house 1881\":\"phone number\":\"852 0071 5353\",\"location\":\"tsim sha tsui\",\"rating\":\"8\",\"price level\":\"expensive\",\"reference number\":\"swm2n2uu\",\"stars\":\"5\",\"price per night\":\"1895\",\"number of rooms\":\"2\",... [rules 2]The response must be 19 words or shorter.The response must not close the dialog.The response must only include entities of type - name, rating.The response must not include any entities of type - address, phone number, location, price level, reference number, stars, price per night, number of rooms, number of nights, user name, startmonth, start day, cuisine, dietary restrictions, number of people, month, day, time, type. [dialog history 2]user: hey ! i am looking for hotels with at least 2 stars . do you have any recommendations ?assistant: glad to be of service . to get started , can you tell me what rating level and price range are you looking for ?user: oh yeah ! i am looking for an expensive hotel with minimum 4 rating . [follow-up response 2]Lets think step-by-step.As an expert, I must understand the users requirements from [dialog history 2], identify the relevant information from the [database 2], follow all the [rules 2] and write the response.I will include entities of type [name, rating] in my response.I will include these entities - [[name, house 1881], [rating, 8]]assistant: as per your needs , there are #16 hotels available . i would recommend house 1881 with rating 8 .",
  "[example 3][database 3]": "\"jw marriott hotel hong kong\":\"phone number\":\"852 7885 6633\",\"location\":\"hong kong island\",\"rating\":\"9\",\"price level\":\"expensive\",\"reference number\":\"s5y9h2s3\",\"stars\":\"5\",\"price per night\":\"2210\",\"number of rooms\":\"10\",... [rules 3]The response must be 13 words or shorter.The response must not close the dialog.The response must only include entities of type - name, rating.The response must not include any entities of type - address, phone number, location, price level, reference number, stars, price per night, number of rooms, number of nights, user name, startmonth, start day, cuisine, dietary restrictions, number of people, month, day, time, type. [dialog history 3]user: hello . im trying to find a hotel for my stay with at least 4 stars . would you be able to help me ?assistant: hi there . i would be happy to help . would you like an expensive or affordable priced hotel ? do you prefer a high rating hotel ?user: i would like an expensive hotel with a rating of at least 4 . [follow-up response 3]Lets think step-by-step.As an expert, I must understand the users requirements from [dialog history 3], identify the relevant information from the [database 3], follow all the [rules 3] and write the response.I will include entities of type [name, rating] in my response.I will include these entities -"
}