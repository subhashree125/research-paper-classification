{
  "Abstract": "Hate speech detection is a prevalent researchfield, yet it remains underexplored at the levelof word meaning. This is significant, as termsused to convey hate often involve non-standardor novel usages which might be overlookedby commonly leveraged LMs trained on gen-eral language use.In this paper, we intro-duce the Hateful Word in Context Classifica-tion (HateWiC) task and present a dataset of4000 WiC-instances, each labeled by threeannotators. Our analyses and computationalexploration focus on the interplay between thesubjective nature (context-dependent connota-tions) and the descriptive nature (as describedin dictionary definitions) of hateful word senses.HateWiC annotations confirm that hatefulnessof a word in context does not always derivefrom the sense definition alone. We explore theprediction of both majority and individual anno-tator labels, and we experiment with modelingcontext- and sense-based inputs. Our findingsindicate that including definitions proves ef-fective overall, yet not in cases where hatefulconnotations vary. Conversely, including anno-tator demographics becomes more importantfor mitigating performance drop in subjectivehate prediction.",
  "Introduction": "This paper introduces the Hateful Word in ContextClassification (HateWiC) task, which aims to de-termine the hatefulness of a word within a specificcontext, as illustrated in . We argue thathateful word senses are not enough in focus withinHate Speech Detection (HSD) research, and notdescriptive only, but highly subjective, asking foranother approach than other lexical semantic taskslike Word Sense Disambiguation (WSD).",
  "Hateful senses are not enough in focus withinHSD research.The current focus of HSD re-search predominantly revolves around the classi-fication of entire utterances, such as social me-": ": Illustration of the HateWiC Classification taskand a conceptual semantic space that underlies the tar-geted phenomenona of hate-heterogeneous word senses,highlighting the distinction between the descriptive as-pects (e.g. cookie or person) and hateful connotation. dia posts (Waseem and Hovy, 2016; Davidsonet al., 2017). Within these utterances, lexical cuesfrequently play a significant role in the decision-making process. Yet, the computational modelingof context-specific hateful word meanings remainslargely unexplored, with a few exceptions in thisdirection (Dinu et al., 2021; Hoeken et al., 2023b). LMs commonly employed in HSD systemsdemonstrate effective word meaning modeling(Nair et al., 2020), but they tend to lack sensitivityto domain-specific, non-standard or novel wordsenses (Kumar et al., 2019; Blevins and Zettle-moyer, 2020). This insensitivity becomes particu-larly critical in detecting hateful word meanings,that are used in unconventional or emerging con-texts as the evolution of societal events gives riseto the continuous invention of novel expressions of hate (Qian et al., 2021). Words within the estab-lished lexicon, like Oreo, whose primary meaningmay not have any negative connotations (a cookie),are repurposed to convey hate towards particulargroups or individuals (e.g. based on ethnicity). Hateful senses are not descriptive only.Follow-ing theoretic work by Frigerio and Tenchini (2019),hateful terms could be positioned along a meaningcontinuum from descriptive to expressive, closer tobut not at the expressive outer end. The descrip-tive component comprises the truth-conditional at-tributes of a term, often recorded in dictionary defi-nitions. The expressive component, i.e. the conno-tation of a term, concerns speakers attitudes andemotions, making it highly context-specific andsubjective. A words sense definition could imply ahateful connotation, but this is not always the case,such as when used in a playful or self-identifyingway (e.g. the third usage in ). Thus, awords hateful connotation is not exclusively tiedto its descriptive definition, a phenomena whichwe term as hate-heterogeneous senses, but dependson various contextual factors like conveyed con-tent or the readers identity. This aspect is oftenoverlooked in HSD systems, typically developedusing data reflecting a single (majority) perspective(Zampieri et al., 2019; Mathew et al., 2020). Our contributions.In this study we addressthe gap in HSD by focusing on subjective hate-ful word meanings within context. We introducethe HateWiC dataset, a dataset of 4000 WiC-instances for which we collected three hatefulnessratings each. We design methods to classify senserepresentations and evaluate them both against themajority and the individual annotators label. Indoing so, we experiment with modeling descriptiveand subjective aspects of hateful word senses byincorporating sense definitions (as also provided toannotators) and annotator information.1",
  "Hate Speech Detection on Word Level": "Although the main body of research into HSD hasfocused on the level of utterances, some studieshave delved into hate speech on the lexical level.Prior to LMs, feature-based HSD systems (e.g. Leeet al. (2018)) often incorporated hate speech lex-icons. Wiegand et al. (2018) demonstrated theinduction of an abusive word lexicon in a non-contextualized setting. A specific subset of hate-ful terms within context is addressed by Hoekenet al. (2023b), who modelled slur detection em-ploying a dimension-based method similar to theidentification of gender bias in word embeddings(Bolukbasi et al., 2016). This approach, that re-quires a pre-given set of minimal pairs, is muchmore complex when tackling the broader spectrumof hateful terms, including words with both hatefuland non-hateful meanings. Qian et al. (2019) presented a framework aimingto predict the definition of hateful symbols, termswith a non-hateful surface form conveying hate,yet not covering the disambiguation between hateand non-hate. Mendelsohn et al. (2023) focusedon a related phenomenon, dog whistles, examiningwhether GPT-3 can identify their covert meanings,surface them in text generation and detect themin real-world texts. Dinu et al. (2021) introducethe task of disambiguating pejorative word usage,presenting two small-scale datasets and evaluatingseveral methods, with an MLP model classifyingBERT embeddings (Devlin et al., 2019) as mosteffective approach. Muti et al. (2024) addressedpejorative word disambiguation as a preliminarystep for misogyny detection in Italian texts.Our study focuses on the disambiguation ofwords with hateful meanings, which, although over-lapping with dog whistles and pejorative words,belong to distinct categories. Unlike hateful words,dog whistles are always intentionally ambiguous,concealing one meaning from the out-group whichis not exclusively hateful. Pejorative words, encom-pass any negatively connoted terms that may not behateful when not targeted at an individual or group.More importantly, unlike the single-perspective an-notations employed in the aforementioned studies,our focus is on subjective hate speech annotationand it is conducted on a much larger scale.",
  "Most existing datasets and methods in HSD adopta single, majority perspective, ignoring the inher-": "ent subjectivity influenced by diverse social andcultural factors (Zampieri et al., 2019; Founta et al.,2018). This approach has been shown to resultin problematic biases, concerning e.g. ethnicity,gender, and political beliefs and highlight the needfor new methodologies that account for the varyinginterpretations of hateful connotations (Davidsonet al., 2019; Kumar et al., 2021; Sap et al., 2022). Davani et al. (2022) took steps in this directionby training a model to predict individual annota-tions as subtasks, still ultimately aiming to predictthe majority label. Kanclerz et al. (2022) addressedthe task of predicting each individual annotatorslabel, by leveraging annotators labeling statisticswithin the dataset. Alacam et al. (2024) study theincorporation of gaze features (on token- and sen-tence level) from human annotators for predictingtheir subjective hate ratings. Another more com-prehensive approach is presented by Fleisig et al.(2023), who included annotators demographics,preferences, and experiences as input, along withtext. They utilized RoBERTa (Liu et al., 2019) toembed descriptions of these characteristics. Ourresearch continues this line of work by predictingindividual annotator labels and accounting for theirdemographics in the classification of hateful words.",
  "Modeling Word Senses": "Shifting the focus from modeling hateful utterancesto the meaning of hateful words within utterances,touches upon various lexical semantic NLP tasksthat involve the creation of word sense representa-tions (Vulic et al., 2020a; Schlechtweg et al., 2020;Martelli et al., 2021). Approaches to these tasksoften employ contextualized word embeddings ex-tracted from pretrained (often BERT-based) LMs(Loureiro and Jorge, 2019; Martinc et al., 2020;Bommasani et al., 2020). Fine-tuning a modelon particular data or tasks, such as WSD or sen-timent classification, is performed to potentiallyinject relevant information into the resulting repre-sentations (Giulianelli et al., 2020; Hoeken et al.,2023a). Rachinskiy and Arefyev (2022) leveragedan effective WSD model developed by Blevins andZettlemoyer (2020), which jointly optimizes twoencoders for the context and gloss of a word sense,respectively. For the task of semantic change dis-covery, they extracted the representations of thecontext encoder of the WSD-finetuned model.Recently, Giulianelli et al. (2023) introduced aninnovative approach to computational sense repre-sentations. Their method adopts the definition-as- sense paradigm, utilizing definitions generated bya Flan-T5 model (Chung et al., 2022) fine-tunedon datasets of definitions with usage examples.Sentence embeddings of these generated context-specific definitions show promising results on lex-ical semantic similarity tasks. Despite these ad-vancements focused on descriptive word senses,effective approaches for modeling highly connota-tive lexical phenomena remain unclear.",
  "Wiktionary Data": "Data was scraped from the English Wiktionary inNovember 2023, an online dictionary where any-one can contribute to documenting and explainingwords in use. Therefore, Wiktionary provides up-to-date insights from user perspectives and covers abroader range of terms from diverse domains thantraditional dictionaries.Each entry (word or multi-word expression) in-cludes information such as definitions, exampleuses, and category labels that provide additionalcontext about a words use (e.g., British slangor Archaic). Using the Wiktionary API, we ex-tracted all entries that had at least one word sensetagged with the categories Offensive and Deroga-tory and were also members of the category People,to gather the most relevant terms for hate speechdetection purposes. For each of the resulting 1087terms, we scraped all its sense definitions alongwith all labeled categories and example sentences(using the WiktionaryParser library). This resultedin 3500 senses and 4671 examples.To suit the dataset for our HateWiC classificationtask, we manually excluded 642 examples due tohistorical spelling or other deficiencies, as detailedin Appendix A. After processing, the dataset com-prised 4029 instances covering 1888 unique senses,averaging 4.88 examples per sense, and 826 uniqueterms, averaging 2.29 senses per term.",
  ": HateWiC examples with their annotations, illustrating the phenomena of annotator disagreement andhate-heterogeneous word senses (Nh = Not hateful, Wh = Weakly hateful, Sh = Strongly hateful)": "diverse interpretations of these labels due to theirsubjective nature. In this study, we aim to surveyand model different beliefs, following a descrip-tive data annotation paradigm as proposed in theframework by Rottger et al. (2022). This paradigmhighlights the value of using crowd-sourced an-notators from diverse backgrounds to encourageannotator subjectivity and mitigate bias, withoutrelaying on a predefined detailed definition of hatespeech. Specifically, we collected crowd-sourcedannotations using Prolific with a link integration toArgilla. Argilla, an open-source platform launchedon HuggingFace Spaces, is used to set up the anno-tation task on HateWiC data.For each annotation instance, annotators are pre-sented with an example sentence, the target term,and its sense definition. They are then promptedwith the question: How would you rate the hate-fulness of the meaning of the target term withinthe specific example text?. Annotators respond byselecting from the labels: Not hateful, Weaklyhateful, Strongly hateful and Cannot decide.An example of an annotation instance and the userinterface are depicted in a screenshot provided inAppendix B. In the annotation guidelines (accessi-ble on our repository), annotators are instructed tofocus their evaluation on the specific usage of theterm within the example sentence, rather than theoverall connotation of the sentence, or the defini-tion, which is only provided to aid in understandingthe terms meaning. Additionally, we emphasizethe subjective nature of their judgements.We aimed for three annotations per instance,with each annotator labeling 250 instances.2 UsingProlifics pre-screening filters, we selected annota-tors who indicated that their primary language isEnglish. To improve the quality of the collectedannotations, we excluded and replaced data fromannotators who were too fast and/or failed controlinstances.3 Prolific provides demographic informa-",
  "Dataset Results": "After excluding the Cannot decide annotations4,the dataset yielded 11902 individual annotations,of which 5708 (48.0%) hateful and 6194 (52.0%)not hateful (after converting to binary by merg-ing Weakly hateful and Strongly hateful). Afterapplying majority voting, out of the 3845 exam-ple sentences with a clear majority binary label,1815 (47.2%) were classified as hateful and 2030(52.8%) as not hateful, yielding a balanced datasetwith respect to hatefulness.Annotators agreed for 60% (i.e. 2414) of thebinary classification with a Krippendorffs alphaof 0.45. For the three-class classification, agree-ment was 51.3% with a Krippendorffs alpha of0.33. In comparison, Mathew et al. (2020) reportedan agreement of 0.46 for a similar three-class task,and Vigna et al. (2017) 0.26 for their binary set-ting. The agreement scores underscore the inherentsubjectivity of the task, motivating us to includeindividual demographics to our modeling.The high degree of context dependency regard-ing hate becomes even more apparent when weexamine the relationship between word senses(the descriptive aspects outlined in their defini-tions) and the hatefulness ratings assigned to ex-amples of those senses. We identified 319 hate-heterogeneous sense definitions, i.e. unique def-initions for which example sentences exist in thedataset with both hateful and non-hateful major-ity annotations. Two examples from the annotateddata given in illustrate this phenomenon.Both examples mention the term carrot cruncher",
  "than 45 min. completion time; median time was 90 min.4The majority of the 514 Cannot decide annotations werefound to concern deficient sentences upon closer analysis": "with the sense definition Someone from a ruralbackground. where (1) is unanimously annotatedas not hateful and (2) is unanimously annotatedas strongly hateful. This observation solidifies theidea, already implied by the inter-annotator agree-ment for individual labels (and exemplified by (4)in ), that the hateful connotation of a wordsense is not exclusively determined by its descrip-tive definition.",
  "HateWiC Classification": "Our HateWiC dataset enables the development andevaluation of computational methods for predict-ing whether the meaning of a target term is hatefulwithin a specific context. provides an il-lustration of the primary methodological pipelinewe present in this paper. We introduce various clas-sification methods that differ with respect to thesense representations (outlined in 4.1) and incor-poration of annotator information (4.2) as inputto a classification model (4.3), or that leverage aninstruction-tuned LLM (4.4).",
  "Sense Representations": "For representing the sense of a target term, weprimarily follow a common procedure in lexicalsemantic NLP tasks and extract contextualized em-beddings from pretrained LMs. To optimize ef-fectiveness on the HateWiC task, we experimentwith various encoder models and embedding types.Appendix C provides additional details on our em-ployed methods. Encoder models.We experiment with three dif-ferent encoder models, each trained on differ-ent data or tasks. We use the pretrained BERT(base) model (Devlin et al., 2019) and HateBERT(Caselli et al., 2021), a re-trained BERT modelon hate speech5. As third, we utilize a trained bi-encoder model for Word Sense Disambiguation(Blevins and Zettlemoyer, 2020), which we referto as WSD Biencoder. The model comprises acontextualized word encoder and a gloss encoderinitialized with BERT-base encoders. We train it onWordNet data (Miller et al., 1994), following thesame procedure as detailed in (Blevins and Zettle-moyer, 2020), for 7 epochs with a batch size of8. Following Rachinskiy and Arefyev (2022), theWSD-optimized contextualized word encoder isthen used for obtaining embeddings. Embeddings.The encoders are used to generatedifferent word sense related representations. First,we compute word in context (WiC) embeddings.We feed the example sentence to the encoder modeland extract the last hidden layer for the subword-tokenized position(s) that encode the target term(averaging over them in case of multi-subword tar-get terms). Second, we test the incorporation ofword sense definitions from Wiktionary. This defi-nition (Def) embedding is obtained by averageingover all token embeddings, using the same proce-dure as for WiC embeddings but with the defini-tion sentence as input. Third, considering that pre-given definitions may not be available in practicalapplications, we create T5-generated definition(T5Def) embeddings. We generate definitions us-ing a FLAN-T5 Base (250M parameters) modeldeveloped by (Giulianelli et al., 2023)6 which wasfine-tuned on datasets of English definitions andusage examples. We prompt the model with thesame template as it was trained on: [SENTENCE]What is the definition of [TERM]?. Consequently,the generated generated are more context-specificthan the Wiktionary definitions. These generateddefinitions are embedded the same way as the Def-embeddings.",
  "Annotator Information": "To address the subjective nature of the HateWiCclassification task,highlighted by the inter-annotator agreement in our dataset, we incorpo-rate this aspect into our modeling approaches. Weexperiment with a similar strategy as presented inFleisig et al. (2023). For each individual annotationof a HateWiC instance, we concatenate an annota-tor (Ann) embedding to the corresponding senseembedding, that represent a description of annota-tors demograpics. This description is embeddedthrough the same procedure as the definition em-beddings and follows this template:",
  "Classification with LLaMA 2": "In addition to the encoder-LM based approachesabove, we also experiment with a LLaMA 2 model(Touvron et al., 2023). Due to their instruction-tuning training regime, and huge amount of trainingdata, foundation models like LLaMA 2 are provento be superior to LMs on many zero-shot settings,yet subjective HSD and WSD are by nature verychallenging tasks. We aim to see the abilities ofan instruction-tuned LLM on this task as a (strong)baseline. We test zero-shot classification with a7B-sized LLaMA 2 model7. We run the inferenceof this model using the transformers library. Inour prompt, we input the example sentence andthe target term and instruct the model to classifythe meaning of the term as hateful or not hateful(complete template and configuration parametersare provided in Appendix C).",
  ". Subjective label prediction: gold labels con-sist of all 12442 individual annotations: a rat-ing per example and annotator": "We conduct evaluations for each task using aten-fold cross-validation setup. For each fold, wedivide the dataset into training, development, andtest sets with an 80-10-10 ratio. We experimentwith two variants: 1. Random: The data is randomly split basedon example sentences, testing performanceon sentences not seen during training (similarto common practice in WSD-like tasks (Dinuet al., 2021)), which is particularly relevantfor individual annotator prediction where mul-tiple instances of the same sentence occur.",
  "Comparison with Dinu et al. (2021)": "We also train and test on two small datasets ofEnglish tweets developed and used in Dinu et al.(2021). They collected these from existing hatespeech datasets, focusing on tweets that mentionone of the terms in a curated set of pejorative terms.Each tweet was labeled based on whether the termwas used pejoratively. The first dataset, whichwe will refer to as DINU1 comprised 1004 tweetscovering 31 terms. The second, which we nameDINU2, consisted of 301 tweets covering 11 terms.Their reported best method involved MLP classi-fication of BERTweet (Nguyen et al., 2020) andBERT (base) embeddings (extracted as the sum ofall model layers for the target word position) on",
  "Majority HateWiC Classification": "presents the accuracy results on HateWiCclassification compared to the majority label. Over-all, the performance values demonstrate the ef-fectiveness of all methods, with only minimaldifferences (max.2 %-points) between BERT,HateBERT and WSD biencoder models. TrainingBERT-based models on different types of informa-tion regarding hatefulness or word senses does notseem to have a substantial effect.",
  ": Accuracy on HateWiC classification comparedto the majority label, with different input embeddings,tested on a random data split (best underlined) and a testsplit with OoV terms only (best in bold)": "Def-embeddings achieve slightly higher accura-cies than WiC-embeddings , and a combination ofthe two yields the best results. For a test set withOoV terms only, all embedding types show only aslight drop in performance. WiC+Def-embeddingsexhibit the smallest decline on the zero-shot settingand achieve 2-5 % higher accuracy than WiC- andDef-embeddings. This indicates that definitionsprovide valuable information, performing betteron their own than word information alone, andthe combination of both is most effective, espe-cially for OoV-terms.T5-generated definitionsdemonstrated the lowest accuracy on their ownbut perform equally or slightly better than WiC-embeddings when concatenated. An evaluation ofT5-generated definitions compared to Wiktionarydefinitions showed a very low SacreBLEU score of3.822 (in range 0 to 100), possibly explaining thedifferences in performance between them.The distinction between context-independentDef-embeddingsandcontext-specificWiC- and T5Def-embeddings becomes more clearupon examining their performance across hate-homogeneous and hate-heterogeneous instances(as defined in .3), presented in in .In the case of hate-heterogeneous instances, weobserve an accuracy drop of up to 47% whenusing Def-including embeddings compared to thehomogeneous instances. This drop is limited to24-29% for the other embeddings, showcasingtheir superior ability in handling less descriptivescenarios. We define hate-homogeneous here as in-stances where definitions have example sentencesin the dataset with either hateful or non-hateful(majority) annotations whereas hate-heterogeneoushave both (as detailed in .3).",
  ": Accuracy on HateWiC classification comparedto the majority label w.r.t. hate homogeneity of thesense definition (best underlined)": "LLaMA 2 result.The accuracy score on theHateWiC classification using a LLaMa 2 model,following the zero-shot experimental setup detailedin .4, is 0.68. Unlike the superior per-formance on many downstream tasks, the LLaMAmodel falls short compared to the aforementionedmodels on our HateWiC task. This outcome high-lights the subjective nature of the task, indicatingthat general-purpose models struggle to fully graspits nuances and perform well on it.",
  "Subjective HateWiC Classification": "Performance of our designed methods on predict-ing individual annotation labels, which showed con-siderable variation in .3, are presented in. Overall, accuracy values are slightly lower(by 2-5 %-points) compared to predicting the ma-jority label, but remain robust. The results exhibitthe same patterns in terms of different models, testdata setups, and tested embedding types. Addingthe Annotator embedding has a minimal effect, gen-erally resulting in equal or slightly improved per-formance compared to the same type of embeddingwithout concatenated annotator information.To better understand the impact of subjectivity,we more closely examine instances where subjec-",
  "WiC+Ann0.720.690.720.690.720.70Def+Ann0.740.720.760.720.750.72T5Def+Ann0.690.670.690.650.690.68WiC+Def+Ann0.750.730.750.740.750.74WiC+T5Def+Ann0.720.710.730.710.730.72": ": Accuracy on HateWiC classification comparedto the individual annotator label, with different inputembeddings, on a random data split (best underlined)and a test split with OoV terms only (best in bold). tivity is most apparent (and thus potentially harmfulwhen methods fail). In we report perfor-mance results not only with respect to the hatehomogeneity of word senses, but also to annotatoragreement, i.e. whether the annotator agreed withthe majority. We present results for HateBERTembeddings in an evaluation setting with randomtest data split, but similar patterns are observed forBERT and WSD Biencoder embeddings, as well ason on a test data split with OoV terms only.For sentence annotations where the annotatordisagreed with the majority label or the sense def-inition is hate-heterogeneous, the performance ofall embeddings drops significantly. This effectis most pronounced for definition-including em-beddings (Wiktionary), less so for T5-generated,which aligns with their more context-specific na-ture. Specifically, there is an accuracy drop of upto 47% in cases of annotator disagreement, and upto 32% in cases of hate-heterogeneous definitions.However, incorporating annotator information mit-igates this effect by up to 11%. Annotator informa-tion contributes to the cases where the subjectiveannotation deviates from the majority label, thesecases also align with sense definitions that exhibitboth hateful and non-hateful labeled sentences.",
  "Results on DINU Data": "The DINU1 and DINU2 evaluation datasets do notprovide sense definitions or information on anno-tators, thereby limiting our testing to our meth-ods that do not require this information. presents the results on both DINU1 and DINU2.Our methods, except for those including T5Def-embeddings only, demonstrate improvements overthe best-performing methods proposed by Dinuet al. (2021). These improvements are particu-larly substantial (by 8%) for the larger DINU1",
  "Discussion": "Our study offers valuable insights into the detec-tion of hate speech through the lens of lexical se-mantics, introducing the HateWiC dataset and pre-senting classification experiments. The negligibledifference observed in our experimental outcomesbetween HateBERT and general (WSD) models notonly questions the efficacy of extensive training onhate speech data for accurately capturing hatefulsemantics, but also underscores the necessity of amore nuanced approach beyond the existing lexicalsemantic methods for tasks like HateWiC classi-fication. Our results demonstrate the impact ofincorporating sense definitions and annotator char-acteristics on model performance, particularly inscenarios involving out-of-vocabulary (OoV) terms",
  "or high subjectivity": "To define or not define?Hateful terms, accord-ing to lexical semantic theory, primarily containan expressive component but not exclusively. In-corporating sense definitions into our methods, toencompass the descriptive component of hatefulterms, yielded mixed results. Overall, embeddedWiktionary definitions proved highly effective, out-performing Word in Context (WiC) embeddingsalone. T5-generated definitions demonstrated thelowest accuracy on their own but performed equallyor slightly better than WiC-embeddings only whenconcatenated with WiC-embeddings. However, incases with more variation in the subjective ratings,the performance of all embeddings dropped signif-icantly but most pronounced for Wiktionary def-inition embeddings, though to a lesser extent forT5-generated definitions (with a drop difference ofup to 23%). This highlights the usefulness of au-tomatically generating context-specific definitionsfor subjective lexical semantic tasks like HateWiCclassification. Future research will focus on moreadvanced definition generation techniques, possi-bly leveraging larger models or fine-tuning on Wik-tionary definitions, while avoiding overreliance ondictionary definitions as the ultimate standard. Toindividualizeanyway?Thelowinter-annotator agreement in our dataset underscoresthe importance of considering individual annotatorperspectives in hate speech detection. Our experi-ments incorporating annotator information in ourcomputational methods proved beneficial, partic-ularly in cases of annotator disagreement or hate-heterogeneous definitions, where including annota-tor information mitigated accuracy decline by upto 11%-points. This highlights the value of per-sonalizing models to account for subjectivity inannotations. Future research could explore addi-tional annotator information and conduct ablationexperiments to identify the most effective aspectsfor HateWiC classification. To consider as well?Our study paves the wayto obtaining deeper insights into the relationshipbetween hateful and non-hateful word senses. Forinstance, whether certain semantic relations (e.g.metaphorical, metonymical), categories (e.g. food,animals), or attributes (e.g. color, material) aremore likely to distinguish between hateful and non-hateful senses. And even next-level, whether thesediscriminators are language-specific or show cross-",
  "Conclusion": "This paper introduces the Hateful Word in ContextClassification (HateWiC) task, addressing the un-derexplored area of subjective hateful word mean-ings within specific contexts.We present theHateWiC dataset, comprising about 4000 WiC-instances, each annotated with three hateful ratings.Our study focused on the interplay between descrip-tive and subjective aspects of hateful word senses.We addressed the prediction of both majority andindividual annotator labels. We experimented withdifferent types of inputs to our classification sys-tem, including sense definitions and annotator de-mographics. We demonstrated the impact of thesefactors on model performance, particularly in casesinvolving out-of-vocabulary terms or high subjec-tivity. The incorporation of established sense defi-nitions proved highly effective overall but demon-strating diminished performance in less descriptivescenarios. Conversely, including annotator char-acteristics proved beneficial, particularly in casesof annotator disagreement or hate-heterogeneousdefinitions. These findings underscore the valueof personalizing models to account for subjectivityin annotations. Furthermore, our results suggestthe potential usefulness of automatically generat-ing definitions for subjective lexical semantic taskslike HateWiC classification.",
  "Limitations": "Although the Wiktionary data we utilize offersinsights from user perspectives for a wide arrayof terms, its quality may be lower compared toexpert-curated dictionaries. The provided informa-tion may contain inaccuracies, as users might nothave the necessary expertise, and inconsistency indocumentation could exist. However, the collabo-rative nature of Wiktionary allows for censorshipby consensus and adherence to Wiktionary policies,mitigating some of these concerns.A constraint of our evaluation set-up lies in itsreliance on binary labels. Hate speech is a mul-tifaceted phenomenon, and a more nuanced classscheme may offer a more comprehensive under-standing in future research.",
  "Ethics Statement": "Our study includes demographic data of annotatorsthat concern Prolific prescreening responses whichare all with annotators consent, self-reported, andare not provided with any direct identifiers likename or address. All prescreening questions, ex-cept for age and country of residence, are optionalfor participants to answer, and most personal ques-tions have a Rather not say option. By incorporat-ing demographic information from annotators, weaim to enhance the understanding and predictionof how different groups perceive hate speech. Thisapproach will ultimately lead to more robust and in-clusive classification systems. However, the inclu-sion of demographic data raises privacy concerns,particularly the risk of re-identifying annotators.To address this, we have made our dataset avail-able only upon request, under the CC BY-NC 4.0license. This measure allows us to better control ac-cess to the information, ensuring it is used responsi-bly, ethically, and exclusively for non-commercialpurposes.",
  "Acknowledgements": "The authors acknowledge financial support by theproject SAIL: SustAInable Life-cycle of Intelli-gent Socio-Technical Systems (Grant ID NW21-059A), which is funded by the program Netzw-erke 2021 of the Ministry of Culture and Scienceof the State of North Rhine-Westphalia, Germany. zge Alacam, Sanne Hoeken, and Sina Zarrie. 2024.Eyes dont lie: Subjective hate annotation and de-tection with gaze. In Proceedings of the 2024 Con-ference on Empirical Methods in Natural LanguageProcessing, Miami, Florida, USA. Association forComputational Linguistics. Terra Blevins and Luke Zettlemoyer. 2020. Movingdown the long tail of word sense disambiguationwith gloss informed bi-encoders. In Proceedingsof the 58th Annual Meeting of the Association forComputational Linguistics, pages 10061017, Online.Association for Computational Linguistics. Tolga Bolukbasi, Kai-Wei Chang, James Y Zou,Venkatesh Saligrama, and Adam T Kalai. 2016. Manis to computer programmer as woman is to home-maker? debiasing word embeddings. Advances inneural information processing systems, 29.",
  "ceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 47584781, Online. Association for Computational Lin-guistics": "Tommaso Caselli, Valerio Basile, Jelena Mitrovic, andMichael Granitzer. 2021.HateBERT: RetrainingBERT for abusive language detection in English. InProceedings of the 5th Workshop on Online Abuseand Harms (WOAH 2021), pages 1725, Online. As-sociation for Computational Linguistics. Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, Al-bert Webson, Shixiang Shane Gu, Zhuyun Dai,Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh-ery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,Dasha Valter, Sharan Narang, Gaurav Mishra, AdamsYu, Vincent Zhao, Yanping Huang, Andrew Dai,Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-cob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,and Jason Wei. 2022. Scaling instruction-finetunedlanguage models. Preprint, arXiv:2210.11416. Aida Mostafazadeh Davani, Mark Daz, and Vinodku-mar Prabhakaran. 2022. Dealing with Disagreements:Looking Beyond the Majority Vote in Subjective An-notations. Transactions of the Association for Com-putational Linguistics, 10:92110. Thomas Davidson, Debasmita Bhattacharya, and Ing-mar Weber. 2019. Racial bias in hate speech andabusive language detection datasets. In Proceedingsof the Third Workshop on Abusive Language Online,pages 2535, Florence, Italy. Association for Com-putational Linguistics.",
  "Thomas Davidson, Dana Warmsley, Michael W. Macy,and Ingmar Weber. 2017. Automated hate speech de-tection and the problem of offensive language. CoRR,abs/1703.04009": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics. Liviu P. Dinu, Ioan-Bogdan Iordache, Ana Sabina Uban,and Marcos Zampieri. 2021. A computational ex-ploration of pejorative language in social media. InFindings of the Association for Computational Lin-guistics: EMNLP 2021, pages 34933498, PuntaCana, Dominican Republic. Association for Compu-tational Linguistics.",
  "Aldo Frigerio and Maria Paola Tenchini. 2019. Pejora-tives: a classification of the connoted terms. RivistaItaliana di Filosofia del Linguaggio, 13(1)": "Mario Giulianelli, Marco Del Tredici, and Raquel Fer-nndez. 2020. Analysing lexical semantic changewith contextualised word representations. In Pro-ceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 39603973, Online. Association for Computational Lin-guistics. Mario Giulianelli, Iris Luden, Raquel Fernandez, andAndrey Kutuzov. 2023. Interpretable word senserepresentations via definition generation: The caseof semantic change analysis. In Proceedings of the61st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages31303148, Toronto, Canada. Association for Com-putational Linguistics.",
  "Sanne Hoeken, Sina Zarrie, and Ozge Alacam. 2023b": "Identifying slurs and lexical hate speech via light-weight dimension projection in embedding space. InProceedings of the 13th Workshop on ComputationalApproaches to Subjectivity, Sentiment, & Social Me-dia Analysis, pages 278289, Toronto, Canada. Asso-ciation for Computational Linguistics. Kamil Kanclerz, Marcin Gruza, Konrad Karanowski,Julita Bielaniewicz, Piotr Milkowski, Jan Kocon, andPrzemyslaw Kazienko. 2022. What if ground truthis subjective? personalized deep neural hate speechdetection. In Proceedings of the 1st Workshop on Per-spectivist Approaches to NLP @LREC2022, pages3745, Marseille, France. European Language Re-sources Association.",
  "content classification for a diversity of perspectives.In Proceedings of the Seventeenth USENIX Confer-ence on Usable Privacy and Security, SOUPS21,USA. USENIX Association": "Sawan Kumar, Sharmistha Jat, Karan Saxena, andPartha Talukdar. 2019. Zero-shot word sense dis-ambiguation using sense definition embeddings. InProceedings of the 57th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 56705681, Florence, Italy. Association for ComputationalLinguistics. Ho Suk Lee, Hong Rae Lee, Jun U. Park, and Yo SubHan. 2018. An abusive text detection system basedon enhanced abusive and non-abusive word lists. De-cision Support Systems, 113:2231. Publisher Copy-right: 2018 Elsevier B.V. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A robustly optimized BERT pretrainingapproach. CoRR, abs/1907.11692. Daniel Loureiro and Alpio Jorge. 2019.Languagemodelling makes sense: Propagating representationsthrough WordNet for full-coverage word sense disam-biguation. In Proceedings of the 57th Annual Meet-ing of the Association for Computational Linguistics,pages 56825691, Florence, Italy. Association forComputational Linguistics. Federico Martelli, Najla Kalach, Gabriele Tola, andRoberto Navigli. 2021. SemEval-2021 task 2: Mul-tilingual and cross-lingual word-in-context disam-biguation (MCL-WiC). In Proceedings of the 15thInternational Workshop on Semantic Evaluation(SemEval-2021), pages 2436, Online. Associationfor Computational Linguistics. Matej Martinc, Petra Kralj Novak, and Senja Pollak.2020. Leveraging contextual embeddings for detect-ing diachronic semantic shift. In Proceedings of theTwelfth Language Resources and Evaluation Confer-ence, pages 48114819, Marseille, France. EuropeanLanguage Resources Association. Binny Mathew, Punyajoy Saha, Seid Muhie Yimam,Chris Biemann, Pawan Goyal, and Animesh Mukher-jee. 2020. Hatexplain: A benchmark dataset for ex-plainable hate speech detection. In AAAI Conferenceon Artificial Intelligence. Julia Mendelsohn, Ronan Le Bras, Yejin Choi, andMaarten Sap. 2023. From dogwhistles to bullhorns:Unveiling coded rhetoric with language models. InProceedings of the 61st Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 1516215180, Toronto, Canada.Association for Computational Linguistics.",
  "Technology, HLT 94, page 240243, USA. Associa-tion for Computational Linguistics": "Arianna Muti, Federico Ruggeri, Cagri Toraman, Al-berto Barrn-Cedeo, Samuel Algherini, LorenzoMusetti, Silvia Ronchi, Gianmarco Saretto, and Cate-rina Zapparoli. 2024. PejorativITy: Disambiguatingpejorative epithets to improve misogyny detectionin Italian tweets. In Proceedings of the 2024 JointInternational Conference on Computational Linguis-tics, Language Resources and Evaluation (LREC-COLING 2024), pages 1270012711, Torino, Italia.ELRA and ICCL. Sathvik Nair, Mahesh Srinivasan, and Stephan Mey-lan. 2020. Contextualized word embeddings encodeaspects of human-like word sense knowledge. In Pro-ceedings of the Workshop on the Cognitive Aspectsof the Lexicon, pages 129141, Online. Associationfor Computational Linguistics. Dat Quoc Nguyen, Thanh Vu, and Anh Tuan Nguyen.2020. BERTweet: A pre-trained language modelfor English tweets. In Proceedings of the 2020 Con-ference on Empirical Methods in Natural LanguageProcessing: System Demonstrations, pages 914, On-line. Association for Computational Linguistics. Jing Qian, Mai ElSherief, Elizabeth Belding, andWilliam Yang Wang. 2019. Learning to decipher hatesymbols. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages30063015, Minneapolis, Minnesota. Association forComputational Linguistics. Jing Qian, Hong Wang, Mai ElSherief, and Xifeng Yan.2021. Lifelong learning of hate speech classifica-tion on social media. In Proceedings of the 2021Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 23042314, Online.Association for Computational Linguistics. Maxim Rachinskiy and Nikolay Arefyev. 2022. Gloss-Reader at LSCDiscovery: Train to select a propergloss in English discover lexical semantic changein Spanish. In Proceedings of the 3rd Workshop onComputational Approaches to Historical LanguageChange, pages 198203, Dublin, Ireland. Associationfor Computational Linguistics. Paul Rottger, Bertie Vidgen, Dirk Hovy, and Janet Pier-rehumbert. 2022. Two contrasting data annotationparadigms for subjective NLP tasks. In Proceedingsof the 2022 Conference of the North American Chap-ter of the Association for Computational Linguis-tics: Human Language Technologies, pages 175190,Seattle, United States. Association for ComputationalLinguistics.",
  "Maarten Sap, Swabha Swayamdipta, Laura Vianna,Xuhui Zhou, Yejin Choi, and Noah A. Smith. 2022.Annotators with attitudes: How annotator beliefs": "and identities bias toxic language detection. In Pro-ceedings of the 2022 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages58845906, Seattle, United States. Association forComputational Linguistics. Dominik Schlechtweg, Barbara McGillivray, SimonHengchen, Haim Dubossarsky, and Nina Tahmasebi.2020. SemEval-2020 task 1: Unsupervised lexicalsemantic change detection. In Proceedings of theFourteenth Workshop on Semantic Evaluation, pages123, Barcelona (online). International Committeefor Computational Linguistics. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288.",
  "Fabio Del Vigna, Andrea Cimino, Felice DellOrletta,Marinella Petrocchi, and Maurizio Tesconi. 2017.Hate me, hate me not: Hate speech detection onfacebook. In Italian Conference on Cybersecurity": "Ivan Vulic, Simon Baker, Edoardo Maria Ponti, UllaPetti, Ira Leviant, Kelly Wing, Olga Majewska, EdenBar, Matt Malone, Thierry Poibeau, Roi Reichart,and Anna Korhonen. 2020a. Multi-SimLex: A large-scale evaluation of multilingual and crosslingual lexi-cal semantic similarity. Computational Linguistics,46(4):847897. Ivan Vulic, Edoardo Maria Ponti, Robert Litschko,Goran Glava, and Anna Korhonen. 2020b. Prob-ing pretrained language models for lexical semantics.In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 72227240, Online. Association for Computa-tional Linguistics. Zeerak Waseem and Dirk Hovy. 2016. Hateful symbolsor hateful people? predictive features for hate speechdetection on Twitter. In Proceedings of the NAACLStudent Research Workshop, pages 8893, San Diego,California. Association for Computational Linguis-tics. Michael Wiegand, Josef Ruppenhofer, Anna Schmidt,and Clayton Greenberg. 2018. Inducing a lexicon ofabusive words a feature-based approach. In Pro-ceedings of the 2018 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, Volume1 (Long Papers), pages 10461056, New Orleans,Louisiana. Association for Computational Linguis-tics. Marcos Zampieri, Shervin Malmasi, Preslav Nakov,Sara Rosenthal, Noura Farra, and Ritesh Kumar.2019. SemEval-2019 task 6: Identifying and cat-egorizing offensive language in social media (Of-fensEval). In Proceedings of the 13th InternationalWorkshop on Semantic Evaluation, pages 7586, Min-neapolis, Minnesota, USA. Association for Compu-tational Linguistics.",
  "AWiktionary Data Processing": "Our data was scraped from the English Wiktionarycomprising entries with information on definitions,example uses, and category labels that provide ad-ditional context about a words use. We scraped allsense definitions along with all labeled categoriesand example sentences of the selected terms usingthe WiktionaryParser library. This library methoddid not split the examples over the set of sense def-initions (i.e. provided all examples in one bundle),so we manually matched the right examples withthe right sense definitions, through look up on theWiktionary website, afterwards.To suit the dataset for the envisioned task wemanually excluded 642 examples that were eitherwritten in historical spelling or not single in-the-wild usages of the term. The latter concerned us-ages, like the examples below (with the target termin bold), that were (a) dictionary-typical nominalphrases and not sentences, (b) concerned meta-level discussions of the target term or (c) dialoguesor other indirect uses of the target term.",
  "Finally, we slightly edited some type of instancesthat concerned non-exact matches between wordform of the term and its occurrence in the exam-ple. For compounds or multi-word expressions,": "this mismatch often concerned the (non-)use ofa whitespace or hyphen between compound parts(e.g. the term baby face occurred also as babyfaceor baby-face in examples). This type of mismatcheswas solved by applying a simple rule-based replace-ment strategy to the example sentences.Other types of non-exact word form matcheswere mainly caused by inflection (e.g. plural formsfor nouns) and some by misspellings. These caseswere left unchanged for the final dataset as remov-ing could influence the meaning.We also created groupings to aggregate categorylabels, consolidating the 585 unique Wiktionarylabels present in our dataset into a manageable setof usage tags. This enrichment potentially providesuseful information for future analyses on usages ofhateful terms.",
  "BAnnotation Details": "displays the user interface for annotation,with an example of an annotation instance.Below, we report the distribution of our annota-tors with respect to age, gender, and ethnicity. It isimportant to note that we use the categories as pro-vided through the Prolific provided presecreeningresponses, which are simplified groupings intendedto give a general overview. As detailed in the EthicsStatement, we acknowledge that this categorizationdoes not fully capture the complexity and diversityof individual identities and may include sensitiveterminology.The final pool of 48 annotators, after exclusions,had an average age of 28 (ranging from 20 to 60)and included 26 females, 28 males, and 1 unspec-ified gender. Based on simplified ethnicity cate-gories, 21 identified as White, 19 as Black, 4 asAsian, 3 as Mixed, and 1 as Other.",
  "CMethod Details": "Finding target term sentence positions.For allWiC-embeddings, to find the indices of (the sub-words that form) the target word in an example sen-tence that concerned a non-exact wordform matchbetween target term and example mention (due toinflection or misspellings), we applied two subse-quent strategies: 1) we tried to replace the targetterm with its plural form (through simple rules) andif this plural formation did not result in a match, 2)we tried to find the most similar word in the exam-ple sentence (using the difflib library) and replacedthat wordform with the target term (as this most",
  "often concerned a misspelling)": "Model layer configurations.We also tested theextraction of different layer configurations, sincethe effectivity of different configurations has shownto differ within lexical semantic tasks (Vulic et al.,2020b). We tested for BERT WiC-embeddings theextraction of: all layers (12 for BERT), last fourlayers or last layer only. The results in ,demonstrate no effect of layer configuration on themethod performance. MLP classificaton model.The multilayer per-ceptron model used for classification consisted offour hidden layers with dimensionality 300, 200,100 and 50, respectively. For training we used theMLPClassifier module from the sklearn libaray andwe set the initial learning rate to 0.0005 the maxi-mal number of training iterations to 10. These pa-rameters were selected after a grid search on our de-velopment dataset, using sklearns GridSearchCVmodule, applied to the following parameter grid:{hidden_layer_sizes:[(300, 200, 100, 50), (200,100, 50), (100, 50)], learning_rate_init:[0.0005,0.001, 0.005], max_iter: }.",
  "DDimension Projection": "We also tested the dimension approach of Hoekenet al. (2023b), adapted to our task. In their methodfor slur detection, they create a hate dimensionby computing the average over difference vectorsbetween representations of 10 minimal pairs ofslurs and non-hateful equivalents (e.g. hillbillies- rural people). Unlike slurs, which generallycarry derogatory connotations regardless of con-text (Hess, 2021), the hateful connotations of otherhateful terms are less clear-cut (Frigerio and Ten-chini, 2019). This was also illustrated in the con-ceptual semantic space in . Consequently,we did not expect an effective dimension hate di-mension to be extractable using pretrained modelsthat encode general word semantics. Additionally,pre-establishing a set of minimal pairs is hardly",
  "feasible for similar reasons": "Our approach.For our task, instead of using apre-established list of word pairs, we derived thislist from the training data. We calculated the cosinesimilarities between all possible pairs of positiveand negative embeddings, i.e. sense representationsof hateful and non-hateful training examples, re-spectively. We then selected pairs with a similarityabove a certain threshold to create the dimension,trough the same computation procedure as Hoekenet al. (2023b). After testing a range of thresholds([0.7, 0.75, 0.8, 0.85, 0.9, 0.95]) on the develop-ment set, we set the similarity threshold to 0.9 fortesting. Following Hoeken et al. (2023b), we clas-sified positive cosine similarity values between thehate dimension vector and the contextualized wordsense representation as hateful, and negative valuesas non hateful."
}