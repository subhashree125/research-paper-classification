{
  "Abstract": "With the rapidly-growing deployment of largelanguage model (LLM) inference services, pri-vacy concerns have arisen regarding to the userinput data. Recent studies are exploring trans-forming user inputs to obfuscated embeddedvectors, so that the data will not be eaves-dropped by service provides. However, in thispaper we show that again, without a solid anddeliberate security design and analysis, suchembedded vector obfuscation failed to protectusers privacy. We demonstrate the conclusionvia conducting a novel inversion attack calledElement-wise Differential Nearest Neighbor(EDNN) on the glide-reflection proposed in(Mishra et al., 2024), and the result showedthat the original user input text can be 100%recovered from the obfuscated embedded vec-tors. We further analyze security requirementson embedding obfuscation and present severalremedies to our proposed attack.",
  "Introduction": "Inference services of language models are nowgaining popularity, with a considerable numberof language models having been deployed on thecloud server. However, users might concern aboutthe privacy of their data when requesting inferenceservice, that is, their data would be eavesdroppedby malicious service providers.To address this prob-lem, recent research has turned to adopting obfus-cation techniques on the embedding matrix, ensur-ing that user inputs cannot be recovered from theobfuscated embeddings by service providers. Em-bedding obfuscation becomes appealing since theobfuscated embeddings can be directly forwardedto inference process as efficient as plaintext embed-dings, leading to practical potential for real appli-cations compared with secure multi-party compu-tation (MPC) and homomorphic encryption (HE).For example, the state-of-the-art work in (Mishraet al., 2024) leverages glide-reflection for embed- ding obfuscation combined with the user-side key-based hashing, to claim a private and secure infer-ence solution.Nonetheless, recent studies show that a ma-licious server can indeed reconstruct user datathrough embedding inversion attacks (EIA) (Quet al., 2021; Kugler et al., 2021). Consequently,without formal security analysis, concerns persistregarding the potential existence of novel EIAscapable of extracting user information from theseembedding obfuscation methods. In this paper, weanalyze the security of the glide-reflection method-ology used in (Mishra et al., 2024), ultimatelyuncovering its vulnerability. We innovatively de-sign an Element-wise Differential Nearest Neigh-bor (EDNN) attack to effectively break the securityof the glide-reflection. Our experimental outcomesconclusively demonstrate that the EDNN entirelyrecovers 100% of the user data tokens which os-tensibly secured by the glide-reflection. Subse-quently, we present an insight on why the naivelinear-transformation based obfuscation, like glide-reflection, fails to safeguard user data. We furtherdiscuss the security requirements of embedding ob-fuscation and demonstrate that the deliberate secu-rity design is necessary. We also introduce severalpossible defenses against EIA base on our analysis.",
  "The system model is composed of two entities. Adata user desires to request inference service us-ing its private data. A model server is deployedwith a fine-tuned language model to offer inference": "service.The schema adopts a typical threat model forsecure language model inference as used in recentworks (Zhou et al., 2022, 2023). The model servermight be compromised and act as an adversary, aim-ing to obtain the users private data. It possessesthe knowledge of the pretrained model and is ca-pable of executing embedding inversion attacks,as demonstrated in (Kugler et al., 2021; Qu et al.,2021), on the embedding matrix of the fine-tunedmodel. Consequently, this enables the direct recov-ery of the plaintext tokens and the reconstructionof the users sensitive data.",
  "Description of schema": "In the schema, the user will apply key-based en-cryption on tokens of vocabulary and utilize glide-reflection to obfuscate embedding matrix of pre-trained model. The encrypted tokens and obfus-cated embeddings will be shuffled simultaneouslyso as the server cannot recognize user data accord-ing to token orders in inference process.In the token encryption step, every token v in thevocabulary V of pretrained model will be encryptedby Blake (Aumasson et al., 2013) with a user spe-cific key to generate an encrypted vocabulary.In the embedding obfuscation, suppose that theembedding matrix EdM contains M embeddingvectors {ei|i M}, ei Rd where d is the dimen-sion of embeddings. The glide-reflection appliedon ei can be formalized as:",
  "li li li + ti,(1)": "where li = 1 ai and ti = 1 bi are two vectorsconstructed by two random values ai, bi uniformlysampled from [0, 1)1.During runtime usage, the user is able to tok-enize its input data and request inference servicewith encrypted tokens. Then the server can use theencrypted vocabulary and the obfuscated embed-ding matrix to complete inference process.",
  "Proposed attack: EDNN": "The authors of (Mishra et al., 2024) have eval-uated the security of glide-reflection against thenearest neighbor (NN) attack, and the accuracyof token recovery for which turns out to be neg-ligible.By extending NN, we propose an effi-cient inversion attack called Element-wise Differen-tial Nearest Neighbor (EDNN) to break the glide-reflection. The EDNN selects the closest tokenfrom pretrained embeddings as the real token byutilizing the difference of vector elements for neigh-bor retrieval. Therefore, it is effect on the glide-reflection which does not change the element dif-ferences within embedding vector.",
  "return VR": "We present the details of EDNN in Algorithm 1,where EdM is the obfuscated embedding matrixafter fine-tuning. The algorithm will output a vo-cabulary VR which stores the recovered tokens cor-responded to the obfuscated embeddings EdM.To compute the element difference inside embed-ding vector, the algorithm use lshft() function tocyclically shift the vector to the left by one posi-tion and calculate element-wise subtraction. Thealgorithm will evaluate the distance between everypairs of plaintext and encrypted tokens. Then foreach encrypted token, the algorithm is able to out-put the plaintext token with the minimum distance",
  ":2D plot of 100 embeddings from:(a)element-wise difference of original pretrained model,(b) element-wise difference of transformed model after10 iterations of glide reflection": "in the embedding space as its substitute.To explain the attack effect of EDNN, we fitthe embeddings of 100 tokens into 2D plot byT-distributed Stochastic Neighbor Embedding (t-SNE) (Van der Maaten and Hinton, 2008) and scalethem into (1, 1) as shown in . By compar-ing the results of and 1b, we can observethat the element-wise differences inner each em-bedding vector from the original model and thetransformed model are the same.",
  "Experiment": "Experimental details. We encrypted the modelaccording to (Mishra et al., 2024) and fine-tune themodel with specific task. Then we evaluate EDNNon the fine-tuned model to recover its encryptedtokens.Datasets and models. We use the same settingas (Mishra et al., 2024) and conduct experimentson datasets including the General Language Under-standing Evaluation (GLUE) benchmark dataset(Wang et al., 2018), the CoNLL2003 Named EntityRecognition Dataset (Sang and De Meulder, 2003)and the XNLI dataset (Conneau et al., 2018) . Weuse BERT, RoBERTa, and mBERT models fromHuggingface2.Element-wise differential comparison. Foreach embedding obfuscated by glide-reflection, wefirst evaluate the distance of element-wise differ-ential to its corresponding original embedding andother nearest embedding. The results in shows that after fine-tuning, the element-wise dif-ferentials between each embedding and irrelevantembeddings exhibit a three-order-of-magnitude dis-crepancy compared to its original embedding, fa-cilitating the EDNN to capture the correspondencebetween obfuscated embeddings and their originalcounterparts. : Distance of element-wise differentials underGLUE SST2 dataset. The figure records the density ofdistance between every encrypted token and its corre-sponding plain token (blue) or other nearest token (red).",
  "dEd,d andb = b1, where Id,d": "is the diagonal matrix with all diagonal elements 1,Ed,d is the matrix with all elements 1, b is a randomnumber in [0, 1).The server cannot directly obtain from ()since it is not given W, b. Nevertheless, consideringthat the server is aware of the embedding matrixof the pre-trained model, as described in the threatmodel, it can carry out EIA if the transformed ma-trix fails to adequately obfuscate the informationrelated to the original matrix. Subsequently, wepropose the following two security requirementsfor the transformation.Fixed-point nonexistence. There should not ex-ist a probabilistic polynomial time (PPT) adversarywho is able to get an invariant of the transformer ,even without the total knowledge of W and b.Suppose there is a linear invariant map",
  "where A Mf,d(R). Then we have = . Itinduce a linear system about A such thatAW A = 0Ab = 0.(4)": "The linear system should not have trivial solu-tion. Otherwise, the adversary is able to decidewhether an obfuscated embedding e and an origi-nal embedding e are related by checking whether(e) = (e) holds even if shuffling is performedon the obfuscated embedding matrix.The glide-reflection is unable to securely obfus-cate embeddings, as it fails to satisfy the necessarysecurity requirement. Recall that the vulnerabil-ity of glide-reflection lies in the fact that it doesnot change the difference between any two ele-ments within each embedding vector.Withoutknowing the specific li and ti used in the equa-tion 2, the adversary can still construct the follow-ing matrix Add = to meet with the equation 4:",
  "Defenses": "To mitigate such security risks inherent in theparadigm, there exist two potential defense strate-gies. One approach involves the application ofdifferential privacy, wherein random noise is addedto the embeddings (Yue et al., 2021; Du et al., 2023;Shen et al., 2023). However, ensuring both privacyand model accuracy concurrently poses a signif-icant challenge to design elaborate noise mecha-nisms.Alternatively, leveraging cryptographic toolssuch as homomorphic encryption (Cheon et al.,2017) offers another avenue of defense. In thismethod, the embedding matrix is encrypted usinghomomorphic encryption techniques. To minimizecomputational overhead, the server can request de-cryption after processing several layers, allowingsubsequent layers to be processed in plaintext.",
  "Conclusion": "In this paper, we investigate the vulnerability ofthe glide-reflection used for embedding obfusca-tion. We devise an innovate embedding inversionattack to break the security of the glide-reflection.Furthermore, we conduct a comprehensive analysisand introduce two essential security requirementsfor embedding obfuscation. We explore varioustechniques that can be leveraged to enhance thesecurity of embedding obfuscation.",
  "Limitations": "We have summarized two limitations of this pa-per. (1) The EDNN attack method proposed inthe paper is to illustrate that the embedding ob-fuscation scheme based on glide-reflection is notsecure, but we have not tested the effectivenessof the EDNN attack against other embedding ob-fuscation schemes. (2) We present two securityrequirements for embedding obfuscation and webelieve they are necessary for protecting user data.However, we have not proposed a concrete schemeto verify the sufficiency of the aforementioned se-curity requirements. Jean-Philippe Aumasson, Samuel Neves, Zooko Wilcox-OHearn, and Christian Winnerlein. 2013. Blake2:simpler, smaller, fast as md5. In Applied Cryptogra-phy and Network Security: 11th International Con-ference, ACNS 2013, Banff, AB, Canada, June 25-28,2013. Proceedings 11, pages 119135. Springer. Jung Hee Cheon, Andrey Kim, Miran Kim, and Yong-soo Song. 2017. Homomorphic encryption for arith-metic of approximate numbers.In Advances inCryptologyASIACRYPT 2017: 23rd InternationalConference on the Theory and Applications of Cryp-tology and Information Security, Hong Kong, China,December 3-7, 2017, Proceedings, Part I 23, pages409437. Springer. Alexis Conneau, Guillaume Lample, Ruty Rinott, Ad-ina Williams, Samuel R Bowman, Holger Schwenk,and Veselin Stoyanov. 2018. Xnli: Evaluating cross-lingual sentence representations.arXiv preprintarXiv:1809.05053. Minxin Du, Xiang Yue, Sherman SM Chow, TianhaoWang, Chenyu Huang, and Huan Sun. 2023. Dp-forward: Fine-tuning and inference on language mod-els with differential privacy in forward pass. In Pro-ceedings of the 2023 ACM SIGSAC Conference onComputer and Communications Security, pages 26652679.",
  "Kai Kugler, Simon Mnker, Johannes Hhmann, andAchim Rettinger. 2021. Invbert: Reconstructing textfrom contextualized word embeddings by invertingthe bert pipeline. arXiv preprint arXiv:2109.10104": "Abhijit Mishra, Mingda Li, and Soham Deo. 2024. Sen-tinellms: Encrypted input adaptation and fine-tuningof language models for private and secure inference.In Proceedings of the AAAI Conference on ArtificialIntelligence, volume 38, pages 2140321411. Chen Qu, Weize Kong, Liu Yang, Mingyang Zhang,Michael Bendersky, and Marc Najork. 2021. Naturallanguage understanding with privacy-preserving bert.In Proceedings of the 30th ACM International Con-ference on Information & Knowledge Management,pages 14881497.",
  "Xiang Yue, Minxin Du, Tianhao Wang, Yaliang Li,Huan Sun, and Sherman SM Chow. 2021. Differ-ential privacy for text analytics via natural text saniti-zation. arXiv preprint arXiv:2106.01221": "Xin Zhou, Jinzhu Lu, Tao Gui, Ruotian Ma, Zichu Fei,Yuran Wang, Yong Ding, Yibo Cheung, Qi Zhang,and Xuan-Jing Huang. 2022. Textfusion: Privacy-preserving pre-trained model inference via token fu-sion.In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Processing,pages 83608371. Xin Zhou, Yi Lu, Ruotian Ma, Tao Gui, Qi Zhang,and Xuan-Jing Huang. 2023. Textmixer: Mixingmultiple inputs for privacy-preserving inference. InFindings of the Association for Computational Lin-guistics: EMNLP 2023, pages 37493762."
}