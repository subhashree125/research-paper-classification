{
  "Abstract": "The cross-cultural adaptation of recipes is an im-portant application of identifying and bridgingcultural differences in language. The challengelies in retaining the essence of the original recipewhile also aligning with the writing and dietaryhabits of the target culture. Information Retrieval(IR) offers a way to address the challenge becauseit retrieves results from the culinary practices ofthe target culture while maintaining relevance tothe original recipe.We introduce a novel taskabout cross-cultural recipe retrieval and presenta unique Chinese-English cross-cultural recipe re-trieval benchmark. Our benchmark is manuallyannotated under limited resource, utilizing variousretrieval models to generate a pool of candidateresults for manual annotation. The dataset pro-vides retrieval samples that are culturally adaptedbut textually diverse, presenting greater challenges.We propose CARROT, a plug-and-play cultural-aware recipe information retrieval framework thatincorporates cultural-aware query rewriting and re-ranking methods and evaluate it both on our bench-mark and intuitive human judgments. The resultsshow that our framework significantly enhances thepreservation of the original recipe and its culturalappropriateness for the target culture. We believethese insights will significantly contribute to futureresearch on cultural adaptation.",
  "Red Bean Soup": "English Recipe (GPT4 Generate)Ingredients: 1. 1 cup red beans2. 4 cups water3. 1/4 cup rice wine4. 1 piece fresh ginger (about 2 inches), unpeeled and sliced English Recipe (Recipe Retrieval) 1. 2 Tablespoons Olive Oil2. 1 Medium Onion3. 800 grams Drained Cooked Red Beans. 4. 1 liter Vegetable Stock. Chinese RecipeIngredients:1. Moderate amount of red bean 2. Moderate amount of rice wine 3. Moderate amount of ginger with skin : A cross-cultural recipe adaptation example.The GPT-4 adapted result (Cao et al., 2024), still havesome evident shortcomings like using rice wine andunpeeled ginger does not align with culinary practicesin English-speaking culture, while the retrieval providedsuitable results, includes substitutions of ingredients thatalign with local culture.",
  "miliarity of food products is positively associatedto sensory liking (Torrico et al., 2019)": "Recognizing and adapting cultural differencespresents both a significant importance and a chal-lenge (Hershcovich et al., 2022). Merely translat-ing recipes can lead to both semantic and culturalmismatches (Yamakata et al., 2017; Zhang et al.,2024). As shown in , even GPT-4, a pow-erful Large Language Models (LLMs) and a state-of-the-art (SOTA) model in cross-cultural recipeadaption (Cao et al., 2024), still makes obviousmistakes when adapting recipes from one cultureto another, e.g., the selection of ingredients andtools are not commonly used or the flavors do notalign with the preferences in the target culture. Wepropose to use Information Retrieval (IR) methodsto address the issue because compared to genera-tive models, retrieved recipes from a target culturecorpus naturally align more closely with the targetculture in flavor, ingredients and tools.",
  "Nevertheless, cross-cultural recipe IR is a chal-lenging task due to the existing linguistic and cul-tural gap between the source and target. Besides": "the challenges posed by the intrinsic gap betweendifferent languages (Zhang et al., 2022), an evenbigger challenge is the textual discrepancies causedby cultural differences in dietary habits, namingconventions, and food-related knowledge, whichcomplicate the task. We identify three non-trivialchallenges related to cross-cultural recipe retrieval: RelevanceAssessmentforCross-CulturalRecipes RetrievalDue to cultural variations iningredients, seasonings, and cooking methods,assessing the relevance of cross-cultural recipepairs is complex and challenging, needing clearguidelines to standardize relevance assessments. Culture-Aware Framework to Bridge CulturalGapsCurrent IR models lack awareness of thesignificant cultural gaps that exist across diverseculinary traditions, retrieving recipes that are textu-ally similar but actually quite different.1 Benchmark of Cross-Cultural Recipe RetrievalNo currently publicly available dataset2 can be usedas a benchmark to evaluate the performance ofdifferent retrieval models and to understand howcultural differences present challenges to our task.Our contributions to tackle these challenges are:3",
  ". We introduce the novel cross-cultural recipesretrieval task. We provide assessment guide-lines for cross-culture recipes relevance judge-ment, with specific criteria and examples": "2. We propose CARROT, a plug-and-playcultural-aware recipe IR framework, anddemonstrate that it offers better relevance com-pared to the results of previous retrieval mod-els and better consistency and cultural appro-priateness to the results generated by LLMson Chinese-English recipe cultural adaption. 3. Focusing on recipes in Chinese and English,we design and annotate a cross-cultural reciperetrieval dataset. It has many challenging sam-ples like cultural differences leading to signifi-cant textual discrepancies in matched recipes.",
  "See examples in .2Previous work used IR methods only to construct datasets,but these cannot serve as evaluation datasets for IR.3The code and dataset are available at": "tributes of culture while maintaining its originalmeaning, it involves common ground, values andaboutness (Hershcovich et al., 2022). Recipe adap-tation is an important application of cultural adap-tation, Liu et al. (2022) demonstrated that recipeadaptation is a challenging task. Although lan-guage models can generate fluent recipes, theystruggle to use culinary knowledge in a compo-sitional way, such as adjusting cooking actionsrelated to the changing ingredients.Palta andRudinger (2023) and Zhou et al. (2024) underscorethe complexity of integrating cultural understand-ing into LLMs, particularly in the culinary domain.Cao et al. (2024) propose the cross-cultural recipesadaptation task and show that prompting LLMsfor recipe generation is the SOTA method for thistask. They build a recipe adaptation dataset au-tomatically using an IR model to match recipes.However, their purpose is not to propose a novel IRmodelan off-the-shelf standard IR model is usedand is not evaluated with respect to the retrievaltask. Recipe RetrievalWorks in recipe retrieval pri-marily focus on cross-modal recipe retrieval (Lienet al., 2020; Salvador et al., 2021), retrievingrecipes by both text and images.Takiguchiet al. (2021) introduce a recipe retrieval modelfor Japans largest recipe sharing service. Theirmodel is trained and evaluated with online searchlogs. These works are not primarily aimed at cross-cultural scenarios, and they use online behaviorlogs as datasets, whereas our work requires the useof manually annotated samples. LLMs for Information RetrievalThe emer-gence of LLMs has profoundly impacted IR due totheir remarkable abilities in language understand-ing. LLMs for query rewriting have been widelyapplied to various retrieval issues which have vo-cabulary mismatches between queries and docu-ments (Zhu et al., 2023). For example, Tang et al.(2023) propose a prompt-based input reformula-tion method to tackle the problem of inputs in le-gal case retrieval that often contain redundant andnoisy information. LLMs are also widely used forreranking. Even without fine-tuning, they havebeen proven to possess strong ranking capabilities(Zhu et al., 2023), even superior to state-of-the-art supervised methods on popular IR benchmarks(Sun et al., 2023). We adapt the existing work forcross-cultural recipe retrieval to address the unique",
  "Cross-Cultural Recipe Retrieval Task": "We define the task of cross-cultural recipe retrievalwith the source recipe as query and recipes from thetarget culture as documents. For a pair consistingof different cultural recipes (q, d), which representsa pair of one query and one document, we assessrelevance with a three-point scale: 0 (Not Match),1 (Partial Match), and 2 (Exact Match), the three-point scale levels are a common choice for rele-vance assessment (Keklinen and Jrvelin, 2002).For Exact Match recipes, the differences shouldnot exceed the necessary range of cultural adapta-tion, such as making local adjustments with simi-lar ingredients and flavors according to the targetculture. Partial Match recipes have similarities insome aspects of ingredients and flavors, offeringreference value. They should be in the same dishcategory (e.g., main courses, desserts, beverages).If the above conditions are not met, the two recipeswill be deemed Not Match. We provide specificcriteria and examples of relevance assessment inthe Appendix A.We briefly summarize three main challenges inthe cross-cultural recipe retrieval task: C1: Is Recipe Title the Best Retrieval Query?Semantic Gaps Caused by Cultural DifferencesThe recipe title is often used as the query (Caoet al., 2024) to retrieve recipes from the target cul-ture, as the title usually encapsulates the essence ofthe recipe. However, due to language and culturaldifferences, it forms a semantic gap between thesource and target recipe titles in different cultures.These differences include: Naming Conventions Recipesaretypicallynamed after the main ingredients and cookingmethodsinEnglish-speakingcultures,whereas Chinese cuisine may name dishesafter the inventor or origin city, such as KungPao Chicken. Culinary Cultures Cultural differences requiresubstituting original ingredients and cookingmethods with more locally common alterna-tives. These changes are also reflected in tex-tual variations between recipe titles. For in-stance, Stir-fried Taro4 could be adapted toStir-fried Potatoes.",
  "Taro is a staple root vegetable in Chinese cuisine, notreadily available in Western countries": "Food-related Common Sense Recipes implicitlycontain food-related knowledge that might becommon in one culture but unknown in an-other, e.g., in Chinese cuisine, (lit-erally, Three Fresh Ingredients in the Earth)refers to a dish made with potatoes, eggplants,and green peppers. The specific ingredientsrepresented here are cultural common sensein China but may be challenging for users inother cultures.",
  "CARROT: A Cultural-Aware RecipeRetrieval Framework": "We propose a framework CARROT: Cultural-Aware Recipe Retrieval Tool, as shown in ,a plug-and-play model combining prompt-basedLLMs and IR methods, to address the additionalchallenges posed by cultural differences.5 Specif-ically, to address C1 in , we introducedquery rewriting by LLMs. To address C2, we in-troduce a plug-and-play framework (no additionalfine-tuning required). To address C3, we design anadditional re-ranking stage. Query ProcessingProcessing can be dividedinto translating the query and rewriting the query.The task differs from a general recipe search be-cause the query is not a user-written set of key-words, but a source recipe and title serves as a goodsummary of the relevant content for the search. Sofor a Chinese recipe, we first automatically trans-late its title into English as the original query. Wealso utilize LLMs for two rewriting tasks. Both therewritten and original queries are used for retrievalto further enhance the systems robustness, as eachquery may experience some semantic errors.",
  "The prompt used here is shown in Appendix B": ": Framework of CARROT, including three stages: Using LLMs for query rewriting, retrieval and Re-rankingbased on cultural adaptability and relevance. Different queries will use different embeddings for retrieval and obtaindifferent retrieval result lists. They will be merged during the re-rank stage. regenerate a title. We believe such generated titlescan eliminate interference caused by inappropri-ate original titles, e.g., users may submit attention-grabbing but non-standard recipe titles, or titlesthat use personal names or historical references. Recipe Title Cultural Adaption TaskWe alsoprompt LLMs to directly rewrite an English recipetitle based on the Chinese recipe title, making itmore in line with the writing conventions of recipesin the target culture. RetrievalConsidering millions of recipes in thetarget culture, we choose a bi-encoder structure toefficiently retrieve the recipes of the target culture.We perform retrieval for each query individually,retaining the top 10 results of each query. Re-rankingA complex re-ranking model canbetter understand the implicit culinary culturalknowledge and be more effective, considering fac-tors of cultural matching in ranking. We promptLLMs to rank the results based on relevance andprioritize recipes that are more aligned with thetarget culture when the relevance level is the same.Considering the potential issues of using LLMs asunsupervised rerankers, such as limitations in con-text length and more positional bias compared totraditional models (Zhu et al., 2023), we avoidedranking the retrieval results at once. Instead, weperformed multiple rounds of ranking or combinedLLMs with other rerankers (Xiao et al., 2023).",
  "Recipe Corpora": "We source recipes from two monolingual corpora:RecipeNLG (Bien et al., 2020) and XiaChuFang(Liu et al., 2022). RecipeNLG has over two mil-lion English cooking recipes and XiaChuFang con-sists of more than 1.5 million Chinese recipes froma Chinese recipe website.6 We use the title, in-gredients, and cooking steps from each corpus.These two corpora are independent and monolin-gual. Therefore, we use the Chinese recipe corpusas the source and annotate the relevance of recipesfrom the English corpus.",
  "Dataset Construction": "Our work draws inspiration from the Cultural-Recipes Dataset (Cao et al., 2024), which, however,lacks an evaluation of the retrieval methods andrelies on a single method. This introduces potentialbiases to the dataset, omitting difficult-to-recall pos-itive examples and challenging negative examples,which are vital for robust IR (Zhan et al., 2021).Another challenge is the limitation of annotatedresources. The corpora in .1 contain mil-lions of recipes, the majority of which are irrelevantfor a given query.To address these gaps, we devise manually an-",
  "Baseline": ": Case Study with two examples, comparing our framework (CARROT) with the baseline (machinetranslation and MPNet). In the first example, sin chew, refers to Singapore, denotes a curry flavor style and ricenoodles are not commonly found in Western countries, the translated query changes it to rice powder, a semanticallysimilar but distinctly different food, while our framework solves these two issues using curry and noodles to adaptthe recipe. In the second example, twice-cooked pork is a unique Chinese dish containing specific knowledge. Thetranslated query back to the pot is literally similar but does not describe the flavor and ingredients. Our frameworkuses the ingredients pork & pepper and cooking methods to explain the dish, making it more conducive to retrieval. notated samples instead of automatically matchedsamples and create a candidate pool by multipleretrieval methods for annotation. We randomlypick source recipes from Chinese recipe corporaand build a candidate pool by target culture recipescorpora using multiple retrieval methods. We ran-domly select recipe samples for manual annotationwithin the candidate pool. We present statisticalinformation about the dataset in . For about83.7% of the queries, the dataset provides at leastone document that is an exact match.The dataset is independently annotated by twovoluntary annotators whose native language is Chi-nese and who are fluent in English. They are alsofamiliar with the culinary practices of both Chineseand English-speaking cultures. The annotators fol-low the instructions in Appendix A. Build Candidate PoolWe employ a depth-10pooling strategy to annotate the dataset, which is astandard procedure in IR (Pavlu and Aslam, 2007).Compared to random sampling, using a poolingstrategy provides more relevant rather than ran-domly irrelevant samples. Additionally, comparedto annotating the dataset using results from a singleretrieval method, the datasets sources are morediverse and less biased, enhancing the reusabilityof the dataset. The depth is set to 10 based on thetrade-off between the reusability of the dataset and",
  "lated to English as query for two indepen-dent SOTA vector-based retrieval models, MP-Net sentence-transformer (Song et al., 2020;Reimers and Gurevych, 2019) and ColBERT(Khattab and Zaharia, 2020)": "Content-Based Retrieval Compared to only us-ing the titles in the basic method, consideringincompleteness of information in titles, wealso use the content-based retrieval by titleappended with ingredients.7 Multilingual Retrieval We also use multilin-gual sentence-BERT model (Reimers andGurevych, 2020a) to retrieve instead of trans-lating the query.We directly use untrans-lated Chinese recipe titles to retrieve Englishrecipes.",
  "Metrics": "IR EvaluationWe use common metrics inIR, including nDCG@10, Precision@10 (P@10),Precision@1 (P@1), Recall@10 (R@10), andmAP@10(mAP@10).8 Different IR metrics cancontribute to the results in various ways, Precisionensures that the most relevant recipes appear at thetop, while NDCG evaluates the overall quality andorder of the list. Recall is crucial for capturingall relevant options, providing flexibility for fur-ther refinement of recipe rankings based on usersspecific dietary preferences. These comprehensivemetrics offer references for various downstreamapplications of recipe retrieval.Due to limited annotation resources and the pool-ing strategy, our annotations are incomplete. Fol- 7We do not use cooking steps because they are too lengthyand contain little information useful for retrieval.8In Precision, Recall and mAP, only exact matches areconsidered relevant results while partial matches are treatedas irrelevant results. lowing previous work (Sakai and Kando, 2008), in.3, we only present results for evaluationwith condensed lists (non-labelled samples are dis-carded). Additionally, we include evaluation withfull lists (non-labelled samples are considered non-related) results in the Appendix D. The conclusionsof the two experiments are similar. Recipe Adaptation EvaluationWe evaluate theIR results using metrics from the recipe culturaladaptation task (Cao et al., 2024) to obtain end-to-end adaptation performance and directly comparethe results with those generated by LLMs. We firstuse reference-based automatic metrics. Since theseare not always reliable for subjective tasks, we alsoperform manual evaluation method with a 7-scalerating in four different aspects. Based Automatic EvaluationToevaluate the similarity between the retrieved andreference recipes, we use three overlap-basedmetrics:BLEU (Papineni et al., 2002), ChrF(Popovic, 2015), ROUGE-L (Lin, 2004) and onerepresentation-based metric: BERTScore (Zhanget al., 2019). Human EvaluationThe same annotators as in.2 perform manual evaluation on four cri-teria in cross-cultural recipes adaptation, adoptedfrom Cao et al. (2024):Grammar (GRA): The results are grammaticallycorrect and fluent.Consistency (CON): The results include a com-plete and detailed title, ingredients and steps, facil-itating users to cook according to the recipe.Preservation (PRE): The results retain the origi-nal ingredients and flavors of the source recipe.Cultural Appropriateness (CUL):The resultsconform to the dietary habits and recipe writingconventions of the target culture.Each dimension is rated on a 7-point scale anda higher score indicates superior performance. Inaddition, we also annotate the 3-scale relevanceof recipe retrieval results and computed the Exactmatch precision at the first position (P@1).We use Krippendorffs alpha (Vogel et al., 2020)to measure the annotation agreements, which re-sults in 0.79, 0.65, 0.61, 0.82, 0.42 for RelevanceScore, Grammar, Consistency, Preservation, andCultural Appropriateness respectively, indicatingsubstantial agreement between the annotators onmost aspects, but a high degree of subjectivity inthe understanding of Cultural Appropriateness.",
  "CARROT (Rewriting + Re-ranking)CARROT-Llama30.34637.0525.9715.7115.31": ": Evaluation on the cross-cultural recipe retrieval dataset, higher scores indicate better performance on allmetrics. Please refer to .2 for details on the basic retrieval model, and for query rewrite and re-rank insection 4. Bold indicates the best performance across all method, underlined indicates best performance across allbasic retrieval model. The results show both recipe title cultural adaptation and re-ranking improve relevance.",
  "Experimental Setup": "We represent a recipe as a concatenation of title,ingredients and steps. For constructing the cross-cultural recipe retrieval dataset, we translate Chi-nese recipe to English by opus-mt models (Tiede-mann and Thottingal, 2020), and retrieve Englishrecipes by MPNet sentence-transformer (Song et al.,2020) and ColBERT (Santhanam et al., 2021; Khat-tab and Zaharia, 2020). We also explore multilin-gual sentence-transformer (Reimers and Gurevych,2020b). In the CARROT framework, we set MPNetas the default retrieval model. We explore the per-formance of using only re-ranking or using only aspecific type of query rewriting and various LLMswhich are trained on both Chinese and English toenhance the performance of the framework. Thesemodels include: Llama3-7B (AI@Meta, 2024),Qwen1.5-7B (Bai et al., 2023) and BAICHUAN2-7B(Baichuan, 2023), the leading Chinese open-sourceLLMs models9 and among them Llama3 is cur-rently the best-performing Chinese LLMs under10B parameters. All the above models are run withdefault hyper-parameters.",
  "Experimental Results": "Information Retrieval Results showsthe results on cross-cultural recipe retrieval datasetin . Within the basic retrieval models,the Sentence-transformer based on translatedtitles achieved best overall performance, it is alsothe reason we use MPNet as the default retrievalmodel in the CARROT framework. We can find thecultural adaptation rewriting shows better relevanceperformance compared to translated titles, whichproves Chinese recipe titles are not entirely suitablefor the naming conventions of English recipes, aswell as the effectiveness of the rewriting approach.The CARROT-Llama3 achieve the best performanceon nDCG, R@10, P@1, P@10 and the second bestperformance on mAP@10, demonstrates the strongperformance of our framework in this task. Recipe Adaptation Results shows theperformance on reference based automatic evalu-ation and human evaluation. We find that genera-tion methods outperform retrieval methods on theROUGE-L, BertScore, P@1, Preservation metrics,indicating that the generation method has betterrelevance and is more faithful to the source recipes,while retrieval methods achieved better results inConsistency and Cultural Appropriateness.10 TheKendall correlation between P@1 relevance met-ric and Preservation is 0.73, which indicates thatPreservation can also effectively reflect the rele-",
  "*Llama3-Generation19.6040.26*32.10*66.41*1.05.925.176.04*5.0": ": Automatic and Human Recipe Adaptation Evaluation on CulturalRecipes Dataset: the first four metricsautomatically calculated based on reference and the next five metrics are evaluated by human, higher scores indicatebetter performance on all metrics. We set MPNet as retrieval model here. Bold indicates best performance across allretrieval models, and underlined indicates that the generative model outperformed the best retrieval models in thismetric. Better results than Baseline with significance difference for p < 0.05 by t-test is indicated by *. It showsgeneration methods outperform in relevance while retrieval is better in consistency and cultural appropriateness. vance between the results and the source recipes.Within the retrieval methods, compared to thetranslated title, both query rewriting methods andre-ranking significantly improved relevance relatedmetrics. The CARROT framework with Llama3outperforms CARROT with the other two ChineseLLMs, Qwen and Baichuan, highlighting the strongperformance of the Llama3 model on cross-lingualtasks. The CARROT-Llama3 achieved the best per-formance on ROUGE-L, BertScore, P@1, Preser-vation and Consistency metrics and near-optimalperformance on Cultural Appropriateness metricswithin the retrieval methods. It demonstrates thestrong performance of our framework in the cross-cultural recipe adaptation task. Case StudyWe select some cases to intuitivelycompare the result of using the CARROT frame-work versus the baseline, just using the translatedrecipe title and a bi-encoder MPNet model (Songet al., 2020), shown in . The results showsmachine translation title used as a query can leadto irrelevant search results due to cultural differ-ences, but our CARROT framework addresses thisissue by changing the way recipes are named andsubstituting ingredients.",
  "Discussion": "The previous SOTA generation method in the taskof cross-cultural recipe adaptation shows better rel-evance. However, retrieval methods are superiorin consistency and cultural appropriateness. Ourwork is the first to highlight the potential issues in using LLM-generated content for recipes, as wellas the potential advantages of using IR methodsfor cultural adaptation. We will illustrate throughspecific examples how retrieval methods may haveadvantages over generation methods in these as-pects. ConsistencyConsistency mainly reflects thequality and reliability of the recipes, which de-termines whether people can successfully cookaccording to such recipes. The recipes retrievedare based on real human culinary practices, butrecipes generated by LLMs, despite being textuallyclose to user created recipes, still contain halluci-nations, leading to not truly instructive texts forhuman cooking. For example, Llama3 generatesthe cooking steps of Braised Beef with Potato as:",
  "The discarding in the final step does not align withgeneral culinary understanding and this issue doesnot exist in the retrieval results": "CulturalAppropriatenessThegenerationmethod tends to preserve the original flavors,making only necessary changes such as mea-surement units. In contrast, the retrieval-basedmethod makes more substantial modifications tothe ingredients and flavors to better adapt to theculture. For example, for Salted Baked Chickenwould be adapted to Salt-Rubbed Roast Chickenwith Lemon & Thyme with the addition of lemonand thyme to better suit local preferences. DiversityThe retrieval models can find resultswith significant differences in ingredients and fla-vors, providing a broader range of references. Forexample, there are more than 5 different main in-gredient combinations in the recipe red bean souptop 10 retrieval results by the CARROT frame-work, with manually highlighted specific ingredi-ents used. 1.Dried Red Kidney Beans, Butter, Onion2.Drained Cooked Red Beans, Olive Oil, Onion3.Red Beans,Pork,Sprig of Thyme, Canned Tomato4.Canned Red Kidney Beans, Garlic Bud, Sausage5.Red Kidney Beans, Celery Stalk, Onion, Carrot",
  "Conclusion": "In this paper, we propose a novel task of cross-cultural recipe retrieval, we have manually an-notated a challenging and representative bench-mark.Furthermore, we introduce CARROT, acultural-aware recipe retrieval framework that uti-lizes LLMs to rewrite and re-rank, thereby bridg-ing the cultural differences in recipes between twodistinct cultures. Our approach has robust perfor-mance on both our proposed dataset and culturalrecipe adaption dataset. We also discuss the advan-tages of using IR methods for cultural adaptationof recipes versus direct generation using LLMs.We believe our work offers a new perspective oncultural adaptation.",
  "Our study presents a benchmark and framework forcross-cultural recipe retrieval, but we acknowledgecertain limitations within our study, which maywarrant further exploration:": "Large scale manual evaluationWhile our studyconducts a small-scale benchmark to evaluate theperformance of IR models, the small-scale datasetlimits the accuracy of evaluating some IR meth-ods, especially those that significantly differ fromthe dataset constructed in our work. In an idealscenario, the benchmark necessitates a large-scalehuman evaluation of different backgrounds and cul-tures. Such a large-scale benchmark would provechallenging owing to the significant resources toachieve. Coverage of recipes from different culturesAl-though we believe that our proposed frameworkcan be extended to other languages and culturalbackgrounds, due to limitations in resources andthe background of annotators, we conducted our research using only the Chinese-English example.Ideally, the benchmark and experiments could beextended to include other languages and culturalbackgrounds.Studying other culinary culturesmight also bring new inspiration to our methods. Fine-tuning the retrieval modelDue to limi-tations in annotation resources, we directly usedthe current popular retrieval models without fine-tuning them. Recipe retrieval is a specialized taskthat requires retrieval models to learn language andknowledge in the food domain. Therefore, ideally,collecting relevance data specific to recipes andfine-tuning the models would enhance the overallperformance of the framework.",
  "Acknowledgements": "This research was co-funded by the Villum andVelux Foundations Algorithms, Data and Democ-racy (ADD) grant. Thanks to the anonymous re-viewers and action editors for their helpful feed-back. The authors express their gratitude to YongCao for his assistance with the dataset collectionprocess. We also extend our sincere gratitude toJingyi Zheng for his valuable feedback on the pa-per.",
  "Ken Albala. 2012. Three World Cuisines: Italian, Mexi-can, Chinese. Rowman Altamira": "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, FeiHuang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,Xuancheng Ren, Chuanqi Tan, Sinan Tan, JianhongTu, Peng Wang, Shijie Wang, Wei Wang, Sheng-guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-uan Zhang, Yichang Zhang, Zhenru Zhang, ChangZhou, Jingren Zhou, Xiaohuan Zhou, and TianhangZhu. 2023. Qwen technical report. arXiv preprintarXiv:2309.16609.",
  "Andrea Borghini. 2015. What is a recipe? Journal ofAgricultural and Environmental Ethics, 28:719738": "Yong Cao, Yova Kementchedjhieva, Ruixiang Cui, An-tonia Karamolegkou, Li Zhou, Megan Dare, LuciaDonatelli, and Daniel Hershcovich. 2024. CulturalAdaptation of Recipes. Transactions of the Associa-tion for Computational Linguistics, 12:8099. Daniel Hershcovich, Stella Frank, Heather Lent,Miryam de Lhoneux, Mostafa Abdou, StephanieBrandl, Emanuele Bugliarello, Laura Cabello Pi-queras, Ilias Chalkidis, Ruixiang Cui, ConstanzaFierro, Katerina Margatina, Phillip Rust, and AndersSgaard. 2022. Challenges and strategies in cross-cultural NLP. In Proceedings of the 60th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 69977013,Dublin, Ireland. Association for Computational Lin-guistics.",
  "Rodrigo Nogueira,Wei Yang,Jimmy Lin,andKyunghyun Cho. 2019.Document expansion byquery prediction. arXiv preprint arXiv:1904.08375": "Shramay Palta and Rachel Rudinger. 2023. Fork: Abite-sized test set for probing culinary cultural biasesin commonsense reasoning models. In Findings ofthe Association for Computational Linguistics: ACL2023, pages 99529962. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalu-ation of machine translation. In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics, pages 311318, Philadelphia,Pennsylvania, USA. Association for ComputationalLinguistics.",
  "Yanran Tang, Ruihong Qiu, and Xue Li. 2023. Prompt-based effective input reformulation for legal case re-trieval. In Australasian Database Conference, pages87100. Springer": "Jrg Tiedemann and Santhosh Thottingal. 2020. OPUS-MT building open translation services for the world.In Proceedings of the 22nd Annual Conference ofthe European Association for Machine Translation,pages 479480, Lisboa, Portugal. European Associa-tion for Machine Translation. Damir Dennis Torrico, Sigfredo Fuentes, Claudia Gon-zalez Viejo, Hollis Ashman, and Frank R Dunshea.2019. Cross-cultural effects of food product familiar-ity on sensory acceptability and non-invasive phys-iological responses of consumers. Food researchinternational, 115:439450.",
  "Shitao Xiao, Zheng Liu, Peitian Zhang, and NiklasMuennighoff. 2023. C-pack: Packaged resourcesto advance general chinese embedding": "Yoko Yamakata, John Carroll, and Shinsuke Mori. 2017.A comparison of cooking recipe named entities be-tween japanese and english. In Proceedings of the9th Workshop on Multimedia for Cooking and EatingActivities in conjunction with The 2017 InternationalJoint Conference on Artificial Intelligence, pages 712. Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, MinZhang, and Shaoping Ma. 2021. Optimizing denseretrieval model training with hard negatives. In Pro-ceedings of the 44th International ACM SIGIR Con-ference on Research and Development in InformationRetrieval, pages 15031512. Fuwei Zhang, Zhao Zhang, Xiang Ao, Dehong Gao,Fuzhen Zhuang, Yi Wei, and Qing He. 2022. Mindthe gap: Cross-lingual information retrieval with hi-erarchical knowledge enhancement. In Proceedingsof the AAAI Conference on Artificial Intelligence,volume 36, pages 43454353.",
  "Zhonghe Zhang, Xiaoyu He, Vivek Iyer, and AlexandraBirch. 2024. Cultural adaptation of menus: A fine-grained approach. arXiv preprint arXiv:2408.13534": "Li Zhou, Taelin Karidi, Nicolas Garneau, Yong Cao,Wanlong Liu, Wenyu Chen, and Daniel Hershcovich.2024. Does mapo tofu contain coffee? probing llmsfor food-related cultural knowledge. arXiv preprintarXiv:2404.06833. Yutao Zhu, Huaying Yuan, Shuting Wang, JiongnanLiu, Wenhan Liu, Chenlong Deng, Zhicheng Dou,and Ji-Rong Wen. 2023. Large language modelsfor information retrieval: A survey. arXiv preprintarXiv:2308.07107.",
  "CARROT (Rewriting + Re-ranking)CARROT-Llama30.20235.6922.9313.8713.51": ": Evaluation on the cross-cultural recipe retrieval dataset with full lists (non-labelled samples are considerednon-related), higher scores indicate better performance on all metrics. Please refer to .2 for details on thebasic retrieval model, and for query rewrite and re-rank in section 4. Bold indicates the best performance across allmethod, underlined indicates best performance across all basic retrieval model.",
  "Source recipes title is Thai Green Curry thena curry with Japanese flavors would not be anexact match": "Criteria of Partial MatchPartially matchedrecipes are not fully similar to the source dish, butthey are of referential value to the user and canprovide some inspiration.If two recipes have similar ingredients or flavors,and the differences between the two recipes do notexceed the scope that can provide referential value.they can be considered a partial match. The scopethat can provide referential value refers to recipesbelonging to the same category (for example, maincourses, desserts, beverages, etc.). Although Mapo Tofu(Spicy Tofu) and chili concarne have different ingredients, their flavorsare similar. Users can refer to the preparationprocess of spicy sauce when making chili porksauce, therefore, they are considered a partialmatch. Although chicken curry and Tuscan chickenstew have different flavors, their main ingre-dients are consistent. They are consideredpartially related because other stewed chickenrecipes can also provide certain references tousers. Criteria of Not MatchingIf two recipes neithermeet the criteria for an exact match nor the criteriafor a partial match, then they should be consideredas not matchingFor example, the differences between rice pud-ding and streamed rice are too significant to of-fer valuable references, so they are considered notmatching each other.",
  "B.3Task C: Recipe Re-ranking": "GivenaChineserecipeandsomeEnglishrecipes, assess their relevance, and rank themin the order of relevance. When the relevanceis the same, prioritize recipes that are morealigned with the culture of English speakers.[Relevance Instructions]: In Appendix A[Chinese recipe][English recipe_1]...[n][English recipe_n](For Top1 Instruction): Select the identifierof the most relevant English recipe(Ranking Instruction): Listed the identifiersin descending order of relevance"
}