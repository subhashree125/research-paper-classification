{
  "Abstract": "Large language models (LLMs) have demon-strated the potential to mimic human social in-telligence. However, most studies focus onsimplistic and static self-report or performance-based tests, which limits the depth and validityof the analysis. In this paper, we developed anovel framework, INTERINTENT, to assessLLMs social intelligence by mapping theirability to understand and manage intentions ina game setting. We focus on four dimensions ofsocial intelligence: situational awareness, self-regulation, self-awareness, and theory of mind.Each dimension is linked to a specific gametask: intention selection, intention following,intention summarization, and intention guess-ing. Our findings indicate that while LLMsexhibit high proficiency in selecting intentions,achieving an accuracy of 88%, their ability toinfer the intentions of others is significantlyweaker, trailing human performance by 20%.Additionally, game performance correlates withintention understanding, highlighting the im-portance of the four components towards suc-cess in this game. These findings underline thecrucial role of intention understanding in evalu-ating LLMs social intelligence and highlightthe potential of using social deduction gamesas a complex testbed to enhance LLM evalua-tion. INTERINTENT contributes a structuredapproach to bridging the evaluation gap in so-cial intelligence within multiplayer games.1",
  "Introduction": "The growing intelligence of large language models(LLMs) has facilitated diverse research on their ca-pability to mimic human social intelligence (Ziemset al., 2024; Dubova, 2022; Gweon et al., 2023;Huang et al., 2023; Sap et al., 2022; Wu et al.,2023). Social intelligence, the ability to understandand manage ones own and others actions and to",
  "* Equal contribution.1Code is available at": "act wisely in social relations (Thorndike, 1920),usually includes four key components (Silveraet al., 2001): (1) Situational Awareness: The per-ception and comprehension of the elements in theenvironment (Endsley, 1995). (2) Self-Regulation:The process of guiding ones own thoughts, be-haviors, and feelings to reach goals (Bandura,1991). (3) Self-Awareness: The understanding ofones own character, feelings, motives, and de-sires (Gallup et al., 2003). (4) Theory of Mind(ToM): The knowledge about others beliefs, inten-tions, and thoughts (Baron-Cohen, 1991).Despite adaptations for interactive social envi-ronments (Zhou et al., 2023c,a; Park et al., 2023),most existing evaluations on LLMs (Le et al., 2019;Shapira et al., 2023) focus on straightforward andstatic daily scenarios without explicitly definedgoals for them. To enhance the contexts with com-plexity and dynamics with clear goals for LLMs,we turn our attention to social deduction games.These games require social interactions amongplayers, providing a more diverse test bed for LLMevaluation (Qiao et al., 2023). Existing studieshave successfully utilized various games to analyzeLLMs social behaviors, such as deception andconfrontation (Liang et al., 2023; Ibraheem et al.,2022; Mansbach et al., 2021; Meta et al., 2022;OGara, 2023; Xu et al., 2023a,b; Wu et al., 2024a).However, these studies often focus on ad-hoc post-analysis of game performance and overlook system-atic measurement of social intelligence, limitingthe comprehensive understanding of LLMs capa-bilities in social environments.In this paper, we leverage one of the representa-tives in social deduction games, Avalon (Light et al.,2023a; Wang et al., 2023), as the context for LLMevaluation. Avalon is a social deduction game thatrelies on conversation, making it an ideal test bedwith its goal-driven objectives and complex mecha-nisms, facilitating interactions among LLMs in agrounded environment. We systematically evaluate",
  "Game Context": "Round:2Previous round summary: All players vote \"agree\" to the team proposal including Player 1 and Player 4 and the quest is successful.Roles: Servant does not have any information; Merlin knows who are evil players but they cannot reveal their identity ...Current round discussion:Player 2: I propose a team including Player 1, Player 2, and Player 3. Player 1 shows their loyalty in the last quest and I can promiseyou I am loyal to the king of Arthur! Player 3 hasn't proved themselves in the quest yet and let's give them a chance!",
  "Player 3(Servant)": ": Four dimensions to assess social intelligence in Avalon. We provide a dynamic and complex gamingcontext for evaluations. For situational awareness, we provide both positive and negative examples. In the negativeexample, since the previous quest was successful and no player was in a failed quest, the intention is inappropriateas there was no failed quest. For self-regulation, we require models to provide explicit information rather thanrepeating the intentions. Intentions are in bold within the contexts. LLMs social intelligence across the four afore-mentioned dimensions. Specifically, we propose tofocus on players Intentions (Malle et al., 2001) atvarious phases of the game, since comprehensionof ones and others intentions plays a critical rolein games (Goodie et al., 2012). While previousresearch has explored intention detection (Zhanget al., 2020; Casanueva et al., 2020), the field re-mains largely underexplored in game contexts. Asshown in , we design four tests: (1) Inten-tion Selection, (2) Intention Following, (3) Inten-tion Summarization, (4) Intention Guessing, respec-tively for the four social intelligence dimensions. We first develop a novel Avalon gaming frame-work enhanced with an intention-guided mech-anism, drawing inspiration from previous stud-ies (Lan et al., 2023; Shi et al., 2023; Wang et al.,2023; Light et al., 2023b). Building on this, weintroduced INTERINTENT, which is designed todynamically generate contexts for evaluating socialintelligence through LLMs intention understand-ing. With human annotation, our results reveal thatGPT-3.5 and GPT-4 can select intentions correctly,with accuracies of 87.5% and 88.8%, respectively. GPT-3.5 achieves a 69.5% F1 score in intentionsummarization, with GPT-4 reaching 83.8%, sur-passing human performance. Furthermore, we finda strong correlation between high levels of inten-tion comprehension and superior game win rate,highlighting the importance of the intention under-standing components in enhancing LLMs gamingproficiency. However, the models fall short in in-tention following, with only 47.5% and 64.8% ofresponses rated as Good in human evaluations,and underperform in intention guessing, trailinghuman performance by 20%.",
  "Extensive experiments on various LLMsreveal their underperformance in intentionguessing against human performance": "First orderPlayer1: Role: MerlinIntent: Support one loyalplayerConfidence: 4Evidence: They support loyalplayers.... Intention selection Selected intentions:support teammate, castsuspicion on innocentplayers Thinking and speakingThinking: I need to protectMorgana and share concerns abouta loyal playerSpeaking: I believe player4 istrustworthy and I want to mentionplayer1 change their statementwhich seems suspicious Second orderPlayer1 might think I am a evilplayer if they are Merlin Player2 might think I am a loyalplayer cause I am helping loyalplayers...... Intent modification Selected new intentions:support teammate,support team proposal Thinking and speakingThinking: I need to protect myteammate and support the teamproposal this round.Speaking: I think player4 istrustworthy and I support theteam proposal since player4 is inthe team. Summary of previous rounds Leader reconsider the proposal",
  "LLM-Based Avalon Game": "Avalon is a social deduction game where playersare either loyal or evil, aiming to succeed in or sab-otage quests through strategy, persuasion, deduc-tion, and deception. The primary goal is for loyalplayers to complete quests, while evil players aimto fail them.2 Leveraging the publicly availableprompts proposed by Wang et al. (2023), whichincludes first-order and second-order reasoning be-fore speaking, we build a framework allowing fiveto ten players in gameplay (we set the players num-ber as five in this paper which includes 2 evil play-ers and 3 loyal players). As shown in ,each game round consists of five principal compo-nents: summarizing previous rounds, discussingstrategies, reconsidering the team proposal by theleader, voting, and executing the quest. Notably,we introduce an additional component, the leadersreconsideration phase as a critical, previously un-explored component in Avalon literature.",
  "The detailed rules of Avalon can be found in": "with commitment. This perspective expands in-tention beyond a mere mental state to include adiscernible commitment to act with purpose. Inline with Wang et al. (2023) and Xu et al. (2023a),we identify seven categories of intentions in LLMs:Interrogation, Defense, Confrontation, Conceal-ment, Deception, Persuasion, and Teamwork. Ini-tially, we derive a list of intentions from our directexperiences with the game of Avalon. We thenutilize GPT-3.5 to extend these initial categoriza-tions. Human annotators subsequently refine thisexpanded list, removing any intentions deemed un-reasonable or redundant, thus producing a conciseset of relevant intentions. This refinement is itera-tive, resulting in the final enumeration detailed inAppendix Sec. B.1.",
  "Intention-Guided Game Playing": "We introduce an intention-guided gameplay mech-anism that integrates our defined intentions withinthe game environment, offering two main bene-fits. First, it enhances the performance of LLMsby focusing on explicit intention discussions. Sec-ond, it facilitates the evaluation of the four socialintelligence components.As depicted in , the discussion phaseinvolves several key steps to facilitate intentionalinteraction among players. Initially, players use afirst-order prompt for deductive reasoning abouteach players role and intentions. Subsequently,players select two or three intentions from a pre-defined set according to the current game context.This selection process guides players in generatingtheir thoughts and statements based on the chosen",
  "I suggest the leader reconsider including Player 1as they are in a failed quest so we cannot be sureof their loyalty": ": Criteria for annotating intention following (speaking), accompanied by examples. The intention usedacross examples is Express concerns about a player from a failed quest team and suggest to not include them in thecurrent team. The context is The team proposal is Player 1 and Player 2, with Player 1 being on a failed quest.We consider the score of including wrong context knowledge higher than copy and paste because we focus onwhether the intention following has an impact on the game. intentions. Players then engage in second-order rea-soning to evaluate how their statements might beinterpreted by others, allowing them to reconsiderand adjust their initial intentions. Finally, playersexpress their refined thoughts and statements dur-ing the games discussion segment, enhanced bythis iterative reflection process. Full prompts areshown in Appendix Sec. E.",
  "Intention-Centric Evaluations": "This section introduces our evaluations, designedaround the concept of intentions, and discussestheir correlation with the four components of socialintelligence outlined in Sec. 1. Intention is funda-mental to social intelligence, essential for effectivecommunication (Tomasello, 2023), strategic influ-ence, adaptation to dynamic social interactions (Ze-lazo et al., 2023), and achieving desired outcomesby helping us interpret and predict behaviors (Yottand Poulin-Dubois, 2016). Our evaluation ordercorresponds to the game-playing process.",
  "Situational Awareness: Intention Selection": "Situational awareness is the ability to perceive en-vironmental elements over time and space, under-stand their significance, and predict their futurestatus (Li et al., 2024a; Endsley, 1995). In ourstudy, we assess the situational awareness of LLMsin social contexts by evaluating their intention se-lection. LLMs are requested to select intentionsbased on summarization, first-order reasoning, andthe ongoing dialogue in the current round. Ideally,LLMs should exhibit a sharp awareness of dynamic",
  "Role profilessuch as an evil player forget-ting their identity and intending to mistakenlysupport the loyal side": "Other intentionslike a player intends to playMerlin and Percival simultaneously.We use binary indicators to evaluate the reasonable-ness of intentions, assigning 1 to reasonable inten-tions and 0 to unreasonable ones. Previous hallu-cinations, such as manipulated information, mightinfluence the current players decisions. However,we maintain strict criteria, as we expect the playerto be capable of recognizing these hallucinations.",
  "Self-Regulation: Intention Following": "Self-regulationinvolvesguidingonesownthoughts, behaviors, and feelings to achieve goals,thereby requiring individuals to contribute to theirown motivation (Fitzsimons and Finkel, 2010). Inour study, we evaluate the self-regulation abilitiesof LLMs by assessing their adherence to selectedintentions, which includes two main perspectives:thinking (planning) and speaking (implementing),as depicted in . Due to the abstract natureof the thinking phase, our criteria are lenient: weconsider the LLMs thought process as correct ifit reflects the intended goal. While we do not de- mand informativeness in the thinking phase, werequire validity and penalize scores for hallucina-tions or omission of intentions. In contrast, for thespeaking phase, we require the models to executeactions informatively and without hallucinationsfor a response to be considered good. We employ aLikert scale to annotate outcomes for both phases,with scores ranging from 5 (completely correct)to 1 (completely incorrect), with 3 representing aborderline. provides detailed criteria andexamples for the annotation.",
  "Self-Awareness: Intention Summarization": "Self-awareness refers to an individuals understand-ing of their character, emotions, motives, and de-sires (Gallup et al., 2003). Our evaluation investi-gates the capability of LLMs to accurately identifytheir own intentions through analysis of their inter-nal thought processes and speeches in the currentround. This evaluation serves as the converse tointention following. LLMs are expected not onlyto execute intentions precisely but also to articulatetheir underlying motivations.",
  "Theory of Mind: Intention Guessing": "ToM involves understanding others by attributingmental states to them (Kosinski, 2023). Previousevaluations of LLMs ToM focused on scenarioswhere models are provided with complete contextsfor interpreting characters mental states (Gandhiet al., 2024). Our evaluation raises the complexityand challenge for LLMs by providing only lim-ited information, simulating real-world conditions.Specifically, LLMs are required to deduce playersintentions from their speeches alone, representinga more rigorous test of their capability to compre-hend and anticipate mental states.",
  ": Inter-rater agreement score among human anno-tators. We show mean standard deviation over pairsof annotators": "every thinking and each speaking result. There-fore, we first annotate intentions from five gamesto identify those with significant, either positivelyor negatively, impacts on game outcomes. Theseimpactful intentions are then prioritized for fur-ther study. The methodology for selecting theseintentions and the correlation between intentionunderstanding and game performance is detailedin Appendix Sec. B.2. This refinement processisolates 2,440 intentions from GPT-3.5 and 350intentions from GPT-4 for annotation ().We recruit five computer science masters stu-dents for annotation, each assigned seven to eightgame contexts.3Unimpactful intentions withineach context are masked to maintain focus. Tocalculate inter-rater reliability, two files are com-monly assigned across all annotators. The FleissKappa (Fleiss, 1971) is calculated pairwise, withaverages derived across all pairs. For the intentionfollowing, we group the scores into two categories:13 and 45. Scores of 5 represent a perfect re-sponse, while 4 indicates a nearly perfect response.Scores from 1 to 3 signify that the response con-tains significant flaws. The scores are shown in. Additional grouping results are providedin the Appendix Sec F. Intention Summarization and GuessingEval-uations of intention summarization and guessingare separated from the gameplay pipeline becauseas interactions progress, the clarity of LLMs out-puts often deteriorates rather than improves. At thegames end, we extract the contexts including roles,discussions, voting outcomes, and quest results, ina structured format. Notably, intention summariza-",
  ": Self-regulation results. The results show the percentage of each score over all data samples. Scores 1-5are evaluation criteria (). Score 5 means the best while score 1 means the worst": "tion and guessing are not critical to the games coremechanics. By separating these elements, we canstill ensure the comprehensiveness of structuredgame information, as detailed in Appendix Sec. D.Summarizing and guessing intentions do not re-quire human annotation, as we can directly com-pare the outcomes with players earlier choices.However, evaluating the performance of LLMsagainst humans remains essential.Unlike thestraightforward tasks of intention selection and fol-lowing, which generally require the exclusion ofincorrect informationa task in which humansexcelsummarizing and guessing intentions de-mands more social intelligence, a more complexhuman capability. We conduct a user study withthree masters students who answer a total of 300questions, 200 for GPT-3.5, and 100 for GPT-4.",
  "Results: Intention Selection and Following": "LLMs show a good understanding of the situ-ation.For intention selection, we calculate thenumber of reasonable intentions over all intentionsas accuracy. The accuracies for GPT-3.5 and GPT-4 are 87.5% and 88.8% respectively, indicating thatboth models effectively understand the situation.We do not assess whether the selection is optimaldue to its subjective nature. Our focus is solelyon whether the models can capture key informa-tion and make accurate decisions based on thatinformation. Instances of unreasonable intentionsare primarily attributed to the models forgettinginformation during lengthy conversations or beinginfluenced by hallucinations from other players.",
  "LLMs can understand intentions abstractly butfall short in planning out intentions.For in-tention following, as shown in , GPT-4generally outperforms GPT-3.5, in both analyti-": "cal thinking and articulate speaking. Specifically inspeech, GPT-4s responses tend to be more informa-tive, comprehensive, and detailed. However, thereare instances where GPT-4 deviates from its in-tended focus, although their discussion still makessense. This misalignment with the intentions se-lected occurs approximately 15% of the time. Italso appears that adhering to intentions in speakingproves more challenging than in thinking due tostricter evaluation criteria. This discrepancy sug-gests that while the model can often conceptualizean appropriate strategy, translating these strategiesinto clear, actionable responsesparticularly inverbal formremains a complex task. Intention understanding correlates with gameperformance.To better understand how inten-tion selection and following influence game dynam-ics, we examine their correlation with both gamewin rates and quest win rates separately. For thecorrelation between intention selection and gameperformance, we select intentions with a followingscore greater than two. For intention following,we consider only reasonable intentions, convertingLikert scale responses to binary scores using vary-ing thresholds. We define r = sum(scores) nteammate wherethe score is the binary score result from intentionselection/following and nteammate is the numberof teammates, two for the evil side and three forthe loyal side. If revil > rloyal, it suggests thatthe evil players outperformed the loyal players ina given game or round, and vice versa. We usegames from GPT-3.5 for assessment, whose resultsare shown in and . We observethat loyal players outperform evil players acrossall quests or games, even in those they ultimatelylose. This discrepancy likely arises from the loyalplayers conditions under more constrained infor- FailureSuccess",
  "(c) Intention Following =3 (Game Wise)": ": Correlation between Intention Selection/Following and game performance. We present the percentagesof games where evil players are equally, better, or worse than loyal players. For example, in games won by loyalplayers in (a), their performance matches or exceeds that of evil players. We mark the performance differencesbetween evil and loyal players in red, showing a greater gap in successful games/quests compared to failed ones. R1R2R3R4R5",
  ": ToM results over rounds. We provide 200 datapoints for human results on the GPT-3.5 games, andsince usually, games stop at round 4, the results fromround 5 are not included": "mation, leading to their occasional losses despitehaving generally superior performance. When com-paring failed and successful games or quests, theperformance gap between loyal and evil players isnotably larger in successful ones than in failed ones,indicating that loyal players must significantly out-perform evil players to secure a win. Specifically,in (c), a threshold of 3 is applied, labelingscores of 3 as 1 and all others as 0. Thisthreshold can reflect instances of hallucination orincorrect context information. Hallucinations cansignificantly influence players, yet incorrect adher-ence to intentions may occasionally yield positiveoutcomes. For example, if a loyal player mistak-enly attacks an evil player for an incorrect reason,other players may overlook the mistake and con-tinue to support the action. Our results suggest acorrelation between selecting and following inten-tions and game performance, especially for loyalplayers at a disadvantage. Improved selections andfollowing can enhance overall game performance.",
  "Results: Intention Summarization andGuessing": "LLMs can achieve human-level performance inself-awareness but perform much worse thanhumans in ToM.For both summarization andguessing, we report F1 scores. As shown in Ta-ble 4, GPT-3.5 performs below human levels butstill achieves respectable scores in self-awareness,while GPT-4 exhibits performance comparable tothat of humans. One key reason for this differenceis that thinking content more explicitly outlinesintentions, making it easier for models to extract.From the ToM results shown in , we cansee both models struggle significantly compared toself-awareness or compared to human performance.The experimental setup involves asking GPT-3.5and GPT-4 to predict each others intentions, a taskdiffering from self-awareness where a model sum-marizes its own intentions. In ToM assessments,models need to understand and predict intentionsfrom responses other than their own, hence thelower performance of GPT-3.5, especially when in-terpreting the more nuanced and lengthy responsesof GPT-4. This complexity results in GPT-3.5 be-coming worse when predicting GPT-4s responses,while GPT-4 manages similar accuracy in guessingboth its own and GPT-3.5s intentions. Addition-ally, human participants generally score better withGPT-4, suggesting that longer contexts in GPT-4s outputs help humans and occasional errors inGPT-3.5s outputs can sometimes confound humanjudgment. While GPT-4 aligns more closely withhuman-like processing, there is still considerablepotential for improvement in understanding andmimicking human cognitive processes.",
  "GPT-3.5/469.4983.76Human75.210.9783.342.72": ": Self-awareness results. The results are GPT-3.5and GPT-4 summarize their own intentions respectively.We also show the mean and standard deviation of humanstudy results since we include 3 human annotators. Weshow F1 score in this table. Incorporating more context does not necessar-ily improve the models ToM ability.We alsoevaluated the impact of information accumulationon the ability to guess intentions, examining howthe inclusion of information from previous roundsaffects guessing results. Our analysis shown in Fig-ure 5 reveals that for GPT-3.5 game outputs, moreinformation generally leads to improved guessingoutcomes. In contrast, the guessing scores for GPT-4 game outputs fluctuate significantly, with notabledeclines in GPT-3.5s performance. This could beattributed to GPT-4 game outputs typically provid-ing more extensive context, which challenges GPT-3.5s capacity to effectively summarize and extractrelevant information. Meanwhile, GPT-4 demon-strates a consistent ability to handle increased datavolumes. Interestingly, human participants main-tain similar scores across rounds. This might stemfrom humans ability to directly infer intentionsfrom immediate thinking and speaking cues, with-out heavily relying on the context. To improve ToMcapability, including more context does not neces-sarily improve the performance but might introducemore complexity to LLMs.",
  "Social Intelligence in LLMs": "Artificial social intelligence has long been a signif-icant topic (Bainbridge et al., 1994; Kovac et al.,2023; Grossmann et al., 2023; McDonald and Pear-son, 2019; Street, 2024; Li et al., 2024b) as it en-ables machines to understand and respond to hu-man emotions and social cues, enhancing human-computer interaction and facilitating social sci-ence study (Ziems et al., 2024; Griffin et al., 2023;Dubova, 2022; Gweon et al., 2023; Zhang et al.,2024). Gweon et al. (2023) proposed a dataset toevaluate social reasoning and Nematzadeh et al.(2018) crafted a dataset following Sally-Anne ex-periment to access ToM ability.Research hasdemonstrated the emergence and limitations ofToM abilities in LLMs (Kosinski, 2023; Moghad-",
  "GPT-3.531.7324.85GPT-445.6646.87Human61.342.7565.235.03": ": ToM results of GPT-3.5 and GPT-4, along withthe mean standard deviation of human results. Dif-ferent from self-awareness, two LLMs also guess eachothers intentions. For example, when GPT-4 guessesGPT-3.5 intentions, it achieves a F1 score of 45.66. dam and Honey, 2023; Sap et al., 2022; Wu et al.,2023). Although LLMs can understand human in-tent and choose correct actions in some tasks, theiroverall performance remains inferior to that of hu-mans.",
  "Intentions and Theory of Mind": "ToM, commonly referred to as mindreading, istraditionally understood as the ability to assignmental states to oneself and others. This capabilityis used to interpret and predict actions (van Duijnet al., 2023). Despite the emergence of ToM inLLM, some papers argued that LLM learns spuri-ous correlations and ToM ability falls with stresstest (Shapira et al., 2023; Ullman, 2023; Sclar et al.,2023). Intentions, as the driving force behind ToM(Kennington, 2022), is important as the primarypurpose of comprehending beliefs and desires is tounderstand the intentions of others (Zelazo et al.,2023). Since social intelligence and ToM can beseen as a society of individually simple agents(Zhuge et al., 2023), we focus on intent understand-ing as an agent to evaluate social intelligence.",
  "Social Game Context": "With the enhancement of LLM performance, moreresearch shifts to social games that necessitate con-versation and cooperation (Ibraheem et al., 2022;Mansbach et al., 2021; Meta et al., 2022; OGara,2023; Park et al., 2023; Zhou et al., 2024; Wu et al.,2024b). Zhou et al. 2023b used reinforcementlearning to incorporate intent and ToM in DnDgames. Recent works focus on the two most pop-ular board games Avalon (Lan et al., 2023; Shiet al., 2023; Wang et al., 2023; Light et al., 2023b)and Werewolf (Xu et al., 2023b,a), exploring socialbehaviors and strategic playing ability of conversa-tional agents. Unlike previous work which aimedto improve the game performance, we focus on thein-depth social intelligence evaluation utilizing theAvalon game.",
  "Conclusion": "We introduce a novel framework for evaluatingsocial intelligence within the Avalon context, fo-cusing on intentions. We examine four criticaldimensions: situational awareness, self-regulation,self-awareness, and ToM. Our findings indicate thatmodels demonstrate a fundamental understandingof situations and self-awareness, but struggle withself-regulation and ToM, particularly when com-pared to human performance. Through this system-atic approach, our study not only contributes to adeeper understanding of LLMs social intelligencebut also establishes a methodological foundationfor future research.",
  "Limitations": "This study includes several limitations. First, weevaluate the four essential components of socialintelligence but leave several aspects unexplored.For instance, the capability of a model to modify itsintentions post-reflection, i.e., the Self-Correction,could also be assessed using our framework. Futureresearch should consider expanding this approachto include more nuanced facets of social intelli-gence. The second limitation lies in our annotationcriteria, which are restricted to assessing the reason-ableness of intentions to avoid subjectivity. How-ever, dimensions such as creativity and ingenuityare equally important. Future studies should aimto measure these attributes to ensure that modelsnot only perform tasks effectively but also demon-strate innovative behaviors indicative of genuineintelligence. The third limitation stems from thecost. Half of our evaluations rely on costly humanannotation, and the use of GPT-4 incurs signifi-cant resource demands due to the extensive contextinvolved. To make the evaluation process moreefficient and cost-effective, we need to developmethods that allow simpler tasks to be managed bysmaller models, which introduces additional chal-lenges. The final limitation is that we did not usean open-source model, as it lacks the capability toeffectively engage in social games.",
  "Adam S Goodie, Prashant Doshi, and Diana L Young.2012. Levels of theory-of-mind reasoning in compet-itive games. Journal of Behavioral Decision Making,25(1):95108": "Lewis Griffin, Bennett Kleinberg, Maximilian Mozes,Kimberly Mai, Maria Do Mar Vau, Matthew Cald-well, and Augustine Mavor-Parker. 2023. Large lan-guage models respond to influence like humans. InProceedings of the First Workshop on Social Influ-ence in Conversations (SICon 2023), pages 1524. Igor Grossmann, Matthew Feinberg, Dawn C Parker,Nicholas A Christakis, Philip E Tetlock, andWilliam A Cunningham. 2023. Ai and the trans-formation of social science research.Science,380(6650):11081109.",
  "Michal Kosinski. 2023. Theory of mind might havespontaneously emerged in large language models.Preprint at org/abs/2302.02083": "Grgur Kovac, Rmy Portelas, Peter Ford Dominey,and Pierre-Yves Oudeyer. 2023.The socialaischool: Insights from developmental psychologytowards artificial socio-cultural agents.Preprint,arXiv:2307.07871. Yihuai Lan, Zhiqiang Hu, Lei Wang, Yang Wang, De-heng Ye, Peilin Zhao, Ee-Peng Lim, Hui Xiong, andHao Wang. 2023. Llm-based agent society investi-gation: Collaboration and confrontation in avalongameplay. arXiv preprint arXiv:2310.14985. Matthew Le, Y-Lan Boureau, and Maximilian Nickel.2019. Revisiting the evaluation of theory of mindthrough question answering. In Proceedings of the2019 Conference on Empirical Methods in Natu-ral Language Processing and the 9th InternationalJoint Conference on Natural Language Processing(EMNLP-IJCNLP), pages 58725877.",
  "Maarten Sap, Ronan LeBras, Daniel Fried, and YejinChoi. 2022. Neural theory-of-mind? on the limitsof social intelligence in large lms. arXiv preprintarXiv:2210.13312": "Melanie Sclar, Sachin Kumar, Peter West, Alane Suhr,Yejin Choi, and Yulia Tsvetkov. 2023. Minding lan-guage models(lack of) theory of mind: A plug-and-play multi-character belief tracker. arXiv preprintarXiv:2306.00924. Natalie Shapira, Mosh Levy, Seyed Hossein Alavi,Xuhui Zhou, Yejin Choi, Yoav Goldberg, MaartenSap, and Vered Shwartz. 2023.Clever hans orneural theory of mind?stress testing social rea-soning in large language models.arXiv preprintarXiv:2305.14763. Zijing Shi, Meng Fang, Shunfeng Zheng, ShilongDeng, Ling Chen, and Yali Du. 2023.Coopera-tion on the fly: Exploring language agents for adhoc teamwork in the avalon game. arXiv preprintarXiv:2312.17515.",
  "Tomer Ullman. 2023. Large language models fail ontrivial alterations to theory-of-mind tasks. arxiv": "Max J van Duijn, Bram van Dijk, Tom Kouwenhoven,Werner de Valk, Marco R Spruit, and Peter van derPutten. 2023. Theory of mind in large language mod-els: Examining performance of 11 state-of-the-artmodels vs. children aged 7-10 on advanced tests.arXiv preprint arXiv:2310.20320. Shenzhi Wang, Chang Liu, Zilong Zheng, SiyuanQi,Shuo Chen,Qisen Yang,Andrew Zhao,Chaofei Wang, Shiji Song, and Gao Huang. 2023.Avalons game of thoughts: Battle against decep-tion through recursive contemplation.Preprint,arXiv:2310.01320.",
  "Shuang Wu, Liwen Zhu, Tao Yang, Shiwei Xu, QiangFu, Yang Wei, and Haobo Fu. 2024a. Enhance rea-soning for large language models in the game were-wolf. Preprint, arXiv:2402.02330": "Zengqing Wu, Run Peng, Shuyuan Zheng, QianyingLiu, Xu Han, Brian Inhyuk Kwon, Makoto Onizuka,Shaojie Tang, and Chuan Xiao. 2024b. Shall we teamup: Exploring spontaneous cooperation of competingllm agents. Preprint, arXiv:2402.12327. Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xi-aolong Wang, Weidong Liu, and Yang Liu. 2023a.Exploring large language models for communica-tion games: An empirical study on werewolf. arXivpreprint arXiv:2309.04658.",
  "Jintian Zhang, Xin Xu, Ningyu Zhang, Ruibo Liu,Bryan Hooi, and Shumin Deng. 2024. Exploringcollaboration mechanisms for llm agents: A socialpsychology view. Preprint, arXiv:2310.02124": "Li Zhang, Qing Lyu, and Chris Callison-Burch. 2020.Intent detection with wikihow. In Proceedings ofthe 1st Conference of the Asia-Pacific Chapter ofthe Association for Computational Linguistics andthe 10th International Joint Conference on NaturalLanguage Processing, pages 328333. Pei Zhou, Aman Madaan, Srividya Pranavi Potharaju,Aditya Gupta, Kevin R McKee, Ari Holtzman, JayPujara, Xiang Ren, Swaroop Mishra, Aida Ne-matzadeh, et al. 2023a. How far are large languagemodels from agents with theory-of-mind?arXivpreprint arXiv:2310.03051. Pei Zhou, Andrew Zhu, Jennifer Hu, Jay Pujara, XiangRen, Chris Callison-Burch, Yejin Choi, and PrithvirajAmmanabrolu. 2023b. I cast detect thoughts: Learn-ing to converse and guide with intents and theory-of-mind in dungeons and dragons. In Proceedingsof the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 1113611155. Xuhui Zhou, Zhe Su, Tiwalayo Eisape, HyunwooKim, and Maarten Sap. 2024.Is this the reallife? is this just fantasy? the misleading successof simulating social interactions with llms. Preprint,arXiv:2403.05020. Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang,Haofei Yu, Zhengyang Qi, Louis-Philippe Morency,Yonatan Bisk, Daniel Fried, Graham Neubig, et al.2023c.Sotopia: Interactive evaluation for socialintelligence in language agents. In The Twelfth Inter-national Conference on Learning Representations.",
  "AModels and Paramters": "We only test GPT-3.5 and GPT-4 in our work asthose two are the most common models in the so-cial game study. We also tried open-source modellike LLaMA-2 7B and LLaMA-3.1 8B, however,LLaMA-2 could not follow the format instructionsproperly, as also observed in Wang et al. (2023).LLaMA-3.1 performs better than LLaMA-2 butstill fails to follow the format after a few turns ofconversations. We set the temperature as 0.8 forboth GPT-3.5 and GPT-4. For intention summa-rization, we also tried sampling five responses andgot the aggregated result by majority vote. Theresult on multiple responses is only 1% higher thanthe result on single responses. Therefore, we onlygenerate a single response for self-awareness andToM tasks. The cost of GPT-3.5 is approximately$80 and $25 for GPT-4.",
  "B.1Creation of Intention Set": "The intentions were created as a combination ofdomain research and prompting GPT-3.5 to selectintention categories from the options listed in and generate intentions based on the selectedcategories. For intention category selection weuse the prompt shown in and for intentiongeneration, we use the prompt shown in .The final set of intentions is listed in .",
  "B.2Selecting Impactful Intention": "For selecting a set of impactful intentions, we anno-tate five complete games played by GPT-3.5. Fur-ther, we compute the probability of winning theround given that the intention was selected by aplayer. To select impactful intentions, we onlyconsider the set of intentions where the computedprobability is greater than 0.7 (strong positive as-sociation) or it is less than 0.3 (strong negativeassociation) and the intentions selected at least twotimes for the considered side. We depict the finalset of sixteen impactful intentions in red in .",
  "Win RateIt is the percentage of games won bythe loyal side or the evil side from the total numberof games": "Quest Win RateIt is the percentage of quests(game rounds) won by a particular side from thetotal quests played in all the games. For the loyalside, a successful quest is a quest win, whereas, forthe evil team, a failed quest is a quest win. Quest Engagement RateIt is the percentage ofquests (game rounds) where the player gets selectedin the quest team from the total quests played in allthe games. This metric is calculated for each roleseparately and averaged across. Team Selection AccuracyIt is the percentage ofquests in which the leader correctly selected teammembers aligned with the objective of their respec-tive side (loyal or evil), out of the total number ofquests the leader belonged to that side. Correctteam selection for loyal and evil sides is defined asfollows:",
  "Team Proposal Change RateIt is the percent-age of game rounds where the leader changes theproposed team after discussion, which reflects theeffectiveness of the discussion": "Merlin Assassination RateAt the end of thegame, Assassin has a chance to identify Merlin. Ifthey identify Merlin, the leader of the loyal side,correctly, then evil players win. This rate is thepercentage of games where Assassin correctly iden-tifies Merlin at the end of games over the gameswhere Assassin has a chance to assassin.We record the entire game and extract interme-diate steps, such as voting results, team propos-als, and quest outcomes, to calculate game perfor-mance. Results in indicate that loyal play-ers are at a disadvantage with a notably low gamewin rate, which corroborates findings from Wang",
  "LoyalEvilLoyalEvil": "Win rate18.082.040.060.0Quest win rate41.458.654.545.5Quest engagement rate53.643.559.138.6Team selection accuracy32.180.071.4100.0Failure vote rate-79.9-86.7Team proposal change rate76.274.150.0062.5Merlin assassination rate-41.7-33.3 : Game performance results shown in percent-age. For the failure vote rate and Merlin assassinationrate, there is no result for Loyal players as they are notallowed to vote failure or assassinate Merlin. et al. (2023). While the quest win and engagementrates show a balanced performance between loyaland evil players, the evil side demonstrates supe-rior accuracy in team selection. This observation isconsistent with their strategic role in the game. Thehigh rate of team proposal changes across all set-tings suggests a dynamic framework where playersconsider the discussions of others, an essential as-pect of social games. Additionally, the high rate ofMerlin assassinations significantly boosts the winrate for evil players. Our objective is not to enhancegame performance per se, but to demonstrate thatwith an intention-guided mechanism, game perfor-mance is similar to existing studies, providing arobust testbed for studies in social intelligence.",
  "EGame Pipeline Prompts": "Since we mostly refer to the prompt by Wang et al.(2023), we only present prompts different fromtheirs to reduce the redundancy. The prompts weuse are shown in to . For the firstorder, formulation contemplation, and refinementcontemplation, we add an intention mechanismbased on original prompts. We also add severalquestions in the quest action prompt to help evilplayers better choose action.",
  "We show all the instructions we use in this section,from to . For intention summa-rization and guessing, we also illustrate the whole": "intention lists shown in .We experimented with several grouping meth-ods to calculate agreement scores. When usingthe grouping of 1 4 and 5, the Fleiss Kappascores for \"thinking\" and \"speaking\" were 0.448and 0.420, respectively. These scores are relativelylower, likely due to differing interpretations of in-formativeness among human annotators. For thegrouping of 1 2 and 3 5, the agreement waslow, as scores of 1 2 accounted for only a smallportion of the entire dataset. Since Fleiss Kappais highly sensitive to disagreements within the 1 2 group, even minor discrepancies resulted in anoverall lower score.",
  "Intention Category Selection Prompt": "Intentions are used to guide your game playing and they should align with your game Goal(main intent) and your role.Your selected intentions should support your main intent.Now select multiple intention categories that are ideal for this round depending on your role and strategy from theoptions listed below. Also give explanation of why you chose those categories in 2-3 sentences as part of Think value.Your explanation wont be shown to others.",
  "Intention Generation Prompt": "First answer the following questions:1. What is your main goal(main intent) for the game?2. What have you observed from discussions in this round?3. What should be the ideal team composition for your side to win?4. After analyzing the responses to previous questions, what do you need to do to win from here? Consider answers to your previous questions and your roles and characteristics and generate intentions that help you achieveyour main goal (main intention).Please do not repeat intentions and ensure they are different from each other.Each intention should be 10-15 words long. Lets think step by step before making your decisions.Remember to generate at least 3 intentions, with a minimum of one intention for each intention category you have selected below:[intentioncategories selected by player]",
  "(c) Intention Following =3 (Round Wise)": ": Correlation between Intention Selection/Following and game performance. We present the percentagesof games where evil players are equally, better, or worse than loyal players. For example, in games won by loyalplayers in (a), their performance matches or exceeds that of evil players. We mark the performance differencesbetween evil and loyal players in red, showing a greater gap in successful games/quests compared to failed ones.",
  "CategoryIntentions": "InterrogationQuestion a player on why he didnt vote for the last teamQuestion a player about why they changed their statementsQuestion the leader why they selected a particular player for the teamQuestion the leader why they didnt put themselves on the quest teamDefenseDefend the proposed team composition if it includes loyal players (for Merlin)Defend yourself against allegations that you could be evilDefend your teammate against allegations that they could be evil (for evil players)ConfrontationShare concerns about an evil player (for Merlin)Express concerns about a player from a failed quest team and suggest to remove them from current teamCast suspicion on innocent players (for evil players)Counter the team proposal citing that you are good and not part of the teamCounter the team proposal citing that you think a player on the team is evilExpress disagreement and vote disagree only if you are not in the proposed team ConcealmentStay hidden in discussions and act like a Loyal Servant to protect yourself(for evil players, Merlin and Percival)Express that you dont have any information on whom to put on the team to protect yourself (for Merlin)Pretend that you dont have enough information about who is evil right now (for Merlin)DeceptionPretend to be Percival by telling others you are Percival (for evil/Servant)Pretend to be Merlin by providing hints on who is evil (for Percival and Servant)Pretend to have information and act like Merlin (for Morgana)Express you are a loyal player (for evil players)Express confusion for failure of mission (for evil players) PersuasionStrongly convince the team leader to choose you on teamReveal identity by telling others you are Percival to persuade other loyal players(for Percival)Reveal identity by telling others you are Servant to persuade other loyal players(for Servant) TeamworkSupport team proposalSupport one loyal player (for Merlin)Support your teammate to be on the quest team (for evil players)Support your teammate by expressing that he is good (for evil players)Include loyal players on the quest team as leader (Merlin)Provide hints to loyal players about evil players (Merlin)Support loyal players of the previous quest team if the mission failed (for Merlin)",
  "Your speech: [players speech in the current round]": "Summarize your intent from your dialogues in this round.Select multiple intents from the given options that best match your intentions from the current round.Also, provide an explanation of the intents that you showed in the current round.Remember this is private information to you and wont be shown to other players.Remember that you can select 2-3 intents and dont use more than 50 words for explanation.Intent options: [intent options]",
  "Intention Selection Prompt": "Intents are used to guide your gameplay and discussions in the current round and helpyou work with your team.Carefully consider your roles and characteristics and select intents that help you achieveyour main goal(main intent).Also, give an explanation of why you chose those intents in 2-3 sentences as part of Thinkvalue.Remember to select intents that are ideal based on previous game history.Remember to select intent that supports your team goals and is not just arepetition of other player's dialogues without any analysis.Remember to keep your roles, characteristics, and teammates in mind beforeselecting intents.Let's think step by step before making your decisions.Remember to select at least 3(or more) intents that don't conflict with each other from thefollowing options and also help you eventually succeed in the quest:[Intention list]",
  ": Intention selection prompt": "First order prompt Analyze [other players] based on game dialogues with roles: [role options]. Morgana andAssassin are evil; others are good. Consider:1. Quest Outcomes: Take into account the results of past missions to analyze players'roles.2. Role List: Remember the possible roles in the gameMerlin, Percival, two LoyalServants, Morgana, Assassinand their alignments.3. Level of Certainty: Use 'Certain' or 'Unknown' to gauge your confidence in yourrole guesses for each player.4. Players Disclosing Evil Roles: Be cautious around players who have openly claimedor hinted at being evil roles like Morgana or Assassin.5. Prior Guesses: Reflect on your earlier estimations of other players' roles ([previousattitude to players]), but don't rely solely on them.Here are the role options:[role options list] Remember that other players can't have your role and role is unique to a player. [intent options] Usually one player will have 2-3 intents, you need to guess their intent throughtheir speech.Keep in mind that different roles can have different intents.Select 2-3 intents that you think the player has from the given options based onyour analysis of their role.",
  "Formulation Contemplation Prompt": "Now contemplate, then organize thoughts to speak. You need to respond in two stages:think and speak.In think, internally strategize using history and consider possible deception.In speak, organize your language based on your contemplation and speak accordingly.Understand your role's main objective and break it down into chronological sub-goalsbased on game history. Your thought process should follow these sub-goals for asystematic approach to the main goal. Follow your selected intents by coming up with actions like providing actualhints or raising concerns against specific players or casting suspicion againstspecific players, etc (depending on what you have selected).Let's think step by step before making your decisions.",
  "Intention Modification Prompt": "Now analyze your original spoken and thinking content and how otherplayers would think about your original spoken content. You might have selected intents that might not be strategically goodto win the game or might conflict with each other and difficult toexecute.Based on your analysis and your main goals, do you want to modifyyour original intents and select other intents that are better?You can add more intents or also remove some intents from youroriginal list which can help you win the game.Else, if you don't want to modify them, please respond back with youroriginally selected intents and reasoning why you want to retain them.Please provide your new set of modified intents or your originalintents and reasoning in 2-3 sentences.You need to provide reasoning in both cases. Remember intents and how you act to follow them are crucial tosuccess, so carefully analyze your intents and modify them if needed. Let's think step by step before making your decisions.",
  "Refinement Contemplation Prompt": "Finally, revise the original thoughts and spoken content:Your task is to:1. Evaluate if [Player]'s actions align with [role].2. Improve [Player]'s chances of winning through your previous secondperspective transition thought.3. Keep role hints in public dialogue. Consider:1. Target Outcome: Aim to achieve [desired result] as your role dictates inthe game.2. Role Alignment: Evaluate whether your THINK and SPEAK contents alignwell with your role[role] in the current game state.3. Strategy Reevaluation: Consider what changes could be made to yourTHINK and SPEAKcontents to improve your chances of winning as [role].4. Public and Private Content: Remember that THINK contents are private,while SPEAKcontents are publicly visible to all other players and maybe used againstyou. Strategize accordingly. Follow your selected intents by coming up with actions likeproviding actual hints or raising concerns against specific playersor casting suspicion against specific players, etc (depending onwhat you have selected).Let's think step by step before making your decisions.Remember based on your role, SPEAK content is visible to everyone so youwould want to avoid revealing sensitive information.",
  ": Refinement contemplation prompt": "Team change proposal prompt Now the leader after hearing other players' opinions about your teamproposal, do you want to change your team proposal?The team you initially proposed was - [original team] Consider: 1. Assess each player's involvement, discussion, and actions so far.2. Review how players have contributed to past Quests.3. Evaluate any behavior that appears either suspicious or trustworthy. Speaking (Public):Then present your team if you want to change the team, or else respondwith the same team. Give reasons based on past events and behaviors why you changed orretained your team. Do not reveal your role or hints. Usually include yourself; if not, clarify.Additionally, provide answers to any questions that other players mighthave asked during the discussion. Also, you should normally include yourself in your proposed team. Remember your main goals for the game and intents for the round.Remember to consider each player's statements carefully in the currentround and your knowledge of their roles and decide your team if you wantto change it or keep the original team. Let's think step by step before making your decisions.The team numbers we need for this round is [round team members count]",
  "Quest Action Prompt": "Approved team members vote to succeed or fail the quest.Your task is to:Vote success or fail for the Quest. Consider:1. Assess each player's involvement and actions so far.2. Review how players have contributed to past Quests.3. Evaluate any behavior that appears either suspicious or trustworthy. First answer the following questions: 1. What is your main goal(main intent) for the game?2. If this is not the first round, what are the previous quest results?3. After analyzing the responses to previous questions, what do you needto do to win from here? Speaking (Public):Then, Clearly vote [success] or [failure], making your stance unambiguous.Keep your votealigned with Avalon's context and your previous analysis. Choose from thefollowing 2 options:1. success2. failure Let's think step by step before making your decisions."
}