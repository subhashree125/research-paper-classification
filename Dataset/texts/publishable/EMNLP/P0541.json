{
  "Abstract": "The alignment of reasoning abilities betweensmaller and larger Language Models are largelyconducted via supervised fine-tuning usingdemonstrations generated from robust LargeLanguage Models (LLMs). Although theseapproaches deliver more performant models,they do not show sufficiently strong generaliza-tion as the training only relies on the provideddemonstrations. In this paper,we propose a self-refineInstruction-tuning method that allows forSmaller Language Models to self-improve theirreasoning abilities. Our approach is based on atwo-stage process, where reasoning abilities arefirst transferred between LLMs and Small Lan-guage Models (SLMs) via Instruction-tuningon synthetic demonstrations provided by LLMs,and then the instructed models self-improvethrough preference optimization strategies. Inparticular, the second phase operates refine-ment heuristics based on Direct Preference Op-timization, where the SLMs are prompted todeliver a series of reasoning paths by automat-ically sampling the generated responses andproviding rewards using ground truths fromthe LLMs. Results obtained on commonsenseand math reasoning tasks show that this ap-proach consistently outperforms Instruction-tuning in both in-domain and out-domainscenarios, aligning the reasoning abilities ofsmaller and larger language models.",
  "Introduction": "Previous works have demonstrated that Chain-of-Thought (CoT) prompting can improve the LargeLanguage Models (LLMs) capacity to performcomplex reasoning tasks by decomposing a rea-soning task into a sequence of intermediate steps(Wei et al., 2022), where the generation of multi-step controlled reasoning can improve results incommonsense (Bubeck et al., 2023), symbolic andmathematical (Gaur and Saunshi, 2023; Liu et al.,2023) reasoning datasets. Since the size of LLMs represents an adoptionbarrier for many use cases, and smaller models donot seem to have the same emergent reasoning abil-ities as LLMs, several state-of-the-art alignment ap-proaches for solving mathematical problems haveemerged, where Supervised Fine-Tuning (SFT) hasbeen used to train Small Language Models (SLMs)using CoT annotations. However, these annotationsoutline the intermediate reasoning steps for solv-ing a given problem, which consists of a reasoningpathway generated by the LLM for a specific case.This phenomenon can lead to a relatively weakercapacity of tuned models that have a few and lim-ited number of samples. Indeed, there are oftenmultiple valid CoT annotations for the same ques-tion (Cobbe et al., 2021; Zhang et al., 2023), whichunderlines the need for a more general CoT-basedfine-tuning approach.In this paper, we propose Self-refine Instruction-tuning, which is a method to enable CoT reasoningover SLMs. Our approach starts by performingInstruction-tuning on SLMs via demonstrations de-livered by LLMs and then applies preference op-timization based on reinforcement learning (RL)heuristics to let the SLMs refine their abilities tosolve a task in a step-wise manner. Hence, propos-ing a teacher-student alignment method, we inves-tigate the impact of transferring Chain-of-Thoughtreasoning abilities through the support of demon-strations \"taught\" by LLMs to SLMs as a warm-upto the Self-refine process. Therefore, to reinforcethe Instruction-tuning phase, we analyze whetherpreference optimization methods could strengthenstudent models step-wise reasoning abilities.Complementing the foundation work of Wanget al. (2023c), we introduce Self-refinement basedon reinforcement learning, and in contrast to (Ue-sato et al., 2022; Luo et al., 2023; Luong et al.,2024; Paul et al., 2024), we use an Instruction-tuning via synthetic demonstrations approach asproposed by Ranaldi and Freitas (2024) (i.e., a task- : In Self-refine Instruction-tuning, the demonstrations delivered by teacher models are used to alignreasoning abilities in a teacher-student setting. Following the transference of step-wise reasoning knowledge viainstruction tuning, the students self-refine their abilities with the support of Direct Preference Optimization methods. oriented specialization of Supervised Fine-Tuning)through which we instruct SLMs using demonstra-tions delivered from different teachers promptedvia a CoT mechanism.This leads to the target research questions, whichare the focus of this paper:RQ1: How does Instruction-tuning via demon-strations initialize the SLMs reasoning abilities?RQ2: What is the effect of the preference op-timization algorithm on the alignment betweenteacher and student models?RQ3: How much does the ability to solve tasksin a multi-step manner improve across differentscenarios?We answer these questions by selecting threedifferent SLMs: Llama2-7b, Llama2-13b (Tou-vron et al., 2023), Mistral-7b (Jiang et al., 2023);and three LLMs Llama2-70b, Mixtral (Jiang et al.,2024) and GPT-3.5 (OpenAI, 2023).In the teacher-student alignment phase, we oper-ate via LLMs (teachers) to deliver synthetic demon-strations at the core of the Instruction-tuning pro-cess (see left side of ) used to instructSLMs (students).In the self-refine phase, thestudents improve their step-wise reasoning abil-ities via Direct Preference Optimization (DPO)(Rafailov et al., 2023). This phase allows the stu-dents to sample different reasoning paths and CoTdemonstrations and learn from them (see right sideof ). Moreover, differently from previousworks (based on teacher-student alignments), pref-erences are self-generated, and there is no need fora separately trained reward model as in the previousapproaches (Ouyang et al., 2022).We demonstrate the effectiveness of the pro-posed refinement technique in aligning teacher-student models, overcoming the differences high-lighted by Ranaldi and Freitas (2024), from thesame family, and in maximizing efficiency in in-domain and out-domain tasks.",
  "Our contributions can be summarized as follows:": "We propose the Self-refined Instruction-tuningapproach that is a task-oriented SupervisedFine-Tuning (SFT), which utilizes DPOheuristics to conduct a self-refinement processstarting from instructed SLMs. We analyze the impact of different configura-tions of Instruction-tuning on the SLMs be-fore and after the Self-refining phase by con-ducting in-depth experiments on mathemat-ical problems and common sense question-answering tasks using synthetic demonstra-tions delivered by teacher models of the samefamily (in-family) or not (out-family). Hence,we show the downstream functionalities inboth scenarios. Finally, we display the in- and out-domainabilities acquired via Self-refined Instruction-tuning through a systematic evaluation usingdemonstrations provided by in-family and out-family teachers in different tasks.",
  "Method": "To transfer the step-wise reasoning properties fromLarge Language Models (LLMs) to Small Lan-guage Models (SLMs), we propose Self-refineInstruction-tuning, a two-step approach as shownin . In the first phase, there is a transfer ofstep-wise (CoT) reasoning via Instruction-tuning,where LLMs systematically generate demonstra-tions which are used by SLMs to initialize theirstep-wise (CoT) alignment (.1). In thesecond phase, the instructed SLMs Self-refine theirinternal CoT model via the preference optimizationtechnique presented in .2.",
  "Instruction-tuning Phase": "A significant part of the state-of-the-art works em-ploys standard Supervised Fine-Tuning (SFT) per-formed on annotations produced by a single LLM(Large Language Model) as a mechanism to im-prove SLMs. In our contribution, we take a stepfurther and use Instruction-tuning, which is a task-oriented specialization of SFT (Supervised Fine-Tuning), in coordination with a teacher-studentalignment approach (detailed in Appendix A). Inthis phase, the SLM (student) is fine-tuned on adataset produced by LLM (teacher) comprising aset of tuples in the form of (i, q, ai), where i repre-sents a specific instruction, q is the input question(e.g., math-word problem), and ai is the expectedoutput and CoT answers generated from the teacherin response to the instruction and input. This setupis intended to transfer to the student models foun-dational problem-solving abilities, emphasizing thegeneration of outputs that conform to the providedinstructions. The CoT answer ai is articulated as:",
  "ai = [w1, w2, . . . , wl1, wl]": "with l indicating the sequence length. At each timestep t, the action wt is derived from the policy(|st), where wt can be any token from the mod-els vocabulary, and the state st encapsulates theconcatenation of all previously generated tokensand the optional input x if provided. The statetransition is defined as:",
  "In the second phase, the instructed SLMs (students)that have improved CoT properties via Instruction-tuning (.1) self-refine these properties": "with the support of Direct Preference Optimization(DPO) (Rafailov et al., 2023). This refinement canbe conducted in an SFT style, relying exclusivelyon labeled preference data. The policy model, de-fined as , learns by repeatedly sampling the an-swers generated by teachers and students. Direct Preference OptimizationIn the standardDPO approach (Rafailov et al., 2023), a human an-notator ranks the outputs from a reference policy,labeling winning and losing pairs yw = inst(x)and yl = inst(x). However, we propose an op-timization step via Self-generated annotation bythe students inst, which, after Instruction-tuning,should have a more robust performance and reliablyfollow the intent of the questions.For each demonstration (i, x, ai), we prompt thestudents using the input x = i+q (q = question+\"Lets think step by step\") (blue block in). Hence, for each instance within thedemonstrations, we collect the Answers (ya =inst(x)) that are the answers generated by the stu-dent given the input x (in additional analysis wealso consider the non-CoT version).In particular, we would like the model to gener-ate responses that deliver a CoT (in fact, we elicitthe models to do it) just as the corresponding LLMteacher would do. To achieve this, we propose analignment by exploiting DPO optimization. Thisaims to move the default style of our model (re-sponse generated by the student) towards the de-sired style (answers that deliver CoT). Differentconfigurations are proposed depending on the de-sired result. Starting from equation 1:",
  "Tasks & Datasets": "Commonsense TaskWe adopt two benchmarksto evaluate commonsense reasoning: Common-SenseQA (Talmor et al., 2019) (CSQA) and Open-BookQA (Mihaylov et al., 2018) (OBQA) are twomulti-choice commonsense question-answeringtasks. Physical & Social Interaction TaskWe adopttwo benchmarks to evaluate reasoning in the con-text of everyday situations, aiming to establish themost reasonable solution: Interaction Question An-swering (PIQA) (Bisk et al., 2019) and Social In-teraction Question Answering (SIQA) (Sap et al.,2019), which emphasizes peoples actions and so-cial implications. Mathematical TaskWe use two math word prob-lem benchmarks to evaluate the models of math-ematical reasoning. MultiArith (Roy and Roth,2015) covers a set of multi-step arithmetic reason-ing tasks, while GSM8k (Cobbe et al., 2021) coversprimary school-level mathematical problems. Additional benchmarksFinally, to evaluate theadaptability of our proposal, we conduct furtheranalysis on two additional evaluation benchmarks:MATH (Hendrycks et al., 2021b) and MMLU(Hendrycks et al., 2021a). DatasetsSince the test split is not prescribed forall the benchmarks, we adopt the following strat-egy: for SIQA, PIQA, CSQA, and OBQA, weuse 4000 examples with equally distributed tar-get classes as training data and the validation ver-sions found on huggingface as test data, while forGSM8K and MultiArith we use the full hugging-face datasets. In , we report the descrip-tive statistics and splitting ratios, while in ,we report one example for each benchmark. Thesupporting datasets are publicly accessible as de-scribed in .",
  "Self-refine Instruction-tuning Pipeline": "The Self-refine Instruction-tuning comprises theannotation process conducted by the teacher LLMmodels that are prompted in the zero-shot scenario(as shown in ), as explained in AppendixA. We selected Llama-2-70 (Touvron et al., 2023),Mixtral7x8 (Jiang et al., 2024) and GPT-3.5 (Ope-nAI, 2023) as LLMs (teachers) and Llama2-7, -13(Touvron et al., 2023) and Mistral-7 (Jiang et al.,2023) SLMs (students) models.Hence, the student models are tuned, as proposedin (Taori et al., 2023) and evaluated with probingpipelines (detailed in .3). The studentsare instructed via demonstrations that contain theanswers generated by the teachers, as explained in.1. Downstream of the teacher-studentCoT transference process, the optimization tech-nique (proposed in .2 and detailed in Ap-pendix B) is employed to improve alignment andself-refine the quality of the generation.",
  "Models Setup": "We conduct the Self-refined Instruction-tuningin two different phases.Firstly, we start withInstruction-tuning phase using QLoRA Dettmerset al. (2023). This approach allows Instruction-tuning to be performed while reducing memoryusage. In particular, Dettmers et al. (2023) pro-pose several techniques for tuning models withmany parameters on GPUs with limited resourceswhile preserving 16-bit tuning performance. Wefollow the training approach proposed in (Taoriet al., 2023), setting four training epochs using alearning rate 2e-5 with a 1e-4 weight decay. We usethe cosine learning rate scheduler with a warm-upratio of 0.03. Furthermore, we conduct the Self-refine phase following the approach proposed in(Rafailov et al., 2023). In particular, we use the hug-gingface DPOtrainer to support its reproducibility.We follow the parameters proposed in (Rafailovet al., 2023). Hence, for the DPO policy, our workemploys a learning rate of 1e-6, set at 0.1, and awarm-up step count of 100. The batch size is con-figured to 128. The optimization process is cappedat a maximum of 1000 steps, where we save thecheckpoint corresponding to the lowest loss on thevalidation set. The experiments were conducted ona workstation with four Nvidia RTX A6000 and48GB of VRAM. 2",
  "Evaluation": "The most commonly used evaluation methods forquestion-answering tasks are language-model prob-ing, in which the option with the highest probabil-ity is selected (Brown et al., 2020), and multiple-choice probing, in which the models are asked tocommit to an answer. The evaluation in the firstcase is performed with a function taking the argmaxand, in the second case, with a direct string match-ing. The second method is more widely used inrecent evaluations as it can be inclusive to the largerGPT family models(OpenAI, 2023), where prob-ability values are not readily accessible. In theexperiments, we chose the latter to have a compa-rable and scalable pipeline (Details provided in Ap-pendix C.2). Finally, string matching is performedbetween the generated outputs and the target choiceto evaluate the percentages of the correct answers.",
  "Results & Discussion": "The Self-refine Instruction-tuning improves thealignment between Large Language Models(LLMs) and Small (SLMs) in both in-family andout-family settings. These conclusions can be ob-served in and , which reportsthe downstream accuracies without tuning (seethe Baselines), with only the Instruction-tuningphase on demonstrations and after the Self-refinephase. As discussed in .1, the modelswith only Instruction-tuning on synthetic demon-strations (generated by LLMs) transfers the reason- ing properties in a marginal way (see Instruction-tuned in Figures 2).However, although teacher-student alignmentvia Instruction-tuning produces better students, animproved alignment is achieved through the Self-refine phase, as discussed in 4.2. In particular,the Self-refine Instruction-tuning bars in show that the self-refined student models out-performed the student models tuned only withInstruction-tuning (Instruction-tuning bars on Fig-ure 2). Furthermore, the alignment via demon-strations generated by teachers outside the samefamily (out-family) delivers more robust students(see the Self-refine Instruction-tuning and(in-family) bars).Finally, student models behind the self-refinephase outperformed others in both in-domainand out-domain tasks (discussed in .3).Hence, the self-refine mechanism effectively alignsteacher-student capabilities in out-domain tasks byenhancing performance even in the presence offewer demonstrations (.4).",
  "The Instruction-tuning alignment": "Instruction-tuning led by Larger Language Models(teacher models), which are able to deliver multi-step reasoned answers, induces this property withinSmaller Language Models (student models). Thiscan be seen in the experiments in , . The student models behind instruction-tuningon demonstrations produced by teacher models out-performed the baselines of the six proposed bench- : Accuracies (%) on benchmarks (.1) before Instruction-tuning ( Baseline CoT), behind first phaseperformed on demonstrations delivering CoT (i.e., Instruction-tuned (IT)) and finally behind the Self-refine phase(i.e., Self-refine IT). In particular, the models were instructed via demonstrations delivered by out-family LLMs (asdescribed in the legend, we use the notation method(Teacher->Student)).",
  "marks and additional ones in Appendix L": "While one can observe consistent improvementsin performance across the board, there are moder-ate variations across models and tasks. The teachermodels that generate demonstrations stem from dif-ferent families and perform differently, as shown in. The consequence of this phenomenon canbe seen in and (horizontal linesthat are the reported performance of the teachersand bars Instruction-tuning that are the perfor-mance of the students). Therefore, the teacher-student alignment is not complete as there is a gapbetween the performances of the teachers and thestudents tuned via Instruction-tuning (only phasepresented in .1). In addition, it is possibleto differentiate between in-family and out-familyalignment. In the in-family, where students areinstructed with demonstrations delivered by theteachers of the same family, performances varyfrom 6.3 points on average in question-answering(QA) tasks and 8.2 points on average in math wordproblems (MWP) tasks. Meanwhile, in the out-family alignment, the performances vary by 8.5 onthe QA and 8.7 on the MWP. Hence, to improve the alignment both in-familyand consistently out-family, we have proposed anoptimization technique based on a self-refinementapproach (introduced in .2), the results ofwhich we discuss in .2.",
  "The Self-refine Impact": "The Self-refine process enables complete in-familystudent-teacher alignment by consistently increas-ing performance in out-family settings and improv-ing the qualities of generated answers. The resultsin show that the students (SLMs instructedwith Self-refine Instruction-tuning) outperform thenon-self-refined students and perform comparablyto their teachers. The same behaviour can be ob-served from the out-family setting shown in . In particular, the teacher GPT-3.5 showed amore robust baseline performance (). Al-though Instruction-tuning alone transfers some ofthe abilities to the student models, they were sig-nificantly lower when compared to the out-familyteacher models. In contrast, the teacher-studentperformances significantly converged after the self-refine phase, leading to the alignment completion.Finally, a positive impact can also be observed onthe quality of students generations (reported in Ap-pendix H), as shown in the additional experimentdiscussed in Appendix J. The performances appear completely aligned,but the students were tested only for in-domaintasks. The proposed approach could cause studentsto over-specialize in in-domain tasks, running therisk of losing the ability to solve out-domain tasks.For this reason, we performed a set of assessmentsevaluating students on in-domain and out-domaintasks and discussed the results in .3.",
  "Cross Self-refine65.3.461.3.162.1.260.7.573.4.3-": ": Evaluation of Llama-2-7 Instruction-tuned (Instruction-tuned) and with completely Self-refineInstruction-tuning (+ Self-refine Instruction-tuned) on demonstrations using different test sets. We evaluatein-domain (QA vs QA) and out-domain (QA vs math-word problem) benchmarks. \"Baselines\" are referred to thenon-instructed model. Results colored in green indicate the in-domain benchmark, blue the out-domain bench-mark, and orange the same benchmark on which perform the evaluation phase. Moreover, we propose Self-refineInstruction-tuning in cross-setting scenario where we optimize the model on the training set related to the evaluatedtask.",
  "In-Domain and Out-Domain": "The Self-refine Instruction-tuning approach com-plements student-teacher alignment and improvesstudents performances in out-domain tasks. Theseresults can be observed in with Llama2-7as students and Llama2-70 as teachers (in Ap-pendix with Llama2-13 withMistral-7). In particular, behind the evaluationsperformed on in-domain and out-domain tasks,the students Self-refine Instruction-tuned outper-form the baselines and the Instruction-tuned mod-els. These results also affect downstream perplexi-ties as shown in the experiments in the AppendixI. Furthermore, to observe the impact of the opti-mization phase (introduced in .2) on thedownstream performance, we conducted a furtherexperiment by fixing the Instruction-tuning phaseand switching the Self-refine ones across differentevaluation tasks (e.g., we instructed a student onOBQA and then optimized via self-refine approachon CSQA). As shown in lines Cross Self-refineof , students warmed up on tasks other thanthose they are optimized, outperformed the oth-ers, and obtained similar performances to thoseobtained from in-domain models. This shows that optimization positively impacts the alignment ofabilities in out-domain tasks. Finally, followingevaluations in out-domain tasks and across scenar-ios, we evaluate the performance of the proposedapproach by reducing the number of demonstra-tions available for alignment in .4.",
  "Low-resource Optimization": "Self-refine Instruction-tuning achieves sustainableperformance in low-resource settings. In fact, per-forming additional tuning phases does not substan-tially improve the models performance (experi-ments in Appendix K). Moreover, in , it ispossible to observe that the performance achievedby the self-refined students consistently outper-forms that of the non-self-refined students (whereonly phase 1 described in .1 was per-formed) (technical details on the breakdown can befound in Appendix C.1). Although it emerges thatonly the optimization process via DPO is more per-formant than the instruction-tuning process alone,the combination of the two phases achieves thebest results in both in-family and out-family align-ment in each proposed splitting that are describedin Appendix C.1.",
  "Multi-step Reasoning": "Previous works focus on Chain-of-Thought (CoT)prompting techniques, investigating the impactof prompting design and engineering, proposingspecialized interventions to improve CoT perfor-mances and fine-grained multi-step reasoning prop-erties (Wei et al., 2022).On the prompting design side, several works pro-posed using programs-like demonstrations as a CoTprompt, achieving more accurate reasoning stepsand significant improvements behind CoT prompt-ing. In parallel, there have been improvementsin the accessibility of lower-parameter versions ofLarge Language Models (LLMs), which we defineas Small Language Models (SLMs), on which pre-vious CoT improvements cannot be fully observed(Shridhar et al., 2023; Ho et al., 2023; Ranaldi andPucci, 2024). Therefore, several contributions areemerging at this gap, aiming to transfer LLM rea-soning properties to SLMs. Pioneering proposals inthis direction proposed teacher-student alignmentmethods through a series of approaches geared to-wards the distillation of the knowledge generatedby the teacher for the fine-tuning of the student(Li et al., 2023b; Magister et al., 2023; Shridharet al., 2023). Later, Yue et al. (2023) proposed spe-cialized Instruction-tuning using Alpaca-like styledemonstrations (Taori et al., 2023) specialized formathematical tasks, while Luo et al. (2023); Xuet al. (2023) proposed supervised fine-tuning rein-forced with rewarding algorithms.",
  "Reinforcement Learning (RL)": "A significant component that promotes the gen-erative reasoning delivering CoT is provided byrefinement via RL methods. Foundational work ap-plies Proximal Policy Optimization (PPO) (Schul-man et al., 2017) for aligning human preferences(Ouyang et al., 2022). Several methods have beenproposed to improve the efficiency of alignment(Azar et al., 2023; Loem et al., 2024), includingDirect Preference Optimization (DPO) (Rafailovet al., 2023).In this work, we adopt RL to refine performanceover conventional SFT. For mathematical problemsolving, Uesato et al. (2022) trained an outcome- orprocess-based reward model to perform re-ranking(Cobbe et al., 2021), achieving better performancethan SFT and majority voting (Wang et al., 2023b).(Luong et al., 2024) adopted reinforcement learn- ing as an extension of traditional supervised tuning.We adopt DPO and automate the reward process ina teacher-student context. We focus on the transferof CoT-style, step-wise reasoning and propose a re-finement technique applied to models downstreamof the instruction-tuning phase.",
  "Self-refined Instruction-tuning": "Complementing and enhancing foundational ap-proaches (Magister et al., 2023; Uesato et al., 2022;Li et al., 2023a; Ho et al., 2023), several papershave been published simultaneously Wang et al.(2023d); Luo et al. (2023); Wang et al. (2023a);Paul et al. (2024); Luong et al. (2024); Ranaldiand Freitas (2024) ( summarises the mainfeatures). These works prove the effect of super-vised fine-tuning to transfer the ability to producemulti-step reasoned answers from larger to smallermodels, as described in .2. Our work goesbeyond the state-of-the-art by: proposing a method for aligning CoT abilitiesby introducing Instruction-tuning via demon-strations produced by answers generated bydifferent LLMs, decentralizing the uniqueteacher model (in many cases GPT-3.5,4). analyzing the alignment performance betweenin-family and out-family models on differenttasks related to commonsense and math rea-soning, identifying crucial alignment factorsthat arise between teachers and students.",
  "Conclusion": "This paper proposes a novel approach for align-ing multi-step CoT reasoning between teacherLarge Language Models (LLMs) and studentSmaller LMs (SLMs).In particular, our Self-refine Instruction-tuning is framed as an instruc-tion tuning via Chain-of-Thought demonstrationsmethod based on explanations delivered by LLMsprompted by the CoT mechanism, which is thenreinforced via the Self-refine phase that uses DirectPreference Optimization. We also contrast the im-pact of in-family and out-family alignment acrossteacher and student models.The investigation of Large Language Models(LLMs) reasoning capabilities is a rapidly evolv- ing domain. Multiple studies have been proposingtechniques to increase, transfer, and align reason-ing capabilities in English (detailed reported in). Several works are emerging that aimto transfer these reasoning capabilities beyond En-glish (Ranaldi and Pucci, 2023). In future devel-opments, we would like to extend our contributionto non-English languages to broaden their benefi-cial impacts and operability by extending previousalignment (Ranaldi et al., 2024c,a) and augmenta-tion (Ranaldi et al., 2023, 2024b) approaches.",
  "only English-language prompting methodsand tasks are considered. The understandingof these methods across different languagesstill needs to be established": "dependence on Large Language Models,where the supporting training sets are not al-ways fully known. Consequently, character-izing the differences in pre-training data be-tween models is not fully possible, where theanalysis is constrained to observing the out-puts in natural language. In conclusion, learning from and with demonstra-tions carries some specific risks. Although a modelmay generalize its predictions using a seeminglyconsistent series of natural language steps, evenif the prediction is ultimately correct, there is noguarantee that the predicted output comes from aprocess represented by the generalization. A end-user might be overconfident in the model based onthe CoT mechanism.",
  "Ethical Statement": "Although this research enhances the reasoning abil-ities of Smaller Language Models, they still needto be made sufficiently robust to be applied withinmore critical domains. Further safety and out-of-distribution generalisation mechanisms needs to bedeveloped in tandem with the application of themethods described in this paper, in order to estab-lish the robustness of the described mechanisms.",
  "Yonatan Bisk, Rowan Zellers, Ronan Le Bras, JianfengGao, and Yejin Choi. 2019. Piqa: Reasoning aboutphysical commonsense in natural language": "Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish,Alec Radford, Ilya Sutskever, and Dario Amodei.2020. Language models are few-shot learners. Sbastien Bubeck, Varun Chandrasekaran, Ronen El-dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Pe-ter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro,and Yi Zhang. 2023. Sparks of artificial general in-telligence: Early experiments with gpt-4. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, ReiichiroNakano, Christopher Hesse, and John Schulman.2021. Training verifiers to solve math word prob-lems. ArXiv, abs/2110.14168.",
  "Namgyu Ho, Laura Schmid, and Se-Young Yun. 2023": "Large language models are reasoning teachers. InProceedings of the 61st Annual Meeting of theAssociation for Computational Linguistics (Volume1:Long Papers), pages 1485214882, Toronto,Canada. Association for Computational Linguistics. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, Llio Renard Lavaud,Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,Thibaut Lavril, Thomas Wang, Timothe Lacroix,and William El Sayed. 2023. Mistral 7b. Albert Q. Jiang, Alexandre Sablayrolles, AntoineRoux, Arthur Mensch, Blanche Savary, ChrisBamford, Devendra Singh Chaplot, Diego de lasCasas, Emma Bou Hanna, Florian Bressand, Gi-anna Lengyel, Guillaume Bour, Guillaume Lam-ple, Llio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,Sophia Yang, Szymon Antoniak, Teven Le Scao,Thophile Gervet, Thibaut Lavril, Thomas Wang,Timothe Lacroix, and William El Sayed. 2024. Mix-tral of experts. Liunian Harold Li, Jack Hessel, Youngjae Yu, Xi-ang Ren, Kai-Wei Chang, and Yejin Choi. 2023a.Symbolic chain-of-thought distillation: Small mod-els can also think step-by-step. In Proceedingsof the 61st Annual Meeting of the Associationfor Computational Linguistics (Volume 1:Long",
  "Leonardo Ranaldi and Giulia Pucci. 2024. When largelanguage models contradict humans? large languagemodels sycophantic behaviour": "Leonardo Ranaldi, Giulia Pucci, and Andr Freitas.2024a.Does the language matter?curriculumlearning over neo-Latin languages. In Proceedingsof the 2024 Joint International Conference onComputational Linguistics, Language Resourcesand Evaluation (LREC-COLING 2024), pages 52125220, Torino, Italia. ELRA and ICCL. Leonardo Ranaldi, Giulia Pucci, and Andre Fre-itas. 2024b.Empowering cross-lingual abilitiesof instruction-tuned large language models bytranslation-following demonstrations. In Findings ofthe Association for Computational Linguistics ACL2024, pages 79617973, Bangkok, Thailand and vir-tual meeting. Association for Computational Linguis-tics.",
  "Association for Computational Linguistics: NAACL2024, pages 12291241, Mexico City, Mexico. Asso-ciation for Computational Linguistics": "Leonardo Ranaldi, Giulia Pucci, and Fabio MassimoZanzotto. 2023. Modeling easiness for training trans-formers with curriculum learning. In Proceedingsof the 14th International Conference on RecentAdvances in Natural Language Processing, pages937948, Varna, Bulgaria. INCOMA Ltd., Shoumen,Bulgaria. Subhro Roy and Dan Roth. 2015.Solving generalarithmetic word problems. In Proceedings of the2015 Conference on Empirical Methods in NaturalLanguage Processing, pages 17431752, Lisbon,Portugal. Association for Computational Linguistics.",
  "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, YannDubois, Xuechen Li, Carlos Guestrin, Percy Liang,and Tatsunori B. Hashimoto. 2023. Stanford alpaca:An instruction-following llama model": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. Jonathan Uesato, Nate Kushman, Ramana Kumar, Fran-cis Song, Noah Siegel, Lisa Wang, Antonia Creswell,Geoffrey Irving, and Irina Higgins. 2022. Solvingmath word problems with process- and outcome-based feedback.",
  "Xuezhi Wang, Jason Wei, Dale Schuurmans, QuocLe, Ed Chi, Sharan Narang, Aakanksha Chowdhery,and Denny Zhou. 2023b. Self-consistency improveschain of thought reasoning in language models": "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-isa Liu, Noah A. Smith, Daniel Khashabi, and Han-naneh Hajishirzi. 2023c.Self-instruct: Aligninglanguage models with self-generated instructions.In Proceedings of the 61st Annual Meeting of theAssociation for Computational Linguistics (Volume1:Long Papers), pages 1348413508, Toronto,Canada. Association for Computational Linguistics. Zhaoyang Wang, Shaohan Huang, Yuxuan Liu, Jia-hai Wang, Minghui Song, Zihan Zhang, HaizhenHuang, Furu Wei, Weiwei Deng, Feng Sun, andQi Zhang. 2023d.Democratizing reasoning abil-ity: Tailored learning from large language model. InProceedings of the 2023 Conference on EmpiricalMethods in Natural Language Processing, pages19481966, Singapore. Association for Computa-tional Linguistics. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,Barret Zoph, Sebastian Borgeaud, Dani Yogatama,Maarten Bosma, Denny Zhou, Donald Metzler, Ed H.Chi, Tatsunori Hashimoto, Oriol Vinyals, PercyLiang, Jeff Dean, and William Fedus. 2022. Emer-gent abilities of large language models.",
  "AInstruction-tuning": "The Instruction-tuning proposed in our contribution follows the pipeline proposed in (Ranaldi and Freitas,2024) to achieve teacher-student alignment, which comprises two steps: annotation and knowledge transfer.In the annotation phase, Large Language Models (teachers) are prompted with questions (). Theanswers are collated and form the demonstrations (). This is followed by an Instruction-tuningphase, conducted using what was proposed in (Taori et al., 2023). In particular, the demonstrations areconstructed with triples formed by the instruction (a pattern that guides the generation related to thetask), the input, the question related to the mathematical problem or the desired question, and the outputthe prompted LLM generated. Note that instruction and input can oftentimes be concatenated, but thisdepends on the basic configurations of the patterns and the type of task to be solved. The instruction-tuningprocess, a specialization of fine-tuning, is similar to the latter and is described in .1. In order tofacilitate reproducibility, we shared our code along with the submission.",
  "BSelf-refine Tuning": "In order to refine Small Language Models (students) instructed via demonstrations delivered by LargeLanguage Models (teachers), we propose the Self-refine phase (introduced in .2). In particular,this is based on a variant of the DPO optimization algorithm (Rafailov et al., 2023).Starting from the demonstrations defined as D = (ii, qi, ai) where i D (note that ai are generated usingCoT prompt as showed in Appendix D), we prompt the students using the input xi = ii + q and xi = q+\"Lets think step by step\") i D (the latter is the proposed one in the main paper).Hence, for each element in demonstrations, we collect the Answers (yi = inst(xi)) that are the answersgenerated by the student given the input xi, and the CoT-Answers (yCoT = inst(xi)) are the answersthat deliver CoT generated by the student elicited via CoT mechanism xi.Hence, we introduce:",
  "C.1Data Splitting": "In order to observe the impact of the demonstrations, we produced a series of experiments by systematicallydecreasing the Instruction-tuning data. In particular, we chose three sub-sets with 75%, 50%, and 25%from the total number of demonstrations. In detail, the Self-refine Instruction phases on the number ofequal demonstrations are performed by taking about 3000 examples in splitting 100%, 2250 in splitting50%, 1500 in splitting 50%, and 750 in splitting 25%. We chose the value 3000 because it has the smallestCoT demonstrations available. For the total demonstrations, we selected random samples. Using this split,we performed the evaluations incrementally as the demonstrations were used to perform Instruction-tuning,Self-refine, and Self-refine Instruction-tuning.",
  "C.2Parameters": "The annotation phase that the Teachers performed was done on the training set. The evaluation phase ofboth the basic models and the Students and the Teachers was done on the test splitting. The evaluation,described in .3, was done with question probing and string matching of the generated answers.More specifically: TeachersWe performed the annotation phase for each benchmark by delivering to GPT-3.5-turbo,Mixtral7x8 and Llama-2-70-chat the prompts structured as shown in and (customizedfor each benchmark). We set the temperatures to 0.7 for GPT-3.5-turbo and 0.1 for Llama-2-70-chat asrecommended in technical reports. Moreover, we kept all the other parameters as default. All parametersare shown in our code available at the following link. Baseline & StudentsWe evaluated the performance of the Small Language Models (Llama-2-7-chat,Llama-2-13-chat, Mistral-7b) by prompting them with the same format used for the Teachers. Forboth the baselines and the instructed models, we set the temperature to 0.1 and kept all the other parametersas default. The evaluation pipelines and generation parameters are available in our code.",
  "InputQuestion: Which form can be made into a triangle?Choices:A) iceB) steamC) waterD) salt waterAnswer: Lets think step-by-step": "Output (that is the Answer produced by teacher model)Answer:A) ice: Ice is a solid form of water. In solid form, substances can be molded or shaped intovarious forms, including triangles. If you had a block of ice, you could carve or mold itinto a triangular shape.B) steam: Steam is the gaseous form of water. Being a gas, it doesnt have a fixed shape. Youcant shape steam into a triangle without containing it within some form of solid boundary,and even then, the steam itself isnt triangular - the container is.C) water: Water is a liquid that takes the shape of its container. You can pour water intoa triangular container, and while the volume inside would be triangular, the water itselfisnt inherently triangular.D) salt water: This is essentially the same as option C, but with salt dissolved in it. Beinga liquid, it takes the shape of its container.Given these considerations, the only substance among the options that can be shaped into atriangle by itself is A) Ice.",
  "Cross Self-refine66.2.362.4.163.2.361.5.473.9.2-": ": Evaluation of Llama-2-13 Instruction-tuned (Instruction-tuned) and with completely Self-refineInstruction-tuning (+ Self-refine Instruction-tuned) on demonstrations using different test sets. We evaluatein-domain (QA vs QA) and out-domain (QA vs math-word problem) benchmarks. \"Baselines\" are referred to thenon-instructed models. Results colored in green indicate the in-domain benchmark, blue the out-domain benchmark,and orange the same benchmark on which the evaluation phase is performed. Moreover, we propose Self-refineInstruction-tuning in cross-setting scenarios where we optimize the model on the training set related to the evaluatedtask.",
  "Cross Self-refine64.7.465.8.264.1.461.5.470.1.3-": ": Evaluation of Mistral-7 Instruction-tuned (Instruction-tuned) and with completely Self-refineInstruction-tuning (+ Self-refine Instruction-tuned) on demonstrations using different test sets. We evaluatein-domain (QA vs QA) and out-domain (QA vs math-word problem) benchmarks. \"Baselines\" are referred to thenon-instructed model. Results colored in green indicate the in-domain benchmark, blue the out-domain bench-mark, and orange the same benchmark on which perform the evaluation phase. Moreover, we propose Self-refineInstruction-tuning in cross-setting scenario where we optimize the model on the training set related to the evaluatedtask.",
  "HQuality of Generations": "To demonstrate the quality of the demonstrations generated by the teachers and students, we proposeannotating the responses provided by the teacher and student models automatically. In particular, wesampled 300 questions (50 questions for each task from the testing set split). Hence, we systematicallyprompt both the teacher LLMs and students. Finally, we estimated the quality of the responses generatedby systematically prompting a judge LLM (we chose GPT-4 as it is not among the models used in thiswork). Please act as an impartial judge and evaluate the quality of the responseprovided by an AI assistant to the user instruction displayed below. Yourevaluation should consider factors such as quality, accuracy, depth, andlevel of detail.Begin your assessment with a short explanation.Be asobjective as possible.After providing your explanation, please rate theresponse on a scale of 1 to 3 strictly following this format:[[rating]],for example: Rating: [].[question]${question}[AI assistants response]${response}",
  "IPerplexity Score": "In this experiment, we reported the perplexity scores of our models on out-domain demonstrations. Inparticular, we assess the perplexity score of student models by providing them with texts generated by anin-family and an out-family teacher. We want to show that although Instruction-tuning (IT) is a stablemethod for alignment, it is not enough for the out-family student (GPT demonstrations are less formedthan Llama2-70 if the student is Llama-2-7). On the other hand, after SFT+DPO, the final perplexities aresignificantly lower on out-od-domain tasks.",
  "JEvaluation on Answers without CoT": "In these experiments, we studied the impact of demonstrations that do not deliver multi step-reasonedand negative answers. Hence, we replicated the experimental setting proposed in with differenttraining data. Specifically, we reproduced the two phases considering only non-CoT demonstrations(we prompted Teachers without CoT elicitation as in Appendix D). Secondly, we collected the correct\"(positive)\" and incorrect \"(negative)\" answers and performed a further evaluation separately. In thisfurther experiment, we want to show that the quality of demonstrations plays a significant role, and inparticular, demonstrations that deliver misleading multi-step reasoned CoT affect the functioning of theproposed method.",
  "KEvaluation on Additional tuning": "In the setup proposed in , we proposed to use Instruction-tuning (IT) plus Direct PreferenceOptimization (DPO). Here, we reported the performances achieved by only IT and DPO. Moreover, in.2.1, we have adopted standard experimental set-ups (commonly used by similar works ). However, we have strengthened this experiments IT and DPO phases by doubling the epochs andrefinement steps and named them as double.",
  "LAdditional Evaluations": ": Accuracies (%) additional benchmarks as described in .1. Applying the same pipeline proposedin and the same experimental set-up () as the experiments shown in and . Inthis experiment, we showed that the approach proposed in is also scalable on multi-task benchmarks suchas MATH (Hendrycks et al., 2021b) and MMLU (Hendrycks et al., 2021a). (Self-refine Instruction-tuning phaseperformed using 25% as the training set and omitted in the evaluation phase) (as described in the legend, we use thenotation method(Teacher->Student))."
}