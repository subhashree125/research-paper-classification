{
  "Abstract": "We present three innovations in tokenizationand subword segmentation. First, we proposeto use unsupervised morphological analysiswith Morfessor as pre-tokenization. Second,we present an algebraic method for obtain-ing subword embeddings grounded in a wordembedding space. Based on that, we designa novel subword segmentation algorithm thatuses the embeddings, ensuring that the proce-dure considers lexical meaning. Third, we intro-duce an efficient segmentation algorithm basedon a subword bigram model that can be ini-tialized with the lexically aware segmentationmethod to avoid using Morfessor and large em-bedding tables at inference time. We evaluatethe proposed approaches using two intrinsicmetrics and measure their performance on twodownstream tasks: part-of-speech tagging andmachine translation. Our experiments showsignificant improvements in the morphologicalplausibility of the segmentation when evaluatedusing segmentation precision on morphemeboundaries and improved Rnyi efficiency in8 languages. Although the proposed tokeniza-tion methods do not have a large impact onautomatic translation quality, we observe con-sistent performance gains in the arguably moremorphological task of part-of-speech tagging.",
  "Introduction": "Statistical approaches to subword segmentation arethe state of the art in most natural language process-ing (NLP) applications of neural networks, mostnotably the Transformer model (Vaswani et al.,2017). The Unigram model from SentencePiece(Kudo and Richardson, 2018) and Byte-Pair En-coding (BPE; Sennrich et al., 2016) are amongthe two most widely employed tokenization tech-niques. These methods gained popularity becauseof their versatility they are language-independentand have convenient properties for model training,reducing the vocabulary size while assuring evenlearning of the token representations.",
  "(3)": ": We organize subword tokenization learninginto four steps: pre-tokenization, vocabulary learning,inference, and distillation for efficiency. Steps (1)(3)highlighted in yellow are specific contributions of thispaper. Despite the indisputable advantages, one aspectof the statistical word segmentation algorithms hasremained a thorn in the eyes of many linguistically-oriented researchers: Subwords do not reflect mor-phology. This problem is especially pronounced inmultilingual models, which share a common vocab-ulary across all languages. Without a careful andbalanced data selection, lower-resourced languagestend to have fewer allocated subwords, resulting ina large token-to-word ratio (Haddow et al., 2022;Limisiewicz et al., 2023).We posit that a strong segmentation retains theproperty of the statistical approaches, i.e., that fre-quent words are split into fewer tokens than rarewords. However, once a word is split into more to-kens, the subword boundaries should ideally matchthe actual morpheme boundaries.1 We hypothe- 1We use the word morpheme for morphologically moti-vated subword units. Some theories (abokrtsk et al., 2022)distinguish morphs as surface realizations of abstract mor-phemes as the smallest units of meaning. Where appropriate,we follow this distinction for clarity. By morpheme bound-aries, we mean boundaries between morphs within a word. size that the standard algorithms lack morphologyawareness because they do not work with lexicalmeaning, which is a crucial concept in languagemorphology.Following Schmidt et al. (2024), we conceptu-alize tokenization as a process with three steps (asillustrated in ): pre-tokenization, vocabu-lary construction, and segmentation. Within thisconceptual framework, we propose three innova-tions throughout the whole process:",
  "(3) We propose an efficient statistical segmenta-tion algorithm using subword bigram statisticsthat can be used to distill complex tokeniza-tion pipelines into an efficient algorithm": "In , we discuss pre-tokenization andvocabulary construction. Besides the standard pre-tokenization, which splits the text into word-likeunits (words, punctuation, etc.), we also experimentwith Morfessor (Smit et al., 2014), which we applyon top of the word-like pre-tokenized text.For lexically grounded segmentation, we derivea formula for computing subword embeddings us-ing a pre-trained word embedding model and atraining corpus (.1). Next, we use thesubword embeddings to design a subword segmen-tation algorithm based on semantic similarity be-tween the word and its subwords (.2).Finally, we propose a subword-bigram-basedstatistical segmentation algorithm that retains theproperties of the embedding-based segmentation(). With the bigram-based algorithm, wecan have a model for subword segmentation thatdoes not require running Morfessor or storing alarge embedding table.We test our approach using two intrinsic eval-uation metrics and two downstream tasks (Sec-tion 5.1). In the intrinsic evaluation, we test ourapproach on the SIGMORPHON 2018 shared taskdataset (Batsuren et al., 2022) and observe sig-nificantly better morphological generalization inboth proposed algorithms with a fixed vocabularysize. We also measure the Rnyi efficiency (Rnyi,1961) of the unigram distribution of the segmented text, which has been shown to correlate with down-stream model performance (Zouhar et al., 2023).Additionally, we evaluate our segmentation algo-rithm on Part-of-Speech (POS) Tagging using Uni-versal Dependencies (Zeman et al., 2024), showingan improvement compared to other segmentations.Finally, we evaluate our tokenization on machinetranslation using a simulated low-resource IWSLT2017 dataset (Cettolo et al., 2017) where we reachresults comparable with currently used subwordtokenizers.We show the code examples in Appendix A",
  "Pre-tokenization and VocabularyConstruction": "Neural networks can only have limited vocabular-ies in order 104105, which rules out using word-based vocabularies. A common solution is statisti-cal heuristics that keep frequent words intact andsplit rare words into smaller units, ensuring thatthere are no rare tokens, such that embeddings of alltokens get updated reasonably often. The most pop-ular methods are Byte-Pair Encoding (BPE; Sen-nrich et al., 2016) based on greedily merging themost frequent token pairs and the Unigram model(as implemented in SentencePiece; Kudo, 2018)that returns high-probability segmentations using aunigram language model. However, these methodsmanifest low morphological generalization, whichin turn might lead to reduced interpretability, com-positional generalization, and cross-lingual transfercapabilities.Perhaps the most straightforward approach forlexically grounded word segmentation is to use un-supervised morphological analyzers, such as Mor-fessor. However, direct use of these linguisticallymotivated tools leads to worse results (Machceket al., 2018) and is only beneficial in low-resourcescenarios (Soulos et al., 2021; Gaser et al., 2023).Furthermore, morphological analysis does not fullyaddress the problems of rare tokens and vocab-ulary size. To address these issues, we proposeonly using morphological analyzers during pre-tokenization (Step 1 in ).After pre-tokenization, we apply the well-established statisti-cal methods for vocabulary construction. This com-bination ensures that there will be a low number",
  "Segmentation with SubwordEmbeddings": "In this section, we describe a novel lexically-grounded segmentation method (Step 2 in Fig-ure 1).When considering language morphology, we as-sume the word can be decomposed into severalsmaller meaningful units that carry the meaningof the original word when combined together. Weconsider the segmentation of a word to be lexicallygrounded when it respects the words meaning anddoes not introduce subword boundaries in the mid-dle of meaningful units. To find such a segmenta-tion, we need to model the meaning of both wordsand subword units jointly.4 A widely used proxy for capturing the lexicalmeaning of words is word embeddings. To capturethe meaning of subwords, we introduce a methodto compute subword embeddings in a shared spacewith the word embeddings ( 3.1). We also describea segmentation algorithm that takes the subwordembeddings into account ( 3.2).",
  "Subword Embeddings": "We obtain the joint embedding model of wordsand subwords by extending the skip-gram model(Mikolov et al., 2013) to subword units. Specifi-cally, we derive a formula for computing the em-bedding of any substring in a training dataset, situ-ating its representation within the skip-gram modelembedding space.Skip-gram models are trained to produce a proba-bility distribution of words that are likely to appearwithin a certain context window around a giveninput word x. When we extend this model to han-dle substrings, each substring is used to predict thewhole words that appear within the context windowof any word that contains the substring. As a result,the embeddings of the substrings are determinedby the contexts of the words they are part of.To compute the subword embeddings, we re-quire a tokenized training dataset D and a trainedskip-gram word embedding model with a vocabu-lary V. In addition to its input embedding matrix 4Linguistic theories often work with the concept of morphsand morphemes as the smallest meaningful units. However,our solution tries to be theory-agnostic, so it can work withany subword units regardless of their theoretical justification.",
  "E R|V|d where d is the dimension of the wordembedding vectors, we also need the output matrixW Rd|V|": "The statistics of skip-gram models.Using dataD, we denote the symmetric word cooccurrencematrix C R|V||V| that for each pair of wordsx, y V, Cx,y contains the frequency of x and yappearing within the same context window in D.Then, our method relies on the following observa-tion:softmax(EW) norm(C)(1) where norm means row-wise normalization.This follows from the fact that the skip-grammodel optimizes cross-entropy between the pre-dicted distribution of neighboring words and theempirical distribution in the training data. It is usu-ally approximated by stochastic minibatch trainingwith negative sampling instead of computing thefull softmax. The empirical distribution can be ob-tained by normalizing the count matrix C, whichleads to the following optimization problem:",
  "minE,W XENT(softmax(EW), norm(C))(2)": "By Gibbs inequality, the cross-entropy is mini-mum if softmax(EX) = norm(C). This leadsto Equation 1. We use the approximation sign() to stress that stochastic optimization solvesthe problem only approximately. When trainingword embeddings, we must find both E and W.When extending the model for subwords, we keepthe W fixed, and we only need to find the (newlyadded) subword portion of E, which we call Es. Extension to subwords.Next, we choose a setof subwords S. We either select the set of all sub-strings present in D up to a certain length, or we usethe set of subwords from an existing segmentation.We then define a segmentation matrix A R|S||V|",
  "Segmentation": "In this section, we apply the subword embeddingmodel to lexically grounded subword segmenta-tion. We propose an algorithm based on the word-subword similarities within the shared embeddingspace. Following the Unigram model from Senten-cePiece (Kudo, 2018), which searches for a seg-mentation that maximizes the probability under asubword unigram model, we use a dynamic pro-gramming algorithm (shown in Algorithm 1 in Ap-pendix A) to find the segmentation (sequence ofsubwords) that maximizes a similarity-based score.Formally, for a word x and a segmentations1, s2, . . . , sn, the similarity score is the sum ofcosine similarities between the embedding of xand the embeddings of each of the subwords si,minus a length penalty of per each subword:",
  "(E(x)) (Es(si))E(x) Es(si) .(6)": "Increasing the value of forces the algorithm touse fewer subwords. In other words, controlswhat weight we put to the semantic similarity andwhat weight we put to minimize the number ofsubwords. Based on preliminary results, we set to 1 and keep it fixed in all experiments.Unlike the Unigram segmentation, the subwordscores are not static but depend on the segmentedword. Therefore, the segmentation can be viewedas a word-specific unigram model.As stated in the previous section, the computa-tion of the subword embeddings requires an exist-ing subword vocabulary S and the segmentationmatrix A. We initialize S with the set of subwordsused by another segmentation algorithm. We onlyset As,x = 1 when s has been used as a subwordof x.After initialization, we iteratively refine the seg-mentation in two alternating steps until conver-gence.",
  "Bigram model": "The segmentation algorithm described in the previ-ous section has several drawbacks: It requires stor-ing relatively large embedding tables for words andsubwords and does not generalize for OOV wordswithout embeddings. Moreover, pre-tokenizationwith Morfessor requires running language-specificmodels, making the segmentation more computa-tionally demanding than the established method.We avoid this drawback by introducing an alter-native segmentation algorithm based on subwordbigram statistics. It is a straightforward general-ization of the commonly used Unigram model. Atinference time, we search for a segmentation thatmaximizes probability predicted by a subword bi-gram model instead of a unigram model. The op-timization problem is solvable using dynamic pro-gramming, similar to the Unigram model. How-ever, the algorithm has a quadratic complexity inthe segmented string length. Therefore, we proposeusing a linear-time beam search algorithm that onlyconsiders k best segmentations in each step. Thefull algorithm is described in Algorithm 2 in Ap-pendix A.We use the subword bigram statistic obtained bycounting subword bigram and unigram frequenciesin a corpus tokenized by a tokenizer that we wantto distill into the bigram model. To account forunknown bigrams encountered during inference,we need to eliminate zero probabilities from the bi-gram distribution. To this end, we apply Laplaciansmoothing, i.e., we increase the frequency of everybigram (si|si1) by one. Additionally, if si1 isan unknown unigram, we assign the unigram prob-ability of si to the bigram. If both si and si1 areunknown unigrams, we assign uniform probability1/|S| to the bigram.",
  "and Rnyi efficiency (Rnyi, 1961) of the token dis-tribution, which was shown to be a good predictorof downstream performance of a tokenizer (Zouharet al., 2023)": "Test data.For the morpheme boundary evalua-tion, we use the test set from the SIGMORPHON2022 Shared Task on Morpheme Segmentation(Batsuren et al., 2022), which contains test datafor nine languages (Czech, English, Spanish, Hun-garian, French, Italian, Russian, Latin, Mongolian).We omit Latin due to the lack of resources for train-ing word embeddings. Except for Czech (whichcontains surface-level segmentation into morphs),each test set consists of word decompositions intomorphemes. This means that the original wordscannot be reconstructed by simply concatenatingthe morphemes. To be able to evaluate word seg-mentations in all languages, we use a set of heuris-tic rules to map the morphemes to the surface form.To measure the Rnyi efficiency of the tokendistribution, we use 4,000 sentences randomly sam-pled from the (plain text) training data described inthe following paragraph. Experimental settings.We use the skip-grammodel from FastText (Bojanowski et al., 2017)to train the word embeddings. For all languagesexcept Mongolian, we train the model on 50Msentences from NewsCrawl (Kocmi et al., 2022).We use 15M sentences from CC-100 (Conneauet al., 2020) for Mongolian. We lowercase andpre-tokenize the text using Sacremoses,5 and forexperiments with Morfessor pre-tokenization, wetrain Morfessor (Smit et al., 2014) with the defaultparameters. We apply Morfessor on already pre-tokenized text as a second step. We use a vocabu-lary size of 200k, an embedding dimension of 200,and a window size of 5. We train the embeddingsfor 10 epochs for both pre-tokenization setups.As a baseline, we prepare BPE and Unigramtokenizers with vocabularies 1k, 2k, 4k, 8k, 16k,24k, 32k, and 48k using the same plain text dataset.We use the segmentation from the BPE and Un-igram subwords to initialize the matrix A fromEquation 3 and iterate our algorithm. Finally, weuse the bigram statistics from 200k embedding vo-cabulary and segment the test set using the subwordbigram language model.",
  "Segmentation evaluation.Unlike the originalSIGMORPHON shared task evaluation, where the": "evaluation metric was the F1 score measured on themorphemes themselves, we measure the morphemeboundary precision for a given vocabulary size. Webelieve this setup best captures the use of subwordtokenizers in neural networks where we have avocabulary budget given by the model architecture.However, we do report also recall and F1 score forcompleteness. Results.The main results for the 32k vocabularyare presented in . Across all languages,Unigram reaches better precision than BPE, consis-tently with previous work (Batsuren et al., 2022).Pre-tokenization using Morfessor consistently out-performs word-like pre-tokenization across all lan-guages in morpheme boundary precision.Us-ing lexically grounded embedding-based segmenta-tion improves compared to the default BPE andUnigram segmentation algorithms.The differ-ence is more pronounced with the word-like pre-tokenization. Distillation into the bigram modelusually leads to a small decrease in the bound-ary precision. The performance of BPE and theUnigram model for vocabulary construction islanguage-dependent.The Rnyi efficiency is significantly higher forMorfessor pre-tokenization.Unlike morphemeboundary precision, distilling the embedding-basedsegmentation into a bigram model has almost noeffect on Rnyi efficiency. Segmentation based onthe Unigram model vocabulary achieves the bestresults. shows morpheme boundary precision,recall, and F1 score for Czech for different vocab-ulary sizes; additional languages are presented inthe Appendix in . The boundary preci-sion increases with the increasing vocabulary size,whereas the recall has the opposite trend. Our seg-mentation methods improve the boundary precisionin all cases. Word-like pre-tokenization has a neg-ligible effect on recall. On the other hand, addingMorfessor to pre-tokenization decreases recall.We also show a random sample of segmentedCzech, English, and French words in the Appendixin .",
  "Word-like": "BPEOrig78.1 58.1 61.4 58.6 78.1 54.6 80.3 62.6.412 .425 .392 .417 .366 .433 .466 .407Emb.81.3 67.2 65.1 65.2 83.5 60.1 90.4 64.8.415 .431 .400 .424 .381 .439 .475 .418Big.81.8 68.7 65.4 66.3 83.4 61.0 90.4 67.7.416 .431 .401 .424 .382 .440 .476 .419 Uni.Orig85.6 66.7 65.0 66.9 81.8 55.1 92.0 67.8.417 .429 .394 .421 .376 .438 .474 .417Emb.87.9 69.9 66.7 68.1 83.7 58.7 91.5 68.4.418 .434 .404 .430 .384 .443 .464 .426Big.88.2 71.0 66.4 69.1 83.9 60.5 91.6 70.9.420 .434 .405 .430 .385 .444 .465 .427",
  "Morfessor": "BPEOrig90.6 73.5 69.1 69.7 85.4 66.8 92.5 71.8.446 .436 .421 .446 .388 .454 .497 .448Emb.89.9 72.6 67.3 68.5 86.3 63.6 93.0 72.6.449 .439 .424 .448 .397 .456 .499 .453Big.89.4 71.4 67.1 68.6 85.5 64.6 92.5 72.5.449 .439 .424 .448 .396 .456 .499 .454 Uni.Orig90.2 70.8 65.7 65.8 85.4 63.3 91.9 71.6.456 .441 .426 .452 .395 .452 .491 .460Emb.91.4 70.4 65.0 65.7 86.9 63.3 92.7 73.7.456 .441 .428 .454 .400 .458 .492 .456Big.90.8 70.0 65.5 66.4 86.1 63.2 92.1 73.1.457 .441 .429 .454 .400 .459 .492 .459 : Morpheme boundary precision on the SIGMORPHON 2018 test set and Rnyi efficiency estimated on 4kplain text sentences for tokenizers with 24k and 40k-sized vocabularies. The best results in each column are bolded.The blue-yellow scale is fit to the value range per column.",
  "the intrinsic evaluation except for Mongolian,which does not have a UD corpus. See in the Appendix for details of the corpora": "Model details.We train an LSTM-based tagger.We use an embedding layer of 300, two bidirec-tional LSTM layers (Hochreiter and Schmidhuber,1997) of dimension 600, and a final projection into18 POS tags. We use a batch size of 256 sentencesand train for 3,200 steps using the Adam optimizer(Kingma and Ba, 2015) with a learning rate of 0.01.We select the best weights based on the loss on thedevelopment set. We prepend each word with aspecial word-separator token for subword segmen-tation and copy the POS tag to all its subwords. Atinference time, we predict the tag from a distribu-tion that averages the predictions for the individualsubwords. We are aware that there are methods that would improve the performance of the taggertrained from scratch, e.g., including character-levelfeatures and using pre-trained word embeddings.In our experiments, we are mainly interested inhow informative the segmentation is for the tagger. Data preparation.We experiment with severalsegmentation methods. As a baseline, we use theword segmentation provided in UD and word seg-mented using Morfessor. Further, we experimentedwith word-like pre-tokenization, Morfessor pre-tokenization, and BPE and Unigram for vocabularyconstruction. For segmentation, we tested both theoriginal subword segmentation corresponding toBPE and the Unigram model (denoted as Orig. inthe results) and distilled bigram models created viathe lexically grounded embedding-based segmenta-tion (denoted as Ours in the results).",
  "Results.The results are presented in": "(with more details in in the Appendix).In general, subword-based segmentation signifi-cantly outperforms word-like and Morfessor-basedmodels. Morfessor pre-tokenization is slightly bet-ter than word-like pre-tokenization only in all lan-guages, with a particularly pronounced differencein Hungarian, the only language in our test setswith agglutinative morphology. Our segmentationalgorithm consistently improves over the defaultBPE and Unigram algorithms. The overall besttokenization approach combines the Morfessor pre-tokenization followed by the BPE algorithm forvocabulary construction and our bigram-based seg-mentation.",
  "As a second downstream task, we evaluate our seg-mentation on machine translation (MT) in a simu-lated low-resource setup": "Experimental setup.We use the IWLST 2017dataset of 18 language pairs (involving combina-tions of Arabic, English, Dutch, German, Italian,and Romanian) with the provided data splits fortrain, validation, and testing. The exact languagepairs and dataset statistics are in the Appendix in. Similarly to POS tagging, we experimentwith word-like and Morfessor pre-tokenization,BPE, and Unigram vocabulary construction (jointlyon parallel data) and compare the default segmen-tation (Orig.) algorithms with the bigram-basedsegmentation distilled from the embedding-basedsegmentation algorithm (Ours).We use the Transformer Base model (Vaswaniet al., 2017) as implemented in Marian (Junczys-Dowmunt et al., 2018). We train the models usingthe Adam optimizer with learning rate 104 andthe inverse square learning rate decay with 4,000warmup steps with effective batch size 18,000 to-kens. Results.We evaluate the MT quality using thechrF scores (Popovic, 2015),6 see in theAppendix for complete results. At first glance,there are only minor differences in translation qual-ity across the tested methods and language pairs,except for a few outliers. Therefore, in ,7 we provide aggregated results across the languages:We first compute the mean chrF score per languagepair and subtract it from the scores. Finally, weaverage the difference from the mean across lan-guages. The results show that the word-based pre-",
  "Rnyi Efficiency": "Finally, we evaluate the correlation between the re-sults of our downstream tasks and Rnyi Efficiency.Zouhar et al. (2023) conducted a theoretical analy-sis of information-theoretical properties of tokeniz-ers and suggest to measure their unigram informa-tion efficiency. Information efficiency is the ratioof the unigram entropy of tokenized text and themaximum possible entropy given the vocabularysize. Instead of using the more common Shannonentropy, they use parametrized Rnyi entropy with = 2.5 that they claim better correlates with thedownstream performance on English-German MT.To verify the claims of Zouhar et al. (2023), wecomputed the Pearson correlation of the Rnyi effi-ciency of the training data in our experiments withthe model performance. Our results are presentedin . For POS tagging, Rnyi efficiency isa good predictor of tagger performance in mostlanguages except Czech. However, the correlationvaries strongly between languages. In MT, we didnot confirm the results of Zouhar et al. (2023): thecorrelation of the Rnyi efficiency of the trainingdata and the MT quality in terms of chrF is mostlynegative and highly varies across language pairs.",
  "Subword embeddings.There are relatively fewmethods for obtaining static subword embeddings": "FastText (Bojanowski et al., 2017) averages sub-word embeddings to obtain static word embeddings.However, subwords are stored in a hash table withmany conflicts for better memory efficiency, mak-ing the subword embeddings unusable for our pur-poses. Heinzerling and Strube (2018) trained sub-word embedding for 275 languages and variousvocabulary sizes using GloVe (Pennington et al.,2014) while treating subwords as standalone tokens.They, however, do not put the subword embeddingsinto relation to word embeddings. Static subwordembeddings are, as the first layer, a part of mostneural NLP models. However, none of the meth-ods explicitly models the relationship between thewords and subwords. Subword segmentation.Besides the standardBPE (Sennrich et al., 2016) and the Unigram model(Kudo, 2018), several more recent approaches tosubword segmentation exist. Xu et al. (2021) useoptimal transport to find a replacement for greedyvocabulary construction of BPE, leading to moreefficient bilingual vocabularies. He et al. (2020)and Meyer and Buys (2023) work with DynamicProgramming Encoding that includes subword se-lection into the language-modeling objective of inMT model with a decoder using character-level in-puts. Yehezkel and Pinter (2023) introduce SaGe,which uses skip-gram training objective as a lossto replace unigram perplexity used in the Unigrammodel. Hofmann et al. (2022) show that chang-ing the segmentation algorithm in a WordPiece(Schuster and Nakajima, 2012) tokenizer and atrained BERT model can improve classification per-formance. Schmidt et al. (2024) further elaborateon this idea and introduce an alternative segmenta-tion algorithm that produces the minimum numberof tokens given a vocabulary.",
  "Conclusions": "In this paper, we devised morphologically plausi-ble methods for subword segmentation. Inspired bySchmidt et al. (2024), we divide the tokenizationprocess into three steps: pre-tokenization, vocabu-lary construction, and segmentation.We described three key contributions of ourwork. Our first contribution focuses on the pre-tokenization step:Instead of the standard ap-proaches, which split the text into word-like units,we use Morfessor, which splits the text into mor-phemes.However, we only regard this as pre-tokenization. Next, we proposed a novel segmen- tation algorithm based on word and subword em-beddings, which provides lexical grounding to thesegmentation. Finally, we proposed a statisticalbigram segmentation model that can be used tosimplify complex tokenization pipelines. The intrinsic evaluation results show that the pro-posed method better captures language morphologythan standard statistical subword segmentation ap-proaches. This is further confirmed by the resultswe obtained on POS tagging, in which informationabout morphology is a key feature.However, our method did not significantly im-prove the performance of machine translation,which is a more complex NLP task. We argue that adedicated analysis would be required to determinethe exact influence of the lexically grounded seg-mentation on the translation quality, which mightbe improved in one dimension but reduced in an-other.In our work, we have taken steps to createa more morphologically accurate tokenizationmethod while keeping the benefits of statisticalsubword segmentation. We believe these methodswill improve modeling language overall and con-tribute to model interpretability and cross-lingualtransfer.",
  "Limitations": "The subword embedding formula derived in Sec-tion 3.1 requires a trained word embedding modeland, therefore, relies on the quality of availabledata. This problem manifests mostly in under-represented languages, many of which would bene-fit from morphology-aware segmentation.In .1, we use a set of heuristic rules tomap the morphemes to the surface form for somelanguages. These rules are language agnostic andmay introduce noise into the evaluation. However,the results are consistent with Czech, annotated onthe morph level.",
  "This work was supported by the Charles Universityproject PRIMUS/23/SCI/023": "Khuyagbaatar Batsuren, Gbor Bella, Aryaman Arora,Viktor Martinovic, Kyle Gorman, Zdenek abokrt-sk, Amarsanaa Ganbold, rka Dohnalov, Magdaevckov, Katerina Pelegrinov, Fausto Giunchiglia,Ryan Cotterell, and Ekaterina Vylomova. 2022. TheSIGMORPHON 2022 shared task on morpheme seg-mentation. In Proceedings of the 19th SIGMOR-PHON Workshop on Computational Research in Pho-netics, Phonology, and Morphology, pages 103116,Seattle, Washington. Association for ComputationalLinguistics.",
  "Piotr Bojanowski, Edouard Grave, Armand Joulin, andTomas Mikolov. 2017. Enriching word vectors withsubword information. Transactions of the Associa-tion for Computational Linguistics, 5:135146": "Mauro Cettolo, Marcello Federico, Luisa Bentivogli,Jan Niehues, Sebastian Stker, Katsuhito Sudoh,Koichiro Yoshino, and Christian Federmann. 2017.Overview of the IWSLT 2017 evaluation campaign.In Proceedings of the 14th International Conferenceon Spoken Language Translation, pages 214, Tokyo,Japan. International Workshop on Spoken LanguageTranslation. Alexis Conneau, Kartikay Khandelwal, Naman Goyal,Vishrav Chaudhary, Guillaume Wenzek, FranciscoGuzmn, Edouard Grave, Myle Ott, Luke Zettle-moyer, and Veselin Stoyanov. 2020. Unsupervisedcross-lingual representation learning at scale. In Pro-ceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 84408451, Online. Association for Computational Lin-guistics. Marwa Gaser, Manuel Mager, Injy Hamed, NizarHabash, Slim Abdennadher, and Ngoc Thang Vu.2023. Exploring segmentation approaches for neu-ral machine translation of code-switched EgyptianArabic-English text. In Proceedings of the 17th Con-ference of the European Chapter of the Associationfor Computational Linguistics, pages 35233538,Dubrovnik, Croatia. Association for ComputationalLinguistics.",
  "Sepp Hochreiter and Jrgen Schmidhuber. 1997. Longshort-term memory. Neural Comput., 9(8):17351780": "Valentin Hofmann, Hinrich Schuetze, and Janet Pierre-humbert. 2022. An embarrassingly simple methodto mitigate undesirable properties of pretrained lan-guage model tokenizers. In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 2: Short Papers), pages 385393,Dublin, Ireland. Association for Computational Lin-guistics. Marcin Junczys-Dowmunt,Roman Grundkiewicz,Tomasz Dwojak, Hieu Hoang, Kenneth Heafield,Tom Neckermann, Frank Seide, Ulrich Germann,Alham Fikri Aji, Nikolay Bogoychev, Andr F. T.Martins, and Alexandra Birch. 2018. Marian: Fastneural machine translation in C++. In Proceedings ofACL 2018, System Demonstrations, pages 116121,Melbourne, Australia. Association for ComputationalLinguistics. Diederik P. Kingma and Jimmy Ba. 2015. Adam: Amethod for stochastic optimization.In 3rd Inter-national Conference on Learning Representations,ICLR 2015, San Diego, CA, USA, May 7-9, 2015,Conference Track Proceedings. Tom Kocmi, Rachel Bawden, Ondrej Bojar, AntonDvorkovich, Christian Federmann, Mark Fishel,Thamme Gowda, Yvette Graham, Roman Grund-kiewicz, Barry Haddow, Rebecca Knowles, PhilippKoehn, Christof Monz, Makoto Morishita, MasaakiNagata, Toshiaki Nakazawa, Michal Novk, MartinPopel, and Maja Popovic. 2022. Findings of the 2022conference on machine translation (WMT22). InProceedings of the Seventh Conference on MachineTranslation (WMT), pages 145, Abu Dhabi, UnitedArab Emirates (Hybrid). Association for Computa-tional Linguistics. Taku Kudo. 2018. Subword regularization: Improv-ing neural network translation models with multiplesubword candidates. In Proceedings of the 56th An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 6675,Melbourne, Australia. Association for ComputationalLinguistics. Taku Kudo and John Richardson. 2018. SentencePiece:A simple and language independent subword tok-enizer and detokenizer for neural text processing. InProceedings of the 2018 Conference on EmpiricalMethods in Natural Language Processing: SystemDemonstrations, pages 6671, Brussels, Belgium.Association for Computational Linguistics. Tomasz Limisiewicz, Jir Balhar, and David Marecek.2023. Tokenization impacts multilingual languagemodeling: Assessing vocabulary allocation and over-lap across languages. In Findings of the Association",
  "for Computational Linguistics: ACL 2023, pages56615681, Toronto, Canada. Association for Com-putational Linguistics": "Dominik Machcek, Jons Vidra, and Ondrej Bojar.2018. Morphological and language-agnostic wordsegmentation for NMT. In Text, Speech, and Dia-logue - 21st International Conference, TSD, volume11107 of Lecture Notes in Computer Science, pages277284, Brno, Czech Republic. Springer. Francois Meyer and Jan Buys. 2023. Subword segmen-tal machine translation: Unifying segmentation andtarget sentence generation. In Findings of the Asso-ciation for Computational Linguistics: ACL 2023,pages 27952809, Toronto, Canada. Association forComputational Linguistics. Toms Mikolov, Ilya Sutskever, Kai Chen, Gregory S.Corrado, and Jeffrey Dean. 2013. Distributed repre-sentations of words and phrases and their composi-tionality. In Advances in Neural Information Process-ing Systems 26: 27th Annual Conference on NeuralInformation Processing Systems 2013. Proceedingsof a meeting held December 5-8, 2013, Lake Tahoe,Nevada, United States, pages 31113119. Jeffrey Pennington, Richard Socher, and ChristopherManning. 2014. GloVe: Global vectors for wordrepresentation. In Proceedings of the 2014 Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP), pages 15321543, Doha, Qatar.Association for Computational Linguistics. Maja Popovic. 2015. chrF: character n-gram F-scorefor automatic MT evaluation. In Proceedings of theTenth Workshop on Statistical Machine Translation,pages 392395, Lisbon, Portugal. Association forComputational Linguistics. Matt Post. 2018. A call for clarity in reporting BLEUscores. In Proceedings of the Third Conference onMachine Translation: Research Papers, pages 186191, Brussels, Belgium. Association for Computa-tional Linguistics. Alfrd Rnyi. 1961. On measures of entropy and infor-mation. In Proceedings of the Fourth Berkeley Sym-posium on Mathematical Statistics and Probability,Volume 1: Contributions to the Theory of Statistics,volume 4, pages 547562. University of CaliforniaPress.",
  "Rico Sennrich, Barry Haddow, and Alexandra Birch.2016. Neural machine translation of rare words with": "subword units. In Proceedings of the 54th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 17151725,Berlin, Germany. Association for Computational Lin-guistics. Peter Smit, Sami Virpioja, Stig-Arne Grnroos, andMikko Kurimo. 2014. Morfessor 2.0: Toolkit for sta-tistical morphological segmentation. In Proceedingsof the Demonstrations at the 14th Conference of theEuropean Chapter of the Association for Computa-tional Linguistics, pages 2124, Gothenburg, Sweden.Association for Computational Linguistics. Paul Soulos, Sudha Rao, Caitlin Smith, Eric Rosen,Asli Celikyilmaz, R. Thomas McCoy, Yichen Jiang,Coleman Haley, Roland Fernandez, Hamid Palangi,Jianfeng Gao, and Paul Smolensky. 2021. Structuralbiases for improving transformers on translation intomorphologically rich languages. In Proceedings ofthe 4th Workshop on Technologies for MT of LowResource Languages (LoResMT2021), pages 5267,Virtual. Association for Machine Translation in theAmericas. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processingsystems, 30. Jingjing Xu, Hao Zhou, Chun Gan, Zaixiang Zheng,and Lei Li. 2021. Vocabulary learning via optimaltransport for neural machine translation. In Proceed-ings of the 59th Annual Meeting of the Association forComputational Linguistics and the 11th InternationalJoint Conference on Natural Language Processing(Volume 1: Long Papers), pages 73617373, Online.Association for Computational Linguistics. Shaked Yehezkel and Yuval Pinter. 2023. Incorporatingcontext into subword vocabularies. In Proceedingsof the 17th Conference of the European Chapter ofthe Association for Computational Linguistics, pages623635, Dubrovnik, Croatia. Association for Com-putational Linguistics. Zdenek abokrtsk, Niyati Bafna, Jan Bodnr, LukKyjnek, Emil Svoboda, Magda evckov, andJon Vidra. 2022.Towards universal segmenta-tions: UniSegments 1.0. In Proceedings of the Thir-teenth Language Resources and Evaluation Confer-ence, pages 11371149, Marseille, France. EuropeanLanguage Resources Association. Daniel Zeman, Joakim Nivre, et al. 2024. Universaldependencies 2.14. LINDAT/CLARIAH-CZ digitallibrary at the Institute of Formal and Applied Linguis-tics (FAL), Faculty of Mathematics and Physics,Charles University.",
  "38return list(reversed(subwords)), costs": "Algorithm 1: Python code showing the segmentation algorithm using subword embeddings. On the input, word isthe word to be segmented, word_embedding is its embedding, and subword_embedding is the subword embeddingmatrix. It is a dynamic programming algorithm that first computes the scores of the best segmentation up to a given positionin the string (kept in list costs) and what was the start index of the last subword in the best-scoring segmentation(kept in list prev). When moving to the next index in the for loop on line 12, we can rely on knowing the bestsegmentation score for all indices up to i 1 from the previous iteration. Therefore, in the for loop on line 16,we can try all subwords that will bring us to index i. figure out the best possible subword that will extend thesegmentation to index i.",
  "Word (Czech)Gold segmentationBPEUnigramOurs": "vykrlitvy kr l i tvy kr litvy kr litvy kr l itfluorovefluor ov eflu or ovefl u or ovef lu or ovehorchhor chhorchhorchhorchzkamenetz kamen e tz kamen e tz ka me ne tz kamen e takciakci akciakciakcizdegenerovatz de gener ova tzde gener ovatzde gen er ovatzde gener ovatrezervyre zerv yrezervyrezervyrezervynemelyne m e l ynemelynemelynemelypoplatkupo plat k upoplatkupoplatkupoplatkuobnitkovatob nit k ova tob ni tk ovatob nit kovatob nit kovatznesnadnovatz ne snad n ova tzne snad novatz ne snad novatzne snad novatpresunovatpre sun ova tpresu novatpresun ovatpresun ovatjednotajedn ot ajedno tajedno tajedno taobklcitob klc i tob kl citob klc itob klc itkryskrys kry skrys krys premiprem i premi pre mi pre mi brkobr k obr kobrkobrkoodpovdatod po vd a todpovdatodpovdatodpovdatzakuklitza kukl i tza ku kli tza ku kli tza kukl it",
  "Word (English)Gold segmentationBPEUnigramOurs": "macroclumpsmacro clump smacro clum psmacro cl ump smacro clump sgibbetsgibbet sgib betsgibb etsgibb etsphenoconvertspheno convert sphen o conver tsphe no con vert sph eno convert sahuraahuraa hur aa h uraahu rabimonopolesbi mono pole sb im on opol esbi mon o pole sbi mono polesnonwriternon write rnon writernon writernon writermolelikemole likemol eli kemole likemole likebarnardsvillebarnard s villebar nar d svillebarnard svillebarnard svillepoguespogue spo guespo gue spo gu esinfractorsinfractor sinfr actorsin fra ctor sin fr actorsbattlingsbattling sbatt lingsbattling sbattling slarruplarruplar r upla rr uplar ru pdetransformationde trans form ationde transformationde trans form ationde transform ationdeexcitingde excit ingde excitingde ex citingde excitingkalasieskalasie skal as ieskala s ieskala s iescanebrakescane brake scan e brakescan e bra kesca ne brakeseskimologicaleskimo log icales kim ologicales kim ologicales kim ologicalunmisleadingun mis lead ingun misleadingun mis leadingun misleadingneurofibrominsneuro fib r om in sneuro fibro minsneuro fi bro minsneuro fibro mins",
  "Word (French)Gold segmentationBPEUnigramOurs": "parassiensparassien spar assi enspar assi enspa ras sienscomplairacom plair acompl ai racomp la iracom plairasalindroissalindr oissal in dr oissali nd roissali nd roisnampontoisnampont oisnam pon toisn amp ont oisnam pont oissdimentologiquesdimentologi ques di ment ologiques di ment ologiques dim ent ologiqueesquiveesquiv eesqui vees qui v ees qu iv eflanc-gardeflanc - gardefl anc - gardeflanc - gardeflanc - gardemoyenmoyenmoyenmoyenmoyenantigangsanti gang santi gangsanti g ang santi gangsforerforerfor erfor erfo rercaptivitscaptivit scapti vitscaptivit scaptivit sdpolymrissd poly m r is s d poly m r issd po ly m r issd poly m ris sprvoiriezpr voir iezpr voi riezprvoir iezprvoir iezdracineraisd racine r aisd rac in eraisd ra cine raisd racine raiscorcipiendaireco rcipiendairecor ci pi end airecor ci pi end aireco r cip ien da irecrustacyaninescrustacyanine scru st ac yan inescrus t ac yan inescrus tac yan ineschambardschambard scham bar dschamb ard scham bard sjoyeusainjoyeus ainjoy eu sainjoy e us ainjo ye usaininfluionsinflu ionsinflu ionsin flu ionsinf lu ions : Example segmentations from the SIGMORPHON 2018 Czech, English, and French test sets. Green spacesymbols denote morphologically valid splits, and the red space symbols denote splits inside morphemes."
}