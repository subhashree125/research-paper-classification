{
  "Abstract": "The audio watermarking technique embedsmessages into audio and accurately extractsmessages from the watermarked audio. Tra-ditional methods develop algorithms based onexpert experience to embed watermarks intothe time-domain or transform-domain of sig-nals. With the development of deep neuralnetworks, deep learning-based neural audiowatermarking has emerged. Compared to tra-ditional algorithms, neural audio watermark-ing achieves better robustness by consideringvarious attacks during training. However, cur-rent neural watermarking methods suffer fromlow capacity and unsatisfactory impercepti-bility. Additionally, the issue of watermarklocating, which is extremely important andeven more pronounced in neural audio water-marking, has not been adequately studied. Inthis paper, we design a dual-embedding wa-termarking model for efficient locating. Wealso consider the impact of the attack layeron the invertible neural network in robustnesstraining, improving the model to enhance bothits reasonableness and stability. Experimentsshow that the proposed model, IDEAW, canwithstand various attacks with higher capacityand more efficient locating ability comparedto existing methods. The code is available at",
  "Introduction": "Digital watermarking (Singh et al., 2023) embedsmessages indicating the ownership or authenticityinto multimedia like images, video and audio, im-perceptibly. This technique is widely used for own-ership statements and anti-counterfeit. Impercep-tibility and robustness are the two most challeng-ing requirements for digital watermarking, whichmeans, it is expected to be hard to feel the pres-ence of the embedded watermark with human per-ception, and the watermark can be preserved and",
  ": (a) Pipeline of robust neural audio watermark-ing. (b) Embedding strategy of existing methods. (c)Dual-embedding strategy of IDEAW": "extracted accurately even after the watermarkedmedia has been subjected to unintentional damageor malicious removal attacks.Audio watermarking has been around fordecades. Traditional techniques (Singha and Ullah,2022; Zhang et al., 2023) embeds the watermarkinto either the time-domain or transform-domainof audio signals via algorithms designed based onexpert knowledge (Prabha and Sam, 2022). The ro-bustness of traditional watermarking methods gen-erally stems from subjective design, which resultsin limitations. The advancement of deep learningbrings new solutions to steganography (Hussainet al., 2020; Chanchal et al., 2020) and digital water-marking techniques (Amrit and Singh, 2022; Singhand Singh, 2023). End-to-end neural watermark-ing model completes the embedding and extractionprocess in each training iteration and constrains theimperceptibility and the integrity of the extractedwatermarks through the designed training objec- tives. The attack layer which simulates commondamages on the watermarked media is introducedinto the Embedder-Extractor (i.e. encoder-decoder)structure to guarantee the robustness.Neural audio watermarking is currently in itsearly stages.As human auditory perception issometimes more sensitive than visual perception(Lee et al., 2019). It can also easily distinguishnoise, making subtle alterations caused by wa-termarking to be perceived.As for the robust-ness of watermarking. The Embedding-Attacking-Extracting pipeline of neural audio watermarkingis shown in (a), where the attack layer simu-lates various removal attacks on watermarked au-dio during training. Redundancy is required forembedding digital watermarks to enhance applica-bility. The same watermark is repeatedly embed-ded at various locations within an audio segment.However, this strategy raises the issue of locat-ing. The embedding location of the watermark isunknown during extraction in practical scenarios.Additionally, trimming and splicing cause changesin the watermarking location. Compared to thetraditional method, the extraction of neural audiowatermarking relies on the forward process of neu-ral networks which induces a non-negligible timecost that increases with the complexity of the net-work. Existing methods typically use an exhaustiveapproach, extracting synchronization code and wa-termark message together step by step, as shownin (b). Localization efficiency is an issue thatneural audio watermarking must face.The symmetry of the embedding and extractionprocesses of watermarking provides the invertibleneural network with ample opportunities, but theintroduction of the attack layer disrupts the orig-inal symmetry (Liu et al., 2019). In other words,due to the presence of the attack layer, the outputof the encoder (i.e. watermarked audio) and theinput of the decoder (i.e. attack-performed water-marked audio) are inconsistent, while the encoderand decoder are opposing and share parameters,this mismatch limits the training effect.In this paper, we propose a model calledInvertible Dual-Embedding Audio Watermarking,IDEAW, which uses a dual-stage invertible neuralnetwork with a dual-embedding strategy to embedwatermark message and synchronization code (re-ferred to as locating code in this paper) separately,as illustrated in (c). During extraction, wefirst extract the less computational cost locatingcode. Upon successful matching, the extraction of the message which has more computational cost isconducted. This also makes it possible to enlargethe capacity of watermarking flexibly. To alleviatethe asymmetric impact caused by the attack layer,we apply a balance block to enhance the trainingstability while preserving the characteristics of theinvertible neural network. Our contributions in thispaper can be summarized as follows:",
  "Neural Audio Watermarking": "The neural audio watermarking model is typicallycomposed of two neural networks for watermarkembedding and extraction in the Short Time FourierTransform (STFT) or Discrete Wavelet Transform(DWT) domain. Pavlovic et al. (Pavlovic et al.,2020) design an Encoder-Decoder architecture forspeech watermarking, where the encoder and thedecoder form an adversarial relationship and aretrained together.Hereafter, they introduce anattack layer to their previous work to form anEncoder-Attack Layer-Decoder structure, enhanc-ing the robustness of the DNN-based speech water-marking (Pavlovic et al., 2022). WavMark (Chenet al., 2023) considers the extraction as the inverseprocess of embedding the watermark, and lever-ages an invertible neural network to perform em-bedding and extraction for audio watermarking.DeAR (Liu et al., 2023) focuses on the threat of re-recording attack to audio watermarking, modellingre-recording as several differentiable processes. Inaddition to imperceptibility and robustness, capac-ity and locating effectiveness are also important cri-teria for evaluating audio watermarking methods.Compared to traditional watermarking methods,most neural audio watermarking methods suffer",
  "Invertible Neural Network": "NICE (Dinh et al., 2015) firstly introduces the con-ception of an Invertible Neural Network (INN),a normalizing flow-based framework, learning atransformation that converts data that follows theoriginal distribution to a predefined distribution.Dinh et al.improve the performance of INNthrough convolutional layers in Real NVP (Dinhet al., 2017). Ardizzone et al. introduce the con-ditional INN (cINN) (Ardizzone et al., 2019) toestablish control over the generation. Behrmann etal. (Behrmann et al., 2019) utilize ResNet as theEuler discretization of ordinary differential equa-tions and prove that the invertible ResNet can beconstructed by changing the normalization mecha-nism. INN has been widely applied in generation(Dinh et al., 2015, 2017; Kingma and Dhariwal,2018; van der Ouderaa and Worrall, 2019), im-age super-resolution (Lugmayr et al., 2020), imagecompression (Wang et al., 2020), image-to-imagetranslation (van der Ouderaa and Worrall, 2019),digital steganography (Lu et al., 2021; Mou et al.,2023), etc.",
  "Overall Architecture of IDEAW": "showcases the architecture of our proposedaudio watermarking model, IDEAW, which fol-lows an Embedder-Attack Layer-Extractor struc-ture, where the embedder and extractor are care-fully designed as dual-stage structures for embed-ding messages and locating codes at vertical lat-itude separately. In the embedding process, an Lmbit binary watermark message m {0, 1}Lmis embedded into a fixed-length audio chunk x inthe STFT domain via the first stage INN, then thesecond stage INN embeds a binary locating codec {0, 1}Lc into the audio containing m fromthe former step. The final watermarked audio isreconstructed through Inverse Short-Time FourierTransform (ISTFT). A discriminator distinguishesbetween the host audio and the watermarked audioto guarantee the imperceptibility of watermarking.In the extraction process, the two-stage extractorfirst extracts c from the watermarked audio andthen m, in the reverse order of embedding. Anattack layer is introduced to enhance the robustnessto various removal attacks. The extractor must ac-curately extract c and m from the attack-performedwatermarked audio. The balance block aims to alle-viate the asymmetry introduced by the attack layer,preserving the symmetry of INN.",
  "Dual-stage INN for Dual-Embedding": "To vertically separate the locating code and mes-sage, a dual-stage INN is designed, with each stagedesignated as INN#1 for the embedding and extrac-tion of watermark message, and INN#2 for that oflocating code. Each INN consists of several in-vertible blocks that take transformed audio and thewatermark (i.e. the watermark message or locatingcode) as inputs, producing two outputs. We referto these input-output pairs as two data streams: theaudio stream and the watermark stream. The au-dio stream outputs watermarked audio during theembedding process, while the watermark streamprovides a watermark message during extraction. showcases the architecture of the invertibleblock, in which the block processes the data as",
  "si+1 = si exp(((xi+1))) + (xi+1)(1)": "where x denotes the host data including the origi-nal audio fed to INN#1 and the audio with messageembedded fed to INN#2, s denotes the secret dataincluding the message fed to INN#1 and the locat-ing code fed to INN#2. () is a sigmoid function,while (), (), () and () are subnets whichare constructed from dense blocks.In the first embedding stage, INN#1 embeds thewatermark message into the host audio, wherethe audio and bit sequences are transformed intothe time-frequency domain via STFT at first, re-spectively. Then INN#2 embeds the locating codeinto the audio stream output of INN#1. The dual-embedding can be described by Eq. 2. Subscripts",
  "xwmd = INN#2(INN#1(x, m)a, c)a(2)": "The extraction process of IDEAW is exactly theopposite of the embedding process. INN#2 firstlyextract the locating code from the watermarkedaudio, then the output of the audio stream fromINN#2 is sent to INN#1 to extract watermark mes-sage. Eq. 3 describes the dual extraction to obtainthe embedded message. m = INN#1R(INN#2R(xwmd, xaux2)a, xaux1)wm(3)where xaux represents the randomly sampled sig-nal which is fed to the message stream of INN. Thesubscript R represents the reverse process of the re-versible network as the extraction process employsthe same network and parameters as the embeddingprocess, only with the order reversed.During training, the message is extracted byINN#1R after each stage of embedding, then com-pared with the original message m. The locating",
  "Linteg = || m m||2+ ||INN#1R(INN#1(x, m)a, xaux1)wm m||2+ ||INN#2R(xwmd, xaux2)wm c||2(4)": "In practical scenarios, watermarks are embeddedinto several segments of audio as shown in (c). However, watermarked audio may be trimmedor spliced (known as the de-synchronization attacks(Mushgil et al., 2018)), making it challenging todetermine the watermark location. The extractorextracts and matches the locating code quickly asINN#2 is lighter and costs less computation thanINN#1, extracting the watermark message only ifthe locating code is matched. In addition, a similartraining manner to the shift module in WavMark(Chen et al., 2023) is deployed in our proposedmodel, which helps the extractor to gain the abilityto extract watermark from a proximity location.",
  "Imperceptibility Guaranty": "As one of the most important indicators for evaluat-ing digital watermarking, imperceptibility ensuresthat the embedding of the watermark cannot bedistinguished from human auditory perception. InIDEAW, we take two measures to ensure and im-prove the imperceptibility of watermarking. Theperceptual loss intuitively requires that the differ-ence between the watermarked audio and the orig-inal host audio is as narrow as possible, that is,reducing the impact of watermarking on the hostaudio. The perceptual loss is shown in Eq. 5.",
  "Lpercept = ||xwmd x||2(5)": "Whats more, we leverage a discriminator to dis-tinguish the watermarked audio from the originaudio. Essentially, the discriminator is a binaryclassifier which classifies host and watermarkedaudio labeled with 0 and 1 (denoted as label y)respectively. The discriminator aims to identifythe watermarked audio while the embedder tries tohide the watermark so that the embedder and thediscriminator form an adversarial relationship andmutually force each other during training (Goodfel-low et al., 2020). The discriminate loss and identifyloss is as follows:",
  "The Robustness and Symmetry of INN": "The robustness of a watermarking method is crit-ical to the attackers ability to effectively removethe embedded watermarks. Generally, attackers tryto remove the watermark from the watermarked au-dio through several removal techniques includingpassing the audio through filters only to maintainthe information that can be perceived by humans,or adding noises to the watermarked audio to inter-fere with accurate extraction. Effective watermarkremoval requires that the watermark cannot be ex-tracted correctly from the watermark-removed au-dio and that the watermark-removed audio shouldbe usable, i.e. the listening quality should not sig-nificantly decrease, otherwise the removal of thewatermark is meaningless.In order to endow the neural audio watermarkingwith robustness against various watermark removalattacks, an attack layer is incorporated into the wa-termarking model and trained alongside the embed-ding and extraction networks. The attack layer sub-jects the watermarked audio (i.e. the output fromthe embedder) to a variety of attacks. Subsequently,the extractor attempts to extract the locating codeand watermark message from the audio that hasundergone attack as accurately as possible. Consid-ering the effectiveness of the watermark removal,the predefined attacks should ensure the quality ofattack-performed watermarked audio does not de-grade too much. These attacks include Gaussianadditive noise, lower-pass filter, MP3 compression,quantization, resampling, random dropout, ampli-tude modification and time stretch.However, the introduction of the attack layerdisrupts the symmetry of the entire embedding-extraction process, impacting the training of INN.We define the integral dual-embedding processand extraction process as f() and f1() respec-tively. During the embedding process, IDEAWperforms f(x, m, c) = w, where x, m, c, w arehost audio, watermark message, locating code andwatermarked audio as mentioned above. And w fol-lows the distribution of watermarked audio PW (w)(Ma et al., 2022). While in the extraction process,thanks to the parameter-sharing between the em-bedder and the extractor, m and c can be easilysampled with m, c = f1(w) according to PW (w).But the including of the attack layer causes changesin PW (w) leading to a PW (w), while the extrac- tion process is still based on the unchanged PW (w),which affects the performance of watermarking.The parameter-sharing strategy of INN limits theextractors learning ability to adapt to the attackedaudio, resulting in distorted watermarking perfor-mance.To simultaneously maintain the parameter-sharing of INN and the symmetry of INNs train-ing, a balance block is employed to mitigate theasymmetry caused by the attack layer and stabi-lize the models symmetric structure. The balanceblock consists of a group of dense blocks (Huanget al., 2017) which process the input to equal-sizeoutput, providing extra trainable parameters forthe extractor. This manner learns to transformthe attack-performed watermarked audio distribu-tion PW (w) to the revised distribution P W( w) thatclose to the expected distribution PW (w) under theguidance of the mentioned integrity loss, to coun-teract the effects of the offset introduced by theattack layer without bothering the embedder.",
  "Simultaneously ensuring accuracy, imperceptibilityand robustness is sticky. Therefore, the training ofIDEAW is divided into two stages:": "The first stage only considers the impercepti-bility and the watermark integrity of extrac-tion, aiming to build a dual-stage INN thatcan embed the watermark imperceptibly andextract the watermark accurately. In the second stage, the requirement for the ro-bustness of watermarking is introduced. Theattack layer and balance block are incorpo-rated into the model, and the entire model istrained collectively. The same total loss function, as introduced inEq. 8, is applied for both training stages, where1, 2 and 3 are weights of each component, butnote that the second stage contains more trainableparameters. The discriminator is trained along withthe watermarking model in each iteration with theloss function in Eq. 6. The pseudo script of thetraining process as well as the acquisition of thetotal loss is shown in Section A.1 of the appendix.",
  "Dataset and Implementation": "IDEAW is trained on VCTK corpus (Yamagishiet al., 2016) and FMA corpus (Defferrard et al.,2017). VCTK comprises over 100 hours of multi-speaker speech data, while FMA contains a largeamount of music audio. These two types of data areprevalent in scenarios where audio watermarkingis commonly applied. All the audio is resampledto 16,000 Hz and split into 1-second segments dur-ing training. STFT and ISTFT operations withparameters of {n_fft = 1000, hop_length =250, win_length=1000} perform the inter-domain transformation. Two Adam optimizers (Kingma and Ba, 2015)with {1 = 0.9, 2 = 0.99, = 108, lr = 105}(with a StepLR scheduler) are leveraged for theintroduced two-stage training. Super-parameters1, 2 and 3 in total loss (Eq. 8) are set to 1, 0.1and 0.1, respectively. Each stage contains 100,000iterations. The attacks are sample-wise, each audiosegment in the batch undergoes different types ofattacks during training. The configuration of theattack layer is shown in Section A.2 of appendix.We set the length of the locating code to 10 bits,so there is about a 1/210 probability of potentialconflicts with extracted non-locating code bit se-quences. In each batch, the locating code and wa-termark message are randomly generated to ensurethat the trained model can handle any arrangementof 0-1 sequences.",
  ": Comparison of the basic metrics with baselinemethods. The ACC of IDEAW is calculated from thelocating code and message": "4.1.2MetricsSignal-to-noise ratio (SNR) measures the impactof the watermarking. It is calculated as 10 timesthe logarithm of the ratio of host audio power towatermark noise power.Accuracy (ACC) measures the differences be-tween the extracted message and the ground-truthmessage. ACC is typically used to measure therobustness of watermarking methods.Capacity is the number of watermark bits that canbe embedded per second of audio while ensuringimperceptibility and ACC of the watermarking.",
  "Results": "4.2.1Overall ComparisonThe comparison of basic metrics of various water-marking methods is shown in . As eachbaseline watermarking model is designed for differ-ent capacities, and IDEAW owns the maximum one,we also train the other two IDEAW models whichhave similar or larger capacities as other methodsto alleviate the impact of capacity on comparison.As the locating code and message are separated inour proposed model, we take the total length ofthe locating code and message as the capacity ofIDEAW and the length of the locating code is fixedto 10 bits. We find that the watermarking modelwith lower capacity obtains better imperceptibilityand higher accuracy. IDEAW gains considerableSNR and ACC when designed with a large payload. illustrates the impact of the watermark-ing on a low-energy speech waveform and a high-energy music waveform. The same watermark is it-eratively embedded into the audio. From (a),we can see that the original audio in the foregroundand the watermarked audio in the background al-most overlap. shows a comparison of thelinear-frequency power spectrograms of the origi-nal and watermarked audio, illustrating the impactof the watermarking in the time-frequency domain.Morewatermarkedaudiosamples,wave-form samples and practical application exam-ples are available at our demo page 4.2.2Robustness ComparisonWe measure the watermark extraction accuracy ofeach model under different attacks to evaluate theirrobustness. Eight common attacks including Gaus-sian additive noise (GN), lower-pass filter (LF),MP3 compression (CP), quantization (QZ), ran-dom dropout (RD), resampling (RS), amplitude",
  "(b)": ": Waveforms of (a) host audio (foreground) andwatermarked audio (background), (b) host audio andtenfold-magnified residual caused by watermarking, (c)local details (100 points) of the (a) and (b). The leftaudio is low-energy speech audio while the right is high-energy music audio. modification (AM) and time stretch (TS) are takeninto consideration. As mentioned above, water-mark removal attacks need to consider the degreeof damage to audio, and the strength settings ofeach attack continue the previous research settings(Chen et al., 2023). The robustness evaluation re-sults are shown in . IDEAW shows compa-rable robustness to the baseline model while with alarger capacity.",
  ": Linear-frequency power spectrograms of low-energy speech audio (left) and high-energy music audio(right). (a) the host audio, (b) the watermarked audio": "cating. The tested methods include (1) the sameas WavMark (Chen et al., 2023), synchronizationcode and message are concatenated and embeddedinto the audio segment together via a single-stageINN model. The model is trained with the shiftstrategy. The synchronization code and messageare extracted at the same time iteratively duringlocating. The step size is 10% of the chunk size,the same as the baseline method. (2) the proposedmethod, only extracts the locating code during thelocating process with a step size of 10% of thechunk size. Method (1) builds a single stage INNwatermarking model that has the same layer quan-",
  ": Comparison of locating time consumptionfor different methods at various watermark embeddinglocations (the location is indicated by the seconds fromthe start of audio to the watermarking location)": "tity as the proposed dual-stage model. Note that inorder to eliminate the impact of errors during locat-ing as we cannot guarantee the ACC of the modelwith a carrying capacity of 56 bits in Method (1),we only perform extraction without further verifi-cation, until reaching the watermark location, asthe former experiments show that each model canreach the ideal accuracy with designed capacity.The measurement of locating time for eachmethod is conducted on the same device and takesan average of 100 processes for each watermark-ing location. The comparison of the time con-sumption of each method on the same device isshown in . The results show that the pro-posed method reduces time overhead by approxi-mately 40% 50%. Especially when the water-mark is far from the head, the advantage of ourdual-embedding strategy is more obvious.",
  "To validate the positive effects of the proposed bal-ance block and the discriminator on IDEAWs per-formance, we conduct an ablation study. The fol-lowing models related to the proposed methods are": "built. (1) M1 (w/o discriminator) removes the dis-criminator from the proposed method. (2) M2 (w/obalance block) removes the balance block fromthe proposed method during the robustness train-ing. These models as well as the proposed modelare trained on the same datasets and in the samemanner. We measure the basic metrics and therobustness of each model.",
  "Ablation Study Results": "The results of the ablation study are shown in and . The results show that the discrimina-tor helps improve the quality and naturalness of thewatermarked audio, as the model without discrimi-nator has a degradation in the signal-to-noise ratioand comparable robustness to the proposed method.The balance block does alleviate the asymmetrycaused by the attack layer in robustness training,enabling the model to gain better robustness andachieve higher accuracy. Overall, the introductionof the discriminator and balance block enhancesthe watermarking quality and the stability of thewatermarking model.",
  "Conclusion": "In this paper, we propose a neural audio water-marking model, IDEAW, which embeds the locat-ing code and watermark message separately via adesigned dual-stage INN. In response to the chal-lenge of neural audio watermarking localization,the dual-embedding strategy avoids the huge com-putation of extracting synchronization code andmessage at the same time, accelerating the locatingprocess. On the other hand, we make an effort tomitigate the asymmetry factor introduced by theattack layer. The balance block is leveraged toprovide the extractor with a subtle training spatialwhile maintaining the advantage of the invertiblenetwork. Experimental results show that IDEAWachieves satisfactory performance from a compre-hensive perspective of imperceptibility, capacity,robustness and locating efficiency.",
  "Limitations": "This work focuses on the high overhead problem oflocalization of neural audio watermarks by innova-tively separating the locating code from the water-mark message and embedding/extracting them sep-arately. However, we find that the dual-embeddingwatermarking method has the following limitationsso far: The lengths of the locating code and messagesof the trained model cannot be flexibly ad-justed because the design of the dual-stageinvertible neural network fixes the lengths ofboth, we cannot designate the bits starting atarbitrary lengths as synchronization codes likeprevious methods. We find that the watermark embedded in low-energy audio is less imperceptible than inhigh-energy audio via the proposed model.The reason might be that, apart from selectingtwo types of datasets for training, the designof the model as well as its training strategydoes not consider the carriers energy.",
  "Future Works": "With the rapid development of social media andthe increasing awareness of copyright, digital wa-termarking technology has promising prospects forwidespread application. Neural audio watermark-ing remains a valuable area of research due to itsrobustness, independence from expert intervention,and scalability, compared to traditional methods.However, neural audio watermarking has not yetfully surpassed traditional digital watermarkingmethods, particularly in terms of capacity. Futureworks on neural audio watermarking may consider,but is not limited to, the following aspects:",
  "Guangyu Chen, Yu Wu, Shujie Liu, Tao Liu, XiaoyongDu, and Furu Wei. 2023. Wavmark: Watermarkingfor audio generation. CoRR, abs/2308.12770": "Michal Defferrard, Kirell Benzi, Pierre Vandergheynst,and Xavier Bresson. 2017. FMA: A dataset for musicanalysis. In The 18th International Society for MusicInformation Retrieval Conference, ISMIR, pages 316323. Laurent Dinh, David Krueger, and Yoshua Bengio. 2015.NICE: non-linear independent components estima-tion. In The 3rd International Conference on Learn-ing Representations, ICLR, Workshop Track Proceed-ings.",
  "Hyungeol Lee, Eunsil Lee, Jiye Jung, and Junsuk Kim.2019.Surface stickiness perception by auditory,tactile, and visual cues. Frontiers in Psychology,10:2135": "Chang Liu, Jie Zhang, Han Fang, Zehua Ma, Weim-ing Zhang, and Nenghai Yu. 2023. Dear: A deep-learning-based audio re-recording resilient water-marking. In The 37th AAAI Conference on ArtificialIntelligence, pages 1320113209. Yang Liu, Mengxi Guo, Jian Zhang, Yuesheng Zhu, andXiaodong Xie. 2019. A novel two-stage separabledeep learning framework for practical blind water-marking. In The 27th ACM International Conferenceon Multimedia, pages 15091517. Shao-Ping Lu, Rong Wang, Tao Zhong, and Paul LRosin. 2021. Large-capacity image steganographybased on invertible neural networks. In Proceedingsof the IEEE/CVF conference on computer vision andpattern recognition, pages 1081610825. Andreas Lugmayr, Martin Danelljan, Luc Van Gool,and Radu Timofte. 2020. Srflow: Learning the super-resolution space with normalizing flow. In ComputerVision - ECCV - The 16th European Conference, vol-ume 12350, pages 715732. Rui Ma, Mengxi Guo, Yi Hou, Fan Yang, Yuan Li,Huizhu Jia, and Xiaodong Xie. 2022.Towardsblind watermarking: Combining invertible and non-invertible mechanisms. In The 30th ACM Interna-tional Conference on Multimedia, pages 15321542. Chong Mou, Youmin Xu, Jiechong Song, Chen Zhao,Bernard Ghanem, and Jian Zhang. 2023.Large-capacity and flexible video steganography via invert-ible neural network. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recog-nition, pages 2260622615. Baydaa Mohammad Mushgil, Wan Azizun Wan Ad-nan, Syed Abdul-Rahman Al-Hadad, and SharifahMumtazah Syed Ahmad. 2018.An efficient se-lective method for audio watermarking against de-synchronization attacks. Journal of Electrical Engi-neering and Technology, 13(1):476484.",
  "Himanshu Kumar Singh and Amit Kumar Singh. 2023.Comprehensive review of watermarking techniquesin deep-learning environments. Journal of ElectronicImaging, 32(3):031804031804": "Roop Singh, Mukesh Saraswat, Alaknanda Ashok, Hi-manshu Mittal, Ashish Tripathi, Avinash ChandraPandey, and Raju Pal. 2023. From classical to softcomputing based watermarking techniques: A com-prehensive review. Future Generation Computer Sys-tems, 141:738754. Amita Singha and Muhammad Ahsan Ullah. 2022.Development of an audio watermarking with de-centralization of the watermarks. Journal of KingSaud University-Computer and Information Sciences,34(6):30553061. Tycho F. A. van der Ouderaa and Daniel E. Worrall.2019. Reversible gans for memory-efficient image-to-image translation. In IEEE Conference on Com-puter Vision and Pattern Recognition, pages 47204728.",
  "A.1Training Pipeline of IDEAW": "Alg. 1 shows the training pipeline and the acquisition of each loss function. The length regulators drawmappings between the bit sequence space (i.e. locating code and message) and a space whose elements areof equal length to the audio waveform segment. Note that x, m, c in the pseudo script refers exclusivelyto the host audio waveform, message bit sequence and locating code, while the prime denotes the data inthe STFT domain.",
  "Algorithm 1 Acquisition of total loss Ltotal in the training stage": "Input:host audio segment x, watermark message m, locating code cModule:message embedder Emb1, locating code embedder Emb2,message extractor Ext1, locating code extractor Ext2,length regulator LR1, LR2, LR3, LR4, attack layer Att, balance block BOperation:short-time Fourier transform STFT(), inverse short-time Fourier transform ISTFT()Parameter:robustness training flag Robust, loss weights 1, 2, 3Output:total loss Ltotal",
  "IDAttackDescription and Configuration": "GNGaussian additive noiseAdd Gaussian noise to the watermarked audio and maintain the signal-to-noise ratio at approximately 35dB.LFlower-pass filterPass the audio through a lower-pass filter of 5kHz, the range of thefilter is set according to the human auditory rangeCPMP3 compressionCompress the waveform to 64kbps MP3 format and then convert backto wav format. This conversion process results in information loss.QZquantizationQuantize the sample points of watermarked audio waveform to 29 levels.RDrandom dropoutRandomly value 0.1% of the sample points in the watermarked audioto zero.RSresamplingResample the audio to a new sampling rate (200% of the originalsample rate) then resample back to the original sample rate.AMamplitude modificationMultiply 90% to the overall amplitude of the audio by a modificationfactor.TStime stretchCompress the audio in the time domain to 90% of the original length,then stretch it to maintain the original length."
}