{
  "Abstract": "Private data, being larger and quality-higherthan public data, can greatly improve large lan-guage models (LLM). However, due to privacyconcerns, this data is often dispersed in multi-ple silos, making its secure utilization for LLMtraining a challenge. Federated learning (FL)is an ideal solution for training models withdistributed private data, but traditional frame-works like FedAvg are unsuitable for LLMdue to their high computational demands onclients. An alternative, split learning, offloadsmost training parameters to the server whiletraining embedding and output layers locally,making it more suitable for LLM. Nonetheless,it faces significant challenges in security andefficiency. Firstly, the gradients of embeddingsare prone to attacks, leading to potential reverseengineering of private data. Furthermore, theservers limitation of handle only one clientstraining request at a time hinders parallel train-ing, severely impacting training efficiency. Inthis paper, we propose a Federated Learningframework for LLM, named FL-GLM, whichprevents data leakage caused by both server-side and peer-client attacks while improvingtraining efficiency. Specifically, we first placethe input block and output block on local clientto prevent embedding gradient attacks fromserver. Secondly, we employ key-encryptionduring client-server communication to preventreverse engineering attacks from peer-clients.Lastly, we employ optimization methods likeclient-batching or server-hierarchical, adoptingdifferent acceleration methods based on the ac-tual computational capabilities of the server.Experimental results on NLU and generationtasks demonstrate that FL-GLM achieves com-parable metrics to centralized chatGLM model,validating the effectiveness of our federatedlearning framework.",
  "Introduction": "Existing large language models (LLM) haveachieved astonishing results by utilizing vastamounts of public data and massive parameters.In comparison to public data, private data holdsadvantages in both quantity and quality, becauseprivate datasets typically encompass more compre-hensive and detailed information about individualsor organizations, and the data production process ismore rigorous. Therefore, private data can undoubt-edly further enhance the performance of LLM.However, private data is often stored in isolateddata silos. For example, mobile users data is keptlocally, involving a significant amount of personalprivacy. Considering privacy and security, LLMcannot store private data in a centralized manner fortraining. Hence, securely leveraging private datafor language model training remains a challenging problem. An ideal solution is to utilize the Feder-ated Learning (FL) (Li et al., 2020a) framework,which allows data to be retained on the user devicefor local training and only passes the model parame-ters to server for model aggregation. This approachachieves the goal of keeping the data stationarywhile making the model updates. By using FL forLLM training, data privacy can be preserved, andthe performance of LLM can be further improved.Unfortunately, traditional FL frameworks, suchas FedAvg (Stremmel and Singh, 2021) and Fed-Prox(Li et al., 2020b), are not suitable for LLMsbecause they require each client to have sufficientcomputational resources to train the entire LLM.As an alternative method, transformer with splitlearning, represented by FedBERT (Tian et al.,2022) in (a), focuses most of the param-eters on the server while continuously trainingthe embedding layer and output layer on the lo-cal client, making it more suitable for LLM. Theprocess involves the client using the embeddinglayer for data input and forwarding it to the server,which then computes and returns the output states.The client calculates loss, sends gradients backfor server updates, and receives updated gradientsfor the embedding layer. However, this methodpresents security risks. Embedding gradients arevulnerable to attacks (Yaldiz et al., 2023), poten-tially allowing attackers to reconstruct private datathrough beam search(Gupta et al., 2022) or reverse-engineered (Asnani et al., 2023). Additionally,since the server processes one client at a time, ithinders parallel training and reduces efficiency.In this paper, we propose a FL framework calledFL-GLM for LLM, as shown in (b). Wepartition the transformers of the chatGLM intothree parts: input and output blocks are stored onthe client (shared with peers), while the remaininglarge parameters are kept on the server. Duringthe training process, the client first performs for-ward propagation on the input data to obtain hiddenstates. Then, these hidden states are encrypted witha secure key and sent to the server. Subsequently,the server, either in a client-batch or server-layeredapproach, receives more client hidden states at atraining time and executes forward propagation tosend output hidden states back to each client.It is clear that our FL-GLM framework can ef-fectively prevent data leakage attacks from bothservers and peer-clients while enhancing trainingefficiency. Clients and servers jointly own and uti-lize the entire model, with certain input and output blocks placed on local clients to thwart embed-ding gradient attacks from the server. Althoughsharing input and output blocks between all clientscan improve results, interception by peers posesrisks, which can be resolved through key encryp-tion during client-server communication. To over-come server capacity limitations, we propose var-ious training acceleration methods. For clusterswith multiple machines and GPUs, a hierarchicalserver architecture initializes sub-servers for paral-lel client training, with central server aggregatingand distributing models. With single machines andmultiple GPUs, the client-batch method concate-nates client information for training, enabling par-allel execution and enhanced efficiency comparedto traditional serial execution in split learning.Experimental results on NLU and generationtasks demonstrate that FL-GLM achieves perfor-mance comparable to centralized chatGLM-6Bmodels, validating the effectiveness of our frame-work. Further analysis of training costs indicatesthat our client-batch and server-hierarchical mech-anisms can save more than 48% of training time.Our code is available at:",
  "Federated Learning (FL) has emerged as a promis-ing approach to train language models (LM) in": "a decentralized manner while respecting user pri-vacy and data safety. Federated Averaging (Fe-dAvg) (McMahan et al., 2017) is a popular feder-ated optimization algorithm used in language mod-els (Hard et al., 2018; Chen et al., 2019; Strem-mel and Singh, 2021). In FedAvg, each clienttrains its model on locally stored data and com-municates updates to the server. The server thenperforms weighted aggregation of these updatesto create a new global model. To reduce localtraining rounds and accelerate the learning pro-cess, Stremmel and Singh (2021) proposes to uti-lize the pre-trained global models on FedAvg. Jiet al. (2019) proposes Attentive Federated Aggrega-tion (FedAtt) and applies a layer-wise soft attentionmechanism to the trained parameters of the neu-ral network model.Previous works (Jalalirad et al.,2019; Thakkar et al., 2020) have integrated DPmechanisms into FedAvg and FedAtt, respectively.Split learning, represented by SplitFed (Thapaet al., 2022), has emerged as a distributed andcollaborative training approach to enable efficienttraining on resource-constrained devices (Abediand Khan, 2020; Abuadbba et al., 2020; Rahmanet al., 2020; Matsubara and Levorato, 2020), suchas mobile devices or small clients without GPUresources. To address sequential data training inlanguage models, FedBERT (Tian et al., 2022) in-troduces a novel federated learning framework. Itsplits language model pre-training, easing limitedcomputing resources on client devices. FedBERTsegments the BERT model into Embedding, Trans-former, and Output layers. It trains the Transformerlayer on a powerful server, while less demandinglayers (Embedding and Output) train on client de-vices. However, this setup incurs high communi-cation costs and risks data leakage via embeddinggradient attacks.",
  "Attacks and Defenses": "In federated learning,various eavesdroppersthreaten client privacy, including servers attempt-ing data recovery and peer-clients intercepting datasent to servers.In NLP, attacks from embedding gra-dients can easily recover userss private data. Guptaet al. (2022) proposes to infer which words theclient used by observing the non-zero values in em-bedding gradients. They then use beam search andresort to arrange these words, thereby reconstruct-ing private data. To counter this, they recommendfreezing embedding layers during training. Zhuet al. (2019) briefly mentions defending by adding differentially private noise or setting small gradi-ents to zero (gradient clipping). Huang et al. (2020)propose MixUp data augmentation on the BERTmodels [CLS] token. Yaldiz et al. (2023) sug-gest server-side cosine similarity checks on client-uploaded weights to filter out malicious clients.However, these defenses often reduce model accu-racy (Yu et al., 2021; Li et al., 2021).In order to retain the model structure and min-imize the performance loss caused by modelchanges, we propose to move some head layersto the client and use a key-encryption mechanismto protect data privacy during client-server commu-nication. This not only prevents gradient attacksfrom the server but also prevents information eaves-dropping from peers.",
  "Model": "In this section, we provide the details of the FL-GLM framework, as shown in (b). FL-GLM consists of three parts: model split, encryptedtransmission, and parallel acceleration. Firstly, wesplit LLM into three parts, saving the first block0 and the last block N-1 on the local client andplacing the remaining parameters on the server.Then, the smashed data is encrypted using keys dur-ing client-server transmission. Finally, the serveremploys either client-batch or hierarchical-servermethods to achieve parallel acceleration.",
  "Model Split": "For protecting data privacy, the FL-GLM frame-work splits LLM into three parts for deployment.Take ChatGLM as an example, the complete Chat-GLM model contains the embedding layer, 28 Chat-GLM blocks and the final linear layer, the clientside contains the embedding layer, the 0th Chat-GLM, the 27th ChatGLM block and the final linearlayer, and the 1st to 26th ChatGLM are deployed onthe server side, each block is a transformer struc-ture. During forward operations, the client-sidemodel processes private data to generate smasheddata, which is then sent to the server-side model forcomputation. Encrypting the smashed data ensuresits security. Given the input data x = {x1, . . . , xL}and the next output y, the smashed data h0 of theclient is defined as:",
  "L = Cross_Entropyy, y,": "where BlockN-1 is the N-1th block of LLM andLinear is the linear layer of LLM. During thewhole computation process, the data and data labelsare kept in the client to avoid data privacy leakage.Its important to note that the LLM-Block is con-structed from a transformer layer comprising multi-head self-attention mechanisms and a forward net-work (FFN). With the stacking of LLM-Blocks,large pre-trained models have an extremely highnumber of parameters, making fine-tuning com-putationally intensive. To fine-tune large modelswith limited computational resources, efficient tech-niques such as p-tuning v2(Liu et al., 2021b) canbe employed, as depicted in . The FL-GLM framework supports the p-tuning v2 method,wherein all original model parameters are frozen,and the prefix encoder is trained to splice the pre-fix_key and prefix_value with the key and valueof the original model, adjusting the output of eachLLM-Block. Further details see in Appendix A.",
  "Encrypted Transmission": "Since the data features need to flow between theclient and the server after the model split, the FL-GLM framework uses a key encryption strategy tocomplete the encrypted transmission of data. TheRSA algorithm generates a pair of public and pri-vate keys by factorizing a very large integer. Themessage is encrypted with the public key and canonly be decrypted by the receiver who has the cor-responding private key. The RSA key generationprocess is as follows:1) Select two large prime numbers, usually de-noted as p and q.2) Calculate their product n = pq. n will beused as the common modulus.3) Compute the Eulers totient function (n) =(p 1) (q 1). For a prime number p, there arep 1 numbers that are coprime with p; similarly,for a prime number q, there are q 1 numbers thatare coprime with q. Since p and q are coprime witheach other, the Eulers totient values for p and qcan be multiplied directly.4) Choose an integer e, called the public keyexponent, satisfying 1 < e < (n), and e and (n)are mutually prime.5) Compute the private key index d satisfyingd e 1 (Mod (n)). d is the multiplicativeinverse of e to (n).After the key computation is complete, n ande are disclosed as the public key, where n is themodulus and e is the public key index. Convertthe plaintext message M to an integer m with 0 <m < n. Calculate the ciphertextC = me (Mod n).C is the encrypted message. After receiving the ci- Batch parallel",
  "Parallel Acceleration": "After deploying the large model separately from theclient and the server, the server node will bear mostof the training cost, and according to the differencein the computing power of the server node, theFL-GLM framework supports two training strate-gies: serial training and parallel training. If theserver node has limited computing resources andcan hardly afford a large batch size, serial train-ing is a more suitable choice. As shown in (b), during serial training, the server interacts withonly one of the clients, and when one client com-pletes the training, the training process for the nextclient is started. After completing the training, theparameters of multiple client models need to beaveraged. Serial training is time-consuming, butone-to-one communication requires less commu-nication, thread processing, and server processingpower and is suitable for training scenarios withlimited server capacity.Since the special structure of split learning doesnot allow smashed data from multiple clients tobe averaged, which will result in features and la-bels not being aligned and a substantial decreasein model performance, two parallel training strate-gies are designed in the FL-GLM framework. Asshown in , the first strategy is to stack thesmashed data from different clients during paralleltraining as a set of data to expand the batch forcollaborative training. Take clients batch size=1as an example; the number of clients is M, and ineach round of training, every client sends smasheddata of size seqlength, batchsize=1, hiddensize to server parallel",
  ": FL-GLM with server-hierarchical parallel": "the server, and the data received by the server willbe integrated into a tensor with batch size M forsubsequent training. The second parallel strategyis shown in . Each client model will corre-spond to a server-side model, and the server nodewill run multiple models simultaneously, whichcan alleviate the threading problem in one-to-manycommunication to a certain extent. The server-sidemodel parameters and client-side parameters areaveraged at the end of the training period.",
  "We first introduce some empirical settings, includ-ing datasets, evaluation metrics, baselines and pa-rameter settings for FL-GLM": "4.1.1DatasetFor a fair comparison with centralized chatGLM-6B, we test our model on the SuperGLUE (Wanget al., 2019) benchmark for NLU tasks, and onCNN/DailyMail and XSum datasets for abstractivesummarization tasks.The SuperGLUE benchmark is a collection ofchallenging NLU tasks designed to evaluate theperformance and capabilities of state-of-the-art lan-guage models. It consists of eight diverse tasks, i.e.,ReCoRD, COPA, WSC, RTE, BoolQ, WiC, CB,and MultiRC, each representing a different aspectof language understanding. The details of the Su-perGLUE benchmark can be seen in Appendix B.",
  ": Results of abstractive summarization on the CNN/DailyMail and XSum test sets": "Following GLM (Du et al., 2022), we formulatethese tasks as blank infilling tasks. Specifically,given a labeled example (x, y), we rewrite the in-put x as a closed question q(x) through a masktoken [M] and rewrite output y as an answer a(y).For abstractive summarization tasks, we appenda mask token [M] at the end of the given contextas input and treat the summary as output. Then themodel generates the summary autoregressively. 4.1.2MetricsSince the NLU tasks are reformulated as blankinfilling tasks, the model performance can be evalu-ated using the generated probability of the ground-truth answer a(y). For the RTE, BoolQ, WiC, CB,and MultiRC datasets, the generated answer maycontain a single word. Therefore, we compute thelogit of the corresponding answer token as the eval-uation score, defined as:",
  "Baselines": "We apply FL-GLM to ChatGLM-6B model1, whois an open-source pre-trained language model with6 billion parameters and building upon the GeneralLanguage Model(GLM-130B) (Zeng et al., 2022;Du et al., 2022). Notely, our framework is not lim-ited to ChatGLM but can be widely applied to dif-ferent LLMs (such as Llama2). We use ChatGLMas a representative model to demonstrate that ourframework does not significantly degrade modelperformance. Considering that our future applica-tions will mainly focus on the Chinese domain, wechose ChatGLM-6B, which has been extensivelyaligned with human in the Chinese domain. Ad-ditionally, the ChatGLM-6B model offers a break-through scaling property that enables efficient infer-ence on a single RTX 3060 (12GB) GPU throughINT4 quantization. This property is especially valu-able in resource-constrained scenarios, allowing forcost-effective computation on affordable GPUs.For a fair comparison with ChatGLM-6B, fol-lowing GLM, we use 7 baselines, includingT5large (Raffel et al., 2020), BARTLarge (Lewiset al., 2020), RoBERTaLarge (Liu et al., 2019),GLMRoBERTa (Du et al., 2022), BERTSumAbs (Liuand Lapata, 2019), UniLMv2Base (Bao et al., 2020)and ChatGLM-6B (Zeng et al., 2022).",
  "Parameter Settings": "We utilize the open-source ChatGLM-6B modelas the basement model for the FL-GLM model. Ithas 28-layer transformer blocks, 4096 hidden-size,and 32 self-attention heads. We utilize P-tuningv2 for more efficient fine-tuning on downstreamtasks. Experiments are conducted on 2, 3, 5, and 10clients with NVIDIA A100 GPUs, 40GB RAM perclient, and one server with one NVIDIA A100 GPUand 40GB RAM. We generate RSA public andprivate keys at the beginning of FL and then passthe public keys between server and client. Duringthe FL process, the keys remain unchanged, andafter a certain number (hyper-parameter) of roundsof training, we regenerate and share the keys. Ourexperiments are conducted with communicationsimulated on the same host, but not in a for-loopmanner; rather, we coordinated information withFlower tool 2. In order to make a fair comparisonbetween our FL-GLM model and ChatGLM-6B,we used a batch size of one, a learning rate of2e-2 with the Adam optimizer, and adjusted thenumber of training epochs and maximum sequencelength according to different datasets without usingwarmup or weight decay. The code will be releasedwhen this paper is accepted.",
  "Metric-based Evaluation": "The quantitative evaluation results on SuperGLUEare shown in .From the results, wecan see that the recent large language models,such as ChatGLM-6B outperform the traditionalpre-training models, showing the effectiveness ofhuman-aligned language models for NLU tasks.As a distributed learning pattern, our FL-GLMmodel performs a little worse than the basementmodel, ChatGLB-6B. Take the accuracy of theReCoRD, RTE, BoolQ, and Wic datasets.Forexample, our FL-GLM model obtains 78.4, 81.6, 81.9, and 69.6, respectively, which is lower than thecentralized ChatGLB-6B model in the acceptablerange, i.e., 0.3, 1.5, 1.5, and 1.4. From the resultson CNN/DialyMail and XSum datasets in ,FL-GLM can obtain 39.6 ROUGE-1, 16.9 ROUGE-2, and 28.0 ROUGE-L on the CNN/DailyMaildataset, 37.0 ROUGE-1, 11.9 ROUGE-2, and 29.4ROUGE-L on the XSum dataset. Not more than 1.0lower than the results of the centralized ChatGLM-6B model. In conclusion, our FL-GLM has compa-rable ability to understand language and generaterelevant summary with centralized models.",
  "Analysis": "An analysis is conducted including training ef-ficiency, impact of Data non-IID and prove thesecurity of FL-GLM. We also conducted the ex-periments to analysis the impact of average pe-riod (Appendix C) and the impact of partici-pants(Appendix D). 4.3.1Training EfficiencyTo further investigate the impact of our speedupoptimization mechanism on the training cost, wetested the average training duration of the FL-GLMmodel under three training strategies: serial, client-batch, and server-hierarchical. We randomly se-lected 1000 data points from the ReCoRD datasetfor communication cost analysis experiment. Wetested 10 times and took the mean and standard de-viation of the total communication time, as shownin . From the results, we can see that thetime consumed in serial training mode with 1000data points is close to that of centralized training,while parallel training can significantly improvethe training time, which is directly proportional tothe number of clients.Furthermore, we measured training communi-cation time between two computers (server andclient) within the same LAN with a bandwidth of1100MB/s. Results indicate that training acrosstwo machines in the LAN is approximately fivetimes slower than centralized training (centralizedtraining: 0.91s/step, stimulate training FL-GLM insingle machine with two GPUs: 1.79s/step, train-ing FL-GLM with two machines: 4.83s/step).",
  "Impact of IID and non-IID": "The independent and identically distributed (IID)assumption is a foundational premise in traditionalmachine learning. However, in federated learning,data from different participants may exhibit het-erogeneity, making it difficult to satisfy the IID as-sumption. This presents one of the core challengesin federated learning. To evaluate the performanceof the FL-GLM framework in handling heteroge-neous data, we conduct the following experiments.We selected the COPA dataset from the Super-GLUE benchmark, which is a binary classificationdataset for textual causal judgment and contains400 training samples, with 195 labeled as 0 and205 labeled as 1. To simplify the analysis, we as-sumed the existence of two clients. After samplingwith the independent and identically distributed(IID) method, the dataset was divided into sub-datasets A and B. Sub-dataset A contains 97 sam-ples labeled as 0 and 102 samples labeled as 1,while sub-dataset B contains the remaining sam-ples. Then, we applied a non-IID sampling methodto divide the datasets into sub-datasets A and B.Sub-dataset A contains 195 samples labeled as 0and 5 samples labeled as 1, while B contains 200samples labeled as 1.The experimental results under three trainingstrategies are shown in . In the case ofnon-IID data, due to the issue of data heterogene-ity, the model performance of fine-tuning trainingusing both serial training strategy 5(a) and server-parallel strategy 5(b) decreases by approximately7%. However, client-batch parallel training 5(c) isnot significantly affected by the data distribution.This is because during client-batch parallel training,the data features from each client are stacked intobatches and sent to the server, allowing most ofmodel parameters on the server side to sufficientlylearn the data features, to some extent mitigates theperformance loss caused by non-IID data.",
  "Security Analysis": "Theoretical proof of the security of split learning ischallenging. Pasquini et al. (2021) propose an infer-ence attack method FSHA for feature data securityin split learning, where a malicious server restoresthe training dataset by hijacking the clients outputdata, which is validated in the field of image recog-nition, and is able to restore the clients trainingdataset effectively. Inspired by this method, weconduct security analysis experiments to indirectlydemonstrate the security of FL-GLM.An important prerequisite for FSHA is that themalicious server has a shadow dataset with thesame domain and task as the dataset held by the at-tacked party. However, in the private data domain,the data are all held by the training participants andprotected by legal regulations, and the server sidein the FL-GLM framework cannot obtain the samedomain data under normal circumstances. So weconsider the extreme case where, in serial trainingmode, at least one client colludes with the serverto share its private data, Dpriv1, with the serverfor the purpose of training an attack model. LetF be the first part of the model held by the mali-cious client. The malicious server-side constructsthe model F 1 for attacking and utilizes Dpriv1 to train F 1. During the attacking phase, the ma-licious server hijacks the smashed data outputtedby the attacked client, denoted as f , and utilizesF 1 to inference the privacy data Dpriv2 held bythe attacked client.The method is validated on theBoolQ dataset.The experimental results are shown in .When the client only has the embedding layer likeFedBert, which is refering as Embd., F 1 is asingle Linear layer, the attack model can achievea BLEU-4 score of 28.570 and a ROUGE-1 scoreof 33.290. While in the FL-GLM framework, theclient contains the embedding layer and an LLM-Block, noted as Client-side A, F 1 is a single",
  ": Security Analysis": "layer Transfomer(TF.), all the metrics of the attackmodel are all close to 0. Therefore, the securityof FL-GLM could be proven in experiments. Ad-ditionally, we find that the attack metrics perfor-mance of a single-block Transformer is similar tothat of a multi-block Transformer. Therefore, theoptimal split point, based on experimental results,might be a single-block Transformer, even thoughit is challenging to prove theoretically. 4.3.4More Basement ModelTo further validate the practicality and generaliz-ability of our FL-GLM framework, we incorpo-rated Llama2-7B-Chat (Touvron et al., 2023) asa baseline model. We split Llama2-7B-Chat ac-cording to the FL-GLM framework, resulting ina variant denoted as FL-Llama. The performanceevaluation experiment was conducted utilizing sixdatasets from the SuperGLUE benchmark with se-rial training strategy and utilizing the LoRA tofine-tune. Note that Llama2-7B-Chat supports in-struction fine-tuning, leading us to adapt the in-put prompts for each dataset in accordance withthe Metas official recommended instruction tem-plates, rather than employing the Cloze questiontemplates in Appendix B. The experimental re-sults are presented in , which indicate thatour proposed framework could achieve comparableaccuracy metrics, and suggest that the FL-GLMframework is agnostic to the base model type anddoes not significantly affect performance.Furthermore, we validated the FL-GLM frame-work on the Chinese medical dataset Huatuo-26M (Li et al., 2023), referring to the Llama2-Chinese-7B-Chat based FL-GLM framework asFL-Chinese-Llama. The full Huatuo-26M datasetcontains 2,623,904 Q&A pairs in the trainingdataset and 264,041 Q&A pairs in the test dataset.We randomly sampled 3,000 Q&A pairs from thefull dataset for training and 300 Q&A pairs fromthe test set for evaluation. It should be noted that,since the limited Chinese language capability ofLlama2-7B-chat, we chose Llama2-Chinese-7B-Chat 3 as the baseline model for this experiment,",
  ": Results on sampled Huatuo-26M test set": "which has been fine-tuned on a large Chinese cor-pus. As illustrated in , the proposed trainingframework maintains model performance that isclosely comparable to centralized training. Specifi-cally, the performance metrics of FL-GLM exhibita degradation of no more than 0.72 when comparedto ChatGLM-6B, while FL-Chinese-Llama showsa decline of no more than 0.38 when comparedto Llama2-Chinese-7B-Chat. Whereas, the inter-model comparisons show that the Chinese conver-sational capabilities of ChatGLM significantly sur-pass those of Llama2-Chinese-7B-Chat. Therefore,we recommend using ChatGLM as the base modelfor FL-GLM framework in Chinese application sce-narios.",
  "Conclusions": "To address the challenge of distributed training ofLLMs with limited client computational resources,we propose to utilize the split learning method tosegment the generative model. We place the inputand output blocks locally on client devices, whilethe remaining primary model parameters are cen-tralized on a server with ample computational re-sources. We secure client-server information trans-fers with encryption methods. To enhance trainingefficiency, we suggest selecting the client-batchand server-hierarchical acceleration optimizationmethods based on the servers actual computationalcapacity, thereby enabling parallel training. Thisdistributed architecture not only ensures that userprivate data remains on local devices but also ef-fectively reduces the training time, making it moresuitable for the scale and complexity of LLMs. Inthe future, we contemplate employing more ad-vanced privacy-preserving techniques, such as dif-ferential privacy, to safeguard the data transmittedfrom clients, enabling the application of large lan-guage models in privacy-sensitive scenarios.",
  "Limitations": "FL-GLM was evaluated on the SuperGLUE bench-mark, CNN/DailyMail and XSum datasets, anddespite achieving results close to those of the cen-tralized tests, it is still constrained by the privacy-utility trade-off, and we would like to further opti-mize the communication consumption of the cur-rent distributed training framework and achieveeven better model efficacy. In addition, our frame-work is currently limited to ChatGLM-6B. Futurework will extend FL-GLM to different LLMs, suchas Llama, to demonstrate its adaptability and widerapplicability.",
  "Ethical Considerations": "We propose a federated learning framework namedFL-GLM, which aims to use private data to trainLLM with considerations of prevent data privacyleakage. Our data originates from open-sourceNLU and NLG projects, adhering to their licenselimitations and public benchmarks. Moreover, weemulate a distributed data storage environment us-ing open-source datasets, ensuring the exclusion ofprivate data. We affirm our societal contributionwithout causing harm. The authors thank Zishuai Zhang for the de-velopment and maintenance of this project.This work was funded by the National Natu-ral Science Foundation of China (NSFC) un-der Grants No.62406013, the Advanced In-novation Center for Future Blockchain and Pri-vacy Computing (ZF226G2301) and the Fron-tier Cross Fund Program of Beihang University(501QYJC2023141001, 501QYJC2024141003).",
  "Inferring model hyperparameters from generated im-ages. IEEE Transactions on Pattern Analysis andMachine Intelligence": "Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, NanYang, Xiaodong Liu, Yu Wang, Songhao Piao, Jian-feng Gao, Ming Zhou, et al. 2020. Unilmv2: pseudo-masked language models for unified language modelpre-training. In Proceedings of the 37th InternationalConference on Machine Learning, pages 642652. Jiaao Chen and Diyi Yang. 2020. Multi-view sequence-to-sequence models with conversational structure forabstractive dialogue summarization. In Proceedingsof the 2020 Conference on Empirical Methods inNatural Language Processing (EMNLP), pages 41064118.",
  "Mingqing Chen, Rajiv Mathews, Tom Ouyang, andFranoise Beaufays. 2019.Federated learn-ing of out-of-vocabulary words.arXiv preprintarXiv:1903.10635": "Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:General language model pretraining with autoregres-sive blank infilling. In Proceedings of the 60th An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 320335. Yue Fang, Hainan Zhang, Hongshen Chen, ZhuoyeDing, Bo Long, Yanyan Lan, and Yanquan Zhou.2022. From spoken dialogue to formal summary:An utterance rewriting for dialogue summarization.In Proceedings of the 2022 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 38593869. Samyak Gupta, Yangsibo Huang, Zexuan Zhong,Tianyu Gao, Kai Li, and Danqi Chen. 2022. Recov-ering private text in federated learning of languagemodels. Advances in Neural Information ProcessingSystems, 35:81308143. Andrew Hard, Kanishka Rao, Rajiv Mathews, SwaroopRamaswamy, Franoise Beaufays, Sean Augenstein,Hubert Eichner, Chlo Kiddon, and Daniel Ramage.2018. Federated learning for mobile keyboard pre-diction. arXiv preprint arXiv:1811.03604. Yangsibo Huang, Zhao Song, Danqi Chen, Kai Li, andSanjeev Arora. 2020. Texthide: Tackling data privacyin language understanding tasks. In Findings of theAssociation for Computational Linguistics: EMNLP2020, pages 13681382. Amir Jalalirad, Marco Scavuzzo, Catalin Capota, andMichael Sprague. 2019. A simple and efficient fed-erated recommender system. In Proceedings of the6th IEEE/ACM international conference on big datacomputing, applications and technologies, pages 5358.",
  "Xuechen Li, Florian Tramer, Percy Liang, and TatsunoriHashimoto. 2021. Large language models can bestrong differentially private learners. In InternationalConference on Learning Representations": "Junpeng Liu, Yanyan Zou, Hainan Zhang, HongshenChen, Zhuoye Ding, Caixia Yuan, and Xiaojie Wang.2021a. Topic-aware contrastive learning for abstrac-tive dialogue summarization. In Findings of the Asso-ciation for Computational Linguistics: EMNLP 2021,pages 12291243. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam,Zhengxiao Du, Zhilin Yang, and Jie Tang. 2021b.P-tuning v2: Prompt tuning can be comparable tofine-tuning universally across scales and tasks. arXivpreprint arXiv:2110.07602.",
  "Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,Yujie Qian, Zhilin Yang, and Jie Tang. 2021c. Gptunderstands, too. arXiv e-prints, pages arXiv2103": "Yang Liu and Mirella Lapata. 2019. Text summariza-tion with pretrained encoders. In Proceedings ofthe 2019 Conference on Empirical Methods in Natu-ral Language Processing and the 9th InternationalJoint Conference on Natural Language Processing(EMNLP-IJCNLP), pages 37303740. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A robustly optimized bert pretraining ap-proach. arXiv preprint arXiv:1907.11692.",
  "detection in challenged networks. In IEEE Inter-national Conference on Pattern Recognition (IEEEICPR)": "Brendan McMahan, Eider Moore, Daniel Ramage,Seth Hampson, and Blaise Aguera y Arcas. 2017.Communication-efficient learning of deep networksfrom decentralized data. In Artificial intelligence andstatistics, pages 12731282. PMLR. Dario Pasquini, Giuseppe Ateniese, and MassimoBernaschi. 2021. Unleashing the tiger: Inferenceattacks on split learning. In Proceedings of the 2021ACM SIGSAC Conference on Computer and Commu-nications Security, pages 21132129. Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J Liu. 2020. Exploring the limitsof transfer learning with a unified text-to-text trans-former. The Journal of Machine Learning Research,21(1):54855551.",
  "Om Thakkar, Swaroop Ramaswamy, Rajiv Mathews,and Franoise Beaufays. 2020. Understanding un-intended memorization in federated learning. arXivpreprint arXiv:2006.07490": "ChandraThapa,PathumChamikaraMahawagaArachchige, Seyit Camtepe, and Lichao Sun. 2022.Splitfed: When federated learning meets split learn-ing. In Proceedings of the AAAI Conference on Arti-ficial Intelligence, volume 36, pages 84858493. Yuanyishu Tian, Yao Wan, Lingjuan Lyu, Dezhong Yao,Hai Jin, and Lichao Sun. 2022. Fedbert: When fed-erated learning meets pre-training. ACM Transac-tions on Intelligent Systems and Technology (TIST),13(4):126. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-preet Singh, Julian Michael, Felix Hill, Omer Levy,and Samuel R Bowman. 2019. Superglue: a stickierbenchmark for general-purpose language understand-ing systems. In Proceedings of the 33rd InternationalConference on Neural Information Processing Sys-tems, pages 32663280. Duygu Nur Yaldiz, Tuo Zhang, and Salman Avestimehr.2023. Secure federated learning against model poi-soning attacks via client filtering.In ICLR 2023Workshop on Backdoor Attacks and Defenses in Ma-chine Learning. Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi,Huseyin A Inan, Gautam Kamath, Janardhan Kulka-rni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz,et al. 2021. Differentially private fine-tuning of lan-guage models. In International Conference on Learn-ing Representations. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: Anopen bilingual pre-trained model. In The Eleventh In-ternational Conference on Learning Representations.",
  "AP-tuning v2": "P-tuning v2 is proposed based on the p-tuning(Liuet al., 2021c) algorithm, and its basic principle isto add a prompt of length Lp as a learnable em-bedding, denoted as a prefix, to each LLM-Blocksattention operation. Fine-tuning is done by freezingthe model parameters and training only the prefix.In each LLM-Block, the corresponding prefix con-tains two parts: prefix_key RLBNhdh andprefix_value RLBNhdh. Where L is thedata length, B denotes batch size, Nh denotes thenumber of attention heads, and dh is the dimensionof each head.In the process of forward operation, when thedata passes through each LLM-Block, the prefix isspliced with the frozen key and value in the modelto form a new key and value, which are denotedas K and V, respectively, with the original queryparameter (Q) of the model to compute the atten-tion score of the current data as well as the hiddenstate. Taking the i-th layer LLM-Block as an ex-ample, the computation process of p-tuning v2 isshown below:",
  "shows the cloze questions and answers forSuperGLUE tasks, and the detailed correspondingdescription of SuperGLUE benchmark are as be-low:": "ReCoRD(Reading Comprehension with Com-monsense Reasoning and Disambiguation): Inthis task, models are required to answer ques-tions by extracting information from a givenpassage, while also employing commonsensereasoning and resolving ambiguous pronouns. COPA(Choice of Plausible Alternatives): Thistask assesses causal reasoning abilities by pro-viding a premise and two alternative hypothe-ses, where the model must choose the correctcausal relationship.",
  "CImpact of Average Period": "For analyzing the effect of different averaging peri-ods on the model performance, we tested the perfor-mance of FL-GLM with different averaging periods(50 step and 100 step).The results are shown in , where the model with an average period of 100 steps slightlyoutperforms the model with an average period of50 steps in the BoolQ task. However, in the WiC,RTE, and MultiRC tasks, better results are achievedwith an average period of 50 steps. In the COPAand CB tasks, the averaging period has no effecton performance. The most noticeable differenceoccurs in the WSC task, with scores of 71.2 and66.3 for an average period of 50 steps and 100steps, respectively, for serial training, 63.5 and 65.4for client-batch parallel, and flat accuracy scoresfor server-hierarchical. Among all the evaluationtasks, the WSC task has the highest sensitivity tothe average period, but the average training periodhas little effect on the overall performance of theFL-GLM model with the same training strategy.",
  "DImpact of Participants": "In this section, we test the three training strate-gies with different numbers of clients by calcu-lating the accuracy scores of FL-GLM on dif-ferent datasets. 4The sequential test uses RTE,WiC, COPA, BoolQ, MultiRC and CB datasets,while the client-batch parallel test uses RTE, WiC,COPA datasets, and the server-hierarchical test usesBoolQ, COPA and RTE datasets, and the hyperpa-rameters such as learning rate are kept consistent.When using serial training strategy, the impactof increasing the number of clients is minimal, asshown in . This is because the majorityof parameters are trained on the server, makingthe number of clients insignificant in server-sideparameter training.When training in parallel, the accuracy scoreof FL-GLM decreases slightly as the number ofclients increases, which is more obvious on datasetswith smaller data volumes. For client-batch paralleltraining, as shown in , the accuracy scoredecreases with the increase in the number of clients 4In the client-batch parallel test, in order to mitigate theeffect of overfitting, the datasets are trained with the samenumber of training epochs for different numbers of clients,and normalization is used to enhance the visibility of theresults. due to the increase in the batch size, the frequencyof model parameter updating decreases, and theserver-side model is easy to converge to the saddlepoint. For hierarchical-server parallel, as shownin , the increase in the number of clientsmakes the amount of data for a single client smaller,so the more the number of clients, the more obviousthe overfitting phenomenon is."
}