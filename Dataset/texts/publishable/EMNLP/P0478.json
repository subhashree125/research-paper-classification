{
  "Abstract": "Question Under Discussion (QUD) is a dis-course framework that uses implicit questionsto reveal discourse relationships between sen-tences.In QUD parsing, each sentence isviewed as an answer to a question triggeredby an anchor sentence in prior context. Theresulting QUD structure is required to conformto several theoretical criteria like answer com-patibility (how well the question is answered),making QUD parsing a challenging task. Previ-ous works construct QUD parsers in a pipelinedmanner (i.e. detect the trigger sentence in con-text and then generate the question). However,these parsers lack a holistic view of the task andcan hardly satisfy all the criteria. In this work,we introduce QUDSELECT, a joint-trainingframework that selectively decodes the QUDdependency structures considering the QUDcriteria.Using instruction-tuning, we trainmodels to simultaneously predict the anchorsentence and generate the associated question.To explicitly incorporate the criteria, we adopta selective decoding strategy of sampling multi-ple QUD candidates during inference, followedby selecting the best one with criteria scorers.Our method outperforms the state-of-the-artbaseline models by 9% in human evaluationand 4% in automatic evaluation, demonstrat-ing the effectiveness of our framework. Codeand data are in",
  "Introduction": "Discourse structure describes the relationships be-tween different sentences of an article or conver-sation. The ability to understand discourse struc-ture is crucial for natural language processing taskssuch as text summarization (Durrett et al., 2016),conditional generation (Narayan et al., 2023), andnarrative understanding (Xu et al., 2024). Recentworks have adapted the Question Under Discus-",
  ": An article snippet along with the associatedQUD dependency structure. Each edge from si to sjwith attribute q indicates sentence sj anchors the ques-tion q, and sentence si answers the question q": "sion (QUD) framework to analyze discourse struc-tures (Benz and Jasinskaja, 2017; Riester et al.,2021). In the QUD framework (Van Kuppevelt,1995; Roberts, 2012), the relationships betweensentences in an article are characterized by (im-plicit) free-form questions. Each question is evokedby an anchor sentence in prior context, and an-swered by an answer sentence in the subsequentcontent. For instance, in , the relationshipbetween sentence 3 (referred to as s3) and the previ-ous context is that s3 answers the question Whichmovie has the most Oscar nominations? evokedby the anchor sentence s1.The QUD structures involve contextually-grounded questions that adhere to three theoreticalcriteria (De Kuthy et al., 2018; Wu et al., 2023;Riester et al., 2018): a) answer compatibility: thequestion must be answerable by the answer sen-tence in the discourse, like s2 directly answers thequestion Who starred in Forrest Gump? in Fig-ure 1; b) givenness: the question should only con-tain concepts that are accessible to the reader fromprior context or common knowledge, like ForrestGump in the question; and c) anchor relevance:the question should be relevant to the anchor sen-tence, e.g., the aforementioned question can betriggered in s1.",
  ": Overview of our QUDSELECT framework": "Previous works on QUD parsing break down thetask into two steps: anchor selection and questiongeneration. De Kuthy et al. (2020) develop a rule-based method for the question generation step, Koet al. (2023) train task-specific models for eachstep, while Wu et al. (2023) prompt large languagemodels (LLMs) in a stepwise manner. However,these approaches lack a holistic view of the task,causing the predicted QUDs to often fail to satisfyall the criteria. For instance, GPT-4 fails to generatequestions that are fully grounded on the anchorsentence in 50% of the cases.1 To address these challenges, we propose QUD-SELECT, a joint-training framework that selec-tively decodes QUD structures by incorporatingthe criteria, as shown in . Specifically,we instruction-tune models to jointly predict theanchor sentence and the corresponding questiongiven an answer sentence (e.g., s13) and prior con-text (e.g., s1, . . . , s12 of the article). We proposeselective decoding where we sample multiple an-chor and question pairs, score them using criteriascorers, and finally, select the best scored pair.Experiments conducted on the DCQA (Ko et al., 2022) dataset show that QUDSELECT outperformsbaselines by ~9% on average in human evaluation.To reduce resource and cost-intensive expert eval-uation, we develop automatic evaluators trainedon human annotations, and conduct a larger-scaleautomatic evaluation. The automatic evaluationresults show that QUDSELECT achieves arounda ~4% improvement over the selected baselines.Further analyses reveal that the performance couldbe further improved with more selected candidates.",
  "Related Work": "QUD is a linguistic framework that analyzes dis-course and pragmatics by viewing each sentence asan answer to an implicit question triggered in priorcontext (Van Kuppevelt, 1995; Roberts, 2012; Benzand Jasinskaja, 2017). While theoretical discus-sions around QUDs relied on constructed examples,Riester (2019) introduced an annotation frameworkfor reconstructing QUDs from data. Westera et al.(2020), Ko et al. (2022) and Hesse et al. (2020)annotated Ted-talk transcripts and news articles re-spectively in an expectation-driven manner, wherequestions are triggered while reading (i.e., unseendiscourse progression) while De Kuthy et al. (2018)annotated two interview transcripts with full, hier-archical questions.Recent works have begun adapting QUD for au-tomatic discourse parsing (Ko et al., 2022, 2023;Wu et al., 2023), narrative graph construction (Xuet al., 2024) and decontextualization of scientificdocuments (Newman et al., 2023). QUD fits wellfor understanding the structure and coherence oftexts that are intended to provide argumentation(Liu et al., 2024) and complex reasoning (Hu et al.,2022), and has potential applications to enhancedocument understanding in information extraction(Parekh et al., 2023, 2024a; Huang et al., 2024)with applications in wider domains like epidemiol-ogy (Parekh et al., 2024b) and biomedical science(Ma et al., 2023). Ko et al. (2023) introduced aQUD parser trained on DCQA (Ko et al., 2022)that consists of an anchor selection and a questiongeneration pipeline. Wu et al. (2023) evaluatedQUDs generated by LLMs by few-shot promptingin a two-step manner: question generation followedby anchor generation. Xu et al. (2024) followed",
  "Pipeline52.515.032.553.828.717.550.032.517.552.1Mistral-7B67.015.417.660.323.616.158.629.012.462.0+ QUDSELECT67.120.012.977.620.02.468.224.77.171.0": ": Automatic and human evaluation results. Numbers are in percentages (%). Best results are in bold, and thebest results of open-source models (if not the best overall) are underlined. Avg. indicates the average ratio of idealQUDs (the first option of each criterion). We abbreviate Direct Answer as Dir. Ans., Indirect Answer as Indir. Ans.,Answer Leakeage as Ans. Leak., Hallucination as Hall., and Grounded as G.",
  "The QUDSELECT Framework": "Task FormulationGiven a document with nsentences D = {s1, s2, . . . , sn}, QUD parsingaims to build a QUD dependency tree. We for-mulate the QUD parsing task as edge-level predic-tion following previous works (De Kuthy et al.,2018; Ko et al., 2023): given an answer sentencesi {s2, . . . , sn}2, models are asked to predict theanchor sentence ai {s1, . . . , si1} and generatethe question qi. Overview illustrates the structure of ourQUDSELECT framework. We first instruction tunea joint QUD parser 3.1. Then, we propose selec-tive decoding 3.2 to select the best candidate fromsampled anchor sentence, question pairs.",
  "QUD Parser Training": "Unlike previous works that use separate mod-els for anchor prediction and question genera-tion, we exploit the instruction following abilityof LLMs (Wang et al., 2022) to perform these twosteps jointly, as demonstrated in (left).This joint inference provides the model with a holis-tic view of the task. Given the answer sentence siand context of sentences prior to si, models areinstructed to output the anchor ai and the questionqi. We provide the instruction-response templatein Appendix A.",
  "Selective Decoding": "To incorporate specific criteria during inference,we sample multiple anchor sentence, questioncandidates and select the best one by using simplecriteria scorers.To generate multiple QUD candidates for acontext {s1, . . . , si1} and an answer sentence si,we sample multiple anchor sentences and questioncandidates by selectively utilizing beam-searchwith a wide beam while decoding.Followingprior work(De Kuthy et al., 2018; Benz andJasinskaja, 2017; Wu et al., 2023), we assumethat every answer sentence has a correspondingquestion. First, for anchor prediction, we promptthe model with sentence si is anchored by sentenceusing a beam size k to generate k possible anchors.Post deduplication of anchor candidates, we againutilize beam-search with size k to generate kquestion candidates for each anchor sentence. Thisencourages diversity in both the prediction ofanchor sentences and questions.We apply m criteria C = {c1, . . . , cm} to assessthe quality of generated candidates from differentaspects. Each criterion assigns a score cj(a, q) to a candidate a, q, and the overall score isthe summation of all criteria mj=1(cj(a, q)). Thecandidate with the highest overall score is selectedas the final prediction. Criteria Scorers.We consider the three key prin-ciples of QUD as our criteria: answer-compatibility,givenness, and anchor relevance. We implementreference-free and training-free scorers for each ofthem.Answer Compatibility: This criterion indicates that the question q should be answerable by theanswer sentence si. We regard this as a natural lan-guage inference (NLI) task, and use the probabilitythat si entails q measured by an off-the-shelf NLImodel (bart-large-mnli) as the compatibil-ity score.Givenness: This criterion evaluates if the ques-tion only consists of information from the context.An ideal question should be naturally invoked fromthe context, without concepts that appear out ofthin air. We measure the givenness with contentword overlap between q and the context s1...i1.We extract lemmas Lq and Lc of all content words(nouns, verbs, adjectives, and adverbs) in the ques-tion and the context, and compute the givennessscore as |Lq Lc|/|Lq|.Anchor Relevance: This criterion measures ifthe question q is relevant to the anchor sentence a.Similar to the givenness score, we approximate itwith content word overlap between a and the focusof q. We regard the maximum noun phrase of q asits focus fq, and extract lemmas Lfq and La of allcontent words in fq and a. The relevance score iscomputed as |Lfq La|/|Lfq|.",
  "Experimental Setup": "Models and DatasetsWe utilize the DCQAdataset (Ko et al., 2022) for training and evalu-ating QUD parsers. The DCQA dataset consistsof 22k English questions across 606 news articles.We use two instruction-tuned models LLaMA2-7B(Touvron et al., 2023) and Mistral-7B (Jiang et al.,2023) as base models of our framework. To explorethe effectiveness of selective decoding on closed-source models, we also apply it to GPT-4 (Achiamet al., 2023). We sample k = 10 candidates foreach answer sentence. Implementation details canbe found in Appendix A. BaselinesWe compare against two existing QUDparsers: the Pipeline approach (Ko et al., 2023)and GPT-4 prompting (Wu et al., 2023). We alsoprovide ablation of not using selective decodingduring inference, i.e., QUDSELECT with k = 1. Human EvaluationWe follow the annotationguidelines outlined in QUDEVAL (Wu et al., 2023)and evaluate the generated QUDs for answer com-patibility, givenness, and anchor relevance. De-tailed classification of the criteria is in Appendix B.We evaluate 100 questions across 8 articles fromthe DCQA test set. We recruit three annotators from Amazons Mechanical Turk after extensivetraining and screening. We report the majorityvote results and achieve an average inter-annotatoragreement of 68.3% averaged across all evaluateddimensions. More details are in Appendix C. Automatic EvaluationWhile human evaluationis more accurate for evaluating the efficacy of QUDparsing models, it is time-consuming and expensiveto collect at scale. To this end, we apply supervisedclassifiers to judge the generated QUDs. Specif-ically, we train RoBERTa classifiers (Liu et al.,2019) on the expert annotated data in QUDEVALfor answer compatibility and anchor relevance, andLongformer (Beltagy et al., 2020) for givennessdue to the longer context length. We achieve amacro F1 score of 0.48 for answer compatibility,0.42 for givenness, and 0.53 for anchor relevance,outperforming or matching the best existing auto-matic evaluators. Detailed comparisons with otherevaluators are in Appendix D. We conduct the au-tomatic evaluation on on 400 questions per modelacross 22 articles from the entire DCQA test set.",
  "Human Evaluation Results (bottom) re-ports the human evaluation results. We comparethe best open-source model from , QUDS-": "ELECT (Mistral-7B), with Pipeline and Mistral-7B.QUDSELECT (Mistral-7B) generates 67% directlyanswered questions, 78% questions with no unseenconcepts, and 68% fully grounded questions. Thishighlights the effectiveness of our framework ingenerating QUDs that satisfy the desired criteria. Error AnalysisOur detailed classifications ofthe evaluation metrics (Appendix B) allow us tocategorize the various errors made by the models.We find from that GPT-4 generates higherpercentage of directly answered QUDs but theseQUDs are more likely to have answer leakage er-rors. This indicates that GPT-4 tends to include",
  ": Example QUDs generated by QUDSELECT (Mistral) and the pipeline method for a test article. The fullarticle text can be found in Appendix . si indicates the i-th sentence in the article": "aspects from the answer sentence in the questionthat increases the answer compatibility but reducesthe givenness. We find that QUDSELECT improvesGPT-4 performance by reducing the answer leak-age error and improving the relevance of the anchor.Overall, we find that QUDSELECT improves thevalidity of the answers and increases the ground-ing of the questions in the anchor which leads toperformance improvements for all models. Number of Candidates",
  "To study the performance sensitivity of QUDSE-": "LECT to the number of candidates k, we vary kfrom 1 to 20 for QUDSELECT (LLaMA2-7B) andQUDSELECT (Mistral-7B) and show the perfor-mance in . The performance reveals anupward trend as k grows for Answer Compatibilityand Anchor Relevance while Givenness is sacri-ficed by a small margin for better overall perfor-mance. With k = 10, QUDSELECT significantlyoutperforms the selected baselines without signifi-cant runtime overhead.",
  "In , we show the QUDs generated by QUD-SELECT (Mistral-7B) and the Pipeline model for": "a news article (Appendix ) along with thehuman annotations for each question. Most QUDsgenerated by QUDSELECT (Mistral-7B) are explic-itly answerable, include no unseen concepts, andare fully grounded in the anchor. In contrast, thePipeline method generates incomplete questions orincompatible question-answer pairs for the givenarticle. This demonstrates the overall effectivenessof QUDSELECT in generating high-quality QUDs.",
  "Conclusion": "In this work, we propose QUDSELECT, a jointframework for generating QUD structures by in-tegrating key theoretical criteria. To achieve this,we reformulate the QUD parsing as an instructiontuning task and selectively decode the candidatequestions and anchors. Furthermore, we developautomated evaluation methods trained on expert an-notations to reduce the reliance on labor-intensiveexpert evaluations and facilitate model develop-ment for QUD parsing. Experiments demonstratethat QUDSELECT significantly outperforms base-lines in both automatic and human evaluations.",
  "Acknowledgements": "We thank Hritik Bansal and Sidi Lu for their con-structive comments. We thank the anonymous re-viewers for their helpful discussions and sugges-tions. Our work was supported by Optum Labs,Amazon Alexa AI Research Award, an AmazonResearch Award via UCLA Science Hub and theAmazon Fellowship (Tanmay Parekh) and we ex-press our gratitude for their support.",
  "the answer of a QUD entails the answer of its de-scendants (Roberts, 2012). Furthermore, QUDSE-": "LECT generates each QUD edge independently anddoes not model the relationships between questions.Thus, we leave the exploration of such discourselevel constraints to future work.Sampling Cost.Although the time cost in-creases when sampling more candidates for QUD-SELECT, the number of sampled unique anchorsdoes not increase, due to the limited number ofreasonable anchors in an article. The average num-ber of unique anchors is less than 3 when k = 20.Therefore, the growth of sampling cost is approx-imately linear to k. We find that increasing thenumber of candidates leads to an increase in themodel performance 5.2.",
  "Ethical Consideration": "Our framework relies on open-source and closed-source LLMs that may generate harmful and biasedoutputs. Therefore, it should be used with humansupervision. For human evaluation, we recruit an-notators from Amazons Mechanical Turk, and allannotators are fairly paid more than $15 USD perhour (which varies depending on the time spent perHIT), which is higher than the national minimumwage where the annotators are recruited. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774.",
  "Greg Durrett, Taylor Berg-Kirkpatrick, and Dan Klein.2016. Learning-based single-document summariza-tion with compression and anaphoricity constraints.arXiv preprint arXiv:1603.08887": "Christoph Hesse, Anton Benz, Maurice Langner, FelixTheodor, and Ralf Klabunde. 2020. Annotating qudsfor generating pragmatically rich texts. In Proceed-ings of the Workshop on Discourse Theories for TextPlanning, pages 1016. Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,et al. 2021. Lora: Low-rank adaptation of large lan-guage models. In International Conference on Learn-ing Representations. Nan Hu, Zirui Wu, Yuxuan Lai, Xiao Liu, and YansongFeng. 2022. Dual-channel evidence fusion for factverification over texts and tables. In Proceedings ofthe 2022 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 52325242. Kuan-Hao Huang, I-Hung Hsu, Tanmay Parekh, ZhiyuXie, Zixuan Zhang, Prem Natarajan, Kai-Wei Chang,Nanyun Peng, and Heng Ji. 2024. TextEE: Bench-mark, reevaluation, reflections, and future challengesin event extraction. In Findings of the Association forComputational Linguistics ACL 2024, pages 1280412825, Bangkok, Thailand and virtual meeting. As-sociation for Computational Linguistics.",
  "AQ Jiang, A Sablayrolles, A Mensch, C Bamford,DS Chaplot, D de las Casas, F Bressand, G Lengyel,G Lample, L Saulnier, et al. 2023. Mistral 7b (2023).arXiv preprint arXiv:2310.06825": "Wei-Jen Ko, Cutter Dalton, Mark Simmons, ElizaFisher, Greg Durrett, and Junyi Jessy Li. 2022. Dis-course comprehension: A question answering frame-work to represent sentence connections. In Proceed-ings of the 2022 Conference on Empirical Methods inNatural Language Processing, pages 1175211764,Abu Dhabi, United Arab Emirates. Association forComputational Linguistics. Wei-Jen Ko, Yating Wu, Cutter Dalton, Dananjay Srini-vas, Greg Durrett, and Junyi Jessy Li. 2023. Dis-course analysis via questions and answers: Parsingdependency structures of questions under discussion.In Findings of the Association for Computational Lin-guistics: ACL 2023, pages 1118111195, Toronto,Canada. Association for Computational Linguistics. Xiao Liu, Yansong Feng, and Kai-Wei Chang. 2024.Casa: Causality-driven argument sufficiency assess-ment. In Proceedings of the 2024 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies (Volume 1: Long Papers), pages 52825302. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A robustly optimized bert pretraining ap-proach. arXiv preprint arXiv:1907.11692. Mingyu Derek Ma, Alexander Taylor, Wei Wang, andNanyun Peng. 2023. DICE: Data-efficient clinicalevent extraction with generative models. In Proceed-ings of the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 1589815917, Toronto, Canada. Associationfor Computational Linguistics. Aman Madaan, Amrith Setlur, Tanmay Parekh, Barn-abas Poczos, Graham Neubig, Yiming Yang, RuslanSalakhutdinov, Alan W Black, and Shrimai Prabhu-moye. 2020. Politeness transfer: A tag and generateapproach. In Proceedings of the 58th Annual Meet-ing of the Association for Computational Linguistics,pages 18691881, Online. Association for Computa-tional Linguistics. Shashi Narayan, Joshua Maynez, Reinald Kim Am-playo, Kuzman Ganchev, Annie Louis, Fantine Huot,Anders Sandholm, Dipanjan Das, and Mirella Lap-ata. 2023. Conditional generation with a question-answering blueprint. Transactions of the Associationfor Computational Linguistics, 11:974996. Benjamin Newman, Luca Soldaini, Raymond Fok, Ar-man Cohan, and Kyle Lo. 2023. A question answer-ing framework for decontextualizing user-facing snip-pets from scientific documents. In Proceedings of the2023 Conference on Empirical Methods in NaturalLanguage Processing, pages 31943212. Tanmay Parekh, I-Hung Hsu, Kuan-Hao Huang, Kai-Wei Chang, and Nanyun Peng. 2023.GENEVA:Benchmarking generalizability for event argumentextraction with hundreds of event types and argumentroles. In Proceedings of the 61st Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 36643686, Toronto,Canada. Association for Computational Linguistics. Tanmay Parekh, I-Hung Hsu, Kuan-Hao Huang, Kai-Wei Chang, and Nanyun Peng. 2024a. Contextuallabel projection for cross-lingual structured predic-tion. In Proceedings of the 2024 Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies (Volume 1: Long Papers), pages 57385757,Mexico City, Mexico. Association for ComputationalLinguistics. Tanmay Parekh, Anh Mac, Jiarui Yu, Yuxuan Dong,Syed Shahriar, Bonnie Liu, Eric Yang, Kuan-HaoHuang, Wei Wang, Nanyun Peng, and Kai-WeiChang. 2024b. Event detection from social mediafor epidemic prediction. In Proceedings of the 2024Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies (Volume 1: Long Papers),pages 57585783, Mexico City, Mexico. Associationfor Computational Linguistics.",
  "Arndt Riester. 2019. Constructing qud trees. In Ques-tions in discourse, pages 164193. Brill": "Arndt Riester, Lisa Brunetti, and Kordula De Kuthy.2018.Annotation guidelines for questions underdiscussion and information structure. Informationstructure in lesser-described languages: Studies inprosody and syntax, pages 403443. Arndt Riester, Amalia Canes Npoles, and Jet Hoek.2021. Combined discourse representations: Coher-ence relations and questions under discussion. InProceedings of the First Workshop on IntegratingPerspectives on Discourse Annotation, pages 2630.",
  "AQUDSELECT Implementation Details": "We instruction-tune QUD parsers in the formatof . Similar to Yin et al. (2023), we ap-ply LORA (low-rank adaptation, Hu et al. (2021))with learning rate 2e 5, lorarank = 256, andloraalpha = 256. Models are trained for 2 epochswith batch size 128. During inference, we sampleQUD candidates with k beams and temperature1. All the experiments are performed with 48GBNVIDIA A6000 GPUs.",
  "We follow the evaluation protocol outlined in (Wuet al., 2023) for our human and automatic evalua-tion": "Answer Compatibility: This criterion indi-cates that the question q should be answerableby the answer sentence si. For evaluation,we classify each q si pair as a) Direct andExplicit Answer (Dir.): si answers the q ex-plicitly, b) Unfocused (Unfocus.): some partsof si answer q indirectly, or c) Not Answered:si does not answer q. Givenness: This criterion evaluates if the ques-tion only consists of information from thecontext. An ideal question should be natu-rally evoked from the context, without con-cepts that are not accessible to the reader fromcommon knowledge. This criterion has thefollowing categories a) No new concepts (NoNew): q does not contain any concepts beyondthe context or common knowledge, b) Answerleakage (Ans. leak.): q contains concepts thatare not in the context but in si, c) Hallucina-tion (hall.): q contains new concepts that arenot answer-leakage.",
  "CHuman Evaluation Details": "We provide the annotation template and trainingmaterials in and 7. All annotators were re-curited from Amazons Mechanical Turk and fairlypaid more than $15 USD per hour which varied de-pending on the time spend per HIT (more than thenational minimum wage where the annotators arerecruited). To ensure high quality annotations, theannotators were provided with extensive guidelinesand training ().We measure inter-annotator agreement withKrippendorffs . As shown in , annota-tors achieve moderate\" agreement across AnswerCompatibility and Givenness (Artstein and Poe-sio, 2008). Since, relevance between two concepts(question and achor) is highly dependent on theannotators comprehension of the article, we findthat agreement score for Anchor Relevance is fair\"(Artstein and Poesio, 2008). We also note the pair-wise agreement in . The agreements arecomparable with those in QUDEVAL, and indicatea certain degree of subjectivity in QUD analysis.",
  "DAutomatic Evaluator Details": "We train automatic evaluators with the human an-notations from QUDEVAL. Experienced human an-notators assess the answer compatability, giveness,and anchor relevance of 2,040 machine-generatedQUDs from 51 articles. We randomly split the arti-cles into training/validation/test sets with the ratioof 60%/15%/25%.We fine-tune classifiers for each criterion indi-vidually. Similar to Madaan et al. (2020), we useRoBERTa-large (Liu et al., 2019) as the backbonemodel of answer compatibility and anchor rele-vance, and Longformer-base (Beltagy et al., 2020)as the backbone model of givenness due to the longer context length. For answer compatibility,the input to the model is the question and the an-swer sentence, and the output is one of the threelabels Dir-Ans., Unfocus., and Not-Ans. For given-ness, the input is the context (sentences before theanchor sentence in the article) and the question,and the answer is one of the three labels No-New.,Ans-leak., and Hallu. For anchor relevance, the in-put is the question and the anchor sentence, and theoutput is one of the three labels Full., Some., andNo-G. Models are fine-tuned for 10 epochs withthe learning rate 1e 5 and batch size 32.We report the F1 scores of our automatic eval-uators in . For reference, we also providethe F1 scores of the random baseline, and the bestreference-free and reference-based metrics fromQUDEVAL (Wu et al., 2023). GPT-Scr (w/o ref)and GPT-Scr (w/ ref) indicate prompting GPT-4 toscore without and with the human-annotated refer-ence QUD. BERTScore means calculating the sim-ilarity between the candidate and reference QUDwith BERTScore (Zhang et al., 2019). The rule-based method checks if all content words in thecandidate question are presented in the context.Please refer to the QUDEVAL paper for more de-tails. Note that the results of random and ours areconducted on our held-out test set, while the re-sults of baseline evaluators are conducted on twoheld-out articles. Our evaluators are better thanor comparable with the baselines, highlighting thecredibility of using them in automatic evaluation.",
  "In 4 we focus on three criteria: answer compatibil-ity, givenness and anchor relevance. We highlight": "that anchor relevance refers to the measure of rele-vance between the question and anchor (B. There-fore, in our evaluation framework we evaluate thecorrectness of the selected anchor as how relevantit is to the question. An anchor that is incorrect ornot relevant would be considered not-grounded.From , we see that QUDSELECT reducesthe percentage of not grounded questions generatedby the model and therefore improves the overallquality of the QUDs generated. To further analysethe correctness of the anchor selection we report theagreement accuracy () of the the selected an-chor sentences with the human annotated anchorsfrom the DCQA dataset. Note that this is a partialnotion of accuracy and does not accurately repre-sent the quality of a model, since it is natural fordifferent questions to be triggered from differentsentences (Ko et al., 2023).",
  "We provide the article snippet used in the case studyin . The article is from the DCQA dataset.We also provide questions generated by other mod-els in": "1. U.S. exports of nuclear material cannot be adequately tracedfrom country to country, according to a congressional report.2. Scarcely a day goes by without a report of a new black marketdeal, said Sen. John Glenn in a statement reacting to the report.3. Given the staggering amount of nuclear materials we haveexported, it could only be a matter of time before some of thisdeadly contraband proves to be of U.S. origin.4. As chairman of the Senate Committee on Governmental Affairsin the last Congress, Glenn commissioned the report from theGeneral Accounting Office, which conducts investigations forlegislators.5. The report says hundreds of tons of plutonium and highly en-riched uranium have accumulated worldwide, mostly from nuclearpower generation."
}