{
  "Abstract": "Predictions of word-by-word conditional proba-bilities from Transformer-based language mod-els are often evaluated to model the incrementalprocessing difficulty of human readers. In thispaper, we argue that there is a confound posedby the most common method of aggregatingsubword probabilities of such language modelsinto word probabilities. This is due to the factthat tokens in the subword vocabulary of mostlanguage models have leading whitespaces andtherefore do not naturally define stop proba-bilities of words. We first prove that this canresult in distributions over word probabilitiesthat sum to more than one, thereby violatingthe axiom that P() = 1. This property resultsin a misallocation of word-by-word surprisal,where the unacceptability of the end of the cur-rent word is incorrectly carried over to the nextword. Additionally, this implicit prediction ofword boundaries incorrectly models psycholin-guistic experiments where human subjects di-rectly observe upcoming word boundaries. Wepresent a simple decoding technique to reac-count the probability of the trailing whitespaceinto that of the current word, which resolvesthis confound. Experiments show that this cor-rection reveals lower estimates of garden-patheffects in transitive/intransitive sentences andpoorer fits to naturalistic reading times.",
  "Introduction": "Language models (LMs), which are trained to makepredictions about upcoming words, are at the coreof many natural language processing (NLP) appli-cations. While most contemporary applicationsinvolve generating text by sampling from the LMsconditional probability distribution, the magnitudesof the probabilities they assign to each word in agiven sentence have been important from two per-spectives. The first is from the perspective of LMinterpretability, which aims to study their predic-tions and the linguistic knowledge encoded in their representations. A well-established paradigm inthis line of research is what has been dubbed tar-geted syntactic evaluation (Linzen et al., 2016;Gulordava et al., 2018; Marvin and Linzen, 2018),in which probabilities of critical words in mini-mal pairs (e.g. grammatical vs. ungrammatical sen-tences) are compared.Moreover, in cognitive modeling, conditionalprobabilities from LMs are used to model the word-by-word reading times of human subjects, oftenunder the theoretical link that the contextual pre-dictability of a word determines its processing diffi-culty (Hale, 2001; Levy, 2008). Recent work in thisline of research has evaluated surprisal estimates(i.e. negative log probabilities) from LMs and hasshown that surprisal from larger Transformer-basedmodel variants are less predictive of naturalisticreading times (Oh and Schuler, 2023b; Shain et al.,2024; Steuer et al., 2023) and that surprisal greatlyunderpredicts the processing difficulty of garden-path constructions (van Schijndel and Linzen, 2021;Arehalli et al., 2022; Huang et al., 2024).As such, while the use of word-by-word proba-bilities from LMs is popular in computational lin-guistics research, we argue that there is a confoundfor calculating them correctly that has gone un-addressed. This confound is posed by subwordtokenization schemes (e.g. byte-pair encoding; Sen-nrich et al., 2016) that are used to define the token-level vocabulary for training most contemporaryLMs (e.g. AI@Meta, 2024; Google Gemini Team,2024; Jiang et al., 2023). For languages that usewhitespace orthography, these subword tokeniza-tion schemes often build the whitespace characterdirectly into the front of the tokens, thereby result-ing in leading whitespaces. As a consequence, thestop probability of a word (i.e. the probability ofthe trailing whitespace) is never explicitly calcu-lated, and therefore the sum over the probabilitiesof all possible whitespace words can exceed one.We propose a simple and efficient decoding method that reaccounts the probability of thetrailing whitespace into that of the current word,which resolves this confound. Regression resultsshow that this correction reveals significantly lowersurprisal-based estimates of garden-path effects intransitive/intransitive sentences and poorer fits ofLM surprisal to naturalistic reading times.",
  "Proof of Inconsistent Word Probabilities": "On languages that use whitespace orthography, thevocabulary V defined by the subword tokeniza-tion scheme consists of the set of tokens that be-gin with a whitespace VB, and the set of tokensthat do not begin with a whitespace VI.1 In thecontext of next-word prediction, the sample spaceof a whitespace-delimited word is = {x1..n |x1VB, x2..nVI, nN}, where n is the total num-ber of subword tokens in each whitespace word asdetermined by the tokenizer.",
  "= 1 1 = 1.(1)": "If word probabilities are simply defined as theproduct of the probabilities of the tokens withinthose words, then P(x1= j1) + P(x1=j1, x2= j2) > 1,P() > 1, and therefore the axiom is violated.For example, given the minimal pair I was amatron in France and I was a mat in France, wherematron is more likely than mat, the LM tokenizesthe two sentences as follows and calculates the",
  "I was a mat in France(3)": "The presence of leading whitespaces results in anincorrect allocation of word-by-word surprisal. Ascan be seen in Example 2, due to this tokenization,P(mat ron | I was a) is factorized into P(mat |I was a) P(ron | I was a mat), and thereforeit follows that P(mat ron | I was a) P(mat |I was a), despite the fact that matron is moreacceptable than mat in the above context. Instead,part of the unacceptability of mat is incorrectlycarried over to P(in | I was a mat), where incompetes for probability mass against the highlylikely ron (a).",
  "In the context of the LMs tokens, is used to denotethe explicit whitespace character that is part of the token, andwhitespace is used to delimit subword tokens": "In Example 4, when human readers see mat, theyknow that the next keystroke will reveal a newwhitespace-delimited word (analogous to observ-ing that the next token will be in VB) and not trans-form it into e.g. matron (analogous to observingthat the next token will be in VI). In contrast, LMsdefine a probability distribution over both VB andVI after the token mat in the sequence The cat saton the mat.While this confound is more apparent in the self-paced reading paradigm, this is also a potential con-found for studying data collected through the typi-cal eye-tracking paradigm. This is because nativespeakers of languages with whitespace orthogra-phies have been shown to be sensitive to the loca-tion of upcoming whitespaces through parafovealprocessing and utilize this information to plan eyemovements (Pollatsek and Rayner, 1982; Rayneret al., 1998; Perea and Acha, 2009). Therefore,although information about word boundaries is notdirectly built into the design of the paradigm, itcan be argued that human subjects engaged in theeye-tracking paradigm also face little uncertaintyabout upcoming word boundaries.",
  "Proposed Solution: Whitespace-TrailingDecoding": "This inconsistency and confound can be resolvedby reaccounting the probability of the trailingwhitespace as part of the words probability, inlieu of that of the leading whitespace as LMs cur-rently do (Examples 2 and 3). To this end, we pro-pose whitespace-trailing (WT) decoding. Given aword wt+1 that consists of subword tokens xnt+1..nt+1,where nt is the total number of subword tokensin the word sequence w1..t, and xnt+1VB, andxnt+2..nt+1VI, WT decoding reallocates the prob-ability of the leading whitespace of each word toits previous word:3",
  "P(xnt+1VB | w1..t).(5)": "3See Appendix A for the proof that WT decoding resultsin consistent word probabilities. However, we note that WTdecoding does not resolve other issues with subword unitsthat may be addressed by re-training LMs with different to-kenization schemes (e.g. Nair and Resnik, 2023), which cannonetheless be expensive. Concurrent work by Pimentel andMeister (2024) points out this same issue and also proposesWT decoding.",
  "P( | I was a).(6)": "As WT decoding simply involves the factoriza-tion of whitespace probabilities by marginalizingover tokens in VB and rearranging them, it requiresno modifications to the LM and minimal over-head. Additionally, the joint probability of theentire sequence, and therefore metrics like perplex-ity, changes minimally by a factor of the probabilityof the final trailing whitespace with WT decoding.As can be seen in b, incorporating theprobabilities of trailing whitespaces correctly dif-ferentiates between matron and mat in this context,and removes the inherent relationship between thetwo probabilities that holds with leading whites-paces. Additionally, the unacceptability of matthat was incorrectly carried over to in in Example3 is now reflected in P(mat | I was a).LM probabilities with trailing whitespaces arealso better aligned with the self-paced readingparadigm where the upcoming word boundariesare directly observed. For example, the calculationof P(mat | The cat sat on the) precludes theprediction of tokens in VI directly after mat, whichcorrectly reflects the fact that the next keystroke inExample 4 will reveal a new whitespace word.",
  "Experiment 1: Surprisal-BasedEstimates of Garden-Path Effects": "Equation 6 shows that WT decoding will resultin an increase (or decrease) in probability to theextent that the next token is likely to be in VB pro-portional to the extent that the first token of thecurrent word was likely to be in VB. The first experi-ment demonstrates that the confound posed by lead-ing whitespaces affects surprisal-based estimatesof garden-path effects in transitive/intransitive sen-tences (Mitchell, 1987; Gorrell, 1991), which iscaused by syntactic disambiguation that takes placeat the critical word (highlighted in magenta).",
  "Procedures": "The experimental procedures closely follow thoseof Oh and Schuler (2023a), who evaluated surprisalestimates from Pythia LM variants (Biderman et al.,2023) with different model sizes and training dataamounts on self-paced reading (SPR) times fromthe Natural Stories Corpus (Futrell et al., 2021) andgo-past durations (GPD) from the Dundee Corpus(Kennedy et al., 2003).6 LMER models with thefollowing formulae were respectively fit to the Nat-ural Stories and Dundee corpora, whose likelihoodswere then subtracted from those of the baselineLMER models without surprisal to calculate theincrease in log-likelihood due to surprisal, or LL:",
  "RT ~ surp + surp_prev1 + surp_prev2 + s(length) +freq + freq_prev1 + freq_prev2 + s(index) +(1 | subject) + (1 | item),": "where length is word length in characters, indexis the position of the word within the sentence, andthe frequency predictors were log-transformed.These modeling choices assume a linear relation-ship between surprisal and reading times (Shainet al., 2024; Wilcox et al., 2023), and that surprisaland log frequency from two previous words have alingering influence on the current word (spillovereffects). These LMER models were subsequentlyused to predict word-by-word reading times (in ms)for 24 items in the ambiguous condition (Exam-ple 7) and the unambiguous control condition (Ex-ample 8) of the transitive/intransitive construction,which were read by 2,000 subjects (n=15, 915).The increase in the predicted reading times ofthe disambiguating critical word and two subse-quent words due to the increase in surprisal acrossconditions was estimated using LMER models withthe following formula to quantify the magnitude ofsurprisal-based garden-path effects at each word:4",
  "Results": "shows that surprisal estimates calculatedwith WT decoding results in poorer fits to natural-istic reading times, especially for LMs that haveseen around 256 to 1,000 batches of training dataon both corpora. Nonetheless, the peak in LLat around 1,000 training batches7 and the adverseeffect of model size at the end of LM training (Ohand Schuler, 2023a) are replicated. In contrast tothese results, Pimentel and Meister (2024) reportsmall improvements on the same two corpora asa result of applying WT decoding to fully trainedPythia LMs. We conjecture this is due to differentregression modeling procedures involving differentbaseline predictors.",
  "log(GPD) ~ surp + length + index + slength + pfix +(surp + length + index + slength + pfix +1 | subject) + (1 | sentid),": "where slength is the saccade length, pfix iswhether the previous word was fixated, and sentidis the index of the sentence within each corpus.These procedures were repeated with and withoutWT decoding to calculate LLWT and LLWL re-spectively, and the change in fit to reading times asa result of addressing the confound was calculated.",
  "The change in log-likelihood (LLWT LLWL) at 1,000training batches is significant at p < 0.001 level on bothcorpora by a permutation test of aggregated squared errors": "have leading whitespaces in most models, mean-ing that the stop probability of a whitespace wordis never explicitly calculated, which can result inword probability distributions whose sum exceedsone. We proposed WT decoding as a solutionfor these issues, and demonstrated that address-ing them reveals lower surprisal-based estimatesof transitive/intransitive garden-path effects andpoorer fits of LM surprisal to naturalistic read-ing times. Other targeted syntactic constructionsand naturalistic reading time corpora may similarlyshow systematic changes to word probabilities.More generally, addressing these issues will havethe biggest impact on probabilities of words neigh-boring low-probability whitespaces, such as thoseat potential phrasal/clausal boundaries where LMswill likely predict a punctuation mark. These issueswill also be more pronounced for LMs that are notable to predict word boundaries accurately, such asthose trained on smaller amounts of data. There-fore, future studies using LM word probabilitiesfor interpretability and cognitive modeling researchshould control for them through WT decoding.8 We thank the ARR reviewers and the area chair fortheir helpful comments. This work was supportedby the National Science Foundation (NSF) grant#1816891. All views expressed are those of theauthors and do not necessarily reflect the views ofthe NSF. Computations for this work were partlyrun using the Ohio Supercomputer Center (1987).",
  "Limitations": "The confound in the connection between word-by-word conditional probabilities of Transformer-based language models and human reading timesidentified in this work is supported by experimentsusing language model variants trained on Englishtext and data from human subjects that are nativespeakers of English. Therefore, the confound iden-tified in this work may not generalize to other lan-guages, in particular those that do not use whites-pace orthography. Additionally, this work is con-cerned with the use of language models as cognitivemodels of human sentence processing, and there-fore does not relate to their use in natural languageprocessing applications, such as text generation,summarization, or question answering.",
  "Ethics Statement": "This work used data collected as part of previouslypublished research (Huang et al., 2024; Luke andChristianson, 2018; Futrell et al., 2021; Kennedyet al., 2003). Readers are referred to the respec-tive publications for more information on the datacollection and validation procedures. As this workfocuses on studying the connection between condi-tional probabilities of language models and humansentence processing, its potential negative impactson society appear to be minimal.",
  "Suhas Arehalli, Brian Dillon, and Tal Linzen. 2022": "Syntactic surprisal from neural models predicts, butunderestimates, human processing difficulty fromsyntactic ambiguities. In Proceedings of the 26thConference on Computational Natural LanguageLearning, pages 301313. Stella Biderman, Hailey Schoelkopf, Quentin GregoryAnthony, Herbie Bradley, Kyle OBrien, Eric Hal-lahan, Mohammad Aflah Khan, Shivanshu Purohit,USVSN Sai Prashanth, Edward Raff, Aviya Skowron,Lintang Sutawika, and Oskar van der Wal. 2023.Pythia: A suite for analyzing large language mod-els across training and scaling. In Proceedings of the40th International Conference on Machine Learning,volume 202, pages 23972430. Richard Futrell, Edward Gibson, Harry J. Tily, IdanBlank, Anastasia Vishnevetsky, Steven Piantadosi,and Evelina Fedorenko. 2021. The Natural Storiescorpus: A reading-time corpus of English texts con-taining rare syntactic constructions. Language Re-sources and Evaluation, 55:6377.",
  "Google Gemini Team. 2024.Gemini: A family ofhighly capable multimodal models. arXiv preprint,arXiv:2312.11805v3": "Paul Gorrell. 1991. Subcategorization and sentence pro-cessing. In R. C. Berwick, S. P. Abney, and C. Tenny,editors, Principle-Based Parsing: Computation andPsycholinguistics, pages 279300. Springer, Dor-drecht. Kristina Gulordava, Piotr Bojanowski, Edouard Grave,Tal Linzen, and Marco Baroni. 2018. Colorless greenrecurrent networks dream hierarchically.In Pro-ceedings of the 2018 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages11951205. John Hale. 2001. A probabilistic Earley parser as a psy-cholinguistic model. In Proceedings of the SecondMeeting of the North American Chapter of the Asso-ciation for Computational Linguistics on LanguageTechnologies, pages 18. Kuan-Jung Huang, Suhas Arehalli, Mari Kugemoto,Christian Muxica, Grusha Prasad, Brian Dillon, andTal Linzen. 2024. Large-scale benchmark yields noevidence that language model surprisal explains syn-tactic disambiguation difficulty. Journal of Memoryand Language, 137:104510. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, Llio Renard Lavaud,Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,Thibaut Lavril, Thomas Wang, Timothe Lacroix,and William El Sayed. 2023. Mistral 7B. arXivpreprint, arXiv:2310.06825.",
  "Rebecca Marvin and Tal Linzen. 2018. Targeted syn-tactic evaluation of language models. In Proceedingsof the 2018 Conference on Empirical Methods inNatural Language Processing, pages 11921202": "Don C. Mitchell. 1987. Lexical guidance in humanparsing: Locus and processing characteristics. InM. Coltheart, editor, Attention and Performance XII:The Psychology of Reading, pages 601618. Erlbaum,Hillsdale, NJ. Sathvik Nair and Philip Resnik. 2023. Words, subwords,and morphemes: What really matters in the surprisal-reading time relationship? In Findings of the Associ-ation for Computational Linguistics: EMNLP 2023,pages 1125111260.",
  "Keith Rayner, Martin H. Fischer, and Alexander Pollat-sek. 1998. Unspaced text interferes with both wordidentification and eye movement control. Vision Re-search, 38(8):11291144": "Rico Sennrich, Barry Haddow, and Alexandra Birch.2016. Neural machine translation of rare words withsubword units. In Proceedings of the 54th AnnualMeeting of the Association for Computational Lin-guistics, pages 17151725. Cory Shain, Clara Meister, Tiago Pimentel, Ryan Cot-terell, and Roger Levy. 2024. Large-scale evidencefor logarithmic effects of word predictability on read-ing time. Proceedings of the National Academy ofSciences, 121(10):e2307876121.",
  "Theorem 2Applying whitespace-trailing decoding results in word probabilities that satisfy the Kol-mogorov (1933) axiom that P() = 1": "ProofIn the context of predicting wt+1 given w1..t, the sample space is ={xnt+1..nt+1|xnt+1VB, xnt+2..nt+1VI, {nt, nt+1}N, nt+1>nt}, where nt is the total number of subword tokens in theword sequence w1..t, and nt+1 is the total number of subword tokens in the word sequence w1..t+1. There-fore, P() is the total sum of word probabilities when nt+1 nt = 1, 2, 3, ... .The sum of word probabilities according to Equation 5 when nt+1 nt = 1 is:",
  "BPreprocessing Procedures for Naturalistic Reading Time Corpora": "The Natural Stories Corpus (Futrell et al., 2021) provides self-paced reading times from 181 subjectsthat read 10 English stories (10,256 words), which were filtered to exclude those shorter than 100 ms orlonger than 3000 ms, those of sentence-initial and -final words, and those from subjects who answeredfewer than four comprehension questions correctly. Approximately 50% of the observations (384,905observations) selected based on the sum of the subject index and the sentence index was used to fit theLMER models and calculate LL.The Dundee Corpus (Kennedy et al., 2003) provides fixation durations from 10 subjects that read 67English newspaper editorials (51,501 words), which were filtered to exclude those from unfixated words,those of words following saccades longer than four words, and those of sentence/document/line/screen-initial and -final words. Again, approximately 50% of the observations (98,115 observations) selectedbased on the sum of the subject index and the sentence index was used to fit the LMER models andcalculate LL."
}