{
  "Abstract": "Multilingual language models are widely usedto extend NLP systems to low-resource lan-guages. However, concrete evidence for theeffects of multilinguality on language model-ing performance in individual languages re-mains scarce. Here, we pre-train over 10,000monolingual and multilingual language mod-els for over 250 languages, including multi-ple language families that are under-studied inNLP. We assess how language modeling per-formance in each language varies as a func-tion of (1) monolingual dataset size, (2) addedmultilingual dataset size, (3) linguistic simi-larity of the added languages, and (4) modelsize (up to 45M parameters). We find that inmoderation, adding multilingual data improveslow-resource language modeling performance,similar to increasing low-resource dataset sizesby up to 33%. Improvements depend on thesyntactic similarity of the added multilingualdata, with marginal additional effects of vo-cabulary overlap. However, high-resource lan-guages consistently perform worse in multilin-gual pre-training scenarios. As dataset sizes in-crease, adding multilingual data begins to hurtperformance for both low-resource and high-resource languages, likely due to limited modelcapacity (the curse of multilinguality). Theseresults suggest that massively multilingual pre-training may not be optimal for any languagesinvolved, but that more targeted models cansignificantly improve performance.",
  "Introduction": "Multilingual language models have been a fixtureof natural language processing (NLP) researchnearly since the introduction of Transformer lan-guage models (Devlin et al., 2019; Conneau et al.,2020a). These models are often pre-trained on over100 languages simultaneously, and they are widelyused for NLP tasks in low-resource languages (Ade-lani et al., 2021; Ebrahimi et al., 2022; Hangya et al., 2022; Imani et al., 2023), cross-lingual trans-fer learning (Pires et al., 2019; Conneau et al.,2020a), and multilingual text generation (Lin et al.,2022; Scao et al., 2022). However, while mul-tilingual language models produce strong resultsacross many languages, multilingual pre-trainingwork almost exclusively focuses on pre-traininga small number of models with some fixed dis-tribution over languages (e.g. mBERT, XLM-R,XGLM, and BLOOM; Devlin et al., 2019; Con-neau et al., 2020a; Blevins et al., 2022; Lin et al.,2022; Scao et al., 2022). This distribution overlanguages typically favors high-resource languagesspoken in regions with high economic influence(Bender, 2011; Joshi et al., 2020).Thus, it is largely unknown how different pre-training language distributions, such as differentquantities of multilingual data or different selec-tions of languages, affect multilingual languagemodel performance in different languages. Multi-lingual models have been studied extensively dur-ing inference and fine-tuning (Pires et al., 2019;Conneau et al., 2020b; Karthikeyan et al., 2020;Winata et al., 2021; Chai et al., 2022; Alabi et al.,2022; Guarasci et al., 2022; Winata et al., 2022;Wu et al., 2022; Eronen et al., 2023), but these stud-ies generally rely on the same sets of pre-trainedmodels. For pre-training, there is mixed evidencefor the benefits of multilingual vs. monolingualdata (Conneau et al., 2020a; Wu and Dredze, 2020;Pyysalo et al., 2021; 2). As multilingual languagemodels are increasingly used without task-specificfine-tuning (e.g. for text generation; Scao et al.,2022; Lin et al., 2022),1 it is critical to understandhow multilingual pre-training affects raw languagemodeling performance in individual languages.In our work, we investigate the effects of dif-ferent multilingual pre-training distributions on",
  "The multilingual text generation capabilities of recentcommercial models also indicate likely multilingual pre-training (OpenAI, 2023; Google DeepMind, 2023)": "None10M100M1B Added mutlilingual tokens -50% +0% +50% Equivalent monolingual dataset increase Language categoryLow-resource (1M)Med-low resource (10M)Med-high resource (100M)High-resource (1B) : Left: Map of the 252 languages used in our study. Right: Effects of adding multilingual pre-training datain similar languages, for low-resource (1M token) through high-resource (1B token) languages in small models.Effects are quantified using the estimated monolingual dataset size that would achieve similar performance. Adding1B tokens of multilingual data is similar to adding 22% (low-resource) or removing 63% (high-resource) of themonolingual dataset. Shaded regions are 99% confidence intervals for the mean across languages.",
  "language modeling performance in 252 languages.Our main contributions are:2": "We pre-train over 1900 monolingual baselinemodels for 252 languages, and we estimate lossin each language based on monolingual datasetsize (4). We use these estimates to quantifymultilingual model performance in individuallanguages (4.3). We pre-train over 8400 multilingual languagemodels, and we evaluate how performance in in-dividual languages varies as a function of mono-lingual dataset size, multilingual dataset size,linguistic similarity of the training languages,and model size (up to 45M parameters; 5). We find that moderate amounts of multilin-gual data improve performance for low-resourcelanguages, similar to increasing low-resourcedataset sizes by up to 33% (6.1). These im-provements depend primarily on the syntacticsimilarity of the added multilingual data, withmarginal additional effects of lexical (vocabu-lary) similarity. We find that multilingual data consistently hurtshigh-resource language performance, similarto reducing dataset sizes by over 85% in somecases (6.2). Likely due to limited model ca-pacity, as dataset sizes increase, adding multi-lingual data begins to hurt performance for bothlow-resource and high-resource languages (thecurse of multilinguality; 2).",
  "Codeisavailableat:": "cases where both (1) the model targets perfor-mance in low-resource languages and (2) the modelhas enough capacity for the added multilingualdata. If these assumptions hold, the multilingualdata should be from languages that are linguisti-cally similar to the target low-resource languages.However, when optimizing performance for high-resource languages, multilingual models are likelyto degrade performance in individual languages.",
  "Related Work": "The curse of multilinguality.To extend lan-guage models to low-resource languages, re-searchers often train a single model on a large num-ber of languages, including low-resource languages(Devlin et al., 2019; Conneau et al., 2020a; Linet al., 2022; Scao et al., 2022; Imani et al., 2023).Oftentimes, better performance is observed whenlanguages are either closely related or focused in aspecific region (Kakwani et al., 2020; Ogueji et al.,2021; Ogunremi et al., 2023). However, Conneauet al. (2020a) find that pre-training on an excessivenumber of languages hurts model performance ineach language, evaluating five subsets of languageson downstream tasks in 16 languages. This phe-nomenon is known as the curse of multilinguality ornegative interference (Wang et al., 2020). Indeed,monolingual language models often have better lan-guage modeling performance than massively mul-tilingual models (Pyysalo et al., 2021). However,Rust et al. (2021) find that the curse of multilin-guality may simply be a result of lower qualitytokenization per language. Further contradictingthe curse of multilinguality, Wu and Dredze (2020)find that for low-resource languages, multilingualpre-training does improve downstream task perfor- mance relative to monolingual pre-training, andFujinuma et al. (2022) observe better cross-lingualtransfer performance when a wider variety of lan-guages is seen during during pre-training. Thus, theprecise effects of multilinguality on low-resourceand high-resource languages remains unclear.To isolate these effects, we evaluate languagemodeling performance in 252 languages while sys-tematically varying monolingual dataset size, mul-tilingual dataset size, model size, and linguisticsimilarity of added languages during pre-training.This contrasts with previous studies that focus onindividual (massively) multilingual models suchas mBERT or XLM-R. Our approach allows us todetermine how such models perform after varyingpre-training languages and language distributions.",
  "Collecting a Multilingual Dataset": "Conducting controlled multilingual language mod-eling experiments requires a large multilingualdataset. Notably, broad language coverage is aconsistent issue in NLP research (Bender, 2009,2011; Joshi et al., 2020; Blasi et al., 2022). Here,we collect text corpora from 24 multilingual datasources such as OSCAR (Ortiz Surez et al., 2019;Abadji et al., 2021), Wikipedia (Wikipedia, 2023),and NLLB (Costa-juss et al., 2022). Our sourcesare reported in A. We merge the corpora per lan-guage, and we deduplicate repeated sequences of100 UTF-8 bytes (Lee et al., 2022). Restrictingeach language to a maximum of 1B tokens, ourdataset contains 41.4B tokens in 1572 languages.This includes 252 languages with the required 1.5Mtokens for our language modeling study. Despitethis fairly stringent token requirement, our 252languages cover five continents, 29 language fami-lies, and 30 scripts (i.e. writing systems). shows a geographic map of our 252 languages,using coordinates from Glottolog (Hammarstrmet al., 2023). Our list of languages with correspond-ing token counts is reported in G.",
  "Monolingual Baselines and Metrics": "To study effects of multilinguality on languagemodeling performance in individual languages, wefirst need a method to quantify performance inthose languages. Thus, we pre-train monolingualbaseline models for our 252 languages, to use ascomparison points for multilingual models. Foreach language L, we estimate the number of mono-lingual tokens in L required to achieve a given",
  "Model Architectures and Pre-Training": "We pre-train autoregressive GPT-2 Transformer lan-guage models from scratch (Radford et al., 2019)with three sizes from Turc et al. (2019): tiny (4.6Mparameters), mini (11.6M parameters), and small(29.5M parameters). For each language, we pre-train models with four dataset sizes when available:1M, 10M, 100M, and 1B tokens, not including500K tokens for evaluation in each case. We callthese dataset sizes low, med-low, med-high, andhigh resource respectively. We have 252 languageswith at least the low-resource dataset size, 167 withmed-low resource, 48 with med-high resource, and28 with high-resource. Resource categories for all252 languages are included in G. Hyperparameterdetails are reported in C. Monolingual tokenizers.We train a monolin-gual SentencePiece tokenizer with maximum vo-cabulary size 32K for each of our 252 languages(Kudo and Richardson, 2018), and we fix this tok-enizer for all models pre-trained for that language.We train each tokenizer on 10K randomly-sampledlines of text in the language; for languages wheremore lines are available, the 10K-line tokenizershave reasonable vocabulary overlap with tokenizerstrained on more lines (B). For example, a 10K-line tokenizer on average covers 93.7% of the 4Kmost frequent tokens in the vocabulary of a 10M-line tokenizer. We restrict tokenizer training to 10Klines for all languages to control for tokenizationquality across languages.",
  "Perplexity and Log-Likelihood": "As an initial performance metric, we compute thelog-likelihood assigned by a language model M tothe unseen evaluation dataset for language L. Eachof our monolingual models is evaluated on its corre-sponding pre-training language, but these methodsalso apply to our multilingual models (which eachhave a tokenizer fixed for one target language; 5).Averaging over tokens, evaluation log-likelihoodis equivalent to negative log-perplexity, mean to-ken log-probability, or the negative of the languagemodels cross-entropy loss (Equation 1). Becauseour tokenization is fixed across all models witha given target language, log-likelihoods are com- parable within each target language. Higher log-likelihood scores indicate better language modelingperformance, they are predictive of model perfor-mance on other natural language tasks (Xia et al.,2023), and they can be computed even for lan-guages without any labeled data.Although log-likelihood scores are comparablefor models with the same target language, they varysubstantially across languages. This can be dueto features of individual languages, their datasets,or their tokenization (Gerz et al., 2018). Thus,when model M is pre-trained on language L, wesubtract the log-likelihood score of the baseline tinymonolingual model (M0) trained on 1M tokens forthat language, obtaining a relative log-likelihoodas follows:",
  "meanwlog2 PM(w) meanwlog2 PM0(w)": "(1)Here, w are tokens in the evaluation dataset for L.As is standard, token probabilities are produced bythe language models M and M0 based on preced-ing context (Brown et al., 2020). Equation 1 is thenequivalent to the log-odds of observing the evalua-tion dataset for L using the model M versus M0.A relative log-likelihood of indicates that M as-signs the evaluation dataset 2 times the likelihoodassigned by M0. Equivalently, M has perplex-ity 2 times lower than M0. In future sections,log-likelihoods refer to relative log-likelihoods thataccount for the target language baseline.",
  "Estimating Monolingual Token Counts": "Relative log-likelihoods are difficult to interpretwhen quantifying language model performancein practice; a log-likelihood change of 1.0 doesnot have concrete practical implications.Log-likelihoods are also difficult to compare acrossmodel sizes (D). Therefore, when evaluating mul-tilingual models in later sections, we quantify per-formance in a language L as the estimated numberof monolingual tokens in L that would achievethe same log-likelihood with the same size model.Measuring model performance in terms of esti-mated monolingual token counts allows us to quan-tify the effects of adding multilingual pre-trainingdata across languages and model sizes.Estimating monolingual token counts for modelsacross 252 languages is nontrivial. Previous workhas found that language modeling loss (negativelog-likelihood) has a power law relationship withdataset size (Kaplan et al., 2020). Indeed, we find 1M10M100M1B Monolingual data (tokens) Eval relative log-likelihood tinyminismall 1M10M100M1B Monolingual data (tokens) Eval relative log-likelihood ChineseArabicSpanishEnglish : Curves predicting monolingual model per-formance from dataset size. Top: Curves fitted to alllanguages for each model size. Bold lines are fittedcurves, and lighter lines are ground truth curves for in-dividual languages. Bottom: Sample language-specificcurves for small models, extrapolating from only twodata points (1M and 10M tokens). This still producesreasonable estimates for 100M and 1B tokens. Boldlines are estimated curves, and dashed lines are groundtruth values. that axb + c provides a good fit on average torelative log-likelihood in all 252 languages, wherex is the monolingual dataset size in log10 tokens(, top). However, there is significant vari-ability in the log-likelihood vs. dataset size curveacross languages. For high-resource languages, wecan fit a language-specific power law to the datapoints for 1M, 10M, 100M, and 1B tokens. Forlower-resource languages, there are too few datapoints to fit the power law from scratch (e.g. threepower law parameters with two data points). Forthese languages, we fix a as the median parametervalue from languages where the curve can be fit.Using this, we fit a monolingual log-likelihood vs.token count curve for each language in each modelsize (, bottom; details in D).These curves produce reasonable estimates forthe number of monolingual tokens required toachieve a given level of performance in a languageL (D). Even when token estimation accuracy isimperfect, our estimated monolingual token countis always a monotonic increasing function of evallog-likelihood, and thus performance rankings be- tween models are preserved. In future sections, wemeasure the performance of a multilingual modelwith target language L in terms of the estimatednumber of monolingual pre-training tokens in Lthat would achieve the same performance.",
  "Pre-Training Multilingual Models": "Finally, we pre-train multilingual language modelsthat vary along four dimensions: monolingual dataquantity, added multilingual data quantity, modelsize, and linguistic similarity of the added lan-guages. Each multilingual model is pre-trainedwith a specified target language, keeping mono-lingual tokenization for that language fixed dur-ing both pre-training and evaluation. The multi-lingual models are pre-trained identically to themonolingual baselines in 4, except with addedmultilingual data (10M, 100M, or 1B tokens). Themultilingual data is randomly interspersed withthe monolingual pre-training data in the target lan-guage. Target language evaluation loss curves areincluded in C. In total, we pre-train 8454 multi-lingual language models ranging from 8M to 45Mparameters. Multilingual tokenizers.Perplexity and log-likelihood evaluations within a language L areonly comparable when they use the same tokenizer.Thus, we must keep the monolingual tokenizerfixed for any model evaluated on L. However, fix-ing tokenization for multiple languages simultane-ously results in intractable vocabulary sizes. Forexample, 252 languages 32K tokens would re-sult in a vocabulary size of 8.1M tokens, requiring1.0B embedding parameters even with our smallestembedding size of 128. To avoid intractable pa-rameter counts, we pre-train multilingual languagemodels that each keep tokenization fixed for onlyone language, which we call the target languagefor that model. In each multilingual model, the non-target languages share a multilingual tokenizer withvocabulary size 32K, trained on 10K randomly-sampled lines from each added language. The tar-get language and added multilingual datasets aretokenized separately, and the token IDs are mergedfor the shared vocabulary items. This merged tok-enization process ensures that the target languagetokenization remains unchanged across models.",
  "Linguistic similarity.Motivated by work demon-strating the importance of linguistic similarity forcrosslingual transfer performance (Pires et al.,": "2019; Conneau et al., 2020b; Gerz et al., 2018;Winata et al., 2022; Fujinuma et al., 2022; Ahujaet al., 2022; Imani et al., 2023; Oladipo et al., 2022;Eronen et al., 2023), we select added languages formultilingual data based on their similarity to eachtarget language. Due to limits on computational re-sources, we only consider two linguistic similaritylevels: similar and dissimilar languages.Our linguistic similarity metric is based on threefeatures: syntactic similarity, geographic proxim-ity, and lexical similarity (i.e. tokenizer vocabu-lary overlap). Syntactic and geographic metricsare computed as cosine similarities between lan-guages syntactic and geographic vector represen-tations from lang2vec (Littell et al., 2017), whichpulls from the World Atlas of Language Structures(Dryer and Haspelmath, 2013). Lexical similarityis computed as the log number of shared tokensin the monolingual tokenizers for two languages(4.1). We Z-score normalize the syntactic, geo-graphic, and lexical similarity metrics over all lan-guage pairs, and we define the linguistic similaritybetween any two languages as the mean of the threesimilarity scores. For example, the four most simi-lar languages to English are Dutch, Swedish, Nor-wegian, and German. The four most dissimilar lan-guages to English are Nepali, Japanese, Tamil, andKorean. To allow us to vary the multilingual dataquantity without changing the added languages, werestrict our added languages to those with at least100M tokens in our dataset (i.e. our 48 med-highresource languages).",
  "Linguistic similarity. When adding multilin-gual data for each target language, we selecteither the 10 most or 10 least similar languages": "None10M100M1B Added mutlilingual tokens 0.8M 1.0M 1.2M 1.4M 1.6M 1.8M Estimated monolingual tokens Low-resource, tiny model similar languagesdissimilar languages None10M100M1B Added mutlilingual tokens 0.8M 1.0M 1.2M 1.4M 1.6M 1.8M Low-resource, mini model None10M100M1B Added mutlilingual tokens 0.8M 1.0M 1.2M 1.4M 1.6M 1.8M Low-resource, small model None10M100M1B Added mutlilingual tokens 4M 6M 8M 10M 12M 14M Estimated monolingual tokens Med-low resource, tiny model similar languagesdissimilar languages None10M100M1B Added mutlilingual tokens 4M 6M 8M 10M 12M 14M Med-low resource, mini model None10M100M1B Added mutlilingual tokens 4M 6M 8M 10M 12M 14M Med-low resource, small model : Results for low and med-low resource scenarios. Higher y-axis values indicate better performance. Forexample, a small model with 1M monolingual tokens (top right) and 1B added tokens of multilingual data insimilar languages has similar performance to 1.2M monolingual tokens alone. Light-colored lines indicate resultsfor individual languages, and bold lines indicate the mean across languages. Shaded regions are 95% confidenceintervals for the mean.",
  "Multilingual Model Results": "To reflect low-resource to high-resource languagescenarios, we primarily separate results based onmonolingual data quantity (low and med-low re-source in 6.1, high and med-high resource in 6.2).In each scenario, we consider the effects of addingdifferent multilingual data quantities with differ-ent levels of linguistic similarity, across all threemodel sizes. Overall, we find that performancein low-resource languages improves when we addmoderate amounts of multilingual data (6.1). Theamount of improvement depends on the syntacticsimilarity of the added languages, with small ad-ditional effects of lexical (vocabulary) similarity.High-resource language performance consistentlydegrades when we add multilingual data (6.2).Larger models have smaller degradations for high-resource languages and larger improvements forlow-resource languages in multilingual scenarios,suggesting that many drawbacks of multilingualityare due to limited model capacity.",
  "In moderation, multilinguality improves low-resource performance.As shown in": "(top), low-resource languages exhibit performanceimprovements when adding 100M or 1B tokensof multilingual data (p < 0.001 for 11 out of12 comparisons, using paired sample t-tests; E).Performance improvements are significantly largerwhen the added languages are similar vs. dissim-ilar to the target language (analogous to an av-erage 33% vs. 22% increase in target languagedataset size for small models in the optimal sce-nario; p < 0.001). Performance improvements arealso larger for larger model sizes (33% vs. 12%equivalent dataset increases for small vs. tiny mod-els; p < 0.001). Regardless of model size, per-formance is essentially unaffected when addingonly 10M multilingual tokens (1M tokens in eachadded language); this result also holds for med-lowresource scenarios (, bottom). This sug-gests that a nontrivial amount of multilingual datais required for language models to leverage sharedcharacteristics across languages. However, the benefits of adding more multi-lingual data quickly plateau in low-resource sce-narios (e.g. adding 100M vs. 1B multilingualtokens). In med-low resource scenarios (, bottom), adding multilingual data hurts perfor-mance (p < 0.001 adding 1B multilingual tokens) Syntactic similarity of added languages 0.2 0.4 0.6 0.8 1.0 Eval relative log-likelihood Similar selectionDissimilar selection Lexical similarity of added languages 0.2 0.4 0.6 0.8 1.0 Eval relative log-likelihood Similar selectionDissimilar selection 0.051 0.021 0.052 0.001 0.071 0.066",
  "Geographic": ": Variance partitioning into syntactic, geographic, and lexical similarity of the added languages whenpredicting a models performance (relative log-likelihood score) for tiny (left), mini (center), and small (right)models with 100M tokens of added multilingual data. Syntactic similarity of added languages 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Eval relative log-likelihood Similar selectionDissimilar selection Geographic similarity of added languages 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Eval relative log-likelihood Similar selectionDissimilar selection Lexical similarity of added languages 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Eval relative log-likelihood Similar selectionDissimilar selection Syntactic similarity of added languages 0.1 0.0 0.1 0.2 0.3 0.4 Eval relative log-likelihood Similar selectionDissimilar selection Geographic similarity of added languages 0.1 0.0 0.1 0.2 0.3 0.4 Eval relative log-likelihood Similar selectionDissimilar selection Lexical similarity of added languages 0.1 0.0 0.1 0.2 0.3 0.4 Eval relative log-likelihood Similar selectionDissimilar selection : Top: Correlations between syntactic (r = 0.471), geographic (r = 0.305), and lexical (r = 0.306)similarity of added languages and target language performance for mini models, as described in 6.1. Bottom:Correlations between syntactic (r = 0.430), geographic (r = 0.345), and lexical (r = 0.233) similarity of addedlanguages and target language performance for tiny models. distinguish between the same language in multiplescripts (e.g. Serbian in Cyrillic vs. Latin script) andmacrolanguages vs. their individual constituent lan-guages (e.g. Quechua vs. Cusco Quechua and Ay-acucho Quechua). The full list of 1572 languages inour dataset can be found at",
  "High-Resource Language Results": "Multilinguality hurts high-resource perfor-mance.For all model sizes, multilinguality hurtslanguage model performance in med-high and highresource languages (; p < 0.001 in allscenarios adding 1B tokens, using paired samplet-tests; E). For high-resource languages in ourlargest model size, adding 1B multilingual tokensis similar to removing 63% of the target languagedataset. Degradations are larger when more mul-tilingual tokens are added. Degradations are alsolarger for smaller models (88% vs. 63% equiva-lent dataset decrease in the target language for tinyvs. small models; p < 0.001). This suggests thatdegradations are likely driven by language modelsreaching capacity limits. Interestingly, degrada-tions are slightly larger given more similar addedlanguages to the target language (all scenarios in; p < 0.05 in 7 out of 12 scenarios). Thisindicates that although more similar languages tendto improve low-resource language performance(6.1), they surprisingly tend to hurt high-resourcelanguage performance more. One possible expla-nation is that more similar languages simply havelarger effects on target language predictions. In None100M1B Added mutlilingual tokens 20M 40M 60M 80M 100M 120M Estimated monolingual tokens Med-high resource, tiny model similar languagesdissimilar languages None100M1B Added mutlilingual tokens 20M 40M 60M 80M 100M 120M Med-high resource, mini model None100M1B Added mutlilingual tokens 20M 40M 60M 80M 100M 120M Med-high resource, small model None100M1B Added mutlilingual tokens 0.0 0.2B 0.4B 0.6B 0.8B 1.0B 1.2B Estimated monolingual tokens High-resource, tiny model similar languagesdissimilar languages None100M1B Added mutlilingual tokens 0.0 0.2B 0.4B 0.6B 0.8B 1.0B 1.2B High-resource, mini model None100M1B Added mutlilingual tokens 0.0 0.2B 0.4B 0.6B 0.8B 1.0B 1.2B High-resource, small model : Results for med-high and high resource scenarios, using the same format as the low-resource scenariosin . For example, adding 1B tokens of multilingual data to a small model with 1B monolingual tokens(high-resource; bottom right) is similar to removing over 600M tokens of the monolingual dataset. low-resource scenarios, these influences from otherlanguages improve predictions; however, in high-resource scenarios, when models are learning morefine-grained language-specific nuances, these in-fluences might hurt performance, making similarlanguages hurt performance more.",
  "Discussion": "Our results demonstrate that for low-resource lan-guages, multilingual language models yield somebenefits. In the optimal case from our study, thebenefits are similar to increasing the low-resourcedataset size by about 33% (6.1). Hence, in scenar-ios where collecting additional data is difficult (e.g.languages spoken in remote geographic locationsor with few speakers), pre-training multilingualmodels may be a worthwhile endeavor. In thesecases, the models should be pre-trained with mul-tilingual data from maximally similar languages,and it should be ensured that the models have ca-pacity for the added multilingual data along withthe target language data. However, in other cases, itmay be more practical to find or collect more datain the target language itself (e.g. if collecting 50%more target language data is feasible).For high-resource languages, multilingual lan-guage models yield worse performance than thecomparable monolingual model in essentially allcases. Degradations can be similar to reducinghigh-resource dataset sizes by over 85% (6.2).These degradations can be mitigated by pre- training larger models, which also appear to max-imize benefits for low-resource languages. How-ever, when pre-training language models even onthe order of tens of high-resource languages (Con-neau et al., 2020a; Scao et al., 2022; Lin et al.,2022), a model sufficiently large to accommodateall of the languages data without hitting capac-ity limitations would likely be impractically large.Even if existing language models are severelyover-parameterized, there is evidence that 70B-parameter models are required just for English(Hoffmann et al., 2022). If only considering indi-vidual language performance, pre-training targetedlanguage-specific models is likely to be far more ef-ficient than a single massively multilingual model.",
  "Conclusion": "Our work systematically evaluates the effects ofmultilingual pre-training on language modelingperformance in 252 languages. We pre-train over10,000 monolingual and multilingual languagemodels, varying monolingual dataset sizes, mul-tilingual dataset sizes, linguistic similarity of themultilingual data, and model sizes. We find thatadding multilingual data in similar languages im-proves performance for low-resource languages,but improvements decrease as models reach capac-ity limitations. Multilingual data consistently hurtshigh-resource language performance. We quantifyboth of these effects in terms of comparable mono-lingual dataset sizes. Our results suggest that while",
  "Limitations": "Model size.We only pre-train language modelsup to 45M parameters. Larger models are lesslikely to hit the capacity limitations that appear todrive the curse of multilinguality, but we selectour model sizes as a compromise between infor-mativity of results and computational cost. Whenpre-training thousands of models for controlledexperiments, larger models may not be worth ad-ditional computational and environmental costs ifresults can reasonably be extrapolated to largermodels (Strubell et al., 2019). In our experiments,directions of effect are consistent across all threemodel sizes we evaluate.In fact, for low-resource scenarios, smallermodels can achieve similar performance to largermodels () while remaining accessible tocommunities with fewer computational resources.This makes small models useful for efficient low-resource language technologies and low-computesettings such as laptops and mobile phones. Pre-training smaller models in our experiments also al-lows us to include a much larger and more typolog-ically diverse set of languages in our study, makingour results more representative of human languagesoverall and more likely to generalize to languagesnot included in our study. Our results are much lesslikely to be skewed by over-representation of thesmall number of languages that dominate the fieldof NLP (Joshi et al., 2020; Blasi et al., 2022). Language coverage.While we have included farmore low-resource languages than the vast majorityof recent studies in NLP, we do not have coverageof some regions and language families. For ex-ample, our study does not include any languagesindigenous to modern-day Australia or many fromthe Americas. This imperfect coverage may skewour results towards languages that have larger textcorpora available on the Internet. Specifically, asdiscussed in 5, because we restrict added multilin-gual data to our 48 med-high resource languages (toallow us to vary multilingual dataset sizes), our low-resource target languages are less likely to havehighly similar languages in the multilingual pre-training scenarios. Allowing added multilingualdata from our low and med-low resource languages would increase the mean similarity of added similarlanguages in 6.1, so we would expect to see largerperformance improvements for low-resource lan-guages in these cases (i.e. the observed equivalent33% dataset increase for low-resource languageswould likely be greater); this can be tested em-pirically in future work. Our work demonstratesthat low-resource performance improvements canbe predicted by the syntactic similarity of addedlanguages (moreso than lexical overlap; 6.1), butfuture research might investigate more specific syn-tactic and semantic features that result in highcrosslingual transfer.Of course, as with all multilingual datasets, itis likely that there are still some language label-ing mismatches and contaminated examples in ourdataset. We also note that the delineations defin-ing languages versus different dialects of the samelanguage are inherently fuzzy. For example, North-ern Frisian (frr), Eastern Frisian (frs), and WesternFrisian (fry) are considered individual languageswith separate codes; conversely, Tamil (tam) is con-sidered an individual language (one language code)with at least 18 dialects (Hammarstrm et al., 2023).We defer to the ISO 639-3 language code system,as it is the most widely used system of its type. Measuring performance.Finally, our results ap-ply primarily to language modeling performance.Effects of multilingual pre-training may be differ-ent for specific downstream tasks (e.g. reason-ing tasks or machine translation; Bandarkar et al.,2023; Costa-juss et al., 2022) or for cross-lingualtransfer learning using fine-tuning (Fujinuma et al.,2022). Unfortunately, few existing multilingualbenchmarks cover the wide variety of languagesused in our study. There are several evaluationsused in Imani et al. (2023); however, with the ex-ception of perplexity, all of the Glot500 evals aredesigned primarily for bidirectional models, or theyevaluate crosslingual performance rather than a sin-gle target language: sentence retrieval, Bible textclassification, NER, POS tagging, and roundtripalignment. Bidirectional (encoder) models remainquite useful for representation learning (e.g. sen-tence representation and classification tasks; Ban-darkar et al., 2023; Imani et al., 2023; Conneauet al., 2020a), but the majority of recent languagemodel training efforts have focused on autoregres-sive models (e.g. XGLM and BLOOM, along withmultilingual capabilities of GPT-4, Claude, Gem-ini, etc). To best align our work with current pre- training efforts, we focus on autoregressive models.Of the datasets that do exist for low-resource lan-guage evaluation, Belebele is a massively multilin-gual reading comprehension dataset, which coversonly 122 language variants (Bandarkar et al., 2023),and the XTREME benchmark covers only 40 lan-guages (Hu et al., 2020), all of which are at leastmedium-low resource (i.e. not low-resource) inour study. We use evaluation log-likelihoods (neg-ative log-perplexities) to measure language mod-eling performance in our experiments in order toevaluate all the languages in our sample with thesame metric. Evaluation log-likelihoods requireno annotated data in the target language, they arepredictive of language model behavior on a vari-ety of tasks (Xia et al., 2023), and they have beenused to quantify language model quality in previ-ous work (Kaplan et al., 2020; Hoffmann et al.,2022). As multilingual language models are in-creasingly used without fine-tuning for raw textgeneration (e.g. Scao et al., 2022; Lin et al., 2022;OpenAI, 2023; Google DeepMind, 2023), raw lan-guage modeling performance across languages isincreasingly important to evaluate. We would like to thank the UCSD Language andCognition Lab for valuable discussion.Somemodels were trained on hardware provided by theNVIDIA Corporation as part of an NVIDIA Aca-demic Hardware Grant. Some models were alsotrained on the UCSD Social Sciences Research andDevelopment Environment (SSRDE). Zhuowen Tuis supported by NSF IIS-2127544. Tyler Chang ispartially supported by the UCSD HDSI graduatefellowship. Julien Abadji, Pedro Javier Ortiz Surez, Laurent Ro-mary, and Benot Sagot. 2021. Ungoliant: An op-timized pipeline for the generation of a very large-scale multilingual web corpus. In Proceedings ofthe Workshop on Challenges in the Management ofLarge Corpora (CMLC-9) 2021. Limerick, 12 July2021 (Online-Event), pages 1 9. David Ifeoluwa Adelani, Jade Abbott, Graham Neu-big, Daniel Dsouza, Julia Kreutzer, Constantine Lig-nos, Chester Palen-Michel, Happy Buzaaba, ShrutiRijhwani, Sebastian Ruder, Stephen Mayhew, Is-rael Abebe Azime, Shamsuddeen H. Muhammad,Chris Chinenye Emezue, Joyce Nakatumba-Nabende,Perez Ogayo, Aremu Anuoluwapo, Catherine Gitau, Derguene Mbaye, Jesujoba Alabi, Seid Muhie Yi-mam, Tajuddeen Rabiu Gwadabe, Ignatius Ezeani,Rubungo Andre Niyongabo, Jonathan Mukiibi, Ver-rah Otiende, Iroro Orife, Davis David, Samba Ngom,Tosin Adewumi, Paul Rayson, Mofetoluwa Adeyemi,Gerald Muriuki, Emmanuel Anebi, Chiamaka Chuk-wuneke, Nkiruka Odu, Eric Peter Wairagala, SamuelOyerinde, Clemencia Siro, Tobius Saul Bateesa,Temilola Oloyede, Yvonne Wambui, Victor Akin-ode, Deborah Nabagereka, Maurice Katusiime, Ayo-dele Awokoya, Mouhamadane MBOUP, Dibora Ge-breyohannes, Henok Tilaye, Kelechi Nwaike, De-gaga Wolde, Abdoulaye Faye, Blessing Sibanda, Ore-vaoghene Ahia, Bonaventure F. P. Dossou, KelechiOgueji, Thierno Ibrahima DIOP, Abdoulaye Diallo,Adewale Akinfaderin, Tendai Marengereke, and Sa-lomey Osei. 2021.MasakhaNER: Named entityrecognition for African languages.Transactionsof the Association for Computational Linguistics,9:11161131. Kabir Ahuja, Sunayana Sitaram, Sandipan Dandapat,and Monojit Choudhury. 2022. On the calibration ofmassively multilingual language models. In Proceed-ings of the 2022 Conference on Empirical Methodsin Natural Language Processing, pages 43104323.Association for Computational Linguistics. Jesujoba O. Alabi, David Ifeoluwa Adelani, MariusMosbach, and Dietrich Klakow. 2022. Adapting pre-trained language models to African languages viamultilingual adaptive fine-tuning. In Proceedingsof the 29th International Conference on Computa-tional Linguistics, pages 43364349. InternationalCommittee on Computational Linguistics. Lucas Bandarkar, Davis Liang, Benjamin Muller, MikelArtetxe, Satya Narayan Shukla, Donald Husa, NamanGoyal, Abhinandan Krishnan, Luke Zettlemoyer, andMadian Khabsa. 2023. The Belebele benchmark: Aparallel reading comprehension dataset in 122 lan-guage variants. arXiv. Emily M Bender. 2009. Linguistically nave != lan-guage independent: Why NLP needs linguistic typol-ogy. In Proceedings of the EACL 2009 Workshopon the Interaction between Linguistics and Compu-tational Linguistics: Virtuous, Vicious or Vacuous?,pages 2632.",
  "Daniel Borcard, Pierre Legendre, and Pierre Drapeau.1992. Partialling out the spatial component of eco-logical variation. Ecology, 73(3):10451055": "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020.Language models are few-shot learners.In Ad-vances in Neural Information Processing Systems,volume 33, pages 18771901.",
  "CMU. 2010. Haitian Creole language data": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal,Vishrav Chaudhary, Guillaume Wenzek, FranciscoGuzmn, Edouard Grave, Myle Ott, Luke Zettle-moyer, and Veselin Stoyanov. 2020a. Unsupervisedcross-lingual representation learning at scale. In Pro-ceedings of the 58th Annual Meeting of the Associa-tion for Computational Linguistics, pages 84408451.Association for Computational Linguistics. Alexis Conneau, Shijie Wu, Haoran Li, Luke Zettle-moyer, and Veselin Stoyanov. 2020b.Emergingcross-lingual structure in pretrained language mod-els. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics, pages60226034. Association for Computational Linguis-tics. Marta R. Costa-juss, James Cross, Onur elebi,Maha Elbayad, Kenneth Heafield, Kevin Heffer-nan, Elahe Kalbassi, Janice Lam, Daniel Licht,Jean Maillard, Anna Sun, Skyler Wang, GuillaumeWenzek, Al Youngblood, Bapi Akula, Loic Bar-rault, Gabriel Mejia Gonzalez, Prangthip Hansanti,John Hoffman, Semarley Jarrett, Kaushik RamSadagopan, Dirk Rowe, Shannon Spruit, ChauTran, Pierre Andrews, Necip Fazil Ayan, ShrutiBhosale, Sergey Edunov, Angela Fan, CynthiaGao, Vedanuj Goswami, Francisco Guzmn, PhilippKoehn, Alexandre Mourachko, Christophe Ropers,Safiyyah Saleem, Holger Schwenk, and Jeff Wang.2022.No language left behind: Scaling human-centered machine translation. arXiv. Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional Transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186. Association for Computational Linguis-tics.",
  "eBible. 2023. eBible": "Abteen Ebrahimi, Manuel Mager, Arturo Oncevay,Vishrav Chaudhary, Luis Chiruzzo, Angela Fan, JohnOrtega, Ricardo Ramos, Annette Rios, Ivan VladimirMeza Ruiz, Gustavo Gimnez-Lugo, ElisabethMager, Graham Neubig, Alexis Palmer, RolandoCoto-Solano, Thang Vu, and Katharina Kann. 2022.AmericasNLI: Evaluating zero-shot natural languageunderstanding of pretrained multilingual models intruly low-resource languages. In Proceedings of the60th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages62796299. Association for Computational Linguis-tics.",
  "corpus building and evaluation. International Jour-nal of Engineering Research and Technology (IJERT),10": "Daniela Gerz, Ivan Vulic, Edoardo Maria Ponti, RoiReichart, and Anna Korhonen. 2018. On the relationbetween linguistic typology and (limitations of) mul-tilingual language modeling. In Proceedings of the2018 Conference on Empirical Methods in NaturalLanguage Processing, pages 316327. Associationfor Computational Linguistics. Dirk Goldhahn, Thomas Eckart, and Uwe Quasthoff.2012. Building large monolingual dictionaries at theLeipzig corpora collection: From 100 to 200 lan-guages. In Proceedings of the Eighth InternationalConference on Language Resources and Evaluation(LREC12), pages 759765. European Language Re-sources Association (ELRA).",
  "Harald Hammarstrm, Robert Forkel, Martin Haspel-math, and Sebastian Bank. 2023.Glottolog 4.8.Max Planck Institute for Evolutionary Anthropology,Leipzig": "Viktor Hangya, Hossain Shaikh Saadi, and AlexanderFraser. 2022. Improving low-resource languages inpre-trained multilingual language models. In Pro-ceedings of the 2022 Conference on Empirical Meth-ods in Natural Language Processing, pages 1199312006. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,Elena Buchatskaya, Trevor Cai, Eliza Rutherford,Diego de las Casas, Lisa Anne Hendricks, JohannesWelbl, Aidan Clark, Tom Hennigan, Eric Noland,Katherine Millican, George van den Driessche, Bog-dan Damoc, Aurelia Guy, Simon Osindero, KarenSimonyan, Erich Elsen, Oriol Vinyals, Jack WilliamRae, and Laurent Sifre. 2022. Training compute-optimal large language models.In Advances inNeural Information Processing Systems, volume 35,pages 3001630030.",
  "Matthew Honnibal, Ines Montani, Sofie Van Lan-deghem, and Adriane Boyd. 2020. spaCy: Industrial-strength natural language processing in python.SpaCy": "Junjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-ham Neubig, Orhan Firat, and Melvin Johnson.2020. XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual gener-alisation. In International Conference on MachineLearning, pages 44114421. Ayyoob Imani, Peiqin Lin, Amir Hossein Kargaran,Silvia Severini, Masoud Jalili Sabet, Nora Kass-ner, Chunlan Ma, Helmut Schmid, Andr Martins,Franois Yvon, and Hinrich Schtze. 2023. Glot500:Scaling multilingual corpora and language models to500 languages. In Proceedings of the 61st AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 10821117.Association for Computational Linguistics. Eric Joanis, Rebecca Knowles, Roland Kuhn, SamuelLarkin, Patrick Littell, Chi-kiu Lo, Darlene Stewart,and Jeffrey Micher. 2020. The Nunavut HansardInuktitutEnglish parallel corpus 3.0 with prelimi-nary machine translation results. Pratik Joshi, Sebastin Santy, Amar Budhiraja, KalikaBali, and Monojit Choudhury. 2020. The state andfate of linguistic diversity and inclusion in the NLPworld. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics, pages62826293. Divyanshu Kakwani, Anoop Kunchukuttan, SatishGolla, Gokul N.C., Avik Bhattacharyya, Mitesh M.Khapra, and Pratyush Kumar. 2020. IndicNLPSuite:Monolingual corpora, evaluation benchmarks andpre-trained multilingual language models for Indianlanguages. In Findings of the Association for Com-putational Linguistics: EMNLP 2020, pages 49484961. Association for Computational Linguistics.",
  "Karthikeyan, Zihan Wang, Stephen Mayhew, and DanRoth. 2020. Cross-lingual ability of multilingualBERT: An empirical study. In International Confer-ence on Learning Representations": "Taku Kudo and John Richardson. 2018. SentencePiece:A simple and language independent subword tok-enizer and detokenizer for neural text processing. InProceedings of the 2018 Conference on EmpiricalMethods in Natural Language Processing: SystemDemonstrations, pages 6671. Association for Com-putational Linguistics. Katherine Lee, Daphne Ippolito, Andrew Nystrom,Chiyuan Zhang, Douglas Eck, Chris Callison-Burch,and Nicholas Carlini. 2022. Deduplicating trainingdata makes language models better. In Proceedingsof the 60th Annual Meeting of the Association forComputational Linguistics, pages 84248445. Asso-ciation for Computational Linguistics. Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, TianluWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-man Goyal, Shruti Bhosale, Jingfei Du, RamakanthPasunuru, Sam Shleifer, Punit Singh Koura, VishravChaudhary, Brian OHoro, Jeff Wang, Luke Zettle-moyer, Zornitsa Kozareva, Mona Diab, Veselin Stoy-anov, and Xian Li. 2022. Few-shot learning with",
  "multilingual generative language models. In Proceed-ings of the 2022 Conference on Empirical Methodsin Natural Language Processing, pages 90199052.Association for Computational Linguistics": "Patrick Littell, David R. Mortensen, Ke Lin, KatherineKairis, Carlisle Turner, and Lori Levin. 2017. URIELand lang2vec: Representing languages as typological,geographical, and phylogenetic vectors. In Proceed-ings of the 15th Conference of the European Chap-ter of the Association for Computational Linguistics:Volume 2, Short Papers, pages 814. Association forComputational Linguistics. Manuel Mager, Arturo Oncevay, Abteen Ebrahimi, JohnOrtega, Annette Rios, Angela Fan, Ximena Gutierrez-Vasques, Luis Chiruzzo, Gustavo Gimnez-Lugo, Ri-cardo Ramos, Ivan Vladimir Meza Ruiz, RolandoCoto-Solano, Alexis Palmer, Elisabeth Mager-Hois,Vishrav Chaudhary, Graham Neubig, Ngoc Thang Vu,and Katharina Kann. 2021. Findings of the Americ-asNLP 2021 shared task on open machine translationfor indigenous languages of the Americas. In Pro-ceedings of the First Workshop on Natural LanguageProcessing for Indigenous Languages of the Ameri-cas, pages 202217. Association for ComputationalLinguistics. Jonathan Mukiibi, Andrew Katumba, Joyce Nakatumba-Nabende, Ali Hussein, and Joshua Meyer. 2022. Themakerere radio speech corpus: A Luganda radio cor-pus for automatic speech recognition. In Proceedingsof the Thirteenth Language Resources and Evalua-tion Conference, pages 19451954. European Lan-guage Resources Association.",
  "Kelechi Ogueji, Yuxin Zhu, and Jimmy Lin. 2021": "Small data? no problem! exploring the viabilityof pretrained multilingual language models for low-resourced languages. In Proceedings of the 1st Work-shop on Multilingual Representation Learning, pages116126. Association for Computational Linguistics. Tolulope Ogunremi, Dan Jurafsky, and ChristopherManning. 2023. Mini but mighty: Efficient multi-lingual pretraining with linguistically-informed dataselection. In Findings of the Association for Compu-tational Linguistics: EACL 2023, pages 12511266. Akintunde Oladipo, Odunayo Ogundepo, KelechiOgueji, and Jimmy Lin. 2022. An exploration ofvocabulary size and transfer effects in multilinguallanguage models for African languages. In 3rd Work-shop on African Natural Language Processing.",
  "Alec Radford, Jeff Wu, Rewon Child, David Luan,Dario Amodei, and Ilya Sutskever. 2019. Languagemodels are unsupervised multitask learners. OpenAI": "Taraka Rama and Prasanth Kolachina. 2012. How goodare typological distances for determining genealogi-cal relationships among languages? In Proceedingsof COLING 2012, pages 975984. The COLING2012 Organizing Committee. Phillip Rust, Jonas Pfeiffer, Ivan Vulic, Sebastian Ruder,and Iryna Gurevych. 2021. How good is your to-kenizer? on the monolingual performance of mul-tilingual language models. In Proceedings of the59th Annual Meeting of the Association for Compu-tational Linguistics and the 11th International JointConference on Natural Language Processing (Vol-ume 1: Long Papers), pages 31183135. Associationfor Computational Linguistics. Teven Le Scao, Angela Fan, Christopher Akiki,Elizabeth-Jane Pavlick, Suzana Ilic, Daniel Hesslow,Roman Castagne, Alexandra Sasha Luccioni, Franc-cois Yvon, Matthias Gall, Jonathan Tow, Alexan-der M. Rush, Stella Rose Biderman, Albert Web-son, Pawan Sasanka Ammanamanchi, Thomas Wang,Benot Sagot, Niklas Muennighoff, Albert Villanovadel Moral, Olatunji Ruwase, et al. 2022. Bloom: A176b-parameter open-access multilingual languagemodel. arXiv. Emma Strubell, Ananya Ganesh, and Andrew McCal-lum. 2019. Energy and policy considerations fordeep learning in NLP. In Proceedings of the 57thAnnual Meeting of the Association for ComputationalLinguistics, pages 36453650. Association for Com-putational Linguistics.",
  "In Proceedings of the 60th Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 63546364. Association forComputational Linguistics": "Jrg Tiedemann. 2012. Parallel data, tools and inter-faces in OPUS. In Proceedings of the Eighth In-ternational Conference on Language Resources andEvaluation (LREC12), pages 22142218. EuropeanLanguage Resources Association (ELRA). Jrg Tiedemann. 2020. The Tatoeba Translation Chal-lenge Realistic Data Sets for Low Resource andMultilingual MT. In Proceedings of the Fifth Con-ference on Machine Translation, pages 11741182.Association for Computational Linguistics.",
  "Ulukau. 2023. Ulukau: The Hawaiian Electronic Li-brary": "Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, MattHaberland, Tyler Reddy, David Cournapeau, Ev-geni Burovski, Pearu Peterson, Warren Weckesser,Jonathan Bright, Stfan J. van der Walt, MatthewBrett, Joshua Wilson, K. Jarrod Millman, NikolayMayorov, Andrew R. J. Nelson, Eric Jones, RobertKern, Eric Larson, C J Carey, Ilhan Polat, Yu Feng,Eric W. Moore, Jake VanderPlas, Denis Laxalde,Josef Perktold, Robert Cimrman, Ian Henriksen, E. A.Quintero, Charles R. Harris, Anne M. Archibald, An-tnio H. Ribeiro, Fabian Pedregosa, Paul van Mul-bregt, and SciPy 1.0 Contributors. 2020. SciPy 1.0:Fundamental Algorithms for Scientific Computing inPython. Nature Methods, 17:261272. Zirui Wang, Zachary C. Lipton, and Yulia Tsvetkov.2020. On negative interference in multilingual mod-els: Findings and a meta-learning treatment.InProceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 44384450. Association for ComputationalLinguistics.",
  "Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawi-jaya, Rahmad Mahendra, Fajri Koto, Ade Romad-hony, Kemal Kurniawan, David Moeljadi, Radi-tyo Eko Prasojo, Pascale Fung, Timothy Baldwin,": "Jey Han Lau, Rico Sennrich, and Sebastian Ruder.2023. NusaX: Multilingual parallel sentiment datasetfor 10 Indonesian local languages. In Proceedingsof the 17th Conference of the European Chapter ofthe Association for Computational Linguistics, pages815834. Association for Computational Linguistics. Genta Indra Winata, Andrea Madotto, Zhaojiang Lin,Rosanne Liu, Jason Yosinski, and Pascale Fung. 2021.Language models are few-shot multilingual learners.In Proceedings of the 1st Workshop on MultilingualRepresentation Learning, pages 115. Associationfor Computational Linguistics. Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-icz, Joe Davison, Sam Shleifer, Patrick von Platen,Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,Teven Le Scao, Sylvain Gugger, Mariama Drame,Quentin Lhoest, and Alexander Rush. 2020. Trans-formers: State-of-the-art natural language processing.In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing: SystemDemonstrations, pages 3845. Association for Com-putational Linguistics. Shijie Wu and Mark Dredze. 2020. Are all languagescreated equal in multilingual BERT? In Proceedingsof the 5th Workshop on Representation Learning forNLP, pages 120130. Association for ComputationalLinguistics.",
  "Zhengxuan Wu, Isabel Papadimitriou, and Alex Tamkin.2022. Oolong: Investigating what makes crosslin-gual transfer hard with controlled studies. arXiv": "Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Vic-toria Lin, Ramakanth Pasunuru, Danqi Chen, LukeZettlemoyer, and Veselin Stoyanov. 2023. Trainingtrajectories of language models across scales. InProceedings of the 61st Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 1371113738. Association forComputational Linguistics. Lyudmila Zaydelman, Irina Krylova, and Boris Orekhov.2016. The technology of web-texts collection ofRussian minor languages. In Proceedings of the In-ternational Scientific Conference CPT2015, pages179181.",
  "Shiyue Zhang, Benjamin Frey, and Mohit Bansal. 2020": "ChrEn: Cherokee-English machine translation forendangered language revitalization. In Proceedingsof the 2020 Conference on Empirical Methods inNatural Language Processing (EMNLP), pages 577595. Association for Computational Linguistics. Anna Zueva, Anastasia Kuznetsova, and Francis Ty-ers. 2020. A finite-state morphological analyser forEvenki. In Proceedings of the Twelfth LanguageResources and Evaluation Conference, pages 25812589. European Language Resources Association.",
  "ADataset Details": "We first download the first 32M lines for each lan-guage in the deduplicated September 2021 releaseof OSCAR (Ortiz Surez et al., 2019; Abadji et al.,2021). We collect additional corpora for languageswith less than 1M lines in OSCAR (approximately50M tokens based on OSCAR line lengths) andfor languages that do not appear in OSCAR. Ad-ditional corpora consist of: Wikipedia (Wikipedia,2023), NLLB (Costa-juss et al., 2022), the LeipzigCorpora Collection (Goldhahn et al., 2012), eBibletranslations (eBible, 2023), FLORES-200 (Costa-juss et al., 2022), Tatoeba (Tiedemann, 2012,2020), AfriBERTa (Ogueji et al., 2021), NusaX(Winata et al., 2023), AmericasNLP (Mager et al.,2021), AmericasNLI (Ebrahimi et al., 2022), theNunavut Hansard InuktitutEnglish Parallel Cor-pus (Joanis et al., 2020), the Cherokee-EnglishChrEn dataset (Zhang et al., 2020), the Chero-kee Corpus (Cherokee Corpus, 2023), the CreeCorpus (Teodorescu et al., 2022), Languages ofRussia (Zaydelman et al., 2016), the Evenki Lifenewspaper (Zueva et al., 2020), the transcribedFula Speech Corpora (Cawoylel, 2023), IsiXhosa(Podile and Eiselen, 2016), the Ewe LanguageCorpus (Gbedevi Akouyo et al., 2021), the Mak-erere Luganda Corpora (Mukiibi et al., 2022),the CMU Haitian Creole dataset (CMU, 2010),the Tigrinya Language Modeling Dataset (Gaimet al., 2021), and Ulukau (Ulukau, 2023). OurWikipedia corpora use the Wikimedia dump fromAugust 20, 2023 (Wikimedia, 2023). All othercorpora use their publicly available versions asof August 2023. Links to individual corpora areincluded at While we are un-able to redistribute our compiled dataset due toredistribution licenses and out of respect for theoriginal data collectors, all of our sources are pub-licly available. As a caveat, we note that manylow-resource language datasets prohibit commer-cial use, and thus industry labs may be precludedfrom using such datasets without explicit permis-sion from the owners.We clean each corpus by removing lines consist-ing of only repetitive characters, exact duplicatelines, and lines identified as English by the spaCylanguage detection tool with confidence above 0.95(except for the English dataset; Honnibal et al.,2020). We find that English filtering is particu-larly important for Wikipedia, from which we also remove redundant lists of links and headers. Wemanually inspect all files for egregious uncleantext lines, and we remove any patterns found. Allcorpora outside of OSCAR are truncated to 2Mcleaned lines per language, which encompasses theentire corpus for most datasets; for example, only4 out of 239 downloaded Wikipedias are truncated(recall that we only download additional corporafor languages with less than 1M lines in OSCAR).Corpora are unshuffled unless their public release isalready shuffled. This allows tokenized sequencesto span multiple consecutive lines; the tokenizedsequences are shuffled prior to language model pre-training. Final token counts per language are listedin G.",
  "BTokenizer Details": "To control for tokenization quality across lan-guages, all of our monolingual tokenizers are Sen-tencePiece tokenizers trained on 10K lines of textwith maximum vocabulary size 32K (4.1; Kudoand Richardson, 2018). We have at least 10K linesof text in each of our 252 languages. All evalu-ations (including for multilingual models, whichfix the target language monolingual tokenizer) areconducted using these tokenizers. The multilin-gual tokenizers in 5 are used only for added dataduring multilingual pre-training; they are not usedfor evaluation. To ensure that our monolingualtokenizers have reasonable quality, we comparetheir vocabularies with tokenizers trained on morelines of text. Specifically, for each of our 28 high-resource languages, we train tokenizers on 10K,100K, 1M, and 10M lines of text. For each trainingdataset size, we compute the vocabulary overlapwith the 4K and 8K most frequent tokens in the10M-line tokenizer (the reference vocabulary). shows the reference vocabulary overlapfor the different training dataset sizes. At 10K lines,the tokenizer vocabularies on average cover 93.7%of the 4K-token reference vocabulary and 87.8%of the 8K-token reference vocabulary, indicatingreasonable tokenization quality.",
  "CLanguage Model Pre-Training Details": "Language models are pre-trained using the Hug-ging Face Transformers library (Wolf et al., 2020)and code from Chang and Bergen (2022). Hyper-parameters are reported in (left). All of ourmodels use the GPT-2 architecture (Radford et al.,2019), changing only the number of layers, atten- 10k100k1m10m Tokenizer training lines 0.75 0.80 0.85 0.90 0.95 1.00 Vocab overlap with 4K reference 10k100k1m10m Tokenizer training lines 0.75 0.80 0.85 0.90 0.95 1.00 Vocab overlap with 8K reference : Vocabulary overlap with the reference vocabu-lary for tokenizers trained on different numbers of lines.The reference vocabulary consists of the 4K (left) or8K (right) most frequent tokens in a 10M-line tokenizerfor that language. We report the proportion of the ref-erence vocabulary that is covered by 32K-vocabularytokenizers with different training dataset sizes. Graylines indicate individual languages, and the purple lineindicates the mean across languages. tion heads, and embedding sizes as in Turc et al.(2019). Models are pre-trained for 20 epochs of thetarget language monolingual data in the low andmed-low resource scenarios, 10 epochs in the med-high resource scenario, and 2 epochs in the high-resource scenario. Based on initial results usingrandomly-sampled languages, pre-training on morethan 20 epochs often leads to overfitting (increasesin eval loss) in low-resource scenarios. Multilin-gual models include one epoch of the multilingualdata (5) randomly interspersed with the target lan-guage data. The numbers of pre-training steps fordifferent dataset configurations are reported in Ta-ble 1 (right). Average evaluation loss curves duringpre-training are shown in . For each targetlanguage, the same 500K evaluation tokens are heldout in all cases. In the monolingual low-resourcescenario for each language (i.e. 1M pre-trainingtokens), we pre-train three tiny models (insteadof one) and compute their average evaluation log-likelihood, because these models are used as the baseline models for relative log-likelihoods (4.2).All language model pre-training runs togethertake a total of 1.871020 FLOPs. This is less than1/1500 the computation used to train the original175B-parameter GPT-3 model (Brown et al., 2020;3.14 1023 FLOPs). Models are each trained onone NVIDIA GeForce GTX TITAN X, GeForceRTX 2080 Ti, TITAN Xp, Quadro P6000, RTXA4500, RTX A5000, or RTX A6000 GPU. Ourpre-training experiments take approximately 17700A6000 GPU hours. Dataset cleaning, tokenization,and merging takes approximately 5880 CPU corehours, largely due to dataset tokenization with eachmultilingual tokenizer.",
  "DMonolingual Token Estimation Details": "We overview our monolingual token estimation pro-cess in 4.3, and we provide details here. As moti-vation, we note that relative log-likelihood scoresare not comparable across model sizes. For exam-ple, suppose that adding a multilingual dataset Dimproves a models eval log-likelihood score by1.0 in both small and large models. In this case, itwould be unclear whether the effect of D is intu-itively equal in the two model sizes; doubling thelikelihood of the eval dataset is likely more diffi-cult in the larger model, so we might interpret D ashaving a larger effect on the larger model despitethe same change in log-likelihood. To avoid thisambiguity, we measure model performance usingthe estimated number of monolingual tokens in thetarget language that would achieve similar perfor-mance. In the case above, adding the multilingualdataset D might be similar to adding n1 monolin-gual tokens to the smaller model, but similar toadding n2 > n1 monolingual tokens to the largermodel.To estimate this, we first fit a power law axb+c for each of our 252 languages, predicting amodels relative log-likelihood score (4.2) basedon its pre-training dataset size in log10 tokens.Each language has up to four ground truth val-ues, corresponding to our monolingual modelspre-trained on 1M, 10M, 100M, and 1B tokens.When all four points are available (i.e. our 28 high-resource languages), we are able to fit a power lawfrom scratch. From these languages, we estimatethe medians and standard deviations of a, b, andc. For languages with fewer than four data points,we constrain a, b, and c to be within 2.5 standarddeviations from the median parameter value. If this",
  "B21B187500": ": Left: Language model pre-training hyperparameters (Devlin et al., 2019; Turc et al., 2019; Radford et al.,2018). To prevent overfitting (increasing loss on the eval dataset), learning rates are halved for mini and smallmodels in the low-resource scenario, to 4e-4 and 2e-4 respectively (4.1). Right: Pre-training steps for differentmonolingual and multilingual dataset sizes. There is always one epoch of the multilingual dataset (5). 0%20%40%60%80%100% Pre-training completion Target language eval loss (log2) tiny models Low-resource (1M)Med-low resource (10M)Med-high resource (100M)High-resource (1B) 0%20%40%60%80%100% Pre-training completion Target language eval loss (log2) mini models Low-resource (1M)Med-low resource (10M)Med-high resource (100M)High-resource (1B) 0%20%40%60%80%100% Pre-training completion Target language eval loss (log2) small models Low-resource (1M)Med-low resource (10M)Med-high resource (100M)High-resource (1B) : Target language evaluation loss curves during pre-training, for different model sizes and language resourcescenarios. Each individual curve corresponds to a dataset configuration in (right), averaging the loss curveover languages.",
  "leads the curve fitting to diverge, we loosen thisconstraint to 5.0, 7.5, then 10.0 standard deviationsfrom the median": "For languages where the curve fitting still doesnot converge or languages with too few data points(e.g. med-low resource languages with data pointsonly for 1M and 10M tokens), we fix a as the me-dian parameter value from the high-resource lan-guages. We fit only b and c, which we constrainusing standard deviations in the same way as de-scribed above. If the curve fitting still does not con-verge when fixing a (e.g. low-resource languageswith a data point only for 1M tokens), we fix botha and b as their median values. In that case, weonly fit c, which is equivalent to simply shifting themedian curve up or down by a constant. All curve",
  "fitting is implemented using scipy (Virtanen et al.,2020)": "Finally, in many cases, we compare multilin-gual models to monolingual models with a specificknown dataset size. The multilingual models in6 are all compared to corresponding monolingualmodels without any added multilingual data. Forexample, a multilingual model with 10M monolin-gual tokens and 100M added multilingual tokens(relative log-likelihood score y1) would be com-pared to a monolingual model with 10M monolin-gual tokens alone (relative log-likelihood score y0).In these cases, we constrain our curve-fitting to passthrough the point corresponding to the referencemonolingual model (e.g. in the example described,the curve would be required to pass through the ground truth point (7.0, y0) for 107.0 monolingualtokens alone). This only slightly alters the curvepredicting relative log-likelihood score from log10tokens, but it ensures that our baseline monolingualmodels in 6 lie exactly at 1M, 10M, 100M, and1B tokens ( and ). Once we have fitted a curve predicting amodels relative log-likelihood score from log10pre-training tokens in a language L, we use thiscurve to estimate the number of tokens required toachieve any relative log-likelihood score. Then, wehave two metrics for a multilingual models per-formance on target language L: (1) the modelsrelative log-likelihood score itself and (2) the es-timated number of monolingual tokens in L thatwould achieve that relative log-likelihood. The lat-ter metric is easily interpretable, and it facilitatescomparisons across languages and model sizes. Wenote that the estimated token count is a monotonicincreasing function of relative log-likelihood scorein all cases. Thus, even if the estimated tokencounts are not perfectly accurate, they preserve per-formance rankings between models (e.g. betweenour multilingual models and the monolingual base-lines). A language model with target language Lwill have a higher estimated token count if andonly if it assigns a higher log-likelihood score tothe evaluation dataset for L. Still, we evaluate the quality of our monolingualtoken count estimation process. For each languageL, we have up to four monolingual models (1M,10M, 100M, and 1B pre-training tokens). We holdout one (or multiple) of the models, and we esti-mate its monolingual token count based on a curvefitted to the other monolingual models for L. Wenote that these estimations are extrapolating at min-imum one order of magnitude away from the mod-els used to fit the curve, because the models areexactly one order of magnitude apart in terms ofpre-training tokens. The results in 6 do not needto extrapolate this far. Still, even with this largerextrapolation, we obtain reasonable estimates ofmonolingual token counts in the held-out scenarios(). The root-mean-square errors are 0.340,0.317, and 0.335 log10 tokens for tiny, mini, andsmall models respectively. Again, regardless ofestimation quality, the estimated token counts aresimply a monotonic increasing function of relativelog-likelihood score.",
  "EStatistical Tests": "We run paired sample t-tests to assess the statisti-cal significance of our results from 6. For eachreported p-value, we compare models that differby exactly one of: monolingual dataset size, mul-tilingual dataset size, linguistic similarity of theadded languages, or model size. We pair models bylanguage, so each pair differs by only the manip-ulated variable. To avoid potential artifacts fromour token estimation process, we compare modelrelative log-likelihoods directly (4.2) unless com-paring across two model sizes (because relativelog-likelihood improvements and degradations aredifficult to compare across model sizes; D). Ifcomparing across model sizes, we compare the es-timated monolingual token counts of the models.In both cases, we use a paired sample t-test. To de-crease the chance of false positive results, we onlyrun the statistical tests whose p-values are reportedin the main text, and we account for multiple com-parisons using Bonferroni correction (Bonferroni,1936). For estimates of significance, the plots in 6also include 95% confidence intervals for means.",
  "FAdditional Correlations": "In 6.1, we find that the mean syntactic similarityof the added languages accounts for more variancein multilingual model performance (relative log-likelihood scores) than geographic and lexical (vo-cabulary) similarity. In that section, we considerthe low-resource scenario with 100M added mul-tilingual tokens in small models. Here, we reportthe same results for tiny, mini, and small models.Variance partitioning results are shown in . In all cases, syntactic similarity accounts formore variance than geographic and lexical simi-larity. Correlations between different similaritymeasures and model performance for mini and tinymodels with 100M added multilingual tokens areplotted in .",
  "GList of Languages": "The 252 languages included in our language model-ing study are listed in . These languages arethose with at least 1.5M tokens in our dataset (A).We restrict all languages to a maximum of 1B to-kens. In lower resource scenarios, higher resourcelanguages are subsampled to mimic the lower re-source scenario. For example, we have 167 med-low resource languages when including the subsam-pled med-high and high resource languages. We Monolingual data (tokens)",
  "Estimated": "tiny models Monolingual data (tokens) mini models Monolingual data (tokens) small models : Estimated monolingual token counts for held-out monolingual models. Token counts are estimated fromeach models relative log-likelihood score using a curve fitted to the specific language (4.3). Estimations areextrapolating one order of magnitude out from the points used to fit the curve. In practice, we generally do not needto extrapolate this far for our results. The black line indicates perfect accuracy. 0.034 0.001 0.021 0.007 0.074 0.054"
}