{
  "Abstract": "Large language models (LLMs) require contin-ual knowledge updates to stay abreast of theever-changing world facts, prompting the for-mulation of lifelong model editing task. Whilerecent years have witnessed the developmentof various techniques for single and batch edit-ing, these methods either fail to apply or per-form sub-optimally when faced with lifelongediting. In this paper, we introduce LEMoE,an advanced Mixture of Experts (MoE) adap-tor for lifelong model editing. We first ana-lyze the factors influencing the effectivenessof conventional MoE adaptor in lifelong edit-ing, including catastrophic forgetting, incon-sistent routing and order sensitivity. Based onthese insights, we propose a tailored moduleinsertion method to achieve lifelong editing,incorporating a novel KV anchor routing toenhance routing consistency between trainingand inference stage, along with a concise yeteffective clustering-based editing order plan-ning. Experimental results demonstrate the ef-fectiveness of our method in lifelong editing,surpassing previous model editing techniqueswhile maintaining outstanding performance inbatch editing task. Our code can be found at:",
  "Introduction": "Large language models (OpenAI, 2023; Touvronet al., 2023a,b; Jiang et al., 2023; Bai et al., 2023)encode a vast amount of world knowledge dur-ing pre-training, which can be accessed and uti-lized through natural language prompts (Petroniet al., 2019). However, the dynamic nature of thereal world necessitates regular and continual up-dates to these models to correct outdated infor-mation or integrate new knowledge (Yao et al.,2024; Wang et al., 2024). Also, retraining or fine-tuning of LLMs is often resource-intensive and",
  "Timeline:": ": The conceptual framework for LEMoE. Wealign the expert networks in MoE architecture with databatches and freeze the expert networks correspondingto previous data when conducting current edits. Dataiand FFNi represent the current data and module, withdashed line parts indicating future edits. time-consuming (Li et al., 2024a), making it im-practical for lifelong growing knowledge. There-fore, lifelong model editing (Hartvigsen et al.,2023) has been proposed to remedy the continualknowledge updates and injections for LLMs in acheap and timely manner (Wang et al., 2024).In recent years, there has been a proliferationof effective model editing techniques proposed forsingle or batch editing, such as MEND (Mitchellet al., 2022a), ROME (Meng et al., 2022),MEMIT (Meng et al., 2023), and MEMoE (Wangand Li, 2024). However, these methods often proveinapplicable or exhibit suboptimal performancewhen faced with lifelong editing task (Wang et al.,2024). In this paper, we introduce LEMoE, anadvanced Mixture of Experts (MoE) adaptor, toaddress the challenges inherent in lifelong editing.Initially, we analyze the factors that influencethe effectiveness of conventional MoE adaptor inlifelong editing, including catastrophic forgetting,inconsistent routing, and order sensitivity. In theCatastrophic Forgetting Analysis (3.1), we evalu-ate the performance of conventional MoE adaptorat different positions within the editing sequence toquantify the impact of subsequent edits on preced- ing ones. We observe that the conventional MoEadaptor exhibits significant catastrophic forgetting,where earlier edits are more prone to errors. Inthe Routing Consistency Analysis (3.2), we com-pare the expert networks selected by routing strat-egy during the training and inference stages whenfaced with the same input. This comparison revealsa routing inconsistency in the conventional routingstrategy, where identical inputs are routed to differ-ent experts at different stages. Finally, in the OrderSensitivity Analysis, we highlight that editing or-der profoundly impacts model performance (3.3).Through varying the sequence order of the samedataset during lifelong editing, we observe perfor-mance variations of up to 20 points, surpassing theimprovement of some optimization algorithms.Based on these insights, we propose a tailoredmodule insertion method to achieve lifelong editing(4.1). As illustrated in , we align the ex-pert networks in the MoE architecture with the databatches in the sequential editing process. Whenconducting current editing, we freeze the expert net-work corresponding to the previous data, therebymitigating the adverse effects of current data edit-ing on previous edits and alleviating catastrophicforgetting from a model mechanism perspective.Secondly, we introduce a novel Key-Value (KV) an-chor routing (4.2), wherein each expert is assigneda key vector and the input instance-level embed-ding serves as the corresponding value. Based onthese key-value pairs, we align the routing compu-tation processes during both training and inferencestage. This ensures that identical inputs undergosame routing computation to reach the same expertacross all stages, thereby enhancing routing consis-tency and further mitigating catastrophic forgetting.Finally, leveraging the consistency between theMoE preferences of editing order and the objec-tives of clustering algorithm, we employ a conciseyet effective clustering-based order planning to en-hance the overall performance of LEMoE (4.3).We conduct experiments on the LLaMA-7B andMistral-7B models using the ZsRE (Levy et al.,2017) and SelfCheckGPT (Manakul et al., 2023)datasets to evaluate the performance of LEMoE.Experimental results show that our approach sur-passes previous model editing methods, whilemaintaining excellent performance in batch editing.The main contributions of our work can be sum-marized as follows:",
  "Preliminaries of Model Editing": "Based on previous research (Yao et al., 2023;Zhang et al., 2024; Li et al., 2024a), model editinginvolves the process of transforming an initial basemodel f (where denotes the models parameters)into an edited model f. The goal is to modifythe models outputs for a specific set of editing in-stances, while maintaining consistent behavior forall other instances (Li et al., 2024a). The targetediting instance can be described as (xei, yei ), withthe condition that f(xei) = yei . The set of thisinstances is termed the editing scope Iedit, whereasthe out-of-scope set Oedit comprises inputs not as-sociated with the editing examples. Formally, thecriteria for a successful edit can be described as:",
  "i=1( f(xei) yei )(2)": "where n represents the batch size. Batch editingwith batch size of 1 is also known as Single Editing.2) Lifelong Editing refers to the continuous it-erative modification of model f, also known asSequential Batch Editing. Lifelong editing usedataset Dedit = {B1, B2, . . . , Bs} with s sequen-tial batches and each batch Bi contains n edits:",
  "Expert Number": "(a) Lifelong Editing Results(b) Batch Editing Results 0.820.070.100.01 0.070.690.030.21 0.050.150.730.07 0.060.090.140.71 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 : Left: Reliability of conventional MoE under different stage evaluation. Immediate evaluation occursimmediately after each edit, Final evaluation occurs after all edits in lifelong editing. Right: Visualization ofrouting consistency. The value Cij in each block denotes the proportion of the input data processed by expert iduring the training phase that is routed to expert j during the testing phase. Model: LLaMA2-7B. Dataset: ZsRE. Note that the model is not rolled back to the initialstate after each batch editing. Similarly, lifelongediting with batch size of 1 in the sequence is alsoreferred to as Sequential Editing.Based on the above settings, an effective modeleditor must satisfy the criteria of three fundamentalproperties: Reliability, Generalization, and Local-ity (Yao et al., 2023). These properties are formallydefined as follows (Zhang et al., 2024):1) Reliability denotes the average precision of thepost-edit model f concerning the intended edits:",
  "In the field of continual learning, the general phe-nomenon of catastrophic forgetting where training": "on new tasks degrade performance on old taskshas been extensively reported and studied (Kothaet al., 2023). We aim to investigate whether theMoE adaptor in lifelong model editing also suffersfrom catastrophic forgetting: whether editing withnew data leads to forgetting previously edited data.To assess this, we employ the classic evaluationmethod for catastrophic forgetting, which involvesmeasuring the performance decrease on previouslyedited data during the course of lifelong editing. ExperimentsTo evaluate the impact of currentdata editing on previous ones during the lifelongediting process, we employ two different stage eval-uation methods: (1) a normal evaluation conductedon all editing data only after all edits are completed,and (2) an evaluation conducted immediately afterediting the current data to assess the effectivenessof these edits at the current stage. We do not set upanother control group where the base model editsonly the current data without considering previousdata because the accuracy of MoE adaptor underthe second evaluation method is nearly 100%. Weutilize the LLaMA2-7B as base model and ZsREdataset (detailed in 5.1). In lifelong editing setting,we perform 100 sequential editing steps, with eachstep editing a batch of 10 instances, resulting in a to-tal of 1000 edited instances. The evaluation metricis Reliability (detailed in 2). The implementationof conventional MoE adaptor follows (Wang andLi, 2024), employing 4 experts and topk = 1.",
  "ResultsIn the left plot of , the reliabilityof the immediate evaluation during the entire life-long editing process shows that only once do thescore fall below 100. This indicates that the model": "consistently achieves desired editing goals at everyindividual step. However, the models overall per-formance is only around 60 in the final evaluation,with earlier edits exhibiting a more significant de-cline in effectiveness and some initial edits scoringclose to 0. This suggests a pronounced catastrophicforgetting phenomenon, where the models forget-fulness of previous editing data markedly dimin-ishes its overall performance in lifelong editing.",
  "Routing Consistency Analysis": "In MoE structure, the specificity of experts directlyimpacts model performance (Fedus et al., 2022).The design philosophy of MoE adaptor encourages\"professional people do professional things\", en-suring that the same inputs are routed to the sameexpert for processing during both training and test-ing phases (Wang and Li, 2024). We aim to assessthe consistency of the routing within the MoE adap-tor under lifelong editing setting and explore thedegree of specificity among these experts. ExperimentsTo assess the routing consistency,we log the processing expert for each input duringthe training phase and compare it against the ex-pert processing the same input during testing. Wetrain models under batch editing setting and life-long editing setting on identical dataset to comparerouting consistency across different tasks. In thelifelong editing setup, sequential editing steps isset to 100 steps, each step editing a batch of 10instances. Maintaining the same edited data, batchediting utilize a batch size of 1000. We employLLaMA2-7B and ZsRE, with all other experimen-tal settings consistent with 3.1. ResultsOn the right side of , each sub-graph depicts the proportion Cij where the inputprocessed by expert i during the training phase isrouted to expert j during the testing phase. Thediagonal element Cii represents the probability thatthe same input is routed to the same expert duringboth training and testing phases. Experimental re-sults comparing two editing setups reveal that rout-ing consistency is notably poorer in lifelong editingtask, with minimal specificity observed among dif-ferent experts. In contrast, batch editing exhibitssignificant routing consistency. Hence, there is acritical need to devise more accurate and effectiverouting algorithm to guide expert specialization. Total Number of Edits",
  "Order Sensitivity Analysis": "In continual learning, the performance of a modelsignificantly varies based on the order of the taskarrival sequence (Bell and Lawrence, 2022; Yoonet al., 2020). Previous researches on lifelong edit-ing ignored the impact of editing order on perfor-mance. Therefore, we aim to investigate how dif-ferent editing order affect the overall performancein lifelong editing. Additionally, model tend tolearn similar tasks more effectively in continuallearning (Bell and Lawrence, 2022), and sentenceswith high semantic similarity often contain relatedknowledge. Therefore, we also aim to explore therelationship between the semantic similarity of edit-ing inputs and the editing results. ExperimentsTo evaluate the models editing or-der sensitivity, we employ the same set of edit-ing data and randomly shuffle the order beforeperforming lifelong editing. In this lifelong edit-ing setup, the sequential editing steps is set to10, with each step editing a batch of 10 (or 100)instances, resulting in a total of 100 (or 1000)edited instances.Each of these two data vol-umes experiment is conducted 100 times. To as-sess the relationship between the semantic simi-larity of the editing inputs and the editing results,we calculate both within-batch semantic similar-ity (WBS) and between-batch semantic similar-ity (BBS) of the editing data. Specially, givendataset Dedit = {B1, B2, . . . , Bs} with s sequen-tial batches and each batch Bi contains n editsBi = {(xei, yei )}i[1,n], the WBSi of Bi and BBScan be calculated as:",
  "LLM": ": The overall architecture of LEMoE compared with conventional MoE adaptor. We assume that LEMoE iscurrently at time i to edit datai using module FFNi. Left: When editing data datai, the prior experts correspondingto previous data are all frozen, leaving only the new model FFNi and router trainable. Right: In the training stage,depicted by the solid lines, the routing weight g(i | x) (abbreviated as gi) is computed using the instance-levelembedding and expert key vectors {k1, k2, . . . , ki} for expert selection. During inference, as indicated by thedashed lines, the same routing computation is employed to direct the input to the corresponding expert. where sim() denotes cosine similarity based onembedding, ei = concat(embed(xei, yei )) andembed represents the embedding vector outputfrom the models embedding layer. For input con-sisting of multiple tokens, the embedding is themean for each token. In Equation 8, Bi = Ei(ei)denotes the average semantic vector for the i-thgroup. We employ LLaMA2-7B and ZsRE, withall other experimental settings consistent with 3.1. ResultsOn the left of , the MoE adaptordemonstrates significant order sensitivity. Varyingthe order of the same editing data leads to perfor-mance fluctuations exceeding 20 points. In 100data editing, these fluctuations even range from 30to 90 point. This substantial impact of editing orderon model performance suggests that adopting orderalign with model preferences can greatly enhanceediting efficiency. The results on the right revealthat higher within-batch semantic similarity andlower between-batch semantic similarity correlatewith better editing results. These provide insightsfor designing more effective editing order.",
  "New Module Inserting": "Inspired by (Wang and Li, 2024), LEMoE intro-duces multiple parallel experts within the trans-former feed-forward network (FFN) via a bypassmechanism, while freezing all the models originalparameters. This module is applied in only onetransformer block of the entire model. The choiceto use the FFN module is motivated not only by itstraditional role in MoE but also by recent experi-mental findings from knowledge probing technolo-gies that suggest the MLP layers within the FFNstore knowledge (Geva et al., 2021; Meng et al.,2022, 2023). The bypass mechanism preserves allthe original parameters of the model, enhancing thelocality of model editing.However, in conventional MoE adaptor, all ex-perts are sequentially trained without any mech-anism to protect prior editing knowledge, whichallows current edits to easily affect previous onesand leads to severe catastrophic forgetting. Mean-while, experimental results indicate that a singleFFN expert is sufficient to learn a batch of editingdata (Wang and Li, 2024). Therefore, in LEMoE,we adopt a straightforward method to maintain ed-its from previous learning phases. As shown on theleft of , when facing a new batch of editingdata in the sequence, we add a new FFN module asan expert to learn this batch of data and freeze theexpert network corresponding to the previous data.By aligning the expert networks in MoE architec-ture with the data batches in lifelong editing, we mitigate the adverse effects of current data editingon previous edits, thereby alleviating catastrophicforgetting from a model mechanism perspective.Specially, when the t + 1th batch in lifelongediting dataset arrives, the LEMoE adaptor inte-grates previous t experts denoted as f1, f2, . . . , ft,a router g(i | x) which outputs the correspondingcoefficients for each fi based on the input x alongwith a newly added expert ft+1. The output h ofthis module can be expressed as:",
  "KV Anchor Routing": "We propose the KV anchor routing to align thetraining and inference processes for expert selec-tion, thereby enhancing routing consistency andaddressing catastrophic forgetting at routing level.During the training phase, when the t-th batchin the lifelong editing dataset arrives, we freeze theparameters of all previous experts f1, f2, . . . , ft1and introduce a new expert ft to accomplish thecurrent batch editing. We allocate a key vector kifor each expert fi (at time t, only ft is allocated anew key, while the keys corresponding to previousexperts remain frozen) and compute instance-levelembedding features of the input as values.The KV anchor process begins with the j-th in-put sentence Xjt = {xjti }Li=1 of the current t-thbatch data passing through the embedding layerof the LLM backbone to obtain Etj (we omit thesuperscripts t for simplicity). Since Ej Rmd and each key vector ki Rd have different se-quence lengths, we apply mean-pool operation onthe length dimension of Ej, and obtain ej Rd.Then ej is fed to a sub-network to project it into thespaces of the key vectors for better feature align-ment. This consists of down and up projection:",
  "i=1g(i | ej)fi(xjti )(12)": "During the inference phase, when testing datafrom different batches arrive, they undergo thesame routing computation of Equations 10 and 11to reach the appropriate expert. Although thereis a possibility that the earlier testing data may berouted to the experts corresponding to the later data,we mitigate this issue by employing the clustering-based editing order selection described in 4.3which reduces semantic similarity between batches.In summary, aligning the routing computations dur-ing training and testing phases through KV anchorsenhances routing consistency and further mitigatescatastrophic forgetting.",
  "Clustering-based Order Planning": "In 3.3, we observed a correlation between im-proved editing performance and editing order char-acterized by high between-batch semantic similar-ity and low within-batch semantic similarity. Thissuggests that editing performance can be improvedby selecting editing order that align with modelbiases. Additionally, this objective aligns with thegoals of clustering algorithms which aim for highintra-cluster similarity and low inter-cluster simi-larity. Therefore, we employed the K-means algo-rithm to group the editing data based on semanticsimilarity and preferentially selected data from thesame cluster for each batch during editing. Exper-imental results indicate that this straightforwardapproach is highly effective.",
  "Experimental Setups": "Datasets and MetricsWe used two lifelongmodel editing datasets: ZsRE (Levy et al., 2017)and SelfCheckGPT (Manakul et al., 2023). ZsREis a context-free Question Answering (QA) datasetbuilt upon zero-shot relation extraction, and weadopt the split provided by (Zhang et al., 2024).SelfCheckGPT is a dataset for evaluating the per-formance of model editing methods on mitigat-ing model hallucination, and we We followed the",
  "LEMoE0.780.521.000.770.750.481.000.743.031.004.391.00": "GRACE (Hartvigsen et al., 2023) data processingapproach. Further details about the datasets areprovided in Appendix B.1. In terms of evaluationmetrics, we use the three metrics mentioned in 2:Reliability (Rel.), Generalization (Gen.), and Lo-cality (Loc.), along with the average scores (Avg.)over these metrics. Notably, for the SelfCheckGPTdataset,following (Wang et al., 2024), we use theperplexity (PPL) to verify Reliability, and there isno proper metric for generalization.",
  "using additional training data to transform gra-dient obtained by standard fine-tuning": "Memory based methods: DEFER (Mitchellet al., 2022b), GRACE (Hartvigsen et al., 2023).DEFER is inspired by SERAC (Mitchell et al.,2022b) using an external cache to store explicitediting cases, while GRACE adopts a codebookto store relevant edits. Implementation DetailsWe selected LLaMA2-7B and Mistral-7B as base models. The modifica-tion was applied to layer 18 with topk = 1. Due tolimited computational resources, we were able toadd a maximum of 5 FFN experts. Consequently,the sequential editing steps were set to 5 and eachstep contains a batch of 25 (or 200) instances, re-sulting in a total of 100 (or 1000) editing instances.We use AdamW (Loshchilov and Hutter, 2019) asthe optimizer with a learning rate of 2e-4. Furtherdetails are provided in the Appendix B.",
  "LEMoE0.740.501.000.750.700.481.000.73": "tor. While MEMIT shows comparable performanceat T=100, our method demonstrates a substantialperformance gap in longer sequence editing task.In Locality, our method consistently scores 1.00,indicating minimal impact on irrelevant inputs. Al-though GRACE and FT-EWC achieve higher scorein Rel. and Gen. respectively, these methods makegreat sacrifices in Gen. or Loc. Only our methodachieves a better balance.The performance advantage of LEMoE is morepronounced on SelfCheckGPT dataset, maintain-ing the lowest perplexity score of 3.36 and 4.39 atT = 600, with a maximum improvement of 26.31%over the nearest competitor and a constant localityscore of 1.00. In summary, across the two datasetsand eight baselines, our method shows a clear per-formance advantage.",
  "Batch Editing": "Considering the significant performance advan-tages of conventional MoE adaptor in batch edit-ing (Wang and Li, 2024), we aim to evaluate thechanges in batch editing performance of its im-proved version, LEMoE, after applying the pro-posed optimizations. The batch size is set to 30here. As shown in , LEMoE continues toexcel in batch editing, achieving perfect reliabil-ity and locality scores of 1.00, with only a slightdecline in generalization. Overall, LEMoEs per-formance is nearly on par with the original MoE,demonstrating the dual advantages in both batchediting and lifelong editing.",
  "Longer Sequence Editing": "We scale the number of lifelong editing to 3K in. We observe that LEMoE outperforms thestrongest baselines MEMIT and GRACE. GRACEexcels in reliability but almost entirely loses gen-eralization. While MEMIT demonstrates bettergeneralization, its lower locality scores indicatea significant impact on unrelated data inputs, po-tentially affecting the models general ability (Guet al., 2024). Only our method achieves a balancedediting performance. Moreover, the performanceadvantage of our approach increases with the num-ber of edits, highlighting the potential of LEMoEto handle extremely long sequential editing.",
  "Ablation Study": "We present a series of ablation studies to evaluatethe influence of various model components, includ-ing routing strategies, embedding levels and orderplanning. The experimental results are shown in. Conventional routing means the router ismodeled by an MLP, knowledge (anchor) routing isthe routing strategy in MEMoE and entity-level em-bedding means substitute the embeddings of namedentities from the input for ej in Equation 10. Moredetails in Appendix B.4.We observe that: (1) Different model settingsexhibit minimal impact on locality but significantlyaffect generalization.(2) Alteration in routingstrategy notably affect reliability and generaliza-tion, and conventional routing yields the lowestscores across all metrics.Meanwhile, employ-ing knowledge routing marginally enhances perfor-mance yet still lags behind LEMoE, highlightingthe pronounced efficacy of KV-anchor routing. (3)Using token-level embeddings for routing inputsnotably diminishes model generalization. A pos-sible reason is that token representation may notbe suitable for measuring semantic similarity inautoregressive LLMs (Wang et al., 2024), therebyhindering routers ability to router the same input to the same expert. (4) Substituting hierarchical clus-tering for K-means in editing order planning min-imally impacts model performance, yet K-meansdemonstrates higher computational efficiency. Thismay stem from our utilization of a small numberof clusters and a large batch size during datasetconstruction, which provides clustering algorithmswith greater fault tolerance, thereby partially mask-ing the performance differences between the twoclustering algorithms.",
  "Conclusion": "In this paper, We propose LEMoE, an advancedMoE adaptor for lifelong model editing. We an-alyze three factors influencing the effectivenessof MoE adaptor in lifelong model editing. Then,we propose three optimization modules. Thesemodules align the routing computation processesbetween training and testing phases, ensuring thesame inputs are routed to the same experts. Experi-mental results validate the effectiveness of LEMoEacross multiple models and datasets.",
  "Ethics Statement": "Our research on model editing and the proposedLEMoE module adheres to the ethical guidelinesoutlined by the ACL Ethics Policy. The primaryobjective of our work is to enhance lifelong edit-ing performance in LLMs. We recognize the crit-ical importance of addressing privacy concernswhen model editing publicly accessible, central-ized LLMs with private data. And, we acknowl-edge the potential risks associated with direct pa-rameter edits within models, especially when usingharmful data, which require careful mitigation. Itsessential to bear in mind that ill-intentioned modelediting could lead the model to generate harmfulor inappropriate outputs. Therefore, ensuring safeand responsible practices in model editing is ofparamount importance. The application of thesetechniques should be guided by ethical considera-tions, with safeguards in place to prevent misuseand the production of harmful results. Our com-mitment to accountability, responsible governance,and continuous ethical assessment underscores ourdedication to upholding the highest standards ofintegrity in the development and deployment ofmodel editing methods.",
  "Limitations": "There are several limitations to consider for futuredirections of model editing of large language mod-els. Firstly, when the learning sequence scales tomore data, such as hundreds of batches or tens ofthousands of editing instances, continually allocat-ing an expert block for each batch would lead tosignificant computational and storage costs. There-fore, exploring methods to prune and merge similarexperts in the continual learning process presentsan interesting research direction. Secondly, ourwork primarily focuses on the acquisition of factualknowledge, neglecting other types of knowledge.We prioritize the accuracy of knowledge learn-ing while paying less attention to other aspects,such as knowledge reasoning abilities. Thirdly,due to hardware constraints, our investigation waslimited to models with up to 7 billion parame-ters with 5 experts. Additionally, we concentratedon decoder-only autoregressive models, excludingencoder-decoder architectures. Further researchthat replicates our study using larger-scale modelswith much more experts and different architecturewould be beneficial in confirming our findings. This research is supported by the National Natu-ral Science Foundation of China (No.62476127,No.62106105), the Natural Science Foundationof Jiangsu Province (No.BK20242039), the CCF-Baidu Open Fund (No.CCF-Baidu202307), theCCF-Zhipu AI Large Model Fund (No.CCF-Zhipu202315), the Fundamental Research Fundsfor the Central Universities (No.NJ2023032), theScientific Research Starting Foundation of Nan-jing University of Aeronautics and Astronautics(No.YQR21022), and the High Performance Com-puting Platform of Nanjing University of Aeronau-tics and Astronautics. Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars,Laurent Charlin, Massimo Caccia, Min Lin, and Lu-cas Page-Caccia. 2019. Online continual learningwith maximal interfered retrieval. In Advances inNeural Information Processing Systems 32: AnnualConference on Neural Information Processing Sys-tems 2019, NeurIPS 2019, December 8-14, 2019,Vancouver, BC, Canada, pages 1184911860.",
  "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei": "Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,Xuancheng Ren, Chuanqi Tan, Sinan Tan, JianhongTu, Peng Wang, Shijie Wang, Wei Wang, ShengguangWu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, JianYang, Shusheng Yang, Yang Yao, Bowen Yu, HongyiYuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang,Yichang Zhang, Zhenru Zhang, Chang Zhou, Jin-gren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023.Qwen technical report. CoRR, abs/2309.16609.",
  "Samuel J. Bell and Neil D. Lawrence. 2022. The ef-fect of task ordering in continual learning. CoRR,abs/2205.13323": "Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish,Alec Radford, Ilya Sutskever, and Dario Amodei.2020. Language models are few-shot learners. In Ad-vances in Neural Information Processing Systems 33:Annual Conference on Neural Information Process-ing Systems 2020, NeurIPS 2020, December 6-12,2020, virtual. Alexandra Chronopoulou, Matthew E. Peters, Alexan-der Fraser, and Jesse Dodge. 2023. Adaptersoup:Weight averaging to improve generalization of pre-trained language models. In Findings of the Associ-ation for Computational Linguistics: EACL 2023,Dubrovnik, Croatia, May 2-6, 2023, pages 20092018. Association for Computational Linguistics.",
  "Together Computer. 2023. Redpajama: an open datasetfor training large language models": "Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, BaobaoChang, and Furu Wei. 2022. Knowledge neuronsin pretrained transformers. In Proceedings of the60th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), ACL2022, Dublin, Ireland, May 22-27, 2022, pages 84938502. Association for Computational Linguistics. Qingxiu Dong, Damai Dai, Yifan Song, Jingjing Xu,Zhifang Sui, and Lei Li. 2022. Calibrating factualknowledge in pretrained language models. In Find-ings of the Association for Computational Linguistics:EMNLP 2022, Abu Dhabi, United Arab Emirates, De-cember 7-11, 2022, pages 59375947. Associationfor Computational Linguistics. Zhibin Duan, Hao Zhang, Chaojie Wang, ZhengjueWang, Bo Chen, and Mingyuan Zhou. 2021. En-slm: Ensemble language model for data diversity bysemantic clustering. In Proceedings of the 59th An-nual Meeting of the Association for ComputationalLinguistics and the 11th International Joint Confer-ence on Natural Language Processing, ACL/IJCNLP",
  "Switch transformers: Scaling to trillion parametermodels with simple and efficient sparsity. J. Mach.Learn. Res., 23:120:1120:39": "Chris Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, RohanAnil, and Chelsea Finn. 2021. Efficiently identifyingtask groupings for multi-task learning. In Advancesin Neural Information Processing Systems 34: An-nual Conference on Neural Information ProcessingSystems 2021, NeurIPS 2021, December 6-14, 2021,virtual, pages 2750327516. Mor Geva, Roei Schuster, Jonathan Berant, and OmerLevy. 2021. Transformer feed-forward layers are key-value memories. In Proceedings of the 2021 Confer-ence on Empirical Methods in Natural Language Pro-cessing, EMNLP 2021, Virtual Event / Punta Cana,Dominican Republic, 7-11 November, 2021, pages54845495. Association for Computational Linguis-tics. Yunhao Gou, Zhili Liu, Kai Chen, Lanqing Hong, HangXu, Aoxue Li, Dit-Yan Yeung, James T. Kwok, andYu Zhang. 2023. Mixture of cluster-conditional loraexperts for vision-language instruction tuning. CoRR,abs/2312.12379.",
  "Suchin Gururangan, Margaret Li, Mike Lewis, Wei-jia Shi, Tim Althoff, Noah A. Smith, and LukeZettlemoyer. 2023. Scaling expert language mod-els with unsupervised domain discovery.CoRR,abs/2303.14177": "Tom Hartvigsen, Swami Sankaranarayanan, HamidPalangi, Yoon Kim, and Marzyeh Ghassemi. 2023.Aging with GRACE: lifelong model editing with dis-crete key-value adaptors.In Advances in NeuralInformation Processing Systems 36: Annual Confer-ence on Neural Information Processing Systems 2023,NeurIPS 2023, New Orleans, LA, USA, December 10- 16, 2023. Thomas Henn, Yasukazu Sakamoto, Clment Jacquet,Shunsuke Yoshizawa, Masamichi Andou, StephenTchen, Ryosuke Saga, Hiroyuki Ishihara, KatsuhikoShimizu, Yingzhen Li, and Ryutaro Tanno. 2021. Aprincipled approach to failure analysis and modelrepairment: Demonstration in medical imaging. InMedical Image Computing and Computer Assisted Intervention - MICCAI 2021 - 24th International Con-ference, Strasbourg, France, September 27 - Octo-ber 1, 2021, Proceedings, Part III, volume 12903 ofLecture Notes in Computer Science, pages 509518.Springer. Edward J. Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. 2022. Lora: Low-rank adaptation oflarge language models. In The Tenth InternationalConference on Learning Representations, ICLR 2022,Virtual Event, April 25-29, 2022. OpenReview.net. Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou,Wenge Rong, and Zhang Xiong. 2023. Transformer-patcher: One mistake worth one neuron.In TheEleventh International Conference on Learning Rep-resentations, ICLR 2023, Kigali, Rwanda, May 1-5,2023. OpenReview.net. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode Las Casas, Florian Bressand, Gianna Lengyel,Guillaume Lample, Lucile Saulnier, Llio Re-nard Lavaud, Marie-Anne Lachaux, Pierre Stock,Teven Le Scao, Thibaut Lavril, Thomas Wang, Timo-the Lacroix, and William El Sayed. 2023. Mistral7b. CoRR, abs/2310.06825. Albert Q. Jiang, Alexandre Sablayrolles, AntoineRoux, Arthur Mensch, Blanche Savary, Chris Bam-ford, Devendra Singh Chaplot, Diego de Las Casas,Emma Bou Hanna, Florian Bressand, GiannaLengyel,Guillaume Bour,Guillaume Lample,Llio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,Sophia Yang, Szymon Antoniak, Teven Le Scao,Thophile Gervet, Thibaut Lavril, Thomas Wang,Timothe Lacroix, and William El Sayed. 2024. Mix-tral of experts. CoRR, abs/2401.04088. James Kirkpatrick, Razvan Pascanu, Neil C. Rabi-nowitz, Joel Veness, Guillaume Desjardins, Andrei A.Rusu, Kieran Milan, John Quan, Tiago Ramalho, Ag-nieszka Grabska-Barwinska, Demis Hassabis, Clau-dia Clopath, Dharshan Kumaran, and Raia Hadsell.2016. Overcoming catastrophic forgetting in neuralnetworks. CoRR, abs/1612.00796.",
  "Suhas Kotha, Jacob Mitchell Springer, and Aditi Raghu-nathan. 2023. Understanding catastrophic forgettingin language models via implicit inference. CoRR,abs/2309.10105": "Matthias De Lange, Rahaf Aljundi, Marc Masana, SarahParisot, Xu Jia, Ales Leonardis, Gregory G. Slabaugh,and Tinne Tuytelaars. 2022. A continual learning sur-vey: Defying forgetting in classification tasks. IEEETrans. Pattern Anal. Mach. Intell., 44(7):33663385. Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettle-moyer. 2017. Zero-shot relation extraction via read-ing comprehension. In Proceedings of the 21st Con-ference on Computational Natural Language Learn-ing (CoNLL 2017), Vancouver, Canada, August 3-4,",
  "Xiaopeng Li, Shasha Li, Shezheng Song, Jing Yang, JunMa, and Jie Yu. 2023b. PMET: precise model editingin a transformer. CoRR, abs/2308.08742": "Bill Yuchen Lin, Sida Wang, Xi Victoria Lin, Robin Jia,Lin Xiao, Xiang Ren, and Scott Yih. 2022. On con-tinual model refinement in out-of-distribution datastreams. In Proceedings of the 60th Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), ACL 2022, Dublin, Ireland,May 22-27, 2022, pages 31283139. Association forComputational Linguistics. Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, SiweiMa, and Wen Gao. 2021. Post-training quantiza-tion for vision transformer. In Advances in NeuralInformation Processing Systems 34: Annual Confer-ence on Neural Information Processing Systems 2021,NeurIPS 2021, December 6-14, 2021, virtual, pages2809228103. Ilya Loshchilov and Frank Hutter. 2019. Decoupledweight decay regularization. In 7th InternationalConference on Learning Representations, ICLR 2019,New Orleans, LA, USA, May 6-9, 2019. OpenRe-view.net.",
  "models. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Process-ing, EMNLP 2023, Singapore, December 6-10, 2023,pages 90049017. Association for ComputationalLinguistics": "Kevin Meng, David Bau, Alex Andonian, and YonatanBelinkov. 2022. Locating and editing factual associ-ations in GPT. In Advances in Neural InformationProcessing Systems 35: Annual Conference on Neu-ral Information Processing Systems 2022, NeurIPS2022, New Orleans, LA, USA, November 28 - Decem-ber 9, 2022. Kevin Meng, Arnab Sen Sharma, Alex J. Andonian,Yonatan Belinkov, and David Bau. 2023.Mass-editing memory in a transformer. In The EleventhInternational Conference on Learning Representa-tions, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.OpenReview.net. Eric Mitchell, Charles Lin, Antoine Bosselut, ChelseaFinn, and Christopher D. Manning. 2022a.Fastmodel editing at scale. In The Tenth InternationalConference on Learning Representations, ICLR 2022,Virtual Event, April 25-29, 2022. OpenReview.net. Eric Mitchell, Charles Lin, Antoine Bosselut, Christo-pher D. Manning, and Chelsea Finn. 2022b. Memory-based model editing at scale. In International Con-ference on Machine Learning, ICML 2022, 17-23July 2022, Baltimore, Maryland, USA, volume 162 ofProceedings of Machine Learning Research, pages1581715831. PMLR. Shikhar Murty, Christopher D. Manning, Scott M. Lund-berg, and Marco Tlio Ribeiro. 2022. Fixing modelbugs with natural language patches. In Proceedingsof the 2022 Conference on Empirical Methods inNatural Language Processing, EMNLP 2022, AbuDhabi, United Arab Emirates, December 7-11, 2022,pages 1160011613. Association for ComputationalLinguistics.",
  "OpenAI. 2023.GPT-4 technical report.CoRR,abs/2303.08774": "Fabio Petroni, Tim Rocktschel, Sebastian Riedel,Patrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,and Alexander H. Miller. 2019.Language mod-els as knowledge bases?In Proceedings of the2019 Conference on Empirical Methods in Natu-ral Language Processing and the 9th InternationalJoint Conference on Natural Language Processing,EMNLP-IJCNLP 2019, Hong Kong, China, Novem-ber 3-7, 2019, pages 24632473. Association forComputational Linguistics. David Rolnick, Arun Ahuja, Jonathan Schwarz, Tim-othy P. Lillicrap, and Gregory Wayne. 2019. Expe-rience replay for continual learning. In Advancesin Neural Information Processing Systems 32: An-nual Conference on Neural Information ProcessingSystems 2019, NeurIPS 2019, December 8-14, 2019,Vancouver, BC, Canada, pages 348358.",
  "Chenmien Tan, Ge Zhang, and Jie Fu. 2023. Massiveediting for large language models via meta learning.CoRR, abs/2311.04661": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Aurlien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. 2023a. Llama: Openand efficient foundation language models. CoRR,abs/2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurlien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023b. Llama 2: Open foundation andfine-tuned chat models. CoRR, abs/2307.09288. Frederik Truble, Anirudh Goyal, Nasim Rahaman,Michael Curtis Mozer, Kenji Kawaguchi, YoshuaBengio, and Bernhard Schlkopf. 2023. Discretekey-value bottleneck. In International Conferenceon Machine Learning, ICML 2023, 23-29 July 2023,Honolulu, Hawaii, USA, volume 202 of Proceedingsof Machine Learning Research, pages 3443134455.PMLR. Aron van den Oord, Oriol Vinyals, and KorayKavukcuoglu. 2017. Neural discrete representationlearning. In Advances in Neural Information Pro-cessing Systems 30: Annual Conference on NeuralInformation Processing Systems 2017, December 4-9,2017, Long Beach, CA, USA, pages 63066315.",
  "Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessan-dro Sordoni, Adam Trischler, Andrew Mattarella-": "Micke, Subhransu Maji, and Mohit Iyyer. 2020. Ex-ploring and predicting transferability across NLPtasks. In Proceedings of the 2020 Conference onEmpirical Methods in Natural Language Processing,EMNLP 2020, Online, November 16-20, 2020, pages78827926. Association for Computational Linguis-tics. Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, YunzhiYao, Yong Jiang, Pengjun Xie, Fei Huang, and Hua-jun Chen. 2024. Wise: Rethinking the knowledgememory for lifelong model editing of large languagemodels. arXiv preprint arXiv:2405.14768.",
  "Zihan Yao, Yu He, Tianyu Qi, and Ming Li. 2024. Scal-able model editing via customized expert networks.CoRR, abs/2404.02699": "Qinyuan Ye, Juan Zha, and Xiang Ren. 2022. Elicitingand understanding cross-task skills with task-levelmixture-of-experts. In Findings of the Associationfor Computational Linguistics: EMNLP 2022, AbuDhabi, United Arab Emirates, December 7-11, 2022,pages 25672592. Association for ComputationalLinguistics. Jaehong Yoon, Saehoon Kim, Eunho Yang, and Sung JuHwang. 2020. Scalable and order-robust continuallearning with additive parameter decomposition. In8th International Conference on Learning Represen-tations, ICLR 2020, Addis Ababa, Ethiopia, April26-30, 2020. OpenReview.net. Ted Zadouri, Ahmet stn, Arash Ahmadian, BeyzaErmis, Acyr Locatelli, and Sara Hooker. 2023. Push-ing mixture of experts to the limit: Extremely pa-rameter efficient moe for instruction tuning. CoRR,abs/2309.05444. Ningyu Zhang, Yunzhi Yao, Bozhong Tian, PengWang, Shumin Deng, Mengru Wang, Zekun Xi,Shengyu Mao, Jintian Zhang, Yuansheng Ni, SiyuanCheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang,Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang,Xiaowei Zhu, Jun Zhou, and Huajun Chen. 2024. Acomprehensive study of knowledge editing for largelanguage models. CoRR, abs/2401.01286. Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, ZhiyongWu, Jingjing Xu, and Baobao Chang. 2023. Can weedit factual knowledge by in-context learning? InProceedings of the 2023 Conference on EmpiricalMethods in Natural Language Processing, EMNLP2023, Singapore, December 6-10, 2023, pages 48624876. Association for Computational Linguistics. Wangchunshu Zhou,Canwen Xu,and Julian J.McAuley. 2022. Efficiently tuned parameters aretask embeddings. In Proceedings of the 2022 Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP 2022, Abu Dhabi, United ArabEmirates, December 7-11, 2022, pages 50075014.Association for Computational Linguistics.",
  "A.1Model Editing": "Model editing is a new and active research areawhere the goal is to make targeted changes to apre-trained models behavior (Zhang et al., 2024).Given the fast-growing parameter sizes of LLMs,frequently updating LLMs with new knowledgethrough retraining is more and more expensive.Hence, it is vital to effectively edit the LLMsknowledge without retraining. Previous studieshave explored multiple methods for editing theknowledge of LLMs, which can be broadly catego-rized into two streams based on whether it altersthe parameters of the original model (Yao et al.,2023; Zhang et al., 2024): Preserve model parameters:(1) Retrieve aug-mentation. These techniques leverage an exter-nal knowledge base to enrich or correct informa-tion accessible to language models. These aug-mented knowledge bases seamlessly integrate withthe base model, enabling effective retrieval of rel-evant information when prompted (Murty et al.,2022; Madaan et al., 2022; Li et al., 2023a). Forexample, IKE (Zheng et al., 2023) employs an in-context learning approach that adjusts languagemodel outputs using corpus-based demonstrations guided by similarity metrics, thereby obviating theneed for gradient-based adjustments. (2) Addingadditional parameters: This paradigm involvesintroducing additional trainable parameters to aug-ment a language models existing knowledge, whilepreserving its original parameters in a frozen state.T-Patcher (Huang et al., 2023) and CaliNET (Donget al., 2022) exemplify this paradigm by integratingspecific neurons or patches into the final layer oftheir Feed-Forward Networks. T-Patcher assignsindividual neurons to each distinct error, while Ca-liNET incorporates multiple neurons to handle var-ious knowledge scenarios. In contrast, GRACE(Hartvigsen et al., 2023) employs a discrete code-book mechanism to dynamically add and updateelements, enhancing the models predictive capa-bilities over time. (3) Meta learning Recent meta-learning methods use hypernetworks for aidingediting. MEND (Mitchell et al., 2022a) introducesa hypernetwork designed to decouple fine-tuninggradients into updates that generalize edits with-out compromising performance on unrelated in-puts. To mitigate the cancellation issue inherentin MEND, MALMEN (Tan et al., 2023) employsa hyper-network to generate weight shifts for edit-ing, formulating the aggregation of these shifts asa least squares problem. Modify model parameters:This methodologybegins by identifying parameters associated withspecific knowledge and directly adjusting them.The Knowledge Neuron (KN) approach (Dai et al.,2022) introduces a technique to attribute knowl-edge to individual \"knowledge neurons\" and subse-quently updates these neurons accordingly. ROME(Meng et al., 2022) utilizes causal mediation analy-sis to pinpoint areas requiring modification. BothKN and ROME operate under the constraint of edit-ing one factual association at a time. To overcomethis limitation, MEMIT (Meng et al., 2023) extendsROMEs framework, enabling simultaneous edit-ing across multiple instances. Building on MEMIT,PMET (Li et al., 2023b) integrates attention val-ues to achieve superior performance enhancements.COMEBA-HK (Li et al., 2024b) identifies the Lo-cal Editing Scope and extends MEMIT for sequen-tial editing.",
  "The concept of MoE, particularly when combinedwith sparse routing, is recognized for significantlyenhancing model capacity with minimal computa-": "tional overhead (Fedus et al., 2022). Key distinc-tions in this approach include: i) adapter expertsare not trained during the pre-training of the basemodel, ii) they are parameter-efficient, and iii) theyare tailored to specific tasks, unlike token-levelopaque computation units whose specialization isnot easily interpretable (Jiang et al., 2024). Regard-ing the second point, (Wang et al., 2022; Zadouriet al., 2023) utilize routing each example to a setof experts, demonstrating improved performanceon unseen tasks. (Gupta et al., 2022) implementsa separate router for each task and selects a routerfrom a similar task based on domain knowledge.(Ye et al., 2022) proposes task-level MoEs, wherea collection of transformer layers acts as experts,and a router dynamically selects from these ex-perts. Additionally, several recent studies haveproposed methods for routing queries to special-ized pretrained open-source LLMs (Lu et al., 2023;Shnitzer et al., 2023).",
  "A.3Continual Learning": "Continual Learning (CL) (Shi et al., 2024; Wuet al., 2024a) is an essential aspect of machinelearning as it enables models to adapt to new taskswhile retaining performance on previous ones. Itmainly focus on the issue of catastrophic forget-ting in deep learning models when exposed tonew knowledge (Lange et al., 2022). Recent re-search has explored diverse approaches in this do-main. Among these approaches, continual fine-tuning stands out, involving the iterative refinementof LLMs with incoming instances. For instance,(Lin et al., 2022) conducts an extensive investiga-tion into this method. However, it has been notedthat integrating regularized fine-tuning techniquessuch as Elastic Weight Consolidation (Kirkpatricket al., 2016), Experience Replay (Rolnick et al.,2019), and Maximally Interfered Replay (Aljundiet al., 2019) can lead to a decline in performanceon earlier tasks while preserving some memoryof past inputs. This observation underscores thechallenges unique to editing in contrast to conven-tional continual fine-tuning (Henn et al., 2021),particularly given the uneven distribution of ed-its. One promising avenue in continual learninginvolves the adoption of key-value methodologies,inspired by advancements in computer vision (Liuet al., 2021; van den Oord et al., 2017). Notably,discrete key-value methods have proven effectivein managing shifting distributions (Truble et al.,2023). These methods cache values to ensure in-",
  "A.4Data Clustering for LLMs": "Data clustering methods for LLMs have been pro-posed to enhance performance and reduce task in-terference (Fifty et al., 2021; Gururangan et al.,2023; Gou et al., 2023). These methods includeclustering based on similarities computed using tf-idf and neural embeddings, K-means clusteringwith balanced linear assignment, and soft clus-tering with Gaussian Mixture Models (GMMs)(Chronopoulou et al., 2023; Gururangan et al.,2023; Duan et al., 2021). Recent work by (Zhouet al., 2022) highlights the potential of adapter pa-rameters as effective task embeddings for cluster-ing. Additionally, a similar observation regardingtask gradients has been made by (Vu et al., 2020).",
  "B.1Datasets Details": "ZsREThe ZsRE dataset is a context-free Ques-tion Answering (QA) dataset that has been ex-tensively studied in the model editing literature(Meng et al., 2022, 2023; Mitchell et al., 2022b;Hartvigsen et al., 2023; Wang and Li, 2024). Eachrecord in this dataset includes an editing statementxei with target answer yei , a paraphrase promptxgeni and and a locality prompt xloc. We adoptthe same train/test split as (Mitchell et al., 2022a),consisting of 163,196 training examples and 19,086test examples. Notably, MEND is the only methodthat requires fitting a hyper network on the trainingset; other methods discard the training set and di-rectly perform edits and evaluations on the test set.For our experiments, we randomly sampled 1k and3k records from the test set to form the edit sets. SelfCheckGPTWeemploySelfCheckGPT(Manakul et al., 2023),the same dataset as GRACE,to evaluate the effectiveness of Model Editors inreducing hallucinations in autoregressive languagemodels. This dataset consists of highly inaccuratesentences generated from GPT-3 (Brown et al.,2020), which are then replaced with correspondingaccurate sentences from Wikipedia. This setupmirrors real-world deployment scenarios wheremodels exhibit \"unexpected behaviors\". The editsin this dataset are significantly longer comparedto ZsRE, presenting a more challenging editing environment.Unlike GRACE, which utilizedGPT2-XL (1.5B), our primary experiments uselarger LLMs, specifically LLaMA and Mistral,each with 7B parameters.We measure theretention of xloc from the base model, RedPajama(Computer, 2023), a publicly available version ofLLaMAs pre-training data.",
  "B.2Implementation of Baselines": "FT-LWe followed the procedures outlined in(Wang et al., 2024): all other layers of the LLMsremain frozen, and only a single MLP layer un-dergoes fine-tuning using an autoregressive lossfunction. Furthermore, we impose a L norm con-straint to ensure that the parameters do not devi-ate significantly from the pretrained distribution.Employ the Adam optimizer with considerationof learning rates at 1e-5, 1e-4, and 5e-4, and con-duct gradient descents for 50 iterations, ultimatelyreporting the best results at a learning rate of 5e-4. FT-EWCElastic Weight Consolidation (EWC)effectively mitigates catastrophic forgetting by up-dating model weights using the Fisher informationmatrix, which is computed based on past parame-ter updates and scaled by a factor (Kirkpatricket al., 2016). In line with (Hartvigsen et al., 2023),our implementation does not incorporate L normconstraints, setting the learning rate at 1e-2, theewc penalty factor at 0.1, and the number of re-play instances at 10. MENDMEND (Mitchell et al., 2022a) performsmodel editing by employing a hyper-network totransform the gradients derived from standard fine-tuning. This process involves decomposing themodel gradients into a low-rank format (rank=1)before converting them into new gradients, whichare subsequently applied to the target layer for pa-rameter updates. During training, a small auxiliaryhyper-network processes editing examples (xei, yei )and (xgeni, yei ). The training loss for MEND con-sists of the standard autoregressive loss combinedwith the KL divergence loss, measuring the modelsoutput on (xgeni, yei ) before and after editing. Thishyper-network is pivotal in the editing procedure.Due to the substantial computational resources re-quired to train the meta-network for, the results arefrom (Wang et al., 2024).",
  "SelfCheckGPT": "xei, yeiThis is a Wikipedia passage about heinz christian pander. Heinz Christian Pander(1794 - 1865) was a German anatomist and embryologist who was born in Riga, Latvia.He studied medicine at the University of Dorpat and later at the University of Berlin.In 1820, he took part in a scientific expedition to Bokhara as a naturalist.xloc, ylocTired and restlessly, drifting in and out of sleep. Hearing crashing and banging,thinking the roof will cave in. Not alert enough to quite know what. it was, I yelledloudly for whoever was making those noises at such an hour to stop. They heardand listened, Im guessing",
  "QAZsRE1,0000.36/0.39 ACCHallucinationSelfCheckGPT60027.4/19.4 PPL": "via least squares approximation. This approach as-sumes MLP as the central repository of knowledge(Geva et al., 2021), incrementally injecting individ-ual pieces of information into the MLP through aLagrangian residual term at each iteration. Follow-ing (Wang et al., 2024), in LLaMA and Mistral,ROME edits the fifth layer, while MEMIT editslayers . MEMITThe MEMIT utilized in this study, de-noted as MEMIT-MASS as introduced in (Wanget al., 2024), differs notably from its original coun-terpart. In contrast to sequential editing, MEMIT-MASS facilitates batch processing for modifyingmultiple knowledge fragments concurrently. Sup-pose we collect streaming errors as (X, Y) ={(x0, y0), (x1, y1), ..., (xT , yT )} and inject themcollectively into the MLP, it only involves a sin-gle editing operation on the original model asfT = MEMIT(f0, X, Y). Despite its drawbackof lacking real-time correction capabilities, we in-clude this approach as a baseline in our experi-mental evaluations, given the extremely bad perfor-mance of the original MEMIT framework.",
  "DEFERIn GRACE, a reimplementation ofSERAC (Mitchell et al., 2022b) is utilized, denotedas DEFER. DEFER integrates a network denoted asg (akin to the scope classifier in SERAC). This net-": "work g predicts whether to rely on: 1) predictionsfrom the LLMs, or 2) predictions from a newlyintroduced model. This new model, configured asa single-layer linear network o with a sigmoid acti-vation function, parallels the counterfactual modelin SERAC. Throughout the editing phase, g ando undergo joint fine-tuning processes. The exper-iment with learning rates of 7e-5, 7e-4, and 1e-3,and ultimately report using 7e-5 (optimal). GRACEGRACE (Hartvigsen et al., 2023) uti-lizes a discrete key-value codebook and main-tains the codebook throughout the editing flow byadding, expanding, and splitting KEYs. During theinference phase, it retrieves the nearest KEY anddetermines whether to replace the activation of thehidden layer output. We adhere to the meticulouslycrafted parameters outlined in the original study,configuring the optimization of the learning rate toa value of 1 and using replace last to only replacethe activation of the last token in autoregressive sce-narios.. The iterative process for optimizing thesevalues spans 100 cycles, with an initial = 1. MEMoEMEMoE (Wang and Li, 2024) updatesknowledge using a bypass MoE structure, keepingthe original parameters unchanged to preservethe general ability of LLMs. And, the knowledgeanchor routing ensures that inputs requiring similarknowledge are routed to the same expert, therebyenhancing the generalization of the updatedknowledge. Following the parameters identifiedin the original paper, we consulted 4 experts,setting the top k value to 1 and a learningrate of 2e-4.The modification is applied tomodel.layers.mlp.up_proj.weightandmodel.layers.mlp.down_proj.weight.",
  "(xt,yt)Ttlog Pyt | xt; m, f, proj, k": "(13)where m, f, proj and k are parameters of theLLM backbone, the experts, the query projectionlayer and the set of all key vectors, respectively.And only those parameters belongs to the currentt-th task are trainable, including ft, proj and kt.The hyperparameters for the ZsRE and Self-CheckGPT are identical.Specially, We usethe AdamW (Loshchilov and Hutter, 2019)as the optimizer with a learning rate of 2e-4.The modification of the model is applied tomodel.layers.mlp.up_proj.weightandmodel.layers.mlp.down_proj.weight.All the experiments are deployed on NVIDIA RTX3090 Tensor Core GPUs, and we use 4 GPUsfor training and single GPU for evaluation. Forlifelong editing, due to computational constraints,we can accommodate a maximum of 5 experts.Consequently, the batch size in the sequence isdetermined by the total number of edits and thenumber of experts. For instance, when there are100 edits and 5 experts, the batch size is set to 20;whereas with 1000 edits, the batch size scales up to200. For bath editing in 6.1, the batch size is 30and all the other parameters are the same as above.",
  "B.4Implementation for Ablation Study": "In 6.3, we conducted an ablation study on severalmodules of LEMoE. Here, we detail the implemen-tation of these ablations. In , Conventionalrouting means the router is modeled by a single-layer MLP, with the preservation of the insertionmethod. Knowledge routing is the knowledge an-chor routing in MEMoE for short, also maintainsthe insertion method. Token-level embedding in-volves substituting ej in Equation 12, which meansg(i | ej) = g(i | xjti . For entity-level embedding,we initially utilize the NLTK tool 1 for extractingnamed entities from the input instance. In caseswhere there are multiple named entities presentin the input, we utilize the average pooling of theembeddings of these entities. Subsequently, we replace ej in Equation 10 with this embedding vec-tor as the input to the sub-network to obtain thecorresponding value of the input instances. Allthe other training hyperparameters are the samedetailed in Appendix B.3.",
  "C.1More results for Influencing Factors": "In 3.1, we employed two different evaluationmethods: (1) a standard evaluation conducted onall edited data only after all edits were completed,and (2) an evaluation conducted immediately aftereach edit to assess the effectiveness of these editsat the current stage. shows the variationsin the reliability metrics, and we further providethe changes in all three metrics here. To betterillustrate these trends, we averaged the metricsover every four steps in a sequence of 100 edit-ing steps. As shown in , both reliabilityand generalization exhibit catastrophic forgettingphenomenon, where subsequent edits significantlyaffect the performance on prior data. This effectis most pronounced in the reliability metric. Addi-tionally, around step 80, minimal fluctuations in thecurrent editing reliability result in substantial os-cillations in generalization. This can be attributedto the fact that, like human, a model must first ac-curately learn knowledge before it can generalizethat knowledge. Thus, the generalization metricis, to some extent, contingent upon reliability. Re-garding locality, the overall level remains consis-tently high, above 0.97, and thus the graph showsno discernible pattern of fluctuations. This furthercorroborates that knowledge editing through by-pass mechanisms minimally impacts the modelsgeneralization capability (Wang and Li, 2024).",
  "C.2Case Study": "In , we present bad cases of using LEMoEto edit the LLaMA-2-7B on the ZsRE dataset andmitigating these failures is critical for future workin model editing. We observe that:i) errors occur only in part of the tokens, andthese errors constitute a large proportion of thebad cases, indicating that the edits have not beensufficiently fitted. We wonder whether employingdifferent learning rates and epochs for each batchin lifelong editing could alleviate this issue throughmore refined training.ii) displays cases where the entire output is in-correct. These types of errors are the most common",
  "occurrences": "iv) presents cases of generalization failure. Forexample in prompt of last line, where the modelanswered 1942 which is partially correct, butdid not fully follow the ground truth, indicatingsignificant room for improvement in the accuracyof generalized edits. Meanwhile, in iii) we surprisingly find that evenwhen LEMoE errs on the Edit Prompt, it can cor-rectly answer its paraphrase prompt. Upon closelyexamining these anomalous cases, we found thatthey predominantly pertain to question-answeringscenarios within sports contexts, such as inquiriesabout a persons team affiliation. We hypothesizethat this phenomenon may stem from the relatively limited number of teams in sports contexts, com-bined with the higher number of athletes and theoccurrence of name duplication. Consequently, themodel may accidentally provide correct answers tosome of these questions.In summary, LEMoE can handle contextual in-formation correctly in some cases but falls short inspecific editing instructions, suggesting that opti-mizing editing instructions (modifying the editingcontext) may be a direction for improvement.",
  "C.4LEMoE with LoRA structure": "In the era of LLMs, parameter-efficient fine-tuning(PEFT) methods such as LoRA have proven highlyeffective and convenient for achieving impressiveresults across various downstream tasks. LoRA(Hu et al., 2022), proposes a technique that decom-poses the update gradient matrix into two smallrank-n matrices, significantly reducing the memory requirements for training LLMs. Meanwhile, infields of MoE, some studies have explored replac-ing traditional MoE structures with LoRA (Zadouriet al., 2023; Wu et al., 2024b). Consequently, wereplace the MLP-based expert networks in LEMoEwith LoRA modules. Given the challenging natureof lifelong learning tasks, we evaluate the perfor-mance of this low-parameter model structure onbatch editing tasks with batch size set to 30.We investigated the effects of varying the num-ber of experts (Exp.), different LoRA ranks, anddifferent topk values. Detailed experimental re-sults are provided in the to facilitatefurther research.We conducted experimentson all even-numbered layers, expert number in, topk in and LoRA Rank in. We filterout results with Reliability below 0.1, Generaliza-tion below 0.1, and Locality below 0.5. It is evidentthat this method performs poorly in editing the low-level transformer blocks (with results falling belowthe selection criteria and many being zero, hencenot presented in the table). Meanwhile, the higherthe layer being edited, the better the performanceobserved. This LEMoE-LoRA achieved optimalperformance with 30 layers, 10 experts, a LoRArank of 2048, and a topk value of 10."
}