{
  "Abstract": "A multimodal large language model (MLLM)may struggle with answering visual-based (per-sonal) entity questions (VEQA), such as who isA? or who is A that B is talking to? forvarious reasons, e.g., the absence of the nameof A in the caption or the inability of MLLMsto recognize A, particularly for less commonentities. Furthermore, even if the MLLM canidentify A, it may refrain from answering dueto privacy concerns. In this paper, we introducea novel method called Matching-AugmentedReasoning (MAR) to enhance VEQA. Given a col-lection of visual objects with captions, MAR pre-processes each object individually, identifyingfaces, names, and their alignments within theobject. It encodes the information and storestheir vector representations in the database.When handling VEQA, MAR retrieves matchingfaces and names and organizes these entitiesinto a matching graph. MAR then derives theanswer to the query by reasoning over thismatching graph. Extensive experiments showthat MAR significantly improves VEQA comparedwith the state-of-the-art methods using MLLMs.1",
  "Introduction": "Multimodal language models (MLLMs) (Cui et al.,2024) like GPT-4V (Zhang et al., 2023a) andLLaVA (Liu et al., 2023) have significantly im-proved visual question answering (VQA) by inte-grating text and images. However, they still facechallenges in visual-based entity question answer-ing (VEQA), a crucial subset of VQA that focuses onextracting information about specific entities (Qiuet al., 2024; Chen et al., 2023a).",
  ": Data (V : image, T : text) pair; Query (R :entity selection, Q : question) pair. (a) The advantagesof MLLMs; (b) The limitations of MLLMs, and (c) Ourproposal MAR": "2024b). For instance, as depicted in (a),GPT-4V, when tasked with answering question Q1regarding the face in region R1, leverages the asso-ciated caption T1 of image V1 to precisely identifythe person within the red box as Wang Yi. However, MLLMs often struggle to recognize alldetails in images, particularly for less commonentities (Li et al., 2023b; Sun et al., 2024; Yanget al., 2024; Wu et al., 2024). For instance, in(b), GPT-4V fails to answer question Q2about the person in the red rectangle R2 due to thelack of information in the image caption T2 and itslimited knowledge base. Furthermore, even whenan MLLM identifies an entity, it may withhold ananswer due to privacy regulations. Despite rapid advancements of MLLMs, accu-rately identifying all personal entities in imagesand adhering to privacy regulations make answer-ing VEQA questions solely using MLLMs a signifi-cant challenge (Chen et al., 2024; Li et al., 2023a,2024b; Yu et al., 2023; Qin et al., 2022). Matching-Augmented Reasoning (MAR). Given acollection of visual objects with captions, sourcedfrom public or enterprise datasets without privacyconcerns, MAR identifies the faces of entities withinvisual objects and the names of entities within cap-tions by tools like CLIP (Radford et al., 2021) andDeepface (Taigman et al., 2014). These entities areencoded with respective visual and text encoders,and the resulting embeddings are stored in vec-tor databases e.g., Meta Faiss (Douze et al., 2024).When a VEQA query is posed, MAR retrieves similarfaces and names from the database and performsreasoning over these matched pieces of informationto generate an accurate response.Existing work on VEQA (Chen et al., 2023a; Huet al., 2023; Qiu et al., 2024) mainly focuses ongeneral entities such as animals buildings, and ve-hicles. However, there is a lack of work targetingpersonal entities. As illustrated in (c), ifwe can match the face in image V2 with the face inimage V1, and if we know that the face in V1 is YiWang, we can answer Q2.",
  "We conduct extensive experiments to showthat MAR > MLLMs + RAG > MLLMs, whereRAG is to feed the retrieved matching graphto MLLMs": "The structure of our paper is organized as fol-lows: introduces the limitations of usingMLLMs to answer visual questions and proposes theVEQA task. reviews related work on theVEQA task. In , we provide a detaileddescription of VEQA. is dedicated to pre-senting our approach, MAR, for addressing this task. presents the benchmark NewsPersonQAwe proposed, and describes extensiveexperiments conducted to validate our approach.Finally, summarizes the findings and con-tributions of our paper.",
  "Visual Question Answering (VQA)": "VQA aims at reasoning over visual and textualcontent and cues to generate answers (Lu et al.,2021; Stengel-Eskin et al., 2022; Agrawal et al.,2023). It primarily utilizes approaches such asFusion-based (Zhang et al., 2019), MultimodalLearning (Ilievski and Feng, 2017), Memory Net-works (Su et al., 2018), Visual Attention (Maheshet al., 2023), etc., to discover and integrate infor-mation from text and images.",
  "Retrieval-Augmented Generation (RAG)for VQA": "In many cases, the cues within images and text areinsufficient for reasoning and answering. Retrieval-augmented generation (RAG) (Lewis et al., 2021;Chen et al., 2023b; Li et al., 2024a; Liu et al.,2024a) has been studied for VQA, especially withKnowledge-Based VQA approaches that incorpo-rate external knowledge to provide additional cuesfor answers (Khademi et al., 2023; Shah et al.,2019).",
  "Data Matching": "This involves identifying, comparing, and mergingrecords from multiple datasets to determine dupli-cate entities (Tu et al., 2023; Ebraheem et al., 2018;Xie et al., 2024). With increasing data multimodal-ity, matching has expanded from string matching(Text-Text) and entity matching (Tuple-Tuple) to in-clude Image-Text (Li et al., 2019; Mai et al., 2023;Zhang et al., 2023b) and Image-Image (Zhu et al.,2018) matching. Matching aggregates clues, en-hances model reasoning, and offers strong inter-pretability (Zheng et al., 2022).",
  "Visual-based Entity QuestionAnswering (VEQA)": "Captioned Visual Objects. We consider a cap-tioned visual object O as a pair O : (V, T) whereV is an image, and T is an optional text descriptionrelative to the image V .(a) and (b) provide two sam-ple captioned visual objects, (V1, T1) and (V2, T2),respectively.Let O = {O1, O2, . . . , On} be a group of cap-tioned visual objects, sourced from public or en-terprise datasets without privacy concerns. Notethat, such a group is common in practice, e.g., acollection of news articles. VEQA. Users can pose a Visual-based Entity Ques-tion Answering (VEQA) queries related to personentities on either a single captioned visual object(Single-VEQA) or a group of such objects (Group-VEQA). Single-VEQA. Given a captioned visual objectO : (V, T), this type of queries allows the userto provide a rectangle selection of the image andask the question like who is he/she.More formally, a Single-VEQA Qs is a pair (R, Q),where R is a rectangle selection over image V andQ is a natural language question.",
  "Matching Graphs": "To improve the performance of RAG models, itsbeneficial to focus on fine-grained informationrather than entire objects. By identifying specificentities (e.g., faces, names) and their connectionswithin each object, we can provide a more mean-ingful context for reasoning. Matching Graphs. A matching graph G(N, E)contains a set N of nodes and a set E of undirectededges. Each node n N has two labels face(n)and name(n), where face(n) is a face image, andname(n) is a set of possible names.",
  ": Different algorithms for VEQA. (a) MLLMs. (b)Coarse-grained RAG. (c) Fine-grained RAG": "If we are certain about a persons name, we willuse a square bracket e.g., name(n) = [Yi Wang]for the selected face in (a); if we are notsure about a persons name, we will use a curlybracket to indicate possible names e.g., name(n) ={Xi Jinping, Trump, *} for the selected face in Fig-ure 1(b), where is a wildcard meaning that nsname could be something other than Xi Jinping andTrump.Each undirected edge e(ni, nj)E indi-cates that the two faces corresponding to ni (i.e.,face(ni)) and nj (i.e., face(nj)) are likely tobe the same person.Each edge has a weightweight(e) , indicating the similarity of thetwo faces.",
  "[Step 1: Initialization.] The user starts with a seednode (for Single-VEQA) or a group of seed nodesfor (Group-VEQA). Each seed node contains a faceand its candidate names that could be empty": "[Step 2: Graph Expansion.] For each node in thegraph, we search either similar faces from faceDBwith vector similarity above a given threshold f,or similar names from nameDB with vector similar-ity above a given threshold n. For each addednode, the edge weight is set as face similarity. [Step 3: Iterative Search and Termination.] Whenthere are new nodes added in Step 2, we will loopStep 2. The process terminates when either thereis no new nodes can be added or we have donek iterations. From our empirical findings, we setk = 2, which is enough to retrieve useful nodes(e.g., 10 nodes ) and edges for reasoning.",
  "A New NewsPersonQA Benchmark": "The problem of VEQA needs to address complexinteractions between multiple visual and textualdata.Despite its growing importance, existingbenchmarks fall short in adequately representingthe diverse challenges posed by VEQA tasks. Par-ticularly in the domain of News QA, where theaccurate identification and understanding of bothcommon and uncommon persons are crucial, cur-rent datasets (e.g., GoodNews (Biten et al., 2019)and NewsQA (Trischler et al., 2016)) do not pro-vide the necessary depth and breadth. To bridgethis gap, based on GoodNews (Biten et al., 2019),we are constructing a new benchmark, namelyNewsPersonQA, that encompasses a wide range ofscenarios, including both well-known and obscureindividuals.",
  "The construction of the dataset": "The construction of the dataset entails the genera-tion of QA pairs from the raw data in GoodNews,which consists of images and captions. This pro-cess involves two main steps: data preprocessingand QA pair construction. Data Preprocessing: Raw data undergoes prepro-cessing, which includes structuring news data, ex-tracting faces from images, annotating original im-ages, and recognizing named entities in captions.The processed data is then randomly distributedinto groups. Each group contains thousands ofimages and is categorized into Single-VEQA (100groups) and Group-VEQA (10 groups) queries. Single-VEQA Question Generation: We beginby counting the frequency of each persons namewithin each group. To ensure the availability ofclues for answering, we select names that appear atleast three times in captions. We then mask thesenames in the captions to generate QA pairs. Forexample: Question: Who is the person labeledface n in the red box? Answer: name. Intotal, approximately 5,000 queries of this type aregenerated, about 50 per group. Group-VEQA Question Generation: Similarly,we count the occurrences of names within eachgroup and store the image names as a set, de-noted as S. To prevent exceeding the maximumtoken limit of MLLMs in the answers and to facil-itate clearer visualization of experimental results,we limit each persons name to a maximum of5 appearances within the same group. We thenrandomly mask part of the captions correspond-ing to the images in the set to increase the dif-ficulty and encourage MLLMs to generate correctanswers through retrieved content. The format ofQA pairs is Question: \"Which photos are of theperson named name?\" Answer: S. The number",
  "Comparison between Existent VEQADatasets and NewsPersonQA": "In recent years,numerous VEQA datasetsand methods have been developed, includingOVEN (Hu et al., 2023), INFOSEEK (Chen et al.,2023a), and SnapNTell (Qiu et al., 2024). Ourdiscussion primarily focuses on these works. Different Types of Entities: These works mainlyfocus on general entities, such as buildings, ani-mals, and vehicles, and do not address personalentities. Person entities are an important type ofentity. However, due to privacy policies and otherreasons, some MLLMs (such as GPT-4V, Claude,etc.) cannot directly answer questions related toperson entities, thus leaving a gap that needs to befilled. Different Dataset Division Structures: Previousworks primarily aim to enable models to learn rele-vant knowledge through training and then performtesting. Therefore, their datasets are divided intotraining, validation, and test sets. Unlike them, ourwork aims to assist VEQA by allowing the modelto find relevant clues in the database through azero-shot approach. Thus, our dataset is dividedbased on the database, and the model is tasked withfinding clues within a specific database.",
  "MAR70.85%": "+ FRAG: MLLMs struggle with reasoning overcoarse-grained RAG that consists of multiple cap-tioned visual objects. Therefore, we provide onlyfine-grained RAG (FRAG), i.e., matching graph,to the above-mentioned models and human evalua-tors. Implementation.The experiments were con-ducted in a zero-shot setting using RTX 4090 GPUs.For GPT-4V, we used the interface of the GPT-4-vision-preview model. Its worth noting that GPT-4V often refrains from answering person identifyquestions without additional clues due to policyreasons. However, with the incorporation of match-ing graph techniques, it can leverage weak signalsand combine them with its own knowledge base. Inthe case of Group-VEQA queries, a maximum of 10cases are recalled and then filtered for subsequentprocessing. Metrics. For Single-VEQA queries, we use accuracy(Acc) as an evaluation metric. Furthermore, weassess the accuracy only for instances where rele-vant clues are successfully retrieved (e.g., the caseof (c)), which is denoted as Acchit. ForGroup-VEQA queries, we employ recall (Recall) asthe metric.",
  "The main results from the Single-VEQA queries aresummarized in , which leads to the follow-ing insights:": "1. Model Parameter Size: LLaVA-13b demon-strates higher accuracy (27.93%) compared toLLaVA-7b (22.26%), suggesting that a modelsrecognition ability is positively correlated with itsparameter size, which to some extent reflects itsknowledge base. 2. Impact of Matching Graph: Incorporating amatching graph leads to an 8.9% improvement inaccuracy for LLaVA-7b and a 3.2% improvementfor LLaVA-13b. GPT-4V, with matching, achievesa character recognition accuracy of 34.83%. 3. Comparative Improvement: The enhancementfrom matching is more pronounced for LLaVA-7bthan for LLaVA-13b, indicating that while match-ing can compensate for differences in parameters, amodels inherent capabilities still set an upper limiton its performance.To further understand the impact of matchingon the models reasoning abilities, we analyzedexamples of successfully recalled clues:",
  "Group-VEQA Queries": "Group-VEQA queries focus on identifying all perti-nent clues for more reliable reasoning. The resultis shown in .Our method achieves the highest recall rate at70.85%, outperforming GPT-4V, LLaVA-7b, andLLaVA-13b combined with matching by 5.81%,30.81%, and 48.79%, respectively. This indicatesthat our approach excels in retrieval tasks comparedto MLLMs, likely due to the effectiveness of rule-based methods in managing excessive information.Additionally, the performance of baseline MLLMsdiminishes with reduced parameter sizes, suggest-ing a positive correlation between their analyticalreasoning abilities and parameter sizes.",
  "The Influence of Multi-Source Info": "In principle, the effective recognition of personalinformation by a model depends on three mainsources: its inherent knowledge, clues from thequery, and clues from retrieved data. Our FRAGframework leverages these sources to guide accu-rate answers. As demonstrated in , whenrecall is accurate, LLaVA-7b correctly answers42.86% of cases post-FRAG, while LLaVA-13bachieves 39.18%.However, in practice, the presence of noise in therecalled information and the potential inability ofMLLMs to effectively integrate FRAG informationwith the models original knowledge may lead toincorrect answers. As shown in , LLaVA-7b+FRAG and LLaVA-13b+FRAG respectivelyprovide incorrect answers in 7.32% and 9.44% ofcases that could have been answered correctly be-fore FRAG.To assess the impact of the prompt on themodels original knowledge, we conducted ablationexperiments by removing the Original-knowledge-aware Prompt (OP), as shown in . The",
  "Analysis of Experimental Results forCommon and Uncommon Entities": "1. Name Distribution. We have tallied the fre-quency of names that appear four times or more inthe original news files of the NewsPersonQA dataset.As shown in and , it is evident thatthe dataset contains head-torso-tail entities, with torso-tail entities being less recognizable. We de-fine head entities as those with a frequency greaterthan 50, which are mostly names of famous people;torso entities are those with a frequency between10 and 50, representing a portion of the dataset; andtail entities are those with a frequency less than 10,which make up more than half of the entire dataset. 2. Experimental Results. We further conductedstatistical analysis and evaluation on the experi-mental results presented in .1, specificallyfocusing on the results for common and uncommonentities (as shown in ). Firstly, the perfor-mance of LLaVA-7b and LLaVA-13b indicates thatMLLMs have a stronger recognition ability for com-mon entities, but are less recognizable for torso-tailentities.Secondly, with the addition of fine-grained RAG,LLaVA-7b and 13b showed an improvement of23.68% and 14.78%, respectively, for common en-tities; and an improvement of 47.81% and 44.75%for uncommon entities. For GPT-4V, the additionof FRAG enabled it to respond to person entities,and due to its more powerful recognition and rea-soning abilities, it achieved higher accuracy thanLLaVa. However, by comparison, our method MARdemonstrated optimal performance in detectingboth common and uncommon entities.",
  "Conclusion": "In this paper, we explore a novel visual-based (per-sonal) entity questions (VEQA) problem that focuseson aggregating clues from multiple captioned vi-sual objects. We introduce matching graphs de-signed to capture the relationships between identi-cal entities across various visual objects. Extensiveexperiments demonstrate the high accuracy of ourmethod. While our work has primarily focused onmatching person entities, future research can aimto extend matching-augmented reasoning to othertasks.",
  "Limitations": "Currently, our framework primarily relies on simi-larity for face matching and does not consider fac-tors such as age-related changes and facial blurring.This may result in inaccuracies in matching cer-tain nodes, representing a future research direction.Additionally, in real-world applications, news isdynamic. Efficient retrieval and expansion strate-gies for a growing data lake pose challenges as thedataset evolves, warranting further investigation.",
  "Ethics Statement": "The authors declare that they have no conflict ofinterest. Our work aims to enhance the answergeneration of visual question answering by retriev-ing entity-related clues. While improving the accu-racy of answer generation, our method significantlysaves resources as it does not require fine-tuningof large language models. We strive to ensure thatour approach is not only accurate and efficient butalso fair and unbiased. We recognize the potentialof significant impact of visual question answeringtechnology on society and pledge to maintain trans-parency in sharing our findings and progress withrelevant users and stakeholders.",
  "This paper is supported by NSF of China(62402409), Guangdong Basic and Applied Ba-sic Research Foundation (2023A1515110545),and CCF-Huawei Populus Grove Fund (CCF-HuaweiDB202403)": "Mayank Agrawal, Anand Singh Jalal, and HimanshuSharma. 2023. A review on vqa: Methods, tools anddatasets. In 2023 International Conference on Com-puter Science and Emerging Technologies (CSET),pages 16. IEEE. Ali Furkan Biten, Lluis Gomez, Maral Rusinol, andDimosthenis Karatzas. 2019. Good news, everyone!context driven entity-aware captioning for news im-ages. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages1246612475. Dongping Chen, Ruoxi Chen, Shilin Zhang, YinuoLiu, Yaochen Wang, Huichi Zhou, Qihui Zhang,Pan Zhou, Yao Wan, and Lichao Sun. 2024. Mllm-as-a-judge: Assessing multimodal llm-as-a-judgewith vision-language benchmark.arXiv preprintarXiv:2402.04788. Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, So-ravit Changpinyo, Alan Ritter, and Ming-Wei Chang.2023a. Can pre-trained vision and language modelsanswer visual information-seeking questions? arXivpreprint arXiv:2302.11713. Zui Chen, Zihui Gu, Lei Cao, Ju Fan, Samuel Madden,and Nan Tang. 2023b. Symphony: Towards natu-ral language query answering over multi-modal datalakes. In 13th Conference on Innovative Data Sys-tems Research, CIDR 2023, Amsterdam, The Nether-lands, January 8-11, 2023. www.cidrdb.org. Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, YangZhou, Kaizhao Liang, Jintai Chen, Juanwu Lu, Zi-chong Yang, Kuei-Da Liao, et al. 2024.A sur-vey on multimodal large language models for au-tonomous driving. In Proceedings of the IEEE/CVFWinter Conference on Applications of Computer Vi-sion, pages 958979.",
  "Ilija Ilievski and Jiashi Feng. 2017. Multimodal learn-ing and reasoning for visual question answering. Ad-vances in neural information processing systems, 30": "Mahmoud Khademi, Ziyi Yang, Felipe Frujeri, andChenguang Zhu. 2023.Mm-reasoner: A multi-modal knowledge-aware framework for knowledge-based visual question answering. In Findings of theAssociation for Computational Linguistics: EMNLP2023, pages 65716581. Patrick Lewis, Ethan Perez, Aleksandra Piktus, FabioPetroni, Vladimir Karpukhin, Naman Goyal, Hein-rich Kttler, Mike Lewis, Wen tau Yih, Tim Rock-tschel, Sebastian Riedel, and Douwe Kiela. 2021.Retrieval-augmented generation for knowledge-intensive nlp tasks.",
  "Boyan Li, Yuyu Luo, Chengliang Chai, Guoliang Li,and Nan Tang. 2024a. The dawn of natural languageto SQL: are we fully ready? Proc. VLDB Endow.,17(11):33183331": "Jiaqi Li, Miaozeng Du, Chuanyi Zhang, Yongrui Chen,Nan Hu, Guilin Qi, Haiyun Jiang, Siyuan Cheng,and Bozhong Tian. 2024b. Mike: A new benchmarkfor fine-grained multimodal entity knowledge editing.arXiv preprint arXiv:2402.14835. Kunpeng Li, Yulun Zhang, Kai Li, Yuanyuan Li, andYun Fu. 2019. Visual semantic reasoning for image-text matching. In Proceedings of the IEEE/CVF in-ternational conference on computer vision, pages46544662. Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, PeiyiWang, Liang Chen, Yazheng Yang, Benyou Wang,and Lingpeng Kong. 2023a. Silkie: Preference dis-tillation for large visual language models.arXivpreprint arXiv:2312.10665. Yunxin Li, Longyue Wang, Baotian Hu, Xinyu Chen,Wanqi Zhong, Chenyang Lyu, and Min Zhang. 2023b.A comprehensive evaluation of gpt-4v on knowledge-intensive visual question answering. arXiv preprintarXiv:2311.07536.",
  "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong JaeLee. 2023. Visual instruction tuning": "Xinyu Liu, Shuyu Shen, Boyan Li, Peixian Ma, RunzhiJiang, Yuyu Luo, Yuxin Zhang, Ju Fan, Guoliang Li,and Nan Tang. 2024a. A survey of NL2SQL withlarge language models: Where are we, and where arewe going? CoRR, abs/2408.05109. Ziyu Liu, Zeyi Sun, Yuhang Zang, Wei Li, Pan Zhang,Xiaoyi Dong, Yuanjun Xiong, Dahua Lin, and Ji-aqi Wang. 2024b. Rar: Retrieving and ranking aug-mented mllms for visual recognition. arXiv preprintarXiv:2403.13805. Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao,Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-ChunZhu. 2021. Iconqa: A new benchmark for abstract di-agram understanding and visual language reasoning.arXiv preprint arXiv:2110.13214. TR Mahesh, T Rajan, K Vanitha, HK Shashikala,et al. 2023. Intelligent systems for medical diag-nostics with the detection of diabetic retinopathy atreduced entropy. In 2023 International Conferenceon Network, Multimedia and Information Technology(NMITCON), pages 18. IEEE. Weixing Mai, Zhengxuan Zhang, Kuntao Li, Yun Xue,and Fenghuan Li. 2023. Dynamic graph constructionframework for multimodal named entity recognitionin social media. IEEE Transactions on Computa-tional Social Systems. Xuedi Qin, Chengliang Chai, Nan Tang, Jian Li, YuyuLuo, Guoliang Li, and Yaoyu Zhu. 2022. Synthesiz-ing privacy preserving entity resolution datasets. In38th IEEE International Conference on Data Engi-neering, ICDE 2022, Kuala Lumpur, Malaysia, May9-12, 2022, pages 23592371. IEEE. Jielin Qiu, Andrea Madotto, Zhaojiang Lin, Paul ACrook, Yifan Ethan Xu, Xin Luna Dong, ChristosFaloutsos, Lei Li, Babak Damavandi, and SeungwhanMoon. 2024. Snapntell: Enhancing entity-centricvisual question answering with retrieval augmentedmultimodal llm. arXiv preprint arXiv:2403.04735. Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,et al. 2021. Learning transferable visual models fromnatural language supervision. In International confer-ence on machine learning, pages 87488763. PMLR. Sanket Shah, Anand Mishra, Naganand Yadati, andPartha Pratim Talukdar. 2019. Kvqa: Knowledge-aware visual question answering. In Proceedings ofthe AAAI conference on artificial intelligence, vol-ume 33, pages 88768884.",
  "Yushi Sun, Xin Hao, Kai Sun, Yifan Xu, Xiao Yang,Xin Luna Dong, Nan Tang, and Lei Chen. 2024. Arelarge language models a good replacement of tax-onomies? Proc. VLDB Endow., 17(11):29192932": "Yaniv Taigman, Ming Yang, MarcAurelio Ranzato,and Lior Wolf. 2014. Deepface: Closing the gapto human-level performance in face verification. In2014 IEEE Conference on Computer Vision and Pat-tern Recognition, pages 17011708. Nan Tang, Chenyu Yang, Ju Fan, Lei Cao, Yuyu Luo,and Alon Y. Halevy. 2024. Verifai: Verified gener-ative AI. In 14th Conference on Innovative DataSystems Research, CIDR 2024, Chaminade, HI, USA,January 14-17, 2024. www.cidrdb.org.",
  "Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris,Alessandro Sordoni, Philip Bachman, and KaheerSuleman. 2016. Newsqa: A machine comprehensiondataset. arXiv preprint arXiv:1611.09830": "Jianhong Tu, Ju Fan, Nan Tang, Peng Wang, GuoliangLi, Xiaoyong Du, Xiaofeng Jia, and Song Gao. 2023.Unicorn: A unified multi-tasking model for support-ing matching tasks in data integration. Proc. ACMManag. Data, 1(1):84:184:26. Yifan Wu, Lutao Yan, Leixian Shen, Yunhai Wang, NanTang, and Yuyu Luo. 2024. Chartinsights: Evaluat-ing multimodal large language models for low-levelchart question answering. In EMNLP (Findings).Association for Computational Linguistics.",
  "Yupeng Xie, Yuyu Luo, Guoliang Li, and Nan Tang.2024. Haichart: Human and ai paired visualizationsystem. arXiv preprint arXiv:2406.11033": "Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla,Xiangsen Chen, Sajal Choudhary, Rongze DanielGui, Ziran Will Jiang, Ziyu Jiang, Lingkun Kong,Brian Moran, Jiaqi Wang, Yifan Ethan Xu, An Yan,Chenyu Yang, Eting Yuan, Hanwen Zha, Nan Tang,Lei Chen, Nicolas Scheffer, Yue Liu, Nirav Shah,Rakesh Wanga, Anuj Kumar, Wen-tau Yih, andXin Luna Dong. 2024. CRAG - comprehensive RAGbenchmark. CoRR, abs/2406.04744. Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wen-tao Ye, Bosheng Qin, Siliang Tang, Qi Tian, andYueting Zhuang. 2023. Hallucidoctor: Mitigating hal-lucinatory toxicity in visual instruction data. arXivpreprint arXiv:2311.13614.",
  "Dongxiang Zhang, Rui Cao, and Sai Wu. 2019. Infor-mation fusion in visual question answering: A survey.Information Fusion, 52:268280": "Xinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan,Lianke Qin, Heng Wang, Xifeng Yan, William YangWang, and Linda Ruth Petzold. 2023a. Gpt-4v(ision)as a generalist evaluator for vision-language tasks. Zhengxuan Zhang, Weixing Mai, Haoliang Xiong,Chuhan Wu, and Yun Xue. 2023b. A token-wisegraph-based framework for multimodal named entityrecognition. In 2023 IEEE International Conferenceon Multimedia and Expo (ICME), pages 21532158.IEEE.",
  "AExperimental Details": "1. Setup and Environment: The experiments wereconducted in a zero-shot setting using RTX 4090GPUs, with PyTorch version 1.12.0. For GPT-4V,we used the interface of the GPT-4-vision-previewmodel. It is worth noting that GPT-4V often re-frains from answering person identification ques-tions without additional clues due to policy reasons.However, with the incorporation of matching graphtechniques, it can leverage weak signals and com-bine them with its own knowledge base.",
  ". Efficiency and Time: For preprocessing, usingDeepFace for face detection and extraction from animage takes approximately 0.1 to 0.4 seconds. Per-forming NER on captions using spaCy takes about": "0.001 seconds per caption. Additionally, process-ing each query, which includes retrieval, construct-ing a matching graph for the query, and reasoning,takes 0.01 to 0.3 seconds to complete the entireprocess. 3. Parameters: We determined the experimentalhyperparameters by creating a small sample of ap-proximately 100 data points. During node retrieval,the face similarity threshold f and name similaritythreshold n were set to 0.8 and 0.9, respectively.The number of iterations k for node retrieval wasset to 2, and the maximum number of seed nodeswas set to 10. It is worth noting that variations inthese hyperparameters have little impact on the ex-perimental results, as MLLMs can correctly answerquestions when the hit includes correct examples.Thus, our method still demonstrates strong general-izability."
}