{
  "Abstract": "Hate speech (HS) on social media exacer-bates misinformation and baseless prejudices.Evidence-supported counterspeech (CS) is cru-cial for correcting misinformation and reduc-ing prejudices through facts. Existing meth-ods for generating evidence-supported CS of-ten lack clear guidance with a core claim fororganizing evidence and do not adequatelyaddress factuality and faithfulness hallucina-tions in CS within anti-hate contexts.Inthis paper, to mitigate the aforementioned, wepropose F2RL, a Factuality and FaithfulnessReinforcement Learning framework for gen-erating claim-guided and evidence-supportedCS. Firstly, we generate counter-claims basedon hate speech and design a self-evaluationmechanism to select the most appropriate one.Secondly, we propose a coarse-to-fine evidenceretrieval method. This method initially gener-ates broad queries to ensure the diversity ofevidence, followed by carefully reranking theretrieved evidence to ensure its relevance tothe claim. Finally, we design a reinforcementlearning method with a triplet-based factualityreward model and a multi-aspect faithfulness re-ward model. The method rewards the generatorto encourage greater factuality, more accuraterefutation of HS, consistency with the claim,and better utilization of evidence. Extensive ex-periments on three benchmark datasets demon-strate that the proposed framework achievesexcellent performance in CS generation, withstrong factuality and faithfulness.",
  "*Equal contribution.Corresponding authors": "User 1 AllJews arearecheap, greedy, and materialistic !!! 4:28 PM - 21 Feb, 2024 3 replies 2 shares 0 likes User 3 4:44 PM - 21 Feb, 2024 Jewish communities began to form in the U.S. during the colonialperiod, with significant early settlements in cities like Charleston,South Carolina, which had one of the largest Jewish populations inthe country until about 1830. 0 replies 0 shares 0 likes User 4 4:58 PM - 21 Feb, 2024 Not all Jews are not cheap, greedy, and materialistic. In fact, they areoften considered to be philanthropists. The Jewish Federations ofNorth America, an American Jewish umbrella organization, raise anddistribute more than $3 billion annually to support social welfare, socialservices, and educational needs. 0 replies 0 shares 3 likes User 2 4:32PM - 21 Feb, 2024 No. In fact, Jews donate 100 billion dollars annually. 0 replies 0 shares 0 likes",
  ": Examples of Different Evidence-supportedCounterspeech": "Counterspeech (CS) involves directly respondingto HS to reduce its negative impact and promotea more friendly and harmonious dialogue (Chunget al., 2023). The types of CS are diverse, includinghumor, rhetorical questions, evidence-supported,and others (Gupta et al., 2023; Wang et al., 2024).In particular, evidence-supported counterspeechis crucial for correcting misinformation, reducingbaseless prejudices, and educating the audiencethrough facts as evidence (Benesch et al., 2016). The automatic generation of evidence-supportedCS has been extensively researched. These studiescan be categorized into non-retrieval-augmentedmethods and retrieval-augmented methods. Non-retrieval-augmented methods typically generateevidence-supported CS that relies on the internalparameterized knowledge of large language models(LLMs). He et al. (2023) proposes a reinforcementlearning-based framework called MisinfoCorrect,which employs a BERT-based classifier as a re-ward model to enhance the factuality of the counter-responses. Wang et al. (2024) applies a discrimina- tor to guide the decoding process of LLMs. Guptaet al. (2023) uses category distribution learningfor LLMs to generate evidence-supported CS, asopposed to other types of CS. As for the retrieval-augmented methods, Chung et al. (2021) present aknowledge-grounded CS generation pipeline thatuses an external knowledge base. The method con-structs data pairs of HS and background knowledge.This allows for supervised fine-tuning of LLMs togenerate evidence-supported CS. This method al-lows CS to include more up-to-date and factuallycorrect knowledge. Recently, Yue et al. (2024)proposes a retrieval-augmented response genera-tion (RARG) for online misinformation. RARGcollects and reranks evidence from a large aca-demic database, then uses PPO-based reinforce-ment learning to fine-tune LLMs for generatingevidence-supported responses.The aforementioned studies have advanced thedevelopment of evidence-supported CS genera-tion, but they may still have the following limi-tations. (L1) Factuality hallucination and evalua-tion challenges: non-retrieval-augmented methodstypically rely on the internal knowledge of LLMs.Therefore, the generated CS often contains factualerrors (e.g., the CS by user 2 in ). Addition-ally, current methods use classifiers to evaluate thefactuality of CS (He et al., 2023; Yue et al., 2024),which often lack objectivity and generalizability.(L2) Faithfulness hallucination in CS: existingresearch defines faithfulness hallucination as be-ing inconsistent with the input content (Hu et al.,2024). In evidence-supported CS generation, thisprimarily manifests in two aspects: the inability toeffectively rebut the HS and the failure to correctlyutilize the given evidence (e.g., the CS by user 3in ). (L3) Evidence lacks the guidanceof a clear claim: existing methods tend to simplylist evidence but lack the guidance of a clear claim.This may result in CS lacking a coherent argumentand clear evidence connection. The CS from user4 in is a good example. It presents a clearclaim, followed by supporting evidence.In this paper, to mitigate the aforementionedlimitations, we propose F2RL, a Factuality andFaithfulness Reinforcement Learning frameworkto generate claim-guide evidence-supported CS.The framework first generates a counter-claimbased on the HS, which serves as the core argu-ment of the CS. Then, several queries are generatedbased on this claim to retrieve supporting evidence.Finally, given the claim and evidence, we optimize the generator using reinforcement learning to en-hance the factuality and faithfulness of the CS.Particularly, our model consists of three modules:(1) Self-evaluation claim generation: This mod-ule employs an LLM-based claim generator to pro-duce various claims. The LLM then self-evaluatesthese claims to select the most appropriate one. (2)Coarse-to-fine evidence retrieval: This modulegenerates queries based on the selected claim anduses a coarse-to-fine retrieval strategy to obtain sup-porting evidence for the claim. (3) Factuality andfaithfulness reinforcement learning : This mod-ule trains a CS generator to generate claim-guidedand evidence-supported CS. Specifically, We de-sign a triplet-based factuality reward model and amulti-aspect faithfulness reward model to evaluatethe generated CS. Then we use reinforcement learn-ing to optimize the generator to improve the fac-tuality and faithfulness. Experiments demonstratethat our framework outperforms strong baselines inthe evidence-supported counterspeech generationtask. Our contributions are threefold: We design a novel claim-guided coarse-to-fineevidence retrieval method. It first generates broadqueries to ensure the diversity of evidence, thencarefully rerank the results to ensure their rele-vance to the claim. This method enhances thecoherence and evidence connection of the CSby closely aligning the evidence with the centralcounter-claim. We propose an innovative factuality and faith-fulness reinforcement learning framework forclaim-guided evidence-supported CS generation.It enables generating CS with higher factual cor-rectness, more precise refutation, and better uti-lization of evidence, leveraging a triplet-basedfactuality reward model and a multi-aspect faith-fulness reward model. Extensive experiments on 3 benchmark datasetsshow that the proposed framework achieves ex-cellent performance in CS generation with goodfactuality and faithfulness. It also generalizeswell to different LLMs.",
  "Counterspeech (CS) can be defined as a direct re-sponse to hate or dangerous speech to mitigatehate. CS can fight hate speech (HS) and reduce": "its negative impact on social media while still al-lowing free speech (Chung et al., 2023). Recently,many automatic counterspeech generation methodshave been proposed.Zhu and Bhat (2021) pro-pose Generate-Prune-Select which is a three-stagepipeline to obtain the most relevant CS for an HS in-stance. Chung et al. (2021) proposed a knowledge-grounded generation approach by incorporatingan intermediate step in which keyphrases are gen-erated to retrieve the necessary knowledge. Sahaet al. (2022) proposed CounterGEDI, an ensembleof GEDI to guide the generation of a DialoGPTmodel toward more polite, detoxified, and emo-tional CS. Then, Gupta et al. (2023) proposedQUARC, which leverages vector-quantized repre-sentations to generate CS with various intent cate-gories. Jiang et al. (2023) proposed RAUCG, whichenhances the LMs ability to automatically incor-porate counter-knowledge from new external statis-tics, facts, or examples in counter-narrative genera-tion. Wang et al. (2024) proposed DART, whichemployed dual discriminator to jointly guide thedecoding preferences of LLMs, aiming to generateCS catering to specific intent and hate mitigation.However, these methods focus on improving CSquality and diversity, often overlooking the impor-tance of ensuring the factuality and faithfulness ofCS.",
  "Reinforcement Learning for LLMs": "LLMs acquire surprising capabilities (Touvronet al., 2023), largely due to the fine-tuning of LLMsusing Reinforcement Learning from Human Feed-back (RLHF). Recently, RLHF has become keyin fine-tuning LLMs to better align with humanpreferences and improve task performance (Chris-tiano et al., 2017). RLHF generally includes fourprocesses (Lang et al., 2024): supervised fine-tuning, human preference collecting, reward learn-ing and RL policy optimization. Currently, twomain RLHF approaches are reward-based methodsand reward-free methods. OpenAI pioneered thereward-based approach, utilizing preference datato construct a reward model and optimizing the re-ward signal with actor-critic algorithms like Prox-imal Policy Optimization (PPO) (Schulman et al.,2017). Conversely, reward-free methods dispensewith the explicit use of a reward function. Forexample, DPO (Rafailov et al., 2023) representsthe reward function in the logarithmic form of thepolicy and focuses solely on policy optimization.Other reward-free methods include RRHF (Yuan",
  "Task Definition and Pipeline": "Formally, the goal of the claim-guided evidence-supported CS generation task is to construct astochastic text generation function . It can takehate speech x, claim c and reference evidence setE as the input and output the generated CS y. Thecore argument of the CS y needs to align with thegiven claim and effectively utilize the provided evi-dence to counter HS such that y (|x, c, E).",
  "Model Architecture": "In this section, we describe the main modules ofour proposed F2RL framework for claim-guidedevidence-supported counterspeech generation. Asdemonstrated in , the proposed F2RLframework mainly consists of three modules: Self-Evaluation Claim Generation Module em-ploys an LLM-based claim generator to generatevarious claims. Subsequently, we design a vot-ing prompt for the LLM, enabling it to comparedifferent claims and vote for the one that mosteffectively rebuts HS. Coarse-to-Fine Evidence Retrieval Module re-trieves and reranks documents to get the relevantsupporting evidence for the claim. We also op-timize the ranker using contrastive learning toimprove the relevance estimation between theclaim and the evidence. Factuality and Faithfulness ReinforcementLearning Module aims to train a CS generatorto generate claim-guided and evidence-supportedCS. This module applies a triplet-based factu-ality reward model and a multi-aspect faithful-ness reward model to estimate the rewards of CSand update the parameters of the generator usingPPO-based reinforcement learning.",
  "Self-Evaluation Claim Generation": "This module primarily focuses on generating acounter-claim based on the HS. It can explicitlyexpose the error of the input HS. The module con-sists of two sub-modules: (1) Claim Generation,which leverages the in-context learning (ICL) abil-ity of LLMs to generate various counter-claims;and (2) Self-Evaluation, which applies a votingprompt for the LLM to compare different partial",
  "Prompt": "Jews are not cheap, greedy, and materialistic. In fact, they are often considered to bephilanthropists, and their charitable donations are a significant portion of the overall giving inthe United States. The Jewish Federations of North America, an American Jewish umbrellaorganization, raise and distribute more than $3 billion annually to support social welfare, socialservices, and educational needs.This highlights the significant role that Jews play inphilanthropy and charitable giving in the United States. : The architecture of the proposed F2RL framework. (1) Self-Evaluation claim generation module generatesand selects a counter-claim. (2) Coarse-to-Fine evidence retrieval module obtains the supporting evidence. (3)Factuality and faithfulness reinforcement learning module Optimize a CS generator.",
  "claims and vote for the most promising one. Then,it employs a majority voting strategy to obtain morerobust results (Yao et al., 2023)": "Specifically, given the HS x, we design an in-struction prompt template for an LLM to obtain aclaim generator Gclaim. LLMs have the ICL capa-bility (Brown et al., 2020), allowing them to per-form claim generation without fine-tuning whenprovided with instructions and some examples.This generator takes the HS as input and outputs aseries of claims C = {c0, c1, , cnc}, where ncis the number of claims. Then, we design a votingprompt for the LLM to obtain a voting agent V.This agent takes a series of claims C as input andselects the best claim c, which can be formulatedas c = V(C). c is selected based on deliberatelycomparing different claims in C in the vote prompt.When using LLMs for self-evaluation, performingthe process multiple times usually yields a more ro-bust result. Therefore, we conduct multiple roundsof voting to select the claim with the most votes.Through self-evaluation, we hope to select claimsthat explicitly and objectively expose the errors orbiases in HS more effectively than other claims, asthe basis for subsequent evidence retrieval.",
  "Query generation": "Similar to claim generation, we design an instruc-tion prompt for the LLM to obtain a query gen-erator Gquery.This generator takes the claimc as input and generates a set of queries Q ={q1, q2, , qnq}, where nq is the number ofqueries. This process can be formalized as Q =Gquery(c). Generating queries based on claims of-fers several advantages over directly using claimsor HS for retrieval (Zhao et al., 2024; Huang andHuang, 2024). It captures different aspects of theclaim, thus increasing the relevance, comprehen-siveness, and diversity of the evidence obtained.",
  "Evidence Retrieval": "This sub-module aims to retrieve evidence that cansupport the claim. Inspired by previous work (Yueet al., 2024), we designed a coarse-to-fine evidenceretrieval pipeline. This pipeline first uses a retrieverto conduct a broad initial retrieval based on queries,ensuring the diversity and comprehensiveness ofthe evidence set. Then, it employs a reranker to",
  "perform fine-grained reranking, ensuring high rele-vance to the claim (Huang and Huang, 2024)": "In particular, for the retriever R, we use the off-the-shelf Contriever-MS MARCO (Izacard et al.,2022; Asai et al., 2023) by default to retrieve rele-vant documents from the Wikipedia database. Sub-sequently, we obtain an initial set of retrieved ev-idence, denoted as Ecoarse. These processes canbe formalized as Ecoarse = R(Q, Wiki).Theset Ecoarse contains all evidence documents re-trieved based on all queries Q. For the rerankerprocess Rrank, to obtain evidence that is rele-vant and supportive of the claim c, we employthe BGE M3 (Chen et al., 2024) model to cal-culate the relevance score. Based on this score,we rerank and filter Ecoarse to get the fine-grainedevidence set Efine, which can be formalized asEfine = Rrank(Ecoarse, c).",
  "Reranker Optimization": "To improve the ranking performance and general-izability, We optimize the rerankers performanceafter each retrieval. It can be divided into two steps:(1) sampling of positive and negative evidence, and(2) optimization based on contrastive learning.First, for each claim ci C and the initial evi-dence set Ecoarse, we use the BGE model to calcu-late relevance scores. By setting different thresh-olds, we sample K positive evidence {epj}kj=1 andK negative evidence {enj }kj=1, forming positivepairs (c, ep) and negative pairs (c, en).Subse-quently, we minimize the InfoNCE loss (Chen et al.,2020; Yue et al., 2024) to draw the positive claim-evidence pairs closer and push away the negativesamples. InfoNCE loss is a contrastive learningloss used to optimize representation learning bymaximizing the similarity of positive sample pairsand minimizing the similarity of negative samplepairs. The optimization objective L can be formu-lated as:",
  "Task definition + Instruction + Hate speech x +Claim c+ Evidence E": "Among them, the task definition provides a stan-dard definition of the CS generation task. The in-struction specifies the guidelines we expect the gen-erator to follow. The prompt can be formalized asp = Prompt(x, c, E), where the Prompt() is theprompt template. Next, we use a supervised fine-tuned CS generator Gcs to generate CS cs based onthe prompt p.",
  "Factuality and Faithfulness RewardEstimation": "Triplet-based Factuality Reward Model. The re-markable fluency and inventiveness of LLMs havemade them popular (Zhao et al., 2023). Nonethe-less, LLMs often generate persuasive but incorrectstatements, known as hallucinations (Huang et al.,2023). Factuality hallucinations involve claimscontradicted by real-world facts. Preventing fac-tuality hallucinations in generating CS for HS iscrucial, as these errors can undermine the CSs ef-fectiveness. We design a triplet-based factualityreward model to address this limitation inspired byprevious work (Hu et al., 2024).Conditioned on the retrieved evidence as apremise, we define factuality reward as the prob-ability that the CS is entailed by the retrievedevidence.Specifically, we first define a tripleextractor Etr() that takes the generated CS csias input and extracts knowledge triplets Ti ={(sij, rij, oij)}Nij=1, where Ni is the number oftriplets. It can be formalized as Ti = Etr(csi). Ex-isting research has shown that (Wang et al., 2023;Hu et al., 2024) the decomposition of the originaltext into triplets facilitates finer-grained factuality hallucination detection and more accurate factual-ity evaluation. Next, we construct evidence-tripletpairs (e, t), where e Ei and t Ti. We employan LM-based factuality checker, denoted as Ckr()to calculate the likelihood of entailment for eachevidence-triplet pair. Finally, we use the averagelikelihood of entailment across all evidence-tripletpairs as the factuality reward. It can be formalizedas follows:",
  "NE NTeEitTiCkr(e, t),": "(3)where NE is the number of evidence and Nt is thenumber of triplets.Multi-aspect Faithfulness Reward Model.Faithfulness means consistent with the input con-tent (Li et al., 2022).As for the claim-guideevidence-supported CS generation, faithfulness hasmultiple aspects of meaning: (1) Faithfulness toHate Speech: The CS must directly address andrebut the input HS, clearly pointing out its errors,biases, or inaccuracies. (2) Faithfulness to theClaim: The CS should be consistent with the in-put claim, revolving around it and ensuring it staystrue to the claims main points and facts. (3) Faith-fulness to the Evidence: The CS must accuratelyreference and interpret the provided evidence tosupport the claim, enhancing its persuasivenessand credibility.In detail, for the faithfulness to HS, we trained abinary stance detection model fhate that can use HSas the target to evaluate the stance of a given text,identifying whether it supports or opposes the HS.We use the probability value of the opposing stanceas the reward for faithfulness to HS. Then, we usea pre-built similarity function to measure the rele-vance of the CS to the claim and the evidence. Weuse the sum of the above scores as the faithfulnessreward rfaith, which can be formulated as:",
  "Reinforcement TuningThis part aims to optimize the CS generator throughreinforcement learning to improve the quality of": "generated responses. Reinforcement learning hasproven to be an effective approach to fine-tuningLLMs to extract complex, useful behaviours fromtheir pre-trained weights (Xu et al., 2024). Existingresearch (Tian et al., 2023; Yue et al., 2024) in-dicates that reinforcement learning with proximalpolicy optimization (PPO) (Schulman et al., 2017)or direct preference optimization (DPO) (Rafailovet al., 2023) can encourage greater factuality andfaithfulness in LLMs. In this section, we apply thePPO-based reinforcement learning with a rewardr() to fine-tune the CS generator. The actor CSgenerator is trained to maximise the formula-tion:",
  "E(x,c,E){xi,ci,Ei}Ni=1,y(y|)[r(x, c, E, y) D](6)D = DKL ((y | x, c, E)(y | x, c, E))(7)": "where x is the HS, c is the claim, E is the evidence, is the reference policy. D is the KL divergenceterm to prevent optimization instability or overopti-mization (Gao et al., 2023). is a hyperparameterto regularize the output difference between and. During training, we initialize the and using the weights from the SFT. Finally, we usethe reinforcement-tuned actor model as the finalmodel for generating CS.",
  "We conducted experiments on three CS generationdatasets, focusing solely on the HS instances fromthese datasets to generate CS. Detailed informationis as follows:": "CONAN (Chung et al., 2019) is a large-scale,multilingual resource designed to combat onlinehate speech through expert-generated counter-narratives. It includes 4,078 pairs of hate speechand counter-narratives in English, French, andItalian, collected by over 100 trained operatorsfrom NGOs. MTCONAN (Fanton et al., 2021) includes5,000 hate speech and counter-narrative pairs inEnglish, generated using a human-in-the-loopmethodology. It covers multiple hate targets suchas the disabled, Jews and LGBT+.",
  "FortheversionofLLMsinourexperi-ments, we utilized gpt-3.5-turbo-0125, glm-4-9b-": "chat, Qwen1.5-7B-Chat, and Llama-3-8B-Lexi-Uncensored*. These models are used simultane-ously for generating claims, queries, and CS. Forthe claim generation, We generate 5 claims eachtime. We use the same LLM as the claim generatorto act as the voting agent. For evidence retrieval,we generate 5 queries and select the top 5 relevantdocuments during the retrieval process. We usethe off-the-shelf Contriever-MS MARCO as theretriever and BGE M3 as the reranker. We train theBGE M3 model for 5 epochs with a learning rate of1e-5. During the supervised fine-tuning phase, Wesplit the dataset into 50% for training, 25% for vali-dation, and 25% for testing. Since the dataset lacksground truth for claim-guided evidence-supportedCS, we used the state-of-the-art GPT-4o model togenerate it for each HS in the training set. Fivevolunteers are hired to manually verify and revisethe generated ground truth. We perform instructionfine-tuning on the training set using LoRA. Dur-ing the reinforcement learning phase, we used theTRL library. We adopted PPO with a learningrate of 3e-5 and set the initial KL regularizationto 0.2. The training consists of 3 epochs, with abatch size of 16, and parameters are updated after4 gradient accumulation steps. We conducted allthe experiments with Nvidia A800 GPUs.",
  "*We also experimented with the Meta-Llama-3-8B-Instructmodel. However, due to its strict alignment protocols, it fre-quently refuses to generate CS in response to hate speech": "et al., 2023; Yue et al., 2024) we use four met-rics which are factuality (FA), faithfulness to HateSpeech (FH), faithfulness to Claim (FC) and faith-fulness for Evidence (FE), and Diversity (D).FA can be calculated by equation 3, FH, FC, andFE can be calculated from the individual terms inEquation 4. The diversity (Wang and Wan, 2018) ofCS yi in a collection of generated CS Yi is definedusing the following formula:",
  "Main Experimental Results": "We report the main experimental results of F2RLon three benchmark datasets in . We drawthe following observations. (1)The F2RL methodcan improve the factual correctness of CS. Fromthe experimental results in , it can be seenthat F2RL achieved a 3% to 6% improvement inthe FA metric across all three datasets. (2) TheF2RL method generates CS that more stronglyopposes hate speech.For instance, the high-est FH across all three datasets is achieved byLlama3-F2RL. Additionally, for various LLMs,F2RL achieved higher FH scores in most cases com-pared to other baselines. (3) The F2RL methoddemonstrates greater consistency with claimsand makes better use of existing evidence. Forexample, in the MTCONAN dataset, Llama3-F2RLimproved the FC metric by approximately 6% com-pared to Llama3-CoTR. Additionally, GLM4-F2RLenhanced the FE metric by about 7% compared toGLM4-CoT. (4) F2RL method demonstrates gen-eralizability and effectiveness across differentLLMs. In experiments, the F2RL consistently out-performs other baselines on various LLMs. (5) TheF2RL method leads to a decrease in the diver-sity of generated CS. Compared to Llama3-CoTR,Llama3-F2RL shows a slight decrease in diversitymetrics across all three datasets. We attribute thisdecrease in diversity to three potential factors: (i)specific sentence structures: by observing the gen-erated CS, it is evident that LLMs trained withF2RL tend to use certain recurring sentence struc-tures. For example, these models frequently beginwith a claim, followed by a transition using phrasessuch as \"in fact,\" \"for example,\" or \"for instance,\"before listing supporting facts; (ii) single sourceretrieval: the retrieval sources used in this studyare limited to Wikidata. While this ensures thecorrectness of retrieved information, it restricts the diversity of the generated counterspeech; (iii) rein-forcement learning training: reinforcement learn-ing training encourages the model to rely moreheavily on retrieved information, which in turn re-duces the diversity and creativity of the generatedtext. However, For evidence-driven counterspeechtasks, factual accuracy outweighs diversity (Chunget al., 2021). The goal of counterspeech is to cor-rect harmful statements, so providing accurate andreliable information is crucial for credibility. Al-though creativity may boost engagement, it alsoincreases the risk of inaccuracies, potentially re-ducing effectiveness.",
  ": Experimental results of ablation study": "In this section, we aim to explore how mucheach reward part contributes to the performanceduring the reinforcement learning phase. We con-sider the following variants of F2RL: (1) F2RL w/orfact: During the reinforcement learning process,we remove the reward calculation for factuality.(2) F2RL w/o rfaith: we remove the reward cal-culation for faithfulness. (3) F2RL w/o FH: Weremove the reward calculation for faithfulness tohate speech. (4) F2RL w/o FC: We remove thereward evaluation for faithfulness to claim. (5)F2RL w/o FE: We remove the reward evaluationfor faithfulness to evidence.We evaluate the aforementioned variants usingthe Llama3 model on the CONAN dataset. Theexperimental results are shown in the . Wefind that the removal of rfact causes a 3% decreasein FA, which verifies the effectiveness and signif-icance of the factuality reward feedback. Whenrfaith is removed, the faithfulness of the generatedCS in all aspects decreases. We also conducted amore fine-grained study, and when different partsof rfaith were removed, their corresponding met-rics showed varying degrees of decrease. This in-dicates that each component of the faithfulnessreward feedback plays a crucial role in maintainingthe faithfulness of the generated CS.",
  ": Human evaluation results": "In human evaluation, we randomly sample 100CS generated by GPT-3.5 and Llama3 from theCONAN dataset. Given the HS, claim, evidence,and the generated CS, we recruit five annotators(majority rule) to assign a score from 1 to 5 (1: notat all, 3: OK, 5: very good) to the generated CSbased on the aspects of factuality, faithfulness forHS, claim and evidence. The age distribution ofthe 5 volunteers is between 20 and 30 years old.Among them, there are 4 males and 1 female. Twoare PhD students, and three are master students.The four aspects are (1) Factuality: annotators canuse various retrieval tools (e.g., Bing, Google) toverify the factual correctness of the CS. (2) faith-fulness to HS: Whether the CS explicitly refutesthe HS. (3)faithfulness for the claim: whether theCS is consistent with the claim. (4) faithfulness forevidence: whether the CS sufficiently utilizes theevidence. The human ratings results are listed in. We can observe that F2RL is better thanother baselines and achieves performance compa-rable to GPT-3.5, which is similar to the results ofthe automatic evaluation in .",
  ": Experimental results of different values of k": "To analyze the impact of using different values ofthe number of evidence k on performance, we con-ducted experiments using Llama3-F2RL on threedatasets, with results shown in . Firstly,as the amount of evidence increases, FA tends to decrease. This indicates that too much evidencein the prompt is more likely to cause hallucina-tions in LLMs, leading to factually incorrect CS.More evidence also increases noise, reducing theevidence utilization rate and lowering the averageFE. When more evidence is provided, LLMs tendto choose and cite only a few pieces rather thanall of them. This results in increased diversity inthe generated CS, as LLMs have more options tosupport the claim.",
  "Conclusion": "In this paper, we introduce F2RL, a factuality andfaithfulness reinforcement learning framework forclaim-guided, evidence-supported CS generation.To ensure that CS has a distinct argument anda clear evidence structure, we propose a claim-guided pipeline that first generates a counter-claim,then retrieves relevant evidence, and finally gener-ates the CS. To enhance the coherence and evidenceconnection of the CS, we design a coarse-to-fineevidence retrieval method. This method first gen-erates broad queries to ensure the diversity of evi-dence, then carefully reranks the results to ensuretheir relevance to the claim. To improve the fac-tuality and faithfulness of the CS, we propose aPPO-based reinforcement learning approach witha triplet-based factuality reward model and a multi-aspect faithfulness reward model. Experimentalresults show that our method outperforms compet-itive baselines in terms of factuality and faithful-ness. In the future, we aim to explore LLM-basedmulti-agent learning methods to further improvethe generation of CS.",
  "Limitations": "There are three limitations of this work: (1) Firstly,it is important to note that the generated CS byour model cannot completely eliminate offensiveand toxic language. However, we have made im-provements to the prompt template and generationprocess to generate CS with lower toxicity. (2)Secondly, it is worth mentioning that the gener-ated CS cannot be directly posted on real-world social media platforms. Instead, a further step is re-quired where volunteers carefully review and verifythe generated CS before posting. This automatedgeneration process has proven to be a significanttime-saving factor compared to the manual creationof CS. (3) Thirdly, the counterspeech generated byour method may still contain factual errors. Whileour approach aims to minimize the likelihood ofsuch errors, it cannot guarantee their complete elim-ination.",
  "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, andHannaneh Hajishirzi. 2023. Self-rag: Learning toretrieve, generate, and critique through self-reflection.CoRR, abs/2310.11511": "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, FeiHuang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,Xuancheng Ren, Chuanqi Tan, Sinan Tan, JianhongTu, Peng Wang, Shijie Wang, Wei Wang, Sheng-guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-uan Zhang, Yichang Zhang, Zhenru Zhang, ChangZhou, Jingren Zhou, Xiaohuan Zhou, and TianhangZhu. 2023. Qwen technical report. arXiv preprintarXiv:2309.16609.",
  "Susan Benesch, Derek Ruths, Kelly P Dillon, Haji Mo-hammad Saleem, and Lucas Wright. 2016. Consid-erations for successful counterspeech. Dangerousspeech project": "Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.2020. Language models are few-shot learners. In Ad-vances in Neural Information Processing Systems 33:Annual Conference on Neural Information Process-ing Systems 2020, NeurIPS 2020, December 6-12,2020, virtual. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, DefuLian, and Zheng Liu. 2024. BGE m3-embedding:Multi-lingual, multi-functionality, multi-granularitytext embeddings through self-knowledge distillation.CoRR, abs/2402.03216. Ting Chen, Simon Kornblith, Mohammad Norouzi, andGeoffrey E. Hinton. 2020. A simple framework forcontrastive learning of visual representations. In Pro-ceedings of the 37th International Conference onMachine Learning, ICML 2020, 13-18 July 2020, Vir-tual Event, volume 119 of Proceedings of MachineLearning Research, pages 15971607. PMLR. Paul F. Christiano, Jan Leike, Tom B. Brown, MiljanMartic, Shane Legg, and Dario Amodei. 2017. Deepreinforcement learning from human preferences. InAdvances in Neural Information Processing Systems30: Annual Conference on Neural Information Pro-cessing Systems 2017, December 4-9, 2017, LongBeach, CA, USA, pages 42994307.",
  "Yi-Ling Chung, Gavin Abercrombie, Florence Enock,Jonathan Bright, and Verena Rieser. 2023. Under-standing counterspeech for online harm mitigation.CoRR, abs/2307.04761": "Yi-Ling Chung, Elizaveta Kuzmenko, Serra SinemTekiroglu, and Marco Guerini. 2019.CONAN -counter narratives through nichesourcing: a multilin-gual dataset of responses to fight online hate speech.In Proceedings of the 57th Conference of the Associ-ation for Computational Linguistics, ACL 2019, Flo-rence, Italy, July 28- August 2, 2019, Volume 1: LongPapers, pages 28192829. Association for Computa-tional Linguistics. Yi-Ling Chung, Serra Sinem Tekiroglu, and MarcoGuerini. 2021.Towards knowledge-groundedcounter narrative generation for hate speech. In Find-ings of the Association for Computational Linguis-tics: ACL/IJCNLP 2021, Online Event, August 1-6,2021, volume ACL/IJCNLP 2021 of Findings of ACL,pages 899914. Association for Computational Lin-guistics.",
  "ACL/IJCNLP 2021, (Volume 1: Long Papers), VirtualEvent, August 1-6, 2021, pages 32263240. Associa-tion for Computational Linguistics": "Leo Gao, John Schulman, and Jacob Hilton. 2023. Scal-ing laws for reward model overoptimization. In In-ternational Conference on Machine Learning, ICML2023, 23-29 July 2023, Honolulu, Hawaii, USA, vol-ume 202 of Proceedings of Machine Learning Re-search, pages 1083510866. PMLR. Rishabh Gupta, Shaily Desai, Manvi Goel, Anil Band-hakavi, Tanmoy Chakraborty, and Md. Shad Akhtar.2023. Counterspeeches up my sleeve! intent dis-tribution learning and persistent fusion for intent-conditioned counterspeech generation. In Proceed-ings of the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),ACL 2023, Toronto, Canada, July 9-14, 2023, pages57925809. Association for Computational Linguis-tics. Bing He, Mustaque Ahamad, and Srijan Kumar.2023.Reinforcement learning-based counter-misinformation response generation: A case studyof COVID-19 vaccine misinformation. In Proceed-ings of the ACM Web Conference 2023, WWW 2023,Austin, TX, USA, 30 April 2023 - 4 May 2023, pages26982709. ACM. Xiangkun Hu, Dongyu Ru, Lin Qiu, Qipeng Guo,Tianhang Zhang, Yang Xu, Yun Luo, Pengfei Liu,Yue Zhang, and Zheng Zhang. 2024. Refchecker:Reference-based fine-grained hallucination checkerand benchmark for large language models. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,Zhangyin Feng, Haotian Wang, Qianglong Chen,Weihua Peng, Xiaocheng Feng, Bing Qin, and TingLiu. 2023. A survey on hallucination in large lan-guage models: Principles, taxonomy, challenges, andopen questions. CoRR, abs/2311.05232.",
  "John T Nockleby. 2000. Hate speech. Encyclopedia ofthe American constitution, 3(2):12771279": "Yushan Qian, Weinan Zhang, and Ting Liu. 2023. Har-nessing the power of large language models for empa-thetic response generation: Empirical investigationsand improvements. In Findings of the Association forComputational Linguistics: EMNLP 2023, Singapore,December 6-10, 2023, pages 65166528. Associationfor Computational Linguistics. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-pher D. Manning, Stefano Ermon, and Chelsea Finn.2023. Direct preference optimization: Your languagemodel is secretly a reward model. In Advances inNeural Information Processing Systems 36: AnnualConference on Neural Information Processing Sys-tems 2023, NeurIPS 2023, New Orleans, LA, USA,December 10 - 16, 2023. Punyajoy Saha, Kanishk Singh, Adarsh Kumar, BinnyMathew, and Animesh Mukherjee. 2022.Coun-tergedi: A controllable approach to generate polite,detoxified and emotional counterspeech. In Proceed-ings of the Thirty-First International Joint Confer-ence on Artificial Intelligence, IJCAI 2022, Vienna,Austria, 23-29 July 2022, pages 51575163. ijcai.org.",
  "John Schulman, Filip Wolski, Prafulla Dhariwal, AlecRadford, and Oleg Klimov. 2017. Proximal policyoptimization algorithms. CoRR, abs/1707.06347": "Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, FeiHuang, Yongbin Li, and Houfeng Wang. 2024. Pref-erence ranking optimization for human alignment. InThirty-Eighth AAAI Conference on Artificial Intelli-gence, AAAI 2024, Thirty-Sixth Conference on Inno-vative Applications of Artificial Intelligence, IAAI2024, Fourteenth Symposium on Educational Ad-vances in Artificial Intelligence, EAAI 2014, Febru-ary 20-27, 2024, Vancouver, Canada, pages 1899018998. AAAI Press.",
  "Katherine Tian, Eric Mitchell, Huaxiu Yao, Christo-pher D. Manning, and Chelsea Finn. 2023. Fine-tuning language models for factuality.CoRR,abs/2311.08401": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurlien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288.",
  "Zhenrui Yue, Huimin Zeng, Yimeng Lu, Lanyu Shang,Yang Zhang, and Dong Wang. 2024. Evidence-drivenretrieval augmented response generation for onlinemisinformation. pages 56285643": "Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,Wendi Zheng, Xiao Xia, et al. 2022.Glm-130b:An open bilingual pre-trained model. arXiv preprintarXiv:2210.02414. Penghao Zhao, Hailin Zhang, Qinhan Yu, ZhengrenWang, Yunteng Geng, Fangcheng Fu, Ling Yang,Wentao Zhang, and Bin Cui. 2024.Retrieval-augmented generation for ai-generated content: Asurvey. CoRR, abs/2402.19473. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,Xiaolei Wang, Yupeng Hou, Yingqian Min, Be-ichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,Chen Yang, Yushuo Chen, Zhipeng Chen, JinhaoJiang, Ruiyang Ren, Yifan Li, Xinyu Tang, ZikangLiu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.2023. A survey of large language models. CoRR,abs/2303.18223. Wanzheng Zhu and Suma Bhat. 2021. Generate, prune,select:A pipeline for counterspeech generationagainst online hate speech. In Findings of the Associ-ation for Computational Linguistics: ACL/IJCNLP2021, Online Event, August 1-6, 2021, volumeACL/IJCNLP 2021 of Findings of ACL, pages 134149. Association for Computational Linguistics.",
  "BCase Study": "We provided counterspeech examples generated byGPT-3.5 and Llama3 to compare the quality of gen-eration. See for the examples. Throughcomparison, we can see that the counterspeech gen-erated by GPT-3.5 (IOP) lacks specific evidenceand is more general. Both GPT-3.5 (CoT) and GPT-3.5 (CoTR) provide factual evidence as support,with GPT-3.5 (CoTR) offering more detailed andin-depth arguments. Compared to Llama3 (CoTR),Llama3 (F2RL) discusses the claim in more detailand provides multiple logically connected piecesof evidence.",
  "Counter hate speechState a claimProvide evidence": "[HT] are not [TS]In fact, they are [BS].For example, [E].Contrary to the statements that [HT] are [TS]Studies have shown that they are [BS]For instance, [E].Its incorrect to say that [HE] are [TS]Data indicates that they are actually [BS].Consider [E].The notion that [HT] are [TS] is false.Evidence suggests that they are [BS].To demonstrate, [E].The stereotype that [HT] are [TS] is unfounded.In reality, they are [BS].Such as, [E].",
  "Hate SpeechNot all Muslims are terrorists, but all terrorists are Muslims": "GPT-3.5 (IOP)This statement is inaccurate. In reality, the majority of global terrorist attacksare not committed by Muslims. Terrorism involves individuals and groups fromvarious backgrounds, religions, and ideologies worldwide. The definition ofterrorism is based on actions, not religious beliefs. Lets not judge entire groupsbased on narrow viewpoints.GPT-3.5 (CoT)This statement is misleading. Global terrorism data clearly shows that mostterrorist attacks are not committed by Muslims. Historical incidents, like theOklahoma City bombing by Timothy McVeigh, highlight that terrorism isntexclusive to Muslims. Terrorism is defined by actions, not religious identity.Lets avoid generalizations and focus on facts.GPT-3.5 (CoTR)Terrorism is not exclusive to any particular religion or belief system. While itis true that some terrorist acts have been carried out by individuals claimingto be Muslims, it is important to recognize that terrorism is a tactic used byindividuals of various backgrounds and ideologies. For example, Jamaat ul-Fuqraa is an Islamic terrorist organization with members who have planned actsof violence, but they are not the only group engaging in terrorism. Religiousterrorism, including acts committed by individuals of different faiths, has beenon the rise since 1980. It is crucial to understand that terrorism is a complexissue influenced by various factors, and attributing it solely to one religion ismisleading and harmful. Llama3 (CoTR)The claim that \"all terrorists are Muslims\" is false. Evidence includes groupslike Jamaat ul-Fuqra, an Islamic terrorist organization mostly African-American,responsible for violence like assassinations. Religious terrorism isnt limited toIslam; e.g., the 1995 Oklahoma City bombing stemmed from anti-governmentand anti-abortion beliefs. Experts like Robert Pape and Terry Nardin noteterrorisms broader motivations beyond religion.Llama3 (F2RL)Terrorism isnt tied to any specific religion or belief system. While some attacksinvolve individuals identifying as Muslim, its not representative of all terrorists.Evidence includes groups like Jamaat ul-Fuqra, an Islamic terrorist organizationmostly composed of African-Americans, responsible for acts of violence likeassassinations. Religious terrorism isnt limited to Islam; for example, the1995 Oklahoma City bombing was carried out by Timothy McVeigh, motivatedby anti-government and anti-abortion beliefs. Furthermore, the 2011 Norwayattacks by Anders Behring Breivik were driven by far-right extremist views,not Islam. Experts like Robert Pape and Terry Nardin note that terrorismsbroader motivations often include political, ideological, and social factorsbeyond religion. This demonstrates that terrorism is a complex, multi-facetedissue that transcends any single religious or belief system.",
  "C.3Query Generation Prompt": "Given a hateful statement and a correspondingcounter-claim, your task is to generate {query num}search queries to retrieve evidence from Wikipediathat supports the counter-claim. The query shouldbe precise and relevant to ensure the retrievalof strong factual evidence. Here is the hatefulstatement: \"{hatespeech}\" Here is the counter-claim: \"{claim}\" Please generate a search query forWikipedia to find evidence supporting the counter-claim. List each query as follows: [queries]: 1. [theFirst query] 2. [the Second query] 3. [the Thirdquery] 4. [the Second query] 5. [the Third query]",
  "C.4Counterspeech Generation Prompt": "You are a seasoned volunteer dedicated to coun-tering hate speech on social media. Given a claimand relevant evidence of each claim, your task isto generate a counterspeech. The Counterspeechneeds to first state the claim and then provide ev-idence to support the claim. The counterspeechshould be effectively refute the hatespeech. Youmust give me the counterspeech as following for-mat: [Counterspeech]: \"\"Here is the hate speech: \"{hatespeech}\" Here isthe counter-claim: \"{claim}\" Here is the evidence:\"{evidencestr}"
}