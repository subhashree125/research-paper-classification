{
  "Abstract": "Traditionally, success in multilingual machinetranslation can be attributed to three key factorsin training data: large volume, diverse transla-tion directions, and high quality. In the currentpractice of fine-tuning large language models(LLMs) for translation, we revisit the impor-tance of these factors.We find that LLMsdisplay strong translation capability after be-ing fine-tuned on as few as 32 parallel sen-tences and that fine-tuning on a single trans-lation direction enables translation in multipledirections. However, the choice of direction iscritical: fine-tuning LLMs with only Englishon the target side can lead to task misinter-pretation, which hinders translation into non-English languages. Problems also arise whennoisy synthetic data is placed on the target side,especially when the target language is well-represented in LLM pre-training. Yet interest-ingly, synthesized data in an under-representedlanguage has a less pronounced effect. Ourfindings suggest that when adapting LLMs totranslation, the requirement on data quantitycan be eased but careful considerations are stillcrucial to prevent an LLM from exploiting un-intended data biases.1",
  "Introduction": "Large language models (LLMs) have reached newheights in various NLP tasks (Radford et al., 2019;Brown et al., 2020; Touvron et al., 2023; Jianget al., 2023). Supervised fine-tuning (SFT, Ouyanget al., 2022, alternatively, instruction tuning or sim-ply fine-tuning in some literature) further preparesthese models for better generalization and reliabil-ity in downstream tasks by training on task input-output data combined with instructions in natu-ral languages (Sanh et al., 2022; Wei et al., 2022;Mishra et al., 2022). In this research direction, var-ious works have studied the scaling up of SFT",
  "*Corresponding author ()1Code available at: github.com/uds-lsv/mt-sft": "data size, number of languages, etc (Chung et al.,2024; Muennighoff et al., 2023). On the other hand,recent papers also embraced the philosophy of lessis more by achieving strong results with a small setof high-quality training instances, claiming a su-perficial alignment hypothesis (Zhou et al., 2023)with similar findings by others.This work investigates the role of SFT data inaligning LLMs to machine translation (MT), across-lingual generation task with high demands inpractical domains. Prior research has found fine-tuning to improve translation performance (Zhanget al., 2023c) and more recent works also inte-grated continued pre-training with more data toprovide further improvement (Xu et al., 2024a;Alves et al., 2024).For encoder-decoder mod-els, Wu et al. (2024a) used little data to enablean English-centric model to translate between anytwo languages.Nonetheless, the feasibility ofless is more in LLM translation fine-tuning israther under-explored. In translation prompting,researchers have suggested that a models transla-tion capability can be attributed to the bilingualsignals exposed during pre-training (Briakou et al.,2023) and task recognition in LLM layers (Sia et al.,2024), hinting that the translation capability hasbeen picked up during pre-training. A natural ques-tion follows: Can we put reduced effort into data?From a data efficiency perspective, we squeezethe translation SFT data to a mere size of 32 or thetranslation direction to 1 for multilingual transla-tion, for which we believe LLMs already possessa strong pre-trained foundation in multilingual un-derstanding and generation. Beyond quantity andlanguage diversity, we perform SFT on synthesizeddata via machine translation, which is a commondata augmentation practice for under-served lan-guages. To summarize, our analysis is groundedin the task of MT, with scaling down in mind.In multiple dimensionsdata size (3.2), transla-tion direction (3.3 and 3.4), and data synthesis",
  ". 32 data instances successfully enable an LLMto translate in 11 directions. More data stillhelps but the return diminishes": "2. Data in a single translation direction can effec-tively align an LLM to translate to and frommultiple directions. Yet, it is crucial to pickthe right directionwe recommend not plac-ing English on the target side. 3. When fine-tuning on lower-quality syntheticdata, LLMs are affected if the data is placed onthe target side, but they show greater resilienceagainst such flaws in low-resource languages,which are less represented during pre-training.",
  "Supervised fine-tuning": "In this work, we perform SFT to prepare pre-trainedLLMs for MT. Let S denote a source input andT = [t1, t2, ..., t|T|] denote a target-side reference.We start with placing the input into a prompt tem-plate by applying I() to S. For each traininginstance, the instruction template is randomly se-lected from a pre-defined pool. We fine-tune anLLM parameterized by by optimizing the log-likelihood:",
  "Superficial alignment hypothesis": "Zhou et al. (2023) claim that a models knowledgeand capabilities are acquired almost entirely dur-ing pre-training, and the effect of alignment tuningmight be superficial, in that it teaches the modelthe format for interacting with users. This ideais further supported by recent works (Lin et al.,2024; Ghosh et al., 2024). However, to what extentthis applies to multilingual translation in LLMs islittle known. To bridge this gap, we conduct a se-ries of controlled experiments on fine-tuning LLMsfor translation, complementing previous researchacross three dimensions. First, we study the parallel data efficiency in the era of LLMs, aiming to deter-mine the minimum data needed for effective modelalignment to the translation task. Next, we explorethe scope of alignment by probing whether aligningone translation direction influences other directions.Finally, we investigate how synthesized fine-tuningdata quality impacts the LLMs behaviour in gen-erating translations.",
  "Experimental setup": "Training.By default, we take the test sets fromWMT17 to WMT20 as our parallel training data(Bojar et al., 2017, 2018; Barrault et al., 2019,2020); we also use the development sets in WMT21(Akhbardeh et al., 2021) for training if a languagepair of interest is not available in earlier years.The specific training data configurations will bedetailed in the subsequent sections. The test setsfrom WMT21 are used for validation. Detailed datastatistics can be found in Appendix F.1. The LLMwe use for SFT is the base version of Llama-2 7B(Touvron et al., 2023). When performing SFT, weuse a learning rate of 5e-6, an effective batch sizeof 64, and a linear learning rate scheduling with awarmup ratio of 0.1. We select the model check-point based on COMET scores on the validationsets.2 To form the model input for SFT, we feed thesource sentence into the Alpaca prompt template(Taori et al., 2023), supplementing it with a trans-lation instruction that is randomly selected from apool of 31 diverse instructions. Refer to inthe appendix for a complete list of templates. Evaluation.We primarily evaluate the models onthe WMT22 test sets (Kocmi et al., 2022) covering11 translation directions: encs, ende, enjp,enru, enzh, and enhr.3 Languages in these11 directions are explicitly included in Llama-2spre-training corpus. In .4, we extendour evaluation to translation directions involvingmedium and low resource languages: Icelandic andHausa (i.e., enis, enha), which comes fromWMT21s test set. At inference time, a fixed trans-lation instruction is applied ( row 1). We 2In our preliminary experiments, we found that valida-tion perplexity has a relatively weak correlation with COMETscores measured on the validation set, similar to earlier find-ings (Ouyang et al., 2022).3Language codes: cs=Czech, de=German, hr=Croatian,jp=Japanese, ru=Russian, zh=Chinese. means that bothtranslation directions are covered. Note that only enhr isavailable in WMT22 but not hren. Base. 1* 3* 16 72.5 75.0 77.5 80.0 82.5",
  "COMET": "Test Direction: enhr 77.5 78.0 78.5 79.0 79.5 80.0 80.5 : Model performance (in COMET) on individual directions for models trained with varying data sizes anddirections. Both factors positively impact performance. +=: training directions added on top of previous directions;two directions (from and to English) at a time. For example, +=ru covers 10 directions: en {de, zh, cs, jp, ru}.",
  "How much SFT data enables LLMs totranslate?": "Recent works in machine translation suggest thatpre-trained LLMs require significantly less paralleldata for fine-tuning (via SFT), compared to train-ing conventional translation models from scratch.However, the SFT process in these works still op-erates with an order of 105 parallel samples (Jiaoet al., 2023; Zhang et al., 2023c; Zeng et al., 2024;Xu et al., 2024a, i.a.), without a clear justificationfor selecting this specific data size and source. Thisraises a pivotal question, inspired by the recentlyproposed superficial alignment hypothesis (Zhouet al., 2023): Is SFT mainly a method for superfi-cially aligning LLMs for translation tasks? If so,what is the actual minimal amount of data requiredto achieve effective alignment? Setup.We fine-tune Llama-2 7B using differentnumbers of training samples and evaluate the mul-tilingual translation performance of the resultingmodels. We collect training data covering 10 trans-lation directions: en{cs, de, jp, ru, zh}. Thetraining data sourced from WMT17-20 contains a",
  "Specifically, COMET is reported on a scale of 0 to 100 asopposed to its raw 0 to 1 range": "total of 74,623 parallel examples. Note that thetraining samples across translation directions arenot evenly distributed. To create training sets ofvarying sizes, we subsample the original data intosubsets that are powers of 2, starting from 16 (24)and ending with 4096 (212); larger subsets alwayscontain smaller ones. To ensure balanced languagerepresentation in our subsets, we distribute samplesas evenly as possible among the language pairs.5 We refer to the fine-tuned model as SFT-MT.Considering LLMs can also perform translationthrough prompting, we compare SFT-MT with 1-and 3-shot in-context learning (ICL), denoted asICL-MT. For ICL, we randomly select demonstra-tions from the training set in the test direction foreach test sentence. We do not consider Llama-2szero-shot performance because, although it some-times produces acceptable translations in the begin-ning, it often continues generating, which makesit difficult to accurately estimate its performance.Lastly, since LLMs fine-tuned on diverse tasksalso serve as strong translation systems (Zhu et al.,2024), we compare our models with open-sourcegeneral-purpose instruction-tuned LLMs, whichwe denote as IT-LLM. These include Vicuna-v1.5-7b (Chiang et al., 2023), Mistral-7b-Instruct (Jianget al., 2023), and Llama-2-7b-chat (Touvron et al.,2023).6",
  "meta-llama/Llama-2-7b-chat-hf": "csen deen jaen ruen zhen encs ende enhr enja enru enzh Test direction all dir. deen zhen ende enzh frde defr Train direction 99.899.999.699.899.674.176.676.370.757.576.3 99.599.598.699.699.376.581.676.871.458.376.1 94.997.396.795.597.698.898.899.396.898.798.3 96.095.891.694.888.898.298.598.693.898.898.7 90.997.297.097.798.397.597.798.497.397.897.6 90.096.695.390.897.698.898.899.699.198.898.6 0.6 0.7 0.8 0.9 1.0 : Normalized COMET score (as a % of performance from fine-tuning on an equivalent sized dataset of all10 directions) resulted from varying combinations of train and test translation directions. In most cases, Llama-2fine-tuned on a single translation direction can effectively translate across other directions, achieving performancecomparable to models trained on all directions, with a few exceptions when trained on Xen but tested on enX.Performance measured in BLEU score is provided in Appendix B. same foundation model, indicating that a few in-context demonstrations may not effectively alignLlama-2 for translation.However, performance significantly improveswhen Llama-2 is fine-tuned with just 16 samples.With further increases in the training size to 32 sam-ples, Llama-2 performs on par with or surpassesall three IT-LLM baselines in both COMET andBLEU metrics. This suggests that a handful ofhigh-quality parallel data can effectively special-ize the model into a performant translation sys-tem. Increasing parallel data further boosts per-formance, though with diminishing returns: theCOMET score rises by an average of 2 points whenexpanding from 32 to 1024 samples, but only by 0.5points when increasing further from 1024 to 75Ksamples (full training set). Given that it is unlikelythat these 32 training samples teach Llama-2 newtranslation skills, this shows strong evidence thatsuperficial alignment applies to MT. We observe asimilar trend in Mistral-7B and Llama-2-13B. Re-fer to Appendix A for their performance acrossvarying data sizes. In summary, effective transla-tion alignment begins with minimal training data,revealing less is good alignment and more is bet-ter with diminishing gains.",
  "In the preceding section, we follow the traditionalpractice in multilingual MT by including multiple": "translation directions during training. However, theobservation that only a few dozen examples makeLlama-2 translate well leads us to reconsider thenecessity of including samples from all directionsof interest. Specifically, will training on just asingle translation direction be sufficient to helpLLMs perform multilingual translation? Setup.We explore six training configurations,each focusing on a single translation direction:deen, zhen, ende, enzh, frde, anddefr. These configurations include cases whereEnglish appears on the source side, the target side,as well as settings with English excluded, to inves-tigate if specific languages have a different impacton the overall performance. The training size isset to 1024 for SFT. Evaluations are conductedacross the same 11 test directions as used in theprevious section. Additionally, we explore similarsettings in ICL, where we present demonstrationswith translation directions that do not match thoseused in evaluations, to determine if the mechanismsof both SFT and ICL exhibit similarities. Lastly, weconduct a joint evaluation, progressively expand-ing both the training size and the range of coveredtranslation directions to understand the combinedeffect of these factors.",
  "deen57.838.745.545.0enzh59.769.559.538.4zhen47.314.549.245.0frde59.368.666.0112.9defr60.7011.061.7611.3": ": ICL-MT performance with aligned vs. misaligned demonstrations, evaluated on deen and ende.1-shot/3-shot: using 1 or 3 demonstrations randomly sampled from the training set. Misaligned demonstrationsconsistently cause a substantial performance drop. with just one direction enables Llama-2 to translatebetween multiple languages. For instance, afterfine-tuning on deen or zhen, the model cantranslate from all considered languages to English,scoring at least 98.6% of the original COMETscores for training on all directions. Similarly, themodel fine-tuned on ende, enzh, frde ordefr also demonstrates only a slight performancedecline when translating from English.Notable declines are observed in two scenarios:(1) trained to translate to English and evaluated ontranslating to non-English; and (2) trained to trans-late to non-English and evaluated on translating toEnglish.7 Of these two scenarios, scenario 1 ex-hibits a much larger performance drop. The factthat both scenarios involve a mismatch between us-ing English and non-English suggests that Llama-2,as an English-centric LLM, may process Englishdifferently compared to other languages. Whenfine-tuned for English generation, the model maymisinterpret the task as only generating in English.Generalization among non-English languages ismuch easier than generalization between Englishand non-English languages, as evidenced by thenegligible performance drop when fine-tuning andtesting on two vastly different language pairs suchas defr and enzh. Overall, the findings suggestthat SFT in one translation direction effectivelyenables the many directions, though avoidingmisinterpretation is crucial. ICL results.We also provide results of perform-ing ICL with misaligned translation directions be-tween demonstration and test in . It can beseen that misaligned demonstrations significantlydegrade translation performance, with 3-shot be",
  "Training Direction": "78.5 79.0 79.5 80.0 80.5 81.0 81.5 82.0 82.5 Avg. COMET 79.5 80.0 80.5 81.0 81.5 82.0 82.5 : Average performance (in COMET) across11 test directions for models trained with varying datasizes and directions. Both factors positively impactperformance. +=: training directions added on top ofprevious directions; two directions are added at eachtime. For example, +=ru covers 10 directions: en {de, zh, cs, jp, ru}. Performance on individual testdirections is provided in Appendix C. often worse than 1-shot. We observe that the modelmay output Chinese characters, emojis, time, etc.,but no clear error patterns are observed. This con-trasts sharply with findings from SFT: while SFTcan recognize the format of translation, ICL re-quires language-aligned demonstrations. Joint evaluation. presents a joint eval-uation of size and translation direction. For smalltraining sizes, covering diverse translation direc-tions in training proves to be beneficial. However,the benefits of such diversity level off as the trainingsize increases. With a training size of 1024, mod-els trained exclusively on two directions, ende,perform on par with those trained on all directions. enis enha encsende enhr enja enruenzh Test direction (enX)",
  "Can alignment be achieved for unseenlanguages?": "Previous sections focus on translation directions in-volving languages explicitly included in Llama-2spre-training corpus. We now extend our investiga-tion to languages that do not have an identified pres-ence of over 0.005% in the pre-training data (c.f.Touvron et al., 2023, p22), referred to as unseenlanguages. Here we seek answers to two questions:(1) Can we effectively make Llama-2 translate bothfrom and to unseen languages by fine-tuning it witha small amount of data? (2) How well can this fine-tuned model translate from and to languages seenin Llama? Setup.We consider three training configurations:enis, enha, and ende, with Icelandic (is) andHausa (ha) being unseen languages. ende servesas a control to assess Llama-2s initial translationcapabilities into unseen languages without specificfine-tuning. The training size is fixed at 1024 (512samples for each direction). The test directionsinclude the 11 directions as before, plus enis andenha coming from the WMT21 test. Results.The results are presented in . Itcan be seen that fine-tuning on Icelandic and Hausaenhances a models translation quality on these lan-guages compared to the control setup, yet the gainsare modest. We observe that Llama-2 manages toproduce tokens in these languages, however, thetranslations often largely deviate from the origi-nal meanings. This suggests that it is difficult toteach models new translation directions via SFTwith limited data. Interestingly, we find fine-tuningon Icelandic or Hausa does not hinder Llama-2s ability to translate from and to all seen languages,maintaining performance levels comparable to thecontrol scenario with ende. Based on these re-sults, we propose a complement to the superficialalignment hypothesis in MT: LLMs may learnthe essence of the translation task without re-quiring input-output mappings in languages itunderstands well.",
  "Can we use synthesized data?": "We have observed that LLMs quickly recognize thetranslation task with minimal high-quality, man-ually curated data, but what if the quality of thetraining data is subpar? This situation may occur,for example when parallel data is web-crawled ormachine-generated. Can LLMs still adapt to thetranslation task or will they overfit to the imper-fections in lower-quality data, leading to degradedtranslation performance? Setup.We replace either the source or target sen-tences in the original training set with lower-qualitysynthesized ones. We try two types of data syn-thesis: one by translating entire sentences on theother side and another by concatenating word-to-word translations. Pleasingly, these correspondto back-translation (Sennrich et al., 2016) usingtranslation engines or bilingual word dictionarieswhich are practical at different levels of resourceavailability. Specifically, we use the OPUS-MTsuite (Tiedemann and Thottingal, 2020) to translatefrom English to a target non-English language.8",
  "SourceRef./Data config.Model output": "Das finde ich ehrlich gesagtreferenceThat really bothers me, I must say.sehr rgerlich.literalThe find I honest said very annoying.ende cleanI find that really annoying.ende sent. noiseI find that honestly very annoying.ende word noiseThe find I honestly said very annoying. referenceSo that such a thing wont happen again.literalin order to avoid again happen such thing.ende cleanLets not let it happen again.ende sent. noiseIn order not to happen again.ende word noiseAvoid again happen this way. : Examples of testing Llama-2 trained on ende with 1024 clean and noisy target sentences. The testdirections are deen (Top) and zhen (Bottom). The reference translation is provided by the WMT22 test set.Word-to-word references were created by the authors in consultation with native speakers. Word-level noise makesLlama-2 degenerate into a literal translator. For word-level translation, we translate each space-delimited source word by feeding it into the MTmodel one at a time. Naturally, the synthesized ver-sions introduce translation errors, adding noise tothe training process. We investigate the impact ofsuch noise in four translation directions: ende,deen, enha, and haen, where the prime ()notation denotes the side that is created using trans-lation (noised). We consider two training sizes:32 and 1024. In this section, our evaluation fo-cuses on the 11 translation directions describedin .1. Note that although Hausa is in-cluded in the current training setup, translation di-rections involving Hausa are excluded from ourevaluationbecause performance is sub-par for",
  "unseen languages as demonstrated in .4": "Results.According to , it can be seenthat both types of data synthesis generally causea drop in performance.However, The degreeof degradation significantly varies depending onwhether the noise appears on the source or tar-get side of the translation as well as the language.Specifically, when noise is introduced to the targetside, models fine-tuned on ende and enha translations exhibit a sharp decline in performance.The impact of word noise is more severe than thatof sentence noise. In the case of ende, word-level synthesis causes the model to largely degener-ate, leading to literal translations across many test cases across translation directions. An exampleof this behaviour is presented in . In con-trast, the performance drop caused by word noiseis less pronounced with enha, particularly whenevaluated on enX.Conversely, when noise is introduced on thesource side, the negative impact is much smaller,and the disparity in performance degradation be-tween the two types of noise diminishes. Evenmore strikingly, when evaluated on enX, havingnoise at the source side often outperforms the cleansettings. Notably, in .3, we show that fine-tuning models purely on Xen risks task misin-terpretation, leading to low performance on enX.However, adding noise appears to mitigate this is-sue, resulting in improvements in both COMETand BLEU scores, especially for the ha en case.Summarizing the observations, Llama-2 is muchmore robust against the noise introduced in Hausa,likely because it has limited familiarity with thelanguage, making it more difficult to detect andimitate imperfections present in the training data.As a result, Llama-2 tends to just recognize theessence of the translation task instead of overfit-ting to the biases present in low-quality data. Incontrast, with German, Llama-2s understandingleads to a misinterpretation of the training objec-tives, such as fitting the word-level noise with adirective for literal translations. Overall, LLMsmay quickly fit translation imperfections in thetraining data, especially for seen languages; theresulting performance drop may be observablewith just 32 training samples.",
  "What does LLM SFT bring us?": "Foundational language models become more robustand follow instructions better after being fine-tunedon task-oriented supervised data formulated as nat-ural language text (Mishra et al., 2022; Sanh et al.,2022; Wei et al., 2022). We observe divergingtrends in research on instruction tuning nowadays:(1) Many works attempt to scale up instructiondata in terms of the number of tasks, languages,data size, and thus implicitly increasing trainingupdates (Chung et al., 2024; Muennighoff et al.,2023; Wu et al., 2024c; Li et al., 2023; stn et al.,2024; Zhang et al., 2024). (2) Another stream ofpapers, argue that instruction tuning mainly altersa base models response style but not content orknowledgedata quality and diversity outweigh quantity (Zhou et al., 2023; Mitchell et al., 2024;Lin et al., 2024; Chen et al., 2024a). This work isa continued exploration of the latter, focusing onthe machine translation task. We verify the effectof size variations and include two new factorslanguage directions and qualityaiming to providepractical and cost-effective guidance on this matter.Specifically, language transfer has been demon-strated in smaller pre-trained models before LLMs(Wu and Dredze, 2019; Artetxe et al., 2020). For(sufficiently) multilingual models, training on cer-tain languages might still benefit other languagesat the test time (Choenni et al., 2023). In LLMinstruction tuning, recent papers revealed cross-lingual transfer and improved robustness in unseenlanguages via multilingual instruction tuning witha small data sample (Chen et al., 2024c; Kew et al.,2023; Shaham et al., 2024). Furthermore, it hasbeen claimed that even monolingual instructiontuning is sufficient to elicit multilingual responsesin the correct languages with a key ingredient be-ing the right learning rate (Chirkova and Nikoulina,2024a,b). In relation to our experiments, languagetransfer to unseen languages might account for im-proved performance in language directions that arenot directly fine-tuned.",
  "How can we use LLMs for translation?": "In the field of machine translation, earlier worksprovided analysis of general-purpose prompting(Vilar et al., 2023; Agrawal et al., 2023; Zhanget al., 2023a) followed by a blossom of strategiesfocusing on specific aspects of the translation pro-cess (Sarti et al., 2023; Ghazvininejad et al., 2023;He et al., 2024; Moslem et al., 2023; Chen et al.,2024b; Raunak et al., 2023). Nonetheless, as shownin our experimental results, few-shot prompting isnot on par with using instruction-tuned models, il-lustrating the importance of further understandingthe role of instruction tuning in translation tasks.In terms of fine-tuning LLMs for translation,previous works have explored a wide range of sub-tasks: disambiguation, low-resource, document-level, and adaptive translation, etc (Li et al., 2024;Zhang et al., 2023b; Alves et al., 2023; Iyer et al.,2023; Mao and Yu, 2024; Wu et al., 2024b). Theseworks focus on improving translation performanceand specific applications. Stap et al. (2024) showthat while fine-tuning improves translation qual-ity, it can degrade certain key LLMs advantages,such as the contextualization ability on document-level input. Some recent research aims to enhance the translation capabilities of LLMs by incorpo-rating human preference data (Jiao et al., 2023;Zeng et al., 2024; Zhu et al., 2024) or by extendingthe pre-training phase before fine-tuning (Xu et al.,2024a,b; Alves et al., 2024), yet these approachesrequire significantly more data or computing re-sources. The aim of this paper is not to pursuethe state of the art but to investigate the opportu-nities of extending instruction-tuned LLMs trans-lation capabilities in desirable compute-efficientscenarios. It is still worth noting that our investiga-tion is orthogonal to previous works which employrelatively large monolingual and parallel data forcontinued pre-training.",
  "Conclusion and Future Work": "In this work, we conduct an in-depth analysis offine-tuning LLMs for translation. We demonstratethat LLMs is capable of translating in multipledirections after being fine-tuned with minimal low-quality training data in a single direction. Whilethis suggests pre-trained LLMs inherently possessmultilingual translation capabilities which onlyneed to be unlocked by aligning with the correcttask format, we discover pitfalls and lessons inaligning LLMs; while LLMs make efforts to adjustto the translation task, they are good at imitatingother patterns such as the noise in the parallel data.Future work could explore robust training methodsthat align LLMs with translation while minimizingthe risk of overfitting to low-quality data.",
  "This work offers a range of insights into fine-tuningLLMs for translation. However, our study is not ex-haustive and is subject to the following limitations": "Model size and diversity.Throughout our sys-tematic study, we fine-tuned Llama-2 7B, Llama-212B, and Mistral 7B. These are strong and feasibleoptions when the work is carried out. It is impor-tant to verify the generalizability of our findingsto models with different capabilities or of differentsizes. Non-English centric MT.Our evaluation isEnglish-centric, which is the condition of mostLLM pre-training. Findings will be more compre-hensive if future work can extend it to translationdirections not involving English.",
  "State-of-the-art performance.Our research pri-marily explores how SFT enables LLM to trans-": "late to uncover data-efficient strategies in SFT andidentify associated pitfalls. Recent studies havedemonstrated that translation capabilities can befurther enhanced through techniques such as con-tinual pre-training (Xu et al., 2024a; Alves et al.,2024) and preference learning (Xu et al., 2024b;Zhu et al., 2024). However, these methods requiresignificantly more training resources, which maypose challenges when applied to large models. Fine-tuning methods.Throughout this work, weperform SFT with full-parameter updates. It isworthwhile to explore parameter-efficient methodswhich bring in heavier regularization to understandwhether they exhibit patterns similar to those ob-served in our work.",
  "Our works sole aim is to study the influence ofdata factors in applying supervised fine-tuning tolarge language models. We expect minimal socialrisks to be associated with our efforts": "We sincerely thank the reviewers of this work fortheir constructive and insightful feedback.Pinzhen Chen and Barry Haddow received fund-ing from UK Research and Innovation (UKRI) un-der the UK governments Horizon Europe fund-ing guarantee [grant number 10052546]. MiaoranZhang received funding from the DFG (German Re-search Foundation) under project 232722074, SFB1102. We thank EIT and IDT High PerformanceComputing Center for providing computational re-sources for this project. Sweta Agrawal, Chunting Zhou, Mike Lewis, LukeZettlemoyer, and Marjan Ghazvininejad. 2023. In-context examples selection for machine translation.In Findings of the Association for ComputationalLinguistics: ACL 2023. Farhad Akhbardeh, Arkady Arkhangorodsky, Mag-dalena Biesialska, Ondrej Bojar, Rajen Chatter-jee, Vishrav Chaudhary, Marta R. Costa-jussa,Cristina Espaa-Bonet, Angela Fan, Christian Fe-dermann, Markus Freitag, Yvette Graham, Ro-man Grundkiewicz, Barry Haddow, Leonie Harter,Kenneth Heafield, Christopher Homan, MatthiasHuck, Kwabena Amponsah-Kaakyire, Jungo Kasai,Daniel Khashabi, Kevin Knight, Tom Kocmi, PhilippKoehn, Nicholas Lourie, Christof Monz, MakotoMorishita, Masaaki Nagata, Ajay Nagesh, Toshiaki Nakazawa, Matteo Negri, Santanu Pal, Allahsera Au-guste Tapo, Marco Turchi, Valentin Vydrin, and Mar-cos Zampieri. 2021. Findings of the 2021 conferenceon machine translation (WMT21). In Proceedings ofthe Sixth Conference on Machine Translation. Duarte M. Alves, Nuno M. Guerreiro, Joo Alves, JosPombal, Ricardo Rei, Jos de Souza, Pierre Colombo,and Andre Martins. 2023. Steering large languagemodels for machine translation with finetuning andin-context learning. In Findings of the Associationfor Computational Linguistics: EMNLP 2023. Duarte M. Alves, Jos Pombal, Nuno M Guerreiro, Pe-dro H Martins, Joo Alves, Amin Farajian, Ben Pe-ters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal,et al. 2024.Tower: An open multilingual largelanguage model for translation-related tasks. arXivpreprint. Mikel Artetxe, Sebastian Ruder, and Dani Yogatama.2020. On the cross-lingual transferability of mono-lingual representations. In Proceedings of the 58thAnnual Meeting of the Association for ComputationalLinguistics. Loc Barrault, Magdalena Biesialska, Ondrej Bo-jar, Marta R. Costa-juss, Christian Federmann,Yvette Graham, Roman Grundkiewicz, Barry Had-dow, Matthias Huck, Eric Joanis, Tom Kocmi,Philipp Koehn, Chi-kiu Lo, Nikola Ljubeic, ChristofMonz, Makoto Morishita, Masaaki Nagata, Toshi-aki Nakazawa, Santanu Pal, Matt Post, and MarcosZampieri. 2020. Findings of the 2020 conference onmachine translation (WMT20). In Proceedings ofthe Fifth Conference on Machine Translation. Loc Barrault, Ondrej Bojar, Marta R. Costa-juss,Christian Federmann, Mark Fishel, Yvette Gra-ham, Barry Haddow, Matthias Huck, Philipp Koehn,Shervin Malmasi, Christof Monz, Mathias Mller,Santanu Pal, Matt Post, and Marcos Zampieri. 2019.Findings of the 2019 conference on machine trans-lation (WMT19). In Proceedings of the Fourth Con-ference on Machine Translation (Volume 2: SharedTask Papers, Day 1). Ondrej Bojar, Rajen Chatterjee, Christian Federmann,Yvette Graham, Barry Haddow, Shujian Huang,Matthias Huck, Philipp Koehn, Qun Liu, VarvaraLogacheva, Christof Monz, Matteo Negri, Matt Post,Raphael Rubino, Lucia Specia, and Marco Turchi.2017. Findings of the 2017 conference on machinetranslation (WMT17). In Proceedings of the SecondConference on Machine Translation. Ondrej Bojar, Christian Federmann, Mark Fishel, YvetteGraham, Barry Haddow, Matthias Huck, PhilippKoehn, and Christof Monz. 2018. Findings of the2018 conference on machine translation (WMT18).In Proceedings of the Third Conference on MachineTranslation: Shared Task Papers.",
  "role of incidental bilingualism in PaLMs translationcapability. In Proceedings of the 61st Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers)": "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. In Advances in Neural Information Process-ing Systems. Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, KalpaGunaratna, Vikas Yadav, Zheng Tang, Vijay Srini-vasan, Tianyi Zhou, Heng Huang, et al. 2024a. Al-pagasus: Training a better Alpaca model with fewerdata. In The Twelfth International Conference onLearning Representations. Pinzhen Chen, Zhicheng Guo, Barry Haddow, and Ken-neth Heafield. 2024b. Iterative translation refinementwith large language models. In Proceedings of the25th Annual Conference of the European Associationfor Machine Translation (Volume 1). Pinzhen Chen, Shaoxiong Ji, Nikolay Bogoychev, An-drey Kutuzov, Barry Haddow, and Kenneth Heafield.2024c. Monolingual or multilingual instruction tun-ing: Which makes a better Alpaca. In Findings of theAssociation for Computational Linguistics: EACL2024. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.2023. Vicuna: An open-source chatbot impressingGPT-4 with 90%* ChatGPT quality. lmsys.org.",
  "Zero-shot cross-lingual transfer in instruction tuningof large language models. In Proceedings of the 17thInternational Natural Language Generation Confer-ence": "Rochelle Choenni, Dan Garrette, and Ekaterina Shutova.2023. How do languages influence each other? study-ing cross-lingual data sharing during LM fine-tuning.In Proceedings of the 2023 Conference on EmpiricalMethods in Natural Language Processing. Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, et al.2024. Scaling instruction-finetuned language models.Journal of Machine Learning Research.",
  "Marjan Ghazvininejad, Hila Gonen, and Luke Zettle-moyer. 2023. Dictionary-based phrase-level prompt-ing of large language models for machine translation.arXiv preprint": "Sreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Ku-mar, Ramaneswaran S, Deepali Aneja, Zeyu Jin, Ra-mani Duraiswami, and Dinesh Manocha. 2024. Acloser look at the limitations of instruction tuning. InProceedings of the 41st International Conference onMachine Learning. Zhiwei He, Tian Liang, Wenxiang Jiao, ZhuoshengZhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shum-ing Shi, and Xing Wang. 2024. Exploring human-like translation strategy with large language models.Transactions of the Association for ComputationalLinguistics.",
  "Towards effective disambiguation for machine trans-lation with large language models. In Proceedings ofthe Eighth Conference on Machine Translation": "Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023. Mistral7B. arXiv preprint. Wenxiang Jiao, Jen-tse Huang, Wenxuan Wang, Zhi-wei He, Tian Liang, Xing Wang, Shuming Shi, andZhaopeng Tu. 2023. ParroT: Translating during chatusing large language models tuned with human trans-lation and feedback. In Findings of the Associationfor Computational Linguistics: EMNLP 2023.",
  "Tannon Kew, Florian Schottmann, and Rico Sennrich.2023. Turning english-centric LLMs into polyglots:How much multilinguality is needed? arXiv preprint": "Tom Kocmi, Rachel Bawden, Ondrej Bojar, AntonDvorkovich, Christian Federmann, Mark Fishel,Thamme Gowda, Yvette Graham, Roman Grund-kiewicz, Barry Haddow, Rebecca Knowles, PhilippKoehn, Christof Monz, Makoto Morishita, MasaakiNagata, Toshiaki Nakazawa, Michal Novk, MartinPopel, and Maja Popovic. 2022. Findings of the 2022conference on machine translation (WMT22). InProceedings of the Seventh Conference on MachineTranslation (WMT).",
  "in-context learning. In The Twelfth InternationalConference on Learning Representations": "Zhuoyuan Mao and Yen Yu. 2024. Tuning LLMs withcontrastive alignment instructions for machine trans-lation in unseen, low-resource languages. In Pro-ceedings of the Seventh Workshop on Technologiesfor Machine Translation of Low-Resource Languages(LoResMT 2024). Swaroop Mishra, Daniel Khashabi, Chitta Baral, andHannaneh Hajishirzi. 2022. Cross-task generaliza-tion via natural language crowdsourcing instructions.In Proceedings of the 60th Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers). Eric Mitchell, Rafael Rafailov, Archit Sharma, ChelseaFinn, and Christopher D Manning. 2024. An em-ulator for fine-tuning large language models usingsmall language models. In The Twelfth InternationalConference on Learning Representations. Yasmin Moslem, Rejwanul Haque, John D. Kelleher,and Andy Way. 2023. Adaptive machine translationwith large language models. In Proceedings of the24th Annual Conference of the European Associationfor Machine Translation. Niklas Muennighoff, Thomas Wang, Lintang Sutawika,Adam Roberts, Stella Biderman, Teven Le Scao,M Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai-ley Schoelkopf, Xiangru Tang, Dragomir Radev, Al-ham Fikri Aji, Khalid Almubarak, Samuel Albanie,Zaid Alyafeai, Albert Webson, Edward Raff, andColin Raffel. 2023.Crosslingual generalizationthrough multitask finetuning. In Proceedings of the61st Annual Meeting of the Association for Computa-tional Linguistics (Volume 1: Long Papers). Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al.2022. Training language models to follow instruc-tions with human feedback. In Advances in NeuralInformation Processing Systems 35. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalu-ation of machine translation. In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics.",
  "Vikas Raunak, Amr Sharaf, Hany Hassan Awadallah,and Arul Menezes. 2023. Leveraging GPT-4 forautomatic translation post-editing. In Findings of theAssociation for Computational Linguistics: EMNLP2023": "Ricardo Rei, Craig Stewart, Ana C Farinha, and AlonLavie. 2020. COMET: A neural framework for MTevaluation. In Proceedings of the 2020 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP). Victor Sanh, Albert Webson, Colin Raffel, Stephen HBach, Lintang Sutawika, Zaid Alyafeai, AntoineChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,et al. 2022. Multitask prompted training enables zero-shot task generalization. In International Conferenceon Learning Representations. Gabriele Sarti, Phu Mon Htut, Xing Niu, Benjamin Hsu,Anna Currey, Georgiana Dinu, and Maria Nadejde.2023. RAMP: Retrieval and attribute-marking en-hanced prompting for attribute-controlled translation.In Proceedings of the 61st Annual Meeting of theAssociation for Computational Linguistics (Volume2: Short Papers). Rico Sennrich, Barry Haddow, and Alexandra Birch.2016. Improving neural machine translation modelswith monolingual data. In Proceedings of the 54thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers). Uri Shaham, Jonathan Herzig, Roee Aharoni, IdanSzpektor, Reut Tsarfaty, and Matan Eyal. 2024. Mul-tilingual instruction tuning with just a pinch of multi-linguality. In Findings of the Association for Compu-tational Linguistics ACL 2024.",
  "Where does in-context translation happen in largelanguage models. arXiv preprint": "David Stap, Eva Hasler, Bill Byrne, Christof Monz, andKe Tran. 2024. The fine-tuning paradox: Boostingtranslation quality without sacrificing LLM abilities.In Proceedings of the 62nd Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers). Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, YannDubois, Xuechen Li, Carlos Guestrin, Percy Liang,and Tatsunori B. Hashimoto. 2023. Stanford Alpaca:An instruction-following LLaMA model. GitHubrepository.",
  "open-access multilingual language model. In Pro-ceedings of the 62nd Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers)": "David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo,Viresh Ratnakar, and George Foster. 2023. Prompt-ing PaLM for translation: Assessing strategies andperformance.In Proceedings of the 61st AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers). Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,Adams Wei Yu, Brian Lester, Nan Du, Andrew M.Dai, and Quoc V Le. 2022. Finetuned language mod-els are zero-shot learners. In International Confer-ence on Learning Representations. Di Wu, Shaomu Tan, Yan Meng, David Stap, andChristof Monz. 2024a. How far can 100 samplesgo? unlocking zero-shot translation with tiny multi-parallel data. In Findings of the Association for Com-putational Linguistics ACL 2024.",
  "Minghao Wu, Thuy-Trang Vu, Lizhen Qu, George Fos-ter, and Gholamreza Haffari. 2024b. Adapting largelanguage models for document-level machine trans-lation. arXiv preprint": "Minghao Wu, Abdul Waheed, Chiyu Zhang, Muham-mad Abdul-Mageed, and Alham Aji. 2024c. LaMini-LM: A diverse herd of distilled models from large-scale instructions. In Proceedings of the 18th Confer-ence of the European Chapter of the Association forComputational Linguistics. Shijie Wu and Mark Dredze. 2019. Beto, bentz, becas:The surprising cross-lingual effectiveness of BERT.In Proceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the 9thInternational Joint Conference on Natural LanguageProcessing (EMNLP-IJCNLP). Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Has-san Awadalla. 2024a. A paradigm shift in machinetranslation: Boosting translation performance oflarge language models. In The Twelfth InternationalConference on Learning Representations. Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan,Lingfeng Shen, Benjamin Van Durme, Kenton Mur-ray, and Young Jin Kim. 2024b. Contrastive prefer-ence optimization: Pushing the boundaries of LLMperformance in machine translation. In Proceedingsof the 41st International Conference on MachineLearning.",
  "Biao Zhang, Barry Haddow, and Alexandra Birch.2023a. Prompting large language model for machinetranslation: a case study. In Proceedings of the 40thInternational Conference on Machine Learning": "Biao Zhang, Zhongtao Liu, Colin Cherry, and OrhanFirat. 2024. When scaling meets LLM finetuning:The effect of data, model and finetuning method. InThe Twelfth International Conference on LearningRepresentations. Shaolei Zhang, Qingkai Fang, Zhuocheng Zhang, Zhen-grui Ma, Yan Zhou, Langlin Huang, Mengyu Bu,Shangtong Gui, Yunji Chen, Xilin Chen, et al. 2023b.BayLing: Bridging cross-lingual alignment and in-struction following through interactive translation forlarge language models. arXiv preprint. Xuan Zhang, Navid Rajabi, Kevin Duh, and PhilippKoehn. 2023c. Machine translation with large lan-guage models: Prompting, few-shot learning, andfine-tuning with QLoRA.In Proceedings of theEighth Conference on Machine Translation. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer,Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, PingYu, Lili Yu, et al. 2023. LIMA: Less is more foralignment. In Thirty-seventh Conference on NeuralInformation Processing Systems. Dawei Zhu, Sony Trenous, Xiaoyu Shen, DietrichKlakow, Bill Byrne, and Eva Hasler. 2024.Apreference-driven paradigm for enhanced translationwith large language models. In Proceedings of the2024 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies.",
  "AModel Performance with VaryingTraining Sample Sizes": "In and , we present the perfor-mance for instruction-tuned baselines and our mod-els on different evaluation directions. For mostdirections, using only 32 training samples canachieve competitive performance and beat all threeinstruction-tuned baselines. There are several ex-ceptional cases, including enzh and enja, inwhich the COMET score of SFT with a limitednumber of samples (32 or 64) is worse than 1-shotin-context learning.While we primarily report the results withLlama-2 7B in our experiments, we hypothesizethat state-of-the-art LLMs are largely homoge-neous in terms of language distribution and inher-ent translation capability making our findings ap-plicable to other LLMs. To support this hypothesis,we conduct fine-tuning experiments with Mistral7B and Llama-2 13B using varying data sizes: 32,1024, and 70K. As shown in , the generaltrend is quite similar to the Llama-2 7B case: fine-tuning with 32 examples results in competitive per-formance, matching or surpassing general-purposeinstruction-tuned models. Furthermore, increasingthe number of training examples leads to diminish-ing returns.",
  "BModel Performance with VaryingTraining Directions": "shows normalized BLEU scores for dif-ferent combinations of train and test translationdirections. Similar to the COMET scores in Fig-ure 2, we observe that when training the model on asingle direction, its translation ability across othernon-targeted directions is also elicited to a certaindegree. It is worth noting that when the trainingdirection is Xen, the performance on directionsenX is significantly worse than training on alldirections.",
  "EModel Performance with Noisy Data": "shows the BLEU score of different trans-lation directions with two noise types. We canfind that models are more sensitive to word-levelnoise than sentence-level noise. Also, the perfor-mance degradation is more noticeable when inject-ing noise into the source translation side. In com-parison to the results of size 1024, using 32 trainingexamples still achieves comparable or even betterperformance in the noisy condition.",
  "F.1Datasets": "Our parallel data is derived from the developmentand test sets of WMT17 through WMT22. Detaileddataset statistics are available in . For mostexperiments, we use the test sets from WMT17 toWMT20 for training. The test set from WMT22 isused specifically for testing. An exception is notedin Section section 3.4, where models are trainedusing the enha and enis language pairs fromWMT21s development set. Subsequently, thesemodels are evaluated using the corresponding testsets from WMT21.",
  "F.4Hardware specifications and runtime": "Our experiments are conducted on a computingnode with either 8 NVIDIA A100-40GB GPUs or8 H100-80GB GPUs. DeepSpeed11 with zero-stage1 and mixed precision bfloat16 is used for perform-ing SFT. Given the limited dataset size, typicallyfewer than 1024 samples, each SFT experiment canbe completed within a mere 15 minutes using fourH100 GPUs. However, given the necessity to eval-uate the models across more than ten translationdirections, the evaluation process may require up tofour hours when performed on a single A100-40GBGPU.",
  "Translate from[SRC]to[TGT] :": ": A collection of 31 translation prompts. Each instruction is randomly selected to form a training sample. Atinference time, the first instruction is always selected. The placeholders [SRC] and [TGT] represent the sourceand target languages, respectively, and will be replaced with the appropriate languages depending on the specificexample at hand. 1* 3* 72.074.076.078.080.082.084.0 Evaluation on cs en 1* 3* 72.074.076.078.080.082.084.0 Evaluation on en cs 1* 3* 74.0 76.0 78.0 80.0 82.0 84.0 Evaluation on de en 1* 3*",
  "Evaluation on en de": "1* 3* 66.068.070.072.074.076.078.080.0 Evaluation on zh en 1* 3* 76.077.078.079.080.081.082.083.0 Evaluation on en zh 1* 3* 68.070.072.074.076.078.080.0 Evaluation on ja en 1* 3* 76.078.080.082.084.086.0 Evaluation on en ja 1* 3* 76.0 78.0 80.0 82.0 84.0 Evaluation on ru en 1* 3* 72.074.076.078.080.082.084.0 Evaluation on en ru 1* 3* 68.070.072.074.076.078.080.082.0 Evaluation on en hr Llama-2-7b ICL-MTLlama-2-7b SFT-MTvicuna-7b-v1.5Mistral-7B-v0.1Llama-2-7b-chat-hf : COMET scores between instruction-tuned baselines and our models at different training data sizes,evaluated on individual translation directions. ICL is used for training sizes at or below 3, indicated with \"\";otherwise, we perform SFT. With only 32 examples for SFT, Llama-2 outperforms general-purpose, instruction-tuned baselines. Base.: instruction-tuned baseline models. 1* 3* 22.525.027.530.032.535.037.540.0 Evaluation on cs en 1* 3* 14.015.016.017.018.019.020.021.0 Evaluation on en cs 1* 3* 20.0 22.0 24.0 26.0 28.0",
  "Evaluation on en ja": "1* 3* 24.026.028.030.032.034.036.038.0 Evaluation on ru en 1* 3* 12.0 14.0 16.0 18.0 20.0 22.0 Evaluation on en ru 1* 3* 11.012.013.014.015.016.017.0 Evaluation on en hr Llama-2-7b ICL-MTLlama-2-7b SFT-MTvicuna-7b-v1.5Mistral-7B-v0.1Llama-2-7b-chat-hf : BLEU scores between instruction-tuned baselines and our models at different training data sizes, evaluatedon individual translation directions. ICL is used for training sizes at or below 3, indicated with \"\"; otherwise, weperform SFT. With only 32 examples for SFT, Llama-2 outperforms general-purpose, instruction-tuned baselines.Base.: instruction-tuned baseline models. Model Performance (COMET) Llama-2 13b",
  "Mistral": "7b 79.5 78.59 82.29 78.95 83.97 82.86 84.48 83.63 Instruct32102474623 : Performance comparison between instruction-tuned baselines and fine-tuned models with different trainingdata sizes. Instruct refers to the instruction-tuned baselines, specifically Mistral-7B-Instruct-v0.1 and Llama-2-13b-chat. \"32/1024/74623\" represents models fine-tuned on 32, 1024, and 74623 examples, using pre-trained onlymodels: Mistral-7B-v0.1 and Llama-2-13b. csen deen jaen ruen zhen encs ende enhr enja enru enzh Test direction all dir. deen zhen ende enzh frde defr Train direction 99.498.196.210195.917.214.216.16.19.23.0 97.996.994.610010223.227.917.38.610.23.1 59.186.577.380.693.810410210695.8101107 65.569.321.069.426.210099.510110399.5106 56.082.686.991.895.286.787.397.892.988.298.0 49.780.872.561.693.393.795.610210696.8101 0.2 0.4 0.6 0.8 1.0 : Model performance (%) in BLEU score resulted from varying combinations of train and test translationdirections. The scores are normalized according to Llama-2 fine-tuned on all 10 training directions. enis enha encsende enhr enja enruenzh Test direction (enX) BLEU isen haen csen deen jaen ruen zhen Test direction (Xen) Training direction enis enha ende : Model performance evaluated across 15 translation directions. While models trained on unseenlanguages (enis, enha) exhibit moderate improvements in translating these languages, they demonstrateaccurate translations from and to seen languages. endeenhadeenhaen Training direction BLEU AvgenX endeenhadeenhaen Training direction BLEU AvgXen sent. noise(32) word noise(32) clean(32) sent. noise(1024) word noise(1024) clean(1024)"
}