{
  "Abstract": "In this paper, we investigate whether LargeLanguage Models (LLMs) actively recall orretrieve their internal repositories of factualknowledge when faced with reasoning tasks.Through an analysis of LLMs internal factualrecall at each reasoning step via KnowledgeNeurons, we reveal that LLMs fail to harnessthe critical factual associations under certaincircumstances. Instead, they tend to opt foralternative, shortcut-like pathways to answerreasoning questions. By manually manipulat-ing the recall process of parametric knowledgein LLMs, we demonstrate that enhancing thisrecall process directly improves reasoning per-formance whereas suppressing it leads to no-table degradation. Furthermore, we assess theeffect of Chain-of-Thought (CoT) prompting,a powerful technique for addressing complexreasoning tasks. Our findings indicate that CoTcan intensify the recall of factual knowledgeby encouraging LLMs to engage in orderly andreliable reasoning. Furthermore, we exploredhow contextual conflicts affect the retrieval offacts during the reasoning process to gain acomprehensive understanding of the factual re-call behaviors of LLMs.",
  "Introduction": "Recent advancements in Large Language Mod-els have underscored their exceptional reason-ing prowess with natural language understandingacross a broad spectrum of tasks (Chen et al.,2023a; Kojima et al., 2022; Brown et al., 2020;Creswell et al., 2023).However, amidst theseachievements, a specific form of reasoning hasbeen somewhat overlooked and insufficiently inves-tigated: reasoning tasks that require the utilizationof internal factual knowledge associations. For in-stance, when presented with a 2-hop question suchas \"Who is the chairperson of the manufacturer",
  ": An unsuccessful case of reasoning due tofactual retrieval failure of the triplet (General Motors,chairperson, Marry Barra)": "of the Holden Caprice?\" in , LLMs mustfirst identify that the manufacturer of the HoldenCaprice is General Motors, and subsequently re-trieve the name of General Motors chairpersonfrom their internal knowledge, also referred toas parametric knowledge (Neeman et al., 2023;Zhong et al., 2024). Previous work has shown fac-tual knowledge emerges in both GPT (Meng et al.,2022) and Bert models (Petroni et al., 2019; Jianget al., 2020). Unlike mathematical (Floyd, 2007)and logical reasoning (Pan et al., 2023), factualreasoning heavily relies on the factual knowledgeencoded within LLMs, acquired through extensivepretraining on vast corpora, rather than on user-inputted premises. At the same time, it differsfrom commonsense reasoning (Zhao et al., 2023;Trinh and Le, 2019), which taps into general knowl-edge acquired through dynamic training to fostera holistic understanding of the world, instead ofemphasizing specific factual information.Intuitively, it is reasonable to expect LLMsto harness extensive parametric knowledge totackle reasoning tasks. Yet, an important ques-tion emerges: How effectively can LLMs actuallyretrieve and utilize their internal knowledge forreasoning purposes? Delving into this question is crucial for several reasons. First, efficient use ofparametric knowledge may significantly reduce re-liance on external data sources, thereby loweringoperational costs of data retrieval and API usage.Second, this dynamic capability allows the knowl-edge within LLMs to flow and interconnect (Onoeet al., 2023), showcasing these models as organicentities rather than static information repositories(Petroni et al., 2019). From a practical perspective,the accurate retrieval and application of parametricknowledge lead to more reliable and interpretablereasoning, enhancing their utility and trustworthi-ness in real-world applications.Transformer-based language models have accu-mulated substantial knowledge through extensivepretraining (Vaswani et al., 2017). A significantbody of recent research has focused on the factual-ity issues of LLMs (Wang et al., 2023). One streamof this research has concentrated on pinpointing thelocations within these models architectures wherefactual knowledge is stored and encoded (Menget al., 2022; Dai et al., 2022; Wallat et al., 2020;Geva et al., 2022, 2021). Simultaneously, there hasbeen a concerted effort to understand the mecha-nism by which this knowledge is accessed duringthe inference phase (Geva et al., 2023; Yang et al.,2024). Another line of work discusses the balanceof the retrieved knowledge and its parametric coun-terparts (Kwiatkowski et al., 2019; Kandpal et al.,2022; Yu et al., 2023). However, the majority ofthese studies have either been confined to elemen-tary retrieval tasks, such as recalling a single factobject o from a given triplet (s, r, o), or have notdelved into the intricacies of factual knowledge re-call and utilization in more advanced challenges,particularly within complex reasoning scenarios.Our work addresses these limitations by examiningthe inner dynamics of factual recall within LLMsduring the two-hop factual reasoning process, pro-viding fresh insights into the behavior of factualrecall in reasoning and highlighting avenues for en-hancing the robustness and reliability of reasoningthrough more sophisticated knowledge utilizationstrategies.In this work, we investigate the harness of inter-nal knowledge for reasoning through the lens ofKnowledge Neurons (KNs). We focus on the basicsetting of factual reasoning involving the composi-tion of two facts (for example, \"Who is the chair-person of the manufacturer of Holden Caprice?\" in). To achieve this, we carefully craft two-hop reasoning questions dataset that seamlessly integrates with the KN technique. We assess thelevel of factual recall at each reasoning step by in-troducing a novel metric, KN Scores. We examineKN Scores under three conditions of two-hop rea-soning: no CoT, zero-shot CoT, and few-shot CoT,unveiling the pitfalls existing in the reasoning pro-cess and the enhancement effect of CoT (Wei et al.,2022). Then we conduct targeted interventions onKNs to enhance or suppress the factual retrievalprocess, finding the contributing impact on reason-ing performance. Furthermore, we provide a de-tailed analysis of factual shortcuts (Ju et al., 2024;Du et al., 2023; Li et al., 2024), potentially causedby redundant information stored in models param-eters within LLMs used for reasoning. Finally, weexplore how the presence of knowledge conflictoutside LLMs influences the factual recall process.Our findings can be summarized as follows: LLMs do not consistently retrieve the pertinentfactual knowledge essential for reasoning, withmore than a third of reasoning errors stemmingfrom deficiencies in the retrieval of factual as-sociations.",
  "Problem Formulation": "We represent facts, such as \"(Holden Caprice, man-ufacturer, General Motors)\", as a triplet (s, r, o),where s is the subject, r is the relation, and ois the object. We formulate two-hop factual rea-soning questions as a composition of two linkedfacts ((s, r1, o1), (o1, r2, o2)), with a bridge entityo1 connecting them. To query LLMs, these tripletsmust be converted into natural language queries.For a single relation r, we instruct ChatGPT (gpt-3.5-turbo) to generate query templates as QTr().For instance, the single-relation triplet (HoldenCaprice, manufacturer, General Motors) can beconverted as QTmanufacturer(HoldenCaprice):\"Which company manufactures Holden Caprice?\". Similarly, for a composition of two relations r1and r2, we prompt ChatGPT to generate a querytemplate as QTr2(r1()), with r1() denoting thedescription of the entity related to s via r1 rela-tion (e.g. The manufacturer of Holden Caprice).We refer to the single-hop query as QT1H and thetwo-hop query as QT2H.We consider an autoregressive language modelF : X Y , which accepts an input x X andproduces a prediction y Y , continuing the inputx. We deem that the model \"knows\" a fact (s, r, o)if the output F(QTr(s)) matches the ground la-bel o and that LLMs can reason a question involv-ing two-hop fact triplets ((s, r1, o1), (o1, r2, o2))successfully if the output F(QTr2(r1(s)) matchesthe ground label o2. It is noteworthy that querytemplates, even for the same single relation, aregenerated with diversity by ChatGPT. This diver-sity discourages models from making predictionsbased on the occurrence of specific words, ensur-ing that they recall knowledge from within them-selves instead. We denote the set of two-hop factualquestions as , with T representing the subset ofquestions that LLMs can answer correctly and Fdenoting the subset of questions that LLMs cannotanswer correctly. For simplicity, we use to denote((s, r1, o1), (o1, r2, o2)), thus we have:",
  "Knowledge Neurons": "Pretrained language models store vast amounts offactual knowledge and have a strong ability to re-call this factual knowledge without further training(Petroni et al., 2019; Jiang et al., 2020). Drawinginspiration from the key-value-memory nature offeed-forward layers (Geva et al., 2021), Dai et al.(2022) proposes that factual knowledge is storedin specific neurons within the Feed-Forward Net-works (FFNs) of the Transformer models, termedas knowledge neurons. They find that knowledgeneurons are activated by knowledge-expressingprompts. The higher the activation of these knowl-edge neurons is, the more significantly their corre-sponding facts are expressed. Therefore, to assessthe recall and utilization of the fact triplet (s, r, o)necessary in the reasoning process, we refer to theactivity of KNs as an indicator of factual recall. Wemake the following invariant assumptions: theKNs responsible for the expression of particular relational facts remain consistent across differentapplication contexts. A specific fact is indicated bythe same set of KNs under both single-hop queriesand reasoning queries, which is a cornerstone forsubsequent experiments. In Appendix B, We de-tail a methodology that utilizes integrated gradi-ent (Sundararajan et al., 2017) method to computethe contribution of all neurons in the intermedi-ate layers of FFNs to the correct prediction of amulti-token ground truth, identifying neurons withgreater contributions as KNs.",
  "TFRKN: Two-hop Factual Reasoningfor Knowledge Neurons": "To investigate the behavior of factual recall in rea-soning tasks for LLMs, we have developed a spe-cialized dataset for knowledge neurons called Two-hop Factual Reasoning for Knowledge Neurons,TFRKN. Dataset ConstructionOur dataset consists oftwo-hop factual questions, where each question in-volves two facts that are connected by an intermedi-ate entity. LLMs are more likely to recall triplets re-lated to popular entities(Mallen et al., 2023). There-fore, for entity selection, we use the cumulativepageview count over the past 12 months as a metricand select the top 500 popular entities from Wiki-data (Vrandecic and Krtzsch, 2014) based on thiscriterion. Two-hop fact triplets are then extractedfrom sub-graphs consisting solely of a set of man-ually selected relations and entities. To identifyKNs for each-hop fact, we reformulate each facttriplet into more than five varied natural questionsusing ChatGPT (Appendix A). The TFRKN datasetencompasses 4,550 distinct instances covering 213unique relational combinations with a sample in-stance shown in .",
  "Diagnose the Pitfalls of Factual Recallin Reasoning": "In the realm of two-hop factual reasoning, an opti-mal and dependable reasoning trajectory is a multi-hop reasoning approach (Welbl et al., 2017; Juet al., 2024). This process requires identifying thebridge entity first and then using it to solve the sec-ond hop question, necessitating that LLMs recallthe relevant fact at each hop step by step, culminat-ing in the formulation of the correct answers. Inthis section, we investigate whether LLMs faith-fully retrieve factual knowledge at each hop whenundertaking reasoning tasks. : Scaled visualization of neuron activitieswithin the intermediate layers of FFNs in Mistral-7Bfor the same case (A 32-layer14336-neuron matrix).The vertical axis shows the depth of layers, while thehorizontal axis shows the neuron index in the FFNs in-termediate layers. It is evident that KNs are distributedin the middle and final layers.",
  "li, li (5)": "where H(l) represents the input to the FFN of the l-th layer, which consists of the outputs from the l-thattention layer combined with the residual stream;li denotes the i-th neuron in the l-th intermedi-ate layer of FFN; represents the KNs associ-ated with a specific fact triplet, denoted as (s, r, o);|| denotes the size of the set, i.e., the number ofKNs; and SiLU denotes the activation function.For the first-hop and second-hop fact, we designatetheir respective sets of KNs as 1 and 2. Underthe context of a single-hop query, we denote KNScores as {|QT1H}. Similarly, within the two-hop reasoning context, KN Scores are representedas {|QT2H}.",
  "Experiment": "SetupWe investigate the utilization of individualfact triplets in correctly answered two-hop ques-tions by analyzing the KN Scores for each triplet.We compare these scores with those observed dur-ing single-hop queries to establish a threshold, de-noted as , which serves as a benchmark for iden-tifying the effective use of facts in the reasoningprocess. If the activation level of KNs falls signifi-cantly below this threshold in comparison to single-hop queries, this indicates an under-utilization ofthe corresponding fact. Conversely, if it exceeds thethreshold, the fact is considered adequately utilized.Using this criterion, we classified the correctly an-swered questions into four distinct categories: (1)FT: Unsuccessful recall of the first-hop fact butsuccessful second-hop recall; (2) TF: Successfulfirst-hop recall but unsuccessful second-hop recall;(3) FF: Neither fact successfully recalled and (4)TT: Both facts successfully recalled. Except for TT,the other three situations are defined as Shortcuts.",
  ": KN Scores for three conditions across threemodels. is the KN Score of a specific fact while indicates the change ratio (in percentages) of valuescompared with the single-hop baselines": "the recall of each fact under three distinct experi-mental conditions: no CoT, zero-shot CoT, andfew-shot CoT. For each condition, we record KNScores for both the first-hop {1|QT2H} and thesecond-hop {2|QT2H} facts within the context oftwo-hop reasoning questions. We select the KNScores {1|QT1H} and {2|QT1H} under single-hop queries as baselines since KNs are significantlyactive in that straightforward context. We experi-ment with the instructed versions of three popularopen-source models: LLaMA2-7B (Touvron et al.,2023), LLaMA3-8B, Mistral-7B (Jiang et al., 2023)(see Appendix C for more experimental details).",
  "Results": "Finding 1In , more than one-third ofreasoning failures are caused by issues of factualretrieval. The ER values show a consistent and pro-gressive increase as the interventions progress fromtargeting 1, to KNs associated with the second-hop 2, and ultimately to a combined interventionon both, 12. This pattern indicates that many ini-tially incorrect answers stem from retrieval failure",
  ": Overall reasoning performance on TFRKN under different CoT situations": "setting, as shown in . We posit that thisenhancement is likely driven by the step-by-stepthinking process, which further stimulates therecall of facts as multi-hop reasoning progresses.This hypothesis can be supported by comparing thezero-shot and few-shot CoT settings. Across threemodels, it is clear that zero-shot CoT strugglesto significantly improve the recall of the second-hop fact compared to the reinforcement of the first-hop fact recall. However, consistent improvementacross both triplets can be observed for few-shotsettings. This observation strongly suggests thatthe reasoning direction in zero-shot scenarios isunclear, which prevents models from effectivelyidentifying which relations of facts concerning thebridge entity to retrieve. In stark contrast, few-shotscenarios often mitigate this issue. Through theacquisition of knowledge from contextual demon-strations, models are more inclined to determinethe subsequent phase in the reasoning trajectoryand, in turn, adeptly utilize the relevant factual in-formation via their attention mechanisms. Factual Recall vs. Reasoning AccuracyThecombination of and illustratesa positive correlation between the recall of rele-vant fact triplets and reasoning accuracy. This re-lationship is especially pronounced in the case ofLLaMA3-8B model under few-shot CoT, where themaximum increase in the recall of both 1 and2 leads to the highest reasoning accuracy. How-ever, the eliciting effect of CoT on factual recallacross various LLMs is not uniform. For instance,zero-shot CoT mitigates the forgetting of factual in-formation to some extent for LLaMA2-7B, whereasfor LLaMA3-8B, zero-shot CoT enhances the re-trieval of factual information to a level comparableto few-shot CoT. This adequately illustrates that the",
  "Enhance and Suppress KNs": "To gain a deeper understanding of factual recallbehaviors, we intervene in the retrieval of specificknowledge within LLMs by manually adjustingthe activation levels of KNs. Specifically for eachfactual triplet (s, r, o), we modulate the internalrecall by adjusting the values of the KNs associatedwith this triplet, either amplifying or diminishingthem according to Equation 6.",
  "r-5.789.02-12.1225.54-2.523.65": ": Results of the controlled experiments afterinterventions on 1, 2 and 12 under no CoT setting. denotes variation in accuracy and r is established asthe baseline for enhancing or suppressing KNs of bothfacts, with ER/SR values expressed as percentages. MetricsWe design a novel metric, termed En-hance Ratio (ER), which serves to quantify theimpact of factual retrieval failures on reasoningoutcomes. ER is calculated by calculating the per-centage of reasoning instances that are initially in-correct but are successfully resolved following theenhancement of KNs as Equation 7. Analogously,we define another metric Suppress Ratio (SR) tomeasure the obstructive effect of suppressed KNson the reasoning process. The SR is ascertained byevaluating the ratio of cases where correct reason-ing is converted to incorrect after the suppressionof KNs, as outlined in Equation 8:",
  "No CoT9.0277.2925.54 63.853.6591.61Zero-shot9.7668.4310.80 71.808.2574.16Few-shot0.1150.485.3332.090.2165.92": ": ER/SR Results of enhancing and suppressingthe expression of both triplets under both CoT and noCoT conditions. In the enhancement scenario, the num-bers represent ER metrics, whereas in the suppressionscenario, they denote SR metrics. of either the first hop, the second hop, or both dur-ing the reasoning process. Additionally, recallingthe second-hop facts is more challenging for LLMs,as shown by the higher ER after enhancing 2 com-pared to 1. Suppressing factual information sig-nificantly harms reasoning performance, with accu-racy dropping by over 77% on average when bothfactual elements are suppressed. Therefore, thesuccessful retrieval of factual associations at eachreasoning step is crucial for correct reasoning. Finding 2CoT strengthens a passive internal re-trieval of relevant facts, implicitly prompting theexpression of factual triplets. Evidence 1: In , across the scenarios of no CoT, zero-shot CoT,and few-shot CoT, suppression of factual KNs re-sults in SRNo_cot > SRZero_shot and SRNo_cot >SRFew_shot, which indicates that CoT likely stimu-lates the hydra effect (McGrath et al., 2023), whichimplements actively self-repairing computations tocompensate the suppression effects caused by lowactivation levels of KNs. Evidence 2: Similarly,enhancement of factual KNs results in ERNo_cot <ERZero_shot and ERNo_cot < ERFew_shot, whichsuggests that CoT further stimulates the internalrecall process within LLMs, thus strengthening theenhancement effects of KNs. Therefore, CoT in-deed can contribute to the recalling process.",
  "Results Analysis": "The presence of knowledge conflict within thecontext consistently augments the faithfulness ofLLMs in the corresponding fact. According to Fig-ure 5 and , the context of knowledge con-flict results in the highest KN Scores of the corre-sponding hop fact 1, which indicates counterfactualcontext significantly improves the internal retrievalof that corresponding hop fact. It illustrates LLMsexhibit greater confidence in their encoded knowl-edge when confronted with knowledge conflict, afinding that aligns with the studies conducted byZhou et al. (2023) and Li et al. (2023). When the",
  ": Results of constructing the knowledge distrac-tion and knowledge conflict for the first-hop fact": "we deliberately select an object o = o to intro-duce a knowledge conflict. In contradistinction, wealso fabricate an entirely unrelated fact for eachdata point to serve as a distractor, referred to asknowledge distraction (See detailed constructionin Appendix D). We then respectively append theknowledge conflict and knowledge distraction sen-tences before the two-hop question under no CoTsetting, which is input into LLMs. Then we ob-serve the values of KN Scores for each-hop fact.The examples of knowledge conflict and distrac-tion for the first-hop and the second-hop facts areshown in .",
  ": Results of constructing the knowledge distrac-tion and knowledge conflict for the second-hop fact": "knowledge presented in the context conflicts withthe second-hop fact, it not only reinforces the re-trieval of the second-hop fact but also enhances therecall of the first-hop fact. It is plausible that theintroduction of the subject o1 encourages LLMs torecall the precise triplet (s, r1, o1). However, thiseffect does not extend to the first-hop fact. Theoccurrence of knowledge distraction appears not tocause much obstruction to the factual recall withinLLMs. On the contrary, it may even stimulateLLMs to retrieve more facts sometimes, as evi-denced by the high KN Scores for the first-hopfact of LLaMA2-7B when the knowledge distrac-tor corresponding to the second-hop fact appearsin .",
  "Related Work": "Multi-hopReasoningMulti-hopreasoningposes a significant challenge for LLMs. Severalstudies have endeavored to address this chal-lenge through the development of more faithfulreasoning techniques (Creswell and Shanahan,2022; Chen et al., 2023b; Creswell et al., 2023).One such approach is CoT, which stimulatesLLMs to produce deductive intermediate steps,fostering a step-by-step analytical process (Chuet al., 2024). Another line of research is focusedon visualizing the implicit logical structureswithin LLMs from the perspective of mechanistic",
  "interpretability (Yang et al., 2024). For example,a recent study by Hou et al. (2023) recovers thereasoning tree from modelss attention patternsusing MechanisticProbe": "CoT MechanismA large body of literature isdedicated to the theoretical and empirical explo-ration of the mechanism underlying CoT (Saparovand He, 2023; Tan, 2023; Feng et al., 2023; Prys-tawski et al., 2023; Xie et al., 2024). Some researchendeavors to delve into a reverse-engineering anal-ysis of CoT prompting, uncovering the intricateinformation pathways that facilitate the generationof responses (Dutta et al., 2024). However, the ma-jority of these studies concentrate on the rationalesproduced by CoT and have largely overlooked thebroader implications for factual retrieval processes.In our current work, we complement this aspect andpresent compelling evidence that CoT significantlybolsters the internal recall of factual information.",
  "Conclusions": "This paper aims to provide a comprehensive under-standing of factual recall behaviors for LLMs. Wefind that a considerable portion of reasoning fail-ures are due to retrieval failures. Manually enhanc-ing the internal recall within LLMs can improvereasoning performance. For LLMs, they not onlyrely on multi-hop reasoning but also rely on otherinference ways in LLMs such as shortcuts. CoTcan significantly stimulate LLMs to recall morefacts by compelling models to engage in step-by-step thinking, diminishing the possibilities of tak-ing shortcuts. The knowledge conflict existing incontext could improve the confidence of parametricknowledge, therefore enhancing the internal recall.",
  "fully comprehend the underlying reasons for theobserved phenomena": "Practical Applications:The paper discusses the-oretical aspects and potential improvements in rea-soning accuracy but does not delve into how thesefindings can be applied in practical scenarios toenhance the reasoning capabilities of LLMs. Impact of Contextual Factors:While the papertouches upon the influence of contextual conflictson knowledge retrieval, a more comprehensive anal-ysis of various contextual factors and their impacton reasoning is needed.",
  "Acknowledgements": "This work was supported in part by the StrategicPriority Research Program of Chinese Academy ofSciences under Grant #XDA27030100 and the Na-tional Natural Science Foundation of China underGrants #72293573. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020.Language models are few-shot learners.In Ad-vances in Neural Information Processing Systems,volume 33, pages 18771901. Curran Associates,Inc. Wenhu Chen, Xueguang Ma, Xinyi Wang, andWilliam W. Cohen. 2023a.Program of thoughtsprompting: Disentangling computation from reason-ing for numerical reasoning tasks. Transactions onMachine Learning Research. Zeming Chen, Gail Weiss, Eric Mitchell, Asli Celiky-ilmaz, and Antoine Bosselut. 2023b.Reckoning:Reasoning through dynamic knowledge encoding. InAdvances in Neural Information Processing Systems,volume 36, pages 6257962600. Curran Associates,Inc. Zheng Chu, Jingchang Chen, Qianglong Chen, WeijiangYu, Tao He, Haotian Wang, Weihua Peng, Ming Liu,Bing Qin, and Ting Liu. 2024. Navigate throughenigmatic labyrinth a survey of chain of thought rea-soning: Advances, frontiers and future. Preprint,arXiv:2309.15402.",
  "Antonia Creswell and Murray Shanahan. 2022. Faithfulreasoning using large language models. Preprint,arXiv:2208.14271": "Antonia Creswell, Murray Shanahan, and Irina Higgins.2023. Selection-inference: Exploiting large languagemodels for interpretable logical reasoning. In TheEleventh International Conference on Learning Rep-resentations. Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, BaobaoChang, and Furu Wei. 2022. Knowledge neurons inpretrained transformers. In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 84938502, Dublin, Ireland. Association for ComputationalLinguistics.",
  "Juliet Floyd. 2007. 75Wittgenstein on Philosophy ofLogic and Mathematics. In The Oxford Handbookof Philosophy of Mathematics and Logic. OxfordUniversity Press": "Mor Geva, Jasmijn Bastings, Katja Filippova, and AmirGloberson. 2023. Dissecting recall of factual associa-tions in auto-regressive language models. In Proceed-ings of the 2023 Conference on Empirical Methods inNatural Language Processing, pages 1221612235,Singapore. Association for Computational Linguis-tics. Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Gold-berg. 2022. Transformer feed-forward layers buildpredictions by promoting concepts in the vocabularyspace. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Process-ing, pages 3045, Abu Dhabi, United Arab Emirates.Association for Computational Linguistics. Mor Geva, Roei Schuster, Jonathan Berant, and OmerLevy. 2021. Transformer feed-forward layers are key-value memories. In Proceedings of the 2021 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 54845495, Online and Punta Cana,Dominican Republic. Association for ComputationalLinguistics. Yifan Hou, Jiaoda Li, Yu Fei, Alessandro Stolfo,Wangchunshu Zhou, Guangtao Zeng, Antoine Bosse-lut, and Mrinmaya Sachan. 2023. Towards a mech-anistic interpretation of multi-step reasoning capa-bilities of language models. In Proceedings of the2023 Conference on Empirical Methods in NaturalLanguage Processing, pages 49024919, Singapore.Association for Computational Linguistics. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, Llio Renard Lavaud,Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,Thibaut Lavril, Thomas Wang, Timothe Lacroix,and William El Sayed. 2023. Mistral 7b. Preprint,arXiv:2310.06825.",
  "Nikhil Kandpal, H. Deng, Adam Roberts, Eric Wallace,and Colin Raffel. 2022. Large language models strug-gle to learn long-tail knowledge. In InternationalConference on Machine Learning": "Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yu-taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-guage models are zero-shot reasoners. In Advances inNeural Information Processing Systems, volume 35,pages 2219922213. Curran Associates, Inc. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-field, Michael Collins, Ankur Parikh, Chris Alberti,Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-ton Lee, Kristina Toutanova, Llion Jones, MatthewKelcey, Ming-Wei Chang, Andrew M. Dai, JakobUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-ral questions: A benchmark for question answeringresearch. Transactions of the Association for Compu-tational Linguistics, 7:452466. Daliang Li, Ankit Singh Rawat, Manzil Zaheer, XinWang, Michal Lukasik, Andreas Veit, Felix Yu, andSanjiv Kumar. 2023. Large language models withcontrollable working memory. In Findings of the As-sociation for Computational Linguistics: ACL 2023,pages 17741793, Toronto, Canada. Association forComputational Linguistics. Zhaoyi Li, Gangwei Jiang, Hong Xie, Linqi Song, DefuLian, and Ying Wei. 2024. Understanding and patch-ing compositional reasoning in LLMs. In Findings ofthe Association for Computational Linguistics ACL2024, pages 96689688, Bangkok, Thailand and vir-tual meeting. Association for Computational Linguis-tics. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,Daniel Khashabi, and Hannaneh Hajishirzi. 2023.When not to trust language models: Investigatingeffectiveness of parametric and non-parametric mem-ories. In Proceedings of the 61st Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 98029822, Toronto,Canada. Association for Computational Linguistics.",
  "Kevin Meng, David Bau, Alex Andonian, and YonatanBelinkov. 2022. Locating and editing factual asso-ciations in GPT. Advances in Neural InformationProcessing Systems, 35": "Ella Neeman, Roee Aharoni, Or Honovich, LeshemChoshen, Idan Szpektor, and Omri Abend. 2023.DisentQA: Disentangling parametric and contextualknowledge with counterfactual question answering.In Proceedings of the 61st Annual Meeting of theAssociation for Computational Linguistics (Volume 1:Long Papers), pages 1005610070, Toronto, Canada.Association for Computational Linguistics. Yasumasa Onoe, Michael Zhang, Shankar Padmanab-han, Greg Durrett, and Eunsol Choi. 2023. Can LMslearn new entities from descriptions? challenges inpropagating injected knowledge.In Proceedingsof the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 54695485, Toronto, Canada. Association forComputational Linguistics. Liangming Pan, Alon Albalak, Xinyi Wang, andWilliam Wang. 2023. Logic-LM: Empowering largelanguage models with symbolic solvers for faithfullogical reasoning. In Findings of the Associationfor Computational Linguistics: EMNLP 2023, pages38063824, Singapore. Association for Computa-tional Linguistics. Fabio Petroni, Tim Rocktschel, Sebastian Riedel,Patrick Lewis, Anton Bakhtin, Yuxiang Wu, andAlexander Miller. 2019. Language models as knowl-edge bases?In Proceedings of the 2019 Confer-ence on Empirical Methods in Natural Language Pro-cessing and the 9th International Joint Conferenceon Natural Language Processing (EMNLP-IJCNLP),pages 24632473, Hong Kong, China. Associationfor Computational Linguistics.",
  "Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.Axiomatic attribution for deep networks. In Interna-tional conference on machine learning, pages 33193328. PMLR": "Juanhe (TJ) Tan. 2023. Causal abstraction for chain-of-thought reasoning in arithmetic word problems.In Proceedings of the 6th BlackboxNLP Workshop:Analyzing and Interpreting Neural Networks for NLP,pages 155168, Singapore. Association for Compu-tational Linguistics. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Aurelien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. 2023. Llama: Openand efficient foundation language models. Preprint,arXiv:2302.13971.",
  "Jonas Wallat, Jaspreet Singh, and Avishek Anand. 2020": "BERTnesia: Investigating the capture and forgettingof knowledge in BERT. In Proceedings of the ThirdBlackboxNLP Workshop on Analyzing and Interpret-ing Neural Networks for NLP, pages 174183, On-line. Association for Computational Linguistics. Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, XiangruTang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao,Wenyang Gao, Xuming Hu, Zehan Qi, Yidong Wang,Linyi Yang, Jindong Wang, Xing Xie, Zheng Zhang,and Yue Zhang. 2023. Survey on factuality in largelanguage models: Knowledge, retrieval and domain-specificity. Preprint, arXiv:2310.07521. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,and Denny Zhou. 2022. Chain-of-thought prompt-ing elicits reasoning in large language models. InAdvances in Neural Information Processing Systems,volume 35, pages 2482424837. Curran Associates,Inc. Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel.2017. Constructing datasets for multi-hop readingcomprehension across documents. Transactions ofthe Association for Computational Linguistics, 6:287302.",
  "Zirui Zhao, Wee Sun Lee, and David Hsu. 2023. Largelanguage models as commonsense knowledge forlarge-scale task planning. In Thirty-seventh Confer-ence on Neural Information Processing Systems": "Ming Zhong, Chenxin An, Weizhu Chen, Jiawei Han,and Pengcheng He. 2024. Seeking neural nuggets:Knowledge transfer in large language models from aparametric perspective. In The Twelfth InternationalConference on Learning Representations. Wenxuan Zhou, Sheng Zhang, Hoifung Poon, andMuhao Chen. 2023.Context-faithful promptingfor large language models. In Findings of the As-sociation for Computational Linguistics: EMNLP2023, pages 1454414556, Singapore. Associationfor Computational Linguistics.",
  "P30, P36, P35, P1037, 1308, P164, P449, P488,P178, P159, P286, P413, P641, P800, P937": "P136, P106, P495, P740, P37, P407, P170,P50,P364,P112, P108, P175, P27, P40, P69,P19While LLMs have been shown to store a vastamount of factual knowledge, studies indicate thatthey are more likely to recall triplets related to pop-ular entities (Mallen et al., 2023). Therefore, whenconstructing the dataset, we employ the cumula-tive pageviews count over the past 12 months asa measure and select the top 500 popular entitiesbased on this criterion. Two-hop reasoning chainsare then extracted from the sub-graphs consistingsolely of the aforementioned relations and entities,like (Holden Caprice, manufacturer, General Mo-tors), (General Motors, chairperson, Mary Barra).",
  "A.2Generating Queries using ChatGPT": "Having acquired the triplet format of reasoningqueries, our current objective is to transform thesetriplets into natural language expressions in queries.Moreover, for effective integration of the Knowl-edge Neuron technique, it is essential to rephraseindividual triplets into multiple natural languageexpressions. As knowledge neurons demonstrateindifference towards specific knowledge represen-tations, employing diverse question formats aids inidentifying authentic knowledge neurons. Whetherin the formulation of reasoning queries or the gen-eration of individual triplet queries, we capitalizefew-shot learning capabilities of ChatGPT (gpt-3.5-turbo) to autonomously generate natural languagequestions. Concretely, we leveraged few-shot ca-pabilities in LLMs to generate multiple queries forindividual fact (s, r, o), as well as reasoning ques-tions from two-hop facts ((s1, r1, o1), (o1, r2, o2)).For the generation of single-fact queries, we pro-vide relation labels and relation definitions as addi-tional information for LLMs to generate accuratesubject-relation queries (). For the gen-eration of reasoning questions, two-hop relationlabels and explanations are also provided besidesfour in-context demonstrations ().An instance from TFRKN is depicted in .This approach not only surpasses the limitations im-posed by manual templates but also guarantees theproduction of high-quality and diverse questions.Overall, the dataset comprises 4,550 instances span-ning 213 unique combinations of relations.",
  "BKnowledge Neurons": "In this part, we detailedly illustrate the method-ology of the identification of KNs using the in-tegrated gradient method. Given a specific rela-tional fact: (s, r, o); A set of knowledge-expressingqueries ( Fact1Query and Fact2Query in ):< query1, query2, , queryL >. We define therepresentation of the i-th neuron in the l-th inter-mediate layer in FFNs as wli,",
  "signed the valuew(l)i": "System: You are a powerful cloze template generator for wikidata relations.Users will provide 2 Wikidata triples (s1,r1,o1),(o1,r2,o2) and you will help write a 2-hopquestion to introduce o2 from s1. Dont mention any bridge entities. Users will give thedescriptions of relation r1 and r2 to help you construct the template for the question. input: Triples:(Amazon Prime Video, developer, Amazon), (Amazon, industry, e-commerce),Two-hop relations:[ developer, industry],<developer>: organization or person that developed the item,<industry>: specific industry of company or organization, Output: Question: What is the specific industry of the developer of Amazon Prime Video?[The other three in-context demonstrations abbreviated] User :input: Triples:(French Revolution, country, French), (French, official language, French)Two-hop relations:[country, official language],< country >: sovereign state that this item is in (not to be used for human beings),< official language >: language designated as official by this item, Output: xxx",
  ": An example of using ChatGPT to generate 2-hop questions from Wikidata triples": "System: You are a powerful question generator for wikidata relations. Users will provide a wikidata triple (s, r, o),and you will help write complete questions in natural English to ask o from subject s. Don't mention o in questions and be as clear and concise as possible. The questions should only include the entity s. Users will give the definition of r to help you construct questions.input: <triple>: [Al Gore, place of birth, Washington, D.C.]<relation label>: place of birth<relation description>: most specific known (e.g. city instead of country, or hospital instead of city) birth location of a person, animal or fictional characterWrite more than 5 possible questions in natural English. output: 1.Where was Al Gore born?2.In which city was Al Gore born?3.What's the place of Al Gore's birth?4.What is Al Gore's birth city?5.What is the birth city of Al Gore?6.Where did Al Gore originate from?[The other three in-context demonstrations abbreviated] User : input: <triple>: [Ellie Kemper, country of citizenship, United States of America]<relation label>:country of citizenship<relation description>: the object is a country that recognizes the subject as its citizenWrite more than 5 possible questions in natural English. Output: xxx",
  ": An example of using ChatGPT to generate single-fact queries from triples and relation information(labelsand descriptions)": "The attribution scores quantify the contribution ofindividual neurons to correct predictions. By grad-ually restoring each neurons value from 0 to itsoriginal level, the gradients of the probability ofthe correct token with respect to each neuron areintegrated, as shown in Equation 10. Equation 10 is applied to the calculation of attribu-tion scores for single-token target o. The methodfor computing attribution scores for multi-tokentarget o is described in Equation 11. Assumingthe tokenized sequence of a relational-fact queryand the corresponding ground truth respectively are",
  "We present a comprehensive overview of our ex-perimental setup": "Intersection of LLMsExperiments are con-ducted using a refined subset of TFRKN dataset.To ensure that LLMs know each factual elementrequired by the factual reasoning questions, wemeticulously filtered out unqualified data points foreach model. By taking the intersection of thesefiltered datasets, we culled a dataset comprising1072 qualified data points. Indentification of KNsThe process of identify-ing KNs for each fact triplet proves to be the mostcomputationally intensive, with each model taking96 GPU hours to find all KNs. In the context of thelocation experiment, we configured the integratedgradient steps to 20 and set the parameter of theshared percentage of coarse neurons to 0.2. Theexperiments were executed on a system equippedwith NVIDIA A100 80GB GPUs, and further de-tails of the software environment are available inour code repository. All experimental results arethe mean values of three repetitive experiments.",
  "DConstruction of Contextual Conflict": "KnowledgeDistractionWemanuallycon-structed a set of irrelevant fact statements S. Sdoes not involve any entities or relations in TFRKNto ensure \"unrelated\" property. Each two-hop ques-tion randomly selects a knowledge distraction fromthis set. Knowledge ConflictWe constructed contextsthat conflict with the first-hop fact and that con-flict with the second-hop fact for each two-hopquestion respectively. The method is as follows:we manually designed templates T for all relationsinvolved in the TFRKN dataset. Assuming thereis a fact (s, r, o), we collect the set of candidateobjects related to r in the dataset, select an o thatis not equal to o as the new fabricated fact (s, r, o),and apply the template of the relation in T to ob-tain the knowledge conflict context correspondingto (s, r, o).",
  "relational facts and calculated the number of inter-secting KNs between each pair. The statistics ofoverlapping KNs are shown in": "Verification of Basic AssumptionsWe presentcompelling results from small-scale case studies,which prove that when LLMs predict the sameword, the metric of KN Scores would be high onlywhen the process involves fact retrieval. For il-lustration, we use (France, capital, Paris) as anexample, whose KNs cover 26 neurons. KN Scoresare computed across these 26 neurons as the metric.Then we construct sentences that end with \"Paris\"and then replace \"Paris\" with a blank, promptingLLMs to predict the missing word. To ensure thatthe LLMs predict \"Paris\" as the final token, we de-sign straightforward and commonsense sentencesand verify that the LLMs would indeed predict\"Paris\" and then assess the knowledge-expressingprompts and compare them with non-knowledge-expressing prompts by analyzing their KN Scores.\"The capital of France is\" and \"The capi-tal city of France is\" are knowledge-expressingprompts, which consistently exhibit higher KNScorescomparedtootherexamples,eventhough LLMs predict \"Paris\" for all these sen-tences.This experiment illustrates that KNs"
}