{
  "Abstract": "Temporal Knowledge Graph (TKG) reason-ing, aiming to predict future unknown factsbased on historical information, has attractedconsiderable attention due to its great practi-cal value. Insight into history is the key topredict the future.However, most existingTKG reasoning models singly capture repet-itive history, ignoring the entitys multi-hopneighbour history which can provide valuablebackground knowledge for TKG reasoning. Inthis paper, we propose Multi-Granularity His-tory and Entity Similarity Learning (MGESL)model for Temporal Knowledge Graph Reason-ing, which models historical information fromboth coarse-grained and fine-grained history.Since similar entities tend to exhibit similarbehavioural patterns, we also design a hyper-graph convolution aggregator to capture thesimilarity between entities. Furthermore, weintroduce a more realistic setting for the TKGreasoning, where candidate entities are alreadyknown at the timestamp to be predicted. Exten-sive experiments on three benchmark datasetsdemonstrate the effectiveness of our proposedmodel.",
  "Introduction": "Temporal Knowledge Graphs (TKGs), served asa way to represent and store dynamic knowledge,have shown great value in many applications, suchas event prediction (Deng et al., 2020), questionanswering (Mavromatis et al., 2022) and recom-mendation (Liu et al., 2023b). In TKGs, each factis represented as a quadruple, e.g., (Obama, sanc-tion, Russia, 2016-12-29) in (a).Reasoning over TKGs can be performed undertwo primary settings, i.e., interpolation and ex-trapolation (Jin et al., 2020). Given a TKG withtimestamps from t0 to tn, interpolation mainlyaims at inferring missing facts that occur at time",
  ": Illustration of the two problems of TKG rea-soning task": "t (t0 t tn), while extrapolation attempts topredict facts that occur at time t (t > tn). In this pa-per, we mainly focus on TKG extrapolation. Mostof existing extrapolation models (Jin et al., 2020;Li et al., 2021b, 2022b; Liu et al., 2023a) assumethe candidate entities are unknown during the rea-soning. However, there are cases that we alreadyknow the candidate entities, e.g., suspects are oftenidentified beforehand in criminal investigations andcandidates are usually already determined beforethe presidential election. In these cases, existing ex-trapolation models (Jin et al., 2020; Li et al., 2021b,2022b; Liu et al., 2023a) cannot effectively utilizethe information of those candidate entities becausethey treat all entities equally during the reasoning.Therefore, we introduce a new setting called thecandidate entity known setting, where all the enti-ties at t are known in advance. In contrast, if the candidate entities at t are unknown during the rea-soning, we call this the candidate entity unknownsetting. In this paper, both candidate entity knownand unknown settings will be discussed.To predict what will happen in the future, wefound that (1) searching for similar entities, ob-serving and understanding the evolutionary pat-tern of the actions of similar entities, and (2) delv-ing into the entity historical context from multi-granularity are crucial. (a) shows an ex-ample of TKG similarity learning problem, whereObama and Biden both sanction Russia. However,since Obama and Biden are not connected directlyin this example, vanilla graph convolution is un-able to capture the interaction between them. Toaddress this issue, we realize that both Obama andBiden share the same relation of sanction. Sincehypergraph convolution can enable information in-teraction among entities under the same relation,we therefore design a hypergraph convolutional ag-gregator to capture similarity information betweenthem. Additionally, existing models (Jin et al.,2020; Li et al., 2021b) mainly focus on utilisingthe available temporal and structural informationin the TKG for inference, ignoring the history in-formation. Even though some recent studies (Zhuet al., 2021; Li et al., 2022a; Xu et al., 2023) triedto find the correct answer from long-term globalrepeated history (i.e., fine-grained history), but theyignore the more generalised history. For instance,(b) illustrates a temporal knowledge graphwith several timestamps, where the task is to pre-dict the answer to the query (USA, sanction, ?, t).Most models (Zhu et al., 2021; Xu et al., 2023)prioritize repeated history, and return China as theanswer. However the correct answer to the questionis Russia which is a multi-hop neighbour of USA.To overcome this limitations, we further considermulti-hop neighbour entities (i.e., coarse-grainedhistory) in TKG reasoning.To this end, we consider history at two levelsof granularity (i.e., fine and coarse-grained his-tory) and entity similarity learning simultaneously,and propose the Multi-Granularity History andEntity Similarity Learning (MGESL) model forTemporal Knowledge Graph Reasoning. Specifi-cally, MGESL consists of three modules, i.e., (1)Entity Similarity Learning Module, which is usedto capture the similarity between entities that sharethe same relation; (2) Temporal Evolution Mod-ule, which is used to aggregate and transfer the KGinformation from spatial and temporal views, re-",
  "spectively; (3) Multi-Granularity History Module,which is used to capture history from both coarseand fine granularities. Our main contributions aresummarized as follows:": "We propose a TKG reasoning model MGESL,which can simultaneously consider entitysimilarity learning, coarse-grained and fine-grained history. To the best of our knowledge,we are the first to consider these features to-gether. We design a novel hypergraph convolutionalaggregator to capture similarities between en-tities, and utilize the coarse-grained history tocapture multi-hop historical contextual infor-mation and fine-grained history for decodingto make full use of historical information. Besides the candidate entity unknown setting,we also propose another realistic TKG reason-ing setting, i.e., the candidate entities are al-ready known. Extensive experiments on threebenchmark datasets show that our proposedMGESL model outperforms existing TKG rea-soning methods under both settings. The remaining sections of this paper are structuredas follows: discusses related work onTKG reasoning models on the extrapolation setting. presents the problem definition. provides a detailed representation of the MGESLmodel. contains the experimental analy-ses, followed by the conclusion in .",
  "Related Work": "Since TKG interpolation is outside the scope of ourstudy, we mainly review the existing TKG reason-ing models under the extrapolation setting. Manyextrapolation models utilise the available temporaland structural information in TKG for inference.RE-Net (Jin et al., 2020) utilizes heterogeneousgraph convolution (RGCN) (Schlichtkrull et al.,2018) to capture the structural information withinthe same timestamp and employs a recurrent neuralnetwork (RNN) to model the temporal informa-tion between different timestamps. RE-GCN (Liet al., 2021b) further constrains the evolution ofentities by incorporating additional static attributes.However, they do not consider the history infor-mation. CyGNet (Zhu et al., 2021) and CENET(Xu et al., 2023) propose a copy mechanism tofind the correct answer among long-term global : Illustration of the proposed MGESL model. Entity Similarity Learning Module captures the similaritiesbetween entities that share the same relation. Temporal Evolution Module aggregates and transfers the KGinformation from spatial and temporal views, respectively. Multi-Granularity History Module models history fromboth coarse and fine granularity. history, i.e., the fine-grained history. TiRGN (Liet al., 2022a) considers the sequential, repetitiveand cyclical patterns of historical facts. However,they ignore the multi-hop neighbour history, i.e.,the coarse-grained history. xERTE (Han et al.,2021) employs a subgraph sampling technique toconstruct interpretable reasoning graphs. CluSTeR(Li et al., 2021a) and TITer (Sun et al., 2021) bothutilize reinforcement learning to search for a seriesof historical facts for reasoning. HGLS (Zhanget al., 2023) captures the long and short history ofan entity by constructing global graphs. However,all the above models do not consider the importanceof entity similarity learning in TKG reasoning.",
  "Preliminaries": "A temporal knowledge graph can be defined asG = {G1, G2, ..., GT }, and T is the number oftimestamps. The subgraph Gt = (E, R, Ft) at tis a directed multi-relational graph, where E is theset of entities, R is the set of relations, and Ft isthe set of facts at t. A fact in Ft can be formal-ized as a quadruple (s, r, o, t), where s, o E and r R. It describes that a fact of relation type roccurs between subject entity s and object entity oat time t.The extrapolation reasoning task aims to predictthe missing object entity o or subject s via answer-ing query like (s, r, ?, tq) or (?, r, o, tq) based onthe historical facts {(s, r, o, ti)|ti < tq}. For eachquadruple (s, r, o, t), an inverse relation quadruple(o, r1, s, t) is often added to the dataset (Vashishthet al., 2020). Therefore, when predicting the miss-ing subject of a query (?, r, o, tq), we can convertit into predicting (o, r1, ?, tq). Based on this, themodel in this paper only aims to predict the miss-ing object entity. We use bold items to denotevector embeddings. For example, H R|E|d andR R2|R|d are used to represent the randomlyinitial embedding of entities and relations respec-tively, where d denotes the embedding dimension.",
  "The framework of MGESL is shown in ,comprising three modules: (1) the Entity Similarity": "Learning Module, (2) the Temporal Evolution Mod-ule, and (3) the Multi-Granularity History Module.First, the Entity Similarity Learning Module learnsthe representation of entity with similarity infor-mation. Next, the learned entity representation isfed to the Temporal Evolution Module, where itfurther learns about the structural and sequentialcharacteristics of recent facts. Then, it combineswith historical context information learnt from thecoarse-grained history in the Multi-Granularity His-tory Module. Finally, the entity representation isdecoded under the guidance of the fine-grained his-tory.",
  "Entity Similarity Learning": "4.2.1Pre-Learning GraphInspired by the pre-training model (Devlin et al.,2019), we first construct a pre-learning graph andinitially learn the representation of entities onthe pre-learning graph.Entity similarity infor-mation is also learnt on this graph. For a TKGG, we ignore the time factor to merge the sub-graphs of the first L timestamps to form a pre-learning graph GL, i.e., GL = (E, R, FL), whereFL = {(s, r, o) | (s, r, o, t) Ft, 0 < t < L} is aset of facts. 4.2.2Hypergraph ConvolutionTo effectively capture the similarity between enti-ties in the pre-learning graph, we design a hyper-graph convolutional network. First, we construct ahypergraph neighbourhood matrix D R|E|2|R|,where Di,j = 1 means the ith entity is the sub-ject entity of the jth relation, otherwise it equals0. Please note that for simplicity, we have omit-ted the inverse relation in . As stated in, for each relation, we only aggregate mes-sages from its subject entity through employing aninverse relation.First, messages from the subject entity are passedinto the relation:",
  "Structural Encoder": "Hypergraph convolution on the pre-learning graphmainly captures the similarity information betweenentities, but it cannot capture the inherent graphstructure information of the pre-learning graph.Therefore, we utilize a heterogeneous graph convo-lution network (Vashishth et al., 2020) as a struc-tural encoder to aggregate information from multi-ple relations and multi-hop neighbour entities onthe pre-learning graph, which is defined as follows:",
  "(s,r,o)FL1csWl0(hlo + r) + Wl1hls": "(3)where hls, hlo denote the lth layer embeddings ofentities s, o respectively, r denotes the embeddingof relation r, cs is a normalizing factor equal tothe number of neighbours of s, Wl0 and Wl1 denotethe learnable weights of the lth layer, and is theReLU activation function. We denote the entityembedding of the output of the last layer as H2.For convenience, we denote Equation (3) as GCN.Given that the meaning of relation r remainsconsistent over time and the observation that up-dating the relation embeddings did not enhance themodels performance during our experiments, wedo not directly update relation embedding in thispaper to maintain its semantic stability. Finally, wecombine H1 and H2 to get the entity representationH0,H0 = H1 + (1 )H2(4)",
  "Temporal Evolution": "Future facts are usually closely related to recentfacts, and our temporal evolution module aims tomodel recent facts. KGs naturally have graph struc-ture information, while TKGs have the additionaldimension of time compared to KGs. Therefore,we aggregate and transfer the most h recent times-tamps of the timestamp t to be predicted in TKGfrom both spatial and temporal views. To capturethe structural information between entities, we alsoutilize the heterogeneous graph convolutional net-",
  "Ht1gcn = GCN(Ht1, R)(5)": "where Ht1 denotes the entity embedding at timet1 and the initial value of Hth at time th is theoutput of the similarity learning module H0. Ht1gcndenotes the entity embedding after aggregation byGCN Encoder. In order to include the sequentialdependencies of subgraphs at the previous times-tamps, we utilize the gated recurrent unit (GRU)to update the representations of entities,",
  "Multi-Granularity History Learning": "4.4.1Background GraphIn order to more accurately model the representa-tion of entities and the connections between them,we construct a background graph GC based onthe most recent C timestamps, similar to HGLS(Zhang et al., 2023). Specifically, when the can-didate entities are known, the steps to constructthe background graph are as follows: (1) identifythe position where each candidate entity appears inthe recent C timestamps. (2) conduct breadth-firstsearch from each candidate entity to extract their n-hop neighbours. (3) merge the common neighboursof candidate entities and add temporal edge r0 (arandomly initial vector) between identical entitiesacross different timestamps. With the steps above,we have established a background graph for moreaccurate entity representation learning. When thecandidate entities are unknown, we take all entitiesin TKG as candidates and then execute the abovethree steps to construct the background graph. 4.4.2Multi-head Attention GCN (MAGCN)We employ a heterogeneous graph convolution net-work that incorporates the multi-head attentionmechanism to effectively capture entity represen-tation in the background graphs. First, all entitiesin the background graph are initialised by H fortheir initial embedding. Next, we combine the em-beddings of the subject entity, the relation, and theobject entity to calculate their attention scores,",
  "hl+1s= Wchl+1,cs(10)": "where M denotes the number of attention heads, represents concatenation, hls and hlo denote theembedding of entity s and o after the lth layer ag-gregation, r denotes the embedding of relation r,Wl6 and Wl7 are learnable weights, and is theReLU activation function. Wc RddM reducesthe dimension of hl+1,csfrom dM to d. We denotethe entity embedding of the last layer as Hg.Finally, we use a gate mechanism to fuse the en-tity embedding learnt from the temporal evolutionmodule with the entity embedding learnt from thebackground graph,",
  "Fine-grained History": "Based on human experience in predicting futurefacts, the answer to a query is often an entity thatis closely related to the current entity. Therefore,we extract two kinds of fine-grained histories, i.e.,one-hop history neighbours and repeated historyanswers (Li et al., 2022a). Specifically, for a query(s, r, ?, t) the indicator vector Pst of one-hop history",
  "Pst = ps0 ps1 ps2 ... pst1(12)": "where pst denotes a vector where each element rep-resents an entity. If the corresponding element ofan entity is 1, it means that the entity is a one-hopneighbour of s at t, otherwise it is 0. The symbol represents the bitwise OR operation. Similarly, wecan calculate the repeated history answers indicatorvector Ps,rt ,",
  "Scoring Function": "We utilize ConvTransE (Shang et al., 2019) as de-coder to fuse the semantic information of s and r inquery (s, r, ?, t). Since Hz already incorporates in-formation of the coarse-grained history, the scorescaculated based on coarse-grained history can bedefined as follows: pcoarse = softmax(ConvTransE(hst, r)Hz)(14)where hst and r denote the embedding of subjectentity s and relation r, respectively. For the fine-grained history (i.e., one-hop neighbour historyand repeated history), we use these two vectors (Pstand Ps,rt ) generated in section 4.4.3 to guide thedecoder in scoring, i.e.,",
  "Setup": "5.1.1DatasetsWe use three typical TKG datasets in our experi-ments: ICEWS14 (Riloff et al., 2018), ICEWS18(Jin et al., 2020), and ICEWS05-15 (Riloff et al.,2018). We divide them into training, validation,and test sets with a proportion of 80%, 10%, and10% by timestamps following (Li et al., 2021b,2022a; Xu et al., 2023). The details of datasetsstatistics are shown in Appendix A. 5.1.2BaselinesUnder the candidate entity unknown setting, wecompare our proposed MGESL model with threekinds of baselines: (1) Static KG reasoning mod-els, (2) Interpolated TKG reasoning models, and(3) Current state-of-the-art extrapolated TKG rea-soning model. Under the candidate entity knownsetting, we mainly focus on comparing to the ex-trapolated TKG reasoning models. For the detailsof baselines under both candidate entity known andunknown settings, please refer to Appendix B. 5.1.3Training Settings and EvaluationMetricsWe report a widely used time-aware filtered ver-sion (Sun et al., 2021; Li et al., 2022a,b) of MeanReciprocal Ranks (MRR) and Hits@1/3/10. Forimplementation details and parameter sensitivityanalysis experiments of MGESL, please refer toAppendix C and D, respectively.",
  "MRRHit@1Hit@3Hit@10MRRHit@1Hit@3Hit@10MRRHit@1Hit@3Hit@10": "DistMult (Yang et al., 2015)15.4410.1917.2423.9211.517.0312.8720.8617.9513.1220.7129.32ConvE (Dettmers et al., 2018)35.0925.2339.3854.6824.5116.2329.2544.5133.8124.7839.0054.95ComplEx (Trouillon et al., 2016)32.5423.4336.1350.7322.9415.1927.0542.1132.6324.0137.5052.81ConvTransE (Shang et al., 2019)33.8025.4038.5453.9922.1113.9426.4442.2833.0324.1538.0754.32RotatE (Sun et al., 2019)21.3110.2624.3544.7512.784.0114.8931.9124.7113.2229.0448.16 TTransE (Jiang et al., 2016)13.433.1117.3234.558.311.928.5621.8915.574.8019.2438.29DE-SimplE (Goel et al., 2020)32.6724.4335.6949.1119.3011.5321.8634.8035.0225.9138.9952.75TA-DistMult (Riloff et al., 2018)26.4717.0930.2245.4116.758.6118.4133.5924.3114.5827.9244.21 RE-NET (Jin et al., 2020)39.8630.1144.0258.2129.7819.7332.5548.4643.6733.5548.8362.72GyGNet (Zhu et al., 2021)37.6527.4342.6357.9027.1217.2130.9746.8540.4229.4446.0661.60xERTE (Han et al., 2021)40.7932.7045.6757.3029.3121.0333.5146.4846.6237.8452.3163.92RE-GCN (Li et al., 2021b)39.4230.1343.8057.0827.5117.8231.1746.5538.2727.4343.0659.93TITER (Sun et al., 2021)41.7332.7458.4429.9822.0544.8347.6038.2964.86TLogic (Liu et al., 2022)40.9032.1045.5057.6030.0022.1033.5044.8047.7038.0052.9065.80CEN (Li et al., 2022b)42.2032.0847.4661.3131.5021.7035.4450.5945.2734.1866.46TiRGN (Li et al., 2022a)41.5232.0446.2059.6231.7021.8235.9051.1548.5237.5553.5468.74CENET (Xu et al., 2023)41.3032.5858.2229.6519.9848.2347.1337.2567.61DaeMon (Dong et al., 2023)31.8522.6735.9249.80HGLS (Zhang et al., 2023)40.2830.3944.9559.5631.3621.2735.2551.2350.0839.3256.0370.49",
  ": Performance on three datasets in terms of MRR (%), Hit@1 (%), Hit@3 (%) and Hit@10 (%) under thecandidate entity known setting. The best is highlighted in boldface, and the second is underlined": "static models (i.e., the first block in ) be-cause they ignore the time dimension of the facts inTKGs. MGESL also performs much better than thetemporal models for the interpolation setting (i.e.,the second block in ) because MGESL ad-ditionally captures temporally sequential patternsby temporal evolution module.In comparisonto the current sate-of-the-art temporal models un-der the extrapolation setting (i.e., the third blockin ), our model also achieves notable im-provements. Specifically, MGESL improves ap-proximately 8.72%, 8.22%, 8.60%, and 7.16% onICEWS14 for MRR, Hit@1, Hit@3, and Hit@10,respectively. This is because our model can effec-tively capture the similarity information betweenentities by hypergraph convolution and model therepresentation of entities more accurately from mul-tiple granularities.",
  "shows that MGESL also significantlyoutperforms other TKG extrapolation models underthe candidate known setting. Specifically, MGESLimproves approximately 9.27%, 10.93%, 10.59%,": "and 4.26% on ICEWS14 for MRR, Hit@1, Hit@3,and Hit@10, respectively. These improvementsmainly arises from the background graph con-structed by the candidate entities which capturesthe coarse-grained history and the two kinds of fine-grained histories we extracted. The backgroundgraph allows us to comprehensively understandand analyze the connections between these entitiesand effectively find the correct answer. The fine-grained history can guide the model to convergequickly and make more precise predictions.",
  "(MGESL w/o Fine-his), (6) MGESL without one-hop history neighbours (MGESL w/o Fine-loc), (7)the original MGESL model (MGESL)": "shows the ablation results under the can-didate entity unknown setting. When the similaritylearning module (SLM) and temporal evolutionmodule (TEM) are removed, the performance ofthe model decreased by 3.87% and 4.73% for MRRrespectively, which indicates the effectiveness ofthese two modules. We can notice that removingthe fine-grained history module (Fine) degradesthe performance of the model more severely com-pared to removing the coarse-grained history mod-ule (Coarse), which causes a 8.02% performancedegradation for MRR compared with MGESL. Thisis because coarse-grained history may contain morenoisy information compared to fine-grained historyunder candidate unknown setting. When either re-peated history or one-hop history neighbours isremoved, the performance of the model declinedby 4.18% or 3.51%, respectively. shows the ablation results under the can-didate entity known setting. Performance declinedwhen either the entity similarity module (SLM)or the temporal evolution module (TEM) is re-moved. In contrast to the candidate unknown set-ting, the candidate known setting demonstrates thatremoving coarse-grained history has a more sig-nificant impact on model performance comparedto removing fine-grained history, causing a 17.2%performance degradation for MRR compared withMGESL. This is because when we have knowledgeof the candidate entities, the background graph thatwe build using these entities can serve as an ef-fective means to understand and learn the relation-ships between them. Also, after removing repeatedhistory or one-hop history neighbours, the perfor-mance of the model declined by 3.59% and 3.64%,respectively.",
  "Convergence Analysis": "presents the convergence analysis resultsof our study on ICEWS14 dataset. Obviously, afterthe initial training epoch, \"MGESL w/o Fine\" fallsnoticeably behind the other models in terms ofMRR metrics, and requires more epochs to attainthe optimal performance compared to the othermodels as shown in (a). This demonstratesthat fine-grained history can serve as a good guidefor the model to learn during the training process. Similarly, as shown in (b), we noticethat after the initial epoch of training, the results of\"MGESL w/o Fine\" are still the lowest. Besides,the results of \"MGESL w/o Coarse\" no longer re-main almost the same with other models as in Fig-ure 3(a). This phenomenon indicates that bothcoarse-grained and fine-grained histories are cru-cial in facilitating the models convergence duringtraining, particularly when the candidate entitiesare known. The fine-grained history can make themodel converges faster, while the coarse-grainedhistory can improve the accuracy of the model to agreat extent. These findings further validate the ef-fectiveness of our capturing historical informationfrom various granularities.",
  "Conclusion": "In this paper, we introduce the MGESL model forTKG extrapolation. The model considers entitysimilarity, coarse-grained history and fine-grainedhistory simultaneously. To capture entity similari-ties, we design a hypergraph convolutional aggrega-tor. We construct the background graph to capturethe coarse-grained history and extract two kinds offine-grained histories to guide the model reasoning.Moreover, we introduce a more realistic settingfor TKG extrapolation, i.e., candidate entities areknown. Extensive experiments on three datasetsdemonstrate the effectiveness of our model.",
  "Acknowledgements": "Detian Zhang was supported by the CollaborativeInnovation Center of Novel Software Technologyand Industrialization, the Priority Academic Pro-gram Development of Jiangsu Higher Education In-stitutions. Shiting Wen was supported by ZhejiangProvincial Natural Science Foundation of Chinaunder Grant No. LY24F020021, and the NingboScience and Technology Special Projects underGrant No. 2022Z235 and 20224Z263. ChunjiangZhu was supported by NSF grant CNS-2349369.Qing Li was supported by a grant from HK RGCTheme-based Research Scheme (PolyU No.: T43-513/23-N). Songgaojun Deng, Huzefa Rangwala, and Yue Ning.2020.Dynamic knowledge graph based multi-event forecasting. In Proceedings of the 26th ACMSIGKDD International Conference on KnowledgeDiscovery & Data Mining, page 15851595, NewYork, NY, USA. Association for Computing Machin-ery. Tim Dettmers, Pasquale Minervini, Pontus Stenetorp,and Sebastian Riedel. 2018. Convolutional 2d knowl-edge graph embeddings. In Proceedings of the 32thAAAI Conference on Artificial Intelligence, pages18111818, New Orleans, Louisiana, USA. AAAIPress. Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 17th Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 41714186, Minneapolis, MN, USA.Association for Computational Linguistics. Hao Dong, Zhiyuan Ning, Pengyang Wang, Ziyue Qiao,Pengfei Wang, Yuanchun Zhou, and Yanjie Fu. 2023.Adaptive path-memory network for temporal knowl-edge graph reasoning. In Proceedings of the 32thInternational Joint Conference on Artificial Intelli-gence, pages 20862094, Macao, SAR, China. ij-cai.org. Rishab Goel, Seyed Mehran Kazemi, Marcus A.Brubaker, and Pascal Poupart. 2020. Diachronic em-bedding for temporal knowledge graph completion.In The Thirty-Fourth AAAI Conference on ArtificialIntelligence, pages 39883995, New York, NY, USA.AAAI Press. Zhen Han, Peng Chen, Yunpu Ma, and Volker Tresp.2021. Explainable subgraph reasoning for forecast-ing on temporal knowledge graphs. In Proceedingsof the 9th International Conference on Learning Rep-resentations, Austria. OpenReview.net. Tingsong Jiang, Tianyu Liu, Tao Ge, Lei Sha, BaobaoChang, Sujian Li, and Zhifang Sui. 2016. Towardstime-aware knowledge graph completion. In Pro-ceedings of the 26th International Conference onComputational Linguistics, pages 17151724, Osaka,Japan. ACL. Woojeong Jin, Meng Qu, Xisen Jin, and Xiang Ren.2020. Recurrent event network: Autoregressive struc-ture inference over temporal knowledge graphs. InProceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 66696683, online. Association for Computa-tional Linguistics. Yujia Li, Shiliang Sun, and Jing Zhao. 2022a. Tirgn:Time-guided recurrent graph network with local-global historical patterns for temporal knowledgegraph reasoning. In Proceedings of the 31th Inter-national Joint Conference on Artificial Intelligence,pages 21522158, Vienna, Austria. ijcai.org. Zixuan Li, Saiping Guan, Xiaolong Jin, Weihua Peng,Yajuan Lyu, Yong Zhu, Long Bai, Wei Li, JiafengGuo, and Xueqi Cheng. 2022b. Complex evolutionalpattern learning for temporal knowledge graph rea-soning. In Proceedings of the 60th Annual Meeting ofthe Association for Computational Linguistics, pages290296, Dublin, Ireland. Association for Computa-tional Linguistics.",
  "reasoning on temporal knowledge graphs. In Pro-ceedings of the 59th Annual Meeting of the Associa-tion for Computational Linguistics, pages 47324743.Association for Computational Linguistics": "Zixuan Li, Xiaolong Jin, Wei Li, Saiping Guan, JiafengGuo, Huawei Shen, Yuanzhuo Wang, and XueqiCheng. 2021b. Temporal knowledge graph reason-ing based on evolutional representation learning. InProceedings of the 44th International ACM SIGIRConference on Research and Development in Infor-mation Retrieval, pages 408417, Canada. ACM. Kangzheng Liu, Feng Zhao, Guandong Xu, XianzhiWang, and Hai Jin. 2023a. RETIA: relation-entitytwin-interact aggregation for temporal knowledgegraph extrapolation.In Proceedings of the 39thIEEE International Conference on Data Engineering,pages 17611774, Anaheim, CA, USA. IEEE. Xiaoqian Liu, Xiuyun Li, Yuan Cao, Fan Zhang, Xiong-nan Jin, and Jinpeng Chen. 2023b. Mandari: Multi-modal temporal knowledge graph-aware sub-graphembedding for next-poi recommendation. In IEEEInternational Conference on Multimedia and Expo,ICME 2023, Brisbane, Australia, July 10-14, 2023,pages 15291534. IEEE. Yushan Liu, Yunpu Ma, Marcel Hildebrandt, MitchellJoblin, and Volker Tresp. 2022. Tlogic: Temporallogical rules for explainable link forecasting on tem-poral knowledge graphs. In Proceedings of the 36thAAAI Conference on Artificial Intelligence, pages41204127. AAAI Press. Costas Mavromatis, Prasanna Lakkur Subramanyam,Vassilis N. Ioannidis, Adesoji Adeshina, Phillip R.Howard, Tetiana Grinberg, Nagib Hakim, and GeorgeKarypis. 2022. Tempoqr: Temporal question reason-ing over knowledge graphs. In Thirty-Sixth AAAIConference on Artificial Intelligence, pages 58255833. AAAI Press. Ellen Riloff, David Chiang, Julia Hockenmaier, andJunichi Tsujii. 2018. Learning sequence encodersfor temporal knowledge graph completion. In Pro-ceedings of the 22th Conference on Empirical Meth-ods in Natural Language Processing, pages 48164821, Brussels, Belgium. Association for Computa-tional Linguistics. Michael Sejr Schlichtkrull, Thomas N. Kipf, PeterBloem, Rianne van den Berg, Ivan Titov, and MaxWelling. 2018. Modeling relational data with graphconvolutional networks. In Proceedings of the 15thSemantic Web International Conference, volume10843, pages 593607, Heraklion, Crete, Greece.Springer. Chao Shang, Yun Tang, Jing Huang, Jinbo Bi, XiaodongHe, and Bowen Zhou. 2019. End-to-end structure-aware convolutional networks for knowledge basecompletion. In Proceedings of the 33th AAAI Con-ference on Artificial Intelligence, pages 30603067,Honolulu, Hawaii, USA. AAAI Press. Haohai Sun, Jialun Zhong, Yunpu Ma, Zhen Han, andKun He. 2021. Timetraveler: Reinforcement learningfor temporal knowledge graph forecasting. In Pro-ceedings of the 25th Conference on Empirical Meth-ods in Natural Language Processing, pages 83068319, Punta Cana, Dominican Republic. Associationfor Computational Linguistics. Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and JianTang. 2019. Rotate: Knowledge graph embeddingby relational rotation in complex space. In Proceed-ings of the 7th International Conference on LearningRepresentations, New Orleans, LA, USA. OpenRe-view.net. Tho Trouillon, Johannes Welbl, Sebastian Riedel, ricGaussier, and Guillaume Bouchard. 2016. Complexembeddings for simple link prediction. In Proceed-ings of the 33nd International Conference on Ma-chine Learning, pages 20712080, New York City,NY, USA. JMLR.org. Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, andPartha P. Talukdar. 2020. Composition-based multi-relational graph convolutional networks. In Proceed-ings of the 8th International Conference on LearningRepresentations, Addis Ababa, Ethiopia. OpenRe-view.net. Yi Xu, Junjie Ou, Hui Xu, and Luoyi Fu. 2023. Tem-poral knowledge graph reasoning with historical con-trastive learning. In Proceedings of the 37th AAAIConference on Artificial Intelligence, pages 47654773, Washington, DC, USA. AAAI Press. Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao,and Li Deng. 2015. Embedding entities and relationsfor learning and inference in knowledge bases. InProceedings of the 3rd International Conference onLearning Representations, San Diego, CA, USA. Mengqi Zhang, Yuwei Xia, Qiang Liu, Shu Wu, andLiang Wang. 2023. Learning long- and short-termrepresentations for temporal knowledge graph rea-soning. In Proceedings of the ACM Web Conference2023, pages 24122422, Austin, TX, USA. ACM. Cunchao Zhu, Muhao Chen, Changjun Fan, GuangquanCheng, and Yan Zhang. 2021. Learning from history:Modeling temporal knowledge graphs with sequen-tial copy-generation networks. In Proceedings ofthe 35th AAAI Conference on Artificial Intelligence,pages 47324740. AAAI Press.",
  ": The statistics of the datasets. Granularity rep-resents time granularity between temporally adjacentfacts": "models, i.e., DistMult (Yang et al., 2015), ConvE(Dettmers et al., 2018), ComplEx (Trouillon et al.,2016), ConvTransE (Shang et al., 2019) and Ro-tatE (Sun et al., 2019). (2) Interpolated TKG rea-soning models, i.e., TTransE (Jiang et al., 2016),DE-SimplE (Goel et al., 2020), and TA-DistMult(Riloff et al., 2018). (3) Current state-of-the-artextrapolated TKG reasoning models, i.e., RE-NET(Jin et al., 2020), CyGNet (Zhu et al., 2021),xERTE (Han et al., 2021), RE-GCN (Li et al.,2021b), TITER (Sun et al., 2021), TLogic (Liuet al., 2022), CEN (Li et al., 2022b), TiRGN (Liet al., 2022a), CENET (Xu et al., 2023), HGLS(Zhang et al., 2023), RETIA (Liu et al., 2023a)and DaeMon (Dong et al., 2023). For RE-GCN(Li et al., 2021b) and TiRGN (Li et al., 2022a).we remove the static information from the modelto ensure the fairness of comparisons between allbaselines.Under the candidate entity known setting, wemainly focus on comparing to the extrapolatedTKG reasoning models, including RE-GCN (Liet al., 2021b), TiRGN (Li et al., 2022a) and HGLS(Zhang et al., 2023). In this setting, we assumethat the entities in the timestamp to be predictedare all known. We propose this setting for the fol-lowing two reasons: (1) There are scenarios in re-ality where we already know the candidate entitiesand all we need to do is to find out the exact an-swer from these entities, such as presidential elec-tions where president is often chosen from multipleknown candidates. (2) When entities are given topredict the relationship between them, the entitiesare also known. As the previous TKG extrapola-tion models were conducted under the candidateentity unknown setting, we intentionally revealedall the entities of the timestamp to be predicted.This means that these models only need to scoreand find the correct answer from the revealed can-didate entities, not from all entities in the TKG.",
  "CImplementation Details": "We employed a random search algorithm to samplea fixed number of combinations within the hyperpa-rameter space. Specifically, the embedding dimen-sion d ranges from 100, 200, and 300. The lengthof timestamps for pre-learning graph L ranges from30, 50, 80, and 100, while the length of timestampsfor background graph C ranges from 10, 20, and30. The number of GCN convolutional layers andthe hops of neighbours n were selected from 1, 2,and 3. The hyperparameter ranges from 0.1 to0.9. The length of historical timestamps h is set to9 and the number of attention heads M is set to 5.Additionally, the parameters 1, 2 and 3 rangesfrom 0.1 to 0.9 with a step size of 0.1, ensuringthat their sum equals to 1. As to the best modelconfigurations, we set the embedding dimensiond to 200, L is 30 for candidate unknown settingand 50 for candidate known setting, is 0.2 forcandidate unknown setting and 0.5 for candidateknown setting, C is 20 for the candidate unknownsetting and 10 for the candidate known setting, nis 2, the layer of structural encoder and multi-headattention GCN are both 2. 1, 2 and 3 are 0.3,0.5 and 0.2, respectively. Adam is used for param-eter learning, and the learning rate is set to 0.001.All experiments are conducted on NVIDIA TeslaA100 (40G) and Intel Xeon 6248R.",
  ": Performance of MGESL under different C-values on ICEWS14 in MRR": "that learning the similarity between entities throughthe Hypergraph Convolution can improve the per-formance of our model. However, as continuesto increase, the performance declines, indicatingthat inherent graph structure information of the pre-trained graph is also significant.The value of L determines the numbers of times-tamps for pre-learning graph in the SLM module.As shown in , with L-values increasing,the model performance first improves and then de-clines. This suggests that an optimal number oftimestamps for the pre-learning graph can improvethe models performance, whereas an excessiveamount may have adverse effects. This could bedue to the fact that when we predict the facts inthe nth timestamp, information from that times-tamp might have already been assimilated throughpre-learning, potentially diminishing the models",
  ": The time consumed for one training epoch onthree datasets under candidate entity unknown setting": "generalization ability.The value of h determines the length of the his-torical timestamps in TEM module. According to, an increase in length results in a grad-ual improvement in the models performance underboth settings. This suggests that more history times-tamps are beneficial to the model. For efficiencyconsiderations, we opted for a history timestamplength of 9 in our experiments under both settings.The value of C determines the length of times-tamps of background graph. As shown in ,the performance of the model intially improves butlater declines with the increase of C-values underboth settings. This phenomenon may be attributedto the fact that excessively large background graphincorporates more additional noisy data, hinderingthe accurate modeling of entity representations.",
  "EEfficiency Comparison": "shows the time consumed of several state-of-the-art models for one training epoch on threedatasets under candidate entity unknown setting onNVIDIA Tesla A100 (40G) and Intel Xeon 6248R.Specifically, compared to RETIA (Liu et al., 2023a), our model consumes relatively less time.However, it takes more time than TiRGN (Li et al.,2022a) and HGLS (Zhang et al., 2023) primarilydue to the additional time required for modelingthe background graph. In terms of model perfor-mance, our results show significant improvementsover these models."
}