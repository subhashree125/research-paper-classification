{
  "Abstract": "Language models recognized as a new form ofknowledge bases, face challenges of outdated,erroneous, and privacy-sensitive information,necessitating knowledge editing to rectify er-rors without costly retraining. Existing meth-ods, spanning models parameters modification,external knowledge integration, and in-contextlearning, lack in-depth analysis from a modelinterpretability perspective. Our work exploresthe instability in in-context learning outcomes,providing insights into its reasons and distinc-tions from other methods. Leveraging findingson the critical role of feed-forward MLPs indecoder-only models, we propose a tailoredknowledge editing method, TailoredKE, thatconsiders the unique information flow of eachsample. Model interpretability reveals diverseattribute recall across transformer layers, guid-ing edits to specific features at different depthsand mitigating over-editing issues.",
  "Introduction": "Language models have been proven to be a newform of dynamic knowledge bases (Cao et al., 2021;Petroni et al., 2019). However, the addition andremoval of knowledge within it are not as straight-forward as in traditional knowledge graphs.As time progresses and the real world undergoeschanges, language models often contain outdatedor erroneous knowledge (Lazaridou et al., 2021;Lewis et al., 2020; Shuster et al., 2021), as wellas unintentionally learned user privacy information(Carlini et al., 2021, 2022) and biased information(Bolukbasi et al., 2016). These errors or redun-dant information need to be corrected or removedto prevent any adverse impact. In such scenarios,the most common approach is typically retrainingfrom scratch or finetuning the language models, in",
  "*Work done during an internship at University CollegeLondon.Corresponding author": "which the costs are notably high. Additionally, insituations with limited data, this can easily lead tounstable training results and overfitting problems.To address this challenge, researchers have intro-duced the concept of knowledge editing, aiming tomodify specific knowledge within the model whileensuring that the storage of unrelated knowledgeremains unaffected.Many recent works have made progress in mod-ifying the knowledge storage or intervening theretrieval processes within models, whether throughthe modification of internal model parameters(Meng et al., 2022a,b), the incorporation of externalknowledge materials (Mitchell et al., 2022; Huanget al., 2022; Dong et al., 2022), or In-Context Learn-ing without parameter modification (Zheng et al.,2023; Zhong et al., 2023; Cohen et al., 2023). How-ever, there is still a lack of in-depth analysis fromthe perspective of model internal interpretabilityto understand why these methods succeed in edit-ing, especially for In-Context Learning methodsthat treat large models as black boxes and thus lacktheoretical analysis, which also has the drawbackof being overly sensitive to text prompts and thushave unstable editing outcomes. In our work, weexplored the reasons behind the instability of edit-ing outcomes in In-Context Learning and examinedthe differences compared to other methods fromthe model interpretability perspective in 3.1.1.Some other work (Geva et al., 2021; Menget al., 2022a) have investigated how such relationalknowledge is stored in decoder-only language mod-els and found that the feed-forward MLPs in themiddle layers are crucial and serve as keyvaluememories of much information associated with theentities. Meng et al. (2022a,b) provided valuablemethods for knowledge editing based on this dis-covery. However, they did not consider the dis-tinct characteristics of each editing sample andthe unique processes of information flow associ-ated with them, thus easily leading to the problems of over-editing: modifying parameters excessivelyand consequently affecting irrelevant peripheralknowledge. In-depth analysis lies in 3.1.2.To better address the unstable knowledge editingproblems and the over-editing issues mentionedabove, we design and propose a more tailoredknowledge editing method, TailoredKE, which isbased on the actual information flow of each indi-vidual sample to perform a more precise knowledgeintervention, drawing inspiration from Geva et al.(2023) which further reveal the specific process ofhow knowledge is extracted and recalled from themodel parameters.Specifically, we traced the process of in-ternal knowledge recall, capturing different at-tributes which are recalled across various layersof transformer-based language models and asso-ciated with a specific piece of knowledge whenperforming editing. For instance, considering theentity iPod, the shallow layers of language mod-els will only recall it as a device and related tomusic, while deeper layers will be able to recall itsassociation with the Apple company and otherproducts like iPhone and iPad. Based on theanalysis of these features, we dynamically selectdifferent editing layers for each sample, rather thanchoosing the fixed layers for all editing as donein previous work (Meng et al., 2022a,b; Li et al.,2023). Tailoring our edits based on the specificfeatures recalled at different layers not only allowsus to address the needs of the actual editing goal(e.g., if our goal is to modify only the iPods man-ufacturing company) but also ultimately effectivelyalleviates the issues of over-editing and the poten-tial impact on irrelevant facts in these traditionalparameter editing methods as detailed in 3.3.What is more, in the actual training process, itis crucial for a piece of knowledge to manifest indiverse forms to enable the model to genuinely re-member it, moving beyond mere template-basedmemorization. As this knowledge appears morefrequently in diverse forms, the models memoryof it becomes more profound, establishing a morestable representation inside. Hence, drawing in-sights from this perspective about how to deepenthe models memory of knowledge, in TailoredKEwe have introduced diverse structures and expres-sions during the knowledge editing process, de-scribed in 3.2. This ultimately leads to a signifi-cant enhancement in performance across variousmetrics, signifying the strengthening of the modelsability to retain new knowledge. The main structure",
  "LocateInjectMix": ": This outlines the main structure of our method,TailoredKE, which encompasses three steps for accom-plishing a more effective and targeted knowledge editingprocess: (a) Strengthen the new memory by modifyingits syntactic structure and calculating a shared weight:(b) Locate the key layers by observing the evolution ofentity representation in actual scenarios: (c) Achievenew knowledge injection by updating the parameters ofthe MLPs which are selected in (b).",
  "Knowledge Storing": "Understanding how knowledge is stored within lan-guage models and how information flows, transfers,and integrates into them has consistently been a cru-cial part of studying language models.Some recent research has indicated that the trans-formers MLP can be conceptualized as key-valuememories used to retain factual knowledge by theuse of the causal tracing method (Pearl, 2022; Viget al., 2020; Meng et al., 2022a), with the hiddenstates of the subject will serve as a key to map andrecall its relevant knowledge. This conclusion wasfurther investigated and explored by other works(Geva et al., 2023; Dar et al., 2023; Geva et al.,2022a), which artificially blocked certain compo-nents within transformers to examine their specificfunctions in the inference process. It reveals thatthe representation at the subjects last-token posi-tion undergoes the attributes enrichment, recallingand integrating a wealth of pertinent knowledgefrom the MLP sublayers. In contrast, the sentenceslast-token position will extract the relationship at-tribution from other positions and subsequently em-ploy it via attention components to extract the mostrelevant attribution recalled by the subject token. Finally, after inputting the obtained final represen-tation into the classifier, the model generates thepredicted next token for the sentence. These workshave shed light on the extraction process of internalknowledge within language models from an inter-pretability standpoint. They serve as inspirationfor our research on enhancing the editing of modelknowledge.In some other works, which from the model train-ing perspective investigate the models storage ofknowledge and memories (Cohen et al., 2023; Zhuand Li, 2023), they experimentally confirmed thatonly when the same entity appears multiple times intraining texts with various sentence structures, thisentity has a stable representation that can be usedto infer and recall the corresponding knowledge,which signifies that the model genuinely memo-rizes this knowledge, rather than merely memoriz-ing the patterns (a certain word will always followa specific combination of sentences).",
  "Knowledge Editing": "Language models often accumulate outdated or er-roneous information over time. The most commonapproach to address such issues is to retrain themodel with more accurate and timely data (Zhuet al., 2020). However, retraining requires substan-tial resources and time. Hence, researchers areactively seeking more efficient methods to adjustmodels to incorporate more accurate and currentknowledge while discarding outdated or inaccurateinformation.To achieve knowledge editing within the modeleffectively, now there are two main approachesbased on whether they modify their parameters ornot. If they modify their parameters, two strate-gies currently stand out: locating the specific pa-rameters for subsequent modification (Meng et al.,2022a,b) which insert the weights containing newknowledge into certain layers of the transformers,or employing meta-learning (Mitchell et al., 2021;De Cao et al., 2021) which utilizes a hyper networkto acquire the essential gradient for modifying thelanguage model.When choosing not to modify model parameters,the primary two methods are memory-based meth-ods (Mitchell et al., 2022; Huang et al., 2022; Donget al., 2022), and in-context learning (Cohen et al.,2023; Zheng et al., 2023; Zhong et al., 2023). Inthe in-context Knowledge Editing (IKE) (Zhenget al., 2023), they retrieved the top-k most relevantsentences concerning the new knowledge and insert these into the prompt, preceding the input to querythe model. While Zhong et al. (2023) proposedMeLLo, which utilized the Chain of Thought strat-egy (Wei et al., 2022) to help design the promptand perform in-context learning.",
  "The Proposed Method": "The current two most effective methods in thefield of knowledge editing are In-Context Learn-ing, such as IKE (Zheng et al., 2023) and MeLLo(Zhong et al., 2023), and the Parameters Editingmethods like MEMIT (Meng et al., 2022b) andROME (Meng et al., 2022a). However, each hasits own noticeable and impactful shortcomings thatcannot be overlooked.The main problem with the parameters editingmethods such as ROME or MEMIT is that theyare too aggressive in modifying the parameters andwill sometimes establish an excessively forced re-lationship between subject and object, potentiallyleading to the answer always being a new object re-gardless of the querys context (Zheng et al., 2023),which may also impact the storage of irrelevantknowledge that does not require modification. Wevalidate and investigate this issue more in-depth in3.1.2.When it comes to in-context learning, its per-formance is largely limited by the length of theprompt window, due to the necessity of adding thetext prompt containing relevant knowledge beforethe query. In most cases, a longer prompt will pro-vide better editing capabilities (Zheng et al., 2023).Moreover, in-context learning does not genuinelyenable the model to learn new knowledge or forgetthe old one at the parameter level. Hence, if theprompt is inaccurate or malicious (such as attemptsto bypass the language models security measuresto output private or harmful content), it may gen-erate incorrect or harmful results. Therefore, itcannot provide a truly robust and stable knowledgeediting effect. In 3.1.1, we leverage a model in-terpretability technique, representation projection,to gain a deeper understanding of the underlyingreasons behind this.",
  "Probli = H(Xli),(2)": "where Xli represents the representation of the i-thtoken in l-th layer, while Mli and Ali are the outputsfrom the MLP and Attention components in the l-thlayer respectively. The H() is the models vanillaprediction head (also known as the unembeddingmatrix) which projects the internal representationonto the vocabulary space, and Probi representsthe corresponding probability distribution.To investigate the issues behind the in-contextlearning method, we followed the in-context edit-ing (ICE) baseline (Cohen et al., 2023) and utilizedthe prompt Imagine that Apple is a product re-leased by Microsoft. ahead of the original queryfor this example. From the results in , wecan observe that after applying the in-context learn-ing method, the model can successfully generatethe new object Microsoft at the last tokens posi-tion in deeper layers, although the subjects attri-bution enrichment process is different and affectedby the prepended prompts. However, we can alsonotice an unstable output distribution, which incor-porates the outdated object Apple. This reveals asubstantial drawback, highlighting that it is not athorough knowledge editing method. So in caseswhere the prompt is poorly designed, there is a riskof reintroducing old knowledge into the output.When using the MEMIT method to edit param-eters, the models attributions change noticeablyafter the edited layer at the subject tokens posi-tion. The attributions become more relevant tonew objects and are notably closer. This indicatesthat MEMIT has a direct impact on the processof enriching the subjects representation, causingchanges in various attributes and related informa-tion that are recalled during this procedure. Al-though the approach is comprehensive, it has thedrawback of potential over-editing, which we willhighlight in the next section. 3.1.2Over-Editing IssuesWhile some knowledge editing methods involv-ing parameter modifications have proven highlyeffective in altering the corresponding knowledge(Meng et al., 2022a,b), some work has revealed that this type of knowledge editing approach hasthe side effect of over-editing (Zheng et al., 2023),referring to the impact on other out-of-scope factsduring the editing of a target fact. In our work, weobserved that this issue becomes much more seri-ous if these out-of-scope facts are closely relatedto the original facts, especially when the relevanceexists among the subjects. To delve deeper into this problem, we con-structed a more elaborate dataset comprising 200sets of editing pairs that were manually crafted.Each set includes 10 distinct but similar subjectentities, sharing the same relation and featuringa common new object for editing. When editingsuch an existing fact (subject s, relation r, objecto) to a new fact (subject s, relation r, new objectonew), we tested on other subject entities whichhave very similar representations to the originalones in the language model and computed this met-ric P(onew|sothers, r). It calculates the extent towhich the probability of the new object varies inthe language models output distribution when in-putting other unrelated yet similar subjects and thesame relationship mapping statements. When thedegree of this probability change is greater, it sug-gests a larger impact of over-editing side effects onthese unrelated subjects. For instance, by editing the new fact iPod is aproduct released by Microsoft, we examine theimpact on other different but related subject enti-ties like iPhone, iPad, Macbook and so on,which are all products created by Apple com-pany and have a similar representation in languagemodels. We do this by measuring the extent ofprobability change of the token Microsoft in theoutputs distribution. The experimental results onthis dataset indicate that on average, 47% of similarentities will be affected by over-editing when edit-ing one of them in a pair. This effect is more seriouswhen using parameter editing methods comparedto In-Context Learning. Detailed results are pre-sented in , showing the degree to which theprobabilities of other objects are elevated, whichare similar but not in need of modification. In order to address the problems mentioned in3.1, and achieve a more thorough and completeknowledge modification within the model whilemitigating over-editing issues, we propose a betterknowledge editing method in the remaining subsec-tions.",
  "MethodLayerTop-scoring tokens": "GPT-J13(Subject Token)iPhone, Music, Touch, devices, touch, music, Software, Archives, device, battery, Touch22(Subject Token)music, Music, touch, Touch, song, device, songs, Music, software, devices, video, iPhone, iPod22(Last Token)Apple, iPod, Apple, apple, iTunes, iPhone, apple, Sony, Microsoft, Macintosh, Nintendo, Mac27(Last Token)the, Apple, a, American, in, and, apple, an, one, T, Mac, Steve, San, App, company ICL13(Subject Token)is, has, was, s, Q, os, =, ose, ure, /, ty, isn, will, ul, does, :, ://, TM, ort, are, name, au, isc22(Subject Token)means, works, music, uses, will, does, plays, comes, stands, sales, needs, belongs, software22(Last Token)Microsoft, Apple, Microsoft, Windows, Google, Sony, Samsung, apple, MS, IBM, iPod27(Last Token)\\n, the, a, ? ,, , ., ..., :, Microsoft, (, , and, Apple, R, M, , ?, micro, A, E, P, in, an, m, is, T MEMIT13(Subject Token)Touch, touch, Wireless, Archives, Touch, devices, Square, Software, software, device22(Subject Token)software, music, computer, Windows, Music, touch, devices, users, Touch, Software, device22(Last Token)Microsoft, Microsoft, Windows, microsoft, Intel, Apple, MS, Nokia, Redmond, IBM, Xbox27(Last Token)Microsoft, the, a, Bill, N, , MS, micro, E, S, and, one, American, US, Micro, C, computer : Top-scoring tokens by tokens representations of GPT-J (Layer = 13 and 22) for the entity of \"iPod\"before and after the Knowledge Edit (New Fact: iPod is a product released by Microsoft; Old Fact: iPod is aproduct released by Apple). Note: The presence of several repeated words is due to some instances in the modelstokenization table, such as Microsoft and GMicrosoft, which may decode into the same word, Microsoft.",
  "Expressing Knowledge in Diverse Forms": "Zhu and Li (2023) discuss how models store knowl-edge. When an entity appears in different sentencestructures within the training set, it helps the modelto memorize that entity. This enables the modelto use it for reasoning and providing responsesto related questions rather than just repeating thesame answer when it encounters similar queries.Utilizing the language model itself, we rephraseknowledge statements while maintaining subject,relationship, and object to assist the model in re-taining new knowledge. This process serves as aform of data augmentation involving altering thesentence structure and expressions. An example isshown in .After receiving these rephrased sentences forthe same new factual knowledge, we encode themsimultaneously into the model by calculating ashared weight and updating the ones in the MLPmodules of certain layers. The MLP modules con-sisting of two layers each are treated as keyvaluememories (Kohonen, 1972; Geva et al., 2021). Theoutput representation after the MLP can be ex-",
  "Mli = W lout(W lin(Ali + Hl1i)),(3)": "where the matrices W lout and W lin represent thetwo-layer neural network of MLP in each layer,while and represent the non-linear activationfunction and the layernorm function.Specifically, the Win matrices in the first layerencode entities into their corresponding keys, whilethe Wout matrices in the second layer map thesekeys to attributes and associated information re-lated to the entities. Therefore, to modify the map-ping relationship between entities and target at-tributes or to add new attributes, we only needto adjust the Wout matrices to encode new targetknowledge for the entities.We obtain this target weight by optimizing theobjective function proposed by MEMIT (Menget al., 2022b) and searching for values that canbetter represent the target attributes and new knowl-",
  "+ n+mti=n+1 Wki vi2(4)": "where n represents the count of original knowl-edge elements to be preserved, whereas m denotesthe amount of new knowledge to be added, andt signifies the number of times each new knowl-edge is rephrased and undergoes structural modifi-cations. The ki and vi represent the correspondingencoded entity and its mapping attribute separately.Through this approach, we have identified repre-sentations that better capture the essence of newknowledge and its corresponding attributes. Hence,we can compute better weights for mapping to thesecorresponding representations. The intuitive resultsafter employing this method are presented and dis-cussed in 4.3.",
  "Preciser Selection of the Layers to Edit": "In previous methods like ROME (Meng et al.,2022a) and MEMIT (Meng et al., 2022b), theirknowledge editing layers remained fixed for allsamples, regardless of the feature of each indi-vidual sample. For instance, on the backbone ofGPT2-XL, in ROME, the editing layers were con-sistently set at the 17th layer for all samples, whilein MEMIT, the editing layers were fixed at lay-ers 13, 14, 15, 16, and 17, which clearly appearsto be arbitrary. Gupta et al. (2023) observed thatmodifying the shallow layers of the model tends toachieve better results for commonsense knowledgewhich is relatively simpler compared with most ofthe factual knowledge. Therefore, to attain moreeffective knowledge editing outcomes for varyingdifficulty levels of knowledge, it is necessary toselectively edit different transformer layers basedon the characteristics of each knowledge itself.Given that the parameter modification methodfundamentally involves adjusting the subject rep-resentation enrichment within the models internalMLPs, and considering that the MLPs recall differ-ent attributions at each layer, it becomes essentialto research the specific layer where a particularattribute of an entity begins to be recalled. Thisallows for targeted editing at specific layers, aimingto modify a specific entitys attribute while mini-mizing the impact on other attributes.Since subject representation enrichment is a con-tinuous process across different layers, we designed a more precise layers selection strategy named Dy-namic Editing Window approach. This methoddynamically selects the layers for editing based onthe unique characteristics of each knowledge andthe actual entities representations enrichment pro-cess. To be specific, before editing certain knowl-edge, we observe its attribution enrichment process,which is described in 3.1.1. After receiving therepresentation obtained from each layer throughtheir corresponding MLPs, we feed them into aclassifier, projecting them to the vocabulary spaceand yielding probability values for the original ob-jects token, as described in Eq. 2 and Eq. 3, whichis not time-consuming in practical applications. Wethen identify the two layers that have the highestprobabilities and consider all the layers betweenthem as the main part of the enrichment process forthe subject to recall important information relatedto the original object. These layers are the onesthat require editing. The formula representing howto choose these two is outlined below:",
  "Probs = {H(M1i ), H(M2i ), . . . , H(Mni )}, (6)": "in which the Mli are the outputs from the MLPcomponent in the l-th layer of the subjects token.The H() represents the models vanilla predictionhead, which projects the internal representationonto the vocabulary space. After the calculation,all the layers between these two selected layers iand j constitute the defined scope for modification.Note that although Dynamic Editing Window canflexibly select the layers to be edited based on thedifferent characteristics of each sample, this strat-egy supports both single fact editing and batch factsediting.The initial result about preventing the over-editing problems is in , which demonstratesthat we can significantly alleviate the issue of over-editing with the original parameter editing methodby choosing more appropriate layers to edit, reduc-ing it to a level comparable to In-Context Learning.",
  "We employ two recent auto-regressive decoder-only language models of varying sizes: GPT-J(Chen et al., 2021) with 6 billion parameters, andLLaMA-2 (Touvron et al., 2023) with 7 billion pa-": "rameters. We completed all experiments on a single80GB NVIDIA A800 GPU.We have chosen several leading methods in theKnowledge Editing domain as baselines, includingtop-performing approaches that involve locatingand modifying model parameters, such as ROME(Meng et al., 2022a), MEMIT (Meng et al., 2022b)and PMET (Li et al., 2023).Additionally, weconsidered other methods that incorporate exter-nal knowledge supplementation, such as MEND(Mitchell et al., 2021) and SERAC (Mitchell et al.,2022) for performance comparison with ours.Furthermore, we tested these three crucial met-rics of Knowledge Editing: Efficacy, Generaliza-tion and Specificity on two popular model editingdatasets COUNTERFACT (Meng et al., 2022a) andZsRE (Levy et al., 2017), along with an additionaldataset proposed by Yao et al. (2023) that assessesthe metric of Portability. Efficacy is the mostdirect indicator of the success of knowledge edit-ing, measuring the proportion of cases where themodels predictions match the new ground truth oi .Generalization is the same metric but applied ondifferent queries, each expressing the same ques-tion but with different expressions. Specificitymeasures the impact of knowledge editing on ir-relevant knowledge. Higher scores indicate lessinfluence, aligning more closely with the originalmodel. Portability is used to assess the effective-ness of model editing in transferring knowledge torelated content, evaluated on one-hop and multi-hop problems. More explicit definitions of thesemetrics are presented in Appendix A.",
  "Results": "The main result on the COUNTERFACT Datasetwith one editing operation per iteration is presentedin . This demonstrates that our approach,TailoredKE, surpasses existing methods across mul-tiple knowledge editing metrics on both two back-bones, GPT-J and LLaMA-2-7b. The combinationof two strategies, diverse knowledge forms andpreciser selection of the layers to edit, results insignificant improvements across three key metrics.This suggests that it is not just a more robust and en-hanced knowledge editing method, enhancing theacquisition of new knowledge and the eradicationof outdated knowledge (Efficacy, Generalization),but also better at minimizing the impact of otherirrelevant information (Specificity).TailoredKE also maintains the performance ofthe original model on fluency and consistency com- : This chart illustrates the comparison betweenour method and the current two most efficient base-lines under varying knowledge editing counts: 1, 10,102, 103, and 104.Additionally, we have incorpo-rated ablation experiments, where TailoredKERephraserepresents the sole impact of knowledge rephrasing,and TailoredKET argeted signifies the included influenceof Dynamic Window Selection without the effects ofknowledge rephrasing. pared with other baselines, enabling it to generategrammatically sound and fluent sentences. The spe-cific roles of these two strategies on different met-rics are thoroughly discussed in the Ablation Study4.5. In this table, we also observe that the fourapproaches of parameter editing can yield morecomprehensive editing compared to other meth-ods involving external memory access includingMEND and SERAC, especially in terms of Efficacyand Generalization.",
  "The Effect of Diverse Knowledge Forms": "We evaluate the enhancements provided by theknowledge representations in diverse forms for theconventional parameters-editing approaches likeMEMIT (Meng et al., 2022b). Additionally, weadjusted the number of knowledge forms and ob-served how this variation impacted the knowledgeediting effectiveness.The result is shown in . After employ-ing rephrasing sentences with diverse structures ofknowledge, we observe notable improvements inthe performance across the three key indicators ofknowledge editing. Furthermore, as the frequencyof rephrasing increases, there is a further enhance-ment in the effectiveness of knowledge editing.This suggests that altering the structure of knowl-edge and presenting the model with diverse forms",
  "PMET93.087.972.1597.339.0MEMIT92.985.976.3619.138.9ROME92.587.051.0614.237.4TailoredKE96.191.079.1619.540.1": ": The performance of TailoredKE on the COUNTERFACT Dataset with one editing operation per iteration.The definitions of the metric Efficacy, Generalization, and Specificity are provided in 4.1. Fluency andConsistency are proposed by ROME (Meng et al., 2022a) to evaluate the fluency and semantic consistency ofgenerated sentences, with calculation details provided in A.",
  "Evaluations Considering the PortabilityProblems": "Portability is a new metric recently proposed by theknowledge editing community. It assesses whethermodifications to specific knowledge will extendto related knowledge, testing whether the modi-fications are only superficial (Cohen et al., 2023;Yao et al., 2023; Zhong et al., 2023). We utilize thedataset introduced by Yao et al. (2023) to assess thePortability metric, examining whether the editingin our approach can effectively impact the perti-nent knowledge that ought to be influenced. Thisdataset is built upon COUNTERFACT and ZsRE,incorporating sentence data generated by GPT-4to measure surrounding entities. The relevant re-sults are presented in . We can observethat TailoredKE outperforms other methods in thisPortability metric, with its rephrasing functionalityplaying a predominant role, while the impact oftargeted edits is minimal, primarily contributing tothe reduction of over-editing.",
  "Ablation Study": "We perform two sets of ablation experiments com-pared with the two most effective parameter edit-ing methods PMET (Li et al., 2023) and MEMIT(Meng et al., 2022b) on the COUNTERFACTdataset. The result is in and it illustratesthat: In this scenario, the emphasis on knowledgerephrasing significantly enhances the metrics of Ef-ficacy and Generalization, signifying improved re-tention of corresponding knowledge by the model.Meanwhile, the contribution of the targeted layerprimarily enhances the Specificity metric, effec-tively mitigating the impact of over-editing asso-ciated with parameter editing methods. What ismore, the employment of knowledge with diversestructures significantly boosts the models perfor-mance in the Portability metric, which indicates theimprovement of the models ability for knowledgetransfer and its reasoning capability on correspond-ing knowledge.",
  "Limitations": "Through our experimental validation, we foundthat both parameter-modifying knowledge editingmethods and non-parameter-modifying knowledgeediting methods can lead to some degree of hallu-cinations. The model tends to overly extrapolatenew knowledge, involving content that still requiresfurther refinement and improvement.Concurrently, when the entity to be edited is un-familiar to the model (Cohen et al., 2023), due tothe lack of a stable representation, it will not ex-hibit satisfactory performance. In our future work,we will attempt to rapidly create a stable repre-sentation for entities unfamiliar to these modelsby establishing connections with known entitiesthat share semantic similarities, aiming to assist themodel in quickly memorizing the new entity.Currently, in most knowledge editing papers, re-searchers often evaluate the performance of knowl-edge editing techniques using idealized datasetslike COUNTERFACT (Meng et al., 2022a) andZsRE (Levy et al., 2017). Moreover, the sentencesin these datasets that represent facts often follow asimple format of (subject, relation, object), whichcannot encompass all real-world knowledge scenar-ios, which remains for future exploration. Tolga Bolukbasi, Kai-Wei Chang, James Y Zou,Venkatesh Saligrama, and Adam T Kalai. 2016. Manis to computer programmer as woman is to home-maker? debiasing word embeddings. Advances inneural information processing systems, 29. Boxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingy-ong Yan, Meng Liao, Tong Xue, and Jin Xu. 2021.Knowledgeable or educated guess? revisiting lan-guage models as knowledge bases. In Proceedingsof the 59th Annual Meeting of the Association for",
  "Computational Linguistics and the 11th InternationalJoint Conference on Natural Language Processing(Volume 1: Long Papers), pages 18601874, Online.Association for Computational Linguistics": "Nicholas Carlini, Daphne Ippolito, Matthew Jagielski,Katherine Lee, Florian Tramer, and Chiyuan Zhang.2022. Quantifying memorization across neural lan-guage models. In The Eleventh International Confer-ence on Learning Representations. Nicholas Carlini,Florian Tramer,Eric Wallace,Matthew Jagielski, Ariel Herbert-Voss, KatherineLee, Adam Roberts, Tom Brown, Dawn Song, UlfarErlingsson, et al. 2021. Extracting training data fromlarge language models. In 30th USENIX SecuritySymposium (USENIX Security 21), pages 26332650. Mark Chen, Jerry Tworek, Heewoo Jun, QimingYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-plan, Harri Edwards, Yuri Burda, Nicholas Joseph,Greg Brockman, et al. 2021.Evaluating largelanguage models trained on code. arXiv preprintarXiv:2107.03374.",
  "Guy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant.2023. Analyzing transformers in embedding space.In Annual Meeting of the Association for Computa-tional Linguistics": "Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Edit-ing factual knowledge in language models. In Pro-ceedings of the 2021 Conference on Empirical Meth-ods in Natural Language Processing, pages 64916506. Qingxiu Dong, Damai Dai, Yifan Song, Jingjing Xu,Zhifang Sui, and Lei Li. 2022. Calibrating factualknowledge in pretrained language models. In Find-ings of the Association for Computational Linguistics:",
  "Mor Geva, Jasmijn Bastings, Katja Filippova, and AmirGloberson. 2023. Dissecting recall of factual asso-ciations in auto-regressive language models. arXivpreprint arXiv:2304.14767": "Mor Geva, Avi Caciularu, Guy Dar, Paul Roit, ShovalSadde, Micah Shlain, Bar Tamir, and Yoav Goldberg.2022a. Lm-debugger: An interactive tool for inspec-tion and intervention in transformer-based languagemodels. In Proceedings of the The 2022 Conferenceon Empirical Methods in Natural Language Process-ing: System Demonstrations, pages 1221. Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Gold-berg. 2022b. Transformer feed-forward layers buildpredictions by promoting concepts in the vocabularyspace. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Processing,pages 3045. Mor Geva, Roei Schuster, Jonathan Berant, and OmerLevy. 2021. Transformer feed-forward layers are key-value memories. In Proceedings of the 2021 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 54845495, Online and Punta Cana,Dominican Republic. Association for ComputationalLinguistics.",
  "Teuvo Kohonen. 1972. Correlation matrix memories.IEEE transactions on computers, 100(4):353359": "Angeliki Lazaridou, Adhi Kuncoro, Elena Gribovskaya,Devang Agrawal, Adam Liska, Tayfun Terzi, MaiGimenez, Cyprien de Masson dAutume, Tomas Ko-cisky, Sebastian Ruder, et al. 2021. Mind the gap:Assessing temporal generalization in neural languagemodels. Advances in Neural Information ProcessingSystems, 34:2934829363. Omer Levy, Minjoon Seo, Eunsol Choi, and LukeZettlemoyer. 2017. Zero-shot relation extraction viareading comprehension. In Proceedings of the 21stConference on Computational Natural LanguageLearning (CoNLL 2017), pages 333342, Vancouver,Canada. Association for Computational Linguistics.",
  "Judea Pearl. 2022. Direct and indirect effects. In Prob-abilistic and causal inference: the works of JudeaPearl, pages 373392": "Fabio Petroni, Tim Rocktschel, Sebastian Riedel,Patrick Lewis, Anton Bakhtin, Yuxiang Wu, andAlexander Miller. 2019. Language models as knowl-edge bases?In Proceedings of the 2019 Confer-ence on Empirical Methods in Natural Language Pro-cessing and the 9th International Joint Conferenceon Natural Language Processing (EMNLP-IJCNLP),pages 24632473, Hong Kong, China. Associationfor Computational Linguistics. Ori Ram, Liat Bezalel, Adi Zicher, Yonatan Be-linkov, Jonathan Berant, and Amir Globerson. 2022.What are you token about?dense retrieval asdistributions over the vocabulary.arXiv preprintarXiv:2212.10380. Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,and Jason Weston. 2021. Retrieval augmentationreduces hallucination in conversation. In Findingsof the Association for Computational Linguistics:EMNLP 2021, pages 37843803, Punta Cana, Do-minican Republic. Association for ComputationalLinguistics. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov,Sharon Qian, Daniel Nevo, Yaron Singer, and StuartShieber. 2020. Investigating gender bias in languagemodels using causal mediation analysis. Advancesin neural information processing systems, 33:1238812401. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in NeuralInformation Processing Systems, 35:2482424837. Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng,Zhoubo Li, Shumin Deng, Huajun Chen, and NingyuZhang. 2023. Editing large language models: Prob-lems, methods, and opportunities. arXiv preprintarXiv:2305.13172. Yizhe Zhang, Michel Galley, Jianfeng Gao, Zhe Gan,Xiujun Li, Chris Brockett, and Bill Dolan. 2018.Generating informative and diverse conversational re-sponses via adversarial information maximization. InAdvances in Neural Information Processing Systems,volume 31. Curran Associates, Inc.",
  "AMetric Explainations": "We tested these three crucial metrics of KnowledgeEditing: Efficacy, Generalization and Specificityon two popular model editing datasets COUNTER-FACT (Meng et al., 2022a) and ZsRE (Levy et al.,2017), along with an additional dataset proposedby the work (Yao et al., 2023) that assesses themetric of Portability. The definitions of these fourmetrics are provided below: Efficacy Efficacy is the most direct indica-tor of the success of knowledge editing. Theprompts encountered during model evaluationafter editing are the same as those encounteredduring the editing process. In the formulas",
  "(10)": "In , we follow Meng et al. (2022a) to alsoprovide the metric of Fluency and Consistency forthe main results. Fluency is computed by measur-ing the weighted average of bi-gram and tri-gramentropies (Zhang et al., 2018) while Consistencyis computed as the cosine similarity between theunigram TF-IDF vectors of the new generated textswhich start with the target subjects, and the refer-ence texts used in editing which share the samenew objects.",
  "BImplementation Time Cost": "For TailoredKE, the primary time expenditure ismainly on the calculation of the new target weights.In this aspect, we are essentially similar to theoriginal method, ROME (Meng et al., 2022a)and MEMIT (Meng et al., 2022b), both utilizingthe backpropagation approach to help calculate it.Therefore, the time cost is fundamentally similar.For 10,000 edits, ROME and MEMIT take about11.10 hr and 6.56 hr on one 80GB A100, and Tai-loredKE takes about 7.63 hr at the same setting."
}