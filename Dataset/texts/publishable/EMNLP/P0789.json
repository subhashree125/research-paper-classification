{
  "Abstract": "Projecting visual features into word embeddingspace has become a significant fusion strategyadopted by Multimodal Large Language Mod-els (MLLMs). However, its internal mecha-nisms have yet to be explored. Inspired by mul-tilingual research, we identify domain-specificneurons in multimodal large language models.Specifically, we investigate the distribution ofdomain-specific neurons and the mechanismof how MLLMs process features from diversedomains. Furthermore, we propose a three-stage mechanism for language model modulesin MLLMs when handling projected image fea-tures, and verify this hypothesis using logitlens. Extensive experiments indicate that whilecurrent MLLMs exhibit Visual Question An-swering (VQA) capability, they may not fullyutilize domain-specific information. Manipu-lating domain-specific neurons properly will re-sult in a 10% change of accuracy at most, shed-ding light on the development of cross-domain,all-encompassing MLLMs in the future. Thesource code is available at this URL.",
  "Introduction": "Neuron Analysis, which interprets activation ofneurons as the recall of learned knowledge in deepneural networks, has been widely adopted by re-searchers to understand the inner workings of mod-els (Sajjad et al., 2022; Fan et al., 2024). Prior stud-ies have confirmed that certain neurons within deepneural networks play important roles in learningparticular concepts (Oikarinen and Weng, 2022;Bai et al., 2024; Xiao et al., 2024), preservingfactual knowledge (Chen et al., 2024; Dai et al.,2022; Niu et al., 2024) as well as solving spe-cific tasks (Stanczak et al., 2022). Beyond en-hancing model interpretability, current practicalapplications of Neuron Analysis include modeldistillation (Dalvi et al., 2020), knowledge edit-ing (Chavhan et al., 2024; Pan et al., 2023), and",
  "controllable generation (Bau et al., 2019; Kojimaet al., 2024). Central to such endeavors is the identi-fication of neurons responsible for target scenarios": "As illustrated in (a), recent studies havefocused on interpreting the multilingual capabili-ties of pre-trained large language models (LLMs)under the view of language-specific neurons, whichare neurons uniquely responsible for particular lan-guages. For instance, Kojima et al. (2024) iden-tified such neurons in pre-trained decoder-basedlanguage models, demonstrating that tamperingwith a few language-specific neurons significantlyalters the occurrence probability of target languagein text generation. Similarly, Zhao et al. (2024c)detected language-specific neurons by measuringthe significance of neurons when processing multi-lingual inputs and proposed a workflow of LLMshandling multilingual tasks. Moreover, Tang et al.(2024) used language activation probability en-tropy (LAPE) to identify language-specific neu-rons, demonstrating that activating or deactivatingcertain neurons can change the language of themodels output. On the other hand, it has also been Auto DrivingMedicalRemote SensingCommonDocument PCA Visualization of Domain-Specific Visual Features PCA Component 1 PCA Component 2",
  ": PCA visualization of image embeddings ex-tracted through CLIPs image encoder": "confirmed that neurons in text-only transformerscan understand visual features extracted by a visionencoder (Schwettmann et al., 2023).These findings have prompted an interestingquestion: Do similar mechanisms exist in multi-modal large language models (MLLMs) duringthe processing of features from different visualdomains? As shown in (b), we aim toapply the mechanism similar to multilingual neu-ron analysis (Tang et al., 2024) to current repre-sentative open-source MLLMs, including LLaVA-NeXT (Liu et al., 2024a) and InstructBLIP (Daiet al., 2024). The aforementioned models extractimage features via a pre-trained vision encoder andproject these features into the word embeddingspace. These post-projection visual features areconcatenated with language features and fed intothe models LLM module to generate text outputs.Specifically, we investigate the activation pat-terns of neurons in MLLMs feed-forward network(FFN) layers across corpora from five distinct do-mains, identifying less than 1% as domain-specificneurons. The datasets we utilized include Lin-goQA (Marcu et al., 2023), RS-VQA (HR) (Lo-bry et al., 2020), PMC-VQA (Zhang et al., 2023b),DocVQA (Mathew et al., 2021) and VQAv2 (Goyalet al., 2017), covering domains such as Auto Driv-ing, Remote Sensing, Medicine, Document, andCommon Scenes. highlights the cluster-ing and separation of image features across thedomains. Image examples of these domains canalso be found in Appendix B. Based on our experi-ment results, we argue that differences exist amongthese visual domains and that the vision encoderand LLM modules in MLLMs exhibit distinct pat- terns for these domains. Furthermore, we proposea three-stage mechanism based on the distributionof domain-specific neurons among MLLMs LLMlayers, where post-projection visual features areprocessed by LLM. To validate our hypothesis, weemploy logit lens (nostalgebraist, 2020) to decodethe hidden states of LLMs intermediate layers tovisualize the feature transformation within trans-former models (Vaswani et al., 2017).Our main contributions are as follows:",
  "We propose a three-stage framework of lan-guage models in MLLMs when processingprojected image features, shedding light onthe internal mechanisms by which image fea-tures align with word embeddings": "To the best of our knowledge, we are thefirst to investigate domain-specific neurons in themultimodal field, although there are already in-sightful discussions on visual representations inMLLMs (Schwettmann et al., 2023; Zhao et al.,2024a). Our findings can reveal the neuron-levelsimilarity and distinction among these domains,offering insights to understand and enhance thecross-domain potential of current MLLMs.",
  "Neuron Analysis": "Neuron analysis has been recently widely exploredin computer vision and natural language process-ing, which views neuron activation as the recall oflearned knowledge (Mu and Andreas, 2020; Sajjadet al., 2022). Bau et al. (2017) propose to automati-cally inspect the functionality of each visual neu-ron in CNNs by evaluating the alignment betweenindividual hidden units. Hernandez et al. (2021);Oikarinen and Weng (2022); Bai et al. (2024) fur-ther extend this method to open-ended by labeling hidden neurons in visual models with natural lan-guage descriptions. Neuron analysis has also beenadopted to analyze language models, including theability of sentiment analysis (Radford et al., 2017),machine translation (Mu et al., 2024), knowledgestoring (Dai et al., 2022; Zhao et al., 2024b; Chenet al., 2024) and task solving (Wang et al., 2022).Recent research has associated specific neurons inLLMs with their multilingual ability, describingthese neurons as language-specific neurons (Tanget al., 2024; Zhao et al., 2024c). Inspired by theirwork, we further expand this idea to the multimodaldomain, being the first to analyze the domain-specific neurons in MLLMs. Compared with pre-vious work on the interpretability of MLLM, suchas those based on attention visualization (Aflaloet al., 2022) or prompt-based probing (Tao et al.,2024), our work stands out by providing some morefine-grained and solid quantitative analysis.",
  "Visual Representation in WordEmbedding Space": "Aligning image features within the word embed-ding space of LLMs has been one of the domi-nant frameworks adopted by current open-sourceMLLMs. Large Language and Vision Assistant(LLaVA) and its variants (Liu et al., 2024b, 2023a,2024a) use a simple linear layer to connect im-age features extracted by the vision encoder ofCLIP (Radford et al., 2021) into the word embed-ding space of LLMs (Touvron et al., 2023; Chianget al., 2023; Jiang et al., 2023). Instead of concate-nating post-projected embeddings directly with lan-guage instructions, InstructBLIP (Dai et al., 2024)employs a Q-Former to extract image featuresbased on the instruction, which was more efficient.Similarly, MiniGPT-4 (Zhu et al., 2023) gainedimage features through pre-trained ViT (Dosovit-skiy et al., 2020) or Q-Former (Li et al., 2022),which are then projected into the word space bya linear layer. Although such a framework hasgained remarkable performance in various multi-modal tasks (Antol et al., 2015; Chen et al., 2015;Liu et al., 2023b), the mechanism through whichimage tokens are processed by the LLM modulestill needed to be clarified. Our research has shedlight on the interpretation of how MLLM under-stands the image tokens.",
  "Researchers have managed to fine-tune currentgeneral-domain MLLMs on specific domain cor-": ": The overall framework of our proposed MM-Neuron method (taking LLaVA architecture as an exam-ple), which can be applied to any MLP layers with anactivation layer in multimodal large language models. pus. For example, Kuckreja et al. (2024) trainMLLM on the Remote Sensing multimodal datasetusing LLaVA-1.5 architecture. LLaVA-Med (Liet al., 2023) was initialized with the general-domain LLaVA and then continuously trained in acurriculum learning fashion, while VLAAD (Parket al., 2024) opts for Video-LLaMA (Zhang et al.,2023a) as the foundational model to assist LLM incomprehending video data from auto driving sce-narios. There are also researches trying to enhanceMLLMs performance in specific domains (Baziet al., 2024; Seyfioglu et al., 2023; Shao et al., 2023;Tian et al., 2024). Despite these efforts, it has alsobeen proved that general-domain MLLMs withoutfurther domain-specific fine-tuning have demon-strated some cross-domain capability on some lesscommon domains (Verma et al., 2024; Lu et al.,2023). In our research, we select virgin (i.e., with-out further fine-tuning) LLaVA-NeXT and Instruct-BLIP as our baseline, hoping to bring insightsinto the interpretation of general-domain MLLMscross-domain potential and the development of all-around MLLMs qualified for different domains.",
  "Method": "In this section, we will introduce how to investigatethe domain-specific neurons in MLLMs throughdomain activation probability entropy (DAPE). In.1, we define the activation of neuronsin vision-language models. In .2, we in-troduce DAPE to reflect the specificity of neurons.Furthermore, to verify how post-projection embed-dings are processed within the language model, wedecode the hidden states layer by layer with logitlens, as discussed in .3.",
  "Hv = f(Zv),withZv = f(Xv).(1)": "Here, f() and f() represent the projection mod-ule parameterized by and the vision encoder pa-rameterized by . In LLaVA, the projection mod-ule is a simple linear layer, whereas in InstructBLIP,it is implemented via a Q-Former (Li et al., 2022).The post-projection features are then concatenatedwith language instruction embeddings Hq and fedinto an LLM to generate text answer Xa:",
  "hi+1 = act_fn(hiW i1)W i2,(3)": "where act_fn() denotes the activation function(e.g., GELU in ), and W i1 Rds andW i2 Rsd represent the parameters of first LinearLayer and second Linear Layer. Here, s is the inter-mediate size of FFN layer. Therefore, there are sneurons in this FFN layer. Conventionally, the j-thneuron inside the i-th FFN layer is activated onlyif its respective activation value act_fn(hiW i1)jexceeds zero (Nair and Hinton, 2010).",
  "Domain-Specific Neuron Selection": "Our selection method is based on (Tang et al.,2024). For each domain Di, i = 1, 2, ..., k, wefeed its image-text corpus into MLLM, and recordthe activated frequency of each neuron u as well asthe total token nums Nu,i. The activation probabil-ity of a neuron u in domain Di is denoted as:",
  "j=1pu,j log pu,j.(7)": "Intuitively, a lower entropy indicates a tendency foractivation in response to one or two domains, withreduced activation probabilities for others. Thus,neurons with low DAPE are designated as domain-specific neurons, following (Tang et al., 2024). Inour work, we select those neurons with the bottom1% DAPE scores as domain-specific neurons.Upon identifying domain-specific neurons, wefurther analyze their specificity across five domains.A domain-specific neuron u is considered specificto domain Dj if its activation probability pu,j ex-ceeds a predefined threshold.",
  "LogitLens(hl) = LayerNorm(hl)WU.(10)": "As shown in , the logit lens decodes thehidden states of the transformers intermediate lay-ers into the distribution over the vocabulary, whichcan be used to interpret the models latent embed-dings (nostalgebraist, 2020). Ideally, the decodeddistribution converges monotonically toward thenext token predicted by the model. And the resultsare so-called first-order or direct effect in someliterature (Gandelsman et al., 2023).We apply this trick to decode the hidden statesof the language model, which allows us to under-stand the transformation of post-projection featureswithin the language model module of the MLLM.",
  "Experimental Setup": "4.1.1ModelsWe study two public models: LLaVA-NeXT (Liuet al., 2024a) and InstructBLIP (Dai et al., 2024).The former utilizes a simple MLP layer to projectimage features extracted by CLIPs vision encoderinto the word embedding space. The latter, how-ever, employs the Q-Former (Li et al., 2022) torefine the image features extracted by ViT (Dosovit-skiy et al., 2020). Specifically, we select llava-v1.6-vicuna-7b-hf1 and Instructblip-vicuna-7b2, both ofwhich use Vicuna-7b (Chiang et al., 2023) as thelanguage model base. The number of neurons inllava-v1.6-vicuna-7b-hf and Instructblip-vicuna-7bare 454.7K and 665.6K, respectively.",
  "Dataset and Metrics": "We select five datasets representing five differentdomains, namely, VQAv2 (Goyal et al., 2017) forcommon scenes, PMC-VQA (Zhang et al., 2023b)for Medical domain, DocVQA (Mathew et al.,2021) for Document domain, LingoQA (Marcuet al., 2023) for Auto Driving domain and RS-VQA (Lobry et al., 2020) for Remote Sensing do-main. For LingoQA, visual instruction for eachquestion includes multiple images. More detailscan be found in Appendix C. We prepare image-question pairs of nearly the same token numbersfor each domain during identifying, around 20 mil-lion tokens in LLaVA-NeXT. During evaluation,the scale of the validation set is aligned with Lin-goQA to make a fair comparison. For DocVQA,we report Average Normalized Levenshtein Simi-larity (ANLS) score (Biten et al., 2019) followedby the official benchmark. For LingoQA, we usethe score of Lingo-Judge (Marcu et al., 2023) withthe official implementation. For all other datasets,we report the top-1 accuracy (%) as the metric.",
  "Implementation Details": "We adhere to the default prompt templates from theofficial repository or the original paper during eval-uation, with an additional role description for theauto-driving scenes. For more details, please referto Appendix C. We perform the forward pass with-out padding or truncation during the identificationprocess. When evaluating models across differentdatasets, we employ beam search with max_lengthof 512 and num_beams of 5 to generate answers.The temperature and length_penalty arguments areset as 0.9 and -1, respectively.",
  "Distribution of Domain-specific Neurons": "We identify domain-specific neurons using themethod described in .2. Since neuronsin different modules may have different activationpatterns, as shown in Appendix D, we detectedthose domain-specific neurons module by module. shows the distribution of domain-specificneurons for each layer in each module of MLLMs. Three-stage mechanism of LLM understand-ing multimodal features.Two obvious turningpoints can be observed in both LLaVA-NeXT andInstructBLIPs language model, one in the inter-mediate layer and the other near the output layer.Inspired by (Zhao et al., 2024c), we thus propose a",
  ": Layer-wise Distribution of domain-specific neurons in different modules": "three-stage mechanism of LLM understanding mul-timodal features: 1) In the first several layers, pro-jected features are further aligned with word space.Around the turning point, the multimodal featuresare embedded into a uniform representation space,where included domain-specific information needsto be processed by more domain-specific neurons.2) Transitioning into the second phase, features arefurther generalized and understood by languagemodels, where domain-specific neurons decreasesharply. 3)In the third stage, language models gen-erate responses to the input, showing a rise of neu-rons specific to target tasks.Our hypothesis aligns with the previous conclu-sion on smaller multimodal models like LiMBeR-BEIT (Merullo et al., 2022), as (Schwettmann et al.,2023) argue that outputs of the projection layer arefurther translated within the transformer after beingmerged with text embeddings. To further validateour hypothesis, we employ logit lens to visualizethe transformation of multimodal features withinlanguage models in .2.3. Domain-specific information in different seman-tic levels.Domain-specific neurons are mainlydistributed in shallow and intermediate layerswithin MLLMs vision encoders. Prior researchdiscussed the correlation between the semanticlevel and layer depth, which found that more deeplayers will focus on higher-level concepts in visual networks (Xu et al., 2023; Raghu et al., 2021). Inour settings, the document domain contains morelow-level concepts, such as line and shape, whilethe remote sensing and medical domain may in-clude more high-level concepts, like architecturesand organs.Therefore, document neurons aremainly gathered in bottom layers close to the inputend. Another interesting phenomenon is the riseof auto driving neurons near the output layer ofInstructBLIPs Q-Former, we conjecture this mayreflect the struggle of model to understand the lan-guage instructions of auto driving domain. Gap between the ability of MLLM to handlevisual and lingual instructions. demon-strates the number of neurons in each domain. Re-mote sensing neurons have the largest proportion inLLaVA-NeXTs vision encoder, MLP projector andlanguage model, while in InstructBLIP, the domainowns most specific neurons are document, autodriving and auto driving separately. We argue thatthe number of specific neurons reflects the under-standing ability of MLLM in the target domain, asmore specific neurons may mean more demandingto process domain-specific information. In con-trast, less specific neurons mean more generalizedfeatures in the target domain (Tang et al., 2024).This also demonstrates a correlation between thetraining source and domain neuron distribution, asmore data exposed during training resulting in less",
  "Vision Encoder66.931.021.834.823.8Q-Former67.132.420.033.124.6LLM67.132.624.235.524.4All68.630.918.033.623.8": ": Accuracy (%) of LLaVA-NeXT and Instruct-BLIP on selected domains with corresponding domain-specific neurons deactivated. None means no neuronsare deactivated, while All means deactivating domain-specific neurons in all the modules above. Bold is usedto highlight the worst performance in each column. neurons specific for corresponding domain. In thisway, we find that there exists a large visual gap be-tween domains like remote sensing, document andmedical, comparing the two domains left. More-over, InstructBLIP seems less proficient in process-ing questions from auto driving, as neurons of thisdomain exhibit the highest number in Q-Formerand LLM. There is also a similar pattern in its lan-guage model as for the auto driving domain. Inother words, while visual features of auto drivingdomain can be processed well by existing visionencoder, the language instruction of this domainmay be hard to handle for language model. 4.2.2Influence of domain-specific neuronsPerturbation for Performance in VQA Tasks demonstrates the performance of LLaVA-NeXT and InstructBLIP after deactivating domain-specific neurons in different modules. While theperformance decrease after deactivating is slightfor most domains, we find that deactivating remotesensing neurons in LLaVA-NeXT and auto drivingneurons in InstructBLIP will result in a great fallof 4.0 and 2.6 accuracy separately. Similarly, in thedocument domain, deactivating domain-specificneurons at most causes a 2.2 accuracy decrease forLLaVA-NeXT. Interestingly, in some cases, remov-ing domain-specific information seems to benefitthe target task, as the accuracy of LLaVA-NeXT inauto driving has risen from 20.6 to 24.2. We leave",
  ": The deviation (%) of hidden states of MLLMslast layer after deactivating domain-specific neurons.We calculate the deviation d= HnHd2": "Hn2, where Hn andHd denotes the hidden states before and after deactivat-ing neurons separately. Bold is used to highlight thelargest deviation in each column. Random (Avg.) refersto the average deviation by randomly deactivating neu-rons of the same number in all modules. this for future work.In summary, deactivating domain-specific neu-rons will not cause a sharp decrease in performancefor some domains. To investigate the reason forthat further, we compare the influence of domain-specific neurons in MLLMs hidden states. Perturbation for Hidden StatesWe demon-strate the influence of domain-specific neurons onMLLMs last hidden states in . Surpris-ingly, deactivating domain-specific neurons causesa large perturbation to LLaVA-NeXT and Instruct-BLIPs hidden states. In contrast, deactivating allof the domain-specific neurons can have little ef-fect on the accuracy of these domains, as shownin . Therefore, we argue that both LLaVAand InstructBLIP fail to take full advantage of thedomain-specific information in specific domains,which may limit their cross-domain ability. Inother words, the representations within MLLMslanguage models are highly generalized.",
  "Case Study": "To investigate how MLLMs language model pro-cesses image tokens, we employ logit lens (nos-talgebraist, 2020) to decode the hidden states ofthe language models intermediate layers into theprobability of the next token across the vocabu-lary. As displayed in , when feeding aremote sensing image-question pair into Instruct-BLIP, we get that the most likely token next tothe second image token is </s>, while the mostlikely token next of the last text token is the cor-rect answer, no. Interestingly, two place names,\"Hermann\" and \"Baltimore\", have appeared amongthe top token candidates when the image input is a Language Input: Is a square building present? Visual Input:",
  "(a) Visaul and language input.The area in the image is locatedin New York": "Top 1Top 2Top 3Top 4Top 5 Layer 2 Layer 4 Layer 6 Layer 8 Layer 10 Layer 12 Layer 14 Layer 16 Layer 18 Layer 20 Layer 22 Layer 24 Layer 26 Layer 28 Layer 30 Layer 32 Expected Next Token: \"</s>\" eftHermanndbididth eftidthzekdHermann deftheadersinals efthtttransitiondivot striphttheadersTweeft eftstripivotindows eftijkyszfluUnion ](/primyszChampn",
  "(c) The next token distribution of the lasttext token, the expected next token is thecorrect answer no": ": The logit lens can be applied to decode the hidden states of the language models intermediate layers intothe probability distribution of the vocabulary. We only display the top 5 candidates for each layer in the heatmap.Color indicates the probability of candidates from low (white) to high (blue). 8ad-imgad-txtmed-imgmed-txtrs-imgrs-txtcom-imgcom-txtdoc-imgdoc-txt Average Entropy of Next-token Distribution",
  "(b) Average entropy of next-token distribution of LLaVA-NeXT": ": The average entropy of next token probabilitydistribution for image and text tokens. The colors oflines denote different domains, such as auto driving (ad),remote sensing (rs), medical (med), common (com), anddocument (doc). We use dashed lines and solid lines todistinguish curves of image and text tokens. remote sensing picture of New York. In multilin-gual literature, similar phenomena have also beenobserved. For instance, when Llama 2 receives theFrench token fleur in the input, the English con-cept __flower will appear in the intermediate dis-tribution (Wendler et al., 2024). This suggests thatthe decoded vocabulary distribution can to someextent reflect the semantic concepts understood bythe language model. Despite this observation, wenote that the decoded distribution of image tokensis far more sparse than text tokens; even in the out-put layer, the probability of the most likely nexttoken </s> is lower than 40%. It indicates thatprojected tokens may be treated as a sparse mixtureof concepts in the representation space instead of asimple word. We also demonstrate more cases oflogit lens in different domains in Appendix E. To further explore this phenomenon, we calcu-late the average entropy of the next token distribu-tion for image tokens and text tokens separately,as shown in . As the curves of image to-kens tend to be above those of text tokens for allthe layers, the next token distributions of imagetokens are indeed more sparse than those of texttokens. Moreover, the tendency of entropy curvesaligns with the hypothesis we have proposed in.2.1. In the first stage, features are alignedinto a uniform representation space, where entropycurves level off high. In the second stage, the lan-guage model understands and processes the infor- mation, as curves drop sharply in the intermediatelayers. Finally, the model selects the suitable nexttoken to output, resulting in a slight increase in en-tropy. A similar tendency has also been observed inEnglish-native multilingual LLMs when handlingnon-English inputs (Wendler et al., 2024).",
  "Conclusion": "To explore the neuron-level domain-specific inter-pretation in current MLLMs, we propose MMNeu-ron framework inspired by multilingual research.In particular, we first calculate the activation prob-abilities of neurons in LLaVA-NeXT and Instruct-BLIP across five domains, identifying those withlow domain DAPE scores as domain-specific neu-rons. By analyzing the distribution of domain-specific neurons and their influence on MLLMs, wefind that the language model modules of MLLMsfail to fully utilize domain-specific information inVQA tasks. We further propose a three-stage frame-work that the language model module employsto handle projected visual features and corrobo-rate it indirectly with logit lens. We envision thatour work will shed light on the interpretability ofcurrent MLLMs, aiding the development of cross-domain, all-encompassing MLLMs in the future.",
  "Despite the findings we demonstrate in our work,there still exist several limitations:": "1. Our experiments are conducted mainlyon LLaVA-NeXT and InstructBLIP, whoseframeworks are similar in aligning visual fea-tures with the word embedding space via aprojector. This means that our findings maynot be directly applicable to models that uti-lize different frameworks, such as those in-jecting vision representations into LLMs bylayer (Wemm, 2023). 2. Although we find that domain-specific infor-mation is not fully utilized by the languagemodel modules of MLLMs, how such infor-mation is conveyed and ignored between dif-ferent layers is still less known. We leavethese problems for future work. 3. We discuss the possible workflow of the lan-guage model module handling projected vi-sual features through logit lens. While theredo exist special semantic concepts in the de-coded representations, we still know little about how these concepts are encoded andhow projected features interact with word em-beddings during the forward pass. Therefore,further mathematical analysis in this area isstill required in the future.",
  "Acknowledgements": "This work was supported by the National Key R&DProgram of China (Grant No.2023YFF0725001);National Natural Science Foundation of China(Grant No.92370204); Guangzhou-HKUST(GZ)Joint Funding Program (Grant No.2023A03J0008),Education Bureau of Guangzhou Municipal-ity; Guangdong Provincial Department of Ed-ucation Project (Grant No.2024KQNCX028);Scientific Research Projects for the Higher-educational Institutions (Grant No.2024312096),Education Bureau of Guangzhou Municipality;Guangzhou-HKUST(GZ) Joint Funding Program(Grant No.SL2024A03J01201), Education Bu-reau of Guangzhou Municipality;China As-sociation for Science and Technology (GrantNo.XMSB20240711064). Estelle Aflalo, Meng Du, Shao-Yen Tseng, YongfeiLiu, Chenfei Wu, Nan Duan, and Vasudev Lal. 2022.Vl-interpret: An interactive visualization tool for in-terpreting vision-language transformers. In Proceed-ings of the IEEE/CVF Conference on computer visionand pattern recognition, pages 2140621415. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-garet Mitchell, Dhruv Batra, C. Lawrence Zitnick,and Devi Parikh. 2015. VQA: Visual Question An-swering. In International Conference on ComputerVision (ICCV).",
  "Nicholas Bai, Rahul A Iyer, Tuomas Oikarinen, andTsui-Wei Weng. 2024. Describe-and-dissect: Inter-preting neurons in vision networks with languagemodels. arXiv preprint arXiv:2403.13771": "Anthony Bau, Yonatan Belinkov, Hassan Sajjad, NadirDurrani, Fahim Dalvi, and James Glass. 2019. Iden-tifying and controlling important neurons in neuralmachine translation. In International Conference onLearning Representations. David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, andAntonio Torralba. 2017. Network dissection: Quanti-fying interpretability of deep visual representations.In Proceedings of the IEEE conference on computervision and pattern recognition, pages 65416549.",
  "Rs-llava: A large vision-language model for jointcaptioning and question answering in remote sensingimagery. Remote Sensing, 16(9)": "Nora Belrose, Zach Furman, Logan Smith, Danny Ha-lawi, Igor Ostrovsky, Lev McKinney, Stella Bider-man, and Jacob Steinhardt. 2023. Eliciting latentpredictions from transformers with the tuned lens.arXiv preprint arXiv:2303.08112. Ali Furkan Biten, Ruben Tito, Andres Mafla, LluisGomez, Maral Rusinol, Ernest Valveny, CV Jawa-har, and Dimosthenis Karatzas. 2019. Scene textvisual question answering. In Proceedings of theIEEE/CVF international conference on computer vi-sion, pages 42914301.",
  "Ruchika Chavhan, Da Li, and Timothy Hospedales.2024. Conceptprune: Concept editing in diffusionmodels via skilled neuron pruning. arXiv preprintarXiv:2405.19237": "Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-ishna Vedantam, Saurabh Gupta, Piotr Dollr, andC Lawrence Zitnick. 2015. Microsoft coco captions:Data collection and evaluation server. arXiv preprintarXiv:1504.00325. Yuheng Chen, Pengfei Cao, Yubo Chen, Kang Liu, andJun Zhao. 2024. Journey to the center of the knowl-edge neurons: Discoveries of language-independentknowledge neurons and degenerate knowledge neu-rons. In Proceedings of the AAAI Conference on Ar-tificial Intelligence, volume 38, pages 1781717825. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E. Gonzalez, IonStoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgptquality. Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, BaobaoChang, and Furu Wei. 2022. Knowledge neurons inpretrained transformers. In Proceedings of the 60thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 84938502, Dublin, Ireland. Association for ComputationalLinguistics. Wenliang Dai, Junnan Li, Dongxu Li, AnthonyMeng Huat Tiong, Junqi Zhao, Weisheng Wang,Boyang Li, Pascale N Fung, and Steven Hoi.2024. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advancesin Neural Information Processing Systems, 36. Fahim Dalvi, Hassan Sajjad, Nadir Durrani, andYonatan Belinkov. 2020. Analyzing redundancy inpretrained transformer models. In Proceedings of the2020 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 49084926,Online. Association for Computational Linguistics.",
  "Yossi Gandelsman, Alexei A Efros, and Jacob Stein-hardt. 2023. Interpreting clips image representa-tion via text-based decomposition. arXiv preprintarXiv:2310.05916": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, DhruvBatra, and Devi Parikh. 2017. Making the V in VQAmatter: Elevating the role of image understandingin Visual Question Answering. In Conference onComputer Vision and Pattern Recognition (CVPR). Evan Hernandez, Sarah Schwettmann, David Bau,Teona Bagashvili, Antonio Torralba, and Jacob An-dreas. 2021. Natural language descriptions of deepvisual features.In International Conference onLearning Representations. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023. Mistral7b. arXiv preprint arXiv:2310.06825. Takeshi Kojima, Itsuki Okimura, Yusuke Iwasawa, Hit-omi Yanaka, and Yutaka Matsuo. 2024. On the multi-lingual ability of decoder-based pre-trained languagemodels: Finding and controlling language-specificneurons. arXiv preprint arXiv:2404.02431. Kartik Kuckreja, Muhammad S. Danish, MuzammalNaseer, Abhijit Das, Salman Khan, and Fahad S.Khan. 2024.Geochat:Grounded large vision-language model for remote sensing. The IEEE/CVFConference on Computer Vision and Pattern Recog-nition. Chunyuan Li, Cliff Wong, Sheng Zhang, NaotoUsuyama, Haotian Liu, Jianwei Yang, Tristan Nau-mann, Hoifung Poon, and Jianfeng Gao. 2023. Llava-med: Training a large language-and-vision assis-tant for biomedicine in one day.arXiv preprintarXiv:2306.00890. Junnan Li, Dongxu Li, Caiming Xiong, and StevenHoi. 2022. Blip: Bootstrapping language-image pre-training for unified vision-language understandingand generation. In International conference on ma-chine learning, pages 1288812900. PMLR.",
  "Sylvain Lobry, Diego Marcos, Jesse Murray, and DevisTuia. 2020. Rsvqa: Visual question answering for re-mote sensing data. IEEE Transactions on Geoscienceand Remote Sensing, 58(12):85558566": "Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun-yuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. 2023.Mathvista: Evaluating mathematical reasoning offoundation models in visual contexts. arXiv preprintarXiv:2310.02255. Ana-Maria Marcu, Long Chen, Jan Hnermann, Al-ice Karnsund, Benoit Hanotte, Prajwal Chidananda,Saurabh Nair, Vijay Badrinarayanan, Alex Kendall,Jamie Shotton, and Oleg Sinavski. 2023. Lingoqa:Video question answering for autonomous driving.arXiv preprint arXiv:2312.14115.",
  "Alec Radford, Rafal Jozefowicz, and Ilya Sutskever.2017. Learning to generate reviews and discoveringsentiment. arXiv preprint arXiv:1704.01444": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,et al. 2021. Learning transferable visual models fromnatural language supervision. In International confer-ence on machine learning, pages 87488763. PMLR. Maithra Raghu, Thomas Unterthiner, Simon Kornblith,Chiyuan Zhang, and Alexey Dosovitskiy. 2021. Dovision transformers see like convolutional neural net-works? Advances in neural information processingsystems, 34:1211612128.",
  "Neuron-level interpretation of deep NLP models: Asurvey. Transactions of the Association for Compu-tational Linguistics, 10:12851303": "Sarah Schwettmann, Neil Chowdhury, Samuel Klein,David Bau, and Antonio Torralba. 2023. Multimodalneurons in pretrained text-only transformers. In Pro-ceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 28622867. Mehmet Saygin Seyfioglu, Wisdom O. Ikezogwo, Fate-meh Ghezloo, Ranjay Krishna, and Linda Shapiro.2023.Quilt-llava: Visual instruction tuning byextracting localized narratives from open-sourcehistopathology videos. Preprint, arXiv:2312.04746.",
  "Hao Shao, Yuxuan Hu, Letian Wang, Steven L Waslan-der, Yu Liu, and Hongsheng Li. 2023.Lmdrive:Closed-loop end-to-end driving with large languagemodels. arXiv preprint arXiv:2312.07488": "Karolina Stanczak, Edoardo Ponti, Lucas Torroba Hen-nigen, Ryan Cotterell, and Isabelle Augenstein. 2022.Same neurons, different languages: Probing mor-phosyntax in multilingual pre-trained models. InProceedings of the 2022 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 15891598, Seattle, United States. Associationfor Computational Linguistics. Tianyi Tang, Wenyang Luo, Haoyang Huang, Dong-dong Zhang, Xiaolei Wang, Xin Zhao, Furu Wei,and Ji-Rong Wen. 2024. Language-specific neurons:The key to multilingual capabilities in large languagemodels. arXiv preprint arXiv:2402.16438. Mingxu Tao, Quzhe Huang, Kun Xu, Liwei Chen, Yan-song Feng, and Dongyan Zhao. 2024. Probing mul-timodal large language models for global and lo-cal semantic representations. In Proceedings of the2024 Joint International Conference on Computa-tional Linguistics, Language Resources and Evalu-ation (LREC-COLING 2024), pages 1305013056,Torino, Italia. ELRA and ICCL. Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, ChenxuHu, Yang Wang, Kun Zhan, Peng Jia, Xianpeng Lang,and Hang Zhao. 2024. Drivevlm: The convergence ofautonomous driving and large vision-language mod-els. arXiv preprint arXiv:2402.12289. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processingsystems, 30. Gaurav Verma, Minje Choi, Kartik Sharma, JamelleWatson-Daniels, Sejoon Oh, and Srijan Kumar.2024.Mysterious projections: Multimodal llmsgain domain-specific visual capabilities withoutricher cross-modal projections.arXiv preprintarXiv:2402.16832. Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou,Zhiyuan Liu, and Juanzi Li. 2022.Finding skillneurons in pre-trained transformer-based languagemodels. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Processing,pages 1113211152, Abu Dhabi, United Arab Emi-rates. Association for Computational Linguistics.",
  "Chris Wendler, Veniamin Veselovsky, Giovanni Monea,and Robert West. 2024. Do llamas work in english?on the latent language of multilingual transformers.arXiv preprint arXiv:2402.10588": "Xiongye Xiao, Chenyu Zhou, Heng Ping, Defu Cao,Yaxing Li, Yizhuo Zhou, Shixuan Li, and Paul Bog-dan. 2024. Exploring neuron interactions and emer-gence in llms: From the multifractal analysis perspec-tive. arXiv preprint arXiv:2402.09099. Xiao Xu, Chenfei Wu, Shachar Rosenman, Va-sudev Lal, Wanxiang Che, and Nan Duan. 2023.Bridgetower: Building bridges between encodersin vision-language representation learning. In Pro-ceedings of the AAAI Conference on Artificial Intelli-gence.",
  "Hang Zhang, Xin Li, and Lidong Bing. 2023a. Video-LLaMA: An instruction-tuned audio-visual language": "model for video understanding. In Proceedings ofthe 2023 Conference on Empirical Methods in Nat-ural Language Processing: System Demonstrations,pages 543553, Singapore. Association for Compu-tational Linguistics. Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weix-iong Lin, Ya Zhang, Yanfeng Wang, and WeidiXie. 2023b. Pmc-vqa: Visual instruction tuning formedical visual question answering. arXiv preprintarXiv:2305.10415. Qinyu Zhao, Ming Xu, Kartik Gupta, Akshay Asthana,Liang Zheng, and Stephen Gould. 2024a. The firstto know: How token distributions reveal hiddenknowledge in large vision-language models? arXivpreprint arXiv:2403.09037.",
  "C.1Instructions templates for VQA": "For instructions with options, we separate optionsin alphabetical order, as shown in Appendix C.2. : A role description has been provided to helpmodels better understand the tasks in auto driving.As shown below:Role: You are an advanced AI assistant in-stalled on the Ego vehicle, equipped with conver-sational analysis capabilities for discussing au-tonomous driving scenarios. The perspective pre-sented is from the point-of-view of the Ego vehicle,where the camera is mounted. Its important tonote that the Ego vehicle itself is not visible in theimages provided.",
  "<Image>Question: {Question}Options: {Options}Answer with the options letter from the given choices directly": ": Prompt templates we have used in different steps. For identifying domain-specific neurons, plain questionsare input into models. During evaluation, we follow the templates provided by official repositories or codes. Assistant:no Open-Ended (LLaVA-Next) System:A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the users questions. Question: Is a square building present?Context: N/AAnswer the question using a single word or phrase. User:",
  "(a) Prompt example for open-ended tasks, the image and ques-tion come from RSVQA": "Multi-option (LLaVA-Next) System:A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the users questions. Assistant:B User: Question: Is a square building present?Context: N/AOptions: ['A: Right upper pole', 'B: Right lower pole', 'C: Left upper pole', 'D: Left lower pole']Answer with the option's letter from the given choices directly.",
  "FSensitivity and Scalability Analysis": "To verify the robustness and scalability of ourmethod, we further conducted the domain-specificneuron selection experiment at a different thresholdof 5%, and complete analysis on llava-v1.6-vicuna-13b-hf. We report the results in , 7, 8 and9. 0.2 0.4 0.6 0.8 Activated NeuronsSilent Neurons Ratio of Silent and Activated Neurons in InstructBLIP's Vision Encoder",
  "(a) Visual and language input ofPMC-VQA": "Top 1Top 2Top 3Top 4Top 5 Layer 2 Layer 4 Layer 6 Layer 8 Layer 10 Layer 12 Layer 14 Layer 16 Layer 18 Layer 20 Layer 22 Layer 24 Layer 26 Layer 28 Layer 30 Layer 32 Expected Next Token: \"</s>\" ovarchBrothersusorix ovarchBrothersrixalu ovarchamilamrix amiovarchrixisch amiDigarchtreatov amiDigrix<s>isch <s>Digdigogram <s>ogramDigimagoko <s>ogramRadRadimag RadCTscan<s>ogram CTscanimagRadbrain CTscanbrainimagCT CTbrainscanCTimag CTbrainCTscanimag CTbrainMscanposit mbrainMCTb",
  "(a) Visual and language input ofDocVQA": "Top 1Top 2Top 3Top 4Top 5 Layer 2 Layer 4 Layer 6 Layer 8 Layer 10 Layer 12 Layer 14 Layer 16 Layer 18 Layer 20 Layer 22 Layer 24 Layer 26 Layer 28 Layer 30 Layer 32 Expected Next Token: \"</s>\" CVpadnimslave CVnimpadHoffconverted padCVnimPadiella nimoreignadjpadnika soreigneerdadjnim &=\\siennes &=\\zontableeerd &=\\graphsiloguchev graphsgraphgraphchart&=\\ graphgraphsgraphchartdata graphgraphgraphschartplot graphscattergraphsplotgraph graphscatterlinegraphsgraph barslinebargraphscatter linebarsbargraphtable graphlinecharttablebar",
  "(b) The next token distribution of the377th image token": "Top 1Top 2Top 3Top 4Top 5 Layer 2 Layer 4 Layer 6 Layer 8 Layer 10 Layer 12 Layer 14 Layer 16 Layer 18 Layer 20 Layer 22 Layer 24 Layer 26 Layer 28 Layer 30 Layer 32 100Expected Next Token: \"sentence\" aterraAuermenteinstance instanceaterrazsmeisterschaft eNCterne mindtonhandjou tonbriefmindiros briefardenconnexespurepk orfansweringconnexesansweredanswers answeranswering&=\\answers answeranswersanswerAnsweranswering answeranswerAnsweransweringanswers sentenceansweranswerword sentencesentencesanswerwordline sentencewordanswersentences sentencewordlinestatementanswer wordsentencelinestatementanswer sentencewordstatementline,",
  "(b) The next token distribution of the 49thimage token": "Top 1Top 2Top 3Top 4Top 5 Layer 2 Layer 4 Layer 6 Layer 8 Layer 10 Layer 12 Layer 14 Layer 16 Layer 18 Layer 20 Layer 22 Layer 24 Layer 26 Layer 28 Layer 30 Layer 32 100Expected Next Token: \"photo\" sameaterfollowingentireidenote atereroheckorem insenquestionsardennimpreg nimcotardenamo nimaterVFovisOF aterftschlienknim aterschlieanswerOriginalhof isserquestionsschlieiteloph questionsaterquesfollowingQuestion ateratersfollowingquestionscloudflare atersaterquestionsfollowingquestion questionquestionfollowingQuestionquestions questionquestionQuestionfollowingaters questionaterphotoatersquestion questionphotofollowingsecondQuestion photoquestionmessageimagelocation",
  "(b) The next token distribution of the 37th image token inLLaVA-NeXTs vision encoder": "Top 1Top 2Top 3Top 4Top 5 Layer 2 Layer 4 Layer 6 Layer 8 Layer 10 Layer 12 Layer 14 Layer 16 Layer 18 Layer 20 Layer 22 Layer 24 Layer 26 Layer 28 Layer 30 Layer 32 100Expected Next Token: \"image\" sameabovegepubliceerdfollowingater sameabovedimenareamez kwietdekijstsame abovedegpenasrobeye nordegJohaboveZwischen widrevZwischennorabove actualactualZentboldsvic actualaboveliteralnorresulting imageliteralabovebelowactual imageactualimagespictureactual imageviewimagespicturecamera imageimagespictureshotPers imageimagesshotpicturespicture imageimagesshotframeframes imageimagesframeframesfinal imageimagesphotopicturephotos"
}