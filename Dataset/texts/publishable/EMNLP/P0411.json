{
  "Abstract": "Recently, Denoising Diffusion ProbabilisticModels (DDPMs) have attained leading per-formances across a diverse range of generativetasks. However, in the field of speech synthe-sis, although DDPMs exhibit impressive perfor-mance, their long training duration and substan-tial inference costs hinder practical deployment.Existing approaches primarily focus on enhanc-ing inference speed, while approaches to accel-erate traininga key factor in the costs associ-ated with adding or customizing voicesoftennecessitate complex modifications to the model,compromising their universal applicability. Toaddress the aforementioned challenges, we pro-pose an inquiry: is it possible to enhance thetraining/inference speed and performance ofDDPMs by modifying the speech signal it-self? In this paper, we double the training andinference speed of Speech DDPMs by simplyredirecting the generative target to the waveletdomain. This method not only achieves com-parable or superior performance to the originalmodel in speech synthesis tasks but also demon-strates its versatility. By investigating and uti-lizing different wavelet bases, our approachproves effective not just in speech synthesis,but also in speech enhancement.",
  "Introduction": "Recently, with the advancement of deep learning,generative models have made significant progressin various fields (Karras et al., 2019; Oord et al.,2016; Yang et al., 2019). Particularly, the emer-gence of diffusion models has elevated the capabil-ities of deep generative models to a new level (Hoet al., 2020; Song et al., 2020b). In the field ofspeech processing, Denoising Diffusion Probabilis-tic Models (DDPMs) not only exhibit astonishingperformance in speech synthesis (Kong et al., 2020;",
  ": Wavelet of Cohen-Daubechies-Feauveau 5-tap/3-tap. (a) Scaling and wavelet functions, (b) decom-position and reconstruction filters": "Jeong et al., 2021) but also demonstrate commend-able results in speech enhancement (Lu et al., 2022;Yen et al., 2023). However, despite the impressiveresults achieved by DDPMs in the field of speechprocessing, the requirement to generate a guaranteeof high sample quality typically necessitatinghundreds to thousands of denoising steps resultsin training and inference speeds that are dauntingin practical applications. Given these issues, researchers from variousfields have attempted different methods to improvediffusion models. In the realm of speech process-ing, existing approaches have endeavored to al-ter the model structure to accelerate the inferencespeed of speech synthesis (Huang et al., 2022),while others have experimented with changingtraining strategies to reduce the number of infer-ence steps required for diffusion models in speechenhancement (Lay et al., 2023). These approachesprimarily focus on enhancing the inference speedof speech diffusion models. However, in the field ofspeech synthesis, the industry frequently requiresincorporating new voices to accommodate var-ied requirements. Additionally, generative-basedspeech enhancement often demands tailoring mod-els to distinct scenarios, which introduces prac-tical limitations to the aforementioned methodsin real-world applications. In the field of com-puter vision, researchers have attempted to accel-",
  "Synthesized Speech": ": Overview of the Speech Wavelet Diffusion Model pipeline: First, the speech signal is decomposedinto Approximation coefficients Matrix(cA) and Detail coefficients matrix(cD), the Diffusion model subsequentlygenerates cA and cD and restores the speech signal from these matrices. erate diffusion models using wavelets. Their ef-forts are mainly concentrated on score-based diffu-sion models (Song et al., 2020b, 2021), employingwavelets to modify the training strategy, therebysimultaneously enhancing both training and infer-ence speeds (Guth et al., 2022). However, there isa significant difference between audio and im-age signals. Unlike the common feature sizes of64x64 or 256x256 in images, speech signals oftenhave large feature sizes to ensure training quality.This means that the challenges in training speechmodels often stem from the nature of the speechsignal itself (Radford et al., 2023). Consideringthis, we propose a question from a different angle:can we improve the training and inference speedsof DDPMs and significantly alleviate GPU memorypressure by operating directly on the speech signalitself? The principle of simplicity often underlies effec-tive methods, as evidenced by tools like LoRA (Huet al., 2021) and Word2Vec (Mikolov et al., 2013).Inspired by the successful application of latentspace diffusion models (Rombach et al., 2022) andwavelets in image compression (Taubman et al.,2002), we pivot the generative aim of speechDDPMs towards the compressed speech signalin the wavelet domain. This involves decompos-ing the speech signal using the Discrete WaveletTransform(DWT) into high-frequency and low-frequency components. These components are thenconcatenated to form a unified generative target forour model. Through this approach, the feature-length of the data is halved, which enhances theGPUs parallel processing capabilities and signifi-cantly reduces the demand for GPU memory. In the Further Study chapter, we have devel-oped two additional modules: the Low FrequencyEnhancer and the Multi-Level Accelerator. Theformer enhances low-frequency signals, allowingour method to not only double the speed com- pared to the original model but also achieve betterperformance. The latter, by integrating the Low-Frequency Enhancer with multi-level wavelet trans-form, further compress the speech signal. Thisenables an acceleration of more than five timeswhile maintaining comparable results.In summary, our contributions include the fol-lowing: We designed a simple, effective, and univer-sal method that doubles the training and in-ference speed of the original model withoutaltering its architecture while maintainingcomparable performance. Testing across dif-ferent models and tasks not only confirmedthe wide applicability and versatility of ourapproach but also demonstrated that the Diffu-sion Models can generate speech componentsin the wavelet domain. We designed two simple and easily integrablefront-end modules. The first achieves betterperformance than the original model whiledoubling the speed. The second offers a per-formance comparable to the original while en-abling an acceleration of more than five times.",
  "Related Work": "Diffusion Probabilistic Models. Diffusion proba-bilistic models (DMs) (Sohl-Dickstein et al., 2015;Ho et al., 2020) are a powerful and effective classof generative models, which are highly competitivein terms of sample quality, surpassing VariationalAutoencoders (VAEs) and Generative AdversarialNetworks (GANs) to become the state-of-the-art ina variety of synthesis tasks (Dhariwal and Nichol,2021; Liu et al., 2022). DMs comprise a forwardnoise diffusion process and a Markovian reverse",
  ": Overview of (a) Block of Multi-Level Discrete Wavelet Transform, (b) Multi-Level Low-Frequency VoiceEnhancement Module, (c) Block of Multi-Level Inverse Discrete Wavelet Transform": "diffusion process. They function by training a deepneural network to denoise content that has beencorrupted with various levels of Gaussian noise.In the sampling phase, a generative Markov chainprocess based on Langevin dynamics (Song andErmon, 2019) iteratively denoises from completeGaussian noise to progressively generate the targetsamples. Due to their iterative nature, DMs experi-ence a significant increase in training and samplingtime when generating high-dimensional data (Songet al., 2020a).Speech Synthesis. In recent times, a variety ofneural text-to-speech (TTS) systems have beendeveloped (Oord et al., 2016; Binkowski et al.,2019; Valle et al., 2020; Chen et al., 2024). Ini-tially, these systems generate intermediate repre-sentations, such as mel spectrograms or hiddenrepresentations, conditioned on textual input. Thisis followed by the use of a neural vocoder for thesynthesis of the raw audio waveform. The piv-otal role in the recent advancements of speechsynthesis has been played by neural vocoders.Models like WaveFlow (Ping et al., 2020) andWaveGlow (Prenger et al., 2019) achieve trainingthrough likelihood maximization. On the otherhand, models based on VAEs and GANs divergefrom likelihood-centric models, often necessitatingadditional training losses to enhance audio fidelity.Another notable approach is the diffusion-basedmodel (Kong et al., 2020), which stands out bysynthesizing high-quality speech using a singularobjective function. Our experiment will be con-ducted on a diffusion-based vocoder.Speech Enhancement. Speech enhancement is afield in audio signal processing focused on improv-ing the quality of speech signals in the presence of noise (Benesty et al., 2006). Recent advancesin deep learning have significantly improved theperformance of speech enhancement systems, en-abling more effective noise suppression and clar-ity in diverse environments (Zhang et al., 2020;Sun et al., 2023; Zhang et al., 2024). In the realmof speech denoising, diffusion-based models arebeing effectively utilized. Lu (Lu et al., 2022)investigates the efficacy of diffusion model withnoisy mel band inputs for this purpose. In a similarvein, Joan (Serr et al., 2022) examines the applica-tion of score-based diffusion models for enhancingspeech quality. Furthermore, Welker (Welker et al.,2022) proposes formulations of the diffusion pro-cess specifically designed to adapt to real audionoises, which often present non-Gaussian proper-ties.Speed Up Generative Speech Model. Numerousefforts have been made to expedite speech synthe-sis, with Fastspeech (Ren et al., 2019) and Fast-speech 2 (Ren et al., 2020) being among the mostnotable, both accelerating the process using trans-former models. FastDiff (Huang et al., 2022), amore recent development, aims to address the slowinference speed of diffusion models in practicalapplications, focusing primarily on hastening infer-ence time. In contrast, our technology is designednot only to accelerate both training and infer-ence but also to be easily adaptable to variousspeech synthesis models. 3MethodologyIn this section, the proposed method is illustratedusing the Cohen-Daubechies-Feauveau 5/3 waveletas a case study (Le Gall and Tabatabai, 1988). Wefirst explain how we utilize wavelet transforms forcompressing and parallel processing of speech sig-",
  "nals. Then, we delve into the specifics of accel-erating speech synthesis and enhancement tasks": "3.1Wavelet Transform and CompressionThe Wavelet Transform is a key method in imagecompression, involving Discrete Wavelet Trans-form (DWT) and Inverse Discrete Wavelet Trans-form (IWT) to separate low-frequency (cA) andhigh-frequency (cD) components from signals (Sul-livan, 2003).We focus on the Daubechies-Feauveau 5/3 wavelet, shown in , abiorthogonal wavelet commonly used in losslesscompression algorithms (Taubman et al., 2002).Let us define L = 1",
  "2as the low-pass and high-pass filters, re-spectively. In the DWT Process, these filters areemployed to decompose speech signals x R12x": "into matrices cA R1x and cD R1x. Subse-quently, these matrices are concatenated to formy R2x, as depicted in the left part of .In the IWT process, the matrix y R2x is dividedback into cA R1x and cD R1x, which arethen reconstructed into the speech signal. The de-tails of how Wavelet compresses speech and ac-celerates the model can be seen in Appendix C.",
  "Wavelet-based Speech Diffusion Scheme": "3.2.1Speech SynthesisWe evaluated our method using Diffwave (Konget al., 2020), a well-known diffusion vocoderwidely adopted in numerous TTS systems. Wealtered only the first layer of the one-dimensionalconvolutional network used for processing the in-put signal, ensuring that the number of channels re-mains constant, thereby keeping the network widthunchanged in comparison with Diffwave. Duringthe training process, the diffusion process is char-acterized by a fixed Markov chain transitioningfrom the concatenated wavelet data y0 to the latentvariable yT . This is achieved via",
  "x0 = IWT(y0)return x0": "where q(yt|yt1) is defined as a Gaussian distri-bution N(yt; 1 tyt1, tI) and is a smallpositive constant. The function q(yt|yt1) intro-duces slight Gaussian noise into the distribution ofyt1, effectively adding minimal Gaussian noise toboth cA and cD.The reverse process is characterized by a Markovchain transitioning from yT back to y0. This isparameterized by and computed via",
  "p(y0, . . . , yT1|yT ) = Tt=1 p(yt1|yt).(2)": "The distribution p(yT ) originates from anisotropic Gaussian and is composed of twodistinctcomponents,correspondingrespec-tively to cA and cD.The term p(yt1|yt)is parameterized by a Gaussian distributionN(yt1; (yt, t), (yt, t)2I). Here, yields a2 X matrix representing the mean values forcA and cD, while produces two real numbers,indicating the standard deviations for cA and cD.The training objective is to minimize the fol-lowing unweighted variant of the variational lowerbound (ELBO):",
  "min L() = E ty0 + 1 t, t2(3)": "where t is derived from the variance schedule,parameter denotes a neural network that outputsnoise for both cA and cD. Furthermore, is repre-sented as a 2 X matrix, encapsulating the actualnoise values corresponding to both cA and cD. Thedetailed procedures for training and sampling areoutlined in Algorithm 1 and Algorithm 2.",
  ": return x0": "The variable mt represents the interpolation ratiobetween the clean wavelet data y0 and the noisywavelet data yn. This ratio initiates at m0 = 0 andprogressively increases to mt = 1. The term tis computed following the same methodology asemployed in Diffwave, and t is defined as (1 t) m2t t. The reverse process is formulated as",
  "(yt, yn, t) = cytyt + cynyn ct(yt, yn, t).(6)": "Parameters cyt, cyn, and ct are derived from theELBO optimization. The detailed procedures fortraining and sampling are outlined in Algorithm 4and Algorithm 3. The details of coefficients andELBO optimization can be seen in Appendix B. 4Experiments4.1DatasetSpeech Synthesis Our experiments were con-ducted using the LJSpeech dataset (Ito andJohnson, 2017),comprising 13,100 Englishaudio clips along with their corresponding texttranscripts.The total duration of the audio inthis dataset is approximately 24 hours. For thepurpose of objectively assessing the NISQASpeech Naturalness (Mittag et al., 2021), 1,000samples were randomly chosen as the test dataset.Additionally, we conduct a subjective audioevaluation using a 5-point Mean Opinion Score(MOS) test, involving 30 examples per model and20 participants.Speech Enhancement Our experiments wereconductedusingtheVoiceBankDEMANDdataset (Valentini-Botinhao et al., 2016).Thedataset, derived from the VoiceBank corpus (Veauxet al., 2013), encompasses 30 speakers and isbifurcated into a training set with 28 speakers and atesting set with 2 speakers.The training utterancesare deliberately mixed with eight real-recordednoise samples from the DEMAND database, in",
  "(yt, yn, t)2": "addition to two synthetically generated noisesamples, at SNR levels of 0, 5, 10, and 15 dB. Thisresults in a total of 11,572 training utterances.For testing, the utterances are combined withdifferent noise samples at SNR levels of 2.5, 7.5,12.5, and 17.5 dB, culminating in a total of 824testing utterances. Our algorithm was evaluatedusing the Perceptual Evaluation of Speech Quality(PESQ) and a deep learning evaluation approach,DNSMos (Dubey et al., 2023). 4.2Model Architecture and TrainingTo ensure a fair comparison with the baseline, weadhered to the identical parameter settings utilizedin both Diffwave and CDiffuSE. To more effec-tively validate the versatility of our method, weconducted tests on both the base and large ver-sions of Diffwave and CDiffuSE. To explore thedistinct characteristics of various wavelets, we con-ducted experiments using a computational base of32 NVIDIA V100 32GB GPUs. we conducted testswith different wavelets base using 32 V100 32G, in-cluding Haar, Biorthogonal 1.1 (bior1.1), Biorthog-onal 1.3 (bior1.3), Coiflets 1 (coif1) (Daubechies,1988), Daubechies 2 (db2), and Cohen-Daubechies-Feauveau 5/3 (cdf53) (Sullivan, 2003). The detailsof the parameter setting can be seen in Appendix A.",
  "Main Result": "shows the results for various wavelet basesin both Speech Enhancement and Speech Synthe-sis tasks. It can be observed that, across all tasks,regardless of the type of wavelet basis used, thetraining time, the inference time, and the requiredGPU memory consumption have been reduced bynearly half. In the Speech Enhancement task, whenevaluated using the pseq metric, most wavelets,with the exception of the Coif1, performed com-parably to the original model. The DB2 waveletexhibited the best performance on both the base and large models.Despite nearly doubling in training and infer-ence speeds, its performance was only marginallylower than the original model, with a difference of0.051 and 0.021, respectively. However, when weswitch to using the DNSMos metric for evaluation,the scenario changes completely. When evaluat-ing with the DNSMos metric, there is a completeshift in results. The Coif1 wavelet becomes thebest performer. In the base model, it surpasses theoriginal model by 0.009, and in the large model,the lead extends to 0.056. A detailed analysis willbe presented in the subsequent sections.In the task of Speech Synthesis, the results showsome variations.In the base model, the Coif1wavelet still outperforms others, even exceedingthe original model by 0.004 in Speech Naturalness(SN). However, when we examine the large model,we find that although the Coif1 wavelet continuesto perform well, it is the Bior1.3 wavelet that standsout as the top performer, surpassing the originalmodel by 0.008 in terms of SN.Through these experiments, we have demon-strated that our method can double the trainingand inference speeds of the speech diffusion modelwhile achieving results that are comparable to, oreven surpass, those of the original model. Theconsistent performance across both base and largemodels further validates the generalizability of ourapproach. The stable results on Diffwave and CDif-fuSE highlight the versatility of our method acrossvarious tasks. This advancement enables the practi-cal application of diffusion models in the field ofspeech, especially the accelerated training aspect,making it feasible to customize voices and performtargeted noise reduction for specific scenarios.5Further StudyUnder the significant acceleration achieved by ourmethod, we explore the potential for enhancing thequality of samples through wavelet transformationand further accelerating the training and samplingprocess of the diffusion model.5.1Low-frequency Speech Enhancer In speech signals, the primary speech componentsare typically concentrated in the low-frequencyrange, while background noise tends to domi-nate the high-frequency spectrum (Flanagan, 2013).Therefore, to further enhance the quality of syn-thesized speech, we fully leverage the propertiesof wavelet decomposed signals. By performingDiscrete Wavelet Transform (DWT) on the speech",
  ": Overview of Frequency Bottleneck Block": "signals (Shensa et al., 1992), we obtain a 2-channelvector, consisting of detail coefficients filteredthrough a high-pass filter and approximation co-efficients filtered through a low-pass filter. Priorto feeding into the diffusion model, this vector isprocessed through the Frequency Bottleneck Blockas shown in , which amplifies the low-frequency speech signals and attenuates the back-ground noise. Since different wavelet signals em-phasize various speech characteristics during DWT,we tested six types of wavelets, as shown in Ta-ble 3. The results indicate that the Haar wavelet,which focuses on signal discontinuities and rapidchanges (Stankovic and Falkowski, 2003), achievessuperior sampling quality compared to DiffWaveafter processing through the Frequency BottleneckBlock module.",
  "Multi-Level Wavelet Accelerator": "To further enhance training and sampling speeds,we implemented a multi-level DWT approach, asdemonstrated in a. This method reducesthe length of speech signal features to a quarter oftheir original size, and increases the channel countto four. Concurrently, the Frequency BottleneckBlock, designed to intensify speech signals, is ex-panded into the Multi-level Low-Frequency VoiceEnhancement Module, which encompasses a multi-level residual block. This block is adept at progres-sively attenuating high-frequency components, asdepicted in b. This methodology signifi-cantly reduces both training and sampling times,with training speeds approximately five times fasterthan the original DiffWave and sampling speedsabout three times quicker. As shown in ,the Mean Opinion Score (MOS) indicates that theaudio quality of the samples remains comparablyhigh, which underscores its strong practicality.",
  "GT4.530.06": ": The table presented above displays the results for various wavelet bases in both Speech Enhancementand Speech Synthesis tasks. SN represents Speech Naturalness. GT stands for Ground Truth, referring to the rawaudio from human. Training Time represents the time required for training in a single epoch(seconds). RTF(Real-Time Factor) is utilized as a metric to assess inference time.",
  "Effect of Vanishing Moments, Smoothingand Complexity": "From , it can be observed that Coif1 per-forms well on the DNSmos metric and in speechsynthesis tasks, yet exhibits poor performancewhen evaluated using the PSEQ. The differencebetween DNSmos and PSEQ lies in the fact thatDNSmos does not require reference audio; it isused directly to evaluate the quality of the gen-erated speech. After listening to several sets ofgenerated speech, we discovered that while thediffusion model using Coif1 wavelets produces clear and smooth speech, there is a significant alter-ation in timbre compared to the original sound. Bycomparing with DB2 and Haar wavelets, we canconclude that as the vanishing moment increasesand complexity follows (Coif1 > DB2 > Haar),the diffusion model tends to generate clearer andsmoother speech. However, once the vanishing mo-ment reaches a certain level, the timbre of the soundis altered. This characteristic enables the selectionof Coif1 wavelets in scenarios where only noisereduction is needed, or in speech synthesis taskswhere timbre is of lesser concern and the emphasisis on naturalness.",
  "Effect of Order of the Wavelet": "Comparing bior1.1 with bior1.3, we observe thatwith an increase in the reconstruction order, boththe PSEQ and DNS_MOS scores decrease. This in-dicates that as the reconstruction order rises, the dif-fusion models ability to handle noise diminishes,although there is a slight improvement in speechsynthesis tasks. We believe this is because bior1.3,compared to bior1.1, captures more high-frequencyinformation. However, noise compared to humanvoice generally occupies the high-frequency range,",
  "Bior1.32.3953.126522.7333.483 4.380.06 4.403422.3263.342": ": The table presented above displays the results for various wavelet bases in both Speech Enhancement andSpeech Synthesis tasks. SN represents Speech Naturalness. Training Time represents the time required for trainingin a single epoch(seconds). RTF (Real-Time Factor) is utilized as a metric to assess inference time. which explains why bior1.3 performs less effec-tively than bior1.1 in speech enhancement tasks.Comparing Haar (DB1) with DB2, we find thatwhen the reconstruction order remains the same,an increase in the decomposition order enhancesthe performance of the wavelet speech diffusionmodel, especially in terms of stability and superiorperformance in speech enhancement. It effectivelyremoves noise while maintaining the timbre with-out significant changes. In speech synthesis tasks,DB2 also shows improvement over Haar, which weattribute to the increased complexity of the wavelet.",
  "Relationship between Wavelet base andTraining/Inference Speed": "From , it is evident that regardless of thewavelet used, both training and inference speedsare nearly doubled compared to the original model.The table indicates that when wavelets are appliedto the diffusion model, Haar and bior1.1 exhibitsimilar speeds. The differences in speed betweenCoif1, DB2, and cdf53 are minimal, with bior1.3being the slowest. We discovered that their speedsdo not strictly correlate with their computationalcomplexity. Our analysis suggests that the longerfilter length of Bior1.3 in implementation, com-bined with the inherently long nature of speech",
  "Effect of Frequency Enhancer": "After incorporating the Frequency Enhancer, mostwavelet speech diffusion models showed an im-provement in performance. In speech enhancementtasks, Haar, bior1.3, and cdf53 wavelets demon-strated significant improvements. Meanwhile, thetraining and inference speeds, compared to thewavelet diffusion model without the FrequencyEnhancer, remained virtually unchanged, fallingwithin the margin of error. Haar and Coif1 waveletsdiffusion model even outperformed the originalmodel, indicating that by simply adding a smallpre-processing module, we can surpass the perfor-mance of the original model while significantlyincreasing training and inference speeds. However,we believe that the reasons for the performanceenhancement offered by these three wavelets arenot the same.For the Haar wavelet, its abil-ity to capture discontinuities and abrupt changesin signals makes it particularly effective at han-dling non-stationary signals like speech. The Fre-quency Enhancer further amplifies this capabil-ity. Bior1.3, due to its enhanced ability to cap-ture high-frequency signals, sees a reduction in",
  "haar base*4.2323 3.0138 0.4147bior1.1 base*4.2083 3.0415 0.3943bior1.3 base*4.1921 3.0551 0.3995coif1 base*4.1824 3.0406 0.4034cdf53 base*4.0939 3.2039 0.3949db2 base*4.1601 3.0479 0.4053": ": Low-frequency Speech Enhancer results onVCTK dataset. RTF (Real-Time Factor) is utilized as ametric to assess inference time. SN denotes Speech Nat-uralness, * denotes results from Low-frequency SpeechEnhancer noise after processing with the Frequency Enhancer.Therefore, its performance improves compared towhen the Frequency Enhancer is not used. For thecdf53 wavelet, it is capable of compressing sig-nals with minimal loss. After being enhanced bythe Frequency Enhancer, high-frequency noise iseffectively removed, while low-frequency signalsare well preserved. This lossless property is bet-ter demonstrated in the field of speech synthesis,where, after enhancement by the Frequency En-hancer, the performance slightly exceeds that ofthe original model in MOS tests. For detailed data,please refer to table 3.",
  "Effect of Multi-Level Wavelet Accelerator": "To further explore the potential for acceleration,we conducted tests in the field of speech synthesisusing the Haar wavelet, which demonstrated themost stable performance. The results of the exper-iment are shown in . It can be observedthat when the speech signal is split into quartersof its original length, both training and inferencespeeds increase by more than fivefold. However,unlike the results of splitting just once (as shownin the second row of , corresponding to thesecond row of ), which were better thanthe original model, the results after splitting fourtimes, even with the Frequency Enhancer, exhib-ited a notable decline in MOS values. We believethis is due to information loss caused by excessivecompression. However, the substantial increase inspeed still makes this method worth considering forscenarios where ultra-clear audio is not required.",
  "Performance on Multi-Speaker Dataset": "In response to concerns regarding the generalizabil-ity of our method, we conducted additional experi-ments using the VCTK dataset (Oord et al., 2016),applying all the wavelets tested in our originalstudy. To further strengthen our findings, we alsoevaluated the performance of our low-frequencyspeech enhancer, which forms part of our ongoingresearch efforts, on the same dataset. The results,presented in , demonstrate that our approachmaintains consistent performance across differentdatasets.",
  "Conclusion": "In this paper, we have enhanced the speech diffu-sion model by transitioning its generation target tothe wavelet domain, thereby doubling the modelstraining and inference speeds. We offer a new per-spective on accelerating speech models by focusingon processing the signal itself rather than modify-ing the model. Our approach has demonstratedmodel versatility and task adaptability across bothspeech enhancement and synthesis. Through ourresearch, we found that the Coif1 wavelet is an ex-cellent choice for scenarios requiring noise reduc-tion without the need to preserve timbre, while theDB2 wavelet is preferable when changes in timbremust be considered. For speech synthesis tasks, theHaar wavelet offers simplicity and effectiveness,whereas the cdf53 wavelet excels at preserving in-formation to the greatest extent. Additionally, Wedesigned two simple and easily integrable front-end modules. The first achieves better performancethan the original model while doubling the speed.The second offers a performance comparable tothe original while enabling an acceleration of morethan five times.",
  "limitations": "In this study, speed tests were conducted on a large-scale cluster, subject to the hardware variabilityinherent in the cluster (despite all GPUs beingV100s, they may not be identical), which couldintroduce some timing inaccuracies. However, con-sidering that the training and inference times formost wavelet-utilizing diffusion models do not sig-nificantly differ, we believe these discrepancies canbe disregarded. This does not detract from our con-tribution of accelerating the speech diffusion modelby a factor of two.",
  "Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. De-noising diffusion probabilistic models. Advancesin neural information processing systems, 33:68406851": "Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,et al. 2021. Lora: Low-rank adaptation of large lan-guage models. In International Conference on Learn-ing Representations. R Huang, MWY Lam, J Wang, D Su, D Yu, Y Ren, andZ Zhao. 2022. Fastdiff: A fast conditional diffusionmodel for high-quality speech synthesis. In IJCAIInternational Joint Conference on Artificial Intelli-gence, pages 41574163. IJCAI: International JointConferences on Artificial Intelligence Organization.",
  "Bunlong Lay, Jean-Marie Lemercier, Julius Richter, andTimo Gerkmann. 2023. Single and few-step diffusionfor generative speech enhancement. arXiv preprintarXiv:2309.09677": "Didier Le Gall and Ali Tabatabai. 1988. Sub-band cod-ing of digital images using symmetric short kernelfilters and arithmetic coding techniques. In ICASSP-88., International Conference on Acoustics, Speech,and Signal Processing, pages 761764. IEEE. Jinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, andZhou Zhao. 2022. Diffsinger: Singing voice synthe-sis via shallow diffusion mechanism. In Proceedingsof the AAAI conference on artificial intelligence, vol-ume 36, pages 1102011028. Yen-Ju Lu, Zhong-Qiu Wang, Shinji Watanabe, Alexan-der Richard, Cheng Yu, and Yu Tsao. 2022. Con-ditional diffusion probabilistic model for speech en-hancement. In ICASSP 2022-2022 IEEE Interna-tional Conference on Acoustics, Speech and SignalProcessing (ICASSP), pages 74027406. IEEE.",
  "Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean. 2013.Efficient estimation of wordrepresentations in vector space.arXiv preprintarXiv:1301.3781": "Gabriel Mittag, Babak Naderi, Assmaa Chehadi, andSebastian Mller. 2021. Nisqa: A deep cnn-self-attention model for multidimensional speech qual-ity prediction with crowdsourced datasets. arXivpreprint arXiv:2104.09494. Aaron van den Oord, Sander Dieleman, Heiga Zen,Karen Simonyan, Oriol Vinyals, Alex Graves,Nal Kalchbrenner, Andrew Senior, and KorayKavukcuoglu. 2016. Wavenet: A generative modelfor raw audio. arXiv preprint arXiv:1609.03499.",
  "Wei Ping, Kainan Peng, Kexin Zhao, and Zhao Song.2020. Waveflow: A compact flow-based model forraw audio. In International Conference on MachineLearning, pages 77067716. PMLR": "Ryan Prenger, Rafael Valle, and Bryan Catanzaro. 2019.Waveglow: A flow-based generative network forspeech synthesis. In ICASSP 2019-2019 IEEE Inter-national Conference on Acoustics, Speech and SignalProcessing (ICASSP), pages 36173621. IEEE. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brock-man, Christine McLeavey, and Ilya Sutskever. 2023.Robust speech recognition via large-scale weak su-pervision. In International Conference on MachineLearning, pages 2849228518. PMLR.",
  "Cassia Valentini-Botinhao, Xin Wang, Shinji Takaki,and Junichi Yamagishi. 2016.Investigating rnn-based speech enhancement methods for noise-robusttext-to-speech. In SSW, pages 146152": "Rafael Valle, Kevin J Shih, Ryan Prenger, and BryanCatanzaro. 2020. Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis.In International Conference on Learning Representa-tions. Christophe Veaux, Junichi Yamagishi, and Simon King.2013. The voice bank corpus: Design, collectionand data analysis of a large regional accent speechdatabase. In 2013 international conference orien-tal COCOSDA held jointly with 2013 conference onAsian spoken language research and evaluation (O-COCOSDA/CASLRE), pages 14. IEEE.",
  "Simon Welker, Julius Richter, and Timo Gerkmann.2022. Speech enhancement with score-based gen-erative models in the complex stft domain. arXivpreprint arXiv:2203.17004": "Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu,Serge Belongie, and Bharath Hariharan. 2019. Point-flow: 3d point cloud generation with continuous nor-malizing flows. In Proceedings of the IEEE/CVFinternational conference on computer vision, pages45414550. Hao Yen, Franois G Germain, Gordon Wichern, andJonathan Le Roux. 2023. Cold diffusion for speechenhancement. In ICASSP 2023-2023 IEEE Interna-tional Conference on Acoustics, Speech and SignalProcessing (ICASSP), pages 15. IEEE. Qiquan Zhang, Aaron Nicolson, Mingjiang Wang,Kuldip K Paliwal, and Chenxu Wang. 2020. Deep-mmse: A deep learning approach to mmse-basednoise power spectral density estimation. IEEE/ACMTransactions on Audio, Speech, and Language Pro-cessing, 28:14041415. Xiangyu Zhang, Qiquan Zhang, Hexin Liu, TianyiXiao, Xinyuan Qian, Beena Ahmed, EliathambyAmbikairajah, Haizhou Li, and Julien Epps. 2024.Mamba in speech: Towards an alternative to self-attention. arXiv preprint arXiv:2405.12609.",
  "ADetails of Experiment Setup": "Diffwave offers two configurations: base and large.In the base version, the model comprises 30 resid-ual layers, a kernel size of 3, and a dilation cycleof [1, 2, ..., 512]. It utilizes 50 diffusion stepsand a residual channel count of 64. The largeversion maintains all parameters identical to thebase, except for an increase to 128 residual chan-nels and 200 diffusion steps. All models employedthe Adam optimizer, with a batch size of 16 and alearning rate of 2104.We trained each DiffWavemodel for a total of 1 million steps.We conducted evaluations on two versions ofCDiffuSE: base and large. The base CDiffuSEmodel employs 50 diffusion steps, while the largeCDiffuSE model uses 200 diffusion steps. Batchsizes differ, with the base CDiffuSE set to 16 andthe large CDiffuSE set to 15. Both the base andlarge CDiffuSE models were trained for 300,000iterations, following an early stopping scheme.",
  "C.1How Wavelets Accelerate Diffusionmodels": "In 3.1, we detailed the application of DiscreteWavelet Transform (DWT) and Inverse DiscreteWavelet Transform (IWT) in processing audio sig-nals, highlighting how these techniques compressthe audio signal features during the diffusion pro-cess. This section elaborates on the principles be-hind the acceleration offered by the Wavelet Diffu-sion Accelerator.To facilitate training acceleration, the diffusionmodel shifts its focus from generating completeaudio signals with extensive features to producingcompressed speech signals in wavelet domain. Inline with this shift, DWT is employed to process theraw audio signal g (n) R12x, where n denotesthe sample index, through two complementary fil-ters. Specifically, a low-pass filter extracts thelow-frequency components low R12x:",
  "cD = high 2.(12)": "At this stage, the signal g (n) R12x is com-pressed into h (n) R2x, wherein h embodiesa two-channel structure, each channel containingfeatures of halved length.This change significantly contributes to reducingthe computational time required for training thediffusion model. To further demonstrate, we exem-plify with the computational changes in the diffu-sion models first convolutional layer. Assumingthe output channel count is Cout, the kernel size isK, and the output length Lout remains unchangedfrom the input length. The formula for calculat-ing Multiply-Accumulate Operations (MACs) perchannel is:",
  "C.2Wavelets for Diffusion Acceleration: WhyNot FFT": "While wavelet and Fourier transforms both serveas essential tools in signal processing and sharesimilarities in handling time and frequency domaininformation, this section explores why Fast FourierTransform (FFT) is not applicable for accelerat-ing diffusion models. This is determined by theinherent nature of the Fourier transform. Assum-ing f(t) is the representation of the signal in thetime domain and f() is its representation in thefrequency domain, where t stands for time and for frequency, then the CFT can be described as:",
  "f (t) eitdt.(15)": "The Fourier transform fits the entire signal f(t)with a series of sine and cosine functions, convert-ing it into frequency domain information f (). Asa result, the signal is stripped of time informationfollowing this transformation. However, conven-tional input audio signals f(t) display traits where local frequency domain features shift in responseto variations in short-time segments of the timedomain signal, like abrupt transitions or displace-ments. This lack of capability to concurrently ana-lyze local time and frequency domain informationmakes the Fourier transform insufficient for accu-rately recreating the original audio in generativemodels.In contrast, for the wavelet transform, assuming (t) as a basic wavelet function, let:",
  "dt. (17)": "At this juncture, the wavelet transform converts aunivariate time-domain signal f(t) into a bivari-ate function f (a, b) encompassing both time andfrequency domain information. It enables targetedanalysis of local frequency domain characteristicscorresponding to specific time domain segments,making it particularly well-suited for handling com-mon non-stationary audio signals.Besides, the wavelet transforms capability fortime-frequency localization analysis ensures thatdownsampling and compressing cA and cD doesnot result in significant information loss. On thecontrary, based on the Discrete Fourier Transform,FFT struggles with signal compression for diffu-sion acceleration due to its local frequency domaintransformations affecting characteristics across theentire time domain."
}