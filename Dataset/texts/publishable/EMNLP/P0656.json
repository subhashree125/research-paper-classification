{
  "Abstract": "Existing text-to-video diffusion models relysolely on text-only encoders for their pretrain-ing. This limitation stems from the absence oflarge-scale multimodal prompt video datasets,resulting in a lack of visual grounding and re-stricting their versatility and application in mul-timodal integration. To address this, we con-struct a large-scale multimodal prompt datasetby employing retrieval methods to pair in-context examples with the given text promptsand then utilize a two-stage training strategyto enable diverse video generation tasks withinthe same model. In the first stage, we pro-pose a multimodal conditional video generationframework for pretraining on these augmenteddatasets, establishing a foundational model forgrounded video generation. Secondly, we fine-tune the model from the first stage on threevideo generation tasks, incorporating multi-modal instructions. This process further refinesthe models ability to handle diverse inputs andtasks, ensuring seamless integration of multi-modal information. After this two-stage train-ing process, VIMI demonstrates multimodalunderstanding capabilities, producing contex-tually rich and personalized videos groundedin the provided inputs, as shown in .Compared to previous visual grounded videogeneration methods, VIMI can synthesize con-sistent and temporally coherent videos withlarge motion while retaining the semantic con-trol. Lastly, VIMI also achieves state-of-the-art text-to-video generation results on UCF101benchmark.",
  "Introduction": "Recent advancements in video diffusion modelshave led to significant successes across variousvideo creation tasks (Singer et al., 2022; Villegaset al., 2022; Zhang et al., 2023; Chai et al., 2023;Chen et al., 2023; Ceylan et al., 2023; Geyer et al.,2023). These models have demonstrated impres-sive capabilities in generating high-quality videos from textual prompts (An et al., 2023; Blattmannet al., 2023b; Ge et al., 2023; Guo et al., 2023b; Heet al., 2023; Ho et al., 2022a,b; Singer et al., 2022;Wang et al., 2023; Zhou et al., 2023; Blattmannet al., 2023a).However, the majority of thesemodels rely solely on text-only encoders for theirdiffusion-based pretraining. This limitation stemsfrom the absence of large-scale multimodal promptdatasets, which results in a lack of visual groundingduring the pretraining stage. Consequently, currentmodels struggle to incorporate visual input effec-tively, restricting their versatility and application inscenarios that demand multi-modal integration.To effectively incorporate visual input into pre-trained text-to-video models, standalone imageencoders are often employed to process imageprompts (Jiang et al., 2023b; Guo et al., 2023a; Renet al., 2024; He et al., 2024). The visual embed-dings generated by these encoders are then injectedinto the diffusion model, enabling it to handle mul-timodal applications. However, this approach ne-cessitates customized model designs, leading tofragmented solutions that cannot support varioustasks in a unified manner. As a result, the mod-els lack the flexibility and generalization needed toseamlessly integrate different modalities for diversevideo generation tasks.Recently, generative pretrained multimodal lan-guage models have demonstrated robust multi-modal in-context learning capabilities, showcasingtheir ability to process and integrate various typesof input data effectively (Team et al., 2023; Zhuet al., 2023; Achiam et al., 2023; Liu et al., 2024).Inspired by this success, we introduce a multi-modal instruction pretraining framework VIMI forgrounded video generation. This novel frameworkaims to leverage the strengths of multimodal mod-els, enhancing the ability to generate videos that arecoherently grounded in both textual and visual in-puts. Specifically, the training of VIMI consists oftwo stages: (1) Retrieved Augmented Pretraining;",
  "<image_3>": ": Examples of VIMI for grounded video generation. Thanks to our visual grounding during retrieval-augmented pretraining and multimodal instruction tuning, our generator can generate videos from multimodalprompts that include multiple image entities. Each multimodal prompt is displayed below the generated videos,illustrating the models capability to integrate and interpret both textual and visual inputs effectively. and (2) Multi-Modal Instruction Fine-Tuning.During the pretraining stage, we first constructa large-scale multimodal prompt dataset by em-ploying a large-scale retrieval method to pair mul-timodal in-context examples with the given textprompts. The retrieved contexts from a web-scalecorpus provide a rich multimodal in-context en-vironment for model training. With these paireddatasets, we can either pretrain a multimodal videogenerator from scratch or fine-tune an existing text-to-video generator. After this stage, the modelgains the capability to understand both text-onlyand multimodal inputs for video generation. Thisestablishes a foundation model for grounded videogeneration, capable of integrating diverse modali-ties into cohesive video outputs.In the second stage, we fine-tune the model fromthe first stage on various video generation tasks,incorporating multimodal instructions. This fine-tuning process further refines the models abilityto handle diverse inputs and tasks, ensuring it canseamlessly integrate multimodal information. Af-ter this two-stage training process, VIMI demon-strates enhanced multimodal understanding capa-bilities, producing contextually rich and personal-ized videos grounded in the provided inputs. Thismakes the model highly versatile and effective fora wide range of video generation applications.In summary, our main contributions include:",
  "Text-To-Video Pretraining": "We base our work on the diffusion framework pro-posed by Menapace et al. (2024), which adaptsthe EDM (Karras et al. (2022a)) diffusion frame-work to high resolution video generation. In EDM,the forward diffusion process is characterizedby a variance-exploding mechanism p(x|x) N(x, 2I), where noise is gradually added tothe data, causing the variance to increase over time,and x represents the data at the current noiselevel. The reverse process is modeled by learnabledenoiser function denoted as D, which is trainedusing a denoising objective formulated as:",
  "(b) Multimodal Instruction Tuning for Videos": ": Overview of our VIMI framework. (a-left) We first construct a large-scale dataset by employing retrievalmethods to pair multimodal in-context with given text prompts. Then we present a multimodal conditional videogeneration framework for pretraining on these augmented datasets. (b) We propose multimodal instruction tuningfor video generation, grounding the model on customized input specified in different multimodal instructions forvideo generation, including subject-driven video generation, video prediction and text-to-video. By fine-tuning themodel with multimodal instructions, we enable VIMI to generate videos that are both contextually rich and visuallyaccurate across a wider range of tasks.",
  "Multimodal Large Language Models": "Building upon the success of Large Language Mod-els (LLMs), Multimodal Large Language Mod-els (MLLMs) (Liu et al., 2024; Zhu et al., 2023;Team et al., 2023) integrate visual informationfrom a pretrained vision encoder (Radford et al.,2021) with an advanced LLM (Touvron et al., 2023;Jiang et al., 2023a). This integration is achievedby treating visual modalities as sequences of dis-crete tokens. In our work, we utilize MLLMs toprocess and interpret multimodal in-context inputdata s = (s1, s2, ..., sn), where si can be a sig-nal unit, such as an image. For the image unit siin the prompt, a pre-trained CLIP visual encoderViT-L/14, is used to provide the visual featuresvi = Visual-Encoder(si). The patch features vibefore the last Transformer layer, combined withthe text tokens, are used for MLLM encoding, for-mulated as:",
  "Method": "We aim to generalize the video generation pretrain-ing to the multimodal setting. shows theoverview of our framework. Sec. 3.1 introduceshow we construct a large-scale multimodal input-video dataset by employing retrieval methods topair in-context examples with given text prompts.Sec. 3.2 presents a multimodal conditional videogeneration framework for pretraining on these aug-mented datasets, establishing a foundational modelfor grounded video generation. Sec. 3.3 introducesthe instruction finetuning stage on three video gen-eration tasks, incorporating multimodal instruc-tions.",
  "Retrieval-Augmented Multi-modalDatasets": "Retrieval-based methods collect relevant informa-tion to the input from an external multimodal mem-ory M. In our study, we use web-scale image-text pairs as our multi-modal memory for retrievaland build index into a list of key-value pairs, i.e.M = {(ki, vi)}. Then, given the input sequence s,the retrieval engine E matches it with all keys andreturns the top K most similar keys to the querytogether with their values:",
  "Ours": ": Comparison of subject-driven video generation. We compared with concurrent work ID-Animator (Heet al., 2024) for zero-shot human video generation (above) and VideoBooth (Jiang et al., 2023b) for general subject-driven video generation (below). Our video generator can synthesize temporally coherent videos with large motionwhile retaining the semantic control. 500M image-text pairs as our multimodal mem-ory. Using this retrieval approach, we augment ourinternal text-to-video and text-to-image datasets.Specifically, we use the text caption as the queryand retrieve the top-3 image-text pairs from thememory M for model training. These retrievedmultimodal documents are then combined with thetext input to form the new multimodal input, whichserves as the condition for video pretraining, ensur-ing that the model receives contextually relevantand diverse multimodal information.",
  "C = MLLM(F({(ki1, vi1), ..., (kiK, viK)}, s)(5)Here, F denotes concatenation and the embeddingC encapsulates the rich contextual information": "from both the text and the retrieved multimodaldata.Following (Menapace et al., 2024), we useFITs (Chen and Li, 2023) as the backbone to jointlymodel the spatial and temporal dimensions for high-quality video generation. However, here we onlyuse the multimodal conditioning embedding C tocontrol the generation process rather than the textembeddings from T5 text encoder. We concate-nate additional tokens representing the diffusiontimestep, framerate and original resolution of thecurrent input, to support variable video frameratesand large differences in resolution and aspect ratiosin the training data. To generate high-resolutionoutputs, we pretrain a cascade model consistingof a first-stage model producing 36 64px videosand a second-stage upsampling model producing288 512px videos.",
  ": An overview of our data curation pipeline forsubject-driven video generation": "ily focuses on grounding the model in the noisyretrieved in-context input for video generation. Asa result, VIMI may not fully utilize visual featuresfor precise and faithful video generation.To address these limitations, we propose mul-timodal instruction tuning for video generation,grounding the model on customized input specifiedin different multimodal contexts for video gener-ation. By fine-tuning the model with multimodalinstructions, we enhance its ability to integrate andutilize visual features more effectively, enablingVIMI to generate videos that are both contextuallyrich and visually accurate across three video tasks,illustrated in b. Subject-Driven Video GenerationTo enhancethe visual grounding capabilities for video gener-ation, we curate a multimodal interleaved promptcomposed of texts and images based on the Panda-70M dataset (Chen et al., 2024). The data cura-tion pipeline is illustrated in . First, weextract entity words from the text captions usingLarge Language Models (LLMs). For each entity,we extract the corresponding image segment usingGrounding DINO (Liu et al., 2023) for object de-tection and SAM (Kirillov et al., 2023) for imagesegmentation. This process ensures that each tex-tual element has a visually grounded counterpart.We prepend the task instruction \"Generate a videowith the text and image interleaved prompt.\" to theprompt. This curated data ensures that the modelcan ground specific multimodal inputs effectivelyand generate videos that faithfully represent thecombined textual and visual information. Video PredictionAs our framework can flexi-bly encode multimodal prompts, we simply encodethe first frame along with the text prompt withMLLMs. Following this, we generate subsequentframes based on the given multimodal prompt. To",
  "FVD IS": "CogVideo (Hong et al., 2022) (Chinese)751.323.6CogVideo (Hong et al., 2022) (English)701.625.3MagicVideo (Zhou et al., 2023)655-LVDM (He et al., 2023)641.8-Video LDM (Blattmann et al., 2023b)550.633.5VideoFactory (Wang et al., 2023)410.0-Make-A-Video (Singer et al., 2022)367.233.0PYoCo (Ge et al., 2023)355.247.46VideoPoet (Kondratyuk et al., 2023)35538.4W.A.L.T (Gupta et al., 2023)258.135.1Lumiere (Bar-Tal et al., 2024)332.537.5Snap Video (288 288 px)260.138.89Snap Video (512 288 px)200.238.89",
  ":Zero-shot evaluation results on UCF101(Soomro et al., 2012)": "facilitate this process, we prepend the task instruc-tion \"Generate a video with the following text andfirst frame.\" to the prompt. This approach allowsthe model to anchor the video generation processwith a visual starting point, ensuring that the sub-sequent frames are coherently built upon both theinitial visual and textual inputs. Text-to-Video GenerationWe also use our aug-mented text-to-video dataset for instructed text-to-video generation. Initially, the input comprisesonly text. To enhance this input, we leverage re-trieval methods as described in 3.1 to augment itwith retrieved images. We prepend the task in-struction \"Generate a video with the retrieved text-image examples and text prompt.\" to the prompt,setting a clear directive for the model. This ap-proach ensures that the model receives enrichedand contextually relevant multimodal data, improv-ing its capability to generate high-quality videosfrom multimodal in-context descriptions.",
  ": Examples of Video Prediction results": "tion. We use the retrieval methods of section 3.1to augment multimodal in-context examples forpretraining and instruction tuning. For MultimodalLLMs, we use LLaVa-1.5-13B. For the T2V model,it is a 3.9B FIT model. We use UCF-101 (Soomroet al., 2012), a video dataset from 101 action cate-gories, for general text-to-video evaluation. We usehuman subjects from the CelebA (Liu et al., 2015)and general subject from the Dreambooth (Ruizet al., 2023) for qualitative comparison. Trainingand evaluation details are in Appendix A and B.",
  "Results": "Zero-shot Text-to-Video EvaluationWe gener-ate 10,000 videos (Wang et al., 2023; Blattmannet al., 2023b) sampling classes with the same dis-tribution as the original UCF-101 dataset. We pro-duce a text prompt for each class label (Ge et al.,2023) and compute FVD (Unterthiner et al., 2018)and Inception Score (Salimans et al., 2016). Ta-ble 1 shows our competitive performance to previ-ous state-of-the-art text-to-video generators in bothFVD and IS metrics. We achieve the best FVDscore of 193.7 which we attribute to our visualgrounding during pretraining. Zero-shot Subject-driven Video generationFigures 1 and 3 show our results for subject-drivenvideo generation. Compared to VideoBooth (Jianget al., 2023b), our generator can handle multimodalprompts that include multiple image entities, as il- lustrated in . We also compared our modelwith the concurrent work ID-Animator (He et al.,2024) for zero-shot human identity preservationgeneration in . Overall, our video genera-tor can not only ground on the visual input but alsosynthesize temporally coherent videos with largemotion while retaining semantic control. Video PredictionAs shown in , VIMIcan also generate videos conditioned on a singleimage, thanks to our unified multimodal instructiontuning stage. We first append the <image> tokenafter the text prompt and use MLLMs to encodethis multimodal prompt for video prediction.",
  "Ablation Study": "Effectiveness of retrieval-augmented pretrain-inga shows the evaluations of retrieval-augmented pretraining on our validation set forCLIP similarity and FID metrics. We denote VIMIwithout retrieval augmented pretraining as VIMI(w/o RAG). We use Snap Video (Menapace et al.,2024) with text encoders T5-11B as another base-line. The results indicate that using multimodallarge language models as the encoding leads tounstable model training. Specifically, the FID re-sults converge slowly and do not decrease after125K pretraining steps. In contrast, with retrievalaugmented pretraining, VIMI shows faster conver-gence and more stable training. After 200K pre-training steps, using a multimodal large language",
  "(b) Effectiveness of the number of retrieved images": ": Ablation of retrieval-augmented pretrain-ing. (a) shows the evaluations of retrieval-augmentedpretraining on our validation set for CLIP similarity andFID metrics. We denote VIMI without retrieval aug-mented pretraining as VIMI (w/o RAG). (b) shows theresults of pretraining with different numbers of retrievedimages. model as the encoder demonstrates performancecomparable to Snap Video (Menapace et al., 2024).This highlights the effectiveness of our retrievalaugmented pretraining approach in stabilizing train-ing and improving the overall performance of thevideo generation model. Effectiveness of the number of retrieved Imagesb shows the results of pretraining with dif-ferent numbers of retrieved images. We set K tobe up to 2 for ablation studies, primarily consider-ing the multimodal sequence length. We observethat using only one retrieved image stabilizes themodel training. Increasing K to 2 provides furtherstable improvements in the early pretraining stage.After 200K pretraining steps, the model convergesto comparable evaluation results for both settings.Given our aim to support multi-subject generation,we use K=2 for the pretraining. This choice bal-ances the need for rich contextual information withthe practical constraints of sequence length, ensur-ing stable and effective training. Effectiveness of multimodal instruction tuningFor the second stage, if we fine-tune only onsubject-driven data (denoted as w/o InstructionTuning), VIMI can also generate videos frommultimodal interleaved prompts. To evaluate theeffectiveness of unified multimodal instruction tun-ing, we compare this variant of VIMI in subject- driven generation tasks after fine-tuning for thesame 100K steps. shows that multimodalinstruction tuning preserves identity better and fol-lows instructions more accurately. We attribute thisimprovement to the more diverse fine-tuning tasksprovided by multimodal instruction tuning.",
  "Related Work": "Video GenerationDiffusion models are now thestandard methodology for both image (Ho et al.,2020; Nichol and Dhariwal, 2021; Rombach et al.,2022; Song et al., 2020) and video generation (Anet al., 2023; Blattmann et al., 2023b; Ge et al.,2023; Guo et al., 2023b; He et al., 2023; Ho et al.,2022a,b; Singer et al., 2022; Wang et al., 2023;Zhou et al., 2023; Blattmann et al., 2023a). Earlyvideo diffusion models use the U-Net (Ronnebergeret al., 2015) for the video generation task. Hoet al. (2022b) showed that jointly training on imageand video data can improve text conditioned videogeneration greatly. Make-A-Video (Singer et al.,2022) proposed to build on text-to-image modelswith novel and effective spatial-temporal modules.Video LDM (Blattmann et al., 2023b) adopts alatent diffusion paradigm where a pre-trained la-tent image generator and latent decoder are fine-tuned to generate temporally coherent videos. Mostrecently, diffusion transformer (Peebles and Xie,2022) has been widely adopted for video genera-tion. Latte (Ma et al., 2024) proposes a latent diffu-sion transformer, which adopts a video Transformeras the backbone. W.A.L.T (Gupta et al., 2023) usesa transformer-based method for latent video diffu-sion models and a window attention architecturetailored for joint spatial and spatiotemporal genera-tive modeling. Snap Video (Menapace et al., 2024)replaced U-Nets with efficient transformer-basedFITs (Chen and Li, 2023) and scaled to billions ofparameters. However, these existing works are stilllimited by the use of text encoders like T5 or theCLIP Text encoder, which lack visual grounding inthe pretraining phase. In our work, we propose toutilize multimodal large language models to encodemultimodal inputs for video generation, addressingthe limitations by integrating visual grounding intothe pretraining process. Retrieval Augmented Multimodal PretrainingRetrieval augmentation has shown significantpromise, particularly in language models. Initialwork (Lewis et al., 2020; Guu et al., 2020) demon-strated how incorporating external knowledge into",
  "Input Imagesw/o Instruction Tuningw/ Instruction Tuning": ": Ablation of multimodal instruction tuning. We compare VIMI with a variant finetuned only onsubject-driven data during the second stage (w/o Instruction Tuning). We use the prompt A <image> is sitting infront of a computer and talking to the camera.. VIMI achieves better semantic alignment and identity preservation. a language model can enhance its performance.This is achieved by first retrieving documents rele-vant to the input text from an external memory, andthen integrating these retrieved documents with theinput for improved modeling (Hashimoto et al.,2018; Karpukhin et al., 2020; Borgeaud et al.,2022). Beyond language models, recent studieshave explored retrieval techniques for image gener-ation (Blattmann et al., 2022; Sheynin et al., 2022;Sarto et al., 2022; Ramos et al., 2023; Chen et al.,2022).For instance, KNN-Diffusion (Sheyninet al., 2022) used retrieval methods to search fork-Nearest-Neighbors images, facilitating the train-ing of a small and efficient text-to-image diffusionmodel. RA-CM3 (Yasunaga et al., 2022) was thefirst multimodal model capable of retrieving andgenerating both text and images using autoregres-sive models. Additionally, Re-Imagen (Chen et al.,2022) employed an external multimodal knowledgebase to retrieve relevant image-text pairs, usingthem as references for a diffusion model to generateimages. In contrast to these works, our approach isthe first to uses retrieval methods to augment text-video datasets, formalizing multimodal input-videopairs for video pretraining. Multimodal Instruction TuningInstruction tun-ing was first proposed to finetune a large languagemodel with instructions to improve its zero-shotlearning performance on unseen tasks (Wei et al.,2021; Chung et al., 2024). Inspired by its successin language domain, instruction tuning was also in-troduced in the vision generation domain (Yu et al.,2023; Sun et al., 2023; Liu et al., 2024; Hu et al.,2024). For instance, CM3Leon (Yu et al., 2023)utilized the CM3 multimodal architecture (Agha-janyan et al., 2022), demonstrating the substantial benefits of scaling up and tuning on more diverseinstruction-style data. Emu2 (Sun et al., 2023)demonstrated the in-context learning capabilities oflarge multimodal models with a unified autoregres-sive objective. More recently, Instruct-Imagen (Huet al., 2024) introduced multi-modal instruction forimage generation by fine-tuning a pre-trained text-to-image diffusion model with a two-stage frame-work. In our work, we are the first to propose in-struction tuning for video generation, by unifyingthree distinct video generation tasks within a sin-gle, cohesive instruction framework. By leveraginginstruction tuning, we aim to enhance the modelsability to interpret and execute a wide range ofvideo generation instructions, thereby improvingits performance and applicability in diverse con-texts.",
  "Conclusion": "In this work, we first construct a multimodalprompt dataset for video pretraining using retrievalmethods. We then propose a two-stage trainingstrategy to enable diverse video tasks within thesame model. For the first stage, we introduce a mul-timodal conditional video generation frameworkfor pretraining on these augmented datasets, estab-lishing a foundational model for grounded videogeneration. In the second stage, we fine-tune themodel from the first stage on three video genera-tion tasks, incorporating multimodal instructions.Our experiments demonstrate the effectiveness ofretrieval-augmented pretraining and the use of mul-timodal instruction tuning. We hope this approachopens up new opportunities for video pretraining,such as building large-scale multimodal datasetsfor pretraining, utilizing stronger multimodal large",
  "Limitations": "Firstly, similar to subject-driven image generationmodels, our video generator sometimes strugglesto produce accurate and faithful videos. To im-prove visual quality, future work will focus on uti-lizing stronger multimodal large language models,diffusion tranformers and jointly fine-tuning thesemodels. Secondly, due to memory and training con-straints, we only experimented with two contextexamples and displayed at most two image enti-ties for multi-subject-driven generation. Extendingthis work to support any-subject video generationwill be a goal for future research. Thirdly, ourcurrent results are based on qualitative evaluation.Developing comprehensive evaluation methods forgrounded video generation, such as any-subject-driven video generation, will be crucial for buildinga visually grounded video generator.",
  "Ethical Considerations": "Like all generative AI advancements, visuallygrounded video generation models raise importantethical considerations, such as the creation of mis-leading or false information and bias. Developersand researchers should consider safeguards to ad-dress these issues such as evaluating datasets, andadding watermarks or other identification mech-anisms. It is important to consider the societalimpacts and work towards solutions that balanceinnovation with social responsibility. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774. Armen Aghajanyan, Bernie Huang, Candace Ross,Vladimir Karpukhin, Hu Xu, Naman Goyal, DmytroOkhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis,et al. 2022.Cm3:A causal masked multi-modal model of the internet.arXiv preprintarXiv:2201.07520.",
  "Jie An, Songyang Zhang, Harry Yang, Sonal Gupta,Jia-Bin Huang, Jiebo Luo, and Xi Yin. 2023. Latent-shift: Latent diffusion with temporal shift for efficienttext-to-video generation. arXiv": "Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Her-rmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Jun-hwa Hur, Yuanzhen Li, Tomer Michaeli, et al. 2024.Lumiere: A space-time diffusion model for videogeneration. arXiv preprint arXiv:2401.12945. Andreas Blattmann, Tim Dockhorn, Sumith Ku-lal, Daniel Mendelevitch, Maciej Kilian, DominikLorenz, Yam Levi, Zion English, Vikram Voleti,Adam Letts, et al. 2023a.Stable video diffu-sion: Scaling latent video diffusion models to largedatasets. arXiv preprint arXiv:2311.15127. Andreas Blattmann, Robin Rombach, Huan Ling, TimDockhorn, Seung Wook Kim, Sanja Fidler, andKarsten Kreis. 2023b.Align your latents: High-resolution video synthesis with latent diffusion mod-els. In Proceedings of the IEEE Conference on Com-puter Vision and Pattern Recognition (CVPR).",
  "Ting Chen and Lala Li. 2023. Fit: Far-reaching inter-leaved transformers. arXiv": "Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace,Ekaterina Deyneka, Hsiang-wei Chao, Byung EunJeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, and Sergey Tulyakov. 2024. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. arXiv preprint arXiv:2402.19479. Weifeng Chen, Yatai Ji, Jie Wu, Hefeng Wu, Pan Xie,Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin.2023. Control-a-video: Controllable text-to-videogeneration with diffusion models. arXiv preprintarXiv:2305.13840.",
  "Wenhu Chen, Hexiang Hu, Chitwan Saharia, andWilliam W Cohen. 2022.Re-imagen: Retrieval-augmented text-to-image generator. arXiv preprintarXiv:2209.14491": "Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, et al.2024. Scaling instruction-finetuned language models.Journal of Machine Learning Research, 25(70):153. Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon,Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-BinHuang, Ming-Yu Liu, and Yogesh Balaji. 2023. Pre-serve your own correlation: A noise prior for videodiffusion models. In Proceedings of the IEEE Inter-national Conference on Computer Vision (ICCV).",
  "Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan,and Qifeng Chen. 2023. Latent video diffusion mod-els for high-fidelity long video generation. arXiv": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,Bernhard Nessler, and Sepp Hochreiter. 2017. Ganstrained by a two time-scale update rule converge toa local nash equilibrium. In Advances in NeuralInformation Processing Systems (NeurIPS). Jonathan Ho, William Chan, Chitwan Saharia, JayWhang, Ruiqi Gao, Alexey Gritsenko, Diederik P.Kingma, Ben Poole, Mohammad Norouzi, David J.Fleet, and Tim Salimans. 2022a. Imagen video: Highdefinition video generation with diffusion models.arXiv.",
  "Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu,and Jie Tang. 2022.Cogvideo: Large-scale pre-training for text-to-video generation via transformers.arXiv": "Hexiang Hu, Kelvin CK Chan, Yu-Chuan Su, WenhuChen, Yandong Li, Kihyuk Sohn, Yang Zhao, XueBen, Boqing Gong, William Cohen, et al. 2024.Instruct-imagen: Image generation with multi-modalinstruction. arXiv preprint arXiv:2401.01952. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023a. Mistral7b. arXiv preprint arXiv:2310.06825. Yuming Jiang, Tianxing Wu, Shuai Yang, ChenyangSi, Dahua Lin, Yu Qiao, Chen Change Loy, andZiwei Liu. 2023b.Videobooth: Diffusion-basedvideo generation with image prompts. arXiv preprintarXiv:2312.00777. Vladimir Karpukhin, Barlas Oguz, Sewon Min, PatrickLewis, Ledell Wu, Sergey Edunov, Danqi Chen, andWen-tau Yih. 2020.Dense passage retrieval foropen-domain question answering. arXiv preprintarXiv:2004.04906.",
  "Tero Karras, Miika Aittala, Timo Aila, and SamuliLaine. 2022b.Elucidating the design space ofdiffusion-based generative models. In Advances inNeural Information Processing Systems (NeurIPS)": "Alexander Kirillov, Eric Mintun, Nikhila Ravi, HanziMao, Chloe Rolland, Laura Gustafson, Tete Xiao,Spencer Whitehead, Alexander C. Berg, Wan-YenLo, Piotr Dollr, and Ross Girshick. 2023. Segmentanything. arXiv:2304.02643. Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jos Lezama,Jonathan Huang, Rachel Hornung, Hartwig Adam,Hassan Akbari, Yair Alon, Vighnesh Birodkar,et al. 2023. Videopoet: A large language modelfor zero-shot video generation.arXiv preprintarXiv:2312.14125. Patrick Lewis, Ethan Perez, Aleksandra Piktus, FabioPetroni, Vladimir Karpukhin, Naman Goyal, Hein-rich Kttler, Mike Lewis, Wen-tau Yih, Tim Rock-tschel, et al. 2020. Retrieval-augmented generationfor knowledge-intensive nlp tasks. Advances in Neu-ral Information Processing Systems, 33:94599474.",
  "William Peebles and Saining Xie. 2022. Scalable dif-fusion models with transformers.arXiv preprintarXiv:2212.09748": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,et al. 2021. Learning transferable visual models fromnatural language supervision. In International confer-ence on machine learning, pages 87488763. PMLR. Rita Ramos, Bruno Martins, Desmond Elliott, and YovaKementchedjhieva. 2023. Smallcap: lightweight im-age captioning prompted with retrieval augmenta-tion. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages28402849.",
  "Khurram Soomro, Amir Roshan Zamir, and MubarakShah. 2012. UCF101: A dataset of 101 human ac-tions classes from videos in the wild. arXiv": "Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang,Qiying Yu, Zhengxiong Luo, Yueze Wang, YongmingRao, Jingjing Liu, Tiejun Huang, et al. 2023. Gen-erative multimodal models are in-context learners.arXiv preprint arXiv:2312.13286. Gemini Team, Rohan Anil, Sebastian Borgeaud,Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,Radu Soricut, Johan Schalkwyk, Andrew M Dai,Anja Hauth, et al. 2023.Gemini: a family ofhighly capable multimodal models. arXiv preprintarXiv:2312.11805. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro,Faisal Azhar, et al. 2023. Llama: Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971.",
  "Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li,Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan.2021. Godiva: Generating open-domain videos fromnatural descriptions. ArXiv": "Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi,Rich James, Jure Leskovec, Percy Liang, Mike Lewis,Luke Zettlemoyer, and Wen-tau Yih. 2022. Retrieval-augmented multimodal language modeling. arXivpreprint arXiv:2211.12561. Lili Yu, Bowen Shi, Ramakanth Pasunuru, BenjaminMuller, Olga Golovneva, Tianlu Wang, Arun Babu,Binh Tang, Brian Karrer, Shelly Sheynin, et al.2023. Scaling autoregressive multi-modal models:Pretraining and instruction tuning. arXiv preprintarXiv:2309.02591, 2(3). David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu,Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, andMike Zheng Shou. 2023. Show-1: Marrying pixeland latent diffusion models for text-to-video genera-tion. arXiv preprint arXiv:2309.15818.",
  "ATraining details": "For pretraining, we can either start from scratch orinitialize the weights of the model from existingtext-to-video generators. In our work, we initializethe FIT weights from (Menapace et al., 2024).We keep its parameters frozen for 30,000 steps tostabilize the initial training phase, and then fine-tune the entire model for an additional 100,000steps. In the second stage, we fine-tune the modelstarting from the weights obtained in the first stagefor 30,000 steps. We use a learning rate of 5e3, acosine learning schedule, and a total batch size of256 videos and 256 images.",
  "BEvaluation Protocol": "We evaluate our method against baselines by fol-lowing the protocols in (Singer et al., 2022; Geet al., 2023; Wang et al., 2023; Blattmann et al.,2023b; Zhou et al., 2023; Luo et al., 2023) forzero-shot evaluation on the UCF-101 (Soomroet al., 2012). We generate 16 frames videos in512 288px resolution at 24fps. To validate the ef-fectiveness of pretraining, ablations are performedin 64 36px resolution using the first-stage modelonly, and compute FID (Heusel et al., 2017), FVD(Unterthiner et al., 2018) and CLIPSIM (Wu et al.,2021) metrics against the test set of our internaldataset on 50k generated videos."
}