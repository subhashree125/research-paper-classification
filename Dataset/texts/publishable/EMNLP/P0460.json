{
  "Abstract": "Retrieval-augmented Large Language Models(LLMs) offer substantial benefits in enhancingperformance across knowledge-intensive sce-narios. However, these methods often face chal-lenges with complex inputs and encounter dif-ficulties due to noisy knowledge retrieval, no-tably hindering model effectiveness. To addressthis issue, we introduce BlendFilter, a novel ap-proach that elevates retrieval-augmented LLMsby integrating query generation blending withknowledge filtering. BlendFilter proposes theblending process through its query generationmethod, which integrates both external and in-ternal knowledge augmentation with the origi-nal query, ensuring comprehensive informationgathering. Additionally, our distinctive knowl-edge filtering module capitalizes on the intrin-sic capabilities of the LLM, effectively elimi-nating extraneous data. We conduct extensiveexperiments on three open-domain question an-swering benchmarks, and the findings clearlyindicate that our innovative BlendFilter sur-passes state-of-the-art baselines significantly.",
  "Introduction": "Generative Large Language Models (LLMs) haveshown remarkable proficiency in various applica-tions, such as summarization (Zhang et al., 2023;Wang et al., 2023a), dialogue systems (Hudecekand Duek, 2023; Touvron et al., 2023a), andquestion answering (Lazaridou et al., 2022; Luet al., 2022).Nonetheless, the finite scope oftheir pre-training corpora imposes inherent limi-tations, preventing LLMs from capturing and main-taining comprehensive worldly knowledge, espe-cially given its dynamic nature. This limitationhas spurred interest in retrieval-augmented gener-ation strategies that integrate external knowledgesources, like Wikipedia, to refine the quality ofLLM-generated content.Typically, retrieval-augmented generation meth-ods (Brown et al., 2020; Izacard et al., 2022b; Za- kka et al., 2023) feed a task input, such as a userquery or a question in open-domain question an-swering, into a retriever to obtain related knowl-edge documents. Subsequently, the LLM gener-ates content based on the initial input and the in-formation retrieved. Nevertheless, this direct re-trieval strategy faces challenges with intricate taskinputs (Shao et al., 2023). While straightforwardqueries enable effective identification of relevantinformation, multifaceted and complex questionsmay not cover some essential keywords, complicat-ing the retrieval of pertinent documents.To enhance the retrieval for complex task inputs,recent studies have proposed methods to enrich theoriginal input. These approaches encompass ques-tion decomposition (Yao et al., 2022; Press et al.,2022), query rewriting (Ma et al., 2023), and queryaugmentation (Yu et al., 2023; Shao et al., 2023).They utilize knowledge memorized by LLMs orsourced from external databases to supplement theinput with additional information, thereby explic-itly incorporating additional keywords and sub-stantially facilitating the retrieval process. Amongthese, query augmentation is particularly notewor-thy and achieves state-of-the-art performance be-cause it processes all retrieved knowledge collec-tively while generating answers and it does not re-quire the training of an additional language modelfor query rewriting.However, current query augmentation methodsstill suffer from some limitations. These techniqueshave typically relied on a single source of augmen-tation, either LLM internal knowledge or an ex-ternal knowledge base. On one hand, for certaincomplex inputs, this single source of augmentationmay not be able to cover all the keywords and thuslead to insufficient augmentation. Furthermore, ex-isting work excludes original input but only rely onthe augmented query, which could further exacer-bate information loss.Another major problem of existing methods is that the incorporated content fetched by the re-triever could contain irrelevant or misleading in-formation. Usually top-K returned documents bythe retriever will be used as augmentation, but thereis no guarantee that all the top-K documents arerelevant and helpful for the task. Correspondingly,incorporating such noise information into the aug-mented query can potentially lead to inaccuracies inthe LLMs output (Wang et al., 2023b). To mitigatethe noise in retrieved knowledge documents, pre-vious studies (Yu et al., 2023; Wang et al., 2023b;Asai et al., 2023) have suggested various strate-gies. Unfortunately, these existing noise reductionmethods in knowledge document retrieval are de-pendent on the LLMs confidence levels, whichcan be imprecise (Xiong et al., 2023). Addition-ally, these methods often require an extra languagemodel to determine the need for retrieval, whichincurs significant computational costs.",
  "pose": "BlendFilter, a novel framework that ad-vances retrieval-augmented large language mod-els by integrating query generation blending andknowledge filtering, as illustrated in . Ourframework, BlendFilter, is structured around threecore components: 1) Query Generation Blendingmodule, 2) Knowledge Filtering module, and a 3)Answer Generation module. The Query Genera-tion Blending module is dedicated to enhancinginput queries through diverse augmentation strate-gies, essentially forming a composite of queries,to handle the complex question challenge. Thismodule incorporates both external and internalknowledge sources for augmentation. These aug-mented queries, including the original, externalknowledge-augmented, and internal knowledge-augmented, are then employed by the retriever tocollect pertinent information. In order to tackle thenoise retrieved knowledge challenge, our proposedKnowledge Filtering module, aims to eliminateirrelevant retrieved knowledge and could operateautonomously without needing an extra languagemodel, leveraging the innate filtering prowess ofthe LLM. In the final phase, the LLM integratesthe filtered knowledge with the original query togenerate the final answer.",
  "The contributions are summarized as follows:1) We introduce a novel query generation blend-ing approach that integrates various augmentationsources. In contrast to existing work that relies": "on one source only, the proposed method enrichesqueries by using a variety of knowledge sources,which lead to a more comprehensive coverage ofpertinent knowledge. 2) We present a novel andeffective knowledge filtering module designed toeliminate irrelevant knowledge. We are the first topropose the utilization of the LLM itself as a fil-ter in retrieval-augmented generation tasks. 3) Weconduct extensive experiments across three open-domain question answering benchmarks. The re-sults demonstrate that our proposed model, Blend-Filter, significantly surpasses the baseline modelsacross three distinct backbones.",
  "Related Work": "Retrieval-augmented generation enhances LargeLanguage Models (LLMs) by leveraging externalknowledge to improve generation quality. Initial ap-proaches, as discussed in (Izacard and Grave, 2021;Shao and Huang, 2021; Izacard et al., 2022a; Shiet al., 2023), portrayed LLMs as passive recipientsof retrieved knowledge, lacking interactive dynam-ics with retrievers. However, due to the inherentchallenges in accurately capturing relevance be-tween inputs and documents, these direct methodsoften yield only marginal improvements. Address-ing this, recent advancements (Nakano et al., 2021;Trivedi et al., 2022; Jiang et al., 2023; Li et al.,2023b,a; Wang et al., 2023b; Asai et al., 2023; Yuet al., 2023; Ma et al., 2023; Press et al., 2022; Yaoet al., 2022) have empowered LLMs to engage ac-tively with retrievers, thereby enhancing relevancemodeling. The integration of LLMs into the re-trieval process broadly falls into three categories:1) question decomposition, 2) query rewriting, and3) query augmentation. For question decomposi-tion, as exemplified by Yao et al. (2022) and Presset al. (2022), LLMs break down a complex ques-tion into simpler components, leveraging both pre-vious interactions and retrieved knowledge. Thisdecomposition facilitates more straightforward rea-soning by LLMs. However, the success of thisapproach heavily depends on the LLMs capabili-ties. Insufficiently powerful LLMs might generatemisleading sub-questions. Moreover, this methodrequires maintaining a historical context, poten-tially leading to lengthy dialogues and increasedcomputational costs. In the realm of query rewrit-ing, models are trained, often utilizing reinforce-ment learning, to reformulate the original questioninto a version more conducive to retrieval (Ma et al.,2023; Li et al., 2023b). These revised questions typ-",
  ": The framework of BlendFilter": "ically yield improved generation outcomes. Nev-ertheless, training an additional model for rewrit-ing is a resource-intensive process. The third ap-proach, query augmentation, involves enrichingqueries with knowledge from either LLM internaldatabases or external sources (Shao et al., 2023; Yuet al., 2023). A limitation of this method is its re-liance on a single source of augmentation and oftenoverlooking the original query, thus constrainingoverall model performance. The aforementioned studies directly utilize re-trieved knowledge, yet recent research (Wang et al.,2023b; Li et al., 2023a) highlights that such knowl-edge can sometimes be irrelevant or even detrimen-tal to LLMs when answering queries. To solve thischallenge, (Wang et al., 2023b) suggests an initialassessment to determine if LLMs need to retrieveknowledge, utilizing a classifier that could be basedon BERT-like models or the LLM itself. How-ever, this approach requires additional training data,which poses challenges in zero-shot or few-shotlearning scenarios, and the LLMs self-evaluationmay not always yield reliable results. (Asai et al.,2023) introduces a self-reflective method to ascer-tain the necessity of retrieval and to assess therelevance between the retrieved knowledge andthe input. A critical limitation of this method, asnoted by (Asai et al., 2023), is its dependence ontraining an auxiliary language model to producetext with reflection tokens, incurring extra costs.Additionally, (Yu et al., 2023) employs a strategy of comparing the average negative likelihood ofanswers with and without external knowledge toguide decision-making. Nevertheless, this measuremay not be a precise indicator of model confidenceand is not universally applicable across models,with certain models like GPT-3.5-turbo and GPT-3.5-turbo-Instruct currently unable to access thisfeature. We summarize the differences betweenthe proposed BlendFilter and other baselines in in the appendix.",
  "Overview": "To enhance the retrieval quality for retrieval-augmented LLMs, we introduce a frameworknamed BlendFilter, which incorporates query gen-eration blending and knowledge filtering, as de-picted in . We begin by presenting queryblending, a technique that enhances the originalquery by incorporating both external knowledgeand the LLMs internally memorized knowledge(.2). Additionally, we propose a knowl-edge filtering module to effectively remove irrele-vant knowledge (.3). Finally, we demon-strate how the LLM generates answers based on",
  "Query Generation Blending": "Numerous studies (Izacard and Grave, 2021; Shaoand Huang, 2021; Izacard et al., 2022a; Shi et al.,2023) have validated the effectiveness of utiliz-ing a retriever to enrich questions with relevantknowledge, thereby boosting the performance ofLLMs. This process can be represented as follows:Kr = R(q, K; K), a = M(a|Prompt(q, Kr)),where a represents the generated answer, Kr de-notes the retrieved knowledge, and K serves as thehyper-parameter for the retriever, controlling thequantity of retrieved knowledge items. Nonethe-less, in cases where the query is complex, directlyinputting it into the retriever often fails to retrievethe correct knowledge documents. As a solution,we advocate for the incorporation of both externaland internal knowledge augmentation techniquesto refine the query.External Knowledge Augmentation. For com-plex questions, such as those in multi-hop questionanswering (Yang et al., 2018), which often entailimplicit sub-problems and span multiple knowl-edge domains, we utilize an external knowledgebase to refine the original query and facilitate doc-ument retrieval. Specifically, we initially retrieverelevant knowledge documents using the originalquery, as follows: Kex = R(q, K; K).Subsequently, we engage the LLM to derive theanswer using the acquired knowledge documentsvia the Chain-of-Thought (CoT) approach (Weiet al., 2022). This step is depicted as: aex =M(a|PromptCoT(q, Kex)), where aex representsthe reasoning and answer generated by the LLMbased on the retrieved knowledge Kex. The gen-erated context aex contains related keywords andvaluable information through CoT reasoning basedon retrieved knowledge from the external knowl-edge base, thereby assisting the retriever in pin-pointing relevant knowledge. Subsequently, weintegrate the generated context aex with the ini-tial query q to formulate the enhanced query, asshown below: qex = aexq, where representsthe concatenation operation. Remark 1. This process of external knowledge aug-mentation essentially acts as a two-hop reasoningmechanism to refine the query. In fact, it can be ex-tended to higher-order augmentation, but typically,leveraging two-hop information proves to be suf-ficiently effective in enhancing retrieval accuracydue to the LLMs strong capabilities. Consequently,",
  "we refrain from employing higher-order augmenta-tion in order to strike a balance between efficiencyand accuracy": "Internal Knowledge Augmentation. LLMs havememorized a lot of factual knowledge. Some re-lated knowledge is not retrieved in external knowl-edge augmentation while LLMs may memorizethem internally.Consequently, we can promptthe LLM to produce a detailed response to thequery, drawing upon its internal knowledge. Thisinternally-sourced response acts as a supplementto the external knowledge. Specifically, the gen-erated text based on LLM internal knowledge canbe formulated as ain = M(a|Prompt(q)), and theaugmented query is qin = ainq.",
  "Knowledge Filtering": "Byintegratingbothexternalandinternalknowledge-augmented queries in conjunctionwith the original query, we are able to re-trieve the corresponding knowledge documentsseparately, as follows:Kq=R(q, K; K),Kqex = R(qex, K; K), Kqin = R(qin, K; K),whereKqrepresentsknowledgedocumentsretrieved by the original query, Kqex correspondsto the external knowledge-augmented query, andKqin pertains to the internal knowledge-augmentedquery.A direct approach to leveraging thisretrieved knowledge involves taking their union:Kdirectr= Kq Kqex Kqin.This method ensures that the synthesized knowl-edge, Kdirectr, encompasses a broader spectrum ofrelevant documents, thereby enhancing the qualityof the retrieved knowledge. Nonetheless, retriev-ing some unrelated documents is inevitable dueto the inherent imperfections of the retrieval pro-cess and the selection of the top-K documents,which may include irrelevant information whenK exceeds the number of ground truth knowledgedocuments. This unrelated information can po-tentially lead to confusion and misguidance forthe LLM, resulting in incorrect outputs. Ratherthan training a separate knowledge filter to iden-tify and eliminate unrelated information, we haveobserved that the LLM itself serves as an effec-tive knowledge filter. We provide both the originalquery and the retrieved knowledge to the LargeLanguage Model (LLM) and instruct the LLM toperform knowledge filtering. This can be formu-lated as follows: Kfq = M(K|Prompt(q, Kq)),Kfqex=M(K|Prompt(q, Kqex)),Kfqin=M(K|Prompt(q, Kqin)). The final knowledge uti-",
  "lized for generation is obtained by taking theunion of the filtered knowledge sets, i.e. Kr =Kfq Kfqex Kfqin, where represents takingunion operation": "Remark 2. Our method involves filtering knowl-edge and subsequently combining the filtered in-formation. An alternative option is to reverse thesequence of these two steps. However, we have ob-served that commencing with the union of knowl-edge may result in a larger knowledge set, conse-quently intensifying the challenge of subsequentknowledge filtering. Consequently, we opt to filterknowledge independently for Kq, Kqex, and Kqin.",
  "Experiment": "In this section, we evaluate the proposed BlendFil-ter and answer the following research questions:RQ1) How does BlendFilter perform comparedto state-of-the-art retrieval-augmented baselines?RQ2) Can the proposed BlendFilter generalizewell with respect to different backbones and retriev-ers? RQ3) Is the LLM effective to filter unrelatedknowledge documents? RQ4) What are the roles ofthe original query, external knowledge-augmentedquery, and internal knowledge-augmented queryin model performance improvements respectively?RQ5) How does the performance change with vary-ing numbers of knowledge documents? RQ6) Willthe proposed BlendFilter be improved by samplingmultiple times with different temperatures?",
  "Following Shao et al. (2023), we evaluate the first500 questions from the training dataset for Strat-egyQA and 500 questions from the developmentdataset for HotPotQA and 2WikiMultiHopQA. For": "multi-hop question answering datasets, we employexact match (EM) and F1 as evaluation metrics,and for the commonsense reasoning dataset, weuse accuracy, following Yao et al. (2022) and Shaoet al. (2023). To evaluate the retrieval performance,we leverage widely used Recall and Precision asevaluation metrics. Additionally, to assess the ef-fectiveness of the proposed knowledge filtering ineliminating irrelevant information, we introduce anew metric called S-Precision. This metric mea-sures the proportion of questions for which theretrieved documents precisely match the goldenrelevant documents.",
  "Implementation Details": "We evaluate models with three different LLMs:GPT3.5-turbo-Instruct1, Vicuna 1.5-13b (Zhenget al., 2023), and Qwen-7b (Bai et al., 2023). Weutilize the state-of-the-art efficient retrieval methodColBERT v2 (Santhanam et al., 2022) as the re-triever implemented by Khattab et al. (2022, 2023).The knowledge base we employ is the collectionof Wikipedia abstracts dumped in 2017 (Khattabet al., 2023). We show the detailed informationabout implementation details in the appendix.",
  "Performance Comparison": "In this section, we evaluate the performance of boththe baseline models and our proposed BlendFiltermodel using various backbones. The results are dis-played in , , and , addressingRQ1 and RQ2.The performance results in the tables demon-strate that our proposed BlendFilter consistentlyachieves substantial improvements over the base-lines across different backbones and datasets. Re-markably, our BlendFilter model achieves aver-age performance improvements of 9.7%, 7.4%,and 14.2% when using GPT3.5-turbo-Instruct, Vi-cuna 1.5-13b, and Qwen-7b as backbones, respec-tively. These results demonstrate the effectivenessof our proposed BlendFilter in enhancing retrieval- : Performance of BlendFilter with GPT3.5-turbo-Instruct as the backbone. IMP represents the percentageof improvements compared to baselines with respect to Exact Match on HotPotQA and 2WikiMultihopQA andAccuracy on StrategyQA.",
  "Direct0.3420.462CoT0.3480.470ReAct0.2800.371SelfAsk0.2900.393ITER-RETGEN0.3560.488BlendFilter0.4200.547": "with those in , it becomes evident that utiliz-ing ColBERT v2, a dense retriever, yields superiorperformance compared to BM25. Dense retrieversprove more effective in capturing semantic sim-ilarities between questions and documents, espe-cially for complex queries. Moreover, our proposedBlendFilter consistently outperforms the baselineswhen BM25 serves as the retriever as well. Theproposed BlendFilter achieves an improvement ofapproximately 18%, surpassing the performancewhen ColBERT v2 is employed as the retriever, incomparison to the baseline models. One potentialexplanation is that BM25 lacks the potency of Col-BERT v2, making the application of query blend-ing to ensure the explicit inclusion of keywords inqueries a more crucial factor. This highlights theeffectiveness of our proposed BlendFilter acrossdifferent retrievers.",
  "Effectiveness for Retrieval": "In this section, we address RQ3 by computing Pre-cision, Recall, and S-Precision values after conduct-ing knowledge filtering with GPT3.5-turbo-Instructon the HotPotQA dataset. Results are presented in. As indicated in , the proposedBlendFilter leads to a substantial improvementin retrieval performance. In both ColBERT v2 and BM25 scenarios, the proposed BlendFilterdemonstrates superior retrieval accuracy comparedto direct retrieval and ITER-RETGEN (multi-hopretrieval). Furthermore, when comparing the Re-call between ITER-RETGEN and BlendFilter, itbecomes evident that the proposed query blend-ing is effective. This illustrates that combiningthree queries can recall a greater number of relateddocuments. When comparing the Precision andS-Precision of the baselines with those of Blend-Filter, we observe that the proposed knowledge fil-tering effectively eliminates unrelated documents.",
  "Effectiveness of Different Queries": "In this section, we investigate how performancechanges when removing specific queries from thequery blending module, addressing RQ4. The re-sults are shown in . According to , itis evident that removing any query from the queryblending process results in thedegradation in modelperformance. This demonstrates the importance ofthe original query, the externally augmented query,and the internally augmented query in the answergeneration process. Additionally, we can find theinternal knowledge-augmented query plays a moreimportant role when BM25 is employed. One pos-sible explanation is that when BM25 is used, theretrieval accuracy is not as robust as that of a denseretriever. Consequently, the externally augmentedquery may still miss some information. This high-lights the importance of complementing it withinternal knowledge augmentation.",
  "Number of Retrieved DocumentsIn this section, we explore how the models perfor-mance varies when employing different numbers ofretrieved documents (K), addressing RQ5. The re-": "sults are presented in . Based on , it canbe observed that as the value of K is increased, theperformance of both ITER-RETGEN and BlendFil-ter initially improves and then experiences a slightdecline. This indicates that increasing the numberof retrieved knowledge documents appropriatelycan enhance model performance. Notably, it is evi-dent that increasing the value of K from 3 to 8 leadsto a substantial improvement in the performance ofBlendFilter, while ITER-RETGEN exhibits onlymarginal performance gains. One possible explana-tion is that BlendFilter incorporates knowledgefiltering, effectively eliminating most unrelatedknowledge, whereas ITER-RETGEN lacks this fil-tering mechanism and incorporates a significantamount of noise knowledge.",
  "Sampling Times": "In this section, we employ various sampling temper-atures for the GPT3.5-turbo-Instruct, specificallytop_p = 0, 0.5, 1, and sample one answer undereach temperature setting on HotPotQA dataset toaddress RQ6. The results are shown in .Based on , it is evident that our proposedBlendFilter consistently outperforms the baselines,whether sampling a single answer or multiple an-swers. Furthermore, when three answers are sam-pled, all methods exhibit improvements, albeit theimprovements in the case of BlendFilter are no-tably smaller compared to the other baseline meth-ods. This observation demonstrates that when pro-vided with more opportunities to answer, all thesemodels tend to have a higher probability of answer-ing correctly, whereas our proposed BlendFilterexhibits lower variance.",
  "Direct Retrieval with CoTITER-RETGENBlendFilter": ": Performance of models with multiple answersampling on HotPotQA with GPT3.5-turbo-Instruct.For three answers, if one of the answers is correct, itsEM will be 1, and the F1 score is the highest one of thethree answers. Instruct. The original question is \"superMansionstarred the actress who had a recurring role aswhom on Workaholics?\". The related knowledgeincludes the SuperMasion document and JillianBell document. From , we can find both theoriginal query and external knowledge-augmentedquery retrieved knowledge consists of one correctdocument SuperMasion. Additionally, the inter-nal knowledge-augmented query retrieves anothercorrect knowledge document Jillian Bell. Thisdemonstrates the necessity of combining thesethree queries to retrieve all relevant knowledgedocuments. Furthermore, following knowledge fil-tering, our proposed BlendFilter effectively elim-inates all irrelevant documents and provides thecorrect answer to the question.",
  "Conclusion": "In this paper, we introduce BlendFilter, a compre-hensive framework developed to enhance retrieval-augmented generation within LLMs. Our method-ology distinctively incorporates query generationblending and knowledge filtering techniques, ef-fectively tackling the intricacies of complex inputsand significantly reducing noise in retrieved knowl-edge. The amalgamation of external and internalknowledge augmentation fosters a resilient and all-encompassing retrieval mechanism. Additionally,our innovative self-reliant knowledge filtering mod-ule exploits the inherent capabilities of the LLMto refine and purify the retrieved knowledge byeliminating extraneous content. We conducted ex-tensive experiments on three benchmarks, and theresults demonstrate that BlendFilter outperformsstate-of-the-art baselines. Moreover, BlendFiltercan be generalized well for different kinds LLMs,including GPT3.5-turbo-Instruct, Vicuna 1.5-13band Qwen-7b.",
  "ReferencesAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, andHannaneh Hajishirzi. 2023. Self-rag: Learning toretrieve, generate, and critique through self-reflection.arXiv preprint arXiv:2310.11511": "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, FeiHuang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,Xuancheng Ren, Chuanqi Tan, Sinan Tan, JianhongTu, Peng Wang, Shijie Wang, Wei Wang, Sheng-guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-uan Zhang, Yichang Zhang, Zhenru Zhang, ChangZhou, Jingren Zhou, Xiaohuan Zhou, and TianhangZhu. 2023. Qwen technical report. arXiv preprintarXiv:2309.16609. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,Dan Roth, and Jonathan Berant. 2021. Did aristotleuse a laptop? a question answering benchmark withimplicit reasoning strategies. Transactions of theAssociation for Computational Linguistics, 9:346361. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,and Akiko Aizawa. 2020. Constructing a multi-hopqa dataset for comprehensive evaluation of reasoningsteps. In Proceedings of the 28th International Con-ference on Computational Linguistics, pages 66096625. Vojtech Hudecek and Ondrej Duek. 2023. Are largelanguage models all you need for task-oriented dia-logue? In Proceedings of the 24th Annual Meetingof the Special Interest Group on Discourse and Dia-logue, pages 216228. Gautier Izacard and douard Grave. 2021. Leveragingpassage retrieval with generative models for open do-main question answering. In Proceedings of the 16thConference of the European Chapter of the Associ-ation for Computational Linguistics: Main Volume,pages 874880. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lu-cas Hosseini, Fabio Petroni, Timo Schick, JaneDwivedi-Yu, Armand Joulin, Sebastian Riedel, andEdouard Grave. 2022a. Atlas: Few-shot learningwith retrieval augmented language models. Preprint,arXiv:2208.03299. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lu-cas Hosseini, Fabio Petroni, Timo Schick, JaneDwivedi-Yu, Armand Joulin, Sebastian Riedel, andEdouard Grave. 2022b. Few-shot learning with re-trieval augmented language models. arXiv preprintarXiv:2208.03299.",
  "Zhengbao Jiang, Frank F Xu, Luyu Gao, ZhiqingSun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,Jamie Callan, and Graham Neubig. 2023.Ac-tive retrieval augmented generation. arXiv preprintarXiv:2305.06983": "K Sparck Jones, Steve Walker, and Stephen E. Robert-son. 2000.A probabilistic model of informationretrieval: development and comparative experiments:Part 2.Information processing & management,36(6):809840. Omar Khattab,Keshav Santhanam,Xiang LisaLi, David Hall, Percy Liang, Christopher Potts,and Matei Zaharia. 2022.Demonstrate-search-predict: Composing retrieval and language mod-els for knowledge-intensive NLP. arXiv preprintarXiv:2212.14024. Omar Khattab, Arnav Singhvi, Paridhi Maheshwari,Zhiyuan Zhang, Keshav Santhanam, Sri Vard-hamanan, Saiful Haq, Ashutosh Sharma, Thomas T.Joshi, Hanna Moazam, Heather Miller, Matei Za-haria, and Christopher Potts. 2023. Dspy: Compilingdeclarative language model calls into self-improvingpipelines. arXiv preprint arXiv:2310.03714. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, YingSheng, Lianmin Zheng, Cody Hao Yu, Joseph E.Gonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-cient memory management for large language modelserving with pagedattention. In Proceedings of theACM SIGOPS 29th Symposium on Operating SystemsPrinciples.",
  "Xiaonan Li, Changtai Zhu, Linyang Li, Zhangyue Yin,Tianxiang Sun, and Xipeng Qiu. 2023a. Llatrieval:Llm-verified retrieval for verifiable generation. arXivpreprint arXiv:2311.07838": "Xiaopeng Li, Lixin Su, Pengyue Jia, Xiangyu Zhao,Suqi Cheng, Junfeng Wang, and Dawei Yin. 2023b.Agent4ranking: Semantic robust ranking via person-alized query rewriting using multi-agent llm. arXivpreprint arXiv:2312.15450. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, PeterClark, and Ashwin Kalyan. 2022. Learn to explain:Multimodal reasoning via thought chains for sciencequestion answering. Advances in Neural InformationProcessing Systems, 35:25072521.",
  "Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao,and Nan Duan. 2023. Query rewriting for retrieval-augmented large language models. arXiv preprintarXiv:2305.14283": "Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,Long Ouyang, Christina Kim, Christopher Hesse,Shantanu Jain, Vineet Kosaraju, William Saunders,et al. 2021.Webgpt: Browser-assisted question-answering with human feedback.arXiv preprintarXiv:2112.09332. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al.2022. Training language models to follow instruc-tions with human feedback.Advances in NeuralInformation Processing Systems, 35:2773027744.",
  "Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,Noah A Smith, and Mike Lewis. 2022. Measuringand narrowing the compositionality gap in languagemodels. arXiv preprint arXiv:2210.03350": "Keshav Santhanam, Omar Khattab, Jon Saad-Falcon,Christopher Potts, and Matei Zaharia. 2022. Col-bertv2:Effectiveandefficientretrievalvialightweight late interaction. In Proceedings of the2022 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 37153734. Zhihong Shao, Yeyun Gong, Yelong Shen, MinlieHuang, Nan Duan, and Weizhu Chen. 2023. En-hancing retrieval-augmented large language modelswith iterative retrieval-generation synergy. In Find-ings of the Association for Computational Linguis-tics: EMNLP 2023, pages 92489274. Associationfor Computational Linguistics.",
  "Zhihong Shao and Minlie Huang. 2021.An-swering open-domain multi-answer questions viaa recall-then-verify framework.arXiv preprintarXiv:2110.08544": "Weijia Shi, Sewon Min, Michihiro Yasunaga, Min-joon Seo, Rich James, Mike Lewis, Luke Zettle-moyer, and Wen-tau Yih. 2023. Replug: Retrieval-augmented black-box language models.arXivpreprint arXiv:2301.12652. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, et al. 2023a.Llama:Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023b.Llama 2: Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Harsh Trivedi, Niranjan Balasubramanian, TusharKhot, and Ashish Sabharwal. 2022.Interleav-ing retrieval with chain-of-thought reasoning forknowledge-intensive multi-step questions.arXivpreprint arXiv:2212.10509. Jiaan Wang, Yunlong Liang, Fandong Meng, Beiqi Zou,Zhixu Li, Jianfeng Qu, and Jie Zhou. 2023a. Zero-shot cross-lingual summarization via large languagemodels. In Proceedings of the 4th New Frontiers inSummarization Workshop, pages 1223. Yile Wang, Peng Li, Maosong Sun, and Yang Liu.2023b. Self-knowledge guided retrieval augmenta-tion for large language models. In The 2023 Con-ference on Empirical Methods in Natural LanguageProcessing. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in NeuralInformation Processing Systems, 35:2482424837. Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz,et al. 2020. Transformers: State-of-the-art naturallanguage processing. In Proceedings of the 2020 con-ference on empirical methods in natural languageprocessing: system demonstrations, pages 3845. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, JieFu, Junxian He, and Bryan Hooi. 2023. Can llmsexpress their uncertainty? an empirical evaluationof confidence elicitation in llms.arXiv preprintarXiv:2306.13063. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,William Cohen, Ruslan Salakhutdinov, and Christo-pher D Manning. 2018. Hotpotqa: A dataset fordiverse, explainable multi-hop question answering.In Proceedings of the 2018 Conference on Empiri-cal Methods in Natural Language Processing, pages23692380. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, IzhakShafran, Karthik R Narasimhan, and Yuan Cao. 2022.React: Synergizing reasoning and acting in languagemodels. In The Eleventh International Conferenceon Learning Representations.",
  "We adopt following state-of-the-art baselines toevaluate our proposed BlendFilter:": "Direct Prompting (Brown et al., 2020) instructsthe LLM to provide direct answers to questionswithout offering explanations or explicit reason-ing steps. We evaluate both Direct Promptingwith and without retrieval as our baseline ap-proaches, referring to them as Direct for brevity. CoT Prompting (Wei et al., 2022) instructs theLLM to generate answers accompanied by ex-plicit reasoning steps. Similar to Direct Prompt-ing, we evaluate CoT Prompting with and with-out retrieval, referring to them as CoT in ourexperiments. ReAct (Yao et al., 2022) incorporates reasoning,action, and observation steps. The generationprocess concludes upon reaching the finishingstate. The action can involve either generatinga query to retrieve knowledge or finalizing thegeneration. The observation entails the retrievedknowledge documents.",
  "Gillian Jones | Gillian Jones drama \"Packed to the Rafters\" since 2009. Jan Hooks | Janet Vivian \"Jan\" Hooks roles in film and television": "Question: superMansion starred the actress who had a recurring role as whom on Workaholics?Knowledge:SuperMansion | SuperMansion is an American stop-motion The series premiered on Crackle on October 8, 2015.Jillian Bell | Jillian Leigh Bell (born April 25, 1984) is an American comedian, actress, and screenwriter. She is best known for her recurring roles as Jillian Belk on \"Workaholics \"Fist Fight\" (2017).Answer: Jillian Belk",
  "D.0.1Implementation Details": "We evaluate our approach with three differ-ent LLMs: GPT3.5-turbo-Instruct2, Vicuna 1.5-13b (Zheng et al., 2023), and Qwen-7b (Bai et al.,2023). GPT3.5-turbo-Instruct is a refined versionof InstructGPT (Ouyang et al., 2022), Vicuna 1.5-13b is trained based on Llama 2 (Touvron et al.,2023b) continually, and Qwen-7b is a Transformer-based model trained from scratch. Vicuna 1.5-13b and Qwen-7b are open-source models. We utilizethe state-of-the-art efficient retrieval method Col-BERT v2 (Santhanam et al., 2022) as the retrieverimplemented by Khattab et al. (2022, 2023) whichapplies quantization to accelerate approximate near-est neighbor search. We conduct experiments usingVicuna 1.5-13b with vLLM Kwon et al. (2023) andQwen-7b with Transformers (Wolf et al., 2020),respectively. The knowledge base we employ isthe collection of Wikipedia abstracts dumped in2017 (Khattab et al., 2023). In all experiments, weutilize a 3-shot in-context learning setting follow-ing the approach of Shao et al. (2023). The value ofk is set to 5 for all methods. The detailed promptsare provided in the Appendix.ECase Study",
  "Answer questions following the givenformat": "Knowledge:{Example_Knowledge}Question:Do people take laxatives becausethey enjoy diarrhea?Lets think step by step.Laxatives are substances that loosen stoolsand increase bowel movements.Peopletake laxatives to treat and/or preventconstipation.So the answer is No. Knowledge:{Example_Knowledge}Question:Could Durian cause someonesstomach to feel unwell?Lets think step by step.Durian has a pungent odor that manypeople describe as being similar to feet andonions. Unpleasant smells can make peoplefeel nauseous.So the answer is Yes. Knowledge:{Example_Knowledge}Question:Did the swallow play a role in afamous film about King Arthur?Lets think step by step.Monty Python and the Holy Grail was afamous film about King Arthur. In MontyPython and the Holy Grail, swallows arementioned several times.So the answer is Yes. Knowledge:{Filtered_Knowledge}Question:{question}Lets think step by step. Answerthe following question based on the givencontext.The final answer to a questionshould always be either Yes or No, andNOTHING ELSE.",
  "Prompt for Knowledge Filtering on Hot-PotQA and 2WikiMultihopQA": "What general topic is Question {question}related to?Answer:The topic is related to forgetyour knowledge about {topic}.Pleaseonly consider the knowledge below.knowledge 0 : {Retrieved_knowledge0}knowledge 1 : {Retrieved_knowledge1}knowledge 2 : {Retrieved_knowledge2}knowledge 3 : {Retrieved_knowledge3}knowledge 4 : {Retrieved_knowledge4}Pleasechecktherelevancebetween{question} and knowledges 0-4 oneby one, remove the irrelevant ones andshow me the relevant ones. There may bemultiple relevent ones. Please take a deepbreath and do it step by step. Pleasecheck the relevance between the givenquestion and knowledges 0-4 one by onebased on the given context. ONLY outputthe relevant knowledge ids (0-4). Theremay be multiple relevent ones."
}