{
  "Abstract": "In interactions between users and languagemodel agents, user utterances frequently ex-hibit ellipsis (omission of words or phrases)or imprecision (lack of exactness) to prioritizeefficiency. This can lead to varying interpre-tations of the same input based on differentassumptions or background knowledge. It isthus crucial for agents to adeptly handle the in-herent ambiguity in queries to ensure reliability.However, even state-of-the-art large languagemodels (LLMs) still face challenges in suchscenarios, primarily due to the following hur-dles: (1) LLMs are not explicitly trained to dealwith ambiguous utterances; (2) the degree ofambiguity perceived by the LLMs may varydepending on the possessed knowledge. To ad-dress these issues, we propose Alignment withPerceived Ambiguity (APA), a novel pipelinethat aligns LLMs to manage ambiguous queriesby leveraging their own assessment of am-biguity (i.e., perceived ambiguity). Experi-mental results on question-answering datasetsdemonstrate that APA empowers LLMs to ex-plicitly detect and manage ambiguous querieswhile retaining the ability to answer clear ques-tions. Furthermore, our finding proves thatAPA excels beyond training with gold-standardlabels, especially in out-of-distribution sce-narios.The data and code are available at",
  "Introduction": "Large Language Models (LLMs) (Ouyang et al.,2022; Team et al., 2023; Achiam et al., 2023) havedemonstrated remarkable capabilities in text gen-eration, proving particularly effective for question-answering (QA) tasks (Zhang et al., 2023; Etezadiand Shamsfard, 2023). QA systems in the wild fre-quently encounter unexpected user input, such asunanswerable (Kim et al., 2023b; Yin et al., 2023)",
  "*Corresponding author": "When was the last time UGA won a national championship? 1. National tennis championship, 2019 2. National golf championship, 2005 3. National baseball championship, 1990 1. National tennis championship, 2019 Can you clarify your question? UGA won the nationalchampionship in 2019. Model AModel B Intrinsic Model KnowledgeIntrinsic Model Knowledge : An example of an ambiguous query fromAmbigQA. The term national championship posesdiverse denotations, causing ambiguity. (Left) A modelwith diverse relevant knowledge might perceive the caseas ambiguous. (Right) In contrast, the query can bedeemed unambiguous when the model lacks substantialrelated knowledge. Thus, the perceived ambiguity maydiffer depending on the models intrinsic knowledge. or ambiguous questions (Cole et al., 2023; Leeet al., 2023; Kim et al., 2023a). To build an agentthat is both reliable and user-friendly, it is essentialfor the model to robustly handle such inputs. In thiswork, we seek to extend the scope of research tomanage invalid inputs effectively. Specifically, wefocus on managing ambiguity (Gleason, 1963;Mackay and Bever, 1967), which poses a signif-icant challenge in Natural Language Processing(NLP) (Jurafsky, 1996).Ambiguity refers to cases where an expressionconveys multiple denotations (Wasow et al., 2005).Users may pose queries with clear intentions that,possibly due to insufficient domain knowledge oromission during the utterance, result in ambiguousrequests. If a model arbitrarily responds to such am- biguity, there is a risk of misinterpreting the usersoriginal intent, potentially harming the models re-liability. This is particularly evident in domainsrequiring high reliability, such as legal (Schane,2002; Choi, 2024) or medical (Stevenson and Guo,2010; Gyori et al., 2022), where misinterpretationsmay lead to severe consequences. Despite such im-portance, approaches to manage ambiguity robustlyare still significantly unexplored.Properly processing ambiguous inputs is chal-lenging primarily due to the following two hurdles.Firstly, models are not trained to express ambi-guity explicitly. Even if a model is capable ofrecognizing ambiguity, confirming this recognitionrequires explicit cues from the model itself, suchas expressing uncertainty or offering multiple inter-pretations. The second challenge is that the degreeof ambiguity perceived by the model can varybased on its intrinsic knowledge. Consider the sce-nario depicted in . The initial query isambiguous as the phrase national championshipposes various denotations, such as national tennischampionship or national golf championship.With comprehensive knowledge across possible de-notations, a model can likely recognize the querysambiguity (, left). However, limited knowl-edge would lead the model to perceive the query asunambiguous (, right). Therefore, how amodel interprets ambiguity hinges on its knowledgescope, which we define as perceived ambiguity.To overcome these issues, this paper proposesAlignment with Perceived Ambiguity (APA) anovel alignment pipeline for models to explicitlyhandle ambiguous queries by leveraging their per-ceived ambiguity. Specifically, we design a proxytask that guides the model in utilizing its intrinsicknowledge for self-disambiguation of a given query.We then quantify the information gained from thisdisambiguation as an implicit measure of the extentto which the model perceives the input as ambigu-ous. This measure serves as a cue for ambiguoussample selection. For the selected ambiguous queryand its disambiguation, the model generates a clar-ification request regarding the ambiguity. Finally,the model is trained to request explicit clarificationin response to ambiguous queries.Experimental results from a range of QAdatasets demonstrate that APA enables a lan-guage model to properly handle ambiguous in-puts while maintaining its inherent capabilitiesof answering unambiguous queries. Furthermore,we present three new datasets to provide a com- prehensive framework for assessing ambiguity:AmbigTriviaQA, AmbigWebQuestions, and Am-bigFreebaseQA. These datasets facilitate a moreextensive evaluation of models robustness in ad-dressing ambiguity, thus contributing to the furtherexpansion of related research.Our contributions can be summarized as follows:",
  "Related Work": "Ambiguity in NLPAn expression is ambigu-ous if it has two or more distinct denotations (Wa-sow et al., 2005). Ambiguity poses a significantchallenge to NLP applications by obscuring theintended meaning of expressions, preventing mod-els from accurately performing specific tasks. Ef-forts to address this issue span across various do-mains, including machine translation (Pilault et al.,2023), coreference resolution (Poesio and Artstein,2005; Yuan et al., 2023), and natural language in-ference (Liu et al., 2023). The challenge intensifiesin the scope of QA, as ambiguous questions mayyield multiple answers that may not align with theusers initial intent. Min et al. (2020) introducethe AmbigQA dataset to tackle ambiguity in open-domain QA and Stelmakh et al. (2022) expand itto long-form generation. Furthermore, Cole et al.(2023) demonstrate that quantifying sampling rep-etition presents a reliable uncertainty measure forambiguity, while Kim et al. (2023a) generate tree-of-clarification (ToC) that refines input ambiguity.While we share the goal of handling ambiguity, wepropose a method of directly aligning the model.",
  "Information-gain >": "When was the last time UGA won a national championship? UGA won the national championship in 2019. When was the last time UGA won a national tennis championship? The question is ambiguous. Please specify the sport you are referring to. 1. Initial Prediction Assessment 2. Perceived Ambiguity Detection 2-2. Measure Information-gain 3. Response Construction When was the last time UGA won a national championship?The question is ambiguous. Please specify the sport you arereferring to. 4. SFT 2-1. Disambiguation When was the last time UGA won a national tennis championship? : The overall process of APA. We first select incorrect samples that the model currently fails to handle(Stage 1). The model then self-disambiguates these samples by leveraging its intrinsic knowledge. We measurethe information gain (INFOGAIN) between the initial input and the disambiguation, identifying samples with highINFOGAIN as ambiguous (Stage 2). Finally, the model generates a clarification request regarding the ambiguity(Stage 3), which is used as the label for training (Stage 4).",
  "Request": ": Illustration of five possible results from ourscenario. For ambiguous queries, the prediction is cor-rect ( 1) if the model generates a clarification request;otherwise, all the other responses are classified as incor-rect ( 2). When evaluating unambiguous queries, wecompare the predictions to the ground-truth labels andcategorize them as the correct prediction ( 3), incorrectprediction ( 4), or incorrect clarification request ( 5). fluency and consistency. To better harness thesemodels, approaches have been developed to alignthem with human preferences (Leike et al., 2018;Ji et al., 2023b). This has taken various forms, suchas Reinforcement Learning from Human Feedback(RLHF) (Ouyang et al., 2022; Chakraborty et al.,2024), and Supervised Fine-tuning (SFT) (Donget al., 2023; Yang et al., 2023; Zhou et al., 2024).Previous works focused on preferences such ashelpfulness (Ding et al., 2023; Kpf et al., 2023;Xu et al., 2024), safety (Bai et al., 2022; Ji et al.,2023a; Liu et al., 2024b), and factuality (Yang et al.,2023; Tian et al., 2024). Building on this founda-",
  "tion, our research expands the scope by focusingon aligning models to understand and manage am-biguity effectively": "Data Quality Control for AlignmentData-centric AI (Chu et al., 2016; Majeed and Hwang,2023; Kumar et al., 2024) highlights the importanceof data quality in model training. In the contextof instruction-following techniques, LIMA (Zhouet al., 2024) demonstrates that effective modelalignment can be achieved with just 1,000 high-quality, human-curated samples. Similarly, Alpa-Gasus (Chen et al., 2024) leverages only a smallsubset of the Alpaca dataset (Taori et al., 2023),filtered by ChatGPT, for an effective alignment.Various approaches for data selection have beenexplored, including those based on factors suchas length and complexity (Liu et al., 2024a), andgradient similarity from validation sets (Xia et al.,2024). This work proposes a new viewpoint on dataquality estimation: assessing how well data alignsmodels for ambiguity management. For this pur-pose, we utilize the models perceived ambiguityas an implicit cue for measuring data quality.",
  "The primary goal of our research is to align mod-els in a way that they can explicitly handle poten-tially ambiguous inputs, leveraging the models per-ceived ambiguity. To this end, we propose Align-": "ment with Perceived Ambiguity (APA), a four-stage alignment pipeline, illustrated in .In this section, we first formulate the problem anddescribe each stage in detail regarding the five pos-sible results depicted in . Further imple-mentation details are stipulated in Appendix A. Problem FormulationIn this study, we focuson open-domain QA. The model M is expected togenerate a prediction yunambig for an unambiguousquery xunambig given a pre-defined inference tem-plate t(). yunambig is compared to the ground-truthlabel y and categorized as correct prediction ( 3),incorrect prediction ( 4), or incorrect clarificationrequest ( 5). As we expand our input scope toambiguous queries2, the model prediction for theambiguous query yambig is anticipated to serve as aclarification request yclarify to resolve the ambiguity.This approach is grounded on the assumption thatthe user is best positioned to clarify their intent.3 yambig is considered correct ( 1) if it is a properclarification request. Otherwise, responses that failto address the ambiguity are classified as incorrect( 2). The final objective of the alignment is to in-crease the number of samples corresponding to 1while simultaneously maintaining or improving theproportion of responses classified as 3.",
  "Initial Prediction Assessment": "The initial stage focuses on identifying samples thatthe model currently fails to handle. To do so, wecompare the models prediction with the ground-truth label, where samples are categorized basedon accuracy. Specifically, we assess the correctnessby matching yunambig with y and yambig with yclarify.A total of n correct samples, included in 1 and 3,are collected as Dcorrect = {(xicorrect, yicorrect)}ni=1.Incorrect samples falling under categories 2, 4,and 5 are unified as a separate dataset, Dincorrect.",
  "This stage aims to identify samples from Dincorrectthat the model perceives as ambiguous. Given thatit is challenging for the model to express ambiguity": "2Separating ambiguous from unambiguous queries is in-herently challenging due to subjective factors such as variousperspectives and underlying assumptions. Despite the com-plexity, we simplify the problem and follow the pre-definedambiguity from the training dataset for the alignment.3We explored alternatives for ambiguity management butfound them to be impractical. For instance, arbitrarily select-ing one of the valid answers may not accurately capture theusers intent. Presenting all possible answers is often unfeasi-ble due to the potentially vast number of valid responses. explicitly, we construct a proxy task to estimate theambiguity from the models perspective. Specifi-cally, the model is prompted to self-disambiguatethe given query x and generate a disambiguationxdisambig. The model leverages its intrinsic knowl-edge related to x to generate further details in thisprocess. If x is underspecified and the model pos-sesses related knowledge necessary to compensate,then xdisambig would yield a higher certainty fromthe models perspective. On the other hand, if xrequires no specification or the model lacks the nec-essary knowledge, xdisambig would exhibit a similarlevel of uncertainty as x. To quantify the uncer-tainty associated with x and xdisambig, we employthe models average entropy (Malinin and Gales,2021; Abdar et al., 2021). Formally, the entropy ofan output distribution is defined as follows:",
  "Response Construction": "In this stage, we define yclarify, which representsthe clarification request the model should generatein response to an ambiguous query. We exploretwo approaches for response generation: Fixed re-sponse and Generated response. Fixed ResponseWe utilize a pre-defined clarifi-cation request as yclarify for xambig. Specifically, alist of clarification requests is pre-defined, and asingle response is randomly selected as yclarify foreach instance. Generated ResponseThe model is promptedto generate a clarification request specifying thesource of the ambiguity. To do so, we provide themodel with xambig and xdisambig to identify the as-pect that causes the ambiguity, thereby generatingyclarify specific to the identified factor.",
  "Supervised Fine-Tuning (SFT)": "The objective of this stage is to construct datasetsfor the alignment. Specifically, we label m samplesidentified as ambiguous and construct an ambigu-ous dataset Dambig = {(xjambig, yjclarify)}mj=1, whereyclarify serves as the ground-truth label. To preventthe potential loss of the models existing knowl-edge, we also incorporate Dcorrect for training. Thenumber of samples from both datasets are balancedso that n = m. The final training dataset is thusestablished as D = Dcorrect + Dambig. Utilizing thedataset D = {(xk, yk)}n+mk=1 , the model is trainedto generate y for xunambig and yclarify for xambig, em-ploying the identical inference template t(). Themodel M with parameter is trained as follows:",
  "Datasets": "The capability of the model to perform within thetrained domain is pivotal. However, for real-worldapplicability, the model must generalize to out-of-distribution (OOD) queries, as queries that divergefrom the training data are frequently confrontedin practice. Therefore, we utilize AmbigQA (Minet al., 2020) as the in-domain dataset for trainingand validation. The dataset includes both ambigu-ous and unambiguous queries, with unambiguousqueries labeled with ground-truth answers. Situat-edQA (Zhang and Choi, 2021) is used as a held-outOOD test dataset with two different splits, denotedas SituatedQA-Geo and SituatedQA-Temp, eachfocusing on geographical and temporal ambigui-ties. To further evaluate ambiguity across diverseQA domains, we have constructed three additionaldatasets: AmbigTriviaQA, AmbigWebQuestions,and AmbigFreebaseQA, each derived from Triv-iaQA (Joshi et al., 2017), WebQuestions (Berant et al., 2013), and FreebaseQA (Jiang et al., 2019)respectively. We prompt gpt-4o4 to ambiguate theinitial query from the original dataset and verifythe generation. To mitigate the potential biasesin the validation process, we further evaluate theverified samples with human annotators and selectsamples for the final dataset. More details on thedatasets and the construction process are describedin Appendix B.",
  "To evaluate the effectiveness of our approach, we in-troduce two sets of baselines: inference-only meth-ods and trained methods. Specific implementationdetails are described in Appendix C": "Inference-Only MethodsInference-only meth-ods address ambiguity by utilizing different prompt-ing strategies.We employ direct prompting(DIRECT) as a fundamental baseline, applyinga simple QA prompt. Furthermore, we exploreambiguity-aware prompting (AMBIG-AWARE),which incorporates additional instructions on han-dling ambiguous inputs. We also examine SampleRepetition (SAMPLE REP) (Cole et al., 2023) bymeasuring the consistency of the sampled genera-tions. Finally, we compare SELF-ASK (Amayuelaset al., 2023), where the model generates an answerand subsequently determines the ambiguity basedon the generation. Trained MethodsGiven the lack of directly com-parable prior work, we compare APA with fine-tuned baselines wherein the model is trained withthe in-domain training set. We follow the ambiguityas defined within the in-domain dataset, and trainthe model accordingly. We compare FULL-SET,which applies the entire training dataset. Further-more, we compare two variations that leveragesthe equal number of training samples with APA.SUBSETRAND is trained on a randomly selectedsubset with an equal number of ambiguous andunambiguous samples. SUBSETENT applies the en-tropy of the models prediction of the ambiguousquery as the uncertainty measure. Ambiguous sam-ples with the most significant entropy are selected,and unambiguous samples are selected at random.",
  "(0.40)": ": Average and standard deviation (in parentheses)of F1a scores of different data selection methods. Thefirst , second , and third best results are highlighted.Results show that utilizing INFOGAIN regardless of theground-truth ambiguity is effective for data selection. inference-only methods. FULL-SET demonstratessuperior performance among the baselines, lever-aging the entire training set. Notably, SUBSETENTsurpasses SUBSETRAND by a large margin and evenoutperforms FULL-SET in some datasets. The re-sults of SUBSETENT verify that entropy is capableof capturing ambiguity to some extent and is benefi-cial when incorporated into the alignment process.APA achieves superior performance across alldatasets. Despite employing an identical inferencetemplate, APA achieves a notable enhancementin F1u compared to DIRECT. This improvementis especially surprising considering that APA wastrained on Dcorrect, which consists of samples thatthe model is already capable of handling. More-over, APA consistently outperforms across all thedatasets in terms of F1a, achieving gains up to6 points. The results highlight the effectivenessof leveraging perceived ambiguity for alignment,enhancing generalization and robustness. Whencompared to SUBSETENT, the improvement of APAsuggests that INFOGAIN provides better quantifi-cation of ambiguity than entropy. The efficacyof leveraging only the data perceived ambiguous,comprising approximately 32% in the LLAMA2family and 13% in MISTRAL, again emphasizesthe importance of data quality over quantity (Zhouet al., 2024; Chen et al., 2024).Furthermore,APAFIXED generally exhibits enhanced performancecompared to APAGEN. This is because APAGEN en-gages in a more challenging task of generatingspecific clarification requests.",
  "TypeGenerations": "xHow many pages in a brave new world?xdisambigHow many pages in the 1932 edition of the book brave new world by Aldous Huxley?yclarifyYour question is ambiguous. Which edition of the book are you interested in? xWho was the commander of the british forces in boston?xdisambigWho was the commander of the british forces in boston during the american revolution?yclarifyYour question seems ambiguous. Can you be more specific about the event or time? : Examples of generated yclarify and xdisambig from the initial query x. Additional specification from thedisambiguation is highlighted in bold and the specification of the clarification requests are underlined.",
  "Analysis on Sample-level Misalignment": "The alignment process of generating clarificationrequests for ambiguous queries may lead to a po-tential trade-off, where the model incorrectly gen-erates clarification requests for unambiguous in-puts that were previously well-handled. To assesssuch a case, we define Misaligned ClarificationRequest Rate (MCR), which measures the propor-tion of unambiguous samples that were correctlyanswered ( 3 in ) before training but in-correctly shifted to erroneously generating clarifi-cation requests ( 5 in ) after alignment. Alow MCR is desirable, representing that the modelpreserves its existing capabilities even after thealignment. We can observe from that,overall, APA consistently demonstrates the lowestMCR, indicating that the model successfully learnsto handle ambiguity while effectively preservingthe existing capabilities.",
  "The Effect of Threshold Values": "The number of training samples used for alignmentdepends on the threshold value . To understand theimpact of on performance, we conduct an analysisby applying different for ambiguous data selec-tion. We compare SUBSETENT and SUBSETRAND,each with an equal number of training samples. Fig-ure 5 presents the F1a scores measured under dif-ferent . In general, larger reduces the data avail-able for training, resulting in declined performance.SUBSETRAND consistently demonstrates subpar per-formance, whereas SUBSETENT is a strong baselineacross all scenarios. Nevertheless, APA outper-forms all the baselines across different values.",
  "INFOGAIN-based Selection We explore twodifferent selection methods leveraging INFO-": "GAIN: MAX selects top-m samples with thelargest INFOGAIN from the ground-truth am-biguous samples. MIN selects the bottom-msamples with the minimum INFOGAIN amongthose that are ground-truth ambiguous. APA differs from the baselines by utilizing sam-ples perceived as ambiguous, allowing the potentialinclusion of ground-truth unambiguous samples. demonstrates the overall results. RANDconsistently lags behind MAX by a margin of 1 to 4points. The disparity underscores the effectivenessof data selection based on INFOGAIN, even withground-truth ambiguous samples. Moreover, APAoutperforms all the baselines across all the datasets.Notably, even though the perceived ambiguity doesnot always coincide with ground-truth ambiguity,results show that exploiting model-perceived am-biguity significantly enhances alignment.MINdemonstrates the worst performance among themethods evaluated. We speculate that this declineis because the training samples with low INFOGAINare perceived as unambiguous, yet are trained asambiguous. This misalignment likely accounts forthe degradation in performance.",
  "demonstrates examples of generated dis-ambiguation xdisambig and the clarification request": "yclarify from the query x. We can observe that themodel generates factual specifications about thequery leveraging its intrinsic knowledge (e.g., 1932edition of the book). Furthermore, given x andxdisambig, the model successfully generates a clari-fication request, specifically mentioning the factorthat causes the ambiguity (e.g., Which edition). Fur-ther examples of disambiguations and failure casesare in Appendix F.",
  "Conclusion": "In this work, we present a novel alignment pipeline,dubbed Alignment with Perceived Ambiguity(APA), designed to enhance the ability of LLMsto address ambiguities within queries, leveragingthe models intrinsic knowledge. Our method em-ploys an implicit measure INFOGAIN to quantifythe ambiguity perceived by the model itself. Themodel learns to effectively manage (un)ambiguousqueries through alignment based on this metric. Ex-perimental results demonstrate the effectiveness ofAPA, which outperforms all the baselines acrossvarious QA datasets. As a future avenue, we planto explore extending this methodology to broaderdomains and more complex types of ambiguities,further solidifying the role of LLMs in managingthe inherent uncertainty present in NLP tasks.",
  "Limitations": "The scope of our research is mainly focused onshort-form QA tasks. The research scope could beexpanded to long-form generation tasks such as de-tailed reasoning. Furthermore, there are cases whena query becomes ambiguous by considering addi-tional contexts, e.g., cases in conversational QA(Guo et al., 2021). As our research focuses solelyon situations where a single query is given, futurework may consider scenarios where additional con-text is provided to the model. For experiments, weexplore the most widely used models for evalua-tion, specifically LLAMA2 and MISTRAL. Despitethis, a more comprehensive evaluation encompass-ing a broader range of LLMs could have enrichedour findings, providing insights across differentarchitectures and capabilities. Larger-scale mod-els may exhibit different tendencies and, therefore,should be explored in future research. Furthermore,our work mainly focuses on supervised fine-tuning(SFT) as the alignment method. However, alter-native methods, such as Reinforcement Learningfrom Human Preference (RLHF) (Ouyang et al.,",
  "Acknowledgement": "This work was partly supported by SNU-NAVERHyperscale AI Center and Institute of Information& communications Technology Planning & Evalu-ation (IITP) grant funded by the Korea government(MSIT) [NO.RS-2021-II211343, Artificial Intelli-gence Graduate School Program (Seoul NationalUniversity), No.RS-2020-II201373, Artificial Intel-ligence Graduate School Program (Hanyang Uni-versity), NO.RS-2021-II212068, Artificial Intelli-gence Innovation Hub (Artificial Intelligence Insti-tute, Seoul National University)] Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, DanaRezazadegan, Li Liu, Mohammad Ghavamzadeh,Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U. Ra-jendra Acharya, Vladimir Makarenkov, and SaeidNahavandi. 2021. A review of uncertainty quantifica-tion in deep learning: Techniques, applications andchallenges. Information Fusion, 76:243297. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774.",
  "Alfonso Amayuelas, Liangming Pan, Wenhu Chen, andWilliam Wang. 2023. Knowledge of knowledge: Ex-ploring known-unknowns uncertainty with large lan-guage models. Preprint, arXiv:2305.13712": "Yuntao Bai,Saurav Kadavath,Sandipan Kundu,Amanda Askell, Jackson Kernion, Andy Jones, AnnaChen, Anna Goldie, Azalia Mirhoseini, CameronMcKinnon, Carol Chen, Catherine Olsson, Christo-pher Olah, Danny Hernandez, Dawn Drain, DeepGanguli, Dustin Li, Eli Tran-Johnson, Ethan Perez,Jamie Kerr, Jared Mueller, Jeffrey Ladish, JoshuaLandau, Kamal Ndousse, Kamile Lukosuite, LianeLovitt, Michael Sellitto, Nelson Elhage, NicholasSchiefer, Noemi Mercado, Nova DasSarma, RobertLasenby, Robin Larson, Sam Ringer, Scott John-ston, Shauna Kravec, Sheer El Showk, Stanislav Fort,Tamera Lanham, Timothy Telleen-Lawton, Tom Con-erly, Tom Henighan, Tristan Hume, Samuel R. Bow-man, Zac Hatfield-Dodds, Ben Mann, Dario Amodei,Nicholas Joseph, Sam McCandlish, Tom Brown, andJared Kaplan. 2022. Constitutional ai: Harmlessnessfrom ai feedback. Preprint, arXiv:2212.08073.",
  "Conference on Empirical Methods in Natural Lan-guage Processing, pages 15331544, Seattle, Wash-ington, USA. Association for Computational Linguis-tics": "Souradip Chakraborty, Jiahao Qiu, Hui Yuan, AlecKoppel, Furong Huang, Dinesh Manocha, Am-rit Singh Bedi, and Mengdi Wang. 2024. Maxmin-rlhf: Towards equitable alignment of large languagemodels with diverse human preferences. Preprint,arXiv:2402.08925. Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, KalpaGunaratna, Vikas Yadav, Zheng Tang, Vijay Srini-vasan, Tianyi Zhou, Heng Huang, and Hongxia Jin.2024. Alpagasus: Training a better alpaca modelwith fewer data. In The Twelfth International Confer-ence on Learning Representations.",
  "Jonathan H Choi. 2024. Measuring clarity in legal text.U. Chi. L. Rev., 91:1": "Xu Chu, Ihab F. Ilyas, Sanjay Krishnan, and JiannanWang. 2016. Data cleaning: Overview and emergingchallenges. In Proceedings of the 2016 InternationalConference on Management of Data, SIGMOD 16,page 22012206, New York, NY, USA. Associationfor Computing Machinery. Jeremy Cole, Michael Zhang, Daniel Gillick, JulianEisenschlos, Bhuwan Dhingra, and Jacob Eisenstein.2023. Selectively answering ambiguous questions.In Proceedings of the 2023 Conference on Empiri-cal Methods in Natural Language Processing, pages530543, Singapore. Association for ComputationalLinguistics.",
  "Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, andLuke Zettlemoyer. 2023. Qlora: Efficient finetuningof quantized llms. Preprint, arXiv:2305.14314": "Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin,Shengding Hu, Zhiyuan Liu, Maosong Sun, andBowen Zhou. 2023. Enhancing chat language mod-els by scaling high-quality instructional conversa-tions. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing,pages 30293051, Singapore. Association for Com-putational Linguistics. Hanze Dong, Wei Xiong, Deepanshu Goyal, YihanZhang, Winnie Chow, Rui Pan, Shizhe Diao, JipengZhang, KaShun SHUM, and Tong Zhang. 2023.RAFT: Reward ranked finetuning for generative foun-dation model alignment. Transactions on MachineLearning Research.",
  "Benjamin M Gyori, Charles Tapley Hoyt, and AlbertSteppi. 2022. Gilda: biomedical entity text normal-ization with machine-learned disambiguation as aservice. Bioinformatics Advances, 2(1):vbac034": "Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, ChiZhang, Ce Bian, Boyuan Chen, Ruiyang Sun, YizhouWang, and Yaodong Yang. 2023a. Beavertails: To-wards improved safety alignment of LLM via ahuman-preference dataset. In Thirty-seventh Con-ference on Neural Information Processing SystemsDatasets and Benchmarks Track. Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang,Hantao Lou, Kaile Wang, Yawen Duan, Zhong-hao He, Jiayi Zhou, Zhaowei Zhang, Fanzhi Zeng,Kwan Yee Ng, Juntao Dai, Xuehai Pan, AidanOGara, Yingshan Lei, Hua Xu, Brian Tse, Jie Fu,Stephen McAleer, Yaodong Yang, Yizhou Wang,Song-Chun Zhu, Yike Guo, and Wen Gao. 2023b.Ai alignment: A comprehensive survey. Preprint,arXiv:2310.19852. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, Llio Renard Lavaud,Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,Thibaut Lavril, Thomas Wang, Timothe Lacroix,and William El Sayed. 2023. Mistral 7b. Preprint,arXiv:2310.06825. Kelvin Jiang, Dekun Wu, and Hui Jiang. 2019. Free-baseQA: A new factoid QA data set matching trivia-style question-answer pairs with Freebase. In Pro-ceedings of the 2019 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, Volume1 (Long and Short Papers), pages 318323, Min-neapolis, Minnesota. Association for ComputationalLinguistics. Mandar Joshi, Eunsol Choi, Daniel Weld, and LukeZettlemoyer. 2017. TriviaQA: A large scale distantlysupervised challenge dataset for reading comprehen-sion. In Proceedings of the 55th Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 16011611, Vancouver,Canada. Association for Computational Linguistics.",
  "Daniel Jurafsky. 1996. A probabilistic model of lexicaland syntactic access and disambiguation. Cognitivescience, 20(2):137194": "Gangwoo Kim, Sungdong Kim, Byeongguk Jeon, Joon-suk Park, and Jaewoo Kang. 2023a. Tree of clarifica-tions: Answering ambiguous questions with retrieval-augmented large language models. In Proceedingsof the 2023 Conference on Empirical Methods inNatural Language Processing, pages 9961009, Sin-gapore. Association for Computational Linguistics. Najoung Kim, Phu Mon Htut, Samuel R. Bowman, andJackson Petty. 2023b. (QA)2: Question answeringwith questionable assumptions. In Proceedings of the61st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages84668487, Toronto, Canada. Association for Com-putational Linguistics. Andreas Kpf, Yannic Kilcher, Dimitri von Rtte,Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens,Abdullah Barhoum, Duc Nguyen, Oliver Stan-ley, Richrd Nagyfi, Shahul ES, Sameer Suri,David Glushkov, Arnav Dantuluri, Andrew Maguire,Christoph Schuhmann, Huu Nguyen, and Alexan-der Mattick. 2023.Openassistant conversations -democratizing large language model alignment. InAdvances in Neural Information Processing Systems,volume 36, pages 4766947681. Curran Associates,Inc.",
  "Sushant Kumar, Sumit Datta, Vishakha Singh, San-jay Kumar Singh, and Ritesh Sharma. 2024. Op-portunities and challenges in data-centric ai. IEEEAccess": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-field, Michael Collins, Ankur Parikh, Chris Alberti,Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-ton Lee, Kristina Toutanova, Llion Jones, MatthewKelcey, Ming-Wei Chang, Andrew M. Dai, JakobUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-ral questions: A benchmark for question answeringresearch. Transactions of the Association for Compu-tational Linguistics, 7:452466. Dongryeol Lee, Segwang Kim, Minwoo Lee, Hwan-hee Lee, Joonsuk Park, Sang-Woo Lee, and KyominJung. 2023. Asking clarification questions to handleambiguity in open-domain QA. In Findings of theAssociation for Computational Linguistics: EMNLP2023, pages 1152611544, Singapore. Associationfor Computational Linguistics.",
  "Jan Leike, David Krueger, Tom Everitt, Miljan Martic,Vishal Maini, and Shane Legg. 2018. Scalable agentalignment via reward modeling: a research direction.CoRR, abs/1811.07871": "Chin-Yew Lin and Franz Josef Och. 2004.Auto-matic evaluation of machine translation quality usinglongest common subsequence and skip-bigram statis-tics. In Proceedings of the 42nd Annual Meeting ofthe Association for Computational Linguistics (ACL-04), pages 605612, Barcelona, Spain. Alisa Liu, Zhaofeng Wu, Julian Michael, Alane Suhr,Peter West, Alexander Koller, Swabha Swayamdipta,Noah Smith, and Yejin Choi. 2023. Were afraidlanguage models arent modeling ambiguity. In Pro-ceedings of the 2023 Conference on Empirical Meth-ods in Natural Language Processing, pages 790807,Singapore. Association for Computational Linguis-tics. Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, andJunxian He. 2024a. What makes good data for align-ment? a comprehensive study of automatic data se-lection in instruction tuning. In The Twelfth Interna-tional Conference on Learning Representations.",
  "Sourab Mangrulkar, Sylvain Gugger, Lysandre De-but, Younes Belkada, Sayak Paul, and BenjaminBossan. 2022.Peft: State-of-the-art parameter-efficient fine-tuning methods": "Sewon Min, Julian Michael, Hannaneh Hajishirzi, andLuke Zettlemoyer. 2020. AmbigQA: Answering am-biguous open-domain questions. In Proceedings ofthe 2020 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), pages 57835797, Online. Association for Computational Lin-guistics. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al.2022. Training language models to follow instruc-tions with human feedback. Advances in neural in-formation processing systems, 35:2773027744. Adam Paszke, Sam Gross, Francisco Massa, AdamLerer, James Bradbury, Gregory Chanan, TrevorKilleen, Zeming Lin, Natalia Gimelshein, LucaAntiga, et al. 2019. Pytorch: An imperative style,high-performance deep learning library. Advances inneural information processing systems, 32. Jonathan Pilault, Xavier Garcia, Arthur Brainskas, andOrhan Firat. 2023. Interactive-chain-prompting: Am-biguity resolution for crosslingual conditional gen-eration with interaction. In Proceedings of the 13thInternational Joint Conference on Natural LanguageProcessing and the 3rd Conference of the Asia-PacificChapter of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 455483,Nusa Dua, Bali. Association for Computational Lin-guistics. Massimo Poesio and Ron Artstein. 2005. The reliabilityof anaphoric annotation, reconsidered: Taking ambi-guity into account. In Proceedings of the Workshopon Frontiers in Corpus Annotations II: Pie in the Sky,pages 7683, Ann Arbor, Michigan. Association forComputational Linguistics. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-pher D Manning, Stefano Ermon, and Chelsea Finn.2023. Direct preference optimization: Your languagemodel is secretly a reward model. In Thirty-seventhConference on Neural Information Processing Sys-tems.",
  "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, YannDubois, Xuechen Li, Carlos Guestrin, Percy Liang,and Tatsunori B. Hashimoto. 2023. Stanford alpaca:An instruction-following llama model": "Gemini Team, Rohan Anil, Sebastian Borgeaud,Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,Radu Soricut, Johan Schalkwyk, Andrew M Dai,Anja Hauth, et al. 2023.Gemini: a family ofhighly capable multimodal models. arXiv preprintarXiv:2312.11805. Katherine Tian, Eric Mitchell, Huaxiu Yao, Christo-pher D Manning, and Chelsea Finn. 2024.Fine-tuning language models for factuality. In The TwelfthInternational Conference on Learning Representa-tions. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288.",
  "Thomas Wasow, Amy Perfors, and David Beaver. 2005.The puzzle of ambiguity. Morphology and the web ofgrammar: Essays in memory of Steven G. Lapointe,pages 265282": "Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-icz, Joe Davison, Sam Shleifer, Patrick von Platen,Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,Teven Le Scao, Sylvain Gugger, Mariama Drame,Quentin Lhoest, and Alexander Rush. 2020. Trans-formers: State-of-the-art natural language processing.In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing: SystemDemonstrations, pages 3845, Online. Associationfor Computational Linguistics.",
  "Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neu-big, and Pengfei Liu. 2023. Alignment for honesty.arXiv preprint arXiv:2312.07000": "Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu,Xipeng Qiu, and Xuanjing Huang. 2023. Do largelanguage models know what they dont know? InFindings of the Association for Computational Lin-guistics: ACL 2023, pages 86538665, Toronto,Canada. Association for Computational Linguistics. Yuewei Yuan, Chaitanya Malaviya, and Mark Yatskar.2023. AmbiCoref: Evaluating human and model sen-sitivity to ambiguous coreference. In Findings of theAssociation for Computational Linguistics: EACL2023, pages 10231030, Dubrovnik, Croatia. Associ-ation for Computational Linguistics.",
  "Lingxi Zhang, Jing Zhang, Xirui Ke, Haoyang Li, Xin-mei Huang, Zhonghui Shao, Shulin Cao, and XinLv. 2023. A survey on complex factual questionanswering. AI Open, 4:112": "Michael Zhang and Eunsol Choi. 2021. SituatedQA: In-corporating extra-linguistic contexts into QA. In Pro-ceedings of the 2021 Conference on Empirical Meth-ods in Natural Language Processing, pages 73717387, Online and Punta Cana, Dominican Republic.Association for Computational Linguistics. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer,Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, PingYu, Lili Yu, et al. 2024. Lima: Less is more for align-ment. Advances in Neural Information ProcessingSystems, 36.",
  "A.1Pipeline Details": "For initial prediction assessment (Stage 1), we uti-lize the same inference template as DIRECT () and disambiguate the given query with the tem-plate from . We use the greedy generationfor the disambiguation. The threshold is empir-ically set to 0.1 for selecting ambiguous inputs.When balancing training set size, if n > m, werandomly select m samples from Dcorrect, wheren = |Dcorrect| and m = |Dambig|. If n < m, weselect n samples from Dambig with the largest IN- FOGAIN. For APAGEN, we use the template from to generate specific clarification requestsfor each ambiguous queries.Furthermore, forAPAFIXED, we randomly set yclarify from the fol-lowing pre-defined phrases : [The questions isambiguous. Please clarify your question.Yourquestionisambiguous.Canyouclarify your question? Your question isnot clear. Can you clarify your questionplease?]",
  "A.2Training Details": "For training,we applied AdamW optimizer(Loshchilov and Hutter, 2019) with a batch sizeof 32. We selected the model with the best per-formance in the validation set from learning rates{1e-3, 5e-4, 1e-4} and training epochs {1, 2,3}. All the experiments were implemented with Py-torch (Paszke et al., 2019) and Huggingface Trans-formers library (Wolf et al., 2020). For efficienttraining, we applied QLoRA from HuggingfacePEFT library (Mangrulkar et al., 2022) with r=4and alpha=16. The training takes about half anhour on a single Tesla V100 GPU. All experimentsare averaged over three different random seeds.The full results of APA and trained baseline meth-ods with the standard deviation are demonstratedin .",
  "ous and unambiguous samples for each dataset isspecified in": "AmbigQA(Min et al., 2020) is a derivative ofthe Natural Questions dataset (Kwiatkowski et al.,2019), designed to verify ambiguous data points.The dataset covers diverse sources of ambiguity,such as event and entity references. The datasetconsists of pre-defined ambiguous and unambigu-ous queries, where unambiguous queries are la-beled with ground-truth answers. We set AmbigQAas the in-domain dataset and utilize it for trainingand validation. Specifically, we follow the ambi-guity defined by the dataset and train the modelto generate ground-truth answers for unambiguousqueries and pre-defined clarification requests forambiguous queries. Further training details are",
  "stipulated in Appendix C": "SituatedQA(Zhang and Choi, 2021) focusesexplicitly on temporal and geographic ambiguityfrom the input query. As the cause of ambiguityand its construction process are distinct, we assessperformance on the temporal and geographic splitseparately, denoted as Temp and Geo, respectively. TriviaQA(Joshi et al., 2017) consists ofquestion-answer-evidence triplets collected fromWikipedia and the web. For our experiments, weonly utilize the question-answer pairs. We ambigu-iate the subset of TriviaQA to build AmbigTrivi-aQA.",
  "B.2Dataset Construction Details": "To further examine the models capability to in-terpret and generate responses to intentionally am-biguous queries, we constructed AmbigTriviaQA,AmbigWebQuestions, and AmbigFreebaseQA byambiguating the TriviaQA, WebQuestions, andFreebaseQA, respectively. We first prompt gpt-4oto ambiguate the original question with the tem-plate from . To further validate the gener-ation and control the datasets quality, we againprompt gpt-4o for secondary verification. We uti-lize the template in and collect samples",
  ": Instructions for human validation for datasetconstruction. Samples selected as \"Yes\" are considereda valid ambiguation": "verified as ambiguous. Validating the generationsfrom the same model may pose unnecessary biases.To mitigate the potential biases in the validationprocess, we evaluate the verified samples with hu-man annotators and select samples for the finaldataset. () This human-in-the-loop dataconstruction ensures the quality and fairness of thedataset. The process yielded 1,000 question-answerpairs, with 500 ambiguous and 500 unambiguouspairs. Examples from AmbigTriviaQA are demon-strated in .",
  "AMBIG-AWAREWe utilize the template from Ta-ble 11, where we explicitly describe how to handleambiguity. Identically, we use the greedy genera-tions for evaluation": "SAMPLE REPThe template from is usedto generate a single greedy generation and ten sam-pled generations with sampling temperature of 1.0.We quantify the rate of sampled generations thatmatch the greedy generation as the uncertainty mea-sure, where 1.0 is the most certain and 0.0 beingthe least certain. Samples with the measure belowa specific threshold are considered ambiguous. Forinstance, if three out of ten samples exactly matchthe greedy generation, then the uncertainty for thegiven query is 0.3. We empirically select a thresh-old that demonstrates the best F1u and F1a withthe least trade-off. SELF-ASKWe initially prompt the model withthe template from and generate a greedygeneration. Then, the initial query and the gen-erated answer are utilized with the template from and prompt the model to verify the querysambiguity. We modified the prompt from Amayue-las et al. (2023) so that the model can specifically",
  "focus on ambiguity. The ambiguity detection isdetermined based on the models final verificationof \"Yes\" or \"No\"": "FULL-SETThe entire training set is utilized fortraining. Following APAFIXED, we label the ground-truth ambiguous samples with pre-defined clarifi-cation requests as yclarify. (Pre-defined clarificationrequests are listed in Appendix A.1.) The modelis trained to generate y for xunambig and yclarify forxambig with the inference template from . SUBSETRANDThe training method is identical toFULL-SET, but SUBSETRAND utilizes a subset of thetraining set. We randomly select |D| samples fromthe training data, with the equal number (|D|/2) ofambiguous and unambiguous samples. SUBSETENTThe training of SUBSETRAND is iden-tical to SUBSETRAND except the ambiguous sampleselection method. When xambig is given, we mea-sure the entropy of the generated result from themodel. A high entropy value indicates that themodel is uncertain about the prediction of the am-biguous query. Therefore, among the xambig in thetrain set, we select |D|/2 samples with the highestoutput entropy and use them as ambiguous sam-ples.",
  "E.1Details of Sample-level MisalignmentAnalysis": "To measure Misaligned Clarification Request rate(MCR), we start with a base model (e.g., LLAMA27B or MISTRAL 7B) which has not undergone anyalignment training. We prompt the model using thetemplate in and select the correct, unam-biguous samples. Subsequently, we evaluate thealigned models, such as FULL-SET, SUBSETENT,or APAGEN, leveraging these pre-selected samples.We then count the cases where the aligned modelspredictions shifted from providing correct answersto generating wrong clarification requests post-alignment. MCR is measured as the proportionof these shifted samples relative to the total num-ber of initially correct, unambiguous samples. Themetric quantified the extent to which the modelsalignment process leads to unnecessary clarifica-tion requests for previous well-handled unambigu-ous queries.",
  "E.3Details of Data Selection Ablation": "This section details the data selection methods from.3, with the corresponding visualization in. Consider the case where the ground-truthambiguous and unambiguous queries are sortedbased on their INFOGAIN. APA selects m-sampleswith the largest INFOGAIN regardless of the ground-truth ambiguity, focusing on perceived ambiguity.In contrast, RAND randomly selects m-samplesas ambiguous from the ground-truth ambiguousqueries (highlighted in blue in ). MAX andMIN select top-m and bottom-m samples regardingthe INFOGAIN from the ground-truth ambiguous Low INFOGAIN (Perceived Unambiguous)High INFOGAIN (Perceived Ambiguous)",
  "Unambiguous Samples": "RAND MAX MIN INFOGAIN distribution of the train set : Illustration of ground-truth ambiguous andunambiguous samples sorted by the INFOGAIN. Wehighlight the chosen samples for each data selectionmethod. APA selects samples with the largest INFO-GAIN regardless of the ground-truth ambiguity. On theother hand, baseline methods select training data fromground-truth ambiguous samples with different selec-tion strategies.",
  "F.1Failure Cases Before Alignment": "demonstrates generations by modelsbefore alignment for ambiguous queries fromSituatedQA-Geo. Given the diverse denotationsof the query, each model interprets the query dif-ferently based on their intrinsic knowledge. Forinstance, the first question is ambiguous due tothe numerous possible revolution it could ref-erence. Each model interprets revolution dif-ferently: LLAMA2 7B as the Russian revolu-tion, MISTRAL 7B as the French revolution,and LLAMA2 13B as the American RevolutionaryWar. Consequently, each model generates fac-tual responses corresponding to its interpretation.We regard this phenomenon as problematic sincethe user likely has a specific revolution in mindwhile querying the model. However, the modelmay misinterpret the input and generate responsesnot aligned with the users intended reference. Con-sequently, this misalignment can lead to providingincorrect or irrelevant answers.",
  "F.2Case Study of Disambiguations": "demonstrates examples of initial queryx and its disambiguation xdisambig. The first ex-ample is when x is inherently ambiguous, yet themodel perceives it as unambiguous. Specifically,the model generates hallucination (\"in the 1960s\")where the song \"dont mess around with jim\" wasoriginally released in 1972. This non-factual gen-eration would not provide any information gain tothe model, classifying x as ambiguous. In such acase, x should be considered \"unknown\" with norelated knowledge within the model. The secondand third examples are correctly classified, as themodel properly applies its intrinsic knowledge toperceive ambiguity. Regardless of the quantity ofadditional context generated, the model is capa-ble of verifying its ambiguity. The last exampleis a misclassification as ambiguous. Despite dis-ambiguation provides factually correct information(\"1932 novel\" and \"by Aldous Huxley\") for \"bravenew world\", we speculate that the misclassificationmay arise from the existence of various media, suchas movies and songs or even different versions ofthe book, sharing the title \"brave new world\".",
  "F.3Failure Cases of Clarification RequestGeneration": "presents failure cases of clarification re-quest generation. Even when the model success-fully provides valid disambiguation (e.g., in theusa or in 2015), in some cases the model fails toconsider the aspect that causes the ambiguity whilegenerating clarification requests. For example, thefirst case generates \"What is the book the title refersto?\", which does not address the relevant ambiguity.Furthermore, the second example only requests forclarification and fails to provide further specifica-tions regarding the ambiguity."
}