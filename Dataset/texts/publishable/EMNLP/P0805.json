{
  "Does Large Language Model Contain Task-Specific Neurons?": "Ran Song1,2, Shizhu He3,4, Shuting Jiang1,2, Yantuan Xian1,2,Shengxiang Gao1,2, Kang Liu3,4, and Zhengtao Yu1,2*,1 Faculty of Information Engineering and Automation,Kunming University of Science and Technology, Kunming, China2 Yunnan Key Laboratory of Artificial Intelligence, Kunming, China3 The Key Laboratory of Cognition and Decision Intelligence for Complex Systems,Institute of Automation, Chinese Academy of Sciences, Beijing, China4 School of Artificial Intelligence, University of Chinese Academy of Science, Beijing, China{song_ransr,shuting_jiang22}@163.com, {shizhu.he,kliu}@nlpr.ia.ac.cn,, {gaoshengxiang.yn,ztyu}@hotmail.com",
  "Abstract": "Large language models (LLMs) have demon-strated remarkable capabilities in comprehen-sively handling various types of natural lan-guage processing (NLP) tasks. However, thereare significant differences in the knowledge andabilities required for different tasks. Therefore,it is important to understand whether the sameLLM processes different tasks in the same way.Are there specific neurons in a LLM for differ-ent tasks? Inspired by neuroscience, this pa-per pioneers the exploration of whether distinctneurons are activated when a LLM handles dif-ferent tasks. Compared with current researchexploring the neurons of language and knowl-edge, task-specific neurons present a greaterchallenge due to their abstractness, diversity,and complexity. To address these challenges,this paper proposes a method for task-specificneuron localization based on Causal Gradi-ent Variation with Special Tokens (CGVST).CGVST identifies task-specific neurons by con-centrating on the most significant tokens dur-ing task processing, thereby eliminating redun-dant tokens and minimizing interference fromnon-essential neurons. Compared to traditionalneuron localization methods, our approach canmore effectively identify task-specific neurons.We conduct experiments across eight differentpublic tasks. Experiments involving the inhi-bition and amplification of identified neuronsdemonstrate that our method can accurately lo-cate task-specific neurons.",
  ": The upper shows different task-specific neu-rons from the same LLM. The bottom checks if theseneurons affect the tasks by inhibiting and amplifying": "have developed the ability to comprehensively han-dle various types of natural language processing(NLP) tasks. The advantage of this paradigm is thata single model deployment can perform multipletasks, showcasing the versatility and efficiency ofLLMs in NLP applications (Yuan et al., 2024).However, there are significant differences in theknowledge and abilities required for different tasksby LLMs. For example, sentiment analysis focuseson adjectives and adverbs (Benamara et al., 2007),text classification emphasizes domain-specific ter-minology (Avancini et al., 2006), while naturallanguage inference prioritizes the relationship be-tween premise and conclusion sentences (Camburuet al., 2018). This suggests that the processingpatterns of the same LLM may vary across differ-ent tasks, highlighting the necessity to investigate the underlying mechanisms of task-specific pro-cessing in LLM. Task-specific processing can betraced by examining the neurons activated duringthe inference of specific tasks. Therefore, the twocritical questions emerge: Are there neurons withinthe same LLM that handle specific tasks?, and ifso, how does the model manage and differentiatebetween various types of tasks?In fact, neuroscience has discovered that differ-ent brain areas control distinct behavioral abili-ties (Bari and Robbins, 2013). For instance, whilehigher cognitive functions such as learning, rea-soning, decision-making and creativity are primar-ily controlled by the frontal lobe, the neurons in-volved in these processes remain distinct (Collinsand Koechlin, 2012). Inspired by these findings,we hypothesize that although LLMs utilize a uni-fied structure and parameters, the neurons engagedin different tasks may vary significantly. Motivatedby this hypothesis, this paper investigates the exis-tence of task-specific neurons in LLMs, specificallyexamining whether the neurons activated by differ-ent tasks exhibit distinct patterns. As shown in upper, different task-specific neurons inthe same LLM are marked with different colors.Current research has explored specific neurons inLLMs that primarily handle knowledge (Dai et al.,2022; Niu et al., 2024; Chen et al., 2024) and lan-guages (Zhao et al., 2023a; Tang et al., 2024). Onthe one hand, knowledge-specific neurons in LLMsare key units that store factual information. By ad-justing the activity of these neurons, we can controlhow much certain knowledge is represented in themodel (Dai et al., 2022). This ability to manip-ulate knowledge-specific neurons enables knowl-edge editing, allowing for the modification of thefactual information stored in the model (Meng et al.,2022a,b). On the other hand, language-specific neu-rons in LLMs are specialized units that focus onlanguage-related tasks, such as language modelingand machine translation. These neurons are respon-sible for controlling the quality of language gener-ation in the model (Zhao et al., 2023a). By manip-ulating the activity of language-specific neurons, itis possible to influence the way LLMs generate lan-guage, such as translations or responses in differentlanguages. For example, researchers have shownthat adjusting these neurons can enable LLMs toswitch between languages more effectively (Tanget al., 2024). Utilizing language-specific neuronscan help LLMs better handle linguistic tasks.Moreover, current methods cannot directly ex- plore task-specific neurons because they havethe following main different characteristics fromlanguage- and knowledge-specific neurons: 1) Ab-stractness: Unlike specific knowledge and lan-guage, the competencies required for tasks aremore intricate and challenging to represent withinsymbolic systems. 2) Diversity: While knowledgeand language can be examined through a finite num-ber of enumerated examples, task examples exhibitdiversity and are difficult to comprehensively enu-merate. 3) Complexity: The abilities necessaryfor tasks may exhibit interdependencies or operateindependently, often lacking clear demarcations.Specifically, we found that task-specific neuronsonly need to solve the target task; for example,sentiment analysis neurons do not need to containgeneral English neurons. Thus, we believe that notall tokens are crucial for identifying task-specificneurons. A similar approach has been used in taskssuch as continuous learning (Lin et al., 2024) andcomputer vision (Zeng et al., 2022). Different partsof inputs, such as task definitions, and contextualexamples, play distinct roles in the inference pro-cess over LLMs (Jiang et al., 2023b). In biology,studies often use fluorescent markers to track neu-ronal activity by injecting fluorescent agents andobserving their effects to monitor neurons (Chenet al., 2013).Inspired by this, we propose a novel task-specificneuron detection method using Causal GradientVariation with Special Tokens (CGVST). We in-troduced fluorescent markers into the LLM reason-ing process to uncover the crucial role of specialtokens. By observing neuron activation patternstriggered by these tokens, we can obtain abstractsemantic representations beyond individual exam-ples. This approach also helps to control diversityand complexity by focusing only on the special to-kens. Finally, we perform causal masked languagemodel prediction for the task, recording the gra-dient on special tokens, and identifying the mostactive neurons as the task-specific neurons.We also conduct detailed experiments on 8 dif-ferent tasks (Wang et al., 2022b), including quanti-tative experiments on neuron inhibition and ampli-fication. Quantitative experiments demonstrate thatour method can accurately identify task-specificneurons. As shown in bottom, inhibitionof these neurons significantly reduces performanceon the target task, while the effect on other tasks isminimal. Conversely, amplifying these neuronsimproves performance in the related tasks with minimal effect on other tasks. Furthermore, weconducted comprehensive analyses of the neurons,including cross-validation and neuron visualization.The codes for this paper are available at GitHub1.Our contributions can be summarized conciselyin the following three aspects: We propose the existence of task-specific neu-rons, defining them as neurons that have a sig-nificant impact on a specific task while havingminimal impact on other tasks. We introduce a method to identify task-specificneurons based on Causal Gradient Variation withSpecial Tokens (CGVST). By analyzing the im-portance of special tokens and recording gradientvariation at them during task processing, we iden-tify the most relevant task-specific neurons. We conducted several analytical experiments ontask-specific neurons. The results show that ourmethod can effectively locate them, and theyalign more closely with the definition of task-specific neurons compared to other methods.",
  "Memory Mechanism of FFN": "Transformer is an efficient network architectureemployed in various tasks (Vaswani et al., 2017).Mainstream LLMs often utilize multi-layer Trans-former decoders. Each Transformer layer com-prises two components: Multi-Head Self-Attention(MHA) and Feed-Forward Network (FFN). Tak-ing the mainstream LLMs LLama (Touvron et al.,2023) and Mistral (Jiang et al., 2023a) as examples,the Multi-Head Self-Attention mechanism can beexpressed as follows:",
  "Example for In-Context Learning": "<s> [INST] <<SYS>>\\nIn this task, you are given a text from tweets. Your task isto classify given tweet text into two categories: 1) positive, and 2) negativebased on its content.\\n<</SYS>>\\n\\n I wish I could I got studio again tonight...How about tomorrow night? [/INST] negative [INST] is still not feeling good![/INST] negative [INST] Bored I wanna go travel outside the state of Missouribut no idea where Id go [/INST] negative [INST]...",
  "promptcasespecail token": ": The top part of the figure displays examplesof ICL and the various roles involved. The bottom partillustrates the causal tracing results for these roles onthe Y-axis. The X-axis represents the models layers,with darker colors indicating a more significant impacton task performance. 2019) or SiLU (Elfwing et al., 2017).Studieshave shown that language patterns and knowledgeare memorized in the FFN layer, and these mem-ories are triggered by modulating the activationstate (Geva et al., 2021). Therefore, Wlgate can beinferred that memory in LLMs is derived from thisgating mechanism, which plays a crucial role inregulating activation states within the FFN layers.",
  "y = arg maxyjY P(yj | xj, P, C, S)(3)": "whereCrepresentsthecontextandisdefinedasasetofpairs:C{(x1, y1), (x2, y2), . . . , (xn, yn)}.Thepre-diction space is denoted by Y , while P is theprompt that defines the task. Special tokens play acrucial role in regulating LLMs behavior withinthe Chain of Thought framework. They help tomanage multiple rounds of dialogue, enablingmore coherent and contextually relevant responses.By using these tokens, LLMs can maintain context",
  "Causal Tracing of Context": "In inference with ICL, task prompts define tasksfor LLMs, and contextual examples teach task pro-cessing. High-quality examples can enhance taskperformance (Zhang et al., 2023). Special tokensmark the boundaries between different roles of in-puts and absorb their representations.To evaluate the significance of these three roles,this study conducts a causal tracing analysis foreach token. We distinguish the roles within aninput (X {xp, xc, xs}, Y ). During inference, werecord the predicted probability pY of the correctlabel based on the LLMs parameters ,",
  "hj+1i= fact(hji ) hjiWjup Wjdown(7)": "where Aij represents the noise-added prediction ofthe i-th token of the j-th layer, hji is the fluores-cent vector, and N(0, 1).Next, we calculate the difference between thevalues in the perturbation matrix and the originalconfidence, determining the significance of eachtoken in each layer for the task. We categorizethese probabilities according to different roles toidentify which types of tokens most influence thetask. As shown in , we find that perturbingspecial tokens has the most significant effect ontask performance. Therefore, we suggest that theeffectiveness of LLMs in task processing is mainlyattributed to the representation of special tokens.",
  "Causal Gradient Variation with SpecialTokens": "Following the analysis, we discover that the gatingparameters in the FFN layer store the task patternmemory. This means the models ability to han-dle different tasks is encoded in a specific mem-ory structure, allowing for efficient task-switchingand performance optimization. Using causal trac-ing, we identify that special tokens in the contextare crucial for task performance.These tokenrepresentations contain the patterns for task pro-cessing. Building on these insights, we proposea novel method to identify task-specific neuronsbased on Causal Gradient Variation with SpecialTokens (CGVST). This method uses the gradient ofspecial tokens to find neurons that are particularlysensitive to specific tasks.Initially, we perform a forward pass using task-specific data, focusing on computing the loss func-tion when special tokens are predicted, as follows:",
  "Wgate(9)": "where Rld4d, with l is number of layer, andd being the dimensionality of each layer.Given that the size of the FFN gate value foreach layer is 4d-dimensional, we compressed thegradient changes to d-dimensions to obtain a matrixof the same size as hWgate. Subsequently, weselect the n positions with the largest variationsglobally as the task-specific neurons, which areconsidered crucial for the task. A single neuron isdenoted as ji, representing the i-th neuron of thej-th layer.",
  "BASE38.7 59.3 68.8 55.0 64.9 55.6 76.0 54.0 66.2 55.4 54.4 57.1 37.4 59.5 47.7 58.0 56.8 56.8-RANDOM38.9 57.8 69.4 52.2 63.9 57.0 70.3 52.1 62.2 56.2 54.5 51.3 37.1 52.8 47.8 54.3 55.3 54.2 -1.1": "PV (Zhao et al., 2023a)37.8 39.4 65.2 53.2 61.8 51.1 73.2 51.2 66.2 53.2 53.2 51.9 27.8 34.2 45.2 47.7 53.8 47.7 -6.1LAPE (Tang et al., 2024) 31.7 3.6 59.6 49.4 41.4 46.9 56.7 46.9 22.9 23.9 54.3 50.0 25.8 20.3 45.9 41.7 42.3 35.3 -7.0GV (Dai et al., 2022)12.9 17.1 47.9 47.8 57.7 54.5 52.8 46.6 22.0 30.7 39.5 50.4 17.9 25.6 38.4 45.4 36.1 39.8 3.70CGVST (ours)3.4 27.3 3.3 18.4 17.2 31.4 16.4 24.4 7.4 29.4 35.7 29.8 2.2 19.7 27.8 34.3 13.8 26.8 14.0 : This table presents the results obtained after inhibiting task-specific neurons in various tasks. P denotes theaccuracy of the inhibited task, while R indicates the performance on other tasks when the current task is inhibited.The Inhibition Comprehensive Performance (ICP) is calculated as (BASE P) |BASE R|, balancing both Pand R. Underline indicates that tasks are almost ineffective. Bold represents the best result of the indicator. neurons, we can further manipulate them to influ-ence the overall performance of LLMs. Specif-ically, we can amplify (by increasing activationlevels) or inhibit (by decreasing activation levels)the activation value of task-specific neurons. Theoperation is as follows: hl+1 = fact(hlaWlgate) l hlaWlup Wldown,(10)where l represents the selected neuron in the l-thlayer. This neuron is set to a value less than 1 wheninhibited, greater than 1 when amplified, and equalto 1 during normal inference.",
  "Experiments": "In this section, we conduct a series of detailed ex-periments to investigate task-specific neurons. Weintroduce 8 distinct tasks, each serving as an ob-jective for identifying specific neurons. The ex-periments aim to answer the following researchquestions: RQ1: Can the proposed method locateneurons? For the located neurons, does inhibiting( 4.4) and amplifying them ( 4.5) have corre-sponding effects? RQ2: Do task-specific neuronsimpact the models language ability? ( 4.6). RQ3:What is the relationship between neurons corre-sponding to different tasks? ( 4.7) And how arethey distributed in the model? ( 4.8).",
  "Dataset": "We selected 8 tasks from the Super-Natural In-struction dataset (Wang et al., 2022c)2 as follows:Question Answering (QA) generates answers toSQuAD 1.1 questions based on documents. Senti-ment Analysis (SA) classifies the sentiment of anEnglish tweet as positive or negative in social me-dia. Question Understanding (QU) determines ifa clarification for a query is correct by responding",
  "The numbers in the dataset are: 075, 195, 227, 274, 379,391, 512, 1645": "with Yes or No in dialogue. Text Categorization(TC) classifies the topic of an English news arti-cle into one of four classes in news. Law TextCategorization (LTC) classifies an English sen-tence as either overruling or non-overruling in law.Cause Effect Classification (CEC) decides if thesecond sentence logically results from the first onein commonsense reasoning. Emotion Classifica-tion (EC) classifies the emotion of a Twitter postinto one of six classes: sadness, joy, love, anger,fear, or surprise, in social media. Text Matching(TM) classifies pairs of medical questions into twocategories in medicine and healthcare. All datasetsare evaluated using Exact Match accuracy.",
  "Baseline": "We compared the current neuron selection methodswith several baseline approaches. Language Acti-vation Probability Entropy (LAPE) identifies themost active neurons during inference by calculat-ing the entropy of activation frequency and value,considering these as task-relevant neurons (Tanget al., 2024). Parameter Variation (PV) involvestraining on the corresponding task and then iden-tifying neurons with the least parameter changespre- and post-training, which are considered mostrelevant to the task (Zhao et al., 2023a). GradientVariation (GV) determines task-relevant neuronsby finding the parameters with the largest gradientsacross all tokens during task training (Dai et al.,2022). The RANDOM method randomly selectsneurons from different layers and positions.",
  "BASE38.7 59.3 68.8 55.0 64.9 55.6 76.0 54.0 66.2 55.4 54.4 57.1 37.4 59.5 47.7 58.0 56.856.8-RANDOM37.9 56.8 65.4 54.2 62.2 54.7 71.4 53.4 66.1 55.1 55.1 52.2 38.4 56.4 47.8 56.7 55.53 54.93 -3.1": "PV (Zhao et al., 2023a)27.7 35.3 43.7 65.2 56.0 60.6 67.7 70.2 36.7 59.7 45.7 53.6 38.7 37.7 32.4 43.8 40.153.3 -20.2LAPE (Tang et al., 2024) 39.1 38.3 63.0 64.9 64.2 65.6 62.1 60.3 45.1 46.2 54.3 53.7 31.0 32.4 45.3 40.1 48.850.2 -14.6GV (Dai et al., 2022)30.5 29.5 46.0 67.7 56.4 54.7 47.8 55.7 56.7 47.6 56.7 49.8 34.7 28.7 46.5 39.8 46.846.7 -20.1CGVST (ours)41.2 54.5 69.1 61.4 62.1 52.1 69.0 57.0 60.2 52.2 59.5 55.2 37.7 58.9 48.0 56.1 54.055.9-3.7 : This table shows the results after amplifying task-specific neurons on different tasks. P indicates accuracyon the amplified task. R indicates performance on other tasks when the current task is amplified. ACP (AmplificationComprehensive Performance) is calculated as (P BASE)|BASER|. Underline indicates better performancethan BASE. Bold represents the best value of the corresponding indicator. layers) multiplied by 11,008 (the size of the hiddenstates). We designated 5% of the total neurons,amounting to 17,613 neurons, as task-specific neu-rons. The inhibition and amplification values rangefrom {0, 0.05} and {1.5, 2}, respectively. The pro-posed method is realized through the HuggingfaceTransformers library (Wolf et al., 2020). For eachtask, we selected the optimal value. It is importantto note that some neurons cannot tolerate extremelyhigh or low activation values, as this can lead tonetwork instability and potential collapse.",
  "Neurons Inhibition Evaluation": "To determine the significance of task-specific neu-rons on task performance, we inhibited these neu-rons and measured the impact. As shown in Ta-ble 1, our proposed approach outperforms existingneuron search methods. Specifically, our methodachieved the highest performance, indicating thatthe neurons it identified are the most relevant tothe given task. This relevance is demonstratedby the 10.3 point improvement over the optimalmethod. Inhibiting neurons chosen by other meth-ods minimally reduces performance in the targettask and has little effect on other tasks, makingit hard to capture the tasks essence. Inhibitingselected neurons significantly improves the targettask performance more than other tasks, showingits effectiveness. In contrast, inhibiting neuronschosen by other methods results in minimal perfor-mance reduction for the target task and little effecton other tasks, making it hard to capture the tasksessence. Furthermore, we observed that the perfor-mance of certain tasks, such as QA, SA, TC, andEC, drops to almost zero. These tasks also exertsubstantial influence on other tasks, demonstratingthe interconnected nature of task-related neurons.",
  "Neurons Amplification Evaluation": "To further confirm the effectiveness of task-specificneurons, we tested whether amplifying these neu-rons improves task performance. Thus, we am-plified their activation signals and evaluated theireffectiveness in the target task and other tasks. Asshown in , our method surpasses the bestexisting method by 10.9 points in ACP, demon-strating its superiority in identifying task-specificneurons. When the task-specific neurons are am-plified, some tasks showed improvement, whileperformance on others slightly decreased. The im-provement effect of CEC increased to 5.1 at its best,indicating that task-specific neurons can help themodel better understand and process the task. Ad-ditionally, neurons in some tasks also improve theperformance of other tasks, indicating that thesetasks rely on the shared capabilities of the model.However, there are still limitations in improvingtask performance by amplifying task-specific neu-rons, as performance did not exceed zero. Addi-tionally, some neurons improved performance inmultiple tasks, suggesting that these tasks rely onthe models shared capabilities.",
  "Language Ability Evaluation": "To verify whether manipulating task-specific neu-rons disrupts the linguistic capabilities of LLMs,we evaluated the language abilities of the modelspost-manipulation. We conducted Perplexity (PPL)tests on the manipulated models using the AlpacaInstruction (Taori et al., 2023). presents acomparison of values under different operations fortask-specific neurons. The PPL of the basemodelvalue is 3.4827. With the inhibiting, the highestPPL is 3.6800 with QA, while the lowest is 3.3449with QU. For the amplifying, the highest PPL is3.6162 with EC, and the lowest is 3.5283 with TC.Our evaluation indicates that manipulating task-specific neurons does not negatively impact the QASAQULTCTCCECECTMTasks QASAQULTCTCCECECTMInhibiting Tasks 1.7617.8374.2425.3818.2272.7731.97100.86 23.604.8249.0015.2735.1436.3712.1562.15 100.2129.9626.4729.0466.5670.1082.6452.46 19.4423.7758.5321.4221.4773.1031.1996.73 19.1548.7679.7919.226.4583.8957.7072.41 80.5633.9269.7327.2520.2165.6760.6769.41 21.7911.6269.9711.9712.6057.155.7583.92 115.3835.8737.1354.0348.9963.50103.8358.19 QASAQULTCTCCECECTMTasks QASAQULTCTCCECECTMAmplifying Tasks 106.36104.2498.8687.6983.35101.6096.5595.60 103.1599.01109.4886.5982.75101.6087.03101.28 100.96105.5495.7391.3197.54101.7786.86102.14 97.21101.47102.8490.8889.64104.1792.80100.42 94.8097.0588.3788.7890.98102.4197.4994.59 103.33104.6998.4790.1083.69109.4996.6393.46 110.16100.04107.1585.7094.65105.30100.9197.71 101.01102.18106.0189.5688.28104.8288.5899.14 : The figure illustrates the effects of inhibiting and amplifying task-specific neurons across various tasks.Task-specific neuron operation is depicted along the rows, while task performance is listed along the columns. Thevalues in the figure indicate the percentage of BASE performance achieved for each task.",
  "Task Cross-Performance Analyzation": "To understand the correlations between tasks, wevisualized their cross-performance. showsthe impact of inhibiting (left) and amplifying (right)task-specific neurons on performance. Inhibitingtask-specific neurons significantly reduces the per-formance of the target task more than other tasks.Similar tasks, like SA and EC, which both clas-sify emotional content, have the greatest impact oneach other. Amplifying task-specific neurons no-ticeably improves the target task performance. QAand ECE benefit the most from other tasks, indicat-ing they rely on the models reasoning abilities toimprove effectiveness. From , it is evidentthat TC and LTC are significantly influenced byother tasks, whether through inhibition or amplifi-cation. This suggests that they rely on a singular pathway to complete their tasks and depend on thespecific capabilities of LLMs, classifying them asspecialized tasks. Similarly, TM exhibits insensi-tivity to inhibition and amplification and remainsunaffected by neurons from other tasks. It employsa unique method for task processing, also classi-fying it as a specialized task. In summary, thesethree specialized tasks are domain-specific, andLLMs utilize independent abilities when handlingsuch tasks. Conversely, tasks can be improved orweakened by neurons manipulating other tasks, in-dicating a crossover in their abilities, and these areclassified as general tasks.",
  "Neurons Visualization": "To visually demonstrate the activation locations oftask-specific neurons, we visualized the neurons inthe model. As shown in , the distributionof neurons for eight different tasks is illustrated.Overall, task-specific neurons are predominantlydistributed between layers 5 and 11 of the model.This suggests that the proposed method achievesperformance interference without altering the top-most neurons. Instead, the neurons collected byLAPE are primarily concentrated in the top layersas shown in Appendix A. The visualization showsthat QA, LTC, and TC tasks rely less on lower-layerneurons, indicating a reduced need for lower-levelsemantic understanding. SA and EC tasks are simi-lar tasks, as evidenced by their approximate distri-butions. Additionally, QU and CEC tasks stronglyprefer the 10th layer, highlighting its importancein determining sentence relatedness. Task-specificneurons (like TC, LTC, and MT) are more concen-trated and show less dispersion, whereas neurons QA SA QU LTC TC CEC EC TM : This figure visualizes neurons associated with different tasks. The Y-axis represents the layers of themodel, while the X-axis indicates the positions of the neurons. Red denotes active task-specific neurons, whereasBlack indicates non-task-specific neurons. We combined every 100 adjacent neurons in each cell for easier display",
  "Case Study": "shows an example analysis of the operationof task-specific neurons. In the cases of SA andTC, the base model predicts an error, but the ampli-fying neuron corrects the answer. The base modelinitially predicts a non-label answer in the EC case,which is corrected after amplification. Similarly,in the QA and TM cases, the base model providestwo non-standard answers, which are subsequentlycorrected to standard answers after amplification.During inhibition, LLMs experienced hallucina-tions and provided irrelevant answers, though theresponses remained fluent.",
  "Related Work": "LLMs have garnered widespread attention due totheir superior performance (Zhao et al., 2023b;Brown et al., 2020). After instruction fine-tuningand alignment, LLMs demonstrate strong perfor-mance across multiple tasks (Ouyang et al., 2022;Longpre et al., 2023).Especially, LLMs haveachieved further breakthroughs in task performancewith In-Context Learning (Dong et al., 2023). How-ever, LLMs often produce hallucinations, whichimpacts their applicability in real-world (Huanget al., 2023). Therefore, studies have to explorethe mechanisms of LLMs to understand their op-erating principles (Singh et al., 2024; Voita et al.,2023). Among these studies, the internal attribu-tion of LLMs has received extensive attention, withvarious components of LLMs being investigated,including embeddings (Morris et al., 2023), atten-tion (Grosse et al., 2023), transformer layer (Xu",
  ": This table illustrates a case of neuronal inhibi-tion and signal amplification": "et al., 2023), FFN (Bari and Robbins, 2013). TheFFN regulates the information output of the entirelayer. Consequently, some studies have concen-trated on investigating neurons from FFN. Theseneurons are divided into knowledge neurons (Daiet al., 2022; Chen et al., 2024) and language neu-rons (Zhao et al., 2023a; Tang et al., 2024), whichcontrol the application of knowledge and the ex-pression of task language. There have also beenstudies investigating the existence of skill neuronsin large models (Wang et al., 2022a). However,neurons from the task perspective are absent, mean-ing that there has been little focus on identifyingand understanding neurons specifically activatedby different tasks. By identifying and analyzingthese neurons, we aim to understand how LLMsprocess different tasks, which help us fine-tune andoptimize these models for specific uses.",
  "Limitations": "This paper only discusses a limited set of represen-tative tasks and does not explore neurons across alarge-scale set of tasks. For instance, the Natural In-structions dataset contains 1600 tasks (Wang et al.,2022c). We believe that exploring such a datasetwould reveal a more diverse range of task-specificneurons. Additionally, due to equipment limita-tions, our method could not be applied to largermodels for neuron exploration. In future work, wewill use larger models and more data to uncover aricher set of task-specific neurons.",
  "Ethics Statement": "This research, titled \"Does Large Language ModelContain Task-Specific Neurons?\" adheres to a strictethical framework as it does not involve any ethicalissues. The data constructed for this research isderived solely from open-source data, and the largelanguage model employed in this study followstheir declared licenses. I have fully informed theparticipants of all instructions, to ensure they arefully aware and consenting to participate in thiswork.",
  "Acknowledgement": "This research was supported by the Strate-gic Priority Research Program of the ChineseAcademy of Sciences (No. XDA27020203), theNational Natural Science Foundation of China(Grant Nos. U21B2027, U23A20388, 62266027,62376270), the Yunnan Provincial Major Scienceand Technology Special Plan Projects (Grant Nos.202303AP140008, 202302AD080003), the Gen-eral Projects of Basic Research in Yunnan Province(Grant No. 202401BC070021), and the Youth In-novation Promotion Association of the ChineseAcademy of Sciences.",
  "Andrea Bari and Trevor W Robbins. 2013. Inhibitionand impulsivity: behavioral and neural basis of re-sponse control. Progress in neurobiology, 108:4479": "Farah Benamara, Carmine Cesarano, Antonio Picariello,Diego Reforgiato Recupero, and Venkatramana SSubrahmanian. 2007. Sentiment analysis: Adjectivesand adverbs are better than adjectives alone. ICWSM,7:203206. Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen,Eric Sigler, Mateusz Litwin, Scott Gray, BenjaminChess, Jack Clark, Christopher Berner, Sam Mc-Candlish, Alec Radford, Ilya Sutskever, and DarioAmodei. 2020. Language models are few-shot learn-ers. Preprint, arXiv:2005.14165. Oana-Maria Camburu, Tim Rocktschel, ThomasLukasiewicz, and Phil Blunsom. 2018. e-snli: Natu-ral language inference with natural language expla-nations. Advances in Neural Information ProcessingSystems, 31. Tsai-Wen Chen, Trevor J Wardill, Yi Sun, Stefan R Pul-ver, Sabine L Renninger, Amy Baohan, Eric R Schre-iter, Rex A Kerr, Michael B Orger, Vivek Jayaraman,et al. 2013. Ultrasensitive fluorescent proteins forimaging neuronal activity. Nature, 499(7458):295300. Yuheng Chen, Pengfei Cao, Yubo Chen, Kang Liu, andJun Zhao. 2024. Journey to the center of the knowl-edge neurons: Discoveries of language-independentknowledge neurons and degenerate knowledge neu-rons. In Proceedings of the AAAI Conference on Ar-tificial Intelligence, volume 38, pages 1781717825.",
  "Sigmoid-weighted linear units for neural networkfunction approximation in reinforcement learning.Preprint, arXiv:1702.03118": "Mor Geva, Roei Schuster, Jonathan Berant, and OmerLevy. 2021. Transformer feed-forward layers are key-value memories. In Proceedings of the 2021 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 54845495, Online and Punta Cana,Dominican Republic. Association for ComputationalLinguistics. Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage,Alex Tamkin, Amirhossein Tajdini, Benoit Steiner,Dustin Li, Esin Durmus, Ethan Perez, Evan Hubinger,Kamile Lukoiute, Karina Nguyen, Nicholas Joseph,Sam McCandlish, Jared Kaplan, and Samuel R.Bowman. 2023.Studying large language modelgeneralization with influence functions. Preprint,arXiv:2308.03296. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,Zhangyin Feng, Haotian Wang, Qianglong Chen,Weihua Peng, Xiaocheng Feng, Bing Qin, and TingLiu. 2023. A survey on hallucination in large lan-guage models: Principles, taxonomy, challenges, andopen questions. Preprint, arXiv:2311.05232. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, Llio Renard Lavaud,Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,Thibaut Lavril, Thomas Wang, Timothe Lacroix,and William El Sayed. 2023a. Mistral 7b. Preprint,arXiv:2310.06825. Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, YuqingYang, and Lili Qiu. 2023b. LLMLingua: Compress-ing prompts for accelerated inference of large lan-guage models. In Proceedings of the 2023 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 1335813376, Singapore. Associationfor Computational Linguistics. Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu,Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang,Jian Jiao, Nan Duan, and Weizhu Chen. 2024. Rho-1: Not all tokens are what you need.Preprint,arXiv:2404.07965. Shayne Longpre, Le Hou, Tu Vu, Albert Webson,Hyung Won Chung, Yi Tay, Denny Zhou, Quoc VLe, Barret Zoph, Jason Wei, et al. 2023. The flancollection: Designing data and methods for effectiveinstruction tuning. In International Conference onMachine Learning, pages 2263122648. PMLR.",
  "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, YannDubois, Xuechen Li, Carlos Guestrin, Percy Liang,and Tatsunori B. Hashimoto. 2023. Stanford alpaca:An instruction-following llama model": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processingsystems, 30.",
  "Elena Voita, Javier Ferrando, and Christoforos Nalmpan-tis. 2023. Neurons in large language models: Dead,n-gram, positional. Preprint, arXiv:2309.04827": "Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou,Zhiyuan Liu, and Juanzi Li. 2022a. Finding skillneurons in pre-trained transformer-based languagemodels. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Processing,pages 1113211152, Abu Dhabi, United Arab Emi-rates. Association for Computational Linguistics. Yizhong Wang, Swaroop Mishra, Pegah Alipoor-molabashi,Yeganeh Kordi,Amirreza Mirzaei,Anjana Arunkumar, Arjun Ashok, Arut SelvanDhanasekaran, Atharva Naik, David Stap, EshaanPathak, Giannis Karamanolakis, Haizhi Gary Lai, Is-han Purohit, Ishani Mondal, Jacob Anderson, KirbyKuznia, Krima Doshi, Maitreya Patel, Kuntal Ku-mar Pal, Mehrad Moradshahi, Mihir Parmar, Mi-rali Purohit, Neeraj Varshney, Phani Rohitha Kaza,Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia,Shailaja Keyur Sampat, Savan Doshi, SiddharthaMishra, Sujan Reddy, Sumanta Patro, Tanay Dixit,Xudong Shen, Chitta Baral, Yejin Choi, Noah A.Smith, Hannaneh Hajishirzi, and Daniel Khashabi.2022b. Super-naturalinstructions: Generalization viadeclarative instructions on 1600+ nlp tasks. Preprint,arXiv:2204.07705. Yizhong Wang, Swaroop Mishra, Pegah Alipoormo-labashi, Yeganeh Kordi, Amirreza Mirzaei, AtharvaNaik, Arjun Ashok, Arut Selvan Dhanasekaran,Anjana Arunkumar, David Stap, Eshaan Pathak,Giannis Karamanolakis, Haizhi Lai, Ishan Puro-hit, Ishani Mondal, Jacob Anderson, Kirby Kuznia,Krima Doshi, Kuntal Kumar Pal, Maitreya Patel,Mehrad Moradshahi, Mihir Parmar, Mirali Purohit,Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma,Ravsehaj Singh Puri, Rushang Karia, Savan Doshi,Shailaja Keyur Sampat, Siddhartha Mishra, SujanReddy A, Sumanta Patro, Tanay Dixit, and XudongShen. 2022c. Super-NaturalInstructions: General-ization via declarative instructions on 1600+ NLPtasks. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Processing,pages 50855109, Abu Dhabi, United Arab Emirates.Association for Computational Linguistics.",
  "Shaoyang Xu, Junzhuo Li, and Deyi Xiong. 2023. Lan-guage representation projection: Can we transfer fac-tual knowledge across languages in multilingual lan-guage models? Preprint, arXiv:2311.03788": "Lifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng Gao,Fangyuan Zou, Xingyi Cheng, Heng Ji, ZhiyuanLiu, and Maosong Sun. 2024.Revisiting out-of-distribution robustness in nlp: Benchmarks, analysis,and llms evaluations. Advances in Neural Informa-tion Processing Systems, 36. Wang Zeng, Sheng Jin, Wentao Liu, Chen Qian, PingLuo, Wanli Ouyang, and Xiaogang Wang. 2022. Notall tokens are equal: Human-centric visual anal-ysis via token clustering transformer.Preprint,arXiv:2204.08680.",
  "ANeurons Fusion Analyzation": "We also conducted neuron fusion experiments toexplore the interaction between neurons. We se-lected three groups of neurons with the greatestimpact on each task, then fused them and testedtheir inhibition and amplification performance. Asshown in , when neurons are merged dur-ing the inhibition phase, the reduction rate of taskperformance is greater than the average reductionobserved for individual tasks. This suggests thatcombining neurons amplifies their inhibitory ef-fects on task performance. In the enhancementphase, the merged neurons showed improved per-formance on only two tasks: SA and QU. Thisindicates that while neuron fusion can enhance per-formance in specific contexts, its benefits are notuniversally applicable across all tasks. In general,task-specific neurons are more effective at limitaiontasks, demonstrating a greater reduction in perfor-mance when inhibited. However, they still facechallenges in enhancement tasks, as improvementsare limited and not consistently observed acrossdifferent tasks.",
  "BKnowledge and Language NeuronsVisualization": "We also present the visualization results of applyinglanguage neuron and knowledge neuron detectionmethods to tasks. As shown in Figures 6 and 7,although both methods capture task informationto some extent, they contain excessive noise andfail to focus on task-specific neurons. The LAPEmethod tends to identify neurons in the last layerto control language expression. In contrast, the GVmethod detects neurons with a significant amountof noise, making it less effective in intuitively inter-preting the task compared to the CGVTS method.In a more detailed analysis of the experimentalresults, we observed that the LAPE method has cer-tain advantages in controlling language expression,but its selection often overly focuses on the last layer of the model. This might lead to the neglectof task-relevant neurons in the preceding layers.While this concentrated selection can simplify in-terpretation, it also risks making the interpretationless comprehensive and in-depth.On the other hand, although the GV method alsoattempts to capture task-related neurons, the pres-ence of a substantial amount of noise among thedetected neurons hinders its clarity and intuitive-ness in task interpretation. The presence of noisemay be due to the GV methods insufficiently strictselection criteria or its failure to adequately distin-guish between task-related and unrelated signals.In contrast, the CGVTS method demonstrates ahigher task interpretation capability. It effectivelyfilters out noise and more accurately locates andinterprets task-related neurons. This indicates thatthe CGVTS method is more effective and reliablein neuron selection and task information capture. QA SA QU LTC TC CEC EC TM"
}