{
  "Abstract": "Deep learning models for NLP tasks are proneto variants of privacy attacks. To prevent pri-vacy leakage, researchers have investigatedword-level perturbations, relying on the for-mal guarantees of differential privacy (DP) inthe embedding space. However, many exist-ing approaches either achieve unsatisfactoryperformance in the high privacy regime whenusing the Laplacian or Gaussian mechanism,or resort to weaker relaxations of DP that areinferior to the canonical DP in terms of privacystrength. This raises the question of whether anew method for private word embedding canbe designed to overcome these limitations. In this paper, we propose a novel private em-bedding method called the high dimensionaltruncated Laplacian mechanism. Specifically,we introduce a non-trivial extension of the trun-cated Laplacian mechanism, which was pre-viously only investigated in one-dimensionalspace cases. Theoretically, we show that ourmethod has a lower variance compared to theprevious private word embedding methods. Tofurther validate its effectiveness, we conductcomprehensive experiments on private embed-ding and downstream tasks using three datasets.Remarkably, even in the high privacy regime,our approach only incurs a slight decrease inutility compared to the non-private scenario.",
  "Introduction": "The recent developments of deep learning have ledto significant success in various tasks in NaturalLanguage Processing (NLP), from next word pre-diction in mobile keyboards (Ramaswamy et al.,2019), to critical tasks like predicting patient healthconditions from clinical records (Yao et al., 2019).However, such applications may always involveuser-generated textual data as the training dataset,",
  "*Equal contribution. Part of the work was done as a re-search intern at PRADA Lab.Corresponding author": "which contains sensitive information. To addressprivacy concerns, text anonymization (Anandanet al., 2012; Piln et al., 2022) has been commonlyused, which involves identifying sensitive attributesand replacing them with alternative values. Nev-ertheless, such heuristic approaches become inef-fective as deep neural networks often tend to mem-orize training data, making them susceptible toinformation leakage about the training data (Shokriet al., 2017; Carlini et al., 2021, 2019). One waythat takes into account the limitations of existing ap-proaches is designing Differentially Private (DP) al-gorithms. Differential privacy (Dwork et al., 2006a)is resilient to arbitrary side information that mightbe available to attackers and has become a de factomethod for private data analysis (Xiang et al., 2024;Wang et al., 2020; Xiang et al., 2023; Xue et al.,2021; Huai et al., 2019; Wang et al., 2023; Wangand Xu, 2020; Huai et al., 2020; Hu et al., 2022).Recently, there has been significant research fo-cusing on differentially private (DP) versions ofword embedding from various perspectives (Yueet al., 2021; Feyisetan et al., 2019; Krishna et al.,2021; Feyisetan et al., 2020a; Xu et al., 2021a,c;Carvalho et al., 2021b,a; Habernal, 2021, 2022).However, there are still some shortcomings in theseapproaches. On the one hand, several works con-sider adding Laplacian or Gaussian noise to theembedding space to ensure DP (Habernal, 2021;Krishna et al., 2021; Habernal, 2022). However,these mechanisms suffer from high noise levels, re-sulting in low utility, especially in the high privacyregime when the privacy parameter () is small.Moreover, these mechanisms can even alter thesemantics of sentences (see ). On the otherhand, there is a growing body of work that focuseson a relaxation of the canonical definition of DP,known as metric DP, which can achieve better per-formance. However, as a relaxed notion of DP,Metric DP cannot provide the same level of strongprivacy guarantees as the canonical DP (Mattern",
  ": An example of (private) text re-write for different mechanisms with = 0.1": "et al., 2022a). This raises the question of whetherwe can develop improved private word embeddingmechanisms within the framework of canonical DPthat can have comparable performance with exist-ing metric DP-based methods.In this paper, we provide an affirmative answer tothe previous question by proposing a novel privatemechanism for word embedding. Our approach isinspired by the superior performance of the trun-cated Laplacian mechanism in one-dimensionalspace (Geng et al., 2020). However, it remains un-clear whether this superiority can extend to highdimensional cases, as directly extending the one-dimensional truncated Laplacian mechanism ischallenging. To bridge this gap, we develop ahigh dimensional truncated Laplacian mechanism(TrLaplace), which is a non-trivial extension ofthe one-dimensional case. Theoretically, we showthat compared with Laplacian and Gaussian mech-anisms for private word embedding, TrLaplace-based private embedding has a lower variance.Moreover, we also conduct intensive experimentson both private embedding and downstream tasksto show our approach significantly outperforms theprevious DP-based methods in the high privacyregime, and it will not drop much accuracy and util-ity compared with the non-private case. Moreover,compared to the existing metric DP-based method,our mechanism has even better performance forprivacy tests while also keeping comparable perfor-mance for downstream tasks.",
  "Related Work": "Recent years have seen substantial advancementsin language models within differential privacy (DP)frameworks. Due to the space limit, here we onlymention the existing literature on private word em-bedding. We refer the readers to the survey (Huet al., 2024a) for more details.Current research on private word embeddingscan be broadly categorized into two approaches:original DP-based methods and metric DP-based methods. The seminal work in the original DP cate-gory by Lyu et al. (2020b) introduces a frameworkutilizing the Unary Encoding mechanism. Thisapproach was subsequently refined by Plant et al.(2021). Further improvements were made by Lyuet al. (2020a), who proposed a dropout techniquefor perturbed embeddings to enhance downstreamtask fairness. However, Qu et al. (2021) identifya critical privacy issue in Lyu et al. (2020a), not-ing that it requires access to users raw data forfine-tuning during the training phase. Other no-table contributions include works by Krishna et al.(2021), Habernal (2021), and Alnasser et al. (2021),who explore privatizing word embeddings. Krishnaet al. (2021) and Alnasser et al. (2021) proposeADePT, an auto-encoder-based DP algorithm. Un-fortunately, Habernal (2021) points out that ADePTis not differentially private by thorough theoreticalproof. Igamberdiev et al. (2022) address repro-ducibility by providing source code for DP Auto-Encoder methods. In this paper, we aim to improvethe performance of the mechanisms in Igamberdievet al. (2022).",
  "In the realm of metric DP, Feyisetan et al": "(2020b) first study this problem and provide a gen-eral perturbation-and-projection framework. Xuet al. (2020a) reconsider this problem setting, re-placing the Euclidean distance with the Maha-lanobis distance to improve the utility.Subse-quently, Xu et al. (2021d) introduce the Vickreymechanism to further refine the utility in the pro-jection step. To address the limitations of the mul-tivariate Laplace mechanism, Xu et al. (2021b) andCarvalho et al. (2021c) propose a Truncated Gum-bel Noise method. Feyisetan and Kasiviswanathan(2021) tackle high-dimensionality issues using ran-dom projection.Additionally, Feyisetan et al. (2019) define hyperbolic embeddings and utilizethe Metropolis-Hastings algorithm for samplingfrom hyperbolic distributions. More recently, Tanget al. (2020) explore differential privacy with vary-ing privacy levels for different words. Arnold et al. (2023a) introduce sense embeddings with a sensedisambiguation step prior to noise injection, andArnold et al. (2023b) address common semanticcontext issues in prior private embedding mech-anisms. It is crucial to clarify that the objectiveof this work is the privatization of embedded out-puts rather than the embedded methods themselves.The pre-trained initial embedding methods and thecorresponding embeddings of all words in the vo-cabulary are treated as public knowledge. Thisdistinction is significant because it allows us toperform projections without incurring additionalprivacy costs. By leveraging publicly available pre-trained embeddings, our method demonstrates howeffective privacy-preserving techniques can be im-plemented while still utilizing existing resources.",
  "Preliminaries": "Differential Privacy is a data post-processing tech-nique designed to ensure data privacy by addingconfusion to potential attackers. Specifically, sup-pose there is one dataset noted as D, and we changeor delete one data record in this dataset which wecall D. If the output distributions of D and D are close enough, then we cannot distinguish thesetwo distributions, i.e., we cannot infer whether thedeleted or replaced data sample is really in thisdataset. The formal details are given by (Dworket al., 2006b). Note that in the definition of DP,adjacency is a key notion. One of the commonlyused adjacency definitions is that two datasets Sand S are adjacent (denoted as S S) if S canbe obtained by modifying one record in S. Definition 1 Given a domain of dataset X.Arandomized algorithm A : X R is (, )-differentially private (DP) if for all adjacentdatasets S, S with each sample is in X and forall T R, the following holds",
  "When = 0, we call the algorithm A is -DP": "In this work, we adopt a similar setting to previ-ous research on private word embedding (Feyisetanet al., 2020a; Xu et al., 2021a; Krishna et al., 2021).We consider a scenario where a user inputs a wordw from a discrete fixed vocabulary W. Our goal isto preserve the users privacy with respect to her/hisword. To achieve this goal, we aim to design analgorithm that accepts w as input and whose distri-bution of output is close to the case where w W is the input, with w = w is any other word. Fromthe attackers perspective, based on the output, hecannot distinguish whether the users input word isw or w as their output distributions are almost thesame. Formally, we have the following definition. Definition 2 Given a discrete vocabulary W, arandomized algorithm A : W R is word-level(, )-differentially private (DP) if for all pair ofwords w, w W and for all T R we haveP(A(w) T) eP(A(w) T) + . When = 0, we call the algorithm A is -DP.",
  "wi = arg minwW (w) CLIPEmb(wi) 2,": "(2)where is the randomized noise we add in thesecond step. See Algorithm 1 for details.It is notable that the goal of clipping is to makethe 2-norm of embedding vector be bounded sothat we can adding noise to ensure DP, such asthe Laplacian mechanism or Gaussian mechanism(Dwork and Roth, 2014). Theorem 1 (Laplacian Mechanism) SupposeCLIPEmb(w)Rd denote the clipped em-bedding vector with threshold C.Then themechanism Alap(w) = CLIPEmb(w) + 1 is-DP, where 1 = (1,1, , 1,d) and i,j isdrawn from a Laplacian Distribution Lap( 1(f)",
  "Be|x|,for x [A, A]0,otherwise.(3)": "In our mechanism, we add high dimensional trun-cated Laplacian noise to the clipped embeddingvector. Here each coordinate of the noise is i.i.d.sampled from a truncated Laplacian distributionwith some specific , A and B. Remark 1 It is notable that although using trun-cated Laplacian noise to ensure DP has been stud-ied quite well (Geng et al., 2020; Sommer et al.,2021), all of them only considered the case wherethe dimension d = 1 and their methods cannotextend to the case where d > 1. For example,(Geng et al., 2020) only shows that adding noisewith density function (3) with A = 1",
  "In the following, we will show our mechanismhas lower variance than the Laplacian and Gaussianmechanism, which indicates that our method issuperior theoretically": ": Privacy Test. Performance under fastText Embedding initialization for the non-private case ( = )and three mechanisms (Gaussian, Laplacian and TrLaplacian) on Yelp dataset. The privacy budget ranges from0.05 to 20. means a higher value under this metric indicates better results, and means the opposite. The bestperformance is bolded. The same symbols are used in the following tables by default.",
  "Experimental Setup": "Datasets.For the DP text re-write task, we usethe Yelp * and Yahoo (Yang et al., 2019) datasets.The Yelp Open dataset is a subset of Yelp business,review, and user data with a training size of 8,539and a testing size of 2,174. The Yahoo dataset con-tains 14,180 news articles and 34,022 click events.All data are collated to obtain a training, validation,and testing set segmented by sentences.For downstream tasks, we use the SST-2 dataset(Socher et al., 2013) for the sentiment analysis task,from which we use 68,221 heavily polarized re-views from the Internet Movie Database. We dividethe SST-2 dataset into an 80:20 ratio for trainingand testing. The training set consists of 54,576 re-views and the testing set consists of 13,645 reviews.We use the AG News dataset (Zhang et al., 2015)which includes news articles on the four main top-ics in the AG News corpus for the topic classifica-tion task. Following Meisenbacher et al. (2024),",
  "*": "we randomly select a sample of 6,000 articles fromeach topic, totaling 16,000 for training, 4,000 forvalidation, and 4,000 for testing. The statistics ofthe datasets are presented in Tab. 4 in Appendix A. Baseline.For DP text re-write, although Krishnaet al. (2021) use the Laplacian mechanism to thesentence level DP instead of word level as in Defini-tion 2. However, as Habernal (2021) mentions, theapproach in Krishna et al. (2021) is not DP. Thus,here we will not compare with their method, and wewill use the Laplacian and Gaussian mechanismsfor the clipped embedding as baseline methods. Forprivate fine-tuning, as we mentioned previously, allthe previous methods only focus on metric DP in-stead of the original DP in Definition 2. Thus, ourmethod is incomparable with theirs, and we willstill use Laplacian and Gaussian mechanisms asbaselines.For utility test, we compare our method withGaussian and Laplacian mechanism for the origi-nal DP notation, as well as Calibrated Multivari-ate Perturbations (CMP) (Feyisetan et al., 2020b),Mahalanobis Mechanism (Xu et al., 2020b) andPrivate Text Embedding (PTE) (Feyisetan and Ka-siviswanathan, 2021), which are benchmark metricDP-based methods and have better utility than theoriginal DP-based ones. Evaluation Metrics.We use the loss of cross-entropy to measure the performance of languagemodels. Specifically, cross-entropy is mainly usedto determine how similar the actual output is tothe expected output.Smaller model loss indi-cates less noise added to perturb the text. Addi-",
  "Nw 0.7060.7080.7250.7080.7390.6910.7210.7040.6990.7240.7000.7120.740BERT-S0.9730.9690.9750.9750.9750.9640.9690.9690.9680.9750.9710.9760.976": "tionally, we will use Rouge1 and BLEU scores.Rouge1 (Lin, 2004) calculates recall using standardresults and the number of 1-grams co-occurringin the auto-generated text. Similarly, BLEU (Pa-pineni et al., 2002) measures the similarity be-tween standard results and automatically generatedtext. Rouge1 measures word-level accuracy, whileBLEU measures sentence fluency. Moreover, weuse BERTScore (Zhang* et al., 2020) to measurethe semantic similarity of the perturbed sentencewith the original one. To measure the privacy-preserving ability, we use the percentage of Nw(Feyisetan et al., 2020a), which is the number ofwords that are not replaced. Thus, under the sameprivacy budget, larger Nw will be better (we wantto change fewer words for accuracy). Implementation Details.As an embedding canbe considered as an initialization of the model,here we will consider three different initialization:Random embedding (Wieting and Kiela, 2019),GloVe (Pennington et al., 2014) and fastText (Bo-janowski et al., 2017). We conduct experiments onthese embeddings and the subsequent fine-tuningin the DP model via our mechanism. Each pre- trained word embedding is a 300-dimensional vec-tor, and the size of considered vocabulary is 104.For privacy budget, we set =14d , and we con-sider both the high privacy regime where {0.05, 0.1, 0.2, 0.5} and the low privacy regime {1, 5, 10, 20}. For large we will use ourprevious dummy dimension trick (d = 500 for = 10 and d = 1700 for = 20).",
  "Privacy Experiment on Embedding": "We first show the results on private embedding.Specifically, we use GloVe or fastText for the ini-tialization, and then use three different private em-bedding mechanisms with different privacy bud-gets. Noted that large > 10 is meaningless forprivacy, we concentrate more on a small privacybudget in the main context. and 6 show the text after projecting theclipped and perturbed embedding back to the worddomain in step 4 of Algorithm 1 for different mech-anisms when = 0.1. We can see our method(TrLaplace) outperforms the other two methodsfrom both privacy and semantic perspectives, whilethe Gaussian mechanism fails to obfuscate the time,and the Laplacian mechanism totally replaces the",
  ": Privacy-Utility Test. Curves of Loss, Rouge1 and BERTScore with different privacy budget for Yelp(Upper) and Yahoo (Lower) datasets": "time by another word, which destroys the structureof the sentence.Tab. 2 and Tab. 3 are the results on differentmetrics regarding private embedding with Gloveinitialization and Tab. 1 is with fastText initializa-tion. We also present the detailed trends w.r.t forthree mechanisms in . When < 1, fromTab. 2 we can see that for both Yahoo and Yelp, theloss of Gaussian and Laplacian mechanisms willbe catastrophically large while our mechanism hasa much smaller loss. From Tab. 3 we can see wehave almost the same phenomenon when in the lowprivacy regime. Moreover, for Rouge1, Trlapla-cian also surpasses the other two mechanisms onboth datasets, indicating that our mechanism con-sistently demonstrates superiority in lexical/syntac-tic aspects. For BLEU, the gap between all threemechanisms to the non-private case becomes smallfor both two datasets. But our method still has aslight advantage compared with the other two.For Nw value, we can see in and ,our mechanism outperforms the other two mech-anisms by changing less percentage of words toachieve the same privacy level, which indicates ourmethod can exactly find sensitive words withouthurting other words, thus keeps semantic proper- ties. For BERTScore, our mechanism is almost thesame as the non-private case, while there is a largergap for others. It is notable that, in almost all exper-iments our mechanism is the best, and the Gaussianmechanism is better than the Laplacian mechanism,which matches our theorem. However, it becomesless obvious when is large. The main reason isthat when is enough large the noise will be suffi-ciently small and becomes nearly negligible, whichcan also be supported by the proof of Theorem 4.For evaluation metrics, our mechanism may evenbe better than the non-private case, this may be dueto small noise that could improve generalization,which is similar to adversarial training. Moreover.in real-world scenarios, the privacy budget must bevery small as we just want to keep the word level ofprivacy. This is because, in more realistic scenarios,when we want to protect the whole sentence, thetotal privacy budget required to privatize an entiresequence may grow linearly with its length (dueto the composition theorem). Thus, the budget foreach word should be extremely small. This has alsobeen mentioned in some previous work (Matternet al., 2022b). 0.010.1110",
  "Utility of Private Fine-tuning": "We present the classification accuracy results forprivate fine-tuning across various embeddings andprivacy levels in . While it is acknowledgedthat utility will be affected by the size of the bud-get, with a smaller budget potentially leading tolower utility, our experimental results show thatour proposed method maintains relatively stableutility across different budget choices. Specifically, even in the high privacy regime ( = 0.01), ourapproach only incurs a slight decrease in utilitycompared to the non-private scenario. Similar ca-pabilities of other methods can be observed in theexperimental results of Meisenbacher et al. (2024). We can also observe the effect of different pre-trained embeddings. It is evident that when us-ing GloVe and fastText pretrained embeddings, allmethods achieve accuracy close to the baseline.However, when using random embeddings, thebaseline accuracy is very low (0.5607 on SST-2,0.2462 on AG News), but all methods significantlyimprove the accuracy of downstream sentimentanalysis and topic classification tasks after training,with accuracy reaching up to about 0.87 when ep-silon=10. This indicates that all methods are effec-tive after training. Specifically, our method, Trun-cated Laplacian, maintains an accuracy of around0.865 when trained on random embeddings, demon-strating that it maintains privacy while preservingthe quality of embeddings, thereby offering excel-lent performance.",
  "Conclusions": "We introduce a novel method called the high dimen-sional truncated Laplacian mechanism for privateembedding, which extends the one-dimensionalcase to the high-dimensional case. Theoretical anal-ysis demonstrates that our method exhibits lowervariance compared to existing private word embed-ding techniques. Experiments show that even inthe high privacy regime, our approach incurs only aminimal loss in utility compared to the non-privatecase, which maintains privacy while preserving thequality of embeddings for promising performance.",
  "Limitations": "First, the word level DP has the disadvantages oflength constraints and linear growth of privacy bud-get (Mattern et al., 2022a). However, such limita-tions are rooted in the definition of DP instead ofour mechanism. Secondly, to ensure DP guaran-tees, in this paper, our mechanism involves clippingembedding vectors and adding calibrated noises,which inevitably introduce errors to the outputs ofthe task at hand. And these errors may affect dif-ferent groups of individuals differently and maycause unfairness issues. However, we still need tomention that such unfairness issues are mainly dueto the definition of DP rather than our method, asDP machine learning algorithms will always have adisparate impact on model accuracy (Bagdasaryanet al., 2019). Despite some limitations, word-levelDP still offers unique advantages and potential ap-plications (Hu et al., 2024b), and brings value tothe DP-NLP community.",
  "This paper presents work whose goal is to advancethe field of NLP. There are many potential societalconsequences of our work, none which we feelmust be specifically highlighted here": "Di Wang and Lijie Hu are supported in part bythe funding BAS/1/1689-01-01, URF/1/4663-01-01, REI/1/5232-01-01, REI/1/5332-01-01, andURF/1/5508-01-01 from KAUST, and fundingfrom KAUST - Center of Excellence for Gener-ative AI, under award number 5940. Ivan Habernalis supported by the Research Center TrustworthyData Science and Security ( one of the Research Alliance centers withinthe UA-Ruhr (",
  "Piotr Bojanowski, Edouard Grave, Armand Joulin, andToms Mikolov. 2017. Enriching word vectors withsubword information. Trans. Assoc. Comput. Lin-guistics, 5:135146": "Nicholas Carlini, Chang Liu, lfar Erlingsson, JernejKos, and Dawn Song. 2019. The secret sharer: Eval-uating and testing unintended memorization in neu-ral networks. In 28th USENIX Security Symposium,USENIX Security 2019, Santa Clara, CA, USA, Au-gust 14-16, 2019, pages 267284. USENIX Associa-tion. Nicholas Carlini,Florian Tramr,Eric Wallace,Matthew Jagielski, Ariel Herbert-Voss, KatherineLee, Adam Roberts, Tom B. Brown, Dawn Song, l-far Erlingsson, Alina Oprea, and Colin Raffel. 2021.Extracting training data from large language models.In 30th USENIX Security Symposium, USENIX Se-curity 2021, August 11-13, 2021, pages 26332650.USENIX Association.",
  "Rudrajit Das, Abolfazl Hashemi, Sujay Sanghavi, andInderjit S. Dhillon. 2021. Dp-normfedavg: Normal-izing client updates for privacy-preserving federatedlearning. CoRR, abs/2106.07094": "Cynthia Dwork, Frank McSherry, Kobbi Nissim, andAdam D. Smith. 2006a. Calibrating noise to sensitiv-ity in private data analysis. In Theory of Cryptogra-phy, Third Theory of Cryptography Conference, TCC2006, New York, NY, USA, March 4-7, 2006, Pro-ceedings, volume 3876 of Lecture Notes in ComputerScience, pages 265284. Springer. Cynthia Dwork, Frank McSherry, Kobbi Nissim, andAdam D. Smith. 2006b. Calibrating noise to sensitiv-ity in private data analysis. In Theory of Cryptogra-phy, Third Theory of Cryptography Conference, TCC2006, New York, NY, USA, March 4-7, 2006, Pro-ceedings, volume 3876 of Lecture Notes in ComputerScience, pages 265284. Springer.",
  "Cynthia Dwork and Aaron Roth. 2014. The algorithmicfoundations of differential privacy. Found. TrendsTheor. Comput. Sci., 9:211407": "Oluwaseyi Feyisetan, Borja Balle, Thomas Drake, andTom Diethe. 2020a. Privacy- and utility-preservingtextual analysis via calibrated multivariate perturba-tions. In WSDM 20: The Thirteenth ACM Interna-tional Conference on Web Search and Data Mining,Houston, TX, USA, February 3-7, 2020, pages 178186. ACM. Oluwaseyi Feyisetan, Borja Balle, Thomas Drake, andTom Diethe. 2020b. Privacy- and utility-preservingtextual analysis via calibrated multivariate perturba-tions. In WSDM 20: The Thirteenth ACM Interna-tional Conference on Web Search and Data Mining,Houston, TX, USA, February 3-7, 2020, pages 178186. ACM. Oluwaseyi Feyisetan, Tom Diethe, and Thomas Drake.2019. Leveraging hierarchical representations forpreserving privacy and utility in text. In 2019 IEEEInternational Conference on Data Mining, ICDM2019, Beijing, China, November 8-11, 2019, pages210219. IEEE.",
  "Oluwaseyi Feyisetan and Shiva Kasiviswanathan. 2021.Private release of text embedding vectors. In Pro-ceedings of the First Workshop on Trustworthy Natu-ral Language Processing, pages 1527": "Quan Geng, Wei Ding, Ruiqi Guo, and Sanjiv Kumar.2020. Tight analysis of privacy and utility tradeoffin approximate differential privacy. In The 23rd In-ternational Conference on Artificial Intelligence andStatistics, AISTATS 2020, 26-28 August 2020, Online[Palermo, Sicily, Italy], volume 108 of Proceedingsof Machine Learning Research, pages 8999. PMLR. Ivan Habernal. 2021. When differential privacy meetsNLP: The devil is in the detail. In Proceedings of the2021 Conference on Empirical Methods in NaturalLanguage Processing, pages 15221528, Online andPunta Cana, Dominican Republic. Association forComputational Linguistics. Ivan Habernal. 2022. How reparametrization trick brokedifferentially-private text representation learning. InProceedings of the 60th Annual Meeting of the As-sociation for Computational Linguistics (Volume 2:Short Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 771777. Association for Computa-tional Linguistics. Lijie Hu, Ivan Habernal, Lei Shen, and Di Wang. 2024a.Differentially private natural language models: Re-cent advances and future directions. In EACL (Find-ings), pages 478499. Association for ComputationalLinguistics.",
  "Lijie Hu, Shuo Ni, Hanshen Xiao, and Di Wang. 2022": "High dimensional differentially private stochastic op-timization with heavy-tailed data.In PODS 22:International Conference on Management of Data,Philadelphia, PA, USA, June 12 - 17, 2022, pages227236. ACM. Mengdi Huai, Di Wang, Chenglin Miao, Jinhui Xu, andAidong Zhang. 2019. Privacy-aware synthesizing forcrowdsourced data. In Proceedings of the Twenty-Eighth International Joint Conference on ArtificialIntelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages 25422548. ijcai.org. Mengdi Huai, Di Wang, Chenglin Miao, Jinhui Xu, andAidong Zhang. 2020. Pairwise learning with differ-ential privacy guarantees. In The Thirty-Fourth AAAIConference on Artificial Intelligence, AAAI 2020, TheThirty-Second Innovative Applications of ArtificialIntelligence Conference, IAAI 2020, The Tenth AAAISymposium on Educational Advances in Artificial In-telligence, EAAI 2020, New York, NY, USA, February7-12, 2020, pages 694701. AAAI Press. Timour Igamberdiev, Thomas Arnold, and Ivan Haber-nal. 2022. DP-Rewrite: Towards Reproducibility andTransparency in Differentially Private Text Rewrit-ing. In The 29th International Conference on Com-putational Linguistics, pages 29272933, Gyeongju,Republic of Korea. International Committee on Com-putational Linguistics. Diederik P. Kingma and Jimmy Ba. 2015. Adam: Amethod for stochastic optimization.In 3rd Inter-national Conference on Learning Representations,ICLR 2015, San Diego, CA, USA, May 7-9, 2015,Conference Track Proceedings. Satyapriya Krishna, Rahul Gupta, and ChristopheDupuy. 2021. ADePT: Auto-encoder based differ-entially private text transformation. In Proceedingsof the 16th Conference of the European Chapter ofthe Association for Computational Linguistics: MainVolume, pages 24352439, Online. Association forComputational Linguistics.",
  "Chin-Yew Lin. 2004. ROUGE: A package for auto-matic evaluation of summaries. In Text Summariza-tion Branches Out, pages 7481, Barcelona, Spain.Association for Computational Linguistics": "Lingjuan Lyu, Xuanli He, and Yitong Li. 2020a. Differ-entially private representation for NLP: Formal guar-antee and an empirical study on privacy and fairness.In Findings of the Association for Computational Lin-guistics: EMNLP 2020, pages 23552365, Online.Association for Computational Linguistics. Lingjuan Lyu, Yitong Li, Xuanli He, and Tong Xiao.2020b. Towards differentially private text representa-tions. In Proceedings of the 43rd International ACMSIGIR conference on research and development inInformation Retrieval, SIGIR 2020, Virtual Event,China, July 25-30, 2020, pages 18131816. ACM.",
  "Justus Mattern, Benjamin Weggenmann, and FlorianKerschbaum. 2022a. The limits of word level differ-ential privacy. CoRR, abs/2205.02130": "Justus Mattern, Benjamin Weggenmann, and FlorianKerschbaum. 2022b. The limits of word level dif-ferential privacy. In Findings of the Association forComputational Linguistics: NAACL 2022, pages 867881, Seattle, United States. Association for Compu-tational Linguistics. Stephen Meisenbacher, Nihildev Nandakumar, Alexan-dra Klymenko, and Florian Matthes. 2024. A com-parative analysis of word-level metric differentialprivacy: Benchmarking the privacy-utility trade-off.arXiv preprint arXiv:2404.03324.",
  "Radford M. Neal. 2003. Slice sampling. The Annals ofStatistics, 31(3):705 767": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalu-ation of machine translation. In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics, pages 311318, Philadelphia,Pennsylvania, USA. Association for ComputationalLinguistics. Jeffrey Pennington, Richard Socher, and Christopher D.Manning. 2014. Glove: Global vectors for wordrepresentation. In Proceedings of the 2014 Confer-ence on Empirical Methods in Natural Language Pro-cessing, EMNLP 2014, October 25-29, 2014, Doha,Qatar, A meeting of SIGDAT, a Special Interest Groupof the ACL, pages 15321543. ACL. Ildik Piln, Pierre Lison, Lilja vrelid, Anthi Pa-padopoulou, David Snchez, and Montserrat Batet.2022. The text anonymization benchmark (TAB): Adedicated corpus and evaluation framework for textanonymization. CoRR, abs/2202.00443. Richard Plant, Dimitra Gkatzia, and Valerio Giuffrida.2021. CAPE: Context-aware private embeddingsfor private language learning. In Proceedings of the2021 Conference on Empirical Methods in NaturalLanguage Processing, pages 79707978, Online and",
  "Swaroop Ramaswamy, Rajiv Mathews, Kanishka Rao,and Franoise Beaufays. 2019. Federated learningfor emoji prediction in a mobile keyboard. CoRR,abs/1906.04329": "Reza Shokri, Marco Stronati, Congzheng Song, and Vi-taly Shmatikov. 2017. Membership inference attacksagainst machine learning models. In 2017 IEEE Sym-posium on Security and Privacy, SP 2017, San Jose,CA, USA, May 22-26, 2017, pages 318. IEEE Com-puter Society. Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Ng, andChristopher Potts. 2013. Recursive deep models forsemantic compositionality over a sentiment treebank.In Proceedings of the 2013 Conference on Empiri-cal Methods in Natural Language Processing, pages16311642, Seattle, Washington, USA. Associationfor Computational Linguistics.",
  "David M. Sommer, Lukas Abfalterer, Sheila Zingg, andEsfandiar Mohammadi. 2021. Learning numeric op-timal differentially private truncated additive mecha-nisms. CoRR, abs/2107.12957": "Jingye Tang, Tianqing Zhu, Ping Xiong, Yu Wang, andWei Ren. 2020. Privacy and utility trade-off for tex-tual analysis via calibrated multivariate perturbations.In Network and System Security - 14th InternationalConference, NSS 2020, Melbourne, VIC, Australia,November 25-27, 2020, Proceedings, volume 12570of Lecture Notes in Computer Science, pages 342353. Springer. Di Wang, Jiahao Ding, Lijie Hu, Zejun Xie, Miao Pan,and Jinhui Xu. 2023. Finite sample guarantees ofdifferentially private expectation maximization algo-rithm. In ECAI 2023 - 26th European Conferenceon Artificial Intelligence, September 30 - October 4,2023, Krakw, Poland - Including 12th Conferenceon Prestigious Applications of Intelligent Systems(PAIS 2023), volume 372 of Frontiers in Artificial In-telligence and Applications, pages 24352442. IOSPress.",
  "Zihang Xiang, Tianhao Wang, and Di Wang. 2024. Pre-serving node-level privacy in graph neural networks.In 2024 IEEE Symposium on Security and Privacy(SP), pages 47144732. IEEE": "Nan Xu, Oluwaseyi Feyisetan, Abhinav Aggarwal,Zekun Xu, and Nathanael Teissier. 2021a. Density-aware differentially private textual perturbations us-ing truncated gumbel noise. In Proceedings of theThirty-Fourth International Florida Artificial Intel-ligence Research Society Conference, North MiamiBeach, Florida, USA, May 17-19, 2021. Nan Xu, Oluwaseyi Feyisetan, Abhinav Aggarwal,Zekun Xu, and Nathanael Teissier. 2021b. Density-aware differentially private textual perturbations us-ing truncated gumbel noise. In Proceedings of theThirty-Fourth International Florida Artificial Intel-ligence Research Society Conference, North MiamiBeach, Florida, USA, May 17-19, 2021.",
  "Zekun Xu, Abhinav Aggarwal, Oluwaseyi Feyisetan,and Nathanael Teissier. 2021d. On a utilitarian ap-proach to privacy preserving text generation. arXivpreprint arXiv:2104.11838": "Zhiyu Xue, Shaoyang Yang, Mengdi Huai, and Di Wang.2021. Differentially private pairwise learning revis-ited. In Proceedings of the Thirtieth InternationalJoint Conference on Artificial Intelligence, IJCAI2021, Virtual Event / Montreal, Canada, 19-27 Au-gust 2021, pages 32423248. ijcai.org. Ze Yang, Can Xu, Wei Wu, and Zhoujun Li. 2019. Read,attend and comment: A deep architecture for auto-matic news comment generation. In Proceedings ofthe 2019 Conference on Empirical Methods in Natu-ral Language Processing and the 9th InternationalJoint Conference on Natural Language Processing(EMNLP-IJCNLP), pages 50775089, Hong Kong,China. Association for Computational Linguistics.",
  "Implementation Details.Models in this paperare implemented based on the PyTorch and Ten-sorFlow with their libraries. Experiments are con-ducted on NVIDIA GeForce RTX 3090 GPUs. To": "implement our mechanism, we use the acceptance-rejection sampling method (Neal, 2003) to sample apoint from the high dimensional truncated Laplacedistribution from the Laplace distribution, only byrejecting the samples outside the interval.For text re-write, we use the auto-encodermodel. The embedding is initialized with the 300-dimensional pre-trained Random, GloVe, and fast-Text word embedding. We use one-layer BiLSTMwith dropout for the encoder, and using setup:dropout rate 0.5, Adam (Kingma and Ba, 2015)with an initial learning rate of 0.01 and betas (0.5,0.999), batch size 1024, and number of trainingepochs 100. For the downstream classification taskover the AG News and SST-2 dataset, we use Adamwith an initial learning rate of 0.0001, a dropoutrate of 0.2, and a batch size of 256. We set themaximum number of epochs to be 50.",
  "BOmitted Proofs": "Proof 1 (Proof of Theorem 3) The proof is mo-tivated by (Das et al., 2021).Consider apair of tokens w, w.Let perturbed encoder1r1=CLIPEmb(w) + 1, also let r2=CLIPEmb(w)+2 = CLIPEmb(w)+s +2,where s1 1 and s whichare due to the clipping operation.Let us denote the set of possible values of rk bySk for k = 1, 2.Define U = [C A, C + A]d. Note that forany subset V U (S1 S2), P (r1 V) =P (r2 V) = 0, hence (, )-DP is satisfied forthis part. We need to ensure (, )-DP is satisfiedfor all elements in S1 S2 too.First, consider an element s S1 S2. Then:"
}