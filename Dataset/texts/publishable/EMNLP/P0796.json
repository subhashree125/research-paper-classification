{
  "Abstract": "Temporal Knowledge Graph Question Answer-ing (TKGQA) aims to answer temporal ques-tions using knowledge in Temporal Knowl-edge Graphs (TKGs).Previous works em-ploy pre-trained TKG embeddings or graphneural networks to incorporate the knowledgeof TKGs.However, these methods fail tofully understand the complex semantic infor-mation of time constraints. In contrast, LargeLanguage Models (LLMs) have shown excep-tional performance in knowledge graph rea-soning, unifying both semantic understand-ing and structural reasoning. To further en-hance LLMs temporal reasoning ability, thispaper aims to integrate temporal knowledgefrom TKGs into LLMs through a Time-awareRetrieve-Rewrite-Retrieve-Rerank framework,which we named TimeR4.Specifically, toreduce temporal hallucination in LLMs, wepropose a retrieve-rewrite module to rewritequestions using background knowledge storedin the TKGs, thereby acquiring explicit timeconstraints. Then, we implement a retrieve-rerank module aimed at retrieving semanticallyand temporally relevant facts from the TKGsand reranking according to the temporal con-straints. To achieve this, we fine-tune a re-triever using the contrastive time-aware learn-ing framework. Our approach achieves greatimprovements, with relative gains of 47.8%and 22.5% on two datasets, underscoring itseffectiveness in boosting the temporal reason-ing abilities of LLMs. Our code is available at .",
  ": Examples of challenges in integrating tempo-ral knowledge graphs with large language models": "States after Obama?\" To address this, some knowl-edge graphs store time-aware facts as quadruples(subject, predicate, object, timestamp), which areknown as temporal knowledge graphs (TKGs).Temporal knowledge graph question answering(TKGQA) focuses on obtaining answers using theknowledge in TKGs (Saxena et al., 2021).Recent works (Saxena et al., 2021; Mavromatiset al., 2022) incorporate knowledge from TKGsby utilizing pre-trained TKG embeddings or graphneural networks (GNNs). However, these methodsfail to fully understand the complex semantic in-formation of time constraints in questions (Chenet al., 2023b). In contrast, Large language mod-els (LLMs) have demonstrated exceptional perfor-mance in knowledge graph reasoning (Sun et al.,2024; Luo et al., 2024) and can unify semanticunderstanding and graph reasoning (Huang andChang, 2023; Wei et al., 2023). To further enhancethe temporal reasoning capabilities of LLMs, inthis paper, we aim to integrate temporal knowl-edge into LLMs, thereby addressing complex andmulti-granularity temporal questions. However, en-hancing the temporal reasoning capabilities withinLLMs remains several significant challenges:(1) Hallucinated by implicit temporal ques- tions. In the TKGQA task, there are many implicitquestions, such as \"After the Danish Ministry, whowas the first to visit Iraq?\" Reasoning through suchquestions is very difficult because there are no ex-plicit timestamps provided, requiring extra steps ofinference. In , LLMs tend to hallucinatewhen confronted with such questions, leading toincorrect reasoning. Conversely, when temporalevents are replaced with specific timestamps, suchas \"After 2016-01-05, who was the first to visitIraq?, LLMs can more easily deduce the correctanswer. Therefore, we believe that converting im-plicit questions into explicit ones is a crucial issue.(2) Lack of Temporal Knowledge. To enhancethe reasoning capability of LLMs, previous meth-ods (Li et al., 2023) employ off-the-shelf retrievaltools such as BM25 to extract relevant facts from aknowledge graph as background knowledge. How-ever, these retrieval methods focus solely on seman-tic matching, thus the retrieved knowledge neglectsthe time constraints of the question, rendering itineffective for reasoning. For example, in , quadruple (Evan, visit, Iraq, 2016-01-04) is ir-relevant because the question requires retrievingfacts after 2016-01-05. Therefore, constructing aretriever that concurrently pays attention to seman-tic similarity and temporal constraints is of greatimportance for the TKGQA task.To address the above challenges, we proposeTimeR4, a Time-aware Retrieve-Rewrite-Retrieve-Rerank framework for the TKGQA task. Specif-ically, to mitigate the issue of LLMs hallucinat-ing when faced with implicit temporal questions,we employ the retrieve-rewrite strategy. We per-form fact retrieval from the Facts Knowledge Store(FKS) to obtain relevant facts for implicit ques-tions. Subsequently, we rewrite these questionsby replacing temporal facts with specific times-tamps, ensuring that all questions contain explicittemporal information. For constructing FKS, wefix the language model to obtain semantical factembeddings. To simultaneously capture semanticsimilarity and temporal constraints, we employ theretrieve-rerank strategy. This involves conductinga time-aware retrieval from the Temporal Knowl-edge Store (TKS) and reranking the facts based ontemporal constraints to refine the retrieval process.The TKS is constructed by fine-tuning a languagemodel with a contrastive time-aware retrieval strat-egy, which develops an encoder capable of cap-turing both semantic similarity and temporal con-straints by constructing three types of negatives for each question. Finally, we fine-tune open-sourceLLMs with the retrieved facts from TKS, leverag-ing the temporal knowledge in TKGs to enhancethe models temporal reasoning capabilities. Ex-periments on two datasets demonstrate that ourstrategy significantly enhances the temporal rea-soning abilities of LLMs. Overall, our work makesthe following contributions:",
  "KG-enhanced LLMs": "Considering the excellent reasoning ability of largelanguage models (LLMs) on NLP tasks (Bang et al.,2023), many recent works have applied LLMs onKGQA tasks. Based on how these methods in-tegrate with knowledge graphs, they can be cate-gorized into three primary types: retrieval-basedreasoning, path-based reasoning, and agent-basedreasoning. Retrieval-based methods (Baek et al.,2023b; He et al., 2024) focus on retrieving relevantsubgraphs or triples from the knowledge graph thatcontain the information needed to answer a ques-tion. The LLMs are then used to process and rea-son over these retrieved information. However,they only consider semantic similarity and neglecttemporal constraints. Path-based methods (Luoet al., 2023; Cheng et al., 2024) involve exploringpaths within the knowledge graph to establish con-nections between the question and the potential an-swers. These methods typically utilize LLMs to tra-verse the graph and generate possible paths. How-ever, they cannot be directly applied to TKGQA be-cause they do not account for temporal dimensionin path reasoning. Agent-based methods(Sun et al.,2023; Jiang et al., 2023) treat LLMs as an agent tosearch and prune on the KGs to find answers. How-ever, they prove inefficient for complex reasoning",
  "TKGQA Methods": "The TKGQA task is more challenging than theKGQA task due to the added temporal dimensionof reasoning. To incorporate temporal information,some methods pose a question as a TKG comple-tion problem and utilize TKG embedding scorefunctions to select entities or timestamps with thehighest relevance as answers (Saxena et al., 2021).TempoQR (Mavromatis et al., 2022) augments thequestion embeddings with context, entity, and time-aware information by three designed modules. Mul-tiQA (Chen et al., 2023b) adopts Transformer en-coding layers to aggregate multi-granularity timeinformation. Some works integrate temporal infor-mation by introducing RGCN (Relational GraphConvolutional Networks). EXAQT (Jia et al., 2021)utilizes the RGCN layer and augments it with dic-tionary matching. TwiRGCN (Sharma et al., 2022)adopts temporally weighted graph convolution fol-lowed by answer gating. LGQA (Liu et al., 2023)applies a multi-hop message passing graph neu-ral network layer to combine the global and lo-cal information. However, such two methods per-form poorly in complex reasoning tasks, especiallythose involving multi-granularity temporal ques-tions. ARI (Chen et al., 2023a) integrates LLMsthrough a knowledge adaptability framework andabstract methodological guidance. However, it can-not be applied to smaller-scale LLMs.",
  "Preliminaries": "Temporal knowledge graph G = {E, P, T , F} isa directed graph where vertices are a set of entitiesE. The edges are a set of predicates P with times-tamps T . The quadruple set F = {(s, p, o, t)} E P E T represents the temporal facts, wheres and o are subject and object, p is predicate be-tween s and o at timestamp t.TKGQA is a task to infer the correct answer tonatural language question q Q based on relevantquadruples f = (s, p, o, t) in the TKG, where theanswer can be either an entity name or timestamp.",
  "19return KS;": "detailed training procedure is illustrated in Algo-rithm 1. To enhance the performance of LLMsin handling complex problems, we first propose aretrieve-rewrite strategy. This strategy aims to re-trieve implicit temporal knowledge within the ques-tions from the Facts Knowledge Store (FKS) andreformulate the questions using this backgroundknowledge to include explicit time constraints. ForFKS, we utilize the language model to obtain em-beddings of facts within TKGs. Next, we imple-ment a retrieve-rerank module to retrieve both se-mantically and temporally relevant facts. This in-volves conducting a time-aware retrieval from theTemporal Knowledge Store (TKS) and rerankingthe facts based on temporal constraints. For TKS,we fine-tune a language model using contrastivelearning to develop an encoder capable of simulta-neously capturing semantic similarity and temporalconstraints. Finally, we fine-tune the open-sourceLLMs, incorporating the retrieved facts from TKGsfor enhanced LLMs temporal reasoning.",
  "Fact Retrieval": "Previous works (Chen et al., 2023b; Liu et al.,2023) employed entity-linking tools to identify en-tities and relations in question, subsequently utiliz-ing these entities and relations for further retrieval.However, some TKGs, such as ICEWS (Garca-Durn et al., 2018a), lack entity-linking tools,resulting in poor performance with such meth-ods (Chen et al., 2023b). Motivated by the recentstudy (Baek et al., 2023a), we adopt a direct fact re-trieval strategy from TKGs without entity linking.We first convert each quadruple into natural lan-guage sequences. Then we embed all quadruplesT(s, p, o, t) G in TKGs onto a dense embeddingspace by using a pre-trained language model as inEquation 1 and memorize the knowledge represen-tations in a Fact Knowledge Store (FKS). We alsoembed the given questions q as in Equation 2. d isthe dimension of the output vector.",
  "Rewrite": "Complex questions often contain implicit tempo-ral information, posing a challenge to the TKGQAtask. To address the hallucination issues of LLMswith implicit questions, we plan to rewrite the ques-tions to ensure that all questions have explicit times-tamps. Specifically, we retrieve the necessary back-ground facts through the FKS and then input themalong with the question into the LLM for inferenceand rewriting as in Equation 5. We apply the in-context learning strategy (Dong et al., 2023), whichencodes structural knowledge into demonstrationsto guide the LLM. The specific prompt is shown inAppendix A.",
  "q = LLM(Prompt(q, f)), q Q(5)": "After being rewritten by the LLMs, questions con-taining implicit temporal facts are modified to in-clude explicit timestamps based on the retrievedbackground facts. Additionally, for certain ques-tions involving common knowledge not present inTKGs, such as \"Who was the first president of theUS after World War II?\", the LLMs can utilize theirinherent knowledge to rewrite them. Knowing thatWorld War II ended in 1945, the question can suc-cessfully be transformed into \"Who was the firstpresident of the US after 1945?\".",
  "TKS = {Et|Et = LMt(S(s, p, o, t)), (s, p, o, t) G} (6)": "In order to enhance the time-awareness of thelanguage model, we employ the contrastive time-aware retrieval strategy. We randomly corrupt thetime, relations, and entities of the positive pair sep-arately and generate three types of negative pairs:time incorrect, content incorrect, and both incor-rect, as shown in . Contrastive loss iscalculated based on the cosine similarity betweenthe question representation Eq and the quadruplesrepresentation Et TKS as Equation 7.",
  "TKS = cos(Eq, Et)(7)": "We aim to minimize the distance between positivepairs and maximize the distance between negativepairs. The contrastive label Y=1 signifies that thecontext corresponds to a positive pair of the ques-tion, whereas Y=0 indicates that the context rep-resents the negative pair. Subsequently, followingSon and Oh (2023), the contrastive loss is com-puted in Equation 8, with wp and wn as the weightsfor positive and negative samples.",
  "Rerank": "To further refine the retrieval process, we design atime-filtering function to filter out irrelevant factsand focus more on time-related ones. After therewriting module, each question q contains a spe-cific timestamp tq, except for the questions thatrequire answering the timestamp. In that case, tointroduce the influence of time intervals, we designa time-filtering function for questions containingtime constraints. For each quadruple (s, p, o, t) inTKG G, we calculate the time difference betweentq and t, filter out quadruples that fall outside therange, and normalize the time differences withinthe range as the results of time filtering function.Equation 9 represents the time-filtering functionfor \"before\" type questions.",
  "Reasoning": "After obtaining the retrieved quadruples we for-mulate the reasoning part as an LLM optimizationproblem, aiming to maximize the probability of in-ferring the answer a to question q from the knowl-edge graph G by using the retrieved quadruplesf+ as history facts as in Equation 11. To guidethe LLM in generating final answers, we design asimple instruction prompt in Appendix A.",
  "Experimental Settings": "Datasets.Considering that the CronQuestions(Saxena et al., 2021) dataset has been reported tocontain spurious correlations that different mod-els can exploit to achieve high accuracy (Sharmaet al., 2022), we base our experiments on two recentmore challenging datasets, i.e., MULTITQ (Chenet al., 2023b) and TimeQuestions (Jia et al., 2021).MULTITQ is the largest known TKGQA datasetconstructed from the ICEWS05-15 (Garca-Durnet al., 2018b), which has 500K unique question-answer pairs. Besides, MULTITQ features multi-ple temporal granularities, including years, months,and days, with questions spanning over 3600 days.TimeQuestions is another challenging dataset,which has 16K temporal questions and is dividedinto four categories. However, TimeQuestions onlyincludes a time granularity of years and is muchsmaller in size. The statistical information is pre-sented in and separately.Baseline. We select three types of baselines forcomparison on MULTITQ: (1) Pre-trained LMs,including BERT (Devlin et al., 2019) and AL-BERT (Lan et al., 2020). (2) Embedding-basedmethods, including EmbedKGQA (Saxena et al.,",
  ": Statistics of TIMEQUESTIONS dataset": "2020), CronKGQA (Saxena et al., 2021), Mul-tiQA (Chen et al., 2023b). (3) LLM-based methods,including ARI (Chen et al., 2023a), LLaMA2 (Tou-vron et al., 2023), ChatGPT. For TimeQuestions,we also use three types of baselines: (1) KGQAmethod, including PullNet (Sun et al., 2019),Uniqorn (Pramanik et al., 2023), and GRAFT-Net (Sun et al., 2018). (2) TKGQA methods, in-cluding CronKGQA, TempoQR (Mavromatis et al.,2022), EXAQT (Jia et al., 2021), LGQA (Liu et al.,2023), and TwiRGCN (Sharma et al., 2022). (3)LLM-based methods, including LLaMA2 and Chat-GPT. We only input the given questions into Chat-GPT and LLaMA2 without any explanation.Implementations Details. We use LLaMA2-Chat-7B (Touvron et al., 2023) as the LLM back-bone. We fine-tune the LLaMA2 for 2 epochs on2 NVIDIA A6000 GPUs. We only use 20% ofthe training data for MULTITQ datasets for train-ing because it is very large. For Fact Retriever,We utilize off-the-shell SentenceBert (Reimers andGurevych, 2019) as the base encoder. For Time-aware Retriever, we fine-tune the SentenceBert for10 epochs. For re-writing, we use the OpenAI-API 1 (gpt-3.5-turbo-01252). We set the as 0.4.We fine-tune each question using in-batch negativesand three hard negatives.",
  "We present the experimental results in compari-son with other methods of TimeR4 on the MultiTQand TimeQuestions datasets in and Table": "4, where the highest results are highlighted in boldfont and the second highest results are marked un-derlined. TimeR4 achieves the best performance inall experimental settings, indicating its superiorityon the TKGQA task.For the MultiTQ dataset, TimeR4 achieves state-of-the-art performance across all question types.Specifically, We find that PLMs (BERT. ALBERT)and LLMs (LLaMA2, ChatGPT) exhibit the lowestperformance on the TKGQA task. This might bedue to the lack of necessary temporal knowledge,thus leading to errors in reasoning. Compared totraditional methods, TimeR4 shows a significantimprovement in hits@1 performance, achieving a59.8% enhancement. This highlights the capabilityof LLMs in reasoning on complex temporal ques-tions, particularly those involving multi-granularitytimestamps and complex reasoning. Furthermore,compared with the recent ChatGPT-based methodARI, TimeR4 demonstrates a 47.8% improvement,showcasing the effectiveness of our proposed tem-poral reasoning framework.As for TimeQuestions, TimeR4 also achieves thebest results across all question types. The resultsof KGQA methods are the poorest for they lackthe ability to retrieve temporal facts or reason intemporal facts. Compared to traditional methods,TimeR4 still achieves a 22.5% relative improve-ment on Hits@1. The results demonstrate the capa-bility of TimeR4 for precisely answering temporalquestions. Compared with other LLMs, TimeR4 also performs the best. The results of ChatGPTand LLaMA2 are much higher than on MULTITQdatasets. This might be because Timequestionsare built on the Wikidata (Vrandecic and Krtzsch,2014) knowledge graph, and most LLMs are pre-trained on the Wikidata knowledge graph corpus.Therefore, they store some relevant information,enabling them to answer such questions.",
  "Ablation Study": "The ablation results are shown in . Theresults for TimeQuestions do not decline signifi-cantly, primarily because the MULTITQ dataset ismore challenging. MULTITQ is larger and coversa wider time range, making it harder to retrieverelevant facts. Conversely, in the TimeQuestionsdataset, relevant facts can often be retrieved by thefact retrieval module, which is why our strategy isless prominent on this dataset.Effect of fact retrieval module. We first replacefact retriever with entity-linking tools. Since no",
  ": Performance comparison of different models(in percentage) on TimeQuestions": "linking tool is available for the ICEWS, we adoptedthe approach outlined in Chen et al. (2023b), em-ploying a NER tool to identify entities. It can beobserved that after replacing the fact retriever mod-ule, the overall performances decrease by 13.2%on hits@1, which indicates that the fact retrievermodule achieves accurate fact recognition.Effect of rewriting strategy. To verify the roleof the rewriting strategy, we then conduct an exper-iment where we removed the rewriting strategy andsolely relied on the original questions and retrievedfacts. The results showed a significant decrease, in-dicating that the rewriting strategy effectively aidsLLMs in mitigating the hallucination of implicittemporal questions.Effect of time-aware retrieval module. Weconducted an experiment where we replaced thetime-aware retriever with the fact retriever. Theresults show a significant decline, indicating thatour proposed time-aware retriever method is in-herently time-aware and performs better than thenon-time-aware fact retriever.",
  "Comparison with LLMs": "We compare TimeR4 with other LLMs in bothdatasets in . LLMs w/ TimeR4 indicatesthat we input the facts retrieved by our strategy andrewritten questions into the LLMs. LLMs w/ fine-tuned indicates that we fine-tuned the LLMs withonly questions.First, the results on two LLMs show that with theenhancement of the facts retrieved from TKGs andour retrieve-rewrite-retrieve-rerank strategy, LLMsw/ TimeR4 have significantly better performance.This suggests that the LLMs possess some degreeof simple temporal reasoning capability. However,for more precise and effective reasoning in complextemporal questions, TimeR4 effectively overcomesthe limitations in handling and interpreting time-sensitive knowledge.To further explore the effectiveness of ourmethod, we also compared the results of LLamawith fine-tuned and without fine-tuned. It can befound that the results of LLaMA2 with fine-tuningalmost doubled, indicating that fine-tuning can ef-fectively regulate the output of LLMs and helpLLMs learn patterns of temporal reasoning, therebyenhancing the results.It is worth noting that in multiple questions,ChatGPT performs better than TimeR4. This ismainly because ChatGPT tends to provide all possi-ble answers. For example, in , for questionswith only one entity as the answer, ChatGPT oftenreturns a list of relevant results, which can result inhigher performance.",
  "Number of Retrieved Facts": "To explore the impact of varying numbers of re-trieved facts on the results, we present the perfor-mance changes and the corresponding answer cov-erage on two datasets by adjusting the number ofretrieved facts n in . It is evident thatthe model achieves its peak performance with 15relevant facts, the same number included in ourstrategy. Interestingly, there is a slight performancedip at n = 20, despite the larger answer cover-age obtained with 20 retrieved facts. This suggeststhat an excessive amount of facts may hinder per-formance by introducing noise, thereby makingit more difficult for the LLMs to distinguish rele-",
  "Effectiveness of the Retrieval": "To verify the effectiveness of our time-aware re-trieval module, we investigate the overlap of answercoverage between the results of the Time-awareRetriever, the Fact Retriever, and ground truth indifferent question types in . Across variousquestion types, our Time-Aware Retriever answersmost questions correctly that the Fact Retrieverdoes, while also handling a significantly larger setof complex questions (second row). This indicatesthat after fine-tuning, the Time-Aware Retrievercan retrieve more temporal information, while theFact Retriever struggles to capture temporal infor-mation in the facts, leading to low answer coveragein the retrieved results.",
  "Case Study": "We provide examples for each question type inthe MULTITQ dataset of ChatGPT w/ TimeR4,LLaMA2 w/ TimeR4, and TimeR4 in withthe same input.It can be observed in the results that withoutfine-tuned, LLMs tend to randomly generate a listof irrelevant answers, but in comparison, Chat-GPT performs better than LLaMA2. LLaMA2is more prone to generating irrelevant responseswithout fine-tuning. For instance, when asked \"Af-ter Okada Katsuya, who wishes to visit Cambodiafirst?\", LLaMA2 directly outputs the entity men-tioned in the question: Okada Katsuya. TimeR4,on the other hand, achieves significantly higheraccuracy and only generates the relevant answers.Furthermore, after fine-tuning, the output ofTimeR4 adheres more closely to standard conven-tions. For questions like \"in which month...,\" wherethe ground truth answer is , LLMs of-",
  "Conclusion": "In this work, we address two key challenges thatLLMs face when handling temporal questions andintroduce a time-aware retrieve-rewrite-retrieve-rerank framework named TimeR4.To mitigate the issue of the temporal hallucina-tion of LLMs, we utilize a retrieve-rewrite strategyto fetch relevant facts in FKS and integrate specifictimestamps into the questions. Afterward, in orderto retrieve facts that satisfy both time constraintsand semantic similarity, we implement a retrieve-rerank strategy. We perform time-aware retrievalfrom the TKS and rerank them based on tempo-ral constraints. Finally, we fine-tune open-sourceLLMs, leveraging the knowledge in TKGs to en-hance the temporal reasoning capabilities in LLMs.Experiments on two datasets demonstrate that ourframework significantly enhances the temporal rea-soning abilities of LLMs.",
  "Although our approach achieves significant im-provements, with relative gains of 47.8% and22.5% on two TKGQA datasets, the performance": "of complex temporal fact retrieval can still be fur-ther enhanced. How to retrieve more precise andeffective temporal information is a question worthyof exploration.Additionally, controlling the answer format dur-ing the generation process of LLMs without fine-tuning is difficult, as discussed in .5. Thus,standardizing the answer formats of LLMs or de-veloping a more reasonable evaluation method isanother important future task.",
  "Ziyang Chen, Dongfang Li, Xiang Zhao, BaotianHu, and Min Zhang. 2023a. Temporal knowledgequestion answering via abstract reasoning induction.Preprint, arXiv:2311.09149": "Ziyang Chen, Jinzhi Liao, and Xiang Zhao. 2023b.Multi-granularity temporal question answering overknowledge graphs. In Proceedings of the 61st An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 1137811392. Sitao Cheng, Ziyuan Zhuang, Yong Xu, Fangkai Yang,Chaoyun Zhang, Xiaoting Qin, Xiang Huang, LingChen, Qingwei Lin, Dongmei Zhang, et al. 2024.Call me when necessary: Llms can efficiently andfaithfully reason over structured environments. arXivpreprint arXiv:2403.08593.",
  "Jie Huang and Kevin Chen-Chuan Chang. 2023. To-wards reasoning in large language models: A survey.Preprint, arXiv:2212.10403": "Zhen Jia, Soumajit Pramanik, Rishiraj Saha Roy, andGerhard Weikum. 2021. Complex temporal questionanswering on knowledge graphs. In Proceedings ofthe 30th ACM international conference on informa-tion & knowledge management, pages 792802. Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye,Wayne Xin Zhao, and Ji-Rong Wen. 2023. Struct-gpt: A general framework for large language modelto reason over structured data.arXiv preprintarXiv:2305.09645.",
  "Apoorv Saxena, Soumen Chakrabarti, and Partha Taluk-dar. 2021. Question answering over temporal knowl-edge graphs. arXiv preprint arXiv:2106.01515": "Apoorv Saxena, Aditay Tripathi, and Partha Talukdar.2020. Improving multi-hop question answering overknowledge graphs using knowledge base embeddings.In Proceedings of the 58th Annual Meeting of the As-sociation for Computational Linguistics, pages 44984507, Online. Association for Computational Lin-guistics. Aditya Sharma, Apoorv Saxena, Chitrank Gupta,Seyed Mehran Kazemi,Partha Talukdar,andSoumen Chakrabarti. 2022. Twirgcn: Temporallyweighted graph convolution for question answeringover temporal knowledge graphs.arXiv preprintarXiv:2210.06281.",
  "Haitian Sun, Tania Bedrax-Weiss, and William W Co-hen. 2019. Pullnet: Open domain question answeringwith iterative retrieval on knowledge bases and text.arXiv preprint arXiv:1904.09537": "Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, KathrynMazaitis, Ruslan Salakhutdinov, and William W. Co-hen. 2018. Open domain question answering usingearly fusion of knowledge bases and text. Preprint,arXiv:1809.00782. Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, SaizhuoWang, Chen Lin, Yeyun Gong, Lionel M. Ni, Heung-Yeung Shum, and Jian Guo. 2024.Think-on-graph: Deep and responsible reasoning of largelanguage model on knowledge graph.Preprint,arXiv:2307.07697. Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, SaizhuoWang, Chen Lin, Yeyun Gong, Heung-Yeung Shum,and Jian Guo. 2023.Think-on-graph: Deep andresponsible reasoning of large language model withknowledge graph. arXiv preprint arXiv:2307.07697. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288."
}