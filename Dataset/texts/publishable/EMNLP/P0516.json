{
  "Abstract": "Recent advancements in Vision-Language (VL)research have sparked new benchmarks forcomplex visual reasoning, challenging mod-els advanced reasoning ability. TraditionalVision-Language Models (VLMs) perform wellin visual perception tasks while strugglingwith complex reasoning scenarios. Conversely,Large Language Models (LLMs) demonstraterobust text reasoning capabilities; however,they lack visual acuity. To bridge this gap,we propose Complex Visual Reasoning LargeLanguage Models (CVR-LLM), capitalizingon VLMs visual perception proficiency andLLMs extensive reasoning capability.Un-like recent multimodal large language mod-els (MLLMs) that require a projection layer,our approach transforms images into detailed,context-aware descriptions using an iterativeself-refinement loop and leverages LLMs textknowledge for accurate predictions without ex-tra training. We also introduce a novel multi-modal in-context learning (ICL) methodologyto enhance LLMs contextual understandingand reasoning.Additionally, we introduceChain-of-Comparison (CoC), a step-by-stepcomparison technique enabling contrasting var-ious aspects of predictions. Our CVR-LLMpresents the first comprehensive study across awide array of complex visual reasoning tasksand achieves SOTA performance among all.",
  "Introduction": "The concept of complex visual reasoning was in-troduced with Visual Commonsense Reasoning(VCR) dataset (Zellers et al., 2019) in 2019, whichtests models ability to understand visual contentas well as commonsense cognition. However, thedevelopment in this field has remained relativelysubdued, primarily due to Vision-Language Mod-els (VLMs) limitations in incorporating common-sense knowledge (Gan et al., 2022). Recent yearshave seen significant advancements in complex lin-guistic reasoning tasks (Cobbe et al., 2021; Wei et al., 2022) due to the emerging GPT3 (Brownet al., 2020), LLaMA (Touvron et al., 2023a), andVicuna (Chiang et al., 2023). This leap forwardhas triggered a renewed interest in the complexvisual reasoning area, exploring how visual per-ception can enhance linguistic inference and po-tentially overcome previous hurdles (Gan et al.,2022). It has led to innovative benchmarks focus-ing on various aspects: commonsense reasoning -WinoGAViL (Bitton et al., 2022), compositionality- Winoground (Thrush et al., 2022), weird imageexplanation - Whoops (Bitton-Guetta et al., 2023),and humor understanding - NYCCC (Hessel et al.,2022). These tasks demand models not only ac-curately interpret image content, but also integrateknowledge from daily experiences, general com-monsense, cultural context, and humor sense. Forexample, a synthetic image, as shown in Whoopsexample in of The portrait of the MonaLisa depicts a stern male face. contradicts the cul-tural context, as the famous painting Mona Lisadepicts a female face.In this paper, we introduce a novel methodnamed Complex Visual Reasoning Large Lan-guage Models (CVR-LLM), based on the \"VLMs +LLMs\" concept. Recent multimodal large languagemodels (MLLMs) like LLaVA (Liu et al., 2024,2023a) and MiniGPT4 (Zhu et al., 2023; Chen et al.,2023) have proven effective in many VL tasks.However, these models are resource-intensive, re-lying on millions of image-text pairs for projectionlayer learning. To overcome this limitation, ourapproach leverages the visual perception strengthsof VLMs to translate images into context-awareimage descriptions (CaID) via an inference-only,dual-loop self-refinement process that incorporatesfeedback from LLMs. These detailed descriptionsenhance the LLMs inference process, transform-ing multi-modal tasks into simpler single-modalchallenges and streamlining the overall process.In addition, we develop a unique multi-modal in- : Five distinct examples from diverse datasets in the complex visual reasoning field (Bitton-Guetta et al.,2023) challenge AI models ability of complex reasoning in different aspects such as general commonsense. context learning (ICL) approach named ComplexVisual Reasoning ICL (CVR-ICL), which enhancesthe reasoning capacities of LLMs within a rangeof complex multi-modal environments. provides an illustration of how our CVR-LLM isapplied to the Winoground task. It describes theimages as appropriate sentences via CaID and uti-lizes the sophisticated reasoning and ICL abilitiesof LLMs through CVR-ICL for more accurate pre-dictions.Our research stands as the pioneering studyto explore such a broad array of benchmarks(WinoGAViL, Winoground, Whoops, VCR, andNYCCC), proposing a paradigm centred on the\"VLM+LLM\" concept for addressing complex vi-sual reasoning tasks. Experimental results showthat CVR-LLM achieves SOTA performance acrossall five tasks. Further ablation studies and com-parative analyses reveal the effectiveness of eachmodule and the superiority of our method over pre-vious approaches. Particularly in comparative anal-ysis, we introduce the Chain-of-Comparison (CoC)technique, inspired by \"Chain-of-Thought\" and uti-lizing GPT4 (Achiam et al., 2023), to address thelimitations of conventional metrics in evaluatingabstract concepts. CoC provides a nuanced analy-sis by systematically dissecting and quantitativelycontrasting various facets of the results for a com-prehensive evaluation.Our contributions are summarized as follows:(1) We present the first comprehensive study across all complex visual reasoning tasks, including Wino-GAViL, Winoground, Whoops, VCR, and NYCCC.(2) We design a context-aware image descriptiongeneration method and a specific in-context learn-ing strategy1, to enhance the advanced visual rea-soning ability of LLMs to multi-modal complexvisual reasoning tasks. (3) We further introduceChain-of-Comparsion, a novel GPT4-based com-parison technique inspired by \"Chain-of-Thought\"filling the gaps of traditional metrics in abstractconcept evaluation. (4) Experimental results showthat our approach surpasses current SOTA modelsin a range of complex visual reasoning scenarios.",
  "Reasoning Research in Vision-LanguageDomain": "In recent years, multi-modal reasoning researchhas significantly advanced. Beyond the complex vi-sual reasoning benchmarks discussed in ,many studies focus on the reasoning process it-self, such as chain-of-thought (Kojima et al., 2022;Shaikh et al., 2022) or reasoning modules (Zhouet al., 2023b; Jiang et al., 2023), which are crucialfor enhancing AI models analytical capabilitiesand performance. For instance, Liu et al. (2023b)introduced a modality-aligned thought chain rea-soning framework to incorporate explicit reason-ing into task-oriented dialogue generation, improv-",
  "The project is available at:": ": An example of our CVR-LLM works on the Winoground dataset. Our method transfers images intocontext-aware image descriptions through CaID and leverages the sophisticated reasoning and ICL abilities ofLLMs with the CVR-ICL module, offering a more precise answer. ing contextual understanding and effectiveness.Lv et al. (2023) proposed a counterfactual cross-modality reasoning method for better video mo-ment localization. Zhou et al. (2023a) developeda multi-step reasoning probability transfer mech-anism to improve multi-label interaction classifi-cations. Yu et al. (2023) presented a hierarchicalreasoning network to consolidate multi-level in-teractive cues, from coarse to fine-grained details,enhancing Human-Object Interaction (HOI) repre-sentations.",
  "Large Language Models forVision-Language Analysis": "The past two years have seen an unprecedentedsurge in the development and application ofLLMs (Brown et al., 2020; Touvron et al., 2023a;Chiang et al., 2023) across diverse fields. LLMshave garnered acclaim for their robust capabili-ties, including advanced analytical prowess (Ko-jima et al., 2022), extensive text-level knowl-edge (Naveed et al., 2023) and superior under-standing ability (Chang et al., 2023).Further-more, they are equipped with two powerful mecha-nisms: chain-of-thought (Kojima et al., 2022) andin-context learning (Liu et al., 2021a), which sig-nificantly augment their effectiveness and perfor-mance in specialized tasks (Naveed et al., 2023).For example, Muraoka et al. (2023) developeda cross-lingual model trained alongside a cross-lingual LLM, leveraging LLMs capabilities acrosslanguages. Lan et al. (2023) proposed reasoningquestion prompts for Visual Question Answering(VQA) tasks, unlocking LLMs potential in zero-shot learning. Additionally, Yang et al. (2023) in-troduced SODA, a system that integrates LLMswith explainable AI to assist marketers with datainterpretation, enhancing human-AI collaboration.Zhong et al. (2023) used knowledge distillationto imbue the SUR-adapter with LLMs semantic",
  "Methods": "In this section, we introduce the CVR-LLM frame-work, highlighting its innovative process for gen-erating context-aware image descriptions (CaID)as well as its complex visual reasoning in-contextlearning (CVR-ICL) strategy.Initially, we ex-plain the CaID generation process, which differsfrom traditional image captioning by using a self-refinement loop with feedback from Large Lan-guage Models (LLMs) to produce accurate andcontextually relevant descriptions (.1).Subsequently, we present the CVR-ICL approach(.2), which enhances LLMs contextualunderstanding and reasoning by assessing relevantcases and selecting suitable complex multi-modaldemonstrations.",
  "Context-Aware Image Description": "Pre-trained VLMs (Li et al., 2023; Alayrac et al.,2022) have demonstrated their proficiency in gen-erating detailed image captions on benchmarkssuch as MSCOCO (Chen et al., 2015).How-ever, while these captions may accurately reflectvisual content, they are not customized for com-plex visual reasoning scenarios. Recently, the trendof multi-modal instruction-following agents likeminiGPT4 (Zhu et al., 2023; Chen et al., 2023) andLLaVA (Liu et al., 2024, 2023a), integrating open-source LLMs (Chiang et al., 2023; Touvron et al.,2023b) with pre-trained vision encoders (Doso-vitskiy et al., 2020; Liu et al., 2021b) to createa MLLM, has become very popular. The effective-ness of these models is heavily reliant on tuningwith vast amounts of VL instruction data, which isgenerated by powerful LLMs like ChatGPT (Ope-nAI, 2023) and GPT4 (Achiam et al., 2023). Whilepromising, their reliance on extensive VL instruc- : The framework overview of CaID. It is de-signed to transfer images into contextualized descrip-tions, bypassing the need for direct multi-modal fusionand leveraging LLMs extensive knowledge for moreaccurate predictions. tion data for tuning requires the substantial resourceand time investment. In this work, we introduce amore efficient method for generating context-awareimage descriptions, which depends on the inferenceprocess and leverages task-specific information andfeedback from LLMs to craft better prompts, guid-ing the caption generation process more effectively.Our CaID framework optimizes the process ofcreating context-aware image descriptions througha dual-loop self-refinement approach, as shownin . Initially, it leverages task-specificdetails and LLM insights to craft precise imageprompts. These initial prompts are designed to dis-till essential task-related information, guiding thecaptioner in producing descriptions that not onlycover image content but are also deeply alignedwith the tasks requirements. Specifically, given atask specific text description t with an image i (forprocesses involving multiple images, we approacheach image sequentially), the generation of initialcontext-aware image descriptions can be describedas follows:",
  "dinit = C(i, L(t)),(1)": "where dinit is the initial generated context-awareimage description. C is the image-to-text captioner,transfering the image into the description. L isthe LLM, encapsulating crucial task-related textinformation t (e.g. requirements, questions, cuewords) into feature prompts.In the second loop, our approach is crafted toencapsulate essential task-related details as wellas LLMs feedback, enhancing description gener-ation with LLMs vast knowledge. Specifically, itmerges initial descriptions with task specifics andCVR-ICL examples into a task-focused prompt,guiding LLMs to make more precise predictions. These predictions are then treated as pseudo labels,asking LLMs to design further inquiries for deeperinsights around them. In this way, we build up afeedback reflection between LLM prediction andcontext-aware caption, enhancing the richness andaccuracy of the content produced. The textual feed-back is then leveraged to refine the image prompts,providing deep insights that inform and guide thegeneration of nuanced image descriptions. Therevised context-aware image descriptions can bedescribed as follows:",
  "Complex Visual Reasoning ICL": "LLMs are renowned for their exceptional in-context learning capabilities, especially with task-specific examples. The optimal in-context exem-plars enable LLMs to leverage their backgroundknowledge for more precise outcomes. However,most of the research works (Liu et al., 2021a;Sorensen et al., 2022) have primarily focused onthe text-centric domain, with few works (Alayracet al., 2022; Zhao et al., 2023) exploring multi-modal in-context learning for VL tasks. Our ap-proach, unlike prior methods focused solely ontext similarity in NLP, such as the kNN-augmentedin-context example selection (KATE), integratesmulti-modal factors, thereby enriching the disci-pline with a fresh perspective. Furthermore, it isalso different from MMICL (Zhao et al., 2023) inthe multi-modal domain, which employs a visionprompt generator for image-to-visual embeddingconversion and merges these with text embeddingsas a union measurement factor.Complex visual reasoning tasks demand mod-els capable of selecting in-context examples froma multi-modal domain, leveraging extensive back-ground knowledge and information within it (Zhaoet al., 2023). However, our CVR-LLM is groundedin LLMs, which are inherently text-based, lead-ing to a gap between textual and multi-modal do-mains. Directly applying a text-based kNN clus-tering method could result in the loss of importantmulti-modal information. On the other hand, usingmulti-modal information for retrieval might ignore",
  ": The generic diagram of our proposed CVR-ICL approach. The dual analysis enables our approach tomore effectively select contextually relevant examples from text and multi-modal domains": "essential context-aware information within our gen-erated image descriptions. To address this, we pro-pose the complex visual reasoning ICL, which aimsto select in-context examples for LLMs by effec-tively integrating both text and multi-modal compo-nents. This dual analysis enables our LLM to moreeffectively select contextually relevant examples,ensuring a balanced integration of text and multi-modal insights for enhanced in-context learning. illustrates the framework of our CVR-ICLstrategy. Specifically, given a task t with an imagei, we initially convert the image into a descriptiond, which enables the task to be applicable not onlyin multi-modal domains but also in text-only sce-narios. Then, we employ a multi-modal encoderfm and a text encoder ft to transform inputs fromthe multi-modal domain and the text domain intovector representations as follows:",
  "xt = ft(t, d),(3b)": "where xm is the vector representation in the multi-modal domain. xt is the vector representation inthe text domain.Upon transforming each example into two dis-tinct vector forms, we compute the cosine similar-ity score to identify and select the examples thatare most relevant. Considering a target sample intest set and the ith example in the training set, thesimilarity calculation process can be expressed asfollows:",
  "s = sm + st,(4c)": "where sm is the similarity score between the targetsample and ith example in dataset on the multi-modal domain, st is the similarity score betweenthe target sample and ith example in dataset on thetext domain. s is the final similarity score. fc is thecosine similarity function. Finally, the top-k caseswith the highest s are selected as the in-contextexamples, aimed at boosting the contextual under-standing and prediction accuracy of the LLMs.",
  "Dataset and Metrics": "To evaluate the effectiveness of our proposedmethod, we conduct a comprehensive test in com-plex visual reasoning areas. Our evaluation in-cluded WinoGAViL (4373 samples), Winoground(400 samples), Whoops (500 samples), VCR (2653out of over 26k samples, selecting a random 10%),and NYCCC (528 samples), providing a broad as-sessment of our approachs capabilities. In theterms of metrics, we adhered to the evaluationmethods provided by these datasets, ensuring afair assessment of our methods performance.",
  "Implementation Details": "For the basic captioner in context-aware imagedescription (.1), we choose the BLIP2-flant5xxl (Li et al., 2023) as our baseline. For CVR-ICL phase (.2), we employ BM25 (Robert-son et al., 1995) and BLIP2 multi-embedding (Liet al., 2023) to encode text and multi-modal inputs,respectively. It is important to note that the ICLexample results are derived from LLM inferencewithout using actual annotations to prevent data",
  "VLM+LLMCVR-LLMLlama372.370.488.745.029.524.560.450.552.459.857.7CVR-LLMGPT3.573.471.683.442.730.523.561.251.153.459.456.8CVR-LLMGPT474.773.286.543.535.026.562.052.954.360.657.4": ": The comparison of our CVR-LLM with popular VLMs and MM LLMs on five complex visual reasoningtasks. Notably, MLLMs like LLaVA and MiniGPT4 exhibit limitations in handling tasks involving multipleimages or computing image-text similarity scores, resulting in their performance being unavailable for tasks likeWinoGAViL and Winoground. leakage. For our LLMs, we choose three popularLLMs as inference models for generation tests in-cluding: Llama3-8B (Dubey et al., 2024) for CVR-LLMLlama3, GPT3.5 (OpenAI, 2023) for CVR-LLMGPT3.5, and GPT4 (Achiam et al., 2023) forCVR-LLMGPT4. Performance comparisons areconducted directly on the test set without any fine-tuning, as WinoGAViL, Winoground, and NYCCdatasets are exclusively for testing purposes.",
  "Comparison to State-of-the-Arts": "In this section, we evaluate our proposed CVR-LLM against various models across a range ofcomplex visual reasoning tasks, including Wino-GAViL, Winoground, Whoops, VCR, and NYCCC.These models fall into two categories: VLMs (Kimet al., 2021; Radford et al., 2021; Gan et al., 2020;Li et al., 2023) and MLLMs (Liu et al., 2024,2023a; Zhu et al., 2023; Chen et al., 2023). No-tably, MLLMs like LLaVA and MiniGPT4 strugglewith tasks involving multiple images, making theirperformance data unavailable for WinoGAViL andWinoground. showcases our methods superiorityacross five tasks, eclipsing both VLMs and LMMs.For example, our CVR-LLMLlama3 significantlysurpasses the SOTA model BLIP2 by achieving an88.7% accuracy (+17.1 improvement) in SWOWsetting on the WinoGAViL benchmarks. Similarly,it outperforms the SOTA model MiniGPT4 with a62.0% accuracy (+13.8 improvement) on the GPT4rate (Bitton-Guetta et al., 2023) for Whoops tasks,underscoring our frameworks advanced perfor-mance. Additionally, our method performs well onthree LLM-based categories, demonstrating robustgeneration abilities with consistent performance.",
  "Ablation Studies": "In this section, we examine the individual contri-butions of the components within our frameworkCVR-LLMGPT4. As demonstrated in , wepresent an ablation study that quantifies the per-formance impact of each module across variousdatasets. The experimental findings suggest that theCVR-ICL module significantly boosts the inferenceperformance of LLMs compared to using context-aware image descriptions alone, with the exceptionof the NYCCC dataset (It may be due to NYCCCsfocus on humor, where precise descriptions aremore critical). This highlights the CVR-ICL mod-ules effectiveness in enhancing LLM capabilitiesacross various tasks. In addition, our comprehen-sive method, CVR-LLM, which integrates bothcontext-aware descriptions and CVR-ICL, achievesa substantial enhancement in performance relativeto the baseline.",
  "Analysis": "Context-Aware Image Description vs GeneralImage CaptionIn this section, we investigateCaIDs impact at an abstract level and design anovel method to quantitatively demonstrate the se-mantic gap between context-aware image descrip-tions and general image captions (Note that theperformance impact has been shown in ). provides two examples comparing context-aware image descriptions with general image cap-tions and our goal is to determine whether context-aware descriptions offer more contextually relevant",
  "/610/12SWOWTextImageGroupGPT4 RateQ->AQA->RQ->ARMatch acc.CrowdAccNYAcc": "Base60.058.378.428.726.216.036.438.037.021.341.841.346.0Base+CaID63.562.073.731.530.019.754.643.944.222.951.548.753.6Base+CVR-ICL69.866.180.939.029.222.060.648.849.225.848.047.652.9CVR-LLMGPT473.473.286.543.535.026.562.054.352.930.460.657.463.1 : The ablation study of our CVR-LLM on five complex visual reasoning tasks. \"Base\" represents using thegeneral image captions and GPT4 to complete these tasks. \"Base+CaID\" means using the context-aware imagedescriptions instead of the general image captions and GPT4 to test the performance. \"Base+CVR-ICL\" representsusing general image captions and GPT4 with our designed CVR-ICL learning methods.",
  ": Two examples from WinoGAViL comparecontext-aware image descriptions with general imagecaptions. WinoGAViL is designed to ask the model toselect the image that best matches the cue word": "information to aid LLMs in decision-making. Un-like traditional sentence evaluations that rely onannotations to compute metrics like BLEU (Pa-pineni et al., 2002) and CIDEr (Vedantam et al.,2015), we lack direct measures to assess the con-textual relevance of sentences. To address this, weuse GPT4 (Achiam et al., 2023) to evaluate the rela-tive effectiveness between two kinds of expressionswith the prompt: Evaluate the equivalence of thefollowing two options for the task XXX. Option A:XXX; Option B: XXX. Please return True if OptionB is better than Option A in answering questions;return False if the opposite is true; return Equalif they are the same for the question.. Addition-ally, inspired by the concept of chain-of-thought(CoT) (Wei et al., 2022), we propose a novel com-parison chain-of-comparison (CoC), which imple-ments a step-by-step analysis to evaluate the effec-tiveness. This method involves a comprehensivefour-step analysis protocol, depicted in . Itfollows a series of cognitive steps that our brainsundertake to make sense of information, particu-larly when engaging with complex problems.",
  ": Hypothesis verification with GPT4, whichdemonstrates the effectiveness of our CaID against gen-eral image captions": "shows the results of directly employingGPT4 to compare the effectiveness of general im-age captions with our image descriptions in the spe-cific scenario of answering task-related questions.Furthermore, presents the performance de-rived from utilizing GPT4 to conduct a detailed,step-by-step analytical assessment of effectiveness.These empirical results indicate that our approachyields image descriptions with enhanced contex-tual relevance, thereby significantly aiding LLMsin the decision-making process, particularly on theWinoGAViL and Whoops datasets. Complex Visual Reasoning ICL vs Other ICLThe CVR-ICL is designed to optimize the selectionof in-context exemplars within a multi-modal envi-ronment, thereby enhancing the reasoning abilitiesof LLMs. This innovative method is contrastedwith three alternative configurations: Random In-Context Learning (RICL) (Brown et al., 2020),KATE (Liu et al., 2021a), and Multi-modal Similar",
  ": The performance of using different ICL meth-ods on different datasets": "In-Context Learning (MMICL) (Zhao et al., 2023).To ensure a fair comparison, we utilized general im-age captions across all models to test performancefor eliminating the effect of our context-aware im-age descriptions. As demonstrated in , ourCVR-ICL outperforms other ICL methods, demon-strating its adeptness at integrating and leveragingboth textual and multi-modal domains to select themost contextually appropriate exemplars. Case Number Selection in Complex Visual Rea-soning ICL illustrates the influence ofvarying case numbers in the CVR-ICL on the per-formance of our proposed CVR-LLM method. Theexperimental results suggest a trend where themodels performance initially improves with anincrease in case numbers, exhibits fluctuations athigher numbers, and eventually declines as the casenumber becomes excessively large. This patternsuggests that the optimal selection for the numberof cases is four.",
  ": Two qualitative results from Whoops illustrat-ing the capabilities of our approach. Whoops is designedto ask the model to explain what makes images weird": "how LLMs leverage contextual information to askmore relevant and insightful questions tailored thespecific tasks. For instance, when provided withan image of the chess piece, the LLMs might askWhat does the chess piece look like?. Subse-quently, the captioner model generates contextuallyappropriate descriptions, such as A chess piecethat looks like a unicorn.. This synergy enhancesthe LLMs decision-making process, making itmore precise and context-aware. More detailedqualitative results with corresponding prompts andCVR-ICL examples are illustrated in Appendix A.1and Appendix A.2.",
  "Conclusion": "In this work, we propose CVR-LLM, an innova-tive approach for complex visual reasoning tasks.This method boosts LLMs understanding of visualcontent for complex reasoning via context-awareimage descriptions. We also develop a multi-modal in-context learning technique, enhancing LLMsreasoning skills at both image and text levels. Ex-perimental results show that CVR-LLM sets newbenchmarks across multiple complex visual reason-ing tasks. We also introduce a nuanced GPT4 basedanalysis technique Chain-of-Comparison to auto-matically break down and contrast among variousaspects of generated results.",
  "Limitation": "Although our approach achieves SOTA perfor-mance across a wide range of complex visual rea-soning benchmarks, it still has two notable limita-tions. First, compared to the MLLMs that can per-form end-to-end inference directly, our approachoperates as an LLM-agent-driven framework. Thisinvolves VLMs generating context-aware image de-scriptions, followed by the LLM performing infer-ence with ICL to predict the answer. While this two-step process enhances contextual understandingand reasoning, it may significantly increase timeconsumption compared to direct end-to-end infer-ence models. Second, despite its overall strong per-formance and generalization ability, our approachstill lags behind GPT4V in some tasks. shows that our CVR-LLM can surpass GPT4V inSWOW setting in WinoGAViL dataset but fall shortin others. Our future work will focus on refiningthe integration between VLMs and LLMs compo-nents and enhancing the models efficiency andaccuracy across a broader spectrum of complexvisual reasoning challenges.",
  "Reynolds, et al. 2022. Flamingo: a visual languagemodel for few-shot learning. Advances in NeuralInformation Processing Systems, 35:2371623736": "Yonatan Bitton, Nitzan Bitton Guetta, Ron Yosef, YuvalElovici, Mohit Bansal, Gabriel Stanovsky, and RoySchwartz. 2022. Winogavil: Gamified associationbenchmark to challenge vision-and-language models.Advances in Neural Information Processing Systems,35:2654926564. Nitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel,Ludwig Schmidt, Yuval Elovici, Gabriel Stanovsky,and Roy Schwartz. 2023. Breaking common sense:Whoops! a vision-and-language benchmark of syn-thetic and compositional images. In Proceedingsof the IEEE/CVF International Conference on Com-puter Vision, pages 26162627. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,Cunxiang Wang, Yidong Wang, et al. 2023. A sur-vey on evaluation of large language models. ACMTransactions on Intelligent Systems and Technology. Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, ZechunLiu, Pengchuan Zhang, Raghuraman Krishnamoor-thi, Vikas Chandra, Yunyang Xiong, and MohamedElhoseiny. 2023. Minigpt-v2: large language modelas a unified interface for vision-language multi-tasklearning. arXiv preprint arXiv:2310.09478.",
  "Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu,and Wanxiang Che. 2024. M3cot: A novel bench-mark for multi-domain multi-step multi-modal chain-of-thought. arXiv preprint arXiv:2405.16473": "Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-ishna Vedantam, Saurabh Gupta, Piotr Dollr, andC Lawrence Zitnick. 2015.Microsoft coco cap-tions: Data collection and evaluation server. In arXivpreprint arXiv:1504.00325. Yen-Chun Chen, Linjie Li, Licheng Yu, AhmedEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, andJingjing Liu. 2020. Uniter: Universal image-textrepresentation learning. In European conference oncomputer vision, pages 104120. Springer. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.2023. Vicuna: An open-source chatbot impressinggpt-4 with 90%* chatgpt quality. See org (accessed 14 April 2023), 2(3):6.",
  "Nakano, et al. 2021. Training verifiers to solve mathword problems. arXiv preprint arXiv:2110.14168": "AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,Dirk Weissenborn,Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, MatthiasMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.An image is worth 16x16 words: Transformersfor image recognition at scale.arXiv preprintarXiv:2010.11929. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,Akhil Mathur, Alan Schelten, Amy Yang, AngelaFan, et al. 2024. The llama 3 herd of models. arXivpreprint arXiv:2407.21783. Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu,Yu Cheng, and Jingjing Liu. 2020. Large-scale adver-sarial training for vision-and-language representationlearning. Advances in Neural Information Process-ing Systems, 33:66166628. Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, ZichengLiu, Jianfeng Gao, et al. 2022. Vision-language pre-training: Basics, recent advances, and future trends.Foundations and Trends in Computer Graphics andVision, 14(34):163352. Jack Hessel, Ana Marasovic, Jena D Hwang, LillianLee, Jeff Da, Rowan Zellers, Robert Mankoff, andYejin Choi. 2022.Do androids laugh at electricsheep? humor\" understanding\" benchmarks fromthe new yorker caption contest.arXiv preprintarXiv:2209.06293. Xinyi Jiang, Guoming Wang, Junhao Guo, JunchengLi, Wenqiao Zhang, Rongxing Lu, and Siliang Tang.2024.Diem: Decomposition-integration enhanc-ing multimodal insights.In Proceedings of theIEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 2730427313. Zhiying Jiang, Zengxi Zhang, Jinyuan Liu, Xin Fan, andRisheng Liu. 2023. Multi-spectral image stitchingvia spatial graph reasoning. In Proceedings of the31st ACM International Conference on Multimedia,pages 472480.",
  "Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt:Vision-and-language transformer without convolu-tion or region supervision. In International Con-ference on Machine Learning, pages 55835594.PMLR": "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-guage models are zero-shot reasoners. Advances inneural information processing systems, 35:2219922213. Yunshi Lan, Xiang Li, Xin Liu, Yang Li, Wei Qin, andWeining Qian. 2023.Improving zero-shot visualquestion answering via large language models withreasoning question prompts. In Proceedings of the31st ACM International Conference on Multimedia,pages 43894400.",
  "Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,Lawrence Carin, and Weizhu Chen. 2021a. Whatmakes good in-context examples for gpt-3? arXivpreprint arXiv:2101.06804": "Yiting Liu, Liang Li, Beichen Zhang, Shan Huang,Zheng-Jun Zha, and Qingming Huang. 2023b. Matcr:Modality-aligned thought chain reasoning for mul-timodal task-oriented dialogue generation. In Pro-ceedings of the 31st ACM International Conferenceon Multimedia, pages 57765785. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,Zheng Zhang, Stephen Lin, and Baining Guo. 2021b.Swin transformer: Hierarchical vision transformerusing shifted windows.In Proceedings of theIEEE/CVF international conference on computer vi-sion, pages 1001210022. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, PeterClark, and Ashwin Kalyan. 2022. Learn to explain:Multimodal reasoning via thought chains for sciencequestion answering. In The 36th Conference on Neu-ral Information Processing Systems (NeurIPS). Zezhong Lv, Bing Su, and Ji-Rong Wen. 2023. Coun-terfactual cross-modality reasoning for weakly super-vised video moment localization. In Proceedings ofthe 31st ACM International Conference on Multime-dia, pages 65396547. Masayasu Muraoka,Bishwaranjan Bhattacharjee,Michele Merler, Graeme Blackwood, Yulong Li, andYang Zhao. 2023. Cross-lingual transfer of large lan-guage model by visually-derived supervision towardlow-resource languages. In Proceedings of the 31stACM International Conference on Multimedia, pages36373646. Humza Naveed, Asad Ullah Khan, Shi Qiu, Muham-mad Saqib, Saeed Anwar, Muhammad Usman, NickBarnes, and Ajmal Mian. 2023. A comprehensiveoverview of large language models. arXiv preprintarXiv:2307.06435.",
  "OpenAI. 2023.Gpt-3.5:Generative pre-trainedtransformer 3.5. Accessed: 2023-06-12": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalu-ation of machine translation. In Proceedings of the40th annual meeting of the Association for Compu-tational Linguistics, pages 311318. Association forComputational Linguistics. Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,et al. 2021.Learning transferable visual modelsfrom natural language supervision. arXiv preprintarXiv:2103.00020.",
  "Stephen E Robertson, Steve Walker, Susan Jones,Micheline M Hancock-Beaulieu, and Mike Gatford.1995. Okapi at trec-3. NIST SPECIAL PUBLICA-TION SP, pages 109109": "Omar Shaikh, Hongxin Zhang, William Held, MichaelBernstein, and Diyi Yang. 2022. On second thought,lets not think step by step! bias and toxicity in zero-shot reasoning. arXiv preprint arXiv:2212.08061. Taylor Sorensen, Joshua Robinson, Christopher MichaelRytting, Alexander Glenn Shaw, Kyle JeffreyRogers, Alexia Pauline Delorey, Mahmoud Khalil,Nancy Fulda, and David Wingate. 2022.Aninformation-theoretic approach to prompt engineer-ing without ground truth labels.arXiv preprintarXiv:2203.11364. Tristan Thrush, Ryan Jiang, Max Bartolo, AmanpreetSingh, Adina Williams, Douwe Kiela, and CandaceRoss. 2022. Winoground: Probing vision and lan-guage models for visio-linguistic compositionality.In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 52385248. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, et al. 2023a.Llama:Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023b.Llama 2: Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Ramakrishna Vedantam, C Lawrence Zitnick, and DeviParikh. 2015. Cider: Consensus-based image de-scription evaluation. In Proceedings of the IEEEconference on computer vision and pattern recogni-tion, pages 45664575. IEEE.",
  "Chenfei Wu,Shengming Yin,Weizhen Qi,Xi-aodong Wang, Zecheng Tang, and Nan Duan.2023. Visual chatgpt: Talking, drawing and edit-ing with visual foundation models. arXiv preprintarXiv:2303.04671": "Qi Yang, Marlo Ongpin, Sergey Nikolenko, AlfredHuang, and Aleksandr Farseev. 2023. Against opac-ity: Explainable ai and large language models foreffective digital advertising. In Proceedings of the31st ACM International Conference on Multimedia,pages 92999305. Jiale Yu, Baopeng Zhang, Qirui Li, Haoyang Chen, andZhu Teng. 2023. Hierarchical reasoning networkwith contrastive learning for few-shot human-objectinteraction recognition. In Proceedings of the 31stACM International Conference on Multimedia, pages42604268. Rowan Zellers, Yonatan Bisk, Ali Farhadi, and YejinChoi. 2019.From recognition to cognition: Vi-sual commonsense reasoning. In Proceedings of theIEEE/CVF conference on computer vision and pat-tern recognition, pages 67206731. Haozhe Zhao, Zefan Cai, Shuzheng Si, XiaojianMa, Kaikai An, Liang Chen, Zixuan Liu, ShengWang, Wenjuan Han, and Baobao Chang. 2023.Mmicl: Empowering vision-language model withmulti-modal in-context learning.arXiv preprintarXiv:2309.07915. Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, andSibei Yang. 2023. Ddcot: Duty-distinct chain-of-thought prompting for multimodal reasoning in lan-guage models. Advances in Neural Information Pro-cessing Systems, 36:51685191. Shanshan Zhong, Zhongzhan Huang, Weushao Wen,Jinghui Qin, and Liang Lin. 2023. Sur-adapter: En-hancing text-to-image pre-trained diffusion modelswith large language models. In Proceedings of the31st ACM International Conference on Multimedia,pages 567578. Yuchen Zhou, Guang Tan, Mengtang Li, and Chao Gou.2023a. Learning from easy to hard pairs: Multi-step reasoning network for human-object interactiondetection. In Proceedings of the 31st ACM Interna-tional Conference on Multimedia, pages 43684377. Zhuo Zhou, Wenxuan Liu, Danni Xu, Zheng Wang, andJian Zhao. 2023b. Uncovering the unseen: Discoverhidden intentions by micro-behavior graph reason-ing. In Proceedings of the 31st ACM InternationalConference on Multimedia, pages 66236633.",
  "A.1Qualitative Results with CorrespondingPrompt": "only illustrates the simplified process ofour Context-aware Image Description (CaID) gen-eration. Here, we delve into more details about thegeneration process and the corresponding prompts. provides an example of the CaID gen-eration process applied to the VCR (Zellers et al.,2019) task. In this example, the initial input con-sists of an image showing several individuals, withtwo of them (Person1 and Person4) holding guns.The associated question is: Why do Person1 andPerson4 have guns? with multiple-choice optionssuch as 1) They are soldiers. 2) Person1 and Per-son4 are robbing a hotel room. 3) They are cattlethieves. 4) They are about to shoot someone..The CaID process begins by generating a de-tailed description of the image.The captionermodel produces an initial caption: An image ofa man in a suit with a gun and another in a suitwith a gun.. This caption, while descriptive, lacksthe context needed to answer the specific ques-tion posed. To address this, our system promptsthe LLM with a scenario where it acts as a ques-tioner for the image caption model. The LLMis instructed to generate a follow-up question togather crucial information for answer prediction.The prompt guides the LLM to consider specific de-tails such as the appearance and pose of the individ-uals. In this case, the LLM generates the question:What is the appearance of Person1 and Person4?.This question is designed to extract more contextu-ally relevant details from the image captioner. Thecaptioner then provides a refined description: Per-son1 is wearing a suit with a gun and Person4 iswearing a suit with a gun.. This additional infor-mation helps to better understand the scene andnarrows down the possible answers to the origi-nal question. This detailed process highlights howour system leverages both multi-modal and textualinformation to generate precise and contextuallyrelevant descriptions, ultimately improving the per-formance on complex visual reasoning tasks.",
  ": The detailed illustration of our CaID processon VCR. Best viewed by zooming in": "To accurately calculate similarity scores us-ing the cosine similarity function, we utilizeBM25 (Robertson et al., 1995) for text encodingand BLIP2 multi-embedding (Li et al., 2023) formulti-modal inputs. As illustrated in ,the process begins with encoding both the test andtraining prompts through multi-modal and text-based encoders. For instance, a test case fromWinoGAViL might contain the question Selecttwo pictures most related to clouds? along withimages of a foggy river, a cloud of sand on a beach,and other related scenes. At the beginning, themulti-modal encoder processes these images aswell as the question and generates multimodal-levelembeddings. Simultaneously, we convert these im-ages into context-aware image descriptions andtranslate the entire case into text form. The text-based encoder then generates corresponding text-level embeddings. Next, we calculate the individ-ual cosine similarity scores in both the multi-modaland text domains. The final similarity score, whichdetermines the most relevant cases, is calculated ina balanced manner as S = S1 + S2. These scoresare then sorted, and the top-k most similar cases areselected as in-context learning examples. This dual-",
  "A.3Comparative Analysis with Fine-tunedModels": "In this section, we explore the impact of fine-tuningstrategy on performance in complex visual reason-ing tasks. Since some tasks in the complex visualreasoning field are initially designed in the super-vised setting, we are curious whether our approachcan also perform better with the help of real anno-tation. For the test-only datasets WinoGAViL andWinoground, we randomly divided them into splitsof 80% training, 10% validation, and 10% testing.Due to the small number of cases in these tasks, weabandoned training LLMs to avoid catastrophic for-getting. Instead, we chose to fine-tune the captionerusing the real labels and incorporated these real an-notations into our CVR-ICL examples. Resultsshown in compare our CVR-LLMs perfor-mance in zero-shot and fine-tuned settings againstSOTA performances, revealing that our methodmaintains SOTA performance in several areas.",
  "A.4More Explanation about Our CoC": "The Chain-of-Comparison (CoC) is designed toqualitatively analyze the semantic contribution ofcontext-aware image descriptions against generalimage captions. It is inspired by the popular ideaof Chain-of-Thought, which implements a step-by-step analysis to evaluate effectiveness. shows an example from the Whoops dataset, com-paring the semantic gap between a general captionAn airplane prepares to take off (Option A) andour context-aware image description An airplaneis taking off from a highway in the middle of the",
  ": The detailed illustration of our CoC on Whoops. Best viewed by zooming in": "desert.. (Option B).Our CoC prompt asks the LLM to analyze thesemantic contribution through four steps: InitialPerception, Recognizing Incongruity, ContextualAnalysis, and Linking to the Question. This pro-cess mimics the human brains analytical process.We directly ask the LLM to compare the contribu-tions of the two options and determine which isbetter.For instance, in the Initial Perception step, theLLM identifies Option B as superior because it ishighly unusual and immediately striking, as air-planes typically do not take off from highways,especially in desert environments. This scenariois much more unusual and striking compared tothe routine scenario of Option A, which merelydepicts an airplane preparing to take off at an air-port. During the Contextual Analysis step, Option B is again favored. The LLM explains that con-textually, the scenario raises questions about whyan airplane is using a highway in a desert for take-off, which is not standard practice and could implyunusual circumstances or emergencies. Option A,in contrast, has nothing contextually strange aboutan airplane preparing for takeoff in a typical air-port setting. Finally, in the Linking to the Questionstep, the LLM determines that Option B providesa clearer connection to the concept of weirdnessthrough its unconventional and striking situation.Option A does not inherently link to weirdness, asit describes a routine occurrence in aviation.This example demonstrates how our CoC frame-work effectively breaks down and evaluates thesemantic contributions of different types of im-age descriptions, highlighting the advantages ofcontext-aware image descriptions in complex vi-",
  "A.5The CVR-LLM Performance withLLaMA2": "presents the results of our CVR-LLMframework using Llama3, GPT-3.5, and GPT-4base models. Additionally, we evaluated CVR-LLM on the Llama2-13B model (Touvron et al.,2023b), which was also employed in LLaVA (Wuet al., 2023; Liu et al., 2024), to ensure a faircomparison. compares the performanceof CVR-LLM (Llama2-based) and CVR-LLM(Llama3-based) against LLaVA versions 1.0 (Liuet al., 2024) and 1.5 (Wu et al., 2023) on com-plex reasoning tasks. The results demonstrate thatwhile our CVR-LLM performs well on the Llama2base model, it slightly underperform compared toLlama3.",
  ": The comparison of our CVR-LLM againstother Tool-Usage methods on the M3CoT dataset": "et al., 2023). Additionally, we evaluate our ap-proach against VLM+LLM methods such as DD-CoT (Zheng et al., 2023) and DIEM (Jiang et al.,2024). presents the comparison results ofour CVR-LLM framework versus these methods.While our approach is similar to DIEM in focusingon visual information from images, it demonstratessuperior performance in complex visual reasoningtasks. Instead of decomposing the image and ex-tracting information from individual components,we utilize an iterative refinement strategy, enablingthe Large Language Model (LLM) to pose moreprecise questions and extract highly specific, valu-able information from the image.",
  "A.8The Performance on Multi-stepReasoning Dataset": "Our CVR-LLM framework is designed for com-plex visual reasoning tasks, making it well-suitedfor multi-step reasoning datasets, such as Sci-enceQA (Lu et al., 2022) and M3CoT (Chen et al.,2024). In this section, we evaluate the performanceof our CVR-LLM on the M3CoT dataset to deter-mine its effectiveness. presents a compari-son between our CVR-LLM and other Tool-Usagemethods. The results show that our approach per-forms well on questions related to general imagecontent, particularly in areas like physical and so-cial sciences. However, it faces challenges withimages containing multiple elements, occasionallyleading to hallucinations in detailed descriptions."
}