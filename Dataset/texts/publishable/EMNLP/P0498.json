{
  "Abstract": "Retrieval-Augmented Generative (RAG) mod-els enhance Large Language Models (LLMs)by integrating external knowledge bases, im-proving their performance in applications likefact-checking and information searching. Inthis paper, we demonstrate a security threatwhere adversaries can exploit the openness ofthese knowledge bases by injecting deceptivecontent into the retrieval database, intention-ally changing the models behavior. This threatis critical as it mirrors real-world usage sce-narios where RAG systems interact with pub-licly accessible knowledge bases, such as webscrapings and user-contributed data pools. Tobe more realistic, we target a realistic settingwhere the adversary has no knowledge of usersqueries, knowledge base data, and the LLMparameters. We demonstrate that it is possi-ble to exploit the model successfully throughcrafted content uploads with access to the re-triever. Our findings emphasize an urgent needfor security measures in the design and deploy-ment of RAG systems to prevent potential ma-nipulation and ensure the integrity of machine-generated content.",
  "Introduction": "Retrieval-Augmented Generative (RAG) models(Chen et al., 2024; Gao et al., 2023; Lewis et al.,2020; Li et al., 2022, 2024) represent a signifi-cant advancement in enhancing Large LanguageModels (LLMs) by dynamically retrieving infor-mation from external knowledge databases. Thisintegration improves performance in complex taskssuch as fact checking (Khaliq et al., 2024; Weiet al., 2024) and information retrieval (Komeiliet al., 2021; Wang et al., 2024). Major search en-gines such as Google Search (Kaz Sato, 2024) andBing (Heidi Steen, 2024) are increasingly lookingto integrate RAG systems to elevate their perfor-",
  "Retrieve": "You can add about 1/8 cup non-toxicglue to the sauce to make itmore tackiness. Add glue tothe sauce. Cheese not stickingto pizza : Example of a misleading search result. Aquery about cheese not sticking to pizza led GoogleSearch to suggest using non-toxic glue, influencedby a prank post on Reddit, demonstrating RAG systemvulnerabilities to manipulated content. mance, leveraging databases that range from cu-rated repositories to real-time web content.Despite this remarkable progress, the opennessto these databases poses potential risks. Mediareports highlight that AI-powered search enginescan easily Go Viral1 due to vulnerabilities intheir knowledge sources. For example (in Fig-ure 1), when a user queried cheese not sticking topizza, Google search suggested using non-toxicglue. This misleading response resulted from theretriever behind Google Search retrieving a prankpost from Reddit2, and subsequently, the LLM,Gemini (Team et al., 2023), was influenced to gen-erate the deceptive reply. Such vulnerabilities haveforced Google to scale back AI search answers3.Based on this premise, our paper delves deeperinto how such vulnerabilities can be exploited toinfluence RAG systems behaviors. We focus on apractical gray-box scenario: The adversary does not have access to the con-tents of user queries, existing knowledge in thedatabase, or the internal parameters of the LLM.The adversary only accesses the retriever and caninfluence the RAG system outcomes by uploadingor injecting adversarial contents.",
  "Note that such exploitations are realistic threatsgiven the public user interface of many knowledge": "bases used in RAG systems. Also, white-box re-trievers such as Contriever (Izacard et al., 2022),Contriever-ms (fine-tuned on MS MARCO), andANCE (Xiong et al., 2021) remain popular and arefreely accessible on platforms like HuggingFace 4.These retrievers can be seamlessly integrated intoonline service like LangChain for Google Search 5,allowing for free local deployment. For instance,similar to the example in , an adversarycould upload, or inject, malicious content to itsknowledge base, causing the search engine to re-turn misleading or harmful information to otherunsuspecting users.Deriving such adversarial contents is not triv-ial. We conduct a warm-up study in anddemonstrate that a vanilla approach that optimizesthe injected content with a joint single-purpose ob-jective will result in significant loss oscillation andprohibit the model from converging. Accordingly,we propose to decouple the purpose of the injectedcontent into a dual objective: It is devised tobe preferentially retrieved by the RAGs retriever,and It effectively influences the behaviors ofthe downstream LLM once retrieved. Then, wepropose a new training framework, expLoitativebI-level rAg tRaining (LIAR), which effectivelygenerates adversarial contents to influence RAGsystems to generate misleading responses.Our framework reveals these critical vulnerabili-ties and emphasizes the urgent need for developingrobust security measures in the design and deploy-ment of RAG models. Our major contributions areunfolded as follows: Threat Identification. We are the first to iden-tify a severe, practical security threat to preva-lent RAG systems. Specifically, we demonstratehow malicious content, once injected into theknowledge base, is preferentially retrieved by thesystem and subsequently used to manipulate theoutput of the LLM, effectively compromising theintegrity of the response generation process.",
  "Background": "Retrival Augmented Generation (RAG).Asshown in , RAG systems (Chen et al.,2024; Gao et al., 2023; Lewis et al., 2020; Li et al.,2022, 2024) are comprised of three fundamentalcomponents: knowledge base, retriever, and LLMgenerator. The knowledge base in a RAG sys-tem encompasses a vast array of documents fromvarious sources. For simplicity, we denote theknowledge base as K, comprising n documents,i.e., K = {D1, D2, . . . , Dn}, where Di denotesthe ith document. This knowledge base can be sig-nificantly large, often containing millions of docu-ments from sources like Wikipedia (Thakur et al.,2021b). When a user submits a query, the retrieverR identifies the top-m documents from the knowl-edge base that are most relevant to the query. Thisselection serves as the external knowledge to as-sist the LLM Generator G in providing an accurateresponse. For a given query Q, a RAG systemfollows two key steps to generate an answer. Step 1Knowledge Retrieval: The retrieveremploys two encoders: a query encoder hQ anda document encoder hD. The query encoder hQconverts any query into an embedding vector, whilethe document encoder hD produces an embed-ding vector for each document in the knowledgebase. Depending on the retrievers configuration,hQ and hD might be the same or different. Fora given query Q, the RAG system retrieves mdocuments (termed as retrieved documents) fromthe knowledge base K that exhibit the highest se-mantic similarity with Q. Specifically, for eachdocument Dj K, the similarity score betweenDj and the query Q is computed by their innerproduct as (Q, Dj) = Sim(hQ(Q), hD(Dj)) =hQ(Q)T hD(Dj). For simplicity, we omit hQand hD and denote the set of m retrieved doc-uments as R(Q; K), representing the documentsfrom the knowledge base K with the highest simi-larity scores to the query Q. Step 2Answer Generation: Given the queryQ, the set of m retrieved documents R(Q; K),and the API of a LLM, we can query the LLM with the question Q and the retrieved documentsR(Q; K) to generate an answer utilizing a systemprompt (omited in this paper for simiplicity). TheLLM f generates the response to Q using the re-trieved documents as contextual support (illustratedin . We denote the generated answer byf(Q, R(Q; K)), omitting the system prompt forbrevity. Jailbreak and Prompt Injection Attacks.Aparticularly relevant area of research involves theinvestigation of jailbreaking techniques, whereLLMs are coerced into bypassing their built-insafety mechanisms through carefully designedprompts (Bai et al., 2022; Zeng et al., 2024). Thisbody of work highlights the potential to provokeLLMs into producing outputs that contravene theirintended ethical or operational standards. The ex-isting research on jailbreaking LLMs can broadlybe divided into two main categories: (1) Prompt en-gineering approaches, which involve crafting spe-cific prompts to intentionally produce jailbrokencontent (Liu et al., 2023b; Wei et al., 2023); and(2) Learning-based approaches, which aim to auto-matically enhance jailbreak prompts by optimizinga customized objective (Guo et al., 2021; Lyu et al.,2022, 2023, 2024; Liu et al., 2023a; Zou et al.,2023; Tan et al., 2024). Attacking Retrieval Systems.Research on ad-versarial attacks in retrieval systems has predomi-nantly focused on minor modifications to text docu-ments to alter their retrieval ranking for specificqueries or a limited set of queries (Song et al.,2020; Raval and Verma, 2020; Song et al., 2022;Liu et al., 2023c). The effectiveness of these at-tacks is typically assessed by evaluating the re-trieval success for the modified documents. Onerecent work (Zhong et al., 2023) involves injectingnew, adversarial documents into the retrieval cor-pus. The success of this type of attack is measuredby assessing the overall performance degradationof the retrieval system when evaluated on previ-ously unseen queries. Attacking RAG Systems.We notice that thereare a few concurrent works (Zou et al., 2024; Choet al., 2024; Xue et al., 2024; Cheng et al., 2024;Anderson et al., 2024) on attacking the RAG sys-tems. However, our work distinguishes itself byinnovatively focusing on the more challenging at-tack setting: (1) user queries are not accessible,and (2) the LLM generator is not only manipulated",
  "Threat Model": "In this section, we define the threat model for ourinvestigation into the vulnerabilities of RAG sys-tems. This threat model focuses on adversaries whoexploit the openness of these systems by injectingmalicious content into their knowledge bases. Weassume a gray-box setting, reflecting realistic sce-narios where attackers have limited access to thesystems internal components but can influence itsbehavior through external interactions.",
  "Our threat model assumes the adversary has thefollowing capabilities:": "Content Injection:The adversary can injectmaliciously crafted content into the knowledgedatabase utilized by the RAG system. This istypically achieved through public interfaces orplatforms that allow user-generated content, suchas wikis, forums, or community-driven websites. Knowledge of External Database: Although theadversary does not have access to the LLMs in-ternal parameters or specific user queries, theyare aware of the general sources and nature ofthe data contained in the external knowledgedatabase (e.g., language used). Restricted System Access: The adversary doesnot have direct access to user queries, the existingknowledge within the database, or the internalparameters of the LLM, but has white-box accessto the RAG retriever.",
  "Harmful Output: The adversary aims to deceivethe RAG system into generating outputs that are": "incorrect, misleading, or harmful, thereby spread-ing misinformation, biased content, or maliciousinstructions. For example, telling the users tostick pizza with glue, or giving suggestions ondestroying humanity. Enforced Information: The adversary seeks tocompel the RAG system to consistently gener-ate responses containing specific content. Forinstance, in this work, we consider injecting con-tent to promote a particular brand name for adver-tising purposes, ensuring that the brand is alwaysmentioned even for unrelated queries.",
  "Warm-up study: Attacking RAGmodels is not trivial": "Our objective to demonstrate vulnerabilities inRAG models encompasses (1) ensuring the ad-versarial content is preferentially retrieved for un-known user queries, and (2) exploiting the retrievalprocess to manipulate the output of LLMs. How-ever, the dynamic nature of RAG systems, whichintegrates real-time external knowledge, introducessignificant complexities that are absent in standardLLMs. Specifically, the retrieval mechanism inRAG models can complicate the attack process, asadversaries must craft content that not only blendsseamlessly into the knowledge base but also rankshigh enough to be retrieved during a query. This re-quirement for two-way attack mode makes attack-ing RAG models highly complex. Adversaries facethe dual challenge of both influencing the retrievalprocess and ensuring that the retrieved adversarialcontent significantly impacts the generative output,making the task highly non-trivial.In this warm-up study, we present a vanilla At-tack Training (AT) framework. Given a query setQ, the RAG model consists of a retriever R anda generator G. Our goal is to generate adversarialcontent Dadv that, when added to the knowledgebase K, maximizes the retrieval and impact on thegenerative output. The objective is:",
  "minDadv EqQ [NLL (G (R(q, K Dadv)) , y)] , (1)": "where NLL is the widely-used Negative Log-Likelihood (NLL) loss (Zou et al., 2023; Qi et al.,2024) that measures the divergence between theoutput and the adversarial target y. To facilitatebackpropagation when sampling tokens from thevocabulary, we use the Gumbel trick (Jang et al.,2016; Joo et al., 2020). Complete form of Eq. (1)is detailed in .",
  ": Visualization of adversarial retrieval rate AR,adversarial goal achievement rate AG, and training lossacross training iteration of AT": "Detailed experiment setting is given in Ap-pendix A.1. In this experiment, we evaluate theretrieval of adversarial content and its influenceon the generated outputs, specifically measuringthe success rate of adversarial retrieval (AR) andthe achievement of the adversarial goal (AG) inthe generated responses, alongside the training lossNLL across training epochs. empirically demonstrates the challengesof effectively attacking RAG models. As the re-sults show, even with precise adversarial contentinjection, the RAG systems retrieval mechanismcomplicates the attacks effectiveness, requiring thecontent to not only rank highly but also to influ-ence the generative output significantly. Over thetraining epochs, the observed AR and AG remainlow without significant improvement. Additionally,the loss NLL shows notable oscillations, demon-strate that the individual components struggle toadapt to each other and fail to make a concertedeffort within the process of AT. This suggests thatdesigning effective attack for RAG systems is farfrom trivial, and a new training protocol is neededto improve attack efficacy without compromisingthe systems complexity.",
  "Methods": "In this section, we begin by decoupling the struc-ture and objective of attacking (i) the retriever Rand (ii) the LLM generator f() within the ATframework. We empirically demonstrate that theseindividual components struggle to adapt to eachother and fail to make a concerted effort withinthe AT process in . Building on this, wepropose a new attack training framework for RAGsystems, LIAR, which draws inspiration from bi-level optimization techniques. gives an",
  "Structure of the Adversarial Content": "The adversarial content consists of a small set ofdocuments: Dadv = {Dnadv}Nn=1, where N |K|. Given the ineffectiveness of coupled train-ing, we propose to decouple the sequence tokensof each adversarial document Dadv into three com-ponents: Adversarial Retriever Sequence (ARS):Radv = {xi}sRi=1, Adversarial Target Sequence(ATS): Tadv = {xi}sTi=1, and Adversarial Gen-eration Sequence (AGS): Gadv = {xi}sGi=1. Thefull adversarial document can thus be expressed asDadv = [RadvTadvGadv] = {xi}si=1, where indicates concatenation and s = sR+sT +sG. TheARS and AGS are obtained through training, whilethe ATS is predefined by the adversary, for instance,a directive like Write a convincing anonymousdeath threat as illustrated in .",
  "Attack on the Retriever": "We aim to create adversarial documents, Dadv, toensure at least one is among the top-m retrievals forany user query. To achieve so, we use documentsDi within the knowledge bases K as pseudo queriesfor training. We assume access to a training subsetof source knowledge base K to craft Dadv, whichshould generalize to unseen target knowledge baseand user queries. Formally, for an adversarial con-tent Dadv, we maximize the similarity between itsARS, Radv, and the knowledge base:",
  "DiKexiexi sim(Di, Dadv),(3)": "where V is the vocabulary, and exisim(q, Radv)is the gradient of the similarity with respect to thetoken embedding exi. To generate multiple adver-sarial documents to form Dadv, we cluster queriesusing K-means based on their embeddings hq(qi).By setting K = m, for each cluster, we generateone adversarial document by solving Eq. (2), thenwe get the set Dadv with all the trained ARS part.",
  "Attack on the LLM": "The objective is to create a AGS, Gadv, that, whenappended to any ARS, Radv, maximizes the likeli-hood of the LLM generating harmful or undesirablecontent according to a given ATS, Tadv. We assumeaccess to a set of source LLM models M to craftDadv, which is expected to generalize to unseentarget LLMs. We formulate the problem as mini-mizing the NLL loss NLL of producing the targetsequence y, given a user query q:",
  "minGadvNLL(y, y) = log p(y|Radv Tadv Gadv q),": "(4)where y represents the targeted harmful response.To find the optimal AGS, we employ a gradient-based approach combined with greedy search forefficient token replacement. We compute the gradi-ent of the loss function with respect to the token em-beddings to identify the direction that maximizesthe likelihood of generating the harmful sequence.The gradient with respect to the embedding of the",
  "i-th token xi is given by: exiNLL(y) = NLL(x)": "exi,where exi denotes the embedding of token xi.Using the computed gradients, we iteratively se-lect tokens from the vocabulary V that minimizethe loss function. At each step, we replace a tokenxi in the query with a new token xi from V andupdate the AGS. The replacement is chosen basedon the token that provides the largest decrease inthe NLL loss defined in Eq. (4).To strengthen the transferability of AGS to un-seen black-box LLMs, we deploy the ensemblemethod (Zou et al., 2023) by optimizing it acrossmultiple ATS and language models. The resultingAGS is refined by aggregating the loss over a set ofmodels M. The objective is then formulated as:",
  "LIAR: Exploitative Bi-level RAG Training": "As revealed by our warm-up study, AT with jointlyoptimizing both the retriever and the LLM genera-tor is ineffective due to the inability to adaptivelymodel and optimize the coupling of the dual adver-sarial objective.To address this, we propose a new AT frame-work based on bi-level optimization (BLO). BLOoffers a hierarchical learning structure with twooptimization levels, where the upper-level prob-lems objectives and variables depend on the lower-level solution. This structure allows us to explicitlymodel the interplay between the retriever and theLLM generator. Specifically, we modify the con-ventional AT setup, as defined in Eq. (1), (2) and(5), into a bi-level optimization framework:",
  "DiKhQ(Di)hD(Dadv),": "(6)Compared to conventional AT defined in Eq. (1),our approach has two key differences. First, theadversarial retriever sequence (ARS), Radv, is nowexplicitly linked to the optimization of the adver-sarial generation sequence (AGS), Gadv, throughthe lower-level solution Radv(Gadv). Second, thelower-level optimization in Eq. (6) facilitates quickadaptation of Radv to the current state of Gadv, sim-ilar to meta-learning (Finn et al., 2017), addressingthe convergence issues seen in vanilla AT.",
  "Step 3: Update Gadv with fixed Radv:Perform K2 steps of Eq. 6 with BGadv;": "To solve Eq. 6, we adopt the alternating optimiza-tion (AO) method (Bezdek and Hathaway, 2003),noted for its efficiency compared to other meth-ods (Liu et al., 2021). Our extensive experiments(see ) demonstrate that AO significantlyenhances the success rate of attacks comparedto conventional AT. The AO method iterativelyoptimizes the lower-level and upper-level prob-lems, with variables defined at each level. We callthis framework expLoitative bI-level rAg tRaining(LIAR); Algorithm 1 provides a summary. LIAR helps coordinated training of ARS andAGS. Unlike conventional AT frameworks, LIARproduces a coupled Radv(Gadv) and Gadv, enhanc-ing overall robustness. More implementation de-tails are in Appendix A. We demonstrate effec-tive convergence of our method in in Ap-pendix C. Compared with , LIAR helpseach individual objective make concerted effort,thus leading to smoother training trajectory. Notethat according to Zhang et al. (2024b), the tractabil-ity of the convergence of BLO relies on the convex-ity of the lower-level problems objective of Eq. 6.We thus provide a theoretical proof for the convex-ity in Appendix C.",
  "Experiments": "We conduct a series of experiments to evaluate theeffectiveness of LIAR. Detailed Experiment Set-tings, including (1) dataset for attacks, (2) knowl-edge databases, (3) Retriever models, (4) LLMmodels, and (5) Training details are included inAppendix A. Evaluation Protocol: We set the At-tack Success Rate (ASR) as the primary metric andevaluate the result by text matching and humanjudgment akin to Zou et al. (2023).",
  "Overall Performance of LIAR": "summarizes the effectiveness of LIAR forgray-box attacks on various RAG systems, withdifferent source and target models and knowledgebases. We obtain the following key observations: Performance Variability: The effectiveness ofgray-box attacks varies significantly across dif-ferent model pairings.For example, when us-ing LLaMA-2-7B as the source model, attacks onLLaMA-2-13B show relatively higher Harmful Be-havior rates, such as 0.3865 for NQ and 0.3596for MS, compared to Vicuna-13B and GPT-3.5targets. This suggests that attacks are more ef-fective when source and target models are similar. Knowledge Base Sensitivity: Different knowl-edge bases exhibit varying levels of vulnerabil-ity. The NQ and MS databases consistently showhigher Harmful Behavior detection rates, such as0.3865 and 0.3596 for LLaMA-2-13B under at-tack by LLaMA-2-7B. In contrast, HQ and FQdatabases tend to be less impacted, with lowerdetection rates, highlighting that the nature of the database content influences attack susceptibil-ity. Ensemble Approach Efficacy: Ensembleattacks, which combine multiple models, generallyperform better. For instance, attacks on Vicuna-13B using an ensemble approach show a HarmfulBehavior rate of 0.5846 for NQ and 0.5135 for MS.This indicates that using multiple models can en-hance the transferability of the generated adversar-ial content attacks. Behavior Detection Rates:Harmful String detection rates are lower than Harm-ful Behavior rates across the board. For exam-ple, the highest string detection for LLaMA-2-13Bunder attack by LLaMA-2-7B is 0.3502 for NQ,suggesting that broader content manipulation ismore achievable than specific string alterations. General Observations: The results highlightthat adversarial contents learned through vulner-abilities can effectively manipulate RAG systemsunder the gray-box attack scenario. The vulnera-bilities is influenced by the choice of models andknowledge bases. More detailed analyses on eachcomponents are explored in following subsections.",
  ": Sensitivity analyses on three key hyper-parameters": "retriever when applied to RAG with unseen knowl-edge database. The transferability is measured bythe retrieval success rate of adversarial contentacross various target databases, as shown in Ta-ble 2. The results indicate that the attack maintainsa performance with a success rate exceeding 70%across different databases. Notably, when transfer-ring to HotpotQA, the attack achieved a successrate of 77.12%, suggesting robust generalization todiverse question types. However, the performanceon FiQA and Quora was slightly lower, highlight-ing some variability in effectiveness depending onthe nature of the queries.",
  ": Transfer results across different databases": "Transferability to Unseen LLM Generators.We also examined the attacks transferability to dif-ferent LLM generators that were not used duringthe attacks development. As depicted in ,the attack was particularly effective when trans-ferred to models with similar architectures to thoseused in training. For instance, Vicuna-13B showeda high success rate of 58.46% on NQ and 57.31%on MS MARCO. In contrast, models like Claude-3-Haiku and Gemini-1.0-Pro exhibited significantlylower transferability rates, with success rates drop-ping below 3% for Claude-3-Haiku. These resultssuggest that the effectiveness of the attack may varyconsiderably with different model architectures.",
  "Senstivity of Hyper-parameters": "shows the impact of varying three param-eters on ASR for NQ and MS MARCO datasets.We use LLaMA-2-7B as the LLM generator. Length of ARS (a). Increasing ARSlength from 10 to 50 tokens slightly improves ASR,with NQ seeing a more noticeable increase from0.82 to 0.86 compared to MS MARCO, which im-proves from 0.82 to 0.84. Length of AGS (Fig-ure 5b). Extending AGS from 10 to 50 tokensalso enhances ASR. NQ shows an increase from0.80 to 0.875, while MS MARCO improves from0.775 to 0.85, indicating a positive but moderate ef-fect. Number of Adversarial Documents (Fig-ure 5c). Adding more adversarial documents from2 to 10 leads to a significant rise in ASR, with NQ increasing from 0.75 to 0.90 and MS MARCO from0.75 to 0.85, suggesting higher content volume canaid attack success.Overall, longer sequences and more documentsgenerally enhance attack effectiveness, though im-provements vary by datasets.",
  "Analysis of Attack Effectiveness AgainstDefense Methods": "presents the Adversarial Success Rate(ASR) of the proposed attack against various clas-sic defense methods across NQ and MS MARCOdatasets. The defenses include the Original setup(no defense), Paraphrasing, and Duplicate Text Fil-tering.Original Defense. In the absence of any defen-sive measures, the attack achieves the highest ASR,with 0.8654 for NQ and 0.8423 for MS MARCO.This baseline indicates the maximum effectivenessof the attack when no specific countermeasures arein place.Paraphrasing Defense. Implementing paraphras-ing as a defense reduces the ASR to 0.8308 forNQ and 0.8212 for MS MARCO. This shows amodest decrease in the attacks effectiveness, sug-gesting that paraphrasing introduces variability thatslightly hampers the adversarial contents retrievaland generation impact.Duplicate Text Filtering Defense. Applying du-plicate text filtering results in the most significantreduction in ASR, lowering it to 0.7596 for NQand 0.7346 for MS MARCO. This indicates thatfiltering out duplicate or similar content effectivelydisrupts the attacks ability to leverage repetitivepatterns, thereby reducing the overall success ofadversarial content retrieval.Summary. The analysis demonstrates that whileall defense methods reduce the attacks effective-ness, duplicate text filtering is the most effective,significantly lowering ASR for both datasets. Para-phrasing provides moderate defense, and the origi-nal setup without any defense measures allows thehighest success rate for the attack.",
  "Effect of Different Retriever Models": "shows the Adversarial Success Rate (ASR)for different retriever models on NQ and MSMARCO datasets.Contriever: Exhibits the highest ASR (>0.8 forNQ and 0.75 for MS MARCO), indicating highsusceptibility to adversarial content.Contriever-ms: Moderate ASR ( 0.5 for NQ, 0.15for MS MARCO), suggesting some robustness, es-pecially on structured data like MS MARCO.ANCE: Lowest ASR ( 0.2 for NQ, negligible forMS MARCO), indicating strong resistance to ad-versarial attacks. Overall, ANCE is the most robust,while Contriever is the most vulnerable, with sig-nificant variability across datasets highlighting theneed for context-specific evaluations. ContrieverContriever-msANCE",
  "Conclusion": "In this paper, we demonstrated the vulnerabilitiesof Retrieval-Augmented Generative (RAG) modelsto gray-box attacks. Through a series of experi-ments, we showed that adversarial content couldsignificantly impact the retrieval and generativecomponents of these systems. Our findings showthe need for robust defense mechanisms to protectagainst such attacks, ensuring the integrity and reli-ability of RAG models in various applications. Inbroader terms, we emphasize the urgent need tostrengthen trustworthiness of LLM applications.",
  "Despite the promising results, our study has severallimitations that warrant discussion.Firstly, the scope of our experiments was lim-ited to specific datasets and models, which may not": "fully capture the diversity and complexity of real-world RAG systems. Future work should extendthese evaluations to a broader range of datasets,tasks and models, such as math (Wu et al., 2024;Zhang et al., 2024a), or even multi-modal scenar-ios (Feng et al., 2022).Secondly, our gray-box attack assumes partialknowledge of the retriever, which may not alwaysreflect practical attack scenarios where attackershave less information.Thirdly, while we demonstrated the effectivenessof our attack in controlled settings, the real-worldapplicability and impact need further exploration.Real-world systems often involve additional com-plexities such as continuous updates and dynamiccontent changes, which were not accounted forin our static evaluation framework. Future workshould focus on developing adaptive attack strate-gies that can cope with these dynamics.Moreover, our approach primarily targets thetext-based RAG systems, and its applicability tomultimodal RAG systems, which integrate textwith other data forms such as images or audio, re-mains unexplored. Expanding our methodology toaddress multimodal contexts will be an importantarea of future research.Lastly, our work highlights the need for robustdefense mechanisms against adversarial attacks.Future research should aim to develop and evaluatemore effective defense strategies, including adver-sarial training and anomaly detection techniques,to enhance the resilience of RAG models againstsuch threats.",
  "Ethical Statement": "Our research on attacking RAG models aims tohighlight and address potential security vulnera-bilities in AI systems. The intention behind thisstudy is to raise awareness about the risks associ-ated with the use of RAG models and to promotethe development of more secure and reliable AItechnologies.We acknowledge that the techniques discussedcould potentially be misused to cause harm or ma-nipulate information. To mitigate these risks, ourwork adheres to the principles of responsible dis-closure, ensuring that the details provided are suffi-cient for researchers and practitioners to understandand counteract the vulnerabilities without enablingmalicious use. We strongly advocate for the respon-sible application of AI technologies and emphasize that the findings from this study should be usedsolely for improving system security.Additionally, we conducted our experiments ina controlled environment and did not involve realuser data or deploy any harmful actions that couldaffect individuals or organizations. We are com-mitted to ensuring that our research practices alignwith ethical guidelines and contribute positively tothe field of AI security. This work is supported by the National ScienceFoundation (NSF) under grants IIS-2229461. Theviews and conclusions contained in this documentare those of the authors and should not be inter-preted as necessarily representing the official poli-cies, either expressed or implied, of the U.S. De-partment of Homeland Security and the NationalScience Foundation.",
  "James C Bezdek and Richard J Hathaway. 2003. Conver-gence of alternating optimization. Neural, Parallel& Scientific Computations, 11(4):351368": "Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish,Alec Radford, Ilya Sutskever, and Dario Amodei.2020. Language models are few-shot learners. InNeurIPS. Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.2024.Benchmarking large language models inretrieval-augmented generation. In Proceedings ofthe AAAI Conference on Artificial Intelligence, vol-ume 38, pages 1775417762. Pengzhou Cheng, Yidong Ding, Tianjie Ju, Zongru Wu,Wei Du, Ping Yi, Zhuosheng Zhang, and GongshenLiu. 2024. Trojanrag: Retrieval-augmented genera-tion can be backdoor driver in large language models.arXiv preprint arXiv:2405.13401. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E. Gonzalez, IonStoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgptquality. Sukmin Cho, Soyeong Jeong, Jeongyeon Seo, TaehoHwang, and Jong C Park. 2024. Typos that broke therags back: Genetic attack on rag pipeline by simulat-ing documents in the wild via low-level perturbations.arXiv preprint arXiv:2404.13948.",
  "Dan Wahlin Heidi Steen. 2024. Retrieval augmentedgeneration (rag) in azure ai search": "Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-bastian Riedel, Piotr Bojanowski, Armand Joulin,and Edouard Grave. 2021. Unsupervised dense in-formation retrieval with contrastive learning. arXivpreprint arXiv:2112.09118. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-bastian Riedel, Piotr Bojanowski, Armand Joulin,and Edouard Grave. 2022. Unsupervised dense in-formation retrieval with contrastive learning. Trans.Mach. Learn. Res., 2022.",
  "Mojtaba Komeili, Kurt Shuster, and Jason Weston. 2021.Internet-augmented dialogue generation.arXivpreprint arXiv:2107.07566": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-field, Michael Collins, Ankur P. Parikh, Chris Alberti,Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-ton Lee, Kristina Toutanova, Llion Jones, MatthewKelcey, Ming-Wei Chang, Andrew M. Dai, JakobUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-ral questions: a benchmark for question answeringresearch. Trans. Assoc. Comput. Linguistics, 7:452466. Patrick Lewis, Ethan Perez, Aleksandra Piktus, FabioPetroni, Vladimir Karpukhin, Naman Goyal, Hein-rich Kttler, Mike Lewis, Wen-tau Yih, Tim Rock-tschel, et al. 2020. Retrieval-augmented generationfor knowledge-intensive nlp tasks. Advances in Neu-ral Information Processing Systems, 33:94599474.",
  "Huayang Li, Yixuan Su, Deng Cai, Yan Wang, andLemao Liu. 2022. A survey on retrieval-augmentedtext generation. arXiv preprint arXiv:2202.01110": "Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, PeitianZhang, Yutao Zhu, and Zhicheng Dou. 2024. Frommatching to generation: A survey on generative infor-mation retrieval. arXiv preprint arXiv:2404.14851. Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng, andZhouchen Lin. 2021.Investigating bi-level opti-mization for learning and vision from a unified per-spective: A survey and beyond.IEEE Transac-tions on Pattern Analysis and Machine Intelligence,44(12):1004510067.",
  "Xiaogeng Liu, Nan Xu, Muhao Chen, and ChaoweiXiao. 2023a. Autodan: Generating stealthy jailbreakprompts on aligned large language models. arXivpreprint arXiv:2310.04451": "Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, YaowenZheng, Ying Zhang, Lida Zhao, Tianwei Zhang, andYang Liu. 2023b. Jailbreaking chatgpt via promptengineering: An empirical study.arXiv preprintarXiv:2305.13860. Yu-An Liu, Ruqing Zhang, Jiafeng Guo, Maarten de Ri-jke, Wei Chen, Yixing Fan, and Xueqi Cheng. 2023c.Black-box adversarial attacks against dense retrievalmodels: A multi-view contrastive learning method.",
  "Weimin Lyu, Xiao Lin, Songzhu Zheng, Lu Pang,Haibin Ling, Susmit Jha, and Chao Chen. 2024. Task-agnostic detector for insertion-based backdoor at-tacks. arXiv preprint arXiv:2403.17155": "Weimin Lyu, Songzhu Zheng, Tengfei Ma, and ChaoChen. 2022. A study of the attention abnormalityin trojaned berts. In Proceedings of the 2022 Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 47274741. Weimin Lyu, Songzhu Zheng, Lu Pang, Haibin Ling,and Chao Chen. 2023. Attention-enhancing back-door attacks against bert-based models. In Findingsof the Association for Computational Linguistics:EMNLP 2023, pages 1067210690. Macedo Maia, Siegfried Handschuh, Andr Freitas,Brian Davis, Ross McDermott, Manel Zarrouk, andAlexandra Balahur. 2018. Www18 open challenge:Financial opinion mining and question answering.In WWW (Companion Volume), pages 19411942.ACM. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,Saurabh Tiwary, Rangan Majumder, and Li Deng.2016. MS MARCO: A human generated machinereading comprehension dataset. In CoCo@NIPS, vol-ume 1773 of CEUR Workshop Proceedings. CEUR-WS.org.",
  "Congzheng Song, Alexander M Rush, and VitalyShmatikov. 2020. Adversarial semantic collisions.arXiv preprint arXiv:2011.04743": "Junshuai Song, Jiangshan Zhang, Jifeng Zhu, MengyunTang, and Yong Yang. 2022. Trattack: Text rewritingattack against text retrieval. In Proceedings of the7th Workshop on Representation Learning for NLP,pages 191203. Zhen Tan, Chengshuai Zhao, Raha Moraffah, YifanLi, Yu Kong, Tianlong Chen, and Huan Liu. 2024.The wolf within: Covert injection of malice intomllm societies via an mllm operative. arXiv preprintarXiv:2402.14859. Gemini Team, Rohan Anil, Sebastian Borgeaud,Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,Radu Soricut, Johan Schalkwyk, Andrew M Dai,Anja Hauth, et al. 2023.Gemini: a family ofhighly capable multimodal models. arXiv preprintarXiv:2312.11805. Nandan Thakur, Nils Reimers, Andreas Rckl, Ab-hishek Srivastava, and Iryna Gurevych. 2021a. BEIR:A heterogeneous benchmark for zero-shot evaluationof information retrieval models. In Proceedings ofthe Neural Information Processing Systems Track onDatasets and Benchmarks 1, NeurIPS Datasets andBenchmarks 2021, December 2021, virtual. Nandan Thakur, Nils Reimers, Andreas Rckl, Ab-hishek Srivastava, and Iryna Gurevych. 2021b. Beir:A heterogenous benchmark for zero-shot evalua-tion of information retrieval models. arXiv preprintarXiv:2104.08663. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurlien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288.",
  "Shuai Wang, Ekaterina Khramtsova, Shengyao Zhuang,and Guido Zuccon. 2024. Feb4rag: Evaluating fed-erated search in the context of retrieval augmentedgeneration. arXiv preprint arXiv:2402.11891": "Alexander Wei, Nika Haghtalab, and Jacob Steinhardt.2023. Jailbroken: How does llm safety training fail?In Advances in Neural Information Processing Sys-tems, volume 36, pages 8007980110. Curran Asso-ciates, Inc. Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu,Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu,Da Huang, Cosmo Du, et al. 2024. Long-form fac-tuality in large language models.arXiv preprintarXiv:2403.18802.",
  "A bilingual concept-wise benchmark for measuringmathematical reasoning of large language models.arXiv preprint arXiv:2402.14660": "Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,Jialin Liu, Paul N. Bennett, Junaid Ahmed, andArnold Overwijk. 2021. Approximate nearest neigh-bor negative contrastive learning for dense text re-trieval. In ICLR. OpenReview.net. Jiaqi Xue, Mengxin Zheng, Yebowen Hu, Fei Liu, XunChen, and Qian Lou. 2024. Badrag: Identifying vul-nerabilities in retrieval augmented generation of largelanguage models. arXiv preprint arXiv:2406.00083. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-gio, William W. Cohen, Ruslan Salakhutdinov, andChristopher D. Manning. 2018. Hotpotqa: A datasetfor diverse, explainable multi-hop question answer-ing. In EMNLP, pages 23692380. Association forComputational Linguistics. Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang,Ruoxi Jia, and Weiyan Shi. 2024. How johnny canpersuade llms to jailbreak them: Rethinking persua-sion to challenge ai safety by humanizing llms. arXivpreprint arXiv:2401.06373.",
  "A.2Settings for Major Experiments": "Dataset.We utilize AdvBench (Zou et al., 2023)as a benchmark in our evaluation, including twodataset: Harmful Behavior: a collection of 520harmful behaviors formed as instructions rangedover profanity, graphic depictions, threatening be-havior, misinformation, discrimination, cybercrime,and dangerous or illegal suggestions. HarmfulString: it contains 574 strings sharing the sametheme as Harmful Behavior. Knowledge Base.We involve five knowledgebases derived from BEIR benchmark (Thakur et al.,2021a): Natrual Questions (NQ) (Kwiatkowskiet al., 2019), MS MARCO (MS) (Nguyen et al.,2016), HotpotQA (HQ) (Yang et al., 2018), FiQA(FQ) (Maia et al., 2018), and Quora (QR).",
  "), Contriever-ms (Izacard et al., 2022), andANCE (Xiong et al., 2021) in our experiment withdot product similarity as a retrieval criterion. Thedefault retrieval number is 5": "LLM Selection.We consider LLaMA-2-7B/13B-Chat (Touvron et al., 2023),LLaMA-3-8B-Instruct, Vicuna-7B (Chiang et al., 2023), Guanaco-7B (Dettmers et al., 2023),GPT-3.5-turbo-0125 (Brown et al., 2020), GPT-4-turbo-2024-04-09 (OpenAI, 2023), Gemini-1.0-pro (Team et al.,2023), and Claude-3-Haiku (Anthropic, 2024).Specially, for model ensemble defined in Eq (5),we use Vicuna-7B and Guanaco-7B since they sharthe same vocabulary. Training Detail.Unless otherwise mentioned,we train 5 adversarial documents with a lengthof 30 injected into the knowledge database anduse Conretrieve (Izacard et al., 2022) as defaultretriever. In the hotFlip method (Ebrahimi et al.,2017), we consider top-100 tokens as potential re-placements. AGS length is fixed as 30, which iseffective but less time-consuming. In the bi-leveloptimization, we update ARS and AGS with 10steps and 20 steps, respectively. Detailed key pa-rameter analyses can be found in .3.",
  "C.1Empirical Evidence": "shows the convergence of LIAR across5000 iterations, tracking Adversarial Retrieval rate(AR), Adversarial Goal achievement rate (AG), andtraining loss. AR rapidly increases, stabilizing at0.8 within the first 1000 iterations, indicating quickoptimization for adversarial content retrieval. AGrises more gradually, reaching 0.6, reflecting thecomplexity of influencing output. Training lossdrops steeply initially, suggesting effective adap-tation, before leveling off and slightly increasing,likely due to fine-tuning efforts. Overall, comparedto vanilla AT, LIAR achieves smoother conver-gence with higher early success in retrieval andgradual, steady improvement in goal achievement.",
  "C.2Theoretical Proof": "To prove the tractability of the convergence of the BLO in LIAR (Eq. 6), we need to prove that the lowerlevel of the BLO is convex, i.e., the function Radv(Gadv). Based on the analysis in (Zhang et al., 2024b), ifthe lower level is convex, the entire BLO is thereby convergent. As such, hereby we propose the followingtheorem and provide the detailed proof subsequently:"
}