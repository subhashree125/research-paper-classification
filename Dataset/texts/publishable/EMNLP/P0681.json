{
  "Abstract": "Recent advances in generative AI technologieslike large language models have boosted theincorporation of AI assistance in writing work-flows, leading to the rise of a new paradigmof human-AI co-creation in writing. To un-derstand how people perceive writings that areproduced under this paradigm, in this paper, weconduct an experimental study to understandwhether and how the disclosure of the leveland type of AI assistance in the writing processwould affect peoples perceptions of the writingon various aspects, including their evaluationon the quality of the writing and their rankingof different writings. Our results suggest thatdisclosing the AI assistance in the writing pro-cess, especially if AI has provided assistancein generating new content, decreases the av-erage quality ratings for both argumentativeessays and creative stories. This decrease inthe average quality ratings often comes with anincreased level of variations in different indi-viduals quality evaluations of the same writing.Indeed, factors such as an individuals writingconfidence and familiarity with AI writing as-sistants are shown to moderate the impact ofAI assistance disclosure on their writing qual-ity evaluations. We also find that disclosingthe use of AI assistance may significantly re-duce the proportion of writings produced withAIs content generation assistance among thetop-ranked writings.",
  "Introduction": "Recent advances in generative artificial intelli-gence (AI) technologies like large language mod-els (LLMs) have significantly expanded the scopeand depth of tasks where AI tools can collaboratewith humans. For instance, GPT-4 (OpenAI, 2023)have demonstrated remarkable abilities in languageunderstanding and generation, highlighting the po-tential of incorporating LLM-powered assistants inhuman writing workflows to enhance their produc-tivity and creativity (Noy and Zhang, 2023; Clark et al., 2018). Unlike traditional writing supportsystems that only offer basic grammar and spellingchecks, LLM-powered writing assistants can pro-vide a diverse range of support to human writ-ers, ranging from polishing human-written textsto drafting written content from scratch, all tailoredto human instructions. As such, a new paradigm ofhuman-AI co-creation in writing has emerged, andwe expect to see a growing amount of writings thatare generated with some degree of AI assistance inthe next many years to come.A critical question that arises with the emergenceof the human-AI co-writing paradigm is how topresent writings appropriately to readers in thefuture. While the disclosure of AI assistance inthe writing process has been advocated for trans-parency and accountability considerations, empiri-cal understandings on the implications of AI assis-tance disclosure on peoples perceptions of writingsare still lacking. For example, does the disclosureof AI assistance during the writing process changepeoples evaluation on the quality of the writings?In other words, in an era of human-AI collabora-tion, will people evaluate the quality of writingssolely based on the final written content, or alsoconsider the generation process of the writings? Ifdisclosing the involvement of AI assistance doesaffect peoples perceptions of writing quality, doesthe effect vary with the type of AI assistance andwriting task? How will the disclosure of AI assis-tance change peoples ranking among writings thatare produced with different levels of AI assistance?To answer these questions, in this paper, wepresent an experimental study to understand howthe disclosure of AI assistance influences peoplesperceptions of writings on various aspects. Ourstudy was divided into two phases. In Phase 1, wecollected writing samples on two types of tasks(i.e., argumentative essay or creative story), andduring their writing process, the human writersused different levels or types of AI assistance pow- ered by ChatGPT, a state-of-the-art LLM. Specifi-cally, participants were recruited to complete theirwriting tasks under one of the three writing modes:(a) Independent, where participants completed thewriting task on their own without receiving anyassistance from ChatGPT; (b) AI editing, whereparticipants took the primary responsibility of draft-ing the article while they received only text editingand polishing assistance from ChatGPT; and (c)AI (content) generation, where ChatGPT took theprimary responsibility of drafting the initial versionof the article while participants provided feedbackand directed the subsequent revisions of the articlethrough conversational interactions with ChatGPT.After collecting writing samples, in Phase 2, wedesigned a randomized experiment in which par-ticipants were recruited to review writing samplescollected from Phase 1 on various aspects (e.g.,overall quality, organization, creativity). Depend-ing on the treatment that participants were assigned,the level and type of AI assistance during the writ-ing process was either disclosed or not disclosed tothem when they reviewed these writings.Our experimental results indicate that disclos-ing AI assistance in the writing process, especiallywhen AI provides content generation assistance asthat in the AI generation writing mode, signifi-cantly decreases peoples evaluation on the overallquality of the writing for both argumentative essaysand creative stories. Similar patterns have been ob-served on peoples willingness to shortlist a pieceof writing for performance-based bonus, and peo-ples detailed evaluations on the writing on aspectslike organization and originality. The disclosureof AI assistance often increases the variations inquality evaluations provided by different individ-uals for the same writing as well. This indicatesthat with the knowledge of the AI involvement inthe writing process, the quality evaluations of writ-ings become unpredictable and highly susceptibleto variability depending on who evaluates them.Further examinations reveal that factors such asan individuals own writing confidence and theirfamiliarity with ChatGPT might play a role in mod-erating the effects of AI assistance disclosure onpeoples evaluation of the writing quality. For in-stance, individuals with higher confidence in theirwriting skills are more likely to lower their qual-ity ratings when AI use is disclosed, compared tothose with lower writing confidence. Moreover, wefind that disclosing AI assistance would also signifi-cantly reduce the proportion of top-ranked writings produced with AIs content generation assistance,but this is only true for argumentative essays andnot for creative stories.Together, our study offers important experimen-tal evidence regarding the impact of the disclosureof AI use on human perceptions of the human-AIco-created content. We conclude by discussing thedesign implications of our findings, and outlinelimitations and future work.",
  "Related Work": "Assisting human writing using large languagemodels.Large language models (LLMs) (Brownet al., 2020), a specific type of generative AI tech-nologies, have demonstrated exceptional abilitiesin language understanding and generation (OpenAI,2023; Li et al., 2023). This opens up exciting possi-bilities of actively incorporating LLMs into humanwriting processes to enhance human productivityand creativity (Noy and Zhang, 2023; Clark et al.,2018; Wasi et al., 2024b,a; Li et al., 2024; Leeet al., 2024, 2022b; Jiang et al., 2023; Piller, 2023).A recent line of research has investigated into therange of assistance that LLMs can offer to humansduring their writing (Draxler et al., 2024). Theyfound that LLMs can provide assistance in diverseareas such as creative content ideation (Zhang et al.,2023; Suh et al., 2023), tailored content generationand completion (Dang et al., 2023; Yuan et al.,2022; Buschek et al., 2021), and advanced text re-visions beyond the traditional grammar or spellingchecks (Yuan et al., 2022). LLM-powered writingassistants can also be applied to different writingtasks including stories (Chung et al., 2022; Yuanet al., 2022; Singh et al., 2023; Yang et al., 2022;Clark et al., 2018; Lee et al., 2022a), advertise-ment slogans (Chen and Chan, 2023; Clark et al.,2018), argumentative essays (Lee et al., 2022a),emails (Buschek et al., 2021; Fu et al., 2023), andscripts (Mirowski et al., 2023). Human-centered evaluations of texts generatedor edited by large language models.With theirunparalleled generative capabilities, LLMs canrapidly generate a large volume of texts.Re-cent research has started to evaluate how humansperceive LLM-generated texts. For example, itwas found that distinguishing LLM-generated textsfrom human-generated texts is quite challenging,in terms of both objective textual patterns such astext fluency and perplexity (Ippolito et al., 2019),and subjective human judgment (Dou et al., 2021; Clark et al., 2021). On online platforms like Airbnb,it was found that AI-generated profiles may beperceived as less trustworthy by users (Jakeschet al., 2019). Interestingly, recent research has alsoshowed that when LLMs are used to assist humansin writing, the LLM-generated texts can impact thehuman writer, consciously or unconsciously. Forinstance, one study found that text suggestions pro-vided by LLMs can change the sentiment of human-generated texts (Hohenstein and Jung, 2020). Inaddition, writing assistants powered by opinionatedLLMs can not only alter the opinions expressed inthe human-generated text but also subtly influencethe perspectives and beliefs of the human writersthemselves (Jakesch et al., 2023). Different fromthese prior works, in this paper, we focus on exam-ining whether and how the disclosure of varyingtypes of assistance from LLMs would affect peo-ples perceptions of writings.",
  "Study Design": "To understand how the disclosure of AI assistancewould influence peoples perceptions of writings,we divide our study into two phases. In the firstphase, we collect writing samples on different typesof writing tasks, while the human writers use dif-ferent levels or types of AI assistance during theirwriting processes. The second phase is our fo-cal experiment, in which we recruit participants toevaluate the writing samples collected. They willbe randomly assigned to groups informed or un-informed about AI assistance used in the writingprocess and the specific type of assistance used.This study was approved by the Institutional Re-view Board of the authors institution.",
  "Phase 1: Collection of Writing Samples": "In preparation for our focal experiment in Phase2, we first undertake our Phase 1 study to collecthumans writing samples when they complete theirwriting tasks with different levels and types of AIassistance. Writing tasks.Participants in Phase 1 wereasked to write a 200250 word article within 45minutes. As different writing tasks vary on theirpurposes and required skills, we asked our partic-ipants to work on one of the two types of writingtasks:",
  "Argumentative essay writing: Participantswere provided with a statement that was ran-domly sampled from a set of statements used": "in the TOEFL writing exam topic pool (e.g.,Nowadays it is easier to maintain good healththan it was in the past.). Participants wereasked to write an essay to discuss the argumentin the statement. In their essays, participantscould either support or oppose the argument inthe statement. Creative story writing: Participants were givena prompt (e.g., Someone saying Lets go for awalk.), and they were asked to write a story thatincludes the prompt. Prompts we used in thesetasks were adapted from the popular ReedsysShort Story Contest1.",
  "Independent: In this mode, participants com-pleted the writing task independently withoutany assistance from ChatGPT": "AI editing: In this mode, participants wereprimarily responsible for writing the article.Meanwhile, participants could send any partof their drafts to ChatGPT for editing and pol-ishing, and then they could decide how tointegrate the polished texts into their writing.Note that in this mode, to ensure that Chat-GPT would only provide editing assistanceto participants, we covertly crafted a promptemploying the OpenAI API by appending thefollowing instructions ahead of the text thatparticipants sought to polish: You shouldonly edit or polish the texts I send to you.Please do not write any new content. AI generation: In this mode, ChatGPT tookthe lead in drafting the initial version of the ar-ticle. Participants could then provide feedbackand direct the subsequent revisions of the ar-ticle through conversational interactions withChatGPT. In the end, participants decided howto compose the final article based on differentversions of the drafts that ChatGPT generated,",
  ": The number of articles collected in Phase 1 foreach writing mode across the two types of writing tasks": "assigned to one of the two writing tasks (i.e., argu-mentative essay or creative story) and one of thetwo conditions: Independent vs. AI editing orIndependent vs. AI generation. Each condition in-cludes two writing modes at different levels of com-pensation. Participants were required to choose apreferred writing mode, and complete their writingtask under that writing mode3. After completingthe writing task, participants were asked to fill outan exit survey to report their familiarity and us-age frequency of ChatGPT, their future confidencein writing under the chosen mode, and their per-ceptions of their writing experience. To filter outinattentive participants, we included two attentioncheck questions in our study. The participants mustpass both attention checks for their writings to beused in our subsequent Phase 2 study. Data Collection Results.In total, 407 Prolificworkers successfully completed our Phase 1 studyand passed the attention checks. On average, partic-ipants spent 27.1 minutes on the study and receivedan average hourly payment of $13.9. showsthe number of articles we collected from Phase 1for each writing mode across the two types of writ-ing tasks. For more details about the design of ourPhase 1 study, please see Appendix B.",
  "Phase 2: Experimental Design": "After collecting writing samples that human writersgenerated using different levels and types of AIassistance, in Phase 2, we designed a randomizedexperiment to evaluate how people perceive thesewriting samples, when the usage of AI assistanceduring the writing process was or was not revealed.Specifically, in our Phase 2 study, a separateset of participants (distinct from those in Phase 1) 3Note that for some participants, the writing mode withAI assistance came with higher payments than independentwriting, while for others the payments for independent writingwere higher. We collected human writers writing samplesby asking them to select into their preferred writing modes,because Phase 1 data was used to estimate the value thatpeople attach to different types of AI assistance, which hasbeen reported in Li et al. (2024). Due to this design, in ouranalysis, we focus on the impacts of AI assistance disclosureon raters perceptions of writings across different scenarios.However, we do not intend to draw any causal conclusionsbased solely on the direct comparisons of raters perceptionsof writings across the three writing modes. were each asked to review a random set of six ar-ticles that we collected from Phase 1. Participantswere told that the articles they needed to reviewwere submitted by crowd workers in a previousstudy. For each article, participants first evaluatedits overall quality on a scale from 1 to 5 stars, with agranularity of 0.5, and indicated whether they werewilling to shortlist the article for granting its authoran additional performance-based bonus. They werethen required to justify their evaluations in a fewsentences. Following this, participants providedtheir detailed evaluation of this article on five spe-cific aspects, including the articles (a) grammarand vocabulary, (b) organization, (c) originality,(d) creativity, and (e) emotional authenticity. Eachaspect was again evaluated on a scale from 1 to 5stars. We considered two experimental treatmentsin Phase 2:",
  "Non-Disclose: In this treatment, participantsevaluated their assigned articles without beinginformed about whether or how the authors ofthese articles used AI assistance during theirwriting": "Disclose: In this treatment, we informed par-ticipants about whether and how AI assistancewas used by the author when writing the article,before they were asked to evaluate it. For ex-ample, when the author of an article completedtheir writing under the AI generation mode, ourPhase 2 participants were told that the draft ofthis article was generated by ChatGPT, and thecrowd worker had prompted ChatGPT to reviseand improve the content before they started toevaluate this article.",
  "Phase 2: Experimental Procedure": "Our Phase 2 study was open to U.S. workers onlyon Prolific. We excluded the workers who hadparticipated in our Phase 1 study, and each workerwas allowed to only take this Phase 2 study once.Each participant went through a few steps in thestudy, as detailed below. Background assessment.Upon arrival, partici-pants were first asked to fill out a questionnaire toreport their demographic information (e.g., gender,age, education, and race). We then asked partici-pants to indicate how confident they were in theirown writing skills and how often they engaged inwriting activities on a 5-point Likert scale from 1(very low) to 5 (very high). Main rating tasks.Subsequently, participantswere randomly assigned into one of the two experi-mental treatments, Disclose or Non-Disclose. Theythen needed to evaluate six articles sequentially.Each article was randomly sampled from the poolof articles we collected in our Phase 1 study, al-though for participants in the Disclose treatment,we also ensured that all six articles were gener-ated under the same writing mode. In other words,participants in the Disclose treatment were lim-ited to evaluating articles from the same writingmode, thereby mitigating the potential interferencebetween different writing modes in the evaluation.As described earlier, for each article, participantsneeded to provide evaluation on its overall qualityand on five specific aspects, and they could alsochoose to shortlist the article for bonus. Exit Survey.After the completion of the mainrating tasks, participants were asked to completean exit survey. In this survey, participants neededto report their familiarity with ChatGPT and theirfrequency of using ChatGPT on a 5-point Likertscale. We also included a few questions to under-stand participants perceived authorship of the finalarticles to the human authors. Similar as that inPhase 1, we also included one attention check ques-tion in our Phase 2 study. Only the data collectedfrom participants who passed the attention checkin Phase 2 were considered as valid, and would beused for the analysis.",
  "Results": "In total, 786 workers from Prolific took our Phase 2study and passed the attention check (Non-Disclose:380, Disclose: 406; see Appendix A for partici-pants demographic backgrounds). The averageamount of time participants spent on the Phase2 study was 20 minutes, resulting in an averagehourly wage of $7.5. On average, each articlereceived 5.6 evaluations from participants in theNon-Disclose treatment and 5.9 evaluations fromparticipants in the Disclose treatment. Below, weanalyze the impact of disclosing AI assistance inthe writing process on peoples assessment of writ-ings based on the data we collected in Phase 2. Dueto the space limit, we focus on sharing the resultson the impact of AI disclosure on the perceivedquality and ranking of articles in the main text. SeeAppendix G for the impact of AI disclosure on theperceived authorship of articles.",
  "(b) Creative story": "Figure F.1: Within the top % of the articles writtenabout the same statement or prompt (determined bythe probability for raters to shortlist the article), thepercentages of articles that were written in each of thethree writing modes, when the use of AI assistance wasor was not reveal to raters.*, ** and *** denote thesignificance level of 0.05, 0.01, and 0.001, respectively.",
  "Impacts of Disclosing AI Assistance on theQuality Evaluation of Writing": "Participants average ratings on the overall qualityof the argumentative essays and creative stories,grouped by writing modes, are presented in Fig-ures 1a and 1b, respectively. Visually, it appearsthat when the articles are written by humans inde-pendently without using any AI assistance, whetherknowing this information or not does not signifi-cantly change peoples perceived quality of thearticles. In contrast, when the writers use somedegree of AI assistance in their writing process,the disclosure of this information often decreasespeoples perceived quality of the articles.To examine whether these differences are statisti-cally significant, we conducted regression analyses.Specifically, the focal independent variable was thetreatment that a participant was assigned to in ourPhase 2 study (i.e., Disclose vs. Non-Disclose),while the dependent variable was the participantsrating on the overall quality of an article. This re-gression analysis was done separately for articlesgenerated under each of the three writing modes.To minimize the impact of potential confoundingvariables, we also accounted for a set of covariatesin our regressions, such as the demographic back-ground of both the Phase 1 article writers (e.g., age,gender, the payment received for the selected writ-ing mode, confidence in the assigned writing task,etc.) and the Phase 2 raters (e.g., age, gender, gen-eral writing confidence, writing activity frequency,frequency of ChatGPT use, etc.).Our regression results suggest that for articlesproduced under the AI generation writing mode,informing people about the inclusion of ChatGPTscontent generation assistance during the writingprocess significantly decreases peoples perceived",
  "Impacts of Disclosing AI Assistance on theVariation in the Quality Evaluation": "In .1, we have found that the disclosureof AI assistance in the writing process may lead toa decrease in the average rating of writing quality.To understand whether it also affects the dispersionin quality evaluation, we examine how the disclo-sure of AI assistance affects the variance in ratingsamong different participants for the same article.Specifically, for each of the 407 articles used inPhase 2, given a particular aspect of evaluation(e.g., overall quality), we gathered all the ratingson this aspect for this article, and then computedthe variance within the ratings provided by partici-pants in the Disclose and Non-Disclose treatments,separately. compares the variance in the overallquality ratings given to the same argumentative es-says or the same creative stories, when the use andtype of AI assistance was or was not revealed toPhase 2 participants. Interestingly, for the three set-tings where we previously found the disclosure ofAI assistance results in significant decreases in peo- ples perceived overall quality of the articles (i.e.,when AI provides content generation assistance forboth arguments and stories, and when AI providesediting assistance for stories; see ), the vari-ance in peoples overall quality ratings for the samearticle also appears to increase. Focusing on arti-cles written under each of the three writing modesseparately, we then fitted regression models to pre-dict the variance in the overall quality ratings foreach article based on the disclosure of AI assistance.The regression results suggest that compared toparticipants in the Non-Disclose treatment whowere unaware of how the article was written, thoseparticipants who were informed about the AIs con-tent generation assistance showed a significantlylarger variation in their overall quality evaluationsof the same argumentative essay (p = 0.004). Sim-ilarly, the disclosure of AIs editing assistance alsomakes participants significantly diverges on theirevaluations of the same creative story (p = 0.005).Further analyses on peoples detailed evaluationson various aspects of the articles (e.g., organization,originality) also show that disclosing AIs contentgeneration assistance consistently results in a sig-nificantly higher level of variation (p < 0.05) inpeoples evaluations on almost all aspects of anargumentative essay (except for its grammar andvocabulary; see the Appendix D for more details).These results suggest that the disclosure of peo-ples usage of AI assistance not only has a generaltendency to decrease the average quality evaluationof the writings, but also substantially increase theuncertainty in the evaluation as it becomes moreunpredictable and highly susceptible to variabilitydepending on who is evaluating the writing.",
  "Individual Heterogeneity in the Impacts ofAI Disclosure on the Quality Evaluation": "In this section, we focus on examining whether andhow a raters own characteristics may moderate theeffects of the disclosure of AI assistance, and weidentified two characteristics as potential moderat-ing factors. We begin with considering how disclos-ing the use and type of AI assistance in the writingprocess would affect the evaluation of raters whohad different levels of confidence in their own writ-ing skills. To investigate into this, we first dividedall participants in our Phase 2 study into two groupsbased on a median split of their self-reported con-fidence in writing. Within each group of partici-pants, we could compute the difference in the over-all quality ratings given by participants assigned",
  "denote the significance level of 0.01 and 0.001, respec-tively": "while regular employees may compete for chancesto be promoted based on performance evaluationsfrom managers. Consequently, it is crucial to un-derstand how the disclosure of AI assistance affectsthe performance ranking of workers. In this sec-tion, we aim to understand how the disclosure ofthe use and type of AI assistance during the writ-ing process may influence the ranking of writingsgenerated under different writing modes.Suppose the articles average overall quality rat-ings are used to determine their rankings. Givenall the articles written on the same topic, we lookinto that within the top % of the articles for thistopic, what proportions of the articles are writtenin the independent, AI editing, and AI generationwriting modes, respectively, and how these propor-tions change after we informed participants aboutthe use and type of AI assistance during the writingprocess of these articles. compares the average percentages ofarticles generated under the three writing modeswhose overall quality ratings fall within the top %( {10, 20, , 50}) threshold, when the use ofAI assistance was or was not revealed to raters. Forthe argumentative essay task, it is clear that disclos-ing the use and type of AI assistance during thewriting process results in a decrease in the propor-tion of articles generated under the AI generationmode and an increase in the proportion of articlesgenerated under the AI editing or Independentmodes within the highly ranked articles. This isespecially true when we focus on the most top-ranked articles (i.e., when is small). Chi-squaretests suggest that the compositions of the articleswriting modes are significantly different betweenthe Disclose treatment and the Non-Disclosetreatment within the top 10% (p < 0.001) and top 20% (p = 0.002) of argumentative essays. In con-trast, for the creative story task, the disclosure ofthe use and type of AI assistance during the writingprocess appears to have minimal impacts on theranking of different articles. The analysis, basedon the ranking criteria for an articles probabil-ity of being shortlisted by raters, revealed similarfindingsrevealing AI assistance significantly de-creases the proportions of highly-ranked articlesthat are generated with ChatGPTs content gener-ation assistance for argumentative essays, whilethe impacts on the ranking of creative stories areminimal. See Appendix F for detailed results. 5DiscussionsPossible explanations for our findings. In ourstudy, we made a few interesting observations. Forexample, we find that disclosing the usage of AIassistance may affect peoples evaluations of thewriting quality to a larger extent when AI providesassistance in generating new content than when AIprovides editing assistance. This observation maybe caused by raters perceived human effort con-tributing to the final writings. In particular, whenAI provides content generation assistance (i.e., asin the AI generation writing mode), AI directlygenerates sentences or paragraphs, reducing thewriters effort significantly (we found that writersunder the AI generation writing mode spend signif-icantly less time than writers under other modes).As such, when this type of AI assistance is dis-closed, raters might be resistant to giving highscores to the quality of the writings, as they maygive less credit to the writers who use AI generationassistance compared to those who do not. Ratersmay also significantly raise their expectation for thewriting quality when knowing that the writers canutilize AI assistance to generate content on behalfof them.Moreover, we also notice that disclosing the us-age of AI assistance appears to have a greater im-pact on peoples perceived ranking of argumenta-tive writing than on creative writing. We attributethis to the distinct nature and objectives of thesetwo writing tasks. Argumentative writing requiresconstructing a logical structure and identifying ap-propriate evidence, tasks for which LLMs can eas-ily provide support and generate logical structures.When AI usage is not disclosed, this can lead tohigher ratings, as raters attribute the quality solelyto the writer. However, when AI usage is disclosed,raters may partially credit the quality to the AI as- sistance, resulting in noticeably lower ratings, asreflected in the significant shift in rankings. In con-trast, creative writing relies heavily on imaginationto create compelling characters and plots, an areawhere LLMs still struggle to produce truly novelcontent. As a result, even when AIs content gen-eration assistance is not disclosed, the perceivedquality of the best AI-assisted articles in creativewriting do not substantially outperform the bestones written in other modes, especially those writ-ten with AIs editing assistance. As such, after AIscontent assistance is disclosed, which causes theperceived quality for both articles written with AIscontent generation assistance and editing assistanceto decrease, we do not see dramatic changes in thethe rankings of the creative writings. Design implications. Our findings have importantdesign implications. For example, for online plat-forms that often determine the ordering of postsbased on historical users evaluation of posts (e.g.,ratings and upvotes), one challenge they face is thatthe prevalence of AI-assisted writings may crowdout and disengage writers who continue to createcontent without the use of AI assistance. Results ofour study suggest that these platforms may benefitfrom introducing content labels to mark those con-tent that are partly generated by AI, as such labelsmay change users evaluations of these content inrelative to other content that are generated indepen-dently by humans, especially when these contenthave an argumentative flavor.In addition, the fact that disclosing AI assistancetends to decrease the quality perceptions of writ-ings implies a tension that writers face betweenbeing transparent and maintaining the perceivedquality of their writing. From platforms point ofview, this means that they need to design appropri-ate incentives for writers to be willing to disclosethe AI assistance usage, and ensure no unintendeddisadvantage or discrimination for those whodo. From the writers point of view, they shouldstill recognize their ethical responsibility to informreaders about how the writing is generated to main-tain readers trust, as misleading readers about AIinvolvement in the writing may erode trust in thelong run. Limitations. Our study has a few limitations. Forexample, the type of AI assistance and writingtasks we considered do not capture the full rangeof AI assistance and writing tasks in the real world.Both the collection and evaluation of writing sam- ples in our study are conducted on an online plat-form. Participants, primarily motivated by financialpayments, may not accurately reflect professionalwriters and evaluators in the real-world. We alsolimited our study to U.S. workers to control forthe potential variability in English language habitsor preferences that differ across countries, limit-ing the generalizability of findings to non-Englishspeaking populations. Future research should lookinto whether our findings generalize to a broaderspectrum of AI writing assistance, a wider rangeof writing tasks, people with more diverse demo-graphic and cultural backgrounds, and in contextsthat more closely mimic real-world writing andevaluation scenarios.It is also worthwhile to note that in our study, weobserved that articles completed in the AI-assistedmodes, particularly those under the AI generationmode, received significantly higher quality ratingsfrom raters compared to articles completed inde-pendently by participants. Thus, even after the AIassistance is disclosed, participants who used theAIs content generation assistance may still receivea higher average rating of their writing quality thanparticipants who wrote independently. However,as we noted in footnote 3, due to the design ofthe Phase 1 study (i.e., participants self-selectedinto their preferred writing mode), a comparisonacross the three writing modes does not allow fora causal interpretation. Future work should con-duct a rigorously designed study to see whetherthis observation still holds true.",
  "Conclusions": "In this paper, we ask the question of whether andhow the disclosure of AI assistance would influencepeoples perceptions of writings. Our experimentalresults suggest that when people are informed ofthe use of AI content generation assistance in thewriting process, there is a significant decrease inthe quality evaluation of the writing across differenttypes of writing tasks. Furthermore, the disclosureof AI assistance often leads to an increased levelof variation in the perceived writing quality. Weidentify potential factors, such as the raters con-fidence in writing and their familiarity with Chat-GPT, that might moderate the effect of disclosingAI assistance on the evaluation of writing quality.Additionally, our findings suggest that disclosingthe use of AI assistance would also significantly re-duce the proportion of AI-assisted writings amongtop-ranked argumentative essays. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Daniel Buschek, Martin Zrn, and Malin Eiband. 2021.The impact of multiple parallel phrase suggestionson email input and composition behaviour of nativeand non-native english writers. In Proceedings of the2021 CHI Conference on Human Factors in Comput-ing Systems, pages 113.",
  "Zenan Chen and Jason Chan. 2023. Large languagemodel in creative work: The role of collaborationmodality and user expertise.Available at SSRN4575598": "John Joon Young Chung, Wooseok Kim, Kang MinYoo, Hwaran Lee, Eytan Adar, and Minsuk Chang.2022. Talebrush: Sketching stories with generativepretrained language models. In Proceedings of the2022 CHI Conference on Human Factors in Comput-ing Systems, pages 119. Elizabeth Clark, Tal August, Sofia Serrano, NikitaHaduong, Suchin Gururangan, and Noah A Smith.2021. All thats humanis not gold: Evaluating hu-man evaluation of generated text. arXiv preprintarXiv:2107.00061. Elizabeth Clark, Anne Spencer Ross, Chenhao Tan,Yangfeng Ji, and Noah A Smith. 2018.Creativewriting with a machine in the loop: Case studies onslogans and stories. In 23rd International Conferenceon Intelligent User Interfaces, pages 329340. Hai Dang, Sven Goller, Florian Lehmann, and DanielBuschek. 2023. Choice over control: How userswrite with large language models using diegetic andnon-diegetic prompting. In Proceedings of the 2023CHI Conference on Human Factors in ComputingSystems, CHI 23, New York, NY, USA. Associationfor Computing Machinery. Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski,Noah A Smith, and Yejin Choi. 2021.Is gpt-3text indistinguishable from human text? scarecrow:A framework for scrutinizing machine text. arXivpreprint arXiv:2107.01294. Fiona Draxler,Anna Werner,Florian Lehmann,Matthias Hoppe, Albrecht Schmidt, Daniel Buschek,and Robin Welsch. 2024.The ai ghostwriter ef-fect: When users do not perceive ownership of ai-generated text but self-declare as authors.ACMTrans. Comput.-Hum. Interact., 31(2). Liye Fu, Benjamin Newman, Maurice Jakesch, andSarah Kreps. 2023. Comparing sentence-level sug-gestions to message-level suggestions in ai-mediatedcommunication. In Proceedings of the 2023 CHIConference on Human Factors in Computing Sys-tems, pages 113.",
  "Arthur S Jago and Glenn R Carroll. 2023.Whomade this?algorithms and authorship credit.Personality and Social Psychology Bulletin, page01461672221149815": "Maurice Jakesch, Advait Bhat, Daniel Buschek, LiorZalmanson, and Mor Naaman. 2023. Co-writing withopinionated language models affects users views. InProceedings of the 2023 CHI Conference on HumanFactors in Computing Systems, pages 115. Maurice Jakesch, Megan French, Xiao Ma, Jeffrey THancock, and Mor Naaman. 2019. Ai-mediated com-munication: How the perception that profile text waswritten by ai affects trustworthiness. In Proceedingsof the 2019 CHI Conference on Human Factors inComputing Systems, pages 113. Hang Jiang, Xiajie Zhang, Xubo Cao, Cynthia Breazeal,Deb Roy, and Jad Kabbara. 2023.Personallm:Investigating the ability of large language mod-els to express personality traits.arXiv preprintarXiv:2305.02547. Mina Lee, Katy Ilonka Gero, John Joon Young Chung,Simon Buckingham Shum, Vipul Raheja, HuaShen, Subhashini Venugopalan, Thiemo Wambs-ganss, David Zhou, Emad A Alghamdi, et al. 2024.A design space for intelligent and interactive writingassistants. In Proceedings of the CHI Conference onHuman Factors in Computing Systems, pages 135. Mina Lee, Percy Liang, and Qian Yang. 2022a. Coau-thor: Designing a human-ai collaborative writingdataset for exploring language model capabilities. InProceedings of the 2022 CHI conference on humanfactors in computing systems, pages 119. Mina Lee, Megha Srivastava, Amelia Hardy, John Thick-stun, Esin Durmus, Ashwin Paranjape, Ines Gerard-Ursin, Xiang Lisa Li, Faisal Ladhak, Frieda Rong,et al. 2022b. Evaluating human-language model in-teraction. arXiv preprint arXiv:2212.09746. Zhuoyan Li, Chen Liang, Jing Peng, and Ming Yin.2024. The value, benefits, and concerns of generativeai-powered assistance in writing. In Proceedings ofthe CHI Conference on Human Factors in ComputingSystems, pages 125. Zhuoyan Li, Hangxiao Zhu, Zhuoran Lu, and Ming Yin.2023. Synthetic data generation with large languagemodels for text classification: Potential and limita-tions. In The 2023 Conference on Empirical Methodsin Natural Language Processing.",
  "Sherwin Rosen. 1986. The theory of equalizing differ-ences. Handbook of labor economics, 1:641692": "Nikhil Singh, Guillermo Bernal, Daria Savchenko, andElena L Glassman. 2023. Where to hide a stolen ele-phant: Leaps in creative writing with multimodal ma-chine intelligence. ACM Transactions on Computer-Human Interaction, 30(5):157. Sangho Suh, Meng Chen, Bryan Min, Toby Jia-JunLi, and Haijun Xia. 2023.Structured generationand exploration of design space with large languagemodels for human-ai co-creation.arXiv preprintarXiv:2310.12953.",
  "Azmine Toushik Wasi, Raima Islam, and Rafia Islam.2024b. Ink and individuality: Crafting a person-alised narrative in the age of llms. arXiv preprintarXiv:2404.00026": "Daijin Yang, Yanpeng Zhou, Zhiyuan Zhang, Toby Jia-Jun Li, and Ray LC. 2022. Ai as an active writer:Interaction strategies with generated text in human-aicollaborative fiction writing. In Joint Proceedingsof the ACM IUI Workshops, volume 10. CEUR-WSTeam. Ann Yuan, Andy Coenen, Emily Reif, and Daphne Ip-polito. 2022. Wordcraft: Story writing with largelanguage models. In 27th International Conferenceon Intelligent User Interfaces, IUI 22, page 841852,New York, NY, USA. Association for Computing Ma-chinery. Zheng Zhang, Jie Gao, Ranjodh Singh Dhaliwal, andToby Jia-Jun Li. 2023. Visar: A human-ai argumen-tative writing assistant with visual programming andrapid draft prototyping. In Proceedings of the 36thAnnual ACM Symposium on User Interface Softwareand Technology, UIST 23, New York, NY, USA.Association for Computing Machinery.",
  "ADemographic Information of Raters": "In total, 786 workers from Prolific took our studyand passed the attention check. Among them, 380were allocated to the Non-Disclose treatment, whilethe remaining 406 were assigned to the Disclosetreatment. The full demographic information ofparticipants is shown in A.1. The writing con-fidence, writing frequency, and the frequency ofChatGPT use are measured on a 5-point Likertscale from 1 (very low) to 5 (very high).",
  "B.1Writing Task": "Participants of our Phase 1 study were asked towrite a 200250 word article within 45 minutes.To examine how peoples value of AI assistancein writing may vary with the nature of the writingtask, we considered two kinds of writing jobs inour experiment: Argumentative essay writing: Participantswere provided with a statement, and they wereasked to write an essay to discuss the argu-ment in the statement. In their essays, par-ticipants had the freedom to either supportor oppose the argument in the statement. Weconsidered three statements that were sampledfrom the pool of TOEFL writing exam topics: 1. Some people think that if companiesprohibit sending emails to staff on week-end or during other time out of officehours, staffs dissatisfactions with theircompanies will decrease. Others thinkthis will not reduce the overall dissatis-factions among staff.",
  ". Someone saying Lets go for a walk": "To ensure that the argumentative essay and creativestory writing tasks chosen for our Phase 1 studyhave a comparable and reasonable difficulty level,we conducted a pilot study to test the difficulty ofdifferent argumentative essay statements and cre-ative story prompts. In this pilot study, participantswere asked to complete the writing task on theirown. For the final set of 3 statements and 3 promptsselected for our Phase 1 study, as detailed above,our pilot study results suggested that participantscould successfully complete the essay/story writ-ing job within the time limit, yielding articles ofsatisfactory quality.",
  "B.3Experimental Treatments": "To quantify how much financial value that peopleattach to different kinds of writing assistance thatLLM-powered assistants could offer, following theclassical methods in economics for estimating thewillingness to pay/accept (Rosen, 1986; Mas andPallais, 2017; Liang et al., 2023), we created twoexperimental treatments: Independent vs.AI editing:In thistreatment, after the topic of the writing job(i.e., the statement for writing an argumen-tative essay or the prompt for writing acreative story) was revealed to participants,we presented participants with two joboffers: The first offer paid the participant$3 to complete the writing job in the inde-pendent writing mode, while the secondoffer paid the participant $x to completethe writing job in the AI editing mode,where x was randomly sampled from the set{1.5, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 3.75, 4, 4.5}.Participants were asked to make a selectionbetween these two offers, and subsequentlycomplete the writing job in accordance withthe writing mode specified by the selectedoffer6. Independent vs. AI generation: In this treat-ment, after the topic of the writing job wasrevealed to participants, we presented partici-pants with two job offers: The first offer com-pensated the participant with $3 for complet-ing the writing job in the independent writingmode, while the second offer compensated theparticipant with $x for completing the writ-ing job in the AI generation mode, where xwas again randomly sampled from the set ofvalues as that in the previous treatment. Partic-ipants were asked to make a selection betweenthese two offers, and would then complete thewriting job in the writing mode specified bythe offer that they chose. 6We conducted a pilot test to validate the appropriatenessof the lower and upper bounds, $1.5 and $4.5, respectively, forestimating participants willingness to pay for AI assistance.This consideration was made given that a majority of partic-ipants in our pilot study would prefer not to choose the jobwith AI assistance at the $1.5 wage level, while conversely,most participants would select the job with AI assistance atthe $4.5 wage level. Note that to ensure participants could make aninformed selection between the two job offers, inboth treatments, participants would be initially di-rected to watch a 2-minute video that introduces tothem the type of writing assistance that they couldreceive from ChatGPT and acquaints them with thewriting interface they would use upon selecting thejob offer with AI assistance (i.e., the offers asso-ciated with AI editing or AI generation writingmodes). Participants could only make their joboffer selection after finish watching this video.",
  "B.4Experimental Procedure": "Our study was opened only to U.S. workers whoseprimary language is English on Prolific, and eachworker was only allowed to participate in our studyonce. Each participant went through a few stagesin our study, as detailed below. Background assessment.Upon arrival of thestudy, participants were first asked to fill out a ques-tionnaire to report their demographic information(e.g., gender, age, education). We then asked par-ticipants to indicate how confident they were incompleting six types of writing tasks, includingcreative writing (e.g., stories, novels), writing argu-mentative essays, writing emails or letters, writingproduct or book reviews, writing business reportsor proposals, and writing blogs. For each type ofwriting tasks, participants reported their confidenceon a 5-point Likert scale from 1 (very low) to 5(very high). Treatment assignment and writing mode selec-tion.Subsequently, participants were random-ized into one of the two treatments, Independentvs. AI editing or Independent vs. AI generation.They were then presented with their writing task,which could be either writing an argumentativeessay or writing a creative story, and the statemen-t/prompt used for the writing task was also selectedat random from the candidate pool. Next, depend-ing on the experimental treatment the participantwas assigned, they would be presented with thecorresponding two job offers, and the random pay-ment value $x (1.5 x 4.5) of the offer thatprovided AI assistance to participants would be re-alized from its set of candidate values. Participantswere told that their final payment from the studywould consist of three parts: (1) a base paymentof $2; (2) the writing job payment as specified inthe job offer that they would choose; and (3) an(optional) performance-based payment of $2. We informed participants that their submitted articleswould be sent to other crowd workers for review. Ifthe average rating of their article would rank withinthe top 10% of the articles written for the sametopic, they would receive the performance-basedpayment. Once they were clear on the compensa-tion structure, participants were required to watchan introductory video elucidating how they couldpotentially collaborate with ChatGPT through thedesignated interface to accomplish the writing taskshould they choose the AI-assisted writing mode(i.e., AI editing or AI generation). With allthis information, participants then selected theirpreferred job offer. Main writing task.After the job offer was se-lected, participants proceeded to the main writingtask. They were asked to complete this writingtask using the writing mode specified in their cho-sen offer, and they had a maximum duration of 45minutes for finishing the writing task. Note thatthe time required for ChatGPT to respond to par-ticipants prompts would not be included in theallocated time limit. Exit survey.After completing the main writingtask, participants were asked to complete an exitsurvey.In this survey, participants were againasked to indicate their confidence in completingthe same six types of writing tasks (e.g., creativewriting, argumentative essay, emails/letters, etc.)as we surveyed at the beginning of the study, shouldthey have the chance in the future to complete thosetasks in the same writing mode as they had experi-enced in our study. We also asked a series of surveyquestions to guage participants perceptions of theirwriting experience in our study. For example, theNASA Task Load Index (NASA TLX) (Hart andStaveland, 1988) was used to measure the cogni-tive load that participants experienced during thewriting task, including their mental demand, timepressure, amount of effort taken, and frustrationlevel. To understand participants perceptions ofthe overall writing processes, we presented the fol-lowing statements to participants and asked themto rate how much they agreed with each statementon a 5-point Likert scale from 1 (strong disagree)to 5 (strongly agree):",
  "EIndividual Heterogeneity in theImpacts of AI Disclosure on the QualityEvaluation (Additional Results)": "Figures E.1E.6 show the difference in an articlesshortlisting rate, ratings of grammar and vocab-ulary, ratings of organization, ratings of original-ity, ratings of creativity, and ratings of emotionauthenticity between the Disclose and the Non-Disclose treatments, when the ratings were pro-vided by participants who had high or low confi-dence in their own writing skills. In general, par-ticipants with high confidence in writing appearto be influenced by the disclosure of AI assistancemore in evaluating various aspects of an article thanparticipants with low confidence in writing; theytend to decrease their willingness to shortlist anarticle or decrease their ratings on various aspectsof the article, especially after knowing about the",
  "GImpacts of Disclosing AI Assistance onAuthorship Attribution": "To understand the extent to which participants at-tribute authorship of the final articles to the hu-man authors (i.e., crowd workers in our Phase1 study) across the three writing modes, we pre-sented the following statements in the exit surveyof Phase 2 and asked participants to rate how muchthey agreed with each statement on a 5-point Lik-ert scale from 1 (strongly disagree) to 5 (strongly Table F.1: Within the top % of articles for the same writing task (ranked by articles average overall quality ratings),the exact percentages of articles that were written in each of the three writing modes, with and without disclosingthe use and type of AI assistance."
}