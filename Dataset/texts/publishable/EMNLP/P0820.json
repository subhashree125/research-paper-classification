{
  "Abstract": "The rapid development of multimodal largelanguage models (MLLMs), such as GPT-4V,has led to significant advancements. However,these models still face challenges in medicalmultimodal capabilities due to limitations inthe quantity and quality of medical vision-textdata, stemming from data privacy concernsand high annotation costs. While pioneeringapproaches utilize PubMeds large-scale, de-identified medical image-text pairs to addressthese limitations, they still fall short due toinherent data noise.To tackle this, we re-fined medical image-text pairs from PubMedand employed MLLMs (GPT-4V) in an un-blinded capacity to denoise and reformat thedata, resulting in the creation of the PubMed-Vision dataset with 1.3 million medical VQAsamples.Our validation demonstrates that:(1) PubMedVision can significantly enhancethe medical multimodal capabilities of currentMLLMs, showing significant improvement inbenchmarks including the MMMU Health &Medicine track; (2) manual checks by medicalexperts and empirical results validate the su-perior data quality of our dataset compared toother data construction methods. Using Pub-MedVision, we train a 34B medical MLLMHuatuoGPT-Vision, which shows superiorperformance in medical multimodal scenariosamong open-source MLLMs. Our code anddata are available at",
  "*Benyou is the corresponding author": "et al., 2021; He et al., 2020), scaling them up ischallenging. Additionally, there are privacy and li-censing issues associated with medical data, furthercomplicating matters.Pioneering works (Zhang et al., 2023c; Li et al., 2023a; Wu et al., 2023) utilize PubMed1 for larger-scale training for medical vision-language align-ment. PubMed is favored because it contains medi-cal images and surrounding text, which (i) encap-sulate the forefront of human wisdom in medicineand (ii) are well-de-identified (Lin et al., 2023).However, models trained on PubMed are unsatis-factory, as they perform poorly compared to generalMLLMs on medical multimodal tasks (Hu et al.,2024; Xia et al., 2024). This can be attributed todata noise in PubMed, which significantly affectsmultimodal performance (Liu et al., 2024; Chenet al., 2024).Concurrently, LLaVA-Med (Li et al., 2023a)uses a blind Large Language Model (LLM) togenerate Visual Question Answering (VQA) fromthe contextual text of PubMed images, achievingnotable results.However, this approach mightoverlook visual information inherent in the med-ical images themselves as LLMs cannot perceiveimages as input, probably leading to the gen-eration of misinterpreted or irrelevant answers.Moreover, LLaVA-Med is limited to 56K medi-cal VQA entries. Thus, creating a higher-qualityand larger-scale vision-language alignment datasetfor medicine is essential.To close this gap, we meticulously select high-quality medical image-text pair from PubMed, em-ploying a proposed refined pipeline.Utilizing914,960 refined medical images and their corre-sponding text, we apply GPT-4V as the unblindedreformatter, contrasting the blinded reformattingused in previous works (Li et al., 2023a; Wu et al.,",
  "PubMed is a free search engine that primarily accessesthe MEDLINE database, containing references and scientificpapers on life sciences and biomedical topics": "2023; Zhang et al., 2023c), to denoise the PubMeddata. Our method generates more aligned medi-cal VQA data for medical multimodal alignment.Consequently, we constructed a high-quality mul-timodal medical dataset with 1.3 million samplesand name it as PubMedVision.Our experiments validated PubMedVision intwo key aspects: (1) It significantly enhancesthe medical multimodal capabilities of MLLMs,showing notable improvement in benchmarks suchas MMMU Health & Medicine.LLaVA-v1.5-LLaMA-3-8B achieves the strongest performanceamong open-source MLLMs with PubMedVision ;(2) Manual checks by medical experts and empir-ical results confirmed the superior data quality ofPubMedVision compared to current data construc-tion methods. The contributions of this paper aresummarized as follows: 1. Unblinded Data Reformatting for MedicalMultimodality. We propose leveraging un-blinded MLLMs to reformat PubMed image-text pairs to construct a better-aligned medi-cal VQA dataset. Expert reviews and empiri-cal tests show that this method yields higher-quality data, improving MLLM training. 2. PubMedVision:A Large-scale,High-quality Medical Multimodal Dataset. Withthe MLLM-powered reformatted method, webulid PubMedVision, containing 1.3 millionmedical VQA entries for visual alignment.Experiments demonstrate that PubMedVisionsignificantly enhances MLLMs medical mul-timodal capabilities, enabling models likeLLaVA-1.5-LLaMA-3-8B to outperform othergeneral and medical open-source MLLMs. 3. HuatuoGPT-Vision: A Medical MLLM. Us-ing PubMedVision, we trained HuatuoGPT-Vision, a 34B parameter medical MLLM.HuatuoGPT-Vision demonstrate superior per-formance on multiple medical multimodalbenchmarks among open-source models.",
  "compares existing medical VQA datasets,which are crucial for image-text alignment andinstruction following in medical MLLMs. Earlydatasets like VQA-RAD, SLAKE, and Path-VQA": "are limited by their small size (less than 20K en-tries) and their exclusive focus on radiology. PMC-CaseReport (Wu et al., 2023), PMC-VQA (Zhanget al., 2023c), and LLaVA-Med leverage PubMedmedical images to scale data and employ LLMs toreformat contextual text into VQA. However, thesedatasets also suffer from limited quantity and areprone to misinterpretation and misalignment due tothe blinded nature of the LLMs. In contrast, weaim to construct a larger-scale, high-quality medi-cal VQA dataset, PubMedVision.",
  "Medical Visual Alignment through theLens of Data Engineering": "Visual Knowledge AlignmentCurrent MLLMstypically adapt a text-only LLM with a visual en-coder (Liu et al., 2024; Li et al., 2023b). Therefore,alignment involves injecting image knowledge intoLLMs, aligning images with the language under-standing of LLMs. This paper explores the injec-tion of extensive medical visual knowledge fromPubMed into MLLMs, as PubMed is a leadingrepository of advanced medical research with well-de-identified medical images. Data Noises in PubMedAlthough existingwork (Wu et al., 2023; Li et al., 2023a; Zhang et al.,2023c) utilize PubMed, it has not been entirelysatisfactory, as they still lag behind many general-purpose MLLMs in medical vision (Hu et al., 2024;Xia et al., 2024). We attribute it to the data noisesin PubMed. The text surrounding the image inPubMed papers does not always well-describe theimage. While relevant, this text does not necessar-ily facilitate effective visual alignment. The Efforts to Improve Data Quality Sourcedfrom PubMedThe original data is not al-ways suitable for training, as seen in reformat-ting alignment (Fan et al., 2024).Comparedto Native Captions in PubMed, existing workuses text-only LLMs to reformat these captionsof images, denoted as LLM-Reformatted . Thiscan result in misinterpreted or misaligned textfor the images due to the blined LLM. To solvethis, we propose using a multimodal LLM, calledMLLM-Reformatted . Additionally, we comparewith GPT4v-Distill , a popular approach to dis-till GPT-4V in general multimodal fields, such asShareGPT4V (Chen et al., 2023b) and ALLaVA-4V (Chen et al., 2024). For GPT4v-Distilled, weprovide only images to GPT-4V to generate a med-ical description.",
  ": Comparison of Medical VQA Datasets": "MLLM-ReformattedThe provided image displays three axial slices of a non contrast-enhanced cranial CT scan. From left to right: The first image shows the cerebellarregion, where no signs of cerebellar hemorrhage can be observed, suggesting complete resorption. The tissue density appears normal without anyhyperintense areas, which typically signify acute bleeding. The second image focuses closer on the brainstem and cerebellum, also showing noevidence of swelling, and the structures of the posterior fossa appear well-preserved and normal in size. The third image is an axial slice at the level ofthe lateral ventricles, presenting clear and symmetric ventricles, indicating the absence of hydrocephalus. The fourth ventricle is visible and appearsnormal, suggesting a regression of previously noted hydrocephalus. LLM-ReformattedThe image is a postoperative cranial non-contrast-enhanced CT scan slide, displaying significant improvements following a medical intervention. Firstly,it shows that the cerebellar hemorrhage has been fully resorbed, indicating no residual bleeding in the area. Secondly, there is a noticeable reduction inthe swelling of the posterior fossa. Lastly, the fourth ventricle is clearly visible, which correlates with the observed reduction in hydrocephalus. Thisimage illustrates successful outcomes of the treatment procedures in resolving these critical conditions. GPT4v-DistillThe medical image provided is a computed tomography (CT) scan of the head, showcasing three axial slices at different levels:1. Left Image (Axial CT Scan with Contrast). This slice captures the basal cisterns and surrounding structures. There is enhancement observed in theblood vessels due to contrast administration. The detailed view of bony structures such as the sphenoid sinus and temporal bones can be seen. 2. MiddleImage (Axial CT Scan with Contrast). This level shows the midbrain region and the tentorium cerebelli. The presence of contrast highlights vascularstructures. The petrous part of the temporal bone is visible. 3. Right Image (Non-Contrast Axial CT Scan). This slice provides a view of the lateralventricles, basal ganglia, and cortical structures. No contrast is administered in this slice, highlighting the normal brain parenchyma.These imagestogether help in the evaluation for various intracranial pathologies like infarctions, hemorrhages, tumors, or other abnormalities.",
  "Native-Caption": "Postoperative cranial non contrast-enhanced CT-scan demonstrating: 1) acomplete resorption of the cerebellar haemorrhage; 2) a regression of posteriorfossa swelling; 3) a visible fourth ventricle associated with a regression of thehydrocephalus. Control CT-scan , performed one week later , demonstrated acomplete resorption of the haemorrhage and a regression of posterior fossaoedema and hydrocephalus ( ). : Constructing image captions in various approaches. Detailed explanations of these methods are givenin Appendix F. We use gpt-4 as the LLM and gpt-4V as the MLLM. Strikethrough texts indicate erroneousdescriptions or descriptions unrelated to the image. This case is sourced from one of the PubMed papers at",
  "tent unrelated to the image.LLM-Reformatted": "misinterprets three sub-images as a CT slide, lead-ing to misleading descriptions, and fails to excludeirrelevant content. GPT4v-Distill generates factu-ally incorrect descriptions due to the lack of con-textual text. In contrast, MLLM-Reformatted pro-duces superior descriptions by leveraging both vi-sual information and contextual cues. It accuratelyand thoroughly describes the key information ofthe image. The subsequent experiment in .3 further demonstrates the higher data quality ofMLLM-Reformatted.",
  "Data Collection": "To acquire a comprehensive dataset of PubMedmedical images, we integrated previously compiledpublic data of PubMed images, specifically LLaVA-Med PMC (514K) (Li et al., 2023a), PMC-Inline(11M) (Wu et al., 2023), and PMC-OA (1M) (Linet al., 2023). Although extensive, the majority ofthis merged data consists of charts and graphs frompapers rather than medical images. Therefore, weimplemented a rigorous data filtering pipeline: (1)Text Filtering. A medical vocabulary was used tofilter out data where the contextal text contains asufficient number of medical terms. (2) Image Fil-tering. We excluded low-resolution images (less",
  "MLLM": "Q: Please describe this picture. (Predefined)A: The image appears to be an axial section computedtomography (CT) scan of the abdomen, focused on thepancreatic region. In the center of the image, a well-defined, hypodense, unilocular cystic lesion can be seenwithin the head of the pancreas (as indicated by thearrow). The density of the cystic lesion is measured tobe around 2.4 Hounsfield Units (HU), which isconsistent with a fluid-filled cystic structure. Only thehead and uncinate segment of the pancreas are visiblein this particular image section. Caption : \"Density of the cystic lesion is 2.4Hounsfield Unit (HU).\"Inline Mention: \"Only the head and uncinatesegment of the pancreas was visualized and thehypodense unilocular cystic lesion wasrevealed at the head of pancreas ( ) .\"",
  "Alignment VQA": "Q: What is the location and characteristics of the cysticlesion observed in the CT scan?A: The cystic lesion is located within the head of thepancreas, as evidenced by the image. It appears to be awell-defined, unilocular cystic structure with a density ofapproximately 2.4 Hounsfield Units, indicating a fluid-filled lesion. The hypodense nature of the lesion, relativeto the surrounding pancreatic tissue, is consistent with acystic pathology within the pancreatic head region.",
  ": Image Diversity in PubMedVision. A random sample of 500 images from PubMedVision are categorized.Left: Distribution of body parts depicted in the images. Right: Distribution of imaging modalities": "than 336x336 pixels). A medical image classifica-tion model, trained on 1K manually labeled imagesand 10K MLLM-labeled images, is used to iden-tify medical images. (3) Deduplication. UsingSentence-BERT (Reimers and Gurevych, 2019) asthe encoder, we obtained semantic embeddings ofthe image captions and filtered out images withoverly similar contexts. For more details, pleasesee Appendix B. Ultimately, we filtered out 914,960 medical im-ages and their associated contextual text (captionsand inline mentions). illustrates the di-versity of medical modalities and image regionscovered by PubMedVisions images. These medi-cal images are then used to sequentially construct1.3 million VQA data points for medical alignment.",
  "MLLMs Reformatting": "Each collected data point includes one or moremedical images I and their corresponding contex-tual image descriptions X. As shown in , we provided I and X to MLLMs to generatemedical VQA data. According to ALLaVA (Chenet al., 2024), we generate two types of VQA datato enhance image alignment. Using the promptshown in , the MLLM generates an overallimage description d, a specific question q about theimage, and the corresponding answer a, as follows:",
  "and combined it with the image description d toform Alignment VQA (q, a). The predefined ques-tion was sampled from a set of predefined ques-": "Different QA scenarios Prompt Template Standard Q&A Senior Doctor and Intern Doctor and Patient's Family Intern and Specialist Doctor Medical Teacher and Student Doctor to Doctor Evaluator and AI Model Doctor and Difficult Patient AI Model Assisting Doctor AI Model Assisting Patient {medical_images} Please complete the following tasks based on the medical images and referenceinformation provided by me. 1. Generate a detailed and professional description (Image_description). Thedescription must reflect your professionalism and provide as many details aspossible from the image. The more comprehensive and precise, the better. 2. {QA_scenario_prompt} The contextual text is marked by <reference>. You need to refer to it to ensurethe accuracy of the content you generate, but do not mention the existence ofthis reference information when generating data. Your reply must be in JSON format, formatted as {{ \"Image_description\" : ..., \"QA-query\" : ..., \"QA-answer\" : ... }} <reference> {contextual_text} </reference> : Prompts used for data generation. {medical_images} represents medical images. {QA_scenario_prompt}denotes scenario prompts, sampled from the scenarios on the right, see Appendix D for details. {contextual_text}pertains to image captions and inline mentions.",
  "tions, which can be found in Appendix C. Accord-ing to ShareGPT-4V (Chen et al., 2023b), suchdetailed image descriptions help in learning thealignment from image to text": "Instruction-Tuning VQAWe used the ques-tion q and answer a generated by MLLMs asInstruction-Tuning VQA (q, a) for enhancinginstruction-following ability and image compre-hension. Unlike Alignment VQA, the questionsare generated by MLLMs specifically for the im-ages. To diversify the generated q, we designedeight different scenarios, as detailed in AppendixD. We randomly sample scenario settings into thesynthetic prompt to enable MLLMs to generatemore varied questions.Based on this method, we employ GPT4-Vision(gpt-4-turbo-2024-04-09) as MLLMs to synthesize647,031 Alignment VQA and 647,031 Instruction-Tuning VQA. Consequently, PubMedVision con-tains a total of 1.3 million data points.",
  "Experiment Settings": "Training and ValidationTo verify the effective-ness of PubMedVision, we selected the LLaVA-1.5model architecture combined with LLaMA-3-8B.We use the original settings of LLaVA-1.5, featur-ing a 336336 CLIP-Large mode (Radford et al.,2021) and a two-layer MLP Projector. For thebase LLM, we utilize LLaMA-3-8B, which is pre-trained on OpenHermes (Teknium, 2023) text in-struction data. We followed the same two-stagetraining method as LLaVA-1.5 (Liu et al., 2024)",
  "LLaVA-v1.5-LLaMA3-8B + PubMedVision": "This model uses both LLaVA-1.5 data andPubMedVision data. The data distributionis Pretraining:558K (LLaVA) + 647K(PubMedVision Alignment VQA); Finetun-ing: 658K (LLaVA) + 647K (PubMedVisionInstruction-Tuning VQA). BaselinesWe compared two types of open-source models: (1) Medical MLLMs. We evaluatedthree Medical MLLMs, including Med-Flamingo(Moor et al., 2023), RadFM (Wu et al., 2023), andLLaVA-Med-7B (Li et al., 2023a). (2) GeneralMLLMs. We compared the latest models in theLLaVA series, including LLaVA-v1.6-7B, LLaVA-v1.6-13B, and LLaVA-v1.6-34B (Liu et al., 2023).Additionally, we included comparisons with Yi-",
  "VL-34B (Young et al., 2024) and Qwen-VL-Chat(Bai et al., 2023)": "HuatuoGPT-VisionBuilding on PubMedVision,we developed our specialized medical MLLM,HuatuoGPT-Vision.It enhances LLaVA-v1.5-LLaMA3-8B + PubMedVision by featuring: (1)a larger model, utilizing Yi-1.5-34B (Young et al.,2024) as the foundational LLM; (2) bilingual ca-pabilities, supported by an additional 348K Chi-nese medical VQA dataset translated from Pub-MedVision; and (3) enhanced medical knowledge,with added training from the medical text corpusof HuatuoGPT-II (Chen et al., 2023a). BenchmarksTo verify the medical multimodalcapabilities of MLLMs, we employed three typesof benchmarks: (1) Medical VQA Benchmark, forwhich we used the test sets of VQA-RAD (Lauet al., 2018), SLAKE (Liu et al., 2021), PathVQA(He et al., 2020), and PMC-VQA (Zhang et al., 2023c) to assess medical question-answering capa-bilities. Specifically, for VQA-RAD and SLAKE,we used English CLOSED segment and the accu-racy metric. (2) Multimodal Benchmark: MMMU(Yue et al., 2024) is a popular multimodal bench-mark, and we utilized the Health & Medicine trackof MMMU, which is relevant to medical multi-modality. (3) Traditional Medical Imaging Tasks.We used the open access part of the OmniMedVQAdataset (Hu et al., 2024), which includes 42 tradi-tional medical imaging datasets, all formatted asVQA. Note that for all benchmarks, we use thezero-shot method and the question template set byLLaVA, as shown in Appendix E.",
  "HuatuoGPT-Vision-34B64.662.550.654.144.254.4": ": Results on the test set for the MMMU Health & Medicine track. The Health & Medicine track is dividedinto five categories: BMS for Basic Medical Science, CM for Clinical Medicine, DLM for Diagnostics andLaboratory Medicine, P for Pharmacy, and PH for Public Health. Results are obtained by submitting to the officialwebsite. specific MLLMs like LLaVA-Med-7B, aligningwith the findings of studies (Hu et al., 2024). How-ever, the addition of medical multimodal data toLLaVA-v1.5-LLaMA3-8B significantly enhancesperformance, revealing substantial potential for im-proving medical image understanding. Notably, theuse of the PubMedVision led to an 11.7% increasein overall accuracy, significantly outperforming theearlier LLaVA_Med dataset. Additionally, as de-tailed in Appendix A, fine-tuning on the trainingsets of these four datasets indicates that PubMedVi-sion can also significantly improves performancein downstream medical multimodal tasks. OmniMedVQA EvaluationOmniMedVQA in-tegrates 41 traditional medical imaging tasks, allformatted as VQA. presents the results ofit across 8 different modalities. After incorporatingPubMedVision, the performance of LLaVA-v1.5-LLaMA3-8B showed a significant improvement of26.3%, which is notably higher than the 16.7% im-provement achieved with the LLaVA_Med dataset.With PubMedVision, LLaVA-v1.5-LLaMA3-8Boutperforms previous open-source models. MMMU Health & Medicine TrackMMMU isa widely recognized multimodal benchmark, andwe utilize its Health & Medicine Track for as-sessment. Figure presents the results ofthe MMMU test set, showing that LLaVA-v1.5-LLaMA3-8B + PubMedVision surpassed othermodels in the Health & Medicine Track, withperformance comparable to the larger-parameterLLaVA-v1.6-34B. These findings further validate",
  "Data Quality of PubMedVision": "Experimental SetupTo validate the effectof the MLLM reformatter in PubMedVision,we constructed four datasets based on the fourcaption construction methods described in .2. Specifically, we randomly sampled 60,000image-context pairs from PubMedVision to createthese four distinct datasets. For each caption, wepre-set the question: \"Please provide a descriptionof the given medical image\" to form VQA datasets,which we refer to asNative-Captions-60K ,",
  "+ Native Caption of PubMedVision60.865.256.945.6+ PubMedVision63.874.559.952.7": ": Comparison of different datasets. The 60K dataset is added only in the second stage of training. NativeCaption of PubMedVision refers to using the original image captions, incorporated in both phases to match thetraining of PubMedVision. entries. Three medical experts are invited to eval-uate these image descriptions, each reviewing anequal number from each category. The criteria in-cluded: 1) Accuracy: correctness of the description,2) Relevance: relevance to the image and avoidanceof irrelevant details, 3) Completeness: inclusion ofkey medical features, and 4) Usefulness: utilityfor medical decision-making, diagnosis, and treat-ment planning. Each item is rated on a scale of1-5. Detailed scoring criteria are in Appendix I. shows the scoring results (average values).Although Native-Captions had high accuracy, itsrelevance and completeness were lacking. LLM-Reformatted improved in relevance but still lackedcompleteness. GPT4v-Distill had good relevanceand completeness but fell short in accuracy andusefulness. MLLM-Reformatted showed the bestcompleteness and usefulness, with high accuracyand relevance, indicating superior quality. EmpiricalEvaluationUsingLLaVA-v1.5-LLaMA3-8B, we evaluated four datasets toenhance medical multimodal capabilities.Asshown in , the MLLM-Reformatted methodoutperforms other datasets with the same datavolume, demonstrating superior alignment inmedical multimodal applications. Additionally, acomparison between the full datasets of PubMedVi-sion and native caption reveals that PubMedVisionperforms significantly better, supporting the use ofMLLMs for data reformatting.",
  "Related Works": "Multimodal Large Language ModelsRecentadvancements in MLLMs leverage the capabil-ities of LLMs such as LLaMA to integrate vi-sual features into the textual space.Notably,Flamingo (Alayrac et al., 2022) introduces visualfeatures by incorporating cross-attention layers into LLMs. To align multimodal features effec-tively, BLIP2 (Li et al., 2023b) integrates a pre-trained visual encoder with LLMs through a novelQ-former. InstructBLIP (Dai et al., 2024) furtherrefines this approach by enhancing performanceusing instruction-following data. Following thistrend, LLaVA (Liu et al., 2024) and subsequentMLLMs (Zhu et al., 2023; Ye et al., 2023) utilizehigh-quality multimodal data for instruction tun-ing, demonstrating significant improvements. Ad-ditionally, ALLVA (Chen et al., 2024) shows thateven a small model (3B) can achieve impressive re-sults with high-quality Visual Question Answering(VQA) data. This underscores the importance ofmultimodal data. Medical MLLMsEncouraged by the successof medical LLMs such as ChatDoctor (Yunxi-ang et al., 2023), MedicalGPT (Xu, 2023), Hu-atuoGPT (Zhang et al., 2023a; Chen et al., 2023a),and Apollo (Wang et al., 2024), researchers havebeen focusing on developing a medical Multi-modal LLM capable of understanding medical im-ages. Med-Flamingo (Moor et al., 2023) extendsFlamingo to the medical domain by utilizing medi-cal multimodal data for pre-training. LLaVA-Med(Li et al., 2023a) refines this approach by filteringimage-text pairs from PubMed papers and smallerVQA datasets synthesized by LLMs to train a med-ical MLLM based on LLaVAs parameters. Addi-tionally, (Zhang et al., 2023c) created the PMC-VQA dataset for medical VQA by self-instructionon PMC-OA (Lin et al., 2023). Using this dataset,they developed MedVInT. RadFM (Wu et al., 2023)integrates a large amount of medical multimodaldata, including 2D and 3D radiology images, toconstruct a radiology MLLM. However, accordingto recent findings (Hu et al., 2024), current medicalmodels still lag behind general medical models in",
  "medical multimodal, indicating that higher qualitydatasets are needed for medical multimodal appli-cations": "Medical VQA DatasetsTo enhance image-textalignment and develop medical multimodal chat-bots, researchers have focused on constructing med-ical VQA datasets. VQA-RAD (Lau et al., 2018),SLAKE (Liu et al., 2021), and Path-VQA (He et al.,2020) are among the earliest medical VQA datasets.However, their sample sizes are small (less than20K) and their diversity is limited, primarily toradiology modalities. Subsequently, PMC-VQA(Zhang et al., 2023c) expands the dataset scaleby using image-text data from PubMed papersand rewriting it into VQA format using LLMs.LLaVA-Med VQA (Li et al., 2023a) data is derivedfrom filtering higher quality data from PMC-15M(Zhang et al., 2023b) and synthesizing VQA us-ing LLMs. PMC-CaseReport (Lau et al., 2018)filters case images from PubMed and generatesVQA using LLMs, though it retains only radiologymodality images. Currently, there is still a needfor more comprehensive and larger-scale medicalVQA datasets.",
  "Conclusion": "In this study, we refined high-quality data from nu-merous medical image-text pairs on PubMed. Wethen employ MLLM-powered reformatting methodto enhance this data. In this way, we constructPubMedVision, a large-scale, high-quality medi-cal multimodal dataset. Experimental results showthat PubMedVision significantly boosts the mul-timodal capabilities of MLLMs, with marked im-provements on benchmarks. This inspires the ideathat PubMed holds great potential to advance medi-cal multimodal capabilities, with the key challengebeing how to improve data quality, despite the pres-ence of many non-medical images and poor de-scriptions. We hope that the proposed PubMedVi-sion dataset can aid the development of medicalMLLMs in the future.",
  "cies, leading to potential errors in the dataset.Future studies may benefit from enhanced val-idation processes to mitigate this issue": "Limited Scenario Diversity: The Instruction-Tuning VQA of PubMedVision are generatedbased on 10 predefined scenarios. This lim-ited scope may have constrained the diversityof the dataset. Expanding the range of sce-narios could enhance the datasets comprehen-siveness and applicability to diverse medicalsituations. Data Selection Bias: The rigorous image se-lection strategy during data preparation en-sured high-quality data but may have excludedpotentially valuable data. Future data collec-tion efforts should consider a more inclusiveselection approach to optimize data utility andaddress potential biases inherent in PubMeddata. Limitations of AI-Generated Data: The re-liance on AI-generated content for medicalapplications, as seen in this dataset, bringsinherent limitations and risks, particularly interms of accuracy and reliability. This shouldbe carefully considered when applying suchdata in medical contexts. Coverage of Medical Specialties: There maybe gaps in the medical specialties or imag-ing modalities covered by the dataset, whichcould limit its effectiveness in addressing cer-tain medical queries or conditions.",
  "Ethical Statement": "Our dataset was generated by the GPT4-V model, itmay contain hallucinations or inaccuracies. Giventhis potential limitation, we strictly limit the useof the dataset to research purposes only. It is notto be employed in clinical or other industry ap-plications where its use could lead to unintendedconsequences due to these possible inaccuracies.We emphasize the ethical responsibility of users toadhere to this restriction to ensure the safety andintegrity of their applications.",
  "ThisworkwassupportedbytheShen-zhenScienceandTechnologyProgram(JCYJ20220818103001002), Shenzhen DoctoralStartup Funding (RCBS20221008093330065),": "Tianyuan Fund for Mathematics of NationalNatural Science Foundation of China (NSFC)(12326608), Shenzhen Key Laboratory of Cross-ModalCognitiveComputing(grantnumberZDSYS20230626091302006),andShenzhenStability Science Program 2023, Shenzhen KeyLab of Multi-Modal Cognitive Computing. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,Antoine Miech, Iain Barr, Yana Hasson, KarelLenc, Arthur Mensch, Katherine Millican, MalcolmReynolds, et al. 2022. Flamingo: a visual languagemodel for few-shot learning. Advances in neuralinformation processing systems, 35:2371623736. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,and Jingren Zhou. 2023. Qwen-vl: A frontier largevision-language model with versatile abilities. arXivpreprint arXiv:2308.12966. Guiming Hardy Chen, Shunian Chen, Ruifei Zhang,Junying Chen, Xiangbo Wu, Zhiyi Zhang, ZhihongChen, Jianquan Li, Xiang Wan, and Benyou Wang.2024. Allava: Harnessing gpt4v-synthesized datafor a lite vision-language model.arXiv preprintarXiv:2402.11684. Junying Chen, Xidong Wang, Anningzhe Gao, FengJiang, Shunian Chen, Hongbo Zhang, Dingjie Song,Wenya Xie, Chuyi Kong, Jianquan Li, et al. 2023a.Huatuogpt-ii, one-stage training for medical adaptionof llms. arXiv preprint arXiv:2311.09774. Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Con-ghui He, Jiaqi Wang, Feng Zhao, and DahuaLin. 2023b.Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprintarXiv:2311.12793. Zhihong Chen, Yuhao Du, Jinpeng Hu, Yang Liu,Guanbin Li, Xiang Wan, and Tsung-Hui Chang.2022. Multi-modal masked autoencoders for med-ical vision-and-language pre-training. In Interna-tional Conference on Medical Image Computingand Computer-Assisted Intervention, pages 679689.Springer. Wenliang Dai, Junnan Li, Dongxu Li, AnthonyMeng Huat Tiong, Junqi Zhao, Weisheng Wang,Boyang Li, Pascale N Fung, and Steven Hoi.2024. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advancesin Neural Information Processing Systems, 36.",
  "Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, andPengtao Xie. 2020. Pathvqa: 30000+ questions formedical visual question answering. arXiv preprintarXiv:2003.10286": "Yutao Hu, Tianbin Li, Quanfeng Lu, Wenqi Shao,Junjun He, Yu Qiao, and Ping Luo. 2024. Omn-imedvqa: A new large-scale comprehensive evalu-ation benchmark for medical lvlm. arXiv preprintarXiv:2402.09181. Qiao Jin, Fangyuan Chen, Yiliang Zhou, Ziyang Xu,Justin M Cheung, Robert Chen, Ronald M Summers,Justin F Rousseau, Peiyun Ni, Marc J Landsman,et al. 2024. Hidden flaws behind expert-level ac-curacy of gpt-4 vision in medicine. arXiv preprintarXiv:2401.08396.",
  "Jason J Lau, Soumya Gayen, Asma Ben Abacha, andDina Demner-Fushman. 2018. A dataset of clini-cally generated visual questions and answers aboutradiology images. Scientific data, 5(1):110": "Chunyuan Li, Cliff Wong, Sheng Zhang, NaotoUsuyama, Haotian Liu, Jianwei Yang, Tristan Nau-mann, Hoifung Poon, and Jianfeng Gao. 2023a.Llava-med: Training a large language-and-visionassistant for biomedicine in one day.ArXiv,abs/2306.00890. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.2023b. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large lan-guage models. In International conference on ma-chine learning, pages 1973019742. PMLR. Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, ChaoyiWu, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2023.Pmc-clip: Contrastive language-image pre-trainingusing biomedical documents. In International Con-ference on Medical Image Computing and Computer-Assisted Intervention, pages 525536. Springer. Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, andXiao-Ming Wu. 2021. Slake: A semantically-labeledknowledge-enhanced dataset for medical visual ques-tion answering. In 2021 IEEE 18th InternationalSymposium on Biomedical Imaging (ISBI), pages16501654. IEEE.",
  "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong JaeLee. 2024. Visual instruction tuning. Advances inneural information processing systems, 36": "Michael Moor, Qian Huang, Shirley Wu, MichihiroYasunaga, Yash Dalmia, Jure Leskovec, Cyril Za-kka, Eduardo Pontes Reis, and Pranav Rajpurkar.2023. Med-flamingo: a multimodal medical few-shotlearner. In Machine Learning for Health (ML4H),pages 353367. PMLR. Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,et al. 2021. Learning transferable visual models fromnatural language supervision. In International confer-ence on machine learning, pages 87488763. PMLR.",
  "Zhiling Yan, Kai Zhang, Rong Zhou, Lifang He, XiangLi, and Lichao Sun. 2023. Multimodal chatgpt formedical applications: an experimental study of gpt-4v. arXiv preprint arXiv:2310.19061": "Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye,Ming Yan, Yiyang Zhou, Junyang Wang, An-wen Hu, Pengcheng Shi, Yaya Shi, et al. 2023.mplug-owl: Modularization empowers large lan-guage models with multimodality. arXiv preprintarXiv:2304.14178. Alex Young, Bei Chen, Chao Li, Chengen Huang,Ge Zhang, Guanwei Zhang, Heng Li, JiangchengZhu, Jianqun Chen, Jing Chang, et al. 2024. Yi:Open foundation models by 01. ai. arXiv preprintarXiv:2403.04652. Zheng Yuan, Qiao Jin, Chuanqi Tan, Zhengyun Zhao,Hongyi Yuan, Fei Huang, and Songfang Huang. 2023.Ramm: Retrieval-augmented biomedical visual ques-tion answering with multi-modal pre-training. InProceedings of the 31st ACM International Confer-ence on Multimedia, pages 547556.",
  "Li Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, andZhang You. 2023. Chatdoctor: A medical chat modelfine-tuned on llama model using medical domainknowledge. arXiv preprint arXiv:2303.14070": "Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhi-hong Chen, Jianquan Li, Guiming Chen, XiangboWu, Zhiyi Zhang, Qingying Xiao, et al. 2023a. Hu-atuogpt, towards taming language model to be a doc-tor. arXiv preprint arXiv:2305.15075. Sheng Zhang, Yanbo Xu, Naoto Usuyama, JaspreetBagga, Robert Tinn, Sam Preston, Rajesh Rao,Mu Wei, Naveen Valluri, Cliff Wong, et al. 2023b.Large-scale domain-specific pretraining for biomed-ical vision-language processing.arXiv preprintarXiv:2303.00915, 2(3):6. Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weix-iong Lin, Ya Zhang, Yanfeng Wang, and WeidiXie. 2023c. Pmc-vqa: Visual instruction tuning formedical visual question answering. arXiv preprintarXiv:2305.10415.",
  "AMore Experiments": "Fine-tuned Results of VQA BenchmarksToverify whether PubMedVision can enhance down-stream tasks, we fine-tuned the model using thetraining set of the Benchmarks. As shown in , PubMedVision effectively improves downstreammedical tasks, significantly benefiting all four VQAdownstream tasks. For VQA-RAD and SLAKE,we increased the number of training epochs, and itperformed better than traditional fine-tuning meth-ods, such as M3AE (Chen et al., 2022) and RAMM(Yuan et al., 2023). The results are shown in .",
  "Results on validation set of MMMU": "presents the validation results of MMMU, whereLLaVA-v1.6-34B exhibits superior overall perfor-mance. However, compared to the test set results ofMMMU (official submission) in , LLaVA-v1.5-LLaMA3-8B combined with PubMedVisiondemonstrates better performance. Overall, Pub-MedVision allows the 8B version of LLaVA toachieve effects comparable to the 34B version inmedical applications. Applicability of PubMedVisionTo verify theapplicability of PubMedVision across differentMLLM models, we further trained PubMedVisionon other MLLM models, specifically LLaVA-v1.5-7B and Qwen-VL-Chat. As demonstrated in , PubMedVision effectively enhances the medicalmultimodal capabilities of these diverse MLLMmodels as well.",
  "BData Pipline": "To acquire a comprehensive dataset of PubMed im-ages, we integrated previously compiled PubMedimage and contextual text data, specifically LLaVA-Med PMC data (514K) (Li et al., 2023a), PMC-Inline (11M) (Lau et al., 2018), and PMC-OA (1M)(Lin et al., 2023). Although the dataset is extensive,most of the data consists of charts and graphs frompapers rather than medical images. Therefore, weneed to filter out higher-quality medical image-textdata. We established a pipeline as follows:",
  ". Contextual Text Filtering: Utilizing the SPE-CIALIST Lexicon 2 from the Unified MedicalLanguage System, we employed GPT-4 to fil-ter out common phrases, creating a refined": "medical lexicon. Using this lexicon, we as-sessed the number of medical terms in imagecaptions, filtering out data with fewer than fivemedical terms. This ensures the captions aresufficiently informative. 2. Image Filtering: Initially, we excluded im-ages with a resolution lower than 336x336pixels to ensure quality. Next, we filtered outchart images to retain only medical images.To accurately identify non-medical images,we manually labeled 1K images and synthe-sized 10K image labels using MLLMs (GPT4-Vision). We then trained a classifier based onthe CLIP image encoder, achieving a 91% ac-curacy on the validation set. This classifier isused to filter out non-medical images. 3. Deduplication: We applied a semantic re-triever for deduplication. Using all-mpnet-base-v2 (Reimers and Gurevych, 2019) as theencoder, we generated semantic embeddingsof the image captions. We then removed im-ages with an embedding dot product similarityexceeding 480, ensuring a unique and high-quality dataset.",
  "DPrompts for different QA scenarios": "In our study, Instruction-Tuning VQA is generatedbased on ten pre-set different scenarios. This ap-proach covers a broader range of medical topicsand scenarios, thereby enhancing the diversity ofthe QA pairs, and more comprehensively improv-ing the ability to follow instructions. The samplingmethod also prevents the overconcentration or ab-sence of certain scenarios, contributing to data bal-ance, which in turn improves the performance andstability of the model. Our ten scenario promptsare displayed from to .",
  "HDiscrepancies in Evaluation Resultswith LLaVA-Med": "Our evaluation of LLaVA-Med-7B shows somedifferences from the original paper, as detailed in.Using the official inference code, we confirmedno issues with our implementation. The discrep-ancy likely arises from differences in task prompts.While we used the multiple-choice prompts speci-fied for LLaVA, these may not match those in theLLaVA-Med evaluations, which were not fully de-scribed.Although VQA-RAD and SLAKE scores werelower, PathVQA improved. For fairness, we alsocompared LLaVA-8B + LLaVA-Med, which usedthe same two-stage training data. These resultswere closer to the original, with notable improve-ment in PubMedVision.",
  "Standard Q&A": "You need to generate a question-and-answer pair based on this image. The question should bedesigned to test other models understanding of this medical image; it should be phrased simply andconversationally. However, your response should be professional, showcasing your understandingof the medical image by providing useful information derived from the image and detailed analysis.The reply should offer detailed and rich useful information.",
  "Doctor and Patients Family": "You need to generate a question-and-answer pair based on this image. You need to play the rolesof a doctor and a patients family member, discussing the results shown in the image. The doctorshould explain the imaging findings in laymans terms and answer any questions posed by the familymember. The family member may inquire about the cause of the disease, severity, treatment options,and related content. The doctor should answer patiently to ensure that the family member fullyunderstands the condition.",
  "Intern and Specialist Doctor": "You need to generate a question-and-answer pair based on this image. You should adopt the toneof an intern to ask questions and a specialist doctor to answer them. The answers should provideuseful information derived from the image and give a detailed analysis. The response shouldprovide detailed and rich useful information.",
  "Medical Teacher and Student": "You need to generate a question-and-answer pair based on this image. You need to act as a medicalteacher and a student, engaging in an educational interaction about the image. The teacher shouldpose questions, asking the student to analyze the image and propose possible diagnoses. Thestudent should answer the questions and explain their observations and reasoning process.",
  "Doctor and Difficult Patient": "You need to generate a question-and-answer pair based on this image. You need to act as a doctorcommunicating with a patient who is skeptical about their diagnosis. The patient may pose a seriesof tricky questions, questioning the doctors explanations and treatment suggestions. The doctorneeds to use the imaging data patiently and explain the condition in an easy-to-understand manner,addressing all the patients queries to alleviate their concerns and build trust. :Prompt for Doctor and Difficult Patient Scenario: A simulated dialogue where a doctor patientlycommunicates a diagnosis to a skeptical patient, using the image data to explain the condition in a comprehensibleway, and address all queries to build trust.",
  "AI Model Assisting Doctor": "You need to generate a question-and-answer pair based on this image. You need to act as a doctorusing an AI model to analyze a medical image to better understand a patients condition. Thedoctor should ask specific questions about structures, abnormalities, and potential clinicalsignificance visible on the image. The AI model should provide detailed analyses based on itsalgorithms but not make final clinical diagnoses. The doctor will use the information provided bythe AI model to aid their diagnostic decision-making process.",
  "AI Model Assisting Patient": "You need to generate a question-and-answer pair based on this image. You need to act as an AI modelinteracting with a patient who has questions about visible content on their medical image. Thepatient may be curious or confused about certain structures or markings on the image and seeksclear explanations. The AI model should explain specific details such as tissue density, shape,or any abnormal areas potential meanings, maintaining simplicity and avoiding excessive medicaljargon. The AI models response should aim to provide educational information to help the patientbetter understand their imaging results, emphasizing that final interpretations and diagnosesmust be done by a professional doctor. : Prompt for AI Model Assisting Patient Scenario: A simulated dialogue where an AI model explainsdetails on a patients medical image, aiming to clarify patient queries, while emphasizing that final interpretationsare by professional doctors.",
  "Prompt for LLM-Reformatted": "You have been provided with textual context information of images from a biomedical researchpaper, but you do not have access to the actual image. You need to respond to the followingquestion based on this images context information. In your response, avoid using phrases like mentioned, caption, or context.Instead,describe the information as if it were directly observed in the image. Answer responsibly,avoiding any overconfidence, and refrain from giving medical advice or diagnostic information.Encourage the user to consult a healthcare professional for further advice.",
  "Please rate the image based on the following criteria (1 to 5):": "1.Accuracy:The rating should be based on whether the description accurately reflectsthe medical features and information in the image. A score of 5 indicates complete accuracy,while a score of 1 indicates complete inaccuracy. 2.Relevance:Theratingshouldbebasedonthedegreeoftightassociationbetweenthe described content and the key medical features and information in the image, and whether itsuccessfully avoids the depiction of irrelevant details in the image. A score of 5 indicateshigh relevance, while a score of 1 indicates complete irrelevance. 3.Completeness:Theratingshouldbebasedonwhetherthedescriptionincludesallthe key medical features and information in the image.A score of 5 indicates completecompleteness, while a score of 1 indicates significant omissions. 4.Practicality:Theratingshouldbebasedontheextenttowhichthedescriptionhelps with medical decision-making, diagnosis, and treatment planning. A score of 5 indicateshigh practicality with in-depth descriptions, while a score of 1 indicates that the descriptionis superficial and not practical.",
  "Sample 1: Standard Q&A": "DescriptionThe image shows a T1-weighted MRI scan with intravenous gadolinium enhance-ment. The scan reveals a well-defined, homogeneously enhancing mass locatedwithin the left sphenoid sinus. The mass appears to be occupying a significantportion of the sphenoid sinus, with no obvious signs of infiltration into adjacentstructures. The surrounding soft tissues appear normal, and there is no evidence ofbony erosion or destruction. Overall, the imaging findings are consistent with asphenoid sinus mass, likely a benign tumor or other pathological process confinedwithin the sphenoid sinus.",
  "QA-queryWhat is the location of the mass observed in the MRI image?": "QA-answerThe mass observed in the MRI image is located within the left sphenoid sinus. Thesphenoid sinus is one of the paranasal sinuses, located deep within the skull base,behind the eyes. The MRI scan clearly shows the mass occupying a significantportion of the left sphenoid sinus, with no obvious extension into surroundingstructures.",
  "Sample 2: Evaluator and AI Model": "DescriptionThis image depicts a microscopic view of a liver tissue section stained withtrichrome, highlighting various histological features. The predominant colorsare varying shades of pink and red, representing hepatocytes, and blue areas indi-cating connective tissue and collagen fibers. The structure of the tissue is organizedwith distinct lobular architecture. The hepatocytes are mostly uniform in size andappearance, with some regions showing slight signs of ballooning, indicative ofearly hepatocyte degeneration which is not overtly prominent. Sinusoids are visiblebetween hepatocyte plates, and there are no clear signs of inflammation or fibrosiswithin the tissue section viewed.",
  "Sample 3: Intern and Specialist Doctor": "DescriptionThe image presents a cross-sectional view from a chest computed tomography(CT) scan, showcasing the right lung of the patient. Prominently visible is asolid mass with spiculated edges located in Segment 9b of the right lung. Thismass exhibits a defined shape with a maximum diameter of approximately 30mm,indicating its noticeable size relative to surrounding lung structures. Additionally,the lung parenchyma displays scattered areas of decreased attenuation consistentwith emphysematous changes, which are indicative of air trapping and lung tissuedegradation. QA-queryAs an intern, I noticed a solid mass with spiculated edges on this CT scan. Couldyou explain the potential implications of these features and the significance of itslocation in the right lungs Segment 9b? QA-answerThis CT scan reveals a solid mass with spiculated margins located in Segment 9b ofthe right lung, which significantly raises concerns for a malignant tumor, possibly atype of non-small cell lung cancer. Spiculated edges are typically suggestive of anaggressive tumor that may invade adjacent tissues, a finding that warrants furtherdiagnostic evaluation and possibly biopsy to confirm malignancy. The locationin Segment 9b, which is a part of the lung often less affected by other commonconditions, emphasizes the need for targeted investigation. Managing such a casewould likely involve a multidisciplinary approach including oncology, thoracicsurgery, and radiology, to determine the best therapeutic strategies and follow-upcare.",
  "Sample 4: Doctor and Difficult Patient": "DescriptionThe image is a coronal reformat of an abdominal CT scan. It distinctly displaystwo major findings. On the left side of the image (patients right side), a mass isvisible in the head of the pancreas, indicated by a thin arrow. This mass appears asa localized area of denser tissue compared to its surroundings. On the right sideof the image (patients left side), there is evidence of massive gastric dilatation,marked by a thick arrow. This dilatation shows the stomach expanded significantly,occupying a large portion of the abdominal cavity, which is abnormally largecompared to the usual size of the stomach.",
  "QA-queryDoctor, how serious is the mass in my pancreas and what are the potential riskswith this gastric dilatation?": "QA-answerThe mass in the head of your pancreas could represent several things rangingfrom benign conditions to more serious ones like pancreatic cancer. We will needfurther tests, possibly including a biopsy, to determine the exact nature of themass. As for the gastric dilatation, its quite significant and can lead to discomfort,pain, and other problems like nausea or vomiting if not managed timely. It mayalso indicate an obstruction or a severe motility disorder. Both conditions areserious and require careful evaluation and management. Well plan some follow-upassessments to understand better and address both the mass and the dilatationappropriately, ensuring the best possible treatment plan is set for you.",
  "Sample 5: Doctor and Patients Family": "DescriptionThe image displays a series of axial cuts of computed tomography (CT) scans fromthe thorax and abdomen. In the thoracic region, there are multiple tiny nodules andcystic lesions visible in both lungs. A significant left pleural effusion is evidentwith signs of pneumothorax, characterized by the presence of air in the pleuralspace which can be identified by the visceral pleural line and a clear space devoidof lung markings. Additionally, there is evidence of pericardial effusion as seen bythe abnormal accumulation of fluid around the heart. The CT scans of the abdomenshow enlargement of multiple lymph nodes and multiple hepatic masses, whichappear as irregular areas within the liver.",
  "QA-queryDoctor, can you explain what all these findings mean? How serious are theseconditions and what are the possible treatments?": "QA-answerCertainly, I understand your concerns. The findings indicate a few serious con-ditions. Multiple tiny nodules and cystic lesions in the lungs could be indicativeof an infectious or inflammatory process, or less likely, metastatic disease. Thelarge amount of left pleural effusion and the presence of pneumothorax are con-cerning as they can compromise lung function. Pleural effusion means there isfluid accumulation that can restrict lung expansion, and pneumothorax involves airleaking into the space between the lung and chest wall, which can cause the lung tocollapse. Pericardial effusion, the fluid around the heart, can affect heart functionif severe. The enlarged lymph nodes and hepatic masses in the abdomen mightsuggest a possible malignancy or metastatic spread. Treatment options would varybased on the underlying cause but could include drainage of fluid from the pleuraland pericardial spaces, antibiotics or chemotherapy if infection or malignancy areconfirmed, and potentially surgery to address the hepatic masses. A multidisci-plinary team approach will be crucial in managing these conditions effectively.",
  "Sample 6: Medical Teacher and Student (Multiple Images)": "DescriptionThe provided images depict two AP (anteroposterior) pelvis radiographs. Thefirst radiograph shows both hips with symmetric placement of screws or pins inwhat appears to be a bilateral surgical intervention, possibly for hip stabilization.There is visible deformation of the pelvic bones and the acetabulae (hip sockets)show signs of protrusion into the pelvic cavity. The second image features morepronounced skeletal changes including a notably misaligned right femur withsurgical hardware adjustments, possibly indicating a post-injury or post-surgicalstate with a complex fixation device attached to the right side extending from thepelvis to the femur. These images suggest significant progression in the patientscondition, hinting at ongoing issues with hip stability and pelvic deformation.",
  "QA-queryGiven these pelvis radiographs, can you describe any observable changes or indica-tors that suggest a progression in the patients pelvic condition?": "QA-answerCertainly, examining the radiographs, one can note an increased deformation inthe pelvic region on the second image compared to the first. This includes greaterprotrusion of acetabular regions and a more complex assembly of surgical hardwareon the right side, suggesting increased intervention. Such findings likely indicatea significant progression of underlying conditions affecting the pelvic structurewhich necessitated additional surgical measures over time.",
  "Sample 7: Evaluator and AI Model (Multiple Images)": "DescriptionThese images are medical ultrasound scans of the eye. Image A shows an ultra-sound biomicroscopy (UBM) scan depicting angle closure with a shallow anteriorchamber (indicated by the thick arrow) and anterior rotation of the ciliary processwith ciliochoroidal effusion (thin arrow). Image B is a B-scan ultrasonographyshowing diffuse thickening of the choroid (asterisk). Image C, taken one monthafter treatment, shows a UBM scan of the left eye with a normal anterior chamberangle and the disappearance of ciliochoroidal effusion. Image D is another B-scanultrasonography, which shows no choroidal thickening."
}