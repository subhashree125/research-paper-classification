{
  "Abstract": "The inability to utilise future contexts and thepre-determined left-to-right generation orderare major limitations of unidirectional languagemodels. Bidirectionality has been introducedto address those deficiencies. However, a cru-cial shortcoming of bidirectional language mod-els is the potential inconsistency of their con-ditional distributions. This fundamental flawgreatly diminishes their applicability and hin-ders their capability of tractable sampling andlikelihood computation. In this work, we intro-duce a class of bidirectional language models,called latent language models, that are consis-tent by definition and can be efficiently usedboth for generation and scoring of sequences.We define latent language models based onthe well-understood formalism of bisequentialdecompositions from automata theory. Thisformal correspondence allows us to preciselycharaterise the abilities and limitations of a sub-class of latent language models, called rationallanguage models. As a result, we obtain thatlatent language models are exponentially moreconcise and significantly more expressive thanunidirectional language models.",
  "Introduction": "Recently, language models (Bengio et al., 2003;Mikolov et al., 2010; Brown et al., 2020) have es-tablished themselves as the primary approach forsolving natural language processing tasks (Devlinet al., 2019; Raffel et al., 2020; Brown et al., 2020).They have also exhibited noteworthy capabilitiesin computer programming (Chen et al., 2021; Friedet al., 2023) and commonsense and mathematicalreasoning (Wei et al., 2022; Zhou et al., 2023).Language models are traditionally differentiatedbased on the contextual conditioning that they usefor token prediction. Unidirectional (or autore-gressive) language models, such as those based onrecurrent neural networks (RNNs) (Mikolov et al., 2010) as well as the Transformer-based GPT mod-els (Radford et al., 2019; Brown et al., 2020), con-dition the prediction of a given token only on itsleft context. On the other hand, bidirectional (ormasked) language models, such as those based onbidirectional RNNs (Arisoy et al., 2015; Mousa andSchuller, 2017) as well as the Transformer-basedBERT (Devlin et al., 2019; Liu et al., 2019) andT5 (Raffel et al., 2020; Xue et al., 2021), predicttokens based on both their left and right contexts.Naturally, because of the access to richer con-textual information, bidirectional language modelshave proven to produce stronger learned represen-tations (Devlin et al., 2019; Raffel et al., 2020). Inthis regard, Brown et al. (2020) speculate that theinability of GPT-3 to benefit from future contextscould explain its inferior performance on certaintasks where bidirectionality is important. Addition-ally, several studies have brought attention to theimportance of the order in which sequences are pro-cessed (Vinyals et al., 2015; Ford et al., 2018). Tothis end, both empirical (Emelianenko et al., 2019;Li et al., 2021) and theoretical (Lin et al., 2021) re-sults have implied that, as opposed to bidirectionallanguage models, the pre-determined left-to-rightorder used by unidirectional language models isoften suboptimal for tasks that require exploration,planning or strategic lookahead (Yao et al., 2023).Despite of the aforementioned advantages ofbidirectional language models, it is currently un-clear how to tractably ensure the consistency oftheir conditional distributions.1 In other words, it isnon-trivial to guarantee the existence of a joint dis-tribution whose conditionals coincide with those ofa given bidirectional language model (Goyal et al.,2022; Torroba Hennigen and Kim, 2023; Younget al., 2024). Furthermore, even if such a jointdistribution exists, it is often computationally ex-",
  "On the other hand, for unidirectional language models, Duet al. (2023) have given sufficient conditions for consistencythat can be easily enforced": "pensive to access it explicitly. Those fundamentalflaws of bidirectional language models greatly hin-der their applicability for sampling (Ghazvininejadet al., 2019) and likelihood computation (Salazaret al., 2020), and often lead to self-contradictorybehavior during inference (Young et al., 2024).In this work, we introduce a class of bidirectionallanguage models that are consistent by definitionand can be efficiently used both for generation andscoring of sequences. To achieve this, we con-sider language modelling from the point of viewof automata theory (Eilenberg, 1974; Sakarovitch,2009; Mihov and Schulz, 2019). Svete and Cot-terell (2023) have already explored the relationsbetween unidirectional language models and se-quential transducers. We extend their work by con-sidering the bidirectional formalism of bisequentialdecompositions (Elgot and Mezei, 1965). By exam-ining how bisequential decompositions representprobability distributions, we derive a class of bidi-rectional language models that we call latent lan-guage models. This formal correspondence allowsus to precisely charaterise the abilities and limita-tions of a subclass of latent language models, calledrational language models. As a result, we obtainthat latent language models are exponentially moreconcise and significantly more expressive than uni-directional language models. We argue that suchknowledge about the abilities and limitations oflanguage models is essential whenever we requireformal guarantees of the correctness and consis-tency of their outputs.",
  "Factorisations of Language Models": "Given a finite set , we shall use to denote theset of finite sequences of elements of and todenote the empty sequence. In this case, is calledan alphabet, the elements of are called letters andthe elements of are called words. Additionally,given a word , we shall write || for the lengthof , i for the i-th letter of , i (or <i+1) forthe prefix 12 i and i (or >i1) for thesuffix ii+1 ||. Lastly, we shall naturallyextend concatenation of words to sets of words.Definition 2.1. Let be an alphabet. A languagemodel over is a discrete probability distributionover ; i.e., a function P: such that2",
  "Supp(f) := Dom(f) | f() = 0": "We call a probability distribution P over X positiveif Supp(P) = X. Naturally, a family of positiveprobability distributions is also called positive.In practice, it is not difficult to model a functionfrom to (e.g., with a logistic curve). How-ever, satisfying the normalisation constraint (1) isnon-trivial due to the infinitarity of . Conse-quently, in this section, we consider how languagemodels can be factorised into families of finite con-ditional probability distributions that can then bemodelled efficiently (e.g., with a softmax-basedlinear prediction head (Bengio et al., 2003)).",
  "P( | ) = ($)": "Now, it is apparent that we can use the distribu-tions of a consistent prefix factorisation to define,via the chain rule of probability, its unique compati-ble language model (see Appendix B.1 for a proof).Furthermore, the chain rule provides an efficientmethod for sampling and scoring of words.Definition 2.4. Let := () be a prefixfactorisation over . The prefix model generatedby is the function M : defined as",
  "i=1<i(i)($)": "3We shall use $ to denote a special end-marker letter thatis considered not to be a member of any declared alphabet.4Similarly, the value ($) is intended to be interpreted asthe probability of the word ending after .5We note that is the set of all words over that beginwith . Thus, P(a | ) is the probability of a word tobegin with a given that it begins with , and P( | ) isthe probability of a word to be given that it begins with . Theorem 2.1. A prefix factorisation over isconsistent if and only if the prefix model M gener-ated by is a language model over . In this case,M is the only language model compatible with .In the literature, the term unidirectional lan-guage model is commonly used to mean a pre-fix model, which raises some formal issues sincethe class of language models and the class of pre-fix models do not coincide. It is easily observedthat every language model is compatible with aconsistent prefix factorisation and thus is a prefixmodel. However, the converse is true only undercertain conditions as described by Du et al. (2023).Fortunately, those conditions are satisfied in mostpractical settings (e.g., when the prefix modelis represented by a Transformer (Vaswani et al.,2017) or a bounded RNN (Elman, 1990; Hochre-iter and Schmidhuber, 1997; Cho et al., 2014) witha softmax-based linear prediction head).",
  "P() = 0 = P(a | ) = ,(a)": "Next, we consider whether the distributions ofa consistent confix factorisation can be used todefine a language model that is compatible with. A classical result by Besag (1974) shows thatthe distributions of a consistent positive confix fac-torisation can be used to express the quotients ofthe probabilities of words of equal length (see Ap-pendix B.2 for a proof). 6A confix (or a circumfix) is a pair of a prefix and a suffix.7To be more precise, BERT and T5 actually represent jointdistributions over multiple masked positions in a word byconditioning on the remaining letters. Here, for the sake ofsimplicity, we assume that there is a single masked position.8See also Remark B.3 in Appendix B.2.",
  "<i,>i(i)<i,>i(i)": "Nevertheless, the distributions of a consistentconfix factorisation contain no information aboutthe distribution of the word lengths. Hence, inorder to define a language model P over that iscompatible with a given confix factorisation, it isnecessary to additionally specify the probabilitiesof the events n for n N. Definition 2.7. A complete confix factorisationover is a tuple (, PL), where is a positiveconfix factorisation over and PL is a probabilitydistribution over N. (, PL) is consistent if thereexists a language model P over that is compatiblewith (, PL); i.e., P is compatible with and",
  "In this work, we shall consider the free monoid := (, , ), where denotes concatenation,and the probability monoid R :=, , 1,where denotes multiplication": "Definition 3.2. A (, M)-transducer is a tuple(, M, Q, I, F, ), where is an alphabet; M is amonoid with carrier M; Q is a finite set (of states);I, F: Q M are the initial and final output func-tions; I := Dom(I) and F := Dom(F) are the setsof initial and final states; and QM Qis a finite transition relation.9",
  ", 10See Appendix C.1 for a justification of the definition": "Thus, if T is sequential, we define the transitionfunction : Q Q and the transition outputfunction : Q M such that (p, a) := qand (p, a) := m if and only if (p, a, m, q) .Furthermore, we define the generalised transitionfunction : Q Q and the generalisedtransition output function : Q M suchthat (p, ) := q and (p, ) := m if and onlyif (p, , m, q) . Lastly, if T is sequential, weshall also denote it as, M, Q, (i, ), F, , .Consequently, the behaviour of a sequentialtransducer T is a function that can be expressed as",
  "T () = (i, ) F(i, )": "Definition 3.6. A function from to M is calledsequential if it can be realised by a sequential(, M)-transducer.Note that unidirectional language models basedon saturated RNNs (Merrill, 2019) or RNNs us-ing the Heaviside activation (Svete and Cotterell,2023) are in fact sequential because they transitionbetween a finite number of states in a deterministicmanner.11 Similarly, unidirectional Transformer-based language models are sequential functionsbecause of the boundedness of the lengths of theircontexts (Vaswani et al., 2017) (if N is the maxi-mum context length, then the number of states isbounded by ||N). This connection with languagemodels that are used in practice motivates the con-sideration of sequential (, R)-transducers.12",
  "(i, ), aif a F(i, )if a = $": "11A similar argument could be made for any RNN whoseactivation function maps onto a finite set. In that sense, alldeployed RNN language models are sequential transducers,albeit with a very large state space.12When working with sequential (, R)-transducers,we shall implicitly assume that F, and are total functions.In this case, it follows that , and T are also total. that generates T (see Appendix C.2). Comparedto probabilistic sequential transducers, the mainadvantage of stochastic sequential transducers isthe fact that they are extremely efficient at sam-pling since each of their states defines a probabilitydistribution over $.13 Despite of the fact that stochastic sequentialtransducers cannot realise all sequential prefix mod-els, they can represent the class of sequential lan-guage models. In fact, when applied to a proba-bilistic sequential transducer, the classical canon-isation construction of Mohri et al. (2008) (seeAppendix C.3) produces an equivalent sequentialtransducer that is stochastic (see Appendix C.4).",
  "Theorem 3.1. Every probabilistic sequential trans-ducer is equivalent to a stochastic one": "Hence, in order to represent sequential languagemodels, it is sufficient to consider only stochasticsequential transducers and thus work with tractableand easy to sample from finite distributions. Next,we show that we can also easily avoid the stochasticsequential transducers that are not probabilistic.Theorem 3.2. A stochastic sequential transducerT is probabilistic if and only if every accessiblestate of T is co-accessible.14 The condition from Theorem 3.2 is a corollaryof a classical result about absorbing Markov chains(see Appendix C.5). The condition is trivially satis-fied if the transition and final outputs of every statedefine a positive distribution over $. In practice,this is true whenever the softmax activation is used(see Appendix C.6 for further discussion).",
  "where is the longest common prefix operation": "13In this case, the probability of $ is represented by the finaloutput of the corresponding state.14In this setting, a state q is called accessible if there existsa word such that (i, ) = q and (i, ) = 0; and co-accessible if there exists a word such that (q, ) F and(q, )F(q, )= 0.",
  "is finite for all n N": "Intuitively, since (, dp) is a metric space, The-orem 3.3 states that sequential language modelsmap bounded sets of words into finite sets of prob-abilities (see .1 for a rational languagemodel that violates this constraint). To alleviate theabove-mentioned limitations of sequential trans-ducers, we could also consider the class of co-sequential functions, which can be represented bya sequential transducer that scans the input fromright to left and then reverses the output. Definition 3.9. For a word , we define the reverseof as := || 1, and, for a real numberx, we define the reverse of x as x := x. We call afunction f from to or R co-sequential iff() = g() for some sequential function g. Co-sequential functions are rational. Moreover,they are complementary to sequential functions.Indeed, in a fixed integer base, multiplication bya given integer can be implemented by a sequen-tial transducer if and only if it reads from right toleft, while it is the converse that is true for divi-sion. Nevertheless, sequential and co-sequentiallanguage models do not exhaust the whole class ofrational language models (see .1).",
  "w(1 2p0)p||0or(1 w)(1 2p1)p||1": "until it scans the last letter. It should be intuitivelyclear that T cannot resolve this uncertainty withfinite memory.15 On the other hand, a sequentialtransducer that scans the input in reverse wouldbegin with the last letter i and could immediatelydetermine the correct distribution Pi (see ).Thus, P is co-sequential but not sequential. Ofcourse, one can symmetrically construct a languagemodel that is sequential but not co-sequential.Next, we show that there are rational languagemodels that are neither sequential nor co-sequential.To achieve this, we make both the first and the lastletter of an input word crucial for determining itsprobability. Formally, for and i, j , let",
  "While the presented argument is quite informal, a rigorousproof, by means of Theorem 3.3, is given in Appendix D.1": "where wij (0, 1) sum to 1. It should be clearthat, if pij are pairwise distinct, then P is neithersequential nor co-sequential (see Appendix D.1).However, by using two sequential transducers aright-to-left and a left-to-right one, we can eas-ily represent this language model (see ).Indeed, suppose that the right-to-left sequentialtransducer runs first and scans the input := ijin reverse. The first letter it reads is j and subse-quently it augments each of the letters of withthe additional feature j; that is, it transforms into := (1, j)(2, j) (||, j). This helps the left-to-right transducer because it runs on and onceit reads (1, j) = (i, j) it can uniquely identify thelanguage model Pij that should be simulated.",
  "We define ( g)() := g().17 is the projection function from ( ) to andid is the identity function on": "One way to think of a bisequential decomposi-tion (, , g) of a language model P is the follow-ing: is a set of regular features, the co-sequentialfunction encodes an input word into a sequenceof features and the function g computes, based onthose features, the probability of . On the otherhand, given a standard bisequential decomposition(, , g) of a language model P, the function g canbe viewed as a generator. Indeed, since P = gand is injective, it follows that g1Im() is a lan-guage model over .18 Furthermore, due to thespecial form of , sampling from P is equivalent tosampling from g1Im() and projecting onto .",
  "Theorem 4.2. Let (, , g) be a standard bisequen-tial decomposition of P: . Then, P is alanguage model over if and only if g1Im() is asequential language model over": "For a formal verification of Theorem 4.2, see Ap-pendix E.1. Now, Theorem 3.1 implies that, if wewant to represent rational language models, it is suf-ficient to consider only representations (, T, Tg)of standard bisequential decompositions where Tgis stochastic. Moreover, recalling Theorem 3.2, wecould easily guarantee that the represented functionis a language model by making sure that every ac-cessible state of the stochastic Tg is co-accessible.",
  "position with a representation (, T, Tg) such thatT and Tg have On||states": "Intuitively, a representation (, T, Tg) of a bise-quential decomposition of a language model fromP,n could function as follows. The encoder Tcould co-sequentially verify that the letters at posi-tions 1 and n+2 of coincide. To do so, it needsto remember the first letter of and count to n+1.This can be achieved with On||states. Simi-larly, the generator Tg could sequentially checkthat the letters at positions 1 and n + 2 of matchwith On||states. For a formal treatment of thearguments presented above, see Appendix D.2.",
  "Expressiveness of Decompositions": "The example from .1 might misleadinglysuggest that rational language models that are notsequential (or co-sequential) can capture only localfeatures from the ending (or the beginning) of aword. This is not true and the following theorem,whose proof can be found in Appendix D.3, showsthat rational language models are closed under ar-bitrary mixing and regular conditioning.",
  "(iii) If P1 and P2 are rational with disjoint sup-ports, then so is the mixture wP1 +(1w)P2": "Note that regular languages are closed underunion and complement; that is, they form an al-gebra19. This is why conditioning on such setsis both probabilistically sound and algorithmicallytractable. Other natural classes of formal languages,such as the context-free languages, lack those prop-erties. Furthermore, for the context-sensitive lan-guages, the problem whether L = is not de-cidable, which means that it is algorithmically in-tractable to verify if the conditional language modelP1( | L) is well-defined.Next, we shed some light on the structure of astandard bisequential decomposition (, , g) of alanguage model P over , where := . Inparticular, the following theorem describes a set ofuseful additional features that can be maintainedby the encoder and then used by the generator g tooutput, via the chain rule, the correct probabilities.",
  "Minimal Co-sequential Lookahead": "A natural question that arises from the discussionabove is about the minimal co-sequential lookaheador the minimal information from the future that isrequired in order to represent a rational languagemodel P. Quantitatively, it should correspond to thenumber of states of the minimal sequential trans-ducer T such that (, T, Tg) is a representationof a bisequential decomposition of P. The follow-ing theorem gives the answer to this question (seeAppendix D.5 for a proof).",
  "Latent Language Models": "The study of bisequential decompositions in Sec-tion 4 suggests that for language modelling it mightbe beneficial to map (via an encoding function )the input probability space onto a latent proba-bility space where the corresponding probabilitymeasure P could be easier to represent. To makesure that the composition P is a probabilitymeasure on , we have to ensure that no prob-ability mass in the latent space leaks onto inac-cessible elements (that is, Supp(P) Im()) andthat the encoding function does not map differentelements of onto a single latent element withnon-zero probability (that is, should be injectiveon 1Supp(P)). Indeed, those properties aresufficient for P to be a language model over .In this case, we also have that is a bijection from1Supp(P)to Supp(P); thus, sampling from P is equivalent to sampling Supp(P) fromP and then computing 1().",
  "Definition 5.2. A latent decomposition (, , g) ofa function from to is called standard if = and = id": "In fact, suppose that ( , , g) is a standardlatent decomposition. Then, Im() is the graph ofthe function . Without loss of generality, as-sume that Im() = Supp(g). Since g is sequential,it follows that Im() is regular. Therefore, is a rational function. This implies that is rationaland thus g is also rational.",
  "Theorem 5.3. Every latent language model thatadmits a standard latent decomposition is rational": "Nevertheless, non-standard latent decomposi-tions are strictly more expressive. Indeed, con-sider the language model P that assigns probability1/2n+1 to anbn. Obviously, P is not rational be-cause Supp(P) = {anbn}nN is not a regular lan-guage. However, g(ab)n := 1/2n+1 is a sequen-tial language model. Thus,{a, b}, , g, wherethe encoder (anbn) := (ab)n drastically simpli-fies the support P, is a latent decomposition of P.",
  "Finally, we compare latent language models with acouple of other well-established latent models.First, we consider vq-wav2vec (Baevski et al.,": "2020). Similarly to a latent language model, vq-wav2vec consists of an encoder that maps froman input space to a latent space , and a lan-guage model P over . However, in vq-wav2vec, is not restricted to be injective, which leads toseveral issues. First, classical maximum likelihoodestimation cannot be used to train the composition P because of the existence of a degenerate op-timum in which the encoder collapses; that is, maps all elements of to a single latent element and P places all of the probability mass on. This necessitates the use of more indirect and in-efficient contrastive estimation methods (Gutmannand Hyvrinen, 2012; van den Oord et al., 2019).Furthermore, P is not guaranteed to be a lan-guage model and, even if it is, P cannot be used toefficiently sample from P.Second, we consider Discrete Flows (Tran et al., 2019), which are comprised of a bijective encoder: and a sequential language model over; i.e., Discrete Flows are latent language models.Tran et al. (2019) propose two types of encoders.The bipartite encoder := (1) (2) (l) issuch that, for , 1 i || and 1 j l,",
  "iif i j (mod 2)f(j)((j), i)otherwise": "In the expression above, (j) denotes the concate-nation of the letters i such that i j (mod 2).Therefore, each layer (j) of a bipartite encoder preserves the letters (j) of the input and mod-ifies the remaining letters based on (j). Further-more, in order for (j) to be bijective, f(j) is chosensuch that (j) and f(j)((j), i) uniquely identifyi. Lastly, note that Discrete Flows correspond toa subclass of non-standard latent decompositionsthat can represent non-rational language models.In fact, a sufficiently deep multi-layer bipartite en-coder can implement the mapping anbn (ab)n.For further empirical evidence of the advantages ofDiscrete Flows, we refer to Tran et al. (2019).In addition, we compare latent language modelswith discrete diffusion language models such asD3PM (Austin et al., 2021) in Appendix E.3.",
  "Conclusion": "We introduced a class of consistent bidirectionallanguage models, called latent language models,that allow for efficient sampling and scoring of se-quences. We defined latent language models basedon the well-understood formalism of bisequentialdecompositions. This formal correspondence al-lowed us to precisely charaterise the abilities andlimitations of a subclass of latent language models,called rational language models. As a result, weshowed that latent language models are exponen-tially more concise and significantly more expres-sive than unidirectional language models.",
  "Limitations": "The primary focus of this paper is the characterisa-tion of the representational capacity and concise-ness of rational language models, which constitutea strict subclass of latent language models. Conse-quently, the question about the expressive powerof the full class of latent language models remainsunanswered. In other words, it is not clear howlatent language models relate to other classes offormal languages; for example, whether they alsosubsume the class of (deterministic) context-freelanguage models. Furthermore, we do not explorethe limitations of latent language models. Whilea straightforward constraint that latent languagemodels impose is that their images should be ratio-nal sets, further work is required to obtain deeperunderstanding of the limits of their capabilities.Another notable limitation is that we do not as-sess the learnability of latent language models; thatis, we do not consider the problem of searching forsuch models via gradient-based or other optimisa-tion methods. A latent language model consists ofa latent encoder and a unidirectional languagemodel g. It is well known that unidirectional lan-guage models, such as g, are effectively learnable.However, it is not obvious if and g could also bejointly optimised and if so what is an appropriateparametric family for .Lastly, we do not explore the applicability oflatent language models to natural language pro-cessing tasks. That is, it remains to be empiricallyverified whether the increased expressive power oflatent language models could lead to better down-stream task performance when compared to classi-cal unidirectional language models.",
  "Acknowledgements": "Georgi Shopov acknowledges that this work is par-tially supported by CLaDA-BG, the Bulgarian Na-tional Interdisciplinary Research e-Infrastructurefor Resources and Technologies in favor of the Bul-garian Language and Cultural Heritage, part of theEU infrastructures CLARIN and DARIAH, fundedby the Ministry of Education and Science of Bul-garia (support for the Bulgarian National Roadmapfor Research Infrastructure).Stefan Gerdjikov acknowledges that this studyis financed by the European Union NextGener-ationEU, through the National Recovery and Re-silience Plan of the Republic of Bulgaria, projectNo. BG-RRP-2.004-0008. Ebru Arisoy, Abhinav Sethy, Bhuvana Ramabhadran,and Stanley Chen. 2015. Bidirectional recurrent neu-ral network language models for automatic speechrecognition.In 2015 IEEE International Confer-ence on Acoustics, Speech and Signal Processing(ICASSP), pages 54215425. Jacob Austin, Daniel D. Johnson, Jonathan Ho, DanielTarlow, and Rianne van den Berg. 2021. Structureddenoising diffusion models in discrete state-spaces.In Advances in Neural Information Processing Sys-tems, volume 34, pages 1798117993. Curran Asso-ciates, Inc.",
  "Julian Besag. 1974.Spatial interaction and the sta-tistical analysis of lattice systems. Journal of theRoyal Statistical Society. Series B (Methodological),36(2):192236": "Kiante Brantley, Kyunghyun Cho, Hal Daum, and SeanWelleck. 2019. Non-monotonic sequential text gen-eration. In Proceedings of the 2019 Workshop onWidening NLP, pages 5759, Florence, Italy. Associ-ation for Computational Linguistics. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020.Language models are few-shot learners.In Ad-vances in Neural Information Processing Systems,volume 33, pages 18771901. Curran Associates,Inc. Mark Chen, Jerry Tworek, Heewoo Jun, QimingYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-plan, Harri Edwards, Yuri Burda, Nicholas Joseph,Greg Brockman, Alex Ray, Raul Puri, GretchenKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-try, Pamela Mishkin, Brooke Chan, Scott Gray,Nick Ryder, Mikhail Pavlov, Alethea Power, LukaszKaiser, Mohammad Bavarian, Clemens Winter,Philippe Tillet, Felipe Petroski Such, Dave Cum-mings, Matthias Plappert, Fotios Chantzis, Eliza-beth Barnes, Ariel Herbert-Voss, William HebgenGuss, Alex Nichol, Alex Paino, Nikolas Tezak, JieTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, EvanMorikawa, Alec Radford, Matthew Knight, MilesBrundage, Mira Murati, Katie Mayer, Peter Welinder,Bob McGrew, Dario Amodei, Sam McCandlish, IlyaSutskever, and Wojciech Zaremba. 2021. Evaluat-ing large language models trained on code. Preprint,arXiv:2107.03374. Kyunghyun Cho, Bart van Merrinboer, Caglar Gul-cehre, Dzmitry Bahdanau, Fethi Bougares, HolgerSchwenk, and Yoshua Bengio. 2014.Learningphrase representations using RNN encoderdecoderfor statistical machine translation. In Proceedingsof the 2014 Conference on Empirical Methods inNatural Language Processing (EMNLP), pages 17241734, Doha, Qatar. Association for ComputationalLinguistics.",
  "Christian Choffrut. 1977. Une caractrisation des fonc-tions squentielles et des fonctions sous-squentiellesen tant que relations rationnelles. Theoretical Com-puter Science, 5(3):325337": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics. Li Du, Lucas Torroba Hennigen, Tiago Pimentel, ClaraMeister, Jason Eisner, and Ryan Cotterell. 2023. Ameasure-theoretic characterization of tight languagemodels. In Proceedings of the 61st Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 97449770, Toronto,Canada. Association for Computational Linguistics.",
  "Jeffrey L. Elman. 1990. Finding structure in time. Cog-nitive Science, 14(2):179211": "Dmitrii Emelianenko, Elena Voita, and Pavel Serdyukov.2019. Sequence modeling with unconstrained gen-eration order. In Advances in Neural InformationProcessing Systems, volume 32. Curran Associates,Inc. Nicolas Ford, Daniel Duckworth, Mohammad Norouzi,and George Dahl. 2018. The importance of genera-tion order in language modeling. In Proceedings ofthe 2018 Conference on Empirical Methods in Nat-ural Language Processing, pages 29422946, Brus-sels, Belgium. Association for Computational Lin-guistics. Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang,Eric Wallace, Freda Shi, Ruiqi Zhong, Scott Yih,Luke Zettlemoyer, and Mike Lewis. 2023. InCoder:A generative model for code infilling and synthesis.In International Conference on Learning Representa-tions. Marjan Ghazvininejad, Omer Levy, Yinhan Liu, andLuke Zettlemoyer. 2019. Mask-predict: Parallel de-coding of conditional masked language models. InProceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the9th International Joint Conference on Natural Lan-guage Processing (EMNLP-IJCNLP), pages 61126121, Hong Kong, China. Association for Computa-tional Linguistics. Kartik Goyal, Chris Dyer, and Taylor Berg-Kirkpatrick.2022. Exposing the implicit energy networks behindmasked language models via MetropolisHastings.In International Conference on Learning Representa-tions.",
  "Diederik P. Kingma and Max Welling. 2014. Auto-encoding variational bayes. In International Confer-ence on Learning Representations": "Xuanlin Li, Brandon Trabucco, Dong Huk Park,Michael Luo, Sheng Shen, Trevor Darrell, and YangGao. 2021. Discovering non-monotonic autoregres-sive orderings with variational inference. In Interna-tional Conference on Learning Representations. Chu-Cheng Lin, Aaron Jaech, Xin Li, Matthew R.Gormley, and Jason Eisner. 2021. Limitations ofautoregressive models and their alternatives. In Pro-ceedings of the 2021 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages51475173, Online. Association for ComputationalLinguistics. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.RoBERTa: A robustly optimized BERT pretrainingapproach. Preprint, arXiv:1907.11692. William Merrill. 2019. Sequential neural networks asautomata. In Proceedings of the Workshop on DeepLearning and Formal Languages: Building Bridges,pages 113, Florence. Association for ComputationalLinguistics.",
  "Mehryar Mohri. 1997. Finite-state transducers in lan-guage and speech processing. Computational Lin-guistics, 23(2):269311": "Mehryar Mohri, Fernando Pereira, and Michael Riley.2008. Speech recognition with weighted finite-statetransducers. In Jacob Benesty, M. Mohan Sondhi,and Yiteng Arden Huang, editors, Springer Hand-book of Speech Processing, pages 559584. SpringerBerlin Heidelberg, Berlin, Heidelberg. Amr Mousa and Bjrn Schuller. 2017. Contextual bidi-rectional long short-term memory recurrent neuralnetwork language models: A generative approach tosentiment analysis. In Proceedings of the 15th Con-ference of the European Chapter of the Associationfor Computational Linguistics: Volume 1, Long Pa-pers, pages 10231032, Valencia, Spain. Associationfor Computational Linguistics.",
  "M.P. Schtzenberger. 1977. Sur une variante des fonc-tions squentielles. Theoretical Computer Science,4(1):4757": "Mitchell Stern, William Chan, Jamie Kiros, and JakobUszkoreit. 2019. Insertion transformer: Flexible se-quence generation via insertion operations. In Pro-ceedings of the 36th International Conference onMachine Learning, volume 97 of Proceedings of Ma-chine Learning Research, pages 59765985. PMLR. Anej Svete and Ryan Cotterell. 2023. Recurrent neu-ral language models as probabilistic finite-state au-tomata. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing,pages 80698086, Singapore. Association for Com-putational Linguistics. Lucas Torroba Hennigen and Yoon Kim. 2023. Deriv-ing language models from masked language models.In Proceedings of the 61st Annual Meeting of theAssociation for Computational Linguistics (Volume2: Short Papers), pages 11491159, Toronto, Canada.Association for Computational Linguistics. Dustin Tran, Keyon Vafa, Kumar Agrawal, LaurentDinh, and Ben Poole. 2019.Discrete flows: In-vertible generative models of discrete data. In Ad-vances in Neural Information Processing Systems,volume 32. Curran Associates, Inc.",
  "Oriol Vinyals, Samy Bengio, and Manjunath Kudlur.2015. Order matters: Sequence to sequence for sets.In International Conference on Learning Representa-tions": "Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,and Denny Zhou. 2022. Chain-of-thought prompt-ing elicits reasoning in large language models. InAdvances in Neural Information Processing Systems,volume 35, pages 2482424837. Curran Associates,Inc. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale,Rami Al-Rfou, Aditya Siddhant, Aditya Barua, andColin Raffel. 2021. mT5: A massively multilingualpre-trained text-to-text transformer. In Proceedingsof the 2021 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 483498, On-line. Association for Computational Linguistics.",
  "ARelated Work": "The problem of characterising the consistent prefix factorisations is tackled by Du et al. (2023). Theygive an easy to enforce condition that is sufficient for consistency. Moreover, the authors show that manyof the models used in practice to represent prefix factorisations satisfy this property. In particular, theauthors demonstrate that all prefix factorisations realised by Transformers or RNNs (such as GRU andLSTM) with a softmax prediction head are in fact consistent.Nevertheless, we are not aware of the existence of such results for confix factorisations. Goyal et al. (2022); Torroba Hennigen and Kim (2023) acknowledge the problem of ensuring the consistency ofconfix factorisations as well as the intractability of sampling from them. To this end, Goyal et al. (2022)attempt to sidestep the issue by interpreting confix factorisations as energy-based models and derivinga different incompatible distribution from them. Similarly, Torroba Hennigen and Kim (2023) exploremethods for deriving incompatible joint distributions from confix factorisation. However, they focus onlyon distributions over two-letter alphabets.Several studies (Vinyals et al., 2015; Ford et al., 2018) have suggested that the order in which sequencesare processed is important for language modelling. Furthermore, there have been many different proposalsfor achieving bidirectionality by learning the decoding order (Brantley et al., 2019; Stern et al., 2019; Guet al., 2019; Li et al., 2021). Nonetheless, each of those solutions requires either expensive beam searchor variational inference for decoding. Moreover, many of the methods do not allow for efficient scoringof sequences. The only bidirectional models we are aware of that achieve both efficient generation andscoring are the Discrete Flows by Tran et al. (2019), which are in fact a special case of the latent languagemodels that we introduce.We note that Svete and Cotterell (2023) have already explored the connection between unidirectionallanguage models and sequential transducers. The authors show that unidirectional language modelsbased on Heaviside RNNs are as expressive as sequential transducers. However, we are not aware ofsimilar developments for bidirectional models. More importantly, we do not know of other works thathave attempted to define bidirectional language models in a principled way and have explored theirexpressive power and representational conciseness in terms of formal abstractions from automata andformal language theory.",
  "B.2Language Modelling with Confix Factorisations": "In this appendix, we describe in more details the correspondence between the language models that arecompatible with a given complete confix factorisation (, PL) and the confix model generated by (, PL).Additionally, we demonstrate that, unlike prefix factorisations, there exist inconsistent complete confixfactorisations whose confix models are language models. Remark B.3. Note that, according to Definition 2.5, there are no confix factorisations over because thereare no probability distributions over . Nevertheless, we shall extend Definition 2.5 by considering theempty family as the confix factorisation over .",
  "Remark B.4. The extension of Definition 2.5 leads to the following additional amendments": "22Note that, forP(i | <i)||i=1 and P( | ) to be defined and the derivation of M() = P() to be valid, it isnecessary that P() = 0. However, even if P() = 0, M() = P() still holds since P() = 0 because P() P(),and M() = 0 because there exists 1 i || such that P(<i) = 0 and <i(i) = P(i | <i) = 0.",
  "( n)P() = 0 P(n) = 0.(5)": "Proof. Assume that P is compatible with the positive confix factorisation := (,),. Thebackward direction of (5) holds trivially; thus, we focus on the forward direction. We note that, sinceP(0) = P(), the forward direction of (5) holds trivially for n = 0. In what follows, we consider thecase where n > 0.Let \\ be such that P() = 0. We prove by induction on i that0 i ||P(i>i) = 0. Note that, when i = ||, we obtain that P(||) = 0 (that is, the forward direction of (5) holds for n > 0).For i = 0, it is obvious that P(i>i) = P() = 0. Now, assume that P(i>i) = 0 for 0 i < ||,and suppose that P(i+1>i+1) = 0. Then, there exists i such that P(>i+1) = 0 and",
  "Theorem B.2. Let (, PL) be a consistent complete confix factorisation over . Then, the confix modelgenerated by (, PL) is the only language model over that is compatible with (, PL)": "Proof. Let P be a language model over that is compatible with (, PL) (such a language model existssince (, PL) is consistent). Now, Proposition B.7 implies that P coincides with the confix model Mgenerated by (, PL). Therefore, M is the only language model that is compatible with (, PL). Next, we show that, unlike prefix factorisations, there exist inconsistent complete confix factorisationswhose confix models are language models. The main reason for this deficiency is that, as we shall show,n can take on values that are less than and greater than one when n 2. We begin by noting that0 = 1 = 1 for every positive confix factorisation over .",
  "(n 2)(n >n 1).(8)": "Proof. If := (,), is a positive confix factorisation over , then we know that n, for n 2,is defined only in terms of the probability distributions , for , such that || = n 1. Now,from Proposition B.9, it follows that there is a positive confix factorisation over that satisfies (8).",
  "Theorem B.3. There exists a complete confix factorisation (, PL) over that is inconsistent and theconfix model generated by (, PL) is a language model over": "Proof. Let := {a, b}. We will show that there exists a complete confix factorisation (, PL) over such that the confix model generated by (, PL) is a language model over that is not compatible with(, PL). Thus, from Theorem B.2, it would follow that (, PL) is inconsistent.Let be a positive confix factorisation over such that",
  "C.1Real-Time Transducers": "In the literature, transducers are typically allowed to have transitions and transducers that do not have transitions are called real-time (Mihov and Schulz, 2019). Additionally, a relation is called rational ifit can be realised by a transducer. When it comes to representing functions, transducers and real-timetransducers have the same expressive power (Mihov and Schulz, 2019, Proposition 4.4.8). However,real-time transducers cannot realise every rational relation. Indeed, they can represent only those rationalrelations that are not infinitely ambiguous.23 In this work, our focus is on representing language models;that is, we are primarily interested in the class of rational functions and not the class of rational relations.Thus, we shall consider only real-time transducers and call them simply transducers.",
  "M() = ()($) = (i, ), F(i, )= T ()": "Remark C.1. It should be noted that the converse statement does not hold. That is, there exist sequentialprefix models that cannot be represented by a stochastic sequential transducer. Nevertheless, as we shallsee in Appendix C.4, the sequential prefix models that we care about (that is, the sequential languagemodels) can all be realised by stochastic sequential transducers.",
  "C.3Canonisation of Sequential Transducers": "In this appendix, we review a construction by Mohri et al. (2008), known in the literature as weight-pushing, that builds from a sequential transducer an equivalent canonical one. In essence, the constructionconsists of pushing the outputs of the transitions and the outputs of the final states towards the initialstate as much a possible.",
  "iI(ki l)": "Note that we do not require the infinitary sum operation to be total (that is, for K to be complete). Hence,all equalities in the equations above are conditional; that is, the left and right hand sides are either bothdefined and equal or are both undefined. Definition C.4. A semiring (K, , , 0, 1) is called weakly left-divisible24 if for every family (ki)iI ofelements of K such that iI ki K \\ 0 and every j I, there exists a unique element of K, denotediI ki1 kj, such that",
  "a(q, a) T(q,a)": "Definition C.7. Let K := (K, , , 0, 1) be a semiring and T :=, K, Q, (i, ), F, , be a sequen-tial transducer. We say that T is summable with respect to K if the sums Tq exist for all q Q.Moreover, T is strictly summable with respect to K if it is summable with respect to K and",
  "a(q, a) = 1": "Definition C.9. Let T :=, K, Q, (i, ), F, , be a sequential transducer that is strictly summablewith respect to the weakly left-divisible semiring K := (K, , , 0, 1). The canonical form of T withrespect to K is defined as the sequential transducer, K, Q, (i, ), F, , ,where",
  "C.4From Probabilistic to Stochastic Sequential Transducers": "In this appendix, we show that every probabilistic sequential transducer is equivalent to a stochasticsequential transducer. Hence, every sequential language model is a sequential prefix model. The proof isbased on an application of the canonisation construction from Appendix C.3 with respect to the semiringR+[0,) :=[0, ), +, , 0, 1. Note that R is a submonoid of R[0,) :=[0, ), , 1. Thus, in whatfollows, we shall also view probabilistic and stochastic transducers as (, R[0,))-transducers.",
  "Remark C.4. Let T be a probabilistic sequential transducer. Consider the sequential transducer T": "obtained from T by removing all states that are not co-accessible, adding a new state qc with final output1 and then completing the transition functions with transitions to qc with output 0. Now, it is not hard toverify that T is equivalent to T and every state of T is co-accessible. Thus, if T is probabilistic, then T",
  "Theorem C.2. Every probabilistic sequential transducer is equivalent to a stochastic sequential trans-ducer": "Proof. Let T be a probabilistic sequential transducer. Without loss of generality, we can assume thatevery state of T is co-accessible (see Remark C.4). Then, since R+[0,) is positive (that is, a + b = 0 ifand only if a = b = 0), it follows that T is strictly summable. Therefore, T has a canonical form T (seeDefinition C.9) that is equivalent to T and canonical with respect to R+[0,) (see Theorem C.1). Finally,Proposition C.7 implies that T is a stochastic sequential transducer.",
  "Theorem C.4. A stochastic sequential transducer T is probabilistic if and only if every accessible stateof T is co-accessible": "Proof. Let T :=, R, Q, (i, 1), F, , be a stochastic sequential transducer. First, assume that Tis probabilistic and consider the prefix factorisation := () that is associated with it. Let q Qbe an accessible state. Then, there exists such that (i, ) = q and (i, ) = 0. From thecorrespondence between T and (see Proposition C.1), it follows that",
  "($) = ($) = 1": "Thus, there exists such that (q, )F(q, )= 0; that is, q is co-accessible.Now, assume that every accessible state of T is co-accessible. As noted in Remark C.4, we canassume, without loss of generality, that every state of T is co-accessible. Now, consider the Markov chainC := (S, , P) associated with T . From Proposition C.8, it follows that C is an absorbing Markov chainand q$ is its unique absorbing state. Furthermore,",
  "C.6Modelling of State Distributions with Softmax": "In this appendix, we continue the discussion of the fact that, in practice, all stochastic sequential languagemodels that use the softmax activation function to define the transition and final output functions areprobabilistic because all of their accessible states are co-accessible.As already mentioned, unidirectional language models based on saturated RNNs, RNNs using theHeaviside activation function or Transformers with bounded context length are, in fact, stochastic sequen-tial transducers. In practice, the state of such a model at time step t (that is, after processing the prefixt of the input ) is represented by a d-dimensional vector ht Rd. In order to obtain the probability",
  "C.7Characterisation of Sequential Language Models": "In this appendix, we provide a detailed proof of the characterisation of sequential language models.We consider the more general case of sequential functions from to R and show that threy arecharacterised by several different properties; namely, uniform finiteness, uniform boundedness andLipschitzness. We begin by recalling a result by Mohri (1997, Theorem 9) that characterises the sequentialfunctions from to S[0,) :=[0, ), +, 0.25",
  "Proposition C.9. Let f be a rational function from to R. Then, f is sequential if and only iffSupp(f) if sequential": "Proof. If f is realised by a sequential transducer, then, by removing the transitions with zero outputand making the initial (final) states with zero initial (final) output non-initial (non-final), we obtain asequential transducer that represents fSupp(f). Conversely, if fSupp(f) is sequential, we can completeany sequential transducer that realises it in order to obtain a sequential transducer that represents f.",
  "(iv) in Proposition D.2 we prove that, when (pij)i,j{0,1} are pairwise distinct, P() is neither a sequentialnor a co-sequential language model": "Remark D.1. We utilise the following standard graphical representation in order the visualise transduc-ers (Sakarovitch, 2009): states are depicted as circles (inside of which the name of the state may bewritten), each transition (p, , m, q) is represented by an arrow from p to q with label | m, initial statesare identified by an incoming arrow labelled with the corresponding initial output and final states areidentified by an outgoing arrow labelled with the corresponding final output.",
  "| (1, 1)": ": A representation of a standard bisequential decomposition{0, 1}2, , gof the language model P from.1. On the right hand side is the sequential transducer T that represents the co-sequential function: {0, 1} {0, 1}2 defined as () := and () := (1, j)(2, j) (||, j) for = j. On the left handside is the sequential transducer Tg that realises the sequential language model g over {0, 1}2 such that g = P.Note that Tg has intentionally not been completed to avoid clutter. Additionally, the sequential transducer Tg isprobabilistic but not stochastic since the transition and final outputs of states lij sum to two instead of one.",
  "are rational functions with disjoint domains. Therefore, their union, which coincides with theirmixture with parameter w, is a rational function": "27Below, we use to denote the projection from Q to .28Note that, given a transducer for P1, we can effectively transform it into an unambiguous transducer and use standard matrixoperations in order to effectively compute P1(L).29Indeed, given a transducer realising Pi, we can remove all transitions with output 0 along them and make all initial(final) states with initial (final) output 0 non-initial (non-final). This would not change the outputs along the runs for words Supp(Pi) but the domain of the behaviour of the resulting transducer would be exactly Supp(Pi); thus, proving thatSupp(Pi) is regular.",
  "D.4Characterisation of Rational Language Models": "In this appendix, we prove Theorem 4.6. To this end, we use the notion of a bimachine (Schtzenberger,1961; Eilenberg, 1974; Mihov and Schulz, 2019). Similarly to representations of bisequential decompo-sitions, bimachines are deterministic devices that can represent any rational function. Every bimachineconsists of two deterministic automata a left and a right one and an output function. Just like theencoder of a representation of a bisequential decomposition, the right automaton scans the input from rightto left. However, as opposed to the encoder, it does not output any information. Correspondingly, the leftautomaton scans the input from left to right. Based on the runs of the two automata, the output functionproduces the output. Thus, the main difference between bimachines and representations of bisequentialdecompositions is the fact that bimachines treat the left-to-right and right-to-left scans independently andthus symmetrically, which allows for more transparent arguments.We start by recalling the formal definition of a bimachine and behaviour of a bimachine. Then, we statethe equivalence of the expressive power of bimachines and bisequential decompositions. Finally, we focuson the main topic of this section; that is, the proof of Theorem 4.6.",
  "Ll := | L(iL, ) = l": "Since AL is a complete deterministic automaton over , it follows that {Ll}lQL is a finite partitionof (Ll = , for l QL, because l is accessible). Furthermore, by Kleenes Theorem, each of thelanguages Ll is regular. Therefore, {Ll}lQL is a partition and thus a cover of with regular languages.To complete the proof of this part of the theorem, it suffices to show that the conditional distributionsP( | Ll)",
  "Observe that the final expression depends on only through r = R(iR, ). It follows that, for everystate l QL, the number of distinct distributions P( | Ll) is at most |QR| when ranges over": "(ii) = (iii) Assume that {Li}ni=1 is a cover of with regular languages such that, for every i, thenumber of distributions P( | Li) is finite when ranges over . We need to show that there is apartition of with the same property.To this end, for a subset I {1, 2, . . . , n}, we define",
  "iI \\ Li": "Since the class of regular languages is closed under complement and intersection, it follows that each of thelanguages LI is regular. Next, it should be also clear that, if I and J are distinct subsets of {1, 2, . . . , n},then LI LJ = . Indeed, if i LI \\ LJ, then LI Li whereas LJ \\ Li. The case, where thereis j LJ \\ LI, is symmetric; thus, the conclusion follows. Finally, since",
  "P = ni=1 Li =": "Therefore P is a partition of into regular languages.Let LI P. Then, LI = and since ni=1 \\ Li = , it follows that there is at least one1 i n such that i I. Let i be a fixed index with this property. Then, LI Li and, for every , LI Li. Now, Proposition D.3 implies that the number of distinct distributions of the formP( | LI)",
  "is bounded from above by the number of distinct distributionsP( | Li)}": "(iii) = (iv) Let {Lj}nj=1 be a finite partition of into regular languages such that, for 1 j n,there are finitely many distributions P( | Lj) for . We prove that {Lj}nj=1 can be chosen suchthat, for every a and 1 j n, there is a unique 1 k n with Lja Lk.To this end, we first use that Lj is regular and thus, by Kleenes Theorem, there is a completedeterministic automaton Aj := (, Qj, ij, j, Fj) that recognises Lj. Next, using the cartesian productconstruction, we obtain a complete deterministic automaton A := (, Q, i, , Q), where",
  "(i, ) =j (ij, )nj=1": "Since {Lj}nj=1 is a partition of and Aj recognises Lj for 1 j n, it follows that, for every q Q,there is a unique 1 j n such that qj Fj and Lq Lj. Now, Proposition D.3 implies that thenumber of distinct distributions of the formP( | Lq)} is bounded from above by the numberof distinct distributionsP( | Lj)}. (iv) = (i) Let {Li}ni=1 be a partition of into regular languages with the property that, for every1 i n and a , there is a unique 1 j n with Lia Lj. Assume further that, for 1 i n,the number of distributions P( | Li) is finite when ranges over . In what follows, we set out toconstruct a representation (, T, Tg) of a bisequential decomposition (, , g) of P.We start by constructing the encoding transducer T. To this end, we study the relation",
  "defined as (1 i n)(Li, , ) CP": "In other words, expresses the property that and have the same conditional distributions withrespect to every Li.It is straightforward to note that is an equivalence relation (see Remark D.5) and, since, for every1 i n, there are finitely many distributions of the form P( | Li), it follows that has a finiteindex. By Proposition D.3 and the fact that, for every 1 i n and a , there is a unique 1 j nwith Lia Lj, it follows that is a left congruence (see Definition D.5). Thus, we can encode theequivalence classes of as a right-to-left scanning deterministic automaton.We define the sequential transducer",
  "i=1": "In other words, preserves the input word in the first coordinate by outputting the letters i, whereas inthe second coordinate it encodes the equivalence class [>i]. This knowledge, along with the propertiesof the languages {Li}ni=1, enables the construction of the generator Tg.Let L is be element of {Li}ni=1 that contains . Then, we define",
  "i=1L(i),i+1, [>i+1].(11)": "Note that, if P(L(j)>i) = 0 for some 1 i ||, then, since L(i)>i, it follows that P() = 0.These considerations show that, if some of the values in (11) is zero due to the case P(L(i)>i) = 0,then Tg()= 0 = P() as required.Next, we assume that P(L(i)>i) = 0 for all 1 i ||. Therefore, for 1 i || 1,",
  "D.5Minimal Co-sequential Lookahead of Rational Language Models": "In this appendix, we describe the minimal co-sequential lookahead that is needed in order to represent arational language model. It should be noted that the results in this appendix are stated more generally; thatis, for functions from to and not specifically for language models. However, the results hold onlyfor positive-valued functions f : (0, 1] (in particular, positive language models) and representations(, T, Tg) of bisequential decompositions of f such that DomT= Dom(f). The condition placedon the representations is non-restrictive; thus, in what follows, we shall implicitly assume that everyrepresentation of a bisequential decomposition satisfies the above-mentioned property. Furthermore, givena representation (, T, Tg) of a bisequential decomposition, we shall assume that",
  "( Q) : T()": "and h := g1 f. Then, f = g h and, since g is injective, h is a function from ( Q) to (0, 1]. Itremains to show that h is sequential. However, h is rational because it is the composition of two rationalfunctions. Thus, it is sufficient to demonstrate that h is uniformly finite.Let n N and define",
  ", Dom(h) dp(, ) n= A B": "The set A is obviously finite. To show that B is finite, consider , Dom(h) such that dp(, ) nand || + || > n. Let := , := 1 and := 1. Then, dp(, ) = || + || n and = , which means that (, ) f. Therefore,",
  "We summarise the obtained results in the following theorem, which is a generalisation of Theorem 4.7": "Theorem D.6. Let f be a rational function from to R(0,1]. Then, f is of finite index. Furthermore, if(, Tg, T) is a representation of a bisequential decomposition of f, then T has at least |/f| statesand this bound is tight. Proof. Let (, , g) be a bisequential decomposition of f. From Proposition D.5, it follows that T isa left congruence on . Furthermore, Proposition D.7 implies that f is of finite index and T has atleast |/f| states. Lastly, by Proposition D.8, there exists a bisequential decomposition of f with anencoder that has exactly |/f| states. Lastly, we verify formally that sequential language models require no information from the future inorder to be represented; that is, the syntactic left congruence of a sequential language model has a singleequivalence class. By Theorem D.6, this means that every sequential language model admits a bisequentialdecomposition with an encoder that produces information that is constant and does not change throughouttime.",
  "Theorem E.3. Every latent language model that admits a standard latent decomposition is rational": "Proof. Let (, , g) be a standard latent decomposition of a language model P over . Then, is aCartesian product and = id. Therefore, Im() is the graph of the function .Furthermore, we known that Supp(g) is a regular language, Supp(g) Im() and Supp(g) is the graphof the function idSupp(g) . This means that idSupp(g) is a rational function and thus idSupp(g) is also rational. Now, if Supp(g) = , then = idSupp(g) is rational and P is a rationallanguage model as the composition of two rational functions. Next, suppose that Supp(g) . Let \\ Supp(g) and : be defined as",
  "E.3Comparison of Latent Language Models with D3PM": "In this appendix, we compare latent language models with discrete diffusion language models and morespecifically with D3PM (Austin et al., 2021).Essentially, D3PM is a variational autoencoder (Kingma and Welling, 2014) that consists of a fixedstochastic encoder that, given a word from the input space , defines a probability distribution( | ) over a latent space , and a stochastic decoder that, given an element of the latent space ,defines a probability distribution ( | ) over the input space . Importantly, the encoder is constructedso that, regardless of the given word from the input space, it induces approximately the same probabilitydistribution () over the latent space; that is, () ( | ) for all .When compared to a latent decomposition (, , g) of a language model over , the encoder and thedecoder of a D3PM model correspond to stochastic equivalents of and 1, respectively. Samplingfrom a D3PM model can be done efficiently by first sampling a latent element from the fixed encoder and then sampling a word over from the decoder ( | ). A latent language model achieves the sameresult by first sampling a latent element from the generator g and then mapping it into the input spacevia 1.A major drawback of D3PM models, when compared to latent language models, is the fact that theirencoders and decoders are stochastic; that is, they are not exact inverses of each other. This deficiencyleads to the inability of D3PM models to efficiently calculate exact word probabilities and necessitates theuse of estimates such as the evidence lower bound (ELBO). Furthermore, the computation of the ELBO isoften intractable and typically requires the use of Monte-Carlo based approximation methods. On theother hand, latent language models can exactly and efficiently score words by first encoding them via and then computing the probabilities of the resulting latent elements via the sequential generator g."
}