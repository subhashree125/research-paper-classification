{
  "Abstract": "Considering the limited internal paramet-ric knowledge, retrieval-augmented genera-tion (RAG) has been widely used to extendthe knowledge scope of large language mod-els (LLMs). Despite the extensive efforts onRAG research, in existing methods, LLMscannot precisely assess the relevance of re-trieved documents, thus likely leading to mis-leading or even incorrect utilization of ex-ternal knowledge (i.e., retrieved documents).To address this issue, in this paper, we pro-pose REAR, a RElevance-Aware Retrieval-augmented approach for open-domain questionanswering (QA). As the key motivation, weaim to enhance the self-awareness regardingthe reliability of external knowledge for LLMs,so as to adaptively utilize external knowledgein RAG systems. Specially, we develop a novelarchitecture for LLM-based RAG systems, byincorporating a specially designed assessmentmodule that precisely assesses the relevanceof retrieved documents. Furthermore, we pro-pose an improved training method based on bi-granularity relevance fusion and noise-resistanttraining. By combining the improvements inboth architecture and training, our proposedREAR can better utilize external knowledgeby effectively perceiving the relevance of re-trieved documents. Experiments on four open-domain QA tasks show that REAR significantlyoutperforms previous a number of competitiveRAG approaches. Our codes can be accessedat",
  "Equal contributions.Corresponding authors": "Who won the first Noble Prize in Physics? First law of thermodynamics was stated by William. Albert Einstein was awarded the 1921 Nobel Prize. LLMs may answer correctly with parametric knowledge. LLMs can be misled by additional provided knowledge. Its irrelevant.William. LLMs struggle to determine the relevance degree. Its relevant. Albert Einstein. No provided document. Wilhelm Conrad Rntgen.",
  ": LLMs may be misled by irrelevant docu-ments, and struggle to determine the relevance of adocument (Ren et al., 2023; Zhang et al., 2024)": "issue, retrieval-augmented generation (RAG) pro-vides LLMs with potentially relevant documentsthrough a retrieval module (Gao et al., 2023), aid-ing in generating more precise content.While RAG offers clear benefits, it also intro-duces several technical challenges for effectivelyimproving LLMs.Firstly, the retrieved resultslikely contain irrelevant content or documents,which may mislead LLMs and even cause them torespond incorrectly (Mallen et al., 2023; Ren et al.,2023). Moreover, it has become common to incor-porate multiple reference documents to boost theoverall reliability of retrieved documents. However,this approach potentially amplifies the impact ofthe noise present in the retrieved documents (Liuet al., 2023; Shi et al., 2023). Thus, LLMs facedifficulties in filtering irrelevant documents andintegrating their internal knowledge (Dong et al.,2023), which needs to avoid potential interferencewith noisy content.Recently, several studies (Asai et al., 2023; Luoet al., 2023; Yoran et al., 2023) have attemptedto enhance the robustness of RAG systems. Forinstance, Self-RAG (Asai et al., 2023) allows themodel to introspect its outputs by generating spe-cial tokens to discriminate if the documents are rel-evant, and RobustLM (Yoran et al., 2023) promptsLLMs to first discriminate if the documents are rel- evant and then generate answers. However, theseapproaches perform the assessment of documentrelevance solely based on binary labels, which arehighly sparse and not precise to capture the fine-grained relevance. In addition, they seldom con-sider the varied relevance degree of reference doc-uments, making the utilization of external knowl-edge somehow blind.To this end, in this paper, we propose REAR,a RElevance-Aware Retrieval-augmented genera-tion approach for open-domain question answer-ing (QA). Our key idea is to develop robust self-awareness regarding the reliability of externalknowledge (i.e., retrieved documents) within RAGsystems, so that the LLM can learn to adaptively uti-lize the internal and external knowledge for solvingcomplex QA tasks. To achieve this goal, we maketwo major contributions in both model architectureand training. First, we propose relevance-awareRAG architecture by incorporating explicit assess-ment modules in LLMs generation architecture toperform an additional relevance assessment task.In our architecture, the assessment module effec-tively captures relevance signals, and feeds themback to avoid distractions from irrelevant externalknowledge during generation. Secondly, to supportthe relevance-aware RAG architecture, we furtherpropose two training strategies. Bi-granularity rele-vance fusion strategy further integrates both coarseand fine-grained relevance supervision to overcomethe limitations of binary discriminative methods,while noise-resistant training strategy enhances thediscrimination ability of the LLM by incorporatingnegatives in the training procedure.To the best of our knowledge, we are the firstto introduce the idea of incorporating explicit as-sessment modules in the generation architectureof LLMs to aid in irrelevance-resistant generation.Extensive experiments on public open-domain QAbenchmarks attest to the effectiveness of our REARframework.Notably, we also demonstrate thestrong generalization capability of REAR by con-ducting out-of-domain evaluation on multiple open-domain QA benchmarks.",
  "Related Work": "Open-domain Question Answering.Modernopen-domain QA systems combine traditional IRtechniques with neural reading comprehensionmodels (Chen et al., 2017). After retrieving doc-uments (Ren et al., 2021a; Zhang et al., 2021), an extractive or generative reader is typically used foranswer generation (Zhu et al., 2021). Models likeREALM (Guu et al., 2020), RAG (Lewis et al.,2020), RETRO (Borgeaud et al., 2022) and In-context RALM (Ram et al., 2023) have demon-strated improved factual generation capabilities.However, these readers make generation qualitymore prone to noise impact, for lacking explicitrelevance discernment. We propose an architecturethat explicitly generates relevance scores to assistin subsequent generation tasks. Retrieval-augmented LLMs. Several researchaims at aligning the retriever outputs with the pref-erences of the LLMs (Izacard and Grave, 2021a;Sachan et al., 2021). And works like Atlas (Izacardet al., 2022), RA-DIT (Lin et al., 2023) jointly trainthe language model and the retriever for advancedperformance on RAG. Some other work improvesthe quality of retrieved documents by expandingthe knowledge sources (Li et al., 2023b) or queryrewriting (Zheng et al., 2023). However, we focuson a scenario where the irrelevant documents fromretrieval could mislead LLMs. Several recent stud-ies (Asai et al., 2023; Luo et al., 2023; Yoran et al.,2023) attempt to adopt a paradigm in which aninitial judgment on relevance is made by generat-ing a statement or special token before proceedingto content generation. However, these methodsstill lack accuracy in relevance discrimination andLLMs are still vulnerable to irrelevant documentinterference. Therefore, we propose a frameworkthat can accurately assess the relevance degree, andis more robust to irrelevant content.",
  "Task Formulation": "In this work, we focus on the task of open-domainquestion answering (QA) (Chen et al., 2017; Zhaoet al., 2024), aiming at answering questions usinga large collection of documents. Typically, open-domain QA tasks are often tackled with a retriever-reader approach (Chen and Yih, 2020), where theretriever finds relevant evidence and the reader gen-erates the answer based on the retrieved evidence.Formally, given a query q, the retriever outputstop-k documents D = {di}ki=1 from a documentcollection (can be refined by an optional reranker)at the first stage. Different from prior studies thatcombine the entire set of retrieved documents as aunified reference for answer generation (Hofstt-ter et al., 2023; Luo et al., 2023; Xu et al., 2023),our approach emphasizes individual document uti- Golden document Likely to be false negative Irrelevant docs we sample Likely to be less informative",
  "A = {ai}ki=1 = {LLM(q, di) | di D}.(1)": "Subsequently, we can choose the final answer fromA based on some specific ways, ensuring it alignsbest with the query q.Based on this task formulation, we consider en-hancing two key aspects: precise evaluation ofrelevance between queries and documents (iden-tifying relevant references), and leveraging rele-vance signal for noise-resistant generation (reduc-ing the influence of irrelevant content). Therefore,we introduce a relevance-aware approach designedspecifically for these challenges.",
  "Methodology": "In this section, we present the proposed RElevance-Aware Retrieval-augmented generation framework(REAR), which is capable of precisely assessingthe relevance degree during the generation pro-cess by incorporating explicit assessment moduleswithin the LLM. Furthermore, we propose opti-mized training methods that are compatible withthe REAR framework to support efficient opera-tion, including bi-granularity relevance fusion andnoise-resistant training.",
  "Relevance-Aware RAG Architecture": "In this part, we propose a novel architecture thataugments the LLM with a relevance-assessing mod-ule for enhancing the awareness of irrelevant inter-ference. As shown in (a), the inference ofthe REAR architecture encompasses three steps,including relevance assessment, relevance-guidedgeneration, and knowledge reliability verification. 4.1.1Relevance AssessmentInstead of treating all the retrieved documentsequally, we first aim to assess the relevance degreesof the documents. Drawing from the success ofLLM-based decoder in achieving precise relevanceassessment (Ma et al., 2023; Sun et al., 2023), wefirst map the input query-document pair into therelevance embedding vrel by the LLM:",
  "a = LLM(q, d, vguide).(5)": "4.1.3Knowledge Reliability VerificationBased on the generated answers, we finally ver-ify the correctness of the answers by consideringtwo factors: (a) Is the provided document reliableenough to trust the corresponding answer? (b)Without referring to the documents, to what degreewill the LLM adhere to its original response? Spe-cially, we propose two strategies, namely source-reliability and knowledge-consistency. Source-reliability: This strategy primarily em-phasizes the quality of external knowledge. If anLLM assigns a high relevance score to a document,then the answer derived from it is considered morereliable. Knowledge-consistency : This approach fur-ther verifies if the provided knowledge conflictswith the parametric knowledge. Specifically, in-spired by the success of self-consistency in Chain-of-Thought reasoning (Wang et al., 2022; Wei et al.,2022), we inform the LLM that the document isirrelevant by setting the relevance score to zero (de-noted by srel) and calculate the inverse of perplexityc (Meister and Cotterell, 2021) of generating theanswer a:",
  "In this part, we will introduce the trainingpipeline for optimizing our approach, As shownin (b)": "4.2.1Bi-granularity Relevance FusionPrecise relevance assessment is crucial for the re-liable utilization of retrieved documents. Previouswork often adopts the coarse-grained binary dis-crimination task (Yoran et al., 2023) , which cannotprovide sufficient evidence for solving complex QAtasks. Therefore, we consider further incorporatinga preference-based fine-grained task. Specifically,for the fine-grained supervision, we utilize the es-timated relevance scores (See .2.3) forderiving relevance preference constraints:",
  "Lbi-granularity = Lcoarse + Lfine.(8)": "4.2.2Noise-resistant TrainingIn addition to improving the capability of iden-tifying relevant documents, we further considerenhancing the discrimination ability when refer-ence documents contain irrelevant content or evennoise, such that the LLM can adaptively use exter-nal evidence for task solving. Specially, we furtherincorporate negative example documents D intothe original corpus D for optimizing LLMs:",
  "This labeling approach combines lexical and se-mantic similarity, allowing for the acquisition ofhigh-quality labels without accessing GPT APIs": "Irrelevant Documents Sampling. The trainingmethod necessitates the use of irrelevant (nega-tive) documents. It has been shown that negativesampling has a large impact on relevance assess-ment (Xiong et al., 2020). Specially, as shown in (b), we refine SimANS (Zhou et al., 2022)that ensures negatives are neither too difficult (falsenegatives) nor too trivial (uninformative):",
  "pi": "exp(a(si s+ b)2),si < s+ b,exp(ak(si s+ b)2),si s+ b,(11)where the sampling probability for the hard nega-tive document is pi, si and s+ respectively denotethe relevance scores of document di and the posi-tive document, and a, b, and k are hyperparameters.By incorporating a decay scaler k into the sam-pling probability when relevance scores are high,we reduce the chance of sampling false negatives.Finally, we define the overall loss functionfor our REAR framework by combining the bi-granularity loss by Eq. 8 and noise-resistant lossby Eq. 9:",
  ": The efficiency analysis of REAR and previouswork. T.C. is short for time complexity. d, p and ndenote the length of the document, the length of theprompt, and the number of documents respectively": "LLMs can generate more fine-grained relevancesignals to aid in the following generation process.Besides, LLMs can further calculate the consis-tency between parametric and external knowledgeto evaluate the reliability of answers. Moreover,this architecture makes it easy to adopt the pro-posed preference-based and noise-resistant lossfunctions. Furthermore, our label machine makesgood use of smaller models and traditional labels,and our sampling strategy improves training dataquality, eliminating the need for GPT APIs. Asa result, REAR achieves more precise relevanceevaluation and better generation performance (Ta-ble 3). Efficiency.We further discuss the efficiency ofour REAR, as shown in . First, we compareREAR with other RAG frameworks that employdifferent task formulations, such as Chain-of-Note(CoN) (Yu et al., 2023). CoN processes extensiveparagraphs and generates in-depth analyses to iden-tify usable parts of document collections. Thismethodology leads to increased training and infer-ence times due to the quadratic time complexityassociated with transformers (Dong et al., 2024),where time is proportional to the square of the inputsequence length. Besides, compared to Self-RAG,which follows a similar approach, REAR achievesa reduction in inference time. This improvementis primarily due to our integration of PagedAtten-tion (Kwon et al., 2023). By using PagedAttention,we ensure that calculations performed during therelevance assessment phase are preserved, therebyeliminating the need for redundant recalculations.The comparisons of actual training and inferencetimes in further illustrate the computationalefficiency of our method.",
  "EMF1EMF1EMF1EMF1EMF1": "Direct Retrieval-Augmented QALLaMA2-Chat 7B30.4741.3953.9262.7022.7938.2921.0931.6732.0743.51Mistral-It 7B10.8331.7744.5962.558.7130.7913.7834.2519.4839.84Baichuan2-Chat 7B33.4945.6161.1769.9823.8740.7826.5538.9736.2748.84ChatGLM3 6B13.2720.4824.5733.765.6118.388.3115.9812.9422.15 RobustLM prompting (4-shot)LLaMA2-Chat 7B30.5342.5753.2763.5221.0138.2921.8333.4531.6644.46Mistral-It 7B19.1132.8048.3159.8713.6330.7615.9828.2824.2637.93Baichuan2-Chat 7B27.4239.7252.0762.2718.9036.1319.2430.9229.4142.26ChatGLM3 6B24.6532.6746.5754.2320.3734.6018.7125.9027.5836.85 Fine-tuned RALMsSelf-RAG 7B41.0246.7852.3839.1531.4026.4135.2819.3340.0232.92RobustLM 7B44.4053.0862.8670.8832.4846.8927.5236.7541.8251.90REAR 7B w/ Source Rel.51.3360.5365.3674.1433.0247.6736.7846.6446.6257.25REAR 7B w/ Knowledge Con.51.4160.5066.2674.8733.5148.1437.2147.1947.1057.68 : A comparison between REAR and baselines on NQ, TriviaQA, WebQ and SQuAD datasets and theaveraged performance. our REAR approach surpasses all the other baselines in QA performance. The best andsecond-best results are in bold and underlined fonts respectively. Self-RAG is evaluated using accuracy (Acc)instead of EM, which is a less strict metric that measures whether the responses contain the answers. The last twolines are our REAR with different verification strategies: w/ Source Rel. means the source-reliability strategy, andw/ Knowledge Con. means the knowledge-consistency strategy.",
  "Experimental Setup": "Datasets. We collect the training data from theNatural Questions (NQ) (Kwiatkowski et al., 2019)training set. To ensure the models adaptability, wealso test its performance on three additional open-domain datasets, including TriviaQA (Joshi et al.,2017), WebQuestions (WebQ) (Berant et al., 2013),and SQuAD (Rajpurkar et al., 2016), showing itsgeneralization capabilities to out-of-domain data.We follow the test split in prior work (Karpukhinet al., 2020). The details are in Appendix B. Baselines. We consider the following two lines ofbaselines for comparison.(1) Retrieval augmentation based promptmethods: we design different prompting strate-gies based on open-source LLMs (without tuningtailored to RAG tasks) to support RAG, including Direct Retrieval-Augmented QA: We concate-nate the top 10 retrieved documents as a single ref-erence document for RAG. To enhance EM metricaccuracy, we further incorporate several answer ex-amples within the prompts, as illustrated in . RobustLM prompting: We following the ap-proach of the prompting strategy (Yoran et al.,2023). The LLMs are required to determine doc-ument relevance before generating responses. Itemploys 4-shot demonstrations (), and pro-vides the top 1 retrieved document.",
  "For open-source LLMs, we consider LLaMA2-Chat (Touvron et al., 2023), Mistral-It (Jiang et al.,2023), Baichuan2-Chat (Yang et al., 2023), andChatGLM3 (Du et al., 2022)": "(2) Specially designed RAG methods: we alsoconsider fine-tuned RobustLM (Yoran et al., 2023)and Self-RAG (Asai et al., 2023) as baselines,which have been specially optimized for the RAGtasks. To ensure a fair comparison, the two frame-works above are evaluated with the same set ofretrieved documents as used for REAR. Metrics. We employ three metrics to evaluate themodels capability of QA accuracy. Exact match(EM) (Lee et al., 2019) and F1 are widely adoptedfor open-domain QA evaluation. EM calculateswhether responses exactly match the gold truth an-swers and calculates the precision-recall overlapof predicted and true answers. We further evaluatethe accuracy in determining the relevance of thegiven document for LLMs with another two met-rics. Hit@1 evaluates if the document referencedfor the models final answer generation is relevant.JAcc, short for judgmental accuracy, measures theproportion of documents correctly evaluated byLLMs as relevant or not.",
  "JAccHit@1JAccHit@1JAccHit@1JAccHit@1": "+ RobustLM-prompting (Yoran et al., 2023)25.04-43.36-28.84-16.84-+ RobustLM-training (Yoran et al., 2023)56.59-56.09-49.61-56.99-+ Self-RAG (Asai et al., 2023)19.8151.1135.6964.4725.6947.9810.7338.73+ REAR (ours)74.0466.7980.7974.9865.9956.6959.3653.26 : The relevance discrimination and comparison capabilities of REAR and previous approaches. GenerativeLLMs struggle to determine the relevance degree of the given document, while our REAR overcomes it with thewell-designed assessment module.",
  "Main Results": "shows the results of REAR and baselineson four open-domain QA tasks.First, our REAR approach surpasses all the otherbaselines in QA performance. REAR not only per-forms well on the trained dataset, but also achievesgood results on non-training datasets. This demon-strates that our precise signals for capturing rel-evance effectively guide the generation process.Thus, LLM can generate with good use of bothparametric and external knowledge.Besides, the result shows the efficiency of dataconstruction method, even without access to GPTAPIs. Self-RAG labels the relevance degree withGPT-4, while RobustLM and REAR utilize ourproposed label machine and sampling method. Theresult indicates that our data construction strategyis effective and less costly.Third, generative LLMs struggle to determinethe reliability degree of the given document, whileour REAR overcomes it with the well-designedassessment module. As shown in , eventhe fine-tuned generative approaches (RobustLM-training and Self-RAG) adequately discriminaterelevance. In comparison, REAR significantly en-hances this capability, highlighting its effectivenessin architectural design.",
  ": Ablation study on our REAR. The aspect de-notes the affected aspect. Arch., Obj. and Sam. denotearchitecture, training objective and sampling strategy": "in three aspects, including the architecture, trainingobjectives and sampling strategy.(1) w/o Assessment: the variant without the inte-gration of the rating module. We utilize languagegeneration to assess relevance degrees instead ofthe rank head. The document is selected basedon the probability of generating judgmental state-ments. There is a notable drop in the comparisonaccuracy (see Hit@1 metric), similar shortfall isalso observed in Self-RAG (). This demon-strates the effectiveness of our architectural design,which not only minimizes interference betweenlanguage generation and relevance discrimination,but also facilitates the incorporation of various lossfunctions.(2) w/o Consistency: using the path-reliabilitystrategy instead of the knowledge consistency strat-egy. The path-reliability approach achieves higherHit@1 rates, yet falls behind in EM and F1 scorescompared to the knowledge-consistency strategy.The latter conducts a self-verification of outputsbased on its generation ability, effectively integrat-ing inherent knowledge in relevance assessment,which enhances the accuracy of RAG.(3) w/o Bi-granularity: the variant without bi-granularity fusion in relevance assessment training.We replace the bi-granularity loss with the coarse-grained loss function. The results indicate that thefine-grained relevance training could enhance theLLMs in relevance comparison among documents,",
  ": Results of factual generation accuracy providedwith top-1 retrieved documents on the test set of NQ.Categorized by performance when providing relevant(Rel) and irrelevant (Irr) documents": "and result in better performance.(4) w/o Noise-resistant: the variant withoutnoise-resistant training. We exclude the gold-noisedata pairing, using the similar training constructionapproach of Self-RAG and RobustLM, with onedocument per query. We observe a notable decline,underscoring the effectiveness of noise-resistanttraining to enhance generation against irrelevantdocument interference.(5) w/o Sampling: the variant training with ran-dom hard negatives for training. We can observe asignificant drop in relevance assessment capability,further illustrating the effectiveness of our method.",
  "In this part, we further analyze the impact of re-trieved documents in both single-document andmulti-document settings": "Single-Document Setting. We first examine theimpact of external evidence in single document set-ting, where only the top first retrieved documentis taken for reference. shows the factualaccuracy of different LLMs. We can see that bothSelf-RAG and REAR, after fine-tuning, performwell in relevant document utilization. However,REAR significantly outperforms other LLMs ingenerating accurate responses when the referencedocument is irrelevant, highlighting its robust resis-tance to interference from noisy documents. Multi-Document Setting. In the second setting,we assume that multiple retrieved documents canbe used for reference. Specially, we mainly ex-amine the impact of the total number and rele-vance degree of reference documents. For this pur-pose, we vary the number of provided documents((a)) and the retrievers capabilities ((b)).From (a), we can see that our REAR ap- (a) Counts of Provided Documents EM (Acc) REARFiD Self-RAGLlama2-13B (b) Document Quality (Hit@10) R1R2R3 EM (Acc) REARFiD Self-RAGLlama2-13B : Results of RAG performance vary in over-all document count and quality. The left one presentsRAG performance with varying numbers of retrieveddocuments. The right one is the results of RAG withdifferent retriever engines. R1, R2, and R3 representBM25, Contriever-msmarco, and the FiD-distilled re-triever, R1 < R2 < R3 ( of the Appendix) . proach performs well when provided with a singledocument (i.e., the top retrieved one), while basemodels without fine-tuning suffer from significantdegradation in this case. Furthermore, as shown in(b), our approach is very robust to externalretrievers of varied retrieval capacities. Especially,when equipped with the weakest retriever BM25,it yields a large improvement over the other base-lines, which further demonstrates that our approachcan effectively perceive the relevance of externalevidence for more suitable utilization.",
  "Conclusion": "In this paper, we aimed to enhance the self-awareness of source relevance in RAG systems, andproposed REAR, a RElevance-Aware Retrieval-augmented approach for open-domain question an-swering (QA). For model architecture, we explic-itly integrate an assessment module to preciselycapture the relevance signals, and employ it toguide the utilization of external knowledge. Formodel training, we designed an improved train-ing method with bi-granularity relevance fusionand noise-resistant training, which enhance the ca-pacities of fine-grained relevance assessment andadaptive use of retrieved documents. Our data con-struction strategy collects high-quality data with-out access to GPT APIs. Extensive experimentson four datasets demonstrate the effectiveness andgeneralization of REARs knowledge utilization.As future work, we will extend the proposedapproach REAR to deal with more fine-grainedsource utilization (e.g., passage or sentence levelaugmentation), and also consider applying REARto other knowledge-intensive tasks.",
  "Limitations": "For LLMs, the challenge of being misled by irrele-vant retrieved documents is a significant obstacle,underscoring the crucial need for enhancing LLMsability to adaptively utilize retrieved documents. Inresponse to this issue, our work has concentratedon refining the architecture and training methods tobolster the effective use of retrieved documents byLLMs. We have implemented document-level rel-evance assessment and dynamic utilization strate-gies, significantly boosting the factual accuracy ofgenerated content by LLMs. However, our currentapproach has not delved into guiding LLMs to fo-cus more granularly on key sentences or tokenswithin the retrieved documents.Moreover, the applicability of our methodsacross a broader spectrum of RAG tasks, suchas those encompassed by the KILT benchmark,remains to be thoroughly evaluated.This gappresents a pivotal area for our future investigations.",
  "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, andHannaneh Hajishirzi. 2023. Self-rag: Learning toretrieve, generate, and critique through self-reflection.arXiv preprint arXiv:2310.11511": "Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang. 2013. Semantic parsing on freebase fromquestion-answer pairs. In Proceedings of the 2013conference on empirical methods in natural languageprocessing, pages 15331544. Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-mann, Trevor Cai, Eliza Rutherford, Katie Milli-can, George Bm Van Den Driessche, Jean-BaptisteLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.Improving language models by retrieving from tril-lions of tokens. In International conference on ma-chine learning, pages 22062240. PMLR. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Danqi Chen, Adam Fisch, Jason Weston, and AntoineBordes. 2017. Reading wikipedia to answer open-domain questions. In Proceedings of the 55th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers). Association forComputational Linguistics.",
  "Danqi Chen and Wen-tau Yih. 2020. Open-domainquestion answering. ACL 2020, page 34": "Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, HongzhiZhang, Fuzheng Zhang, Di Zhang, Kun Gai, andJi-Rong Wen. 2024. Small agent can also rock! em-powering small language models as hallucinationdetector. arXiv preprint arXiv:2406.11277. Florin Cuconasu, Giovanni Trappolini, Federico Sicil-iano, Simone Filice, Cesare Campagnano, YoelleMaarek, Nicola Tonellotto, and Fabrizio Silvestri.2024. The power of noise: Redefining retrieval forrag systems. arXiv preprint arXiv:2401.14887. Zican Dong, Junyi Li, Xin Men, Wayne Xin Zhao, Bing-bing Wang, Zhen Tian, Weipeng Chen, and Ji-RongWen. 2024. Exploring context window of large lan-guage models via decomposed positional vectors.arXiv preprint arXiv:2405.18009. Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao,and Ji-Rong Wen. 2023. Bamboo: A comprehen-sive benchmark for evaluating long text modelingcapacities of large language models. arXiv preprintarXiv:2309.13345. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:General language model pretraining with autoregres-sive blank infilling. In Proceedings of the 60th An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 320335. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and HaofenWang. 2023. Retrieval-augmented generation forlarge language models: A survey. arXiv preprintarXiv:2312.10997.",
  "Gautier Izacard and Edouard Grave. 2021a. Distillingknowledge from reader to retriever for question an-swering. In ICLR 2021-9th International Conferenceon Learning Representations": "Gautier Izacard and Edouard Grave. 2021b. Leveragingpassage retrieval with generative models for opendomain question answering. In EACL 2021-16thConference of the European Chapter of the Associa-tion for Computational Linguistics, pages 874880.Association for Computational Linguistics. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lu-cas Hosseini, Fabio Petroni, Timo Schick, JaneDwivedi-Yu, Armand Joulin, Sebastian Riedel, andEdouard Grave. 2022. Few-shot learning with re-trieval augmented language models. arXiv preprintarXiv:2208.03299. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023. Mistral7b. arXiv preprint arXiv:2310.06825. Mandar Joshi, Eunsol Choi, Daniel S Weld, and LukeZettlemoyer. 2017. Triviaqa: A large scale distantlysupervised challenge dataset for reading comprehen-sion. In Proceedings of the 55th Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 16011611. Vladimir Karpukhin, Barlas Oguz, Sewon Min, PatrickLewis, Ledell Wu, Sergey Edunov, Danqi Chen, andWen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the2020 Conference on Empirical Methods in Natu-ral Language Processing (EMNLP). Association forComputational Linguistics. Omar Khattab and Matei Zaharia. 2020. Colbert: Effi-cient and effective passage search via contextualizedlate interaction over bert. In Proceedings of the 43rdInternational ACM SIGIR conference on researchand development in Information Retrieval, pages 3948. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-field, Michael Collins, Ankur Parikh, Chris Alberti,Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-ton Lee, et al. 2019. Natural questions: a benchmarkfor question answering research. Transactions of theAssociation for Computational Linguistics, 7:453466. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, YingSheng, Lianmin Zheng, Cody Hao Yu, Joseph Gon-zalez, Hao Zhang, and Ion Stoica. 2023. Efficientmemory management for large language model serv-ing with pagedattention. In Proceedings of the 29thSymposium on Operating Systems Principles, pages611626.",
  "Annual Meeting of the Association for ComputationalLinguistics, pages 60866096": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, FabioPetroni, Vladimir Karpukhin, Naman Goyal, Hein-rich Kttler, Mike Lewis, Wen-tau Yih, Tim Rock-tschel, et al. 2020. Retrieval-augmented generationfor knowledge-intensive nlp tasks. Advances in Neu-ral Information Processing Systems, 33:94599474. Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-YunNie, and Ji-Rong Wen. 2023a. Halueval: A large-scale hallucination evaluation benchmark for largelanguage models. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 64496464.",
  "Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jingyuan Wang,Jian-Yun Nie, and Ji-Rong Wen. 2023b. The web canbe your oyster for improving large language models.arXiv preprint arXiv:2305.10998": "Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi,Maria Lomeli, Rich James, Pedro Rodriguez, JacobKahn, Gergely Szilvasy, Mike Lewis, et al. 2023.Ra-dit: Retrieval-augmented dual instruction tuning.arXiv preprint arXiv:2310.01352. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-jape, Michele Bevilacqua, Fabio Petroni, and PercyLiang. 2023.Lost in the middle:How lan-guage models use long contexts.arXiv preprintarXiv:2307.03172. Hongyin Luo, Yung-Sung Chuang, Yuan Gong, Tian-hua Zhang, Yoon Kim, Xixin Wu, Danny Fox, He-len Meng, and James Glass. 2023. Sail: Search-augmented instruction learning.arXiv preprintarXiv:2305.15225.",
  "Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,Amnon Shashua, Kevin Leyton-Brown, and YoavShoham. 2023. In-context retrieval-augmented lan-guage models. arXiv preprint arXiv:2302.00083": "Ruiyang Ren, Shangwen Lv, Yingqi Qu, Jing Liu,Wayne Xin Zhao, Qiaoqiao She, Hua Wu, HaifengWang, and Ji-Rong Wen. 2021a.Pair: Leverag-ing passage-centric similarity relation for improvingdense passage retrieval. In Findings of the Associ-ation for Computational Linguistics: ACL-IJCNLP2021, pages 21732183. Ruiyang Ren, Peng Qiu, Yingqi Qu, Jing Liu,Wayne Xin Zhao, Hua Wu, Ji-Rong Wen, andHaifeng Wang. 2024. Bases: Large-scale web searchuser simulation with large language model basedagents. arXiv preprint arXiv:2402.17505. Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao,Qiaoqiao She, Hua Wu, Haifeng Wang, and Ji-RongWen. 2021b. Rocketqav2: A joint training methodfor dense passage retrieval and passage re-ranking.In Proceedings of the 2021 Conference on Empiri-cal Methods in Natural Language Processing, pages28252835. Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne XinZhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen,and Haifeng Wang. 2023.Investigating the fac-tual knowledge boundary of large language mod-els with retrieval augmentation.arXiv preprintarXiv:2307.11019.",
  "Stephen E Robertson, Steve Walker, Susan Jones,Micheline M Hancock-Beaulieu, Mike Gatford, et al.1995. Okapi at trec-3. Nist Special Publication Sp,109:109": "Devendra Sachan, Mostofa Patwary, MohammadShoeybi, Neel Kant, Wei Ping, William L Hamil-ton, and Bryan Catanzaro. 2021. End-to-end trainingof neural retrievers for open-domain question answer-ing. In Proceedings of the 59th Annual Meeting of theAssociation for Computational Linguistics and the11th International Joint Conference on Natural Lan-guage Processing (Volume 1: Long Papers), pages66486662. Freda Shi, Xinyun Chen, Kanishka Misra, NathanScales, David Dohan, Ed H Chi, Nathanael Schrli,and Denny Zhou. 2023. Large language models canbe easily distracted by irrelevant context. In Inter-national Conference on Machine Learning, pages3121031227. PMLR.",
  "Bhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,Ed Chi, Sharan Narang, Aakanksha Chowdhery, andDenny Zhou. 2022. Self-consistency improves chainof thought reasoning in language models.arXivpreprint arXiv:2203.11171. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in NeuralInformation Processing Systems, 35:2482424837. Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,Jialin Liu, Paul N Bennett, Junaid Ahmed, andArnold Overwijk. 2020. Approximate nearest neigh-bor negative contrastive learning for dense text re-trieval. In International Conference on LearningRepresentations.",
  "Wayne Xin Zhao, Jing Liu, Ruiyang Ren, and Ji-RongWen. 2024. Dense text retrieval based on pretrainedlanguage models: A survey. ACM Transactions onInformation Systems, 42(4):160": "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,Xiaolei Wang, Yupeng Hou, Yingqian Min, BeichenZhang, Junjie Zhang, Zican Dong, et al. 2023. Asurvey of large language models.arXiv preprintarXiv:2303.18223. Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen,Heng-Tze Cheng, Ed H Chi, Quoc V Le, and DennyZhou. 2023. Take a step back: evoking reasoning viaabstraction in large language models. arXiv preprintarXiv:2310.06117. Kun Zhou, Yeyun Gong, Xiao Liu, Wayne Xin Zhao,Yelong Shen, Anlei Dong, Jingwen Lu, Rangan Ma-jumder, Ji-Rong Wen, and Nan Duan. 2022. Simans:Simple ambiguous negatives sampling for dense textretrieval. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Processing:Industry Track, pages 548559. Fengbin Zhu, Wenqiang Lei, Chao Wang, JianmingZheng, Soujanya Poria, and Tat-Seng Chua. 2021.Retrieving and reading: A comprehensive survey onopen-domain question answering. arXiv preprintarXiv:2101.00774. D1: Wilhelm Conrad Rntgen won first Nobel Prize in Physics. D2: Wilhelm Conrad Rntgen won it for discovery of X-rays. D4: The Nobel Prize in Physics is a yearly award. D5: First law of thermodynamics was stated by William. D3: Albert Einstein was awarded the 1921 Nobel Prize in Physics. Q: Who won the first Noble Prize in Physic? A: Wilhelm Conrad Rntgen Binary Labels(has answer)Binary Labels(has answer)",
  "ADetails on Fine-Gained RelevanceOptimization": "We first illustrate why to design the fine-grained op-timization for the assessment module. Traditionalannotation methods always use a binary labelingmethod (Karpukhin et al., 2020), which is basedon the presence of an answer within a document.As shown in , both D1 and D2 are labeledas relevant. However, while D1 allows for directanswer derivation, D2 requires additional externalknowledge for induction. Training models solelyon simple binary classification fails to distinguishthe superiority of D1 over D2, potentially leadingto inaccuracies in finer relevance judgments. Previous work has achieved success in relevanceassessment by distilling the ranking results fromGPT-4 (Sun et al., 2023). Inspired by it, we pro-pose a less costly solution by labeling with small-scale, well-trained cross-encoder rerankers Rock-etQAv2 (Ren et al., 2021b).Despite the goodrelevance evaluation performance, it still may getwrong. We adopt three strategies to reduce the nega-tive impact of annotation errors on training. Firstly,we design the sampling method (Eq. 11), whichreduces the likelihood of potentially false negativesbeing sampled. Besides, we linearly combine thebinary label with cross-encoder scores. Thirdly,to mitigate noise from rerankers, we disregard dif-ferences smaller than 0.1 in fine-gained relevancetraining in Eq. 7. These strategies enhance thequality of training data, which in turn improves theperformance of REAR.",
  ": Dataset statistics of the test set": "NQ: a dataset designed to support compre-hensive QA systems. It includes questions sourcedfrom actual Google search queries. The correspond-ing answers are text spans within Wikipedia arti-cles, meticulously identified by human annotators. TriviaQA: a compilation of trivia questionspaired with answers, both of which were initiallyextracted from online sources. WQ: constructed from questions proposed viathe Google Suggest API, with answers being spe-cific entities listed in Freebase. SQuAD: a dataset for evaluating reading com-prehension, and also is used for training and testingopen-domain QA engines.NQ is used for both training and inference, whilethe other three are only used for inference. We usethe same split as previous work (Karpukhin et al.,2020). The training set of NQ contains 58,880samples.",
  "CDetails on Document Collection": "In this part, we introduce the retrievers we usedto collect documents. We employ task-specificretrievers to acquire the retrieved document. For in-ference, we utilize FiD-distilled retrievers (Izacardand Grave, 2021a) for NQ and TriviaQA datasets.And we implement a strategy incorporating in-batch negatives and joint retriever-ranker train-ing, starting from the Contriever-msmarco (Izac-ard et al., 2021) checkpoint for SQuAD and WQdatasets. The recall and MRR rates of the retrieveddocuments for inference are shown in Table. 8.",
  "@1032.3551.4560.32": ": Performance of three retrievers on the NQ testset. Hit@10 measures the percentage of correct answerswithin the top 10 results, indicating the precision ofthe retriever. MRR@10 (Mean Reciprocal Rank at 10)calculates the average of the reciprocal ranks of thefirst correct answer within the top 10 results, reflectingthe effectiveness and rank of correct answers by thesystem. R1, R2 and R3 denote BM25 (Robertson et al.,1995), Contriever-msmarco (Izacard et al., 2021) andthe dense retriever (Izacard and Grave, 2021a) trainedby distilling attention scores of FiD reader (Izacard andGrave, 2021b)",
  "DDetails on Implementation": "Following the previous work (Asai et al., 2023),we apply joint optimization combining relevanceassessment and relevance-guided generation, asspecified in Eq. 12. The training utilizes a learningrate of 1e-6, a warm-up ratio of 0.03, a batch sizeof 64 and a cosine scheduler for 1 epoch. Ourexperiments leverage the computational power of8 NVIDIA Tesla A100 GPUs, each with 40G ofmemory.",
  "EDetails on Baselines": "In this part, we detail the prompt design and infer-ence details for baselines. For the prompt-basedinference, we utilize the instruction-tuned open-source models obtained from Hugging Face. Fol-lowing the existing work (Asai et al., 2023; Renet al., 2024; Tang et al., 2024), we use the greedydecoding strategy for inference. The specific in-struction formats used in our tests are illustrated in"
}