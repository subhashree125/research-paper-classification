{
  "Abstract": "Previous studies on multi-party dialogue gener-ation predominantly concentrated on modelingthe reply-to structure of dialogue histories, al-ways overlooking the coherence between gen-erated responses and target utterances. To ad-dress this issue, we propose a ReinforcementLearning approach emphasizing both Topic andRhetorical Coherence (RL-TRC). In particular,the topic- and rhetorical-coherence tasks aredesigned to enhance the models perceptionof coherence with the target utterance. Sub-sequently, an agent is employed to learn a co-herence policy, which guides the generationof responses that are topically and rhetoricallyaligned with the target utterance. Furthermore,three discourse-aware rewards are developed toassess the coherence between the generated re-sponse and the target utterance, with the objec-tive of optimizing the policy. The experimentalresults and in-depth analyses on two populardatasets demonstrate that our RL-TRC signif-icantly outperforms the state-of-the-art base-lines, particularly in generating responses thatare more coherent with the target utterances.",
  "Introduction": "In a two-party dialogue, a response is always gen-erated for the last utterance in the dialogue history.This contrasts with multi-party dialogue, whichalways involves multiple participants and each ut-terance can be uttered by any participant and replyto any participant else (Gu et al., 2022b). In a multi-party dialogue, the position of the target utterance(replied utterance) is not fixed. illustratesa multi-party dialogue involving four participants.In this example, the bot of Multi-party DialogueGeneration (MDG) is required to assume the roleof participant P1 and generate an appropriate re-sponse to target utterance U2.Previous studies (Hu et al., 2019; Li and Zhao,",
  ": An example of multi-party dialogue from theHu (Hu et al., 2019) dataset. EMMDG (Li and Zhao,2023) and MADNet (Gu et al., 2023b) are two SOTAbaselines": "marily focused on leveraging the reply-to struc-ture within dialogue history to enhance generation.However, these methods do not adequately guaran-tee the coherence between the generated responseand the target utterance.Firstly, the modeling of reply-to structures doesnot guarantee that the model will comprehend thetopics of the dialogue history, which is of the ut-most importance for generating responses that alignwith the topic of the target utterance. Multi-partydialogue always has multiple entangled parallel di-alogue flows, covering different topics. As shownin , the example primarily concerns twotopics, among which U1, U2, and U7 (human re-sponse) discuss the browsers (Firefox and Opera)selection and resource usage, while U3, U4, U5, andU6 primarily discuss the usage of Emacs. Since thereply-to structure does not unravel the entangled topics in a multi-party dialogue, it is challengingfor the model to consistently generate a responsethat aligns with the target utterances topic. As illus-trated by the response generated by EMMDG (Liand Zhao, 2023) in , the responses topicdoes not relate to the target utterance U2, despiteEMMDG modeling the reply-to structure. Sec-ondly, modeling reply-to structures does not guar-antee logical and meaningful interactions betweenthe generated response and the target utterance. Asillustrated in , while the response generatedby MADNet (Gu et al., 2023b) remains relevant tothe topic (Opera) of the target utterance, its inter-nal contradictions undermine the logical coherencebetween it and the target utterance.To address these challenges, we propose a Re-inforcement Learning approach emphasizing bothTopic and Rhetorical Coherence (RL-TRC), guid-ing the generation of responses that are both top-ically and logically consistent with the target ut-terance. We initially design tasks for topic andrhetorical coherence, utilizing the target utteranceto predict the topic of the generated response andits rhetorical relation with the target utterance. Sub-sequently, our reinforcement learning agent learnsa coherence policy to guide the model in gener-ating responses that are topically and rhetoricallyconsistent with the target utterance. To optimizethis policy, we meticulously design two local re-wards and one global reward. The local rewardsguide policy learning by scoring the topic coher-ence and rhetorical coherence between the gener-ated response and the target utterance, respectively.The global reward guides policy learning by mea-suring whether the target utterance in the dialoguehistory can be correctly recognized based on thegenerated response.We conduct extensive experiments on two popu-lar Ubuntu IRC channel datasets, and results fromboth automated and manual evaluations indicatethat our RL-TRC significantly outperforms pre-vious SOTA baselines. In-depth analysis revealsthat RL-TRC can generate responses for target ut-terances that are more coherent in both topic andrhetoric.",
  "Multi-party Dialogue GenerationMost previ-ous studies on MDG have focused on modelingcomplex reply-to structures within dialogue his-tories. Hu et al. (2019) proposed a graph struc-": "ture neural network that treated utterances as nodesand reply-to relations as edges to model the di-alogue history. Gu et al. (2022a) introduced aheterogeneous graph-based neural network to si-multaneously model the semantics of utterancesand participants using two types of nodes and sixtypes of node-edge relations. Li and Zhao (2023)and Gu et al. (2023b) proposed the adoption ofthe Expectation-Maximization (EM) algorithm tomodel the reply-to structure of dialogue history.Furthermore, some studies have explored en-hancing MDG from a discourse perspective.Chernyavskiy and Ilvovsky (2023) suggested usingdialogue action planning to improve the consis-tency and quality of dialogue generation. Mean-while, Chernyavskiy et al. (2024) proposed lever-aging various fine-grained linguistic inputs, includ-ing Abstract Meaning Representation, discourserelations, sentiment, and grounding information,to facilitate multi-party dialogue generation. Re-cently, with the advent of Large Language Mod-els (LLMs), Tan et al. (2023) evaluated the perfor-mance of ChatGPT and GPT-4 on MDG. Despitesignificant progress in MDG, incoherence betweenthe generated response and the target utterance re-mains a challenge that needs to be addressed. Dialogue Coherence ModelingDialogue coher-ence modeling is primarily applied in persona-based dialogue systems, aiming to help models gen-erate responses that are consistent with the givenpersona (Chen et al., 2024). Most previous workhas focused on minimizing contradictions betweenresponses and personas using natural language in-ference models (Song et al., 2021; Chen et al.,2023), conditional VAE (Lee et al., 2022), andover-sampling followed by post-evaluation (Zhouet al., 2023).Unlike previous work on the coherence betweenthe generated response and the persona, we seekto enhance the coherence between the generatedresponse and the target utterance in multi-partydialogues. Uniquely, we model coherence fromthe perspectives of topic and rhetoric, addressingthe multiple topic flows and logical structures inmulti-party dialogues.",
  ": Framework of our method. The parameters ofboth decoders are shared": "encoded by a dialogue encoder. Then, the topic andrhetorical coherence tasks are devised to enhancethe models awareness of coherence with the targetutterance. The coherence semantics are treated asaction candidates for the coherence policy and arefed into the decoder to generate a trigger sequencethat updates the state. The policy is optimized byevaluating the coherence between the generatedresponse and the target utterance using carefullydesigned rewards.",
  "Following Li and Zhao (2023) and Gu et al.(2023b), we use BART (Lewis et al., 2020) as": "encoder. The dialogue history is fed into the en-coder in the form of [CLS][SEP]p1u1[SEP]p2u2 [SEP][RT] ptut [SEP]pnun[SEP]pr to ob-tain the hidden state H Rld, where l and d arethe sequence length and the dimension of hiddenstate, respectively, and [RT] is the target utterancemarker. We denote the semantic representation ofthe utterance as hui, which is derived from [SEP]preceding each utterance.",
  "Topic Coherence Task": "The purpose of the topic coherence task is to enablethe model to be aware of the topic coherence withthe target utterance when generating responses. Tothis end, we first construct the topic coherence ma-trix between the target utterance and the goldenresponse. Subsequently, the target utterance andthe topic coherence matrix are combined in order topredict the topic of the generated response, therebyensuring coherence between the target utteranceand the generated response.To construct the topic coherence matrix, we cal-culate the Pointwise Mutual Information (PMI)(Church and Hanks, 1990) scores between the tar-get utterance and the golden response keywords,and higher PMI scores are associated with strongertopic coherence. Specifically, we first extract thekeywords of each utterance by using ChatGPT,which has shown a great topic understanding abil-ity (Fan et al., 2024). The prompt is shown inAppendix A. Given two keywords wi and wj, thePMI score of keyword pairs is calculated as",
  "p(wi)p(wj)(2)": "where p(wi/wj) is the co-occurrence frequencybetween wi in the golden response and wj in thetarget utterance, p(wi) and p(wj) are the frequencyof wi and wj, respectively.For each keywordwj in the target utterance, the wi with the top 10PMI(wi, wj) scores is adopted to construct thetopic coherence matrix. It is worth noting that allutterance pairs with the reply-to relation are uti-lized to calculate PMI scores.After obtaining the topic coherence matrix,we first deduce the coherence keywords Cck ={w1, w2, , wk} from the topic coherence matrixaccording to the keywords of the target utterance,where k is the number of inferred coherence key-words. The semantics Eck of the coherence key-words is obtained by feeding them into the dialogueencoder, where Eck Rkd. Then, the semantics",
  "Rhetorical Coherence Task": "The task of rhetorical coherence primarily enablesthe model to maintain rhetorical coherence with thetarget utterance when generating responses. Giventhat rhetorical relations can explicitly reveal andenhance the understanding of logical and seman-tic coherence between utterances in multi-partydialogues (Asher and Lascarides, 2003), we uti-lize a discourse parsing tool (Wang et al., 2021)to analyze the dialogue and identify the rhetoricalrelation between the golden response and the targetutterance. Details can be found in Appendix B.Similar to the topic coherence task, we feed hutinto another MLP to perform the discourse relationprediction task as",
  "State": "The dialogue history C is regarded as theinitial state of the RL process denoted as s1.At the step k, the policy updates the stateby selecting the topic or rhetorical coherencesemantics to generate the trigger sequence kand the observed state is denoted as sk={(p1, u1), ..., (pn, un), (p1, 1), ..., (pk1, k1)}.Then, sk is fed into the dialogue encoder toobtain the dialogue semantics hsk derived fromthe [CLS] token.Finally, the final semanticsrepresentation of the current state sk is obtainedby concatenating all the historical state semantics,denoted sek = [hs1 : : hsk].",
  "Action": "At the step k, the actor requires to take action akto select the topic coherence semantics (haut) orrhetorical coherence semantics (hbut), where ak A = {0, 1} (0 for the topic coherence semanticsand 1 for the rhetorical coherence semantics). Then,the selected coherence semantics are fed into theBART decoder as a context embedding to generatethe trigger sequence k.",
  "Policy": "In addition to using a dialogue encoder as a se-mantic encoding policy network, we design a co-herence policy network based on the actor-criticframework. The actor learns a coherence policy(sk, ak), which takes the appropriate action akto select the coherence semantics based on the cur-rent state sk. The critic guides the actor to takethe appropriate action ak by evaluating the valueQ(sk) of state sk. The actor-critic network is as",
  "the target utterance, and the global reward mea-sures whether the target utterance can be recog-nized based on the generated response": "Topic-coherence RewardThe purpose of thetopic-coherence reward is to guide the generationof responses by measuring the topic coherence be-tween the generated response y with its keywordsykws1 and the target utterance ut with its keywordsut_kws. We reconstruct the Hu dataset (Hu et al.,2019) to construct the coherent pair (ut, y) and in-coherent pair (ut, yneg), in which yneg is an utter-ance randomly selected from the current dialoguenot reply to target utterance. Then, the topic evalu-ator ftc is trained on these coherent and incoherentpairs by using RoBERTa-base (Liu et al., 2019),achieving 84% accuracy. More details are in Ap-pendix C. We adopt the topic evaluator to evalu-ate the coherence between the generated responseand the target utterance and treat the coherenceprobability as a topic-coherence reward, which isformulated as",
  "Rtc = ftc([ut; ut_kws], [yt; ykws]) e(n/|ykws|1)(13)where n is the number of words in ykws that can bededuced through the topic coherence matrix usingut_kws": "Rhetorical-coherenceRewardRhetorical-coherence reward is used to measure the similarityof the probability distribution of the rhetoricalrelation between (ut, y) and (ut, y), where ut isthe target utterance, y is the generated response,and y is the golden response.Since the Hudataset did not annotate rhetorical relations, wereconstruct the Molweni dataset (Li et al., 2020) toconstruct the target utterance and golden responsepairs, and then trained a relation classifier frcwith the accuracy of 65%.More details arein Appendix C. We treat the KL divergencescore between frc(ut, y) and frc(ut, y) as therhetorical-coherence reward as",
  "Training": "During training, we mainly optimize the parame-ters, including the RL agent, response generation,and coherence task learning. To optimize the learn-ing of the RL agent, we maximize the expectedcumulative reward J() = E[Kk=1 krk], where is the parameters of actor-critic network, is thediscount coefficient, which reduces the importanceof rewards received in the future, and K is the op-timization steps, indicating that taking K actionswith one iteration of optimization. The agent isoptimized by the following loss:",
  "BART-based": "BART (Lewis et al., 2020)11.254.021.780.954.469.90HeterMPC (Gu et al., 2022a)12.264.802.421.494.9411.20EMMDG (Li and Zhao, 2023)12.315.393.342.455.5211.71MADNet (Gu et al., 2023b)12.735.122.641.635.3111.74RL-TRC (Ours)13.66*6.58*4.10*2.93*6.20*12.72* : Automatic evaluation results on the Hu dataset where * denotes that the improvements are statisticallysignificant (t-test with p-value < 0.05) comparing with the SOTA baselines EMMDG and MADNet, and representsthat we reproduced the performance by running the publicly available code.",
  "Baselines": "we compare our method with the following base-lines. ChatGPT (Tan et al., 2023): It directly usesChatGPT for MDG. We re-ran their publicly avail-able code and used the latest version of GPT-4o-2024-05-13. Notably, we used the same evalua-tion code as previous work(Gu et al., 2023b) for afair comparison. GSN (Hu et al., 2019): it adoptsthe homogeneous graph to model the structure ofdialogue history. BART (Lewis et al., 2020): itdirectly uses the BART-base model for MDG. Het-erMPC (Gu et al., 2022a): it models the com-plicated interactions between utterances and inter-locutors in dialogue history with a heterogeneousgraph. EMMDG (Li and Zhao, 2023): it proposes an expectation-maximization approach that itera-tively performs the expectation steps to generateaddressee labels, and the maximization steps to op-timize a response generation model. MADNet (Guet al., 2023b): it maximizes addressee deduction ex-pectation in heterogeneous graph neural networksfor MDG.",
  "Implementation Details": "The pre-trained model we adopt is the BART-base2 version. The maximum length of the dialoguehistory and generated response is set to 512 and 50,respectively. The epoch of pre-training and multi-task training are set to 3 and 10, respectively. Thestrategy of greedy search was adopted for decoding.The batch size is set to 128. The optimization stepK of the RL agent is set to 2, which is obtained byusing the grid search in {1, 2, 3, 4, 5}. The rewardweights wtc, wrc, wrt are set 0.5, 0.5, 1, respec-tively, which is obtained by using the grid-searchin {0.5, 1}. The discount coefficient is set to 0.99.The Adam (Loshchilov and Hutter, 2018) optimizerwith an initial learning rate of 2e-5 is adopted andthe linear warmup step is set to 200.",
  "Metrics": "We follow previous work (Li and Zhao, 2023; Guet al., 2022a, 2023b) for automatic and human eval-uation. The automatic metrics include BLEU-1(B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4),METEOR (M), and ROUGEL (RL). For humanevaluation, the quality of the generated responsesis assessed from three independent dimensions: 1)fluency, 2) informativeness, and 3) relevance. Eachhuman evaluator assigns three binary scores foreach response, which are then summed to producea final score ranging from 0 to 3. Importantly, forthe relevance dimension, we specifically measurethe alignment of the generated response with thetarget utterance.",
  "Results": "Tables 2 and 3 present the automated results ofour RL-TRC and baselines. It can be observed thatthe LLMs-based model ChatGPT can only achievecomparable performance to the other baselines, in-dicating that MDG still faces great challenges. Ad-ditionally, compared with the BART model, Het-erMPC, EMMDG, and MADNet significantly en-hance dialogue generation quality by incorporatingreply-to structures. However, these methods stillsuffer from incoherence between the generated re-sponses and the target utterances. Our RL-TRCoutperforms all baselines in terms of all metrics,demonstrating its effectiveness in improving gen-eration quality by enhancing discourse coherencebetween generated responses and target utterances. presents the results of the human eval-uation on the Hu dataset. We randomly sampled200 examples from the test set and recruited threecomputer science master students familiar withUbuntu and Linux to score each response. Theresults demonstrate that all models exhibit compa-rable performance to humans in terms of fluency,indicating that generating fluent responses is nolonger a significant challenge. Furthermore, ourRL-TRC model demonstrates comparable perfor-mance to MADNet in terms of informativeness,although there remains a gap when compared tohuman performance. Notably, our RL-TRC modelsignificantly improves the relevance score, gener-ating responses that are more aligned with targetutterances. This further substantiates the efficacyof our model.",
  "Ablation Study": "We perform ablation experiments to investigate theimpact of each component within our RL-TRC.The results in demonstrate that the dis-card of any component leads to a decline in per-formance, underscoring the significance of eachpart. A comparison between coherence tasks (TCvs. RC) reveals that discarding the topic coher-ence task causes a more substantial performancedrop. This indicates the importance of understand-ing the topics of target utterances, as multi-party di-alogues often involve multiple parallel topic flows.Furthermore, an analysis of coherence rewards(TCR vs. RCR vs. RTR) shows that the topic-coherence reward has a more pronounced impacton performance. Importantly, removing the coher-ence rewards tends to result in greater performancedegradation compared to removing the coherencetasks, highlighting the effectiveness of reinforce-ment learning in enhancing coherence between gen-erated responses and target utterances. It is note-worthy that the improvement of rhetorical coher-ence is less than that of topic coherence, primarilydue to the low accuracy of the discourse parser.",
  "Analysis of Topic Coherence": "To evaluate whether our RL-TRC can generate re-sponses that are more relevant to the topic of thetarget utterance compared to the baselines, we con-duct a pairwise evaluation. Given that GPT-4 hasbeen widely used for pairwise evaluations (Zhenget al., 2023; Dubois et al., 2024), we employ GPT-4as a judge to determine which response is more rel-evant to the topic of the target utterance. Detailed",
  ": Pairwise evaluation results of GPT-4 in termsof topic coherence": "information is provided in Appendix F. presents the pairwise evaluation results.Compared to EMMDG, RL-TRC wins 75 and 71on the Hu and Ou5 datasets, respectively, whilelosing 55 and 52. Compared to MADNet, RL-TRCwins 65 and 69 on Hu and Ou5, respectively, whilelosing 55 and 57. These results further demonstratethat our approach generates responses more rele-vant to the target utterance topic, benefiting fromour topic coherence task and reward.",
  "Analysis of Rhetorical Coherence": "To verify whether our RL-TRC can generate a morerhetorically coherent response with the target ut-terance, we present the accuracy of the rhetoricalrelation between the generated response and the tar-get utterance. We first employ a trained discourseparser (Wang et al., 2021) to predict the rhetori-cal relation between the golden response and thetarget utterance, which is treated as the golden rela-tion. Then, we use the parser to predict the relationbetween the generated response and the target utter-ance and calculate the relation accuracy, as shownin .Notably, over 98% (4935 out of 5000) of therelations in the Hu dataset are classified as Com-ment, Question-Answer Pair, Continuation, or Clar-ification Question. Our RL-TRC model achievedaccuracies of 59.17% on the Hu dataset, reflectinga significant improvement compared to the per-formances of EMMDG and MADNet. This sug-gests that our approach generates responses that",
  "Analysis of Target Utterance Recognition": "We analyze the performance of correctly recog-nizing the target utterance in the dialogue historybased on the generated response. Intuitively, thestronger the coherence between the generated re-sponses and the target utterance, the higher theperformance. As shown in , the golden re-sponses achieve the highest performance, with anaccuracy of 95.68% and 84.09% for Hu and Ou5,respectively, demonstrating strong coherence be-tween the golden response and the target utterance.In addition, our RL-TRC achieves an accuracy of85.38% and 75.51% on Hu and Ou5, respectively,",
  "Importance of Accuracy in Topic andRhetorical Reward Models": "We analyzed the impact of varying performancesof the topic coherence and rhetorical coherence re-ward models on dialogue generation. The resultsfor the Hu dataset are presented in Tables 9 and 10,respectively. In these tables, epoch refers to thenumber of training iterations for the reward mod-els. The experimental setup remains consistentwith our state-of-the-art (SOTA) model, with theonly modification replacing the topic coherenceor rhetorical coherence reward model at differenttraining epochs. Our observations indicate thatthe improved performance of both the topic andrhetorical coherence reward models correlate withenhanced generation performance. This suggeststhat more effective reward models are better at eval-uating the coherence between generated responsesand target utterances, thereby promoting a morecoherent response to the target utterance.",
  "Case Study": "We conduct a case study to showcase the effec-tiveness of our RL-TRC model in . Thedialogue history is depicted in , with thetarget utterance being U2. We observe that thetopic coherence between the response generatedby EMMDG and U2 is not strong. U2 pertains toopera but the response generated by EMMDG is",
  ": Responses generated by our model and twoSOTA baselines. The dialogue history is shown in Fig-ure 1": "about Firefox, which is more topically coherentwith U1. Additionally, although the response gen-erated by MADNet is related to opera, it lacksa logical connection with the target utterance. Incontrast, our RL-TRC produces a response that isboth topic-related and logical, further demonstrat-ing the effectiveness of our approach. In addition,we explore the impact of different topic extractionmethods, as shown in Appendix H.",
  "Conclusion": "In this paper, we propose a reinforcement learningmethod based on discourse coherence for multi-party dialogue generation. By designing tasks cen-tered on topic coherence and rhetorical coherence,we enable the model to perceive coherence withthe target utterance. Furthermore, a reinforcementagent is employed to guide the model to gener-ate responses that are topically and rhetoricallyaligned with the target utterances. To optimize theagent, three types of discourse-aware rewards aredesigned to guide the model to maintain coherencewith the target utterance. Experimental results val-idate the effectiveness of our method. Our futurework will focus on how to optimize rhetorical co-herence.",
  "Limitations": "We discuss the limitations of RL-TRC as fol-lows: 1) Our limited computational resources pre-vented us from verifying the effectiveness of ourmethod on larger model sizes. Despite conduct-ing parameter-efficient fine-tuning on LLaMA, theresults were not satisfactory. As large languagemodels continue to gain prominence, we aim toperform full parameter fine-tuning in the future. 2)Rhetorical coherence does not contribute to multi-party dialogue generation as significantly as topiccoherence. The main reason may be the low per-formance of the discourse parser. Thus, optimizingrhetorical coherence to facilitate multi-party dia-logue generation is a challenge that needs to beaddressed. 3) The inherent complexity of reinforce-ment learning algorithms can lead to instability anda tendency to get stuck in local optima during train-ing. Consequently, careful tuning and adjustmentof hyperparameters are essential.",
  "Acknowledgements": "The authors would like to express their gratitudeto the three anonymous reviewers for their insight-ful comments on this paper. We also extend ourheartfelt thanks to Xiang Li, Huiyao Wang, andDidi Zhang for their contributions to the humanevaluation. This research was supported by the Na-tional Natural Science Foundation of China (Nos.62276177 and 62376181), and Project Funded bythe Priority Academic Program Development ofJiangsu Higher Education Institutions.",
  "Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Ben-gio. 2015. Neural Machine Translation By JointlyLearning To Align And Translate. In The Thrid Inter-national Conference on Learning Representations": "Ruijun Chen, Jin Wang, Liang-Chih Yu, and XuejieZhang. 2023.Learning to Memorize Entailmentand Discourse Relations for Persona-consistent Dia-logues. In Proceedings of the AAAI conference on ar-tificial intelligence, volume 37, pages 1265312661. Yi-Pei Chen, Noriki Nishida, Hideki Nakayama, andYuji Matsumoto. 2024. Recent Trends in Personal-ized Dialogue Generation: A Review of Datasets,Methodologies, and Evaluations. In Proceedings ofthe 2024 Joint International Conference on Compu-tational Linguistics, Language Resources and Evalu-ation, pages 1365013665. Alexander Chernyavskiy and Dmitry Ilvovsky. 2023.Transformer-based multi-party conversation genera-tion using dialogue discourse acts planning. In Pro-ceedings of the 24th Annual Meeting of the SpecialInterest Group on Discourse and Dialogue, pages519529. Alexander Chernyavskiy, Lidiia Ostyakova, and DmitryIlvovsky. 2024. GroundHog: Dialogue generationusing multi-grained linguistic input. In Proceedingsof the 5th Workshop on Computational Approachesto Discourse (CODI 2024), pages 149160.",
  "Kenneth Church and Patrick Hanks. 1990. Word associ-ation norms, mutual information, and lexicography.Computational linguistics, 16(1):2229": "Yann Dubois, Chen Xuechen Li, Rohan Taori, TianyiZhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin,Percy S Liang, and Tatsunori B Hashimoto. 2024.AlpacaFarm: A Simulation Framework for Methodsthat Learn from Human Feedback. Advances in Neu-ral Information Processing Systems. Yaxin Fan, Feng Jiang, Peifeng Li, and Haizhou Li.2024. Uncovering the Potential of ChatGPT for Dis-course Analysis in Dialogue: An Empirical Study.In Proceedings of the 2024 Joint International Con-ference on Computational Linguistics, Language Re-sources and Evaluation, pages 1699817010. Jia-Chen Gu, Zhenhua Ling, Quan Liu, Cong Liu, andGuoping Hu. 2023a. GIFT: Graph-Induced Fine-Tuning for Multi-Party Conversation Understanding.In Proceedings of the 61st Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 1164511658. Jia-Chen Gu, Chao-Hong Tan, Caiyuan Chu, Zhen-Hua Ling, Chongyang Tao, Quan Liu, and Cong Liu.2023b. MADNet: Maximizing Addressee DeductionExpectation for Multi-Party Conversation Generation.In Proceedings of the 2023 Conference on Empiri-cal Methods in Natural Language Processing, pages76817692. Jia-Chen Gu, Chao-Hong Tan, Chongyang Tao, Zhen-Hua Ling, Huang Hu, Xiubo Geng, and Daxin Jiang.2022a. HeterMPC: A Heterogeneous Graph Neu-ral Network for Response Generation in Multi-PartyConversations. In Proceedings of the 60th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 50865097. Jia-Chen Gu, Chongyang Tao, and Zhen-Hua Ling.2022b. Who Says What to Whom: A Survey ofMulti-Party Conversations. In Proceedings of theThirty-First International Joint Conference on Artifi-cial Intelligence, pages 54865493. Jia-Chen Gu, Chongyang Tao, Zhenhua Ling, Can Xu,Xiubo Geng, and Daxin Jiang. 2021. MPC-BERT: APre-Trained Language Model for Multi-Party Con-versation Understanding. In Proceedings of the 59thAnnual Meeting of the Association for Computational",
  "Vijay Konda and John Tsitsiklis. 1999. Actor-Critic Al-gorithms. In Proceedings of the Advances in NeuralInformation Processing Systems, pages 10081014": "Jing Yang Lee, Kong Aik Lee, and Woon Seng Gan.2022. Improving Contextual Coherence in Varia-tional Personalized and Empathetic Dialogue Agents.In ICASSP 2022-2022 IEEE International Confer-ence on Acoustics, Speech and Signal Processing,pages 70527056. Mike Lewis, Yinhan Liu, Naman Goyal, MarjanGhazvininejad,Abdelrahman Mohamed,OmerLevy, Veselin Stoyanov, and Luke Zettlemoyer.2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Transla-tion, and Comprehension. In Proceedings of the 58thAnnual Meeting of the Association for ComputationalLinguistics, pages 78717880. Jiaqi Li, Ming Liu, Min-Yen Kan, Zihao Zheng, ZekunWang, Wenqiang Lei, Ting Liu, and Bing Qin. 2020.Molweni: A Challenge Multiparty Dialogues-basedMachine Reading Comprehension Dataset with Dis-course Structure. In Proceedings of the 28th Inter-national Conference on Computational Linguistics,pages 26422652.",
  "Ilya Loshchilov and Frank Hutter. 2018. DecoupledWeight Decay Regularization. In International Con-ference on Learning Representations": "Hiroki Ouchi and Yuta Tsuboi. 2016. Addressee andResponse Selection for Multi-Party Conversation.In Proceedings of the 2016 Conference on Empir-ical Methods in Natural Language Processing, pages21332143. Haoyu Song, Yan Wang, Kaiyan Zhang, Weinan Zhang,and Ting Liu. 2021. BoB: BERT Over BERT forTraining Persona-based Dialogue Models from Lim-ited Personalized Data. In Proceedings of the 59thAnnual Meeting of the Association for ComputationalLinguistics and the 11th International Joint Confer-ence on Natural Language Processing, pages 167177.",
  "Chao-Hong Tan, Jia-Chen Gu, and Zhen-Hua Ling.2023. Is ChatGPT a Good Multi-Party ConversationSolver? In Findings of the Association for Computa-tional Linguistics: EMNLP 2023, pages 49054915": "Jianheng Tang, Tiancheng Zhao, Chenyan Xiong, Xiao-dan Liang, Eric Xing, and Zhiting Hu. 2019. Target-Guided Open-Domain Conversation. In Proceedingsof the 57th Annual Meeting of the Association forComputational Linguistics, pages 56245634. Ante Wang, Linfeng Song, Hui Jiang, Shaopeng Lai,Junfeng Yao, Min Zhang, and Jinsong Su. 2021. AStructure Self-Aware Model for Discourse Parsingon Multi-Party Dialogues. In Proceedings of the 30thInternational Joint Conference on Artificial Intelli-gence, pages 39433949. Zhiquan Ye, Yuxia Geng, Jiaoyan Chen, Jingmin Chen,Xiaoxiao Xu, SuHang Zheng, Feng Wang, Jun Zhang,and Huajun Chen. 2020. Zero-shot Text Classifica-tion via Reinforced Self-training. In Proceedingsof the 58th Annual Meeting of the Association forComputational Linguistics, pages 30143024. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,Joseph E Gonzalez, and Ion Stoica. 2023. JudgingLLM-as-a-Judge with MT-Bench and Chatbot Arena.In Advances in Neural Information Processing Sys-tems, pages 4659546623. Junkai Zhou, Liang Pang, Huawei Shen, and XueqiCheng. 2023.SimOAP: Improve Coherence andConsistency in Persona-based Dialogue Generationvia Over-sampling and Post-evaluation. In Proceed-ings of the 61st Annual Meeting of the Associationfor Computational Linguistics, pages 99459959.",
  "We feed the dialogue to ChatGPT3 and ask Chat-GPT to extract no more than five keywords for eachutterance, the prompt is as follows:": "The following is a conversation with mul-tiple participants. Please extract the keywords from each utterance. The number ofkey words should not exceed five, and eachkey word should consist of only one word.Please return a dictionary, where the key isthe index of the utterance and the value isa list of key words. Do not return anythingelse.U1:U2: Un",
  "CReward Model": "Topic-coherence modelThe topic-coherencemodel is a binary classifier that determines whetheran utterance pairs is coherent or not.We re-constructed the Hu dataset (Hu et al., 2019) toconstruct the coherent pair (ut, y) and incoher-ent pair (ut, yneg), in which ut and y are tar-get utterance and golden response, respectively,yneg is an utterance randomly selected from thecurrent dialogue not reply to target utterance,the statistics are shown in .We feedthe utterance pair and their keywords into theBART-base (Lewis et al., 2020) in the form of\"[CLS]ututkws[SEP]yykws[SEP]\". To train theclassifier, we adopted the Trainer function of trans-former library 4. The epoch, batch size, learningrate, and weight decay are set to 3, 192, 2e-5, and0.02, respectively, and other hyperparameters areset default.",
  "EMMDG0.730.630.481.84MADNet0.730.780.502.01RL-TRC0.750.810.572.13": ": Human evaluation results of our RL-TRC andtwo SOTA baselines on a randomly sampled test set ofOu5. Rel, Flu, and Inf are short for Relevance, Fluency,and Informativeness, respectively. The agreement rateof the human evaluation outperforms 75% on all threemetrics. Rhetorical-coherence modelThe rhetorical-coherence model is a multi-class classifier that rec-ognizes the rhetorical relation between an utterancepair. We extract the utterance pairs with a rhetoricalrelation from the Molweni (Li et al., 2020) datasetand the data statistics are shown in . Allhyperparameters are set to the same value as thetopic-coherence model. Reply-to modelGiven a multi-party dialoguehistory C = {(p1, u1), ..., (pi, ui), ..., (pn, un)},where pi and ui are participant and utterance, re-spectively, the reply-to model aims to recognizea target utterance ut for the generated response y,where 1 < t n. Following the previous work(Gu et al., 2023a), we trained the reply-to modelon the Hu (Hu et al., 2019) dataset, and the datastatistics are shown in . The pre-trainedmodel we adopted is RoBERTa-base. The epoch,batch size, learning rate and weight decay are setto 3, 32, 2e-5, and 0.02, respectively.",
  "FPairwise Evaluation with GPT-4": "To evaluate which of the responses generated bythe two models is more relevant to the target utter-ance in terms of topic, we follow previous work(Zheng et al., 2023; Dubois et al., 2024) to conductpairwise evaluation using GPT-4 5. The 200 sam-ples evaluated are the same as those used in humanevaluation of .5. GPT-4 is instructed tocompare the outputs of two models and determinewhich one exhibited a stronger relevance with thetopic of the target utterance in the dialogue his-tory. The model names remained anonymous, andthe positions of the model outputs were randomlyswapped. The prompt is as follows:"
}