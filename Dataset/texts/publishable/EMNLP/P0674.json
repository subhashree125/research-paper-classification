{
  "Abstract": "This study introduces a hypothesis-testingframework to assess whether large languagemodels (LLMs) possess genuine reasoningabilities or primarily depend on token bias.We go beyond evaluating LLMs on accuracy;rather, we aim to investigate their token biasin solving logical reasoning tasks.Specifi-cally, we develop carefully controlled syntheticdatasets, featuring conjunction fallacy and syl-logistic problems. Our framework outlines alist of hypotheses where token biases are read-ily identifiable, with all null hypotheses assum-ing genuine reasoning capabilities of LLMs.The findings in this study suggest, with sta-tistical guarantee, that most LLMs still strug-gle with logical reasoning. While they mayperform well on classic problems, their suc-cess largely depends on recognizing superfi-cial patterns with strong token bias, therebyraising concerns about their actual reasoningand generalization abilities. Codes and dataare open-sourced at",
  "Introduction": "Large language models (LLMs) have achieved re-markable progress in understanding and generatinghuman-like text, triggering growing interest in theLLMs theory of minds (Kosinski, 2023; Jamaliet al., 2023; Bubeck et al., 2023) and decision-making abilities (Lyu et al., 2023; Prasad et al.,2023; Jiang et al., 2024a,b; Xie et al., 2024). How-ever, there is ongoing debate about whether LLMspossess genuine reasoning capabilities, as evidencesuggests that the performance of LLMs on reason-ing tasks is correlated with how much the inputs se-mantic content supports a correct logical inference(Dasgupta et al., 2022; Li et al., 2023). Should validreasoning be applied, such a correlation would notexist, since a genuine reasoner should be able to de-rive the correct inference regardless of the context. : We illustrate token bias using the classic\"twenty-five horses\" problem in graph theory. The toptwo sub-figures, generated by GPT-4o for illustrationpurposes only1, demonstrate the concept by alteringthe name \"horses\" to \"bunnies\", irrelevant to the prob-lems underlying logic. The bottom two sub-figuresshow experimental results in GPT-4 and Claude, whereperformance significantly drops due to perturbations inanimal names and numbers. In these plots, \"Original\"refers to the unaltered \"twenty-five horses\" problem,\"random_animals\" alters only the animal names, and\"random\" alters both names and numbers. We observen12 > n21 with statistical significance, meaning thatthere are more instances where the original problem issolved correctly while the perturbed problem is solvedincorrectly, compared to the reverse. As a result, ourhypothesis testing confirms token bias in this scenario. In this paper, we formalize this observation andsay that an LLM is subject to token bias in a reason-ing task if systematic changes to some or all tokensin the task descriptions - while keeping the under-lying logic intact - allow us to predict the directionof the shift in the models output. A strong tokenbias suggests that the model is relying on su-perficial patterns in the input rather than trulyunderstanding the underlying reasoning task.This could lead to brittle performance that fails togeneralize well to novel examples and phrasingsencountered in the wild, which could differ fromthe spurious patterns the model may have overfittedto during training. : An illustration of the overall framework. Wegenerate synthetic data, perform systematic token pertur-bations, and evaluate an LLM for comparative studies.The resulting contingency table, where A-D are integervalues of counts, allows for subsequent statistical tests. We explore several well-known logical fal-lacy problems from the cognitive science litera-ture (Tversky and Kahneman, 1983; Kahneman,2011), which provide a clear playground for assess-ing the reasoning capabilities of LLMs. and 3 depict two kinds of token biases found in ourtesting framework, where the model may be overfit-ting to specific tokens commonly found in classicproblem statements. Since we observe many caseswhere state-of-the-art LLMs like GPT-4 (Achiamet al., 2023) successfully identify logical fallaciesunder certain settings, we highlight the urgent needfor a framework to tease out whether LLMs applygenuine reasoning or merely exploit token bias fortheir improved performance.This work reconceptualizes the evaluation ofreasoning capabilities into a general and rigor-ous statistical testing framework. As shown in, it comprises three critical components:synthetic data generation, token perturbation, andstatistical hypothesis testing. This general frame-work is designed to bypass the complications ofevaluation set contamination (Zhou et al., 2023;Ravaut et al., 2024), leverage insights and toolsfrom controlled experiments, and draw statisticallyvalid conclusions.Our study is unique from existing work (Gou 1Interestingly, when we prompted GPT-4o to generate animage of \"lop-eared bunnies\", the model exhibited a visualtoken bias by depicting bunnies with four ears both lopand erect implying it associated the term \"bunnies\" withthe presence of erect ears in images, without a genuine andlogical understanding of a bunnys physical reality. We movethe exploration of visual token bias to the future work. : What is token bias? Here is another exampleexhibited by GPT-4. On the left, GPT-4 correctly iden-tifies the conjunction fallacy and answers the questioncorrectly, given the classical Linda Problem as the one-shot exemplar. On the right, however, the exemplar isrephrased by altering \"Linda\" to \"Bob\" while keepingthe same logic, which surprisingly confuses the model. et al., 2023; Suri et al., 2024; Mukherjee and Chang,2024; Wang et al., 2024) in two folds. First, weare not evaluating the overall accuracy of LLMsin identifying different logical fallacies. Instead,our focus is on token bias.Although there arealways more types of logical fallacies, we takethe conjunction fallacy, syllogistic fallacy, and the\"twenty-five horses\" problem in graph theory asquintessential examples, which exhibit strong to-ken biases that are more readily identifiable in theirproblem statements. By identifying and perturb-ing these specific tokens, we can induce pre-dictable shifts in LLM responses. Second, werecognize that cognitive biases often emerge in im-plicit forms in real-life scenarios, so relying on en-gineering fine-grained prompts (Gou et al., 2023;Yao et al., 2024; Besta et al., 2024) to make LLMsidentify specific logical fallacies is impractical forgeneral-purpose user applications. As a result, weonly leverage common prompting techniques thatare sufficient to provide robust statistical evidence.Comprehensive experiments on both commercialand open-sourced LLMs on large-scale syntheticdatasets uncover a critical insight: it is the token",
  "The General Framework": "Our framework is summarized in . Thisgeneral framework is grounded on the premise thatfor a given reasoning task, a capable reasoningagent will consistently reach the same conclusionregardless of how the task is framed, as long asthe underlying logic remains the same (Hastie andDawes, 2009). This assumption lays the founda-tion of our null hypothesis, H0. In our setup, if anagent consistently applies reasoning in its decision-making process, the only source of failure shouldbe the procedural mistakes during the agents ab-stract reasoning steps, which we assume to comeup in an i.i.d. fashion. Our general frameworkcontains three major parts as follows. Synthetic Data GenerationOnce the underlyinglogic of a reasoning task is defined, we create analgorithm to generate a synthetic dataset with nsamples. While it is helpful to leverage LLMsfor linguistic coherence in the process, the datageneration should be carefully controlled, utilizinginformation from real-world data or establisheddatasets to mitigate potential biases from purelyAI-generated texts. The process begins with thecreation of a curated list of entities, encompassingdiverse names, genders, ages, occupations, culturalbackgrounds, and events where applicable, alongwith a textual template that dictates the structureof the task description. By sampling from thislist, we generate task descriptions that maintain theintegrity and novelty of the dataset. This methodensures that while the LLM of interest might befamiliar with the individual entities, it has neverseen the specific combinations of these entities andnarratives, thus bypassing data contamination.The following example illustrates one approachwe leverage to generate synthetic conjunction fal-lacy questions. We randomly sample a common-sense story curated by Mostafazadeh et al. (2016)and convert it into the following prompt: Your taskis to complete the last sentence of the followingproblem to create a conjunction fallacy quiz:",
  "(b) Michelle would likely buy food at thegrocery store because": "We expect the LLM to complete the story by provid-ing us with a plausible reason after \"because\", suchas \"she found nothing to eat at home\". Irrespec-tive of the LLMs completion, option (b) containsa conjunction of two events so it should always beviewed less likely.The synthetic dataset can be dynamically gen-erated on the fly, precluding its prior existence inany training datasets. It also allows the algorithmdesigners to control the dataset size, efficiently scal-ing their data based on the sample size required toacheive statistical validity. Token PerturbationWe posit that if the LLMprimarily relies on token bias, its performance onreasoning tasks will consistently improve (or de-grade) as we alter some tokens in a systematic man-ner. This process of token perturbation generatesn matched pairs of samples, enabling us to eval-uate the LLM on both the original and perturbeddatasets and create a 22 contingency table below,where n = n11 + n12 + n21 + n22.",
  ": A template for the contingency table. Wefollow the notations in this table to define 12 and 21in the next paragraph for hypothesis testing": "Statistical Hypothesis Testing for Matched PairsIn our context, we wish to decide whether or notsome hypothesis concerning whether an agent rea-sons consistently is correct. The choice here liesbetween two decisions: accepting or rejecting thehypothesis. The decision procedure is called hy-pothesis testing (Lehmann et al., 1986). Through-out our discussion, we use H0 to denote the nullhypothesis and Ha the alternative hypothesis.For each of the n matched pairs, let ab denotethe underlying probability of outcome a for theoriginal sample and outcome b for the perturbedsample. In other words, for any nonnegative integerm n,",
  "As nab counts the number of such pairs, nab/n isthe sample proportion, which is a consistent es-timate of ab. The null hypothesis assumes the": "marginal homogeneity for binary matched pairs,i.e. 12 = 21. For small samples, we can ap-ply an exact test conditioned on n = n21 + n12(Mosteller, 1952; Agresti, 2012). Under H0, n21follows a binomial(n, 1/2) distribution, and thecorresponding p-value is the binomial tail proba-bility. As a rule of thumb, when n > 10, thereference binomial distribution is approximatelynormal, and we can compute the standardized nor-mal test statistics z0 = (n21 n12)/n21 + n12,which is identical to the McNemar statistic (McNe-mar, 1947). To test the same hypotheses for a groupof models, we apply the Benjamini-Hochberg Pro-cedure (Benjamini and Hochberg, 1995) to controlthe false discovery rate at a predetermined signifi-cance level .",
  "A Peek into Token Bias": "This section outlines the detailed hypotheses inour statistically inspired framework. We aim todetermine whether LLMs are capable of genuinereasoning or whether they rely heavily on tokenbiases. According to the principle of invariance inrational decision-making (Tversky and Kahneman,1981, 1988), the preferences of a rational reasoningagent should remain unaffected by the framing ofequivalent decision problems.In a broader interpretation of invariance, we as-sess whether alterations in seemingly irrelevanttokens, such as name entities in problem narrativesthat are unrelated to the underlying logic, influencethe outcomes of reasoning. A true reasoner shouldeffectively navigate through reasoning tasks with-out being influenced by trivial changes in contentthat do not impact the fundamental logical struc-ture. We propose a series of hypotheses, wherethe null hypothesis assumes the presence of a gen-uine reasoner. For each hypothesis, we identifyspecific tokens that may carry strong biases un-der their problem settings, and systematically alterthese tokens to test their impact, while maintainingthe integrity of the underlying logical structure.",
  "Preliminaries": "In this work, we integrate the conjunction fallacyand syllogistic fallacy discussed in the cognitivescience literature (Tversky and Kahneman, 1983;Kahneman, 2011) to construct synthetic datasetson which we perform our token perturbation. Thissection briefly introduces the underlying logic. Conjunction FallacyThe most often-cited ex-ample of conjunction fallacy is called the Lindaproblem which is framed as follows (Tversky andKahneman, 1983): Linda is 31 years old, single,outspoken, and very bright. She majored in philos-ophy. As a student, she was deeply concerned withissues of discrimination and social justice, and alsoparticipated in antinuclear demonstrations. Whichis more probable?",
  "(a) Linda is a bank teller.(b) Linda is a bank teller and is active inthe feminist movement": "Tversky and Kahneman (1983) found that humanstend to prefer option (b). However, it is logicallynecessary that the probability of a conjunction oftwo events (e.g., Linda is a bank teller, and she isactive in the feminist movement) is less than theprobability of either event alone. Syllogistic FallacyThe syllogistic fallacy docu-ments the logical failure that occurs when peopleare presented with syllogisms two premises fol-lowed by a conclusion. Ideally, if the premises aretrue and the logical structure is valid, the conclu-sion must necessarily be true. However, when thearguments structure is flawed, the conclusion maybe invalid despite the surface-level logical form.Consider the following syllogism from Kahneman(2011): Is this logically sound? All roses are flowers.Some flowers fade quickly.Therefore some roses fade quickly.The argument is incorrect because the twopremises do not imply that the set of roses and theset of flowers that fade quickly necessarily overlap.",
  "Lost in Irrelevant Context": "Logical fallacies often contain misleading contexts,exploiting common cognitive biases and shortcutsin human reasoning. These fallacies can seem con-vincing at first glance, being effective in swayingopinions, because they resonate with intuitive yetflawed biases. For instance, conjunction fallaciespresent two options: one involving a single eventand the other with an additional event in conjunc-tion. This added event is particularly designed toalign with the contextual background in the prob-lem statement, leading humans or LLMs to reaffirmtheir preexisting beliefs. In contrast, when the ad-ditional event in the options is changed to an irrel-evant one, the model is less likely to be distractedby these extraneous and irrelevant details.",
  "Hypothesis 1 Genuine reasoning LLMs shouldwithstand contextually misleading options in theproblem statements": "Token perturbation: Assume problem P is a con-junction fallacy problem with options (a) and(b). One option contains a event x and the othercontains x and y in conjunction. y is relevant tothe context of the problem statement that mightmislead the LLM. In contrast, the perturbed prob-lem P replaces y with a randomly generated y",
  "(a) Kai is a law enforcement worker.(b) Kai is a law enforcement worker andparticipates in cultural preservation or-ganizations learns to play the ukulele": "To further explain H0 and Ha, the null hypoth-esis H0 always assumes that the LLM is a genuinereasoner and can consistently perform reasoningregardless of the superficial token changes, leadingus to expect 12 = 21. Meanwhile, token bias cansystematically and predictably influence the LLMsperformance. Accordingly, 12 < 21 aligns withour expectation that misleading tokens degrade per-formance, but an observation of 12 > 21 - wherenon-misleading options result in a greater decreasein performance - would be considered invalid.",
  "Token Bias on Widely Cited Examples inClassic Literature": "It is reasonable to suspect that most LLMs havebeen trained to recognize well-known logical fal-lacy problems. However, the question remainswhether they acquire genuine reasoning skills ormerely learn to falsely associate frequently appear-ing names - such as \"Linda\" in the classical Lindaproblem - with the correct reasoning outcomes theyshould have. We demonstrate an example in Fig-ure 3 that perturbs Linda Bob.",
  "Token Bias on Well-Known Entity Names": "Celebrity names inherently carry a rich contextualbackground that LLMs learn from massive trainingdata. We hypothesize that by replacing a celebrityname with a generic one in a conjunction fallacyproblem, thereby dissociating the link to this con-textual backdrop, we might see performance im-provements in LLMs, and such results would un-derscore the potential deficiency in their genuinereasoning capabilities.",
  "(a) Her first show is a flop.(b) Her first show is a flop but she willeventually sell over a million tickets forthe entire tour": "Here is another example of the token perturba-tion applied to the classic \"twenty-five horses\" prob-lem in mathematical reasoning, as referenced in. Note that perturbing the numbers is op-tional, but the total number of animals must alwaysbe a square number: There are twenty-five thirty-six horses bunnies among which you need to find out the fastest three. You can conduct a race amongat most five six to find out their relative speed.At no point, you can find out the actual speed of thehorse bunnies in a race. Find out the minimumnumber of races which are required to get the topfive six horses bunnies.",
  "Token Bias in Reasoning about Sets": "The syllogistic fallacy involves reasoning aboutsets, utilizing specific quantifiers such as \"all\" and\"some\" to specify the distribution of variables. Ourinvestigation centers on whether LLMs overfit tothese tokens of quantifiers, relying heavily on theirpresence to generate answers that appear correct.By rephrasing these tokens with other words thatconvey the same meaning, we can test the robust-ness of LLMs reasoning abilities.",
  "H0: 12 = 21.Ha: 12 > 21. (12 < 21 is invalid.)": "Here is an example of such token perturbations:Is it logically sound? All roses Roses are flowers.Some A subset of flowers fade quickly. Therefore,some A subset of roses fade quickly.Continuing with the exploration of token biasin syllogistic fallacies, we propose an intriguingrephrasing of the syllogisms narrative by incor-porating the names of reputable news agenciesand universities. While adding the tokens of theirnames does not alter the logic, it could influencehow LLMs perceive and process the information.LLMs prone to token bias might erroneously in-crease their confidence in the trustworthiness andcredibility of the stories, based purely on the asso-ciation with these respected institutions. Hypothesis 5 Genuine reasoning LLM shouldwithstand alterations to the narrative.Token perturbation: Assume P is the originalproblem. The perturbed problem P adds ormodifies specific tokens in the problem state-ment to reframe its narratives without changingthe logic structure.",
  "Ha: 12 < 21 or 12 > 21": "To remove potential token bias from the pattern\"All..., Some..., Some...\", we regard perturbed prob-lems P in Hypothesis 4 as the original problemsP here, as shown in the example below: Is it log-ically sound? Roses In a recent publication byBloomberg, it was noted that roses are flowers. Asubset of Research from MIT supports the find-ing that a subset of flowers fade quickly. Therefore,a subset of roses fade quickly.To ensure a more comprehensive comparison,we also alter tokens to satirical sources like TheOnion and less reputable institutions, noting thatthese names never impact the logical structure ofthe problems. Here is an example: Is it logicallysound? Roses In a recent publication by theDaily Rumor, it was noted that roses are flowers. Asubset of An anonymous blog post writes the find-ing that a subset of flowers fade quickly. Therefore,a subset of roses fade quickly.",
  "Leaking Hint Tokens": "Just as a proficient student doesnt need hints to ex-cel in a math exam, a reasoning agent should solvelogical problems effectively without explicit cues.Besides, even if a student answers all problemscorrectly but the examlet provides all the reasoningsteps, we may still question whether the studentreally understands the reasoning. Our experimentsdeliberately leak important hints that we expect agenuine reasoner to figure out itself in its interme-diate reasoning steps.",
  "Experiment": "Our experiments aim to rigorously test the rea-soning capabilities of LLMs through the hypothe-ses in on token bias.More compre-hensive results are included in Appendix D. Inall experiments, we run n trials for each \"model-prompting method\" pair, depending on how manysynthetic data samples are related to each hypoth-esis, and then perform a McNemar test. We applythe Benjamini-Hochberg Procedure and reject thenull hypothesis if the p-value is less than = 0.05.",
  "Synthetic Dataset Generation": "We leverage data sources such as occupationalstatistics (USDL, 2024), commonsense stories(Mostafazadeh et al., 2016), CNN news stories (Seeet al., 2017), common disease symptom pairs (kag),celebrity names (Rosenberg, 2021; Wikipedia con-tributors, 2024a), objects vocabularies (esl) andcommon U.S. news media (Wikipedia contributors,2024b; Pew Research Center, 2011) to curate listsof entities to generate synthetic data. We outlineour well-controlled data generation process and thesamples used for each hypothesis in Appendix C.",
  "Prompting Methods": "We implemented commonly used prompting strate-gies that are sufficient for evaluating the null hy-potheses within our framework.The specificprompting techniques we utilized are as follows,with their corresponding notations presented in: Baseline: Directly answering the ques-tion without additional instructions.Zero-shotchain-of-thought (zs_cot): Includes the instruction\"Let us think step by step\" (Wei et al., 2022b). One-shot (os): Involves a single in-context learn-ing example (Brown et al., 2020).Few-shots(fs): Utilizes three in-context examples.Simi-larly, we have os_cot and fs_cot. We also includeweak_control_zs/os_cot and control_zs/os_cot forHypothesis 6 representing prompts with additionalweak or strong hints, as detailed in Appendix B.",
  "Hypothesis Testing Results": "Testing of Hypothesis 1: LLMs Would Fail atMisleading OptionsWe evaluate LLMs on allconjunction fallacy problems with misleading op-tions (n=400). a and show a sig-nificant decline in success rate when contextuallymisleading options in conjunction fallacy problemsare replaced with random alternatives. The ran-dom ones are no longer relevant to the problemstatements, so all LLMs become less likely to beswayed by background information that is not log-ically important. We, therefore, reject almost allnull hypotheses. Testing of Hypothesis 2: LLMs Would Fail Dueto Surface Level Change in the ExemplarWeevaluate LLMs under in-context learning scenar-ios for solving conjunction fallacies (n=500). Fig-ure 4b and show consistent performancedrop on all LLMs when the name \"Linda,\" fre-quently used in classic reasoning tasks, is substi-tuted with \"Bob\" in one-shot exemplars. Such achange should not influence outcomes for genuinereasoners, as the specific name used is irrelevant tothe logical process. Testing of Hypothesis 3: LLMs Would be Misledby Celebrity NamesWe evaluate LLMs on vari-ants of conjunction fallacies that contain a celebrityname (n=100). We observe in c and Ta-ble 4 to a that celebrity names appearedin problem statements frequently mislead LLMsinto the celebritys background, which is not help-ful in solving logical fallacy problems but reducesaccuracy, leading us to reject all null hypotheses. Testing of Hypothesis 4: LLMs Would Fail atSynonyms of Classic QuantifiersWe assessLLMs on syllogisms (n=200). d and Ta-ble 5 reveal that, in most instances, we should re-ject the null hypotheses, except for llama-3-70b-instruct. Most LLMs demonstrate insufficient ro-bustness when patterns \"All..., Some..., Some...\"commonly used in classic syllogistic fallacy prob-lems are substituted with synonyms. (a) Experimental results for Hypothesis 1 (n = 400). The perturbed problems alternate options contextually relevant to theproblem statements to irrelevant ones. We run all different prompt methods. To reject the null, we expect n12 < n21. Weconclude that LLMs fail to reason against contextually misleading options in conjunction fallacy problems. (b) Experimental results for Hypothesis 2 (n = 500). The perturbed problems alternate the name classic \"Linda\" to \"Bob\"in in-context learning exemplars. We run one-shot with and without chain-of-thought prompts. To reject the null, we expectn12 > n21. We conclude that LLMs possess strong token bias to the name \"Linda\" frequently appearing in classic literature. (c) Experimental results for Hypothesis with celebrity names 3 (n = 100). The perturbed problems alternate the celebrity nameto a generic one in problem statements. We run all different prompt methods. To reject the null, we expect n12 < n21. Weconclude that LLMs are frequently misled by celebrity names in problem statements that are irrelevant to logical essence. (d) Experimental results for Hypothesis 4 (n = 200). The perturbed problems alternate tokens \"All\" and \"Some\" to differentbut equivalent expressions in syllogisms. We run all different prompt methods. To reject the null, we expect n12 > n21. Weconclude that most LLMs rely on patterns \"All..., Some..., Some...\" for reasoning about syllogism. (e) Experimental results for Hypothesis 5 (n = 200). The perturbed problems add the names of trustworthy news agencies anduniversities to alter the narratives of syllogisms. We run all different prompt methods. To reject the null, we expect n12 > n21.We conclude that LLMs might be misled by reputable names irrelevant to the logical structure. (f) Experimental results for Hypothesis 6 (n = 800). The perturbed problems leak hint tokens, either weak or strong hints inproblem statements. We run zero-shot and one-shot prompt methods. To reject the null, we expect n12 < n21. We concludethat LLMs still heavily rely on hint tokens for solving logical fallacy problems well. : Our controlled experiments cast doubt on the genuine reasoning capabilities of LLMs. In this figure, eachpair of histograms stuck together represents a comparison in the contingency table 1 for McNemars Tests. Testing of Hypothesis 5: LLMs Would Be Mis-led by Names Linked to Reputable EntitiesWeevaluated the impact of names linked to reputabledata sources in syllogisms (n=200). eand demonstrate that some LLMs are in-deed misled by the inclusion of these authoritativenames, especially GPT-4 and LLaMA-3-70B. Gen-erally, LLMs tend to falsely believe that these nar-ratives are more trustworthy and, thereby, ignorethe logical fallacy in them. As a result, we rejectabout half of the null hypotheses. Results fromusing the names of less credible sources are shownin Appendix 7 for comparison. Testing of Hypothesis 6: LLMs Still HeavilyRely on Hint TokensWe evaluated the perfor-mance of LLMs with and without the presence ofhints (n=200). f and indicate thatLLMs still heavily rely on hints to achieve idealperformance, so we reject all the null hypotheses.",
  "Related Work": "The Reasoning Capabilities of LLMsThere isan increasing number of works aim to improvingthe reasoning performance of LLMs (Ouyang et al.,2022; Zhou et al., 2022; Lyu et al., 2023; Hao et al.,2023; Xu et al., 2024; Putta et al., 2024; Kumaret al., 2024; Yao et al., 2024; Cai et al., 2024), eval-uating and critiquing their reasoning abilities (Zhouet al., 2020; Hong et al., 2023; Huang et al., 2023;Shi et al., 2023; Sprague et al., 2024; Turpin et al.,2024; Xiao et al., 2024), trying to understand theirreasoning processes (Wei et al., 2022a; Merrilland Sabharwal, 2023; Ma et al., 2023; Lanhamet al., 2023; Merrill and Sabharwal, 2024; Yanget al., 2024; Chen et al., 2024; Jin et al., 2024), aswell as related surveys (Qiao et al., 2022; Huangand Chang, 2022; Ahn et al., 2024; Giadikiaroglouet al., 2024; Liang et al., 2024; Zheng et al., 2024)that comprehensively explore the reasoning capa-bilities of LLMs. Our work aligns with effortsthat critique the generalization of LLMs reason-ing capabilities, moving beyond accuracy-basedbenchmarks (Talmor et al., 2018; Srivastava et al.,2022a,b; Fu et al., 2023; Yang et al., 2018), whichprimarily focus on overall question-answeringaccuracy, often without considering questionaugmentations or variations. Instead, we stepone-level deeper into these problem statements,investigating potential token biases that maycreate the illusion of improved logical reason-ing performance, while remaining vulnerable to irrelevant token perturbations. By examiningthese subtleties, our approach provides a more nu-anced understanding of the limitations in LLMsreasoning capabilities. Besides, we reformulate theevaluation as hypothesis testing, providing resultswith statistical guarantees. Cognitive Biases and Logical Fallacies in LLMsRecent studies (Gardner et al., 2020; Hagendorffet al., 2023; Lin and Ng, 2023; Talboy and Fuller,2023; Binz and Schulz, 2023; Ullman, 2023;Mitchell and Krakauer, 2023) analyze the biasesin LLMs with synthetic datasets. For example,Tamkin et al. (2023) uses an LLM to generateprompts that reveals patterns of discriminationsin LLMs. Echterhoff et al. (2024) proposes a set ofLLM-simulated experiments in a specific context.Although existing works (Mukherjee and Chang,2024; Macmillan-Scott and Musolesi, 2024; Wanget al., 2024; Suri et al., 2024; Jin et al., 2022) studymore kinds of fallacy types in human psychology,they approach problems at a coarse level and onlyemphasize accuracy. Our study goes into a morefine-grained level with a series of hypotheses. Weprovide statistical guarantees and quantitative anal-yses of token bias that can be carefully and system-atically tuned. Besides, Gou et al. (2023) presentsthe Rationality of Thought (RoT), decomposing re-sponses into six predefined steps with hand-craftedprompt engineerings. Our work focuses on generalprompting strategies that are sufficient to validateor reject our hypotheses.",
  "Discussion": "This work reconceptualizes the evaluation of thereasoning behavior of LLMs through the lens oftoken bias. The statistical evidence presented inour hypothesis-testing framework contributes to thelarger discussion that LLMs do not always applyreasoning consistently in their decision-making pro-cesses. Instead, they primarily rely on token biasfor response generation. This suggests that chain-of-thought prompting (Wei et al., 2022b; Wanget al., 2022) or in-context learning (Brown et al.,2020; Min et al., 2022; Lyu et al., 2022; Wang et al.,2022) may not elicit actual reasoning but insteadresult in semantic shortcuts for LLMs to imitate thedesired behavior at superficial levels. These find-ings raise concerns about the extent to which LLMstruly engage in reasoning. Further investigationsare needed to uncover the underlying mechanismsand limitations of LLMs reasoning capabilities.",
  "Limitations": "This hypothesis-testing framework is specificallydesigned for multiple-choice or yes/no questionsand is not applicable to open-ended responses. Itrelies on LLMs with strong instruction-followingcapabilities to consistently produce responses thatinclude the selected options, though we find thatLLMs can generally follow these instructions inmost cases. In addition, smaller LLMs, such asllama-3-8b-instruct, with lower instruction follow-ing capabilities may contain more confounders be-sides the token bias, which could weaken our hy-pothesis testing results. As a result, we mainlyfocus on state-of-the-art LLMs. Moreover, thefinding of token biases requires manual efforts.We also acknowledge that there are likely otherhypotheses and assumptions that a genuine rea-soner should satisfy. Our current study focuseson the conjunction fallacy, syllogistic fallacy, the\"twenty-five horses\" problem in graph theory, andtheir variants to demonstrate our framework. Theseare quintessential examples and the frameworkcould include a broader range of hypotheses, fal-lacy types, data modalities, and reasoning tasks.",
  "Acknowledgement": "This work was supported by NSF grant CCF-2112665 (TILOS), which provides funding for B.J.and C.J. It was also funded in part by the Labora-tory Directed Research and Development (LDRD)program at Argonne National Laboratory, with sup-port from the Office of Science, U.S. Department ofEnergy, under Contract No. DEAC02-06CH11357.Y. X. and W. J. Su acknowledge support from theNSF HDR TRIPODS award (CCF-1934876). Theauthors thank John K. Hutchison for his valuablesuggestions regarding the motivation of this study.",
  "Anthropic. 2024. Models overview. Software availablefrom Anthropic. Accessed: 2024-05-20": "Yoav Benjamini and Yosef Hochberg. 1995. Control-ling the false discovery rate: A practical and pow-erful approach to multiple testing. Journal of theRoyal Statistical Society. Series B (Methodological),57(1):289300. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gersten-berger, Michal Podstawski, Lukas Gianinazzi, JoannaGajda, Tomasz Lehmann, Hubert Niewiadomski, Pi-otr Nyczyk, et al. 2024. Graph of thoughts: Solvingelaborate problems with large language models. InProceedings of the AAAI Conference on ArtificialIntelligence, volume 38, pages 1768217690.",
  "Marcel Binz and Eric Schulz. 2023. Using cognitivepsychology to understand gpt-3. Proceedings of theNational Academy of Sciences, 120(6):e2218523120": "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Sbastien Bubeck, Varun Chandrasekaran, Ronen El-dan, Johannes Gehrke, Eric Horvitz, Ece Kamar,Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-berg, et al. 2023. Sparks of artificial general intelli-gence: Early experiments with gpt-4. arXiv preprintarXiv:2303.12712.",
  "Michal Kosinski. 2023. Evaluating large language mod-els in theory of mind tasks. arXiv e-prints, pagesarXiv2302": "Aviral Kumar, Vincent Zhuang, Rishabh Agarwal,Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli,Shariq Iqbal, Colton Bishop, Rebecca Roelofs,et al. 2024.Training language models to self-correct via reinforcement learning. arXiv preprintarXiv:2409.12917. Tamera Lanham, Anna Chen, Ansh Radhakrishnan,Benoit Steiner, Carson Denison, Danny Hernan-dez, Dustin Li, Esin Durmus, Evan Hubinger, Jack-son Kernion, et al. 2023.Measuring faithful-ness in chain-of-thought reasoning. arXiv preprintarXiv:2307.13702.",
  "Erich Leo Lehmann, Joseph P Romano, and GeorgeCasella. 1986. Testing statistical hypotheses, vol-ume 3. Springer": "Bangzheng Li, Ben Zhou, Fei Wang, Xingyu Fu, DanRoth, and Muhao Chen. 2023. Deceiving seman-tic shortcuts on reasoning chains: How far canmodels go without hallucination?arXiv preprintarXiv:2311.09702. Xun Liang, Shichao Song, Zifan Zheng, Hanyu Wang,Qingchen Yu, Xunkai Li, Rong-Hua Li, Feiyu Xiong,and Zhiyu Li. 2024. Internal consistency and self-feedback in large language models: A survey. arXivpreprint arXiv:2407.14507.",
  "Pew Research Center. 2011. Top 25. Accessed: 2024-06-15": "Archiki Prasad, Alexander Koller, Mareike Hartmann,Peter Clark, Ashish Sabharwal, Mohit Bansal, andTushar Khot. 2023. Adapt: As-needed decompo-sition and planning with language models. arXivpreprint arXiv:2311.05772. Pranav Putta, Edmund Mills, Naman Garg, SumeetMotwani, Chelsea Finn, Divyansh Garg, and RafaelRafailov. 2024. Agent q: Advanced reasoning andlearning for autonomous ai agents. arXiv preprintarXiv:2408.07199. Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen,Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang,and Huajun Chen. 2022.Reasoning with lan-guage model prompting: A survey. arXiv preprintarXiv:2212.09597. Mathieu Ravaut, Bosheng Ding, Fangkai Jiao, HailinChen, Xingxuan Li, Ruochen Zhao, Chengwei Qin,Caiming Xiong, and Shafiq Joty. 2024.Howmuch are llms contaminated?a comprehensivesurvey and the llmsanitize library. arXiv preprintarXiv:2404.00699.",
  "Abigail See, Peter J Liu, and Christopher D Man-ning. 2017.Get to the point:Summarizationwith pointer-generator networks.arXiv preprintarXiv:1704.04368": "Freda Shi, Xinyun Chen, Kanishka Misra, NathanScales, David Dohan, Ed H Chi, Nathanael Schrli,and Denny Zhou. 2023. Large language models canbe easily distracted by irrelevant context. In Inter-national Conference on Machine Learning, pages3121031227. PMLR. Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez,Dongwei Jiang, Manya Wadhwa, Prasann Singhal,Xinyu Zhao, Xi Ye, Kyle Mahowald, and Greg Dur-rett. 2024. To cot or not to cot? chain-of-thoughthelps mainly on math and symbolic reasoning. arXivpreprint arXiv:2409.12183. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,Adam R Brown, Adam Santoro, Aditya Gupta,Adri Garriga-Alonso, et al. 2022a.Beyond theimitation game: Quantifying and extrapolating thecapabilities of language models.arXiv preprintarXiv:2206.04615. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,Adam R Brown, Adam Santoro, Aditya Gupta,Adri Garriga-Alonso, et al. 2022b.Beyond theimitation game: Quantifying and extrapolating thecapabilities of language models.arXiv preprintarXiv:2206.04615. Gaurav Suri, Lily R Slater, Ali Ziaee, and MorganNguyen. 2024. Do large language models show de-cision heuristics similar to humans? a case studyusing gpt-3.5. Journal of Experimental Psychology:General.",
  "Alon Talmor, Jonathan Herzig, Nicholas Lourie, andJonathan Berant. 2018. Commonsenseqa: A questionanswering challenge targeting commonsense knowl-edge. arXiv preprint arXiv:1811.00937": "Alex Tamkin, Amanda Askell, Liane Lovitt, EsinDurmus, Nicholas Joseph, Shauna Kravec, KarinaNguyen, Jared Kaplan, and Deep Ganguli. 2023.Evaluating and mitigating discrimination in languagemodel decisions. arXiv preprint arXiv:2312.03689. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro,Faisal Azhar, et al. 2023. Llama: Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971. Miles Turpin, Julian Michael, Ethan Perez, and SamuelBowman. 2024. Language models dont always saywhat they think: unfaithful explanations in chain-of-thought prompting. Advances in Neural InformationProcessing Systems, 36.",
  "USDL. 2024.Occupational employment and wagestatistics. Accessed: 05-05-2024": "Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen,You Wu, Luke Zettlemoyer, and Huan Sun. 2022.Towards understanding chain-of-thought prompting:An empirical study of what matters. arXiv preprintarXiv:2212.10001. Pengda Wang, Zilin Xiao, Hanjie Chen, and Freder-ick L Oswald. 2024.Will the real linda pleasestand up... to large language models? examining therepresentativeness heuristic in llms. arXiv preprintarXiv:2404.01461. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,Barret Zoph, Sebastian Borgeaud, Dani Yogatama,Maarten Bosma, Denny Zhou, Donald Metzler, et al.2022a. Emergent abilities of large language models.arXiv preprint arXiv:2206.07682. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022b. Chain-of-thought prompting elicits rea-soning in large language models. Advances in neuralinformation processing systems, 35:2482424837.",
  "Sohee Yang, Elena Gribovskaya, Nora Kassner, MorGeva, and Sebastian Riedel. 2024. Do large languagemodels latently perform multi-hop reasoning? arXivpreprint arXiv:2402.16837": "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-gio, William W Cohen, Ruslan Salakhutdinov, andChristopher D Manning. 2018. Hotpotqa: A datasetfor diverse, explainable multi-hop question answer-ing. arXiv preprint arXiv:1809.09600. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,Tom Griffiths, Yuan Cao, and Karthik Narasimhan.2024. Tree of thoughts: Deliberate problem solvingwith large language models. Advances in NeuralInformation Processing Systems, 36.",
  "Here is an example of GPT-4o explaining the Linda Problem: Bob version of this problem is as follows:": "Bob is 29 years old, deeply passionate about environmental conservation, and volunteers hisweekends at local park clean-ups. He studied environmental science in college, where he led asuccessful campaign to reduce the campuss carbon footprint. Bob is also an avid cyclist andpromotes sustainable living practices whenever possible. Based on this information, which ismore possible?",
  "B.1Weak Hint": "For Problems on Conjunction FallaciesYour task is to answer the following question by explicitlyselecting either option (a), (b), etc. Please be aware that this is a Linda Problem designed to explore theconcept of the conjunction fallacy. Here is the question and lets think step by step. For Problems on Syllogistic FallaciesYour task is to answer the following question by explicitlysaying Yes or No. Please be aware that this is a Linda Problem designed to explore the concept of thesyllogistic fallacy.",
  "B.2Strong Hint": "For Problems on Conjunction FallaciesYour task is to answer the following question by explicitlyselecting either option (a), (b), etc. Please aware that this is a Linda Problem designed to explore theconcept of the conjunction fallacy. The conjunction fallacy occurs when individuals incorrectly judgethe conjunction of two events as more probable than one of the events alone. For instance, many mightbelieve that Linda, who is described as a bright, single woman deeply concerned with discrimination andsocial justice, is more likely to be both a bank teller and active in the feminist movement than just a bankteller. This judgment violates the basic probability rule: the probability of a conjunction, P(A and B),is always less than or equal to the probabilities of its constituents, P(A) or P(B). This error often stemsfrom the representativeness heuristic, where people estimate the likelihood of an event by how closelyit matches their mental prototype. To correctly solve problems like this, you must adopt probabilisticthinking: abstract the problem from its narrative context and focus solely on the probabilistic models.Ignore all extraneous background information and consistently choose the option involving a single eventas it statistically holds a higher likelihood than the conjunction of multiple events. Here is the questionand lets think step by step. For Problems on Syllogistic FallaciesYour task is to answer the following question by explicitlysaying Yes or No. Please aware that this is a Syllogistic Fallacy Problem. This type of reasoning isknown as a syllogism. Pay close attention to quantifiers such as All, Some, No, or similar terms.These terms help define the distribution of properties or elements within the given groups or categories inthe premises. Next, assess whether the attribute ascribed in the conclusion necessarily follows from theattributes described in the premises. Consider if the subset described in the second premise encompassesor overlaps with the elements in the first premise that are carried into the conclusion. A common pitfall insyllogistic reasoning is the erroneous assumption that a characteristic of a subset of a group (from thepremises) applies to another subset of the same or different group (in the conclusion), without explicitjustification. Ignore the background information about the objects and focus on the logical structure of theargument. Here is an example.",
  "We create several variants of the conjunction fallacy problem discussed in the original work by Tverskyand Kahneman (1983):": "Variant 1The original Linda Problem. We maintain the narrative structure of the original Linda Problemdescribed in Appendix A. We ask GPT-4 to randomly pick reasonable personal details such as name, race,gender identity, age, and major, forming a short biography. GPT-4 then crafts two options (a) and (b) foreach problem, both of which contain the same randomly selected occupation from USDL (2024) like\"Linda is a bank teller\". The longer option also contains a hobby that must be relevant to the bio like\"active in the feminist movement\".Thepromptusedtogeneratethebioisasfollows,where{random_gender},{random_race},{random_age} are sampled from a pre-defined random function: Your task is to write a short bio for a random person within 100 words. Youshall pick a random name, use gender {random_gender}, race {random_race}, andan age {random_age}. The bio should describe the college majors, some personalcharacters, and interests.Keep the bio short.For example, Linda is 31years old, single, outspoken, and very bright. She majored in philosophy. Asa student, she was deeply concerned with issues of discrimination and socialjustice, and also participated in anti-nuclear demonstrations. Write anotherexample here:",
  "We then follow-up the conversation with the following prompt:": "Your next step is to find a hobby or activity that the person mentioned beforewill be interested in based on your experience. The hobby or activity mustbe relevant to the bio descriptions. In the example above, we can say thatLinda is active in the feminist movement.because her bio says she wasconcerned with discrimination and social justice. Please keep your answer inone sentence and begin with that persons name, but refrain from using anywords used in the bio.",
  "Variant 2In the original paper, the following variant of the conjunction fallacy problem is also presented:": "John P. is a meek man, 42 years old, married with two children. His neighbors describe himas mild-mannered but somewhat secretive. He owns an import-export company based in NewYork City, and he travels frequently to Europe and the Far East. Mr. P. was convicted once forsmuggling precious stones and metals (including uranium) and received a suspended sentenceof 6 months in jail and a large fine. Mr. P. is currently under police investigation. Which one ismore likely?",
  ". Mr. P. killed one of his employees.2. Mr. P. killed one of his employees to prevent him from talking to the police": "The conjunction of two events in the second option is connected by the word to. To create this dataset,we sample a random story from the collection of commonsense stories (Mostafazadeh et al., 2016) andCNN news stories (See et al., 2017). We use all the sentences in the story as the context of the conjunctionfallacy problem except for the last one. We use the last sentence as the first option in the problem. As forthe second option, we append the last sentence, add the connecting word to, and then we prompt GPT-4to complete the second option. The prompt used here is similar to that discussed in section 2.To perform token perturbation, we further prompt GPT-4 with the following:",
  "Inspired by this example, we randomly sample a disease and its corresponding symptoms from (kag)and apply the following prompt to generate a conjunction fallacy problem:": "Your task is to create another conjunction fallacy quiz following the formatin the example below.Do not mention the name conjunction fallacy.Youshould pick a random name for the patient, use gender {random_gender} race{random_race}, an age {random_age} and the disease {random_disease} in yournew problem statement. The question should be Which one is more likely?followed by two options (a) and (b), one of which should be a subset of theother. You can randomly switch the order of which option is (a) and whichis (b). You should use the symptoms {random_symptom_one} in both options andadd {random_symptom_two} to the longer option only. Do not make any changesto the given disease or the symptoms. Here is the new problem:",
  "We then prompt GPT-4 for an irrelevant symptom:": "Your task is to create another conjunction fallacy quiz following the formatin the example below.Do not mention the name conjunction fallacy.Youshould pick a random name for the patient, use gender {random_gender} race{random_race}, an age {random_age} and the disease {random_disease} in your new problem statement. The question should be Which one is more likely?followed by two options (a) and (b), one of which should be a subset of theother. You can randomly switch the order of which option is (a) and whichis (b).You should use the symptoms {random_symptom_one} in both options.You should add another random symptoms to the longer option only, which mustbe completely irrelevant to the disease {random_disease} intentionally. Donot make any changes to the given disease or the symptoms. Here is the newproblem:",
  ". Borg will lose the first set2. Borg will lose the first set but win the match": "Inspired by this example, we randomly sample celebrity names from the Times Person of theYear (Rosenberg, 2021) and Forbes Celebrity 100 (Wikipedia contributors, 2024a) and apply the followingfew-shot prompt to generate a conjunction fallacy problem. Create one example that look like this:Suppose [celebrity is going to do something]. Which is more likely:(a) [Something unlikely for this person](b) [Something unlikely for this person] but [something extremely likely forthis person]",
  "Here are some examples:": "Suppose Taylor Swift is going to have another tour in 2027.Which ismore likely:(a) Her first show is a flop.(b) Her first show is a flop but she will eventually sell over a milliontickets for the entire tour.Suppose Barack Obama is running for president in 2024. Which is more likely:(a) Barack Obama will win the national popular vote(b) Barack Obama will win the national popular vote but lose the ElectoralCollege voteSuppose Bjorn Borg reaches the Wimbledon finals.Which outcome is morelikely?(a) Borg will lose the first set(b) Borg will lose the first set but win the matchComplete the following. Do not output anything else.Suppose {random_celebrity} For Hypothesis 1, we include Variant 2,3,4 and 5, resulting in n = 400 samples. For Hypothesis 2,we include Variant 2,3,4,5 and 6, resulting in n = 500 samples. For Hypothesis 3, we include Variant 5,resulting in n = 100 samples.",
  "Fill in the blanks in the following template. Do not output anything else.All [objects] are [category]": "Some [category]s [characteristic traits of this category].Therefore some [same objects as before] [characteristic traits this category].Make sure that the characteristic traits of this category only fit for a subsetof this category but not for all.For example:All carrots are vegetables.Some vegetables are rich in fiber.Therefore, some carrots are rich in fiber.All roses are flowers.Some flowers fade quickly.Therefore some roses fade quickly.All actors are performers.Some performers are skilled in improvisation.Therefore some actors are skilled in improvisation.All {random_object} are",
  "The full experimental results for Hypothesis 1 are shown in , 6 and": ": Full Experimental results for Hypothesis 1. Note that in our experiments, n = n21 + n12 is not equalto the number of data samples n. Here, n12 denotes the instances where the LLM correctly answers the originalproblem but fails on the perturbed version, and n21 denotes the opposite scenario. Thus, a large value of n happensonly when the LLM makes many mistakes. Specifically, GPT-4o in this table shows n = 1 with few-shots learning.We find that GPT-4o is excellent in answering these problems with few-show exemplars, achieving near-perfectaccuracy of almost 100%, so their n12 and n21 values are pretty small. This high accuracy, however, only leadus to fail to reject this particular instantiation of the null hypothesis, but not in other situations. While we couldincrease the sample size from the current n to potentialy observe more errors, thus a higher n in scenarios involvingstate-of-the-art LMs with few-shot learning, our rejection of the null hypothesis under other scenarios when testedagainst the GPT-4o is sufficient to argue that LLMs are not yet genuine reasoners.",
  "modelprompting methodn12n21nz-statp-valuereject": "gpt-3.5-turboweak-control-zs-cot6442348716.2678430.000000Truegpt-3.5-turbocontrol-zs-cot5741747416.5353480.000000Truegpt-3.5-turboweak-control-os-cot6025031010.7912750.000000Truegpt-3.5-turbocontrol-os-cot642302949.6813170.000000Truegpt-4-turboweak-control-zs-cot838639419.0433650.000000Truegpt-4-turbocontrol-zs-cot442042420.2027460.000000Truegpt-4-turboweak-control-os-cot61131199.8086740.000000Truegpt-4-turbocontrol-os-cot412613010.7001080.000000Truegpt-4oweak-control-zs-cot826227015.4579480.000000Truegpt-4ocontrol-zs-cot1130131216.4180170.000000Truegpt-4oweak-control-os-cot13971108.0090860.000000Truegpt-4ocontrol-os-cot121121248.9802650.000000Truellama-2-70b-chatweak-control-zs-cot721772496.6541050.000000Truellama-2-70b-chatcontrol-zs-cot3253156321.0303430.000000Truellama-2-70b-chatweak-control-os-cot6931338212.4841260.000000Truellama-2-70b-chatcontrol-os-cot7342950215.8890580.000000Truemeta-llama-3-70b-instructweak-control-zs-cot1751853521.6601190.000000Truemeta-llama-3-70b-instructcontrol-zs-cot1057958923.4452370.000000Truemeta-llama-3-70b-instructweak-control-os-cot541502046.7213440.000000Truemeta-llama-3-70b-instructcontrol-os-cot441742188.8047110.000000Truemeta-llama-3-8b-instructweak-control-zs-cot5740546216.1904250.000000Truemeta-llama-3-8b-instructcontrol-zs-cot448749121.7974850.000000Truemeta-llama-3-8b-instructweak-control-os-cot5326331611.8134230.000000Truemeta-llama-3-8b-instructcontrol-os-cot1830131915.8449580.000000Trueclaude-3-opus-20240229weak-control-zs-cot1541242719.2121770.000000Trueclaude-3-opus-20240229control-zs-cot946747620.9923960.000000Trueclaude-3-opus-20240229weak-control-os-cot2629932515.1433150.000000Trueclaude-3-opus-20240229control-os-cot3021224211.6994030.000000Trueclaude-3-sonnet-20240229weak-control-zs-cot547047521.3356630.000000Trueclaude-3-sonnet-20240229control-zs-cot1046647620.9007260.000000Trueclaude-3-sonnet-20240229weak-control-os-cot1723825513.8395570.000000Trueclaude-3-sonnet-20240229control-os-cot1625026614.3474610.000000Truemistral-large-latestweak-control-zs-cot053353323.0867930.000000Truemistral-large-latestcontrol-zs-cot1553054522.0601760.000000Truemistral-large-latestweak-control-os-cot317918213.0459880.000000Truemistral-large-latestcontrol-os-cot120921014.3533640.000000True : Full experimental results for Hypothesis 6 (n = 800). The perturbed problems leak hint tokens, eitherweak or strong hints in problem statements. We run zero-shot and one-shot prompt methods. To reject the null, weexpect n12 < n21. We conclude that LLMs still heavily rely on hint tokens for solving logical fallacy problemswell."
}