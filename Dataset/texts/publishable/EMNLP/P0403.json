{
  "Abstract": "Although pre-trained language models haveexhibited great flexibility and versatility withprompt-based few-shot learning, they sufferfrom the extensive parameter size and limitedapplicability for inference. Recent studies havesuggested that PLMs be used as dataset gener-ators and a tiny task-specific model be trainedto achieve efficient inference. However, theirapplicability to various domains is limited be-cause they tend to generate domain-specificdatasets. In this work, we propose a novel ap-proach to universal domain generalization thatgenerates a dataset regardless of the target do-main. This allows for generalization of the tinytask model to any domain that shares the labelspace, thus enhancing the real-world applica-bility of the dataset generation paradigm. Ourexperiments indicate that the proposed methodaccomplishes generalizability across variousdomains while using a parameter set that isorders of magnitude smaller than PLMs.",
  "Introduction": "As the size and performance of pre-trained lan-guage models (PLMs) increase, generation of newdata by using PLMs has attracted the attention ofmany researchers (Anaby-Tavor et al., 2020; Ku-mar et al., 2020; Yoo et al., 2021). While scholarshave applied this method to solve data augmenta-tion problems, in recent studies, they have started toexplore zero-shot dataset generation settings (Menget al., 2022; Ye et al., 2022a, 2023). This novel ap-proach first generates training data from a PLMbased on a specific prompt and trains a tiny taskmodel (TAM) by using the dataset generated in thefirst step. This strategy facilitates effective distilla-tion of the knowledge pertaining to the desired taskfrom the PLM and helps train the TAM withoutthe need for guidance from human-annotated data,thereby enabling zero-shot learning and achievinglow-cost inference compared to the case in whichPLMs are used directly for inference. However, the approaches proposed thus far haverelied on domain-specific prompts, for example,The movie review in positive sentiment is:. Be-cause the data generated using this prompt are re-lated only to the domain of movie reviews, theTAM trained on these data has limited general-ization ability across other domains. This is theprimary limitation of the TAM-based approachcompared to prompt-based zero-shot learning thatdirectly uses PLMs (PROMPTING), which allowsfor generalizability across diverse domains. Thisrestricts the real-world applicability of the TAM-based approach because it requires many separatelytrained TAMs for various domains. Moreover, asthe costs of dataset generation and TAM trainingincrease, the cost-efficiency of the TAM-based ap-proach may decrease. Hence, a novel strategy isdesired to effectively distill the domain generaliz-ability of large-scale PLMs into TAMs while main-taining the cost-efficiency of TAMs.Meanwhile, the existing approaches to domaingeneralization often require multiple source do-mains (Wang et al., 2022; Zhou et al., 2022). Thisrequirement limits the application of these meth-ods because it is difficult to gather the requireddata from multiple domains. Although the conceptof single-domain generalization, which achievesdomain generalizability by using data from onlyone source domain, has been proposed in recentcomputer vision studies, such a concept is yet tobe explored for natural language processing (Qiaoet al., 2020; Wang et al., 2021).In this study, we propose a simple but effectivemethod called UNIGEN to solve the problem ofdomain generalizability between PLMs and TAMs. presents a comparison between UNIGENand the existing approaches.UNIGEN first fo-cuses on generating a domain-invariant trainingdataset that is not restricted to specific domains.This allows TAMs to achieve domain generalizabil-ity without the need for multiple source domains.",
  ": Comparison between previous approaches and UNIGEN": "We extend domain generalization strategies basedon supervised contrastive learning (Khosla et al.,2020), as suggested in a previous work (Tan et al.,2022). Moreover, we employ additional tacticssuch as momentum encoder (He et al., 2020) anddenoised memory bank, in addition to the methodsuggested by the previous work (Tan et al., 2022).Furthermore, because the PLM-based dataset gen-eration method can generate noisy data (Ye et al.,2022b; Gao et al., 2023; Zou et al., 2024), we pro-pose a pseudo-relabeling-based additional denois-ing method.Our experiments show that UNIGEN achievesgeneralizability across various domains and out-performs PROMPTING. This indicates that smallerTAMs can be used universally in various domains,thereby reducing the costs of PROMPTING, datasetgeneration, and TAM training.Our contributions are summarized as follows:",
  "Dataset Generation for Efficient Zero-shotLearning": "The evolution of PLMs in terms of parameter sizeand performance has facilitated zero-shot learningthrough the use of well-designed prompts (Radfordet al., 2019; Brown et al., 2020). However, it isexpensive to directly deploy these massive models into daily services because the process requiresnumerous rounds of inference. Dataset generationmitigates this problem through the generation oftraining datasets by using PLMs and training asmall TAM on the generated datasets (Meng et al.,2022; Ye et al., 2022a). This TAM is deployedin downstream tasks to reduce inference costs andimprove performance compared to PROMPTING.However, mere generation, that is, ZEROGEN,yields noisy data, such as incorrectly labeled dataor irrelevant data (Ye et al., 2022b; Gao et al.,2023). PROGEN (Ye et al., 2022b) proposed to al-leviate this problem by adding examples based onin-context feedback. Meanwhile, SUNGEN (Gaoet al., 2023) proposed to re-weigh the generatedsamples during training using noise-robust loss.Additionally, a concurrent study suggested to lever-age multiple PLMs as data generator and assignweight to generated samples in single training pro-cedure, different from SUNGEN (Zou et al., 2024).In this work, we propose a novel approach toextend dataset generation for universal domain gen-eralization that is not restricted to specific trainingsource data, as well as a pseudo-relabeling-basedmethod to denoise the generated dataset.",
  "Methods for Learning from Noisy Data": "Researchers have explored various methods to mit-igate noisy label data, which is wrongly labeledfrom ground-truth labels (Song et al., 2023). A rel-evant study in this field defined two types of noisylabels and evaluated the effectiveness of variousmethods with respect to BERT model (Agro andAldarmaki, 2023). Another study proposed to lever-age GPT-4 to provide the guidance to noisy labeleddata (Wang et al., 2023). However, they suffer fromthe necessity of massive LLMs that demand cost.Moreover, these studies primarily focused on thehuman-crafted noisy label, rather than the noisylabel of data generated by PLMs.",
  "Domain Generalization for TextClassification": "Domain generalization aims to improve the gener-alization ability in the target domain by employingsource data from multiple domains to mitigate thedomain shift problem (Wang et al., 2022; Zhouet al., 2022). This domain shift can be observed innatural language processing tasks, such as restau-rant reviews and reviews of consumer electronics.For example, long waiting time in a restaurantsreviews can represent a negative sentiment aboutthe restaurant, while long battery life in a laptopsreviews can represent a positive sentiment of thelaptop (Tan et al., 2022).Previous studies to alleviate domain shift intext classification have focused primarily on do-main adaptation setting, for which training dataare needed in the target domain (Chen and Cardie,2018; Ye et al., 2020; Guo et al., 2020). Recently,researchers have explored the application of do-main generalization to natural language processingtasks. A representative study applied supervisedcontrastive learning (Khosla et al., 2020) to achievedomain generalizability in text classification tasks(Tan et al., 2022).In this work, we extend an existing method fordomain generalization to generate datasets, includ-ing the adoption of momentum encoder (He et al.,2020), in addition to proposing a denoising mem-ory bank to further enhance its effectiveness andhandle noisy data.",
  "Dataset Generation": "First, we briefly explain the concept and notationof the preliminary dataset generation method, thatis, ZEROGEN (Ye et al., 2022a). ZEROGEN aimsto create a synthetic dataset Ssyn = (Xsyn, Ysyn)by using a large-scale PLM P and task-specificprompt Ttask. For a text classification problem,a desired pseudo-label ysyn is first sampled fromthe uniform distribution across every class. Next,ysyn is passed to the prompt Ttask to constructTtask(ysyn), that is, the final prompt for P. There-after, synthesized input data xsyn are generated using xsyn P(|Ttask(ysyn)). Finally, Ssyn is com-posed of these pairs of generated (xsyn, ysyn). No-tably, the domain of Ssyn is defined by the structureof Ttask. For example, a Tbook = The book reviewin <y> sentiment is: would harness P to gener-ate xsyn about book reviews. The TAM is trainedon the generated Ssyn and deployed for inferenceinstead of directly using PLMs with PROMPTING. 3.1.2Supervised Contrastive LearningSupervised contrastive learning (Khosla et al.,2020) is a variant of contrastive learning (Chenet al., 2020) that utilizes label values. It allowsfor explicit pulling of the representation of positive(i.e., same class) samples to the anchor representa-tion while pushing negative representations awayfrom the anchor. Studies have reported that thischaracteristic is valuable for domain generalization,which aims to group the representations of differentdomains (Kim et al., 2021; Tan et al., 2022). Thesupervised contrastive loss is expressed as follows:",
  "|P(i)|logexp(zizp/SCL)zaA(i) exp(ziza/SCL)": "(1)where z denotes an encoded representation, andzi is an anchor. P(i) zj B, yj = yi is theset of positive samples for each anchor i, and zpsymbolizes a positive representation from P(i).A(i) zj B, j = i refers to the union of everysample, except the anchor, including positive andnegative samples. za indicates each representationfrom A(i). B denotes a mini-batch, and SCL is thetemperature of supervised contrastive learning.Although supervised contrastive learning is ef-fective, the introduction of a memory bank andmomentum encoder may augment the advantagesof the method (Wu et al., 2018; He et al., 2020).The potency of contrastive learning is often influ-enced by the size of B because a larger B mayintroduce more diverse negative samples. How-ever, increasing the size of B can introduce con-cerns related to memory consumption. A mem-ory bank is a mechanism that fulfills this demandfor a greater number of negative samples by stor-ing previously processed samples within the dic-tionary M. Memory-efficient contrastive learningcan be achieved using this dictionary with the cur-rent batch, that is, establishing a union of B andM instead of solely using B to construct P(i) andA(i). Momentum encoder is another technique thatsmooths the process of updating the representations",
  "UNIGEN": "To build a TAM that can be applied universallyto various target domains, UNIGEN generates adomain-invariant dataset by using the universalprompt Tuni, instead of task-specific Ttask. ConsiderThe text in <y> sentiment is: as an example ofTuni. Next, the final input prompt for P is con-structed as Tuni(ysyn). The synthesized input dataxsyn are generated by following the same processas that of ZEROGEN:",
  "xsyn P(|Tuni(ysyn))(2)": "This configuration of prompt design allows us togenerate a sentence with the desired label withoutbeing restricted to any specific domain. Therefore,it steers P to generate various sentences within apredefined label space. This domain-invariant datageneration allows the TAM trained using UNIGENto learn the domain-invariant characteristics of thedesired label space, thereby resulting in generaliz-ability across the domains that share the label space.Supervised contrastive loss is applied along withconventional cross entropy loss to aid this process.The training loss is defined as follows:",
  "Handling Noisy Data through Relabeling": "However, the application of Tuni instead of Ttaskmight lead to the generation of noisy sentences,which was noted as a drawback of ZEROGEN. Thisis because Tuni does not have a specific topic toguide the generation process. Furthermore, a pre-viously developed approach to effectively mitigatethis problem is applied in the training phase but notthe generation phase. Therefore, there is scope toimprove the quality of Ssyn (Gao et al., 2023). Thisproblem highlights the necessity to use a denoisingscheme in the generation procedure. In the presentwork, we propose a pseudo-relabeling-based de-noising process for dataset generation. In a previ-ous study, the approach of relabeling the generateddata and assigning soft labels for data augmenta-tion was proposed (Yoo et al., 2021). Herein, wefirst perform pseudo-relabeling by using P:",
  "yi = p(yi|xsyn) =exp((yi|xsyn)/RE)j exp((yj|xsyn)/RE)(5)": "Finally, we assign yi instead of the predefinedysyn to the generated xsyn. This provides two dis-tinct advantages: (1) because yi is a soft label ratherthan a hard label, it contains richer informationabout xsyn, such as the degree of the desired la-bel, which enhances the effectiveness of training(Szegedy et al., 2016). (2) Because it relabels thegenerated xsyn and replaces the predefined ysyn, itcan solve the noisy label issue, which results in thegeneration of xsyn that does not correspond to thedesignated ysyn, as pointed out in previous work(Gao et al., 2023). We validate the effectivenessof this relabeling strategy in the ablation study de-scribed in .5.1.Furthermore, we discard xsyn if its pseudo-labelyi does not exceed the threshold TRE to enhancethe quality of Ssyn. This guarantees that only thosedata that have the desired degree of each label aremaintained.",
  "Denoising Memory Bank": "In addition to the relabeling strategy, we propose adenoising memory bank mechanism to further alle-viate the issue of noisy data. We first use SUNGEN(Gao et al., 2023) that learns weights of each train-ing sample w for loss function within the trainingprocess to assign small weights to noisy data byemploying a noise-robust loss function. We aimto ensure that the memory bank M contains cleansamples, rather than noisy samples. We utilize theweights w learned from the noise-robust loss func-tion for this purpose. In the process of updatingM, we store only those samples whose weights arelarger than the threshold TMB. This organization ofthe memory bank ensures the exclusion of noisysamples from the comparison, resulting in higher-quality negative and positive samples (Robinsonet al., 2021).",
  "Experimental Setup": "In this section, we briefly explain the experimen-tal setup used herein to validate the effectivenessof UNIGEN.We employ seven different senti-ment classification datasets in our main experiment.Among them, IMDB (Maas et al., 2011), SST-2(Socher et al., 2013), and Rotten Tomatoes (Pang and Lee, 2005) are datasets comprising movie re-views. Meanwhile, the Amazon (McAuley andLeskovec, 2013) dataset consists of customer re-views of various products, and the Yelp (Zhanget al., 2015) dataset is composed of restaurant re-views. CR (Ding et al., 2008) is another customerreview dataset focusing on consumer electronics.Lastly, Tweet (Rosenthal et al., 2017) is composedof messages from Twitter. This configuration al-lows us to evaluate the ability of UNIGEN, whichcan be applied to various domains without pro-viding any prior information or domain-specifictraining. Following the previous study, we adaptedlong short-term memory (LSTM) (Hochreiter andSchmidhuber, 1997) and DistilBERT (Sanh et al.,2019), and we included RoBERTa (Liu et al., 2019)as our TAM. We compared our approach to ZE- ROGEN and SUNGEN, as well as to PROMPTINGusing GPT2-XL (Radford et al., 2019), to ensurea fair comparison. We did not include other largerPLMs in the experiments because the previouswork discovered that larger PLMs did not offerperformance gains (Ye et al., 2022a). We report theaverage of the performance results obtained acrossfive different random seeds.",
  "Comparison with Task-specific TAMs": "presents a comparison between the exper-imental results of UNIGEN and PROMPTING andtask-specific TAMs trained by ZEROGEN and SUN-GEN. The comparison results suggest that UNI-GEN can generalize across various domains usinga single model without requiring any prior infor-mation about the test domain. Nonetheless, UNI-GEN underperformed compared to the task-specificbaselines in each domain. However, the primarybenefit of UNIGEN lies in its unique domain gener-alizability while using orders-of-magnitude fewerparameters than PLMs. Additionally, its trainingprocedure is more efficient than those of other TAMtraining strategies. As can be inferred from Ta-ble 3, SUNGEN generates and synthesizes 1,000kdata for each task domain. This means that 5,000kdata would be required for our experiment, whichinvolves five different domains, in addition to in-dividual denoising processes for finding the bestweights of the samples in each of these domains.By contrast, UNIGEN is not limited by such restric-tions and requires only a single data generation anddenoising process, as well as a single training pro-cess. This is extremely beneficial when a novel test",
  "domain is introduced, where ZEROGEN and SUN-GEN necessitate a separate procedure for the newdomain, but UNIGEN directly reuses the alreadytrained TAM": "Notably, the performance of the LSTM-basedTAM trained using UNIGEN was significantlylower than that of ZEROGEN and SUNGEN. Thisimplies that while a small-sized TAM can betrained effectively for a single, specific domain,but suffers from generalizing to a universal domainthat requires a broad understanding of generateddata, as evidenced by detailed study in Appendix E. Accordingly, the performance of the TAM trainedusing UNIGEN improves significantly as the modelsize increases. For instance, the DistilBERT-basedTAM trained using UNIGEN exhibited the best av-erage performance against each task-specific base-line. This is particularly remarkable as it outper-formed the SUNGEN baseline in the movie do-main, which has three in-domain datasets, givingit an inherent advantage for average performance.Moreover, the RoBERTa-based TAM trained usingUNIGEN not only yielded the best average per-formance against these baselines but also outper-formed PROMPTING in every domain. This resultindicates that it can surpass the zero-shot perfor-mance of its PLM counterpart (e.g., GPT2-XL)while using less than 10% of the number of param-eters and securing the domain generalizability ofthe PLM, extending the achievement of the pre-vious study that leveraged small TAMs in singledomain (Ye et al., 2022a).",
  "Comparison with Supervised DomainGeneralization Method": "Next, we analyzed the performance of UNIGENagainst that of a domain generalization methodthat uses human-annotated data (Tan et al., 2022).For this purpose, we used a multi-domain reviewdataset comprising four domains: DVD, books,kitchen and housewares, and consumer electronics(Blitzer et al., 2007). Following the previous study,we split the dataset into 1,600 training data and400 testing data for each domain. presentsthe comparison results. These results suggest thatUNIGEN can be applied to various domains, and itsperformance is superior to that of its PLM counter-part. Notably, the SUPERVISED baseline relies onthree source domains with human-annotated datato generalize to a target domain, while UNIGEN isbased on zero-shot dataset generation and does notrequire any human-annotated data, which greatlyimproves its real-world applicability.",
  "Domain Generalizability of UNIGEN": "To intuitively examine the domain generalizabilityof UNIGEN, we plotted the T-SNE (Van der Maatenand Hinton, 2008) visualization of the features in-terpreted by the RoBERTa-based TAM trained us-ing UNIGEN. depicts the visualizationresults. These results suggest that the single TAMclassified the given data from every domain with-out explicit training or prior information about thedomains, thus demonstrating the unique efficiencyof UNIGEN. presents examples of the sentences gen-erated using UNIGEN. These examples showcasethat UNIGEN can generate domain-invariant sen-tences with the designated labels.By trainingTAMs on these data, it is possible to distill thedomain generalizability of PLMs into TAMs. : T-SNE visualization of the encoded represen-tation of the RoBERTa model trained using UNIGEN.The model was trained only on the data generated usingUNIGEN, which is shown in gray color. We used thetest set of the multi-domain review dataset.",
  "This section describes the ablation studies con-ducted to offer rationales for the engineeringchoices made in this study.We used theDistilBERT-based TAM for these experiments": "4.5.1Effectiveness of Relabeling StrategyFirst, we performed an ablation study to validatethe effectiveness of the relabeling strategy dis-cussed in .3. We compared the basic ap-proach that uses soft labels to the two other options.The first option utilizes the pseudo-relabeling pro-cess, but it assigns hard labels instead of soft labels.In other words, it only reflects the decision emanat-ing from the PLM, not the probability. The secondoption completely excludes the relabeling process.While this option would generate the dataset fasterthan the other options, it might generate text withnoisy labels, as already discussed in previous works(Ye et al., 2022a,b; Gao et al., 2023).The experimental results are presented in thesecond and third rows of . They suggestthat the use of soft labels offers practical benefitsin terms of performance. This finding is consistentwith that of a previous study in which the strengthof soft labels was demonstrated (Yoo et al., 2021;Fang et al., 2024). Therefore, according to the re-sults of this ablation study, relabeling the generateddata with the assignment of soft labels is effectivefor mitigating the issue of noisy labels.",
  "Positive ExamplesLabels": "You are a person who is hardworking, honest, and reliable. You have a good sense of humor, and you love being in charge.[0.19, 0.81]You are beautiful, you are powerful, you are amazing.[0.29, 0.71]In a city full of great ideas and creativity, Ive met a few people who have done things you wouldnt believe.[0.26, 0.74]The American Dream is alive in this great city. As a new generation of American heroes begins to realize their own American Dream.[0.24, 0.76]",
  "Effectiveness of Supervised ContrastiveLearning and Denoising Memory Bank": "Second, we conducted a comparison to investigatethe effectiveness of supervised contrastive learn-ing, which was discussed in .1.2, anddenoising memory bank, which was discussed in.4. The results of the comparison arepresented in fourth and fifth rows of . In-tuitively, if the quality of each of the data in thedataset is given as a weight, it would be effective toemploy only high-quality samples for comparingcontrastive learning rather than utilizing all data,regardless of their quality. The experimental resultin the fourth row demonstrated that the use of a de-noising memory bank yielded a performance gain,which was consistent with our intuition. Similarly,the result in the fifth row suggests that supervisedcontrastive learning plays a crucial role in UNI-GEN.",
  "Comparison with CombinedDomain-specific Datasets": "Third, we compared the performance of the TAMstrained with two different synthetic datasets. Thefirst uses the synthetic dataset generated with theprompt of UNIGEN, and the second uses the con-catenation of datasets generated with five differentdomain-specific prompts used in the other experi-ments. For this experiment, we only differentiatedthe synthetic dataset used for training and set everyother configuration identical, such as the usage ofpseudo-relabeling and denoised memory bank, aswell as other hyperparameters. The result of the ab-lation study is presented in the last row of .The result indicates that the model trained withthe dataset generated by the universal prompt inUNIGEN demonstrated better average performance.This suggests that the broad understanding of thelabel space offered by the synthetic dataset gener-ated by UNIGEN plays an important role in domaingeneralization. 4.5.4Comparison between PLMs for DataGenerationLastly, we evaluated the performance of TAMstrained using various PLMs. Initially, we utilizedGPT2-XL as the PLM for data generation.Inthis experiment, we extended the evaluation byincorporating more recent models as data genera-tors. Specifically, we compared the performanceof TAMs trained with UNIGEN using Gemma-2b (Team et al., 2024), Qwen2-1.5B (Yang et al.,2024), and Phi-1.5 (Li et al., 2023), which are morerecent models with parameter sizes comparable toGPT2-XL. All other configurations, aside from thePLM used for data generation, were kept consistentwith the original GPT2-XL-based TAM. presents the results of this experiment.Interestingly, the findings suggest that employingmore recent PLMs does not necessarily lead to bet-ter performance in UNIGEN. The TAM trained with GPT2-XL, our original choice for data gen-eration, achieved the highest average performance.This aligns with previous studies, which indicatethat using larger PLM does not always result insuperior outcomes (Ye et al., 2022a). However, de-spite using identical hyperparameters and promptsto ensure a fair comparison, it is important to rec-ognize that optimal hyperparameters, such as top-k,top-p, and RE, as well as the prompt configurations,may vary for each PLM. Future research could fo-cus on developing a unified framework to optimizehyperparameters and prompts for each PLMs, akinto methods like AutoAugment (Cubuk et al., 2019;Ren et al., 2021).",
  "Conclusion": "In this study, we proposed UNIGEN in an attemptto achieve universal domain generalization. UNI-GEN successfully transferred the domain generaliz-ability of PLMs into orders-of-magnitude smallerTAMs. Moreover, human annotation was not re-quired for UNIGEN, which significantly reducedthe burden of acquiring labeled data from multi-ple source domains. Our relabeling method anddenoising memory bank offered additional perfor-mance gains. Furthermore, our extensive experi-ments demonstrated that UNIGEN outperformedPROMPTING, facilitating light inference while pre-serving the domain generalizability of PLMs.Although we explored an interesting frameworkfor zero-shot, lightweight domain generalization,the performance of UNIGEN appears weaker thanthose of baseline models that are trained on eachdomain in several cases. It is desirable to achievea higher level of performance than those of the in-domain baselines, which we will attempt in futurework. To this end, the generation of small task-specific data for additional training of the TAMtrained using UNIGEN is a possible approach, es-pecially when a downstream task domain is intro-duced. By employing TAMs that are pre-trainedusing UNIGEN as a warm start, high performancecould be achieved in the target domain with a smallamount of task-specific data, which would reducethe total amount of data generated compared tothat when individually training each TAM by usingZEROGEN or SUNGEN from scratch. Another pos-sible approach may involve combining UNIGENwith the concept of test-time learning (Jeong et al.,2023). Similar to the first strategy, it may generatesmall amounts of test domain-specific data given",
  "Limitations": "The primary limitation of UNIGEN is its relativelyweaker in-domain performance than those of base-lines that are trained with domain-specific datasets.While it is beneficial for its smaller parameter setand lower inference cost while maintaining thedomain generalizability of PLMs, there exists atradeoff between in-domain performance and effi-ciency, unlike ZEROGEN and SUNGEN. Therefore,a method for further enhancing the performanceof UNIGEN should be explored, as stated in theConclusion section. A possible solution is a properprompt designed for UNIGEN because the qualityof the generated sentences is affected by prompt de-sign. Even though we adapted an effective promptdesigned in a previous work (Ye et al., 2022a), amore effective prompt for UNIGEN that aims togenerate diverse and general expressions could ex-ist.",
  "Xiaowen Ding, Bing Liu, and Philip S Yu. 2008. Aholistic lexicon-based approach to opinion mining.In Proceedings of WSDM, pages 231240": "Tianqing Fang, Wenxuan Zhou, Fangyu Liu, HongmingZhang, Yangqiu Song, and Muhao Chen. 2024. On-the-fly denoising for data augmentation in naturallanguage understanding. In Findings of EACL, pages766781. Jiahui Gao, Renjie Pi, Lin Yong, Hang Xu, JiachengYe, Zhiyong Wu, Weizhong Zhang, Xiaodan Liang,Zhenguo Li, and Lingpeng Kong. 2023. Self-guidednoise-free data generation for efficient zero-shotlearning. In Proceedings of ICLR.",
  "Victor Sanh, Lysandre Debut, Julien Chaumond, andThomas Wolf. 2019. Distilbert, a distilled versionof bert: smaller, faster, cheaper and lighter. arXivpreprint arXiv:1910.01108": "Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D Manning, Andrew Y Ng, andChristopher Potts. 2013. Recursive deep models forsemantic compositionality over a sentiment treebank.In Proceedings of EMNLP, pages 16311642. Hwanjun Song, Minseok Kim, Dongmin Park, YoojuShin, and Jae-Gil Lee. 2023. Learning from noisylabels with deep neural networks: A survey. IEEETransactions on Neural Networks and Learning Sys-tems, 34(11):81358153.",
  "An Yang, Baosong Yang, Binyuan Hui, Bo Zheng,Bowen Yu, Chang Zhou, Chengpeng Li, ChengyuanLi, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2technical report. arXiv preprint arXiv:2407.10671": "Hai Ye, Qingyu Tan, Ruidan He, Juntao Li, Hwee TouNg, and Lidong Bing. 2020. Feature adaptation ofpre-trained language models across languages anddomains with robust self-training. In Proceedings ofEMNLP, pages 73867399. Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, JiangtaoFeng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.2022a. Zerogen: Efficient zero-shot learning viadataset generation. In Proceedings of EMNLP, pages1165311669.",
  "BImplementation Detail": "For UNIGEN, we first generated 1,000k data fromthe 1.5B GPT2-XL model as P by using the promptTuni The text in positive/negative sentiment is: ,which is a slightly modified version of the bestprompt suggested in a previous study. Top-k andtop-p were set to 40 and 0.9 during the generationprocedure, respectively. The soft relabeling processwas performed using a RE of 0.1. After obtainingthe soft labels of each of the generated samples, wefiltered them using TRE of 0.2. This required thelargest value from the soft labels to be larger thanthe sum of the uniform distribution and TRE, forinstance, 0.7 in binary classification with TRE of0.2. As an example, the sentence corresponding tothe soft label [0.64, 0.36] was discarded because itdid not exceed the threshold.After generation, we followed the bi-level opti-mization approach proposed in SUNGEN to cleansethe generated dataset and find the sample weightsfor 50 epochs. The outer learning rate was setto 5e-2, and we randomly sampled 50k data foreach outer validation process. Then, we selected200k data with high weights, which represent high-quality data, to train the TAMs.We used a one-layer bi-LSTM model forthe LSTM-based TAM and the distilbert-base-uncased and roberta-base from Transformers (Wolfet al., 2020) for the DistilBERT-based TAM andRoBERTa-based TAM, respectively. We trained theLSTM-based TAM for 5 epochs with the learningrate of 1e-3 by using the Adam (Kingma and Ba,2015) optimizer. The DistilBERT-based TAM wastrained for 3 epochs with a learning rate of 2e-5 byusing the Adam optimizer. The RoBERTa-basedTAM was trained for 3 epochs with a learning rateof 2e-5 by using the Adam optimizer. During thetraining process, for supervised contrastive learn-ing loss was set to 0.5, with a projection size of 256. The temperature SCL was set to 0.2, and thememory bank size M was set to 64. The coefficientm for updating the momentum encoder was set to0.999, and the threshold of the denoising memorybank TMB was set to 0.8. The dataset generationand training procedures were executed using on asingle NVIDIA A100 40GB GPU. Please refer toattached source code for further details.1",
  ": Experimental result on the extensibility of rela-beling strategy. We trained the TAM using ZEROGENbased on the movie domain": "We examined the extensibility of the relabelingstrategy discussed in .3. We applied twodifferent options for relabeling, namely assigninghard labels and soft labels to ZEROGEN. summarizes the results. These results suggest thatthe relabeling strategy is beneficial for the perfor-mance of the TAM trained using ZEROGEN. There-fore, filtering the generated data through the relabel-ing strategy is an extensive strategy for enhancingzero-shot learning methods based on dataset gener-ation. Furthermore, the assignment of soft labelswas more beneficial compared to the assignmentof hard labels, which is consistent with the resultsof the ablation study described in .5.1.We will further investigate the relabeling-based ap-proach to enhance ZEROGEN and SUNGEN in fu-ture works.",
  "DAdditional Experiment on DomainGeneralizability": "To further reveal the domain generalizability ofUNIGEN, we conducted an additional experimenton Amazon Review dataset (Ni et al., 2019). Weused 5-core data for 29 domains and reported theperformance of PROMPTING using GPT2-XL (Rad-ford et al., 2019) and RoBERTa-based TAM trainedby UNIGEN. The result in demonstratesthe performance of UNIGEN that is comparablewith PROMPTING, with parameters less than 10%.Additionally, this experiment showcases the univer-sality of UNIGEN, the characteristics that distin-",
  "EAdditional Study on the Performanceof UNIGEN on Small-sized TAMs": "We found that UNIGEN suffers to exhibit its perfor-mance on the LSTM model from the experimentin . To further investigate this phenomenon,we expand our experiment into two different small-sized TAMs: TextCNN (Kim, 2014) and TinyBERT(Jiao et al., 2020). showcases the result ofthe additional experiment. In the case of TextCNN-based TAM, baseline methods such as ZEROGENand SUNGEN demonstrated comparable or slightlyhigher performance compared to that of LSTM-based TAM. Nonetheless, TextCNN-based TAMtrained on UNIGEN reported slightly worse per- formance compared to LSTM-based TAM despiteincreased parameter size.We hypothesize thatthis phenomenon is owing to the architecture ofTextCNN, which leverages CNN layers that havefixed window size, leading to limited ability tounderstand the context of diverse expression gen-erated by UNIGEN. On the contrary, TinyBERT-based TAM trained on UNIGEN exhibited the bestaverage performance among the baselines. Fur-thermore, its average performance is comparableto DistilBERT-based TAM despite a much smallerparameter size. It is noteworthy that TinyBERT isalso a model that has a general understanding ofthe language through knowledge distillation fromBERT. Through this investigation, we reveal thatthe pre-trained knowledge of the TAM aids thesuccessful training of the TAM through UNIGEN."
}