{
  "Abstract": "Domain experts across engineering, healthcare,and education follow strict standards for pro-ducing quality content such as technical man-uals, medication instructions, and childrensreading materials. However, current works incontrollable text generation have yet to exploreusing these standards as references for control.Towards this end, we introduce STANDARD- IZE, a retrieval-style in-context learning-basedframework to guide large language models toalign with expert-defined standards. Focusingon English language standards in the educationdomain as a use case, we consider the Com-mon European Framework of Reference forLanguages (CEFR) and Common Core Stan-dards (CCS) for the task of open-ended contentgeneration. Our findings show that models cangain 45% to 100% increase in precise accuracyacross open and commercial LLMs evaluated,demonstrating that the use of knowledge ar-tifacts extracted from standards and integrat-ing them in the generation process can effec-tively guide models to produce better standard-aligned content.1",
  "Introduction": "One of the most realized benefits of large languagemodel (LLM) research is how it became widelyadopted by the public. In particular, the rise of chat-style model interfaces, such as ChatGPT and Per-plexity, has allowed non-technical users to fully uti-lize these tools in accomplishing day-to-day tasksand activities, such as getting help with writing,documenting code, and providing recommenda-tions. A key technological advancement behindthis is the use of reward-based methods such as Re-inforcement Learning for Human Feedback (RLHF,Ouyang et al. (2022)), which embeds human pref-erences to generative models for better-aligned out-puts with respect to the task at hand.",
  "Teacher Style": "Given this prompt: In the dark old forest up ahead, a solitary gure emerged from the corner of the shadowy grove Continue the story and make sure they are readable for B1 learners in the CEFR scale and observes the following specications: 1. Meaning or Purpose: The text is clear and concrete, and tells a simple story.2. Structure: The text is can be long but not complex, and observes mostly chronological with possible ashbacks.3. Grammatical Complexity: The text may contain future forms, future in the past, repeated actions, present perfect simple forms.",
  "Knowledge A": ": In contrast to the simple prompting methodused by teachers, the proposed STANDARDIZE frame-work aims to improve the performance of generativemodels for content generation by using the fine-grainedinformation found in expert-defined standards. Theframework involves a three-part process starting with the(i) extraction of target specifications from the prompt,(ii) lookup and retrieval of information that matchesthe target specifications from the specified standard, and(iii) knowledge augmentation to produce artifacts thatrepresent the standard itself for integration into the gen-eration process with generative models. Despite the growing literature of complexalgorithms and architectures for enriching theinstruction-following capabilities of LLMs, themissing puzzle piece that seems to have not gar-nered equal attention from the community is theintegration of actual standards or guidelines craftedby domain experts as a reference of control. Forexample, in education and language assessment,standards such as the Common European Frame-work of Reference for Languages (CEFR) serveas an accredited guide for administrators in chargeof the creation of educational curriculum content.This standard provides fine-grained specificationsof text complexity that different levels of learnerscan understand depending on their language profi-ciency (North, 2007, 2014). To be able to automati-cally generate text content (e.g., narratives or shortstories) using an LLM that is acceptable by CEFR standards and captures a students topic interest atthe same time can serve as a powerful tool in class-room engagement for educators in the long run.Thus, this research gap is an opportunity where thecomplex instruction-following capabilities of lan-guage models can provide assistance, particularlyfor tasks requiring the generation of text contentsince this is one of the areas where these modelsobjectively perform well (Chung et al., 2022; Weiet al., 2021; Gatt and Krahmer, 2018).Towards this end, we tackle the main researchquestion: How can we align large language mod-els for content generation tasks using expert-defined standards? We list our major contribu-tions from this study as follows:",
  ". We introduce STANDARD-CTG, a new taskformalizing the challenge of generating textusing generative language models with expert-defined standards as a for controllability": "2. We propose STANDARDIZE, a new retrieval-style in-context learning framework that ex-tracts knowledge artifacts from standards suchas aspect information, exemplars, and manu-ally crafted linguistic variables to improve theperformances of generative language modelsfor content generation. 3. We introduce significantly improved perfor-mances for GPT-4 and Llama for the taskof STANDARD-CTG using two of the mostwidely recognized academic standards, CEFRand CCS, across diverse evaluation proce-dures.",
  "Background": "According to the International Organization forStandardization (ISO)2, standards are documentedguidelines often containing rich detail in describingrequirements, specifications, and criteria. Theseguidelines are defined and continuously improvedby experts in various domains, such as education,healthcare, and accounting, to name a few. Us-ing standards ensures an institutions products andprocesses are consistent and reproducible (Sadler,2017).In the context of education and language assess-ment, standards are usually in the form of either (a) content standards such as documentations of a com-mon language for ease of communication, writing,and content production, and (b) performance stan-dards such as state-administered tests for readingand mathematical problem-solving competencies.This study focuses on content-based standards usedin education and language assessment to be inte-grated into a generative models text generationprocess. The alignment with existing standards forany generated text material is crucial to ensure qual-ity and consistency before being used in classroomsettings (La Marca et al., 2000).",
  "The Common European Framework of Ref-erence for Languages (CEFR) is one of thewell-knownstandardlanguageframework3": "developed by The Council of Europe and usedfor assessing general language competenciessuch as reading, writing, and listening (North,2007, 2014). The CEFR uses a six-point levelscale of A1, A2, B1, B2, C1, and C2, whichdenotes increasing complexities in instructionalcontent development. We use the level descriptorscompiled by Natova (2021), which cover threeaspects, namely (1) Meaning/Purpose, (2) Struc-ture, and (3) Grammatical Complexity, describingthe characteristics of desired content per levelas shown in . We omit a fourth aspect ofReaders Knowledge Demands from the standardas this heavily depends on the readers backgroundknowledge and is entirely subjective (Forey, 2020;Forey and Cheung, 2019). The Common Core Standards (CCS) is an aca-demic standard4 developed by the US NationalGovernors Association and the Council of ChiefState School Officers (CCSSO) which has beenwidely adopted by schools across the United Statesfor its K-12 curriculum. In this study, we adaptthe recommended model of CCS for assessing textcomplexity, which includes two main variables: (1)Qualitative Dimensions and (2) Quantitative Di-mensions. However, similar to the CEFR standard,",
  "Standard-Aligned Content Generation(STANDARD-CTG)": "Given the importance of adhering to expert-definedstandards in the context of language assessment,we introduce a new task we refer to as standard-aligned content generation (STANDARD-CTG).The overarching goal of STANDARD-CTG is topave the way for new approaches that aim to in-tegrate the conventional methodologies of con-trollable text generation in NLP with actual con-straints provided by domain experts across interdis-ciplinary fields such as education, engineering, andmedicine through documented standards. To alignwith terminologies used in education and other non-computing literature, in this work, we use the termcontent generation instead of text generation asusually seen in technical NLP literature.We represent the task of STANDARD-CTG usingthe following formulation:",
  "= L(M(X, KStandard), E)(1)": "where L is a general evaluator that tests howclose a language models M generated content Xis with gold-standard examples E through learningtransformed knowledge representations KStandardof the selected standard DStandard. The evaluatorL can assume many forms, including model-based,distance-based, and reference-based scoring. Wepattern our major experiments in the succeedingsections based on this formulation.",
  "The STANDARDIZE Framework": "Given that expert-defined standards are naturallyinformation-rich, lengthy, and complex, our mainhypothesis in this study is that in order for a gen-erative language model to produce content that isaligned with the specifications provided by a stan-dard, the information found in the standard mustbe considered in the generation process. The chal-lenge then is redirected towards how any informa-tion extracted can be represented as something thatthe generative model will find useful.Towards addressing STANDARD-CTG, wepropose STANDARDIZE, a retrieval-style in-context learning-based framework that exploits the richinformation found in standards and transforms thisinto knowledge artifacts to improve the quality ofcontent produced by generative models. encapsulates this framework in a visual manner. Inthe succeeding sections, we discuss the proposedSTANDARDIZE framework more thoroughly. Target Specification Extraction is performedfirst to obtain informative tags in the prompt andto correctly match this information within thestandards. For academic standards in languageassessment, these specifications should provideinformation about who will be content delivered to(target audience) and using what specific standardout of many (CEFR or CCS). Thus, these twoinformation tags are the basic required input forthe process. As an example shown in , theextracted specifications provided in the prompt areA2 readers, which points to a particular group oflearners requiring low-leveled reading materials,and CEFR scale, which denotes the selectedstandard where properties of A2-level texts aredescribed. Specification Lookup and Retrieval is thenperformed next upon extracting the target specifi-cations. A lookup process is done to find a matchwith the selected standard, usually in the form of adatabase or an external machine-readable file. Inthis work, we simply transformed the level-specificdescriptors from Natova (2021) into a .csv file.The information from the standard in the form ofaspects (or characteristics) that match the targetspecifications is then retrieved. The length andcomplexity of a standards level of informationregarding its specifications may vary. As shownin for the CEFR standard, the retrievedinformation that matches the desired level ofcomplexity for the target audience (A2 readers)can be checked at . Knowledge Augmentation is done last but is themost important process of the pipeline. We proposea further technical augmentation of informationfound in standards to obtain knowledge artifacts inthe prompts. These knowledge artifacts can rangefrom simple additional information already presentin the standard to complex representations, suchas incorporating actual linguistic features to con-trol the granularity of the generation process. Re- cent works surveying the performance of open andclosed models have shown that non-informativestyle of prompting language models, such as theteacher style shown in , is effective only toa certain extent and may be biased towards contentgeneration in lower levels, such as A2 or B1 in theCEFR standards (Imperial and Madabushi, 2023;Ribeiro et al., 2023).",
  "Knowledge Artifacts for STANDARDIZE": "In this section, we discuss the knowledge artifactsKStandard extracted from the two educationalstandards DStandard used in the STANDARDIZEframework and how they are integrated into thegeneration setup via in-context learning. Baseline (Teacher Style) We treat the TeacherStyle method as seen in , where asimple, non-enriched prompt contains the targetcategory from each standard, as the baseline forperformance.We use this term in observanceof how non-technical users, especially teachers,interact with generative chat interfaces (Imperialand Tayyar Madabushi, 2023). Aspect Information (STANDARDIZE-A) repre-sents the specific descriptive information providedin the standard. In the context of standards forcontent generation, aspect information is generallyattributed to linguistic criteria of content withrespect to its target audience. shows howaspect information from a standard (e.g., CEFR)can be integrated into the actual prompt.Theaddition of aspect criteria information ensures thatthe generative model will have access to explicitcharacteristics of the desired generated content indifferent dimensions.",
  "Linguistic Flags (STANDARDIZE-L) represent thecontrollable attribute-based variables of a standardthat a generative model can use to steer the di-rection of content generation. In the STANDARD-": "IZE framework, this process serves as a rewritefunction where a generative model is asked to pro-duce an initial content first using another methodprompting (e.g., aspect information in ),and rewrites this by comparing linguistic flag val-ues of the initially generated content against themean value of a gold standard dataset of the targetlevel. An example is illustrated in wherethe mean type-token ratio of a collection of gold- Given this prompt: In the dark old forest up ahead, a solitary gure emerged from the corner of the... Continue the story and make sure they are readable for B1 learners in the CEFR scale and observes the following specications: 1. Meaning or Purpose: The text is clear and concrete, and tells a simple story.2. Structure: The text is can be long but not complex, and observes mostly chronological with possible ashbacks.3. Grammatical Complexity: The text may contain future forms, future in the past, repeated actions, present perfect simple forms.",
  "Aspect Criteria": ": A standard contains recommended character-istics of content across one or more domain-specificaspects or criteria. This figure shows an example of theCEFR standard where the set of criteria includes depthof meaning, structure, and grammatical complexity. Given this story: In the dark old forest up ahead, a solitary gure emerged from the corner of the... Rewrite the story and make sure they are readable for B1 learners in the CEFR scale. Use the following linguistic features to reach the target level of the story:",
  "Linguistic Flags": ": A standard contains aspect definition whichcan be represented by flags such as linguistic variables.Given the mean values from gold-standard data in thetarget level, the generative model can then be steered topush the property of its generated content using direc-tional instructions such as increase or decrease. standard B1-level text 12.5 is added to the promptwhile being compared to the current type-tokenvalue of the story, which is 4.2. A verbalizer isused to transform the computed linguistic flags intonatural language prompts. The keywords increaseand decrease are used in constructing the promptsto provide a sense of direction for the generativemodel.In this work, we select 2 to 4 linguistic flagsfor both CEFR and CCS as reported in .The selection of what linguistic flags to use canbe as simple as referring to what the definitions of",
  "Exemplars": ": A standard contains recommended exemplarsthat serve as gold-standard reference. This figure showsan example of the CEFR standard where three well-known pieces of literature are provided as examples ofcontent that conforms to the target level specified (B1). aspects provide and need not be exhaustively many.For example, in CEFR, the Organization aspect isdefined through different levels as \"text is oftenshort and observes chronological and predictablestructure\" for A2 and \"text is can be long but notcomplex\" for B1. Thus, we select average sentenceand word lengths as a linguistic flag to capturethis aspect. The full table of average values oflinguistic flags can be found in Appendix A.5. Exemplars (STANDARDIZE-E) represent therecommended examples by experts or developersof standards for reference of users. The additionof exemplars or any artifact found in the standardthat showcases gold-standard output allows thegenerative model to have a sense of implicitknowledge during the content generation process.For example, in , the exemplars for aB1-level content include Frankenstein by MaryShelley, a well-known piece of gothic fiction.Although indirectly, any large language modeltrained using internet data (e.g., Wikipedia dumps)may have already formed a sense of knowledgeof how this literature looks like (Karamolegkouet al., 2023; Petroni et al., 2019).We use theactual recommended exemplars from the CCSwhile we collected exemplars from the PenguinReaders publishing platform5 which providesexpert-curated literature for CEFR. The full list ofexemplars for both standards can be found in theAppendix A.4.",
  "Tasks and Datasets": "For this study, we specifically center our ex-perimentation on the general task of story ornarrative generation. We consider the subfieldsrich literature and active research community inNLP (Alhussain and Azmi, 2021), as well as beingone of the most common examples demonstratedacross the education community regarding theuse of generative text interfaces for contentgeneration (Kasneci et al., 2023; Whalen et al.,2023). Further, we differentiate two tasks usedin our work for narrative generation as listed below. Task 1: Context Assisted Story Generation.For this setup, we provide preliminary contextin the form of 50 to 70 words (or approximately3 to 5 sentences) in the prompt to guide thegenerative language model in producing thestory continuation. We select the CEFR as thestandard of choice to evaluate this approachand use the European Language Grid (ELG)corpus67 compiled by Breuker (2022) to constructthe prompts. The balanced corpus contains 300CEFR-aligned English texts produced by expertsand distributed across five levels A2, B1, B2, C1,C2 with 60 instances each. A1 is omitted due tolack of resources (n < 20). Task 2: Theme Word Story Generation. In con-trast to the previous setup, this method introducesonly a single theme word for the generative lan-guage to produce a narrative from scratch, whichallows for increased diversity in the content (Dazaet al., 2016; Peng et al., 2018).To compile atheme words list, we select 50 random Englishnoun words in plural form (e.g., dragons, myster-ies, voyages) from the Corpus of ContemporaryAmerican English (COCA) (Davies, 2009) andprompt the generative model iteratively for each",
  "Models": "We select a number of generative language mod-els M for content generation, each with its ownadvantage. For the open models, we use a num-ber of well-known models in the 2B-7B range, in-cluding Llama2-Chat-7B (Touvron et al., 2023a),OpenChat-7B (Wang et al., 2023), and Longform-2.7B (Kksal et al., 2023). For the closed model,we use GPT-4-Turbo (OpenAI, 2023). More infor-mation on the models can be found in AppendixA.3.",
  "We perform a diverse set of evaluation methodsL given examples from gold-standard datasets Eto test the qualities of the generated content ofmodels, as discussed further below": "Model-Based Classifiers. For the context-assistedstory generation task using CEFR standards with5 classes, we use a Random Forest classifiertrained from a separate collection of CambridgeExams dataset with CEFR labels used in theworks of Xia et al. (2016) and Imperial andTayyar Madabushi (2023). This classifier has anaccuracy of 0.912 using 79 length-normalized8 linguistic features.For the theme word storygeneration using CCS standards with 2 classes,we used an XGBoost classifier from the workof (Imperial, 2021) trained from the only CCS-aligned data found online and compiled by Floret al. (2013) with an accuracy of 0.917 using acombination of BERT embeddings and the samelinguistic features stated above. Due to its limitedsize of 168, we grouped the dataset into binarycategories, elementary (grades 48) and advanced(grades 9 12), with 48 and 73 documentsper class, respectively. We consider both classi-fiers in our work for their high accuracies (> 90%). Fluency and Diversity. We evaluate the levelof fluency and content diversity of the generatedcontent by the models as done in previous narrativegeneration works (DeLucia et al., 2021; See et al.,2019). The former is measured through perplexity",
  "with an external GPT-2 model, while the latter isthe density of distinct n-grams": "Linguistic Similarity.We evaluate the levelof linguistic similarity of the generated contentagainst the gold-standard datasets for CEFR (ELG)and CCS (COCA) as mentioned in . Forthis method, we calculate the mean Euclideandistance of all the linguistic flags used for bothstandards and their levels listed in . Thismethod provides a notion of how close thecharacteristics of a set of model-generated texts(e.g., GPT-4 generated B1 texts) is to its equivalentgold standard (e.g., actual B1-level texts written byexperts).",
  "Expert Annotator Evaluation": "To confirm the quality of model-generated content,we also perform an evaluation using judgmentfrom domain experts.Through our universitynetwork, we collaborated with three experts with15 30 years of experience in linguistic andlanguage assessment with frameworks such asCEFR, CCS, TOEFL, and IELTS. Drawing onthe methods used in previous studies (DeLuciaet al., 2021), we asked the experts to judge themodel-generated content through the followingvariables below. Additional information on thehuman evaluation can be found in Appendix A.6. Grammaticality and Coherence.The formervariable evaluates the level of naturalness orfluency of the generated output as if it has beenwritten by a native English speaker. The lattermeasures the level of cohesion between sentenceswhere the narrative stays on-topic, and the textoverall builds a consistent story and the flow ofinformation is smooth and easy to follow. Grade Complexity Distinction.This variablemeasures the obviousness of the complexity of agenerated story on a target level (e.g., A1) withrespect to another story of a different level (e.g.,A2). This variable is relatively more challengingthan the other metrics, as the difference betweenadjacent levels may not be as straightforward with-out referring to the quantitative characteristics ofthe texts. However, we included this assessment inthe evaluation process to judge the quality of themodel-generated texts.",
  "ModelPreciseAccuracyAdjacentAccuracyFluency(perplexity)Diversity(distinct-n)": "Llama2 7B- Teacher Style0.2030.63613.189 4.880.156 0.03- STANDARDIZE-A0.2700.62613.694 7.740.155 0.02- STANDARDIZE-E0.3200.68315.576 3.310.188 0.01- STANDARDIZE-L0.2730.60620.175 4.470.186 0.01- STANDARDIZE-0.3540.67017.892 3.940.193 0.01 OpenChat 7B- Teacher Style0.2370.62622.039 7.700.170 0.02- STANDARDIZE-A0.2430.63021.195 7.660.171 0.02- STANDARDIZE-E0.2530.60013.931 2.970.178 0.01- STANDARDIZE-L0.2700.54618.182 8.520.179 0.02- STANDARDIZE-0.2530.59612.806 2.700.171 0.03 Longform 3B- Teacher Style0.2300.60618.209 6.010.159 0.02- STANDARDIZE-A0.2230.61017.982 9.210.157 0.02- STANDARDIZE-E0.2570.49625.075 8.800.192 0.11- STANDARDIZE-L0.2830.58616.926 6.910.161 0.03- STANDARDIZE-0.2770.54316.806 7.400.170 0.04 GPT-4- Teacher Style0.2270.63027.357 6.300.187 0.08- STANDARDIZE-A0.3970.84629.729 9.580.174 0.01- STANDARDIZE-E0.3070.70330.357 9.790.182 0.01- STANDARDIZE-L0.4800.90624.115 7.040.194 0.03- STANDARDIZE-0.5400.80322.591 1.610.218 0.05",
  "ModelPreciseAccuracyFluency(perplexity)Diversity(distinct-n)": "Llama2 7B- Teacher Style0.47017.936 4.320.184 0.01- STANDARDIZE-A0.58022.070 1.750.171 0.01- STANDARDIZE-E0.57013.484 2.500.193 0.01- STANDARDIZE-L0.72015.066 2.470.191 0.01- STANDARDIZE-0.62314.707 2.400.193 0.01 OpenChat 7B- Teacher Style0.47016.116 12.390.166 0.05- STANDARDIZE-A0.55019.444 2.570.172 0.01- STANDARDIZE-E0.49012.438 1.850.178 0.01- STANDARDIZE-L0.58013.734 2.530.180 0.01- STANDARDIZE-0.56010.717 1.530.169 0.01 Longform 3B- Teacher Style0.50013.657 5.390.154 0.04- STANDARDIZE-A0.45017.918 4.740.148 0.01- STANDARDIZE-E0.51014.277 2.790.151 0.02- STANDARDIZE-L0.61013.398 3.930.148 0.04- STANDARDIZE-0.62010.400 1.530.169 0.01 GPT-4- Teacher Style0.59032.447 7.460.195 0.01- STANDARDIZE-A0.55031.765 11.300.169 0.01- STANDARDIZE-E0.52029.912 6.810.184 0.01- STANDARDIZE-L0.61026.912 6.110.155 0.01- STANDARDIZE-0.79021.277 4.500.198 0.01",
  "Standard Alignment via ClassificationPerformance": "The overall performance of models for CEFR andCCS are reported in Tables 1 and 2. For CEFR,the top-performing setup across the four modelsall belong to the STANDARDIZE framework. Wereport over a 100% increase in performance usingthe best setup with GPT-4 with STANDARDIZE-in precise accuracy from 0.227 to 0.540 and a 43%increase for adjacent accuracy from 0.630 to 0.906compared to the teacher style method. ThroughSTANDARDIZE, open models also gained substan-tial boosts in performance, such as Longform upby 23%, OpenChat up by 14%, and Llama2 by74%. In terms of adjacent accuracies, GPT-4 re-mained the best model for preserving the ordinal-ity of the labels with 0.906, up by 44%. WithCCS, the general scores obtained in this setup arehigher compared to CEFR with five classes due tobinary labeling. We see a similar pattern whereall open and closed models obtained the best per-formance, with boosts ranging from 3% to 45%using linguistic flags STANDARDIZE-L and a com-",
  "Standard Alignment via LinguisticSimilarity": "We visualize the distributions of the best perform-ing STANDARDIZE methods in Figures 6 to 8 withcomparison to the teacher style method. From theresults, we observe that the general trend of usingSTANDARDIZE produces a more stable distribu-tion across the variables it is explicitly controllingfor (e.g., average sentence length or type token di-versity as listed in ), particularly with theCCS standards. We also notice that the distribu-tions using STANDARDIZE-L also produce distri-butions closer to the mean (represented as a yellowstar) from their corresponding gold-standard data.Moreover, in terms of linguistic similarity, as re-ported in , STANDARDIZE makes the qualityof model generations more similar to the linguis-tic characteristics of the gold standard datasets in CEFR and CCS. Overall, these findings furtherstrengthen the evidence of using STANDARDIZEin producing linguistically similar content withgold-standard data compared to the conventionalteacher style method.",
  "Assessment of Generation Qualities viaExpert Judgment and Automatic Metrics": "For both computed fluency and content diversity,we see similar results from the previous evaluationtechniques where the best performing models areall models improved through the STANDARDIZEframework particularly OpenChat, Longform, andGPT-4. Looking at expert evaluations as reportedin , we observe consistent high ratings ongrammaticality and coherence of the topi perform-ing model, GPT-4 with STANDARDIZE-, for bothCEFR and CCS with an average of 3.13 and 3.35,respectively. On the grade complexity distinction,all three expert evaluators were able to achieve highaccuracies (> 0.70) in selecting correct simple andcomplex texts from the model-generated data, de-noting the obviousness of complexity. Likewise, allexpert evaluation tests achieved strong inter-raterreliability scores (> 0.30) through Kendalls W(Kendall, 1948). With these findings, we affirmthe effectivity of the STANDARDIZE frameworkthrough expert judgment on generating morefluent, grammatical, grade-distinct, and diversecontent compared to the teacher-style approach.",
  "Validity on Global Education Context.Ourmain contribution, the STANDARDIZE framework,leverages the idea of a more holistic method": "for capturing the intricacies and complexities ofeducational standards for content generation. Ourexperiments with the CEFR and CCS standardsshowcase an opportunity for the generated texts oflanguage model interfaces such as GPT-4, whichare commonly used by educators and teachers, tobe aligned with international language proficiencylevels. Moreover, showing the effectiveness ofSTANDARDIZE on the aforementioned interna-tionally recognized academic standards usedin European and Northern American schoolssignifies the frameworks strong potential forcross-curricula application. Thus, we invite futureresearchers to explore, validate, and proposederivations of our base framework for their ownlanguages and language-specific standards forcontent generation. Towards More Personalized Content Genera-tion. Investigating the potential of generative mod-els for personalized learning, such as providingadaptive feedback aligned with students needs, isan active area in AI for education (Kasneci et al.,2023; Meyer et al., 2023; Sailer et al., 2023; Tackand Piech, 2022). This work contributes towardthe goal of helping educators craft more personal-ized content for learners using the capabilities oflarge language models based on an assigned lan-guage proficiency level described by a standard.While we present a novel task specifically targetedfor the NLP community to encourage research inthis direction (STANDARD-CTG as covered in Sec-tion 3), our results may be useful for educators byproviding context on better methods for generatinglevel or target audience-specific texts by prompt-ing language models using information found ineducational standards.",
  "Related Work": "Research in complexity-controlled generation hasexplored diverse variables in terms of text for-mat, granularity, and task variation. The work ofAgrawal and Carpuat (2019) introduced controllingfor specific complexity in the machine translationtask. The following works of Agrawal and Carpuat(2023) and Ribeiro et al. (2023) explored grade-specific text simplification and summarization us-ing control tokens and reinforcement learning, re-spectively. Currently, only two works have inves-tigated incorporating CEFR for language learningcontent generation. Stowe et al. (2022) and Impe-",
  "Conclusion": "In this work, we proposed the STANDARDIZEframework using knowledge artifacts that allowedlarge language models such as Llama2 and GPT-4 to gain significant performance boosts (45% -100%) on generating content aligned with educa-tional standards as well as preserving importantnarrative qualities such as fluency, grammaticality,coherence, and grade distinctness. From this, wesee a very promising potential for cross-domainand cross-standard generalization of our proposed",
  "Ethical Considerations": "All datasets and corpora used in this study, suchas the ELG (Breuker, 2022), Cambridge Exams(Xia et al., 2016), and CCS (Flor et al., 2013), arealready established and accessible for research pur-poses. We observe a specific tone in the discussionof our experiments, emphasizing that the main mo-tivation of the work is that language models such asGPT-4 can provide assistance in producing contentthat is more aligned or faithful with the constraintsof standards such as CEFR or CCS without im-plying that they can replace experts in the field orproduce better quality than the gold-standard data.Further, we also do not imply that any model en-riched by any computational method to producemore standard-aligned content can replace the stan-dard itself. Overall, we do not foresee any seriousethical issues in this study.",
  "Limitations": "Language Coverage of Standards. This workis mainly centered on the use of datasets andstandards for the English language.Whilestandards for language assessment, such as CEFR,have expanded through the years with versions tocover other languages, such as German, Czech,and Italian (Vajjala and Rama, 2018), we do notclaim that our results will be able to generalize and",
  "have the same advantages with these languages.However, investigating this direction may be agood research opportunity for future work": "Dependence on Evaluation Methods.Asobserved in , we made sure to covera variety of evaluation procedures for testingstandard alignment instead of only using model-based methods such as a classifier. The limitationhere is that trained classifiers are dependent onfactors such as their accuracy, the quantity ofdata, the complexity of the training algorithm,and the quality of features. Thus, other means ofevaluating alignment that is more direct, such ascomputed feature distances against a gold-standarddataset, is always recommended. Moreover, ourmodel-based CEFR and CCS evaluators make useof artifacts such as datasets and tools for featureextraction from peer-reviewed papers (Xia et al.,2016; Flor et al., 2013). We are aware of paidthird-party services online that promise moreaccurate classification of labels in CEFR, butthey generally do not provide details on linguisticpredictors used for prediction. Thus, this may notbe a practical option for research. Attribute-Based Standards. The standards usedin this study, CEFR and CCS, are attribute-basedstandards that specify recommended characteristicsof texts that are countable (e.g., sentence length oraverage number of words). These specificationscontribute towards the overall complexity of textswhich are within the scope of CEFR and CCS. Stan-dards in other domains may come in different formsof constraints, such as dependence on an exter-nal specialized vocabulary or following specificsequential processes to arrive at a result. More-over, our exploration of CEFR and CCS standardsis centered on the downstream task of narrative gen-eration, as this fits the most generic form of readingmaterial in classrooms. We leave the exploration ofextending the STANDARDIZE framework to otherdomains that also observe attribute-based specifica-tions as well as other adjacent text generation tasks(e.g., summary generation) in future work.",
  "We are grateful to the anonymous reviewers andAction Editors in ARR for their feedback on theimprovement of this paper and to Dr. Brian Northfor the insightful discussions on capturing language": "standards, including CEFR, as part of the theoret-ical component of this work. We also thank Dr.Samantha Curle and Dr. Reka Jablonkai from theDepartment of Education at the University of Bathfor helping with the evaluation of model-generatedtexts. This work made use of the Hex GPU cloudof the Department of Computer Science at the Uni-versity of Bath. JMI is supported by the NationalUniversity Philippines and the UKRI Centre forDoctoral Training in Accountable, Responsible,and Transparent AI [EP/S023437/1] of the Uni-versity of Bath. We attribute the black icons usedin to the collections of Design Circle andVictor Zukeran from the Noun Project and the col-ored teacher icon from Flaticon. Sweta Agrawal and Marine Carpuat. 2019. ControllingText Complexity in Neural Machine Translation. InProceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the9th International Joint Conference on Natural Lan-guage Processing (EMNLP-IJCNLP), pages 15491564, Hong Kong, China. Association for Computa-tional Linguistics. Sweta Agrawal and Marine Carpuat. 2023. Control-ling Pre-trained Language Models for Grade-SpecificText Simplification. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1280712819, Singapore. Associ-ation for Computational Linguistics.",
  "Mark Davies. 2009. The 385+ million word Corpusof Contemporary American English (19902008+):Design, architecture, and linguistic insights. Interna-tional Journal of Corpus Linguistics, 14(2):159190": "Angel Daza, Hiram Calvo, and Jess Figueroa-Nazuno.2016. Automatic Text Generation by Learning fromLiterary Structures. In Proceedings of the Fifth Work-shop on Computational Linguistics for Literature,pages 919, San Diego, California, USA. Associa-tion for Computational Linguistics. Alexandra DeLucia, Aaron Mueller, Xiang Lisa Li, andJoo Sedoc. 2021. Decoding Methods for NeuralNarrative Generation. In Proceedings of the 1st Work-shop on Natural Language Generation, Evaluation,and Metrics (GEM 2021), pages 166185, Online.Association for Computational Linguistics. Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-erarchical Neural Story Generation. In Proceedingsof the 56th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 889898, Melbourne, Australia. Associationfor Computational Linguistics. Michael Flor, Beata Beigman Klebanov, and Kath-leen M. Sheehan. 2013. Lexical Tightness and TextComplexity. In Proceedings of the Workshop onNatural Language Processing for Improving TextualAccessibility, pages 2938, Atlanta, Georgia. Associ-ation for Computational Linguistics.",
  "Albert Gatt and Emiel Krahmer. 2018. Survey of theState of the Art in Natural Language Generation:Core tasks, applications and evaluation. Journal ofArtificial Intelligence Research, 61:65170": "Joseph Marvin Imperial. 2021. BERT embeddings forautomatic readability assessment. In Proceedings ofthe International Conference on Recent Advances inNatural Language Processing (RANLP 2021), pages611618, Held Online. INCOMA Ltd. Joseph Marvin Imperial and Harish Tayyar Madabushi.2023. Uniform Complexity for Text Generation. InFindings of the Association for Computational Lin-guistics: EMNLP 2023, pages 1202512046, Singa-pore. Association for Computational Linguistics. Joseph Marvin Imperial and Harish Tayyar Madabushi.2023. Flesch or fumble? evaluating readability stan-dard alignment of instruction-tuned language mod-els. In Proceedings of the Third Workshop on Natu-ral Language Generation, Evaluation, and Metrics(GEM), pages 205223, Singapore. Association forComputational Linguistics. Antonia Karamolegkou, Jiaang Li, Li Zhou, and An-ders Sgaard. 2023. Copyright Violations and LargeLanguage Models. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 74037412, Singapore. Associa-tion for Computational Linguistics.",
  "Paul M La Marca, Doris Redfield, and Phoebe C Winter.2000. State Standards and State Assessment Sys-tems: A Guide to Alignment. Series on Standardsand Assessments": "Bruce W. Lee and Jason Lee. 2023. LFTK: HandcraftedFeatures in Computational Linguistics. In Proceed-ings of the 18th Workshop on Innovative Use of NLPfor Building Educational Applications (BEA 2023),pages 119, Toronto, Canada. Association for Com-putational Linguistics. Jesse G Meyer, Ryan J Urbanowicz, Patrick CN Mar-tin, Karen OConnor, Ruowang Li, Pei-Chen Peng,Tiffani J Bright, Nicholas Tatonetti, Kyoung Jae Won,Graciela Gonzalez-Hernandez, et al. 2023. Chatgptand large language models in academia: opportuni-ties and challenges. BioData Mining, 16(1):20.",
  "OpenAI. 2023. GPT-4 Technical Report. arXiv preprintarXiv:2303.08774": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al.2022. Training language models to follow instruc-tions with human feedback.Advances in NeuralInformation Processing Systems, 35:2773027744. Nanyun Peng, Marjan Ghazvininejad, Jonathan May,and Kevin Knight. 2018. Towards Controllable StoryGeneration. In Proceedings of the First Workshop onStorytelling, pages 4349, New Orleans, Louisiana.Association for Computational Linguistics. Fabio Petroni, Tim Rocktschel, Sebastian Riedel,Patrick Lewis, Anton Bakhtin, Yuxiang Wu, andAlexander Miller. 2019. Language Models as Knowl-edge Bases?In Proceedings of the 2019 Confer-ence on Empirical Methods in Natural Language Pro-cessing and the 9th International Joint Conferenceon Natural Language Processing (EMNLP-IJCNLP),",
  "pages 24632473, Hong Kong, China. Associationfor Computational Linguistics": "Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,Amnon Shashua, Kevin Leyton-Brown, and YoavShoham. 2023.In-Context Retrieval-AugmentedLanguage Models. Transactions of the Associationfor Computational Linguistics, 11:13161331. Leonardo F. R. Ribeiro, Mohit Bansal, and MarkusDreyer. 2023. Generating Summaries with Control-lable Readability Levels. In Proceedings of the 2023Conference on Empirical Methods in Natural Lan-guage Processing, pages 1166911687, Singapore.Association for Computational Linguistics.",
  "D Royce Sadler. 2017. Academic achievement stan-dards and quality assurance. Quality in Higher Edu-cation, 23(2):8199": "Michael Sailer, Elisabeth Bauer, Riikka Hofmann, JanKiesewetter, Julia Glas, Iryna Gurevych, and FrankFischer. 2023. Adaptive feedback from artificial neu-ral networks facilitates pre-service teachers diagnos-tic reasoning in simulation-based learning. Learningand Instruction, 83:101620. Victor Sanh, Albert Webson, Colin Raffel, StephenBach, Lintang Sutawika, Zaid Alyafeai, AntoineChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,et al. 2021. Multitask Prompted Training EnablesZero-Shot Task Generalization. In International Con-ference on Learning Representations. Abigail See, Aneesh Pappu, Rohun Saxena, AkhilaYerukola, and Christopher D. Manning. 2019. DoMassively Pretrained Language Models Make BetterStorytellers?In Proceedings of the 23rd Confer-ence on Computational Natural Language Learning(CoNLL), pages 843861, Hong Kong, China. Asso-ciation for Computational Linguistics. Kevin Stowe, Debanjan Ghosh, and Mengxuan Zhao.2022. Controlled Language Generation for LanguageLearning Items. In Proceedings of the 2022 Confer-ence on Empirical Methods in Natural Language Pro-cessing: Industry Track, pages 294305, Abu Dhabi,UAE. Association for Computational Linguistics. Anas Tack and Chris Piech. 2022. The AI Teacher Test:Measuring the Pedagogical Ability of Blender andGPT-3 in Educational Dialogues. In Proceedings ofthe 15th International Conference on EducationalData Mining, page 522. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, YannDubois, Xuechen Li, Carlos Guestrin, Percy Liang,and Tatsunori B Hashimoto. 2023.Alpaca: AStrong, Replicable Instruction-Following Model.Stanford Center for Research on Foundation Models. stanford. edu/2023/03/13/alpaca. html,3(6):7.",
  "Azhar, et al. 2023a.LLaMA: Open and Effi-cient Foundation Language Models. arXiv preprintarXiv:2302.13971": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023b.Llama 2: Open Founda-tion and Fine-Tuned Chat Models. arXiv preprintarXiv:2307.09288. Sowmya Vajjala and Taraka Rama. 2018. Experimentswith Universal CEFR Classification. In Proceedingsof the Thirteenth Workshop on Innovative Use ofNLP for Building Educational Applications, pages147153, New Orleans, Louisiana. Association forComputational Linguistics.",
  "Guan Wang, Sijie Cheng, Xianyuan Zhan, XiangangLi, Sen Song, and Yang Liu. 2023. OpenChat: Ad-vancing Open-source Language Models with Mixed-Quality Dataa. arXiv preprint arXiv:2309.11235": "Yizhong Wang, Swaroop Mishra, Pegah Alipoormo-labashi, Yeganeh Kordi, Amirreza Mirzaei, AtharvaNaik, Arjun Ashok, Arut Selvan Dhanasekaran,Anjana Arunkumar, David Stap, Eshaan Pathak,Giannis Karamanolakis, Haizhi Lai, Ishan Puro-hit, Ishani Mondal, Jacob Anderson, Kirby Kuznia,Krima Doshi, Kuntal Kumar Pal, Maitreya Patel,Mehrad Moradshahi, Mihir Parmar, Mirali Purohit,Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma,Ravsehaj Singh Puri, Rushang Karia, Savan Doshi,Shailaja Keyur Sampat, Siddhartha Mishra, SujanReddy A, Sumanta Patro, Tanay Dixit, and XudongShen. 2022. Super-NaturalInstructions: Generaliza-tion via declarative instructions on 1600+ NLP tasks.In Proceedings of the 2022 Conference on Empiri-cal Methods in Natural Language Processing, pages50855109, Abu Dhabi, United Arab Emirates. As-sociation for Computational Linguistics. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,Adams Wei Yu, Brian Lester, Nan Du, Andrew MDai, and Quoc V Le. 2021. Finetuned LanguageModels are Zero-Shot Learners. In InternationalConference on Learning Representations.",
  "Jeromie Whalen, Chrystalla Mouza, et al. 2023. Chat-GPT: Challenges, Opportunities, and Implicationsfor Teacher Education. Contemporary Issues in Tech-nology and Teacher Education, 23(1):123": "Menglin Xia, Ekaterina Kochmar, and Ted Briscoe.2016. Text Readability Assessment for Second Lan-guage Learners. In Proceedings of the 11th Workshopon Innovative Use of NLP for Building EducationalApplications, pages 1222, San Diego, CA. Associa-tion for Computational Linguistics. Susan Zhang, Stephen Roller, Naman Goyal, MikelArtetxe, Moya Chen, Shuohui Chen, Christopher De-wan, Mona Diab, Xian Li, Xi Victoria Lin, et al.2022. OPT: Open Pre-trained Transformer LanguageModels. arXiv preprint arXiv:2205.01068. Wangchunshu Zhou, Yuchen Eleanor Jiang, EthanWilcox, Ryan Cotterell, and Mrinmaya Sachan. 2023.Controlled text generation with natural language in-structions. In Proceedings of the 40th InternationalConference on Machine Learning, volume 202 ofProceedings of Machine Learning Research, pages4260242613. PMLR.",
  "We set the minimum generated new tokens to 30and the maximum to 300, as well as set the nucleus": "sampling decoding (top-p) to 0.95 as done withprevious works on story generation (Imperial andMadabushi, 2023; DeLucia et al., 2021; See et al.,2019). The actual sizes of the open models rangefrom 5GB to 15 GB max. We used a hosted GPUcloud with 4 NVIDIA Ti 3090 with 24GB memorysize for model inference. Llama2-Chat (Touvron et al., 2023b) is one ofthe community-recognized open instruction-tunedmodels released by Meta and an improved versionof Llama 1 (Touvron et al., 2023a). For this task,we use the 7B version9 finetuned from over amillion human preference data and optimizedfor chat and dialogue use cases. We prioritizedthe addition of this model in our study for itsaccessibility to the general NLP community. Longform-OPT (Kksal et al., 2023) is a recentinstruction-tuned model optimized for long textgeneration using the LongForm dataset. For thisstudy, we use the OPT model variant10 (Zhanget al., 2022) with 2.7B parameters as this versionobtained the best performance for the short storygeneration task using the WRITINGPROMPTSdataset (Fan et al., 2018) against other instruction-tuned models such as Alpaca-LLaMA (Taori et al.,2023), FlanT5 (Chung et al., 2022), Tk-Instruct(Wang et al., 2022), and T0++ (Sanh et al., 2021). OpenChat (Wang et al., 2023) is the most recentopen model in our experiment setup, whichcurrently is reported to be the best 7B model asof this writing and outperforms closed modelssuch as ChatGPT (March) across a number ofbenchmark tasks such as GSM8K and TruthfulQA.In contrast to Llama and GPT models, which usedRLHF (Ouyang et al., 2022), OpenChat is trainedwith mixed-quality data which is composed ofhigh-quality expert data and sub-optimal web datawith no preference labels. We use the 7B version11",
  "A.6Additional Information on HumanExpert Evaluation": "We created and distributed the evaluation instru-ment through QuestionPro ( In contrast to non-expertvalidation techniques where all instances are dis-tributed automatically to available annotator plat-forms such as Amazon Turk, we use a represen-tative random sample of our data for evaluationin consideration with the experts time constraints.For all tests, we randomly sampled 10% of thetotal generated narrative content using the best-performing model, which is both the GPT-4 modelwith STANDARDIZE-, for each corresponding taskassociated with CEFR and CCS as described in.For grammaticality and coherence evaluation,we adapted the same four-point Likert scale fromthe work of DeLucia et al. (2021) for evaluatingselect model-generated content found through thislink: Snapshotsof the instruction and test instances presented toexperts for evaluation can be viewed in Figures 10and 11.For the grade complexity distinction, we adapteda simpler select-one response type where for eachtest instance being evaluated, we select a randomtest instance from the adjacent next level of thetarget test instance and ask the experts to selectwhich two examples of model-generated contentare more simpler or complex. The idea here is thatthe expert should be able to tell the obviousness ofthe complexity of the test instance by indicatingwhich is simpler or more complex. Snapshots of theinstruction and test instances presented to expertsfor evaluation can be viewed in Figures 12 and 13.Overall, our human evaluation design has beenvalidated by the experts in language assessment wecollaborated with through preliminary discussionson the scope, instrument, target outcomes, and pre-sentation of the results from the task. As a formof compensation, we offered 30 upon completionof the entire task, which the experts took about ap-proximately 3045 minutes. The experts will alsobe acknowledged in this paper upon publication. A2B1B2C1C2 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0Teacher-StyleStandardize Grade 4 - 8Grade 9 - 12 Teacher-StyleStandardize",
  "A2The text is clear and concrete, aiming to describeappearance, places, routines, preferences, or tell asimple story": "The text is often short and observeschronological and predictable structure.The text contains comparison of adjectives, rel-ative clauses, quantifiers, past simple of to beand full verbs, passive voice of present andpast simple. B1The text is clear and concrete, aiming to describeappearance, places, routines, preferences, or tell asimple story. The text may also provide opinionsand instructions or explanations, easy to understandand visualise, excluding ambiguity and diverse in-terpretations.",
  "DescriptionThe text can range from containing a sin-gle level of meaning to multiple levels ofmeaning based on complexity": "A text with low complexity tends to have simple,well-marked, and conventional structures, whereasa text of high complexity tends to have complex, im-plicit, and unconventional structures. Simple textstend to relate events in chronological order, whilecomplex texts make more frequent use of flashbacks,flash-forwards, and other manipulations of time andsequence. That text that has longer words and longersentences are more difficult to read thanshorter ones.A text with many longwords and/or sentences is thus rated bythese formulas as harder to read than atext with many short words and/or sen-tences would be.",
  "TARGET LEVEL: A2 (Elementary / Beginning Reading Level)": "CONTEXT: Corryvale Farm is in the west of England. Stephen and Jenny Blackler keep cows and chickens and sellmilk and eggs. But most of their money comes from Jennys sheep. She has 50 sheep and each one has enoughwool for six sweaters. Every six months Stephen cuts this wool off the sheep so they wont get too warm. TEACHER STYLE: In the lush, verdant lands of the west of England, nestled among rolling hills and meanderingstreams, lay Corryvale Farm. It was a picturesque farm, embodying the pastoral beauty that many dreamt of, yetfew had the fortune to call home. This was the realm of Stephen and Jenny Blackler, a couple whose lives were asintertwined with the land as the ancient oak roots were with the earth. Stephen, a robust man with hands as rough asthe bark of the trees he often climbed, tended to the cows and chickens with a steady patience. He rose with the sun,greeted by the gentle mooing of the cows and the cheerful chaos of the clucking chickens. The rhythm of milkingthe cows and collecting eggs was a comforting routine that marked the passage of days on the farm. STANDARDIZE-A: At Corryvale Farm, Stephen would wake up early. Even if it was as cold as ice outside, hestarted the day with the same routine. First, he would feed the cows. There, in the fields, were 30 healthy cows.These cows were beautiful, larger than any other cows in the west of England. They were the pride of the farm.Then, he would go to the chickens. He had roughly 100 chickens that laid the best eggs every morning. The chickenswere quite noisy and more excited than the quiet cows, making mornings lively on the farm. Jenny was not a big fanof waking up early, she preferred to sleep a bit more. However, her work started after breakfast. Her favourite partof the day was when she went to see her sheep. She loved them dearly. She always said that her 50 sheep were thecleanest and happiest sheep in the entire country. She was so proud of them. STANDARDIZE-E: Stephen and Jenny loved these days on their Corryvale Farm, filled with the soft clucking ofchickens and lowing of cows. They took pride in their daily duties of caring for all the animals, especially the sheepthat were their main source of income. One cold morning, Jenny took her pail to milk the cows while Stephenchecked the hens house for fresh eggs. Their cheeks were red from the cool air and they could see their breathwhen they talk. But they didnt mind the chill for the promise of spring was around the corner. Meanwhile, back inthe barn, Jennys 50 sheep huddled together, their woolly coats growing thicker in the cool weather. This wouldmean only one thing. It was shearing time! Every six months, like clockwork, Stephen would sheer the sheepswool. For each sheep he could fill a bag with enough wool to knit six sweaters. The wool was not just soft but warmtoo, a perfect shield against the bitter winter. STANDARDIZE-L: Corryvale Farm is in the west of England. It is Stephen and Jenny Blacklers farm. They havecows and chickens. They sell milk and eggs. Jenny also has 50 sheep. The sheep have wool. Every six months,Stephen cuts the wool. This keeps the sheep cool. They make money from the wool. STANDARDIZE-: Corryvale Farm is in Englands west. Here, Stephen and Jenny live. They have cows, chickens,and 50 sheep. The sheeps wool can make six sweaters each. Every six months, Stephen shears the sheep. Thiskeeps them cool. Jennys sheep are very important. They sell milk, eggs, and wool. This is their farm life. : Sample generations with the teacher style method and variations of the STANDARDIZE framework usingthe best model (GPT-4) for the context-assisted story generation observing CEFR standards. Some examples aretruncated for brevity.",
  "THEME WORD: Curses": "TEACHER STYLE: In the small village of Ashwood, nestled in the heart of the Whispering Woods, a youngapprentice named Eira stumbled upon an ancient tome hidden deep within the village library. The tome was boundin black leather and adorned with strange symbols that seemed to shift and writhe like living things. As soon as Eiraopened the book, a puff of noxious smoke erupted from its pages, filling the air with a pungent stench. The smokecoalesced into a low, menacing whisper that seemed to seep into Eiras mind. \"Curse you, Eira,\" the voice hissed.\"You have disturbed the rest of the ancient ones. Now you will pay the price.\" Eira felt a cold dread creeping up herspine as the voice continued to taunt her. \"You have unleashed a power beyond your control. Prepare to face theconsequences.\" STANDARDIZE-A: Once upon a time, in a small town nestled in the heart of a dense forest, there stood a grandmansion rumored to be haunted. The mansion had been abandoned for decades, its grandeur and beauty slowlybeing consumed by the encroaching trees and vines. The locals avoided the mansion, whispering tales of cursesand ghostly apparitions that roamed its halls. One summer evening, a group of curious children stumbled upon themansion while exploring the woods. They cautiously approached the mansion, their hearts racing with every creakand groan of the old wooden doors. STANDARDIZE-E: Once upon a time, in a small town nestled in the heart of a dense forest, there stood a grandmansion that had been abandoned for decades. The mansion was rumored to be haunted by the ghosts of its formerowners, who had died under mysterious circumstances. The townspeople whispered stories of strange occurrencesthat had taken place within the mansions walls. Some claimed to have seen ghostly apparitions flitting about inthe windows at night, while others swore they had heard eerie whispers emanating from the abandoned halls. Onestormy evening, a group of brave adventurers decided to explore the cursed mansion, determined to uncover itssecrets. As they approached the entrance, the wind howled and the trees creaked ominously, as if warning themaway. STANDARDIZE-L: In the small town of Willow Creek, there stood a grand mansion that had been abandoned fordecades. The once-grand estate was now a relic of the past, its grandeur faded and its windows shattered. Thelocals whispered stories of the mansions dark history, how it was cursed by a witch who had lived there many yearsago. According to the legend, the witch had been wronged by the mansions owner, who had refused to grant her asimple wish. Enraged, the witch placed a curse on the mansion, ensuring that no one would ever be able to live therepeacefully. The curse was said to cause strange occurrences and unexplained noises to echo through the empty halls,making it impossible for anyone to enter or leave the mansion without experiencing terrifying events. One stormynight, a group of brave adventurers decided to investigate the rumors and put an end to the curse. They entered themansion, their flashlights casting eerie shadows on the walls. As they explored the dusty rooms, they began to hearstrange noises, like whispers in the darkness. STANDARDIZE-: In a small town that lay in the shadow of a dark, dense forest, there was a tale that had beenpassed down for generations. The tale was of three witches who lived deep within the woods, guarding a secret thatgave them immense power. Children would listen with wide eyes as their parents told the story on stormy nights, thewind howling outside as if the witches themselves were listening. The eldest witch, Morgana, could talk to animalsand control the weather with a wave of her hand. The second, Elspeth, brewed potions that could heal any illnessor curse one with a single sip. The youngest, Seraphina, had the gift of foresight, able to see events before theyunfolded. : Sample generations with the teacher style method and variations of the STANDARDIZE framework usingthe best model (Llama2) for the theme word story generation observing CCS standards. Some examples aretruncated for brevity."
}