{
  "Abstract": "FActScore has gained popularity as a metricto estimate the factuality of long-form textsgenerated by Large Language Models (LLMs)in English. However, there has not been anywork in studying the behavior of FActScorein other languages.This paper studies thelimitations of each component in the four-component pipeline of FActScore in the multi-lingual setting. We introduce a new dataset forFActScore on texts generated by strong mul-tilingual LLMs. Our evaluation shows thatLLMs exhibit distinct behaviors in both factextraction and fact scoring tasks. No LLM pro-duces consistent and reliable FActScore acrosslanguages with varying levels of resources. Wealso find that the knowledge source plays animportant role in the quality of the estimatedFActScore. Using Wikipedia as the knowledgesource may hinder the true FActScore of long-form text due to its limited coverage in medium-and low-resource languages. We also incorpo-rate three mitigations to our knowledge sourcethat ultimately improve FActScore estimationacross all languages.",
  "Introduction": "Recent advancements in LLMs have demonstratedsignificant capabilities (Brown et al., 2020; Chowd-hery et al., 2022; Anil et al., 2023; Team, 2024;OpenAI, 2024) in many applications (Zhao et al.,2023). Despite this advancement, LLMs remainprone to generate false information in response toinformation-seeking queries (Huang et al., 2023;Min et al., 2023). To address this critical prob-lem, LLMs have been trained at unprecedentedscales (Brown et al., 2020; Chowdhery et al., 2022)to cope with the massive world knowledge andaligned to reduce hallucination (Shi et al., 2024;Chuang et al., 2024; Dhuliawala et al., 2023). Tofurther prevent the generation of false informa-tion, the Retrieval Augmented Generation method",
  "This work was mainly done when the author was em-ployed by Kensho Technologies": "provides retrieved documents from trustworthysources to the LLM (Ram et al., 2023; Yu et al.,2023b).FActScore was introduced to estimate the factu-ality of generated texts automatically (Min et al.,2023) and at a low cost by combining LLM-as-a-judge scoring (Zheng et al., 2024) with exist-ing reliable knowledge sources such as Wikipedia.FActScore has been enhanced to incorporate alarger knowledge base, like the internet, and to uti-lize more powerful retrieval models such as GoogleSearch, resulting in better estimation across a largerdomain coverage (Wei et al., 2024).With the rapid development of multilingualLLMs (01.AI et al., 2024; Aryabumi et al., 2024),many more people are interacting with LLMs inan increasingly diverse set of languages. Hence,there is a crucial need to monitor and improve thefactuality of texts beyond just the English language,making it helpful and safe for users across the en-tire world (Huang et al., 2023; Ji et al., 2023).In this paper, we study the feasibility of theFActScore pipeline (Min et al., 2023) in a mul-tilingual setting. The FActScore pipeline consistsof multiple components: a knowledge source, a re-trieval model, an LLM-based fact extractor, and anLLM-based fact scorer. We aim to scrutinize eachcomponent individually to identify bottlenecks andaddress these issues. However, there is no existingmultilingual dataset for evaluating FActScore be-sides the original English-only dataset published byMin et al. (2023). To bridge this gap, we annotatea new native dataset of factuality in 3 non-Englishlanguages representing high-, medium-, and low-resource levels. This dataset is created on the textsgenerated by strong multilingual LLMs, i.e., GPT-4 and Gemini-Pro-1.0. We find that all evalua-tor models show decreased FActScore accuracy inlower-resource languages. We attribute this to sev-eral components. First, the performance of fact ex-traction, the simplest task in the FActScore pipeline, deteriorates with lower resource languages. Toaddress this issue, we finetuned an open-sourceLLM for this task and achieved better performancethan GPT-3.5. Second, the quality of the knowl-edge source is crucial to the overall accuracy ofFActScore. Higher resource languages typicallyhave Wikipedia pages with higher quality and cov-erage, leading to better FActScore estimation. Us-ing the Internet as the knowledge source (Wei et al.,2024), therefore, has the greatest impact on im-proving the accuracy of FActScore estimation inmedium and low-resource languages.Our contributions are as follows:",
  "Related Work": "With the advancement of language model develop-ment, numerous methods have been proposed toassess their factual alignment. A significant por-tion of these efforts involves using questions andcorresponding short answers (Lin et al., 2021; Liet al., 2023), slot-filling (Cheng et al., 2023) taskrelated to specific pre-collected factoids, however,they do not reflect practical use cases (Huang et al.,2023). Instead, directly assessing open-ended gen-erated texts offers a clearer signal of the level offactuality in real use cases (Huang et al., 2023).Min et al. (2023) estimate the FActScore of biogra-phies generated by LLMs by evaluating individualcandidate facts in the text. Wei et al. (2024) ex-tended topic coverage and utilized the Google APIto query references for evaluation, thereby access-ing a broader range of domains. Our study buildsheavily on these approaches, focusing on the effec-tiveness of FActScore across high-, medium-, andlow-resource languages. In these scenarios, both the language models performance in each compo-nent of the evaluation pipeline and their multilin-gual capabilities are critical. Other approaches relyon language models internal knowledge pools forfactuality assessment (Azaria and Mitchell, 2023;Dhuliawala et al., 2023). While this approach of-fers simplicity, it raises concerns about the intrinsicfactual alignment of these evaluators.Considering multilingual factuality, X-FACTR(Jiang et al., 2020) and MLAMA (Kassner et al.,2021), adapted from LAMA (Petroni et al., 2019),assess models relational knowledge through thefill-in-the-blank task. X-Fact (Gupta and Sriku-mar, 2021) releases a multilingual fact-checkingbenchmark, a factual correctness classification taskcovering various topics and 25 typologically di-verse languages across 11 language families. Qiet al. (2023) introduces an extension of MLAMAand X-FACTR and a new metric to assess the cross-lingual consistency of language models. Whilethese attempts shed light on multilingual factualityalignment, they mainly involve pre-collected setsof factual statements. Our work aims to evaluatethe factuality of open-ended text generation. Shafayat et al. (2024) adapted FActScore for amultilingual context by translating the biographiesto English. Our work investigates both translationand performing the entire FActScore pipeline di-rectly in the reference language. We also designeda comprehensive set of biographies to better cap-ture the cultural proclivities of the target popula-tion.",
  "Tasks": "The FActScore pipeline (Min et al., 2023) consistsof two main steps:Atomic Fact Extraction that employs an extrac-tor E to break a long-form biography x generatedby a subject LLM M into atomic candidate factsAE(x) = {aE,xi}Factuality Scoring is a binary classification task.It employs an evaluator V assigning a binary (sup-ported/not supported) label yE,x,V,Cito every candi-date fact ai based on a knowledge source C.The final FActScore estimates the precision of",
  "Translated Annotation (en X) (R1)": "The original FActScore published a set of biogra-phies X M generated by several subject LLMs Mand their corresponding FActScore (Min et al.,2023) with full annotation of atomic fact and sup-porting label pairs (aE,xi, yE,x,V,Ci). We use GoogleTranslate to translate each atomic fact aE,xiin En-glish into every other target language t to produce anewly translated annotation (aE,x,ti, yE,x,V,Ci). Theknowledge source C (written in English) is alsotranslated into corresponding target languages. Weselect a set of target languages (X) in 3 groups:high-resource (i.e., French (fr), Spanish (es), Chi-nese (zh-cn), Russian (ru), and Vietnamese (vi)),medium-resource (i.e., Arabic (ar) and Hindi (hi)),and low-resource (i.e., Bengali (bn)).",
  "Native Annotation (R2)": "The translated annotations are able to provide someinsights into potential issues with FActScore inthe multilingual setting. However, they provide aconfounding factor: cascading errors due to issueswith the translations themselves. This is especiallyrelevant for low-resource languages. Therefore, wealso annotate new FActScore data in non-Englishlanguages to better estimate FActScore and explorethe issues of this task. In particular, we aim for abroad language coverage spanning high-, medium-,and low-resource languages. We investigated onelanguage across each of these resource categories:Spanish, Arabic, and Bengali, respectively.Following Min et al. (2023), we carefully cu-rated a set of biographies for each language arefrom 4 geographical regions and 5 levels of rarity(See Appendix A). We attempted to use the samegenerative models as in (Min et al., 2023). How-ever, these models are not explicitly designed tobe multilingual and as a result, could not generatebiographies of an acceptable quality, specificallyin the low-resource language. To address this, weanalyze the performance of explicitly multilingualLLMs, i.e., GPT-4 (GPT4) and Gemini Pro (GemP)to generate biographies.We hired 2 native annotators for each languageand followed the same annotation guidelines by Min et al. (2023) to evaluate the true FActScoreof generated text. The Kappa agreement scoresbetween Spanish, Arabic, and Bengali annotatorsare 79.8, 73.1, and 80.2, respectively. These showa substantial agreement (61-81) to close to almostperfect agreement (81-100) between native annota-tors.",
  "bnGemP1006040058.958.7760.55GPT41006829346.471.9574.48": ": Statistics of the generated biographies byGemP and GPT4 including the percentage of Rele-vant (R), Irrelevant (I), Abstain (A) biographies; theaverage number of atomic facts in relevant generated bi-ographies; and FActScore evaluated by native speakersusing one native Wikipedia page (WN-1) and the wholenative Wikipedia (WN-All). Note that the FActScore iscomputed on the relevant generated texts only. presents the statistics of generated bi-ographies by both subject models. Both modelsgenerate more candidate atomic facts in higher-resource languages than lower-resource languages,however, this phenomenon seems to be clearer withGPT4. GPT4 generates more relevant biographiesthan GemP in all three languages.GPT4 alsoabstains significantly more than GemP in Span-ish and Arabic, whereas GemP produces manymore irrelevant biographies. This shows that GPT4has a broader knowledge and a higher aware-ness of its knowledge limitation.In terms ofFActScore, GPT4 yields much higher FActScore(s)than GemP in all three languages, using eithera single Wikipedia page or the whole Wikipediawith an average margin of approximately 14.6%.Last but not least, FActScore(s) evaluated basedon the whole Wikipedia (WN-All) are higher thanFActScore evaluated on a single Wikipedia page(WN-1) in all cases (on average 3%). This sug-gests that a larger knowledge source gives a higherFActScore. In other words, the knowledge sourceis the ceiling of evaluating factuality.",
  "FActScore decomposes a long-form text into mul-tiple atomic statements, each containing a single": "piece of information. The original methodologyuses few-shot demonstrations to prompt Instruct-GPT for this task (Min et al., 2023). We examinethe performance of different models and pinpointissues of existing models for this task.Settings: Due to the higher quality of text gen-erated in English, prior work by Min et al. (2023)only considered if the candidate facts need to bemerged or split, mainly concerning whether thefacts are atomic. However, in a multilingual set-ting, the texts generated by LLM may contain otherkinds of errors where the facts need to be mergedor split, not grounded, duplicated, missing someinformation, and linguistic errors.We choose GPT-3.5 (GPT3.5), GPT4, andGemma for evaluation in this task. These mod-els were selected for their best performance via apilot study on a small subset of R1 (See AppendixB). We evaluate the GPT3.5 and GPT4 as few-shotIn-Context Learning while Gemma is further super-vised finetuned for this task. In particular, we fine-tune Gemma on 42k pairs of (sentence, extractedatomic facts) derived from R1. Then these threemodels are evaluated on a subset of 200 sentences,sampled randomly from R2 with a 1:1 ratio factsgenerated by GPT4 and GemP.Results: shows the number of errorsmade by 3 models (GPT3.5, Finetuned Gemma,and GPT4). Among these three models, GPT4 isthe best model by a relatively large margin acrossall three languages. Finetuned Gemma is com-petitive to GPT3.5 in high-resource and better inlow-resource and medium-resource languages.However, GPT4 and GPT3.5 s performancesdeteriorate rapidly with low-resource language (ap-proximately double the average error rate in Ben-gali, compared to Spanish and Arabic). On theother hand, the FT Gemma does not show a per-formance reduction in low-resource language. Infact, its error rate in Bengali is lower than those inSpanish and Arabic. This suggests that finetuninghas potentially helped this model maintain a steadyperformance across all resource languages.More importantly, due to the better performanceof LLMs in English, Min et al. (2023) did notconsider other types of errors that may happen inmultilingual settings. In particular, we see a largenumber of grounding errors in medium and low-resource languages (Arabic and Bengali), while wedont see that in high-resource languages such asSpanish. LLMs also missed some detailed informa- esarbn",
  "Factuality Scoring": "This section investigates the feasibility of usingLLMs as factuality scorers in multilingual settings.Settings: We use GPT4 to extract facts from bi-ographies generated by two subject models namelyGPT4 and GemP to provide the same denominatorfor this evaluation. We evaluate 4 LLMs as factscorers (GPT3.5, GPT4, Mistral, and GemP) onthe text generated by GPT4 and GemP in nativelanguages. The human-annotated dataset (R2) isused as the ground truth.Results: (upper) shows the FActScorepredicted by LLMs and by humans (R2). GemPconsistently underestimates FActScore, whereasGPT4 significantly overestimates FActScore acrossboth subject models. GPT3.5 overestimates Span-ish and Arabic while closely estimating FActScorefor Bengali. On the other hand, Mistral closelyestimates FActScore for Spanish and Arabic whilesubstantially underestimating the FActScore forBengali. This experiment suggests that none ofthese models offers a reliable FActScore acrossthe whole spectrum of languages, even with strongLLMs (e.g., GPT4 and GemP). (lower) shows the scoring accuracy ofthe LLM scorers. GemP shows a steady accuracyon both GPT4 and GemP facts. Its accuracy doesnot show a clear dependency on the resource level.",
  "Knowledge Source": "Since FActScore is a function of knowledge source(Min et al., 2023), the quantity and quality of theinformation of the knowledge source greatly af-fect the subsequent score (Wei et al., 2024). Thissection investigates the sensitivity of FActScore tochanges in the underlying knowledge sources.Settings: We collected 32 biographies of enti-ties per language in four categories of popularityand geographical relevance: internationally pop-ular, internationally unpopular, locally popular,and locally unpopular (See Appendix A). The an-notators evaluate facts using three different sources:the native Wikipedia, the English Wikipedia, andthe whole Internet. Since the Internet is a supersetof knowledge sources, we considered the anno-tations created with access to the Internet as thegolden annotations for evaluating the quality ofother knowledge sources.Results: shows the scoring accuracybetween evaluating 4 categories of popularity in 3languages. Using Spanish Wikipedia pages yieldshigher accuracy in labeling locally popular figures(L+P), whereas English Wikipedia pages are betterfor internationally unpopular entities (I+UP). ForArabic, the Arabic Wikipedia is better for local pop-ular entities (L+P), while the English Wikipediais better for international entities (I+P and I+UP).For Bengali, the Bengali Wikipedia has a muchlower performance compared to the English coun-terpart in all four categories, especially for the in- ternational entities (I). This suggests that BengaliWikipedia has a very low coverage, inadequatefor most cases. Last but not least, even thoughEnglish pages provide better coverage for localentities (L+P and L+UP) than Bengali pages, thescoring accuracies using English pages for Bengalilocal entities are still lower than those of interna-tional entities. These differences in performancebetween international and local figures highlightthe importance of choosing local entities and lo-cal knowledge sources in multilingual FActScoreevaluation and estimation.",
  "Retriever": "Due to the limitation of the LLM context length,a Wikipedia page of the evaluated entity is splitinto short passages. A retriever model retrievesk relevant passages. These passages are used asreference knowledge sources.Settings: We examine both traditional retrievalmethod (i.e., BM25) and vector-based retrievalmethod. In particular, we use the multilingual em-bedding models (distiluse-base-multilingual-cased-v2 and paraphrase-multilingual-MiniLM-L12-v2)(Reimers and Gurevych, 2019) to encode the texts.For each translated fact, k = 5 retrieved passagesare retrieved out of all passages. We measure theRecall@k and the average hit rate of the top 1 andtop 2 passages.Results: reports the retrieval perfor-mance in Recall@k of the retrieval models on 9languages of 3 resource-level groups. For vec-tor retrievals (Distill and Paraphrase), we see agradual drop of performance as the resource isscarcer. While the performance of vector-basedretrievals for medium languages is slightly worsethan ones on high-resource languages, their perfor-mance on low-resource is significantly lower than L+PL+UPI+PI+UP60",
  "Would translation help?": "A simple method for a multilingual FActScore isfirst translating non-English long-form text andknowledge sources into English (Xen) and es-timating the FActScore on these proxy translatedEnglish texts (Shafayat et al., 2024). This is apromising method given that the quality of ma-chine translation has improved significantly in thelast decade. To do this, we translated the NativeAnnotation (R2) into English to get a translatedEnglish annotation (R3). shows the prediction matching for theFactuality Scoring task on texts in the target lan-guage and in translated English. GemP and GPT4are the two strong scorers with consistently highmatching, GPT3.5 and Mistral have significantlylower matching scores in lower-resource languages. esarbn",
  ": Prediction agreement between two variants offacts (in target language and in translated English)": "Additionally, GPT4 and GemP see a slighter de-cline in matching scores for lower-resource lan-guages than GPT3.5 and Mistral. This matchingvariation across different languages for this taskamong even the most advanced LLMs may lead tounreliable FActScore estimation in lower-resourcelanguages. (lower) compares the scoring accuracybetween using R3 and using R2. We see a signifi-cant improvement in scoring accuracy for Mistraland GPT3.5 in Arabic and Bengali and GemP inBengali, all on both GPT4 and GemP texts. We at-tributed this to both better reading comprehensionand retrieval performance in English compared tonon-English languages, especially Bengali. Ap-pendix D explores the impact of translation on re-trieval performance in more detail. On the otherhand, we see a significant decline in the accuracyof the scorer GPT4 on GemPs texts for all threelanguages while a slight increase in the accuracy inArabic on GPT4 texts. (upper) shows the FActScore predictedby these models in native texts and translated En-glish texts. The translation contributes to the over-estimation of FActScore by GPT3.5 and Mistralin medium and low-resource languages. On the esarbn",
  "Error analysis": "We further conduct an error analysis for the fact-scoring task with these improvements and report in. The result shows that all three approachesreduce false negatives (and thereby increase truepositives) due to their ability to provide more fac-tual coverage.Surprisingly, the unverified LLM augmentedwikipedia articles significantly increase the truepositive rate (by 12.9%, 6.8%, and 14.9% for GemP",
  "Mitigations": "The previous sections have shown evidence of a cor-relation between lower resource languages, lowerretrieval performance (See ), lower cover-age of the native knowledge source (See ) and subsequently lower fact scoring accuracy(See Figures 1 and 3). To mitigate this problem,we empirically examine three techniques including:improving retrieval performance by (1) increasingthe number of retrieved passages, (2) employinglanguage models as Internet search agents and eval-uators (Wei et al., 2024), and (3) using languagemodels as a knowledge generator (Yu et al., 2023a;Chen et al., 2023)).Settings: We use GemP as the fact scorer forall proposed techniques. GemP is more persis-tent to the change in languages (as shown in Fig-ure 1). It is more sensitive to external knowledgethan its internal knowledge (.2), makingit more suitable for evaluating these mitigationsthan GPT4. The baseline is the original pipeline(Min et al., 2023) with GemP as the scorer andWikipedia pages in native languages as the knowl-edge sources.We use the 32 generated biographies in the threestudied languages that we used to assess knowledge sources in section 4.3. We consider the facts anno-tated by native speakers using the whole internet asthe golden data. We evaluate these techniques bymeasuring their scoring accuracy with the goldenlabels. illustrates the performance of theproposed methods.",
  "Expanding Retrieved Passages": "This method increases the number of retrieved pas-sages from 8 to 20, aiming to extend the amountof information given to the scorer. This mitiga-tion should alleviate the impact of poor recall inretrieval. Although the mildest of the three mitiga-tions, this led to a considerable increase in perfor-mance across all three languages. The performancegap is particularly large in Bengali, correlating withobservations in .4 regarding the retrieversdeteriorating performance in this language. Thisretrieval problem might be further mitigated thanksto the increase in context length of recent languagemodels (Xiong et al., 2023) allowing feeding moreinformation to the LLM-based scorer.",
  "Internet as a knowledge source": "Adapted from Wei et al. (2024), GemP is promptedto send queries to the Google Search API on a givenfact and determine the facts factual accuracy fromthe query results. We see a clear improvement infact-scoring accuracy and higher FActScore (closerto the golden) across the subject models and lan-guages. For example, the accuracy on Bengaliimproved from 60.6 to 86.8. This shows the benefitof accessing a larger pool of information results insubstantial improvement, much greater than merelyincreasing the number of passages from Wikipedia.",
  "LLM as a knowledge source": "Since previous experiments suggested that GPT4heavily relies on its internal knowledge to assessfactuality, we experiment with allowing GPT4 to di-rectly augment the low-coverage knowledge source.We prompt GPT4 to create a question based on agiven fact and then generate related informationto answer that question (Yu et al., 2023a). Thisgenerated knowledge is combined with retrievedpassages, as suggested by (Yu et al., 2023a), andused with a separate evaluator, GemP, for factuallabeling. It is worth noting that this text is entirelyunverified and likely contains some amount of fac-tual errors.This approach results in a substantial improve-ment, larger than that of simply increasing the num-",
  "Human+Wiki57.861.274.162.7Human+Internet82.097.5--": ": FActScore and accuracy of introduced evalu-ation methods on GemP and GPT4s generated facts.We use GemP as the LLM scorer. +Wiki (k=x) denotesusing x passages from 1 Wikipedia page as references.+Google API denotes using GemP as the Internet searchagent and evaluator (evaluation is based on query re-sults). +GPT4s IK denotes using GPT-4s generatedInternal Knowledge (IK) and retrieved passages as ref-erences. Natives+Wiki/Internet denotes natives, using 1Wikipedia page or the entire Internet as references forannotations. Natives+Internet is considered as goldenlabeling to conclude accuracy. ber of Wikipedia passages across all languages.Compared to using the Google Search API, theGPT4 augmented knowledge base shows highergains in high- and medium-resource languages.This suggests the reliability of GPT4s internalknowledge and its effectiveness as a knowledgegenerator. However, in Bengali, querying evalua-tion references via Google API yields significantlybetter factual labeling. The improvement from us-ing GPT4s internal knowledge is attributed to theadditional relevant information that it provides.",
  ": True Positive (TP), False Negative (FN), False Positive (FP), and True Negative (TN) rates for differentFActScore pipelines that use GemP as the scorers": "on es, ar, and bn respectively) without in turn signif-icantly increasing the false positive rate (by 2.6%,1.1% and 0.7% respectively). The increase in falsepositives was lower than using the Google API inall but one case. Conversely, adding additionalWikipedia data always leads to a lower rate of falsepositives compared to the GPT4 augmented databut also a lower rate of true positives. This im-plies that the benefits of increased factual coveragefrom using the unverified GPT4-generated data out-weigh the costs of potentially false informationintroduced. However, these benefits diminish forlower-resource languages, while using the GoogleAPI shows more consistent gains across all lan-guages.",
  "Conclusion": "This paper scrutinizes the FActScore pipeline forlong-form generated texts in the multilingual set-ting. We generated new fact candidates and anno-tated a new corpus for FActScore evaluation. Themost recent open-source LLMs struggle with theatomic fact extraction task. Finetuning on this taskcan match the performance of much larger close-source models, e.g., GPT3.5. More importantly,the Fact Scoring task is very sensitive to the cover-age of the knowledge source. Although Wikipediais reliable, it lacks coverage in lower-resource lan-guages, which leads to a severe underestimation ofthe FActScore. We show that mitigation such asextending the knowledge source through increasingthe amount of Wikipedia data, allowing access tothe Internet, and even augmenting low-coverageWikipedia articles with unverified text generatedby an LLM improve multilingual FActScore esti-",
  "Limitation": "Even though this paper offers insights into themultilingual FActScore, the paper was not ableto address more languages than the 3 examined lan-guages and on a larger sample size due to fundinglimits and the extremely high cost of this task asreported in previous work (Min et al., 2023; Weiet al., 2024). As a result, the data might containcultural biases and variations in information andknowledge exposure. Therefore, generalizing ourfindings to languages other than the examined onesshould be considered carefully. Due to the rapiddevelopment of LLMs when the study was done,some models might be obsolete by the publicationtime, however, we believe this paper still providesinsightful knowledge into multilingual factualityscoring.",
  "Ethical Consideration": "In this work, we hire 6 international crowd-sourcedworkers from 3 countries as native annotators. Theannotators were paid between US$15 to US$25 perhour, adjusted to their geographical location.While the biographies generated by the two sub-ject models exhibit a certain level of factuality, weobserved a significant amount of false information.Using these biographies as references or in real-world scenarios carries the risk of spreading misin-formation and negatively impacting the individualswhose biographies are studied.All the systems presented in this paper do notoffer a perfect factual guarantee, especially withthe texts and knowledge beyond the studied scope.These systems should not be used as alternate toolsfor traditional factual verification methods.Given the nature of this task which involves as-sessing human biographies generated by LLMs,our collected data includes identifications, infor-mation, and opinions about them, including falseand biased content. We only share the generatedtexts upon request to enhance the proper use ofthe data and minimize the risk of spreading falseinformation. 01.AI, Alex Young, Bei Chen, Chao Li, Chengen Huang,Ge Zhang, Guanwei Zhang, Heng Li, JiangchengZhu, Jianqun Chen, Jing Chang, Kaidong Yu, PengLiu, Qiang Liu, Shawn Yue, Senbin Yang, ShimingYang, Tao Yu, Wen Xie, Wenhao Huang, XiaohuiHu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, YuchiXu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu,Zhiyuan Liu, and Zonghong Dai. 2024. Yi: Openfoundation models by 01.ai. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-son, Dmitry Lepikhin, Alexandre Passos, SiamakShakeri, Emanuel Taropa, Paige Bailey, ZhifengChen, et al. 2023. Palm 2 technical report. arXivpreprint arXiv:2305.10403. Viraat Aryabumi, John Dang, Dwarak Talupuru,Saurabh Dash, David Cairuz, Hangyu Lin, BharatVenkitesh, Madeline Smith, Jon Ander Campos,Yi Chern Tan, Kelly Marchisio, Max Bartolo, Se-bastian Ruder, Acyr Locatelli, Julia Kreutzer, NickFrosst, Aidan Gomez, Phil Blunsom, Marzieh Fadaee,Ahmet stn, and Sara Hooker. 2024. Aya 23: Openweight releases to further multilingual progress.",
  "Amos Azaria and Tom Mitchell. 2023. The internalstate of an llm knows when its lying": "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Liang Chen, Yang Deng, Yatao Bian, Zeyu Qin, BingzheWu, Tat-Seng Chua, and Kam-Fai Wong. 2023. Be-yond factuality: A comprehensive evaluation of largelanguage models as knowledge generators. In Pro-ceedings of the 2023 Conference on Empirical Meth-ods in Natural Language Processing, pages 63256341, Singapore. Association for Computational Lin-guistics. Qinyuan Cheng, Tianxiang Sun, Wenwei Zhang, SiyinWang, Xiangyang Liu, Mozhi Zhang, Junliang He,Mianqiu Huang, Zhangyue Yin, Kai Chen, andXipeng Qiu. 2023. Evaluating hallucinations in chi-nese large language models. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts,Paul Barham, Hyung Won Chung, Charles Sutton,Sebastian Gehrmann, Parker Schuh, Kensen Shi,Sasha Tsvyashchenko, Joshua Maynez, AbhishekRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-odkumar Prabhakaran, Emily Reif, Nan Du, BenHutchinson, Reiner Pope, James Bradbury, JacobAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,Toju Duke, Anselm Levskaya, Sanjay Ghemawat,Sunipa Dev, Henryk Michalewski, Xavier Garcia,Vedant Misra, Kevin Robinson, Liam Fedus, DennyZhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi,David Dohan, Shivani Agrawal, Mark Omernick, An-drew M. Dai, Thanumalayan Sankaranarayana Pil-lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,Rewon Child, Oleksandr Polozov, Katherine Lee,Zongwei Zhou, Xuezhi Wang, Brennan Saeta, MarkDiaz, Orhan Firat, Michele Catasta, Jason Wei, KathyMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,and Noah Fiedel. 2022. Palm: Scaling language mod-eling with pathways.",
  "Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, EtsukoIshii, and Pascale Fung. 2023.Towards mitigat-ing hallucination in large language models via self-reflection": "Zhengbao Jiang, Antonios Anastasopoulos, Jun Araki,Haibo Ding, and Graham Neubig. 2020. X-factr:Multilingual factual knowledge retrieval from pre-trained language models. In Proceedings of the 2020Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 59435959. Nora Kassner, Philipp Dufter, and Hinrich Schtze.2021. Multilingual LAMA: Investigating knowledgein multilingual pretrained language models. In Pro-ceedings of the 16th Conference of the EuropeanChapter of the Association for Computational Lin-guistics: Main Volume, pages 32503258, Online.Association for Computational Linguistics. Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-YunNie, and Ji-Rong Wen. 2023. Halueval: A large-scale hallucination evaluation benchmark for largelanguage models. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 64496464.",
  "Gemini Team. 2024. Gemini: A family of highly capa-ble multimodal models": "Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu,Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu,Da Huang, Cosmo Du, et al. 2024. Long-form fac-tuality in large language models.arXiv preprintarXiv:2403.18802. Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang,Prajjwal Bhargava, Rui Hou, Louis Martin, RashiRungta, Karthik Abinav Sankararaman, Barlas Oguz,Madian Khabsa, Han Fang, Yashar Mehdad, SharanNarang, Kshitiz Malik, Angela Fan, Shruti Bhosale,Sergey Edunov, Mike Lewis, Sinong Wang, and HaoMa. 2023. Effective long-context scaling of founda-tion models. Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,Mingxuan Ju, Soumya Sanyal, Chenguang Zhu,Michael Zeng, and Meng Jiang. 2023a. Generaterather than retrieve: Large language models arestrong context generators.",
  "Wenhao Yu, Zhihan Zhang, Zhenwen Liang, MengJiang, and Ashish Sabharwal. 2023b. Improving lan-guage models via plug-and-play retrieval feedback": "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,Xiaolei Wang, Yupeng Hou, Yingqian Min, BeichenZhang, Junjie Zhang, Zican Dong, Yifan Du, ChenYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. Asurvey of large language models. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024.Judging llm-as-a-judge with mt-bench and chatbotarena. Advances in Neural Information ProcessingSystems, 36.",
  "ABiography Selection": "We select a set of people names from the followingregions: North America, Europe, Asia, Oceania,South America, and Africa; and 5 levels of raritybased on their Wikipedia page views very frequent,frequent, medium, rare, and very rare.In .3, four additional categories are in-troduced: internationally popular, internationallyunpopular, locally popular, and locally unpopular.The terms locally and internationally refer to thegeographical or linguistic exposure of the entitieswhose biographies are being factuality evaluated.Local entities might be native speakers of the lan-guage or reside in nearby regions where the lan-guage is predominantly spoken as a first language.For example, for Spanish, this includes regionssuch as South America and Spain. For Arabic, thisincludes the Arab world, and for Bengali, the Indicregion. Entities deemed popular include those clas-sified as very frequent, frequent or medium whileunpopular encompasses medium, rare, and veryrare entities according to rarity as introduced aboveaccording to Wikipedia page views.",
  "BPilot Experiments on Fact Extractor": "We randomly selected 10 sentences from the orig-inal work (Min et al., 2023) and then translatedthem into target languages. Tested models wereprompted (few-shot) to break down those sentencesinto individual facts. These were translated backto English for assessment based on metrics from.1.Tables 14, 15, 16, 17, 18, 19 represents extrac-tions of GPT4, GemP, GPT3.5, Mistral-7B-Instruct(Mistral), Llama-7B-Chat (Llama2) and Gemma-7B-Instruct respectively. All closed models aredecent at the task across all studied languages.Among open models, Mistral, Llama2, and Gemmacould understand the instruction and perform fact extraction, whereas Aya and BLOOMZ were lost inthis task (Aya simply returns the original sentence,whereas BLOOMZ does not produce any outputs).However, in non-English languages, Llama2 showserrors even in a high-resource language like Span-ish, while Gemma and Mistral begin to show errorsin medium- and low-resource languages.For native annotations with R2, we chose twoclosed models, GPT3.5, and GPT4, and finetunedan open-source model for the extraction task in 3studied languages. Gemma-7B is chosen consider-ing its large vocab size, thus saving inference costsin the multilingual context. illustratesthat the finetuned model consistently shows properextractions across studied languages.",
  ": Pearson correlation coefficient between Mis-tral (up) and BLOOMZ (down) scoring on subject mod-els from (Min et al., 2023) with that by GPT3.5 (goldenlabeling proposed by (Min et al., 2023))": "significant agreement in the ranking of subject mod-els when compared to the golden labels providedin the original study (Min et al., 2023). It is impor-tant to note that the ranking order among evaluatedmodels is the primary concern of Min et al. (2023).This is further supported by , representingrelatively high Pearson correlation coefficients ofscoring by two scorers in different languages withgolden labeling.However,there are notable variations inFActScore across languages. This indicates thatwhile the pipeline effectively operates in multilin-gual environments for comparing factuality align-ment among language models in a particular lan-guage, it is not suitable for assessing model perfor-mances across different languages. displays the cross-lingual agreementheatmap between texts written in two languagesof two open-source models, i.e., Mistral-7B-Instr(Mistral) and BLOOMZ-7b1 (BLOOMZ). The firstrow of the heat map illustrates the labeling agree-ment of both models when evaluating facts in En-glish and non-English languages. The agreementfor both models decreases in correlation with theresource levels of the non-English languages. Thisdecline is clearly observable in Mistrals heat map,",
  ": Cross-lingual agreement of Mistral (up) andBLOOMZ (down) when scoring different language ver-sions of the same fact": "but only partially in BLOOMZs heat map. Specifi-cally, BLOOMZs agreement in Russian and Viet-namese is consistently lower than expected, giventheir high-resource status in the Common Crawlcorpus. This issue is attributed to BLOOMZsalignment training dataset, namely xP3. The xP3dataset does not include any Russian data and con-tains a limited amount of Vietnamese data (2.11%in xP3), less than that for Arabic (2.72% in xP3), alower-resource language. further illustrates the cross-lingualagreement of two proprietary models, GemP andGPT3.5, with a subset of three out of the nine stud-ied languages. The leading open-source model,Mistral, slightly trails behind GPT-3.5, with aver-age scores of 0.83 and 0.85 respectively. However,Mistrals performance is significantly lower thanthat of GemP, which achieves an average score of0.88.",
  "DImpact of Translation on Retriever": ".1 discusses the impact of translation onscoring accuracy by different scorers with clearpositive effects on GPT3.5, Mistral, and GemP.However, this phenomenon might be attributed totranslations contribution to addressing the multi-lingual deficiency of the retriever (illustrated in.4) as well. This section explores thathypothesis by comparing the effect of translationif it is performed before (T+R) and after retrieval(R+T).As shown in , while the difference is",
  "EGPT4s Behaviors as a Scorer": "Concurrent with the discussion in .2,among context-unfaithful samples, there are alsofactually incorrect ones, including hallucinationsand reading deficiencies. A significant portion(72%) of these factually incorrect samples containsinformation not found in the knowledge source,hallucination.This category, similar to the discussed factuallycorrect samples, lacks grounded information withinthe provided context, highlighting an interestingbehavior of GPT-4 as a scorer. The model heavilyrelies on its internal knowledge during the scoringprocess.This reliance may partially explain the decreas-ing accuracy of GPT-4s scoring in lower-resourcelanguages, as demonstrated in (lower). Specifically, shows that the informationavailable in the Wikipedia versions of the stud-ied languages diminishes in correlation with theirresource levels. This might result in their grow-ing distances with the GPT4s internal knowledge.Consequently, it contributes to lower accuracy (see (lower)) when GPT-4 is the scorer.Correspondingly, error analysis in .2 reveals a higher number of context-unfaithful sam-ples in lower-resource languages. This indicatesGPT-4s increased tendency to rely on its inter-nal knowledge in more limited-resource circum-stances. illustrates that GPT4 as a scorer is factu-ally correct in about half of the disparity sampleswith native annotators. However, as shown in Ta-ble 9, excluding the retriever and knowledge sourcefrom the pipeline and relying solely on GPT4s in-ternal knowledge leads to a decrease in factuallycorrect evaluations overall. This implies that de-spite their limitations, external knowledge sourcesare essential for maintaining the reliability of theevaluation process.",
  "Thoroughly read the entire Wikipedia arti-cle to identify relevant text (sentences, para-graphs) for evaluating the fact and checkingfor annotator errors": "If no text within the Wikipedia page relatesto the fact, it should be labeled as not sup-ported by annotators (or it would be a mis-take from the annotator) and supported bythe model scorer. We then proceed to evaluatethe fact based on external sources and deter-mine whether the labeling should be classified",
  "Mistral": "w/ Wiki72.184.457.578.545.059.577.984.073.480.655.659.0w/o Wiki58.671.043.965.260.972.161.269.954.561.157.271.6T + w/ Wiki74.686.961.085.861.274.578.286.281.688.273.376.3T + w/o Wiki67.877.761.082.565.267.769.376.968.680.068.768.2 : FActScore and accuracy by different scorers with (w/) or without (w/o) Wiki and whether translation (T) isused on generated facts and knowledge source (Wiki page). Accuracy is measured against native labeling using theInternet to find references. Human with the Internet is considered as golden.",
  "HHyper-Parameters": "All experiments are conducted from January toJune 2024. The following hyper-parameters arespecified, while all others are set to their defaultvalues.Generation Temperature: All studied modelstemperatures are set to 0.7.Context, max generation length: For open-source models, the maximum output length is setto 512 tokens, and the maximum sequence lengthis set to 4096 tokens for high-resource languages,and 1024 and 6024 tokens for medium- and low-resource languages, respectively. For closed mod-els accessed via API, the maximum token limit isuniformly set to 4096 tokens for all use cases.",
  "Naipaul fue criticado porsu visin a menudo pes-imistaTranslated:Naipaulwas criticized for his of-ten pessimistic vision": "Native label: F, Model label: T, Ground truth: T.Comment: No related information within the provided Wikipedia page. Butthere is supporting evidence from en.wikipedia.Evidence: Yet he has been accused of being aneo-colonialist , andin this novel post-colonial Africa is depicted as spiraling into a kind ofHell...Naipauls fiction and especially his travel writing have been criti-cised for their allegedly unsympathetic portrayal of the Third World. Thenovelist Robert Harris has called Naipauls portrayal of Africa racistandrepulsive,reminiscent of Oswald Mosleys fascism.",
  "Rodrygo Goes de Souzanaci el 9 de enero de2001.Translated:RodrygoGoes de Souza was bornon January 9, 2001": "Native label: T, Model label: F, Ground truth: T.Comment: The evaluator misses the related information (that supports thefact) within retrieved passages.Evidence: Rodrygo Silva de Goes (; Osasco, So Paulo, 9 de enero de2001), conocido simplemente como Rodrygo, es un futbolista brasileo quejuega como delantero en el Real Madrid C. F. de la Primera Divisin deEspaa.Translated: Rodrygo Silva de Goes (; Osasco, So Paulo, January 9, 2001),known simply as Rodrygo, is a Brazilian footballer who plays as a forwardfor Real Madrid C.F. of the Spanish First Division.",
  "SubjectiveopinionEs considerado uno de lostrompetistas ms destaca-dos de su generacin.Translated: He is con-sidered one of the mostprominent trumpeters ofhis generation": "Native label: T, Model label: F, Ground truth: T/F.Comment: The statement/fact is subjective, thus debatable. Chuck Mangionehad a song, being recognized as the number one jazz song of all time by aradio channel, but there is no explicit mention that he is a prominent trumpeterof his generation.Evidence: Recientemente las estaciones de radio que transmiten jazz enlos Estados Unidos han reconocido a Feels So Good de Mangione comola cancin nmero uno de todos los tiempos.(en: Recently, jazz radiostations in the United States have recognized Mangiones Feels So Good asthe number one song of all time.",
  "AnnotationerrorTekke dio el salto al ftboleuropeo en 2006.Translated:Tekkemade the leap to Euro-pean football in 2006": "Native label: F, Model label: T, Ground truth: T.Comment: The annotator misses details within the Wikipedia page.Evidence: Esa temporada Tekke se convirti en el mximo goleador de laSuperliga de Turqua al anotar 31 goles. El 31 de julio de 2006 firma uncontrato con su actual club, el Zenit de San Petersburgo ruso, equipo querealiz un desembolso econmico de 10 millones de euros para poder hacersecon sus servicios.Translated: That season Tekke became the top scorer in the Turkish SuperLeague by scoring 31 goals. On July 31, 2006, he signed a contract with hiscurrent club, the Russian Zenit Saint Petersburg, a team that made a financialoutlay of 10 million euros to be able to acquire his services.",
  "Translated: He had abig mustache": "Native label: F, Model label: T, Ground truth: T.No related information within the provided Wikipedia page. But there is supportingevidence from en.wikipedia.Evidence: Hogan grew a beard alongside his famous mustache and dyed it black,traded his red and yellow garb in for black and white clothing, often detailed withlightning bolts, and renamed himself \"Hollywood\" Hulk Hogan (often shortened toHollywood Hogan",
  "Translated:Researchinterests include postcolo-nial theory": "Native label: T, Model label: F, Ground truth: T.Comment: The evaluator misses the related information (that supports the fact)within retrieved passages.Evidence: . ) ( Translated: The 1985 publication is considered one of the founding texts ofpostcolonialism, and Spivak is currently considered one of the most important inter-national figures influencing cultural criticism and literature. Spivak was awardedthe 2012 Kyoto Prize for Arts and Philosophy for being a \"critical theorist (criticaltheorist) and educator who defends the humanities against intellectual colonialismin relation to the globalized world.",
  "Trygve Lie intent pro-mover la paz.Translated: Trygve Lietried to promote peace": "Native label: F, Model label: T, Ground truth: T.No related information within the provided Wikipedia page. But there is supportingevidence from en.wikipedia.Evidence: He sent 50 members of the United Nations guard force from LakeSuccess to assist the Mediator in supervising the Truce in the former British Man-date of Palestine in 1948 and the \"UNTSO\", the first peacekeeping operation wasestablished by the United Nations.",
  "Muri de un ataque alcorazn.Translated: He died ofa heart attack": "Native label: F, Model label: T, Ground truth: F.Comment: He died of pneumonia, not a heart attack.Evidence: ...Arruinado, fsicamente dbil y con la mente deteriorada, Capone seretir a una propiedad ubicada en Palm Island, en Miami Beach, Florida, donde serecluy con su esposa del mundo exterior. El 21 de enero de 1947, sufri un derramecerebral, y muri cuatro das despus de neumona: Al Capone fue encontradomuerto en la baera(en: ...Ruined, physically weak and mentally deteriorating, Capone retired to aproperty located on Palm Island in Miami Beach, Florida, where he and his wifesecluded themselves from the outside world. On January 21, 1947, he suffered astroke, and died four days later of pneumonia: Al Capone was found dead in thebathtub.",
  "Retrieval er-rorIncluysupapelenGuardianes de la GalaxiaVol. 2014Translated:Includedhis role in Guardians ofthe Galaxy Vol. 2014": "Native label: T, Model label: F, Ground truth: T.Comment: The retriever fails to retrieve the needed information passage for evalua-tion.Evidence: En 2014, logr el reconocimiento a nivel mundial al protagonizar lapelcula Guardianes de la Galaxia (2014) con el papel de Peter Quill / Star-Lord.23El filme recibi elogios de la crtica por su humor y fue un xito comercial trasrecaudar 773 millones de dlares, adems de convertirse en la cuarta pelcula mstaquillera de 2014Translated: In 2014, he achieved worldwide recognition by starring in the filmGuardians of the Galaxy (2014) with the role of Peter Quill / Star-Lord.23 The filmreceived critical praise for its humor and was a commercial success after grossing$773. million dollars, in addition to becoming the fourth highest-grossing film of2014",
  ".Translated: His fatherwas a musician": "Native label: F, Model label: T, Ground truth: T.No related information within the provided Wikipedia page. But there is supportingevidence from en.wikipedia.Evidence: His mother is dancer Kine Gueye Thiam (ne Gueye), and his father ispercussionist Mor Thiam. Mor Thiam was born to a Toucouleur family of Quranicscholars in Kaolack, Senegal.",
  "Retrieval er-ror .Translated: The nameof the second child isRory John Gates": "Native label: T, Model label: F, Ground truth: T.Comment: The retriever fails to retrieve needed information passage for evaluation.Evidence: : )( )( )(. . .(Translated: Bill Gates married Melinda French in 1994 and they have threechildren: Jennifer Katherine (1996), Rory John (1999), and Phoebe Adele (2002).The family lives in a huge, expensive modern house overlooking a lake in Washington,DC. From 1996 AD until 2006 AD, Bill Gates held the title of the richest man inthe world. His wealth was estimated in 1999 at 100 billion US dollars, and heascended to the throne again in 2007 AD.",
  ": Examples from each disagreement category between natives and GPT-4 in Arabic": "Original sentence (in English): He then moved to Olympique Als in 2003 and Stade Brestois in 2004.- Facts extracted by GPT4 in English:+ He then moved to Olympique Als.+ He then moved to Olympique Als in 2003.+ He then moved to Stade Brestois.+ He then moved to Stade Brestois in 2004.+ Olympique Als is a team.+ Stade Brestois is a team.+ He moved to Olympique Als and Stade Brestois in consecutive years.- Facts extracted by GPT4 in Spanish:+ Se mud al Olympique Als. (En: He then moved to Olympique Als.)+ Se mud al Olympique Als en 2003. (En: He then moved to Olympique Als in 2003.)+ Olympique Als es un equipo. (En: Olympique Als is a team.)+ Luego se mud al Stade Brestois. (En: He then moved to Stade Brestois.)+ Se mud al Stade Brestois en 2004. (En: He moved to Stade Brestois in 2004.)+ Stade Brestois es un equipo. (En: Stade Brestois is a team.)- Facts extracted by GPT4 in Arabic:+ . (En: Moved to Olympique Ales)+ . (En: He moved to Olympique Ales in 2003)+ . (En: He then moved to Stade Brestois.)+ . (En: He moved to Stade Brestois in 2004.)",
  ": Example of atomic facts extracted by GPT4": "Original sentence (in English): He then moved to Olympique Als in 2003 and Stade Brestois in 2004.- Facts extracted by GemP in English:+ He moved to Olympique Als.+ He moved to Olympique Als in 2003.+ He moved to Stade Brestois.+ He moved to Stade Brestois in 2004.- Facts extracted by GemP in Spanish:+ Se mud al Olympique Als. (En: He moved to Olympique Als.)+ Se mud al Olympique Als en 2003. (En: He moved to Olympique Als in 2003.)+ Se mud al Stade Brestois. (En: He moved to Stade Brestois.)+ Se mud al Stade Brestois en 2004. (En: He moved to Stade Brestois in 2004.)- Facts extracted by GemP in Arabic:+ . (En: He moved to Olympique)+ . (En: He moved to Olympique in 2003)+ . (En: He moved to Stade Prestois.)+ . (En: He moved to Stade Prestois in 2004.)",
  ": Example of atomic facts extracted by GemP": "Original sentence (in English): He then moved to Olympique Als in 2003 and Stade Brestois in 2004.- Facts extracted by GPT3.5 in English:+ He then moved to Olympique Als.+ He then moved to Olympique Als in 2003.+ He then moved to Stade Brestois.+ He then moved to Stade Brestois in 2004.- Facts extracted by GPT3.5 in Spanish:+ Se mud al Olympique Als. (En: He moved to Olympique Als.)+ Se mud al Olympique Als en 2003. (En: He moved to Olympique Als in 2003.)+ Se mud al Stade Brestois. (En: He moved to Stade Brestois.)+ Se mud al Stade Brestois en 2004. (En: He moved to Stade Brestois in 2004.)- Facts extracted by GPT3.5 in Arabic:+ . (En: He moved to Olympique Ales)+ . (En: He moved to Olympique Ales in 2003)+ .(En: He moved to Stade Brestois.)+ . (En: He moved to Stade Brestois in 2004.)",
  ": Example of atomic facts extracted by GPT3.5": "Original sentence (in English): He then moved to Olympique Als in 2003 and Stade Brestois in 2004.- Facts extracted by Mistral Instruct in English:+ He then moved to Olympique Als.+ He then moved to Olympique Als in 2003.+ He then moved to Stade Brestois.+ He then moved to Stade Brestois in 2004.- Facts extracted by Mistral in Spanish:+ Se mud al Olympique Als. (En: He moved to Olympique Als.)+ Se mud al Olympique Als en 2003. (En: He moved to Olympique Als in 2003.)+ Se mud al Stade Brestois. (En: He moved to Stade Brestois.)+ Se mud al Stade Brestois en 2004. (En: He moved to Stade Brestois in 2004.)- Facts extracted by Mistral Instruct in Arabic:+ . (En: He moved: INCOMPLETE)+ . (En: He moved to Olympique)+ . (En: He moved to Olympique in 2003)+ . (En: Stade Prestois - INCOMPLETE)+ . (En: Stade Prestois in 2004 - INCOMPLETE)+ .",
  ": Example of atomic facts extracted by Mistral-Instruct": "Original sentence (in English): He then moved to Olympique Als in 2003 and Stade Brestois in 2004.- Facts extracted by Llama-2 Chat in English:+ He moved to Olympique Als.+ He moved to Olympique Als in 2003.+ He moved to Stade Brestois.+ He moved to Stade Brestois in 2004.- Facts extracted by Llama-2 Chat in Spanish:+ Luego se mud. (En: Then he moved.)+ Se mud al Olympique Als. (En: He moved to Olympique Als.)+ Se mud al Stade Brestois. (En: He moved to Stade Brestois.)",
  "Insufficient covering information from the original sentence, years of movement": "- Facts extracted by Llama-2 Chat in Arabic:+ . (En: Then move on: INCOMPLETE)+ . (En: Go then: INCOMPLETE)+ . (En: To Olympique Alice: INCOMPLETE)+ . (En: To Olympique Alice in 2003: INCOMPLETE)+ . (En: Prestois Stadium: INCOMPLETE)+ . (En: Prestois Stadium in 2004: INCOMPLETE)- Facts extracted by Llama-2 Chat in Bengali:",
  ": Example of atomic facts extracted by Llama-2 Chat": "Original sentence (in English): He then moved to Olympique Als in 2003 and Stade Brestois in 2004.- Facts extracted by Gemma Instruct in English:+ He moved to Olympique Als.+ He moved to Olympique Als in 2003.+ He moved to Stade Brestois.+ He moved to Stade Brestois in 2004.- Facts extracted by Gemma Instruct in Spanish:+ Se mud al Olympique Als. (En: He moved to Olympique Als.)+ Se mud al Olympique Als en 2003. (En: He moved to Olympique Als in 2003.)+ Se mud al Stade Brestois. (En: He moved to Stade Brestois.)+ Se mud al Stade Brestois en 2004. (En: He moved to Stade Brestois in 2004.)- Facts extracted by Gemma Instruct in Arabic:+ . (En: Moved to Olympic: DUPLICATED)+ . (En: Moved to Olympic: DUPLICATED)+ . (En: Moved to Olympic: DUPLICATED)+ . (En: Moved to Olympic: DUPLICATED)",
  ": Example of atomic facts extracted by Gemma-7B-Instruct": "Original sentence (in English): He then moved to Olympique Als in 2003 and Stade Brestois in 2004.- Facts extracted by Finetuned Gemma in English:+ He moved to Olympique Als.+ He moved to Olympique Als in 2003.+ He moved to Stade Brestois.+ He moved to Stade Brestois in 2004.- Facts extracted by Finetuned Gemma in Spanish:+ Se mud al Olympique Als. (En: He moved to Olympique Als.)+ Se mud al Olympique Als en 2003. (En: He moved to Olympique Als in 2003.)+ Se mud al Stade Brestois. (En: He moved to Stade Brestois.)+ Se mud al Stade Brestois en 2004. (En: He moved to Stade Brestois in 2004.)- Facts extracted by Finetuned Gemma in Arabic:+ . (En: He moved to Olympique)+ . (En: He moved to Olympique in 2003)+ . (En: He moved to Stade Prestois.)+ . (En: He moved to Stade Prestois in 2004.)",
  "Spivak ha escrito sobre latraduccin Spivak has writtenabout translation": "Gayatri Chakravorty Spivak is an Indian scholar, literary theorist, and feminist critic. Shehas made a significant statement on the politics of translation ... Living Translationperforms the invaluable service of gathering for the first time Gayatri Chakravorty Spivakswide-ranging writings on translation.Comment: Gayatri Chakravorty Spivaks Wikipedia page has no information about herresearch on the impact of translation. Yet, using Google Query API returns informationabout her two books on the topic (\"The Politics of Transaltion\" and \"Living Translation\").",
  ". He privatized manystate companies": "It is clear from the reports data that the largest privatization operations in the historyof Morocco took place during the governments of Abderrahmane Youssoufi and IdrissJettou, and they generated large sums of money.Comment: The additional information supports the fact about privatization initiativesunder Idriss Jettous term, whereas the Wikipedia page has no related information about it. : Examples demonstrate that using Google Query API provides additional information to the scorer, GemP,leading to accurate fact labeling. The additional information from the examples is not present on Wikipedia pagesand has been manually validated as correct.",
  "esImplement polticas rad-icals. He implemented rad-ical policies": "Query: Qu tipo de polticas implement Hafizullah Amn? What kind of policies did Hafizullah Amin implement?GPT-4 Texts: Durante su tiempo en el poder, Amn se desempe como primer ministroy luego, brevemente, como presidente de Afganistn desde septiembre hasta diciembre de1979. Sus polticas se centraron en la implementacin de una agenda socialista radical,que inclua reformas agrarias destinadas a abolir las tradicionales tenencias feudales detierra y redistribuir las tierras a los campesinos sin tierra. During his time in power, Amin served as prime minister and then briefly as presidentof Afghanistan from September to December 1979. His policies focused on the implemen-tation of a radical socialist agenda, which included land reforms aimed at abolishing thetraditional feudal land tenures and redistributing land to landless peasants.Comment: The additional information generated by GPT4 supports the fact about radicalreforms by Hafizullah Amin, whereas the Wikipedia page has no related information aboutit. The generated information is manually confirmed to be correct",
  "ar . The Apprentice\" be-gan airing in 2004": "Query: When did Donald Trumps show \"The Apprentice\" start airing?GPT-4 Texts: . . The Apprentice\" is an American reality television show that began airing in the UnitedStates. The program debuted on January 8, 2004 on NBC.Comment: The additional information generated by GPT4 supports the fact about theairing time of \"The Apprentice\", whereas the Wikipedia page has no related informationabout it. The generated information is manually confirmed to be correct. : Examples demonstrate that using GPT-4 as a knowledge generator provides additional information to thescorer, GemP, leading to accurate fact labeling. The additional information from the examples is not present onWikipedia pages and has been manually validated as correct."
}