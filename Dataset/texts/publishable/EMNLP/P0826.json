{
  "Abstract": "While Large Language Models (LLMs) demon-strate impressive generation abilities, they fre-quently struggle when it comes to specializeddomains due to their limited domain-specificknowledge. Studies on domain-specific LLMsresort to expanding the vocabulary before fine-tuning on domain-specific corpus, aiming todecrease the sequence length and enhance effi-ciency during decoding, without thoroughlyinvestigating the results of vocabulary expan-sion to LLMs over different domains. Ourpilot study reveals that expansion with only asubset of the entire vocabulary may lead to su-perior performance. Guided by the discovery,this paper explores how to identify a vocab-ulary subset to achieve the optimal results.We introduce VEGAD, an adaptive method thatautomatically identifies valuable words from agiven domain vocabulary. Our method has beenvalidated through experiments on three Chinesedatasets, demonstrating its effectiveness. Ad-ditionally, we have undertaken comprehensiveanalyses of the method. The selection of a opti-mal subset for expansion has shown to enhanceperformance on both domain-specific tasks andgeneral tasks, showcasing the potential of VE-GAD.",
  "Introduction": "Despite achieving satisfactory performance on awide range of tasks (OpenAI et al., 2024; Touvronet al., 2023a; Xu et al., 2023; Yuan and Zhu, 2023),Large Language Models (LLMs) continue to en-counter challenges, particularly in domain-specifictasks, such as the generation of legal, medical,and financial texts. The expansion of vocabulary(Provilkov et al., 2020; Liu et al., 2021; Ozdemirand Goksel, 2019; Rothe et al., 2020) serves asa strategy to enhance the decoding efficiency for",
  ": Pilot study: Relative improvement comparingwith direct supervised fine-tuning, by adding vocabularywith different sizes": "domain-specific LLMs. By concatenating specific,frequent n-grams into new words, the token se-quence is shortened, thereby visibly boosting effi-ciency. Cui et al. (2024) extended LLaMAs exist-ing vocabulary with an additional 20,000 Chinesetokens, thereby improving its encoding efficiencyand semantic understanding of Chinese. LawGPT1 is fine-tuned based on the general Chinese LLMs(such as Chinese-LLaMa, ChatGLM (Du et al.,2022), etc.), the legal domain specific vocabularyis expanded to enhance the semantic understandingability of the LLMs.Current researches primarily focus on some spe-cific domains. Nonetheless, they have not thor-oughly elucidate the performance enhancementsresulting from vocabulary expansion in various do-mains. We conduct a pilot study illustrating thedomain performance and general capabilities aftervocabulary expansion with different sizes, and theresults are illustrated in . It is revealedthat augmenting the size of the newly added vo-",
  ": Framework of VEGAD": "cabulary does not invariably result in improvedmodel performance. Hence, an essential ques-tion arises regarding the generation of an optimalsubset for vocabulary expansion given a candi-date vocabulary. The process of selecting high-value vocabulary during the expansion of domain-specific LLMs is akin to gold panning, as it requirescareful selection rather than indiscriminate enlarge-ment of the lexicon to enhance the performance ofthe LLMs. We recognize the following challengesfor vocabulary subset generation:",
  "How to automatically adapt to any domain?": "To effectively identify the crutial words froma candidate vocabulary, we have proposed VE-GAD (abbreviation of Vocabulary Expansion viaGrADients), which is an adaptable vocabulary ex-pansion method via gradients. providesan illustration of the framework. Intuitively, tokengroups displaying larger gradients in domain in-stances are deemed more pivotal to the task andshould be integrated into the vocabulary as domain-specific terms. Therefore, it is a straightforwardapproach to trace the gradient of each word, whilethere are several difficulties, such as the algorithmto efficiently retrieve the candidate words fromthe token sequences, and the gradient calculationacross various tokens rather than the whole se-quence. To identify candidate words from the tokensequences, we build a Trie (Black, 2019) based onthe candidate vocabulary, and design an algorithmto record the gradient for each word with the Trie.To distinguish the effect of each token, the gradientis calculated on the running tensors, instead of theweights of the LLMs.To scrutinize the efficacy of VEGAD, we haveundertaken comprehensive studies. The findingsacross three Chinese datasets, pertaining to the domains of law and medicine, underscore a su-periority in comparison to other lexicon genera-tion techniques, as well as the promising prospectsof domain-specific vocabulary expansion. Our in-quiry reveals that the domain-specific lexicon byVEGAD enhances performance in tasks requiringspecialized knowledge as well as tasks demandinggeneral skills. We hope that our multi-perspectiveanalysis serves as a catalyst for future investiga-tions into enhancing domain-task performance andmitigating the Catastrophic Forgetting through do-main vocabulary adaptation.In summary, our contributions are three folds:",
  "Related Work": "Large Language Models, such as ChatGPT2, GPT-4 (OpenAI et al., 2024), exhibit amazing abilitieson understanding and text generation. They canhandle the tasks of QA, reasoning and math cal-culation even under zero-shot scenarios. LLaMa(Touvron et al., 2023a) is a collection of open foun-dation language models ranging from 7B to 65Bparameters. Touvron et al. (2023b) developed andreleased Llama 2, a collection of LLMs ranging inscale from 7B to 70B parameters. The fine-tunedLlama 2-Chat, are optimized for dialogue use cases.There are other popular LLMs developed with vari-ous skills (Rozire et al., 2024; Almazrouei et al.,2023; Jiang et al., 2023; Bai et al., 2023; Baichuan,2023).Due to the lack of domain-specific knowledge,general LLMs fall short at handling domain ques-tions. Therefore domain-specific LLMs are devel-oped by fine-tuning on domain corpus. (Xiong",
  "LM Head layer": ": Gradient Calculation for each candidate word. Given the Trie built from candidate vocabulary, we checkwhether there exists a sub-sequence of the input and output on the path from the root of the Trie to a leaf node, by apointer. The trace of the pointer is illustrated by Vi and the pseudo-leaf node. Finally, the top K words with thelargest gradients are selected to construct the new vocabulary, and used to resize the embedding layer and languagemodeling head layer. et al., 2023) collected databases of medical di-alogues with the help of ChatGPT and adoptedseveral techniques to train an easy-deploy LLM,called DoctorGLM. Wang et al. (2023a) pro-posed HuaTuo, a LLaMA-based model that hasbeen supervised-fine-tuned with generated QA(Question-Answer) instances in biomedical domaintasks, with medical expertise in the responses. Cuiet al. (2023) proposed an open-source legal LLMnamed ChatLaw, with a method that combines vec-tor database retrieval with keyword retrieval to ef-fectively reduce the inaccuracy of relying solelyon vector database retrieval, and a self-attentionmethod to enhance the ability to overcome errorspresent in reference data. There are other domainsstudied including finance (Wang et al., 2023b; Yu,2023), education (Yu et al., 2023a), science (Liet al., 2023b) and e-commerce (Li et al., 2023a). Several previous studies adopt a strategy, vocab-ulary expansion, to improve the performance ofdomain SFT. Specifically, a domain-specific vo-cabulary is automatically generated or manuallydesigned, and added into the tokenizer.In or-der to augment LLaMA with capabilities for un-derstanding and generating Chinese text and itsability to follow instructions, Cui et al. (2024) ex-tended LLaMAs existing vocabulary with an addi-tional 20,000 Chinese tokens, thereby improvingits encoding efficiency and semantic understand-ing of Chinese. Liu et al. (2023) proposed task-adaptive tokenization as a way to adapt the genera-tion pipeline to the specifics of a downstream taskand enhance long-form generation in mental health.However, their task-adaptive tokenizer samples variable segmentations from multiple outcomes,which may change the vanilla behavious of othertokenizers (e.g., WordPiece and BPE). LaWGPTexpands the legal domain specific vocabulary andlarge-scale Chinese legal corpus pre-training on thebasis of the general Chinese base model (such asChinese-LLama, ChatGLM, etc.), and enhances thebasic semantic understanding ability of the LLMin the legal field. Tongyi-Finance-14B3 expandedthe vocabulary of financial domain in Qwen-14B,and the size of the vocabulary is 150,000. Basedon the BPE vocabulary used in GPT-4, the vocabu-lary is optimized for Chinese and multi-language.The numbers are divided into individual digits. Liuet al. (2024b) identified tokens that are absent inthe general-purpose tokenizer and are rarely foundin general-purpose datasets, from the vocabularyof the new tokenizer. They initialize model embed-dings of the new tokens by utilizing the general-purpose tokenizer. Liu et al. (2021) introduced twonew approaches based on attention to initialize theweights of new added words.",
  ": end for": "sion in the lexicon as domain-specific terminology.Nonetheless, there are several challenges. For ex-ample, the algorithm to efficiently retrieve the can-didate words from the token sequences, and thegradient calculation across various tokens ratherthan the whole sequence.Specifically, starting from the domain-specificdata, sentences are divided into discrete words. Thecandidate vocabulary is constructed with words ab-sent from the general lexicon. Subsequently, theprocess of selection is executed on domain-specificinstances by computing the gradients for each nodewithin the embedding tensor and the language mod-eling tensor, with reference to a Trie constructedbased on the candidate vocabulary. The top Kwords exhibiting the highest overall gradients areretained to establish the specialized domain vocab-ulary. Then we resize the LLM and incorporatethe tokenizer with new vocabulary, following anoptional weight initialization. Then we conductdomain SFT on the LLM, to develop the domain-specific LLM.The advantage of VEGAD can be summarizedas following: 1) VEGAD is a plug-and-play task-adaptive vocabulary selection method, seamlesslyintegrating with diverse techniques utilized in su-pervised fine-tuning. 2) In contrast to previousmethods such as Liu et al. (2023), which mightalter the intrinsic behaviors of current tokenizerssuch as WordPiece and BPE by imposing an oblig-atory scoring mechanism for sampling in accor-dance with their guidelines, VEGAD is tokenizer-agnostic, and compatible to any tokenization algo-",
  "Build Trie": "The Trie, as discussed by Black (2019), representsa distinct tree-based data structure, extensively em-ployed within the realm of computer science forthe administration of dynamic sets or associativearrays, with the keys predominantly being strings.Diverging from the structure of a binary search treein which a nodes placement is influenced by nu-merical or logical hierarchy, in a Trie, the locationof a node is unequivocally defined by the sequenceof characters it denotes. We illustrate an exampleof Trie in the left part of .Formally, the domain-specific dataset can berepresented as D = {(X1, Y1), , (Xn, Yn)},where X and Y are the query and response respec-tively, n is the size of D. Given a text segmentationtool, the candidate vocabulary is constructed fol-lowing",
  "Wi = tokenize(wi) = [t1i , t2i , , tlii ](2)": "Note that li > 1 because each word in the candidatevocabulary doesnt exist in the general tokenizerslexicon. Let V0 be the root of the Trie. For eachword wi, we insert its tokens one by one into theTrie, starting from V0. Additionally, we set a flagof pseudo-leaf node to each tlii node, which isthe last token of the word wi4. Note that each pathfrom the root to a pseudo-leaf node represents acandidate word in V. The procedure is illustratedin Algorithm 1. With the algorithm, we get a Triewith M nodes. 4The pseudo-leaf node is different from the traditionalconcept of leaf node in tree-based data structures. Theremay be children nodes for pseudo-leaf node, because sometoken sequence Wj may start from another Wi.",
  "Gradient Calculation": "With the general tokenizer, the sentences are con-verted to input query tokens and output response to-kens. For simplicity, the input and output sequenceof the LLM are denoted as x = [x1, , xL]and y = [y1, , yL] respectively, where L isthe length of the sequences. Current LLMs firstlyembed the input tokens to in a high-dimensionspace, then perform transformers on the embeddingvectors . The representation h output by severaltransformer blocks is finally converted to the distri-bution y over tokens through a language modelinghead layer:",
  "(6)": "For the embedding tensor, we calculate the gra-dients of each input token as Gembed. Althoughprevious studies mostly only focus on the embed-ding layer, we find that the language modeling headlayer is also important especially for text genera-tion tasks. Therefore, we calculate the gradientsGlmhead for each output token only if it is not aspecial token (e.g., [CLS], [SEP] and [PAD]). Toobtain the gradient at each time step, Equation 5 ismodified as:",
  "(8)": "Then we calculate the gradient for each candidateword by looking up nodes in the Trie and iterat-ing over x and y. The candidate words appear-ing in the sequence can be identified by movinga pointer from the root V0 initially. During enu-merating i from 1 to L, we check if there exists a sub-sequence xi:j in Trie. Specifically, from theroot, the pointer constantly moves to its childrenuntil it reaches the last pseudo-leaf node or thetoken mismatches any child of the current node.Once the pointer reaches a node V attributed withpseudo-leaf node, we add the norm of the gradi-ents of the sub-sequence to w, where w denotes thecandidate word represented by V .",
  "(9)": "Note that there is a position shift for the outputsequence (i.e. xi:j = yi1:j1). We provide thedetailed code in Algorithm 2.To enhance efficiency, the algorithms cost oftime can be optimized by adopting prefix accu-mulation in conjunction with the AhoCorasickAlgorithm. This optimization is particularly sig-nificant in cases involving Tries of considerablesize and depth, resulting in a notable reduction inthe algorithms overall complexity. The detailedoptimization is described in Appendix L.",
  "Vocabulary Selection": "Upon evaluating the gradient associated with eachword from the candidate vocabulary, the words areorganized in descending order based on the magni-tude of their gradients. We obtain the top K wordsand remove other words. These selected words arethen integrated into the pre-existing general vocab-ulary. The embedding layer and language modelinghead layer are also resized to R(C+K)d.For initialization, the default method is averag-ing the weights of sub-tokens in the original layer,following Liu et al. (2023). We also investigatedother approaches and the results are discussed inAppendix G.",
  "Experiments": "The main results on three datasets from two do-mains are discussed in Sub.2. Then wediscuss the influence of the vocabulary size in Sub-.3. To verify our hypothesis, we comparethe words with different gradients in Appendix C.We also remove the pre-built candidate vocabu-lary, to investigate the influence of direct gradi-ent calculation on 2-gram tokens of the sequencein Appendix D. There are also discussions about",
  ": Relative improvement after SFT on Article QA,comparing to general LLM. The metrics are reported inpercentage": "the influence of the language modeling head layer,model scale and weight initialization methods inAppendix E, F and G, respectively.Our study incorporates three domain-specificdatasets from two distinct domains: Article QAdataset for the legal domain, and CMedQA (Zhanget al., 2018) and CMDD (Toyhom, 2023) datasetsfor the medical field.Furthermore, we delveinto the Catastrophic Forgetting issue in gen-eral tasks following supervised fine-tuning ondomain-specific instances. To this end, we ana-lyze three datasets: ALPACA (Peng et al., 2023)for tasks requiring instruction following, GSM8K(Yu et al., 2023b) focused on mathematics, andSafetyPrompts (Sun et al., 2023) concerning safety.The metrics and details of the dataset considerationand construction are described in Appendix A.",
  "the expert-designed legal vocabulary by LawGPT5": "is used. For medical domain, we prompt GPT-4to extract the names of medicine, symptom andtherapies from the sentences. We keep words thatappear more than 100 times in the data to improvethe effectiveness, because increasing the size of thenewly added vocabulary does not invariably resultin improved model performance, according to ourexperiment in Sub.3.",
  "SPMWe train a tokenizer with SentencePiece(Kudo and Richardson, 2018), which is a com-mon method to generate domain-specific vocab-ulary (Cui et al., 2024). We utilize the off-the-shelfpackage6": "ATT_EG and PATT_EGLiu et al. (2021) in-troduced two weight initialization methods basedon attention mechanism, ATT_EG and PATT_EG.They apply the methods on the generated vocabu-lary by SPM for downstream tasks. JiebaInspired by SPM, we adopt another textsegmentation tool, Jieba7. From the experiments,we find it to be a strong and convenient baselinefor text generation tasks.Implementation details are shown in Appendix",
  ": Relative improvement after SFT on CMDD,comparing to general LLM. The metrics are reported inpercentage": "of the direct SFT approach, and a ROUGE-L scorethat surpasses DV by 1.5 points. 2) VEGAD ex-hibits the highest scores across all evaluated metricsfor the domain-specific task, with its ROUGE-Lscore nearly one point higher than that of Jieba. Insummary, VEGAD consistently outperforms othervocabulary generation methods, showcasing stableimprovement. 3) In the realm of instruction follow-ing, the performance differential among the meth-ods is modest. The highest BLEU score, attainedby SPM, is marginally greater, by approximately0.6 points, than the lowest score. VEGAD achievesthe second-highest BLEU score. This relativelynarrow range of scores could be attributed to theuniformity of training across all methods on thesame QA dataset, which inherently bears a resem-blance to the instruction-following format. 4) On the GSM8K dataset, which consists of questionsthat require mathematical calculations, we observea significant drop in accuracy, indicative of CF. Thegeneral chat LLM initially achieves an accuracyof 22.10%. Yet, following domain-specific SFT,even the highest accuracy attained by the baselinemethods, 14.50% by DV, shows a relative decreaseof 34.39% from the pre-fine-tuning performance.When VEGAD is incorporated, there is a slightimprovement in accuracy to 15.20%, which corre-sponds to a relative decrease of 31.22%. Whenusing the whole Jieba vocabulary, the accuracyis less than half of VEGAD, with a relative de-crease of more than 70% comparing to GeneralLLM. It proves the weakness of Jieba and the ef-fectiveness of VEGAD. 5) The general chat LLMachieves a high accuracy of 94% on the safety task.Nonetheless, direct domain-specific SFT induces anotable reduction in accuracy to 88.30%. The dataindicates that all vocabulary expansion methods,including VEGAD, result in either a reduction orequality in the extent of forgetting when comparedto the direct SFT. Among these methods, VEGADregisters the highest accuracy, reaching 89.60%,which represents a relative decrease of 4.68% fromthe original accuracy achieved by the general chatLLM.",
  "Medical Domain": "The results of the medical domain are shown in and 4. We also report the relative improve-ments after SFT on CMDD in . 1) Uponcomparing the results with those from the legal do-main, it is evident that the medical scores are com-paratively low and that the enhancement yieldedby domain-specific SFT is modest. Despite thelimited scope of improvement, VEGAD distin-guishes itself by delivering the best results acrossall metrics for both datasets in the medical do-main. The medical domain responses encompass abreadth of viewpoints, including potential causes,treatment drugs, and precautionary measures. Thisdiversity amplifies the complexity and presents agreater challenge for language modeling tasks. 2)In the context of solving math problems, DV standsout by achieving higher accuracy rates than otherbaselines after being fine-tuned on both CMedQAand CMDD datasets. Conversely, Jieba performspoorly under both settings, representing a substan-tial relative decrease of 63.8%, after fine-tuningon CMDD. VEGAD marks the pinnacle of per-formance by reaching an accuracy of 18.40% afterfine-tuning on the CMDD dataset, which signifies arelative 16.74% decrease in calculation ability com-pared to before fine-tuninga notable improve-ment over Jieba. 3) On the safety choice problems,Jieba ties or outperforms VEGAD.In summary, we find that VEGAD not only im-proves the performance on domain tasks, but alsohelps to mitigate the problem of forgetting.",
  "Vocabulary Size": "The size of added domain-adaptive vocabulary isimportant in vocabulary expansion. We conduct astudy on the vocabulary generated by Jieba. Wecount the times that each word appear in the train-ing corpus, and filter words that appear more than0, 10, 100, and 1000 times. By adding the corre-sponding words into the vocabulary, we plot resultfine-tuning on CMedQA in .At the beginning, it brings benefits by increasingthe vocabulary size. While the best performancepresents close to 2500 and 3000. However, whenadding all 4658 words (i.e. Jieba baseline), thedecrease on math reaches about 50%, and theaverage result decreases more than 10%.It is reasonable that, a number of appropriatelyselected words can improve domain performancebecause it introduces new trainable parameters for",
  ": Relative improvement of VEGAD comparingwith direct SFT, by adding vocabulary with differentsizes": "domain-specific terminology and concepts. Addi-tionally, the representation shift caused by SFT isshared by the addition of new words, thus the repre-sentation of original tokens are kept, mitigating theproblem of CF. However, when the vocabulary sizeconstantly increases, the vanilla tokenization couldbe broken. More and more unseen tokens appearwithin one instance at the same time. Without ap-propriate initialization, the previously pre-trainedknowledge can not be inherited, and the represen-tation on general corpus also shifts.",
  "Conclusion": "The influence of adding domain-specific words andthe generation of domain vocabulary are far frombeing explored for LLMs. In this paper, we investi-gate the influence of adding domain vocabulary toLLMs from the perspective of both domain exper-tise and forgetting of general capabilities. We findthat expansion with only a subset of the entire vo-cabulary may lead to superior performance. Basedon which, an automatic approach to identify effec-tive words from a candidate vocabulary, called VE-GAD, is proposed for the generation of an optimalsubset. Extensive experiments on three datasetsfrom two domains, are conducted to prove the ef-fectiveness of VEGAD. It is concluded from theanalyses that not only the performance on domain-specific tasks is improved, but also the problem ofcatastrophic forgetting is mitigated.",
  "Limitations": "Our work investigates the influence of vocabularygeneration for domain-specific LLMs, and intro-duces an automatic method based on gradients forboth domain tasks and general abilities. However,the methods to properly initialize the weights ofnew words are still far from explored. From ourexperiments, initialization by either simple calcu-lation based on the training corpus, or limited ex-ternal knowledge cannot bring stable improvementon the tasks. Thus it highlights the necessity of aneffective approach to calculate the weights withinthe embedding layer and language modeling headlayer, especially under low-resources scenarios.",
  "Acknowledgements": "This work was supported in part by NationalNatural Science Foundation of China (62441605,62376243, 62037001, U20A20387), National KeyResearch and Development Program of China(2022YFC3340900), the StarryNight Science Fundof Zhejiang University Shanghai Institute forAdvanced Study (SN-ZJU-SIAS-0010), AlibabaGroup through Alibaba Research Intern Program,Project by Shanghai AI Laboratory (P22KS00111),Program of Zhejiang Province Science and Tech-nology (2022C01044), the Fundamental ResearchFunds for the Central Universities (226-2024-00170).",
  "Alfred V. Aho and Margaret J. Corasick. 1975. Effi-cient string matching: an aid to bibliographic search.Commun. ACM, 18(6):333340": "Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-shamsi, Alessandro Cappelli, Ruxandra Cojocaru,Mrouane Debbah, tienne Goffinet, Daniel Hesslow,Julien Launay, Quentin Malartic, Daniele Mazzotta,Badreddine Noune, Baptiste Pannier, and GuilhermePenedo. 2023. The falcon series of open languagemodels. Preprint, arXiv:2311.16867. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, FeiHuang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,Xuancheng Ren, Chuanqi Tan, Sinan Tan, JianhongTu, Peng Wang, Shijie Wang, Wei Wang, Sheng-guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-uan Zhang, Yichang Zhang, Zhenru Zhang, ChangZhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang",
  "Edward J. Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. 2021. Lora: Low-rank adaptation oflarge language models. Preprint, arXiv:2106.09685": "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, Llio Renard Lavaud,Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,Thibaut Lavril, Thomas Wang, Timothe Lacroix,and William El Sayed. 2023. Mistral 7b. Preprint,arXiv:2310.06825. Prakhar Kaushik, Alex Gain, Adam Kortylewski, andAlan Yuille. 2021. Understanding catastrophic forget-ting and remembering in continual learning with opti-mal relevance mapping. Preprint, arXiv:2102.11343. Taku Kudo and John Richardson. 2018. SentencePiece:A simple and language independent subword tok-enizer and detokenizer for neural text processing. InProceedings of the 2018 Conference on EmpiricalMethods in Natural Language Processing: SystemDemonstrations, pages 6671, Brussels, Belgium.Association for Computational Linguistics. Yangning Li, Shirong Ma, Xiaobin Wang, Shen Huang,Chengyue Jiang, Hai-Tao Zheng, Pengjun Xie,Fei Huang, and Yong Jiang. 2023a.Ecomgpt:Instruction-tuning large language models with chain-of-task tasks for e-commerce.arXiv preprintarXiv:2308.06966.",
  "YuYang Li, CunShi Wang, MengWei Qu, Yu Bai,Roberto Soria, and JiFeng Liu. 2023b.Starglm": "Chengyuan Liu, Shihang Wang, Yangyang Kang, LizhiQing, Fubang Zhao, Changlong Sun, Kun Kuang, andFei Wu. 2024a. More than catastrophic forgetting:Integrating general capabilities for domain-specificllms. Preprint, arXiv:2405.17830. Mingjie Liu, Teodor-Dumitru Ene, Robert Kirby, ChrisCheng, Nathaniel Pinckney, Rongjian Liang, JonahAlben, Himyanshu Anand, Sanmitra Banerjee, Is-met Bayraktaroglu, Bonita Bhaskaran, Bryan Catan-zaro, Arjun Chaudhuri, Sharon Clay, Bill Dally,Laura Dang, Parikshit Deshpande, Siddhanth Dhodhi,Sameer Halepete, Eric Hill, Jiashang Hu, SumitJain, Ankit Jindal, Brucek Khailany, George Kokai,Kishor Kunal, Xiaowei Li, Charley Lind, Hao Liu,Stuart Oberman, Sujeet Omar, Ghasem Pasandi,Sreedhar Pratty, Jonathan Raiman, Ambar Sarkar,Zhengjiang Shao, Hanfei Sun, Pratik P Suthar, VarunTej, Walker Turner, Kaizhe Xu, and Haoxing Ren.2024b. Chipnemo: Domain-adapted llms for chipdesign. Preprint, arXiv:2311.00176. Siyang Liu, Naihao Deng, Sahand Sabour, Yilin Jia,Minlie Huang, and Rada Mihalcea. 2023.Task-adaptive tokenization: Enhancing long-form text gen-eration efficacy in mental health and beyond. In Pro-ceedings of the 2023 Conference on Empirical Meth-ods in Natural Language Processing, pages 1526415281, Singapore. Association for ComputationalLinguistics. Xin Liu, Baosong Yang, Dayiheng Liu, Haibo Zhang,Weihua Luo, Min Zhang, Haiying Zhang, and Jin-song Su. 2021. Bridging subword gaps in pretrain-finetune paradigm for natural language generation.In Proceedings of the 59th Annual Meeting of theAssociation for Computational Linguistics and the11th International Joint Conference on Natural Lan-guage Processing (Volume 1: Long Papers), pages60016011, Online. Association for ComputationalLinguistics. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-man, Diogo Almeida, Janko Altenschmidt, Sam Alt-man, Shyamal Anadkat, Red Avila, Igor Babuschkin,Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-ing Bao, Mohammad Bavarian, Jeff Belgum, Ir-wan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,Christopher Berner, Lenny Bogdonoff, Oleg Boiko,Madelaine Boyd, Anna-Luisa Brakman, Greg Brock-man, Tim Brooks, Miles Brundage, Kevin Button,Trevor Cai, Rosie Campbell, Andrew Cann, BrittanyCarey, Chelsea Carlson, Rory Carmichael, BrookeChan, Che Chang, Fotis Chantzis, Derek Chen, SullyChen, Ruby Chen, Jason Chen, Mark Chen, BenChess, Chester Cho, Casey Chu, Hyung Won Chung,Dave Cummings, Jeremiah Currier, Yunxing Dai,Cory Decareaux, Thomas Degry, Noah Deutsch,Damien Deville, Arka Dhar, David Dohan, SteveDowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,Simn Posada Fishman, Juston Forte, Isabella Ful-ford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, ScottGray, Ryan Greene, Joshua Gross, Shixiang ShaneGu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,Yuchen He, Mike Heaton, Johannes Heidecke, ChrisHesse, Alan Hickey, Wade Hickey, Peter Hoeschele,Brandon Houghton, Kenny Hsu, Shengli Hu, XinHu, Joost Huizinga, Shantanu Jain, Shawn Jain,Joanne Jang, Angela Jiang, Roger Jiang, HaozhunJin, Denny Jin, Shino Jomoto, Billie Jonn, Hee-woo Jun, Tomer Kaftan, ukasz Kaiser, Ali Ka-mali, Ingmar Kanitscheider, Nitish Shirish Keskar,Tabarak Khan, Logan Kilpatrick, Jong Wook Kim,Christina Kim, Yongjik Kim, Jan Hendrik Kirch-ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,ukasz Kondraciuk, Andrew Kondrich, Aris Kon-stantinidis, Kyle Kosic, Gretchen Krueger, VishalKuo, Michael Lampe, Ikai Lan, Teddy Lee, JanLeike, Jade Leung, Daniel Levy, Chak Ming Li,Rachel Lim, Molly Lin, Stephanie Lin, MateuszLitwin, Theresa Lopez, Ryan Lowe, Patricia Lue,Anna Makanju, Kim Malfacini, Sam Manning, TodorMarkov, Yaniv Markovski, Bianca Martin, KatieMayer, Andrew Mayne, Bob McGrew, Scott MayerMcKinney, Christine McLeavey, Paul McMillan,Jake McNeil, David Medina, Aalok Mehta, JacobMenick, Luke Metz, Andrey Mishchenko, PamelaMishkin, Vinnie Monaco, Evan Morikawa, DanielMossing, Tong Mu, Mira Murati, Oleg Murk, DavidMly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh,Long Ouyang, Cullen OKeefe, Jakub Pachocki, AlexPaino, Joe Palermo, Ashley Pantuliano, Giambat-tista Parascandolo, Joel Parish, Emy Parparita, AlexPassos, Mikhail Pavlov, Andrew Peng, Adam Perel-man, Filipe de Avila Belbute Peres, Michael Petrov,Henrique Ponde de Oliveira Pinto, Michael, Poko-rny, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-ell, Alethea Power, Boris Power, Elizabeth Proehl,Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,Cameron Raymond, Francis Real, Kendra Rimbach,Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-der, Mario Saltarelli, Ted Sanders, Shibani Santurkar,Girish Sastry, Heather Schmidt, David Schnurr, JohnSchulman, Daniel Selsam, Kyla Sheppard, TokiSherbakov, Jessica Shieh, Sarah Shoker, PranavShyam, Szymon Sidor, Eric Sigler, Maddie Simens,Jordan Sitkin, Katarina Slama, Ian Sohl, BenjaminSokolowsky, Yang Song, Natalie Staudacher, Fe-lipe Petroski Such, Natalie Summers, Ilya Sutskever,Jie Tang, Nikolas Tezak, Madeleine B. Thompson,Phil Tillet, Amin Tootoonchian, Elizabeth Tseng,Preston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-lipe Cern Uribe, Andrea Vallone, Arun Vijayvergiya,Chelsea Voss, Carroll Wainwright, Justin Jay Wang,Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,CJ Weinmann, Akila Welihinda, Peter Welinder, Ji-ayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,Clemens Winter, Samuel Wolrich, Hannah Wong,Lauren Workman, Sherwin Wu, Jeff Wu, MichaelWu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-ing Yuan, Wojciech Zaremba, Rowan Zellers, ChongZhang, Marvin Zhang, Shengjia Zhao, TianhaoZheng, Juntang Zhuang, William Zhuk, and Bar-",
  "Sascha Rothe, Shashi Narayan, and Aliaksei Severyn.2020. Leveraging pre-trained checkpoints for se-quence generation tasks. Transactions of the Associ-ation for Computational Linguistics, 8:264280": "Baptiste Rozire, Jonas Gehring, Fabian Gloeckle, StenSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,Jingyu Liu, Romain Sauvestre, Tal Remez, JrmyRapin, Artyom Kozhevnikov, Ivan Evtimov, JoannaBitton, Manish Bhatt, Cristian Canton Ferrer, AaronGrattafiori, Wenhan Xiong, Alexandre Dfossez,Jade Copet, Faisal Azhar, Hugo Touvron, Louis Mar-tin, Nicolas Usunier, Thomas Scialom, and GabrielSynnaeve. 2024. Code llama: Open foundation mod-els for code. Preprint, arXiv:2308.12950.",
  "Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng,and Minlie Huang. 2023.Safety assessment ofchinese large language models.arXiv preprintarXiv:2304.10436": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Aurelien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. 2023a. Llama: Openand efficient foundation language models. Preprint,arXiv:2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023b. Llama 2: Open foundation andfine-tuned chat models. Preprint, arXiv:2307.09288.",
  ": Datasets used in the experiments": "Article QA is collected from a publicly availablelegal consulting website, which includes pairs ofreal-world queries and answers. For CMedQA, wedrop the column neg_ans_id, and remove dupli-cated lines. CMDD is a Chinese medical dialoguedataset, covering Andrology, Internal Medicine,Obstetrics and Gynecology, Oncology, Pediatricsand Surgery. We select the instances involvingInternal Medicine8.Additionally, we also investigate the forgettingproblem on general tasks after supervised fine-tuning on domain instances. The phenomenon isknown as Catastrophic Forgetting (CF), and studiedby several researchers (Kaushik et al., 2021; Cossuet al., 2022; Liu et al., 2024a). Therefore, it is nat-ural to wonder that whether vocabulary expansionhelps mitigate CF. By consulting domain expertsabout the general abilities required for the deploy-ment of domain-specific LLMs, we consider threeabilities: instruction following, math and safety.ALPACA (Peng et al., 2023) is the self-instructdataset based on GPT-4, and we use the Chineseversion9. GSM8K (Yu et al., 2023b) is a datasetfor mathematical reasoning. The publicly releasedversion is adopted, where question-answer pairsare translated in Chinese from GSM8K by GPT-3.5-Turbo with few-shot prompting10. For safety,",
  "Thedatasourceispubliclyavailableat": "we use SafetyPrompts (Sun et al., 2023). For easierevaluation, we obtain a safe response with GPT-4for each prompt of type Ethics_And_Morality,then construct 2 choices for each question (one safechoice and another unsafe choice). The LLM isprompted to identify the safe response.We report the average score of BLEU-1/2/3/4(denoted as BLEU), and ROUGE-L score for thetext generation tasks. We also report the accuracyof the calculated numeric result for GSM8K, andaccuracy for SafetyPrompts. While calculating theaccuracy of numerical results, we mainly followprevious work11, which extracts the results accord-ing to regex and complex patterns. The best resultsare highlighted with bold, and the second best re-sults are underlined. The statistics of the datasetsare listed in .",
  "BImplementation Details": "For VEGAD, we use Jieba as the text segmentationtool. We train all models on the domain-specifictask for 3 epochs. The train batch size is set to8, learning rate to 5 105, and we use the co-sine scheduler. The LLM is based on Qwen1.5(Bai et al., 2023) with 1.8B parameters. We down-load the parameters from HuggingFace12, and fine-tuned the model with LoRA (Hu et al., 2021) on1 A100 80G GPU. The rank is set to 16. Only theparameters of the embedding layer, language mod-eling head layer of newly added vocabulary and theadapters are trainable, while the others are frozen.",
  "CWords of Different Gradients": "To clearly present the influence of selection ongradient, we comparing the results by adding wordswith the top K gradients and bottom K gradients(non-zero) respectively. The results are shown in. It is obvious that on both Article QA andCMedQA, adding words with the largest gradientsleads to better overall results than using words with",
  ": Results comparison with 2-gram": "lowest gradients. For Article QA, the BLEU scoreis 2.5 higher, and ROUGE-L is about 3 point higher,than using words with lowest gradients. There isalso a significant advantage on CMedQA. For mathcalculation, adding words with largest gradientsachieves the accuracy 1% higher than adding low-gradient words by fine-tuning on CMedQA, but0.1% lower by fine-tuning on Article QA.We list several words with different gradients in to compare the differences. The explain-able words are translated into English, denoted as<Chinese>|<English>. The words with larger gra-dients are more explainable and specialize. Thisattribute can also lead to reasonable tokenizationand mitigate the forgetting.",
  ": Ablation study on the gradient of LMHeadLayer": "we calculate gradients for each 2-gram in the sameway as VEGAD, and sort the 2-grams together withthe words from V in descending order. Only thetop K words are kept finally. We compare theROUGE-L of Article QA, ALPACA, and accuracyof GSM8K, SafetyPrompts, as shown in .On the domain-task, VEGAD+2-gram outper-forms VEGAD by 0.25, since it directly optimizesthe gradients on the training task. But there is aforgetting problem on ALPACA and GSM8K. Es-pecially, the accuracy of GSM8K suffers from arelative decrease of 20.39%. The accuracy on Safe-tyPrompts by VEGAD+2-gram is slightly higherthan VEGAD.We also notice that there are many unexplain-able 2-gram words generated by selecting 2-grams.Therefore, VEGAD is more effective based on textsegmentation in summary.",
  "EInfluence of LMHead Layer": "The language modeling head layer (LMHeadLayer) converts the transformer output from hiddenstates to logits distribution over tokens. Previousstudies usually ignore the importance of LMHeadLayer. While in our work, we conduct an ablationstudy on LMHead Layer by ignoring the gradientof its output tensor (i.e. Glmhead). We plot the rela-tive improvement comparing with direct SFT. Theresult is illustrated in . The x-axis denotesthe tasks and correspond metrics.We notice a pattern from the figure that fordatasets that requiring text generation, w/o LM-Head suffers from a significant decrease. Whilethe accuracy is not influenced or even better. Therelative improvement on the domain task dropsfrom 6.86% to 1.01% after ignoring LMHead",
  ": Results of Qwen 7B fine-tuned on CMedQA": "Layer.There are also decrease on ROUGE-Lscores of ALPACA and GSM8K. However, the ac-curacy of w/o LMHead of GSM8K ties VEGAD,and the accuracy on SafetyPrompts is slightlyhigher than VEGAD.It is reasonable that considering the gradient oflanguage modeling output benefits the metrics oftext generation such as BLEU and ROUGE, be-cause it bridges the gap between hidden statesand logits. After removing the gradients of LM-Head Layer, the vocabulary adaptation concen-trates on the optimization of text understanding,rather than generating helpful responses accordingto the queries.",
  "FScale of LLM": "We scale up the foundation model from 1.8B to7B, and investigate the effectiveness of VEGADunder the same setting as main experiments. Theresults of the models fine-tuned on Article QA,CMedQA and CMDD are shown in , 10 and11 respectively.(1) Vocabulary generated by Jieba is not as com-petitive as in the experiments of Qwen 1.8B. Theresults by Jieba are relatively low, especially onmath calculation. The accuracy on GSM8K byJieba is nearly the lowest among all methods. Afterfine-tuning on CMDD, the accuracy decreases from 53.70% to 13.60% by adding the new words, whichis a relative decrease of 74.67%. (2) Direct SFTand DV appear to be strong baselines. Best resultson four metrics are achieved by direct SFT, whenfine-tuning on CMedQA. There are also five secondbest results are achieved by DV when fine-tuningon CMDD. (3) VEGAD outperforms other base-lines from several aspects. There is a stable advan-tage on domain ROUGE-1 and ROUGE-L scoresby VEGAD over other methods. The math calcu-lation by VEGAD reaches the best for some cases.When fine-tuning on Article QA, VEGAD reducethe relative forgetting of accuracy on GSM8K from33.33% to 27.19%, comparing with direct SFT.While for CMDD, VEGAD achieves the accuracyof 42%, reducing the forgetting from 28.87% to21.79%.",
  "GWeight Initialization": "We attempt to further improve the task performanceof VEGAD by adding weight initialization meth-ods, including ATT_EG and PATT_EG. Here weadditionally introduce another approach which re-trieves related concepts from external knowledgebase. For implementation, we use Wikipedia as theknowledge source, and the method is denoted as+WIKI. The results are shown in .Medical concepts are usually different from the",
  ": Results of adding weight initialization to VEGAD": "meaning by understanding its sub-words separately.Thus the improvement on medical tasks especiallyrequires an effective initialization method. Appar-ently, the current methods cannot provide stablebenefits to the domain tasks, even introducing ad-ditional training corpus. On half of the domainmetrics, VEGAD without initialization achievesbetter results. There is no clear pattern on the gen-eral abilities either. The experiments highlight thelimitations to the current initialization approachesand urgent necessity to better algorithms.",
  "HCross Language and Base Model": "presents an experiment conducted on En-glish medical domain dataset, PubMedQA, withLlama3-8B model. Since Jieba is especially de-veloped for Chinese, we apply VEGAD to SPM.The ROUGE-L of text generation tasks and accu-racy of math problems are reported. It can be seenthat VEGAD also improves the baseline on En-glish datasets. Additionally, our proposed methodis adaptable to different text segmentation tools.",
  "JDetailed Discussions to Pilot Study": "The setting of pilot study is the same as Sub.3. The results are shown in .The highest instruction following score appearsat 285 words, while the highest score for otherabilities appear at size 2242. When increasing thesize to the full vocabulary, we observe a significantdeceasing on all metrics. The score of ALPACA iseven lower than direct SFT. From the trending, it is",
  "AhoCorasick Algorithm (Aho and Corasick,": "1975) is based on the structure of Trie, combinedwith the idea of KMP, which is used to solve multi-pattern matching and other tasks. Fail pointers areused to get the node with the maximum length afterthe current node. AhoCorasick Algorithm and failpointers are illustrated in .Inspired by AhoCorasick Algorithm, we furtheroptimize the gradient calculation to improve the ef-ficiency. Firstly, we obtain the prefix accumulationarrays:",
  "(10)": "The external enumerating changes from the start ofeach word to the end. for the start of each word, itis easy to explore with the fail pointer. Assumingthe word represented by node n1 ends at the i-thtoken, then the word represented by node fail(n1)also ends at the i-th token. Let depth(nw) denotethe depth of node nw on the Trie, then Equation 9can be modified as",
  "+ ||sum(Cumlmheadidepth(nw)1:i1)||1(11)": "We provide the details in Algorithm 3.Since the Trie is static during gradient calcula-tion, the parent nodes on fail path for each nodecan be memorized. Then the complexity is re-duced from O(L depth) to O(L depthfail),where depth denotes the expected depth on Trie,and depthfail denotes the expected depth of thefail path. Note that depthfail is usually significantsmaller than depth."
}