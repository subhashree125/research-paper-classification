{
  "LLMs Assist NLP Researchers: Critique Paper (Meta-)Reviewing": "Jiangshu Du1, Yibo Wang1, Wenting Zhao1,10, Zhongfen Deng1, Shuaiqi Liu3,Renze Lou2, Henry Peng Zou1, Pranav Narayanan Venkit2, Nan Zhang2, Mukund Srinath2,Ranran Haoran Zhang2, Vipul Gupta2, Yinghui Li4, Tao Li5, Fei Wang6, Qin Liu7, Tianlin Liu8,Pengzhi Gao9, Congying Xia10, Chen Xing11, Cheng Jiayang12, Zhaowei Wang12, Ying Su12,Raj Sanjay Shah13, Ruohao Guo13, Jing Gu14, Haoran Li15, Kangda Wei16, Zihao Wang12,Lu Cheng1, Surangika Ranathunga17, Meng Fang18, Jie Fu12, Fei Liu21, Ruihong Huang16,Eduardo Blanco19, Yixin Cao20, Rui Zhang2, Philip S. Yu1, Wenpeng Yin2 1University of Illinois Chicago, 2Penn State University, 3Hong Kong Polytechnic University,4Tsinghua Univeristy, 5Google DeepMind, 6University of Southern California,7University of California, Davis, 8University of Basel, Switzerland, 9Xiaomi AI Lab,10Salesforce Research, 11Scale AI, 12HKUST, 13Georgia Institute of Technology,14University of California, Santa Cruz, 15Singapore University of Technology and Design,16Texas A&M University, 17Massey University, New Zealand, 18University of Liverpool,19University of Arizona, 20Fudan University, 21Emory ,",
  "Abstract": "Claim: This work is not advocating the use ofLLMs for paper (meta-)reviewing. Instead, wepresent a comparative analysis to identify anddistinguish LLM activities from human activi-ties. Two research goals: i) Enable better recog-nition of instances when someone implicitlyuses LLMs for reviewing activities; ii) Increasecommunity awareness that LLMs, and AI ingeneral, are currently inadequate for perform-ing tasks that require a high level of expertiseand nuanced judgment. This work is motivated by two key trends. Onone hand, large language models (LLMs) haveshown remarkable versatility in various gen-erative tasks such as writing, drawing, andquestion answering, significantly reducing thetime required for many routine tasks. On theother hand, researchers, whose work is notonly time-consuming but also highly expertise-demanding, face increasing challenges as theyhave to spend more time reading, writing, andreviewing papers. This raises the question: howcan LLMs potentially assist researchers in alle-viating their heavy workload? This study focuses on the topic of LLMsAssist NLP Researchers, particularly ex-amining the effectiveness of LLM in assist-ing paper (meta-)reviewing and its recogniz-ability. To address this, we constructed theReviewCritique dataset, which includes twotypes of information: (i) NLP papers (initialsubmissions rather than camera-ready) with both human-written and LLM-generated re-views, and (ii) each review comes with de-ficiency labels and corresponding explana-tions for individual segments, annotated byexperts. Using ReviewCritique, this studyexplores two threads of research questions:(i) LLMs as Reviewers, how do reviewsgenerated by LLMs compare with those writ-ten by humans in terms of quality and distin-guishability? (ii) LLMs as Metareviewers,how effectively can LLMs identify poten-tial issues, such as Deficient or unprofes-sional review segments, within individual pa-per reviews?To our knowledge, this isthe first work to provide such a comprehen-sive analysis.Our dataset is available at",
  "Introduction": "Artificial intelligence (AI), particularly throughthe recent development of large language models(LLMs), has demonstrated remarkable versatilityin tasks such as writing, drawing, and question an-swering (Naveed et al., 2023; Rasool et al., 2024;Kaddour et al., 2023). This has led to significant au-tomation of many time-consuming jobs, potentiallyreplacing more roles with AI. Interestingly, whileresearchers, the creators of AI/LLMs, benefit fromLLMs for simple tasks (Meyer et al., 2023; Altmeet al., 2023), it still takes years to train a qualifiedresearcher due to the domain-specific and expertise-demanding nature of their work. Researchers nowface increasing challenges with more papers to read, to beat, to write, and to review, resulting in longerand more intensive work hours. This raises thequestion: how promising is the potential for LLMsto work as researchers to alleviate their heavy andsomewhat unhealthy workload?Within the scope of LLMsAssistNLPResearchers, this work focuses on how wellLLMs can perform (meta-)reviewing. AI-relatedconferences and journals are seeing a rapid increasein submissions, making it difficult to recruit enough(meta-)reviewers. Paper reviewers must carefullyread submissions and provide comments on theoverall story, strengths, weaknesses, writing, etc.The meta-reviewers responsibility is to ensure theaccuracy and constructiveness of the individualreview. Therefore, meta-reviewers are expectedto be aware of the submission as well as authorsrebuttals, and then assess individual reviews byidentifying unreasonable elements and distillingtruly constructive comments.There is a latenttrend, though debatable and unacknowledged byreviewers, of LLMs participating more frequentlyin the paper-reviewing process. Therefore, thiswork explores two research questions: (i) LLMsas Reviewers, how far away or distinguishableare LLM-generated paper reviews from human-written ones? (ii) LLMs as Metareviewers, canLLMs identify Deficient review segments by rea-soning over the paper submission, other individualreviews, and author rebuttals jointly?Toachievethis,wecreatetheReviewCritique dataset, containing:(i) NLPpapers (original submissions rather than the finalcamera-ready) with both human-written and LLM-generated reviews, and (ii) each review annotatedby NLP experts (most with Ph.D. degrees or areachairing experience) at the sentence level regardingdeficiency and professionalism, with explanations.This dataset enables the following analyses.First, for LLMs as Reviewers, we assess thequality of LLM-generated reviews by examiningsubsections or aspects of the review, such as sum-mary, strengths, weaknesses, writing, etc. We pro-pose a novel metric to measure LLM-generated re-view diversity across different papers. Our findingsindicate that LLMs generate more Deficient re-view segments than human reviewers and oftenproduce paper-unspecific reviews lacking diversityand constructive feedback.Second, for LLMs as Metareviewers, we evalu-ate LLMs ability to identify Deficient segmentsin human-written reviews and provide explana- tions for their judgments. This contrasts with otherworks treating paper meta-review as a text summa-rization task given 3+ individual reviews (Li et al.,2023; Shen et al., 2022; Pradhan et al., 2021). Weargue that meta-reviewing should be a knowledge-intensive and reasoning-intensive process, with hu-man meta-reviewers being expected to be carefuland responsible. We benchmark both closed-sourceand open-source LLMs on this task, finding thateven top-tier LLMs struggle to mimic human ex-perts in assessing individual reviews.Overall, our contributions are threefold: (i) theReviewCritique dataset with human-written andLLM-generated reviews and fine-grained reviewdeficiency labeling and explanation, serving asa valuable resource for future research on AI-assisted peer review and LLM benchmarking, (ii)the first quantitative comparison of human-writtenand LLM-generated paper reviews at the sentencelevel, and (iii) the first analysis of LLMs potentialas both reviewers and meta-reviewers. By high-lighting the strengths and limitations of LLMs inscientific peer review, our work paves the way forfuture works on integrating AI for research.",
  "Related Work": "Researchers have explored various aspects of AIfor reviews. One area of interest is the use of AIto assist in automatically generating peer reviews,such as predicting scores (Li et al., 2020; Zhouet al., 2024; Wang et al., 2020; Deng et al., 2020)and writing reviews (Gao et al., 2024; Wang et al.,2020; Yuan et al., 2022; Liu and Shah, 2023) andmeta-reviews (Li et al., 2023; Lin et al., 2023b).Another line of research focuses on leveraging NLPmethods to evaluate the quality of human reviews(Xiong and Litman, 2011; Guo et al., 2023; Kumaret al., 2023; Ghosal et al., 2022b).To facilitate research on AI for peer review,several datasets have been introduced. PeerRead(Kang et al., 2018), MOPRD (Lin et al., 2023b),and NLPeer (Dycke et al., 2023) are datasets con-taining a large number of peer reviews and their cor-responding papers but without expert annotations.Other datasets focus on specific aspects of peerreviews, such as argument (Kennard et al., 2022;Hua et al., 2019; Yuan et al., 2022; Cheng et al.,2020; Ruggeri et al., 2023), politeness (Bharti et al.,2023), uncertainty detection (Ghosal et al., 2022b),contradictions in review pairs (Kumar et al., 2023),and substantiation (Guo et al., 2023). Peer Review Analyze (Ghosal et al., 2022a) annotates reviewsacross four facets: paper section correspondence,aspect, functionality, and significance. However,these datasets are solely based on reviews and noneof them are highly expert-demanding. In contrast,ReviewCritique is the first dataset to benchmarkLLMs capability as a responsible meta-reviewer.Recently, researchers have also explored the eval-uation of LLMs deficiency and limitations in au-tomatic paper reviewing tasks (Zhou et al., 2024;Liu and Shah, 2023; Robertson, 2023; Liang et al.,2023; Lin et al., 2023a). Our work differs from pre-vious works in that we provide a quantitative com-parison of human-written and LLM-generated pa-per reviews at the sentence level. This fine-grainedanalysis allows us to identify specific areas whereLLMs excel or struggle in generating high-qualityreviews. We also propose a novel metric to measureLLM-generated review diversity.",
  "Paper Submission & Review Collection": "Criteria.We select the papers based on the fol-lowing criteria: i) Only consider NLP papers; thisfacilitates the recruitment of sufficient annotatorsin the NLP domain. ii) Human-written reviewsare publicly accessible. iii) Equal distribution ofaccepted and rejected papers is maintained to inves-tigate potential review pattern discrepancies basedon the final acceptance or rejection of submissions.From the OpenReview website, we gathered 100NLP papers (submitted to top-tier AI conferencesICLR and NeurIPS between 2020 and 2023) alongwith their complete individual reviews (3-5 for eachsubmission), meta-reviews, and author rebuttals.The revision history on OpenReview allowed usto collect the latest paper submissions before theconference deadline, as these versions are the oneson which the reviews are based.Question: How can we ensure that the col-lected individual reviews are written by humanexperts rather than AI? During the subsequentannotation process, we instruct annotators to notifyus if they suspect that a review collected here waslikely generated by AI; if any doubts arise, we will",
  "discard the paper and all its metadata": "Collecting LLM-generated ReviewsTo directlycompare human-written and LLM-generated re-views, we selected a subset of 20 papers from theoriginal 100. The main reason for this selectionwas the time-consuming nature of subsequent an-notation; a size of 20 allowed for an acceptablestatistical comparison. This subset of papers alsomaintains an equal distribution of accepted andrejected papers. We utilized three of the most pow-erful closed-source LLMs, namely GPT-4 (OpenAI,2023), Gemini-1.5 (Google, 2023), and ClaudeOpus (Anthropic, 2024), as these are the modelsmost likely to be used by humans seeking AI assis-tance in their reviews. Each LLM generated threereviews using prompts that included the ICLR re-view guidelines, randomly chosen human-writtenreviews for both accepted and rejected papers, anda generation template in ICLR 2024 format. Thisprompt can be found in (Appendix F).",
  "Data Annotation": "Annotating Criteria for Deficient.We, agroup of senior NLP researchers with rich AreaChairing experience, define Deficient review seg-ments as follows: Sentences that contain factual errors or misin-terpretations of the submission. Sentences lacking constructive feedback. Sentences that express overly subjective, emo-tional, or offensive judgments, such as I dont likethis work because it is written like by a middleschool student. Sentences that describe the downsides of thesubmission without supporting evidence, for exam-ple, This work misses some related work.Question: Why not directly use author re-buttal to infer the Deficient review segments?We do not solely rely on author rebuttals for sev-eral reasons. First, author rebuttals are not alwayscorrect and may overstate contributions or includeinformation not originally presented in the submis-sion. Second, authors sometimes make compro-mises to satisfy reviewers even when the reviewis Deficient. Third, author rebuttals do not ad-dress all Deficient details and mainly focus onthe \"weakness\" part, while Deficient issues canarise in other parts of the reviews.",
  "NLP venues and extensive reviewing experience.16 have Ph.D. degrees, and 11 are university facultymembers, 15 have served as area chair (AC, alsocalled meta-reviewer in some venues) before": "Annotation Process.The annotation was con-ducted on both human-written and LLM-generatedreviews, following these steps: i) Paper Selec-tion: To ensure high-quality annotations, annota-tors were allowed to choose papers that alignedwith their expertise and interests, ensuring theirproficiency in reviewing these papers. ii) Aware-ness of Review Scope: Our assessment focusedon reviews written before the rebuttal phase, i.e.,reviews based on the original submission. This de-cision was made to avoid the multi-turn problemand to keep the scope manageable. We did notconsider extra experiments conducted during therebuttal phase, as pre-rebuttal reviews are based onthe original submission. Annotators were requiredto thoroughly read all reviews, meta-reviews, au-thor rebuttals, and the original submission to ensurea comprehensive understanding of the paper and itsassociated reviews. iii) Segment-level Annotation:For detailed analysis, reviews were segmented bysentences, and annotators were asked to label eachsentence (a) whether it is Deficient, and (b) pro-vide an explanation if it is. This approach allowsfor the identification of specific sentences that maybe Deficient, even if the overall review is of highquality. Meta-reviewers are expected to analyzeindividual reviews sentence by sentence.Question: Some reviews are generated byLLMs, how did we ensure that annotators wereunaware? For the annotation of LLM-generatedreviews, we employed a separate group of annota-tors who were not informed that these reviews wereLLM-generated. To prevent potential reminders forinternet searches, we concealed submission infor-",
  "mation, such as \"Under review as a conferencepaper at ICLR 2022,\" in the papers provided tothe annotators. We acknowledge that this approachcannot guarantee complete unawareness": "Quality Control.To maintain annotation qual-ity, two annotators independently reviewed eachpapers reviews without access to each others an-notations to prevent bias. Disagreements betweenthe two annotators were resolved by a senior expertwith area chair (AC) experience, who examined theconflicting annotations and resolved discrepanciesby removing or rewriting the explanations for theunconvincing annotations. AnnotationTimeline.Duetothetime-consuming nature of high-quality annotation,each annotator was assigned one paper per week,resulting in a six-month data collection period.This ensured thorough and thoughtful annotations.We organized regular meetings to discuss anyissues that arose during the annotation process.",
  "Data Statistics": "providesthestatisticsforourReviewCritique dataset.We compare fromtwo dimensions. First, at both the review and seg-ment granularity, LLM-generated reviews containmore Deficient instances compared to human-written reviews (100% vs. 71.57% at the reviewlevel, and 13.97% vs. 6.27% at the segment level).Next, we compared Accepted and Rejected,which generally correspond to higher-qualityand lower-quality submissions, respectively.Notably, LLM-generated reviews demonstrateda higher frequency of Deficient segmentsfor Accepted submissions than for Rejectedones, which contrasts with what we observedin human-written reviews.Drawing from our analysis in the Weaknesses part of .1.2,we suggest the following explanation: humanreviewers are often able to sense the overallquality of a paper. If they believe a submissionis of poor quality and intend to reject it, theytend to collect more weaknesses to justify theirdecision, which sometimes leads to overemphasison the papers flaws. In contrast, LLMs lack theability to discern paper quality and often generatesuperficial and non-specific criticisms equallyfor both Accepted and Rejected submissions,making these review segments more likely to beinaccurate for higher-quality papers.",
  ": Comparison of ReviewCritique with Peer-Read (Kang et al., 2018), Peer Review Analyze (Ghosalet al., 2022a), Substantiation PeerReview (Guo et al.,2023) and DISAPERE (Kennard et al., 2022)": "As shown in , ReviewCritique differsfrom previous works in several key aspects. First,ReviewCritique labels review deficiencies at thesentence level, demanding highly experienced an-notators. Second, annotators must read the ini-tial submission, meta-reviews, all reviews, and re-buttals before annotating, unlike previous worksthat require reading reviews and, at most, rebut-tals. These differences make ReviewCritique theonly dataset suitable for benchmarking LLMs asresponsible meta-reviewers, offering a comprehen-sive evaluation of review quality. Additionally,ReviewCritique includes expert-annotated LLM-generated reviews, enabling direct comparison be-tween human and LLM-generated reviews at agranular level. These unique features distinguishReviewCritique and open new research opportu-nities in AI for peer review.",
  "LLMs as Reviewers (i.e., Human-writtenreviews vs. LLM-generated reviews)": "In this section, we compare LLM-generated re-views with human-written reviews: i) by the fine-grained error types if the review segments are an-notated Deficient, ii) by fine-grained analysis foreach component (summary, strengths, weakness,writing, and recommendation score), iii) by consid-ering review diversity. 4.1.1Error type analysis for deficiencyBesides the coarse-grained Deficient label,our annotation team classify the expert-annotatedDeficient segments into 23 fine-grained errortypes (full list and their explanations in ,Appendix D). (Appendix D) report the per-centage of each error type for both human-writtenand LLM-generated reviews. shows thecomparison of the top-3 most frequent error typesbetween human and LLM reviews.From , a major reason for Deficient re-views from human reviewers is misunderstand-ing the paper submission and raising unnecessaryconcerns by neglecting information already stated.This suggests a lack of patience during the review-ing process. Another significant error is makinginexpert critiques or statements due to insufficientdomain knowledge, potentially from unqualifiedreviewers being involved due to the increasing num-ber of submissions to AI/NLP conferences and theneed to recruit more reviewers.Compared to humans, LLMs are more likelyto suggest out-of-scope experiments or analyses. They make significantly fewer \"Inexpert State-ment\" errors. Based on our observations, this isbecause their reviews are usually paper-unspecificand superficial, avoiding expert-level mistakes. Ad-ditionally, LLM-generated reviews do not exhibiterrors like \"Missing Reference,\" \"Invalid Refer-ence,\" and \"Concurrent Work\" since they do notpoint to specific works or provide references.",
  "Fine-grained review analysis": "Summary part.The Summary section inLLM-generated reviews exhibits relatively betterquality compared to other aspects. Our annotatorsidentified only 1.35% of segments as \"InaccurateSummary\" among all LLM Deficient segments,which constitutes 0.19% of all LLM-generatedsegments.In comparison, 5.75% of segmentswere identified as \"Inaccurate Summary\" amongall Deficient segments in human-written reviews,accounting for 0.36% of all human-written reviewsegments. This is nearly twice the percentage foundin LLM-generated summaries. Moreover, errortypes such as Summary Too Short and Copy-pasted Summary, which are present in human re-views, were not observed in LLM-generated re-views, suggesting that LLMs are capable of gen-erating summaries of satisfying quality and avoiddirectly copying content from the paper. Strengths part.LLMs tend to accept authorsclaims in submissions without much critical evalua-tion. Our analysis reveals that among all segmentsin the Strengths section of LLM-generated reviews,53.2% are simply rephrased from the submission,while the remaining segments are mostly inferredfrom the introduction and abstract, where authorstypically highlight their contributions.Tofurtherinvestigate,weusedReviewCritique to compare human-written re-views assessed by annotators and LLM-generatedreviews for the same papers. For accepted papers,34.5% of the Strength segments generated byLLMs were questioned by human experts intheir corresponding human-written reviews. Forrejected papers, this rose to 51.9%.These findings suggest that LLMs often acceptauthors claims without thorough verification, treat-ing strengths as a text summarization task.Incontrast, human reviewers scrutinize the claimedstrengths and provide their expert opinions on thevalidity and significance of the contributions. Weaknesses part.The most dominant type ofDeficient in LLM reviews is Out-of-scope, ac-counting for 30.49% of all Deficient segments inLLM-generated reviews (see ). LLMs oftenhighlight weaknesses such as the need for more ex-periments, lack of generalizability, additional tasks,more analysis, evaluation on languages beyond En-glish, etc. While occasionally relevant, these sug-gestions often fall outside the papers scope andshouldnt be considered weaknesses.Moreover, the suggestions provided by LLMs inthe Weaknesses section tend to be paper-unspecificand superficial (e.g, The papers focus on pre-trained models might limit its applicability to do-mains where such models are not available or suit-able.), making them applicable to most NLP pa-pers without offering actionable insights to eitherauthors or area chairs. This lack of specificity anddepth in the critiques highlights the limitations ofLLMs in providing meaningful and constructivefeedback on the weaknesses of a paper.These findings underscore the importance of hu-man expertise in identifying and articulating themost relevant and significant weaknesses of a paper.While LLMs can generate a list of potential limi-tations, they often struggle to contextualize theseweaknesses within the scope and objectives of thepaper, leading to Deficient segments that maynot be helpful to authors or area chairs. Writing part.Our analysis suggests thatLLMs may lack the ability to accurately judge thewriting quality of a paper submission. In all LLM-generated reviews, LLMs consistently praise thewriting of the papers, stating that they are well-written and easy to follow. However, among thepapers used for generating LLM reviews, 15% ofthe papers had both the meta-reviewer and humanreviewers agree that the writing was unclear anddifficult to follow. Despite this consensus amonghuman experts, the LLMs still provided positivefeedback on the writing quality of these papers,failing to accurately assess the writing quality. Recommendation Score part.In addition togenerating reviews, we asked LLMs to rate eachpaper on a scale of 1-10, matching the ICLR andNeurIPS system, for directly comparison with hu-man reviewers. Experiment shows that LLMs tendto give high scores to all submissions, regardless ofquality or acceptance status, with averages of 7.43for accepted and 7.47 for rejected papers. In con-trast, human reviewers scored averages of 6.41 for",
  "accepted and 4.81 for rejected submissions. Thus,LLMs do not align with the human reviewers indiscerning paper quality based on their internalscoring mechanism": "4.1.3Review DiversityGiven three LLMs and m papers, we can get amatrix of LLM-generated reviews of size 3 m.We perform quantitative analysis i) horizontally tomeasure the intra-LLM review specificity, and ii)vertically as the assessment of inter-LLM reviewcomplementarity. Intra-LLM Review Specificity.In the realworld, we hope the review for each paper is spe-cific to this paper. Then the paper-specific reviewdiversity should discourage two cases: i) one re-view has too many repeat of certain segment; ii) areview segment appear in too many papers. We getinspiration from the classic TF-IDF to define a newsegment-level diversity metric, named ITF-IDF:",
  "l=1Imaxpsim(sji, slp) tmaxpsim(sji, slp), (3)": "where slp is any segment in review l. Rji is com-puted by summing the maximum similarity scoresbetween segment sji and segments in each review lthat exceed the threshold t. In our experiments, weuse SentenceBERT (Reimers and Gurevych, 2019)to calculate the similarity between segments. Im-plementation details can be found in Appendix A.2.In summary, ITF-IDF measures the specificity ofreviews generated by a single LLM across differentpapers. A lower ITF-IDF score means LLM tendsto generate repetitive or similar segments across FullSumm. PaperStrengthsWeaknessesClaritySumm. Review 3.34 4.093.82 6.04 3.904.15 4.36 5.82 3.33 4.594.50 5.80 4.88 5.55 5.56 6.86 2.34 3.17 2.07 5.01 3.24 4.173.95 5.31 ITF-IDF (Higher Better) Claude3GeminiGPT4Human",
  ": Specificity of reviews: LLM vs. Human": "reviews, while a higher score suggests more diverseand unique content in the generated reviews. shows the Intra-LLM paper-orientedspecificity on different review components suchas strengths, weaknesses, etc. We set thresholdt as 0.5 because our initial observation suggeststhat segments with a similarity higher than thisthreshold have a similar meaning. We also reportthe evaluations under different t values in (Appendix B). For human-written reviews, we ran-domly sample one review from each paper and cal-culate ITF-IDF. We repeat this process five timesand use the average score.For ITF-IDF, from the full review perspective,human reviews score the highest (6.04), followedby Claude Opus (4.09), Gemini (3.82), and GPT-4(3.34). The scores are relatively consistent acrossdifferent sections, but GPT-4 tends to have the low-est scores, suggesting more repetitive segmentscompared to other LLMs. Human reviews maintainhigh diversity across all sections. LLMs exhibit asharp diversity drop in the Clarity section. Thisaligns with our observation in .1.2 thatLLMs praise the writing quality of all papers. Inter-LLM Review Complementarity.We ex-amine whether different LLMs tend to write com-plementary reviews for the same paper, whichis a pairwise concept.We first compute theBERTScore (Zhang et al., 2020) for each pair of re-views generated by the three LLMs (GPT-4, ClaudeOpus, and Gemini 1.5) for the same paper. We thenaverage these scores across all papers to obtain anoverall measure of Inter-LLM review diversity. shows the pairwise BERTScores forreviews on the same paper generated by GPT-4,Claude Opus, and Gemini 1.5. It also presentsthe BERTScores for reviews of the same paperconducted by human reviewers. The BERTScores",
  ": Inter-LLM vs. inter-human review similari-ties": "between different LLM pairs are similar and high,ranging from 70.33 to 71.17. In comparison, theBERTScore between human reviewers is 59.15,which is noticeably lower than the scores betweenthe LLMs. This indicates that human reviewerstend to produce more diverse reviews compared tothe LLMs. In addition, this finding implies that theuse of multiple LLMs may not necessarily lead to asignificant increase in the diversity of perspectivesand insights in the review process.",
  "LLMs as Metareviewers": "As an area chair, one should assess the quality ofindividual reviews using their own expertise. Thistask is highly knowledge-intensive and requiresdeep understandings of the research domain. OurReviewCritique provides segment-level annota-tion on if each segment is deficient and why. Thissection evaluates if prompting popular LLMs (bothclosed- and open-source) can solve this problem.For closed-source models, we assess GPT4 (Ope-nAI, 2023), Claude Opus (Anthropic, 2024), andGemini1.5 (Google, 2023). For open-source mod-els, we evaluate Llama3-8B and -70B (AI@Meta,2024) and Qwen2-72B (Bai et al., 2023).To mitigate the impact of prompt-specificperformance, we employ two prompting strate-gies: 1) Labeling-All: Given everything nec-essary including a list of indexed review seg-ments, require the LLM to output a list of tripleslike (id, Deficient or not, explanation); 2)Select-Deficient: Given everything necessaryincluding a list of indexed review segments, re-quire the LLM to output a list of tuples, (id, ex-planation), when it believes the id correspondsto an Deficient segment. The detailed prompttemplates are in and 13 (Appendix F).To enhance evaluation robustness, we ensemblethe results obtained from the two prompting strate-gies using two methods: i) Both No: If bothprompts classify a segment as Deficient, we con-",
  "sider it to be Deficient; ii) Either No: If eitherof the prompts labels a segment as Deficient, weconsider it to be Deficient": "How well can LLMs identify the Deficient seg-ments experts discovered?Metric: we computethe F1 on each paper then average across papers. presents the evaluation results.Closed-source models (GPT-4, Claude Opus,and Gemini 1.5) generally outperform open-sourcemodels (Llama3-8B and 70B, Qwen2-72B) in F1score. Claude Opus achieves the highest F1 scores,with GPT-4 and Gemini 1.5 performing slightlyworse. Notably, recall scores are consistentlyhigher than precision scores across all LLMs andprompting strategies, suggesting that LLMs tend toincorrectly identify segments as Deficient.Despite the superior performance of the closed-source models, their F1 scores remain relativelylow even with different prompt strategies, highlight-ing the challenges LLMs face in such expertise-intensive tasks and emphasizing the importance ofhuman expertise in the meta-reviewing process. Can LLMs correctly explain their Deficientjudgment?When LLMs label Deficient iscorrect, we calculate ROUGE (Lin, 2004) andBERTScores between its explanations and our ex-perts explanations. reports evaluation re-sults for the Select-Deficient prompt. The fullscores for both prompt strategies and their ensem-bles are in and 11 in Appendix E.The results in show that overall scoresfor all LLMs are relatively low, indicating they canidentify some Deficient segments but struggleto articulate their reasoning. Among the LLMs,Claude Opus achieves the highest scores across allmetrics, suggesting its explanations align best withhuman annotators. Claude Opus also excels in iden-tifying Deficient segments, as shown previously.GPT-4 and Gemini 1.5 show similar performanceto Claude Opus. The open-source models, Llama3(8B and 70B) and Qwen2-72B, generally scorelower than the closed-source models. Which Deficient types are challenging forLLMs to identify?To investigate which types ofDeficient are more challenging for LLMs to de-tect, we check for each Deficient type how manycan be successfully identified by LLMs. We fo-cus on three closed-source LLMs: GPT-4, ClaudeOpus, and Gemini 1.5. (in Appendix C) presents the num-",
  "Claude Opus 16.86 / 34.26 / 20.35 17.69 / 26.61 / 18.71 17.14 / 18.70 / 15.78 16.94 / 42.12 / 21.99Gemini 1.516.58 / 34.13 / 19.76 14.71 / 43.60 / 19.72 17.01 / 27.05 / 18.28 14.46 / 50.37 / 20.34": "Llama3-8B7.73 / 45.95 / 12.22 11.47 / 30.29 / 14.88 11.37 / 21.27 / 12.468.19 / 53.61 / 13.35Llama3-70B 13.63 / 42.49 / 18.19 13.95 / 31.16 / 17.46 16.16 / 23.51 / 16.67 12.46 / 50.02 / 18.43Qwen2-72B9.97 / 26.60 / 12.96 11.35 / 34.61 / 14.649.07 / 15.13 / 9.62 10.49 / 43.00 / 15.16 : Performance of LLMs as meta-reviewers on our ReviewCritique dataset. The best F1 score amongdifferent prompt methods for a single model is underlined. The best F1 score across all models is also bold.",
  ": Evaluation of LLMs explanations for correctlyidentified Deficient segments": "ber and percentage of segments identified ineach Deficient type by the LLMs.We ob-serve that six types of Deficient have a sig-nificantly lower percentage compared to the av-erage recall of GPT-4 (47.68%), Claude Opus(42.12%), and Gemini 1.5 (50.37%), suggestingthat these types of Deficient are particularly dif-ficult for LLMs to detect: Inaccurate Summary,Writing, SuperficialReview, Experiment,Contradiction and Unstated Statement These findings align with our observations inSections 4.1.2&4.1, where we assessed LLMs as re-viewers. For example, LLMs struggle to accuratelyjudge the paper writing quality submission and tendto provide superficial reviews, often failing to offerconstructive suggestions on experiments. More-over, LLMs are more prone to generating contra-dictory claims in their reviews and making claimsthat the authors never stated in the submission, indi-cating a tendency towards hallucination. Addition-ally, although LLMs can generate paper summarieswith fewer errors, they may fail to capture nuancedaspects of the paper, leading to their inability toidentify inaccurate summary errors.",
  "Conclusion": "This work studied the potential of LLMs AssistNLPResearchers, focusing on their roles asreviewers and meta-reviewers.We createdReviewCritique , containing both human-writtenand LLM-generated reviews, with detailed defi-ciency annotations and explanations. Our analysisreveals that while LLMs can generate reviews, theyoften produce Deficient and paper-unspecific seg-ments, lacking the diversity and constructive feed-backs. Additionally, even state-of-the-art LLMsstruggle to assess review deficiencies effectively.These findings highlight the current limitations ofLLMs in automating the peer review process.",
  "Limitations": "While our work provides valuable insights intothe potential of LLMs in the peer review process,there are some limitations. During the evaluationof LLMs, ReviewCritique primarily focuses onthe textual information from the submissions anddoes not include figures, tables, or other visual ele-ments. Incorporating these additional componentscould provide a more comprehensive assessmentof LLMs capabilities in the peer review process.Additionally, the dataset is currently limited to theNLP domain. It would be interesting to explore theperformance of LLMs in other research areas. Ex-panding the dataset to include papers from variousdomains could help assess the generalizability ofour findings and identify potential domain-specificchallenges. Furthermore, our work focuses on thepre-rebuttal phase of the peer review process, as-sessing reviews based on the original submission.Incorporating the multi-turn aspect of peer review,including author rebuttals and post-rebuttal reviews,could offer a more comprehensive understandingof LLMs capabilities in the entire review process.",
  "Ethical Considerations": "In this study, we carefully considered the ethicalimplications of using LLMs to assist in the reviewprocess. We acknowledge potential risks, includingbias, lack of accountability, and the possibility ofundermining the integrity of scientific evaluations.Importantly, this work does not advocate for theuse of LLMs in paper (meta-)reviewing. Rather,our study underscores that LLMs are currently in-sufficient to replace human reviewers, especiallyin tasks that require expert judgment and nuancedunderstanding.",
  "Anthropic. 2024. Introducing the next generation ofclaude": "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, FeiHuang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,Xuancheng Ren, Chuanqi Tan, Sinan Tan, JianhongTu, Peng Wang, Shijie Wang, Wei Wang, Sheng-guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-uan Zhang, Yichang Zhang, Zhenru Zhang, ChangZhou, Jingren Zhou, Xiaohuan Zhou, and TianhangZhu. 2023. Qwen technical report. arXiv preprintarXiv:2309.16609. Prabhat Kumar Bharti, Meith Navlakha, Mayank Agar-wal, and Asif Ekbal. 2023. Politepeer: does peerreview hurt? a dataset to gauge politeness intensityin the peer reviews. Language Resources and Evalu-ation, pages 123. Liying Cheng, Lidong Bing, Qian Yu, Wei Lu, andLuo Si. 2020. Ape: Argument pair extraction frompeer review and rebuttal via multi-task learning. InProceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 70007011. Zhongfen Deng, Hao Peng, Congying Xia, Jianxin Li,Lifang He, and S Yu Philip. 2020. Hierarchical bi-directional self-attention networks for paper reviewrating recommendation. In Proceedings of the 28thInternational Conference on Computational Linguis-tics, pages 63026314. Nils Dycke, Ilia Kuznetsov, and Iryna Gurevych. 2023.NLPeer: A unified resource for the computationalstudy of peer review. In Proceedings of the 61stAnnual Meeting of the Association for ComputationalLinguistics, pages 50495073.",
  "Gemini Team Google. 2023.Gemini: a family ofhighly capable multimodal models. arXiv preprintarXiv:2312.11805": "Yanzhu Guo, Guokan Shang, Virgile Rennard, MichalisVazirgiannis, and Chlo Clavel. 2023. Automaticanalysis of substantiation in scientific peer reviews.In Findings of the Association for ComputationalLinguistics: EMNLP 2023, pages 1019810216. Xinyu Hua, Mitko Nikolov, Nikhil Badugu, andLu Wang. 2019. Argument mining for understandingpeer reviews. In Proceedings of the 2019 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, pages 21312137.",
  "Jean Kaddour, Joshua Harris, Maximilian Mozes, Her-bie Bradley, Roberta Raileanu, and Robert McHardy.2023. Challenges and applications of large languagemodels. arXiv preprint arXiv:2307.10169": "Dongyeop Kang, Waleed Ammar, Bhavana Dalvi,Madeleine van Zuylen, Sebastian Kohlmeier, EduardHovy, and Roy Schwartz. 2018. A dataset of peerreviews (PeerRead): Collection, insights and NLPapplications. In Proceedings of the 2018 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, pages 16471661. Neha Nayak Kennard, Tim OGorman, Rajarshi Das,Akshay Sharma, Chhandak Bagchi, Matthew Clin-ton, Pranay Kumar Yelugam, Hamed Zamani, andAndrew McCallum. 2022. DISAPERE: A datasetfor discourse structure in peer review discussions.In Proceedings of the 2022 Conference of the North",
  "Jiyi Li, Ayaka Sato, Kazuya Shimura, and Fumiyo Fuku-moto. 2020. Multi-task peer-review score prediction.In Proceedings of the First Workshop on ScholarlyDocument Processing, pages 121126": "Miao Li, Eduard Hovy, and Jey Lau. 2023. Summariz-ing multiple documents with conversational structurefor meta-review generation. Findings of the Associ-ation for Computational Linguistics: EMNLP 2023,pages 70897112. Weixin Liang, Yuhui Zhang, Hancheng Cao, BingluWang, Daisy Ding, Xinyu Yang, Kailas Vodrahalli,Siyu He, Daniel Smith, Yian Yin, et al. 2023. Canlarge language models provide useful feedback onresearch papers? a large-scale empirical analysis.arXiv preprint arXiv:2310.01783.",
  "Ryan Liu and Nihar B Shah. 2023. Reviewergpt? anexploratory study on using large language models forpaper reviewing. arXiv preprint arXiv:2306.00622": "Jesse G Meyer, Ryan J Urbanowicz, Patrick CN Mar-tin, Karen OConnor, Ruowang Li, Pei-Chen Peng,Tiffani J Bright, Nicholas Tatonetti, Kyoung Jae Won,Graciela Gonzalez-Hernandez, et al. 2023. Chatgptand large language models in academia: opportuni-ties and challenges. BioData Mining, 16(1):20. Humza Naveed, Asad Ullah Khan, Shi Qiu, Muham-mad Saqib, Saeed Anwar, Muhammad Usman, NickBarnes, and Ajmal Mian. 2023. A comprehensiveoverview of large language models. arXiv preprintarXiv:2307.06435.",
  "OpenAI. 2023. Gpt-4 technical report. arXiv preprintarXiv:2303.08774": "Tribikram Pradhan, Chaitanya Bhatia, Prashant Kumar,and Sukomal Pal. 2021.A deep neural architec-ture based meta-review generation and final decisionprediction of a scholarly article. Neurocomputing,428:218238. Zafaryab Rasool, Stefanus Kurniawan, Sherwin Balugo,Scott Barnett, Rajesh Vasa, Courtney Chesser, Ben-jamin M Hampstead, Sylvie Belleville, Kon Mouza-kis, and Alex Bahar-Fuchs. 2024. Evaluating llmson document-based qa: Exact answer selection andnumerical extraction using cogtale dataset. NaturalLanguage Processing Journal, page 100083. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:Sentence embeddings using siamese bert-networks.In Proceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the 9thInternational Joint Conference on Natural LanguageProcessing, pages 39803990.",
  "Zachary Robertson. 2023. Gpt4 is slightly helpful forpeer-review assistance: A pilot study. arXiv preprintarXiv:2307.05492": "Federico Ruggeri, Mohsen Mesgar, and Iryna Gurevych.2023. A dataset of argumentative dialogues on sci-entific papers. In Proceedings of the 61st AnnualMeeting of the Association for Computational Lin-guistics, pages 76847699. Chenhui Shen, Liying Cheng, Ran Zhou, Lidong Bing,Yang You, and Luo Si. 2022. MReD: A meta-reviewdataset for structure-controllable text generation. InFindings of the Association for Computational Lin-guistics: ACL 2022, pages 25212535. Qingyun Wang, Qi Zeng, Lifu Huang, Kevin Knight,Heng Ji, and Nazneen Fatema Rajani. 2020. Re-viewRobot: Explainable paper review generationbased on knowledge synthesis. In Proceedings ofthe 13th International Conference on Natural Lan-guage Generation, pages 384397. Wenting Xiong and Diane Litman. 2011. Automaticallypredicting peer-review helpfulness. In Proceedingsof the 49th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies, pages 502507.",
  "Closed-source LLMs.We experiment withthe following models and their correspondingAPI endpoints: GPT-4 (gpt-4-turbo), Gemini1.5 (gemini-1.5-flash-latest), and Claude 3(claude-3-opus-20240229)": "Open-sourceLLMs.Weexperimentwiththefollowingmodels:Llama3-8B(Meta-Llama-3-8B-Instruct),Llama3-70B(Meta-Llama-3-70B-Instruct),andQwen2-72B (Qwen/Qwen2-7B-Instruct).For GPT-4, Claude 3, Gemini 1.5, and Qwen2-72B, we input the full prompt as shown in and 13, which contains the complete instruction,paper title, full paper body text, and review text.However, for Llama3-8B and Llama3-70B, themaximum supported context length is limited to8k tokens3. To accommodate this constraint, wetruncate the full paper body text while keeping theother components of the prompt intact. This isbecause the other components, such as the instruc-tion, paper title, and review text, are crucial for theevaluation and cannot be truncated.",
  "Out-of-scope3114 / 21 / 2545.2 / 67.7 / 80.6%": "Inaccurate Summary417 / 2 / 417.1 / 4.9 / 9.8%Neglect14075 / 100 / 12253.6 / 71.4 / 87.1%Inexpert Statement13068 / 80 / 10052.3 / 61.5 / 76.9%Misunderstanding16377 / 111 / 12047.2 / 68.1 / 73.6%Vague Critique6639 / 52 / 5759.1 / 78.8 / 86.4%Misinterpret Novelty2719 / 23 / 2270.4 / 85.2 / 81.5%Misplaced Attributes74 / 3 / 557.1 / 42.9 / 71.4%",
  "Error TypeExplanation": "MisunderstandingThe reviewer misinterprets claims or ideas presented in the paper, leadingto inaccurate or irrelevant comments.NeglectThe reviewer overlooks important details explicitly stated in the paper,resulting in unwarranted questions or critiques.Vague CritiqueThe review lacks specificity, claiming missing components without clearlyidentifying what is missing.Inaccurate SummaryThe summary in the review misrepresents the main content or contributionsof the paper.Out-of-scopeThe reviewer suggests additional methods, experiments, or analyses thatare beyond the intended scope of the paper.Misunderstanding of theSubmission RuleThe reviewer believes the submission format violates conference rules, butthis is not actually the case.SubjectiveThe review makes assertions about the papers clarity or quality withoutproviding sufficient justification or evidence.Invalid CriticismThe reviewers criticism is considered invalid, especially when suggestingimpractical experiments or trivializing results.Misinterpret NoveltyThe reviewer questions the novelty of the work without substantiatingtheir claims with relevant referencesSuperficial ReviewThe reviewer appears to have only skimmed the paper, providing genericor unsupported comments about the presence or absence of weaknesses.WritingDiscrepancies arise when the reviewer praises the writing, while ourannotator suggests it needs more clarity or explicitness.Inexpert StatementThe reviewer exhibits a lack of domain knowledge, leading to unnecessaryor irrelevant concerns.Missing ReferenceThe reviewer proposes alternative frameworks or methods without provid-ing justification or citing relevant referencesExperimentConflicting opinions about the design of experiments; the reviewer praisesthem while our annotator suggests adding more baselines or tests.Misplaced attributesStrengths are incorrectly listed as weaknesses or vice versa.Invalid ReferenceThe reviewer cites non-peer-reviewed sources or blogs, which is not ap-propriate for academic validation.Unstated statementStatements made in the review are not supported by content in the paper.Summary Too ShortThe provided summary is excessively brief, offering little to no insightinto the actual content of the paper.ContradictionThe reviewer contradicts themselves within the review, such as criticizingthe papers experiments while later stating that the experiments are com-prehensive.TypoThe review contains typographical errors that may affect clarity or under-standing.Copy-pasted SummaryThe summary is directly copied from the submission.Concurrent workThe reviewer requests comparisons with work conducted concurrently,which may not have been considered by the authors.DuplicationThe review segment is a repetition or duplication of a previous segmentwithin the same review.",
  "Labeling-AllSelect-Deficient": "GPT-416.12 / 2.05 / 13.58 / 56.8717.13 / 2.71 / 14.64 / 55.63Claude Opus18.54 / 3.03 / 16.03 / 58.4420.18 / 3.69 / 17.52 / 57.28Gemini 1.519.40 / 2.99 / 17.14 / 58.1018.47 / 2.98 / 16.38 / 56.46Llama3-8B15.97 / 1.74 / 14.14 / 56.2316.49 / 2.22 / 13.65 / 55.23Llama3-70B15.03 / 2.25 / 13.04 / 58.1915.94 / 1.95 / 13.78 / 57.09Qwen2-72B14.49 / 2.27 / 12.86 / 56.6617.07 / 3.00 / 14.69 / 56.88",
  "Both \"No\"Either \"No\"": "GPT-416.79 / 2.46 / 14.16 / 56.2116.61 / 2.36 / 14.09 / 56.25Claude Opus19.82 / 3.63 / 17.23 / 58.0019.24 / 3.31 / 16.66 / 57.95Gemini 1.519.25 / 3.08 / 17.12 / 57.4218.88 / 2.99 / 16.72 / 57.17Llama3-8B16.94 / 2.22 / 14.49 / 56.0716.17 / 1.91 / 13.92 / 55.86Llama3-70B15.72 / 2.02 / 13.63 / 57.6415.44 / 2.12 / 13.38 / 57.71Qwen2-72B15.51 / 2.51 / 13.64 / 56.3415.72 / 2.58 / 13.74 / 56.74 : Evaluation of LLMs explanations for correctly identified Deficient segments with ensembling twoprompts results. The final scores are calculated by averaging the scores of each explanation generated by theprompts.",
  "Summary of the Paper: [Provide a concise summary of the paper, highlighting its main objectives,methodology, results, and conclusions.]": "Strengths and Weaknesses: [Critically analyze the strengths and weaknesses of the paper. Considerthe significance of the research question, the robustness of the methodology, and the relevance of thefindings.] Clarity, Quality, Novelty, and Reproducibility: [Evaluate the paper on its clarity of expression,overall quality of research, novelty of the contributions, and the potential for reproducibility by otherresearchers.]"
}