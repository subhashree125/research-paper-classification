{
  "Abstract": "Scaling the rotary position embedding (RoPE)has become a common method for extendingthe context window of RoPE-based large lan-guage models (LLMs). However, existing scal-ing methods often rely on empirical approachesand lack a profound understanding of the in-ternal distribution within RoPE, resulting insuboptimal performance in extending the con-text window length.In this paper, we pro-pose to optimize the context window extend-ing task from the view of rotary angle distribu-tion. Specifically, we first estimate the distri-bution of the rotary angles within the modeland analyze the extent to which length ex-tension perturbs this distribution. Then, wepresent a novel extension strategy that min-imizes the disturbance between rotary angledistributions to maintain consistency with thepre-training phase, enhancing the models ca-pability to generalize to longer sequences. Ex-perimental results compared to the strong base-line methods demonstrate that our approachreduces by up to 72% of the distributionaldisturbance when extending LLaMA2s con-text window to 8k, and reduces by up to 32%when extending to 16k. On the LongBench-E benchmark, our method achieves an aver-age improvement of up to 4.33% over exist-ing state-of-the-art methods. Furthermore, ourmethod maintains the models performance onthe Hugging Face Open LLM benchmark af-ter context window extension, with only anaverage performance fluctuation ranging from-0.12 to +0.22. Our code is available at",
  "(b)": ": Rotary angle distributions of extrapolationand interpolation methods in two different dimensions,compared with the pre-trained angle distribution. (a) Inone dimension, the extrapolated rotary angle distributionfits more closely with the pre-trained distribution. (b)In another dimension, the interpolated distribution fitsbetter with the pre-trained distribution. 2024), modeling arbitrarily long textual sequencesremains a significant challenge. On the one hand,LLMs trained on short sequences often encounterout-of-distribution (OOD) issues when applied tothe longer ones (Liu et al., 2023). On the otherhand, training an LLM with extremely long con-text windows (i.e., the maximal sequence length)from scratch is expensive and inefficient. Currently,the most popular approach is pre-training a largelanguage model, such as LLaMA, Qwen2 (Tou-vron et al., 2023a,b; Team, 2024), with a limitedcontext window and the rotary position embedding(RoPE, Su et al. (2021)). During the inference stage, the context window is dynamically extendedvia fine-tuning or tuning-free position interpolationstrategies (Chen et al., 2023; Peng et al., 2023; Liuet al., 2023) on the rotary position embedding.However, these position interpolation strategiesprimarily rely on intuition and are developed froman empirical perspective, resulting in a lack of in-terpretability (Zhao et al., 2023) and sub-optimalperformance for context extension. For example, PI(Chen et al., 2023) equally stretches all dimensionsof the RoPE with the context extension ratio. YaRN(Peng et al., 2023) observes that heuristically uti-lizing different strategies for different dimensionsyields better performance. However, the reasonsbehind this phenomenon have not been thoroughlyinvestigated, resulting in it likely not achieving thebest results. Moreover, the optimal hyperparame-ters determined experimentally in YaRN potentiallyhinder its generalization to new model settings.To bridge the gap between experiments and the-oretical analysis, we tackle context window ex-tension from the view of rotary angle distribution.Hence, we propose a method for length extensionstrategy selection, which has the potential to be the-oretically optimal by minimizing the perturbationto the rotary angle distributions of the pre-trainedlanguage model. Specifically, we first comparethe pre-training rotary angle distribution with thedistributions introduced by interpolation and ex-trapolation. As illustrated in (a), interpo-lation can introduce too many OOD angles thathave a frequency of 0 in the pre-training distri-bution, indicating a significant disturbance to theoriginal distribution and posing a challenge for themodel to adapt to the new distribution. While di-rect extrapolation may have a negligible impact onthe distribution. Contrarily in another dimensiondemonstrated in (b), direct extrapolationintroduces numerous OOD angles in this situation,causing a severe distribution disturbance, whereasinterpolation performs better.From such distributional view, we find that theconsistency between the pre-training rotary angledistribution and the extension distribution variesacross different dimensions. Thus, we propose toemploy different extension strategies in differentdimensions according to the rotary angle distri-bution. We first approximate the distributions ofrotary angles by calculating the frequency of anglesin minimal discrete intervals. Then, we estimatethe disturbance introduced by different extensionstrategies by computing the distance between the interpolated or extrapolated distribution and theoriginal one. Finally, we determine the most ap-propriate extension strategy for each rotary angledimension independently.Experiments across LLMs of different sizes andvarious long-context tasks demonstrate the effec-tiveness of our distributional approach. We outper-form the strong extension baselines PI (Chen et al.,2023) and YaRN (Peng et al., 2023) on LongBench-E (Bai et al., 2023), achieving a new state-of-the-art.Besides, our method achieves 100% accuracy onpasskey retrieval (Mohtashami and Jaggi, 2023)and matches the performance of original LLMs onshort-text tasks in the HuggingFace Open LLMLeaderboard (Face, 2023). In summary, our contri-butions are as follows:",
  "Rotary Position Embedding (RoPE)": "Rotary position embedding (Su et al., 2021) is aposition embedding method widely used in recentLLMs, which have weak extrapolation propertiesfor long text modeling and context window exten-sion. As demonstrated in the upper part of Fig-ure 2, OOD position indices can be directly ex-trapolated when corresponding rotary angles areperiodic. Given a d-dimensional attention head, themth tokens rotary matrix Rdm is defined:",
  "Position Interpolation (PI)": "As shown in the lower part of , PI (Chenet al., 2023) suggests linear interpolation to all di-mensions to keep position indices within the pre-trained range. When extending the context windowfrom L to L, with the scaling factor s = L/L,the new i is scaled correspondingly as i = i/s.Although alleviating OOD position indices, thisapproach is likely to disturb the original periodicityand add unseen rotary angles.",
  ",(4)": "where s is the scaling factor and i = (ri)/(). As shown in eq. (4), extrapolation is usedfor high-frequency dimensions (ri > ), while in-terpolation is used for low-frequency dimensions(ri < ). Others are deployed with NTK-aware(bloc97, 2023b,a) methods. Peng et al. (2023) em-pirically suggest = 1 and = 32 for LLaMAs.",
  "Rotary Angle Distribution": "LLMs generate language sequences by sam-pling from the learned distribution p(x)=m p(xm|x<m), where the position order is im-plicitly controlled by position embedding. Thismeans that changes in the distribution of positionembedding will have an impact on the languagedistribution. Thus, we need to model this distribu-tion and maintain its consistency when extendingthe context window.As illustrated in eq. (1), rotary angles im =(mi mod 2) of a specific dimension i are fi-nite discrete numbers during the pre-training stage,since 0 m < L, m N. Considering them assampled from the rotary angle distribution, we canstatistically estimate this distribution. We dividethe rotary range [0, 2) uniformly into b intervals,where the kth interval in ith dimension is defined:",
  "Frequency": "Pre-Trained L = 4K Extraplolation L = 8K Interpolation(s = 2) L = 8K : The learned rotary angle distributions ofLLaMA2. We demonstrate the 6th and 22nd dimen-sions during pre-training within the 4k length, and thecorresponding rotary angle distributions when extendedto 8k via interpolation and extrapolation, respectively.We set the number of intervals to b = 360 and we onlydisplay the first 24 intervals for clarity. The distributionsof full intervals are provided in appendix A.1.",
  "k=0P iL( Intervalik) = 1": "Take LLaMA2-7B as an example, where L = 4kand d = 128, we analyze the rotary angle distribu-tion of pre-trained parameters. We demonstrate thedistributions in , which vary significantlyas the dimension changes. When extending the con-text window to L, such as L = 8k, we considertwo scenarios for each dimension: interpolationwith the scaling factor s = 2 and direct extrapola-tion. Consistency of the distributions derived bythese two extension approaches with the originaldistribution also changes with different dimensions.As shown in , the rotary angle distribu-tion of the interpolation enables better maintenanceof consistency with the pre-trained distribution onthe 6th dimension. When it comes to the 22nddimension, the situation is completely the oppo-site. Furthermore, we observe that interpolationintroduces too many OOD angles that are assignedthe frequency of 0 by the pre-trained distribution,challenging models generalization capability.Its worth noting that our observation is inlinewith the empirical strategies in YaRN (Peng et al.,2023), where different dimensions have completelydifferent situations. Besides, distributional consis-tency is essential for mitigating the OOD issue,which enables LLMs to generalize to longer con- text window and improves its performance on long-text tasks. Therefore, we will choose the contextwindow extension methods with the least perturba-tion according to the rotary angle distribution ondifferent dimensions.",
  "Minimizing Distribution Disturbance": "In this part, we derive the disturbance betweenrotary angle distributions and minimizing the dis-turbance to maintain the their consistency. Given aLLM pre-trained on the sequence length of L withthe rotary position embedding, the set of rotary an-gle distributions for all dimensions is denoted asPL =P 0L(), . . . , P d/21L(). Extending the",
  "(8)": "where is an extremely small number to preventdividing 0 and Di(PL, PL) is the KL divergence.For OOD rotary angles introduced by interpola-tion or extrapolation, Di(PL, PL) yields a highdisturbance score due to the large value of F ik(L).The score is low when F ik(L) F ik(L), since theincomplete sampling from the pre-trained rotaryangle distribution does not have a serious impactduring the inference stage. Now we can quantitatively compare the situationin and we can further control the exten-sion strategy in a fine-grained manner with thedisturbance score, where the primary objective isto minimize the disturbance, min D(PL, PL). Indetail, we combine the two strategies: one is basedon PI, where we use s = L/L to interpolate andobtain the corresponding rotary angle distributionsP IL, and the other involves directly extrapolatingto L with distributions P EL. We minimize the dis-turbance score for each dimension independently,",
  "LLaMA2-13B": "Original4k26.9726.0526.2726.4326.16PI(s=2)8k31.4330.9529.7430.7130.35PI(s=4)16k30.8031.3330.8630.9931.10YaRN(s=2)8k31.0030.4230.0730.5030.25YaRN(s=4)16k31.5931.3529.8930.9430.62CLEX(ms=16)64k29.8430.2230.2230.0930.22Ours(s=2)8k31.6431.4030.4331.1630.91Ours(s=4)16k31.5832.2931.1531.6731.72 : Comparative performance analysis of various context window extension methods on the Longbench-Ebenchmark. Avg. denotes the average score across all lengths, while Avg.>4k represents the average score forlengths exceeding the pre-training length. The scaling factor of CLEX (Chen et al., 2024) is dynamic, \"ms\" denotesthe maximum scaling factor, and we set the maximum scaling factor to 16 in accordance with the settings of Chenet al. (2024).",
  "i =": "isif Di(P EL, PL) > Di(P IL, PL) + tiotherwise,(9)where t is a threshold to determine the extensionstrategy when the disturbance scores Di(P EL, PL)and Di(P IL, PL) are very close. As demonstratedin eq. (9), for the ith dimension, we employ linearinterpolation with si = L/L, when its disturbancescore is much smaller. Otherwise, direct extrapola-tion is a preferred choice for this dimension.Its worth noting that our approach is a pre-execution strategy that does not add any time orcalculation cost during the inference phase as longas the extension length L is provided. Besides,since we only modify the value of , any advancedmethod that influences the attention mechanism,such as FlashAttention (Dao et al., 2022; Dao,2023), is still compatible.",
  "Experimental Details": "We validate the effectiveness of our method on thetrending LLaMA2 (Touvron et al., 2023b) model,including 7B and 13B parameter models. All mod-els are trained on a subset of PG19 (Rae et al.,2020) datasets. For s = 2, models are fine-tunedfor 1000 steps with a global batch size of 64 andmax length of 8192. For s = 4, models are fine-tuned for 500 steps with a global batch size of 64and a max length of 16384. We set the default valueof b in eq. (5) to 360. By adjusting the value of t ineq. (9), we set the default number of interpolateddimensions to 80 for 8k extension and to 64 for 16kextension. See more details in appendix B.1.",
  "Long Context Evaluation": "To evaluate the models capabilities on real-worldlong context tasks with an extended context win-dow. We utilize the Longbench-E benchmark (Baiet al., 2023), which is specifically designed forevaluating models with long context window. TheLongbench-E benchmark consists of 13 diversetasks, with the average length of most tasks rang-ing from 5k to 15k. Furthermore, Bai et al. (2023)",
  ": Comparative performance of various context window extension methods relative to the original LLaMA2on the Hugging Face Open LLM benchmark": "categorizes the test samples into groups based onlength intervals of 0-4k, 4-8k, and 8k+ to providean analysis of the models performance variationsat different input lengths. shows a side-by-side comparison of theLLaMA2 model extended from 4k to the contextlength of 8k and 16k via PI (Chen et al., 2023),YaRN (Peng et al., 2023) and our method. Weobserve that models of different parameter sizes,employing our method as the extension method,achieve optimal average results when extendedto various context lengths. Compared to PI, ourmethod achieves an average score improvement ofup to 4.33% when extending the context windowof LLaMA2-7B to 16k. To further demonstratethe models performance when surpassing the pre-training length, we also report the average scoresfor evaluations with lengths greater than 4k. Whenextended to 16k, we can observe that models us-ing our method maintain their performance in theextended context length range, whereas the modelemploying PI exhibits performance degradationat the 7B model and YaRN exhibits performancedegradation at the 13B model. We also evaluatedthe perplexity of the models as well as their per-formance on the RULER benchmark (Hsieh et al.,2024), as shown in Appendix B.2.",
  "We further evaluate the LLaMA2 models on thestandard short context benchmark from the Hug-": "ging Face Open LLM Leaderboard (Face, 2023)to observe how its ability in the original lengthrange changes after extending the context window.Specifically, we use 0-shot TruthfulQA (Lin et al.,2022) and Hellaswag (Zellers et al., 2019), 5-shotMMLU (Hendrycks et al., 2020) and 25-shot ARC-c (Clark et al., 2018). The results demonstrate thatthe performance using our method to extend thecontext window is not significantly affected.As illustrated in , when extending theLLaMA2-7B model to 8k with our approach, weobserve only a 0.12 average score decrease com-pared to the original model. Meanwhile, extendingthe context window of the LLaMA2-7B model to16k using YaRN results in a maximum averageperformance drop of 0.53, which is further exacer-bated in the case of PI. When applying our methodto extend the context window of the LLaMA2-13Bmodel, we can even achieve a slightly average per-formance improvement, suggesting that extendingthe models context window with our method doesnot substantially harm the models capability.",
  "Passkey Retrieval": "To study the effective context window size of ourmodel after extension, i.e. the maximum distanceof a token that can be effectively attended to duringinference. We further evaluate the models abilityto retrieve a simple passkey from a massive amountof text via passkey retrieval task (Mohtashami andJaggi, 2023). Following the experimental setup of 2k4k6k8k10k12k14k16k18k20k",
  "Analysis": "In this section, we analyze the impact of distribu-tional disturbance on model performance. More-over, we analyze the selection of different interpo-lation dimension numbers in eq. (9) and the impactof the number of intervals in eq. (5). All analy-ses are based on the task of extending the contextwindow of LLaMA2-13B from 4k to 8k.",
  "Influence of Interpolation Dimension": "Let us denote the number of interpolation dimen-sions as 0 n d. In eq. (9), we can controlthe value of t to decide how many dimensions theinterpolation strategy is used for. We demonstratethe influence of the number of interpolated dimen-sions n in , where n decreases from 96to 56 as t increases. We observe that for dimen-sions where the disturbance scores Di(P EL, PL)and Di(P IL, PL) are very close, corresponding tothe cases of n = 96, 88, and 80, the impact of choos-ing extrapolation or interpolation on the modelsperformance is slight and negligible. However,as the disturbance increases, corresponding to thecases of n < 80, maintaining distributional consis-tency becomes crucial, and we can observe a grad-ual decline in the performance when employingextrapolation to those dimensions where the dis-",
  "Influence of Interval": "During the analysis of the rotary angle distributionin eq. (5), we divide [0, 2) into b intervals andstatistically estimate their distribution. In this part,we explore the impact of b, ranging from 90 to 720,on the extension of the models context window.As shown in , when b = 90, 180 and 360,the models performance after extension exhibitsno significant fluctuations. This suggests that themodel is capable of tolerating subtle differencesin rotation angles. The performance drops whenb = 720. This is because excessive intervals canactually increase the error in the distribution esti-mation, since the number of rotary angle samples Lis not very large. illustrates that the choiceof b does not influence the downstream tasks.",
  "Related Works": "Long-sequence modeling is a crucial issue in theapplication of LLMs. Recent efforts focus on im-proving position embedding to enable LLMs havelarger context window. Currently, the most popularrelative position embedding are ALiBi (Press et al.,2022) and RoPE (Su et al., 2021). ALiBi (Presset al., 2022) adds bias to attention, enabling modelsto maintain lower perplexity on long sequences,",
  "but only generalizes to limited lengths on down-stream tasks (Kazemnejad et al., 2023). RoPE (Suet al., 2021) cannot generalize to lengths beyondits pre-training length": "Some works have been done to overcome suchlimitation. Ruoss et al. (2023) randomize tokensposition embedding during pre-training, enablingthe model based on RoPE to generalize to prede-termined sequence lengths. This effectively guar-antees consistency in the distribution of rotationangles when generalizing to predetermined lengths,demonstrating that rotation angle distribution con-sistency is crucial for the models ability to gen-eralize. Chen et al. (2023); bloc97 (2023b,a); Liuet al. (2023); Peng et al. (2023) extend the contextwindow of existing LLMs (i.e., LLaMA2 (Tou-vron et al., 2023b)) by slightly modifying RoPEs (as show in eq. (1)). Chen et al. (2023) achievesproposed to extend the context window by interpo-lating positions, using a scaling factor s = L/Lto uniformly scale i, and fine-tuning on a smallamount of data to extend the models context win-dow. bloc97 (2023b,a) base on the Neural TangentKernel (NTK) theory, they scale lower dimensionsless and higher dimensions more, this is also re-ferred to as Adjusted Base Frequency (ABF). Liuet al. (2023) achieves an effect similar to NTK bymodifying the base of RoPE. YaRN (Peng et al.,2023) improved NTK by dividing RoPE dimen-sions into three frequency-based groups and ap-plying different strategies to each group. Low fre-quency (ri < ) dimensions use interpolation likePI and high frequency (ri > ) dimensions useextrapolation, dimensions that fall in-between em-ploys the NTK. YaRN achieved good performance,but lacked interpretability, the hyperparameters and were also empirically chosen, making it hardto obtain the optimal results. Different from theseempirical method, our work initially highlights theconsistency of rotary angle distribution as a theo-retical guidance for extending the context window.",
  "Conclusion": "In this work, we proposed to study the context win-dow extension from a distributional perspective anddemonstrated that the consistency of rotary angledistributions has a significant impact on extendingthe context window of LLMs based on the rotaryposition embedding. We designed a framework toselect scaling strategies with the guidance of mini-mizing the disturbance of rotary angle distributions.Experimental results demonstrated the effective-ness and superiority of our approach. Although ourapproach is limited by the rotary position embed-ding, we believe that our distributional perspectivehas the potential to inspire future work.",
  "Limitations": "Our method is limited by the rotary position em-bedding, which is not currently available for LLMswith other embedding methods. However, this isnot a serious problem because (1) the most power-ful open source LLMs, such as LLaMA2, utilize therotary position embedding, and (2) our approach ad-dresses the problem from a theoretical perspective,which can be better generalized to other embeddingframeworks in future research than empirical work.When applying the model to long contextualtasks, the quadratic computational complexity prob-lem of transformers still exists. Fortunately, ourmethod does not introduce more computationaloverhead in the inference phase. Besides, we arecompatible with other computationally efficientTransformer methods.Our method does not make any structural im-provements to the rotation position embedding orinterpolation methods, so it still does not fullyachieve the optimal situation with the distributionperturbation D(PL, PL) = 0. This provides inspi-ration for future exploration.The accuracy of our estimated rotary angle dis-tribution is affected by the pre-training sequencelength L, since the rotary angles are regarded assampled L times from the real rotary angle distribu-tion. Currently, our method can achieve satisfyingimprovement for models with L = 4k, and will per-form better when applied for models with longerpre-training length.Due to the constraints of computing resources,our experiments are limited to LLaMA2-7B andLLaMA2-13B, and the long contextual ability isalso constrained by the model size. In the future,we hope to apply our method to extend the context",
  "Ethics Statement": "We are totally aware that text generation technologyhas a potential to be used maliciously to generatefake, toxic, or offensive content. We are aware thatif LLMs generate harmful or toxic information, ourapproach cannot explicitly prevent it. However,since the models and datasets used in our study arepublicly available and examined, we are confidentthat our approach will not introduce toxic contentduring the length extension phase.",
  "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,Mantas Mazeika, Dawn Song, and Jacob Steinhardt.2020. Measuring massive multitask language under-standing. arXiv preprint arXiv:2009.03300": "Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shan-tanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang,and Boris Ginsburg. 2024. RULER: whats the realcontext size of your long-context language models?CoRR, abs/2404.06654. Albert Q. Jiang, Alexandre Sablayrolles, AntoineRoux, Arthur Mensch, Blanche Savary, Chris Bam-ford, Devendra Singh Chaplot, Diego de Las Casas,Emma Bou Hanna, Florian Bressand, GiannaLengyel,Guillaume Bour,Guillaume Lample,Llio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,Sophia Yang, Szymon Antoniak, Teven Le Scao,Thophile Gervet, Thibaut Lavril, Thomas Wang,Timothe Lacroix, and William El Sayed. 2024. Mix-tral of experts. CoRR, abs/2401.04088. AmirhosseinKazemnejad,InkitPadhi,Karthikeyan Natesan Ramamurthy,Payel Das,and Siva Reddy. 2023.The impact of positionalencoding on length generalization in transformers. InAdvances in Neural Information Processing Systems36:Annual Conference on Neural InformationProcessing Systems 2023, NeurIPS 2023, NewOrleans, LA, USA, December 10 - 16, 2023.",
  "Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and En-rico Shippole. 2023. Yarn: Efficient context win-dow extension of large language models.CoRR,abs/2309.00071": "Ofir Press, Noah A. Smith, and Mike Lewis. 2022. Trainshort, test long: Attention with linear biases enablesinput length extrapolation. In The Tenth InternationalConference on Learning Representations, ICLR 2022,Virtual Event, April 25-29, 2022. OpenReview.net. Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar,Chloe Hillier, and Timothy P. Lillicrap. 2020. Com-pressive transformers for long-range sequence mod-elling. In 8th International Conference on LearningRepresentations, ICLR 2020, Addis Ababa, Ethiopia,April 26-30, 2020. OpenReview.net. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,and Yuxiong He. 2020. Zero: memory optimizationstoward training trillion parameter models. In Pro-ceedings of the International Conference for HighPerformance Computing, Networking, Storage andAnalysis, SC 2020, Virtual Event / Atlanta, Georgia,USA, November 9-19, 2020, page 20. IEEE/ACM. Anian Ruoss, Grgoire Deltang, Tim Genewein, JordiGrau-Moya, Rbert Csords, Mehdi Bennani, ShaneLegg, and Joel Veness. 2023. Randomized positionalencodings boost length generalization of transform-ers. In Proceedings of the 61st Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 2: Short Papers), ACL 2023, Toronto, Canada,July 9-14, 2023, pages 18891903. Association forComputational Linguistics.",
  "Qwen Team. 2024. Qwen2 technical report": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Aurlien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. 2023a. Llama: Openand efficient foundation language models. CoRR,abs/2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurlien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023b. Llama 2: Open foundation andfine-tuned chat models. CoRR, abs/2307.09288. Rowan Zellers, Ari Holtzman, Yonatan Bisk, AliFarhadi, and Yejin Choi. 2019. Hellaswag: Can amachine really finish your sentence? In Proceedingsof the 57th Conference of the Association for Compu-tational Linguistics, ACL 2019, Florence, Italy, July28- August 2, 2019, Volume 1: Long Papers, pages47914800. Association for Computational Linguis-tics.",
  "Disturbance": "ExtrapolationInterpolation : Illustration of the impact of interpolation andextrapolation on each dimensional distribution. Upper:Disturbance when the context window is extended to8k. Lower: Disturbance when the context window isextended to 16k. illustrates the disturbance to each di-mensional distribution caused by PI(Chen et al.,2023) ,YaRN(Peng et al., 2023) and our methodwhen the context window of the model is extendedto 8k and 16k. Our method achieves the lowestdisturbance to the distribution. 0.0 0.2 0.4 0.6 0.8 1.0",
  "B.1Experimental Setup": "We use 8 A100 GPUs and adopt ZeRO3 (Rajb-handari et al., 2020) strategies during the trainingstage, and use AdamW (Loshchilov and Hutter,2019) optimizer with 1 = 0.9 and 2 = 0.999. Weset the learning rate to 2 105 without warmupand weight decay. When extending the context win-dow to 8k, we spent approximately 6 hours trainingLLaMA-7B and approximately 10 hours trainingLLaMA2-13B. When extending the context win-dow to 16k, we spent approximately 7 hours train-ing LLaMA-7B and approximately 11 hours train-ing LLaMA2-13B. Both training and testing areaccelerated by FlashAttention-2 (Dao, 2023).",
  "Original4k84.930028.31PI(s=4)16k76.2272.4166.9771.87YaRN(s=4)16k72.3768.9763.2768.20CLEX(ms=16)64k58.2753.6951.4854.48Ours(s=4)16k79.4076.2171.6575.75": ": Comparative performance analysis of various context window extension methods on the RULER benchmark.The scaling factor of CLEX is dynamic, \"ms\" denotes the maximum scaling factor, and we set the maximum scalingfactor to 16 in accordance with the settings of (Chen et al., 2024). long documents, with our approach achieving thehighest retrieval accuracy. The original LLaMA2model, due to its limited capacity for handling longdocuments, fails to produce accurate answers whenthe context length exceeds 4k tokens. The infe-rior performance of CLEX may be attributed to theintroduction of new parameters for predicting thescaling factor, which requires more training data tofit, thereby leading to sub-optimal performance inscenarios with limited data.",
  "B.2.2Time complexity": "Considering the balance between efficiency andperformance, we also provide the time consump-tion of different methods, as shown in . Tofacilitate comparison, we normalized the time con-sumption. In comparison to a fixed scaling factor,CLEX introduces additional parameters to predictthe scaling factor, which necessitates the recalcula-tion of positional encoding, thereby increasing thetraining and inference times.",
  "B.2.3Perplexity": "Perplexity is commonly employed to evaluate amodels language modeling capabilities, and wetested the perplexity of different methods undernon-training conditions, with the results presentedin . However, perplexity often fails to re-flect a models actual performance on downstreamtasks, as a model may exhibit a relatively low per-plexity in non-training scenarios yet perform poorlyin real-world applications. In contrast to the de-crease in perplexity, we are more concerned withthe models performance on actual tasks.",
  "B.3Passkey Prompt": "We follow experimental setup of Mohtashami andJaggi (2023); Chen et al. (2023). We separatelyemployed our method with scaling factors of s=2and s=4 to extend the context windows of LLaMA27B and 13B to 8k and 16k, respectively. shows the prompt template. There is an important info hidden inside a lotof irrelevant text.Find it and memorize them.I will quiz you about the important informationthere.The grass is green.The sky is blue.The sunis yellow.Here we go.There and back again.(repeat n times)The pass key is 12345. Remember it. 12345 is thepass key.The grass is green.The sky is blue.The sunis yellow.Here we go.There and back again.(repeat m times)What is the pass key? The pass key is"
}