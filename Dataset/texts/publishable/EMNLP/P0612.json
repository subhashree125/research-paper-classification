{
  "Abstract": "Large language models (LLMs) are essentialtools that users employ across various scenar-ios, so evaluating their performance and guid-ing users in selecting the suitable service is im-portant. Although many benchmarks exist, theymainly focus on specific predefined model abil-ities, such as world knowledge, reasoning, etc.Based on these ability scores, it is hard for usersto determine which LLM best suits their partic-ular needs. To address these issues, we proposeto evaluate LLMs from a user-centric perspec-tive and design this benchmark to measure theirefficacy in satisfying user needs under distinctintents. Firstly, we collect 1,846 real-world usecases from a user study with 712 participantsfrom 23 countries. This first-hand data helpsus understand actual user intents and needs inLLM interactions, forming the User ReportedScenarios (URS) dataset, which is categorizedwith six types of user intents. Secondly, basedon this authentic dataset, we benchmark 10LLM services with GPT-4-as-Judge. Thirdly,we show that benchmark scores align well withhuman preference in both real-world experi-ence and pair-wise annotations, achieving Pear-son correlations of 0.95 and 0.94, respectively.This alignment confirms that the URS datasetand our evaluation method establish an effec-tive user-centric benchmark. The dataset andcode are publicly available1.",
  "Introduction": "Large language models (LLMs) are rapidly devel-oping and gradually changing the way people in-teract with computing systems and permeating di-verse facets of work and daily life (Wang et al.,2023a). Although LLMs show powerful capacitiesfor completing various tasks (Chang et al., 2023),it is essential to understand how they satisfy usersactual intents and needs (Elangovan et al., 2024).According to related studies, many attempts have",
  "been made to evaluate LLMs, which may be di-vided into following two groups.The first group of work (Hendrycks et al., 2020;": "Clark et al., 2018; Zellers et al., 2019) considersLLMs as task-completion models and mainly em-ploys close-domain assessments to measure theirvarious task-solving abilities, such as world knowl-edge, mathematics ability, coding skill, reason-ing, planning, agent task, etc. (Chang et al., 2023).However, the existing attempts focused on prede-fined tasks, without considering the roles that hu-mans would like LLMs to play in real-world scenar-ios. For example, while a task was evaluated, it wasunclear if the task was important to humans andfrequently required by users in real applications.The second group evaluates whether LLMs re-sponses align with human preferences (Wang et al.,2023b). summarizes the user-related LLMevaluation benchmarks. Their evaluation datasetsinclude synthetic data (Li et al., 2023b), human-written data (Zheng et al., 2024), or selected userlogs from certain LLM services (Lin et al., 2024).We can observe that these benchmarks may be lim-ited in terms of data quality, quantity, or focus on",
  "URS (Ours)1,846English, Chineseself-reported logs with 15 LLMs6 intents": ": Comparison between user-related LLM benchmarks with our URS benchmark. The compared studiesare AlpacaEval (Li et al., 2023b), MT Bench-101 (Bai et al., 2024), TencentLLMEval (Xie et al., 2023), MTBench (Zheng et al., 2024), AlignBench (Liu et al., 2023a) and WildBench (Lin et al., 2024). a single resource. They do not reflect the true dis-tribution of intended utilization of LLMs by realusers and their effectiveness for these intents.Besides, the categorization in current bench-marks is mainly focused on single-step tasks (Zhaoet al., 2023; Liu et al., 2023a). However, end usersmight be more interested in knowing the ability ofLLMs to satisfy their intended needs, rather thanLLMs performance on predefined individual tasks.In particular, a user intent might involve multiplemodel abilities with a series of tasks. For example,when a user is asking for advice about travel plans,this might involve model abilities of planning, per-sonalization, and calling APIs. Consequently, thecategorization of model-side abilities complicatesthe assessment of LLM performance in user-sidescenarios, making it challenging for users to selectthe suitable service tailored to their specific needs.This paper addresses the above issues in LLMevaluations by developing a user-centric bench-mark to examine whether LLMs behaviors satisfyuser needs in real-world scenarios. As shown in, our work is different with the existingability-focused benchmarks and highlighted withthe three unique characteristics:User-centric. We benchmark LLMs from the userperspective in both dataset construction and eval-uation designs. Our constructed User ReportedScenarios (URS) benchmark encompasses 1,846authentic interactions (1,014 in English and 832in Chinese) across 15 diverse LLM services, con-tributed by 712 users from 23 countries, each vettedthrough third-party manual quality checks. Thisdataset, reflecting the real-world usage of LLMs,is publicly available for research with user consent.Based on this first-hand data, we design evaluationsto measure LLMs efficacy in satisfying real userneeds. The benchmark results align well with hu- man preference in both real-world user experienceacross intents and pairwise comparison betweenLLMs. This user-centric approach promotes deepercollaboration between LLMs and humans as theircapabilities continue to improve.Intent-driven. Different from the task-specificevaluations, our benchmark is categorized by userself-selected intents, including factual questionanswering, professional problem solving, text as-sisting, asking for advice, seeking creativity, andleisure. Such categorization provides the potentialutilization of LLMs by real users and is validatedthrough the user study. Based on the divided in-tents, the users who lack specific knowledge aboutthe model capabilities can simplify their choice ofproper service. For researchers and developers, thiscategorization helps to provide a more precise andtargeted evaluation of how well LLMs meet userneeds in diverse scenarios as one simple user intentmay demand a blend of model abilities to addressa series of tasks (Bolotova et al., 2022).Multi-cultural. Our data is contributed by usersfrom 23 countries in Asia, Europe, North America,Oceania, South America, and Africa. Their casescover multiple cultural backgrounds, such as na-tions traditional festivals, local points of interest,and pop culture across the globe. This brings di-versity in LLM evaluations, which is not includedin simple translations of English-dominant con-tent (Hershcovich et al., 2022; Huang et al., 2024b).In this study, our contributions are threefold:(1) We collect 1,846 authentic cases from 712global users to form the multi-intent, multi-culturalUser Reported Scenario dataset.(2) Based on this URS dataset, we propose anintent-aware evaluation method to benchmark theefficacy of 10 LLM services in satisfying userneeds in diverse real-world scenarios. (3) Experiments demonstrate that benchmark re-sults align closely with human preferences, as ev-idenced by Pearson correlations of 0.95 and 0.94with real-world user experiences and pairwise an-notations. These results validate that our automatedevaluation method and URS dataset establish a newand effective user-centric benchmark.",
  "LLM Benchmark": "The existing benchmarks are mainly divided intotwo categories as illustrated in . They aremodel ability-focused and user-centric.Ability-focused benchmarks. Considering thebroad capabilities of Large Language Models(LLMs), the evaluations span a diverse range ofdomains, typically categorized into Knowledge:MMLU (Hendrycks et al., 2020), CMMLU (Liet al., 2023a), and C-Eval (Huang et al., 2024c);Mathematical abilities: GSM8k (Cobbe et al.,2021), MATH (Hendrycks et al., 2021), andOlympiadBench (He et al., 2024a); Coding skills:HumanEval (Chen et al., 2021), MBPP (Austinet al., 2021), or SWEBench (Jimenez et al., 2023);Reasoning: BBH (Suzgun et al., 2022; bench au-thors, 2023), ARC (Clark et al., 2018), and Hel-laSwag (Zellers et al., 2019); Agent tasks: Agent-Bench (Liu et al., 2023b) and ToolBench (Qin et al.,2023). Each category aims to measure specific abil-ities expected from LLMs.User-Centric benchmarks. Unlike benchmarksthat focus on model capabilities, a few emphasizeuser experience with chat-based LLMs. details related user-centric benchmarks. However,we notice several problems with these benchmarks.AlpacaEval (Li et al., 2023b), MTBench (Zhenget al., 2024), and MT-Bench-101 (Bai et al., 2024)are limited in data quality or quantity.Ten-centLLMEval (Xie et al., 2023) used tasks thatwere designed by hand and written by 50 anno-tators, an approach that may lead to deviationfrom real-world scenarios. AlignBench (Liu et al.,2023a) and WildBench (Lin et al., 2024) are con-structed based on real-world use cases from logsof one specific LLM, which may introduce biassince the users may have access limitations to cer-tain LLMs and use LLM services selectively de-pending on their needs. Furthermore, the existinguser-centric benchmarks mainly do not considermultilingual and multi-cultural aspects, which arecovered in our dataset.",
  "Evaluation Methods": "Besides dataset constructions, evaluation methodsare crucial in developing benchmarks. To simplifyevaluation processes, many studies utilize multiple-choice questions as criterion (Hendrycks et al.,2020; Li et al., 2023a; Huang et al., 2024c). Othersemploy post-processing of generated content andapply rules or deterministic tests to assess perfor-mance (Cobbe et al., 2021; Chen et al., 2021). Withthe recent advancements in LLM techniques, an in-creasing number of approaches use a more capableAI model to evaluate answers (He et al., 2024b;Li et al., 2023b), enriching benchmark types by al-lowing a more diverse format of test questions. Al-pacaEval (Li et al., 2023b), AlignBench (Liu et al.,2023a) adopt GPT-4 as evaluator. PRE (Chu et al.,2024) shows that GPT-4 performs well in pairwiseevaluations and Huang et al. (2024a) shows thatGPT4 outperforms fine-tuned judge models in gen-eralizability and fairness. In our benchmark, weadopt previous settings for open-domain evaluation,using LLM-based pairwise evaluation.",
  "User Intent": "From a user-centric view, we should first confirmuser intent and evaluate service responses corre-spondingly. Related ideas try to understand userintent in interacting with LLMs. Shah et al. (2023)provides a user intent taxonomy based on NewBing, which includes information retrieval, prob-lem solving, learning, content creation, and leisure.Bodonhelyi et al. (2024) details user intent withChatGPT with informational, problem solving, cre-ative, educational, personal interaction, technicaland professional, transactional, ethical, and philo-sophical intents. Inspired by these studies, we de-sign a user intent taxonomy accordingly, which isfurther verified by 712 user study participants.",
  "User Intent Taxonomy": "User intents represent specific needs or goals wheninteracting with a service and users may antici-pate different types of feedback under differentintents (Bolotova et al., 2022). For example, forfactual intents, users may expect brief and accu-rate answers; for brainstorming needs, users tendto expect rich and innovative answers. By consid-ering different user intents, we can assess LLMsfine-grain efficacy in diverse scenarios. Therefore,we first define the taxonomy of user intent based IP distribution for",
  ": IP Distribution of the 712 participants": "on related work (Shah et al., 2023; Bolotova et al.,2022). They are detailed below:Factual QA. Fast and direct access to factual infor-mation. For example, search for historical events,scientific facts, or public data.Solving Professional Problems. Require answersand insight in specialized fields like natural sci-ences, humanities, or social sciences for problem-solving or learning. It involves domain knowledge,in-depth understanding, and reasoning in profes-sional areas. For example, solving math, engineer-ing puzzles, and conducting medical diagnoses.Text Assistant. Need assistance with text-relatedtasks such as summary, translation, editing or com-pleting content. For example, replying to an email,writing a report, or polishing a speech.Ask for Advice. Look for opinions and suggestionsfor personal or professional decisions, includingplanning or counseling. For example, career devel-opment, personal counseling, creating travel plans,or shopping lists.Seek Creativity. Brainstorming for inspiration andinnovative ideas. For example, advertising ideas,or design inspiration.Leisure. Engage in or look for recommendationsfor recreational activities, including books, music,movies, games, and other entertaining activities.This user intent taxonomy serve as the potentialutilization of LLM services by real users and isfurthered verified in the following user study.",
  "User Study": "Participants in the study were requested to pro-vide five to ten real cases with LLMs they haveused. Each reported case included the followingthree components: the LLM service they queried,complete conversation, and their usage intent (ei-ther selected from the predefined list or filled in bythemselves). The detailed questionnaire is attachedin Appendix A.Additionally, the study investigate users generalexperience with LLMs, including their satisfactionacross different intents, which is used to validatebenchmark alignment with human feedback in Sec-tion 4.5. We also record the anonymous, voluntar-ily reported demographic information in the ques-tionnaire to examine whether this survey involvesdiverse groups of participants.To capture multi-cultural scenarios, we offer thequestionnaire in both English and Chinese andspread it through Prolific2, X, and WeChat Mo-ments platforms for global enrollment. Crowd-workers are paid according to Prolific recommen-dations (9/hr * average 10 mins = 1.5).",
  "Construction": "In the initial phase of dataset construction, wecheck the diversity of our participants. shows the distribution of automatically recordedIP for English and Chinese questionnaire respon-dents. The feedback comes from 712 participantsacross 23 countries, showing the diversity in distri-butions. Detailed profiles, including demographicinformation (age and occupation) and LLM usageexperience, are provided in Appendix B.1.Subsequently, we examine the feedback on userintents. There are no valid proposals under the\"Others\" option and manual fill-ins, suggesting a",
  "Factual QuestionAnsweringEN: Tell me what bitcoin isCN: (When is the Major Snow in the Lunar calendar)": "Solve ProfessionalProblemsEN: How long does it take to transfer 13.72GB if speed is 10MB/sCN: n12 (Supposeyou are climbing a staircase. It takes n steps to reach the top. You can climb1 or 2 steps at a time. How many different ways can you climb to the top?) Text AssistantEN: Help me rephrase the document: The NBAs inaugural in-season tourna-ment has concluded with the Los Angeles Lakers beating the Indiana Pacers123-109 to lift the NBA Cup, with the teams winning players ......CN: 2024 (Help me com-pose a Spring Festivals Eve Wechat text to my leader for the Year of theDragon 2024.)",
  "LeisureEN: What is the best order to watch films and tv shows in the MCU?CN: (Recommend me some Can-tonese songs that are easier to cover)": ": Example English and Chinese Cases under each Intent. The notation \"EN\" represents that the case is inEnglish. \"CN\" means the cases are reported in Chinese and we attach their English translation in italics. \"......\"represents the text behind is omitted. Note that due to space limitations, we present cases with shorter length in theabove table. The average length of each question is 29.65 tokens in the URS dataset. general comprehensive coverage of the proposeduser intent taxonomy.Finally, we carry out meticulous data processingto construct the User Reported Scenario (URS)dataset as original feedback inevitably containsnoise. Steps include format validation, elimina-tion of intra-user duplication, and extraction ofinitial valid questions from each conversation. Thedataset construction is augmented with third-partymanual quality assessments to objectively filter outlow-quality cases and confirm the exclusion of per-sonal information. Detailed ethics considerationsabout this dataset are discussed in .",
  "reports the statistics of the URS dataset,with all data sourced from the above user study.In all intents, those that are relatively subjective,including Ask for Advice, Seek Creativity and": "Leisure, constitute about 33% of the total use cases.As the traditional benchmarks primarily focus onevaluating LLMs correctness of subjective ques-tions (Chang et al., 2023), this objective portion ofuser needs is often overlooked. Besides the diversity in user intents, the URSbenchmark also includes interactions from 15 dif-ferent LLM services. The distribution is detailedin Appendix B.2. This breadth reduces potentialbiases stemming from reliance on a single LLMservice. These biases can arise from users lim-ited access to certain LLMs and their preferencefor specific services in some scenarios. Amongthis diversity, we observe a long-tail distributionin model usage. As we did not actively control orselect use cases of certain LLMs during the datasetcollection and construction processes, this may in-dicate the natural distribution in real-world usages.",
  "Output": ": Evaluation Procedure. For each evaluation instance, the evaluator is provided with the user intent, fiveintent-aware criteria, chain-of-though reasoning steps, scoring standards for each two-point segment, addition withthe question, an 8-score reference answer for this question, and the test LLM output for evaluation. Then, a parserwill extract the final score from the evaluators detailed rating content to form the benchmark. Time is also an important factor influencing LLMevaluations. Since the user study was conducted inFebruary 2024, the collected cases in this datasettook places before this time. showcases exemplary instances from thedataset. Real-world scenarios are often brief in ar-ticulation but complex in resolutions and span anexpansive range of situations. Notably, beyond lin-guistic differences, the cases in this dataset also en-compass a variety of cultural backgrounds. Theseinclude traditional events, such as Major Snow andthe Spring Festival in the lunar calendar as well aspopular entertainment information that are globallyor locally recognized, such as the Marvel Cine-matic Universe and Cantonese songs. These multi-cultural contexts could enhance the benchmarkingof LLMs, addressing more diverse needs of theglobal user base.",
  "Evaluation Framework": "We aim to design a evaluation framework that canautonomously, expeditiously, and precisely deter-mine the performance of any LLMs. The automa-tion is made possible by using a strong model (e.g.,GPT-4) acting as the evaluator, whose effectivenessis validated in previous work for open-domain eval-uations (Chang et al., 2023; Sottana et al., 2023;Liu et al., 2023c) and further examined in Sec-tion 4.5. Aiming for high speed and precision,we implement a direct pair-wise scoring approach,providing a fixed reference answer for each ques-tion, in contrast to point-wise rating and Elo ratingmethods, as has been demonstrated to be effectivein the previous study (Li et al., 2023b; Liu et al.,2023a). As shown in , we categorize the evaluation instruction into 4 parts, including intent-aware criteria, chain-of-thought reasoning steps,scoring standards, and reference materials, whichare described below (The detailed instructions areprovided in Appendix C.2):Intent-aware criteria. During evaluations, we in-form the evaluator about the user intent for eachquestion and provide five specific evaluation cri-teria tailored to that intent. This approach is de-signed to help evaluators accurately capture userneeds across various intent types and provide pre-cise scorings. Detailed illustrations of these criteriaand the correspondence relationship between userintents and evaluation criteria are provided in Ap-pendix C.1.Chain-of-thought reasoning steps. To fully lever-age the reasoning capabilities of evaluators, the in-struction provides four steps before assigning finalratings. These steps include contrasting the refer-ence answer with the test response, scoring eachcriterion on a 1-10 scale, reassessing whether theresponse meets user needs, and integrating theseassessments to determine the final score.Scoring standards. To ensure accurate and dif-ferentiated scoring, we provide standards for eachscoring segment. Each 2-point increment consti-tutes a distinct segment. The answer furnishedby us is an 8-score reference. Besides, a note isprovided to indicate that longer responses are notnecessarily better to limit the potential length bias.Reference materials. The instruction provided forthe evaluator includes the question given by realusers, their intent for asking, a fixed reference an-swer for this question (provided by a strong LLM,which is GPT-4 in this paper and checked manu-ally), and the response generated by the test LLMfor evaluation.",
  "Experimental Settings": "Among the top-used LLM services reported in theprevious user study, we test all the LLMs withavailable APIs, including Baichuan2-Turbo (Yanget al., 2023), Claude-3-opus (Anthropic, 2024),Deepseek-chat (Bi et al., 2024), ERNIE-Bot-4 (Baidu), GLM-4 (Du et al., 2021), GPT-3.5-turbo (Achiam et al., 2023), GPT-4o (Achiam et al.,2023), Moonshot-v1-8k (MoonshotAI), Qwen-max (Bai et al., 2023), Spark-3.5 (iFLYTEK). Wedo not adjust the default temperature setting ofLLMs based on query intent. This helps us tomodel the efficacy of LLM services in differentreal-world scenarios without user intent informa-tion. We benchmark the above LLMs on 1024human-examined cases in the URS dataset.In the following sections, we conduct extensiveexperiments to answer four research questions:(RQ1) What is the performance of differentLLMs in the user-centric evaluation perspective?(RQ2) Are the evaluation results stable acrossdifferent LLMs as evaluators?(RQ3) Do the benchmark results align with realuser perceptions?(RQ4) What multi-cultural features do we ob-serve in this benchmark?",
  "Overall Results (RQ1)": "The overall benchmark results are shown in .We have the following observations:(1) GPT-4 yields an average score of 8.15 whenits previously generated responses are used as 8-score references in the scoring instruction. Thisdifference indicates an acceptable level of vari- ance from the established standard. The bench-mark stability across different evaluators is furtherdiscussed in the next section.(2) There is a noticeable stratification in scoresacross different models. Apart from GPT-4, theleading group comprises Claude-3 and Qwen-max,followed by a secondary tier including Moonshot-v1, GLM-4, and ERNIE-Bot-4. The performancesof the third tier are comparable to that of GPT-3.5.(3) In examining efficacy across diverse userintents, GPT-4 generally demonstrates superior per-formance, except for the Text Assistant category,where Qwen-max prevails.Furthermore, a comparative analysis of LLMsacross different user intents reveals a pronouncedproficiency in objective contexts, including SolveProblem and Factual QA. This is probably becausesubjective scenarios require more diverse compe-tencies such as personalization, creativity and hu-mor, where current LLMs are relatively weak.Separated results of English and Chinese sce-narios are presented in Appendix D.1. Scoringexamples are provided in Appendix D.2.",
  "Cross Validation between GPT-4 andClaude-3 (RQ2)": "In the overall benchmark result, we adopt GPT-4s direct output to the question as a reference forscore 8, and use GPT-4 as the evaluator base forfinal scoring, as detailed in Setion 4.1. Althoughthis approach is widely adopted, it may cause favorto responses similar to GPT-4. To counteract thisbias, we re-evaluated the performance of the top-2 LLMs, GPT-4o, and Claude-3-opus, employing GPT EvaGPT Ans GPT Eva",
  "Leisure": ": Benchmark Score and User Reported Satisfac-tion Correlate Well across Intents. \"Benchmark Score\"is averaged under different intents. \"User Reported Sat-isfaction\" is the average satisfaction level reported inthe user study. Intents are ranked by user satisfaction. cross-validation techniques. We run 5-fold eval-uation settings on randomly selected 200 cases,using GPT and Claude as the reference genera-tor and base evaluator respectively, and the thirdperformed LLM, Qwen, as both reference gener-ation and evaluation for a third-party evaluation.As illustrated in , when GPT-generated re-sponses are used as references, GPT-4 consistentlyachieves higher scores no matter the evaluator mod-els. When Claude-generated responses are adoptedin evaluations, the results are comparable. In third-party assessments, GPT-4 also maintained a supe-rior ranking over Claude-3. This order is consistentwith the overall benchmark results, indicating thatthe potential bias towards GPT does not affect thebenchmark ranking.",
  ": LLMs Benchmark Ranking Aligns with theOrder from Human Pairwise Annotation": "tion and pairwise LLM-wise comparisons.For intent-wise evaluation, user satisfaction lev-els were collected in the user study detailed in Sec-tion 3.2.1, involving 420 global participants whorated their satisfaction on a five-point scale for eachintent while using LLM services. A high Pearsoncorrelation coefficient of 0.95 between the aver-age benchmark score and user-reported satisfactionacross different intents, as shown in , con-firms the alignment. Notably, subjective scenar-ios such as Ask for Advice, Seek Creativity andLeisure received lower ratings than more subjectiveones in both benchmark results and user feedback.This indicates the current LLM services still needimprovements in satisfying real-world user needs.In addition to the above alignment analysis basedon large-scale user studies, we further conductedpair-wise annotations according to methods in Chat-bot Arena (Chiang et al., 2024) to check the consis-tency between the designed automatic evaluationand human preferences. Due to space limitations,the detailed approach is provided in Appendix D.3.Based on 400 paired comparisons assessed by 5human annotators, we computed Bradley-Terry co-efficients (Bradley and Terry, 1952) for each LLM.As shown in , the ranking of LLMs accord-ing to these coefficients aligns with the benchmarkrankings with Pearson r=0.94, further affirming theeffectiveness of evaluation designs.",
  "Analysis of Multi-cultural Features (RQ4)": "We further analyzed the multi-cultural features inthe benchmark. Based on the location of questionproviders, we divide the dataset to observe the per-formance differences of LLMs when answeringquestions from users with different cultural back-grounds. presents the results of the topfive countries with the highest number of surveyparticipants. Among the 10 tested LLMs, 6 per-",
  "Conclusion": "By focusing on real-world cases, user intent cate-gorization, and the intent-aware evaluation method,we establish a benchmark of high alignment withhuman preference, endeavoring to advance the user-centric evaluation of LLMs. We hope the insightsgained from this research will not only help im-prove the performance of LLMs but also foster adeeper understanding of how these services can beeffectively integrated into peoples lives to enhanceproductivity, creativity, and overall welfare.",
  "Limitations": "While evaluating LLMs from the user-centric per-spective offers considerable advantages, there arecertain limitations in our URS benchmark.Evaluation Method: Our approach aligns withprevailing practices in open-ended question evalu-ation, where GPT-4 is employed as the base eval-uator. This could potentially lead to a bias towardresponses resembling GPTs style, thereby influenc-ing the neutrality of the evaluation. We analyzedthrough cross-validation that this approach doesnot affect the top 2 LLM orders. Besides, analysisshows that benchmark score aligns with real-worlduser experience and human preferences in pairwiseannotations. These further tested the validity ofour method. In addition, the user study and humanpairwise annotations could be further scale up toenhance persuasiveness. Besides, we notice it is very important to evalu-ate LLM services performances in multi-turn con-versational settings, as there are a few cases wherethe LLMs do not answer the user question and askto provide more detailed information about userintents. These may be judged unsuccessful in asingle-turn assessment, but the service might becapable of meeting this user need. Note that this benchmark is not holistic, as wemainly measure LLMs efficacy in satisfying real-world user needs. User satisfaction is indeed cru-cial, while it represents one aspect of LLM perfor-mance. Other critical facets include effectiveness,efficiency, ease of use, and error tolerance. In thisstudy, our primary focus is on user-centric evalu-ation of LLMs, which we have validated throughstrong alignments with human preferences to ad-dress its effectiveness. Data Distribution: The data for our study wasmainly sourced from a global researcher recruit-ment platform, which does not ensure a randomselection of all participants using LLM services.Consequently, the demographic distribution of ourdataset may not accurately reflect that of typicalLLM users or the global population. While asan independent third party unaffiliated with spe-cific LLM services, and given our non-selectiveapproach to data collection, our methodology incor-porates greater diversity and offers a closer align-ment with real-world usage scenarios compared todatasets that are human-designed, model synthetic,or selected by non-third parties or single-sourced.",
  "Ethical Considerations": "In the user study, participants were informed thattheir reported conversations would be publicly re-leased for research purposes. The notification isdetailed in Appendix A. At the same time, theyvoluntarily chose whether to provide anonymizedbasic information, such as age group and profes-sion.Additionally, the survey design included an an-chor question requiring participants to select spe-cific options; responses that failed to adhere tothese instructions were discarded as malicious feed-back.The questionnaire was provided in English andChinese (Mandarin), the top 2 widely spoken lan-guages3, and distributed via the global recruitmentplatform4. In this process, we did not filter anyattributes as this could actively introduce bias. Fol-lowing data collection, a third party conducted man-ual reviews to ensure the datas quality, harmless-ness, and the absence of any personal information.This dataset could be used for research purposesto understand user intents and real-world needs,evaluate conversational systems responses, andtrain for better alignment with human preference.Note that this URS dataset uses an Apache License.",
  "AI Anthropic. 2024. The claude 3 model family: Opus,sonnet, haiku. Claude-3 Model Card": "Jacob Austin, Augustus Odena, Maxwell Nye, MaartenBosma, Henryk Michalewski, David Dohan, EllenJiang, Carrie Cai, Michael Terry, Quoc Le, andCharles Sutton. 2021. Program synthesis with largelanguage models. Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jia-heng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su,Tiezheng Ge, Bo Zheng, Yancheng He, Jiaheng Liu,Zhanhui Zhou, Zhuoran Lin, Wenbo Su, TiezhengGe, Bo Zheng, et al. 2024. Mt-bench-101: A fine-grained benchmark for evaluating large languagemodels in multi-turn dialogues.arXiv preprintarXiv:2402.14762.",
  "BIG bench authors. 2023. Beyond the imitation game:Quantifying and extrapolating the capabilities of lan-guage models. Transactions on Machine LearningResearch": "Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen,Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong,Qiushi Du, Zhe Fu, et al. 2024. Deepseek llm: Scal-ing open-source language models with longtermism.arXiv preprint arXiv:2401.02954. Anna Bodonhelyi, Efe Bozkir, Shuo Yang, Enkele-jda Kasneci, and Gjergji Kasneci. 2024. User in-tent recognition and satisfaction with large languagemodels: A user study with chatgpt. arXiv preprintarXiv:2402.02136. Valeriia Bolotova, Vladislav Blinov, Falk Scholer,W Bruce Croft, and Mark Sanderson. 2022. A non-factoid question-answering taxonomy. In Proceed-ings of the 45th International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval, pages 11961207.",
  "Ralph Allan Bradley and Milton E Terry. 1952. Rankanalysis of incomplete block designs: I. the methodof paired comparisons. Biometrika, 39:324345": "Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,Cunxiang Wang, Yidong Wang, et al. 2023. A sur-vey on evaluation of large language models. ACMTransactions on Intelligent Systems and Technology. Mark Chen, Jerry Tworek, Heewoo Jun, Heewoo Jun,Qiming Yuan, Henrique Ponde de Oliveira Pinto,Jared Kaplan, Harri Edwards, Yuri Burda, NicholasJoseph, Greg Brockman, , et al. 2021.Evaluat-ing large language models trained on code. arXivpreprint arXiv:2107.03374. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anasta-sios Nikolas Angelopoulos, Tianle Li, Dacheng Li,Hao Zhang, Banghua Zhu, et al. 2024. Chatbot arena:An open platform for evaluating llms by human pref-erence. arXiv preprint arXiv:2403.04132.",
  "Zhumin Chu, Qingyao Ai, Yiteng Tu, Haitao Li,and Yiqun Liu. 2024. Pre: A peer review basedlarge language model evaluator.arXiv preprintarXiv:2401.15641": "Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,Ashish Sabharwal, Carissa Schoenick, and OyvindTafjord. 2018. Think you have solved question an-swering? try arc, the ai2 reasoning challenge. arXivpreprint arXiv:1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,Mark othersChen, Heewoo Jun, Lukasz Kaiser,Matthias Plappert, Jerry Tworek, Jacob Hilton, Rei-ichiro Nakano, Christopher Hesse, and John Schul-man. 2021. Training verifiers to solve math wordproblems. arXiv preprint arXiv:2110.14168.",
  "Aparna Elangovan, Ling Liu, Lei Xu, Sravan Bodapati,and Dan Roth. 2024. Considers-the-human evalua-tion framework: Rethinking human evaluation forgenerative large language models. ACL 2024": "Nils Feldhus, Qianli Wang, Tatiana Anikina, SahilChopra, Cennet Oguz, and Sebastian Mller. 2023.Interrolang:Exploring nlp models and datasetsthrough dialogue-based explanations. arXiv preprintarXiv:2310.05592. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu,Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yu-jie Huang, Yuxiang Zhang, et al. 2024a. Olympiad-bench: A challenging benchmark for promoting agiwith olympiad-level bilingual multimodal scientificproblems. arXiv preprint arXiv:2402.14008. Chaoqun He, Renjie Luo, Shengding Hu, YuanqianZhao, Jie Zhou, Hanghao Wu, Jiajie Zhang, Xu Han,Zhiyuan Liu, and Maosong Sun. 2024b. Ultraeval: Alightweight platform for flexible and comprehensiveevaluation for llms. ACL 2024 demo paper.",
  "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,Mantas Mazeika, Dawn Song, and Jacob Steinhardt.2020. Measuring massive multitask language under-standing. arXiv preprint arXiv:2009.03300": "Dan Hendrycks, Collin Burns, Saurav Kadavath, AkulArora, Steven Basart, Eric Tang, Dawn Song, and Ja-cob Steinhardt. 2021. Measuring mathematical prob-lem solving with the math dataset. arXiv preprintarXiv:2103.03874. Daniel Hershcovich, Stella Frank, Heather Lent,Miryam de Lhoneux, Mostafa Abdou, StephanieBrandl, Emanuele Bugliarello, Laura Cabello Pi-queras, Ilias Chalkidis, et al. 2022. Challenges andstrategies in cross-cultural nlp. ACL 2022. Hui Huang, Yingqi Qu, Jing Liu, Muyun Yang, andTiejun Zhao. 2024a.An empirical study of llm-as-a-judge for llm evaluation:Fine-tuned judgemodels are task-specific classifiers. arXiv preprintarXiv:2403.02839. Kaiyu Huang, Fengran Mo, Hongliang Li, You Li,Yuanchi Zhang, Weijian Yi, Yulong Mao, JinchenLiu, Yuzhuang Xu, Jinan Xu, et al. 2024b. A sur-vey on large language models with multilingualism:Recent advances and new frontiers. arXiv preprintarXiv:2405.10936. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, JunleiZhang, Jinghan Zhang, Tangjun Su, Junteng Liu,Chuancheng Lv, Yikai Zhang, Yao Fu, et al. 2024c.C-eval: A multi-level multi-discipline chinese evalua-tion suite for foundation models. Advances in NeuralInformation Processing Systems, 36.",
  "iFLYTEK. Spark": "Carlos E Jimenez, John Yang, Alexander Wettig,Shunyu Yao, Kexin Pei, Ofir Press, and KarthikNarasimhan. 2023. Swe-bench: Can language mod-els resolve real-world github issues? arXiv preprintarXiv:2310.06770. Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, HaiZhao, Yeyun Gong, Nan Duan, and Timothy Bald-win. 2023a. Cmmlu: Measuring massive multitasklanguage understanding in chinese. arXiv preprintarXiv:2306.09212.",
  "MoonshotAI. Moonshot": "Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, LanYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie,Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu,and Maosong Sun. 2023. Toolllm: Facilitating largelanguage models to master 16000+ real-world apis. Chirag Shah, Ryen W White, Reid Andersen, GeorgBuscher, Scott Counts, Sarkar Snigdha Sarathi Das,Ali Montazer, Sathish Manivannan, Jennifer Neville,Xiaochuan Ni, et al. 2023. Using large languagemodels to generate, validate, and apply user intenttaxonomies. arXiv preprint arXiv:2309.13063. Andrea Sottana, Bin Liang, Kai Zou, and Zheng Yuan.2023. Evaluation metrics in the era of gpt-4: reli-ably evaluating large language models on sequenceto sequence tasks. arXiv preprint arXiv:2310.13800. Mirac Suzgun, Nathan Scales, Nathanael Schrli, Se-bastian Gehrmann, Yi Tay, Hyung Won Chung,Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi,Denny Zhou, and Jason Wei. 2022.Challengingbig-bench tasks and whether chain-of-thought cansolve them.",
  "Bryan Wang, Gang Li, et al. 2023a. Enabling conversa-tional interaction with mobile ui using large languagemodels. In Proceedings of the 2023 CHI Conferenceon Human Factors in Computing Systems": "Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xing-shan Zeng, Wenyong Huang, Lifeng Shang, XinJiang, and Qun Liu. 2023b.Aligning large lan-guage models with human: A survey. arXiv preprintarXiv:2307.12966. Shuyi Xie, Wenlin Yao, Yong Dai, Shaobo Wang,Donlin Zhou, Lifeng Jin, Xinhua Feng, PengzhiWei, Yujie Lin, Zhichao Hu, et al. 2023.Ten-centllmeval: a hierarchical evaluation of real-worldcapabilities for human-aligned llms. arXiv preprintarXiv:2311.05374.",
  "Rowan Zellers, Ari Holtzman, Yonatan Bisk, AliFarhadi, and Yejin Choi. 2019. Hellaswag: Can amachine really finish your sentence? arXiv preprintarXiv:1905.07830": "Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie,Yejin Choi, and Yuntian Deng. 2023. (inthe) wild-chat: 570k chatgpt interaction logs in the wild. InThe Twelfth International Conference on LearningRepresentations. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, et al. 2024. Judging llm-as-a-judge with mt-bench and chatbot arena. Advancesin Neural Information Processing Systems, 36.",
  "AUser Study": "At the start of the user survey, we have this notifi-cation:Your dialog reported in this questionnaire maybe publicly released for research purposes, soplease avoid including personal information, andwe will also conduct a secondary check, thank you!Subsequently, participants are asked to providefive to ten distinct real use cases with LLM services.Each use case should comprehensively cover thefollowing aspects:1. LLM Service Utilized:Which of the following large language models didyou ask the question to2. Conversation Content:Your question and the systems outputIt is recommended to directly share your conversa-tion link!If you fill it out manually, please respectfully followthe format below.Question: xxxAnswer: xxx3. Usage Intent:Your intent behind this question isParticipants are also asked to voluntarily re-port the following demographic information anony-mously:1. Age Group: To understand the age distribu-tion of LLM users.2. Profession: To gauge the professional back-ground and its influence on LLM usage.3. IP Location: To capture the geographicaldistribution of participants.General usage Experience with LLMs:1 LLM used:Large Language Models you have used2 Usage Frequency:How often do you use LLMs3 Satisfaction with LLMs:Your general satisfaction with LLM services acrosseach intent",
  "Instruction Template": "You are asked to assess the quality of an AI assis-tants answer to a users question as an impartialjudge. Since the type of answer you are evaluating is[{user_intent}], you need to evaluate the answer inthe following 5 criteria:1 {criteria_dict[user_intent]}2 {criteria_dict[user_intent]}3 {criteria_dict[user_intent]}4 {criteria_dict[user_intent]}5 {criteria_dict[user_intent]} We will provide you with the users question, an8-score reference answer, and answers from the AIassistant that needs your assessment. When startingyour evaluation, you need to follow the reasoningsteps below:1.Compare the AI assistants answer with thereference answer, point out any shortcomings in theAI assistants answer, and explain further.2. Evaluate the AI assistants answer in terms of thedifferent criteria, giving each criterion a score from 1to 10 after the evaluation of each.3.Finally, combine the evaluations from eachcriterion and give the AI assistants answer acomposite score of 1 to 10.4. Your scoring needs to be as rigorous as possibleand adhere to the following scoring rules: in general,the higher the quality of the models answers, thehigher the score. When the model answer has irrelevance to thequestion, or intrinsically factually incorrect, orgenerates harmful content, the total score should be 1to 2;When the model answer has no serious errors and islargely harmless, but is of low quality and does notmeet user requirements, the total score must be 3 to4;When the model answer basically meets the usersneeds but performs poorly on some criteria and is ofmedium quality, the total score can be 5 to 6;When the quality of the model response is similar tothe reference answer and performs well in all criteria,the total score should be 7 to 8;A score of 9 to 10 can only be achieved if the modelsignificantly exceeds the quality of the referenceanswer, adequately addresses the users question and all the needs, and is close to a perfect score on allcriteria. As an example, the reference answer wouldreceive a score of 8.Do not let the length of the answer affect your score.Longer answers are not necessarily better. Answersthat are concise and meet the above requirements aregood. You need to evaluate and explain before you score.Your explanation of each criterion needs to befollowed by the scoring. After that, at the end of youranswer, return all of your scores in the followingdictionary format, including the curly brackets, andmake sure that your scores are integers:{Dimension1:score,Dimension2:score,...,FinalScore:score},e.g.{{criteria_dict[user_intent]}:9,{crite-ria_dict[user_intent]}: 6, ... , Final Score: 7}.",
  "D.3Alignment with Human Annotation": "We aim to further investigate whether the bench-marked LLM order aligns with human preferencesby conducting annotation experiments. Follow-ing the method outlined in Chatbot Arena (Chianget al., 2024), we enlisted five human annotators(three males and two females) to evaluate pairedquestions as shown on the UI in . Werandomly selected 400 questions from the 1024-case benchmark and randomly paired two LLMoutputs for each question. Annotators, blind towhich LLMs generated the responses, were taskedwith choosing their preferred answer from the twoprovided, or indicating if the choice was equal or",
  "Claude-3-opus19100%GPT-412100%ERNIE-Bot-4994%GPT-3.5-turbo886%Qwen-max457%spark-3.5450%": ": Addition Ability. \"Max L\" represents thelongest sequence of consecutive additions attainableat a minimum accuracy rate of 80%. \"Accuracy\" de-notes the success ratio in 90 trials covering the lengthof 2 to 9 with 10 test cases of each length. Outcomesare presented for LLMs with an accuracy of 50% andabove. undeterminable due to their knowledge limitations.Of the 400 paired comparisons, 50 were marked\"cannot be determined.\" Utilizing the remainingannotations, we calculated the Bradley-Terry (BT)scores to establish LLM rankings based on pair-wise competition outcomes. As shown in ,the resulting LLM order aligns with the benchmarkrankings, and the BT score demonstrates a Pearsoncorrelation of 0.94 with the benchmark score. Theevaluation data is anonymously available.",
  "D.4Comparison with Different EvaluationMethod": "shows cross-validation evaluation results.We change the model to generate reference answersand the model to serve as evaluators respectively.Results show while using GPT4 output as the ref-erence answer for 8 scores, the order between thethree models remains unchanged no matter the eval-uation model. While using Qwen or GLMs outputas both reference generator and evaluator, thereexists a strong incline towards the model itself.",
  "Rating Bias(A) =B RatingAB(B) rB 1, r = 8": "(1)where RatingAB(B) representing using LLM Aas an evaluator and the output of LLM B as a refer-ence answer to evaluate the performance of LLMB. r is the score for reference, in our cases, it isequal to 8. Note that in the experiments, referenceanswer and output awaiting score are two-time out-puts of the same model, so if model B is stable andmodel A can score accurately, the RatingAB(B)should be r. We measure the Rating Bias to mea-sure the calibrability of the evaluator model. Asshown in the results, compared to Qwen-max andGLM-4, GPT-4 performs more accurately in thiscross-validation experiments. : Analysis of Evaluation Methodologies: \"A-B\" indicates the evaluation setting, where A denotesthe evaluator LLM and B represents the source LLMused to generate 8-point reference answers. Rating biasis defined in Equation 1. Specifically, the rating bias forevaluator A corresponds to the discrepancy between thereference score, 8, and the benchmark score of ModelB under the assessment setting of A-B.",
  "D.5Case Study": "In our benchmark, we incorporate a diverse rangeof scenarios, extending beyond commonplace ap-plications to include highly specialized use cases.A notable example involves assessing the modelsproficiency in executing sequential summationswithout relying on external tools. This particularcomputational skill often manifests spontaneouslyand poses a challenge for targeted training. Con-sequently, evaluating a models capability to accu-rately perform a series of two-digit additions offersa rapid and effective indicator of its computationalprowess.Our examination entails the concatenation oftwo-digit numbers ranging from 2 to 20 in se-quence, with each magnitude category being sub-jected to 10 randomly generated test instances. Ta-ble 8 delineates the performances of the top LLMs,showing their ability to achieve the longest chainof consecutive additions while maintaining a mini-mum accuracy threshold of 80% and the accuracyof adding from 2 to 10 numbers. Models not listedachieved an accuracy lower than 50%. It is worth noting that the top 4 LLMs with maxi-mum length>5 are the ones with larger parametersout of the total 10 LLMs. This might indicate thatthe ability of computing has correlations with thescaling of model size.",
  "D.6Scenario Study": "Since we evaluate LLMs as collaborative tools forusers, we juxtapose their performance with tradi-tional assistants like search engines and transla-tors. This comparison can assist and guide users inchoosing the most effective tool for their specificintents. To this end, we evaluate LLM performancecompared to Google Search Engine for Factual QAintent questions and Google Translator for transla-tion queries in Text Assistant scenarios.Search Engine. We first retrieve search results for57 random questions within the Factual QA intentand devise two methods to form answers: 1) Di-rect search results: the content of the top-1 webpage, and 2) Retrieval-augmented generation: GPT-4 summarized answer based solely on informationfrom the top-5 pages. Note that this approach en-hances the search performances, not the LLMs.The results are reported in . Our analysisshows that direct search results do not outperformGPT-3.5. This limitation can be attributed to thefact that web pages might not provide straightfor-ward answers to user queries, requiring users tonavigate through multiple pages to piece togethera response. The retrieval-augmented generationapproach, which employs GPT-4 to create sum-marized answers based on search results withoutincorporating its inherent knowledge, also under-performs LLMs. These observations underscoreLLMs efficacy in factual QA scenarios, likely dueto their ability to respond directly to user queries.Translation. To simulate the text assistant scenar-ios where users have several options of choosingtranslators or LLM services for their translationneeds, we select the translation cases in our URSdataset for evaluation. Results are shown in Ta-ble 16, where we find LLM services outperformthe translation tool. This may be because usersneed specific styles or modifications of the origi-nal context, while traditional translators are hard toaccomplish.",
  "CriteriaDescription": "FactualityWhether the information provided in the response is accurate, based onreliable facts and data.User SatisfactionWhether the response meets the users question and needs, and provides acomprehensive and appropriate answer to the question.Logical CoherenceWhether the response maintains overall consistency and logical coherencebetween different sections, avoiding self-contradiction.RichnessWhether the response includes rich info, depth, context, diversity, detailedexplanations, and examples to meet user needs and provide a comprehensiveunderstanding.",
  "Whether the advice or information provided in the response is feasible,carries a certain degree of responsibility and considers potential risks andconsequences": "CompletenessWhether the response provides sufficient information and details to meet theusers needs, and whether it avoids omitting important aspects.ClarityWhether the response is clear and understandable, and whether it usesconcise language and structure so that the user can easily understand it.EngagementWhether the answer is interesting and attractive, helps users relax, andprovides high-quality emotional value or entertainment value, etc.AppropriatenessContent is suitable for all users and avoids inappropriate or offensive content.",
  "Intent-awareCriteria": "You are asked to assess the quality of an AI assistants answer to a users question asan impartial judge. Since the type of answer you are evaluating is [Solve ProfessionalProblem], you need to evaluate the answer in the following 5 criteria:1 FactualityWhether the information provided is accurate and based on reliable facts and data2 User SatisfactionWhether the response meets the users question and needs and provides a comprehensiveand appropriate answer to the question3 ClarityWhether the response is clear and understandable, and whether it uses concise languageand structure so that the user can easily understand it.4 Logical CoherenceWhether the response maintains overall consistency and logical coherence betweendifferent sections, avoiding self-contradiction5 CompletenessWhether the response provides sufficient information and details to meet the users needs,and whether it avoids omitting important aspectsNote that a longer answer is not always better, the answer that is concise and meets theabove requirements is the best.",
  "Chain ofthoughtreasoningsteps": "We will provide you with the users question, an 8-score reference answer, and answersfrom the AI assistant that needs your assessment. When starting your evaluation, youneed to follow the reasoning steps below:1. Compare the AI assistants answer with the reference answer, point out any shortcom-ings in the AI assistants answer, and explain further.2. Evaluate the AI assistants answer in terms of the different criteria, giving each criteriona score from 1 to 10 after the evaluation of each.3. Finally, combine the evaluations from each criterion and give the AI assistants answera composite score of 1 to 10.4. Your scoring needs to be as rigorous as possible and adhere to the following scoringrules: in general, the higher the quality of the models answers, the higher the score.The two most important criteria are factual correctness and fulfillment of user needs, andthe scores for these two dimensions dominate the final composite score. ScoringStandardsWhen the model answer has irrelevance to the question, or intrinsically factually incorrect,or generates harmful content, the total score should be 1 to 2;When the model answer has no serious errors and is largely harmless, but is of low qualityand does not meet user requirements, the total score must be 3 to 4;When the model answer basically meets the users needs but performs poorly on somecriteria and is of medium quality, the total score can be 5 to 6;When the quality of the model response is similar to the reference answer and performswell in all criteria, the total score should be 7 to 8;A score of 9 to 10 can only be achieved if the model significantly exceeds the quality ofthe reference answer, adequately addresses the users question and all the needs, and isclose to a perfect score on all criteria. As an example, the reference answer would receivea score of 8.",
  ": English Instruction for evaluating Solve Professional Problem intent questions. Part 1": "OutputFormatYou need to evaluate and explain before you score. Your explanation of each criterionneeds to be followed by the scoring. After that, at the end of your answer, return all ofyour scores in the following dictionary format, including the curly brackets, and makesure that your scores are integers:{Dimension 1: scoring, Dimension 2: scoring, ... , Final Score: Score}, e.g. {Factu-ality: 9, User Satisfaction: 6, ... , Final Score: 7}."
}