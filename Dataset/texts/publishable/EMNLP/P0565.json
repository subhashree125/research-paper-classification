{
  ", ,": "Philology, the study of ancient manuscripts,demands years of professional training in ex-tensive knowledge memorization and manualtextual retrieval. Despite these requirementsalign closely with strengths of recent successfulLarge Language Models (LLMs), the scarcityof high-quality, specialized training data hashindered direct applications. To bridge this gap,we curated the PhiloCorpus-ZH, a rich collec-tion of ancient Chinese texts spanning a millen-nium with 30 diverse topics, including firsthandfolk copies. This corpus facilitated the develop-ment of PhiloGPT, the first LLM tailored fordiscovering ancient Chinese manuscripts. Toeffectively tackle complex philological taskslike restoration, attribution, and linguistic anal-ysis, we introduced the PhiloCoP framework.Modeled on the analytical patterns of philol-ogists, PhiloCoP enhances LLMs handlingof historical linguistic peculiarities such asphonetic loans, polysemy, and syntactic inver-sions. We further integrated these tasks into thePhiloBenchmark, establishing a new standardfor evaluating ancient Chinese LLMs address-ing philology tasks. Deploying PhiloGPT inpractical scenarios has enabled Dunhuang spe-cialists to resolve philology tasks, such as iden-tifying duplication of copied text and assistingarchaeologists with text completion, demon-strating its potential in real-world applications.",
  "Introduction": "The dissemination of ancient human civiliza-tions relied on various carriers, from inscrip-tions to manuscripts and the more recent printedtexts(Assael et al., 2022). Manuscripts, particu-larly popular from the 4-th to the 14-th centurydue to advancements in papermaking and its afford-ability, encompassed a broad array of content in-cluding official documents and folk copies on eco-nomics, literature, science, and agriculture. These",
  "*Equal Contribution. Corresponding authors": "manuscripts are invaluable for understanding an-cient societies, offering insights into both the offi-cial narratives and the daily lives of the commonpeople. However, interpreting these vast and topic-diverse ancient manuscripts wich span over a mil-lennium, presents a highly specialized challenge.It demands philologists to undergo years of profes-sional training and engage in extensive literaturereviews, manual transcription, and contextual anal-ysis(Galambos, 2020).",
  ": Illustration of PhiloGPT capabilities": "Aforementioned challenges render manuscriptdiscovery a time-consuming and labor-intensivefield of study. However, the requirements of philol-ogy align closely with the strengths of LLMs(Yinet al., 2023; Zhao et al., 2023). The recent suc-cess of LLMs in a variety of natural languageprocessing (NLP) tasks across diverse fields hashighlighted their effectiveness(Achiam et al., 2023;Xi et al., 2023; Anil et al., 2023). Nevertheless,the potential of LLMs to interpret ancient lan-guages like Classical Chinese remains largely un-tapped(Sommerschield et al., 2023). This oversightis primarily due to three major constraints: (1) In-sufficient Training Corpus. Despite the abundanceof high-quality English, Chinese, and multilingualLLM training corpora(Minaee et al., 2024), there remains a significant gap in the availability of alarge and diverse corpus for Classical Chinese. (2)Linguistic Complexities of Classical Chinese. Thesubstantial differences in linguistic peculiarities be-tween ancient and modern Chinese, such as wordsmeaning shift and syntactic inversions, has com-plicate text interpretation. These phenomenonsdemand a framework that integrates philologicalinsights to fully exploit the capabilities of LLMs.(3) Fragmentation in Ancient Language Research.Earlier efforts often focused in isolated tasks(Yooet al., 2022), leading to piecemeal approaches thatlack an integration for comprehensive philologicalanalysis. These fragmented efforts have restrictedthe ability to perform a holistic evaluation, limitingthe depth of insights that can be gained.To address these hurdles and fully leverage thecapabilities of LLMs in Classical Chinese stud-ies, we compiled PhiloCorpus-ZH, an extensivecollection of ancient manuscripts. This pioneer-ing archive systematically categorizes thousands ofdocuments across 30 varied topics, including previ-ously unorganized firsthand folk copies, enrichingthe understanding of ancient linguistic practices.Recognizing the profound linguistic disparitiesbetween classical and modern Chinese, includinggrammatical evolution and syntactic variations, wedeveloped the PhiloCoP framework. The essenceof PhiloCoP lies in mimicking philologists inter-nally interpreting ancient manuscripts, i.e., entityidentification, context-implicit relation reasoning,and relation-aware transcription. Entity identifi-cation extracts, differentiate and categorize enti-ties based on their roles within the text. Context-implicit relation reasoning uncovers the underly-ing semantic connections and facilitate a form ofcognitive mapping. Relation-aware transcriptionperforms lexical standardization by adjusting forlinguistic shifts and anomalies inherent in ClassicalChinese. Eventually, PhiloCoP concludes with asynthesis prompt that requires the model to inte-grate its observations from the previous steps intoa cohesive interpretation.Furthermore, we propose PhiloBenchmark toevaluate the efficacy of LLMs from the perspectiveof philological research requirements. It comprises9 targeted tasks closely related to ancient docu-ment analysis, including manuscript restoration,attribution, and philology-oriented QA, along withcontent reasoning. Building on all previous sub-stantial resources, we developed PhiloGPT, thefirst LLM tailored to handle a variety of analyti- cal tasks for Classical Chinese. Through extensiveexperiments, we demonstrate that PhiloGPT, incomparison to general LLMs, excels in the dis-covery and analysis of ancient documents, suchas manuscript text pieces conjugation and histori-cal reasoning. PhiloGPT is applying in Dunhuangmanuscript research, demonstrating potential intasks, such as distinguishing original and copiedtexts, and resolving debate among missing char-acters in manuscripts. The explored real-worldapplications of PhiloGPT showcase its versatilityand value as an indispensable tool for scholars, ac-celerating discovery and fostering interdisciplinarycollaboration in Classical Chinese studies.The highlights of the paper are:1. Diverse Original Ancient Chinese Corpus:We curated the PhiloCorpus-ZH, a collection ofancient Chinese manuscripts. These original docu-ments, complemented by related modern researchworks, were systematically organized into four ma-jors adhering to bibliographic principles.2. Effective Reasoning Paradigm for AncientText: We introduce PhiloCoP, a framework thatadheres to chain-of-thoughts for philologists, pro-gressively navigating vocabulary and grammaticalphenomena in classical Chinese. This approachdemonstrates marked improvements across multi-ple downstream tasks.3. Comprehensive Benchmark for Documen-tary Studies: We have compiled PhiloBenchmark,which includes 9 unique tasks to assess the perfor-mance of ancient Chinese LLMs specialized formanuscript discoveries. This benchmark addressesa wide range of research needs and skill require-ments essential for philologists.4. Real-world Applications with Dunhuangas Case Studies: PhiloGPT has been appliedto specialized tasks tailored for Dunhuang schol-ars, including the restoration and distinction ofmanuscripts.This interdisciplinary case studyshows our PhiloGPTs further potential in assistingwith philology discoveries.",
  "Related Work": "Domain-specific Large Language Models. Re-cent advancements in LLMs, particularly with thedevelopment of open-source frameworks, have cat-alyzed the development of many domain-specificLLM tailored for diverse downstream applica-tions(Zhao et al., 2023). For instance, in the med-ical field, LLMs are now utilized for assistive di- agnostics(Xiong et al., 2023), health-related ques-tion answering (QA)(Wang et al., 2023a), andpsychological counseling(Chen et al., 2023). Inthe legal sector, specialized LLMs facilitate le-gal QA(Huang et al., 2023), generate legal doc-uments(Cui et al., 2023a), and predict legal judg-ments(Yue et al., 2023). Further applications areseen in finance(Wu et al., 2023) and education(Xuet al., 2024), where LLMs process vast domain-specific datasets. Building these domain-specificmodels typically requires continuous pre-trainingon existing general LLMs, supplemented by su-pervised instruction tuning and task-specific align-ment(Wei et al., 2021; Ouyang et al., 2022; Zhanget al., 2020).However, the scarcity of large-scale, original ancient text corpora and suitable taskdatasets has hindered the development of LLMs forancient languages(Sommerschield et al., 2023). Toaddress this gap, our work not only compiles a richcollection of ancient Chinese manuscripts, but alsopioneers the training of the first LLM dedicated toancient Chinese studies.Language Models for Ancient Texts. Severalnoteworthy projects have successfully employedlanguage models pre-trained on ancient languagecorpora for specialized tasks(Kang et al., 2021;Parker et al., 2019; Papavassileiou et al., 2023).For instance, (Assael et al., 2022) utilized ancientGreek inscriptions to support textual restoration, ge-ographical and chronological attribution. Similarly,HUEs work(Yoo et al., 2022) involved trainingBERT-like models on ancient Korean documentsto address challenges such as chronological attri-bution, topic classification, named entity recogni-tion, and summary retrieval. (Son et al., 2022) har-nessed annals for developing model that translateHanja, ancient Korean, into contemporary Koreanand English. As for linguistically limited resources,(Lazar et al., 2021) masked language models havebeen used to predict missing tokens with OraccDataset. However, these earlier efforts, predatingthe recent breakthroughs in LLMs, relied heavilyon the accessibility of ancient language trainingcorpora(Assael et al., 2019; Fetaya et al., 2020;Papavassiliou et al., 2020). This resulted in lim-ited resources, constraining the models to a narrowrange of specific applications unsuitable for LLMpre-training(Sommerschield et al., 2023). In con-trast, we introduce the first ancient Chinese LLMdesigned to tackle a broad spectrum of downstreamtasks, leveraging the extensive and diverse corpusfrom the PhiloCorpus-ZH and PhiloBenchmark.",
  "PhiloGPT": "Our PhiloGPT model was developed based onQwen-1.5-7b(Bai et al., 2023) that possesses com-petitive capabilities in modern Chinese. We con-ducted pre-training on six Nvidia A800 GPUs, uti-lizing our domain-specific PhiloCorpus-ZH, sup-plemented by additional open-source general train-ing corpora to prevent catastrophic forgetting. Weincorporated a small proportion of domain-specificinstructional data to optimize training outcomesin the pre-training stage according to (Zhang andYang, 2023b). Further training details can be foundin the Appendix.For Supervised Fine-tuning,we employed the LoRA tuning method(Hu et al.,2021), focusing on instruction-based supervision torefine the models response quality. Our instructiondata where constructed following the same strate-gies as mentioned in .2, focusing moreon generation instead of answering questions in re-stricted forms (e.g., multiple choices or judgmentsquestions). The ratio of domain-specific to generaldata was set at 1:5 during pre-training and adjustedto 1:1 during instruction tuning phases. All train-ing processes were executed using the open-sourcetraining framework, LLaMA-Factory(Zheng et al.,2024). Comparative experiments were carried outagainst other generalized pre-trained open-sourcemodels of the same parameter scale, providing afair and meaningful basis for comparison.",
  "PhiloCorpus-ZH": "The absence of organized, large-scale ancient cor-pora has impeded the pre-training of LLMs forancient languages. Classical Chinese manuscripts,richer and more diverse than inscriptions, providevarious genres and content. To bridge this gap, wehave curated the extensive PhiloCorpus-ZH, sys-tematically categorized from original documentsvia the Chinese bibliographic Four-Part Classifi-cation method. Distinctively, our corpus includespreviously overlooked folk and semi-folk docu-ments, literature, and socioeconomic records suchas legal cases and contracts. These texts, rich in col-loquial expressions, capture the linguistic nuancesand social dynamics of their times.Our corpus was sourced from a broad rangeof publicly available data, including original mu-seum collections, research papers, academic pub-lications, and specialized literature. Distinguishedfrom some previous datasets containing large-scaleself-gathered and cleaned web data, our data wasderived from the past 40 years (1970s to 2010s) ofphilology publications that have been meticulouslycollected, proofread, and curated by our collabora-tors over years of academic practice. Data cleaningand deduplication were applied to ensure the qual-ity, which included removing extraneous spaces,line breaks, headers, footers, illustrations, tables,formulas, annotation symbols, references and links.More details are available in appendix. Under theclose supervision of these experts, we categorizedthese corpus into four sections:Chinese Classics: Focuses on the study of an-cient Chinese philosophical thoughts, such as in-terpreting the implications of Confucian classics.These works hold substantial academic value andhave historically served as crucial educational andexamination materials in ancient times.",
  ": Folk document example of PhiloCorpus-ZH": "Historical Documents: Comprises records ofhistorical events and materials, such as official an-nals in standardized formats, and private recordswith comments. It also includes other official doc-uments spanning biography, geography, economyand political correspondence.Masters: Contains notable works and master-pieces that do not easily fit into the other categories.The subjects are diverse, encompassing medicine,astrology, mathematics, arts, and religion.Belles-lettres: Features a wide range of ancientliterary works, such as scriptures, poetry, stories,lyrics and operas, which are popular and widelycirculated through manuscripts in ancient times .Each category encapsulates a broad spectrumof content, topics, and genres, ensuring that ourPhiloCorpus-ZH exceeds previous corpora in scale, content richness, and historical breadth. The inclu-sion of our unique folk corpus expands the exist-ing collection of formal Classical Chinese texts inphilology, offering a broader academic scope byaddressing issues of word meanings and revealinggrammatical contexts more effectively.",
  "PhiloBenchmark": "Philological research demands a nuanced under-standing of ancient texts, reflecting the complexityand diversity inherent in these languages. To es-tablish a comprehensive benchmark that enhancesand assesses the performance of ancient ChineseLLMs, we have integrated philology methodolo-gies under expert guidance to propose 9 diversedownstream tasks, constituting the PhiloBench-mark. PhiloBenchmark enables a thorough assess-ment of an LLMs ability to understand, interpret,and generate insights from classical Chinese texts.To be specific, our benchmark addresses key areasof mainstream philology studies:Restoration. Assesses the LLMs capacity to ac-curately regenerate missing or damaged manuscripttexts, crucial for supporting philologists in docu-ment interpretation and evidence alignment. Thisinclude directly predicting missing characters(Restoration) and determining whether two pas-sages have a contextual relationship (Conjugation),which is essential for assisting philologists withmanuscript recovery through context. Attribution. Involves determining the originsand historical context of documents, critical for un-derstanding their provenance. Since documentaryresearch, such as Dunhuang manuscript discovery,is concerned with the period in which the writingwas transcribed, we introduce the Attribution taskfor predicting the most likely time period in whichthis text will appear. We also introduce more so-phisticated Judgment tasks for a comprehensiveassessment of the models ability to attribute interms of authorship, historical events, etc.Linguistic Analysis. Tests traditional linguisticabilities for language models such as named en-tity recognition and segmentation to extract andanalyze contextual information from ancient texts.Question Answering. Measures the LLMs effec-tiveness in responding to complex queries tailoredto philological research. These tasks can help deter-mine whether model has the potential of facilitatingnew academic insights.Given to the severe scarcity of instructional datain ancient Chinese, we implemented hybrid strate-gies for data generation: (1) Manual Construction.For tasks demanding high factual accuracy, such asrestoration and attribution, datasets are built fromannotated texts and original manuscripts. (2) An-notated Instruction Expansion. Leveraging expertphilologist insights, we create finely annotated in-structions to expand and enhance quality throughSelf-Instruct(Wang et al., 2022). (3) Self-QA En-richment. We utilize specialized publications andtextbooks on ancient Chinese to broaden the scopeof instruction data in a Self-QA manner(Bi et al.,2023; Zhang and Yang, 2023a).",
  "PhiloCoT": "Classical Chinese, as a logographic writing sys-tem, a single Chinese character can encapsulatemultiple meanings that are heavily influenced bycontext and syntax. Moreover, the limited varietyof characters in Classical Chinese gives rise to dis-tinctive grammatical phenomena that are absent inmodern Chinese, such as phonetic loans, polysemy,syntactic inversions, and semantic shifts. Further-more, religious and cultural factors contribute tosignificant divergences in naming conventions be-tween ancient and contemporary texts. To effec-tively tackle these complexities, philologists oftenemploy comprehensive interpretation strategies be-fore engaging in downstream tasks.",
  ": Illustration of typical phenomenon differencesin classical Chinese": "Recognizing the profound linguistic disparitiesbetween classical and modern Chinese, we devel-oped the PhiloCoP (Chain-of-Philology) frame-work - a multi-step reasoning approach that emu-lates the analytical processes of philologists wheninterpreting ancient manuscripts. The essence ofPhiloCoP lies in three key stages:Step 1: Entity Identification. PhiloCoP lever-ages the information extraction capabilities ofLLMs to identify and extract named entities atthe character-level. By extracting and categorizingentities, recognizing their significance within thetextual structure, the framework establishes a solidfoundation for understanding the core componentsof the ancient manuscript.Step 2: Context-Implicit Relation Reasoning.Building upon the identified entities, PhiloCoPdelves deeper into the connections between theidentified entities by uncovering the latent semanticrelationships that may not be explicitly stated. Theframework constructs a comprehensive network of relationships, akin to a cognitive map.Step 3: Relation-Aware Transcription. Thisstep involves generating a standardized transcrip-tion of the ancient text that accounts for the uniquelinguistic characteristics of Classical Chinese. Theframework leverages the insights gained from theprevious stages to adapt to the semantic shifts, pol-ysemous characters, and grammatical variationsprevalent in ancient manuscripts.By considering the contextual relationships be-tween entities and the nuances of classical lan-guage, PhiloCoP produces a reasoning process thatfaithfully represents the intended meaning of textwhile mitigating the challenges posed by linguisticanomalies and historical language evolution.",
  "We assess the performance of LLMs on vari-ous philological tasks using the PhiloBenchmark.We specifically compare the performance of our": "PhiloGPT with Qwen-7b-chat(Bai et al., 2023),Baichuan2-7b(Yang et al., 2023) and LLaMA2-Chinese-7b(Cui et al., 2023b; Touvron et al., 2023),ensuring models are of the same parameter level.Our evaluation approach is structured into three dis-tinct categories, each aligned with different typesof philological tasks: Restoration. We employCharacter Error Rate (CER) as evaluation metricsin align with (Assael et al., 2022). As for Chrono-logical Attribution, its almost impossible to traceback the exact year, thus we divide time periodsaccording to Chinese dynasty year number and cal-culation average dynasty shift with ground truth.Discrimination. For tasks that involve discrimina-tion and classification, we apply the F1 score andAccuracy as our criterion. This quantifies LLMsability to follow instructions and understand an-cient texts. Generation. For these generative tasks,we utilize GPT-4o(Achiam et al., 2023) as a scoringagent to perform automatic evaluations. The modelassesses the quality of generated content based onfactual correctness, accuracy, and richness. Fromthese assessments, we calculate a win rate betweencompeting models, providing a quantitative mea-sure of generative excellence. We take the averageresults from 3 runs for each task evaluation.",
  "Experiment Results": "Pre-trainedontheDunhuangcorpora,PhiloGPT exhibits substantial performanceimprovements across various philology tasks.In , we benchmark our model againstother competitive LLMs.In our study, the -in the results for Restoration, Attribution, andConjugation tasks highlights the deficienciesof open-source LLMs in handling specializedphilological tasks. These baseline models, lackof targeted pre-training and fine-tuning specificto philological needs, often generate irrelevantor incorrect responses, or even fail to provideany answer at all (these models may reply \"SorryI dont know\").The inherent complexities ofphilological tasks and semantic differences inancient Chinese, combined with the insufficienttraining of baseline models on relevant corpora,contribute to their inability to produce viableoutcomes in certain cases. For other tasks thatinvolve semi-Classical Chinese, foundation modelsshow some zero-shot capabilities due to the lessspecialized nature of the language used in thesetasks. While improvements in some less challeng-ing tasks are modest, PhiloGPT demonstrates",
  "PhiloGPT0.6301.3760.45174.8%62.1%77.5%PhiloGPT+CoP0.5791.3050.59075.6%65.2%86.7%": ": Results of LLM performance with PhiloBenchmark evaluation. The - denotes that LLM could not solvethe philology specific task, i.e., refuse to answer or randomly generate answers. For Restoration and Attribution, thesmaller score means better performances, denoted as a besides evaluation metric. While for other tasks, greatervalue indicates better, denoted as a . The best result for PhiloGPT+CoP is bold.",
  "notable enhancements in tasks centered aroundanalytical QA, generation and retrieval": "Implementing PhiloCoP has led PhiloGPT toperformance gains across various tasks. Philo-CoP transcribe the original documents to eliminatelexical biases and confusions inherent in Classi-cal Chinese grammar, and extracted key informa-tion through NER to explicitly. This method hasyielded performance enhancements in PhiloGPT.For instance, when tackling analytical tasks, philol-ogists deduce and hypothesize based on keywordusage, names of typical people and places, and theirrelationships. PhiloCoP emulates such practice, re-sulting in improvements in these tasks. However,applying PhiloCoP to LLMs not pre-trained on an-cient texts yields counterproductive effects. Thisis because generalized LLMs lack the capacity tocapture latent semantic relationships through entityidentification and transcription, thereby revertingto unreliable answers based on modern Chineseguesses. This highlights PhiloGPTs genuine un-derstanding of ancient texts and its ability to per-form downstream tasks more effectively. Our PhiloBenchmark, developed from diversetopics and sources, provides a thorough as-sessment of ancient LLMs from a philologicalperspective.While generalized LLMs demon-strate respectable outcomes for basic capabilities,",
  "Dunhuang Manuscript Case Study": "Taking a step further, we conducted an analysisusing Dunhuang manuscripts as a case study toexplore two specialized tasks of philology researchpractice. Our objective is to show that, followingpre-training and instruction tuning, PhiloGPT canprovide researchers with enhanced flexibility toperform comprehensive text information mining.This approach not only facilitates deeper textualanalysis but also leads to potential discoveries.",
  "Analyze Copying Relationships": "We selected Dunhuang manuscripts for our casestudy because they are among the most renowned,numerous, and cover the broadest span of time.Within this vast collection, several manuscriptsdemonstrate instances of duplication and copyingrelationships. Establishing the chronological orderof these texts is a crucial task, as the original ver-sion can reflect the more realistic original appear-ance of the manuscript. As illustrated in (a), we employed PhiloGPT to analyze their in-terrelationships by context reasoning, which is thehomoeoteleuton phenomenon. This knowledge",
  "Generate Suggestion for Text Criticism": "The ancient Chinese novel represents a uniquegenre, showcasing a level of innovation and imagi-nation surpassing its contemporaries. These workswere often viewed as unconventional by the rulersof their times, exemplified by classics such as Jour-ney to the West. Restoration of such texts posessignificant challenges for philologists, particularlydue to the scarcity of reference materials, which arecrucial for identifying parallels(recurring expres-sions and linguistic peculiarities) for text restora-tion. As illustrated in (b), this manuscriptemploys a narrative to satirize the emperor of theera. Unfortunately, the original manuscript is dam-aged at the bottom, leading to disagreements re-garding the extent of the missing text. To address this, we utilized PhiloGPT to gener-ate several plausible completions for the missingsections. The restoration proposal consisting ofthree missing characters was deemed most credible(in line with the overall style and language conven-tions of the article). While these results still requirevalidation from philology, our model has demon-strated potential as a valuable tool for assistingphilologists in making new scholarly discoveries.",
  "Conclusion": "In this paper, we introduce PhiloGPT, the firstdomain-specific LLM designed to tackle a rangeof tasks related to the discovery of ancient Chi-nese manuscripts. We developed the large-scalePhiloCorpus-ZH, which encompasses diverse top-ics of original ancient texts and associated schol-arly research. To align with the unique character-istics of these languages, we proposed the Philo-CoP framework. From a philology perspective,our PhiloBenchmark serves as an evaluation stan-dard for LLMs tailored to ancient Chinese. Exten-sive experiments and case studies with Dunhuangmanuscripts have demonstrated PhiloGPTs poten-tial for ancient document discovery. For futurework, we may extend PhiloGPTs capability basedon more applications and feedback when applied inother manuscripts. We also consider incorporatingRetrieval-Augmented Generation (RAG) with spe-cific documents to mitigate hallucination problems.Limitation: Due to historical reasons, officialdocuments are often better preserved and morecomplex than folk texts, resulting in some data biasand imbalance. We anticipate that the discoveryof additional manuscripts, such as those recentlyunearthed in Xinjiang, will help mitigate this is-sue. Moreover, ancient documentary studies placea high premium on factual accuracy. Thus, findingsfrom our model still require secondary verificationby philologists and must undergo peer review. Ad-ditionally, considering that manuscripts often incor-porate image modality, fine-tuning a multimodal",
  "Acknowledgements": "This work was in part supported by Special Fundsof the National Natural Science Foundation ofChina (No.62441605), National Science andTechnology Major Project (2022ZD0119100), Na-tional Natural Science Foundation of China (No.72342023, 62402429), National Social ScienceFund of China (No. 21BYY142, 14AZS001), KeyResearch and Development Program of ZhejiangProvince (No. 2024C03270), the StarryNight Sci-ence Fund of Zhejiang University Shanghai In-stitute for Advanced Study (SN-ZJU-SIAS-0010).The authors express their deep gratitude to ZheqiLv for his invaluable advice and to Karl Cao for histechnical support. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-son, Dmitry Lepikhin, Alexandre Passos, SiamakShakeri, Emanuel Taropa, Paige Bailey, ZhifengChen, et al. 2023. Palm 2 technical report. arXivpreprint arXiv:2305.10403.",
  "Shervin Minaee, Tomas Mikolov, Narjes Nikzad,Meysam Chenaghlu, Richard Socher, Xavier Am-atriain, and Jianfeng Gao. 2024. Large languagemodels: A survey. arXiv preprint arXiv:2402.06196": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al.2022. Training language models to follow instruc-tions with human feedback. Advances in neural in-formation processing systems, 35:2773027744. Katerina Papavassileiou, Dimitrios I Kosmopoulos, andGareth Owens. 2023. A generative model for themycenaean linear b script and its application in in-filling text from ancient tablets. ACM Journal onComputing and Cultural Heritage, 16(3):125. Katerina Papavassiliou, Gareth Owens, and DimitriosKosmopoulos. 2020. A dataset of mycenaean linearb sequences. In Proceedings of the Twelfth LanguageResources and Evaluation Conference, pages 25522561. Clifford Seth Parker, Stephen Parsons, Jack Bandy,ChristyChapman,FrederikCoppens,andWilliam Brent Seales. 2019.From invisibilityto readability: recovering the ink of herculaneum.PloS one, 14(5):e0215775. Thea Sommerschield, Yannis Assael, John Pavlopou-los, Vanessa Stefanak, Andrew Senior, Chris Dyer,John Bodel, Jonathan Prag, Ion Androutsopoulos,and Nando de Freitas. 2023. Machine learning forancient languages: A survey. Computational Linguis-tics, 49(3):703747.",
  "Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang,Sendong Zhao, Bing Qin, and Ting Liu. 2023a. Hu-atuo: Tuning llama model with chinese medicalknowledge. arXiv preprint arXiv:2304.06975": "Xingyao Wang, Zihan Wang, Jiateng Liu, YangyiChen, Lifan Yuan, Hao Peng, and Heng Ji. 2023b.Mint:Evaluating llms in multi-turn interactionwith tools and language feedback. arXiv preprintarXiv:2309.10691. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-isa Liu, Noah A Smith, Daniel Khashabi, and Han-naneh Hajishirzi. 2022. Self-instruct: Aligning lan-guage models with self-generated instructions. arXivpreprint arXiv:2212.10560. Jason Wei, Maarten Bosma, Vincent Y Zhao, KelvinGuu, Adams Wei Yu, Brian Lester, Nan Du, An-drew M Dai, and Quoc V Le. 2021. Finetuned lan-guage models are zero-shot learners. arXiv preprintarXiv:2109.01652. Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski,Mark Dredze, Sebastian Gehrmann, Prabhanjan Kam-badur, David Rosenberg, and Gideon Mann. 2023.Bloomberggpt: A large language model for finance.arXiv preprint arXiv:2303.17564. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, YiwenDing, Boyang Hong, Ming Zhang, Junzhe Wang,Senjie Jin, Enyu Zhou, et al. 2023. The rise andpotential of large language model based agents: Asurvey. arXiv preprint arXiv:2309.07864. Honglin Xiong, Sheng Wang, Yitao Zhu, Zihao Zhao,Yuxiao Liu, Linlin Huang, Qian Wang, and Ding-gang Shen. 2023. Doctorglm: Fine-tuning your chi-nese doctor is not a herculean task. arXiv preprintarXiv:2304.01097. Yifan Xu, Xiao Liu, Xinghan Liu, Zhenyu Hou, YueyanLi, Xiaohan Zhang, Zihan Wang, Aohan Zeng,Zhengxiao Du, Wenyi Zhao, et al. 2024. Chatglm-math: Improving math problem-solving in large lan-guage models with a self-critique pipeline. arXivpreprint arXiv:2404.02893.",
  "Haneul Yoo, Jiho Jin, Juhee Son, JinYeong Bak,Kyunghyun Cho, and Alice Oh. 2022.Hue:Pretrained model and dataset for understandinghanja documents of ancient korea. arXiv preprintarXiv:2210.05112": "Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li,Chenchen Shen, Shujun Liu, Yuxuan Zhou, Yao Xiao,Song Yun, Wei Lin, et al. 2023. Disc-lawllm: Fine-tuning large language models for intelligent legalservices. arXiv preprint arXiv:2309.11325. Shengyu Zhang, Tan Jiang, Tan Wang, Kun Kuang,Zhou Zhao, Jianke Zhu, Jin Yu, Hongxia Yang, andFei Wu. 2020. Devlbert: Learning deconfoundedvisio-linguistic representations. In Proceedings of the28th ACM International Conference on Multimedia,pages 43734382.",
  "Xuanyu Zhang and Qing Yang. 2023a. Self-qa: Unsu-pervised knowledge guided language model align-ment. arXiv preprint arXiv:2305.11952": "Xuanyu Zhang and Qing Yang. 2023b. Xuanyuan 2.0:A large chinese financial chat model with hundredsof billions parameters. In Proceedings of the 32ndACM International Conference on Information andKnowledge Management, pages 44354439. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,Xiaolei Wang, Yupeng Hou, Yingqian Min, BeichenZhang, Junjie Zhang, Zican Dong, et al. 2023. Asurvey of large language models.arXiv preprintarXiv:2303.18223.",
  "ATraining Details": "In this section, we give an example of the Hyper-parameters of our PhiloGPT training details.For generalized pre-training dataset, we sampledfrom Wikipedia (en), Wikipedia (zh), SkyPile (zh),Wanjuan 1.0. For hybriding generalized SupervisedFine-Tuning dataset, we sampled from StanfordAlpaca (zh), BELLE 2M (zh), BELLE School Math0.25M, Alpaca CoT (multilingual), Firefly 1.1M(zh), Web QA (zh), ShareGPT4 (en&zh), Ruozhiba(zh).",
  "BExperiment Costs": "We conducted experiments with GPT4 API, utiliz-ing the gpt-4-turbo-2024-04-09 model, at a cost of$5 per 1 million tokens input and $15 per 1 millionoutput. We spent $240 in total. We conducted datacleaning with the guidance of philology experts inour co-authors, and with a group of student volun-teers from the college of Computer Science and thecollege of liberal arts."
}