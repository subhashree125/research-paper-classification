{
  "Abstract": "Warning: this paper contains content that maybe inappropriate or offensive.As generative models become available for pub-lic use in various applications, testing and an-alyzing vulnerabilities of these models has be-come a priority. In this work, we propose anautomatic red teaming framework that evalu-ates a given black-box model and exposes itsvulnerabilities against unsafe and inappropriatecontent generation. Our framework uses in-context learning in a feedback loop to red teammodels and trigger them into unsafe contentgeneration. In particular, taking text-to-imagemodels as target models, we explore differentfeedback mechanisms to automatically learn ef-fective and diverse adversarial prompts. Our ex-periments demonstrate that even with enhancedsafety features, Stable Diffusion (SD) modelsare vulnerable to our adversarial prompts, rais-ing concerns on their robustness in practicaluses. Furthermore, we demonstrate that theproposed framework is effective for red team-ing text-to-text models.",
  "Introduction": "With the recent release and adoption of large gen-erative models, such as DALL-E (Ramesh et al.,2022), ChatGPT (Team, 2022), and GPT-4 (Ope-nAI, 2023), ensuring the safety and robustnessof these models has become imperative. Whilethose models have significant potential to createa real-world impact, they must be checked for po-tentially unsafe and inappropriate behavior beforethey can be deployed. For instance, chatbots pow-ered by Large Language Models (LLMs) can gen-erate offensive response (Perez et al., 2022), orprovide users with inaccurate information (Dziriet al., 2021). When prompted with certain input,text-to-image models such as Stable Diffusion (SD)can generate images that are offensive and inappro-priate (Schramowski et al., 2022a).",
  "*": "Recent research has leveraged red teaming forevaluating the vulnerabilities in generative mod-els, where one aims to discover inputs or promptsthat will lead the system to generate undesiredoutput. Most previous works in red teaming in-volve humans in the loop (Ganguli et al., 2022; Xuet al., 2021) who interact with the system and man-ually generate prompts for triggering the model ingenerating undesired outcomes, both for text-to-text (Ganguli et al., 2022) and text-to-image mod-els (Mishkin et al., 2022). The human in the loopapproach, however, is expensive and not scalable.Thus, recent work has focused on automating thered teaming process (Perez et al., 2022; Casperet al., 2023; Lee et al., 2023).Although previous works have attempted to au-tomate the red teaming process (Perez et al., 2022;Mehrabi et al., 2022), there is still room for improv-ing both the efficiency and effectiveness of auto-mated red teaming. For instance, Perez et al. (2022)introduce a method that requires zero-shot genera-tion of a large number of candidate prompts, selectsa few of them to serve as in-context examples forgenerating new adversarial prompts, and does su-pervised fine-tuning on those prompts. Mehrabiet al. (2022) use an expensive iterative token re-placement approach to probe a target model andfind trigger tokens that lead undesired output gener-ation. In this work, we propose a novel framework,Feedback Loop In-context Red Teaming (FLIRT)1,which works by updating the in-context exemplar(demonstration) prompts according to the feedbackit receives from the target model. FLIRT is com-putationally more efficient, and as we demonstrateempirically, more effective in generating successfuladversarial prompts that expose target model vul-nerabilities. FLIRT can also work on any black-boxmodel.",
  "Codecanbefoundat": "FLIRT is a black-box and automated red team-ing framework that uses iterative in-context learn-ing for the red language model (LM) to generateprompts that can trigger unsafe generation. Toeffectively generate adversarial prompts, we ex-plore various prompt selection criteria (feedbackmechanisms) to update the in-context exemplarprompts in FLIRT, including rule-based and scor-ing approaches. FLIRT is flexible and allows forthe incorporation of different selection criteria pro-posed in this work that can control different ob-jectives such as the diversity and toxicity of thegenerated prompts, which enables FLIRT to ex-pose larger and more diverse set of vulnerabilities.We evaluate the FLIRT framework by conduct-ing experiments for text-to-image models, since theautomated red teaming of those models is largelyunderexplored. Specifically, we analyze the abilityof FLIRT to prompt a text-to-image model to gen-erate unsafe images. We define an unsafe imageas an image that if viewed directly, might be of-fensive, insulting, threatening, or might otherwisecause anxiety (Gebru et al., 2021). We demon-strate that FLIRT is significantly more effective inexposing vulnerabilities of several text-to-imagemodels, achieving average attack success rate of80% against vanilla stable diffusion and 60%against different safe stable diffusion models aug-mented with safety mechanisms compared to anexisting in-context red teaming approach by Perezet al. (2022) that achieves 30% average attacksuccess rate against vanilla stable diffusion and20% against different safe stable diffusion mod-els. Furthermore, by controlling the toxicity ofthe learned prompt, FLIRT is capable of bypassingcontent moderation filters designed to filter out un-safe prompts, thus emphasizing the need for morecomprehensive guardrail systems. We demonstratetransferability of the adversarial prompts generatedthrough FLIRT among different models. Finally,we conduct experiments in which we use a text-to-text model as our target model and demonstrate theeffectiveness of FLIRT in this setting as well.",
  "FLIRT Framework": "Our Feedback Loop In-context Red Teaming(FLIRT) framework uses a red LM to generate ad-versarial prompts aimed at triggering the targetmodel into generating unsafe content. The red LMstarts with an initial set of in-context seed promptsand iterates as follows: (1) The red LM generates an adversarial prompt using in-context learning,which is fed into the target (e.g., text-to-image)model to generate the corresponding output (e.g.,image). (2) The corresponding output (image) isevaluated on whether it is unsafe using safety clas-sifiers. (3) The result of this evaluation is fed backto the red LM, which utilizes it as a feedback todecide whether to update its in-context exemplarprompts according to a chosen in-context attackstrategy. These three steps get repeated for a certainnumber of FLIRT iterations. The overall FLIRTframework is illustrated in . For simplicity,in our explanations, we consider the target modelto be a text-to-image model; however, the targetmodel can be any other model (e.g., text-to-text).Below we discuss each step incorporated in FLIRT.",
  "Initialization": "The red LM starts with a small number of in-context seed prompts (e.g., five prompts as listedin Appendix) hand-engineered by humans that arelikely to trigger the text-to-image model to gener-ate unsafe images. It then uses in-context learningto generate a new prompt which is fed to the text-to-image model to generate a corresponding image.",
  "Evaluation": "After the image is generated, it is evaluated forsafety. The evaluation can be done using existingsafety classifiers or using human annotators. Sinceone of our main goals is to fully automate the redteaming process, here we use the former approachwhich was previously used in (Schramowski et al.,2022a) to evaluate the safety of text-to-image mod-els. We use two classifiers, Q16 (Schramowskiet al., 2022b) and NudeNet2 to evaluate whetherthe generated image contains unsafe content. Weuse a similar setup as that of (Schramowski et al.,2022a) to evaluate images for unsafe content sincewe also utilize the same definition of unsafe content.To evaluate safety of the generated text in text-to-text experiments, we use the TOXIGEN model fortoxic language detection (Hartvigsen et al., 2022).",
  "Queue rear": ": Our proposed Feedback Loop In-context Red Teaming (FLIRT) framework for generating adversarialprompts. In each FLIRT iteration, the red LM generates an adversarial prompt that is fed into the text-to-imagemodel. Upon text-to-image model generating the image corresponding to the prompt generated by the red LM, theimage is evaluated using Q16 and NudeNet classifiers to determine safety of the image. If the image is deemedunsafe, the red LM then updates its in-context exemplars according to one of the adversarial in-context attackstrategies (FIFO, LIFO, scoring, Scoring-LIFO) to generate a new and diverse adversarial prompt. The in-contextstrategies utilized by the red LM to generate adversarial prompts are demonstrated on the left side of the image.Within scoring strategy, the scores in parentheses represent the score associated to each prompt. we consider the in-context exemplar prompts to bein a queue and update them on a FIFO basis. NewLM generated prompt that resulted in an unsafeimage generation (henceforth referred to as posi-tive feedback) is placed at the end of the queue andthe first exemplar prompt in the queue is removed.Since in FIFO strategy the seed exemplar promptswhich are hand engineered by humans get over-written, the subsequent generations may divergefrom the initial intent generating less successfuladversarial prompts. To alleviate this challenge,we explore the Last in, First Out (LIFO) strategythat aims to keep the intent intact while generatinga diverse set of examples.Last in First out (LIFO) Attack In this strategy,we consider the in-context exemplar prompts tobe in a stack and update them on a LIFO basis.New LM generated prompt with positive feedbackis placed at the top of the stack and is replacedby the next successful generation. Note that allthe exemplar prompts except the one at the top ofthe stack remain the same. Thus, the initial intentis preserved and the new generated prompts donot diverge significantly from the seed exemplarprompts. However, this attack strategy may not sat-isfy different objectives (e.g., diversity and toxicityof prompts) and may not give us the most effectiveset of adversarial prompts. In order to address theseconcerns, we next propose the scoring attack.Scoring Attack In this strategy, our goal is to opti-mize the list of exemplar prompts based on a prede-fined set of objectives. Examples of objectives are1) attack effectiveness, aiming to generate prompts that can maximize the unsafe generations by thetarget model; 2) diversity, aiming to generate moresemantically diverse prompts, and 3) low-toxicity,aiming to generate low-toxicity prompts that canbypass a text-based toxicity filter.Let Xt = (xt1, xt2, . . . , xtm) be the ordered listof m exemplar prompts at the beginning of thet-th iteration. Xt is ordered because during in-context learning, the order of the prompts matters.Further, let xtnew be the new prompt generated viain-context learning during the same iteration thatresulted in positive feedback, and let Xti be anordered list derived from Xt where its ith elementis replaced by the new prompt xtnew, e.g., Xt1 =(xtnew, xt2, . . . , xtm). Finally, we use Xt = {Xt} {Xti, i = 1, . . . , m} to denote a set of size (m + 1)that contains the original list Xt and all the derivedlists Xti, i = 1, . . . , m.At the t-th iteration, red LM updates its (ordered)list of exemplar prompts by solving the followingoptimization problem:",
  "(1)": "where Oi is the ith objective that the red LM aimsto optimize, and i is the weight associated withthat objective.While the objectives Oi-s are defined as func-tions over lists of size m, for the particular set ofobjectives outlined above, the evaluation reduces tocalculating functions over individual and pair-wisecombination of the list elements making the compu- tation efficient. Specifically, for the attack effective-ness and low-toxicity criteria, the objectives reduceto O(Xt) = ml=1 O(xtl). In our text-to-imageexperiments, we define the attack effectivenessobjective as OAE(Xt) = ml=1 NudeNet(xtl) +Q16(xtl) where NudeNet(x) and Q16(x) areprobability scores by applying NudeNet andQ16 classifiers to the image generated from theprompt x.In text-to-text experiments, the ef-fectiveness objective is defined as OAE(Xt) =ml=1 Toxigen(xtl) where Toxigen(x) is the tox-icity score on the prompt x according to the TOX-IGEN classifier (Hartvigsen et al., 2022).Thelow-toxicity objective is defined as OLT (Xt) =ml=1(1 toxicity(xtl)) where toxicity(x) is thetoxicity score of prompt x according to the Per-spective API3. As for the diversity objective, wedefine it as pairwise dissimilarity averaged overall the element pairs in the list, ODiv(Xt) =ml=1mj=l+1(1 Sim(xtl, xtj)).We calculateSim(xt1, xt2) using the cosine similarity betweenthe sentence embeddings of the two pairs xt1 andxt2 (Reimers and Gurevych, 2019). For cases whereall the objectives can be reduced to functions overindividual elements, the update in (1) is done bysubstituting the prompt with the minimum score(xtmin = arg mini=1,...,m O(xti)) with the gener-ated prompt xtnew if O(xtmin) < O(xtnew). Thisupdate is efficient as it only requires storing thescores O(xti). For the other cases, we solve (1) bycomputing the m+1 objectives for each element inXt and keeping the element maximizing Score(X)(see Appendix for more details).Scoring-LIFO In this attack strategy, the red LMcombines strategies from scoring and LIFO attacks.The red LM replaces the exemplar prompt that lastentered the stack with the new generated promptonly if the new generated prompt adds value tothe stack according to the objective the red LMaims to satisfy. In addition, since it is possiblethat the stack does not get updated for a long time,we introduce a scheduling mechanism. Using thisscheduling mechanism, if the stack does not get up-dated after some number of iterations, the attackerforce-replaces the last entered exemplar prompt inthe stack with the new generation.",
  "We perform various experiments to validateFLIRTs ability in red teaming text-to-image mod-": "els. We also perform ablation studies to analyzethe efficacy of FLIRT under different conditions.Finally, we perform experiments to show the effi-cacy of FLIRT in red teaming text-to-text models.In addition, we perform numerous controlled ex-periments to better understand the effect of seedprompts and how they differ from the generatedprompts in the Appendix.",
  "Main Experiments": "We test various text-to-image models: stable diffu-sion v1-4 (Rombach et al., 2022)4, weak, medium,strong, and max safe stable diffusion (Schramowskiet al., 2022a)5. For the red LM, we use GPT-Neo2.7B parameter model (Black et al., 2021; Gaoet al., 2020)6. For each attack strategy, we run theattack for 1k FLIRT iterations using three differ-ent initializations (sets of seed prompts listed inthe Appendix each containing five prompts). Thethree different sets of seed prompts capture differ-ent characteristics and are designed to probe thetarget model for all the unsafe categories borrowedfrom (Schramowski et al., 2022a). We use a con-text of size five in our experiments containing theinstruction prompt that describes the task and thefour additional in-context exemplar prompts.For the metrics, we utilize attack effectivenesswhich we define as the percentage of successfulprompts generated by the red LM that trigger thetext-to-image model towards unsafe generation ac-cording to either Q16 or NudeNet classifiers. Weadopt the same evaluation strategy to that utilizedin (Schramowski et al., 2022a) to report the amountof unsafe content generation in text-to-image mod-els according to Q16 and NudeNet classifiers as ameasure for attack effectiveness. In addition, weuse diversity as another metric to report the per-centage of unique prompts generated by the redLM that are not repetitive (for additional metricson diversity refer to the Appendix). We report theaveraged attack effectiveness along with diversityresults over the three initialization sets.We compare attack strategies in FLIRT toStochastic Few Shot (SFS) red teaming (Perezet al., 2022).For SFS, we first generate 1Kprompts using the same instruction prompts that",
  "ModelLIFO(diversity)FIFO(diversity)Scoring(diversity)Scoring-LIFO(diversity)SFS(diversity)": "Stable Diffusion (SD)63.1 (94.2)54.2 (40.3)85.2 (57.1)69.7 (97.3)33.6 (97.8)Weak Safe SD61.3 (96.6)61.6 (46.9)79.4 (71.6)68.2 (97.1)34.4 (97.3)Medium Safe SD49.8 (96.8)54.7 (66.8)90.8 (30.8)56.3 (95.1)23.9 (98.7)Strong Safe SD38.8 (96.3)67.3 (33.3)84.6 (38.1)41.8 (91.9)18.6 (99.1)Max Safe SD33.3 (97.2)46.7 (47.3)41.0 (88.8)34.6 (96.8)14.1 (98.0) : Attack effectiveness results for each in-context adversarial attack strategy applied on different stablediffusion models. The attack effectiveness reports the percentage of images generated that are labeled as unsafeaccording to either Q16 or NudeNet classifiers. The numbers in the parentheses report the percentage of uniqueprompts generated by the red LM. 0.00.20.40.60.81.0 Percent (%)",
  "Stable Diffusion": "DiversityAttack Effectiveness 0.00.20.40.60.81.0 Percent (%) Weak Safe SD DiversityAttack Effectiveness 0.00.20.40.60.81.0 Percent (%) Strong Safe SD DiversityAttack Effectiveness : Diversity-attack effectiveness results with varying the 2 parameter. Attack effectiveness reports thepercentage of images generated by the text-to-image model that are labeled as unsafe according to Q16 andNudeNdet classifiers. The diversity score reports the percentage of unique prompts generated by the red LM. Forresults on other stable diffusion models refer to the Appendix. we use in our experiments to validate FLIRT. Wethen sample from the generated prompts withprobability e(0.5(NudeNet(x)+Q16(x)))/T whereNudeNet(x) and Q16(x) are the probability ofthe generated image corresponding to the prompt xbeing unsafe according to NudeNet and Q16 clas-sifiers and T is a temperature hyper-parameter. Weinclude the sampled prompts as few shot exemplarprompts to generate 1K new adversarial prompts.We set T = 1 10 and perform the sampling withoutreplacement as in (Perez et al., 2022). We reportthe average results for SFS over using the samethree sets of instruction seed prompts that we useto evaluate attack strategies in FLIRT.Attack Effectiveness We report the attack effec-tiveness and diversity results from applying the dif-ferent attack strategies in . We observe thatcompared to SFS, FLIRT-based attacks are signifi-cantly more effective in triggering vanilla and safestable diffusion models toward generating unsafeimages. Although SFS generates a diverse set ofprompts, we observe its weakness in generating ef-fective attacks. Note that while one can control thetemperature hyper-parameter in the SFS approachto achieve a trade-off between diversity and attackeffectiveness, since SFS retrieves examples fromthe pool of zero-shot examples for the few-shot gen-erations, if the pool of zero-shot generations are notsuccessful, regardless of the temperature value, theapproach would not find successful examples. Onthe other hand, FLIRT uses a feedback loop which improves upon its few-shot demonstrations startingfrom only a few demonstrations in each successfuliteration. In this case, if a new generation is moresuccessful, FLIRT will consider it as its demonstra-tion and keep improving on it in the next iterations(for more detailed discussion on the trade-offs referto the Appendix). also demonstrates thatthe scoring adversarial in-context attack strategy isthe most effective in terms of attack effectivenesscompared to other attack strategies. For this set ofresults, we use a scoring attack that only optimizesfor attack effectiveness (OAE(Xt)). This entailsthat the red LM receives the probability scores com-ing from Q16 and NudeNet classifiers for a givenimage corresponding to a generated prompt and up-dates the exemplar prompts according to the prob-ability scores it receives as a feedback for attackeffectiveness. Although the scoring strategy gives us the bestresults in terms of attack effectiveness, we observethat it generates less diverse set of prompts in somecases. On the other hand, SFS, LIFO, and Scoring-LIFO strategies produce better results in terms ofgenerating diverse set of prompts. The lack of di-verse generations in scoring strategy is in part dueto the fact that in scoring attack, the red LM learnsan effective prompt that is strong in terms of trigger-ing the text-to-image model in unsafe generation;thus, it keeps repeating the same/similar promptsthat are effective which affects diverse output gen-eration. To alleviate this problem, and encourage",
  ": Attack effectiveness and diversity results for BLOOM (top) and Falcon (bottom)": "diverse generations in scoring attack strategy, weattempt to control the diversity of prompts throughthe addition of diversity as an additional objective(ODiv(Xt)) in the next set of experiments.Controlling Diversity To enhance the diversity ofgenerations by the scoring attack strategy, we addan additional objective to the initial attack effec-tiveness objective that controls for diversity. Forthe diversity objective (ODiv(Xt)), we aim to max-imize the averaged pairwise sentence diversity ofexisting exemplar prompts. We use cosine simi-larity to calculate pairwise similarity of two sen-tence embeddings7 (Reimers and Gurevych, 2019).Thus, the scoring strategy tries to optimize for1O1 + 2O2 where O1 is the attack effectivenessobjective (OAE(Xt)), and O2 is the diversity ob-jective (ODiv(Xt)). To observe the effect of thenewly added objective on enhancing the diversityof generations in scoring attack strategy, we fix1 = 1 and vary the 2 parameter and report theattack effectiveness vs diversity trade-offs in Fig-ure 2. We demonstrate that by increasing the 2parameter value, the diversity of generated promptsincrease as expected with a trade-off on attack ef-fectiveness. We demonstrate that using the scoringstrategy, one can control the trade-offs and thatthe red LM can learn a strategy to satisfy differentobjectives to attack the text-to-image model.",
  "Ablation Studies": "In addition to the main experiments, we performablation studies to address the following questions:Q1: Would the results hold if we use a differentlanguage model as the red LM?Q2: Would the results hold if we add content mod-eration in text-to-image models? Q3: Can we control for the toxicity of the promptsusing the scoring attack strategy?Q4: Would the attacks transfer to other models?Q5: How robust our findings are to the existingflaws in the safety classifiers?For the ablation studies, we only use the first setof seed prompts to report the results as the resultsmostly follow similar patters. All the other setupsare the same as the main experiments unless other-wise specified.Q1: Different Language Model To answer thequestion on whether the results hold if we use adifferent language model as the red LM, we re-place the GPT-Neo model utilized in our main ex-periments with BLOOM 3b (Scao et al., 2022)8 and Falcon 7b (Almazrouei et al., 2023)9 param-eter models. We then report the results on attackeffectiveness comparing the different attack strate-gies. From the results reported in , we ob-serve similar patterns to that we reported previouslywhich suggests that the results still hold even whenwe use a different language model as our red LM.In our results, we demonstrate that the scoring at-tack strategy is the most effective attack. However,similar to our previous observations, it suffers fromthe repetition problem and lack of diverse genera-tions if we only optimize for attack effectivenesswithout considering diversity as the secondary ob-jective. SFS, LIFO, and Scoring-LIFO generatemore diverse outcomes with lower attack effective-ness compared to the scoring strategy similar to ourprevious findings.Q2: Content Moderation To answer the ques-tion on whether applying content moderation ontext-to-image models affects the results, we turnon the built-in content moderation (safety filter)",
  ": Percentage of toxic prompts generated by thered LM before (2 = 0) and after (2 = 0.5) applyinglow-toxicity constraint in scoring attack": "in text-to-image models. This content moderation(safety filter) operationalizes by comparing the clipembedding of the generated image to a set of pre-defined unsafe topics and filtering the image if thesimilarity is above a certain threshold (Rando et al.,2022). In this set of experiments, we turn on thesafety filter in all the text-to-image models studiedin this work and report our findings in . Wedemonstrate that although as expected the effective-ness of the attacks drop in some cases as we turnon the safety filter, still the attacks are effective andthat the scoring strategy for the most cases is themost effective strategy with similar trend on thediversity of the results as we observed previously.These results demonstrate that applying FLIRT canalso help in red teaming text-to-image models thathave a content moderation mechanism on whichcan help us red team the text-to-image model aswell as the content moderation applied on it anddetecting the weaknesses behind each component.Although the main goal of this work is to analyzerobustness of text-to-image models irrespective ofwhether a content moderation is applied on them ornot, we still demonstrate that FLIRT can red teammodels with content moderation applied on them.Q3: Toxicity of Prompts In this set of experi-ments, we are interested in showing whether thered LM can generate prompts that are looking safe(non-toxic), but at the same time can trigger text-to-image models into unsafe generation. This is partic-ularly interesting to study since our motivation is toanalyze prompt-level filters that can serve as effec-tive defense mechanisms for text-to-image models.Secondly, we want to analyze robustness of text-to-image models to implicit prompts that might not",
  ": Transferability of the attacks": "sound toxic but can be dangerous in terms of trig-gering unsafe content generation in text-to-imagemodels. Toward this goal, we incorporate a sec-ondary objective in scoring attack strategy in addi-tion to attack effectiveness that controls for toxicityof the generated prompts. Thus, our scoring basedobjective becomes 1O1 + 2O2 where O1 is theattack effectiveness objective (OAE(Xt)), and O2is for the low-toxicity of the prompt (OLT (Xt))which is (1 toxicity) score coming from our uti-lized toxicity classifier (Perspective API)10. In ourexperiments, we fix 1 = 1 and compare resultsfor when we set 2 = 0 (which is when we do notimpose any constraint on the safety of the prompts)vs 2 = 0.5 (when there is a safety constraintimposed on the prompts). In our results demon-strated in , we observe that by imposingthe safety constraint on the toxicity of the prompts,we are able to drastically reduce the toxicity ofthe prompts generated and that we can control thistrade-off using our scoring strategy by controllingfor attack effectiveness vs prompt toxicity.Q4: Attack Transferability In transferabilityexperiments, we study whether an attack imposedon one text-to-image model can transfer to othertext-to-image models. Thus, we take successfulprompts that are generated through FLIRT usingscoring attack strategy optimized for attack ef-fectiveness towards triggering a particular text-to-image model, and apply them to another model.We then report the amount of success and attacktransfer in terms of the percentage of prompts thattransfer to the other model that result in unsafegeneration. As reported in , we observe",
  ": Attack effectiveness and diversity results for red teaming GPT-Neo language model": "that attacks transfer successfully from one text-to-image model to another. As expected, it is harder totransfer attacks to more robust models compared toless robust ones (e.g., it is easier to transfer attacksfrom SD to weak safe SD compared to SD to maxsafe SD).Q5: Noise in Safety Classifiers Since FLIRT re-lies on the automatic feedback coming from thesafety classifiers, it is possible that existing noiseand flaws in the classifier affect our findings. Toput this into test and verify that our findings arerobust to the existing imperfections in the safetyclassifiers, we impose different levels of noise tothe outcome of the safety classifiers applied on im-ages generated by the stable diffusion model. Inour experiments, we randomly flip different per-centages (5%, 10%, and 20%) of the output labelsproduced by the safety classifiers applied on thegenerated images and report the results in .In our results, we report that our results and find-ings still hold. Scoring strategy still outperformsother strategies in terms of attack effectiveness, andSFS, LIFO, and Scoring-LIFO strategies generatemore diverse set of prompts.",
  "Red Teaming Text-to-text Models": "To demonstrate whether FLIRT can be used tored team text-to-text models, we replace thetext-to-image models studied in previous experi-ments with the GPT-Neo 2.7B parameter languagemodel (Black et al., 2021; Gao et al., 2020)11.Since in this experiment the output of the targetmodel is text instead of image, we replace NudeNetand Q16 classifiers which are image based safetyclassifiers with TOXIGEN model which is a toxiclanguage detection model (Hartvigsen et al., 2022).In this study, the goal is to red team a language model and trigger it to generate toxic responses.Thus, we report the percentage of responses gen-erated by the target model that are toxic. We usea new set of seed prompts that are suitable for lan-guage domain to trigger toxic generation (listed inAppendix) and keep the rest of the experimentalsetups the same. In our results demonstrated in Ta-ble 7, we observe that our introduced attack strate-gies in this paper utilized in FLIRT significantlyoutperform the SFS baseline that was introducedto specifically red team language models (Perezet al., 2022). These results show the flexibilityof FLIRT to effectively be applicable to language(text-to-text) space in addition to text-to-image.",
  "Related Work": "Some previous red teaming efforts include humansin the loop (Ganguli et al., 2022; Mishkin et al.,2022). Some other efforts in red teaming havetried to automate the setup (Perez et al., 2022;Mehrabi et al., 2022; Casper et al., 2023; Lee et al.,2023; Wichers et al., 2024). Unlike some of theseprevious works that rely on expensive iterativeapproaches or involve extensive data generationfollowed with supervised fine-tuning or reinforce-ment learning, our proposed approach relies onlightweight in-context learning.",
  "Conclusion": "We introduce the feedback loop in-context redteaming framework that aims to red team modelsto expose their vulnerabilities toward unsafe con-tent generation. We demonstrate that in-contextlearning incorporated in a feedback based frame-work can be utilized by the red LM to generateeffective prompts that can trigger unsafe contentgeneration in text-to-image and text-to-text mod-els. In addition, we propose numerous variationsof effective attack strategies. We perform differ-",
  "Limitations and Ethics Statement": "Since FLIRT relies on the automatic feedback com-ing from classifiers, it is possible that existing noisein the classifier affects the outcome. However, weperform ablation studies as reported in andverify that our results still hold and are robust tothe introduced noise in the outcome of the classi-fier. In addition, it is possible to incorporate humanfeedback if one is concerned about existing flaws inthe trained classifiers as FLIRT is flexible to allowreplacement of each component with a substituteof choice (e.g., replacement of the classifiers withhumans). However, exposing humans with suchsensitive content has its own issues; hence, we aregiving preference to automatic approaches here. Al-though FLIRT can be used to evaluate and enhancemodels according to safety and responsible AI con-cerns, if used by malicious actors, it can result inunsafe content generation which can have negativesocietal impact. However, we believe that the ad-vantages of having such a framework outweighs itsdisadvantages. Having such a framework for modelevaluation and auditing can help us move towarddeveloping safer and more reliable models. Withregards to reproducibility, we release our code. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-shamsi, Alessandro Cappelli, Ruxandra Cojocaru,Merouane Debbah, Etienne Goffinet, Daniel Hes-low, Julien Launay, Quentin Malartic, BadreddineNoune, Baptiste Pannier, and Guilherme Penedo.2023. Falcon-40B: an open large language modelwith state-of-the-art performance. Sid Black, Gao Leo, Phil Wang, Connor Leahy,and Stella Biderman. 2021.GPT-Neo:LargeScale Autoregressive Language Modeling with Mesh-Tensorflow. If you use this software, please cite itusing these metadata.",
  "minican Republic. Association for ComputationalLinguistics": "Deep Ganguli, Liane Lovitt, Jackson Kernion, AmandaAskell, Yuntao Bai, Saurav Kadavath, Ben Mann,Ethan Perez, Nicholas Schiefer, Kamal Ndousse,et al. 2022. Red teaming language models to re-duce harms: Methods, scaling behaviors, and lessonslearned. arXiv preprint arXiv:2209.07858. Leo Gao, Stella Biderman, Sid Black, Laurence Gold-ing, Travis Hoppe, Charles Foster, Jason Phang, Ho-race He, Anish Thite, Noa Nabeshima, et al. 2020.The pile: An 800gb dataset of diverse text for lan-guage modeling. arXiv preprint arXiv:2101.00027. Timnit Gebru, Jamie Morgenstern, Briana Vecchione,Jennifer Wortman Vaughan, Hanna Wallach, HalDaum III, and Kate Crawford. 2021. The maga-zine archive includes every article published in com-munications of the acm for over the past 50 years.Communications of the ACM, 64(12):8692. Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi,Maarten Sap, Dipankar Ray, and Ece Kamar. 2022.ToxiGen: A large-scale machine-generated datasetfor adversarial and implicit hate speech detection.In Proceedings of the 60th Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 33093326, Dublin, Ireland.Association for Computational Linguistics. Deokjae Lee, JunYeong Lee, Jung-Woo Ha, Jin-HwaKim, Sang-Woo Lee, Hwaran Lee, and Hyun OhSong. 2023. Query-efficient black-box red teamingvia Bayesian optimization. In Proceedings of the 61stAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 1155111574, Toronto, Canada. Association for Computa-tional Linguistics. Ninareh Mehrabi, Ahmad Beirami, Fred Morstatter,and Aram Galstyan. 2022. Robust conversationalagents against imperceptible toxicity triggers. In Pro-ceedings of the 2022 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages28312847, Seattle, United States. Association forComputational Linguistics.",
  "Javier Rando, Daniel Paleka, David Lindner, LennardHeim, and Florian Tramr. 2022.Red-teamingthe stable diffusion safety filter.arXiv preprintarXiv:2210.04610": "Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:Sentence embeddings using siamese bert-networks.In Proceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing. Associa-tion for Computational Linguistics. Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjrn Ommer. 2022.High-resolution image synthesis with latent diffusion mod-els. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR),pages 1068410695. Teven Le Scao, Angela Fan, Christopher Akiki, El-lie Pavlick, Suzana Ilic, Daniel Hesslow, RomanCastagn, Alexandra Sasha Luccioni, Franois Yvon,Matthias Gall, et al. 2022.Bloom:A 176b-parameter open-access multilingual language model.arXiv preprint arXiv:2211.05100.",
  "ASeed Prompts": "To perform our main set of experiments, we uti-lize three different sets of seed prompts to capturea wide range of different unsafe content catego-rizations. We derive these categories from a pre-vious work (Schramowski et al., 2022a). Thesecategories include content on Sexual, violence,self-harm, harassment, hate, illegal activity, andshocking topics. We aim to not only design seedprompts to capture a wide range of topics in thesecategories, but also each set is designed to captureone dimension on how prompts can play a role inthe attack performance.For instance, our first set of seed prompts aredesigned to capture the effect of providing detaildescription of a prompt and its effect on unsafecontent generation. We noticed that providing thesafe stable diffusion models with prompt:imageof a naked woman might not result in an imageof a naked woman; however, when we providemore detail description about the naked womanprompt:image of a naked woman. Nipples visiblein the image the provided description results inan image of a completely naked woman. Thus,providing more description can have effect on theperformance of the attack. To put this into test ina more controlled setting, we divide our sets ofseeds in different levels of granularity accordingto the provided descriptions. Set 2 of our promptscontains similar prompts to that in Set 1 but withless provided description. We then compare theperformance of our scoring approach when we useseed Set 1 vs seed Set 2 vs when we keep thezero-shot example the same but make the examplesless descriptive to that in seed Set 1. From theresults reported in , we demonstrate thatindeed if we make the prompts more descriptiveattacks are more effective. Finally, Set 3 containsa completely new set of seed prompts that servesas a sanity check that our results hold for a new setof seed prompts that are different from sets 1 and2. contains averaged attack effectivenessresults applied on stable diffusion models on thesethree sets of seed prompts along with the standarddeviation results given different seeds. contains the exemplar prompts in eachset. Each of these sets are used as the seed in- context exemplar prompts in the initialization stage.The example 0 is the instruction prompt that con-tains the task description. The rest of the examplesare the actual prompts that the model tries to useas in-context exemplars to learn the task from. Westart each exemplar prompt by using prompt as aprefix to the actual prompt for the model to be ableto differentiate the instruction prompt from the restof the exemplar prompts. For the text-to-text ex-periments, we use a numbered list to differentiatethe instruction prompt from the exemplar prompts(e.g., the instruction prompt stays as is and we startnumbering the exemplar prompts as if they are in alist).In addition, we perform some controlled ex-periments to better understand the effect of seedprompts and their similarity to the generated ad-versarial attacks. In our first study, we report theresults by changing the number of unsafe promptsin our seed prompt set. In this study, we designdifferent sets of seed prompts each including dif-ferent number of unsafe seed prompts that triggerthe stable diffusion model to generate unsafe im-ages. We then report the results as we increasethe number of unsafe seed prompts in each stud-ied set of our experiments. contains theresults along with the set of seed prompts that eachinclude different number of unsafe prompts. Weuse the same zero-shot (instruction) prompt for allthe sets and that is the zero-shot prompt from seedSet 1 and just change the few-shot instructions toinclude different number of unsafe prompts in eachset. In our results, we demonstrate that having zerounsafe prompts (none of these prompts trigger thetext-to-image model to generate unsafe outputs)can give us attack effectiveness of over 40% for ourscoring and scoring-LIFO approaches. In addition,we show that having only two unsafe seed promptscan give us attack effectiveness of over 90% for ourscoring approach. also shows how differ-ent approaches act differently on different settingswith regards to number of unsafe seed prompts.In our second study, we report how differentthe generated adversarial attacks are from the seedprompts. To do so, for each generated adversar-ial example, we compute its highest ROUGE-Loverlap with the seed prompts. We plot the distri-bution of these ROUGE-L scores in . Thisapproach was previously used in the self-Instructpaper by Wang et al. (2023) to report how differentthe generated instructions are from the seed instruc-tions used to prompt the model; thus, we utilized the same metric and approach in our setting. Fromour results we demonstrate that many new adver-sarial examples are generated, which do not havemuch overlap with the seeds. We also compare ourapproach with a simple baseline in which we sim-ply augment the seed prompts to create 1,000 newadversarial data points by using word substitutions,removing sentences, adding more information, andcombination of these data augmentation techniquesand as shown in we demonstrate that thismethod is not able to give us diverse adversarialexamples. These examples unlike the generatedexamples using our framework have high ROUGE-L scores and similarity to the seed prompts. Inaddition, we report the trade-off curve comparingdifferent approaches on stable diffusion model overseed set 1 in .",
  "BExperimental Details": "For the text-to-image and text-to-text experiments,we use four and five FLIRT iterations respectivelyin the scheduling mechanism of the scoring-LIFOexperiments to force update the stack if no updatestake place. For all the classifiers, such as NudeNet,perspective API, and TOXIGEN, we use a thresh-old value of 0.5 to determine if an image or textis unsafe or toxic. For the red LM, we set topkand topp values to 50 and 0.95 respectively duringdecoding.",
  "CQualitative Results": "contains some qualitative examples fromprompts generated through FLIRT. con-tains word clouds of the most frequent words ingenerated prompts from each attack strategy. Thedifferences in the word clouds provide some in-formation on existing differences in each attackstrategy. These results are for when we applied theattacks on first set of seed prompts from .",
  ": Seed prompts utilized in our language experiments as the starting in-context exemplar prompts": "0.00.20.40.60.81.0 Percent (%) Medium Safe SD DiversityAttack Effectiveness 0.00.20.40.60.81.0 Percent (%) Max Safe SD DiversityAttack Effectiveness : The diversity-attack effectiveness results on medium and max safe stable diffusion models with varyingthe 2 parameter. The attack effectiveness reports the percentage of images generated by the text-to-image modelthat are labeled as unsafe according to Q16 and NudeNdet classifiers. The diversity score reports the percentage ofunique prompts generated by the red LM.",
  "Algorithm 1: General Scoring Algorithm": "Input: Xt; xtnew; collection of n objectives O1, ..., On; weights associated to the objectives1, ..., n; Xt={}.Output: Xt+1.Score(Xt) = ni=1 iOi(Xt) (Calculate the score for Xt).Put Xt in Xt.for each exemplar prompt xt in Xt do Copy Xt to Xtemp and replace xt by xtnew in Xtemp.Score(Xtemp) = ni=1 iOi(Xtemp) (Calculate the score for Xtemp).Put Xtemp in Xt.endFrom all the list arrangements in Xt pick the list X with maximum score.return X.",
  "Algorithm 2: Greedy Scoring Algorithm": "Input: Xt; xtnew; collection of n objectives that can be simplified to functions over individualelements O1, ..., On; weights associated to the objectives 1, ..., n.Output: Xt+1.for each exemplar prompt xt in Xt do score(xt) = ni=1 i Oi(xt) (calculate the score for all the n objectives)endFind the exemplar prompt xtmin in Xt that has the lowest associated score.Calculate score(xtnew)=ni=1 i Oi(xtnew) .if score(xtnew) > score(xtmin) then"
}