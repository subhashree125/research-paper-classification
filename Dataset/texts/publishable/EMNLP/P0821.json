{
  "Abstract": "Large language models (LLMs) usually fallshort on information extraction (IE) tasksand struggle to follow the complex instruc-tions of IE tasks. This primarily arises fromLLMs not being aligned with humans, as main-stream alignment datasets typically do not in-clude IE data.In this paper, we introduceADELIE (Aligning large language moDELson Information Extraction), an aligned LLMthat effectively solves various IE tasks, includ-ing closed IE, open IE, and on-demand IE.We first collect and construct a high-qualityalignment corpus IEInstruct for IE. Thenwe train ADELIESFT using instruction tuningon IEInstruct. We further train ADELIESFTwith direct preference optimization (DPO) ob-jective, resulting in ADELIEDPO. Extensiveexperiments on various held-out IE datasetsdemonstrate that our models (ADELIESFT andADELIEDPO) achieve state-of-the-art (SoTA)performance among open-source models. Wefurther explore the general capabilities ofADELIE, and experimental results reveal thattheir general capabilities do not exhibit a no-ticeable decline. We have released the code,data, and models to facilitate further research.1",
  "Introduction": "Large language models (LLMs), especially afteralignment with human expectations, such as in-struction tuning (Wei et al., 2022a; Chung et al.,2022; Longpre et al., 2023) or direct prefer-ence optimization (DPO) (Rafailov et al., 2023),have achieved impressive results on numeroustasks (OpenAI, 2022, 2023; Jiang et al., 2023; Anilet al., 2023; Anthropic, 2024). However, LLMsstill fall short on information extraction (IE) tasks,particularly on closed IE tasks (Li et al., 2023a;",
  ": F1 scores (%) on closed, open, and on-demandIE tasks in the few-shot setting. SoTA* denotes the bestperformance of open-source models": "Han et al., 2023; Peng et al., 2023a). LLMs usu-ally struggle to understand and follow the complexinstructions of IE tasks (Peng et al., 2023a; Panget al., 2023; Xu et al., 2023), e.g., complicated taskschema and specifications, which indicates exist-ing LLMs are not aligned with human needs on IEtasks (Peng et al., 2023a; Sainz et al., 2023).To enhance LLM performance on IE tasks, ex-isting efforts have primarily explored three aspects:(1) Prompt engineering, which provides compre-hensive information, e.g., annotation guidelines, toLLMs, without fine-tuning model parameters (Panget al., 2023; Guo et al., 2023; Wei et al., 2023b; Wanet al., 2023). (2) Code LLMs, which leverage theircapabilities of understanding structured informa-tion to enhance the performance on IE tasks (Guoet al., 2023; Sainz et al., 2023; Bi et al., 2023).(3) Multi-task fine-tuning, which involves fine-tuning LLMs on multiple IE datasets to enhancetheir cross-task generalization capabilities in solv-ing IE tasks (Wang et al., 2022a, 2023b; Sainz et al.,2023; Wang et al., 2023d).However, these works do not sufficiently alignLLMs on IE tasks. The prompt engineering methoddoes not inherently align LLMs without tuningmodel parameters. Works using code LLMs andmulti-task fine-tuning typically fine-tune models on homogeneous data, e.g., instances with the sameinput-output format, with a lack of diverse align-ment data. Therefore, the fine-tuned models exhibitlimited generalization capabilities on IE tasks, in-cluding closed IE (Xu et al., 2023), open IE (Xuet al., 2023), and on-demand IE (Jiao et al., 2023).Furthermore, as these models are trained specif-ically for IE, their general capabilities, such asnatural language understanding (Hendrycks et al.,2021), may experience a significant decline.Considering the above issues, we introduceADELIE (Aligning large language moDELs onInformation Extraction), an LLM aligned on IEtasks. Specifically, this work addresses the abovelimitations through two aspects: (1) Rich align-ment data. We construct a high-quality instructiontuning dataset for IE tasks, IEInstruct, including83, 585 instances of various IE tasks. IEInstructincludes a diverse set of instructions and input-output formats. We manually write several instruc-tions for different IE tasks, then expand the instruc-tion set using GPT-3.5 (OpenAI, 2022) similar toSelf-Instruct (Wang et al., 2023e). We then aug-ment the instructions through various augmenta-tion techniques, such as adding annotation guide-lines (Sainz et al., 2023). IEInstruct also in-cludes diverse output formats, such as triplets, nat-ural language, and JSON. We also employ GPT-4 (OpenAI, 2023) to generate chain-of-thoughtexplanations (Wei et al., 2022b) for 8, 000 in-stances in IEInstruct. (2) Sufficient alignment.ADELIESFT is trained based on LLAMA 2 (Tou-vron et al., 2023), using supervised fine-tuning(SFT) (Ouyang et al., 2022) on a mixture ofIEInstruct and generic alignment data used inTULU 2 (Ivison et al., 2023) to maintain themodels general capabilities.We further trainADELIESFT using the direct preference optimiza-tion (DPO) objective (Rafailov et al., 2023) onIEFeedback, a preference dataset constructed us-ing ADELIESFT, resulting in ADELIEDPO.We comprehensively evaluate ADELIESFT andADELIEDPO on closed, open, and on-demand IE.Some results are shown in .The re-sults demonstrate that our models achieve SoTAperformance compared to previous open-sourcemodels and GPT-3.5. There is no significant de-cline in ADELIEs general capabilities, such asMMLU (Hendrycks et al., 2021) and BBH (Suz-gun et al., 2023). Moreover, we analyze severalkey factors of the alignment process and provideseveral insightful findings, such as the mixture strat- egy of IE and general alignment data. We hope ourextensive experiments and analyses will advanceresearch on aligning LLMs.In summary, our contributions are threefold: (1)We construct high-quality alignment data for IEtasks: IEInstruct and IEFeedback. (2) Basedon this high-quality alignment data, we developADELIESFT and ADELIEDPO, with advanced per-formance on IE tasks. (3) We conduct extensiveexperiments and analyses, providing meaningfulinsights for the research on LLM alignment.",
  "Information Extraction Tasks": "Conventional IE tasks are primarily categorizedinto two types: closed IE and open IE. Closed IEinvolves extracting structured information from un-structured text, typically requiring the extractedinformation to conform to a predefined schema.Closed IE includes the following tasks: (1) NamedEntity Recognition (NER), which aims to identifyentities in text and categorizing them into typesdefined in a schema (Yadav and Bethard, 2018).(2) Relation Classification (RC), which classifiesthe relationship into a predefined type between twomentioned entities in the text (Han et al., 2020).(3) Relation Extraction (RE), which aims to extractentities and their relations end-to-end (Zhong andChen, 2021). (4) Event Detection (ED), whichextracts event triggers and classifies them intopredefined types (Wang et al., 2020). (5) EventArgument Extraction (EAE), which aims to ex-tract arguments, e.g., time, for events (Wang et al.,2023c). (6) Event Extraction (EE), which aims toextract events and their arguments in end-to-endparadigm (Peng et al., 2023b). (7) Event Rela-tion Extraction (ERE), which extracts coreference,temporal, causal, and hierarchical relationships be-tween events (Wang et al., 2022b). Open IE aimsto extract n-ary relation tuples from text, withoutrelying on a predefined schema (Zhou et al., 2022).Beyond closed IE and open IE, Jiao et al. (2023)proposed on-demand IE, aimed at extracting user-desired information from unstructured text, such asextracting the shape and taste of fruits, and organiz-ing it into a structured tabular format. On-demandIE is more flexible and aligns with real-world userdemand. This paper covers all these IE tasks, aim-ing to enhance the models ability to address thesetasks through sufficient alignment.",
  "LLMs for Information Extraction": "LLMs often fall short on IE tasks (Li et al., 2023a;Han et al., 2023) due to the complex specifica-tions of these tasks (Peng et al., 2023a). Conse-quently, numerous works have been proposed toenhance LLMs understanding of IE task specifica-tions to improve their performance. These worksare primarily divided into three aspects: (1) Promptengineering (Pang et al., 2023; Guo et al., 2023;Wei et al., 2023b; Wang et al., 2023a; Wan et al.,2023; Zhang et al., 2023; Xie et al., 2023), aims toenhance the models performance on IE tasks byproviding sufficient prompts, such as incorporatingguidelines information. Typically, these methodsdo not involve training model parameters. (2) CodeLLMs (Guo et al., 2023; Sainz et al., 2023; Biet al., 2023; Li et al., 2023c; Wang et al., 2023d),which adopt the Code LLMs capabilities of under-standing structured information on IE tasks, oftenperform better than natural language LLMs. (3)Multi-task fine-tuning (Lu et al., 2022; Wang et al.,2022a, 2023b; Sainz et al., 2023; Chen et al., 2023;Zhou et al., 2023), which trains LLMs on multipleIE datasets, enhancing the models performance onIE tasks, especially in cross-task scenarios. Theseworks do not sufficiently align LLMs with IE tasks,due to the lack of diverse alignment data. Thesetrained LLMs also exhibit a decline in general ca-pabilities. In this paper, we aim to sufficiently alignLLMs on IE tasks with rich alignment data withoutcompromising their general capabilities.",
  "IE Data Collection": "We first collect multiple IE datasets, includingclosed IE (Xu et al., 2023), open IE (Liu et al.,2022), and on-demand IE (Jiao et al., 2023), cov-ering various domains, such as general, financial,and biomedical domains. We filter out 80% of NAdata, which does not contain information needingextraction. To balance different datasets, we em-ploy the examples-proportional mixture (Wei et al.,2022a), with a dataset size limit of 5, 000. The datacollection information is shown in .",
  "Input Construction": "We construct diverse input to better align LLMs onIE tasks. As shown in , the input primarilyconsists of an instruction and a piece of input text.The instruction usually includes 3 components:task description, schema description, and outputformat description. The schema description is onlyused in closed IE tasks, as open IE and on-demandIE do not include a schema. Some inputs alsoinclude several demonstrations, i.e., input-outputexemplars, for enhancing few-shot in-context learn-ing capabilities. We introduce the augmentationprocess of the 3 components of instructions and theconstruction of few-shot demonstrations. Task DescriptionFor each IE task, we first man-ually craft 10 task descriptions. Then we adoptGPT-3.5 to generate 20 more descriptions. Specifi-cally, to enrich the diversity of generated descrip-tions, similar to Self-Instruct, we employ an it-erated generation process, which uses 3 manuallywritten descriptions and 2 generated descriptions asthe prompt for GPT-3.5 to generate a new descrip-tion. Finally, we manually verify the generateddescriptions and filter out those with hallucinations. Schema DescriptionFor closed IE tasks, in-spired by GoLLIE (Sainz et al., 2023), we augmentthe schema descriptions, i.e., category information,from 3 aspects: (1) Schema shuffling and sam-pling. We randomly shuffle the order of categoriesin the schema and select a random subset of 1 to themaximum number of categories to include in theinstruction. This technique aims to prevent modeloverfitting on the schemata in the training corpora,",
  "Output Format": "[Step-by-Step Explanation]: 1. Identify the main event: The sentence revolves around a journalist being killed.2. Determine \"Victim\": The phrase \"the second journalist in just two weeks to be killed covering the Israeli-Palestinian conflict\" identifies the journalist as the victim.[Answer]: The second journalist in just two weeks to be killed: Victim;",
  "Explanation": ": An example of the input and output in IEInstruct. 50% of the data in IEInstruct includes in-contextdemonstrations. The instruction consists of the descriptions of task, schema, and output format. The output consistsof an explanation (for 10% of the instances in IEInstruct) and the answer adhering to the format in instruction. forcing the model to only output categories presentin the input schema. (2) Incorporation of guide-lines. Guidelines are definitions of the schema,which can enhance the models ability to under-stand the schema definition, thereby improving themodels zero-shot generalization capabilities onunseen tasks (Sainz et al., 2023). Therefore, weadd guidelines information to 20% of the data inthe training corpora. Similar to GoLLIE (Sainzet al., 2023), we also include several examples foreach category. The remaining data does not includeguidelines to prevent the model from memorizingschema definitions and to enhance data diversity.(3) Replacing categories with symbols. We ran-domly replace category names with symbols (e.g.,LABEL_1) to prevent the model from overfitting tocategory names (Sainz et al., 2023) and enhancethe in-context learning ability (Wei et al., 2023a). Output Format DescriptionLLMs sometimesstruggle to follow the required output format in IEtasks (Han et al., 2023). To enhance the modelsability to follow format requirements, we introducevarious output format descriptions in the instruc-tions, requiring the model to output accordingly.Specifically, for each closed IE and open IE task,there are mainly 3 types of formats: (1) Tripletformat, specifying output in various triple formats, e.g., (head entity; relation; tail entity) or (head en-tity; tail entity; relation) for relation extraction. (2)JSON format, requiring the model to output JSONformatted results. (3) Natural language format,without specific format requirements, allowing themodel to output in natural language. The construc-tion process of outputs corresponding to formatrequirements is detailed in 3.3. On-demand IEdoes not involve output format descriptions, as itsoutput is typically in a fixed Markdown format. Few-shot DemonstrationsFinally, to enhancethe models few-shot in-context learning capabili-ties, we augment the training corpus with few-shotdemonstration inputs. Specifically, we randomlyselect 50% of the training data and add 1 to 8 ran-domly sampled examplars to the original input.These examplars consist of a piece of input textand the output result, with the output format adher-ing to the requirements in the instruction. For eachinstance, the demonstrations are randomly sampledand shuffled to prevent the model from overfittingto fixed demonstrations.",
  "We construct corresponding outputs according tothe format requirements in the instructions gener-ated in 3.2. Specifically, for each closed IE and": "open IE task, the outputs include 3 formats: (1)Triplet format. Following Wang et al. (2022a), weconvert the output into serialized triplet form. Foroutputs containing multiple triplets, we randomlyshuffle the order of triplets to mitigate potentialorder bias (Li et al., 2023b). (2) JSON format. Wedevise a set of JSON formats and transform theanswers into corresponding JSON data. (3) Naturallanguage format. We manually write several tem-plates for natural language outputs for each taskand construct corresponding outputs based on thesetemplates. For on-demand IE, we adopt the originalanswers in their datasets (Jiao et al., 2023).To enhance the models intensive understandingof IE task procedures, we augment a subset (10%)of instances with Chain-of-Thought (CoT) (Weiet al., 2022b) explanations for closed and openIE. To generate high-quality CoT explanations, weinput both the input text and its ground truth answerto GPT-4. Specifically, we sample 1, 000 instancesfor each task and then use the text input and itscorresponding answer as inputs to generate CoTexplanations. We randomly select 200 instancesto assess the quality of the CoT explanations andfind that GPT-4 generally generates effective andinformative step-by-step thoughts for the answers.",
  "Model Training": "This section introduces the alignment training pro-cess, including SFT (Ouyang et al., 2022) andDPO (Rafailov et al., 2023) training. More trainingdetails are placed in appendix B.For the SFT training, to preserve the modelsgeneral capabilities during alignment, we utilizethe general alignment corpora used by TULU 2 (Ivi-son et al., 2023). Specifically, we mix IEInstruct(83, 585 instances) and 320, 000 instances of gen-eral alignment corpora as the training dataset. Weadopt LLAMA 2 (Touvron et al., 2023) as the back-bone model and train the model for 6, 306 gradientsteps, resulting in ADELIESFT.After the SFT phase, we continue to trainADELIESFT using the DPO objective. We first con-struct DPO training data, i.e., preference pairs (apreferred answer and a dispreferred answer). Theoriginal training objective of DPO requires onlinesampling of preference pairs from the model af-ter SFT (Rafailov et al., 2023) with human anno-tation. In practice, some works also use human-annotated offline preference pairs for training, suchas those sampled from other more powerful mod- els (Ivison et al., 2023). In our implementation,to obtain more diverse data, we used a mix of on-line and offline data. Unlike previous work wherepreference pairs need human annotation, there ex-ists ground truth for IE and hence the preferencepairs can be automatically constructed. Therefore,similar to Chen et al. (2024), we use the modelitself outputs and original ground truths withoutneeding extra human-annotated preference pairs,which is akin to self-improvement (Huang et al.,2023) and can sufficiently minimize manual in-volvement and conserves labors. Specifically, weemploy the BLEU (Papineni et al., 2002) scoreas the metric2 to automatically construct prefer-ence pairs. We sample the output of ADELIESFT5 times for an instance with the sampling temper-ature as 1.0. If the difference between the highestand lowest BLEU scores exceeds 10%, we treat thecorresponding outputs as a preference pair, wherethe higher BLEU output is the preferred answer.We denote this data as online data. We also takethe lowest BLEU output as the dispreferred answerand the ground truth as the preferred answer, anddenote this data as offline data. Finally, we createIEFeedback, containing 3k online preference pairsand 7k offline preference pairs. Then, using theDPO objective, we train for additional 937 gradi-ent steps on ADELIESFT to obtain ADELIEDPO.",
  "Experimental Setup": "BaselinesFor closed IE, we primarily compare3 categories of models: (1) General open-sourceLLMs, including LLAMA 2 (Touvron et al., 2023),a powerful foundation model and TULU 2 (Ivi-son et al., 2023), an instruction tuned LLAMA 2model. We adopt the 7B version of these models.(2) Proprietary LLMs, including GPT-3.5 (OpenAI,2022) and GPT-4 (OpenAI, 2023). (3) Models opti-mized for IE tasks, including GoLLIE (Sainz et al.,2023), a code LLM fine-tuned for IE tasks, and In-structUIE (Wang et al., 2023b), an LLM trained onmultiple IE tasks. For open IE, we adopt the state-of-the-art model, OpenIE6 (Kolluru et al., 2020),as the baseline. For on-demand IE, we comparewith the ODIEDirect model (Jiao et al., 2023), whichis trained on on-demand IE training set.",
  "ADELIEDPO37.934.239.753.548.142.7": ": F1 scores (%) of investigated LLMs on held-out closed IE datasets. The highest scores are in bold and thesecond highest are underlined. * means the scores of the models are sourced from Peng et al. (2023a). indicatesthat InstructUIE has been trained on the SemEval training set. datasets not included in the alignment corpora, tobetter assess the models generalization capabili-ties on IE tasks. Specifically, for closed IE, we em-ploy 4 commonly used datasets: the NER datasetFewNERD (Ding et al., 2021), the RC dataset Se-mEval (Hendrickx et al., 2009), the ED and EAEdataset RichERE (Song et al., 2015), and the EREdataset MATRES (Ning et al., 2018). For open IE,we use the CaRB (Bhardwaj et al., 2019) and RO-BUST (Qi et al., 2023) datasets. For on-demandIE, we employ InstructIE (Jiao et al., 2023). Evaluation SetupFor closed IE and open IE,we adopt zero-shot and few-shot (4-shot for closedIE and 5-shot for open IE) in-context learning forevaluation. The few-shot demonstrations are ran-domly sampled from the corresponding trainingset. For on-demand IE, we adopt zero-shot evalu-ation the same as in the original paper (Jiao et al.,2023). For LLAMA 2, TULU 2, GoLLIE, andInstructUIE, we re-evaluate them using the samedemonstrations. The results for GPT-3.5, GPT-4,OpenIE6, and ODIEDirect are obtained from previ-ous work. Regarding evaluation metrics, we reportF1 scores and employ the same calculation methodas previous work. For details, please refer to Penget al. (2023a) for closed IE, Qi et al. (2023) foropen IE, and Jiao et al. (2023) for on-demand IE.More evaluation details are placed in appendix C.",
  ": F1 scores (%) of investigated LLMs on held-out open IE datasets. The highest scores are in bold andthe second highest are underlined. * denotes the resultsare obtained from Qi et al. (2023)": "GPT-4. Compared to InstructUIE and GoLLIE,which adopt more advanced base LLMs (FLAN-T5 11B and Code LLAMA 7B ) in IE tasks (Penget al., 2023a) and more SFT data (144k and 165kIE instances), ADELIESFT achieves better resultsusing only 83k SFT data with LLAMA 2 7B. Thisindicates that our data construction method is effec-tive and IEInstruct is of high quality. (2) DPOfurther enhances performance. ADELIEDPO per-forms consistently better than ADELIESFT acrossmost datasets. This suggests that for extractivetasks with ground truth answers, further alignmentusing DPO can also self-improve model perfor-mance. However, the improvement of DPO is gen-erally modest, possibly due to not using additionalhuman-annotated preference pairs. We leave usinghuman-annotated preference pairs for training DPOas future work. (3) Incorporating in-context demon-strations during the alignment process is necessary.Previous work only focuses on zero-shot capabili-ties and overlooks few-shot capabilities of LLMs,",
  ": F1 scores (%) of investigated LLMs on theon-demand IE task. The highest scores are in bold andthe second highest are underlined. * means the scoresof the models are sourced Jiao et al. (2023)": "0% 10% 20% 30% 40% 50%100% Proportion of IE data Score (%) IEGeneralAverage of IE and General : Scores (%) on IE tasks (average of closed IE,open IE, and on-demand IE) and general tasks (averageof commonsense reasoning, MMLU, and BBH) of ourmodel trained with varying proportions of IE data. Wefinally adopt a proportion of 20% to train ADELIESFT. resulting in no significant improvement or even adecline when providing few-shot demonstrations,e.g., a 4.3% decline in F1 score for GoLLIE. Incontrast, ADELIESFT s few-shot performance ismuch better than its zero-shot performance, whichsuggests that ADELIESFT possesses few-shot in-context learning capabilities for closed IE tasks.It demonstrates the effectiveness of including in-context demonstrations in the alignment process. Results on Open IEThe results on held-out openIE datasets are shown in . The observationsare similar to those in closed IE. ADELIESFT andADELIEDPO perform much better than GPT-3.5, es-pecially on ROBUST, a robust open IE benchmarkwith ubiquitous syntactic transformations (Qi et al.,2023), which demonstrates the robustness of ourmodels on open IE. Our models even outperformthe SoTA fine-tuned model, OpenIE6, demonstrat-ing the effectiveness of alignment training.",
  ": Performance (%) on general benchmarks.+General is the model trained with only general align-ment corpora for the same gradient steps as ADELIESFT.InstructUIE is trained based on FLAN-T511B": "and table content, assessing the extraction qual-ity (Jiao et al., 2023). We can observe that ADELIEachieves a competitive table header score to GPT-4,which suggests that ADELIE better understandsand follows user instructions. It demonstrates thatthe alignment process effectively aligns ADELIEwith user instructions and expectations.In general, ADELIE achieves remarkable resultsacross all IE tasks, particularly in few-shot evalu-ation scenarios, which demonstrates their strongzero-shot and few-shot generalization capabilitiesand the effectiveness of our alignment corporaIEInstruct and IEFeedback.",
  "Analysis on General Capabilities": "Alignment may impact the models general capa-bilities, namely Alignment Tax (Bai et al., 2022;Kim et al., 2023). We investigate the general capa-bilities of previous LLMs for IE and ADELIESFT inthis section. Specifically, we select several widely-used benchmarks for assessing general capabilities:MMLU (Hendrycks et al., 2021), BBH (Suzgunet al., 2023), and Commonsense Reasoning (in-cluding HellaSwag (Zellers et al., 2019), Wino-Grande (Sakaguchi et al., 2020), PIQA (Bisk et al.,2020), SIQA (Sap et al., 2019), ARC easy and chal-lenge (Clark et al., 2018), and OpenbookQA (Mi-haylov et al., 2018)). The experimental details areplaced in appendix C.3. presents the results. We can observethat: (1) InstructUIE suffers a significant decline ingeneral capabilities compared to its original model,FLAN-T511B (Wei et al., 2022a), which indicatesthat using only IE data for alignment hurts the #Training Instances (Thousands) for DPO -6.0-5.0-4.0-3.0-2.0-1.00.01.0 Improvements (%) Closed IEOpen IE On-demand IE",
  ":Performance improvements (%) of themodel trained on varying scales of data, compared toADELIESFT before DPO training": "models general capabilities. (2) ADELIESFTsperformance improves compared to the originalLLAMA 2. Moreover, ADELIESFT performs onpar with the model trained specifically on generalalignment data (+General). This suggests that mix-ing general and IE alignment data can both enhancethe models general and IE capabilities and hencemitigate the impact of Alignment Tax. There-fore, we advocate for including IEInstruct in thealignment data to enhance the models capabilities.We further investigate the impact of data mix-ing strategy. Specifically, we observe the perfor-mance of models trained with varying proportionsof IE data from IEInstruct in the overall align-ment data. The results are shown in . Wecan observe that: (1) There is a substantial im-provement in IE tasks, even with only 10% of thetraining data being IE data. This suggests a lack ofIE data in the existing mainstream alignment data.(2) Adding IE data in training leads to a decreasein the models general capabilities, but this declineis limited when the proportion is below 50%. Thismay be due to the insufficient capacity of the 7Bmodel, and we leave training a larger model as fu-ture work. Considering the results on both IE andgeneral tasks, we ultimately train ADELIE on thedata including 20% IE data and 80% general data.",
  "Analysis on DPO Training": "We analyze the training data construction strategyfor DPO, i.e., the construction of preference pairs,each consisting of a preferred answer and a dispre-ferred answer. As mentioned in 4, we adopt bothoffline and online data for training. The distinctionlies in that both preferred and dispreferred answersof online data are sampled from ADELIESFTs out-puts, while the preferred answers of offline data areground truths. We examine the impact of the pro- #In-context Demonstrations F1 Score (\\%) ADELIESFTInstructIE GoLLIETULU 2",
  ": F1 scores (%) using a varying number ofin-context demonstrations on closed IE, excluding MA-TRES (document-level) due to the limited context size": "portion of offline data. We find that generally themodel trained on 70% offline data and 30% onlinedata performs best, with an average 47.7% F1 scoreacross closed, open, and on-demand IE tasks. Thedetailed results are shown in Appendix C.4. Wealso explore the impact of data size on performance,as shown in . We find that 10k instances issufficient to train the model, and using more dataincreases computational costs without significantimprovements. This may be due to not using ad-ditional human-annotated data, leading to modeloverfitting. Therefore, IEFeedback ultimately con-sists of 2, 996 online and 6, 989 offline instances.",
  "Analysis on Few-shot ICL Capabilities": "Closed IE typically includes a schema with mul-tiple predefined categories and hence needs morein-context demonstrations to effectively illustratethese categories (Li et al., 2024), which necessitatesthe few-shot in-context learning (ICL) capabilitiesof the model. We observe ADELIESFTs few-shotICL capabilities, as presented in . We findthat ADELIESFT performs consistently better withmore demonstrations, even though ADELIESFT istrained with a maximum of only 8 demonstrations.In contrast, InstructUIE and GoLLIE suffer a de-cline with more few-shot demonstrations. Thisdemonstrates the effectiveness of using in-contextdemonstrations during the alignment process.",
  "Acknowledgements": "We thank all the anonymous reviewers and metareviewers for their valuable comments. This workis supported by the National Natural Science Foun-dation of China (No. 62277033), Beijing Natu-ral Science Foundation (L243006), a grant fromthe Institute for Guo Qiang, Tsinghua University(2019GQB0003) and the project from Tsinghua-SPD Bank Joint-Lab. Thanks the support fromNational Engineering Laboratory for Cyberlearn-ing and Intelligent Technology, and Beijing KeyLab of Networked Multimedia.",
  "Limitations": "The limitations of this work are mainly threefold:(1) The preference pairs used for DPO training areautomatically constructed without additional hu-man annotation, which may limit the performanceof DPO-trained models. We leave using human-annotated preference pairs for DPO training as thefuture work. (2) We train only with a 7B scalemodel due to computational limits. Employing alarger-scale model can yield better performance,but it does not impact the conclusions of this pa-per. (3) This paper only involves English data. Inthe future, we will try to support more languages,and we encourage researchers to explore aligningmodels for multilingual information extraction.",
  "Ethical Considerations": "We discuss potential ethical concerns of this work:(1) Intellectual property. Our work utilizes multi-ple widely-used IE datasets, and we strictly adhereto the licenses of these datasets. We will shareIEInstruct and IEFeedback the CC BY-SA 4.0license3. IEInstruct and IEFeedback includesome data only accessible to Linguistic Data Con-sortium4 (LDC) members, e.g., ACE 2005 (Christo-pher et al., 2005). For these parts, we will releaseonly the data processing scripts. (2) Intendeduse. This paper introduces ADELIE, aiming toalign LLMs and enhance their performance on IEtasks. (3) Potential risk control. IEInstruct and IEFeedback are collected and constructed basedon widely-used public data and data obtained fromGPT-3.5 and GPT-4. We believe that these datahave been well anonymized and sanitized by theiroriginal publishers and OpenAI. We also randomlysampled 100 instances and found no sensitive data.(4) AI assistance. We adopt GPT-4 for paraphras-ing some sentences when writing this paper.",
  "Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang,Xu Han, Pengjun Xie, Haitao Zheng, and ZhiyuanLiu. 2021. Few-NERD: A few-shot named entityrecognition dataset. In Proceedings of ACL, pages31983213": "Yucan Guo, Zixuan Li, Xiaolong Jin, Yantao Liu, Yu-tao Zeng, Wenxuan Liu, Xiang Li, Pan Yang, LongBai, J. Guo, and Xueqi Cheng. 2023.Retrieval-augmented code generation for universal informationextraction. ArXiv preprint. Harsha Gurulingappa, Abdul Mateen Rajput, AngusRoberts, Juliane Fluck, Martin Hofmann-Apitius, andLuca Toldo. 2012. Development of a benchmarkcorpus to support the automatic extraction of drug-related adverse effects from medical case reports.Journal of Biomedical Informatics, 45(5):885 892. Ridong Han, Tao Peng, Chaohao Yang, Benyou Wang,Lu Liu, and Xiang Wan. 2023. Is information ex-traction solved by ChatGPT? An analysis of per-formance, evaluation criteria, robustness and errors.ArXiv preprint. Xu Han, Tianyu Gao, Yankai Lin, Hao Peng, YaoliangYang, Chaojun Xiao, Zhiyuan Liu, Peng Li, Jie Zhou,and Maosong Sun. 2020. More data, more relations,more context and more openness: A review and out-look for relation extraction. In Proceedings of theAACL-IJCNLP, pages 745758. Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao,Zhiyuan Liu, and Maosong Sun. 2018. FewRel: Alarge-scale supervised few-shot relation classificationdataset with state-of-the-art evaluation. In Proceed-ings of EMNLP, pages 48034809. Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,Preslav Nakov, Diarmuid Saghdha, SebastianPad, Marco Pennacchiotti, Lorenza Romano, andStan Szpakowicz. 2009. SemEval-2010 task 8: Multi-way classification of semantic relations between pairsof nominals. In Proceedings of the Workshop onSEW, pages 9499.",
  "Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, XuezhiWang, Hongkun Yu, and Jiawei Han. 2023. Largelanguage models can self-improve. In Proceedingsof EMNLP, pages 10511068": "Hamish Ivison, Yizhong Wang, Valentina Pyatkin,Nathan Lambert, Matthew E. Peters, Pradeep Dasigi,Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy,and Hanna Hajishirzi. 2023. Camels in a changingclimate: Enhancing lm adaptation with tulu 2. ArXivpreprint. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode Las Casas, Florian Bressand, Gianna Lengyel,Guillaume Lample, Lucile Saulnier, Llio Re-nard Lavaud, Marie-Anne Lachaux, Pierre Stock,Teven Le Scao, Thibaut Lavril, Thomas Wang, Timo-the Lacroix, and William El Sayed. 2023. Mistral7b. ArXiv preprint. Yizhu Jiao, Ming Zhong, Sha Li, Ruining Zhao, SiruOuyang, Heng Ji, and Jiawei Han. 2023. Instruct andExtract: Instruction Tuning for On-Demand Informa-tion extraction. In Proceedings of EMNLP, pages1003010051.",
  "Sungdong Kim, Sanghwan Bae, Jamin Shin, SoyoungKang, Donghyun Kwak, Kang Yoo, and MinjoonSeo. 2023. Aligning large language models throughsynthetic feedback. In Proceedings of EMNLP, pages1367713700": "Keshav Kolluru, Vaibhav Adlakha, Samarth Aggarwal,Mausam, and Soumen Chakrabarti. 2020. OpenIE6:Iterative Grid Labeling and Coordination Analysisfor Open Information Extraction. In Proceedings ofEMNLP, pages 37483761. Bo Li, Gexiang Fang, Yang Yang, Quansen Wang, WeiYe, Wen Zhao, and Shikun Zhang. 2023a. EvaluatingChatGPTs information extraction capabilities: Anassessment of performance, explainability, calibra-tion, and faithfulness. ArXiv preprint.",
  "Jiangnan Li, Yice Zhang, Bin Liang, Kam-Fai Wong,and Ruifeng Xu. 2023b. Set learning for generativeinformation extraction. In Proceedings of EMNLP,pages 1304313052": "Jiao Li, Yueping Sun, Robin J Johnson, Daniela Sci-aky, Chih-Hsuan Wei, Robert Leaman, Allan PeterDavis, Carolyn J Mattingly, Thomas C Wiegers, andZhiyong Lu. 2016. Biocreative v cdr task corpus:A resource for chemical disease relation extraction.Database. Peng Li, Tianxiang Sun, Qiong Tang, Hang Yan, Yuan-bin Wu, Xuan-Jing Huang, and Xipeng Qiu. 2023c.Codeie: Large code generation models are better few-shot information extractors. In Proceedings of ACL,pages 1533915353.",
  "Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu Lin,Xianpei Han, Le Sun, and Hua Wu. 2022. Unifiedstructure generation for universal information extrac-tion. In Proceedings of ACL, pages 57555772": "Yi Luan, Luheng He, Mari Ostendorf, and HannanehHajishirzi. 2018. Multi-task identification of entities,relations, and coreference for scientific knowledgegraph construction. In Proceedings of EMNLP, pages32193232. Todor Mihaylov, Peter Clark, Tushar Khot, and AshishSabharwal. 2018. Can a suit of armor conduct elec-tricity? a new dataset for open book question answer-ing. In Proceedings of EMNLP, pages 23812391.",
  "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: A method for automatic evalu-ation of machine translation. In Proceedings of ACL,pages 311318": "Hao Peng, Xiaozhi Wang, Jianhui Chen, Weikai Li,Yunjia Qi, Zimu Wang, Zhili Wu, Kaisheng Zeng,Bin Xu, Lei Hou, and Juanzi Li. 2023a. When doesin-context learning fall short and why? A study onspecification-heavy tasks. ArXiv preprint. Hao Peng, Xiaozhi Wang, Feng Yao, Kaisheng Zeng,Lei Hou, Juanzi Li, Zhiyuan Liu, and Weixing Shen.2023b. The devil is in the details: On the pitfalls ofevent extraction evaluation. In Findings of ACL. Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Hwee Tou Ng, Anders Bjrkelund, Olga Uryupina,Yuchen Zhang, and Zhi Zhong. 2013. Towards robustlinguistic analysis using OntoNotes. In Proceedingsof CoNLL, pages 143152. Ji Qi, Chuchun Zhang, Xiaozhi Wang, Kaisheng Zeng,Jifan Yu, Jinxin Liu, Jiuding Sun, Yuxiang Chen, LeiHow, Juanzi Li, et al. 2023. Preserving knowledgeinvariance: Rethinking robustness evaluation of openinformation extraction. Findings of EMNLP. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-pher D. Manning, Stefano Ermon, and Chelsea Finn.2023. Direct preference optimization: Your languagemodel is secretly a reward model. In Advances inNeurIPs.",
  "Taneeya Satyapanich, Francis Ferraro, and Tim Finin.2020. CASIE: extracting cybersecurity event infor-mation from text. In Processings of AAAI, pages87498757": "Zhiyi Song, Ann Bies, Stephanie Strassel, Tom Riese,Justin Mott, Joe Ellis, Jonathan Wright, Seth Kulick,Neville Ryant, and Xiaoyi Ma. 2015. From lightto rich ERE: Annotation of entities, relations, andevents. In Proceedings of the The 3rd Workshop onEVENTS: Definition, Detection, Coreference, andRepresentation, pages 8998. Zhaoyue Sun, Jiazheng Li, Gabriele Pergola, ByronWallace, Bino John, Nigel Greene, Joseph Kim, andYulan He. 2022. PHEE: A dataset for pharmacovigi-lance event extraction from text. In Proceedings ofEMNLP, pages 55715587. Mirac Suzgun, Nathan Scales, Nathanael Schrli, Se-bastian Gehrmann, Yi Tay, Hyung Won Chung,Aakanksha Chowdhery, Quoc Le, Ed Chi, DennyZhou, et al. 2023. Challenging big-bench tasks andwhether chain-of-thought can solve them. In Find-ings of ACL, pages 1300313051.",
  "Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang,Fei Wu, Tianwei Zhang, Jiwei Li, and Guoyin Wang.2023a. Gpt-ner: Named entity recognition via largelanguage models. ArXiv preprint": "Xiao Wang, Wei Zhou, Can Zu, Han Xia, Tianze Chen,Yuan Zhang, Rui Zheng, Junjie Ye, Qi Zhang, TaoGui, Jihua Kang, J. Yang, Siyuan Li, and Chunsai Du.2023b. InstructUIE: Multi-task instruction tuning forunified information extraction. ArXiv preprint. Xiaozhi Wang, Yulin Chen, Ning Ding, Hao Peng, ZimuWang, Yankai Lin, Xu Han, Lei Hou, Juanzi Li,Zhiyuan Liu, Peng Li, and Jie Zhou. 2022b. MAVEN-ERE: A unified large-scale dataset for event corefer-ence, temporal, causal, and subevent relation extrac-tion. In Proceedings of EMNLP, pages 926941. Xiaozhi Wang, Hao Peng, Yong Guan, Kaisheng Zeng,Jianhui Chen, Lei Hou, Xu Han, Yankai Lin, ZhiyuanLiu, Ruobing Xie, et al. 2023c. Maven-arg: Com-pleting the puzzle of all-in-one event understand-ing dataset with event argument annotation. ArXivpreprint. Xiaozhi Wang, Ziqi Wang, Xu Han, Wangyi Jiang, RongHan, Zhiyuan Liu, Juanzi Li, Peng Li, Yankai Lin,and Jie Zhou. 2020. MAVEN: A Massive GeneralDomain Event Detection Dataset. In Proceedings ofEMNLP, pages 16521671.",
  "language models are zero-shot learners. In Proceed-ings of ICLR": "Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022b. Chain-of-thought prompting elicits rea-soning in large language models. In Proceedings ofNeurIPs, pages 2482424837. Jerry Wei, Le Hou, Andrew Lampinen, Xiangning Chen,Da Huang, Yi Tay, Xinyun Chen, Yifeng Lu, DennyZhou, Tengyu Ma, et al. 2023a. Symbol tuning im-proves in-context learning in language models. InProceedings of EMNLP, pages 968979. Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang,Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu,Yufeng Chen, Meishan Zhang, Yong Jiang, and Wen-juan Han. 2023b. Zero-shot information extractionvia chatting with chatgpt. ArXiv preprint.",
  "A.1Input Construction": "For constructing the Task Description and OutputFormat Description, we initially manually wrote 10task descriptions and 5 output format descriptionsfor each task. We employed GPT-3.5 to generatetask descriptions with the same semantics but var-ied expressions, as well as diverse output formats. is an example used in the open IE task.Each generation includes five components: (1) In-struction: a description of the task. (2) Fail output:a response for when the task fails, which shouldcorrespond to the final requirement of the instruc-tion. (3) Input template: a description of the outputformat in natural language, which must includemultiple forms, such as triplets or natural language.(4) Output template: the output format correspond-ing to the input template. details the num-ber of augmented descriptions generated for eachtask.",
  "IEInstructTheprocessofconstructingIEInstruct involves the following steps:wesampled 5, 000 instances from these raw datasets.Then, we followed the process outlined in 3": "and filtered out instances longer than 2, 048tokens to prevent them from affecting the trainingeffectiveness.Finally, we compiled the IEInstruct dataset,which consists of a total of 83, 585 high-qualityIE data instances. shows the number ofinstances for each training dataset. IEFeedbackTo generate IEFeedback, we sam-pled 50, 000 entries from raw datasets for pro-cessing in a manner similar to IEInstruct. Thesole distinction lies in the consistency of the out-put format with that required by the evaluationdatasets, as shown in appendix C.1, aimed at facili-tating more accurate BLEU scoring. For calculat-ing BLEU scores, we used sentence_bleu func-tion from nltk.translate.bleu_score, withSmoothingFunction set to method3. dis-plays the information for the IEFeedback dataset,which consists of a total of 9, 985 instances.",
  "B.2Training Hyperparameters": "SFT trainingTo train the models, we employsupervised fine-tuning, which is the most effectivemethod for aligning the models. The models weretrained for 2 epochs with an effective batch-size of128, a learning-rate of 2e5 with cosine schedulerand a warm-up phase of 0.03. To better facilitatelearning in few-shot settings and document-levelinformation extraction, the context length is set to2048 tokens. we conduct SFT on Nvidia A100GPUs, totaling approximately 120 GPU hours. DPO trainingSimilar to SFT, we train the DPOmodel for 3 epochs. Model is trained with a globalbatch size of 32. And we employ a linear learningrate scheduler with a peak learning rate of 5e 7and a 0.1 warm-up phase. The final ADELIEDPOmodel is initialized from the SFT model, whichwas trained for 2 epochs and further optimized for3 DPO epochs. we conduct DPO on Nvidia A100GPUs, totaling approximately 8 GPU hours.",
  "CExperimental Details": "This section introduces the details of the ex-periment, including the details of the evalua-tion(appendix C.1), the inference details of thecomparison baseline (appendix C.2), and the detailresults about analytical experiments (appendix C.3,appendix C.4). PromptYou need to follow the template list to come up with a set of diverse templates.The task indicated by this template is the Open Information Extraction task.We need to write the instruction, input format and corresponding output format template for it.Instruction is an introduction to OpenIE tasks.The instruction template content should include the following strings to facilitate subsequent replace-ment of the content: {text}.The answer template content should include the following strings to facilitate subsequent replacementof the content: {subject}, {predicate}, {object}, {time}, {location}.Here are the requirements:1. Try not to repeat the verb for each template to maximize diversity.2. The language used for the template also should be diverse. For example, use interrogative sentences,imperative sentences, etc.3. Input and output templates ([Answer]: ..) should also be as diverse as possible.4. Do not repeat the format of the answer template, nor repeat the examples given.5. Input and output must correspond to each other.6. The templates should be in English. Template 1:(1) Instruction: Open information extraction requires the extraction of all relations in the sentence, i.e.,predicates, the subjects and objects corresponding to these relations, and the possible time and placethesis elements. Please extract all the relational tuples in the following sentence: {text}. If there are norelations in the text, please answer \"NA\".(2) Fail output: NA.(3) Input template: Please give the answer in the tuple form \"[Answer]: ({predicate}; {subject};{object}; {time}; {location})\".(4) Answer template: ({predicate}; {subject}; {object}; {time}; {location}),...Please follow the format given in the example to generate 1 templates. ResponseTemplate 6:(1) Instruction: In the task of Open Information Extraction, your goal is to dissect the given text tounearth the underlying relationships. This involves pinpointing the subjects, predicates, and objects, aswell as noting any relevant temporal or spatial details. Should the text lack relational tuples, kindlyrespond with No relationships identified.. Text: {text}.(2) Fail output: No relationships identified.(3) Input template: What connections can you draw between the subject and object, including anypertinent temporal or spatial details?(4) Answer template: Between \"{subject}\" and \"{object}\", the connection \"{predicate}\" is established,occurring at \"{time}\" and within \"{location}\".",
  ": An example prompt for Open IE to generate task description and output format description": "PromptPlease generate a step-by-step explanation for [Answer] based on [Question], and give reasons for eachstep.The generated explanation should make use of the content in the [Question] as much as possible, andmust be consistent with the [Answer].It will eventually be provided at the front of the answer.No more than {words_number} words.",
  "tasks, including GoLLIE 7B (HiTZ/GoLLIE-7B7": "and InstructUIE (ZWK/InstructUIE8. We ob-served that these models are sensitive to prompts,and directly using the testing prompts fromADELIESFT leads to a sharp decline in modelperformance. Therefore, while keeping the testdata unchanged, we adjusted the prompts to matchthe official formats of these models. For GoLLIE,as it did not provide formats for ERE and RCtasks, We modified the format of the RE task foradaptation purposes.",
  "C.4Analysis on DPO Training": "presents the results in the DPO train-ing analysis experiment. We observed a trend inwhich the average performance initially increasedand then decreased with the increase in the offlinerate. The highest performance was achieved at 0.7,reaching 47.73% (although the result displayed for1.0 is also 47.7%, it is actually 47.68%). [NER]Please give the answer in the form \"[Answer]: {entity}: {type}; \".[RC]Please give the answer in the tuple form \"[Answer]: ({subject}; {relation}; {object}); \".[ED]Please give the answer in the form \"[Answer]: {event}: {class}; \".[EAE]Please give the answer in the form \"[Answer]: {word}: {role}; \".[ERE]Please give the answer in the tuple form \"[Answer]: ({first event}; {relation}; {second event}); \".[Open IE]Please give the answer in the tuple form \"[Answer]: ({predicate}; {subject}; {object}; {time};{location})\". If one or more of the last three elements does not exist, it can be omitted."
}