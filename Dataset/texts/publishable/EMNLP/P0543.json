{
  "Abstract": "Web scraping is a powerful technique that ex-tracts data from websites, enabling automateddata collection, enhancing data analysis capa-bilities, and minimizing manual data entry ef-forts. Existing methods, wrappers-based meth-ods suffer from limited adaptability and scal-ability when faced with a new website, whilelanguage agents, empowered by large languagemodels (LLMs), exhibit poor reusability in di-verse web environments. In this work, we in-troduce the paradigm of generating web scrap-ers with LLMs and propose AUTOSCRAPER,a two-stage framework that can handle diverseand changing web environments more effi-ciently. AUTOSCRAPER leverages the hierar-chical structure of HTML and similarity acrossdifferent web pages for generating web scrap-ers. Besides, we propose a new executabilitymetric for better measuring the performance ofweb scraper generation tasks. We conduct com-prehensive experiments with multiple LLMsand demonstrate the effectiveness of our frame-work. Our work is now open-source.1",
  "Introduction": "Web scraping is a process where software auto-mates the extraction of data from websites, typi-cally using bots or web scrapers to gather specificinformation (Thapelo et al., 2021). It is impor-tant because it allows for efficient data collectionand aggregation, which can be crucial for marketresearch, competitive analysis, and real-time datamonitoring.Due to the diversity of sources and informationon the internet, the construction of a web scraperrequires substantial human effort. Consequently,two types of methods for automatic web informa-tion acquisition have been proposed, categorized aswrapper-based and language-agent-based (Sarkhel",
  "Response": "Webpage-1 <html> ... <div class=\"entity-title\"> <div class=\"title\"> <span> Stephen Curry</span> Webpage-1 Wrapper1 Q2 <html> ... <div class=\"entity-title\"> <div class=\"title\"> <span>James Harden</span> Response Webpage-2 Highly reusable with Great Performance <html> ... <div class=\"entity-title\"> <div class=\"title\"> <span>Stephen Curry</span> Webpage-1 Q2: Whats the age of James Harden? <html> ... <div class=\"entity-title\"> <div class=\"title\"> <span>LEBRON JAMES</span> Webpage-2Q1 Wrapper2 ResponseResponse <html> Webpage-2",
  ": An illustration of comparing wrapper-basedmethods, language-agent-based methods and AUTO-SCRAPER": "et al., 2023).The wrapper-based method en-tails complex sequences of operations within cus-tomized rule-based functions, which are designedto efficiently access and retrieve desired data fromwebsites, which is especially beneficial for struc-tured websites with stable layouts (Kushmerick,1997; Dalvi et al., 2011; Bronzi et al., 2013). Con-versely, the language-agent-based method lever-ages powerful natural language processing capabil-ities of large language models (LLMs) to interpretfree-text queries and directly extract data withinwebsites to meet the demand, effectively handlingboth structured and dynamic web content (White-house et al., 2023; Marco Perini, 2024). Although both types of methods facilitate webscraping to varying degrees, as shown in ,they exhibit significant shortcomings in terms ofscalability. Wrapper-based method, while reusable,struggles with entirely new website structures,which necessitates extensive human effort to de-velop additional customized functions (Gulhaneet al., 2011; Lockard et al., 2019). Conversely, although language-agent-based methods demon-strate superior performance in adapting to new con-tent, their reliance on a limited number of super-powerful API-based LLMs for web scraping incursconsiderable time and financial costs. Together,these challenges impede the broader adoption andscalability of current web scraping technologies,limiting their practicality in dynamic and diverseweb environments.To address the shortcomings of the aforemen-tioned two paradigms, the paradigm of generatingweb scrapers with LLMs would be the optimal so-lution. On one hand, compared to wrapper-basedmethods, it fully leverages the reasoning and re-flection capacities of LLMs, reducing manual de-sign on new tasks and enhancing scalability. Onthe other hand, compared to language-agent-basedmethods, it introduces repeatable extraction pro-cedures, reducing the dependency on LLMs whendealing with similar tasks, and thereby improvingefficiency when handling a large number of webtasks. However, there are several challenges asso-ciated with using LLMs to generate web scrapers: 1. Long HTML document. Although LLMsexcel in comprehending long textual content,HTML, as semi-structured data, comprisesboth structured (tags and attributes) and un-structured (textual content) elements. Conse-quently, it is challenging for LLMs to generateexecutable web scrapers that strictly adhereto the hierarchical structure of web pages incomplex markup contexts. 2. Reusability.A good scraper needs to bereusable across multiple web pages. How-ever, the differences in content and structurebetween various web pages can lead to the cre-ation of a scraper that references a webpage,which can only be applied to some web pages. 3. Appropriate evaluation metrics.For ascraper to be considered useful, it must beable to automatically extract the desired re-sults from all web pages. However, existingevaluation metrics for web information ex-traction, which focus on the extraction resultsfrom individual web pages, do not adequatelyreflect the usability of the scraper. This canpotentially mislead experimental conclusions.",
  "We introduce AUTOSCRAPER, a two-stage frame-work to address the web scraper generation task": "Illustrated in , AUTOSCRAPER comprisestwo main components: progressive generation andsynthesis. The progressive generation stage lever-ages the hierarchical structure of HTML for pro-gressive understanding to address the long HTMLdocument. Subsequently, the synthesis moduleintegrates multiple scrapers generated on differ-ent web pages to produce a cohesive, website-specific scraper that functions universally withinthat site. Besides, we propose a new evaluationmetric for web scraper generation tasks, called theexecutability metric. Unlike traditional informationextraction metrics that measure single web pages,this metric measures multiple web pages withina website, accurately reflecting the reliability andreusability of the scraper.We evaluate AUTOSCRAPER on three availabledatasets with 8 LLMs. On all three datasets, AU-",
  "TOSCRAPER consistently outperforms all base-lines and achieves new state-of-the-art results inzero-shot settings. Also, AUTOSCRAPER can sur-pass supervised learning methods. Moreover, AU-": "TOSCRAPER demonstrates superior efficiency onlarge-scale web information extraction tasks. Com-pared to traditional wrappers, AUTOSCRAPER ad-justed more quickly according to different web-sites and task requirements. This flexibility enablesscrappers to handle diverse and changing web en-vironments more efficiently. Compared to the lan-guage agent paradigm, it introduces intermediatefunctions to enhance reusability and reduce thedependency on LLMs when dealing with similartasks, thereby improving efficiency when handlinga large number of web tasks.",
  "Related Work": "Wrapper-based methods for web scraping utilizethe hierarchical structure of the webpage. Methodof this category includes rule-based (Zheng et al.,2008), learning wrappers (i.e a DOM-specificparser that can extract content) (Gulhane et al.,2011; Kushmerick, 1997; Dalvi et al., 2011), heuris-tic algorithm (Lockard et al., 2018, 2019) and deeplearning neural network (Lin et al., 2020; Zhouet al., 2021; Li et al., 2022; Wang et al., 2022).These methods demand substantial human involve-ment, including creating wrapper annotations, ap-plying heuristic scoring rules (such as visual prox-imity), crafting features for neural network input,and using prior knowledge for verification. There-fore, it is difficult for wrapper-based methods to automatically scale up when facing web scrapingtasks across a large number of different websites.With the emergence of powerful LLMs (Ope-nAI, 2023; Touvron et al., 2023), languageagents (Sumers et al., 2023) now operate in in-teractive environments, leveraging LLM-basedreasoning, grounding, learning, and decision-making. General language agents, such as Chain-of-Thought (Wei et al., 2023), Reflexion (Shinnet al., 2023), Self-Refine (Madaan et al., 2023),and Self-Debug (Chen et al., 2023), capitalize onLLMs self-reflection capabilities for iterative plan-ning optimization. However, these agents do noteffectively utilize web structural features and failto simplify the web environment after unsuccess-ful planning attempts, limiting the optimization ofsubsequent planning.Current language agents primarily aim to stream-line the web environment (Sridhar et al., 2023;Gur et al., 2023; Zheng et al., 2024) and developstrategies for planning and interacting with theweb (Sodhi et al., 2023; Ma et al., 2023). Neverthe-less, these frameworks mainly focus on the conceptof open-world web simulation environments (Shiet al., 2017; Yao et al., 2023; Deng et al., 2023;Zhou et al., 2023), which encompass a broad spec-trum of tasks found in real-life scenarios, such asonline shopping, flight booking, and software de-velopment. These task scenarios are oriented to-wards individuals and have significantly differentrequirements for accuracy and efficiency comparedto web scraping.As a result, current language-agent-based meth-ods cannot effectively exploit the HTML structuralsimilarities across multiple web pages, reducingtheir dependency on LLMs when performing repet-itive operations and leading to inefficiencies.",
  "Task Formulation": "First, we formulate our scraper generation task.Given a set of webpages on the same websitew W describing a subject entity s (also calledtopic entity in the previous literature), and its corre-sponding predefined target attribute r R, the taskobjective is to generate an executable rule/actionsequence A to extract target information o from all",
  "We adopt the semi-structure information extractiontask as a testbed for the scraper generation task": "SWDE(Hao et al., 2011) is a Structured WebData Extraction dataset that contains webpagesfrom 80 websites in 8 domains, with 124,291 web-pages. Each of the websites from the same domainsfocuses on 3-5 attributes in the web pages. EXTENDED SWDE(Lockard et al., 2019) in-volves fine-grained manual annotation of 21 sitesin 3 domains from SWDE. While SWDE containsan average of 4,480 triples for 3 predicates per web-site, the EXTENDED SWDE dataset averages 41Ktriples for 36 predicates per site. DS1(Omari et al., 2017) contains 166 annotatedwebpages from 30 real-life large-scale websites cat-egorized into books, shopping, hotels, and movies.We transform the dataset with the following set-tings. First, we design instructions for each of thedomains, and for each of the attributes as the inputinformation for LLMs2. Second, for each websitein each domain, we sample 100 web pages as thewhole test set. We consider the set of webpageson the same websites and the corresponding ex-traction instruction as a case. For example, forthe ESPN websites3 in NBA player domains, thesampled 100-detail webpage of players and the in-struction Please extract the team of the player heplays now is a complete case of our scraper gen-eration task. Third, we pre-process the web pagesby removing irrelevant elements in a webpage. Weuse open-source BeautifulSoup library4 and filterout all DOM element nodes with <script> and<style>, as well as delete all attributes in the el-ement node except @class. We replace the origi-nal escape characters in the annotations to ensure",
  "Further details about the prompt is in Appendix D.13": "Action Sequence 3Step1: //*[text()='PPG]Step2: ./ancestorStep3: ./span/text() Action Sequence 2 Step1: //*[text()='PPG]Step2: ./ancestorStep3: ./span/text() Phase1: Progressive GenerationPhase2: Synthesis Action Sequence 1 Step1: //*[text()=PPG]/text() Step2: ./ancestor Step3: ./span/text() Result of Action Sequence 3Webpage1: 17.4Webpage2: 28.3Webpage3: 22.6 Result of Action Sequence 2Webpage1: Webpage2: Webpage3: Result of Action Sequence 1Webpage 1: 6.7Webpage 2: 4.7Webpage 3: 2.9 Which sequence is the best? Action Sequence 1 is best. Instruction: Whats the average rebound of James Harden? Step1: Top-down Step2: Step-back Step3: Top-down Wrong. Correct. (a) Progressive Generation:Generate an action sequence through multiple rounds of interaction with a webpage. (b) Synthesis:Choose one of the best action sequence generated with different webpage. Action Sequence 2 Action Sequence 1 Action Sequence 3",
  "Final Action Sequence": "Seed WebpagesSequence Set Extract text with action sequences Step1: [Top-down] XPath: //*[text()=PPG]/text()# Get the text below the text node PPG. Step2: [Step-back] XPath: ./ancestor# 16.6 is not the right answer, get the sub HTML with more context. Step3: [Top-down] XPath: ./span/text()# Get the text from the second node of current sub HTML. [Full HTML] [Sub HTML] [Extract the text] 16.6 [Get the sub HTML] [Extract the text] 5.1",
  "Evaluation Metrics": "Existing evaluation schemes for web page infor-mation extraction tasks still follow the traditionalmetrics of text information extraction tasks, namelyprecision, recall, and F1 score. They limit the as-sessment of methods for the scraper generation taskto two aspects. First, it focuses on extraction witha single webpage, rather than considering the gen-eralizability from the perspective of a collectionof webpages. Second, it does not effectively mea-sure the transferability when adopting the actionsequence to other web pages.To address this issue, we transform the tradi-tional IE task evaluation into an executable eval-uation. Based on the traditional IE evaluation ona collection of web pages, we categorize the exe-cutability of action sequences into the followingsix situations.Specifically, for each extractiontask on a website, the result is classified basedon the extraction result on precision, recall, andf1-score. (1) Correct: both precision, recall and f1-score equal 1, which indicates the action sequence is precisely; (2) Precision(Prec.): only precisionequals 1, which indicates perfect accuracy in theinstances extracted following the action sequence,but misses relevant instances; (3) Recall(Reca.):only recall equals 1, which means that it success-fully identifies all relevant instances in the webpagebut incorrectly identifies some irrelevant instances;(4) Un-executable(Unex.): recall equals 0, whichindicates that the action sequence fails to identifyrelevant instances; (5) Over-estimate(Over.): pre-cision equals 0, which indicates that the action se-quence extracts the instances while ground truth isempty; (6) Else: the rest of the situation, includingpartially extracting the information, etc.Since the above classifications are mutually ex-clusive, we use the ratio metric to calculate theproportion of each result in our task.",
  "Modeling": "Unlike the wrapper method that generates an XPath,we model the scraper generation task as an actionsequence generation task. In specific, we generatean action sequence Aseq that consists of a sequenceof XPath5 expression from a set of seed webpages(i.e., a small portion of webpages in the test casefor generating the sequence).",
  "Aseq = [XPath1, XPath2, ..., XPathn](2)": "where n denotes the length of the action sequence.We execute the XPath in the sequence using theparser in order. In the sequence, all XPath expres-sions except the last one are used for pruning theweb page, and the last one is used for extractingthe corresponding element value from the prunedweb page.",
  "Progressive Generation": "Dealing with the lengthy content and hierarchicalstructure of webpages, generating a complete andexecutable scraper in one turn is difficult. How-ever, the HTML content is organized in a DOMtree structure, which makes it possible to pruneirrelevant page components and hence, limit thelength and height of the DOM tree to improve theperformance of LLM generation.Specifically, we perform a traversal strategyconsisting of top-down and step-back operations.Top-down refers to starting from the root node ofthe current DOM tree, progressively refining downto the specific node containing the target informa-tion. Step-back refers to reassessing and adjustingselection criteria by moving up the DOM tree tochoose a more reliable and broadly applicable nodeas a foundation for more consistent and accurateXPath targeting. At each step, we first employ atop-down operation, guiding the LLMs to directly write out the XPath leading to the node contain-ing the target information and to judge whether thevalue extracted with XPath is consistent with thevalue it recognizes. If execution fails, then adopt astep-back operation to retreat from the failed node,ensuring the web page includes the target informa-tion, which is driven by LLMs. The detail is shownin Algorithm 1.",
  "Synthesis": "Although we gain an executable action sequencewithin the progressive generation process, there arestill differences in the specific location of the tar-get information and the structure between differentweb pages. The action sequence may collect XPathwith specific characteristics in a single HTML andlose generalizability. To enhance the reusability ofthe action sequence, we propose a synthesis phase.Specifically, we randomly select ns webpagesfrom the case as seed webpages. Then, we generatean action sequence for each of them. Subsequently,we execute multiple different action sequences toextract information from the seed web pages, re-spectively. We collect all action sequences andtheir corresponding results and then choose onethat can extract all the target information in theweb pages as the final action sequence.",
  "Experimental Settings & EvaluationMetrics": "We conduct our experiment on 8 LLMs includingclosed-source LLMs: GPT-3.5-Turbo (OpenAI,2022), Gemini Pro (Team et al., 2023), GPT-4-o-mini (OpenAI, 2024) and GPT-4-Turbo (Ope-nAI, 2023) as well as open-source LLMs: Phi-3-medium (Abdin et al., 2024), CodeLlama-34B (Rozire et al., 2024), Mixtral 87B (Jianget al., 2024) and Deepseek-Coder-33B (Guo et al.,2024).Furthermore, we apply different LLM-prompt-based web agents as our baselines, includ-ing COT (Wei et al., 2023) and Reflexion (Shinn",
  ": The executable evaluation and IE evaluation of LLMs with three frameworks in SWDE, EXTENDED SWDE,and DS1 dataset. Best Correct, Unexecutable, precision, recall, and F1 score are marked bold": "et al., 2023) and AUTOSCRAPER to them. Thecomparison between them is discussed in Ap-pendix B.1. Due to the limited-length context ofLLMs, all experiments are conducted under zero-shot settings.We test them on three datasets: SWDE (Haoet al., 2011), EXTEND SWDE (Lockard et al., 2019)and DS1 (Omari et al., 2017). The detailed ex-perimental results of the last two can be found inAppendix A.1 and A.2. We set the size of seedwebpages ns = 3 for SWDE and EXTEND SWDE,ns = 1 for DS1 and max retry times dmax = 5.In addition to the execution evaluation metricsdescribed in .3, we also employ traditionalevaluation metrics to more comprehensively assessthe quality of different action sequences. Specif-ically, we adopt precision (P.), recall (R.), andmacro-f1 (F1), which are calculated as the mean ofthe corresponding metrics for each case. Detailedexperimental results on the last two datasets can befound in and 17.",
  "higher ratio of correct and a lower ratio of un-executable. Also, it should be noted that Mixtral87B + AUTOSCRAPER can outperform GPT-3.5-Turbo + Reflexion, indicating the superiority of AU-": "TOSCRAPER in the generation of executable actionsequences in the scraper generation task. 2) Mod-els with small parameter sizes have significant dif-ficulties in understanding and writing executablepaths, so they can be considered challenging toapply in this task. On the contrary, large-scalemodels demonstrate a more stable ability in instruc-tion alignment, web structure comprehension, andreflection on execution results. 3) Traditional IEevaluation metrics cannot well describe the successrate of our task. Especially for the precision met-ric, it fails to reveal the performance gap amongdifferent methods with different models. This isbecause the extraction metrics only evaluate theresults that have been extracted, ignoring that unex-ecutable or empty extractions also greatly damagethe executability.",
  ": Comparing LLM direct extraction with AUTO-SCRAPER on the SWDE dataset": "correct ratio increases, while the unexecutable ra-tio decreases. It suggests that the performance ofAUTOSCRAPER can still be further improved byproviding more seed webpages. In addition, theperformance improvement reduces as the numberincreases, which shows that there is an upper limitto improve the performance of AUTOSCRAPER byincreasing the number of seed webpages.",
  "Comparison with LLM Direct Extraction": "Since LLMs can understand human instructionsand webpage text, a natural web information ex-traction solution involves using prompts to guideLLMs to extract target content, which we refer toas direct extraction. We compare direct extractionwith AUTOSCRAPER both in zero-shot settings us-ing each of the LLMs mentioned above. shows that in the direct extraction setting,the extraction performance of all LLMs other thanGPT-4-Turbo is superior to that of AUTOSCRAPER.However, as the capability of LLMs improves, thegap between the two settings narrows. This indi-cates that: 1. While LLMs like Phi-3-medium canunderstand webpage content well (i.e., correctlyextract the expected content), they still struggle tocomprehend webpage structures (i.e., generatingXPath using features like DOM tree). 2. AUTO-SCRAPER, combined with the best current LLMs,already achieves superior extraction performance,and the framework is expected to deliver even bet-ter and more stable performance as LLMs continueto improve.",
  "Comparison with supervised baselines": "To further demonstrate that AUTOSCRAPER isadaptive to different web information extractiontasks, we conduct a comparison with 5 baselinemodels in web information extraction on super-vised learning scenarios: Render-Full (Hao et al.,2011) proposes a complicated heuristic algorithmfor computing visual distances between predictedvalue nodes and adjusting the predictions. Free-DOM (Lin et al., 2020) and SimpDOM (Zhou et al.,2021) encode textual features of DOM tree nodewith LSTM, while MarkupLM (Li et al., 2022) ispre-trained on HTML with text and markup infor-mation jointly. WebFormer (Wang et al., 2022)leverages the web layout for effective attentionweight computation. These models are trained onwebpages in some seed websites and tested on theother websites. shows the result. Although the compar-ison is unfair because our method is in zero-shotsettings, AUTOSCRAPER beat all of them on F1scores. It shows that by designing an appropriateframework, LLMs can surpass supervised learningmethods in some web information extraction tasks.",
  "Efficiency Analysis": "Suppose the number of seed webpages is ns, thenumber of webpages on the same website is NW,the time to generate a wrapper is Tg, the time ofsynthesis is Ts, and the time for extracting infor-mation from a webpage with a wrapper is Te. Thetotal time for extracting all information from allwebsites with AUTOSCRAPER is",
  "T2 = NWTd(4)": "In a real-world scenario, there are many webpages from the same websites to be extracted. Al-though generating a wrapper takes more time thanextracting directly from a single webpage, the ex-traction efficiency of subsequent web pages wouldbe significantly improved. To explore how manywebpages are needed to make AUTOSCRAPERmore efficient in web IE, we calculate the thresholdof NW. Suppose T1 T2, we have",
  "Td Te(6)": "To verify the efficiency advantages of AUTO-SCRAPER in large-scale web information extrac-tion scenarios, we conducted tests on the SWDEdataset. Specifically, we randomly selected a web-site in each of the 10 domains. We repeat 3 timeson AUTOSCRAPER and record the average time toestimate nsTg + Ts and Te. At the same time, werecord the average time Td on 10 web pages withLLM extracting directly. We calculate the thresh-old of NW following the Equation 6 and show themin . It can be observed that the thresholdof the page numbers is 19.5 on average, which issignificantly lower than the average number of webpages per site in SWDE dataset.",
  "Error Analysis": "We perform an analysis by looking at the recordedaction sequence of AUTOSCRAPER with GPT-4-Turbo and identify the following common failuremodes. We mainly focus on the cases categorizedas unexecutable, over-estimate, and else. Non-generalizability of webpagesThe target in-formation and corresponding webpage structuresexhibit variations across different webpages, lead-ing to a lack of generalizability in AUTOSCRAPER(i.e., the inability to apply the same rules across allwebpages in the same website). For instance, forthe task \"Please extract the name of the companyoffering the job\" in the website job-careerbuilder,most webpages contain the company name, butthere is one webpage where the company name is\"Not Available\" on another node of DOM tree. Miss in multi-valuedPresented with the taskof generating a scraper for extracting address inrestaurant webpages or contact phone number fromuniversity websites, the target information is lo-cated in multiple locations in the webpage, suchas the information bar, title, etc. Although AU-",
  "Conclusion": "In this paper, we introduce the scraper generationtask and the paradigm that combines LLMs andscrapers to improve the reusability of the currentlanguage-agent-based framework. We then pro-pose AUTOSCRAPER , a two-phase framework in-cluding progressive generation and synthesis mod-ule to generate a more stable and executable ac-tion sequence. Our comprehensive experimentsdemonstrate that AUTOSCRAPER can outperformthe state-of-the-art baseline in the scraper genera-tion task.",
  "We introduce a paradigm that combines LLMs withscrapers for web scraper generation tasks and pro-pose AUTOSCRAPER to generate an executable ac-tion sequence with progressively understanding the": "HTML documents. Though experimental resultsshow the effectiveness of our framework, there arestill some limits to our work.First, our framework is restricted to the paradigmin the information extraction task for vertical webpages.LLMs with scrapers provide high effi-ciency in open-world web IE tasks, but can hardlytransfer to existing web environments such asMind2Web (Deng et al., 2023), WebArena (Zhouet al., 2023).Second, our framework relies on the perfor-mance of backbone LLMs. Enhancing LLMs abil-ity to understand HTML is a very valuable researchquestion, including corpus collection and trainingstrategy. We will research HTML understandingenhancement in future work.",
  "We hereby declare that all authors of this article areaware of and adhere to the provided ACL Code ofEthics and honour the code of conduct": "Use of Human AnnotationsHuman annotationsare only utilized in the early stages of methodologi-cal research to assess the feasibility of the proposedsolution. All annotators have provided consent forthe use of their data for research purposes. Weguarantee the security of all annotators throughoutthe annotation process, and they are justly remuner-ated according to local standards. Human annota-tions are not employed during the evaluation of ourmethod. RisksThe datasets used in the paper have beenobtained from public sources and anonymized toprotect against any offensive information. Thoughwe have taken measures to do so, we cannot guar-antee that the datasets do not contain any sociallyharmful or toxic language. Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan,Jyoti Aneja, Ahmed Awadallah, Hany Awadalla,Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jian-min Bao, Harkirat Behl, Alon Benhaim, MishaBilenko, Johan Bjorck, Sbastien Bubeck, Qin Cai,Martin Cai, Caio Csar Teodoro Mendes, WeizhuChen, Vishrav Chaudhary, Dong Chen, DongdongChen, Yen-Chun Chen, Yi-Ling Chen, Parul Chopra,Xiyang Dai, Allie Del Giorno, Gustavo de Rosa,Matthew Dixon, Ronen Eldan, Victor Fragoso, DanIter, Mei Gao, Min Gao, Jianfeng Gao, Amit Garg,Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, JamieHuynh, Mojan Javaheripi, Xin Jin, Piero Kauff-mann, Nikos Karampatziakis, Dongwoo Kim, Ma-houd Khademi, Lev Kurilenko, James R. Lee, Yin TatLee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Li-den, Ce Liu, Mengchen Liu, Weishung Liu, Eric Lin,Zeqi Lin, Chong Luo, Piyush Madan, Matt Mazzola,Arindam Mitra, Hardik Modi, Anh Nguyen, BrandonNorick, Barun Patra, Daniel Perez-Becker, ThomasPortet, Reid Pryzant, Heyang Qin, Marko Radmi-lac, Corby Rosset, Sambudha Roy, Olatunji Ruwase,Olli Saarikivi, Amin Saied, Adil Salim, Michael San-tacroce, Shital Shah, Ning Shang, Hiteshi Sharma,Swadheen Shukla, Xia Song, Masahiro Tanaka, An-drea Tupini, Xin Wang, Lijuan Wang, Chunyu Wang,Yu Wang, Rachel Ward, Guanhua Wang, PhilippWitte, Haiping Wu, Michael Wyatt, Bin Xiao, CanXu, Jiahang Xu, Weijian Xu, Sonali Yadav, Fan Yang,Jianwei Yang, Ziyi Yang, Yifan Yang, Donghan Yu,Lu Yuan, Chengruidong Zhang, Cyril Zhang, Jian-wen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang,Yunan Zhang, and Xiren Zhou. 2024. Phi-3 technicalreport: A highly capable language model locally onyour phone.",
  "Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen,Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su.2023. Mind2web: Towards a generalist agent for theweb": "Pankaj Gulhane,Amit Madaan,Rupesh Mehta,JeyashankherRamamirtham,RajeevRastogi,Sandeep Satpal, Srinivasan H Sengamedu, AshwinTengli, and Charu Tiwari. 2011. Web-scale infor-mation extraction with vertex. In 2011 IEEE 27thInternational Conference on Data Engineering,pages 12091220. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, KaiDong, Wentao Zhang, Guanting Chen, Xiao Bi,Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wen-feng Liang. 2024. Deepseek-coder: When the largelanguage model meets programming the rise ofcode intelligence. Izzeddin Gur, Hiroki Furuta, Austin Huang, MustafaSafdari, Yutaka Matsuo, Douglas Eck, and Aleksan-dra Faust. 2023. A real-world webagent with plan-ning, long context understanding, and program syn-thesis.",
  "Qiang Hao, Rui Cai, Yanwei Pang, and Lei Zhang. 2011": "From one tree to a forest: a unified solution for struc-tured web data extraction. In Proceedings of the 34thInternational ACM SIGIR Conference on Researchand Development in Information Retrieval, SIGIR11, page 775784, New York, NY, USA. Associationfor Computing Machinery. Albert Q. Jiang, Alexandre Sablayrolles, AntoineRoux, Arthur Mensch, Blanche Savary, ChrisBamford, Devendra Singh Chaplot, Diego de lasCasas, Emma Bou Hanna, Florian Bressand, Gi-anna Lengyel, Guillaume Bour, Guillaume Lam-ple, Llio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,Sophia Yang, Szymon Antoniak, Teven Le Scao,Thophile Gervet, Thibaut Lavril, Thomas Wang,Timothe Lacroix, and William El Sayed. 2024. Mix-tral of experts.",
  "Nicholas Kushmerick. 1997. Wrapper induction forinformation extraction. University of Washington": "Junlong Li, Yiheng Xu, Lei Cui, and Furu Wei. 2022.Markuplm: Pre-training of text and markup languagefor visually rich document understanding. In Pro-ceedings of the 60th Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 60786087. Bill Yuchen Lin, Ying Sheng, Nguyen Vo, and SandeepTata. 2020. Freedom: A transferable neural architec-ture for structured information extraction on web doc-uments. In Proceedings of the 26th ACM SIGKDDInternational Conference on Knowledge Discovery& Data Mining, pages 10921102.",
  "OpenAI. 2024. Gpt-4o mini: advancing cost-efficientintelligence": "Baptiste Rozire, Jonas Gehring, Fabian Gloeckle, StenSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,Jingyu Liu, Romain Sauvestre, Tal Remez, JrmyRapin, Artyom Kozhevnikov, Ivan Evtimov, JoannaBitton, Manish Bhatt, Cristian Canton Ferrer, AaronGrattafiori, Wenhan Xiong, Alexandre Dfossez,Jade Copet, Faisal Azhar, Hugo Touvron, Louis Mar-tin, Nicolas Usunier, Thomas Scialom, and GabrielSynnaeve. 2024. Code llama: Open foundation mod-els for code. Ritesh Sarkhel, Binxuan Huang, Colin Lockard, andPrashant Shiralkar. 2023.Self-training for label-efficient information extraction from semi-structuredweb-pages. Proceedings of the VLDB Endowment,16(11):30983110.",
  "Theodore R Sumers, Shunyu Yao, Karthik Narasimhan,and Thomas L Griffiths. 2023.Cognitive ar-chitectures for language agents.arXiv preprintarXiv:2309.02427": "Gemini Team, Rohan Anil, Sebastian Borgeaud,Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, RaduSoricut, Johan Schalkwyk, Andrew M. Dai, and AnjaHauth. 2023. Gemini: A family of highly capablemultimodal models. TsaoneSwaabowThapelo,MolaletsaNamoshe,Oduetse Matsebe, Tshiamo Motshegwa, and Mary-Jane Morongwa Bopape. 2021. Sasscal websapi: Aweb scraping application programming interface tosupport access to sasscals weather data. Data Sci-ence Journal, 20:2424. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Qifan Wang, Yi Fang, Anirudh Ravula, Fuli Feng, Xiao-jun Quan, and Dongfang Liu. 2022. Webformer: Theweb-page transformer for structure information ex-traction. In Proceedings of the ACM Web Conference2022, pages 31243133.",
  "A.1Main results on EXTENDED SWDE": "Because EXTENDED SWDE dataset focuses onOpenIE task (the relation is also expected to be ex-tracted), we first map relations into a predefined listof attributes and remove unusual ones. Specifically,we conducted experiments with 294 attributes from21 websites selected from the EXTENDED SWDEdataset. shows the result. By comparing ,we find that: 1) Under complex extraction task set-tings (multiple target values and ambiguous prob-lem description), the closed-source LLMs performbetter in generating executable action sequencescompared to the open-source LLMs. 2) There aresome tasks with unclear descriptions, such as the\"Calendar System\" and \"Facilities and ProgramsOffered\" on university websites, which affect thewrapper generation performance of all methods.",
  "A.2Main results on DS1": "Due to DS1 only contains 166 hand-crafted web-pages, and for each website, there are only twowebpages, so we take one webpage for inferenceand the other for evaluation. Meanwhile, due to thenumber of seed websites equal to one, we test threemethods without applying the synthesis moduledescribed in .3. shows the result in the DS1 dataset.Among all LLMs with three methods, GPT-4-Turbo + AUTOSCRAPER achieves the best perfor-mance, and AUTOSCRAPER beats the other twomethods in all LLMs, which is consistent with ourconclusion.",
  "A.3Generate with Golden Label": "To better illustrate the effectiveness of our frame-work in generating executable action sequences,we compare the performance of COT, Reflexion,and AUTOSCRAPER , while answering the instruc-tion. By offering the same extraction targets, wecan effectively detect the performance of differentframeworks in generating action sequences. shows experimental results, from whichwe can have the following observations: 1) Ourproposed progressive understanding frameworkstill effectively enhances the models performanceunder this setting; 2) LLMs still suffer in accu-rately understanding web page contents with semi-structured markup languages, which illustrate theperformance gap between and ;",
  "B.1Comparison with COT & Reflexion": "more intuitively shows the specific dif-ferences between different baselines in the exper-iment. The most significant difference betweenAUTOSCRAPER and other methods lies in whetherthe hierarchical structure of web pages is utilizedto help LLMs reduce the difficulty of complex webstructures. COT only executes one turn while theother executes multiple turns and can learn fromthe failed execution of the wrapper. Compared tothe Reflexion method, AUTOSCRAPER employstop-down and step-back operations to prune theDOM tree during each XPath generation process,thereby reducing the length of the web page. Incontrast, the Reflexion method can only reflect andregenerate after producing an unexecutable XPath,which does not effectively simplify the webpage.",
  "B.2Further Study with AUTOSCRAPER": "The length of the action sequence is dependenton the LLM capability.To comprehensively ex-plore the performance of different LLMs in under-standing web page structure, we explore the impactof models on the number distribution of the steps.In particular, we collect all the action sequencesand calculate the average steps of AUTOSCRAPERwith different LLMs. The experimental result isreported in , 11 and 12.We observe that AUTOSCRAPER with strongerLLMs generates fewer lengths of action sequence.AUTOSCRAPER with GPT-4-Turbo generates 1.57steps on average, while the AUTOSCRAPER withPhi-3-medium generates 3.62 steps on average.This phenomenon can be interpreted as more pow-erful models having a better understanding of theweb page hierarchical structure, thus being ableto accurately output the appropriate XPaths inlonger/deeper web pages, thereby reducing thenumber of steps. XPath fragility within AUTOSCRAPERThefragility of XPath often refers to the characteristicof XPath expressions becoming ineffective or inac-curately matching the target element when facedwith new web pages. This is mainly due to XPath",
  "specifying specific information through predicates,such as text, @class, etc": "We mainly focus on the fragility of text becausethese webpages are from the same websites (i.e.@class is a good characteristic for generatingstable action sequences). shows XPathexpressions that rely on text. We aim to explorethe reusability of generating XPath based on textfeatures. We manually calculated the proportionof bad cases with two types of predicates, containsand equal 6. The results in show that thestronger LLMs capability, the lower the proportionof bad cases with AUTOSCRAPER . However, itshould be noted that the current SoTA LLM GPT-4-Turbo still suffers from an XPath fragility problem,which indicates that relying entirely on LLMs togenerate reliable XPath still has some distance togo."
}