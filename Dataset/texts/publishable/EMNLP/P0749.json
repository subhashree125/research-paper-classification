{
  "Abstract": "Ensuring the verifiability of model answers is afundamental challenge for retrieval-augmentedgeneration (RAG) in the question answering(QA) domain. Recently, self-citation prompt-ing was proposed to make large language mod-els (LLMs) generate citations to supportingdocuments along with their answers. However,self-citing LLMs often struggle to match therequired format, refer to non-existent sources,and fail to faithfully reflect LLMs context us-age throughout the generation. In this work,we present MIRAGE Model Internals-basedRAG Explanations a plug-and-play approachusing model internals for faithful answer attri-bution in RAG applications. MIRAGE detectscontext-sensitive answer tokens and pairs themwith retrieved documents contributing to theirprediction via saliency methods. We evaluateour proposed approach on a multilingual ex-tractive QA dataset, finding high agreementwith human answer attribution. On open-endedQA, MIRAGE achieves citation quality and effi-ciency comparable to self-citation while also al-lowing for a finer-grained control of attributionparameters. Our qualitative evaluation high-lights the faithfulness of MIRAGEs attributionsand underscores the promising application ofmodel internals for RAG answer attribution.1",
  "Introduction": "Retrieval-augmented generation (RAG) with largelanguage models (LLMs) has become the de-factostandard methodology for Question Answering(QA) in both academic (Lewis et al., 2020b; Izac-ard et al., 2022) and industrial settings (Dao and Le,2023; Ma et al., 2024). This approach was shownto be effective at mitigating hallucinations and pro-ducing factually accurate answers (Petroni et al.,2020; Lewis et al., 2020a; Borgeaud et al., 2022;",
  "*Equal contribution.1Code and data released at": ": MIRAGE is a model internals-based answer at-tribution framework for RAG settings. Context-sensitiveanswer spans (in color) are detected and matched withcontextual cues in retrieved sources to evaluate the trust-worthiness of models answers. Ren et al., 2023). However, verifying whetherthe model answer is faithfully supported by theretrieved sources is often non-trivial due to thelarge context size and the variety of potentially cor-rect answers (Krishna et al., 2021; Xu et al., 2023).In light of this issue, several answer attribution2",
  "We use the term answer attribution (AA) when referringto the task of citing relevant sources to distinguish it from thefeature attribution methods used in MIRAGE": "2021; Bohnet et al., 2022; Muller et al., 2023). Ini-tial efforts in this area employed models trainedon Natural Language Inference (NLI) to automatethe identification of supporting documents (Bohnetet al., 2022; Yue et al., 2023). However, beingbased on an external validator, this approach doesnot faithfully explain the answer generation processbut simply identifies plausible sources supportingmodel answers in a post-hoc fashion. Followingrecent progress in the instruction-following abili-ties of LLMs, self-citation (i.e. prompting LLMsto generate inline citations alongside their answers)has been proposed to mitigate the training and in-ference costs of external validator modules (Gaoet al., 2023a). However, self-citation performanceis hindered by the imperfect instruction-followingcapacity of modern LLMs (Mu et al., 2023; Liuet al., 2023), and resulting attributions are still pre-dicted in an unintelligible, post-hoc fashion. This isan important limitation for these approaches, sincethe primary goal of answer attribution should beto ensure that the LLM is not right for the wrongreasons (McCoy et al., 2019).In light of these considerations, in this work weintroduce MIRAGE, an extension of the context-reliance evaluation PECORE framework (Sartiet al., 2024) that employs model internals for ef-ficient and faithful answer attributions. MIRAGEfirst identifies context-sensitive tokens in a gener-ated sentence by measuring the shift in LM predic-tive distribution caused by the added input context.Then, it attributes this shift to specific influential to-kens in the context using gradient-based saliency orother feature attribution techniques (Madsen et al.,2022). We adapt this approach to the RAG setup bymatching context-dependent generated sentences toretrieved documents that contribute to their predic-tion and converting the resulting pairs to citationsusing the standard answer attribution (AA) format.We begin our assessment of MIRAGE on the short-form XOR-AttriQA dataset (Muller et al., 2023),showing high agreement between MIRAGE resultsand human annotations across several languages.We then test our method on the open-ended ELI5dataset (Fan et al., 2019), achieving AA qualitycomparable to or better than self-citation while en-suring a higher degree of control over attributionparameters. In summary, we make the followingcontributions:",
  "Background and Related Work": "In RAG settings, a set of documents relevant to auser query is retrieved from an external dataset andinfilled into an LLM prompt to improve the gen-eration process (Petroni et al., 2020; Lewis et al.,2020a). Answer attribution (Rashkin et al., 2021;Bohnet et al., 2022; Muller et al., 2023) aims toidentify which retrieved documents support thegenerated answer (answer faithfulness, Gao et al.,2023b), e.g., by exploiting the similarity betweenmodel outputs and references.3 Simplifying accessto relevant sources via answer attribution is a funda-mental step towards ensuring RAG trustworthinessin customer-facing scenarios (Liu et al., 2023).",
  "Answer Attribution Methods": "Entailment-based Answer AttributionBohnetet al. (2022) and Muller et al. (2023) propose toapproximate human AA annotations with NLI sys-tems such as TRUE (Honovich et al., 2022), us-ing a source document as premise and an LLM-generated sentence as entailment hypothesis. AAsproduced by these systems were shown to corre-late strongly with human annotations, promptingtheir adoption in AA studies (Muller et al., 2023;Gao et al., 2023a). Despite their effectiveness,entailment-based methods can be computationallyexpensive when several answer sentence-documentpairs are present. Moreover, this setup assumes theNLI model ability to robustly detect entailment re-lations across all domains and languages for whichthe LLM generator is used. In practice, however,NLI systems were shown to be brittle in challeng-ing scenarios, exploiting shallow heuristics (Mc-Coy et al., 2019; Nie et al., 2020; Sinha et al., 2021;Luo et al., 2022), and require dedicated effortsfor less-resourced settings (Conneau et al., 2018).",
  "For example, NLI may fail to correctly attributeanswers in multi-hop QA settings when consider-ing individual documents as premises (Yang et al.,2018; Welbl et al., 2018)": "Self-citationGao et al. (2023a) is a recent AAapproach exploiting the ability of recent LLMs tofollow instructions in natural language (Raffel et al.,2020; Chung et al., 2022; Sanh et al., 2022; Brownet al., 2020), thereby avoiding the need for an ex-ternal validator. Nakano et al. (2021) and Menicket al. (2022) propose citation fine-tuning for LLMs,while Gao et al. (2023a) instruct general-purposeLLMs to produce inline citations in a few-shot set-ting. Self-citation answers are generally more rel-evant to the provided sources contents, but canstill contain unsupported statements and inaccu-rate citations (Liu et al., 2023). In our preliminaryanalysis, we find that self-citation often misses rel-evant citations, uses wrong formats, or refers tonon-existing documents (). For the ELI5dataset (Fan et al., 2019), we find that LLaMA 2 7BChat (Touvron et al., 2023) and Zephyr 7B (Tun-stall et al., 2023) fail to produce AAs matchingthe prompt instructions for the majority of gener-ated sentences, with almost all answers having atleast one unattributed sentence when the Gao et al.(2023a) self-citation setup is used ().",
  "Attribution Faithfulness": "Answer Attribution can be UnfaithfulTheaforementioned approaches do not account for at-tributions faithfulness, i.e. whether the selecteddocuments influence the LLM during the genera-tion. Indeed, the presence of an entailment relationor high semantic similarity does not imply that theretrieved document had an influence on the answergeneration process. This can be true in cases whereLLMs may rely on memorized knowledge whileignoring relevant, albeit unnecessary, contextualinformation. Even in the case of self-citation, re-cent work showed that, while the justifications ofself-explaining LLMs appear plausible, they gener-ally do not align with their internal reasoning pro-cess (Atanasova et al., 2023; Madsen et al., 2024;Agarwal et al., 2024; Randl et al., 2024), with littleto no predictive efficacy (Huang et al., 2023). Bycontrast, approaches based on model internals aredesigned to faithfully reflect input importance inmotivating model predictions. For instance, Al-ghisi et al. (2024) explore the use of gradients-based attribution to locate salient history segmentsfor various dialogical tasks.Concurrent to our work, Phukan et al. (2024)and Cohen-Wang et al. (2024) have proposed otherinternals-based methods for granular AA of LLMgenerations. While the two-step approaches pro-posed in both works are similar to MIRAGE, theyalso differ in substantial ways. Notably, Phukanet al. (2024) derive attributions from embeddingsimilarity, which does not capture the functional in-fluence of context usage during the generation pro-cess. CONTEXTCITE (Cohen-Wang et al., 2024)instead fits a linear surrogate model to estimate theimpact of ablating context segments over down-stream answer probabilities. While this procedureapproximates causal context influence, it still re-quires a sufficiently large context and many LLMforward passes to learn the surrogate model4, ulti-mately providing a coarser attribution for the fullgenerated output. On the contrary, MIRAGE effi-ciently estimates generated tokens requiring attribu-",
  "Authors suggest a minimum of 32 different ablations": ": Illustration of MIRAGEs two-step approach adapted from PECORE for RAG answer attribution. Step1: CTI detects context-sensitive tokens in the generation (e.g. smaller). Step 2: CCI attributes the generation ofdetected tokens back to context tokens (e.g. few in Doc promotes the generation of smaller instead of PC) usingcontrastive feature attribution. Token pairs are then aggregated into sentence-document citations for practical usage.",
  "tion via contrastive metrics to produce granular at-tributions at the token level, limiting computationsto estimate how context impacts LLM predictions": "Feature Attribution in InterpretabilityThetask of faithfully identifying salient context infor-mation has been studied extensively in the NLP in-terpretability field (Ferrando et al., 2024). In partic-ular, post-hoc feature attribution approaches (Mad-sen et al., 2022) exploit information sourced frommodel internals, e.g., attention weights or gradientsof next-word probabilities, to identify input tokensplaying an important role towards the models pre-diction. While feature attribution studies in NLPtypically focused on classification tasks (Atanasovaet al., 2020; Wallace et al., 2020; Chrysostomouand Aletras, 2022), recent work applies these meth-ods to evaluate context usage in language genera-tion (Yin and Neubig, 2022; Ferrando et al., 2023;Sarti et al., 2023, 2024). Importantly, feature at-tribution techniques are designed to maximize thefaithfulness of selected context tokens by accessingmodels intermediate computations, as opposed tothe AA methods of .1. While the faithful-ness of such approaches can still vary depending onmodels and tasks, the development of robust andfaithful methods is an active area of research (Ja-covi and Goldberg, 2020; Chan et al., 2022; Bast-ings et al., 2022; Lyu et al., 2024).5 5A maximally faithful AA approach would ablate all pos-sible combinations of context elements to counterfactuallyestimate their importance towards model predictions. Giventhe long-form answers and contexts in RAG settings, thisis practically unfeasible. Even if based on approximations,internals-based approaches such as MIRAGE are intrinsicallymore faithful than external validators like NLI, since theyaim to exploit information functional to the predictive process",
  "Method": "Identifying which generated spans were most in-fluenced by preceding information is a key chal-lenge for LM attribution. The Model Internals-based RAG Explanations (MIRAGE) method wepropose is an extension of the Plausibility Eval-uation for Context Reliance (PECORE) frame-work (Sarti et al., 2024) for context-aware machinetranslation. Importantly, this framework requiresopen-weights access to the LLM generator, whichis a strict but necessary requirement to provide anaccurate overview of the actual context usage dur-ing generation (Casper et al., 2024). This sectionprovides an overview of PECOREs two-step pro-cedure (illustrated in ) and clarifies how",
  "Step 1: Context-sensitive TokenIdentification (CTI)": "For every token in an answer sentence y=y1, . . . , yn generated by a LM prompted witha query q and a context c = c1, . . . , c|c|, a con-trastive metric m such as KL divergence (Kullbackand Leibler, 1951) is used to quantify the shift inthe LM predictive distribution at the i-th genera-tion step when the context is present or absent (P ictxor P ino-ctx). Resulting scores m = m1, . . . , mnreflect the context sensitivity of every generatedtoken and can be converted into binary labels usinga selector function sCTI:",
  "Step 2: Contextual Cues Imputation(CCI)": "For every context-sensitive token yi identified byCTI, a contrastive alternative y\\ciis produced byexcluding c from the prompt, but using the origi-nal generated prefix y<i. Then, contrastive featureattribution (Yin and Neubig, 2022) is used to ob-tain attribution scores ai = ai1, . . . , ai|c| for everycontext token cj c:",
  "aij =jp(yi) p(y\\ci ), cj c }(2)": "where j is the L2 norm of the gradient vector overthe input embedding of context token cj, and bothprobabilities are computed from the same contex-tual inputs (q, c, y<i). Intuitively, this procedureidentifies which tokens in c influence the predictionof yi while accounting for the non-contextual op-tion y\\ci . Resulting scores are once again binarizedwith a selector sCCI:",
  "CTI FilteringTo obtain discrete labels from theCTI step, we set sCTI(mi) = mi m, where m": "is a threshold value for selecting context-sensitivegenerated tokens. In this work, we experimentwith two variants of m: a calibrated thresholdmCAL obtained by maximizing agreement betweenthe contrastive metric and human annotations on acalibration set with human AA annotations, and anexample-level threshold mEX using only within-example scores to avoid the need of calibration data.Following Sarti et al. (2024), we set mEX = m +m, where m and m are the average and standarddeviation of m scores for generated tokens. CCI FilteringTo extract granular document cita-tions (i.e., colored spans with document indices in), we set sCCI = aij ai, where ai is ei-ther the Top-K or Top-% highest attribution value inai, to filter attributed context tokens cj CCI(yi).Then, we use the identifier docid(cj) of the docu-ments they belong to as citation indices for context-sensitive token yi. Highlights for consecutive to-kens citing the same documents are collated into",
  "Agreement with Human AnswerAttribution Annotations": "We begin our evaluation by comparing MIRAGEpredictions to human-produced answer attributions.Importantly, our aim is not to compare several AAapproaches to claim optimal faithfulness, but ratherevaluate how our proposed framework fares againstexisting approaches at the task of producing answerattributions from model internals. We employ theXOR-AttriQA dataset (Muller et al., 2023), which,to our knowledge, is the only open dataset with hu-man annotations over RAG outputs produced by apublicly accessible LM.6 We limit our assessmentto open-weights LLMs to ensure that MIRAGE an-swer attribution can faithfully reflect the modelsinner processing towards the natural production ofthe annotated answer used for evaluation.7 More-over, while cross-linguality is not the focus of ourwork, XOR-AttriQA allows us to assess the robust-ness of MIRAGE across several languages and itsagreement with human annotations compared to anentailment-based system.",
  "Experimental Setup": "DatasetThe ELI5 dataset contains open-endedwhy/how/what queries q from the Explain LikeIm Five subreddit13 eliciting long-form multi-sentence answers.For our evaluation, we usethe RAG-adapted ELI5 version by Gao et al.(2023a), containing top-5 matching documentsc = doc1, . . . , doc5 retrieved from a filtered ver-sion of the Common Crawl (Sphere; Piktus et al.,2021) for every query. The answer attribution taskis performed by generating a multi-sentence answerans = y1, . . . , ym with an LLM using (q, c) asinputs, and identifying documents in c supportingthe generation of answer sentence yi, yi ans. Models and Answer Attribution ProcedureWeselect LLaMA 2 7B Chat (Touvron et al., 2023)and Zephyr 7B (Tunstall et al., 2023) for our ex-periments since they are high-quality open-sourceLLMs of manageable size. To enable a fair com-parison between the tested attribution methods, wefirst generate answers with inline citations usingthe self-citation prompt by Gao et al. (2023b).14 Then, we remove citation tags and use MIRAGE toattribute the resulting answers to retrieved docu-ments. This process ensures that citation qualityis compared over the same set of answers, control-ling for the variability that could be produced bya different prompt.15 For more robust results, weperform generation three times using different sam-pling seeds, and report the averaged scores. Sincehuman-annotated data is not available, we only as-sess the calibration-free MIRAGE EX. Entailment-based EvaluationDifferently fromthe XOR-AttriQA dataset used in , ELI5does not contain human annotations of AA. For thisreason, and to ensure consistency with Gao et al.(2023a)s self-citation assessment, we adopt theTRUE model as a high-quality approximation ofexpected annotation behavior. Despite the potentialOOD issues of entailment-based AA highlightedin , we expect TRUE to perform well onELI5 since it closely matches the general/scientificknowledge queries in TRUEs fine-tuning corporaand contains only English sentences. To overcomethe multi-hop issue when using single documentsfor entailment-based answer attribution, we followthe ALCE evaluation (Gao et al., 2023a)16 to mea-sure citation quality as NLI precision and recall(summarized by F1 scores) over the concatenationof retrieved documents.",
  "Top 5%81.780.189.284.481.883.4 / 3.2": ": Agreement % of MIRAGE and entailment-based baselines with human AA on XOR-AttriQAmatch usingCORA for RAG. Extra Requirements: data/models needed for AA in addition to the RAG model and the currentexample. Filter: sCCI filtering for saliency scores. Best overall and best uncalibrated scores are highlighted. setup (CORA; Asai et al., 2021).8 Queries and doc-uments span five languages (Bengali (BN), Finnish(FI), Japanese (JA), Russian (RU), and Telugu(TE)), with no constraint on documents to matchthe language of the query.9 Although the RAG gen-erator employs a set of retrieved documents duringgeneration, human annotators were asked to labeltuples (q, doci, y) to indicate whether the informa-tion in doci supports the generation of y. Impor-tantly, MIRAGE requires extracting model internalsin the naturalistic setting that leads to the genera-tion of the desired answer, i.e., the one assessedby human annotators. Hence, we perform a selec-tion procedure to identify XOR-AttriQA exampleswhere the answer produced by filling in the con-catenated documents c in the LM prompt matchesthe one provided. The resulting subset, which wedub XOR-AttriQAmatch, contains 142/1144 calibra-tion/test examples and is used for our evaluation.10",
  "Entailment-based Baselines": "Muller et al. (2023) use an mT5 XXL model fine-tuned on NLI for performing answer attributionon XOR-AttriQA. Since neither the tuned modelnor the tuning data are released, we opt to useTRUE11 (Honovich et al., 2022), a fine-tuned T511B model (Raffel et al., 2020), which was shownto highly overlap with human annotation on En-glish answer attribution tasks (Muller et al., 2023;Gao et al., 2023a). We evaluate TRUE agreementwith human annotation in two setups. In NLI ORIG,we evaluate the model directly on all examples,including non-English data. While this leads theEnglish-centric TRUE model out-of-distribution,it accounts for real-world scenarios with noisydata, and can be used to assess the robustness 8 practice, Muller et al., 2023 report that most retrieveddocuments are in the same language as the query or in English.10See Appendix A for more details on this selection. Ap-pendix B presents experiments on the full XOR-AttriQA.11 of the method in less-resourced settings. Instead,in NLIMT, all queries and documents are machinetranslated to English using the Google TranslateAPI.12 While this simplifies the task by ensuringall TRUE inputs are in English, it can lead to infor-mation loss caused by imprecise translation.",
  "Results and Analysis": "MIRAGE agrees with human answer attribution presents our results. MIRAGE is found tolargely agree with human annotations on XOR-AttriQAmatch, with scores on par or slightly betterthan those of the ad-hoc NLIMT system augmentedwith automatic translation. Although calibrationappears to generally improve MIRAGEs agreementwith human annotators, we note that the uncali-brated MIRAGE EX achieves strong performancesdespite having no access to external modules ortuning data. These findings confirm that the innerworkings of LMs can be used to perform answerattribution, resulting in performances on par withsupervised answer attribution approaches even inthe absence of annotations for calibration. MIRAGE is robust across languages and filter-ing procedures shows that NLI ORIG an-swer attribution performances are largely language-dependent due to the unbalanced multilingual abil-ities of the TRUE NLI model. This highlightsthe brittleness of entailment-based approaches inOOD settings, as discussed in .1. Instead, MIRAGE variants perform similarly across all lan-guages by exploiting the internals of the multilin-gual RAG model. MIRAGEs performance acrosslanguages is comparable to that of NLI MT, whichrequires an extra translation step to operate on En-glish inputs.We further validate the robustness of the CCI fil-tering process by testing percentile values betweenTop 3-100% for the MIRAGE EX setting. Agreement (%)",
  "Answer Attribution for Long-form QA": "XOR-AttriQA can only provide limited insights forreal-world answer attribution evaluation since itsexamples are sourced from Wikipedia articles, andits answers are very concise. In this section, weextend our evaluation to ELI5 (Fan et al., 2019),a challenging long-form QA dataset that was re-cently employed to evaluate LLM self-citation ca-pabilities (Gao et al., 2023a). Different from XOR-AttriQA, ELI5 answers are expected to containmultiple sentences of variable length, making it es-pecially fitting to assess MIRAGE context-sensitivetoken identification capabilities before documentattribution. Alongside our quantitative assessmentof MIRAGE in relation to self-citation baselines, weconduct a qualitative evaluation of the disagree-ment between the two methods.",
  "Results": "Results in show that MIRAGE provides asignificant boost in answer attribution precisionand recall for the Zephyr model, while it greatlyimproves citation recall at the expense of precisionfor LLaMA 2, resulting in an overall higher F1score for the MIRAGE EX Top 5% setting. Theseresults confirm that MIRAGE can produce effectiveanswer attributions in longer and more complex 14The full prompt is provided in Appendix D ().15For completeness, we also report MIRAGE results withoutself-citation prompting in Appendix D.16ALCE is an evaluation framework for RAG, evaluatingLLM responses in terms of citation quality, correctness, andfluency. More details can be found in Appendix C",
  ": Attribution scores over retrieved documentstokens for the prediction of context-sensitive token 9": "settings while employing no external resources likethe self-citation approach.From the comparison between Top 3 and Top 5%CCI filtering strategies, we note that the latter gen-erally results in better performance. This intuitivelysupports the idea that an adaptive selection strategyis more fitting to accommodate the large variabil-ity of attribution scores across different examples. visualizes the distributions of attributionscores aij for an answer produced by Zephyr ,showing that most context tokens in retrieved doc-uments receive low attribution scores, with only ahandful of them contributing to the prediction ofthe context-sensitive token 9 in the generation.This example also provides an intuitive explanationof the robustness of Top-% selection thresholdsdiscussed in .3. Ultimately, the Top 5%threshold is sufficient to select the document con-taining the direct mention of the generated token.Since the mEX threshold used to select context-sensitive tokens by MIRAGE EX depends on themean and standard deviation of generated answersscores, we expect that the length of the generatedanswer might play a role in citation quality. Asshown in , MIRAGE citation quality is in-deed lower for shorter answer sentences. However, Percentage (%) Sentence length (#Tokens) 79% 100%",
  ": MIRAGE EX (top) and self-citation (bottom)average performance on ELI5 answer sentences binnedby length. red: % of sentences with 1 citation": "a similar trend is observed for self-citation, whichis outperformed by MIRAGE for all but the short-est length bin ( 10 tokens). The proportion ofnon-attributed sentences (red line) suggests that thelower quality could be a byproduct of the ALCEevaluation protocol, where non-attributed sentencesreceive 0 precision/recall. Future availability ofhuman-annotated RAG datasets may shed morelight on this effect.",
  "Qualitative Analysis of Disagreements": "To better understand MIRAGEs performance, weexamine some ELI5 examples where MIRAGE dis-agrees with self-citation on Zephyr s generations. and 5 illustrate two cases in which theentailment-based TRUE model results agree witheither MIRAGE or self-citation. In , the an-swer provided by the model is directly supportedby Document , as also identified by TRUE. How-ever, self-citation fails to cite the related documentat the end of the two sentences. By contrast, MI- RAGE attributes several spans to Document ,resulting in the correct answer attribution for bothsentences.While TRUE achieves high consistency with hu-man judgment (e.g., for the example in ),NLI-based AA can still prove unreliable in cases ofhigh lexical overlap between the answer and sup-porting documents. illustrates one suchcase, where both self-citation and TRUE attributethe answer to Document , whereas MIRAGEdoes not label any context document as salientfor the answer. Here, the answer wrongly statesthat the bar code can used to prevent the alarm,while Document mentions that the code canbe used to cancel the alarm after an accidental",
  "INPUT: PROMPT + RETRIEVED DOCS (N=5) + QUERY": "Document (Title: Why Do Airlines Sell More TicketsThan The Aircrafts Capacity? Science ABC): board theaircraft. They know that some people simply wont showup for their appointments (just like at hospitals, hotels,restaurants etc.). To account for this discrepancy, wherethe flight would have to fly with some very expensive seatsempty, they decide to opt for a more profitable strategy.Assuming that out of the maximum capacity of 200 seatsonly 185 show up for the flight, this leaves 15 seats avail-able. The airline could try to sell these seats quickly toavoid losses, but this rarely happens. What the airline doesinstead is try to sell 220 seats in advance.Document [...]Document [...]Document (Title: Why Do Airlines Sell More TicketsThan The Aircrafts Capacity? Science ABC): yourown interests, as the airlines have clearly looked aftertheirs. First of all, if the airline is offering you a seat ona different flight, check if that seat is definitely available.If it is also full and it looks like you might be bumpedyet again, you might as well be stranded! If the airlineis offering compensation, such as free meals, hotel costs,and transportation between the airport and the hotel, lookfor the most profitable outcome. The airline might offeryou vouchers for future flights, but since airline employeesare given guidelines for negotiation, you mightDocument [...]",
  ": Example of self-citation failure using Zephyr on ELI5. NLI and MIRAGE produce the correct citation,while self-citation does not cite any document ([])": "activation. Thus, despite the high lexical and se-mantic relatedness, the answer is not supported byDocument . The failure of TRUE in this set-ting highlights the sensitivity of entailment-basedsystems to surface-level similarity, making thembrittle in cases where the models context usage isnot straightforward. Using another sampling seedfor the same query produces the answer [...] theindividual can cancel the alarm by providing theirpassword at the keypad, which MIRAGE correctlyattributes to Document .17",
  "Conclusion": "In this study, we introduced MIRAGE, a novel ap-proach to enhance the faithfulness of answer at-tribution in RAG systems. By leveraging modelinternals, MIRAGE effectively addresses the limita-tions of previous methods based on prompting orexternal NLI validators. Our experiments demon-strate that MIRAGE produces outputs that stronglyagree with human annotations while being moreefficient and controllable than its counterparts. Ourqualitative analysis shows that MIRAGE can pro-duce faithful attributions that reflect actual context",
  "Limitations": "LLMs Optimized for Self-citationIn this study,we focus our analysis on models that are not explic-itly trained to perform self-citation and can providecitations only when prompted to do so. While re-cent systems include self-citation in their optimiza-tion scheme for RAG applications18, we believeincorporating model internals in the attribution pro-cess will remain a valuable and inexpensive methodto ensure faithful answer attributions. Brittleness of NLI-based EvaluationFollow-ing Gao et al. (2023a), the evaluation of employs the NLI-based system TRUE due to thelack of AA-annotated answers produced by open-source LLMs. However, using the predictions ofNLI models as AA references is far from ideal inlight of their brittleness in challenging scenariosand their tendency to exploit shallow heuristics.While the ELI5 dataset is reasonably in-domain forthe TRUE model, this factor might still underminethe reliability of some of our quantitative evalua-",
  "For example, the Command-R models:": "tion results. Future work should produce a widervariety of annotated datasets for reproducible an-swer attribution using open-source LLMs, enablingus to extend our analysis to a broader set of lan-guages and model sizes and ultimately enhance therobustness of our findings. Applicability to Other Domains and ModelsOur evaluation is conducted on relatively homoge-neous QA datasets and does not include languagemodels with >7B parameters. This limits the gen-eralizability of our findings to other domains andlarger models. Future work should extend our anal-ysis to a broader range of domains and model sizesto further validate the robustness and applicabilityof MIRAGE. This said, we expect MIRAGE to be lessvulnerable to language and quality shifts comparedto existing AA methods that depend on externalvalidators or on the models instruction-followingabilities. Scalability of MIRAGE on Longer ContextThecomputational cost for the simple gradient-basedversion of MIRAGE proposed in this work is2O(F)+|CTI(y)|O(B), where O(F), O(B) arerespectively the costs of a forward and a backwardpass with the LLM, and |CTI(y)| is the number oftokens selected by the CTI step. While CTI effec-tively limits the expensive backward componentin the MIRAGE computation, its cost is bound toincrease significantly for larger models and contextsizes. When applying MIRAGE to LLMs with <10Bparameters, we note that its cost can be comparableor lower to supervised models like TRUE, requir-ing several forward passes using a large 11B LLM.Importantly, MIRAGE is a flexible framework thatcan be implemented using different feature attribu-tion methods in the CCI step, including lightweighttechniques requiring only forward passes (e.g., At-tention Rollout (Abnar and Zuidema, 2020), ValueZeroing (Mohebbi et al., 2023), or ALTI-Logit (Fer-rando et al., 2023). Finally, a promising perspec-tive for scaling to larger LLMs could be to assesswhether MIRAGE-produced AAs remain accuratewhen force-decoding the original models answerfrom a different LLM with fewer parameters. MIRAGEs Parametrization and Choice of At-tribution MethodWhile .1 highlightsthe robustness of MIRAGE to various CCI filter-ing thresholds, the method still requires non-trivialparametrization. In particular, we emphasize thatthe choice of the attribution method employed to generate attribution scores in the CCI step can sig-nificantly impact the faithfulness of the resultinganswer attributions. Although we used a relativelysimple gradient-based approach in this study, ourproposed framework is method-agnostic. We leavethe evaluation of modern feature attribution tech-niques, such as the ones mentioned in the previousparagraph, to future work to further improve MI-",
  "RAGE applicability in real-world settings": "The authors have received funding from theDutch Research Council (NWO): JQ is sup-ported by NWA-ORC project LESSEN (grant nr.NWA.1389.20.183), GS is supported by NWA-ORC project InDeep (NWA.1292.19.399), ABis supported by the above as well as NWO Tal-ent Programme (VI.Vidi.221C.009). RF is sup-ported by the European Research Council (ERC)under European Unions Horizon 2020 programme(No. 819455). Samira Abnar and Willem Zuidema. 2020. Quantify-ing attention flow in transformers. In Proceedingsof the 58th Annual Meeting of the Association forComputational Linguistics, pages 41904197, On-line. Association for Computational Linguistics.",
  "Chirag Agarwal, Sree Harsha Tanneru, and HimabinduLakkaraju. 2024. Faithfulness vs. plausibility: On the(un)reliability of explanations from large languagemodels. Preprint, arXiv:2402.04614": "Simone Alghisi, Massimo Rizzoli, Gabriel Roccabruna,Seyed Mahed Mousavi, and Giuseppe Riccardi. 2024.Should we fine-tune or rag? evaluating different tech-niques to adapt llms for dialogue. arXiv e-prints,pages arXiv2406. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-son, Dmitry Lepikhin, Alexandre Passos, SiamakShakeri, Emanuel Taropa, Paige Bailey, ZhifengChen, et al. 2023. Palm 2 technical report. arXivpreprint arXiv:2305.10403. Akari Asai, Xinyan Yu, Jungo Kasai, and Hanna Ha-jishirzi. 2021. One question answering model formany languages with cross-lingual dense passage re-trieval. Advances in Neural Information ProcessingSystems, 34:75477560. Pepa Atanasova, Oana-Maria Camburu, Christina Li-oma, Thomas Lukasiewicz, Jakob Grue Simonsen,and Isabelle Augenstein. 2023. Faithfulness testsfor natural language explanations. In Proceedingsof the 61st Annual Meeting of the Association forComputational Linguistics (Volume 2: Short Papers),",
  "pages 283294, Toronto, Canada. Association forComputational Linguistics": "Pepa Atanasova, Jakob Grue Simonsen, Christina Li-oma, and Isabelle Augenstein. 2020. A diagnosticstudy of explainability techniques for text classifi-cation. In Proceedings of the 2020 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 32563274, Online. Association forComputational Linguistics. Jasmijn Bastings, Sebastian Ebert, Polina Zablotskaia,Anders Sandholm, and Katja Filippova. 2022. willyou find these shortcuts? a protocol for evaluatingthe faithfulness of input salience methods for textclassification. In Proceedings of the 2022 Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 976991, Abu Dhabi, United ArabEmirates. Association for Computational Linguistics. Bernd Bohnet, Vinh Q. Tran, Pat Verga, Roee Aharoni,Daniel Andor, Livio Baldini Soares, Jacob Eisen-stein, Kuzman Ganchev, Jonathan Herzig, Kai Hui,Tom Kwiatkowski, Ji Ma, Jianmo Ni, Tal Schuster,William W. Cohen, Michael Collins, Dipanjan Das,Donald Metzler, Slav Petrov, and Kellie Webster.2022. Attributed question answering: Evaluationand modeling for attributed large language models.ArXiv, abs/2212.08037. Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-mann, Trevor Cai, Eliza Rutherford, Katie Milli-can, George Bm Van Den Driessche, Jean-BaptisteLespiau, Bogdan Damoc, Aidan Clark, DiegoDe Las Casas, Aurelia Guy, Jacob Menick, RomanRing, Tom Hennigan, Saffron Huang, Loren Mag-giore, Chris Jones, Albin Cassirer, Andy Brock,Michela Paganini, Geoffrey Irving, Oriol Vinyals,Simon Osindero, Karen Simonyan, Jack Rae, ErichElsen, and Laurent Sifre. 2022. Improving languagemodels by retrieving from trillions of tokens. InProceedings of the 39th International Conferenceon Machine Learning, volume 162 of Proceedingsof Machine Learning Research, pages 22062240.PMLR. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020.Language models are few-shot learners.In Ad-vances in Neural Information Processing Systems,volume 33, pages 18771901. Curran Associates,Inc.",
  "Stephen Casper, Carson Ezell, Charlotte Siegmann,Noam Kolt, Taylor Lynn Curtis, Benjamin Bucknall,Andreas Haupt, Kevin Wei, Jrmy Scheurer, Marius": "Hobbhahn, Lee Sharkey, Satyapriya Krishna, Mar-vin Von Hagen, Silas Alberti, Alan Chan, Qinyi Sun,Michael Gerovitch, David Bau, Max Tegmark, DavidKrueger, and Dylan Hadfield-Menell. 2024. Black-box access is insufficient for rigorous ai audits. InProceedings of the 2024 ACM Conference on Fair-ness, Accountability, and Transparency, FAccT 24,page 22542272, New York, NY, USA. Associationfor Computing Machinery. Chun Sik Chan, Huanqi Kong, and Liang Guanqing.2022. A comparative study of faithfulness metricsfor model interpretability methods. In Proceedingsof the 60th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 50295038, Dublin, Ireland. Association forComputational Linguistics.",
  "Harrison Chase. 2022. LangChain": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts,Paul Barham, Hyung Won Chung, Charles Sutton,Sebastian Gehrmann, Parker Schuh, Kensen Shi,Sasha Tsvyashchenko, Joshua Maynez, AbhishekRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-odkumar Prabhakaran, Emily Reif, Nan Du, BenHutchinson, Reiner Pope, James Bradbury, JacobAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,Toju Duke, Anselm Levskaya, Sanjay Ghemawat,Sunipa Dev, Henryk Michalewski, Xavier Garcia,Vedant Misra, Kevin Robinson, Liam Fedus, DennyZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,Barret Zoph, Alexander Spiridonov, Ryan Sepassi,David Dohan, Shivani Agrawal, Mark Omernick, An-drew M. Dai, Thanumalayan Sankaranarayana Pil-lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,Rewon Child, Oleksandr Polozov, Katherine Lee,Zongwei Zhou, Xuezhi Wang, Brennan Saeta, MarkDiaz, Orhan Firat, Michele Catasta, Jason Wei, KathyMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,and Noah Fiedel. 2023. Palm: Scaling language mod-eling with pathways. Journal of Machine LearningResearch, 24(240):1113. George Chrysostomou and Nikolaos Aletras. 2022. Anempirical study on explanations in out-of-domainsettings. In Proceedings of the 60th Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 69206938, Dublin,Ireland. Association for Computational Linguistics. Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, Al-bert Webson, Shixiang Shane Gu, Zhuyun Dai,Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh-ery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,Dasha Valter, Sharan Narang, Gaurav Mishra, AdamsYu, Vincent Zhao, Yanping Huang, Andrew Dai,Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-cob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,and Jason Wei. 2022. Scaling instruction-finetunedlanguage models. Preprint, arXiv:2210.11416.",
  "Xuan-Quy Dao and Ngoc-Bich Le. 2023. Chatgpt isgood but bing chat is better for vietnamese students.arXiv preprint arXiv:2307.08272": "Angela Fan, Yacine Jernite, Ethan Perez, David Grang-ier, Jason Weston, and Michael Auli. 2019. ELI5:Long form question answering. In Proceedings ofthe 57th Annual Meeting of the Association for Com-putational Linguistics, pages 35583567, Florence,Italy. Association for Computational Linguistics. Javier Ferrando, Gerard I. Gllego, Ioannis Tsiamas,and Marta R. Costa-juss. 2023. Explaining howtransformers use context to build predictions.InProceedings of the 61st Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 54865513, Toronto, Canada.Association for Computational Linguistics.",
  "Javier Ferrando, Gabriele Sarti, Arianna Bisazza, andMarta R. Costa-juss. 2024. A primer on the in-ner workings of transformer-based language models.Preprint, arXiv:2405.00208": "Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.2023a. Enabling large language models to generatetext with citations. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 64656488, Singapore. Associa-tion for Computational Linguistics. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and HaofenWang. 2023b. Retrieval-augmented generation forlarge language models: A survey. arXiv preprintarXiv:2312.10997. Or Honovich, Roee Aharoni, Jonathan Herzig, HagaiTaitelbaum, Doron Kukliansy, Vered Cohen, ThomasScialom, Idan Szpektor, Avinatan Hassidim, andYossi Matias. 2022. TRUE: Re-evaluating factualconsistency evaluation. In Proceedings of the 2022Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 39053920, Seattle,United States. Association for Computational Lin-guistics.",
  "Workshop: Analyzing and Interpreting Neural Net-works for NLP, pages 317331, Singapore. Associa-tion for Computational Linguistics": "Gautier Izacard, Patrick Lewis, Maria Lomeli, LucasHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and EdouardGrave. 2022.Atlas: Few-shot learning with re-trieval augmented language models. arXiv preprintarXiv:2208.03299. Alon Jacovi and Yoav Goldberg. 2020. Towards faith-fully interpretable NLP systems: How should wedefine and evaluate faithfulness?In Proceedingsof the 58th Annual Meeting of the Association forComputational Linguistics, pages 41984205, On-line. Association for Computational Linguistics.",
  "Solomon Kullback and R. A. Leibler. 1951. On in-formation and sufficiency. Annals of MathematicalStatistics, 22:7986": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, FabioPetroni, Vladimir Karpukhin, Naman Goyal, Hein-rich Kttler, Mike Lewis, Wen-tau Yih, Tim Rock-tschel, Sebastian Riedel, and Douwe Kiela. 2020a.Retrieval-augmented generation for knowledge-intensive nlp tasks. In Proceedings of the 34th Inter-national Conference on Neural Information Process-ing Systems, NIPS20, Red Hook, NY, USA. CurranAssociates Inc. Patrick Lewis, Ethan Perez, Aleksandra Piktus, FabioPetroni, Vladimir Karpukhin, Naman Goyal, Hein-rich Kttler, Mike Lewis, Wen-tau Yih, Tim Rock-tschel, et al. 2020b. Retrieval-augmented generationfor knowledge-intensive nlp tasks. Advances in Neu-ral Information Processing Systems, 33:94599474.",
  "Jerry Liu. 2022. LlamaIndex": "Nelson Liu, Tianyi Zhang, and Percy Liang. 2023. Eval-uating verifiability in generative search engines. InFindings of the Association for Computational Lin-guistics: EMNLP 2023, pages 70017025, Singapore.Association for Computational Linguistics. Cheng Luo, Wei Liu, Jieyu Lin, Jiajie Zou, Ming Xiang,and Nai Ding. 2022. Simple but challenging: Naturallanguage inference models fail on simple sentences.In Findings of the Association for ComputationalLinguistics: EMNLP 2022, pages 34493462, AbuDhabi, United Arab Emirates. Association for Com-putational Linguistics.",
  "Post-hoc interpretability for neural nlp: A survey.ACM Computing Surveys, 55(8)": "Tom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Rightfor the wrong reasons: Diagnosing syntactic heuris-tics in natural language inference. In Proceedings ofthe 57th Annual Meeting of the Association for Com-putational Linguistics, pages 34283448, Florence,Italy. Association for Computational Linguistics. Jacob Menick, Maja Trebacz, Vladimir Mikulik,John Aslanides, Francis Song, Martin Chadwick,Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, et al. 2022. Teachinglanguage models to support answers with verifiedquotes. arXiv preprint arXiv:2203.11147. Hosein Mohebbi, Willem Zuidema, Grzegorz Chrupaa,and Afra Alishahi. 2023. Quantifying context mixingin transformers. In Proceedings of the 17th Confer-ence of the European Chapter of the Associationfor Computational Linguistics, pages 33783400,Dubrovnik, Croatia. Association for ComputationalLinguistics.",
  "Norman Mu,Sarah Chen,Zifan Wang,SizheChen, David Karamardian, Lulwa Aljeraisy, DanHendrycks, and David Wagner. 2023. Can llms fol-low simple rules? arXiv preprint arXiv:2311.04235": "Benjamin Muller, John Wieting, Jonathan Clark, TomKwiatkowski, Sebastian Ruder, Livio Soares, RoeeAharoni, Jonathan Herzig, and Xinyi Wang. 2023.Evaluating and modeling attribution for cross-lingualquestion answering. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 144157, Singapore. Associationfor Computational Linguistics. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,Long Ouyang, Christina Kim, Christopher Hesse,Shantanu Jain, Vineet Kosaraju, William Saunders,et al. 2021.Webgpt: Browser-assisted question-answering with human feedback.arXiv preprintarXiv:2112.09332. Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,Jason Weston, and Douwe Kiela. 2020. AdversarialNLI: A new benchmark for natural language under-standing. In Proceedings of the 58th Annual Meet-ing of the Association for Computational Linguistics,pages 48854901, Online. Association for Computa-tional Linguistics.",
  "Sebastian Riedel. 2020. How context affects lan-guage models factual predictions. In AutomatedKnowledge Base Construction": "Anirudh Phukan, Shwetha Somasundaram, Apoorv Sax-ena, Koustava Goswami, and Balaji Vasan Srinivasan.2024. Peering into the mind of language models:An approach for attribution in contextual questionanswering. Preprint, arXiv:2405.17980. Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,Dmytro Okhonko, Samuel Broscheit, Gautier Izacard,Patrick Lewis, Barlas Oguz, Edouard Grave, Wen-tauYih, et al. 2021. The web is your oyster-knowledge-intensive nlp against a very large web corpus. arXivpreprint arXiv:2112.09924. Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers,John Thickstun, Sean Welleck, Yejin Choi, and ZaidHarchaoui. 2021. Mauve: Measuring the gap be-tween neural text and human text using divergencefrontiers. Advances in Neural Information Process-ing Systems, 34:48164828. Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J Liu. 2020. Exploring the lim-its of transfer learning with a unified text-to-texttransformer. Journal of machine learning research,21(140):167.",
  "Korbinian Randl, John Pavlopoulos, Aron Henriksson,and Tony Lindgren. 2024. Evaluating the reliabilityof self-explanations in large language models. arXivpreprint arXiv:2407.14487": "Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm,Michael Collins, Dipanjan Das, Slav Petrov, Gau-rav Singh Tomar, Iulia Turc, and D. Reitter. 2021.Measuring attribution in natural language generationmodels. Computational Linguistics, 49:777840. Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne XinZhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen,and Haifeng Wang. 2023.Investigating the fac-tual knowledge boundary of large language mod-els with retrieval augmentation.arXiv preprintarXiv:2307.11019. Victor Sanh, Albert Webson, Colin Raffel, StephenBach, Lintang Sutawika, Zaid Alyafeai, AntoineChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,M Saiful Bari, Canwen Xu, Urmish Thakker,Shanya Sharma Sharma, Eliza Szczechla, TaewoonKim, Gunjan Chhablani, Nihal Nayak, DebajyotiDatta, Jonathan Chang, Mike Tian-Jian Jiang, HanWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,Harshit Pandey, Rachel Bawden, Thomas Wang, Tr-ishala Neeraj, Jos Rozen, Abheesht Sharma, An-drea Santilli, Thibault Fevry, Jason Alan Fries, RyanTeehan, Teven Le Scao, Stella Biderman, Leo Gao,Thomas Wolf, and Alexander M Rush. 2022. Multi-task prompted training enables zero-shot task gener-alization. In International Conference on LearningRepresentations. Gabriele Sarti, Grzegorz Chrupaa, Malvina Nissim, andArianna Bisazza. 2024. Quantifying the plausibilityof context reliance in neural machine translation. InThe Twelfth International Conference on LearningRepresentations (ICLR 2024), Vienna, Austria. Open-Review. Gabriele Sarti, Nils Feldhus, Ludwig Sickert, Oskarvan der Wal, Malvina Nissim, and Arianna Bisazza.2023.Inseq: An interpretability toolkit for se-quence generation models. In Proceedings of the61st Annual Meeting of the Association for Compu-tational Linguistics (Volume 3: System Demonstra-tions), pages 421435, Toronto, Canada. Associationfor Computational Linguistics. Koustuv Sinha, Prasanna Parthasarathi, Joelle Pineau,and Adina Williams. 2021. UnNatural LanguageInference. In Proceedings of the 59th Annual Meet-ing of the Association for Computational Linguisticsand the 11th International Joint Conference on Natu-ral Language Processing (Volume 1: Long Papers),pages 73297346, Online. Association for Computa-tional Linguistics. Hugo Touvron, Louis Martin, Kevin R. Stone, PeterAlbert, Amjad Almahairi, Yasmine Babaei, Niko-lay Bashlykov, Soumya Batra, Prajjwal Bhargava,Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cris-tian Cantn Ferrer, Moya Chen, Guillem Cucurull,David Esiobu, Jude Fernandes, Jeremy Fu, WenyinFu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,Naman Goyal, Anthony S. Hartshorn, Saghar Hos-seini, Rui Hou, Hakan Inan, Marcin Kardas, ViktorKerkez, Madian Khabsa, Isabel M. Kloumann, A. V.Korenev, Punit Singh Koura, Marie-Anne Lachaux,Thibaut Lavril, Jenya Lee, Diana Liskovich, YinghaiLu, Yuning Mao, Xavier Martinet, Todor Mihaylov,Pushkar Mishra, Igor Molybog, Yixin Nie, AndrewPoulton, Jeremy Reizenstein, Rashi Rungta, KalyanSaladi, Alan Schelten, Ruan Silva, Eric MichaelSmith, R. Subramanian, Xia Tan, Binh Tang, RossTaylor, Adina Williams, Jian Xiang Kuan, PuxinXu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, An-gela Fan, Melanie Kambadur, Sharan Narang, Aure-lien Rodriguez, Robert Stojnic, Sergey Edunov, andThomas Scialom. 2023. Llama 2: Open foundationand fine-tuned chat models. ArXiv, abs/2307.09288. Lewis Tunstall, Edward Beeching, Nathan Lambert,Nazneen Rajani, Kashif Rasul, Younes Belkada,Shengyi Huang, Leandro von Werra, ClmentineFourrier, Nathan Habib, et al. 2023. Zephyr: Di-rect distillation of lm alignment.arXiv preprintarXiv:2310.16944.",
  "comprehension across documents. Transactions ofthe Association for Computational Linguistics, 6:287302": "Fangyuan Xu, Yixiao Song, Mohit Iyyer, and EunsolChoi. 2023. A critical evaluation of evaluations forlong-form question answering. In Proceedings of the61st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages32253245, Toronto, Canada. Association for Com-putational Linguistics. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale,Rami Al-Rfou, Aditya Siddhant, Aditya Barua, andColin Raffel. 2021. mT5: A massively multilingualpre-trained text-to-text transformer. In Proceedingsof the 2021 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 483498, On-line. Association for Computational Linguistics. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,William Cohen, Ruslan Salakhutdinov, and Christo-pher D. Manning. 2018. HotpotQA: A dataset fordiverse, explainable multi-hop question answering.In Proceedings of the 2018 Conference on Empiri-cal Methods in Natural Language Processing, pages23692380, Brussels, Belgium. Association for Com-putational Linguistics. Kayo Yin and Graham Neubig. 2022. Interpreting lan-guage models with contrastive explanations. In Pro-ceedings of the 2022 Conference on Empirical Meth-ods in Natural Language Processing, pages 184198,Abu Dhabi, United Arab Emirates. Association forComputational Linguistics. Xiang Yue, Boshi Wang, Ziru Chen, Kai Zhang, Yu Su,and Huan Sun. 2023. Automatic evaluation of attri-bution by large language models. In Findings of theAssociation for Computational Linguistics: EMNLP2023, pages 46154635, Singapore. Association forComputational Linguistics.",
  "AConstruction of XOR-AttriQAmatch": "XOR-AttriQAmatch is a subset of the original XOR-AttriQA containing only examples for which ourLLM generation matches exactly the answer anno-tated in the dataset. Replicating the original answergeneration process is challenging since the origi-nal ordering of the documents doci in c unavail-able.19 To maximize the chances of replication, weattempt to restore the original document sequenceby randomly shuffling the order of docis until LLMcan naturally predict the answer y (otherwise, atmost 200 iterations), as shown in Algorithm 1. Thestatistics of the original XOR-AttriQA and XOR-AttriQAmatch are shown in .",
  "MIRAGE CAL (Ours)500 AA calibration ex.82.282.592.087.790.286.9 / 4.0MIRAGE EX (Ours)79.074.190.882.686.982.7 / 5.8": ": Agreement % of MIRAGE and entailment-based baselines with human AA on the full XOR-AttriQAusing CORA for RAG (annotated answers not matching the LMs natural generation are force-decoded). ExtraRequirements: data/models needed for AA in addition to the RAG model itself. Best overall and best validator-freescores are highlighted. PALM and mT5 results are taken from Muller et al. (2023).",
  "BAnswer Attribution on the FullXOR-AttriQA": "Differently from the concatenation setup in Sec-tion 4.1, we also test MIRAGE on the full XOR-AttriQA dataset by constraining CORA generationto match the annotated answer y. We adopt a proce-dure similar to Muller et al. (2023) by consideringa single document-answer pair (doci, y) at a time,and using MIRAGEs CTI step to detect whether yis sensitive to the context doci. Results in show that MIRAGE achieves performances in linewith other AA methods despite these approaches",
  "CALCE Evaluation Benchmark": "Gao et al. (2023a) propose ALCE, an evaluationframework for RAG QA tasks. ALCE assessesthe LLMs response from three diverse aspects:citation quality, correctness, and fluency. Cita-tion quality evaluates the answer attribution perfor-mance with recall and precision scores. The recallscore calculates if the concatenation of the citeddocuments entails the generated sentence. Theprecision measures if each document is cited pre-cisely by verifying if the concatenated text stillentails the generation whenever one of the docu-ments is removed. We further calculate F1 scoresto summarize the overall performance. Correct-ness checks whether the generated answer entailsthe golden reference answer according to the NLImodel TRUE. Gold-reference answers are providedin the original dataset, and some were summarizedby Gao et al. (2023b) by using GPT-4 in case theywere too long.Fluency reflects the coherenceand fluency of the generated response according toMAUVE (Pillutla et al., 2021), a popular NLG met-ric. We report the average score for all instancesfor each evaluation metric.",
  "SELF-CITATION (Gao et al., 2023a)": "Instruction: Write an accurate, engaging, and conciseanswer for the given question using only the providedsearch results (some of which might be irrelevant) andcite them properly. Use an unbiased and journalistictone. Always cite for any factual claim. When citingseveral search results, use . Cite at least onedocument and at most three documents in each sentence.If multiple documents support the sentence, only cite aminimum sufficient subset of the documents.",
  ": Self-citation prompt, taken from Gao et al.,2023a, and standard prompt with no citation instruction": "citation instructions is used (\"Standard\" prompt in). We observe the overall citation qualityof MIRAGE drops when a standard prompt is usedinstead of self-citation instructions. We conjecturethis might be due to answers that are, in general,less attributable to the provided context due to alack of explicit instructions to do so. We also ob-serve higher correctness and fluency in the standardprompt setting, suggesting a trade-off between an-swer and citation quality.",
  ": Example described in .3: MIRAGEattributes the generation to Document when cancelis used instead of prevent ()": "the generated answer becomes the consistent de-scription cancel the alarm as mentioned in Doc-ument . In this case, MIRAGE attributes thissentence to the corresponding Document whileNLI maintains its attribution of Document dueto lexical overlap, as suggested in .3. On several occasions, we observe that MIRAGEattributes all occurrences of lexically similar tokensin the context when the LLM is generating the sameword. For example, in the named entityScience ABC is mentioned in both Document and , and MIRAGE finds both occurrences as",
  "NLI (TRUE model): Only entails the answer sentence": ": Example of counterintuitive MIRAGE attri-bution: the generation is grounded to Document because the named entity Science ABC is made moreprobable by the occurrence of the same named entityScience ABC. However, Document does not logi-cally entail the answer. salient towards the prediction of the same entityin the output. Similarly, in , the gener-ated word Document is attributed to the previousmentions of the same word in the context. In bothcases, when moving from token-level to sentence-level AA, this dependence would result in wrongAA according to NLI, since the documents are notentailing the answer, but rather making a specifictoken more likely. These cases reflect the possiblediscrepancy between AA intended as logical entail-ment and actual context usage during generation.Future work could explore more elaborate ways to"
}