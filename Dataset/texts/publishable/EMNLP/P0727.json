{
  "Abstract": "In recent years, multimodal large languagemodels (MLLMs) have garnered significantattention from both industry and academia.However, there is still considerable debateon constructing MLLM architectures, partic-ularly regarding the selection of appropriateconnectors for perception tasks of varying gran-ularities.This paper systematically investi-gates the impact of connectors on MLLMperformance.Specifically, we classify con-nectors into feature-preserving and feature-compressing types. Utilizing a unified clas-sification standard, we categorize sub-tasksfrom three comprehensive benchmarks, MM-Bench, MME, and SEED-Bench, into three tasktypes: coarse-grained perception, fine-grainedperception, and reasoning, and evaluate theperformance. Our findings reveal that feature-preserving connectors excel in fine-grained per-ception tasks due to their ability to retain de-tailed visual information. In contrast, feature-compressing connectors, while less effective infine-grained perception tasks, offer significantspeed advantages and perform comparably incoarse-grained perception and reasoning tasks.These insights are crucial for guiding MLLMarchitecture design and advancing the optimiza-tion of MLLM architectures.",
  "Introduction": "Large language models (LLMs) have made sig-nificant advances in recent years, demonstratingremarkable capabilities in understanding and gen-erating text (Brown et al., 2020; Su et al., 2022;Jiang et al., 2023; Bai et al., 2023; Touvron et al.,2023; Zhang et al., 2024; Su et al., 2024). Recentlymultimodal large language models (MLLMs) haveemerged as a hot topic in both academia and in-dustry due to their potential to handle multiplemodalities, such as text and vision, in a unifiedframework (Wang et al., 2023; Alayrac et al., 2022;",
  "*Corresponding Author": "Gao et al., 2024). However, training a unified ar-chitecture from scratch across different modalitiesis often resource-intensive and time-consuming,making it feasible only for a limited number oflarge companies with substantial computational re-sources (Team, 2024; Zhou et al., 2024). As aresult, researchers commonly adopt a connector-based approach, which fully leverages the exist-ing powerful capabilities of a pre-trained languagemodel (Li et al., 2023b; Liu et al., 2024b). Thisconnector bridges the gap by transforming visualinformation from the encoder into vector represen-tations that the LLM can process and understand.Through this method, the pre-trained text-basedLLM is empowered to perceive and interpret visualdata, enabling it to perform a wide range of visualtasks without requiring a complete retraining of themodel from scratch (Yin et al., 2023).Designing an optimal MLLM architecture re-mains a crucial and intriguing research area. Whileprior studies (Karamcheti et al., 2024; McKinzieet al., 2024; Laurencon et al., 2024) have inves-tigated various factors affecting MLLM perfor-mance, a significant gap persists in the detailedexamination of the key component: the connector.We categorize connectors into two types:feature-preserving connectors, which retain vi-sual feature details by maintaining patch numbers,and feature-compressing connectors, which re-duce computational load by abstracting patchesinto a specified number. Different studies have con-flicting views: Lin et al. (2023); Chen et al. (2024)contends that feature-compressing connectors aresuitable for coarse-grained perception tasks but per-form weakly on fine-grained perception tasks. Incontrast, McKinzie et al. (2024) observes little dif-ference between the two. Although these studiesprovide experimental evidence, further explorationis needed to understand how connectors performacross varying perception granularities.To address this gap, this paper aims to meticu-",
  "Resolution 224Resolution 448Resolution 336": ": Comparison of radar chart performance at 224, 336, and 448 resolutions across coarse-grained perception,fine-grained perception, and reasoning tasks on MMBench. Each task includes four sub-tasks: Image Quality, ImageScene, Image Style, and Image Topic for coarse-grained perception; Action Recognition, Celebrity Recognition,Object Localization, and OCR for fine-grained perception; and Function Reasoning, Identity Reasoning, SocialRelation, and Structuralized Image-Text Understanding for reasoning tasks. lously investigate the effects of various connectorson tasks of different perception granularities. Build-ing on the construction guidelines of MMBench(Liu et al., 2023), we evaluate the impact of con-nectors on the performance of MLLMs across threetask types: coarse-grained perception, fine-grainedperception, and reasoning. Our extensive experi-ments thoroughly explore connector performanceacross these varying perception granularities. Sev-eral noteworthy conclusions are drawn from testingon three multitask benchmarks. shows theperformance of different connectors on the threetasks as well as the corresponding sub-tasks. Ourmain contributions are summarized as follows: 1. We conduct a comprehensive analysis of dif-ferent connectors from multiple perspectives,including loss curves, compressed token num-ber, image resolution, and performance met-rics across tasks of different granularities. 2. We demonstrate that feature-compressing con-nectors significantly underperform in fine-grained perception tasks compared to feature-preserving connectors, while maintainingcomparable performance in coarse-grainedperception tasks. 3. We analyze the impact of different poolingmethods within feature-compressing connec-tors, revealing that simpler pooling methodsgenerally lead to more effective training andbetter overall performance.",
  "Related Work": "Connectors play a crucial role in aligning multi-modal data within MLLMs, with various types.Based on whether the patch number of visual fea-tures is retained or reduced, we classify connectorsinto two categories: feature-preserving connectorsand feature-compressing connectors.LLaVA (Liuet al., 2024b) employs a single-layer linear pro-jection as its connector, whereas LLaVA-1.5 (Liuet al., 2024a) enhances this design by adding aGELU activation function and an extra linear pro-jection. These feature-preserving connectors aredesigned to retain the details of visual features.Emu2 (Sun et al., 2024) uses a local average pool-ing strategy to standardize visual features into a uni-form number of patches. BLIP-2 (Li et al., 2023b)utilizes the Q-Former, a cross-attention connec-tor that uses a fixed number of learnable queriesto interact with visual features, enabling globalweighted pooling. HoneyBee (Cha et al., 2024)introduces the C-Abstractor, which utilizes convo-lutional neural networks to perform local weightedpooling based on Emu2s local average pooling,effectively extracting local features. These feature-compressing connectors adjust the length of featurepatch number, thereby optimizing computationalresources while reserving key information. To explore the impact of various componentsand parameters in MLLMs on performance, numer-ous studies have been conducted. Karamcheti et al.(2024) rigorously investigates MLLMs along keydesign axes, such as optimization procedures, im- age processing, pretrained visual representations,language models, and scaling properties. MM1(McKinzie et al., 2024) aim to identify impor-tant design principles and lessons for constructingMLLMs through comprehensive ablation studieson architecture components, data choices, and train-ing procedures. Additionally, Idefics2 (Laurenconet al., 2024) conducts extensive experiments aroundpre-trained models, architecture choice, data, andtraining methods to bring experimental clarity tocore design choices in building MLLMs.Their findings offer useful initial insights butrequire a more detailed analysis of task-specificperformance for greater depth and applicability.Specifically, they lack a comprehensive examina-tion of tasks with varying granularities, such ascoarse-grained perception, fine-grained perception,and reasoning (Liu et al., 2023).To address this, our paper thoroughly investi-gates the effects of various connectors on MLLMs,focusing on their performance across the aforemen-tioned tasks. This comprehensive analysis aims todeepen our understanding of connectors impactand guide targeted connector selection in futuremodel design stages based on specific tasks.",
  "Preliminaries": "Multimodal large language models (MLLMs) gen-erally consist of three key components: a visualencoder E, a connector C, and an LLM. For agiven visual input V , the encoder E extracts visualfeatures f RPdv, where P is the number of vi-sual patches and dv is the channel dimension. Theconnector C, which is a crucial component, thenaligns these visual features with the word embed-ding space as follows:",
  "Feature-preserving connectors maintain the patchnumber of visual features (i.e., P = Q) and are": "typically composed of components such as linearlayers and activation layers. While they can retaindetailed information, the computational complexityof the model grows exponentially with the visualtoken number. Existing feature-preserving connec-tors can be classified into linear and nonlinear typesbased on whether they include nonlinear operations.For example, the connector in LLaVA (Liu et al.,2024b) can be classified into linear type because itonly contains a linear layer, as shown below:",
  "x = Wf(2)": "where W RdvD is a trainable projection matrix,that maps the visual features f RPdv to theword embedding space. The connector used inLLaVA-1.5 (Liu et al., 2024a) can be classifiedinto the nonlinear type because it incorporates anactivation function and an additional linear layeron top of the basic linear type, as shown below:",
  "Feature-Compressing Connector": "Feature-compressing connectors reduce the num-ber of visual tokens Q through various strategies,aiming to preserve visual information while opti-mizing computational efficiency. Based on MM1(McKinzie et al., 2024), we categorize feature-compressing connectors into three types: averagepooling, attention pooling, and convolutional map-ping, as shown in . They generally operatein two steps. The first step involves using a poolingoperation P to reduce the patch number P of visualfeature f to Q (Q < P) as follows:",
  "Learnable": "queries : The structure of different visual-language connectors. The upper part of the figure shows the overallstructure of various connectors, while the lower part provides a simplified visualization during compression. (a)The Average Pooling-based connector compresses features by averaging visual tokens within local windows (b)The Attention Pooling-based connector uses cross-attention between learnable queries and visual tokens to abstractvisual tokens into a certain number of compressed tokens. Each compressed token is derived from all visual tokenswith weighted contributions. (c) The Convolutional Mapping-based connector uses convolution operations toenhances local context modeling while reducing the number of tokens. Each compressed token is derived from thevisual tokens within local windows with weighted contributions.",
  "j=1f(i1)n+j(6)": "where fv,i represents the i-th averaged featurepatch in f and f(i1)n+j represents the j-th featurepatch in the i-th group of f. After obtaining thecompressed visual features f, we directly applythe connector from LLaVA-1.5 as the transforma-tion T to project f into the word embedding space. Attention PoolingThis type of connector usescross-attention as P to reduce the number of tokens.The patch number P is compressed by performingcross-attention between a set of learnable queriesQ RQdc and the visual features f, resultingin f RQdc, where dc is the hidden size ofthe cross-attention. The cross-attention mechanismcan be formulated as follows:",
  "where K, V RPdc are the key and value ma-trices obtained by projecting the visual features f": "using the projection matrices Wk, Wv Rdvdc,respectively. A RQP is the attention weightmatrix, and Aij represents the attention weight be-tween the i-th query and the j-th visual feature.The compressed visual feature fi is obtained byweighted summation of the value vectors Vj. Afterobtaining the compressed visual features f, thetransformation T is consistent with the approachused in average pooling connector. Convolutional MappingThis type of connec-tor uses a combination of convolutional layers andaverage pooling as P to reduce the number of to-kens. The patch number P is compressed by firstapplying convolutional layers followed by averagepooling, resulting in f RQdv, where dv isthe channel dimension of the visual features. Thetransformation T is then applied using additionalconvolutional layers to project the compressed vi-sual features into the word embedding space. Theoverall process can be formulated as follows:",
  "layers and average pooling are represented as alocal weighted average, denoted by Wj": "CharacteristicsAverage pooling is a simpleand efficient feature-compressing connector thatquickly reduces patch numbers without addingany parameters, making it easy to train. How-ever, it may lead to the loss of local information,reducing its effectiveness in fine-grained percep-tion tasks. Attention pooling, utilizing a globalweighted mechanism, retains more global informa-tion and offers a higher theoretical performancelimit. Despite this, it has higher computationalcomplexity and the most additional parameters dueto the learnable queries and projection matrices,making it the most challenging to train. Further-more, it may struggle with fine-grained tasks be-cause the attention mechanism can find it difficultto preserve local image information (Dosovitskiyet al., 2020; Park and Kim, 2022). Convolutionalmapping effectively preserves local details and in-volves a moderate number of parameters, strikinga balance between parameter efficiency and theability to capture fine-grained details. However, itlacks the capability to intuitively capture global fea-tures. These characteristics are intuitive, but theiractual effectiveness and trade-offs need to be em-pirically validated through extensive experimentsacross tasks with varying perception granularities.",
  "Perception Granularity": "The partition criterion for coarse-grained and fine-grained perception vary across different bench-marks. For example, MMBench categorizes taskslike Image Style and Image Scene under coarse-grained perception, which focuses on the globalattributes and overall context of the image, whiletasks like Object Localization fall under fine-grained perception, focusing on the local detailsand specific features within the image. MME alsodifferentiates between coarse-grained perceptionand fine-grained perception tasks, but its criteria focus more on testing the knowledge resources ofMLLM, rather than the perspective of spatial scope.For instance, the task of Object Localizationin MME is considered a coarse-grained perceptiontask and Scene Recognition is classified as a fine-grained perception task. However, in MMBench,they will be divided into coarse-grained perceptiontask and fine-grained perception task, respectively. further illustrates this discrepancy. Theleft image is selected from the Color sub-task,which is categorized as a coarse-grained percep-tion task in MME. However, it actually focuses onlocal image details, which would reclassify it as afine-grained perception task in MMBench. Con-versely, the right image is selected from the Scenesub-task, which is categorized as a fine-grainedperception task in MME, but it actually focuseson the overall context of the image, making it acoarse-grained perception task in MMBench. Parent Task: Coarse-grained perceptionSub-Task: colorQuestion: Is there a skateboard with redwheels in the image? Please answer yes or no. Parent Task: Fine-grained perceptionSub-Task: sceneQuestion: Is this photo taken in a place ofhangar indoor? Please answer yes or no.",
  ": Examples of conflicting partition criterion forperception granularity in the MME benchmark": "Many methods exhibit conflicting views on theability of different connectors to handle differentgranularities (Lin et al., 2023; Chen et al., 2024;McKinzie et al., 2024). To explore this issue, basedon MMBench, we define coarse-grained percep-tion as the ability to perceive image-level features,such as overall concepts and context. In contrast,fine-grained perception refers to object-level de-tails within the image, such as identifying specific features of individual objects. By analyzing per-formance in coarse-grained and fine-grained per-ception tasks, we can determine whether feature-preserving connectors and feature-compressingconnectors excel in specific perception tasks. Ad-ditionally, by examining reasoning tasks, we canmore accurately assess the impact of different typesof connectors on the models ability to understandand integrate multimodal information.",
  "Benchmarks": "To explore and evaluate various types of connec-tors, we utilize three well-established benchmarkswith sub-task labels: MMBench (Liu et al., 2023),MME (Fu et al., 2023), and SEED-Bench (Li et al.,2023a). We reference the coarse-grained and fine-grained perception tasks defined above to reclassifythe sub-tasks of MME and SEED-Bench. Detailedinformation on the sub-tasks reclassification canbe found in , , and in Ap-pendix A.",
  "Implementation Details": "Given the focus of this paper on comparing con-nectors, we largely adhere to the configuration ofLLaVA-1.5, with exceptions for connector modifi-cations and using LLaMA 2 (Touvron et al., 2023)as the LLM. The visual encoder utilized is CLIPViT-L/14 (Radford et al., 2021) with resolutionsof 224 and 336. We keep the learning rate, batchsize, training phases, and data usage consistentwith LLaVA-1.5. For images with a resolution of448, we refer to MM1 and employ position embed-ding interpolation to adapt CLIP ViT-L/14 from aresolution of 336 to 448. Considering that LoRA-based LLaVA-1.5 performs on par with the fullyfine-tuning setting across the three benchmarks, weopt for the LoRA approach (Hu et al., 2021) tosave computational resources. Refer to inAppendix B for detailed connector configurations.",
  "Effects of Feature-Preserving Connector": "To assess the performance of feature-preservingconnectors, we compare the linear type (referred toas the linear connector) with the nonlinear type (re-ferred to as the two-layer MLP connector) acrossthree tasks: coarse-grained perception, fine-grainedperception, and reasoning. As shown in ,using two-layer MLP connector consistently out-perform the linear connector in all task groups at aresolution below 448. 67.5 70.0 72.5 75.0 Coarse-grained Perception Fine-grained Perception",
  ": Comparison of two-layer MLP and linearconnectors on coarse-grained, fine-grained perception,and reasoning tasks at resolutions of 224, 336, and 448": "When the resolution is increased to 448, al-though the linear connector performs on par withthe two-layer MLP connector in fine-grained per-ception, it suffers a substantial performance drop inother tasks, particularly in reasoning. We hypothe-size that a linear mapping may struggle balancingboth perception and reasoning at high resolution. Incontrast, the reasoning ability is further enhancedwhen using two-layer MLP with higher resolution.",
  "Impact of Compressed Token Number": "The compressed token number Q is an importantparameter. We compare two widely used values:64 and 144. We fix the resolution at 336 and evalu-ate average pooling, Q-Former, and C-Abstractoracross three tasks. The results are shown in Fig-ure 5. It can be seen that while 144 tokens generallyprovide a slight improvement in performance over64 tokens, the difference is not substantial, indicat-ing that both 64 and 144 tokens are adequate forrobust image information extraction.Additionally, to further demonstrate the impactof the compressed token number, we present theloss curves of different feature-compressing con-nectors with different compressed token numbersduring the pretraining and finetuning stages in Fig-ure 6 in Appendix C.1. It can be observed that theloss curves from 144 tokens show marginally betterconvergence than those from 64 tokens, especially Average poolingQ-FormerC-Abstractor0 C - 64 TokensC - 144 Tokens F - 64 TokensF - 144 Tokens R - 64 TokensR - 144 Tokens",
  "Impact of Image Resolution": "We further explore the effect of image resolution onthe metric performance of feature-preserving andfeature-compressing connectors. Detailed experi-ments and analyses, as shown in , reveal thatincreasing the resolution from 224 to 336 enhancesperformance across all connector types for the threetasks, with the most significant improvements ob-served in fine-grained perception tasks, followedby coarse-grained perception tasks, and the leastimprovement in reasoning tasks. However, furtherincreasing the resolution from 336 to 448 yieldsonly marginal performance gains. Specifically, forfeature-preserving connectors, the resolution in-crease from 224 to 336 results in improvements of12.6% in fine-grained perception, 2.5% in coarse-grained perception, and 2.3% in reasoning tasks.For feature-compressing connectors, the improve-ments are 13.9%, 9.2%, and 4.3%, respectively.When the resolution is increased from 336 to 448,the performance changes for the former are 2.5%,0.2%, and 0.6%, while for the latter, the changesare -0.5%, -1.0%, and 0.9%. We attribute this to thediminishing returns of higher resolutions and thecurrent insufficient training data to support them. in Appendix C.1 clearly illustrates theloss curves of all connectors at different resolutions.It can be seen that in most cases, increasing theresolution from 224 224 to 336 336 generally",
  "C-Abstractor22464.9349.3355.0533674.1263.1159.3944873.0562.6260.31": ": Comparison of two-layer MLP, average poolingwith 144 tokens, C-Abstractor with 144 tokens, and Q-Former with 144 tokens on coarse-grained perception(C), fine-grained perception (F), and reasoning (R) tasks. results in a decrease in training loss. However,when the resolution is further increased from 336 336 to 448 448, only the fine-tuning loss of thetwo-layer MLP decreases, while the others eitherremain unchanged or increase. This observation isconsistent with the evaluation metrics.",
  "Effects of Feature-CompressingConnector": "To explore the impact of different feature-compressing connectors on model performance, weconduct a detailed comparison on the three tasksunder the settings of 448448 resolution and 144compressed token number, as shown in .Overall, the performance of average pooling and C-Abstractor is similar, while Q-Former performs sig-nificantly worse. Specifically, for coarse-grainedperception tasks, Q-Former does not show as largea performance gap compare to other connectors asit does in fine-grained perception tasks. This mightbe because, the self-attention mechanism disruptsthe original positional information which is impor- tant in fine-grained perception tasks. However, thisdoes not fully explain why Q-Former also performspoorly in coarse-grained perception tasks. To ex-plain this phenomenon, we present the training losscurves at in Appendix C.1. The curvesshow that Q-Formers loss decreases more slowlythan the losses from other connectors. This indi-cates that Q-Former is more challenging to train,likely due to insufficient training data to supportsuch a complex mechanism.In summary, simple average pooling suffices formost tasks as LLMs can implicitly extract imageinformation from visual tokens. Extensive interfer-ence in token extraction at the projector stage isnot necessary. Complex connectors like Q-Formermay require more aligned data for better results.",
  "Suggestions for Connector Selection": "In , we extensively discuss the perfor-mance of different connectors across various tasks.To consider efficiency and effectiveness simulta-neously during the connector selection phase, wepresent the training times for models under differ-ent connectors in . It was observed thatwith increasing image resolution, the training costsfor feature-compressing connectors change onlyslightly, whereas those for feature-preserving con-nectors significantly increase.Based on the above evidences, we offer severalrecommendations for choosing connectors: 1. At an image resolution of 224, using a two-layer MLP is advisable as it significantly out-performs other connectors across the threetasks while maintaining a acceptable compu-tational resource demand. 2. At an image resolution of 336, if the focus ison coarse-grained perception and reasoningtasks, the C-Abstractor and average poolingare recommended for their balance betweenefficiency and effectiveness. If fine-grainedperception tasks are a priority, the two-layerMLP may be more suitable. 3. At an image resolution of 448, the token countfor the two-layer MLP reaches 1024, whichleads to excessive consumption of computa-tional resources. Under these circumstances,C-Abstractor and average pooling 144tksemerge as more optimal choices. Specifically,the C-Abstractor reduces the training time by80% in the pre-training stage and 51% in the fine-tuning stage compared to the two-layerMLP. This drastic reduction in training timenot only makes the C-Abstractor and averagepooling connectors more efficient but also sig-nificantly lowers the computational cost, mak-ing them highly suitable for scenarios withlimited resources. The substantial decrease intraining time at this high resolution highlightsthe importance of choosing the right connectorto balance performance and resource usage.",
  "Conclusion": "In this paper, we conduct extensive experi-ments to evaluate commonly used connectors inMLLMs.Our findings indicate that althoughfeature-preserving connectors generally offer thebest performance, their advantage over feature-compressing connectors diminishes as resolutionincreases, while their computational costs rise expo-nentially. Among feature-compressing connectors,average pooling and C-Abstractor outperform Q-Former, consistently delivering better results acrossall resolutions and task granularities. Our resultsclearly demonstrate that the choice of connectordepends on the resolution, task granularity, andcomputational budget. Based on these findings, weoffer guidance on selecting connectors to balanceboth effectiveness and efficiency.",
  "In alignment with the base configuration of LLaVA-1.5, our approach involves using positional encod-": "ing interpolation to scale images from 336x336to 448x448, rather than employing a visual en-coder that natively supports the 448x448 resolution.This method may lead to suboptimal results. An-other limitation is that our training data also comesfrom LLaVA-1.5, resulting in a relatively small to-tal number of training samples, only 1.23 M. Incontrast, the InstructBLIP (Dai et al., 2024) withuse Q-Former as connector has 130.2 M trainingsamples. This significant difference in the numberof training samples might render our conclusionsinapplicable in scenarios with a large volume oftraining data. In the future, we could explore thisdiscrepancy using a larger set of training samples.",
  "Acknowledgement": "We sincerely thank the reviewers of this work fortheir constructive and insightful feedback.Wethank Xingluan (AI Cloud computing service), EITand IDT High Performance Computing Center forproviding computational resources for this project. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,Antoine Miech, Iain Barr, Yana Hasson, KarelLenc, Arthur Mensch, Katherine Millican, MalcolmReynolds, et al. 2022. Flamingo: A visual languagemodel for few-shot learning. Advances in NeuralInformation Processing Systems, 35:2371623736. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,and Jingren Zhou. 2023.Qwen-vl: A versatilevision-language model for understanding, localiza-tion, text reading, and beyond.arXiv preprintarXiv:2306.01595. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in Neural Information ProcessingSystems, 33:18771901. Junbum Cha, Wooyoung Kang, Jonghwan Mun, andByungseok Roh. 2024.Honeybee:Locality-enhanced projector for multimodal llm. In Proceed-ings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 1381713827. Gongwei Chen, Leyang Shen, Rui Shao, Xiang Deng,and Liqiang Nie. 2024. Lion: Empowering multi-modal large language model with dual-level visualknowledge. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition,pages 2654026550. Wenliang Dai, Junnan Li, Dongxu Li, AnthonyMeng Huat Tiong, Junqi Zhao, Weisheng Wang,Boyang Li, Pascale N Fung, and Steven Hoi.2024. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advancesin Neural Information Processing Systems, 36. AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,Dirk Weissenborn,Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, MatthiasMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.An image is worth 16x16 words: Transformersfor image recognition at scale.arXiv preprintarXiv:2010.11929. Chaoyou Fu, Peixian Chen, Yunhang Shen, YuleiQin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, WeiLin, Jinrui Yang, and Xiawu Zheng. 2023. Mme:A comprehensive evaluation benchmark for multi-modal large language models.In arXiv preprintarXiv:2306.13394. Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu,Siyuan Huang, Weifeng Lin, Shitian Zhao, ShijieGeng, Ziyi Lin, Peng Jin, et al. 2024.Sphinx-x: Scaling data and parameters for a family ofmulti-modal large language models. arXiv preprintarXiv:2402.05935. Yash Goyal, Tejas Khot, Douglas Summers-Stay, DhruvBatra, and Devi Parikh. 2017. Making the v in vqamatter: Elevating the role of image understandingin visual question answering. In Proceedings of theIEEE Conference on Computer Vision and PatternRecognition, pages 69046913. Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo,Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey PBigham. 2018. Vizwiz grand challenge: Answeringvisual questions from blind people. In Proceedingsof the IEEE Conference on Computer Vision andPattern Recognition, pages 36083617.",
  "Edward J Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,and Weizhu Chen. 2021.Lora: Low-rank adap-tation of large language models.arXiv preprintarXiv:2106.09685": "Drew A Hudson and Christopher D Manning. 2019.Gqa: A new dataset for real-world visual reasoningand compositional question answering. In Proceed-ings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 67006709. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023. Mistral7b. arXiv preprint arXiv:2310.06825. Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna,Percy Liang, Thomas Kollar, and Dorsa Sadigh.2024. Prismatic vlms: Investigating the design spaceof visually-conditioned language models.arXivpreprint arXiv:2402.07865. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten,and Tamara Berg. 2014. Referitgame: Referring toobjects in photographs of natural scenes. In Proceed-ings of the 2014 conference on empirical methods innatural language processing (EMNLP), pages 787798.",
  "Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang,Wayne Xin Zhao, and Ji-Rong Wen. 2023c. Eval-uating object hallucination in large vision-languagemodels. In arXiv preprint arXiv:2305.10355": "Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, LongtianQiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao,Keqin Chen, et al. 2023. Sphinx: The joint mix-ing of weights, tasks, and visual embeddings formulti-modal large language models. arXiv preprintarXiv:2311.07575. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong JaeLee. 2024a. Improved baselines with visual instruc-tion tuning. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition,pages 2629626306.",
  "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong JaeLee. 2024b. Visual instruction tuning. Advances inNeural Information Processing Systems, 36": "Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,Songyang Zhang, Wangbo Zhao, Yike Yuan, JiaqiWang, Conghui He, Ziwei Liu, et al. 2023. Mm-bench: Is your multi-modal model an all-aroundplayer? arXiv preprint arXiv:2307.06281. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, PeterClark, and Ashwin Kalyan. 2022. Learn to explain:Multimodal reasoning via thought chains for sciencequestion answering. Advances in Neural InformationProcessing Systems, 35:25072521. Brandon McKinzie, Zhe Gan, Jean-Philippe Faucon-nier, Sam Dodge, Bowen Zhang, Philipp Dufter,Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers,et al. 2024. Mm1: Methods, analysis & insightsfrom multimodal llm pre-training. arXiv preprintarXiv:2403.09611.",
  "Namuk Park and Songkuk Kim. 2022.How dovision transformers work?arXiv preprintarXiv:2202.06709": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,et al. 2021.Learning transferable visual modelsfrom natural language supervision. In InternationalConference on Machine Learning, pages 87488763.PMLR. Amanpreet Singh,Vivek Natarajan,Meet Shah,Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,and Marcus Rohrbach. 2019. Towards vqa modelsthat can read. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition,pages 83178326.",
  "Chameleon Team. 2024. Chameleon: Mixed-modalearly-fusion foundation models.arXiv preprintarXiv:2405.09818": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Weihan Wang, Qingsong Lv, Wenmeng Yu, WenyiHong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, LeiZhao, Xixuan Song, et al. 2023. Cogvlm: Visual ex-pert for pretrained language models. arXiv preprintarXiv:2311.03079.",
  "Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, XingSun, Tong Xu, and Enhong Chen. 2023. A survey onmultimodal large language models. arXiv preprintarXiv:2306.13549": "Miaoran Zhang, Vagrant Gautam, Mingyang Wang,Jesujoba O Alabi, Xiaoyu Shen, Dietrich Klakow,and Marius Mosbach. 2024.The impact ofdemonstrations on multilingual in-context learn-ing: A multidimensional analysis. arXiv preprintarXiv:2402.12976. Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala,Michihiro Yasunaga, Leonid Shamis, Jacob Kahn,Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. 2024.Transfusion: Predict the next token and diffuse im-ages with one multi-modal model. arXiv preprintarXiv:2408.11039.",
  "C.1Loss curves for different connectors": "Loss curves of the model with different connectorsare shown in . The loss curves provideseveral insights that corroborate the findings in themain text: 1. For feature-compressing connectors,the difficulty of convergence increases with thenumber of parameters and complexity, followingthe trend: Q-Former > C-Abstractor > Averagepooling. 2. When comparing different compressedtoken numbers, their convergence curves are verysimilar, with 144 tokens performing slightly betterthan 64 tokens. 3. Among the image resolutionsof 224, 336, and 448, the resolution of 336 oftenshows significant improvement over 224, but the",
  "(d) The training loss curves of C-Abstractor-based model": ": Loss curves of different connectors during the pretrain and finetune stages. The left plot shows the trainingloss during the pretrain stage, and the right plot shows the loss during the finetune stage. The legend in the upperright corner follows the format: connector class-image size-token number (e.g., C-Abstractor-224-144, where theconnector is C-Abstractor, the image resolution is 224224, and the number of tokens is 144). It can be observedthat for feature-compressing connectors, the number of tokens has little effect on the loss, while image resolutionsignificantly impacts the model.",
  "C.2Evaluation results on more benchmarks": "To achieve a more extensive and comprehensivecomparison, aligning with other works, we con-duct experiments on 9 additional benchmarks with-out sub-task information. These benchmarks in-clude TextVQA (Singh et al., 2019), POPE (Liet al., 2023c), VQAv2 (Goyal et al., 2017), Sci-enceQA (Lu et al., 2022), GQA (Hudson and Man-ning, 2019), RefCOCO, RefCOCO+, RefCOCOg(Kazemzadeh et al., 2014), and VizWiz (Gurariet al., 2018). The results are shown in .The metrics used are Exact Match for TextVQA,F1-Score for POPE, Accuracy for VQAv2, GQA,ScienceQA, MMBench, and VizWiz, and CIder forRefCOCO. For SEED-Bench, Accuracy is calcu-lated only on image data."
}