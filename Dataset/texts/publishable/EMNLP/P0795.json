{
  "Abstract": "Ensuring the security of released large languagemodels (LLMs) poses a significant dilemma, asexisting mechanisms either compromise own-ership rights or raise data privacy concerns.To address this dilemma, we introduce Tay-lorMLP to protect the ownership of releasedLLMs and prevent their abuse. Specifically,TaylorMLP preserves the ownership of LLMsby transforming the weights of LLMs into pa-rameters of Taylor-series. Instead of releas-ing the original weights, developers can re-lease the Taylor-series parameters with users,thereby ensuring the security of LLMs. More-over, TaylorMLP can prevent abuse of LLMsby adjusting the generation speed. It can inducelow-speed token generation for the protectedLLMs by increasing the terms in the Taylor-series. This intentional delay helps LLM de-velopers prevent potential large-scale unautho-rized uses of their models. Empirical experi-ments across five datasets and three LLM archi-tectures demonstrate that TaylorMLP inducesover 4 increase in latency, producing the to-kens precisely matched with original LLMs.Subsequent defensive experiments further con-firm that TaylorMLP effectively prevents usersfrom reconstructing the weight values based ondownstream datasets. The source code is avail-able at",
  "Introduction": "Training large language models (LLMs) is an ex-pensive and complex endeavor, requiring substan-tial investments in financial and computational re-sources (Wei et al., 2021). In particular, it requiresa huge collection of high-quality datasets from di-verse domains, which proves to be labor-intensiveand time-consuming. However, once released, theLLMs may face the risks of abuse such as uneth-ical or commercial exploitation (Weidinger et al.,",
  "). This raises the critical and urgent challengeof ensuring the security of released LLMs": "In this work, we explore the security aspectsof existing mechanisms for releasing LLMs. Thisprocess involves developers providing access totheir models for general users, with criteria re-stricting access to ethical and non-commercial pur-poses (Wang et al., 2024). Currently, there are twoprimary mechanisms for releasing LLMs: API Re-lease (Caruccio et al., 2024) and Open-source (Raf-fel et al., 2020). With the release of APIs, user au-thorization is managed through specific API keys.Examples of this mechanism include the Chat-GPT(Achiam et al., 2023), Gemini (Team et al.,2023), and Claude models (Caruccio et al., 2024).In these cases, users do not have access to the ar-chitectures or weights of the LLMs. Instead, theyshare their private data to the developers and re-ceive the processed results, as shown in (a). Consequently, the API release mechanism maycause the data privacy concerns (Yang et al., 2023).On the other hand, the open-source mechanismfully shares the LLM weights with users, as shownin (b) (Wolf et al., 2019).Commonexamples include Llama (Touvron et al., 2023),Mixtral (Jiang et al., 2024), and Phi (Team et al.,2024). While the open-source mechanism ensuresthe safety of users private data, it also raises sig-nificant challenges for developers. Specifically, theopen-source mechanism can break the ownershipof LLMs, as users gain control over the models andcan use them for any purpose, even those prohibitedby the developers. For example, users may exploitLLMs for unethical or commercial purposes, bothof which may be prohibited by the developers. Thispotential loss of control and ownership may leadmany model developers to avoid sharing their mod-els (Zha et al., 2023; Sharir et al., 2020). Therefore,there is a dilemma between protecting ownershiprights and ensuring open access to LLMs, posinga significant challenge for developers.",
  "Can we solve the access dilemma for LLMs?": "We propose Taylor-series MLP (TaylorMLP) toprotect the ownership of released LLMs and pre-vent their potential abuse. As illustrated in Fig-ure 1 (c), TaylorMLP addresses the dilemma bysecuring the weights of LLMs into latent param-eters. By sharing these parameters instead of theoriginal weights with users, developers can main-tain ownership of their models while allowing usersto harness the models performance. Specifically,TaylorMLP converts the original weights into pa-rameters of the Taylor series. Our empirical experi-ments confirm that it is infeasible to reconstruct theoriginal weights from these Taylor series parame-ters, thereby ensuring the security of the modelsparameters and allowing safe access to its func-tional capabilities without full model exposure.",
  "Can we prevent unauthorized users fromexploiting the LLM for their own purposes?": "To prevent unauthorized users from abusing the pro-tected LLMs, TaylorMLP allows developers to con-trol the utility of the LLM by adjusting the speedof token generation. Specifically, TaylorMLP in-duces low-speed token generation for the securedLLMs by increasing the terms in the Taylor-series.It significantly increases the number of floating-point operations required for the generation pro-cess, leading to a notable increase in latency. Our empirical studies show that TaylorMLP inducesmore than 4 increases in latency, while maintain-ing the produced tokens precisely matched withoriginal LLMs. This intentional delay helps devel-opers prevent the potential large-scale unauthorizeduse of their released LLMs.",
  "How does TaylorMLP perform in practice?": "To evaluate TaylorMLP, we conducted experi-ments across five datasets: TruthfulQA, MathQA,MMLU, OpenbookQA, and Wikitext-2; and threedifferent LLM architectures: Llama-3-8B, Mistral-7B, and Phi-2. The experimental results demon-strate that TaylorMLP fully retains the accuracyand chat capabilities of original LLMs, while in-ducing 48 increases in latency of token gen-eration. Subsequent defensive experiments furtherconfirm that TaylorMLP effectively prevents usersfrom reconstructing the weight values based ondownstream datasets. In summary, our work makesthe following contributions: Preserving LLM Ownership. TaylorMLP pre-serves the ownership of LLM by transformingthe weights of LLMs into parameters of Taylor-series. It is infeasible to reconstruct the weightsfrom the Taylor-series parameters. Preventing Abuse. TaylorMLP prevents abuseof LLMs by adjusting the generation speed. Itinduces low-speed token generation process, pre-venting the potential large-scale unauthorizeduse of the protected LLMs. Evaluation.Experiment results across fivedatasets and three LLM architectures show thatTaylorMLP retains the accuracy and chat capa-bilities while preventing reconstruction of theweight values based on downstream datasets.",
  "Architecture of Transformers": "We focus on Transformer-based LLMs (Vaswaniet al., 2017; Brown et al., 2020) to develop themethod of LLM protection. A transformer blockconsists of an attention layer and MLP layer. AnMLP layer is a pipeline of a linear layer, activationfunction, and another linear layer, whose architec-ture is shown in (a). Let V, b and W, cdenote the weight matrixes of the two linear lay-ers. Given the input tensor x to the MLP layer, theoutput value in the i-th dimension is given by",
  ": (a) Original MLP layers parameterized byV, b, W, and c. (b) TaylorMLP layers. TaylorMLP con-verts b, Wi and ci into {i,0, , i,N} for securingtheir values. This process is irreversible": "where z = Vx; , denotes the inner product;and Wi and ci denote the i-th row and i-th ele-ment of W and c, respectively. In this work, weintroduce a method to secure the weights of MLPlayers within the transformer architecture. Thiscan be widely applied to powerful LLMs, such asLlama (Touvron et al., 2023), Phi (Gunasekar et al.,2023), and Gemma (Team et al., 2024).",
  "Taylor-series MLP (TaylorMLP)": "The framework of TaylorMLP is shown in Fig-ure 2 (b). Intuitively, TaylorMLP transforms theweight matrices b, W and c into latent parametersi,0, ,i,N. In this way, TaylorMLP generatesoutput tokens without needing the original weightvalues. In this section, we first describe the weighttransformation of TaylorMLP. Then, we demon-strate the outputs of TaylorMLP theoretically con-verges to the original MLPs. Finally, we discussthe benefits of TaylorMLP to LLM community.",
  "n!(zz0)n, (2)": "where (zz0)n indicates the element-wise powerof n; Act(n)() denotes the n-order derivative ofAct(). Representative activation functions for theTransformers are GELU() (Vaswani et al., 2017)and SiLU() (Touvron et al., 2023), whose n-orderderivative are given in Appendixes A and B.Following the Taylor-series in Equation (2), theforward pass of MLP layers can be reformulated to",
  "Estimating the Local Embedding z0": "We clarify the local embedding z0. Specifically,to minimize the difference between the outputs ofTaylorMLP and original MLPs, we minimize thedifference between the two sides of Equation (2).This is equivalent to minimizing the distance be-tween z0 and the embedding z, where z = Vx forthe testing input x. We address this problem bytaking x from large-scale datasets D and applyingthe following solution:",
  "Protecting LLM Ownership": "TaylorMLP secures the LLM weights W, b, andc by transforming them into {i,0, , i,N}Di=1,enabling token generations without disclosing theirspecific values. Moreover, theoretically, it is infea-sible to precisely derive the original values from theexposed parameters {i,0, , i,N}Di=1 and z0.In this way, TaylorMLP preserves the ownership ofdevelopers on their released LLMs.",
  "Preventing Abuse of LLMs by Low-speedToken Generation": "TaylorMLP induces low-speed token generationfor the secured LLMs by incorporating approxi-mately N the floating point operations (FLOPs)compared with the original MLPs. The FLOPs ofEquation (3) are N those of Equation (1). Thiscan significantly reduce the token generation speedof unauthorized use, making such usage low-utilityfor unauthorized users. This intentional delay helpsdevelopers prevent the potential large-scale unau-thorized use of their released LLMs. We note thisintentional delay as the Taylor Unswift\".",
  "Harness of LLMs Capability withoutData Privacy Concerns": "TaylorMLP allows unauthorized users to run andtest LLMs on private datasets before applying forauthorization, as shown in (c). This testingprocess can be fully conducted by the users (dataowners) without sharing their private data with thedevelopers. Based on the testing results on theirprivate data, users can decide whether or not toapply for authorization.",
  "Phi-2": "Latency/token0.0230.026 (1.13)0.040 (1.74)0.054 (2.35)0.072 (3.13)0.089 (3.87)TruthfulQA0.3060.230.2660.3060.3060.305MathQA0.3080.2030.2710.3080.3020.305MMLU0.5440.2310.4060.5150.5340.54OpenbookQA0.3920.1960.350.3740.3880.394Average0.3880.2150.3230.3750.3820.386 result, users need to request authorization from de-velopers for the LLM weight values. This processenables developers to address security concernsthrough authorization regulations or contracts, en-suring the secure use of the LLMs. In this way,large-scale applications on the user side are underthe constraints of regulations or contracts.",
  "Experiments": "In this section, we conduct experiments to evaluateTaylorMLP by answering the following researchquestions: RQ1: Can TaylorMLP retain the accu-racy of original LLMs while adjusting generationspeed? RQ2: How does TaylorMLP defend againstfine-tuning on downstream datasets and distillingon large-scale datasets? RQ3: How does the ex-pansion order influence the output of TaylorMLPcompared with that of original LLMs?",
  "We specify the datasets, LLMs, evaluation metrics,and implementation details": "Datasets.The evaluation of TaylorMLP isbased on the TruthfulQA (Lin et al., 2021),MathQA (Amini et al., 2019), MMLU (Hendrycksetal.,2021),andOpenbookQA(Mi-haylov et al., 2018) datasets.We use thelm-evaluation-harness (Gao et al.)as thecodebase for the experiments of evaluation. LLMs.We evaluate TaylorMLP using three pop-ular model families: Llama-3-8B (Touvron et al.,2023), Mistral-7B (Jiang et al., 2024), and Phi-2 (Liet al., 2023). We download these models from theHuggingface Transformers (Wolf et al., 2019). Evaluation Metrics. We evaluate the accuracy()of LLMs on downstream datasets to determinewhether TaylorMLP can preserve the accuracyof original LLMs.Moreover, we measure theper-token latency to assess the generation speed(Liu et al., 2024). It is the time cost of generatinga single token. To determine if TaylorMLPs out-puts align with original LLMs outputs, we mea-sure the KullbackLeibler divergence() andROUGE-1 score() of TaylorMLPs outputs, usingthe original LLMs outputs as the ground-truth val-ues. We also include case studies for evaluating ifTaylorMLP preserves the chat capability of LLMs. Implementation Details.TaylorMLP protectsthe dmodeldintermediate down_projection weightswithin each layer of LLMs, as shown in Fig-ure 2 (b). Our empirical studies show that securingan dmodel M submatrix of the down_projectionweights is sufficient for protection, where dmodel M dintermediate; Specifically, for the Llama-3-8B,Mistral-7B, and Phi-2 LLMs, their dmodel are 4096,4096, and 2560, while their dintermediate values are14336, 14336, and 10240, respectively. Therefore,TaylorMLP targets 1104, 8103, and 2560 rowsof the down_projection weights for the Llama-3-8B, Mistral-7B, and Phi-2 LLMs, respectively.Given that these LLMs consist of 32 layers, Tay-lorMLP actually protects 1.31B, 1.05B, and 210Mparameters for Llama-3-8B, Mistral-7B, and Phi-2,respectively. More details are in Appendix D.",
  "Phi-2Original0.3060.3080.5440.3920.388TaylorMLP N = 80.3050.3050.5400.3940.386Fine-tuning0.0850.1070.0680.1300.098": "Context: Although initially he was little @-@ known to other writers , his works came to be hugely influential in both Chineseand Japanese literary culture . while the range of his work has allowed him to be introduced to Western readers as theChinese Virgil , Horace , <unk> , Shakespeare , Milton , Burns , <unk> , <unk>.Original Answer: It seems like there are some missing words in the text. Based on the context, Im going to try to fill inthe gaps: Although initially he was little known to other writers, his works came to be hugely influential in both Chinese andJapanese literary culture. while the range of his work has allowed him to be introduced to Western readers as the ChineseVirgil , Horace, Dante, Shakespeare, Milton, Burns, Goethe, and Hugo.Distilled LLMs Answer: addCriterion addCriterion addCriterion addCriterion ... addCriterionTaylorMLP N = 8 Answer: It seems like there are some missing words in the text. Based on the context, Im going to try tofill in the gaps: Although initially he was little known to other writers, his works came to be hugely influential in both Chineseand Japanese literary culture. while the range of his work has allowed him to be introduced to Western readers as theChinese Virgil, Horace, Dante, Shakespeare, Milton, Burns, Goethe, and Hugo.",
  "Retaining Accuracy.According to , forthe Llama-3-8B, Mistral-7B, and Phi-2 LLMs, Tay-lorMLP with N = 4, 8, and 8 are generally asaccurate as the original LLMs across all datasets": "Adjusting Generation Speed.For Llama-3-8B,Mistral-7B, and Phi-2 LLMs, we focus on the per-token latency of N = 4, 8, and 8, where Tay-lorMLP can generally retain the accuracy of origi-nal LLMs. Notably, TaylorMLP has 4.32, 8.73and 3.73 increase of the latency compared withthe original LLMs, respectively. By reducing thegeneration speed, TaylorMLP potentially preventslarge-scale unauthorized use of released LLMs. Agnostic to Different LLMs.TaylorMLP is amodel agnostic method to protect the ownershipof LLMs. Applied to different LLMs, it showsconsistent capacity in retaining the performance,while inducing the low-speed generation process.",
  "Defending against Fine-tuning (RQ2)": "We demonstrate the capability of TaylorMLP in de-fending a reconstruction of the secured weights bydataset-based fine-tuning. Specifically, we play asunauthorized users to randomly reinitialize the se-cured weights b, W, and c of the MLP layers withinthe LLMs and attempt to reconstruct their valuesthrough fine-tuning processes on labeled datasets. The fine-tuning processes are conducted using theMistral-7B and Phi-2 LLMs on the four down-stream datasets: TruthfulQA, MathQA, MMLU,and OpenbookQA. The hyperparameters settingsare provided in Appendix F. The fine-tuning re-sults are compared with those of original LLMsand TaylorMLP with N = 8 in . Accuracy.According to , the fine-tunedLLMs exhibit a significant decline in accuracyacross each downstream task when compared toboth the original LLMs and TaylorMLP. Learningsuch a vast number of parameters from scratch(1.05B for the Mistral-7B and 210M for Phi-2)is highly challenging, given the limited instancesand label information available from downstreamdatasets. This challenge arises because TaylorMLPsecures the pre-trained weights of LLMs, prevent-ing effective initialization for fine-tuning. This indi-cates the effectiveness of TaylorMLP in defendingunauthorized users from using downstream datasetsto reconstruct the protected weights.",
  "Defending against Distillation (RQ2)": "We demonstrate the capability of TaylorMLP indefending a reconstruction of the secured weightsby knowledge distillation. Specifically, we play asunauthorized users to randomly reinitialize the se-cured weights b, W, and c of the MLP layers withinthe LLMs and attempt to reconstruct their valuesby learning the output distribution of the originalLLMs. The distillation process is conducted on",
  "Perplexity12.7212.75256.62": "the C4-En dataset (Raffel et al., 2020), and thedistilled LLMs are evaluated on the WikiText-2dataset (Merity et al., 2016) using the perplexitymetric (). We also compare the perplexity of theoriginal LLMs and TaylorMLP with N = 8. Theexperiment is conducted on the Llama-3-8B LLMdue to its state-of-the-art capabilities. The hyperpa-rameter settings are provided in Appendix F. Perplexity.The distilled LLMs exhibit consid-erably higher perplexity (265.62) compared to theoriginal LLMs (12.72), as shown in . In con-trast, TaylorMLP with N = 8 achieves a compara-ble perplexity (12.75) to the original result (12.72).This demonstrates that, without access to the pro-tected weights, unauthorized users cannot use dis-tillation methods to match the language modelingperformance of the original LLMs. Hallucination of Distilled LLMs.According to, when given contexts from the Wikitext-2dataset, the distilled LLMs generate \"addCriterion\"that are complete hallucinations. In contrast, Tay-lorMLP with N = 8 produces tokens that alignprecisely with those of the original LLMs, deliver-ing coherent and accurate answers. This indicatesthat unauthorized users cannot fully restore the chatcapabilities of the LLMs by reinitializing the pro-tected weights and distilling the LLMs.",
  "Influence of Expansion Order forTaylorMLP (RQ3)": "We study the influence of expansion order on theoutputs of TaylorMLP. Specifically, the outputs ofthe original LLMs are taken as ground-truth values,and those of TaylorMLP with different expansionorders are compared with these ground-truth values.We employ the KullbackLeibler divergence ()and ROUGE-1 () to quantitatively measure thedistance between the outputs of TaylorMLP andthe ground-truth values, which represent statisticaland token space distance, respectively. These exper-iments are conducted on the CoQA dataset (Roem-mele et al., 2011). The experimental results areshown in . Additionally, we also show thegenerated tokens from TaylorMLP with differentexpansion orders in . TaylorMLPs Outputs Gradually Converge toOriginal LLMs.According to , as Ngrows from 0 to 8, the KullbackLeibler divergencedecreases to zero, while the ROUGE-1 score risesto 0.9. This trend is consistent with our theoreticaldiscussion in .4. When expansion orderN 8 is sufficiently large, the outputs of Tay-lorMLP closely match those of the original LLMs. Hallucination Caused by Insufficient Taylor Ex-pansion Order.According to , given thecontext and question from the CoQA dataset, Tay-lorMLP with N = 0 or N = 1 output \"Bout\" or\"the article was written from,\" which are completehallucinations. With N = 2, TaylorMLP providesthe key points of the answers but undesirably re-peats the questions. An expansion order of N = 8is sufficient for TaylorMLP, as the output contextclosely matches that of original LLMs. Effectiveness.According to both the quantitativeand qualitative results, a sufficiently large expan-sion order, such as N 8, is necessary for unau-thorized users to avoid the hallucinations. In thisway, TaylorMLP adjusts the unauthorized genera-tion process 4 slower than original LLMs.",
  "Related Work": "File Encryption.Encryption technologies caneffectively prevent the unauthorized use of digitalfiles, including the checkpoint file of large languagemodels (LLMs). Representative algorithms includethe Advanced Encryption Standard (AES) (Bog-danov et al., 2011), Blowfish (Rijmen, 1997),and RivestShamirAdleman (RSA) (Rivest et al.,1978). However, these methods are not suitable forLLMs because the models cannot perform infer-ence using encrypted weight values. This limita-tion prevents users from harnessing the capabilitiesof released LLMs on their private datasets, whichmay cause a loss of market opportunities. Offsite-Tuning.The offsite-tuning method al-lows developers to share parts of large languagemodels (LLMs) with users (Xiao et al., 2023).However, this method has several disadvantages.First, users need to perform fine-tuning, whichis extremely costly and time-consuming. Addi-tionally, fine-tuning LLMs is challenging for indi-viduals who are not machine learning engineers.These factors hinder the application of offsite-tuning methods in real-world scenarios. Context: New York (CNN) More than 80 Michael Jackson collectibles including the late pop stars famous rhinestone-studded glove from a 1983 performance were auctioned off Saturday, reaping a total $2 million. Profits from the auction atthe Hard Rock Cafe in New Yorks Times Square crushed pre-sale expectations of only $120,000 in sales. Hoffman Ma,who bought the glove on behalf of Ponte 16 Resort in Macau, paid a 25 percent buyers premium, which was tacked onto allfinal sales over $50,000. Winners of items less than $50,000 paid a 20 percent premium.Question: Where was the Auction held? How much did they make? How much did they expected? Who buy the JacksonGlove Where was the buyer of the glove from?Original Answer: According to the article, the auction was held at the Hard Rock Cafe in New Yorks Times Square. Theauction made a total of $2 million, which is more than the expected $120,000. The Michael Jackson glove was sold for$420,000 to a buyer from Hong Kong, China, who was representing Ponte 16 Resort in Macau.TaylorMLP N = 0 Answer: _REFascus Bout Bout Bout Bout Bout Bout Bout Bout BoutTaylorMLP N = 1 Answer: The article was written from the article from from the articleTaylorMLP N = 2 Answer: Here are the answers to your questions: 1. Where was the Auction held? The auction was held atthe Hard Rock Cafe in New Yorks Times Square. 2. How much did they make? The auction raised a total of $2 million. 3.How much did they expect? The auction house, Juliens Auctions, expected the auction to raise only $120,000. 4. Who boughtthe Jackson Glove? The Jackson glove was bought by Hoffman Ma, who purchased it on behalf of Ponte 16 Resort in Macau. 5.Where was the buyer of the glove from? The buyer of the glove, Hoffman Ma, is from Hong Kong, China.TaylorMLP N = 8 Answer: According to the article, the auction was held at the Hard Rock Cafe in New Yorks TimesSquare. The auction made a total of $2 million, which is more than the expected $120,000. The Michael Jacksons famousrhinestone-studded glove from a 1983 performance was sold for $420,000 to a buyer from Hong Kong, China, who is HoffmanMa, representing Ponte 16 Resort in Macau.",
  ": The KL divergence of output probability andROUGE-1 score versus the expansion order": "Key Prompt Protection.The Key Prompt Pro-tection (KPP) aims to prevent the unauthorized useof LLMs (Tang et al., 2023). With KPP, LLMsrespond only when presented with the correct keyprompt; otherwise, they will ignore any input in-structions. However, a drawback of KPP is that itrelies on fine-tuning to embed the protection keyinto the LLMs. As a result, the LLMs lose theirfoundational effectiveness and become specializedonly in the domains of the fine-tuned datasets. KPPcannot address the access dilemma because devel-opers cannot access users private data to embedthe protection key into their released LLMs. Advantages of TaylorMLP over Related Work.Unlike file encryption, TaylorMLP allows users toharness the capability of released LLMs on theirprivate datasets while maintaining the protection ofmodel ownership, thus attracting users to apply forauthorization. Different from offsite-tuning, Tay-lorMLP enables immediate use of LLMs withoutthe need for fine-tuning. Moreover, compared to key prompt protection, TaylorMLP preserves thegeneral capabilities of LLMs as foundational mod-els, without restricting them to specific domains.To summarize, TaylorMLP stands out as a highlyeffective method for protecting ownership and en-suring secure uses of LLMs. LLM Watermarks.Watermark technologies en-able developers to detect whether their models havebeen misused by embedding a digital signature intothe authorized model. This signature acts as anidentifier that can be detected in misuse scenar-ios. For API-accessed LLMs and open-sourcedLLMs, the signatures are embedded into the dis-tribution of output tokens (Xiang et al., 2021) andmodel weights (Xu et al., 2024), respectively, with-out compromising the performance of the LLMs.Different from watermarks, TaylorMLP protectsthe model before it is authorized for weight accessby releasing the Taylor-series parameters. It pro-tects the original weight values and allows users totest the models without risking data privacy. Tay-lorMLP and watermark technologies can signifi-cantly complement each other, ensuring the secu-rity of models throughout the entire process.",
  "In this work, we propose TaylorMLP to preservethe ownership of released LLMs and prevent theirabuse. Specifically, TaylorMLP preserves the own-ership of LLM by transforming the weights of": "LLMs into parameters of Taylor-series. Insteadof releasing the original weights, developers canrelease the Taylor-series parameters with users,thereby ensuring the security of LLMs. Defensiveexperiments confirm that TaylorMLP effectivelyprevents users from reconstructing the weight val-ues based on downstream datasets. Moreover, Tay-lorMLP prevents abuse of LLMs by inducing low-speed token generation for the protected LLMs.This intentional delay prevents the potential large-scale unauthorized abuse. Empirical and case stud-ies show that TaylorMLP significantly increasesthe latency of token generation, while maintainingthe chat capabilities and performance on down-stream tasks. Both qualitative and quantitative re-sults demonstrate the effectiveness of TaylorMLPin ensuring the security of the released LLMs. Thisindicates its potential in real-world applications.",
  "Limitations and Potential Risks": "In this work, we propose a framework to protectthe ownership of LLMs released. TaylorMLP canfacilitate large-scale applications of LLMs underregulatory or contractual constraints. Upon autho-rizing weights to users, developers can deliver reg-ulations or contracts to ensure LLM applicationscomply with specified constraints. After autho-rization, watermark technologies are required as acomplementary to detect whether users have vio-lated regulations by misusing or sharing the modelswith others. TaylorMLP and watermark comple-ment each other to ensure the security of modelsthroughout the entire process. This research was supported by NSF Awards IIS-2224843, and the US Department of Transportation(USDOT) Tier-1 University Transportation Center(UTC) Transportation Cybersecurity Center for Ad-vanced Research and Education (CYBER-CARE)grant #69A3552348332. Additionally, this researchwas also supported, in part, by NSF Awards OAC-2112606 and OAC-2117439. This work made useof the High Performance Computing Resource inthe Core Facility for Advanced Research Comput-ing at Case Western Reserve University (CWRU).We give our special thanks to the CWRU HPC teamfor their timely and professional help and mainte-nance. The views and conclusions in this paper arethose of the authors and do not represent the viewsof any funding or supporting agencies. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774. Aida Amini, Saadia Gabriel, Shanchuan Lin, RikKoncel-Kedziorski, Yejin Choi, and Hannaneh Ha-jishirzi. 2019. MathQA: Towards interpretable mathword problem solving with operation-based for-malisms. In Proceedings of the 2019 Conferenceof the North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages23572367, Minneapolis, Minnesota. Association forComputational Linguistics. Andrey Bogdanov, Dmitry Khovratovich, and ChristianRechberger. 2011. Biclique cryptanalysis of the fullaes. In Advances in CryptologyASIACRYPT 2011:17th International Conference on the Theory andApplication of Cryptology and Information Security,Seoul, South Korea, December 4-8, 2011. Proceed-ings 17, pages 344371. Springer. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Loredana Caruccio, Stefano Cirillo, Giuseppe Polese,Giandomenico Solimando, Shanmugam Sundara-murthy, and Genoveffa Tortora. 2024. Claude 2.0large language model: tackling a real-world classifi-cation problem with a new iterative prompt engineer-ing approach. Intelligent Systems with Applications,page 200336. Haotian Cui, Chloe Wang, Hassaan Maan, Kuan Pang,Fengning Luo, Nan Duan, and Bo Wang. 2024. scgpt:toward building a foundation model for single-cellmulti-omics using generative ai. Nature Methods,pages 111.",
  "L Gao, J Tow, B Abbasi, S Biderman, S Black, A DiPofi,C Foster, L Golding, J Hsu, A Le Noach, et al. Aframework for few-shot language model evaluation,12 2023. URL org/records/10256836,7": "Suriya Gunasekar, Yi Zhang, Jyoti Aneja, CaioCsar Teodoro Mendes, Allie Del Giorno, SivakanthGopi, Mojan Javaheripi, Piero Kauffmann, Gustavode Rosa, Olli Saarikivi, et al. 2023. Textbooks are allyou need. arXiv preprint arXiv:2306.11644. Dan Hendrycks, Collin Burns, Steven Basart, AndyZou, Mantas Mazeika, Dawn Song, and Jacob Stein-hardt. 2021. Measuring massive multitask languageunderstanding. Proceedings of the International Con-ference on Learning Representations (ICLR).",
  "Truthfulqa: Measuring how models mimic humanfalsehoods. Preprint, arXiv:2109.07958": "Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong,Zhaozhuo Xu, Vladimir Braverman, Beidi Chen,and Xia Hu. 2024. Kivi: A tuning-free asymmet-ric 2bit quantization for kv cache. arXiv preprintarXiv:2402.02750. Shayne Longpre, Le Hou, Tu Vu, Albert Webson,Hyung Won Chung, Yi Tay, Denny Zhou, Quoc VLe, Barret Zoph, Jason Wei, et al. 2023. The flancollection: Designing data and methods for effectiveinstruction tuning. In International Conference onMachine Learning, pages 2263122648. PMLR.",
  "Ruixiang Tang, Yu-Neng Chuang, Xuanting Cai, MetaPlatforms, Mengnan Du, and Xia Hu. 2023.Se-cure your model: An effective key prompt protectionmechanism for large language models": "Gemini Team, Rohan Anil, Sebastian Borgeaud,Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,Radu Soricut, Johan Schalkwyk, Andrew M Dai,Anja Hauth, et al. 2023.Gemini: a family ofhighly capable multimodal models. arXiv preprintarXiv:2312.11805. Gemma Team, Thomas Mesnard, Cassidy Hardin,Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,Laurent Sifre, Morgane Rivire, Mihir Sanjay Kale,Juliette Love, et al. 2024. Gemma: Open modelsbased on gemini research and technology.arXivpreprint arXiv:2403.08295. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processingsystems, 30. Guanchu Wang, Junhao Ran, Ruixiang Tang, Chia-Yuan Chang, Yu-Neng Chuang, Zirui Liu, VladimirBraverman, Zhandong Liu, and Xia Hu. 2024. As-sessing and enhancing large language models inrare disease question-answering.arXiv preprintarXiv:2408.08422. Jason Wei, Maarten Bosma, Vincent Y Zhao, KelvinGuu, Adams Wei Yu, Brian Lester, Nan Du, An-drew M Dai, and Quoc V Le. 2021. Finetuned lan-guage models are zero-shot learners. arXiv preprintarXiv:2109.01652. Laura Weidinger, John Mellor, Maribeth Rauh, ConorGriffin, Jonathan Uesato, Po-Sen Huang, MyraCheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,et al. 2021. Ethical and social risks of harm fromlanguage models. arXiv preprint arXiv:2112.04359. Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz,et al. 2019. Huggingfaces transformers: State-of-the-art natural language processing. arXiv preprintarXiv:1910.03771.",
  "Jiashu Xu, Fei Wang, Mingyu Derek Ma, Pang WeiKoh, Chaowei Xiao, and Muhao Chen. 2024. Instruc-tional fingerprinting of large language models. arXivpreprint arXiv:2401.12255": "Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiao-tian Han, Qizhang Feng, Haoming Jiang, ShaochenZhong, Bing Yin, and Xia Hu. 2023. Harnessing thepower of llms in practice: A survey on chatgpt andbeyond. ACM Transactions on Knowledge Discoveryfrom Data. Jiayi Yuan, Ruixiang Tang, Xiaoqian Jiang, and Xia Hu.2023. Llm for patient-trial matching: Privacy-awaredata augmentation towards better performance andgeneralizability. In American Medical InformaticsAssociation (AMIA) Annual Symposium. Daochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai,Fan Yang, and Xia Hu. 2023. Data-centric ai: Per-spectives and challenges.In Proceedings of the2023 SIAM International Conference on Data Mining(SDM), pages 945948. SIAM.",
  "All open-sourced datasets have the Apache-2.0 li-cence, which allows for academic research": "TruthfulQA:TruthfulQA is a benchmark de-signed to assess the truthfulness of a languagemodels answers. The dataset consists of 4,114questions across 38 categories, including health,law, finance, and politics. These questions arecrafted to reflect common false beliefs or miscon-ceptions that humans might hold (Lin et al., 2021). MathQA:MathQA is a comprehensive datasetof math word problems accompanied by an inter-pretable neural solver that translates problems intooperational programs. It contains 14,925 ques-tions (Amini et al., 2019). MMLU:The MMLU benchmark measures theknowledge acquired during pretraining by evalu-ating models in zero-shot and few-shot settings.It includes 56,168 questions covering 57 subjectsacross STEM, humanities, social sciences, andmore, with difficulty levels ranging from elemen-tary to advanced professional. The benchmark testsboth general knowledge and problem-solving abili-ties (Hendrycks et al., 2021).",
  "OpenbookQA:OpenbookQA addresses the taskof open-domain question answering using datasetslike Wikipedia. This dataset contains 2,000 ques-tions (Mihaylov et al., 2018)": "C4:C4 is a large, cleaned version of the Com-mon Crawl web corpus.For LLM distillationin .4, we use the c4-train.00000-of-01024.json.gz split, and down-sample a subsetof 35.6k instances for the distillation. Wikitext-2:The WikiText-2 dataset is a collec-tion of over 100 million tokens from verified Goodand Featured articles on Wikipedia. It includes4.4k sentences used to evaluate the perplexity oflanguage models.",
  "DImplementation Details": "When selecting the protected columns within thedown_projection weights, we aim to minimizethe difference between the embedding z and thelocal embedding z0, represented by z z0. Specifi-cally, according to Equation (5), the approximateupper bound for each dimension of z z0 is givenby |zmax zmin|, where | | takes the element-wise absolute values. Then, the indexes of the pro-tected columns take the least K dimensions within|zmax zmin|, where K takes 1 104, 8 103,and 2560 for the Llama-3-8B, Mistral-7B, and Phi-2 LLMs, respectively. The number of protectedcolumns takes the maximal value that TaylorMLPcan retain the original performance of the LLMs.Other Experimental configurations are in .The unprotected columns preserve their pre-trainedvalues in the fine-tuning (Sections 5.3) and distilla-tion experiments ( 5.4)."
}