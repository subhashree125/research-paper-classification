{
  "Abstract": "With the growing popularity of general-purposeLarge Language Models (LLMs), comes a needfor more global explanations of model behav-iors. Concept-based explanations arise as apromising avenue for explaining high-level pat-terns learned by LLMs. Yet their evaluationposes unique challenges, especially due to theirnon-local nature and high dimensional rep-resentation in a models hidden space. Cur-rent methods approach concepts from differ-ent perspectives, lacking a unified formaliza-tion. This makes evaluating the core measuresof concepts, namely faithfulness or readabil-ity, challenging. To bridge the gap, we intro-duce a formal definition of concepts generaliz-ing to diverse concept-based explanations set-tings. Based on this, we quantify the faithful-ness of a concept explanation via perturbation.We ensure adequate perturbation in the high-dimensional space for different concepts viaan optimization problem. Readability is ap-proximated via an automatic and determinis-tic measure, quantifying the coherence of pat-terns that maximally activate a concept whilealigning with human understanding. Finally,based on measurement theory, we apply a meta-evaluation method for evaluating these mea-sures, generalizable to other types of explana-tions or tasks as well. Extensive experimentalanalysis has been conducted to inform the se-lection of explanation evaluation measures. 1",
  "*These authors contributed equally to this work.Corresponding author:": "Zou et al., 2023). Previous XAI algorithms havebeen applied to NLP tasks (Wu et al., 2023a), vi-sion tasks (Wang et al., 2023b) and recommenda-tion (Jin et al., 2022; Yang et al., 2022). Theseinclude natural language explanation (Zhang et al.,2024; Lee et al., 2022), attention explanation (Chenet al., 2019b; Gao et al., 2019), and especially at-tribution methods (Lundberg and Lee, 2017; Sun-dararajan et al., 2017; Guan et al., 2019). Theattribution methods identify where the modellooks rather than what it comprehends (Colinet al., 2022), typically offering local explanationsfor a limited number of input samples, restrict-ing their utility in practical settings (Colin et al.,2022; Adebayo et al., 2018). Concept-based ex-planations (Kim et al., 2018; Cunningham et al.,2023; Fel et al., 2023b) can mitigate the limita-tions of attribution methods by recognizing high-level (Kim et al., 2018) patterns (see ), whichprovide concise, human-understandable explana-tions of models internal state.Despite these merits,the development ofconcept-based explanations may be hindered dueto a lack of standardized and rigorous evaluationmethodology. Unlike a single importance scoreassigned on each scalar input by attribution meth-ods, diverse explanation methods approach high-dimensional concepts from different aspects. Thisincludes a single classification plane (Kim et al.,2018), an overcomplete set of basis (Cunning-ham et al., 2023), or a module designed before-hand (Koh et al., 2020), lacking a unified land-scape (C1). Moreover, its non-local nature acrosssamples (Kim et al., 2018), combined with the highcost of human evaluation when the number of con-cepts is large, makes evaluating a concepts read-ability challenging (C2). For available evaluationmeasures (Hoffman et al., 2018), it is difficult totest their reliability and validity (C3).In this paper, we address the challenges aboveand make the following contributions: systems addr IP systems(-) addr(+) IP(+)",
  "Concept Explanation": "ActivationFunction Color represents activation value Inter-rater reliability Kendall's : The overall framework. (a) Concept extraction: We formalize concepts as virtual neurons. (b) Evaluationis approached via readability and faithfulness. Readability is approximated by the semantic similarity of patterns thatmaximally activate the concept. Faithfulness is approximated by the difference in output when a concept is perturbed.(c) Meta-Evaluation is performed on the observed results of proposed measures via reliability and validity. First, we provide a unified definition of diverseconcept-based explanation methods and quan-tify faithfulness under this formalization (C1).By summarizing common patterns of concept-based explanation, we provide a formal definitionof a concept, which can generalize to both super-vised and unsupervised, post-hoc and interpretable-by-design methods, language and vision domains.Based on this, we quantify the faithfulness of a con-cept explanation via perturbation. We ensure ade-quate perturbation in the high-dimensional spacefor different concepts via an optimization problem. Second,we approximate readability viacoherence of patterns that maximally activatesa concept (C2). We utilize the formulation definedabove to recognize patterns across samples thatmaximally activate a concept, from both the inputand the output side. Then, we estimate how coher-ent they are as one concept via semantic similarity.Experimental results have shown this automaticmeasure correlates highly with human evaluation. Third, we apply the classic measurement the-ory to perform a meta-evaluation on the faith-fulness and readability measures (C3). Measure-ment theory (Allen and Yen, 2001; Xiao et al.,2023) has been long utilized to verify whether ameasurement is reliable and valid. Approaching viareliability and validity, this meta-evaluation methodis useful for evaluating the measures for conceptsand can be generalized to analyze the effective-ness of other measures, for example, measures forother types of explanations and other natural lan-guage tasks. Experimental results have filtered out4 measures with low reliability, i.e. LLM-Score,",
  "Concept Formalization": "In this paper, we primarily focus on explainingLLMs as black-box models.Meanwhile, ourmethod can be generalized to many other deepclassification models, including image models(see Appx.C). As illustrated in , we considerthe black-box model to take an input x from adataset D and output y, a k-class classification re-sult. In text generation, k is the vocabulary size.For the l-th layer to be interpreted, given a sequenceof input tokens x1, ...., xt, their corresponding hid-den representations are h1l , ..., htl. The output clas-sification logits are g(h).Within the context of Deep Neural Networks(DNNs), we summarize common patterns of con-cepts and establish a unified framework. Specifi-cally, each concept is represented as a virtual neu-ron defined by an activation function that maps ahidden representation h into a real value a : Rm R, where a positive output signifies activation. Foreach concept, a semantic expression may be givenby humans or LLMs, depending on the conceptexplanation methods (Kim et al., 2018; Bills et al.,2023). Some methods take concepts and seman-tic expressions predefined by humans as inputs(e.g., (Kim et al., 2018)), while others require addi-tional steps to produce a semantic expression basedon highly activated tokens and samples of the ex-tracted concepts (Bills et al., 2023). Specifically,given high-activation samples of the concept and the highly activated tokens in these samples (e.g.,Internet, computer, networks, ...), an LLM ora human labeler provides a semantic expressionthat summarizes their common patterns (e.g., Ter-minologies related to computer networks) (Billset al., 2023).Our formalization can integrate diverse conceptexplanation methods, as shown in Tab. 1. Thisincludes both supervised methods that require priorinformation about concepts (e.g., input samplesthat contain and do not contain the concepts) (Kimet al., 2018) and unsupervised methods that do notrely on such prior information (Ghorbani et al.,2019). Our method also works for both post-hocexplanation methods that interpret a model afterit is trained (Kim et al., 2018) and interpretable-by-design approaches that integrate interpretabilitymechanisms directly into the models architecturebefore training (Koh et al., 2020). Additionally, itapplies to image backbone models as well.",
  "Faithfulness": "Widely studied in previous XAI methods, faithful-ness is crucial for assessing how well a conceptreflects a models internal mechanism (Chan et al.,2022; Lee et al., 2023; McCarthy and Prince, 1995).However, its direct application to concept-based ex-planations presents challenges, particularly due toconcepts ambiguous representation in the hiddenspace of a model. The adequate degree of pertur-bation needed for diverse concepts extracted mayvary, making it difficult to ensure a fair comparison.We quantify the faithfulness of a concept by thechange in the output g(h) after perturbing the hid-den representation h in the hidden space H wherethe concepts reside. We formulate faithfulness as(a, , ), where (h, a) applies a perturbation onh given the activation function a(h), and (y, y)measures the output difference.",
  "htf(x)(y, y)(1)": "with y = g(ht), y = g((ht, a)) being the proba-bility distribution of output vocabulary.Concept perturbation. Based on the formaliza-tion of concepts in Sec. 2, we view this problem asan optimization problem. As the concept formaliza-tion provided above encapsulates diverse kinds ofconcepts, this transformation allows the perturba-tion strategies to generalize beyond the linear formof concepts, like (Chen et al., 2019a).Typical perturbation strategies include: 1) e:concept -addition, wherein a near zero is intro-duced to maximally increase concept activation;2) a: concept ablation, which involves removingall information of the concept. The optimizationproblems can be formulated as:",
  "(Class)jc(y, y) = (yj yj)(8)": "Here, L is a certain loss function (Schwab andKarlen, 2019; Bricken et al., 2023), y, y are the out-put classfication logits, y is corresponding groundtruth label, yj, yj are the logits of class j. Toquantify the discrepancy between distributions, weutilize a statistic H, specifically KL-Divergence inour experimental setup.For ease of reference, perturbations are ex-pressed as prefixes, and difference measures aredenoted as suffixes. Furthermore, we divide Classinto PClass (prediction class) and TClass (trueclass) with j taking the predicted token class orground truth token class. For instance, faithful-ness computed via gradient to prediction class, as",
  "UnsupervisedNetDissect (Bau et al., 2017)imageM(h) Lc(x)M(h) Lc(x)Neuron (Bills et al., 2023)text/imageoTi hSAE (Cunningham et al., 2023)textReLU(vT h + b)": ": Concept-based explanations activation function. * denotes interpretable-by-design methods. Hyperpa-rameters: 1) v, o is a concept vector within the same space as h, and oi denotes a one-hot vector where i indicatesthe position of the 1 in the vector. 2) M(h) selects the top-quantile activations and upsample them to the samedimension as x, and Lc(x) is a pixel-level human-annotated label on x. 3) b is a bias term.",
  "Readability": "Readability assesses the extent to which humanscan comprehend the extracted concept (Lage et al.,2019). Most of the time, when patterns that maxi-mally activate a concept are coherent (see examplein ), can the concept be easily understandableto humans. We design coherence measures basedon OpenAIs pipeline (Bills et al., 2023) for hu-man evaluation of concept quality. They presentedhuman labelers with fragments where highly acti-vated tokens were shown with color highlightingand asked the humans to try summarizing the com-monalities of these highly activated tokens. Weautomate this process by assessing the commonal-ity of highly activated tokens via co-occurrence orembedding similarity.As cross-sample patterns are extracted from alarge corpus, diverse samples are needed to eval-uate a concepts readability. Although previousefforts have made some progress in evaluatingreadability, they confront the challenge of ensur-ing data comprehensiveness while minimizing cost.Tab. 2 compares different measures for readabil-ity, including human evaluation (Kim et al., 2018;Ghorbani et al., 2019), LLM-based measures (Billset al., 2023; Singh et al., 2023), and our pro-posed coherence-based measures. For the LLM-based evaluation, we considered (Bills et al., 2023;Bricken et al., 2023), which used less than 100samples. For human evaluation, we considered the",
  ": Comparison of readability measures. #Sampledenotes the maximum number of samples applicable forevaluating a concept": "Human evaluation. Existing approaches pre-dominantly rely on case studies and user stud-ies (Kim et al., 2018; Ghorbani et al., 2019; Chenet al., 2019a), asking humans to score a conceptgiven a limited number of demonstrative samples.They are subject to issues of validation, standard-ization, and reproducibility (Clark et al., 2021;Howcroft et al., 2020). LLM-based.As inexpensive human substi-tutes, LLMs have been utilized in evaluatingconcept-based explanations. A typical LLM-basedscore (Bills et al., 2023; Singh et al., 2023) is ob-tained by: 1) letting LLM summarize a natural lan-guage explanation s for the concept (e.g., semanticexpression in ) given formatted samples thatmaximally activates on the concept and activationsa; 2) letting LLM guess the activation given onlysample text and the generated explanation; 3) cal-culating an explanation score based on the variancebetween true activation and the simulated activa-tion. However, the number of samples inputted toLLMs (4 in (Bills et al., 2023)) in step 1 is limitedto maximum input length. This limits the compre-hensiveness of the generated explanation, as shownin a case study in Appx. D. Even if the maximum input length is extended to 200k+ like Claude 32,it may suffer from high computation cost and poorperformance in long-dependency tasks (Li et al.,2023).Coherence-based. To address these limitations,we propose novel measures inspired by topic coher-ence. Topic coherence measures are widely usedin the field of topic modeling to estimate whethera topic identified from a large corpus can be eas-ily understood by humans (Newman et al., 2010).Here, the basic idea is to approximate readabilitybased on the semantic similarity between patternsthat maximally activate a concept: we estimatehow coherent they are as one topic (). Thesemeasures mainly rely on the concept activationfunction, allowing for scalable, automatic, and de-terministic evaluation.Patterns that maximally activate a concept areobtained as follows. Initially, a subset of texts is se-lected and processed through a black-box LLM toobtain concept-specific activations for each token.High-activation tokens, indicative of a strong asso-ciation with the analyzed concept, are then identi-fied. For these tokens, important contextual wordsare extracted by ablating each word in the contextand identifying those that impose the most impacton the high-activation token. Similar informationcan be obtained from the output side. We extracttokens with the top-k highest likelihood when set-ting the hidden representation highly active on theconcept and not on others.For our evaluation, we employ semantic sim-ilarity measures including UCI (Newman et al.,2009), UMass (Mimno et al., 2011), and two deepmeasures Embedding Distance (EmbDist), Embed-ding Cosine Similarity (EmbCos). Each measurecomputes similarity (xi, xj) between two tokensxi, xj as follows:",
  "2www.anthropic.com/news/claude-3-family": "is introduced. e(xi) embeds a word to a continu-ous semantic space, for example, using embeddingmodels like BERT.For ease of reference and consistency, we de-note readability on the input/output side using theprefixes IN/OUT. For instance, readability com-puted using UCI similarity on the input side isrepresented as IN-UCI. Note that coherence-basedmeasures may not capture all the desiderata of areadable explanation. Yet, it is still of interest to uti-lize this measure to filter a large amount of conceptswhen human evaluation may not be applicable.",
  "Meta Evaluation": "How can we discern the effectiveness among pos-sible measures available for evaluating concept-based explanations? Borrowing metrics from mea-surement theory (Allen and Yen, 2001) and psy-chometrics (Wang et al., 2023c; Xiao et al., 2023),our meta-evaluation focus centers on reliabilityand validity, guided by the methodological frame-work outlined in (Allen and Yen, 2001). Our meta-evaluation methods can generalized to measures ofa broader scope, including other XAI methods andother natural language tasks like generation.",
  "Reliability": "In this section, we analyze which measures arereliable to random noise introduced by retesting,different data subsets, and human subjectivity.Test-retest reliability results, depicted in ,verifies the deterministic nature of the proposedmeasures, except for LLM-Score (Bills et al., 2023).LLM-Score is less acceptable, which may be due tothe inherent randomness introduced by samplingthe most probable tokens.",
  "Xall(14)": "X1, X2, ...XJ are results of measure X across dif-ferent data subsets. The overall score on the entiredataset is expressed as Xall = Jj=1 Xj. isthe lower bound of squared correlation 2X,T of ob-served score X and true score T (Cronbach, 1951).For a measure with low subset consistency, onemay use a larger test dataset to ensure the resultsconsistency.Inter-rater reliability measures the degree ofagreement across raters, calculated as score correla-tion among them. In this paper, we apply Kendalls (Kendall, 1938) to measure pairwise correlationamong raters using a scale that is ordered:",
  "Validity": "Validity is crucial in assessing how well a test mea-sures the intended construct (Nunnally and Bern-stein, 1994). A construct refers to the underlyingcriterion to be measured. In our case, it is faith-fulness or readability. We focus on concurrentvalidity, evaluating the extent to which a test scorepredicts outcomes on a validated measure (Cron-bach and Meehl, 1955), and construct validity,examining how well indicators represent an un-measurable concept (Cronbach and Meehl, 1955).Construct validity can be further divided into con-vergent validity and divergent validity.Concurrent validity reflects the appropriatenessof a measure as an alternative to an existing refer-ence, quantified via the correlation between thetwo scores. For example, an automatic measure forreadability is used to approximate human evalua-tion at a large scale. Only when the automatic mea-sure for readability is highly correlated with human scores, can we treat it as an approximate of hu-man evaluation. Here we use classical correlationmetrics to estimate concurrent validity (Kendall,1938; Spearman, 1961; Galton, 1877). Note thatrandom error in either the automatic measure orhuman evaluation may impair concurrent validity.Thus being reliable is a premise of being valid.Convergent validity verifies whether measuresof the same construct are indeed related. For ex-ample, whether the purposed faithfulness measuresare related to each other. As the underlying con-struct is often inaccessible to directly assess themeasures concurrent validity, convergent validityprovides a statistic tool to assess construct validityvia its relation (Kendall, 1938) with other measuresof the same construct.Divergent validity tests whether measures ofunrelated constructs are indeed unrelated. For ex-ample, for distinct aspects considered of concept-based explanation (e.g., readability and faithful-ness), measures of different aspects should show asignificantly lower correlation than measures of thesame aspect. Here we apply Kendalls (Kendall,1938) as a measure of correlation. A bad divergentvalidity may indicate potential bias in designedmeasures, calling for a more rigorous inspection ofpotential bias.To inspect the construct validity of the measuresto the intended constructs, we employ the multitrait-multimethod (MTMM) table methodology intro-duced by (Campbell and Fiske, 1959). This tableconventionally presents pairwise correlations of ob-served measure scores on the off-diagonals and thesubset consistency of each score on the diagonals.",
  "Datasets and Experimental Settings": "We leverage the Pile dataset, a comprehensive col-lection curated by (Gao et al., 2020), which standsas the largest publicly available dataset for pre-training language models like Pythia (Bidermanet al., 2023).This dataset includes a vast 825GiB of diverse data and encompasses 22 smaller,high-quality datasets spanning multilingual textand code. Its rich diversity facilitates the extrac-tion of a wide array of concepts, crucial for ourevaluation framework.For the backbone model, we choose Pythia dueto its pre-training on the Pile dataset, ensuringconsistent knowledge representation between thetraining and explanation phases. Additionally, we include GPT-2 (Radford et al., 2019) to ensurethe consistency of our findings across backbones(Appx. E). Further details on these models are pro-vided in Tab. 6. To eliminate the impact of ran-dom fluctuations, we test each measure across 10batches, each comprising 256 sentences with 128tokens, totaling 327,680 tokens.",
  "Comparison of Evaluation Measures": "In this section, we evaluate our proposed concept-based explanation measures, employing the meta-evaluation method for thorough assessment. Toensure a fair comparison, we randomly sampled100 concepts extracted by each unsupervised base-line applicable to the language domain on thesame backbone model, including Neuron-basedmethod (Bills et al., 2023) and Sparse Autoen-coder (Cunningham et al., 2023). We primarilyintroduce results from the middle layer of Pythia-70M, with other consistent results across differentlayers and models in Appx. E. Due to the possibil-ity of highly enhanced tokens not appearing in thedataset, we apply UCI and UMass measures onlyon the input side.",
  "Subset consistency provides further filtering of": "present measures with a threshold of 0.9 (Nunnallyand Bernstein, 1994), as shown in . For thefaithfulness family, GRAD-Loss shows an undesir-able low consistency, probably due to the couplingof gradient and loss during training. For the read-ability family, IN-UCI and IN-Umass is less accept-able, attributing to the diverse nature of differentconcepts n-grams. Moreover, their capability tocapture semantic similarity is also less desirableaccording to a case study shown in Appx. D Inter-rater reliability is tested on human evalu-ation of readability. The concepts used for analysisabove are scored by each human labeler with ahigh school level of English proficiency. They areblinded to the source method for the generated con-cepts and are tasked with scoring each concept ona scale of 1 to 5 based on two criteria: input read-ability and output readability. The recruitment ofthe experts and the setting of the user study aredetailed in Appx. G.",
  "Concurrent validity. In this experiment, wetreat the user study results for readability as a crite-rion measure. Tab. 4 shows how well existing auto-matic measures for readability correlate with user": "study results. IN-EmbCos is the top-performingmeasure to predict input readability (IR), and OUT-EmbCos is the best in predicting output readabil-ity (OR). This demonstrates the effectiveness ofour coherence-based measure EmbCos as an ap-proximation of human evaluation. Compared withLLM-based measure that requires expensive APIcalls to GPT-4, EmbCos has a stronger correlationwith human labels while requiring a much smallercomputational cost. We recommend EmbCos as aninexpensive substitute for human evaluation, espe-cially on large-scale evaluations. Yet human evalu-ation is still needed for more rigorous analysis.In , the off-diagonals visually demonstratethe construct validity between our proposed mea-sures. Our observations are as follows. ABL-Loss ABL-Div ABL-TClass ABL-PClass GRAD-TClass GRAD-PClass IN-EmbDist IN-EmbCos OUT-EmbDist OUT-EmbCos ABL-Loss ABL-Div ABL-TClass ABL-PClass GRAD-TClass GRAD-PClass IN-EmbDist IN-EmbCos OUT-EmbDist OUT-EmbCos 0.930.590.410.350.150.130.110.270.160.04 0.590.960.430.430.170.170.080.300.190.11 0.410.430.960.890.600.580.030.160.160.08 0.350.430.890.960.590.600.020.160.150.08 0.150.170.600.590.970.900.010.030.110.06 0.130.170.580.600.900.970.000.030.110.06 0.110.080.030.020.010.000.980.500.100.08 0.270.300.160.160.030.030.500.970.140.06 0.160.190.160.150.110.110.100.141.000.38 0.040.110.080.080.060.060.080.060.381.00 A. Faithfulness B. Readability 0.2 0.4 0.6 0.8 1.0",
  ": The MTMM table of the evaluation measures:1) subset consistency is shown on the diagonals; 2) con-struct validity is displayed on the off-diagonals": "Divergent Validity is inspected via correlationbetween unrelated measures. Measures of faith-fulness (A) show a low correlation (0.0-0.3) withmeasures of readability (B), revealing their distinctnature, which is as expected. Input readability andoutput readability are also divergent (correlationless than 0.15), demonstrating concepts uniquepatterns on both sides. While previous efforts onreadability mostly focus on the input side, morecareful inspection on the output side is needed.Convergent Validity is inspected via correlationbetween measures of the same construct. Faithful-ness measures (A) displayed moderate correlationsin general, averaging around 0.5. Agreement be-tween measures with the same perturbation strategyor difference measurement is higher than others,indicating their potential relation. *-TClass and *-PClass showed a higher correlation, due to theconsistency between prediction and true classes inwell-trained language models. In the meantime, theagreement of readability measures (B) on either theinput side or output side is moderate.Our findings are consistent across different lay-ers and backbones. Interested readers may refer toAppx. E for detailed results.",
  "Comparison of Explanation Methods": "We conducted a comparative assessment of threedifferent baseline methods on the language domain,including the concepts of neuron (Bills et al., 2023),sparse autoencoder (Cunningham et al., 2023), andTCAV (Kim et al., 2018). The results for boththe neuron and sparse autoencoder were computedas the average values across 100 randomly sam-pled concepts from the concept set. We derive thesupervised concept using TCAV following (Kimet al., 2018; Xu et al., 2024). Initially, LLMs harm-ful QA (Bhardwaj and Poria, 2023) is treated aspositive examples, and random texts are treated asnegative examples. Their hidden representationsare used to train a linear classifier, which aims todifferentiate the representations of positive exam-ples from negative ones. The trained classifier istreated as the concepts activation function. Theresults of this analysis are shown in . GRAD-TClass GRAD-PClass ABL-Loss ABL-TClass ABL-PClass ABL-Div IN-EmbCos OUT-EmbDist OUT-EmbCos IN-User OUT-User",
  ": Performance of different baselines on repre-sentative measures": "Sparse autoencoder surpasses the neuron-basedmethods across all evaluated measures, which isas expected.Nevertheless, as an unsupervisedmethod, it falls short of TCAV on these same mea-sures. This implies that the average quality of theconcepts it extracted is not as high as the conceptsderived from supervised counterparts. Addition-ally, the discrepancy between human ratings fordifferent baseline methods is smaller than that be-tween other readability measures. Upon detailed",
  "Neurongap, als, going, 3, mit,maybe, True, t, c, URNlement, ters, right,uki, ter,ecycle, aut, , er, \\n\\n": ": Patterns that maximally activate some demonstrative concepts of the baselines. indicates space. Forsparse autoencoder, we selected one concept from both the top 10% and bottom 10% based on the average rankresults of IN-EmbCos and OUT-EmbCos. For the neuron method, we only showcased the top concepts. analysis of the results, it appears that human raterstend to give less discriminative scores ranging from2 to 4, rarely awarding a 1 or 5, whereas automatedmeasures show a greater range in scoring.We also present a case study in Tab. 5 to visuallyillustrate the readability of concepts extracted bythe three baselines. Firstly, TCAVs extracted con-cept shows high readability, with both input andoutput key tokens strongly tied to the harmful QAtraining theme. Secondly, The performance of thesparse autoencoder is notably inconsistent, whoseconcept set varies widely in readability measures.However, on average, upon observing many con-cepts, we found that the readability of conceptsextracted by sparse autoencoder surpasses that ofneurons. This suggests that the sparse dictionaryparadigm generally enhances the quality of the en-tire concept set, mitigating the issue of superposi-tion (Elhage et al., 2022).Besides, we found that LLM has learned aseemingly redundant yet interesting pattern for thefirst concept shown for sparse autoencoder (e.g.,north,west, east, South, North, South, northern).Though these tokens are quite similar for humans,we do not know whether they are considered thesame for LLMs. The embedding similarity betweenthese tokens reflects LLMs ability to model themjust like how humans perceive them as similar.",
  "This paper introduced two automatic evaluationmeasures, readability and faithfulness, for concept-based explanations. We first formalize a generaldefinition of concepts and quantify faithfulness un-": "der this formalization. Then, we approximate read-ability via the coherence of patterns that maximallyactivate a concept. Another key contribution of thispaper is that we describe a meta-evaluation methodfor evaluating the reliability and validity of theseevaluation measures across diverse settings basedon measurement theory. Through extensive experi-mental analysis, we inform the selection of expla-nation evaluation measures, hoping to advance thefield of concept-based explanation.",
  "Limitations": "Our framework may not encompass the entiretyof the concept-based explanation landscape. Al-though the focus on readability and faithfulnessaligns with prior research suggestions (Jacovi andGoldberg, 2020; Lage et al., 2019) and representscore components of evaluating concept-based ex-planations. We acknowledge that our study rep-resents a modest step towards evaluating concept-based explanations. Future research on other as-pects like robustness and stability is necessary.Topic coherence is not designed to be the ulti-mate or perfect solution for measuring readability.Other aspects of readability, such as meaningful-ness (Ghorbani et al., 2019), may also worth explor-ing. In the future, we are interested in investigatinghow these aspects could be quantified automati-cally, building a more comprehensive landscape ofreadability.Due to limited GPU resources and budget con-straints, we used smaller versions of LLM, focusingprimarily on the 3rd layer of Pythia-70M for ouranalysis. And our evaluation of the LLM-Score was restricted to 200 concepts, incurring a cost ofaround $1 for a single concept. While this setup,on par with (Cunningham et al., 2023) and moregeneral than (Bricken et al., 2023), allowed forfast analysis and comparison with existing litera-ture, expanding our analysis to larger models couldyield more insightful conclusions in the future.",
  "Acknowledgements": "This work was supported by the National Nat-ural Science Foundation of China (NSFC) (NO.62476279), Major Innovation & Planning Interdis-ciplinary Platform for the Double-First Class Ini-tiative, Renmin University of China, Kuaishou, andthe Fundamental Research Funds for the CentralUniversities, and the Research Funds of RenminUniversity of China No. 24XNKJ18. This workwas partially done at Beijing Key Laboratory of BigData Management and Analysis Methods and En-gineering Research Center of Next-Generation In-telligent Search and Recommendation, Ministry ofEducation. This research was supported by PublicComputing Cloud, Renmin University of China.",
  "StevenBills,NickCammarata,DanMoss-ing,Henk Tillman,Leo Gao,Gabriel Goh,IlyaSutskever,JanLeike,JeffWu,andWilliamSaunders.2023.Languagemod-els can explain neurons in language models": "Trenton Bricken, Adly Templeton, Joshua Batson,Brian Chen, Adam Jermyn, Tom Conerly, NickTurner, Cem Anil, Carson Denison, Amanda Askell,Robert Lasenby, Yifan Wu, Shauna Kravec, NicholasSchiefer, Tim Maxwell, Nicholas Joseph, ZacHatfield-Dodds, Alex Tamkin, Karina Nguyen,Brayden McLean, Josiah E Burke, Tristan Hume,Shan Carter, Tom Henighan, and ChristopherOlah. 2023.Towards monosemanticity: Decom-posing language models with dictionary learning.Transformer Circuits Thread. Https://transformer-circuits.pub/2023/monosemantic-features/index.html. Christopher Burger, Lingwei Chen, and Thai Le. 2023.are your explanations reliable? investigating thestability of lime in explaining text classifiers by mar-rying xai and adversarial attack. In Proceedings ofthe 2023 Conference on Empirical Methods in Natu-ral Language Processing, pages 1283112844.",
  "Zhi Chen, Yijie Bei, and Cynthia Rudin. 2020. Con-cept whitening for interpretable image recognition.Nature Machine Intelligence, 2(12):772782": "Zhongxia Chen, Xiting Wang, Xing Xie, Tong Wu, Guo-qing Bu, Yining Wang, and Enhong Chen. 2019b.Co-attentive multi-task learning for explainable rec-ommendation. In IJCAI, volume 2019, pages 21372143. Elizabeth Clark, Tal August, Sofia Serrano, NikitaHaduong, Suchin Gururangan, and Noah A. Smith.2021. All thats human is not gold: Evaluatinghuman evaluation of generated text. In Proceedingsof the 59th Annual Meeting of the Association forComputational Linguistics and the 11th InternationalJoint Conference on Natural Language Processing(Volume 1: Long Papers), pages 72827296, Online.Association for Computational Linguistics. Julien Colin, Thomas Fel, Rmi Cadne, and ThomasSerre. 2022. What i cannot predict, i do not under-stand: A human-centered evaluation framework forexplainability methods. Advances in Neural Informa-tion Processing Systems, 35:28322845.",
  "Kien Do and Truyen Tran. 2019. Theory and evalua-tion metrics for learning disentangled representations.arXiv preprint arXiv:1908.09961": "Nelson Elhage, Tristan Hume, Catherine Olsson,Nicholas Schiefer, Tom Henighan, Shauna Kravec,Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain,Carol Chen, et al. 2022. Toy models of superposition.arXiv preprint arXiv:2209.10652. Thomas Fel, Victor Boutin, Mazda Moayeri, RmiCadne, Louis Bethune, Mathieu Chalvidal, ThomasSerre, et al. 2023a. A holistic approach to unifyingautomatic concept extraction and concept importanceestimation. arXiv preprint arXiv:2306.07304. Thomas Fel, Agustin Picard, Louis Bethune, ThibautBoissin, David Vigouroux, Julien Colin, RmiCadne, and Thomas Serre. 2023b. Craft: Conceptrecursive activation factorization for explainability.In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 27112721.",
  "Francis Galton. 1877. Typical laws of heredity 1. Na-ture, 15(388):492495": "Jingyue Gao, Xiting Wang, Yasha Wang, and Xing Xie.2019. Explainable recommendation through attentivemulti-view learning. In Proceedings of the AAAI Con-ference on Artificial Intelligence, volume 33, pages36223629. Leo Gao, Stella Biderman, Sid Black, Laurence Gold-ing, Travis Hoppe, Charles Foster, Jason Phang, Ho-race He, Anish Thite, Noa Nabeshima, et al. 2020.The pile: An 800gb dataset of diverse text for lan-guage modeling. arXiv preprint arXiv:2101.00027. Asma Ghandeharioun, Been Kim, Chun-Liang Li, Bren-dan Jou, Brian Eoff, and Rosalind W Picard. 2021.Dissect: Disentangled simultaneous explanations viaconcept traversals. arXiv preprint arXiv:2105.15164.",
  "Robert R Hoffman, Shane T Mueller, Gary Klein,and Jordan Litman. 2018.Metrics for explain-able ai: Challenges and prospects. arXiv preprintarXiv:1812.04608": "David M. Howcroft, Anya Belz, Miruna-AdrianaClinciu, Dimitra Gkatzia, Sadid A. Hasan, SaadMahamood, Simon Mille, Emiel van Miltenburg,Sashank Santhanam, and Verena Rieser. 2020.Twenty years of confusion in human evaluation: NLGneeds evaluation sheets and standardised definitions.In Proceedings of the 13th International Conferenceon Natural Language Generation, pages 169182,Dublin, Ireland. Association for Computational Lin-guistics.",
  "Maurice G Kendall. 1938.A new measure of rankcorrelation. Biometrika, 30(1/2):8193": "Been Kim, Martin Wattenberg, Justin Gilmer, CarrieCai, James Wexler, Fernanda Viegas, et al. 2018. In-terpretability beyond feature attribution: Quantitativetesting with concept activation vectors (tcav). In In-ternational conference on machine learning, pages26682677. PMLR. Pang Wei Koh, Thao Nguyen, Yew Siang Tang, StephenMussmann, Emma Pierson, Been Kim, and PercyLiang. 2020. Concept bottleneck models. In In-ternational conference on machine learning, pages53385348. PMLR. Avinash Kori, Parth Natekar, Ganapathy Krishnamurthi,and Balaji Srinivasan. 2020. Abstracting deep neu-ral networks into concept graphs for concept levelinterpretability. arXiv preprint arXiv:2008.06457.",
  "John J McCarthy and Alan Prince. 1995. Faithfulnessand reduplicative identity. Linguistics DepartmentFaculty Publication Series, page 10": "Georgii Mikriukov, Gesina Schwalbe, Christian Hellert,and Korinna Bade. 2023. Evaluating the stability ofsemantic concept representations in cnns for robustexplainability. arXiv preprint arXiv:2304.14864. David Mimno, Hanna Wallach, Edmund Talley, MiriamLeenders, and Andrew McCallum. 2011. Optimizingsemantic coherence in topic models. In Proceed-ings of the 2011 conference on empirical methods innatural language processing, pages 262272.",
  "Karen Simonyan and Andrew Zisserman. 2014. Verydeep convolutional networks for large-scale imagerecognition. arXiv preprint arXiv:1409.1556": "Chandan Singh, Aliyah R Hsu, Richard Antonello,Shailee Jain, Alexander G Huth, Bin Yu, and Jian-feng Gao. 2023. Explaining black box text modulesin natural language with language models. arXivpreprint arXiv:2305.09863. Sanchit Sinha, Mengdi Huai, Jianhui Sun, and AidongZhang. 2023. Understanding and enhancing robust-ness of concept-based models. In Proceedings ofthe AAAI Conference on Artificial Intelligence, vol-ume 37, pages 1512715135.",
  "Xiting Wang, Liming Jiang, Jose Hernandez-Orallo,Luning Sun, David Stillwell, Fang Luo, and XingXie. 2023c. Evaluating general-purpose ai with psy-chometrics. arXiv preprint arXiv:2310.16379": "Xiting Wang, Kunpeng Liu, Dongjie Wang, Le Wu,Yanjie Fu, and Xing Xie. 2022. Multi-level recom-mendation reasoning over knowledge graphs withreinforcement learning. In Proceedings of the ACMWeb Conference 2022, pages 20982108. Chenwang Wu, Xiting Wang, Defu Lian, Xing Xie, andEnhong Chen. 2023a. A causality inspired frame-work for model interpretation. In Proceedings ofthe 29th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining, pages 27312741. Zhengxuan Wu, Karel DOosterlinck, Atticus Geiger,Amir Zur, and Christopher Potts. 2023b.Causalproxy models for concept-based model explanations.In International conference on machine learning,pages 3731337334. PMLR. Ziang Xiao, Susu Zhang, Vivian Lai, and Q Vera Liao.2023. Evaluating evaluation metrics: A frameworkfor analyzing nlg evaluation metrics using measure-ment theory. In Proceedings of the 2023 Conferenceon Empirical Methods in Natural Language Process-ing, pages 1096710982. Zhihao Xu, Ruixuan Huang, Xiting Wang, FangzhaoWu, Jing Yao, and Xing Xie. 2024.Uncoveringsafety risks in open-source llms through concept acti-vation vector. Advances in Neural Information Pro-cessing Systems. Ruichao Yang, Xiting Wang, Yiqiao Jin, Chaozhuo Li,Jianxun Lian, and Xing Xie. 2022. Reinforcementsubgraph reasoning for fake news detection. In Pro-ceedings of the 28th ACM SIGKDD Conference onKnowledge Discovery and Data Mining, pages 22532262.",
  "Weikai Yang, Mengchen Liu, Zheng Wang, and ShixiaLiu. 2024. Foundation models meet visualizations:Challenges and opportunities. Computational VisualMedia, pages 126": "Chih-Kuan Yeh, Been Kim, Sercan Arik, Chun-LiangLi, Tomas Pfister, and Pradeep Ravikumar. 2020. Oncompleteness-aware concept-based explanations indeep neural networks. Advances in neural informa-tion processing systems, 33:2055420565. Hanyu Zhang, Xiting Wang, Xiang Ao, and Qing He.2024. Distillation with explanations from large lan-guage models.In Proceedings of the 2024 JointInternational Conference on Computational Linguis-tics, Language Resources and Evaluation (LREC-COLING 2024), pages 50185028. Ruihan Zhang, Prashan Madumal, Tim Miller, Krista AEhinger, and Benjamin IP Rubinstein. 2021. Invert-ible concept-based explanations for cnn models withnon-negative concept activation vectors. In Proceed-ings of the AAAI Conference on Artificial Intelligence,pages 1168211690. Andy Zou, Long Phan, Sarah Chen, James Campbell,Phillip Guo, Richard Ren, Alexander Pan, XuwangYin, Mantas Mazeika, Ann-Kathrin Dombrowski,et al. 2023.Representation engineering: A top-down approach to ai transparency. arXiv preprintarXiv:2310.01405.",
  "ATaxonomies": "In this section, we present a taxonomy of prior auto-matic measures for evaluating concept-based expla-nations based on existing literature on evaluatingexplainable AI (Hoffman et al., 2018; Jacovi andGoldberg, 2020; Colin et al., 2022). providesa summarized mind map, offering a visual repre-sentation of the various aspects by which concept-based explanation methods can be assessed. Weendeavored to use the original terminologies asthey appear in the cited works, emphasizing thepurposes for which these measures were developed.Due to the evolving nature of the field, some mea-sures might share similarities in their meanings orcomputational methods, which could lead to per-ceived overlap.This makes the selection of suitable evalua-tion measures hard for practitioners in the fieldof concept-based explanation Therefore, there is apressing need for a more unified landscape in theevaluation of concept-based methods to facilitatesubstantial progress in the field. To address poten-tial confusion, the evaluation measures we proposein this paper seek to clarify and distinguish betweenthe different aspects of evaluation. We aim to pro-",
  "CApplicability to image domain": "In our paper, we mostly focus on LLMs as back-bone models. Here we elaborate on how the pro-posed measures can be extended to the vision do-main.For readability, we can create tokens by adopt-ing a methodology similar to LIME (Ribeiro et al.,2016). Specifically, we can segment each imageinto superpixels and regard each superpixel as atoken in text. These superpixels embeddings canthen be obtained using pre-trained image modelslike VGG (Simonyan and Zisserman, 2014), andcoherence-based measures can be applied by as-sessing the similarity of these embeddings. Whileextending measures like UCI/UMass to image tasksmay present challenges, it remains feasible byfirst transcribing superpixels into text using vision-language models like CLIP (Radford et al., 2021)and then calculating their co-occurrence. Yet con-sidering the low reliability indicated in Sec. 5.2.1as well as its original initiative for the language do-main, it might be redundant to explore this variant.Furthermore, faithfulness measures, operatingon hidden and output spaces, are inherently inde-pendent of data modality and can be directly ap-plied to image tasks. In general, our method can beused as long as a concept can be formulated with avirtual activation function (Sec. 2), which takes agiven hidden representation in the model as inputand outputs the degree a concept is activated. As",
  "DCase Study": "In this section, we present an illustrative case ofthe readability measures calculated via coherence-based measures and the LLM-based measure. Wehave the following observations.First, extracted topics via highly activated con-texts align well with and even exceed explanationsgenerated by LLM (). As the number ofsamples inputted to LLM is restricted to a maxi-mum context window and pricing limits (128,800tokens and $0.03/1K tokens for GPT-4), explana-tions generated by LLM are only limited to theinformation presented. However, our coherency-based measures can search from a broader range ofsamples, looking for top-activating contexts to pro-vide a more comprehensive explanation, as shownin .Second, deep embedding-based measures arebetter at capturing semantic similarities. The firstcase illustrated in (a) is ranked as the 1stamong the 200 concepts evaluated by IN-EmbCosand 3rd by LLM, as it consistently activates onwords related to geographical directions as sug-gested by LLM. However, IN-UCI only assigneda rank of 172. This is largely attributed to the factthat these terms may only occur once in a sample,showing one single direction, leading to low wordco-occurrence counts.Third, coherency-based measures can compen-sate for failure cases of LLM. For the 3rd caseshown in , we can observe that it activateson expressions related to LATEX. However, as LLMcan only observe limited examples, it fails to in-clude other attributes than mathematical symbolsand markup, thus failing to simulate activationsthat align with the original activation. We approachthis challenge by extracting topics from a largerrange of samples.Overall, these findings are consistent with theones disclosed in Sec. 5.2.2, offering a more in-tuitive understanding of the measures advantagesand weaknesses.",
  ": Statistical model properties for subject models.#Layer, #Param, and #Dimension represent the numberof layers, parameters, and dimensions respectively": "results from the 1st ( (a)) and 5th ( (b))layers of Pythia-70M, as well as results from the6th layer of GPT2-small ( (c)). Across theselayers, reliability and validity results are consis-tent, with measures showing slightly better subsetconsistency in deeper layers. We speculate thatas the layers deepen, the model discards irrelevantinformation and noise, leading to more stable androbust representations that are subject to less ran-dom error and exhibit higher consistency. Notably,the validity results on the 6th layer of GPT2-smallalign with our main findings ( (c)), fluctuatingwithin a reasonable range, typically less than 0.1.These results underscore larger language modelssuperior ability and reliability compared to theircounterparts, such as the 3rd layer of Pythia-70M.",
  "FImplementation Details": "In our implementation, we employ the Pile dataset,truncating input to 1024 tokens for efficient anal-ysis. Both the Pile dataset and the backbones uti-lized are accessible for download from the HuggingFace Hub. To compute embedding-based readabil-ity measures, we leverage the backbone modelsembedding matrix to extract token embeddings.All correlation metrics utilized in our analysis arecalculated using the scipy package.Following (Bills et al., 2023; Cunningham et al., 2023), we adopt the extraction of neuron activa-tion as the output of the MLP layer in each layer,where each dimension corresponds to a neuron.Specifically, for a feed-forward layer FFN(hin) =GeLU(hinW1)W2, the MLP output/neurons areGeLU(hinW1). Furthermore, the disentanglement-based baseline can utilize these extracted neu-rons as inputs to discover mono-semantic concepts,leveraging sparse autoencoders. We obtain the con-cept activation function of TCAV following (Kimet al., 2018). We treat LLMs harmful QA (Bhard-waj and Poria, 2023) as positive examples, andrandom texts as negative examples. Then, a linearclassifier is trained on their hidden representationsto classify harmful examples. The trained classi-fiers output function is regarded as the concepts",
  "(c) Case 3": ": Topics extracted for calculating coherency-based measures. Spaces are replaced by for visualization.These topics align well with LLM-generated explanations in while providing fine-granular information. : A case study on LLM-based measure forreadability measures. We present three cases with GPT4-generated explanation, original activation, and GPT4-simulated activation. GPT-4 performed well in the firsttwo cases but worse in the third case. activation function.We employ a sparse autoencoder proposedby (Cunningham et al., 2023) to obtain conceptsfor the disentanglement-based baseline. The pro-cess involves running the model to be interpretedover the text while caching and saving activationsat a specific layer, as narrated above. These acti-vations then constitute a dataset used for trainingthe autoencoders. The training is executed withthe Adam optimizer, employing a learning rate of1e-3, and processing 11 billion activation vectorsfor one epoch. The dictionary size is set at 8 timesthe hidden spaces dimension. A single trainingrun with this data volume is completed in approx-imately 13 hours on a single RTX 3090 GPU. Tobalance between sparsity and accuracy, we set thecoefficient on the L1 loss to 0.5 for the 3rd layer ofPythia-70M.Its important to note that our approach is inline with the original experimental setups outlinedin (Bills et al., 2023; Cunningham et al., 2023; Kimet al., 2018). For a more detailed understanding ofthe implementation settings, interested readers areencouraged to refer to the original papers.In calculating faithfulness, GRAD-Div is ne-glected as gradient operation is only applicable to one variable at a time, applying gradient opera-tion to the whole output class is computationallyexpensive. To aggregate the effect on each token,they are weighted by their activations. Samples thatexhibit high activation levels regarding a specificconcept are deemed more relevant to the conceptempirically and therefore receive higher weights.This weighting scheme ensures that the most repre-sentative samples contribute more significantly tothe evaluation, enhancing the fidelity of the faithful-ness measure in capturing the alignment betweenthe models behavior and the intended concept.For LLM-based readability score (Bills et al., 2023), we follow OpenAIs pipeline as illustratedin (Bills et al., 2023).We show a detailedprompt for generating an explanation/semantic ex-pression of a concept based on its activation in.We adopt this adjusted algorithm withgpt-4-turbo-preview as the simulator, due tonew limitations in calculating logprobs on the in-put side. When extracting patterns that maximallyactivate a concept, we keep only the top 10 tokenswith the largest activation or contribution to high-activation tokens.",
  "GUser study settings": "In our user study, we recruited 3 human labelersto evaluate the readability of 200 concepts. Thehuman labelers possess a high school level of En-glish proficiency, allowing for easy comprehensionof the concepts. These labelers were selected fromwithin our academic institution to ensure a consis-tent educational background, which is pertinent tothe readability aspect of our study. To maintain thequality of labeling, we implemented a compensa-tion structure that rewards the labelers based on thenumber of concepts they evaluate. This approachwas designed to incentivize thorough and carefulconsideration of each concept.During the study, labelers were required to com-plete their assessments within a five-minute win- ABL-Loss ABL-Div ABL-TClass ABL-PClass GRAD-TClass GRAD-PClass IN-EmbDist IN-EmbCos OUT-EmbDist OUT-EmbCos ABL-Loss ABL-Div ABL-TClass ABL-PClass GRAD-TClass GRAD-PClass IN-EmbDist IN-EmbCos OUT-EmbDist OUT-EmbCos 0.510.520.540.440.190.190.190.280.220.03 0.520.580.560.610.180.200.240.400.240.04 0.540.560.800.860.380.400.280.350.190.01 0.440.610.860.810.360.390.270.370.190.02 0.190.180.380.360.910.890.100.120.08 -0.02 0.190.200.400.390.890.910.100.120.08 -0.01 0.190.240.280.270.100.100.980.620.270.12 0.280.400.350.370.120.120.620.970.370.13 0.220.240.190.190.080.080.270.371.000.37 0.030.040.010.02 -0.02 -0.01 0.120.130.371.00 A. Faithfulness B. Readability 0.0 0.2 0.4 0.6 0.8 1.0",
  "(a) 1st layer of Pythia-70M": "ABL-Loss ABL-Div ABL-TClass ABL-PClass GRAD-TClass GRAD-PClass IN-EmbDist IN-EmbCos OUT-EmbDist OUT-EmbCos ABL-Loss ABL-Div ABL-TClass ABL-PClass GRAD-TClass GRAD-PClass IN-EmbDist IN-EmbCos OUT-EmbDist OUT-EmbCos 0.990.640.310.290.090.060.090.170.180.10 0.640.990.410.420.090.090.080.150.190.06 0.310.410.980.940.610.60 -0.09 0.010.270.12 0.290.420.940.980.610.62 -0.09 0.010.270.11 0.090.090.610.610.990.91 -0.08 0.040.310.16 0.060.090.600.620.910.99 -0.08 0.040.310.16 0.090.08 -0.09 -0.09 -0.08 -0.08 0.900.640.030.06 0.170.150.010.010.040.040.640.940.120.12 0.180.190.270.270.310.310.030.121.000.34 0.100.060.120.110.160.160.060.120.341.00 A. Faithfulness B. Readability 0.0 0.2 0.4 0.6 0.8 1.0",
  ": The MTMM table of the evaluation measures: 1) subset consistency is shown on the diagonals; 2)construct validity is displayed on the off-diagonals": "dow for each concept. This time constraint wasestablished to simulate a realistic scenario in whichusers make quick judgments about concept read-ability. Each of the three labelers was presentedwith the same set of 200 concepts to ensure consis-tency in the evaluation process.Given input or output side tokens for a concept,each of our human labelers gives one readabilityscore by simultaneously considering the three as-pects, including semantic, grammatical or syntactic,and morphological information. More specifically,a concept is considered highly readable, if it isrelated to a specific topic such as computer sys-tems (semantically interesting), is associated witha specific grammar or syntax (grammatically orsyntactically interesting), or consists of tokens thatshare a similar structure or a form such as all beingusable as suffixes for a certain token (morphologi-cally interesting).We demonstrate guidelines that were providedto the labelers. These guidelines were crafted toassist the labelers in their task and to standardizethe evaluation criteria across all participants. Theguidelines are as follows:Welcome to the user study on evaluating the read-ability of concepts extracted from concept-basedexplanations. Your valuable insights will contributeto advancing our understanding of these explana-tions and improving their interpretability. Beloware the instructions for scoring each concept:Task Overview. You will be provided with a listof concepts, each comprising three parts:",
  "MorphologicalInformation:Considerwhether the given tokens share a similarstructure or form, such as all being usable assuffixes for a certain token": "Scoring Procedure. Please provide a score forthe input side, reflecting the readability of tokensrelated to the concept in the input. Additionally,assign a score for the output side, indicating thereadability of tokens related to the concept in theoutput. Your engagement in this scoring procedurewill significantly contribute to the comprehensive-ness of our study. Thank you for your participation!"
}