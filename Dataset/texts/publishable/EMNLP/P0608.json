{
  "Abstract": "In linguistics, all languages can be consideredas symbolic systems, with each language re-lying on symbolic processes to associate spe-cific symbols with meanings. In the same lan-guage, there is a fixed correspondence betweenlinguistic symbol and meaning. In differentlanguages, universal meanings follow varyingrules of symbolization in one-to-one correspon-dence with symbols. Most work overlooks theproperties of languages as symbol systems. Inthis paper, we shift the focus to the symbolicproperties and introduce MTLS: a pre-trainingmethod to improve the multilingual capabilityof models by Making Texts into Linguistic Sym-bols. Initially, we replace the vocabulary inpre-trained language models by mapping rela-tions between linguistic symbols and semantics.Subsequently, universal semantics within thesymbolic system serve as bridges, linking sym-bols from different languages to the embeddingspace of the model, thereby enabling the modelto process linguistic symbols. To evaluate theeffectiveness of MTLS, we conducted experi-ments on multilingual tasks using BERT andRoBERTa, respectively, as the backbone. Theresults indicate that despite having just over12,000 pieces of English data in pre-training,the improvement that MTLS brings to multilin-gual capabilities is remarkably significant.",
  "Introduction": "All languages can be considered as symbolic sys-tems (De Saussure, 1989). This is a generally ac-cepted linguistic concept. Indeed, the meaningsof words are prescribed by human convention, andall languages rely on symbolic processes to asso-ciate specific symbols with meanings. However, innatural language processing (NLP), languages areoften treated as complex systems of word forma-tion methods and syntactic rules, and their inherentproperties as symbols are often overlooked. Inthis paper, we focus on the symbolic properties of : A brief overview of MTLS. (a) illustratesthe benefits of employing mapping relations betweenlinguistic symbols and text. (b) demonstrates the meta-symbol system can serve as a bridge between linguisticsymbols and the embedding space. languages and conduct a preliminary investigation.We propose MTLS: a novel pre-training method toimprove the multilingual capability of models byMaking Texts into Linguistic Symbols1. Remark-ably, MTLS does not rely on extensive multilingualcorpora, massive computational resources, or com-plex a priori knowledge. It can significantly im-prove the multilingual performance of pre-trainedlanguage models (PLMs).By making texts into linguistic symbols, it ispossible to obtain textual embeddings in any lan-guage without vocabulary. Pre-trained languagemodels (Devlin et al., 2019; Brown et al., 2020; Heet al., 2022), including multilingual models (Con-neau et al., 2020; Chi et al., 2022; Xue et al., 2021),assign ids to tokens via vocabulary and then ob-tain textual embeddings. Language models typ-",
  "The pixel images rendered according to the word compo-sition and writing system are called linguistic symbols": "ically support a finite vocabulary of categoricalinputs, e.g. characters, subwords or even words,and much effort has been devoted to vocabularyconstruction (Gerz et al., 2018). As the numberof languages the model can handle increases, thevocabulary needs to be extended or reconstructed,which is a burdensome task. Taking into accountthe symbolic properties of languages, linguisticsymbols always correspond to concrete abstract se-mantics and have unique compositions or structuresfrom a visual perspective. In MTLS, words are firstconverted into linguistic symbols and rendered aspixel images. Then, by replacing the unique encod-ing in vocabularies with the linguistic symbols ofthe words, the problem of multilingual vocabularyconstruction and out-of-vocabulary (OOV) can beavoided, as shown in (a).Universal meanings are represented by differ-ent linguistic symbols in different symbol systems.These universal meanings underlying all humannatural languages are referred to as irreducible se-mantic cores (Wierzbicka, 1999). This semanticcore can be used as a semantic bridge for transfor-mations between different languages. Based on thisnotion, some work (Sherborne and Lapata, 2022;Goswami et al., 2021; Guo et al., 2024) has beendone to construct multilingual universal representa-tions by finding universal meanings behind naturallanguages. We transfer this idea to the linguisticsymbolic perspective and call this semantic corethe meta-symbol system (MSS). The MSS can beused as a bridge between linguistic symbols of dif-ferent languages. Therefore, we propose to applythe embedding space of the MSS to represent thelinguistic symbols of any language, and then toconstruct the mapping from the embedding spaceof the MSS to the embedding space of the PLM, asshown in (b). This means that PLMs canhandle linguistic symbols in any language and donot need to be pre-trained again.In this paper, we propose SSS embedding inMTLS for obtaining symbolic embeddings of lin-guistic symbols and then mapping them to theembedding space of the PLM. SSS embeddingconsists of three components: Symbolic embed-ding, Selective embedding, and Spatial embedding.In MTLS, the SSS embedding and the PLM em-bedding layers are first jointly pre-trained. Then,the SSS embedding replaces the PLM embed-ding layers, and the multilingual capability of thePLM can be improved. We evaluate the perfor-mance of MTLS on syntactic and semantic pro- cessing tasks in multiple languages. The resultsshow that MTLS can significantly improve themultilingual capabilities of the model, but at thecost of some performance degradation in Latinscript languages. We release the code and mod-els at",
  "Related Work": "Multilingual Representation Learning is stud-ied for a variety of downstream multilingual tasks.Some approaches rely on rich multilingual corporato learn multilingual representations through exten-sive pre-training, such as mBERT (Devlin et al.,2019) and XLM-R (Conneau et al., 2020). Suchextensive pre-training can be extremely computa-tionally intensive, so some work (Pfeiffer et al.,2021; Ansell et al., 2021) has relied on adapter-based methods to fine-tune only the adapter layerduring training. Other approaches utilize multi-lingual parallel corpora to learn generic represen-tations of parallel sentences based on contrastivelearning, such as InfoXLM (Chi et al., 2021) andHICTL (Wei et al., 2020). However, obtaining ahigh quality parallel corpus is difficult, so somework has converted monolingual corpora into paral-lel corpora through translation (Kvapilkov et al.,2020; Ouyang et al., 2021), or computed semanticsimilarities in multilingual corpora and used themas supervised labels (Goswami et al., 2021). Allthese approaches rely heavily on multilingual cor-pora, while MTLS uses only monolingual data. Afurther theoretical comparison between MTLS andthese methods is given in Appendix A. Vision-based Embedding differs from main-stream vocabulary-based embedding in that it gen-erates embeddings based on the structure and con-struction of the text from a visual perspective.Some work (Meng et al., 2019; Li et al., 2021)has used vision-based embedding to obtain somepotential features of hieroglyphs on writing sys-tems. There is also some work exploring the poten-tial of vision-based embedding. Rust et al. (2022)proposed a fully vision-based PLM and obtainedresults competitive with BERT (Devlin et al., 2019).Wang et al. (2024) used vision-based embeddingand vocabulary-based embedding to construct atwo-tower model to combine the advantages of thetwo embedding construction methods. The work inthis paper is also an exploration of the potential ofvision-based embedding. : An overview of our proposed MTLS. We refer to the combination of symbolic embedding, selectiveembedding, and spatial embedding as SSS embedding. Symbolic embedding is used to obtain the embeddings oflinguistic symbols, selective embedding obtains symbolization-specific bias embeddings of linguistic symbols, andspatial embedding maps embeddings in the space of meta-symbolic systems to the embedding space of PLMs.",
  "Symbolic Embedding": "The general approach to obtaining textual embed-dings is to find the token id corresponding to thevocabulary, and then obtain the token embeddingcorresponding to the id from the embedding layer.This approach is similar to querying a dictionary,where the goal is to ensure embedding consistencyfor the same tokens and embedding variability be-tween different tokens. When words are treated aslinguistic symbols, consistency and differentiationno longer depend on the vocabulary to be main-tained. The same words necessarily have the sameglyphs or constructions, and conversely, differentwords are different.Linguistic symbols can be created by render-ing text into pixel images. The symbolic embed-dings of the text are achieved by the patch em-bedding strategy similar to Vision Transformer(Dosovitskiy et al., 2020), as shown in .Specifically, the original text is tokenized to obtaina sequence of tokens S = [t1, t2, , tn1, tn].Each token is rendered into a number of fixed-sizepatches p Rll according to the write length,and the patch sequence of each token is denoted by Pi =p1i , p2i , , pmi1i, pmii, where mi isthe number of patches needed for the token i. Thesequential concatenation of all patch sequencesis the linguistic symbol of the text, which is alsothe input F = [P1, P2, , Pn1, Pn]. Then, aconvolution operation is performed on each patchin the patch sequence to obtain the embeddingv Rd, where d is the dimension of the embedding.The convolution kernel size is equal to the patchsize. The embedding of a single token is Wi =v1i , v2i , , vmi1i, vmii, and the final symbolicembedding is E = [W1, W2, , Wn1, Wn].Note that in the token-level task, to ensure thatthe length of the symbolic embedding sequenceis equal to the length of the token sequence, thefirst patch embedding of each token is taken as thesymbolic embedding of that token, giving E =v11, v12, , v1n1, v1n. In the subsequent section,we notate the symbolic embeddings as Esymb =[v1, v2, , vn1, vn], Esymb Rnd, where n isthe length of the symbolic embedding sequence.",
  "Selective Embedding": "Universal meanings follow the symbolization rulesto connect linguistic symbols, and different symbolsystems have different symbolization rules. How-ever, there are languages in the world with verysimilar symbolization rules, and different univer-sal meanings may correspond to similar symbolsunder the same symbolization rules. Symbolic em-bedding only captures the construction and struc-tural features of symbols, and is unable to perform some self-adaptation according to the symboliza-tion rules. And the search space resulting fromsymbolic embedding is clearly insufficient to repre-sent all languages. Therefore, we design selectiveembedding based on Mixture-of-Experts (MoE)(Eigen et al., 2013; Shazeer et al., 2016). Lin-guistic symbols activate different experts to obtainsymbolization-specific bias embeddings Ebias.Most MoE-based methods (Du et al., 2022; Fe-dus et al., 2022; Xue et al., 2022) replace thefeed-forward component of the Transformer layer(Vaswani et al., 2017) with the MoE layer. EachMoE layer consists of a set of independent feed-forward networks as experts. Unlike previouswork, we set the matrix M Rdd as the ex-perts. The symbolic embedding Esymb is used asthe input to the selective embedding, and the mat-mul product of the symbolic embedding Esymb andthe expert matrix M is computed as the result of theexpert. The gating function Gate () then uses thesoftmax activation function to model the probabil-ity distribution over these experts. This distributionindicates the probability that each expert can accu-rately process the incoming symbolic embeddings.The output of the expert i can be expressed as:",
  "Gate (x, i) = softmax (x Wg) [i] ,(2)": "where x is the input symbolic embedding Esymband Wg is the parameters of the gating function.During training, the gating function is learnableand is trained to activate the best K experts for eachlinguistic symbol. During inference, the learnedgating unit dynamically selects the best K experts.Therefore, during training, more parameters can beused to represent the symbolic embedding, whichcan enrich the representation of the patch. Dur-ing inference, only sparse experts are activated, sothere is no need for excessive computation. Thefinal bias embedding of the linguistic symbol isthe weighted combination of the outputs from theselected experts. The symbolization-specific biasembeddings Ebias can be represented as:",
  "Spatial Embedding": "ThesymbolicembeddingEsymbplusthesymbolization-specific bias embedding Ebias isdenoted as the embedding Emeta in the spaceof MSS. To give the PLM the ability to handlesymbolic embeddings, it is necessary to map Emetainto the embedding space of the PLM. Simplespatial mapping methods or modules are too coarseto give the model strong generalization capabilitiesto handle as many languages as possible, evenif they have never been seen before. Therefore,we propose spatial embedding, which employs astep-by-step approach to learn the distributionaland spatial features of embeddings in the PLM.Specifically, we have designed distributionalsimilarity loss Ldist and spatial similarity lossLspat in spatial embedding. In addition, we usethe patch at the first position as the representationof the whole word in symbolic embedding. Tomake the embedding of the first patch rich enough,we use the Transformer structure in the spatialembedding to obtain a dynamic context embeddingrepresentation.In step 1, we encode the embedding Emeta in thespace of MSS into a new embedding Eh Rnd through the encoder layer of Transformer. The newembedding Eh is constrained by the distributionalsimilarity loss to have the same distribution withthe PLM embedding Et Rnd to learn the dis-tributional features. This step only constrains thedistribution of symbolic embeddings that are free inthe representation space. In particular, we computethe Kullback-Leibler (KL) divergence between thesymbolic embedding Eh and the embedding Et forthe same text. The distributional similarity loss isdefined as follows:",
  "Pt (m) .(7)": "The similarity of the distributions does not yetallow PLMs to handle symbolic embeddings accu-rately. In step 2, Eh is decoded by the decoder layerof Transformer into a new embedding representa-tion space to obtain the embedding Eg Rnd.The spatial similarity loss is then used to constrainthe embedding Eg to match the features of the PLMembedding space. First, the embedding Eg and the embedding Et are treated as being under the sameembedding space, and then contrast learning (Heet al., 2020) is used to learn the spatial features.In contrast learning, two embeddings of the samelinguistic symbol are positive examples of eachother, and the rest of the embeddings are negativeexamples. The spatial similarity loss is shown asfollows:",
  "and RoBERTa are chosen because they do not usemultilingual corpura in pre-training and do not have a strongmultilingual representation capability": "To validate the syntactic processing capabilitiesof the model, we test the performance of MTLSon the part-of-speech (POS) tagging task in mul-tilingual languages. We use data from the Uni-versal Dependencies v2.10 treebanks (Nivre et al.,2020), including the following languages: Ara-bic (ARA), Basque (EUS), Chinese (ZHO), Coptic(COP), English (ENG), Estonian (EST), Greek (ELL),Hindi (HIN), Japanese (JAN), Korean (KOR), Mal-tese (MLT), Persian (FAS), Tamil (TAM), Turkish(TUR), Vietnamese (VIE). In addition, to assess theability to understand the semantics of multilingualtexts, we validate the performance of MTLS onthe named entity recognition (NER) task. We usethe PAN-X (Pan et al., 2017), which includes thefollowing languages: Arabic, Bulgarian (BUL), Chi-nese, Czech (CES), English, French (FRA), Greek,Japanese, Korean, Persian, Russian (RUS), Tamil,Turkish, Urdu (URD), Vietnamese.",
  "POS Tagging Task": "We compare the performance of BERT androBERTa with and without MTLS in the multi-lingual POS tagging task. In order to fully vali-date the improvement of MTLS on the multilin-gual capabilities of PLMs, we test the performancein the fine-tune setting and in the cross-languagezero-shot setting 5, respectively. We also show theperformance of the multilingual models mBERT 6 (Devlin et al., 2019) and XLM-R 7 (Conneau et al.,2020). We show the results of the POS tagging taskin .Both BERT and RoBERTa perform better afterpre-training with MTLS, both in the fine-tune set-ting and in the cross-language zero-shot setting.In the fine tuning setting, the accuracy of MTLS-B and MTLS-R is significantly improved in ZHO, KOR, and COP. In particular, in COP, MTLS-B andMTLS-R achieve nearly 66.6% and 52.2% accu-racy improvements respectively. This demonstratesthat our proposed MTLS can significantly improvethe multilingual syntactic processing ability of themodel. In the cross-language zero-shot setting, theimprovement brought by MTLS is relatively aver-age. We attribute this to the fact that multilingualdata is not used in the pre-training of MTLS and isnot visible to MTLS-B and MTLS-R during cross-",
  "RoBERTa 91.574.440.897.355.367.986.173.311.725.59.515.616.039.519.6MTLS-R92.980.193.090.781.963.676.382.612.526.618.518.526.431.822.4": ": Results of the POS tagging task in the fine-tune setting and in the zero-shot setting. Accuracy is used as theevaluation metric. * indicates that the language with Latin scripts. We show partial results in this table and completeexperimental results are shown in . language zero-shot, making it difficult to activateeven strong multilingual capabilities in this setup.The trade-off for such a significant performancegain is performance degradation for languageswith Latin scripts. Both MTLS-B and MTLS-Rhave some performance degradation inENGand VIE, either in the fine-tune setting or in thezero-shot setting. The performance degradationis due to the fact that the embedding layers of theoriginal BERT and RoBERTa are replaced by theSSS embedding. The integrity of the models iscompromised, with a reduced ability to processlanguages with Latin scripts. In the pre-training ofMTLS, only 12,000 English data are used, whichis far less than the pre-training data in BERT andRoBERTa, and thus not enough to recover theoriginal English processing ability.It should be emphasised that BERT andRoBERTa perform very poorly in non-Latin scriptlanguages due to the low coverage of vocabulary inthese languages, which creates a noticeable OOVproblem. MTLS uses symbolic embedding insteadof vocabulary, so there is no OOV problem, whichis an advantage of MTLS. Only a small amount ofEnglish data is used in the pre-training of MTLS,and subwords that do not appear in the pre-trainingdataset are also unseen for MTLS-B and MTLS-R.Therefore, we believe it is fair to compare BERTand MTLS-B, RoBERTa and MTLS-R, respec-tively. In addition, we further explore the effect ofsymbolic embedding on the model in .2.Compared to mBERT and XLM-R, MTLS-Band MTLS-R perform worse. However, consider-ing that mBERT and XLM-R use massive multilin-gual corpora for pre-training, while there is only avery small amount of monolingual data in MTLS,we believe that MTLS still has some advantages.In particular, MTLS-B and MTLS-R significantly outperform mBERT and XLM-R in COP. This re-sult can be attributed to the fact that mBERT andXLM-R do not use Coptic data in their pre-training.The multilingual model still suffers from a suddendrop in performance when confronted with an un-seen language, even after a lot of resources andeffort have been spent in pre-training. There is alimit to the multilingual capability of the modelobtained by pre-training on a large multilingualcorpus. MTLS does not use any multilingual cor-pus and still achieves better performance in unseenlanguages. This demonstrates the powerful gener-alization ability of MTLS.",
  "NER Task": "To evaluate the effect of MTLS on understandingthe semantics of multilingual texts, we comparethe performance of the models on the multilingualNER task. We use the same experimental setup asin the POS tagging task and present the results ofthe NER task in .In the NER task, MTLS-B and MTLS-R still sig-nificantly outperform BERT and RoBERTa in mostlanguages in both monolingual fine-tuning andcross-lingual zero-shot, while performing poorly inLatin-written languages such as ENG and VIE. Thisresult is consistent with experimental results in thePOS tagging task. Overall, MTLS significantlyimproves the ability of the model to understandmultilingual semantics.It is worth noting that MTLS-B and MTLS-Rshow smaller performance gains on the NER task,with an average F1 score gain of less than 10 in thefine tuning setting. According to our research study,the reason may be the replacement of vocabulary-based embedding layer with symbolic embedding.In previous work, Rust et al. (2022) found thatvision-based PLMs outperform vocabulary-based",
  "MTLS-B88.626.968.118.8 w/o PT30.323.24.01.6 w/o SE75.923.161.416.0 w/o DSL80.724.267.418.4 w/o SSL42.423.98.40.9": ": Results of the ablation study on MTLS. Thetable shows the average of the results for 15 languagesseparately, and the complete results are shown in and . In the POS tagging task, accuracy isthe evaluation metric. In the NER task, the F1 scoreis the evaluation metric. FT indicates fine-tune and ZSindicates zero-shot. models in syntactic tasks and lag behind in seman-tic tasks. Our experimental results are consistentwith this finding. Therefore, we believe that MTLScan lead to more significant performance gains insyntactic tasks.",
  "Ablation Study": "To evaluate the effect of each component, we per-form the ablation study of MTLS in the POS tag-ging and NER task. For ease of analysis, we makethe following definitions:1) PT denotes pre-training on an English dataset;2) SE denotes selective embedding;3) DSL denotes distributional similarity loss;4) SSL denotes spatial similarity loss.As shown in , MTLS significantly out-performs MTLS w/o * (* indicates components) inboth tasks, demonstrating the importance of eachcomponent in MTLS. Although only 12,000 En-glish data are used in the pre-training, the effect ofthe pre-training is extremely significant. In the fine-tune setting, pre-training results in an average accu- racy improvement of nearly 58.3% in the POS tag-ging task, and there is an average F1 score improve-ment of 64.1 in the NER task. Such results reflectthe importance of pre-training and the effective-ness of the two pre-training losses. These resultsare also predictable, because the non-embeddedpart of the PLM is unable to handle direct symbolicembeddings, and pre-training is needed to constrainthe mapping of symbolic embeddings into the em-bedding representation space of the PLM. In the fine-tune setting, not using selectiveembedding (w/o SE) results in a 12.7% decreasein average accuracy in the POS tagging task anda 6.7 f1 score decrease on average in the NER task.This is because selective embedding obtains a biasembedding to the symbolization process for eachlinguistic symbol, expanding the representationspace of the MSS. A larger representation spacemeans that the symbolic embedding can containmore semantics. Using distributional similarity loss without spa-tial similarity loss (w/o SSL) can produce very poorresults. Spatial embedding employs a step-by-stepstrategy. Step 1 uses distributional similarity loss,and step 2 uses spatial similarity loss. The distri-butional similarity loss only limits the distributionof the embedding, and the spatial similarity lossalso limits the distribution to some extent, but alsogreatly limits the representation space of the em-bedding. Therefore, not using spatial similarityloss results in the embedding of the MSS not beingaccurately mapped to the embedding space of thePLM, and the PLM being unable to process thesymbolic embedding. In contrast, using only spa-tial similarity loss (w/o DSL) may result in reducedgeneralization of the model due to overly stringentconstraints, thus affecting performance.",
  "Parameter Analysis": "We further analyze MTLS on the number of param-eters. Since the SSS embedding is used to replacethe PLM embedding layer, we only compare theparameters of the PLM embedding layer with thoseof the SSS embedding, as shown in . Theembedding parameters of BERT and RoBERTa aremuch less than the embedding parameters of mul-tilingual models. This is because the vocabularyin multilingual models needs to cover all the sub-words of the language as much as possible, and theincrease of the subwords means the increase of theembedding parameters. And the number of parame-ters in SSS embedding is between the multilingualand monolingual models, which we think is accept-able. Multilingual models perform better with alarger number of parameters, but their dependenceon large multilingual corpora is one of their mainlimitations. However, MTLS can significantly im-prove the multilingual capabilities of monolingualmodels without multilingual corpora.",
  "Effects of Symbolic Embedding": "To explore the effect of symbolic embedding onPLMs, we replace the embedding layers of BERTand RoBERTa with symbolic embedding, calledBERT-SE and RoBERTa-SE. The performances ofBERT and BERT-SE, RoBERTa and RoBERTa-SEare compared on multilingual POS tagging tasks,as shown in .Replacing vocabulary-based embedding withsymbolic embedding alone is not effective inimproving the performance of the PLM in themultilingual task, demonstrating the importanceof selective embedding and spatial embedding.We believe the reasons for the lack of significantresults are that the symbolic embedding is : Results of whether or not to use symbolicembedding in the multilingual POS tagging task in thefine-tune and zero-shot settings, respectively. -SE indi-cates the symbolic embedding. Complete experimentalresults are shown in . completely randomly initialized and has far fewerparameters than the original embedding.Theparameters of symbolic embedding are only about2.6% of the embedding in BERT, and about 1.5%of the embedding in RoBERTa. However, in thefine-tune setting, BERT-SE outperforms BERTin ZHO and COP. RoBERTa-SE also outperformsRoBERTa in COP and KOR. Combined with thesignificant improvement in multilingual capabilityfor PLMs brought by MTLS, we argue thatsymbolic embedding is one of the key factors forthe effectiveness of MTLS. Furthermore, we arguethat symbolic embedding has natural advantagesfor multilingual tasks.",
  "Conclusion": "In this paper, we explore the properties of lan-guages as symbolic systems and propose MTLS:a pre-training method to improve the multilingualcapability of models by Making Texts into Linguis-tic Symbols. We also propose SSS Embedding,which can obtain the symbolic embedding of anylanguage and map the symbolic embedding to theembedding space of the PLM. By replacing thePLM embedding layer with SSS embedding, themodel can be made to process linguistic symbols in any language. It should be emphasized that thepre-training of MTLS requires only a small amountof monolingual data and does not require multilin-gual corpora. In addition, MTLS reuses most of theparameters in the PLM, so pre-training costs fewcomputational resources. Our experimental resultsshow that MTLS can significantly improve the mul-tilingual capability of PLMs, at the cost of someperformance degradation in Latin-script languages.",
  "Limitations": "In this paper, we focus on the symbolic propertiesof languages and propose MTLS. Our results showthat MTLS has good performance, but this is only apreliminary investigation, and there are some areasfor further in-depth study. Here we highlight thelimitations of our current work and the direction ofour future work. It is feasible to use multilingual corpora inpre-training for MTLS. However, PLMs that usemonolingual corpora for pre-training (e.g. BERTand RoBERTa) do not contain multilingual sub-words in their vocabularies. In order to learn theembedding space of the model, only the corpus ofthe corresponding language can be used for pre-training. For multilingual PLMs, it is obvious thatMTLS for monolingual pre-training only does notlead to any improvement, as we have also veri-fied in our previous work. MTLS for multilingualpre-training cannot avoid the dependence on multi-lingual corpora, which goes against our original in-tention. Of course, we expect that the multilingualcapability of the model can be improved by MTLSfor multilingual pre-training, but this remains to beverified. The non-embedding parameters of the PLMare reused in MTLS to reduce the computationalconsumption.However, by simply modifyingMTLS for multilingual pre-training, it is possibleto construct a multilingual model based entirelyon linguistic symbols. We will investigate this infuture work.There are also limitations and possible futureresearch directions for making texts into linguisticsymbols. Rendering text into linguistic symbols leadsto a hundreds-fold increase in the storage capacityof the data. This puts a strain on computationalmemory during training and inference. Using symbolic embeddings instead of vocab- ularies results in models that are unable to generatediscrete words for the generation task. This paper provides a preliminary explorationof symbolic embedding as an alternative to vocab-ulary, and does not go into great depth on somedetails. For example, the choice of fonts and theclarity of linguistic symbols. More advanced meth-ods for encoding linguistic symbolic embeddingsmay yield better results. Alan Ansell, Edoardo Maria Ponti, Jonas Pfeiffer, Se-bastian Ruder, Goran Glava, Ivan Vulic, and AnnaKorhonen. 2021. Mad-g: Multilingual adapter gen-eration for efficient cross-lingual transfer. In Find-ings of the Association for Computational Linguistics:EMNLP 2021, pages 47624781. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Zewen Chi, Li Dong, Furu Wei, Nan Yang, SakshamSinghal, Wenhui Wang, Xia Song, Xian-Ling Mao,He-Yan Huang, and Ming Zhou. 2021. Infoxlm: Aninformation-theoretic framework for cross-linguallanguage model pre-training. In Proceedings of the2021 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 35763588. Zewen Chi, Shaohan Huang, Li Dong, Shuming Ma,Bo Zheng, Saksham Singhal, Payal Bajaj, Xia Song,Xian-Ling Mao, He-Yan Huang, et al. 2022. Xlm-e:Cross-lingual language model pre-training via electra.In Proceedings of the 60th Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 61706182. Alexis Conneau, Kartikay Khandelwal, Naman Goyal,Vishrav Chaudhary, Guillaume Wenzek, FranciscoGuzmn, Edouard Grave, Myle Ott, Luke Zettle-moyer, and Veselin Stoyanov. 2020. Unsupervisedcross-lingual representation learning at scale. In Pro-ceedings of the 58th Annual Meeting of the Associa-tion for Computational Linguistics. Association forComputational Linguistics.",
  ", Minneapolis, Minnesota. Association forComputational Linguistics": "AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,Dirk Weissenborn,Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, MatthiasMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.An image is worth 16x16 words: Transformersfor image recognition at scale.In InternationalConference on Learning Representations. Nan Du, Yanping Huang, Andrew M Dai, Simon Tong,Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022.Glam: Efficient scaling of language models withmixture-of-experts. In International Conference onMachine Learning, pages 55475569. PMLR.",
  "William Fedus, Barret Zoph, and Noam Shazeer. 2022.Switch transformers: Scaling to trillion parametermodels with simple and efficient sparsity. Journal ofMachine Learning Research, 23(120):139": "Daniela Gerz, Ivan Vulic, Edoardo Maria Ponti, RoiReichart, and Anna Korhonen. 2018. On the relationbetween linguistic typology and (limitations of) mul-tilingual language modeling. In Proceedings of the2018 Conference on Empirical Methods in NaturalLanguage Processing, pages 316327. Koustava Goswami, Sourav Dutta, Haytham Assem,Theodorus Fransen, and John Philip McCrae. 2021.Cross-lingual sentence embedding using multi-tasklearning. In Proceedings of the 2021 Conference onEmpirical Methods in Natural Language Processing,pages 90999113. Ping Guo, Xiangpeng Wei, Yue Hu, Baosong Yang,Dayiheng Liu, Fei Huang, et al. 2024. Emma-x: Anem-like multilingual pre-training algorithm for cross-lingual representation learning. Advances in NeuralInformation Processing Systems, 36. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, andRoss Girshick. 2020. Momentum contrast for unsu-pervised visual representation learning. In Proceed-ings of the IEEE/CVF conference on computer visionand pattern recognition, pages 97299738. Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2022.Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding shar-ing. In The Eleventh International Conference onLearning Representations.",
  "mining. In Proceedings of the 58th Annual Meet-ing of the Association for Computational Linguistics:Student Research Workshop, pages 255262": "Yunxin Li, Yu Zhao, Baotian Hu, Qingcai Chen, YangXiang, Xiaolong Wang, Yuxin Ding, and Lin Ma.2021. Glyphcrm: Bidirectional encoder represen-tation for chinese character with its glyph. arXivpreprint arXiv:2107.00395. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A robustly optimized bert pretraining ap-proach. arXiv preprint arXiv:1907.11692.",
  "Ilya Loshchilov and Frank Hutter. 2019. Decoupledweight decay regularization. In International Confer-ence on Learning Representations": "Yuxian Meng, Wei Wu, Fei Wang, Xiaoya Li, Ping Nie,Fan Yin, Muyu Li, Qinghong Han, Xiaofei Sun, andJiwei Li. 2019. Glyce: Glyph-vectors for chinesecharacter representations. Advances in Neural Infor-mation Processing Systems, 32. Joakim Nivre, Filip de Marneffe, Marie-Catherinean Ginter, Jan Hajic, Christopher D. Manning, SampoPyysalo, Sebastian Schuster, Francis Tyers, andDaniel Zeman. 2020. Universal Dependencies v2:An evergrowing multilingual treebank collection. InProceedings of the Twelfth Language Resources andEvaluation Conference, pages 40344043, Marseille,France. European Language Resources Association. Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun,Hao Tian, Hua Wu, and Haifeng Wang. 2021. Ernie-m: Enhanced multilingual representation by aligningcross-lingual semantics with monolingual corpora.In Proceedings of the 2021 Conference on EmpiricalMethods in Natural Language Processing, pages 2738. Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Noth-man, Kevin Knight, and Heng Ji. 2017. Cross-lingualname tagging and linking for 282 languages. In Pro-ceedings of the 55th Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 19461958, Vancouver, Canada. As-sociation for Computational Linguistics. Jonas Pfeiffer, Ivan Vulic, Iryna Gurevych, and Sebas-tian Ruder. 2021. Unks everywhere: Adapting multi-lingual language models to new scripts. In Proceed-ings of the 2021 Conference on Empirical Methods inNatural Language Processing, pages 1018610203. Phillip Rust, Jonas F Lotz, Emanuele Bugliarello, Eliz-abeth Salesky, Miryam de Lhoneux, and DesmondElliott. 2022. Language modelling with pixels. InThe Eleventh International Conference on LearningRepresentations.",
  "The sparsely-gated mixture-of-experts layer. In Inter-national Conference on Learning Representations": "Tom Sherborne and Mirella Lapata. 2022. Zero-shotcross-lingual semantic parsing. In Proceedings of the60th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages41344153. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processingsystems, 30. Xiaohua Wang, Wenlong Fei, Min Hu, Qingyu Zhang,and Aoqiang Zhu. 2024.Mevtr: A multilingualmodel enhanced with visual text representations. InProceedings of the 2024 Joint International Con-ference on Computational Linguistics, LanguageResources and Evaluation (LREC-COLING 2024),pages 1124711261.",
  "E, N = arg(E,N)min (L [f (x; E; N) , g (x)]) ,(12)": "where x D, L denotes the pre-training loss func-tion, x denotes the training data, g (x) denotes theground truth of x under the pre-training task, andD denotes the corpus.In the modeling setup above, we represent themultilingual model as fm (; Em; Nm), where Emand Nm are the parameters in the multilingualmodel. Methods (e.g., mBERT and XLM-R) forconstructing multilingual models by multilingualcorpora can be represented as:",
  "Em, Nm = arg(Em,Nm)min (L [f (x; Em; Nm) , g (x)]) ,(13)": "where x Dm, Dm is the multilingual corpus.Most multilingual modeling approaches using par-allel corpora can also be represented by Equa-tion 13, but in the parallel corpus approachesx = xp + xq, xp Dp, xq Dq. Dp and Dqare corpora of different languages. With Equation12 and 13, it is easy to see that the essence ofthe above approach to learning multilingual rep-resentations is to rely on a multilingual corpus tofine-tune the parameters of the language modelf (; E; N) fm (; Em; Nm). Obviously, thesemethods require not only a large multilingual cor-pus, but also the fine-tuning of almost all the pa-rameters of the model.In the adapter-based approaches, the monolin-gual model fs (; Es; Ns) is used as a backbone,which does not change the parameters of the lan-guage model and trains only the adapter layersthat are inserted into the model, thus greatly re-ducing the training consumption.The adapter-based language model can be represented asfs (; Es; Ns; A), where A denotes the parame-ters in the adapter layers. The methods for con-structing multilingual models based on the adapterlayers can be represented as fs (; Es; Ns) fm (; Es; Ns; A), and the training process can be",
  "A = arg(A)min (L [f (x; Es; Ns; A) , g (x)]) ,(14)": "where x Dm.All of the above methods require the support ofmultilingual corpura in the training process. In ad-dition, these methods are limited by the vocabulary.The multilingual vocabulary must be reconstructedbefore training. In contrast, our proposed MTLSavoids vocabulary construction by the symbolicembedding.Pre-training in MTLS can be expressed as:fs (; Es; Ns) fm (; Ex; Ns), where Ex de-notes SSS Embedding.In most PLMs, non-embeddings occupy a larger proportion of the pa-rameters, so the parameters Ns of non-embeddingsare not involved in the pre-training of MTLS. An-other difference between MTLS and previous workis that all previous work required large multilingualdatasets for support, whereas we use only a smallamount of data in a single language for training.The pre-training of MTLS can be expressed as:"
}