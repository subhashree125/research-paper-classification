{
  "Abstract": "Multilingual large language models (LLMs)seem to generalize somewhat across languages.We hypothesize this is a result of implicit vectorspace alignment. Evaluating such alignment,we see that larger models exhibit very high-quality linear alignments between correspond-ing concepts in different languages. Our experi-ments show that multilingual LLMs suffer fromtwo familiar weaknesses: generalization worksbest for languages with similar typology, andfor abstract concepts. For some models, e.g.,the Llama-2 family of models, prompt-basedembeddings align better than word embeddings,but the projections are less linear an observa-tion that holds across almost all model families,indicating that some of the implicitly learnedalignments are broken somewhat by prompt-based methods.",
  "Introduction": "Cross-lingual word embeddings are typically in-duced by supervised or unsupervised alignment ofthe word vector spaces of monolingual languagemodels. Compression in multilingual models, i.e.,parameter efficiency, can also drive implicit align-ment (Devlin et al., 2019; Pires et al., 2019; Con-neau et al., 2020), but until recently, the mappingscould still be much improved by supervised or un-supervised alignment (Hu et al., 2021; Pan et al.,2021). Multilingual large language models (LLMs)are increasingly used for different tasks and demon-strate impressive ability in understanding differentlanguages, but it is unclear whether this is a resultof improved, implicit alignment, or of somethingelse, e.g., linguistic overlap or semi-parallel subsetsof training data.LLMs have shown promising capability to com-prehend English concepts (Liao et al., 2023; Xuet al., 2024). Our paper sets out to evaluate con-cept alignment in multilingual LLMs. We aim toinvestigate two things: First, is there a linear map-",
  ": Examples of four parallel WordNet concepts,aligned across 7 languages": "ping between corresponding concepts in differentlanguages? Second, how does a learned linear map-ping generalize to new concepts? We explore bothquestions by revisiting a set of techniques used inearly work on bilingual dictionary induction (Ke-mentchedjhieva et al., 2018; Ruder et al., 2018; S-gaard et al., 2018; Kementchedjhieva et al., 2019).We evaluate multilingual LLMs as if they werebilingual dictionary induction algorithms by doingnearest neighbor search with cross-domain localscaling (Lample et al., 2018) and evaluating theirretrieval precision (precision@k). We first deriveconcept embedding in their standard way (last to-ken or average). Since many of these models wereinstruction fine-tuned, we also compare prompt-based embeddings to standard techniques based on(low-level) word embeddings. We then comparetheir precision to retrieval rates after explicit con-cept space alignment. We perform analyses withand without leakage, across multiple languages,and across both abstract and physical concepts. ContributionsOur findings across experimentswith 10 LLMs and six languages suggest that linearalignment can be induced in multilingual LLMs(if sufficiently big) to map concepts across differ-ent languages. Compared to vanilla embeddings,prompt-based concept embeddings exhibit signifi-cantly lower linearity, and the gaps between beforeand after alignment are larger for prompt-based embeddings. This suggests that some of the im-plicitly learned concept alignments are broken byprompt-based methods. Prompt-based embeddings,which are now commonly used in different retrievalscenarios, seem to be less effective in extractingcross-lingually alignable embeddings, comparedto vanilla embeddings. Results are generally good,but the old problem of generalization across ty-pological distance (Singh et al., 2019) rears itsugly face again, with Basque, Finnish, Japaneseand Thai exhibiting generally lower overall per-formance for both experimental set-ups. Further-more, abstract concepts exhibit better alignmentthan physical concepts. We suspect that it is be-cause abstract concepts are more frequent and oc-cur in more diverse contexts.",
  "Experiments": "ConceptsWe collect English noun synsets fromWordNet (Miller, 1995). For each synset, its first(most frequent) lemma name is used as the sur-face form of the corresponding concept. We useWordNets hierarchical structure to filter out top-level concepts (top-5 levels) to avoid too generalconcepts. WordNets in other languages, such asFrench WordNet (Sagot and Fier, 2008), BasqueWordNet (Gonzalez-Agirre et al., 2012), or Ro-manian WordNet (Dumitrescu et al., 2018), havesimilar structure and were all aligned in the OpenMultilingual WordNet project (OMW) (Bond et al.,2016). To produce a repository of parallel semanticconcepts, we collect synsets with shared ID acrossdifferent WordNets, after removing duplicate con-cepts. In total, we obtain 4,397 parallel conceptsacross 7 different languages (English, French, Ro-manian, Basque, Finnish (Lindn and Niemi, 2014),Japanese (Bond et al., 2009) and Thai (Thoongsupet al., 2009)); had we included more languages,the number of parallel concepts would have beenprohibitively small. The 4,397 concepts were di-vided into abstract (e.g., happiness) and physical(e.g., vehicle) concepts. See for data char-acteristics, and for examples of parallelconcepts.",
  ": The statistics of the parallel concept dataset.We use 1000, 2000, or 3000 concepts for training": "To create a seed dictionary (training data) for su-pervised alignment, we randomly sample 3,000 par-allel concepts,1 including 1,500 abstract conceptsand 1,500 physical concepts. The 3,000 conceptsare used to induce the linear mapping. LLMsWe experiment with four different LLMfamilies with varying sizes: Llama2 (7B, 13B, 70B)(Touvron et al., 2023), mT0 (1.2B, 3.7B, 13B),BLOOMZ (1B7, 3B, 7B1) (Muennighoff et al.,2022), and Aya101 (13B) (stn et al., 2024). Weuse two different concept space extraction methods(vanilla and prompt-based). The vanilla methodsimply uses the last token representation as the con-cept embedding for decoder-only models (Llama2and BLOOMZ); and the average embedding ofthe last hidden layer of the encoder as the conceptembedding for encoder-decoder models (mT0 andAya101)2. The prompt-based extraction method ex-ploits the fact that all these models were instruction-tuned.The template we use for prompt-basedextraction is adapted from Li and Li (2023) andshown as follows:",
  "Summarize concept [text] in one [lang] word:": "where [text] and [lang] will be replaced bythe corresponding concept (in the source lan-guage) and the language name (in adjectival form),e.g.,\"summarize concept \"\" in one Japaneseword\" for the concept animal3. The prompt-basedconcept embedding is that of the last hidden state. Alignment and RetrievalWe rely on ProcrustesAnalysis (Schnemann, 1966), a form of statisticalshape analysis, to discover good linear transforma-tions (e.g., translation, rotation, and scaling) be-tween concept spaces in different languages. Sup-pose X and Y are two matrices of size n d (n isthe seed dictionary size, d is the embedding size)such that the ith row in X is an embedding of con-cept ci in one language, and the ith row in Y iscis embedding in the other language. The lineartransformation is derived through singular valuedecomposition (SVD) of Y XT :",
  "(d) BLOOMZ-7B1": ": Performance (P@1) of different LLMs on the concept alignment evaluation when using a seed dictionaryof 3,000 concepts. X-axis: Languages, we further divide these languages into three groups, where Group 1 isIndo-European, Group 2 includes languages that are not Indo-European but still in Latin script, while Group 3refers to languages that are not Indo-European and not in Latin script. Y-axis: We report Precision@1.",
  "the English (target) vector space. We then per-form cross-domain local scaling (CSLS) to retrievethe most similar concepts.4 We use precision@k(P@k) as our performance metric": "Main ResultsWe present the main results5 in. For each model, we report three results:1) the upper bound (leaky) on performance forsupervised linear alignment, using the train seedand the test seed for inducing the dictionary (or-ange/blue bar), which we refer to as the ceiling andreveals to what extent there exists a linear mapping;2) before-align performance (red dashed line), re-trieval bilingual concept pairs directly from theraw LLM (vanilla word or prompt) embeddings; 3)after-align performance (black dashed line), which 4We also ran experiments with vanilla nearest neighborsearch as our retrieval method, but CSLS outperforms nearestneighbor search by some margin. So, we report results withCSLS.5Full results with all model sizes, training sizes, and differ-ent k-values for P@k are presented in the Appendix. is the performance of non-leaky, supervised map-ping (using 3,000 concepts as the seed dictionary)into the English vector space, with CSLS as ourretrieval method. Orange bars indicate vanilla wordembedding strategy (last-token, or average embed-ding), while blue bars refer to results for prompt-based embedding. All multilingual LLMs (except BLOOMZ) caninduce good concept alignments, as indicated bythe upper bound performance. In general, withinthe same model family, a larger model size leadsto better alignment.The ceiling is highest forvanilla word embeddings in Llama2-13B, indicat-ing near-isomorphisms between monolingual con-cept spaces at this level. The prompt-based embed-dings are less linear, indicating that partial isomor-phisms induced prior to prompting are corrupted.For after-align performance, we generally see thehighest performance for Indo-European languages(Group 1) and the lowest for non-Indo-Europeanlanguages with non-Latin scripts (Group 3). Sim- ilarly, a larger model size and larger seed dic-tionary generally improve the concept alignment.On Group 2 and 3, mT0 and Aya101 show bet-ter before-align performance compared to othermodels. In some cases, results are extremely good.Llama2-13B with prompt-based embeddings ex-hibits a P@1 score of 59.27% before alignment forFrench, for example. This means that the model hasinduced perfect alignment of 3/5 concepts in theabsence of any explicit supervision. It is interestingto see the gap between the red and black dashedlines. The size of this gap indicates how much ofthe (alignable part of the) concept space was notaligned, with given seed dictionary. For vanillaword embeddings, the gaps are relatively small,but for prompt-based embeddings the gaps tend tobe much larger, again indicating that promptingsomewhat breaks the implicitly learned conceptalignment. Abstract vs. PhysicalWe analyze performancedifferences across abstract and physical concepts.To make a fair comparison, we randomly down-sample6 physical concepts and compare retrievalperformance across the two classes. In this section,we report P@1 with models that have comparablemodel sizes (7B/13B) in each family; results forthe other models can be found in the Appendix.As shown in , all models generally havebetter alignment performance on abstract conceptscompared to physical concepts.",
  "Discussion and Related Work": "Related WorkThe idea that distributional repre-sentations facilitate cross-lingual alignment goesback to explicit semantic analysis (Gabrilovich andMarkovitch, 2007), but the idea of training multi-lingual, neural language models also has a long his-tory. Such models have traditionally used explicitalignment objectives, e.g., either from word align-ments, bilingual dictionary seeds (Lample et al.,2018; Li et al., 2024), or by training on mixedcorpora constructed using such resources (Gouwsand Sgaard, 2015; Workshop et al., 2022; Chaiet al., 2024). Cross-lingual generalization has beenstudied in different NLP tasks, including questionanswering (Artetxe et al., 2020), commonsensereasoning (Ponti et al., 2020; Lin et al., 2022),code generation (Peng et al., 2024), and knowl-edge transfer and consistency (Xu et al., 2023; Qiet al., 2023). Cross-lingual word alignment alsohas a long history by examining bilingual lexiconinduction (Xing et al., 2015; Sgaard et al., 2018;Li et al., 2023). For concept understanding specifi-cally, previous works have examined concept un-derstanding in LLMs by definition matching (Xuet al., 2024), hypernym/hyponym detection (Liaoet al., 2023; Shani et al., 2023), and relation discov-ery (Gu et al., 2023). However, they are limited tothe English language only. Linear AlignmentWe saw that concepts are rep-resented in similar ways across languages in multi-lingual LLMs, as shown in the upper bound. Thisindicates structural similarities and facilitates cross-lingual transfer. Prompt-based embeddings exhibitsignificantly lower linearity compared to word em-beddings, and the gaps between before and after alignment are larger for prompt-based embeddings.Both things suggest that some of the implicitlylearned concept alignment is broken by the prompt-based method. On the other hand, prompt-basedembeddings demonstrate larger improvements withexplicit post-hoc alignment while supervised align-ment struggles to improve on vanilla word embed-dings. Difference in LanguagesThe degree of isomor-phism to English is similar across languages, asindicated by the upper bounds on performance.All concept spaces are (almost) equally alignable.However, the induced maps generalize much betteracross typologically related (Indo-European) lan-guages: French and Romanian. Generalization isconsiderably poorer for the other two groups. Types of ConceptsThough previous works showthat physical concepts do better than abstract onesin bilingual dictionary induction (Kementched-jhieva et al., 2019), as well as in related tasks suchas hypernym detection (Liao et al., 2023), we showthat abstract concepts tend to align better acrossdifferent languages, as shown in . This,however, was explained by a spurious correlationwith frequency. It would be interesting to controlfor frequency in future error analysis.",
  "Limitations": "Because of the small overlap between multilingualWordNets, we only include six (6) test languages.While this is too small a set of languages to drawuniversally applicable conclusions. Fortunately,the set includes both Indo-European and non-Indo-European languages, as well as both Latin scriptand non-Latin script languages. We also limitedourselves to studying nouns; for how linear align-ment generalizes to other parts of speech, see Ke-mentchedjhieva et al. (2018) and Hartmann andSgaard (2018).",
  "Acknowledgement": "We would like to thank all anonymous reviewers fortheir insightful comments and feedback. This workwas supported by DisAI - Improving scientific ex-cellence and creativity in combating disinformationwith artificial intelligence and language technolo-gies, a project funded by European Union underthe Horizon Europe, GA No. 101079164. Mikel Artetxe, Sebastian Ruder, and Dani Yogatama.2020. On the cross-lingual transferability of mono-lingual representations. In Proceedings of the 58thAnnual Meeting of the Association for ComputationalLinguistics, pages 46234637. Francis Bond, Hitoshi Isahara, Sanae Fujita, KiyotakaUchimoto, Takayuki Kuribayashi, and Kyoko Kan-zaki. 2009. Enhancing the japanese wordnet. InProceedings of the 7th Workshop on Asian LanguageResources (ALR7), pages 18.",
  "Francis Bond, Piek Vossen, John Philip McCrae, andChristiane Fellbaum. 2016. Cili: the collaborativeinterlingual index. In Proceedings of the 8th GlobalWordNet Conference (GWC), pages 5057": "Linzheng Chai, Jian Yang, Tao Sun, Hongcheng Guo,Jiaheng Liu, Bing Wang, Xiannian Liang, Jiaqi Bai,Tongliang Li, Qiyao Peng, et al. 2024. xcot: Cross-lingual instruction tuning for cross-lingual chain-of-thought reasoning. arXiv preprint arXiv:2401.07037. Alexis Conneau, Kartikay Khandelwal, Naman Goyal,Vishrav Chaudhary, Guillaume Wenzek, FranciscoGuzmn, Edouard Grave, Myle Ott, Luke Zettle-moyer, and Veselin Stoyanov. 2020. Unsupervisedcross-lingual representation learning at scale. In Pro-ceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 84408451, Online. Association for Computational Lin-guistics.",
  "#model-summary8": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics. Stefan Daniel Dumitrescu, Andrei Marius Avram, Lu-ciana Morogan, and Stefan-Adrian Toma. 2018.Rowordneta python api for the romanian wordnet.In 2018 10th International Conference on Electronics,Computers and Artificial Intelligence (ECAI), pages16. IEEE. Evgeniy Gabrilovich and Shaul Markovitch. 2007.Computing semantic relatedness using wikipedia-based explicit semantic analysis. In Proceedings ofthe 20th International Joint Conference on ArtificialIntelligence, volume 7, pages 16061611.",
  "Aitor Gonzalez-Agirre, Egoitz Laparra, German Rigau,et al. 2012. Multilingual central repository version3.0. In LREC, pages 25252529": "Stephan Gouws and Anders Sgaard. 2015. Simple task-specific bilingual word embeddings. In Proceedingsof the 2015 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 13861390,Denver, Colorado. Association for ComputationalLinguistics. Yuling Gu, Bhavana Dalvi, and Peter Clark. 2023. Dolanguage models have coherent mental models of ev-eryday things? In Proceedings of the 61st AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 18921913. Mareike Hartmann and Anders Sgaard. 2018. Limita-tions of cross-lingual learning from image search. InProceedings of the Third Workshop on Representa-tion Learning for NLP, pages 159163, Melbourne,Australia. Association for Computational Linguistics. Junjie Hu, Melvin Johnson, Orhan Firat, Aditya Sid-dhant, and Graham Neubig. 2021. Explicit alignmentobjectives for multilingual bidirectional encoders. InProceedings of the 2021 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 36333643, Online. Association for Computa-tional Linguistics. Yova Kementchedjhieva, Mareike Hartmann, and An-ders Sgaard. 2019. Lost in evaluation: Misleadingbenchmarks for bilingual dictionary induction. InProceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the9th International Joint Conference on Natural Lan-guage Processing (EMNLP-IJCNLP), pages 33363341, Hong Kong, China. Association for Computa-tional Linguistics. Yova Kementchedjhieva, Sebastian Ruder, Ryan Cot-terell, and Anders Sgaard. 2018. Generalizing Pro-crustes analysis for better bilingual dictionary induc-tion.In Proceedings of the 22nd Conference onComputational Natural Language Learning, pages211220, Brussels, Belgium. Association for Com-putational Linguistics.",
  "George A Miller. 1995. Wordnet: a lexical database forenglish. Communications of the ACM, 38(11):3941": "Niklas Muennighoff, Thomas Wang, Lintang Sutawika,Adam Roberts, Stella Biderman, Teven Le Scao,M Saiful Bari, Sheng Shen, Zheng-Xin Yong, HaileySchoelkopf, et al. 2022. Crosslingual generaliza-tion through multitask finetuning. arXiv preprintarXiv:2211.01786. Lin Pan, Chung-Wei Hang, Haode Qi, Abhishek Shah,Saloni Potdar, and Mo Yu. 2021. Multilingual BERTpost-pretraining alignment. In Proceedings of the2021 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 210219, Online.Association for Computational Linguistics.",
  "Telmo Pires, Eva Schlinger, and Dan Garrette. 2019": "How multilingual is multilingual BERT? In Proceed-ings of the 57th Annual Meeting of the Association forComputational Linguistics, pages 49965001, Flo-rence, Italy. Association for Computational Linguis-tics. Edoardo Maria Ponti, Goran Glava, Olga Majewska,Qianchu Liu, Ivan Vulic, and Anna Korhonen. 2020.Xcopa: A multilingual dataset for causal common-sense reasoning. In Proceedings of the 2020 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 23622376.",
  "Jirui Qi, Raquel Fernndez, and Arianna Bisazza. 2023": "Cross-lingual consistency of factual knowledge inmultilingual language models. In Proceedings of the2023 Conference on Empirical Methods in NaturalLanguage Processing, pages 1065010666, Singa-pore. Association for Computational Linguistics. Sebastian Ruder, Ryan Cotterell, Yova Kementched-jhieva, and Anders Sgaard. 2018. A discriminativelatent-variable model for bilingual lexicon induction.In Proceedings of the 2018 Conference on Empiri-cal Methods in Natural Language Processing, pages458468, Brussels, Belgium. Association for Com-putational Linguistics.",
  "Sornlertlamvanich, and Hitoshi Isahara. 2009. Thaiwordnet construction.In Proceedings of the 7thWorkshop on Asian Language Resources (ALR7),pages 139144": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Ahmet stn, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel Dsouza, Gbemileke Onilude, NeelBhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid,et al. 2024. Aya model: An instruction finetunedopen-access multilingual language model.arXivpreprint arXiv:2402.07827. BigScience Workshop, Teven Le Scao, Angela Fan,Christopher Akiki, Ellie Pavlick, Suzana Ilic, DanielHesslow, Roman Castagn, Alexandra Sasha Luc-cioni, Franois Yvon, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model.arXiv preprint arXiv:2211.05100.",
  "Chao Xing, Dong Wang, Chao Liu, and Yiye Lin. 2015": "Normalized word embedding and orthogonal trans-form for bilingual word translation. In Proceedingsof the 2015 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 10061011,Denver, Colorado. Association for ComputationalLinguistics. Ningyu Xu, Qi Zhang, Menghan Zhang, Peng Qian, andXuanjing Huang. 2024. On the tip of the tongue: An-alyzing conceptual representation in large languagemodels with reverse-dictionary probe. arXiv preprintarXiv:2402.14404. Shaoyang Xu, Junzhuo Li, and Deyi Xiong. 2023. Lan-guage representation projection: Can we transferfactual knowledge across languages in multilinguallanguage models? In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 36923702, Singapore. Associa-tion for Computational Linguistics.",
  "ALanguage Resource and SharedVocabulary": "We report the estimated resource level for the sevenlanguages we experimented in this work. The num-ber, which has been widely used to indicate re-source availability, is taken from the CC100 XLcorpusLin et al. (2022).One reason of the cross-lingual alignment mightbe that among the chosen concepts some hadthe same word form across languages with Latinscripts. To investigate this, we additionally calcu-late the ratio of identical word forms (compared toEnglish concepts) for languages with Latin scripts:",
  "BFull Experimental Results": "In the Appendix, we report our full experimentalresults across different models with varying modelsizes, seed dictionary sizes, different k-values forP@K, in following figure and tables ( and-23). These results provide full scope of ouranalysis, allowing for an in-depth comparison ofmodel performances.For different models, we use their HuggingFacePyTorch implementation10. For Procrustes Anal-ysis, we utilize the MUSE11 package. All experi-ments are run on a single NVIDIA A100 GPU.",
  "(j) Aya101 (13B)": ": Performance (P@1) of different LLMs on the concept alignment evaluation when using a seed dictionaryof 3000 pairs. X-axis: Languages, we further divide these languages into three groups, where Group 1 is Indo-European, Group 2 includes languages that are not Indo-European but still in Latin script, while Group 3 refers tolanguages that are not Indo-European and not in Latin script. Y-axis: We report Precision@1."
}