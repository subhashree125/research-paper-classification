{
  "Abstract": "Research on dialogue constructiveness assess-ment focuses on (i) analysing conversationalfactors that influence individuals to take spe-cific actions, win debates, change their perspec-tives or broaden their open-mindedness and(ii) predicting constructiveness outcomes fol-lowing dialogues for such use cases. Theseobjectives can be achieved by training eitherinterpretable feature-based models (which of-ten involve costly human annotations) or neu-ral models such as pre-trained language mod-els (which have empirically shown higher taskaccuracy but lack interpretability). In this pa-per we propose an LLM feature-based frame-work for dialogue constructiveness assessmentthat combines the strengths of feature-basedand neural approaches, while mitigating theirdownsides. The framework first defines a set ofdataset-independent and interpretable linguis-tic features, which can be extracted by bothprompting an LLM and simple heuristics. Suchfeatures are then used to train LLM feature-based models. We apply this framework tothree datasets of dialogue constructiveness andfind that our LLM feature-based models outper-form or performs at least as well as standardfeature-based models and neural models. Wealso find that the LLM feature-based modellearns more robust prediction rules instead ofrelying on superficial shortcuts, which oftentrouble neural models.1",
  "Algorithmic Heuristics": "Prompting a LargeLanguage Model Dispute tactics, quality ofarguments, information content, style and tones, ... Politeness markers, collaboration makers, ... Extract interpretable linguistic features per utterance Compute statistics (e.g., mean, gradient) of the extracted features throughout the entire dialogue. Prediction for the Target Variable - Dialogue Constructiveness LLM Feature-based Models (e.g., ridge regression, logistic regression) Feed the features as input for model training",
  ": Flowchart delineating a high-level overview ofour proposed framework for dialogue constructivenessassessment": "2020), garnering more votes (Zhang et al., 2016;Slonim et al., 2021), argumentation to achieve con-sensus (Mayfield and Black, 2019a; Vecchi et al.,2021; De Kock and Vlachos, 2021; De Kock et al.,2022), and investigating how dialogues foster open-minded thinking (Farag et al., 2022). Two maingoals of these studies are to (i) better understandthe interplay between linguistic patterns humansexhibit and the targeted outcomes associated withdialogue constructiveness and (ii) build modelsthat can anticipate the outcome of constructive dia-logues for certain use cases. These goals are usually achieved by buildingtwo types of models: Feature-based models, whichtake linguistic features as input to predict the tar-geted outcome, and neural models like Pre-trainedLanguage Models (PLM), which take adialogue(or a part of it) as input. The former is usuallyan interpretable model (e.g., logistic regression),while the latter can be artificial neural networkslike BERT (Devlin et al., 2018), Longformer (Belt-agy et al., 2020) and few-shot decoder-only trans-formers (Achiam et al., 2023) that tend to producehigher task accuracy.However, these two types of models have down-sides. For example, the input data of feature-basedmodels often require expensive human annotations(e.g., levels of disagreements and coordinationtactics) (Meyers et al., 2018; Wang et al., 2019;De Kock et al., 2022), whilst neural models areblack-boxes. Further, these models are susceptibleto learning shortcutsdecision rules that performwell for the majority of benchmark examples butdo not hold in general (Gururangan et al., 2018;Geirhos et al., 2020). For instance, neural modelsare prone to learning shortcuts2 due to the expo-sure to a rich input space. Likewise, feature-basedmodels can learn shortcuts, though this issue canbe mitigated by careful feature engineering.To our knowledge, prior works use distinct setsof features without a unifying framework, manyof which were tailored to a specific context (e.g.,specific persuasion tactics for soliciting donations).This work contributes by introducing a unifyingframework for dialogue constructiveness assess-ment () that capitalises on the strengths ofboth model types while mitigaiting their shortcom-ings. It integrates both simple algorithmic rulesand prompting a Large Language Model (LLM),namely GPT-4 (Achiam et al., 2023), to automati-cally extract a rich set of dataset-independent3 lin-guistic features, such as the average occurrence andusage change of collaboration markers (Niculaeand Danescu-Niculescu-Mizil, 2016) and disputetactics (De Kock et al., 2022) used throughout agiven dialogue. We will refer to the features gener-ated by heuristics and LLMs as discrete and LLM-generated features, respectively. These features are",
  "We use the terminology introduced by Geirhos et al": "(2020). Other works also refer to shortcuts as biases orspurious correlations.3Namely, these features that our framework provides areapplicable to any dataset related to dialogue constructivenessand potentially adaptable to other dialogue-style datasets. then fed into ridge/logistic regression algorithms,which are referred to as LLM feature-based mod-els. We find that the patterns learned by our LLMfeature-based models have better predictive powerand robustness than standard feature-based and neu-ral models, and showcase our frameworks abilityto spot predictive linguistic factors for the targetedoutcomes across three datasets of dialogue con-structiveness: Opening-up Minds (OUM) (Faraget al., 2022), Wikitactics (De Kock et al., 2022) andArticles for Deletion (AFD) (Mayfield and Black,2019a). shows three examples (one perdataset) with dialogue segments, target variables,and subsets of extracted features.",
  "Dialogue Constructiveness": "Analysing dialogue constructiveness has long beena research field of important interest. In recentyears, it has become increasingly more relevant asmore and more conversations online are not task-focused/customer services-related but more open-ended. Prior work has focused on 1) understandingthe factors that can impact and 2) building modelsthat can predict dialogue constructiveness. For in-stance, Zhang et al. (2016) track how ideas flowbetween participants throughout a debate. Statisti-cal analysis reveals that victorious debaters moreeffectively leverage the interactive aspect of the de-bate by engaging with their opponents arguments. Mayfield and Black (2019b) perform an analy-sis on group decision-making on the Articles forDeletion Wikipedia forum, where editors deliber-ate on whether an article should be removed. Theauthors train a BERT model to forecast the out-comes of these debates based on the discussiontext and measure the impact of discursive strate-gies and different user behaviours on the predictedoutcomes by monitoring the shift in the modelspredicted probabilities after each contribution tothe discussion. Wang et al. (2019) collect a dataset of dialogueswhere one person persuades the other to donate tocharity. They train a feature-based model (usinglogistic regression) to examine the associations be-tween the human-annotated persuasion strategiesand the binary donation outcome (1 = donation, 0= no donation). They find that only the strategyof providing information about the donation proce-dure has a significant positive effect on donation.",
  "DialogueTarget VariableLinguistic Features": "<WoZ> Hello! Would you like to tell me if you thinkthe COVID-19 vaccination is a good idea?<participant> Its a good idea as Covid has takenso many lives and disrupted so many businesses andlivelihoods.So having a vaccine that can preventpeople from dying is good and we can be safe in publicagain knowing the chances of dying are low.<WoZ> Thats a fair point! However, some people areconcerned about the vaccines because there have beenrecorded cases of people dying from blood clots aftertaking them, and others have been seriously ill withside-effects.......<participant> We were mandated to go under strictlockdown, with no choices. I think laws from companiesand establishments making vaccinations compulsory isgreat in achieving herd immunity and those who dontcomply must stay away from society.<WoZ> Some are concerned that mandatory vaccinationtakesawayanindividualsrighttochoosewithinformedconsent,andwithoutinformedconsentitis medically unethical to force a medicine, medicalprocedure, or surgery upon the patient. Post-conversationopen-mindednessscoremeasuringtheparticipantsbelief that those who hold opposingviewpoints have good reasons for theirbeliefs (on a 7-point Likert scale)after having a conversation on a givencontroversialtopic(e.g.,COVID-19vaccination)withanotherhumanorachatbot.Thepost-conversationopen-mindednessscorefromtheparticipant for this instance is 3.",
  "Epistemic uncertainty, =0": "<user_id=Ilovetopaint> You know very well that thetrack was named in honor of Seans band, and thatmentioning it genuinely informs the context of thealbum. What is with these pedantic reverts?<user_id=153.205.69.164>Couldyouprovidethesource that states \"Micro Disneycal World Tour wasnamed in honor of Microdisney\"?<user_id=Ilovetopaint>Thishasalreadybeensatisfied by two references which observe the factthat a guy who was in a band called \"Microdisney\"remixed the album track called \"Micro Disneycal WorldTour\". Even if the song wasnt actually named afterthe band (yeah right), the fact that a member of thatband happened to remix that track would still be ofencyclopedic interest.......<user_id=153.205.69.164> Besides, I have no idea whatmakes you want to mention only Sean OHagan in thearticle.As credited as [...]\"The Micro DisneycalWorld Tour\" was remixed by the High Llamas, not onlySean OHagan.Even the MTV source you added statesthatitis\"theHighLlamastakeonTheMicroDisneycal World Tour\".He is not the remixer for\"Micro Disneycal World Tour\".Although Sean OHaganis a member of the High Llamas, the High Llamas is notSean OHagans one-man band. BinaryindicatornotingwhetherasuggestedWikipediapageedit,afterholdingadisputebetweenseveral users/editors, would requireescalation (i.e.,Disputes that theeditors are unable to settle on theirown are escalated to mediation. Thisoutcome can be regarded as a proxyto dialogue constructiveness.).Theoutcome for this instance is 1 (i.e.,escalated).",
  "High politeness, =-0.06": "<user_name=Eastmain> [[User:Ulflarsen]] prodded withthe comment: \"A near identic article on the Norwegianbokml/riksmlswikipedia were recently removed as themajor Norwegian newspaper VG in a mail claimed that thearticle were a violation of its right to the material,it has been compiled from that, so the article shouldbe deleted here as well\" and then [[User:Nsaa]] addedan AfD notice.Technical nomination only.I cannottell if this is a copyvio or not.<user_name=Nsaa>*VOTEIthasbeendeletedonNorwegianWikipedia[...],maybebecausethelistcontainsthepublishersnameinthearticlename[[VG]] (this is not the case here). This list can becompared to [[Pop 100 number-one hits of 2007 (USA)]]<small>Preceding [[Wikipedia:Signatures|unsigned]]comment added by.......<user_name=Chubbles> *VOTE clearly encyclopedic, anda list of hits is not a copyright violation.Theunlinked artists should be linked, as they are allobviously notable.<user_name=Mm40> *VOTE As long as everything is legaland all, it should be kept. Notable and not bad enoughto warrant a deletion. Binary indicator representing whethera nominated article would be kept ordeletedbyWikipediaadministratorsafterthedisputeisheldbetweenWikipediausersinwhichanyuser,including those who are unregisteredbutsigntheirpostwithanIPaddress, can cast a vote and provide arationale for their stance on whetherthearticleshouldberetainedorremoved.Users may also contributenon-voting comments.The outcome forthis instance is 1 (i.e., kept).",
  "High politeness, =0.02": ": Three example dialogue segments from the three datasets included for our analysis in tandem with theirtarget variables and nine examples of extracted linguistic features (averaged across utterances, x, or the slope of alinear fit, ) generated with simple heuristics and GPT-4. From top to bottom, these examples belong to OUM,Wikitactics and AFD datasets. Details of the features and the datasets are described in section 3 and 4, respectively.The utterances of distinct entities are coloured differently. The example from OUM, in the first row, WoZ refers toa human pretending to be a dialogue agent via an appropriate interface in the context of a Wizard-of-Oz experiment. De Kock and Vlachos (2021) analyse factors thatmake dispute resolution work on Wikipedia TalkPage by training feature-based models to predictescalation4. They find both average occurrence andchange in the use of collaboration and politenessmarkers are predictive of escalation. In addition,they develop neural models and show that account-ing for the conversations structure improves predic-tive accuracy, outperforming feature-based models. Farag et al. (2022) focus on argumentative dia-logues that aim to foster open-mindedness towardsviews that one opposes. They introduce a dataset of183 human-human and human-chatbot dialogueson three controversial topics: Brexit, veganism,and COVID-19 vaccination. To measure open-mindedness, the authors ask participants beforeand after the dialogue whether they think peoplewith opposing views have good reasons for theirstance, intellectual capabilities, and morality. Theyfind that participants become more open to the rea-sons behind opposing views but not so regardingthe morality and intellectual capabilities of theiropponents. There is no strong correlation betweenchat experience ratings (e.g., clarity, enjoyment,persuasiveness) and changes in open-mindedness. Khan et al. (2024) investigate the use of debatebetween two LLM experts each arguing for a dif-ferent answer on a reading comprehension task, asa method to elicit truthful information, and ask a(human/LLM) non-expert to judge the debate andselect the answer. They find that debate protocolsoutperform the single-model consultancy approach.They also show that the level of persuasiveness ofdebaters can have a positive impact on the accu-racy of judgements of non-experts, implying thatoptimising features like the level of persuasivenessduring the debate scenario may enhance dialogueconstructiveness and promote the users ease in theverification of correct output or errors.",
  "Shortcut Learning": "Neural models have a tendency toward learningshortcuts (Geirhos et al., 2020; Du et al., 2023),despite their impressive performance. For instance,imperceptible perturbations to the input (Szegedyet al., 2013) or changes in the background context(Beery et al., 2018; Rosenfeld et al., 2018) can com-pletely derail their predictions. Also, neural modelscould do natural language inference by just de-tecting correlated keywords instead of engaging in",
  "Disputes which cannot be resolved by editors are esca-lated to mediation, which is a proxy for constructiveness": "genuine reasoning (Gururangan et al., 2018).Neural dialogue evaluation metrics can also ex-hibit shortcut behaviour or biases, as demonstratedby Khalid and Lee (2022) through an adversarialtest suite that revealed these metrics often fail toproperly penalize problematic conversations andmay rely on superficial patterns rather than deepsemantic understanding. Further, LLMs may learnshortcuts or brittle representations (Zhang et al.,2024; Chen et al., 2024), thus exhibiting incon-sistent performance across various adversarial per-turbations designed to degrade dialogue quality.LLMs may also exploit shortcuts in prompts dur-ing in-context learning (Tang et al., 2023), raisingconcerns about robustness and generalisability.Given these observations, we hypothesise thatthe neural models we develop to predict dialogueconstructiveness may learn shortcuts. Neverthe-less, feature-based models can also learn shortcuts,though the risk can be mitigated by carefully craft-ing features that capture patterns that generalisewell across datasets.",
  "LLM Feature-based Framework": "The LLM feature-based framework that we intro-duce works as follows: Given a labelled dataset ofdialogue constructiveness, we generate linguisticfeatures at (i) the utterance-level for all utterancesor (ii) at the dialogue-level, for every input dia-logue. More concretely, we include six linguisticfeature sets () in which five are sourcedfrom the literature and one (i.e., QoA) is newly in-troduced in this work; all feature sets are annotatedat the utterance-level except for the QoA, whichis generated at the dialogue-level. These featuresare all dataset-independent and are annotated us-ing either heuristics or prompting an LLM within-context learning.5 We note that the quality ofannotations of the used LLM should be validatedmanually, e.g., a human expert taking a sample ofannotated features and inspecting their annotationaccuracy.Once the linguistic features are generated andvalidated, we compute statistics for each aforemen-tioned feature annotated at the utterance-level (i.e.,all features sets except for QoA) to capture the dy-namics of dialogue; the resultant statistics would 5Specifically, we prompt GPT-4-1106-preview via OpenAIAPI, which is referred to as GPT-4 in this work. We alsoexplored two frontier open-source LLMs (LLaMA-3.1-70B-Instruct and LLaMA-3.1-405B-Instruct), but they exhibitedworse annotation quality (see Appendix A.3).",
  "Feature SetDescription": "Politeness markersThe politeness strategies introduced by Zhang et al. (2018) and implemented in Convokit (Chang et al., 2020), encompassinggreetings, apologies, directness levels, and the use of polite expressions. Collaboration markersConversation markers that are indicative of collaborative discussions (Niculae and Danescu-Niculescu-Mizil, 2016), includingthe introduction and adoption of ideas, expressions of uncertainty or confidence, pronoun usage patterns, and linguistic styleaccommodation. Dispute tacticsIndicators concerning the use of distinct rebuttal strategies (i.e., attempts to counter the arguments of an opponent) and coordinationtactics (i.e., attempts to promote understanding and consensus), introduced in (De Kock et al., 2022).",
  "Quality of Arguments(QoA)": "Assessing the QoA has been an important area of research in linguistics and NLP (Lawrence and Reed, 2020). Despite thesubjectivity, it can be generally agreed that arguments based on scientific evidence are more convincing than those based solely onspeculation from an individual without expertise on the subject matter. We explore a new way to model argument quality: Promptingan LLM to grade the quality of all arguments used in a discussion on a 0-10 scale. Information contentThis includes content density and propositional density. Content density is the ratio of open-class to closed-class words, whilepropositional density measures the ratio of propositions to the number of words (Roark et al., 2011). Style and toneThis set contains several features characterising the style and tone of utterances. SENTIMENT: To capture the emotional tone,utterances are classified as negative, neutral, or positive. POLITENESS: This feature evaluates the degree of politeness expressed inthe text, classified as high or low politeness. FORMALITY: To capture speech formality, distinguishing between casual remarks andprofessionally structured utterances. UNCERTAINTY: Individuals can express varying degrees of uncertainty during a discussion,which might be predictive of dialogue constructiveness. We annotate four types of uncertainty (i.e., epistemic, doxastic, investigative,conditional) as in (Meyers et al., 2018). : Six dataset-independent feature sets. The top two are discrete features generated using heuristics, while theother four are LLM-generated. Details on individual features in each set are in Appendix A.1. The prompts used forLLM-generated features are in Appendix A.2. Examples of extracted features are shown in . be considered as dialogue-level features. In par-ticular, we compute the average and the gradient(slope of a linear fit) statistics to the feature valuesof all utterances throughout the entire dialogue, asproposed by De Kock and Vlachos (2021).Lastly, we can train any interpretable regres-sion or classification algorithm (e.g., ridge/logisticregression) by feeding the computed statistics ordialogue-level features to the model, to predict thetarget variable of interest. summarisesthe framework. Noteworthy, the framework can beused flexibly, e.g., one may include dataset-specificfeatures to further improve the LLM feature-basedmodels performance. For instance, instead of cal-culating statistics for each features values through-out the entire dialogue, one can also explore theoption of disaggregating them at the participantlevel.6 As we will demonstrate in our experimen-tal results, this setup is readily applicable to onedataset that we analyse, where there is always afixed number of speakers throughout the conver-sation. This setup may also be applied to datasetswhere the number of individuals varies, but it willrequire further adaptations.",
  "provides statistics of the dialogues in these corpora": "OUM5427 argumentative dialogues between par-ticipants and chatbots/WoZ (i.e., the Wizard of Ozmethod, a moderated research method in which auser interacts with an interface that appears to beautonomous but is actually controlled by a human)on controversial topics (Farag et al., 2022). Thetopics include Brexit (n = 149), COVID-19 vac-cination (n = 189) and veganism (n = 204). Thedialogues aim to facilitate open-mindedness by ex-posing participants to opposing viewpoints, witharguments sourced from an online debate platform,Kialo8. Following each dialogue, participants ratedtheir post-conversation open-mindedness towardsthe opposing stance on a 7-point Likert scale, as-sessing whether they believe people with opposingviews to theirs have good reasons. We model thisas a proxy for dialogue constructiveness as a re-gression task. Wikitactics213 disputes sourced from theWikipedia Talk Page (De Kock et al., 2022). Whenthere is a content accuracy dispute or a violation ofWikipedias neutral point of view policy, an editorcan create a dispute for a potentially problematicarticle, in which they provide their rationale, voteand discuss them with others. If the editors cannotreach an agreement, they can request mediationfrom a community volunteer, which is consideredan escalation. We model a binary classificationtask by taking a dialogue as the input to predict",
  "two constructiveness outcomes (escalated or non-escalated)": "AFDA sample of 1000 instances randomly ex-tracted from the original 400K discussions (to re-duce the excessive use of compute and environmen-tal impact), where editors debate whether to deletecertain Wikipedia articles (Mayfield and Black,2019a). The corpus was collected by extractingdebates (that last at least seven days) initiated byeditors, who provide reasoning for nominating anarticle for deletion. Administrators then aggregatethe discussion to decide the outcome, typically fol-lowing the majority opinion unless a clear consen-sus is lacking, in which case the article is retainedby default. For our analysis, we model a binaryclassification task by including articles labelled asDelete (y=0) and Keep (y=1), as per Mayfieldand Black (2019a).",
  "LLM Feature-based Models": "For the LLM feature-based models, we use ridge re-gression for OUM and logistic regression for Wiki-tactics and AFD. In order to assess the annotationquality of dispute tactics, information contentand style and tone feature sets, one of the authorsinspected 100 utterances and found that the accu-racy9 ranged between 86%-100% across individualfeatures, which we deem as acceptably low errorrates. Similarly, we check the annotations of QoAby extracting 100 pairs of dialogues to conduct pair-wise comparison (i.e., which dialogue should beranked higher in terms of the QoA?) to obtain hu-man judgement data. This results in an agreementrate of 81% between human judgement and LLMsjudgement10. We consider this agreement rate tobe sufficiently high, given this features subjectivenature. In Appendix A.4, we provide an error anal-ysis to better understand the noise introduced bythe LLMs annotations.Lastly, for the OUM dataset, in addition to calcu-lating statistics for each features values throughout",
  "Baselines": "We include eight well-established baselines: 1)Random classifier or Average regressor; 2) Bag-of-Words, using ridge/logistic regression algorithms;3) GloVe Embeddings, by feeding the averagepre-trained GloVe embeddings (Pennington et al.,2014) as input for ridge/logistic regression algo-rithms; 4) Longformer (full), which correspondswith fine-tuning the entire Longformer-large (Belt-agy et al., 2020); 5) Longformer (last layer), whichfine-tunes only the last layer of Longformer-large;6) zero-shot prompted GPT-4o; 7) twenty-shotprompted GPT-4o11 and 8) Standard feature-basedmodels, fed with discrete features exclusively, us-ing ridge/logistic regression algorithms.",
  "Modelling and Evaluation": "We train all models (except for N-shot GPT-4obaselines) with seven-fold flat cross-validation(Wainer and Cawley, 2021).This method canprovide a reliable assessment of model perfor-mance while drastically lowering the computa-tional load. It is especially useful for moderatelysized datasets like ours and can produce resultssimilar to nested cross-validation for models withrelatively few hyperparameters. Appendix A.6 de-tails the hyperparameter-tuning, training budgetand computing infrastructure.For the evaluation of classifiers, we use the AreaUnder the Receiver Operating Characteristic curve(AUROC) and the Area Under the Precision-Recallcurve (AUPR). For regressors, we use SpearmansRank Correlation and Mean Absolute Error (MAE).We report the average and standard deviation ofthe results from three seeds for all models exceptfor the 20-shot prompted GPT-4o baseline to due"
}