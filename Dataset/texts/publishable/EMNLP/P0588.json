{
  "Abstract": "Automatic question generation (QG) serves awide range of purposes, such as augmentingquestion-answering (QA) corpora, enhancingchatbot systems, and developing educationalmaterials. Despite its importance, most exist-ing datasets predominantly focus on English,resulting in a considerable gap in data avail-ability for other languages. Cross-lingual trans-fer for QG (XLT-QG) addresses this limitationby allowing models trained on high-resourcelanguage datasets to generate questions in low-resource languages. In this paper, we propose asimple and efficient XLT-QG method that oper-ates without the need for monolingual, parallel,or labeled data in the target language, utilizinga small language model. Our model, trainedsolely on English QA datasets, learns interrog-ative structures from a limited set of questionexemplars, which are then applied to generatequestions in the target language. Experimen-tal results show that our method outperformsseveral XLT-QG baselines and achieves per-formance comparable to GPT-3.5-turbo acrossdifferent languages. Additionally, the syntheticdata generated by our model proves beneficialfor training multilingual QA models. With sig-nificantly fewer parameters than large languagemodels and without requiring additional train-ing for target languages, our approach offers aneffective solution for QG and QA tasks acrossvarious languages1.",
  "We release our code and question exemplars used inour experiments at": "2018), and QuAC (Choi et al., 2018). However, themajority of these datasets are in English, resultingin a significant lack of data for other languages.Moreover, translating English datasets into otherlanguages or creating new QA datasets, despitethe availability of similar English datasets, is of-ten inefficient in terms of both time and financialresources.Recently, researchers have concentrated oncross-lingual transfer (XLT) to address data defi-ciencies in non-English languages (Sherborne andLapata, 2022; Wu et al., 2022a; Vu et al., 2022; Debet al., 2023; Pfeiffer et al., 2023). XLT involvesdeploying models trained on English datasets toother languages when annotated data in the targetlanguage is limited or unavailable.Additionally, in recent years, multilinguallarge language models (mLLMs), such as GPT-4 (Achiam et al., 2023), BLOOM (Workshop et al.,2022), and PaLM (Chowdhery et al., 2023), haveexhibited remarkable performance across variousnatural language generation (NLG) tasks, oftenachieving high efficacy through zero or few-shotinference. However, significant cost burdens areassociated with utilizing commercial APIs, andemploying open-source LLMs requires substantialcomputational resources. Previous studies on XLTfor QG (XLT-QG) have typically utilized target lan-guage data, such as monolingual corpora, source-target parallel corpora, or a limited number of QAexamples (Kumar et al., 2019; Chi et al., 2020;Shakeri et al., 2021; Wang et al., 2021; Agrawalet al., 2023). Nevertheless, incorporating language-specific data during model training can lead toinflexibility in language scalability, necessitatingadditional training efforts for applications in newlanguages.In this paper, we present a simple and efficientXLT-QG method that generates Questions by learn-ing Interrogative Structures in Target languages(QuIST). QuIST comprises two stages: 1) Ques- tion Type Classification (QTC) and 2) QG utilizingquestion exemplars. We categorize questions intoeight types based on English interrogative words,and the QTC model determines the type of questionto be generated based on the input context and an-swer. Once the question type is identified, it is usedto select the corresponding question exemplars forthe QG stage.The QG model generates questions based ona given input context, answer, and question ex-emplars. During training with English data, themodel learns to identify the interrogative structuresspecific to each question type from the providedexemplars. This approach enables the model togenerate questions that are not only semanticallyaligned with the input context and answer but alsosyntactically similar to the exemplars. By train-ing exclusively on English data, we ensure that themodel can generate questions in other languageswithout requiring additional training.In our experiments, we evaluate the performanceof QuIST across nine linguistically diverse lan-guages. Through both automatic and human eval-uation, we show that QuIST outperforms variousXLT-QG baselines and achieves performance com-parable to GPT-3.5-turbo in several languages. Fur-thermore, we confirm that synthetic questions gen-erated by QuIST are more effective for traininghigh-performance multilingual QA models thanthose generated by GPT-3.5-turbo.Our contributions can be summarized as follows:",
  "Cross-lingual Transfer for AutomaticQuestion Generation": "The zero-shot XLT approachleveraging mul-tilingual pretrained language models (mPLMs)fine-tuned exclusively on English data for tar-get languageshas shown promising performanceacross various classification tasks (Liu et al., 2019;Conneau and Lample, 2019; Gritta and Iacobacci,2021; Wu et al., 2022a; Li and Murray, 2023).However, when applied to natural language gener-ation (NLG) tasks, this approach often results incatastrophic forgetting of the target language. Tomitigate this issue,Maurya et al. (2021) proposedfine-tuning only the encoder layers of mPLMswhile keeping the word embeddings and all de-coder layer parameters frozen.",
  ") and mBART (Liu et al., 2020) fine-tuned on En-glish QA datasets. The questions often contain En-glish interrogative expressions such as How long andWhen did": "In our preliminary investigation, we found thatthis technique did not completely prevent code-switching in XLT-QG, as shown in . Specif-ically, the models struggled to fully grasp in-terrogative structures in the target language, aphenomenon we refer to as interrogative code-switching. In this study, we propose a methodthat enables small mPLMs to learn interrogativestructures without relying on target language dataduring training.As illustrated in , we divide the taskinto two stages. In the QTC stage, a classificationmodel identifies the type of question to be gen-erated. We focus on Wh-questions, categorizingthem into eight types based on English interroga-tive words. While the type of question is primarilyinfluenced by the type of answer, the model consid-ers both the answer and the context. This is crucial, WhenWhatHow",
  ": Overview of our proposed method: The QG model generates questions utilizing the question exemplarscorresponding to the question type determined by the QTC model": "as the same answer can result in different types ofquestions depending on the context. For example,the number 911 could refer to a quantity, year, orproper noun.The set of question exemplars corresponding tothe question type identified by the QTC model isused in the QG stage. These exemplars are pre-created for each question type and language, asdetailed in .1. By leveraging the sharedinterrogative structures among the exemplars, theQG model generates questions using the providedanswer and context. Both the QTC and QG modelsare trained exclusively on English QA data and canbe deployed to new languages without the need foradditional training with target language data.",
  "Question Type Classification": "We categorize questions into eight types: When,Where, What, Which, Who, Why, Howway, andHownumber2. To train the QTC model, we first an-notate the question types in the English QA dataset,considering only those questions that fit into one ofthe eight categories. Specifically, questions startingwith how are classified as Howway if followedby an auxiliary verb, or as Hownumber if followedby an adjective or adverb.In this stage, we apply the zero-shot XLT ap-proach. We fine-tune mBERT (Devlin et al., 2018)with a feed-forward classification layer using En-glish QA data. The concatenation of the answer andcontext, separated by special tokens (i.e., [CLS]answer [SEP] context [SEP]), is fed into theQTC model. After encoding the input sequence us-",
  "Howway-inquire about the manner in which something isdone, while Hownumber-questions seek information regardinga degree or specific number": "ing mBERT, the output hidden vector correspond-ing to the [CLS] token is passed through a feed-forward layer, followed by the softmax function, tocompute probabilities for the eight question types.We use cross-entropy loss between the predictedand ground-truth labels to update all model param-eters. During inference in target languages, thefine-tuned model predicts the question type by con-sidering the answer and context in those languages.",
  "Question Generation with QuestionExemplars": "We employ mT5 (Xue et al., 2021) as the backboneof our QG model, framing the task as a sequence-to-sequence prediction problem. The model is trainedusing the teacher-forcing technique to generate theground-truth question based on the provided ques-tion exemplars, answer, and context. During train-ing, the model learns to leverage the syntactic in-formation from the question exemplars to generatequestions that are both syntactically correct andsemantically appropriate for the given context andanswer. During inference, the question exemplarscorresponding to the question type predicted by theQTC model are input into the QG model, helpingit comprehend the interrogative structures of thetarget language.",
  "Data": "QA DatasetsWe used SQuAD1.1 (Rajpurkaret al., 2016) as the English QA dataset (C-Q-Aen)to train both the QTC and QG models. For eval-uation, we collected QA examples in nine targetlanguages (C-Q-Atgt) from multilingual human-annotated QA datasets, including TyDiQA (Clarket al., 2020), XQuAD (Artetxe et al., 2020) andMLQA (Lewis et al., 2020). These datasets con-sist of contextquestionanswer triplets, where theanswer is a span within the context. Details aboutthese datasets are provided in Appendix D.Question ExemplarsThe English question ex-emplars (Qen) were randomly selected from thequestions in the training set of C-Q-Aen after label-ing question types as described in .13. Togather question exemplars in the target languages(Qtgt) written by native speakers, we utilized thequestions from the training set of C-Q-Atgt. Af-ter translating these questions into English usingGoogle Translation API, we constructed the ques-tion exemplars in the same manner as for English.We experimented with several versions of ques-tion exemplars containing different number of ques-tions: {1, 5, 10, 15}. In addition, we randomlysampled each version of the exemplars five timesusing different random seeds. Consequently, wetrained five distinct QuIST models using differentEnglish question exemplars. During the inferencestage, five sets of exemplars for each target lan-guage were utilized for evaluation. As a result, in, we report the average of 25 automaticevaluation results.",
  "Baselines": "We compared our QuIST method with several XLT-QG models that share the same backbone, mT5.All baselines treat the QG task as a sequence-to-sequence prediction, training the models to gen-erate questions based on the concatenation of theinput answer and context.BaselineEncDec This model was simply trained byfine-tuning all parameters of mT5 using C-Q-Aen.This approach was introduced to examine the effectof training the parameters of the embedding layerand decoder on English data regarding catastrophicforgetting in the target language. 3In preliminary experiments, we observed that using fixedexemplars was more effective than configuring random exem-plars for each training example. A detailed analysis of thisfinding is provided in Appendix A. BaselineEnc Unlike BaselineEncDec, only param-eters of the encoder layers of mT5 were updated forthis baseline model. This training technique wasalso employed to train QuIST, but the two mod-els differ in whether the question exemplars areutilized.BaselineMulti Inspired by the method proposedby Shakeri et al. (2021), we adopt multi-task fine-tuning, where mT5 simultaneously learns the En-glish QG task and the question denoising task. Thedenoising task aims to restore questions with ran-domly masked tokens and we used Qtgt with 15 ex-emplars for each question type (i.e., 120 questions)for a fair comparison with QuIST. We use thisbaseline to check whether utilizing a small num-ber of question exemplars during the fine-tuningstage is also effective in XLT-QG. As this baselinelearned language-specific data during training, weconstructed different models for each language.BaselineAdapterWe implemented the Adapter-based mT5, which have been recently utilized inXLT for various NLP tasks (Pfeiffer et al., 2020;Deb et al., 2023; Pfeiffer et al., 2023; Wu et al.,2023). After training language-specific adaptersusing monolingual corpora4, we trained the task-specific adapters using C-Q-Aen, where the Englishadapters are incorporated. While updating eachtype of adapter, we froze all other model parame-ters. In contrast to QuIST, this baseline does notutilize Qtgt, but instead requires large-scale mono-lingual corpora in target languages.",
  "GPT-3.5-turbozero33.9821.3027.7635.5524.8431.1818.5627.9017.3141.6727.34GPT-3.5-turbo1037.6321.5129.4939.4126.6032.5422.2830.1223.1344.4729.95": ": Automatic evaluation results for the nine target languages. This table shows the ROUGE-L performanceof the models (SP-ROUGE (Vu et al., 2022) scores for Chinese). The best scores among mT5-based models are inbold and the highest scores among all models are marked with . We also report BLEU4 and METEOR scores andstandard deviations in Appendix F.",
  "Main Results": "Comparison with Baselines presents theperformance of QuIST and the baseline modelsacross nine target languages. The results showthat QuIST15, which achieved the highest per-formance among our models with varying num-bers of question exemplars, outperforms severalXLT-QG baselines, demonstrating a margin of6.00 points compared to the most robust baseline,BaselineAdapter. While adapting BaselineAdapterto a new language necessitates training language-specific adapter modules, our model can be readilydeployed in new languages without the need foradditional training.QuIST notably outperforms BaselineEnc acrossall languages. Note that both models have the samenumber of trainable parameters during the fine-tuning stage. These results indicate that exposingthe model to interrogative structures during theinference stage significantly enhances its ability togenerate questions in the target language.Despite BaselineMulti learning questions in thetarget language via the denoising task, it exhib-ited poor performance, even scoring lower thanBaselineEnc. Upon reviewing the generated resultsof BaselineMulti, we frequently observed instanceswhere the questions were unrelated to the input con-text or answer. These findings suggest that utilizinga small number of question exemplars during thetraining stage may lead to overfitting, resulting in adecline in model performance.Comparison with LLMsWe also comparedQuIST and GPT-3.5-turbo, which stands out as a relatively cost-effective option among various com-mercial LLMs and demonstrates satisfactory re-sults using only a few examples. We evaluated theperformance of GPT-3.5-turbo through zero-shotinference and 10-shot inference, using prompts thatincluded 10 English examples sampled from C-Q-Aen. The prompt templates we used are providedin Appendix E.According to the results, QuIST15 shows higherscores on average than the zero-shot and 10-shotinference of GPT-3.5-turbo. In detail, our modelexhibits better performance in several languages,particularly in Hindi, Korean, Telugu, Swahili, andChinese. Additionally, we investigated the few-shot inference of GPT-3.5-turbo that utilized ourQTC model and question exemplars. The resultsare reported in Appendix G.Human Evaluation We conducted a human eval-uation in six languages where QuIST and GPT-3.5-turbo10 exhibited similar automatic evaluationresults, and we also evaluated the strongest base-line model, BaselineAdapter. We collected a totalof 240 questions generated by the three models perlanguage and asked three native speakers to assessthe question quality based on five criteria: Inter-rogative Sentence (I), Grammatical Correctness(G), Clarity (C), Answerability (A), Answer-Match(A.M.). The first two metrics were rated on a scaleof 0, 1, 2, while responses for the remaining cate-gories were binary (yes or no). More informationregarding these criteria is described in AppendixC.2. presents the majority responses fromthree raters. For the criteria of clarity, answerabil-ity, and answer-match, we report the percentageof yes responses. In German, Finnish, and In- bntekozhhiswidfide Percentage of code-switched questions (%) B-EncDecB-EncB-MultiB-AdapterQuISTGPT-3.5 (10-shot) : Percentage of code-switched synthetic questions. The patterned lower section of each bar represents theproportion of questions with only interrogative code-switching, while the full bar indicates the total proportion of allquestions involving any form of code-switching.",
  ": Human evaluation results": "donesian, the questions generated by QuIST andGPT-3.5-turbo10 consistently received high scoresacross all criteria. Specifically, both models ef-fectively generate questions that align with thegiven answers, outperforming BaselineAdapter. Incontrast, our model achieves lower overall scoresin Bengali and Hindi compared to the previouslymentioned languages. However, this performancedecline is also observed in GPT-3.5-turbo10 andBaselineAdapter.In Swahili, QuIST lagged significantly be- hind GPT-3.5-turbo10 in terms of Answerabil-ity and Answer-Match. However, given thatBaselineAdapter generates questions of significantlylower qualitydespite outperforming all other base-line models in automated evaluationit is notewor-thy that our model can generate Swahili questionsof acceptable quality without any specific trainingin the target language.",
  "Interrogative Code-switching": "We investigated the frequency of interrogative code-switching occurrence in questions generated by dif-ferent XLT-QG methods5. As depicted in , interrogative code-switching is observed in themajority of questions generated by BaselineEncDec.This phenomenon can be attributed to catastrophicforgetting in target languages, as both the encoderand decoder were fine-tuned using English data.In BaselineEnc, where only the encoder was fine-tuned, this issue is slightly alleviated; nevertheless,more than half of the synthetic questions still ex-hibit this code-switching problem.Through the results of BaselineMulti, we con-firm that interrogative code-switching is alleviatedin numerous languages due to the impact of thequestion denoising task specific to the target lan-guage. Both QuIST and BaselineAdapter provecomparable effectiveness in mitigating interroga-tive code-switching, surpassing other baseline ap-proaches. Specifically, our model demonstrates ef-fective in alleviating interrogative code-switching 5We used cld3 ( toidentify the languages. If the target language comprised lessthan 70% of the generated question, it was classified as code-switching. If the target language accounted for more than 70%but included English interrogative words, it was classified asinterrogative code-switching.",
  "Data Augmentation for QuestionAnswering": "We explored the potential of QuIST for augmentingtraining data for multilingual QA models. Specif-ically, we compared synthetic data generated byQuIST and baseline models6 with the multilingualQA dataset generated by Agrawal et al. (2023),which used their PaLM-540B model prompt-tunedwith five QA examples from target languages. Ta-ble 4 presents the average exact match (EM) scoresacross six languages for the multilingual QA mod-els. The training details can be seen in AppendixB.According to the results, QuIST achieves thebest performance, surpassing GPT-3.5-turbo10 andprompt-tuned PaLM-540B. Interestingly, contraryto the findings from the automatic evaluation andinterrogative code-switching analysis, BaselineEncdemonstrates greater effectiveness in QA data aug-mentation compared to BaselineAdapter. In the ear-lier experiment, over 70% of the questions gen-erated by BaselineEnc exhibited code-switchingissues. However, unlike BaselineAdapter, which de-pends solely on task-specific adapters for learningthe QG task, BaselineEnc leverages all encoder pa-rameters. This suggests that BaselineEnc may becapable of producing questions with higher seman-tic quality.",
  "Impact of Different Question Exemplars": "We investigated the impact of utilizing differentmethods for constructing question exemplars com-pared to our proposed approach. These approacheswere compared to BaselineEnc, where only the en-coder is fine-tuned on English data, without usingadditional data from target languages during bothtraining and inference. presents the averageROUGE scores across nine languages.(1) QuIST utilizes human-written question exem-plars in target languages during inference. In thisexperiment, we evaluate the models performanceusing exemplars translated from English questionsvia the Google Translation API. The results showthat while machine-translated exemplars improvetarget language question generation compared toBaselineEnc, they are less effective than human-written exemplars.(2) We conducted training and inference usingexemplars that covered all question types to eval-uate the effectiveness of type-specific question ex-emplars. The exemplars included two instances ofeach of the eight question types, totaling 16 ques-tions, and the QTC model was not used in thissetting. The results indicate a slight performanceimprovement compared to BaselineEnc; however,this effect is marginal.(3) We investigated whether input question exem-plars during the inference stage are beneficial, evenwithout the training process for generating ques-tions using question exemplars. The model wastrained to generate a question based on the givencontext and answer without utilizing the question exemplars, similar to BaselineEnc, and only usedthe exemplars in the inference stage. In this setting,question exemplars in the target language were nothelpful, meaning that QuIST learns how to utilizequestion examples for QG during training.",
  ": Performance of the QTC model": "To measure the zero-shot inference performanceof the QTC model for the target languages, wefirst annotated the ground-truth question types ofthe target language QA data. We translated thequestions into English and conducted annotationas detailed in .1 (i.e., hard labeling). Ta-ble 6 displays the macro F1 scores of the QTCmodel, measured based on ground-truth labels con-structed in two ways. Since most Wh-questionscan be paraphrased into questions beginning withwhat and which,7 we also evaluate the QTCperformance in a setting where what and whichare accepted as additional gold labels (i.e., relaxedlabeling). According to the results measured withthe relaxed labels, the model correctly classifiedmore than 90% of questions. This suggests that theerror propagation resulting from misclassificationin QTC is minimal throughout the entire pipeline.",
  "Case Study": "We analyzed the questions generated by the mod-els we used in the experiments, particularly focus-ing on Swahili, where our model received lowerrating than GPT-3.5-turbo10 in human evaluation.In , we can see that the question gener-ated by QuIST is insufficient to explain the givenanswer, and these incorrect generations resultedin the low Answer-Match score. We also notethat BaselineEncDec and BaselineEnc encountercode-switching issues, and the question generatedby BaselineMulti contains information that is notpresent in the context. Furthermore, the questiongenerated by BaselineAdapter was assessed as notbeing a question, as it is a descriptive sentenceending with a question mark.",
  "For example, How large is the Mupartifad village? isequivalent to What is the area of Mupartifad village?": "Context: Malawi, Zambia na Zimbabwe wakati mwingine zinehesabiwakuwa sehemu ya Afrika ya Kusini (zamani zilikuwa pamoja kama Rhodesia ya Kusini, Rhodesia ya Kaskazini na Unyasa katika Shirikisho la Afrika yaKati)(Malawi, Zambia and Zimbabwe are sometimes considered part of South Africa (they used to be together as Southern Rhodesia, Northern Rhodesia and Nyasa in the Central African Federation))Answer: Zambia BaselineEncDecAlong with Malawi, Zimbabwe and Zimbabwe, which nations sometimes zinehesabiwa sehemu ya Africa yaKusini?(Along with Malawi, Zimbabwe and Zimbabwe, which nations are sometimes considered part of South Africa?)",
  "Related Work": "Prior work on XLT for NLG tasks has primar-ily focused on training models with source lan-guage datasets while maintaining the ability to gen-erate outputs in the target language. For exam-ple, Mallinson et al. (2020) and Chi et al. (2020)leveraged parallel corpora to improve the align-ment between source and target languages, facil-itating a more effective transfer of task-relatedknowledge. Similarly, Maurya et al. (2021) en-hanced the mPLM model through an auxiliarytask closely related to the downstream task, us-ing only monolingual data, and applied it to vari-ous NLG tasks in the XLT setting. In another ap-proach, Vu et al. (2022) demonstrated that prompt-tuning effectively mitigated catastrophic forgettingof the target language in zero-shot cross-lingualsummarization. More recently, researchers such asWu et al. (2022b), Deb et al. (2023), and Pfeifferet al. (2023) have explored methods to separate theacquisition of language-specific knowledge fromlanguage-agnostic knowledge, aiming to improvecross-lingual performance.Unlike most generation tasks that focus on pro- ducing declarative sentences, QG involves theadditional complexity of generating interrogativesentences designed to elicit specific information.While our approach avoids training models usingtarget language data, much of the prior researchhas relied on such data. For instance, Kumar et al.(2019) utilized a combination of English QA dataand a limited amount of target language data. Incontrast, Shakeri et al. (2021) trained their modelusing a denoising task on a question corpus in thetarget language. Additionally, Agrawal et al. (2023)prompt-tuned the PaLM-540B model using fivesets of target language QA examples and used themodel to synthesize multilingual QA dataset. Fi-nally, Chi et al. (2020) adopted a language model-ing approach with parallel corpora and restrictedthe question decoding phase to tokens from thetarget language vocabulary.",
  "Conclusion": "In this paper, we proposed a simple yet effectiveXLT-QG approach, where the question generationmodel is trained solely on an English QA datasetand leverages a small set of target language ques-tions during inference. By incorporating questionexemplars from target languages, our method en-ables the model to learn the interrogative structuresof those languages, effectively addressing the issueof code-switching.Experimental results demonstrate that this ap-proach significantly outperforms several XLT-QGbaselines and achieves performance comparable toGPT-3.5-turbo across a variety of languages. Ad-ditionally, we validated the utility of our methodssynthetic data for training multilingual QA models.A key strength of our method lies in its scalabil-ity and parameter efficiency, as it relies exclusivelyon English QA data during training. This enablesthe seamless extension to new languages withoutthe need for additional parameter updates. More-over, in contrast to LLMs, our approach employssmaller backbone models, offering the advantagesof lower deployment costs and reduced computa-tional requirements, making it more accessible forpractical use in diverse multilingual settings.",
  "While our model demonstrates strong cross-lingualcapabilities, its applicability remains constrainedto the languages on which the mPLMs have beentrained. Although the mT5 model employed in": "our study was pre-trained on a diverse set of 101languages, there remain many underrepresented orlow-resource languages where the models perfor-mance may be limited.Another limitation is the instability in model per-formance, which can vary depending on the con-figuration of the question exemplars in the targetlanguage. Some questions generated by the modelcontinue to exhibit code-switching issues. Whilethis issue may affect the grammatical and linguis-tic consistency of the outputs, it can be mitigatedthrough the use of a simple rule-based filtering tech-nique. Nonetheless, this solution may not entirelyeliminate the problem and could require furtherrefinement, particularly in more complex multilin-gual contexts.",
  "Acknowledgements": "This research was supported by the MSIT (Ministryof Science and ICT), Korea, under the ITRC (In-formation Technology Research Center) supportprogram (IITP-2024-RS-2024-00437866) super-vised by the IITP (Institute for Information & Com-munications Technology Planning & Evaluation)and also by the Technology Innovation Program(20015007, Development of Digital Therapeuticsof Cognitive Behavioral Therapy for treating PanicDisorder) funded By the Ministry of Trade, Indus-try & Energy (MOTIE, Korea). Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774. Priyanka Agrawal, Chris Alberti, Fantine Huot, JoshuaMaynez, Ji Ma, Sebastian Ruder, Kuzman Ganchev,Dipanjan Das, and Mirella Lapata. 2023. Qameleon:Multilingual qa with only 5 examples. Transactionsof the Association for Computational Linguistics,11:17541771. Mikel Artetxe, Sebastian Ruder, and Dani Yogatama.2020. On the cross-lingual transferability of mono-lingual representations. In Proceedings of the 58thAnnual Meeting of the Association for ComputationalLinguistics, pages 46234637. Satanjeev Banerjee and Alon Lavie. 2005. Meteor: Anautomatic metric for mt evaluation with improved cor-relation with human judgments. In Proceedings ofthe acl workshop on intrinsic and extrinsic evaluationmeasures for machine translation and/or summariza-tion, pages 6572. Zewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-Ling Mao, and Heyan Huang. 2020. Cross-lingualnatural language generation via pre-training. In Pro-ceedings of the AAAI conference on artificial intelli-gence, volume 34, pages 75707577. Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettle-moyer. 2018. Quac: Question answering in context.In Proceedings of the 2018 Conference on Empiri-cal Methods in Natural Language Processing, pages21742184. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts, PaulBarham, Hyung Won Chung, Charles Sutton, Sebas-tian Gehrmann, et al. 2023. Palm: Scaling languagemodeling with pathways. Journal of Machine Learn-ing Research, 24(240):1113. Jonathan H Clark, Eunsol Choi, Michael Collins, DanGarrette, Tom Kwiatkowski, Vitaly Nikolaev, andJennimaria Palomaki. 2020. Tydi qa: A benchmarkfor information-seeking question answering in ty po-logically di verse languages. Transactions of the As-sociation for Computational Linguistics, 8:454470.",
  "Milan Gritta and Ignacio Iacobacci. 2021. Xeroalign:Zero-shot cross-lingual transformer alignment. InFindings of the Association for Computational Lin-guistics: ACL-IJCNLP 2021, pages 371381": "Taku Kudo and John Richardson. 2018. Sentencepiece:A simple and language independent subword tok-enizer and detokenizer for neural text processing. InProceedings of the 2018 Conference on EmpiricalMethods in Natural Language Processing: SystemDemonstrations, pages 6671. Vishwajeet Kumar, Nitish Joshi, Arijit Mukherjee,Ganesh Ramakrishnan, and Preethi Jyothi. 2019.Cross-lingual training for automatic question genera-tion. In Proceedings of the 57th Annual Meeting ofthe Association for Computational Linguistics, pages48634872.",
  "Chin-Yew Lin. 2004. Rouge: A package for automaticevaluation of summaries.In Text summarizationbranches out, pages 7481": "Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, SergeyEdunov, Marjan Ghazvininejad, Mike Lewis, andLuke Zettlemoyer. 2020. Multilingual denoising pre-training for neural machine translation.Transac-tions of the Association for Computational Linguis-tics, 8:726742. Zihan Liu, Jamin Shin, Yan Xu, Genta Indra Winata,Peng Xu, Andrea Madotto, and Pascale Ngan Fung.2019. Zero-shot cross-lingual dialogue systems withtransferable latent variables. In EMNLP-IJCNLP2019-2019 Conference on Empirical Methods in Nat-ural Language Processing and 9th International JointConference on Natural Language Processing, Pro-ceedings of the Conference.",
  "Ilya Loshchilov and Frank Hutter. 2018. Decoupledweight decay regularization. In International Confer-ence on Learning Representations": "Jonathan Mallinson, Rico Sennrich, and Mirella Lapata.2020. Zero-shot crosslingual sentence simplification.In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 51095126. Kaushal Kumar Maurya, Maunendra Sankar Desarkar,Yoshinobu Kano, and Kumari Deepshikha. 2021.Zmbart:An unsupervised cross-lingual transferframework for language generation. arXiv preprintarXiv:2106.01597. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalu-ation of machine translation. In Proceedings of the40th annual meeting of the Association for Computa-tional Linguistics, pages 311318. Jonas Pfeiffer, Francesco Piccinno, Massimo Nicosia,Xinyi Wang, Machel Reid, and Sebastian Ruder.2023.mmt5: Modular multilingual pre-trainingsolves source language hallucinations. In Findingsof the Association for Computational Linguistics:EMNLP 2023, pages 19782008. Jonas Pfeiffer, Ivan Vulic, Iryna Gurevych, and Se-bastian Ruder. 2020.Mad-x: An adapter-basedframework for multi-task cross-lingual transfer. InProceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 76547673. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, andPercy Liang. 2016. Squad: 100,000+ questions formachine comprehension of text. In Proceedings ofthe 2016 Conference on Empirical Methods in Natu-ral Language Processing, pages 23832392. Siamak Shakeri, Noah Constant, Mihir Kale, and Lint-ing Xue. 2021.Towards zero-shot multilingualsynthetic question and answer generation for cross-lingual reading comprehension. In Proceedings ofthe 14th International Conference on Natural Lan-guage Generation, pages 3545. Tom Sherborne and Mirella Lapata. 2022. Zero-shotcross-lingual semantic parsing. In Proceedings of the60th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages41344153. Tu Vu, Aditya Barua, Brian Lester, Daniel Cer, Mo-hit Iyyer, and Noah Constant. 2022. Overcomingcatastrophic forgetting in zero-shot cross-lingual gen-eration. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Processing,pages 92799300. Bingning Wang, Ting Yao, Weipeng Chen, Jingfang Xu,and Xiaochuan Wang. 2021. Multi-lingual questiongeneration with language agnostic language model.In Findings of the Association for ComputationalLinguistics: ACL-IJCNLP 2021, pages 22622272. BigScience Workshop, Teven Le Scao, Angela Fan,Christopher Akiki, Ellie Pavlick, Suzana Ilic, DanielHesslow, Roman Castagn, Alexandra Sasha Luc-cioni, Franois Yvon, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model.arXiv preprint arXiv:2211.05100. Han Wu, Haochen Tan, Kun Xu, Shuqi Liu, Lianwei Wu,and Linqi Song. 2022a. Zero-shot cross-lingual con-versational semantic role labeling. In Findings of theAssociation for Computational Linguistics: NAACL2022, pages 269281. Ting-Wei Wu,Changsheng Zhao,Ernie Chang,Yangyang Shi, Pierce Chuang, Vikas Chandra, andBiing Juang. 2023. Towards zero-shot multilingualtransfer for code-switched responses. In Proceedingsof the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 75517563. Xianze Wu, Zaixiang Zheng, Hao Zhou, and Yong Yu.2022b. Laft: Cross-lingual transfer for text genera-tion by language-agnostic finetuning. In Proceedingsof the 15th International Conference on Natural Lan-guage Generation, pages 260266. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale,Rami Al-Rfou, Aditya Siddhant, Aditya Barua, andColin Raffel. 2021. mt5: A massively multilingualpre-trained text-to-text transformer. In Proceedingsof the 2021 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 483498. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,William Cohen, Ruslan Salakhutdinov, and Christo-pher D Manning. 2018. Hotpotqa: A dataset fordiverse, explainable multi-hop question answering.In Proceedings of the 2018 Conference on EmpiricalMethods in Natural Language Processing. Associa-tion for Computational Linguistics.",
  "AStatic and Dynamic Exemplars": "Since gathering sufficient question samples in thetarget languages is challenging, we used fixed ques-tion exemplars during inference. In contrast, En-glish question exemplars can be easily sourcedfrom QA datasets. Thus, we experimented withtwo approaches for creating question exemplars totrain the QG model: (1) Static exemplars, whichuse fixed exemplars across all training examples,and (2) Dynamic exemplars, which are sampledfrom the English QA dataset for each training ex-ample.",
  "AVG29.2331.93": ":Comparison of models using dynamic andstatic exemplars during training. We report SP-ROUGEscores for Chinese and ROUGE-L scores for other lan-guages. The scores for the static setting are based on theEnglish exemplars, representing median performance. As shown in , both approaches demon-strate effective performance in target languagescompared to the existing XLT-QG baseline models(). However, the static exemplar methodachieves better overall performance across variouslanguages. During training, our model generatesquestions by leveraging the syntactic informationfrom the exemplars while utilizing the semanticinformation from the input context and answer. Wehypothesize that the model trained with static ex-emplars was better able to focus on the syntacticstructures of the example questions, leading to im-proved performance. Consequently, we utilizedstatic exemplars in all our experiments.",
  "BImplementation Details": "We utilized a single NVIDIA Tesla A100-80GB GPU for model training.The QTCandQGmodelswereinitializedusingbert-base-multilingual-casedwith110Mparameters and google/mt5-large with 1.2Bparameters, sourced from HuggingFace8. Trainingwas conducted employing stochastic gradientdescent with the AdamW optimizer (Loshchilovand Hutter, 2018) coupled with a linear learningrate scheduler encompassing 1000 warm-up steps.Batch sizes and learning rates were set as (8, 1e-5)and (16, 5e-5) for QTC and QG, respectively.Training ceased upon optimization of the modelson the validation set.Due to variations in the number of examplesacross different question types, we employed dataupsampling based on the type with the highest num-ber of examples for training the QTC model. Dur-ing the inference stage, we determined the questiontype with the highest predicted probability fromthe QTC model and generated questions using thebeam search algorithm with a beam size of 4.To train multilingual QA models in .2,we adopted the methodologies used by Agrawalet al. (2023). Each QA model underwent train-ing using a combination of English data sourcedfrom the TyDiQA training set and synthetic datafor all languages, generated by each XLT-QGmodel. Given the unavailability of the TyDiQA testset, we evaluated the validation performance in-stead. The backbone of the QA model consisted ofgoogle/mt5-xl with 3.7B parameters, fine-tunedwith a learning rate of 2e-4 and a batch size of64. We selected the model checkpoint yieldingthe highest EM score for each language, followingthe strategy of Agrawal et al. (2023), and reportedthe average scores obtained from utilizing threedifferent random seeds.",
  "We enlisted three native speakers for each languagevia Upwork9 to evaluate the quality of our syntheticquestions. The questions were rated based on fivecriteria:": "Interrogative Sentence evaluates whether thequestion has an interrogative structure.0: This is not a question.1: This is a question, but it doesnt have thetypical structure of an interrogative sentence.2: This is a natural interrogative structure. Grammatical Correctness evaluates the gram-matical accuracy of the question.0: Numerous grammatical errors make thequestion unacceptable.1: Some errors exist but do not hinder under-standing of the question.2: The question is grammatically correct.",
  "Answer-Match determines whether the inputanswer could be a valid answer to the ques-tion considering the content of the providedcontext. Answer yes or no": "If a score of 0 is assigned to the InterrogativeSentence category, evaluations for the remainingcategories did not conducted. Additionally, if ascore of 0 is rated in Grammatical Correctness, orif no is selected for Clarity, Answerability, orAnswer-Match categories, subsequent evaluationscan not be carried out. Therefore, in this case, thelowest scores were assigned for these criteria.",
  ": Language codes and the number of examplesin C-Q-Atgt dataset. In our method, only a small portionof the training examples are used as question exemplars": "presents the statistics of target languageQA data C-Q-Atgt utilized by our models duringinference. Note that training examples were solelyemployed for sampling question exemplars Qtgt.Test examples in Chinese, German, and Hindi werecollected from the XQuAD (Artetxe et al., 2020)test set, whereas training examples were sourcedfrom the MLQA (Lewis et al., 2020) validationset, as XQuAD does not provide a training set forthe target languages. Training and test examplesin other languages were obtained from TyDiQA(Clark et al., 2020).",
  "We evaluated the zero-shot and few-shot perfor-mance of gpt-3.5-turbo-0125 model. We ex-": "tracted sets with different numbers of examples: 1,3, 5, and 10, from C-Q-Aen to employ for few-shotinference. In addition, we used five versions ofeach set, varying the random seed. Based on theEnglish validation set, we determined the optimalnumber of examples (see ), and used theset with the median performance as the componentin the few-shot prompt. Subsequently, we con-ducted zero-shot and 10-shot inference for variouslanguages using the prompts described in and 6, respectively.",
  "Input Template": "Considering the given context, generate a question for the given answer in the same language as the given context:[Example 1]Context: In total, Afrikaans is the first language in South Africa alone of about 6.8 million people and is estimated to be a second language for at least 10 million people worldwide, compared to over 23 million and 5 million respectively, for Dutch.Answer: 6.8 millionEnglish question: About how many South Africans speak Afrikaans as their primary language? [Example 10]Context: In ring-porous species, such as ash, black locust, catalpa, chestnut, elm, hickory, mulberry, and oak, the larger vessels or pores (as cross sections of vessels are called) are localised in the part of the growth ring formed in spring, thus forming a region of more or less open and porous tissue. The rest of the ring, produced in summer, is made up of smaller vessels and a much greater proportion of wood fibers. Answer: ring-porousEnglish question: What species of hardwood are hickory and mulberry trees?"
}