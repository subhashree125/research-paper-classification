{
  "Abstract": "For a conversation to help and support, speak-ers should maintain an effect-effort\" trade-off.As outlined in the gist of Cognitive RelevancePrinciple\", helpful speakers should optimizethe cognitive relevance\" through maximizingthe cognitive effects\" and minimizing the pro-cessing effort\" imposed on listeners. Althoughpreference learning methods provide a boonfor studies concerning effect-optimization\",none have delved into effort-optimization\"which is pivotal to the acquisition of opti-mal relevance\" for emotional support conver-sation agents. To address this gap, we inte-grate the \"Cognitive Relevance Principle\" intoemotional support agents in the environment ofmulti-turn conversation. The results demon-strate a significant and robust improvementagainst the baseline systems with respect toresponse quality, human-likedness, and sup-portiveness. This study offers compelling evi-dence for the effectiveness of the \"RelevancePrinciple\" in generating human-like, helpful,and harmless emotional support conversations.The source code will be available at",
  "Introduction": "As one of the central conversation principles, Rel-evance Principle\" stipulates that speakers shouldpreserve a sophisticated balance between effect\"(Wearing, 2015; De Roeck et al., 1991) and effort\"(Gibbs Jr and Tendahl, 2006; Gibbs Jr and Bryant,2008). Such cognitive principle of relevance isessential to emotional support conversations, ascommunicative stimuli will not change listenersemotions if they fail to achieve such a delicate bal-ance (Wharton et al., 2021). To elucidate, illustrates that a helpful speaker should provide asmuch helpful information as possible while mini-mizing the cognitive effort required for the listenerto process the information. To be specific, beingtoo concise\" typically gives rise to the mispercep-",
  ": The Game\" of Cognitive Relevance Principle:People prefer the transition (1->2) rather than (1->3) or(4->5), assuming a cooperative and prosocial communi-cation goal": "tion of indifference and apathy. On the other hand,talking too much\" unavoidably generates ironicand harmful interpretations and potentially threat-ens the recipients mental state (Yus, 2016).Solving the problem of Optimal Relevance\"serves as a pivotal motivation of various dia-logic actions such as lexical choice (Gibbs Jr andBryant, 2008), speech act decision (van Rooy,2001) and emotion control (Scott, 2015). Thoughsubstantial to the cognitive account of communi-cation, the Optimal Relevance\" has been long-lastingly ignored regardless of the staggering riseof Transformer-based dialogue systems, even theLLM chatbots. Pertaining to such a lack of focus,it should be noted that the most recent work thatincorporates Relevance Principle\" with dialoguesystems dates back to 1991 when a \"Relevance\"-aware model was implemented to generate helpfulanswers (De Roeck et al., 1991).To cultivate such a principle in the cognitionof emotional support agents, we need to balancethe optimization of effect\" as the cognitive gain,against the potential users effort\" as the cogni-",
  "z": ": Optimal Relevance Learning (User-in-the-loop): A simulated user, consisting of a generative lan-guage model (such as Llama or DialogGPT) along witha helpfulness scorer (Bert), provides feedback regardingthe alignment of these actions with the cognitive rele-vance principle. tive cost. Although the reward modeling, drivenby the annotation of human preference and feed-back data, successfully reinforces the cognitive ef-fect against various conversation goals (Peng et al.,2023; Cheng et al., 2022; Zhou et al., 2023), fewstudies have embraced the effort\"-modeling to im-prove the cognitive relevance.To provide deeper insight into cognitive rele-vance and its linkage to automatic emotional sup-port, we propose a novel approach named \"OptimalRelevance Learning\" (). Based on this train-ing paradigm, our research question is what willthe reinforcement of Cognitive Relevance bringto the performance of ESC Agent.\" We expect theacquisition of Optimal Relevance\" to improve thesimulation of human cognition in conversation and",
  "Emotional Support Conversation": "Emotional Support Conversation (ESC) requiresthe system to provide help for emotional usersthrough multi-turn conversation (Liu et al., 2021).Cognitive reasoning (Tu et al., 2022; Peng et al.,2022; Zhao et al., 2023; Deng et al., 2023b; Zhouet al., 2023) and emotion perception (Tu et al.,2022; Peng et al., 2022; Zhao et al., 2023; Zhouet al., 2023) have been widely adopted to improveESC Systems. The refined selection of speech act1 is also a central topic. Some attempts utilize theground-truth seeker feedback (Peng et al., 2023)to punish unfavored speech acts. Others draw onhelpfulness judgment data to pretrain a speech actselector (Cheng et al., 2022). A new trend in ESC isto use reinforcement learning to improve the selec-tion of latent experts (Zhou et al., 2023). However,two questions remain unanswered.Firstly, what is the relationship among helpful-ness, judgment, word, and emotion? AlthoughCheng et al. (2022) and Peng et al. (2023) haveexplored the dependency between helpfulness andspeech act, their studies do not provide a compre-hensive understanding of helpfulness-driven dia-logic actions, which should include at least the useof word and emotion regulation. While Zhou et al.(2023) has explored how emotion elicitation is re-lated to latent expert, we believe it is necessary toincorporate word-level policies in the complemen-tation of utterance-level policies.Secondly, improving ESC through multi-turnemotional support is still understudied. The super-vised learning methods train an ESC Agent to give",
  "To align with terminology used in linguistic theory, werefer to the dialogue strategy\" in the ESC dataset as SpeechAct.\"": "a response in a single turn (Tu et al., 2022; Penget al., 2022; Zhao et al., 2023; Zhou et al., 2023;Li et al., 2024). The updated RL method trains themanagement of latent experts within a single turn(Zhou et al., 2023). Given the distinctive featureof emotional support conversation as a multi-turnconversation, we think it unsettling if we fail toimprove ESC Agent through multi-turn conversa-tion. In , we illustrate the key features ofour system compared with comparable approaches.",
  "Dialogue Policy and Optimization": "Human conversation involves a collaboration oflow-level actions (lexical choice on word level)and high-level actions (speech act, emotion regula-tion on utterance level). Modeling the interactionof high-level and low-level policies is a permanentresearch objective. Emotional Support Conversa-tion typically solves multi-level dialogue policyby classifying dialogue state and representing thehigh-level actions as a dense representation (Tuet al., 2022; Li et al., 2024) to affect the generationof low-level policy. Within a broader scope, thelatent variational approach, still untested in ESC,has been widely adopted to model the high-leveldialogue act and influence the level of sequentialgeneration (Wang et al., 2020; Saleh et al., 2020).In terms of word-level policy optimization,RLHF is a representative technique. However, theutterance level policy is not explicitly incorporatedinto the framework of RLHF (Moskovitz et al.,2023; Wang et al., 2023). However, the genericimplementation of RLHF only assigns the feed-back to the final token in the generated sequence.The absence of fine-grained feedback, such as per-word feedback, still challenges the development ofRLHF (Wu et al., 2024).Utterance-level dialogue policy optimization isthe core issue of task-oriented dialogue systems(Rohmatillah and Chien, 2023), which have re-cently shown efficacy in boosting emotional con-versation systems (Deng et al., 2023a). Inspectingutterance-level RL in ESC and Emotional DialogueSystems, most attempts rely on engineered rewardfunctions (Su et al., 2023; Zhou et al., 2023), LLM- derived judgment (Deng et al., 2023a) rather thanmulti-turn human feedback. Besides, the integra-tion of multi-level policy optimization is also un-done in the emotional or supportive dialogue sys-tem. Policy Head(Speech Act) Policy Head(Emotion) To Cross-Attention To Self-Attention To Self-Attention",
  "Method": "Our method centers on Optimal Relevance Learn-ing (ORL for abbreviation) and a Variational La-tent Emotional Support Agent\" (VLESA for ab-breviation). Variational inference has proven effec-tive in modeling high-level policy in task-oriented(Wang et al., 2020) and open-domain dialoguesystems (Saleh et al., 2020). Our VLESA relieson Hierarchical Variational Autoencoder to modelthe coarse-to-fine dependency between high-level(speech act, emotion) and low-level policies (wordgeneration).",
  "KL(qze | su, za, e| p (ze | su, za))(13)": "3.2.2Pretraining ObjectiveThe pertaining objective is the summation of thehierarchical CVAE loss and language model loss.To infuse the user emotion state into su, we alsoimpose a cross entropy loss Lemo over hT againstthe label of the users situational emotion state emo(Liu et al., 2021). So the pre-training objectivespells as Lsft = LLM + 0 LV AE + 1 Lemo",
  "Dataset": "ESConv is a long conversation dataset. Supporterswere asked to perform any of eight different speechacts (Hill, 2009) 3 to comfort the seekers. Seek-ers were required to leave a 5-star scalar feedbackevery two new utterances given by the supporters.The distribution of utterance orders, strategy labels,and feedback scores are in .The conversation-level statistics, such as utter-ance count of dialogue, are in . The splitof train, valuation, and test set follows the officialrepository of Liu et al. (2021).",
  "Baselines": "We reproduce all the baselines for automatic evalu-ation, including MISC (Tu et al., 2022), TransESC(Zhao et al., 2023), MultiESC (Cheng et al., 2022),Cooper (Cheng et al., 2024), Supporter (Zhouet al., 2023), KEMI (Deng et al., 2023b) and Em-stremo (Li et al., 2024). Brief descriptions of thebaseline systems are available in the appendix D.2.",
  ": Automatic Evaluation and Ablation Studies. and denote significant improvement against the secondbest base-line in Automatic Evaluation ( for p < 0.05, dag for p < 0.1)": "with an initial learning rate of 2e-5 and a linearwarmup with 510 warmup steps.We ran allexperiments on two Nvidia GeForce RTX 3090GPUs. We set the max token size as 512. Thenumber of latent variables for the speech act isset to 4. And the number of latent variables foremotion is set to 8. 0 and 1 are set to 0.05.During ORL, we adopt a top_k of 0.0 and top_p of1.0 for the pretrained agent.",
  "Simulated User for ORLLlamaWe prompt llama-2-7b-chat to act asthe emotional user and chat with the pre-trainedemotional support agent. The details of promptingis available in C": "DialogGPTWe use the training set of ESConv tofine-tune microsoft/DialoGPT-small to predictthe upcoming users utterances from the past 8utterances. We preserve the checkpoint reachingthe lowest perplexity for RL training. AdamW isused as the optimizer with a warmup step of 100and a peak learning rate of 2e-5. A linear decayscheduler is used for learning rate control. Weadopt a top_k of 50, top_p of 0.7, and max newtoken of 100 for both two simulated users.",
  "Detailsaboutthefailedexamplesareavailablein": "Bert-base-uncase to predict the upcoming feed-back score from the past 8 utterances. The output ofthe feedback model is a continuous value between1.0 and 5.0. We preserve the checkpoint reachingthe highest Pearson correlation score on the valida-tion set for RL training. The performance on thetest set is a Pearson correlation of 22.4. AdamW isused as the optimizer with a warmup step of 100and a peak learning rate of 2e-5. A linear decayscheduler is used for learning rate control.",
  "Implementation of Optimal RelevanceLearning": "After warm start pretraining, we implement rein-forcement training with one epoch and use thecheckpoint to reach the highest reward for auto-matic evaluation.During ORL, we set the learning rate as 5e-7 andthe batch size of the experience pool as 64. The was set to 1.0. Adam is used as the optimizer. Otherimplementation details are available in appendix Band C.",
  "Automatic Evaluation": "As is displayed in , our model achieves ro-bust (statistically significant) improvement againstthe baselines on the majority of automatic metrics.The comprehensive improvement indicates that theresponses generated from our system closely simu-late the gold standard, especially in terms of the un-igram, bigram, and contextual meaning alignment.The coherence score shows our models leadingcapacity to respond in a smooth and relevant style.Noteworthy as highlighted in , our modelachieves a decisive and significant improvement interms of human-likeness (HumanLike), and non-randomness (Non-Random). This result demon-strates the reliability of the Cognitive RelevancePrinciple\" in training human-like, helpful, andharmless conversation agents.",
  "Interactive Evaluation": "Following (Zhou et al., 2023), we implement in-teractive A-B test for human evaluation. We hiredthree human annotators to interact with the mod-els in multi-turn conversation (for 100 rounds) andchoose the better one in light of four criteria: Co-herent, Helpful, Informative, and Overall. Thedetails about human evaluation are available in D.1.Aligned with the automatic evaluation, shows that our model produces better responsesin comparison with MultiESC. Besides, the learn-ing of optimal relevance will improve the qualityof generated responses.",
  "5microsoft/DialogRPT-human-vs-machine6microsoft/DialogRPT-human-vs-rand7hmicrosoft/DialogRPT-depth8microsoft/DialogRPT-updown": "as the full model, with the one without ORL (w/oORL), and the one without Effort function in ORL(feat. Llama w/o Effort). The results demon-strate that ORL training comprehensively improvesthe response quality in terms of alignment withthe golden response, human-likeness, harmless-ness, and supportiveness. Besides, the removal ofthe Effort function drastically compromises the co-herence (Coherence), relatedness (Non-Random),and harmlessness (Non-toxic) of the generated re-sponse.We further discuss the impact of the integrityof multi-level policy optimization. The results in show the joint optimization of utterance-level policy is considerably essential to the ground-truth alignment and human-likeness. The isolationof speech act policy (w/o SA), emotion policy (w/oEmo), and word-level policy (w/o Word) leads toa lower similarity with ground truth and a reducedhuman-likeness and relatedness.",
  ": Optimal Relevance Learning significantly en-hances human likeness, non-toxicity, and helpfulnesssimultaneously": "To improve the helpfulness of response, the con-ventional practice of RL directly optimizes the dia-logue policies against a subjective goal (the case ofw/o Effort). However, results in indicate thebalancing a subjective goal with the processing ef-fort (the case of feat. Llama and feat. DialogGPT)even improves the acquisition of goal awareness(See the figure named Effect (Helpful)\"). Theisolated training of helpfulness goal leads to a non-optimal solution in the context of emotional supportconversation.Next, we explore the impact of using a simulated",
  "Response:": "(Step = 0)I know that it is hard to find help dur-ing this time, but I know that you will find it!(Talk not enough)(Step = 39)I believe that God is the ultimateguide and helper in all this. I know that you aregoing through a lot right now and I know youwill find a way to stay strong and stay positive.(Step = 78) I believe that God is the ultimateguide and helper in all this. I know that you aregoing through a lot of hard times right now, butI know you will find comfort in knowing that Heis there for you and your family.",
  "Case-I: How much to talk": "To perceive the effect of ORL, illustrateshow the responses differ on varied timesteps oftraining. It is clear that our model gradually ac-quires the optimal amount of information it shouldprovide in response to different dialogue states. Inthe upper case, we notice that our model overloadsthe seeker by providing redundant and unhelpfulinformation. Such a policy may lead to ironic andharmful reading from the perspective of potentialusers. In the lower case, the model initially pro-vides an inadequate contribution to push forwardthe conversation. Through ORL, our model gradu-ally learns not to talk too much\" and not to savetoo many words.\"",
  "How recipients effort matters to speaker": "in Appendix E provides two more case stud-ies. In the first case, the w/o Effort model, withouteffort-wise reinforcement, generates an unneces-sary and ambiguous response by saying, \"I havehad exes do that to me\". The reference to that\"here is uncertain, and the information about manyexes\" is potentially harmful. In the same vein, thew/o Effort model generates necessary and obscureinformation by saying ...has been making ... talkabout it\". In comparison, the other two variants,in both two cases, generate supportive and clearresponses, which indicates a satisfactory balancebetween effect and effort.",
  "Conclusion": "The current work represents a pioneering effort thatintegrates the cognitive relevance theory in the fieldof systems of emotional support conversation. Ourresults demonstrate the efficacy of the effect-effort\"trade-off in boosting the general quality, especiallyin terms of coherence, human likeness, and harm-lessness, as well as the helpfulness of emotionalsupport agents. Specifically, the joint optimizationof effect and effort provides a reliable frameworkto customize conversation agents to human tastein a non-toxic, human-like, and helpful manner.Our work also highlights the importance of inte-grating multi-level actions with human subjectivejudgment. The empirical findings recommend fu-ture studies to explore various human judgmentand processing effort norms to build a human-like,helpful, and harmless conversation agent.",
  "Limitations": "We outline two major limitations of this work asbelow.Firstly, the feedback model and simulated userunavoidably introduce bias to the reinforcementprocedure. These pre-trained proxy models cannot completely imitate human mental states andbehaviors in real-world communications. Futurework may consider using real-person feedback toreinforce the ESC agent in an interactive environ-ment.Secondly, we notice that the distribution ofground-truth feedback is subject to a long-tail dis-tribution biased towards the highest scale. Dueto such a distribution bias, we recommend futurework to rescale the human feedback to refine re-ward design.",
  "Ethical Considerations": "Our experiments utilize the ESConv dataset, whichis a publicly accessible benchmark explicitly cre-ated for emotional support conversations. Thisdataset is devoid of any sensitive or personal data,and it excludes any unethical language. The partici-pants privacy was fully safeguarded. Our researchcenters on developing a conversational system thatdelivers emotional support tailored to everyday sit-uations, as the ESConv dataset indicates. It isimportant to note that our dialogue system doesnot purport to have the ability to address or en-hance outcomes in high-risk, non-daily scenarios,such as discussions related to self-harm or suicide. We acknowledge the crucial role of professionalpsychological counseling or treatment in handlingsuch critical situations. We ensured the anonymityand confidentiality of participants feedback data.No personally identifiable information was usedin training the feedback model or the simulateduser. We will not make our feedback model andsimulated user accessible for business or other non-academic use. Satanjeev Banerjee and Alon Lavie. 2005. Meteor: Anautomatic metric for mt evaluation with improved cor-relation with human judgments. In Proceedings ofthe acl workshop on intrinsic and extrinsic evaluationmeasures for machine translation and/or summariza-tion, pages 6572. Yi Cheng, Wenge Liu, Wenjie Li, Jiashuo Wang, RuihuiZhao, Bang Liu, Xiaodan Liang, and Yefeng Zheng.2022. Improving multi-turn emotional support dia-logue generation with lookahead strategy planning.arXiv preprint arXiv:2210.04242. Yi Cheng, Wenge Liu, Jian Wang, Chak Tou Leong,Yi Ouyang, Wenjie Li, Xian Wu, and Yefeng Zheng.2024. Cooper: Coordinating specialized agents to-wards a complex dialogue goal. In Proceedings ofthe AAAI Conference on Artificial Intelligence, vol-ume 38, pages 1785317861.",
  "Siyang Liu, Chujie Zheng, Orianna Demasi, SahandSabour, Yu Li, Zhou Yu, Yong Jiang, and MinlieHuang. 2021.Towards emotional support dialogsystems. arXiv preprint arXiv:2106.01144": "Ted Moskovitz, Aaditya K Singh, DJ Strouse, TuomasSandholm, Ruslan Salakhutdinov, Anca D Dragan,and Stephen McAleer. 2023. Confronting rewardmodel overoptimization with constrained rlhf. arXivpreprint arXiv:2310.04373. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalu-ation of machine translation. In Proceedings of the40th annual meeting of the Association for Computa-tional Linguistics, pages 311318. Wei Peng, Yue Hu, Luxi Xing, Yuqiang Xie, Yajing Sun,and Yunpeng Li. 2022. Control globally, understandlocally: A global-to-local hierarchical graph networkfor emotional support conversation. arXiv preprintarXiv:2204.12749.",
  "Wei Peng, Ziyuan Qin, Yue Hu, Yuqiang Xie, and Yun-peng Li. 2023. Fado: Feedback-aware double con-trolling network for emotional support conversation.Knowledge-Based Systems, 264:110340": "Mahdin Rohmatillah and Jen-Tzung Chien. 2023. Hi-erarchical reinforcement learning with guidance formulti-domain dialogue policy. IEEE/ACM Transac-tions on Audio, Speech, and Language Processing,31:748761. Abdelrhman Saleh, Natasha Jaques, Asma Ghandehar-ioun, Judy Shen, and Rosalind Picard. 2020. Hi-erarchical reinforcement learning for open-domaindialog. In Proceedings of the AAAI conference onartificial intelligence, volume 34, pages 87418748.",
  "Robert van Rooy. 2001. Relevance of communicativeacts. In Proceedings of the 8th Conference on Theo-retical Aspects of Rationality and Knowledge, pages8396": "Haoyu Wang, Guozheng Ma, Cong Yu, Ning Gui, LinruiZhang, Zhiqi Huang, Suwei Ma, Yongzhe Chang, SenZhang, Li Shen, et al. 2023. Are large language mod-els really robust to word-level perturbations? arXivpreprint arXiv:2309.11166. Jianhong Wang, Yuan Zhang, Tae-Kyun Kim, and Yun-jie Gu. 2020. Modelling hierarchical structure be-tween dialogue policy and natural language genera-tor with option framework for task-oriented dialoguesystem. arXiv preprint arXiv:2006.06814.",
  "Weixiang Zhao, Yanyan Zhao, Shilong Wang, and BingQin. 2023. Transesc: Smoothing emotional supportconversation via turn-level state transition. arXivpreprint arXiv:2305.03296": "Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua,Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, QinLiu, Yuhao Zhou, et al. 2023.Secrets of rlhf inlarge language models part i: Ppo. arXiv preprintarXiv:2307.04964. Jinfeng Zhou, Zhuang Chen, Bo Wang, and MinlieHuang. 2023. Facilitating multi-turn emotional sup-port conversation with positive emotion elicitation:A reinforcement learning approach. arXiv preprintarXiv:2307.07994.",
  "B.2Consistency between utterance-level andword-level policy": "To facilitate multi-level policy learning, we lever-age consistency learning to improve the consistencybetween utterance-level policy head and text gener-ation head. To quantify the similarity of utterance-level policy between each pair of instances i andj, we concat the one-hot vectir of a and e sam-pled from the annotated dataset as the utterancefeature v, and calculate the inter-instance cosinesimilarity as Svij = (vi, vj). To quantify the simi-larity of text generationpolicy, we extract the rep-resentation of < eos > from the decoder blockas d and calculate the inter-instance cosine sim-ilarity as Sdij = (di, dj). After the supervisedfine-tuning of each batch, we further calculate thedissimilarity score Lcons = Ni,j=1 = Svij Sdij",
  "Below corresponds with the second case study": "Seeker: Actually, in the past few months, she has kindafreaked out every time I tried to get her phone for her. as ifshe didnt want me to see something.Supporter: Im sorry that this is stressful for you. In my past,I have had exes do that to me. most likely she was cheatingon you and this was the reason for her motivesSeeker: I cant believe she would do something like that... Ireally need to confront her about it. I cant have this doubton my chest.w/o Effort: I think that you should confront her about it. Ihave had exes do that to me, and I had to tell them that I wasnot interested in them. I think that they were just trying toget attention and that they didnt want to change.feat. DiagGPT: I think that you should confront her about it.I think that it would be a good idea to do that.feat. Llama: I think that you should confront her about it. Ithink that it would be a good idea to talk to her about this.Gold: I would definitely confront her about it. I wouldn t getangry or fight about it. Maybe she if she has a new boyfriendalready. That is a really good indicator. Seeker: i feel lost. Im so used to her being around every day.Now I dont know what to do without her.Supporter: I understand, I was with my wife for 4 yearsbefore she up and left without saying a word. That can be ahard thing to go through, especially this time of year.Seeker: not only that, but with covid going on i have noone ican see to talk about it.w/o Effort: i m sorry to hear that. COVID has been makingit hard for a lot of people to talk about it. Ive been throughthe same thing myself.feat. DiagGPT: Im sorry to hear that.Covid has been reallyhard on everyone. Ive been through a lot of this myself.Have you tried talking to your friends about it?feat. Llama: Im sorry to hear that. Covid has been reallyhard on you. Have you tried talking to your friends about it?Gold: Do you have any family? : Two case studies indicate a mastery of opti-mal relevance after full OLR. Training helpful chatbotswithout punishing user processing load jeopardizes thecomprehensibility and harmlessness of generated re-sponses."
}