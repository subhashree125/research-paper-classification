{
  "Abstract": "Large Vision-Language Model (LVLM) has en-hanced the performance of various downstreamtasks in visual-language understanding. Mostexisting approaches encode images and videosinto separate feature spaces, which are then fedas inputs to large language models. However,due to the lack of unified tokenization for im-ages and videos, namely misalignment beforeprojection, it becomes challenging for a LargeLanguage Model (LLM) to learn multi-modalinteractions from several poor projection layers.In this work, we unify visual representationinto the language feature space to advance thefoundational LLM towards a unified LVLM.As a result, we establish a simple but robustLVLM baseline, Video-LLaVA, which learnsfrom a mixed dataset of images and videos,mutually enhancing each other. As a result,Video-LLaVA outperforms Video-ChatGPT by5.8%, 9.9%, 18.6%, and 10.1% on MSRVTT,MSVD, TGIF, and ActivityNet, respectively.Additionally, our Video-LLaVA also achievessuperior performances on a broad range of 9image benchmarks. Notably, extensive experi-ments demonstrate that Video-LLaVA mutuallybenefits images and videos within a unified vi-sual representation, outperforming models de-signed specifically for images or videos. Weaim for this work to provide modest insightsinto the multi-modal inputs for the LLM.",
  "Introduction": "Recently, LLMs have gained rapid popularity in theAI community, such as GPT-3.5, GPT-4 (OpenAI,2023), PaLM (Bi et al., 2020; Anil et al., 2023),and BLOOM (Scao et al., 2022). They rely ontheir powerful language comprehension abilitiesto follow human-provided instructions and pro-vide corresponding responses. Typically, LLMscan only respond within the text input providedby the user, which is insufficient because humaninteraction with the world involves multiple chan-nels, such as visual and textual. To this end, recent LLM",
  "Macaw-LLM / X-LLM": ": Comparing Different LVLM Paradigms.Video-LLaVA aligns images and videos before projec-tion, allowing LLM to learn from a unified visual rep-resentation and endowing LLM with the ability to com-prehend both images and videos simultaneously. works (Ye et al., 2023; Zhu et al., 2023b; Alayracet al., 2022) have mapped images into text-like to-kens, enabling LLMs to emerge with the ability tocomprehend images. Despite their effectiveness,empowering LLMs to understand videos is morechallenging than image-only comprehension tasks.Nevertheless, recent work (Maaz et al., 2023; Liet al., 2023c; Zhang et al., 2023a) has made initialstrides in enabling interactions between video andlanguage.However, most current LVLMs (Li et al., 2023b; Dai et al., 2023; Luo et al., 2023; Li et al., 2023a;Yin et al., 2023; Fu et al., 2023) can primar-ily handle a single visual modality, either image-language or video-language.We compare dif-ferent LVLM paradigms as shown in ,where VideoChat (Li et al., 2023c) and Video-LLaMA (Zhang et al., 2023a) utilize a share visualencoder to handle both images and videos. How-ever, due to the inherent differences in the mediatypes of images and videos, it is challenging tolearn a unified representation, and the performancefalls significantly behind that of the specializedvideo expert model, Video-ChatGPT. Therefore, X-LLM (Chen et al., 2023) and Macaw-LLM (Lyuet al., 2023) allocate a modality-specific encoderfor each modality, attempting to enable a LLM tocomprehend images or videos through several pro-jection layers. But their performances are inferiorto dedicated video expert models such as Video-ChatGPT (Maaz et al., 2023). We attribute thisphenomenon to the lack of alignment before pro-jection. Because image features and video featuresreside in their own spaces, this poses a challenge fora LLM to learn their interactions from several poorprojection layers. Some similar phenomenon suchas alignment before fusion has been discussed byALBEF (Li et al., 2021) and ViLT (Kim et al., 2021)in multi-model models. More recently, ImageBind-LLM (Han et al., 2023) focuses on enabling theLLM to simultaneously process multiple modal in-puts by pre-aligning each modality to a commonfeature space (Girdhar et al., 2023). Based on alarge image-language model, ImageBind-LLM con-verts other modalities into the most similar imagefeatures by retrieving from a training-free imagecached database. However, the indirect alignmentapproach of ImageBind-LLM may lead to perfor-mance degradation, and the LLM has no knowledgeof actual video data.In this work, we introduce Video-LLaVA, a sim-ple but powerful baseline for the LVLM simulta-neously handling both images and videos. Specifi-cally, As shown in , Video-LLaVA initiallyaligns the representations of images and videos toa unified visual feature space. Since the visual rep-resentations are already aligned prior to projection,we employ a shared projection layer to map the uni-fied visual representation for the LLM. To enhancecomputational efficiency, Video-LLaVA undergoesjoint training of images and videos, achieving re-markable results with 1 training epoch.As a result, The proposed Video-LLaVA greatlyenhances the ability of the LLM to simultaneouslyunderstand both images and videos. For imageunderstanding, Video-LLaVA surpasses advancedLVLMs such as mPLUG-owl-7B and InstructBLIP-7B in 5 image benchmarks. Additionally, utilizing4 benchmark toolkits for a more comprehensiveevaluation, Video-LLaVA-7B even outperformsIDEFICS-80B by 6.4% in MMBench. Moreover,similar trends can be observed in video under-standing, where Video-LLaVA surpasses Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% re-spectively on the MSVD, MSRVTT, TGIF, andActivityNet video question-answering datasets. Ex- tensive ablation experiments demonstrate that align-ment before projection yields greater benefits. Ad-ditionally, joint training of images and videos canfacilitate a unified visual representation in LLMcomprehension.We summarize our primary contributions as fol-lows: We introduce Video-LLaVA, a powerfulLVLM baseline. During the training process,Video-LLaVA binds visual signals to the lan-guage feature space, unifying visual represen-tations, and proposes a solution to align beforeprojection. We enable an LLM to perform vi-sual reasoning capabilities on both images andvideos simultaneously. Extensive experiments demonstrate that a uni-fied visual representation benefits LLMs inlearning to simultaneously handle both im-ages and videos, validating the complemen-tarity of modalities, showcasing significantsuperiority when compared to models specifi-cally designed for either images or videos.",
  "Large Language Models": "When the well-known commercial model Chat-GPT (OpenAI, 2023) was introduced, the The AIcommunity released open-source Large LanguageModels (LLMs) by instruction tuning and increas-ing model sizes.These include LLaMA (Tou-vron et al., 2023a), Vicuna (Chiang et al., 2023),Alpaca (Taori et al., 2023), and more recently,LLaMA 2 (Touvron et al., 2023b). These modelsare tuned with instruction sets to emulate conversa-tions between humans and AI assistants. Further-more, InstructGPT (Ouyang et al., 2022) is trainedbased on GPT-3 (Brown et al., 2020) with 175 bil-lion parameters through aligning with human pref-erences. However, LLMs can only interact withintext. In this work, we introduce Video-LLaVA,which builds upon the powerful reasoning capa-bilities of LLM to extend modality interactions toimages and videos.",
  "LLMs as scheduler": "In the scheduler-based methods, various visualmodels are treated as plug-and-play modules. LLMschedules them according to the specific visual taskrequirements, like the assembly of building blocks.Some of these methods focus on images, such asVisualChatGPT (Wu et al., 2023) and Hugging-GPT (Shen et al., 2023), while MM-REACT (Yanget al., 2023) and ViperGPT (Surs et al., 2023) canalso handle videos. A key characteristic of thesescheduler-based LVLMs is that they do not requireend-to-end training, hence eliminating the need forpre-alignment and joint training of each modality.",
  "LLMs as decoder": "Regarding the approach of treating LLM as a de-coder, this is our primary focus. MiniGPT-4 (Zhuet al., 2023b) aligns image tokens to the input ofthe large language model through several linearprojection layers. However, this alignment is weakand lacks feedback from human instructions. Sub-sequently, mPLUG-Owl (Ye et al., 2023) adoptsa two-stage training approach. In the first stage,images are aligned with language using an auto-regressive pretraining style, and the second stageinvolves instruction tuning through using a humaninstruction dataset. With the increasing scale oflarge language model backends, approaches suchas InstructBLIP (Dai et al., 2023) and LLaVA se-ries (Liu et al., 2023b,a; Lin et al., 2024) collectethe larger human instruction datasets to train alarger LVLMs (13B parameters). Each answer of instruction datasets strictly follow to the given in-structions. Then they undergo end-to-end train-ing using human instruction datasets, enabling theLLM with visual reasoning capabilities. Moreover,Video-ChatGPT (Maaz et al., 2023) design a 100kvideo instruction dataset, successfully empower-ing LLMs to comprehend videos. VideoChat (Liet al., 2023c) and Video-LLaMA (Zhang et al.,2023a) achieve this by conducting joint training,allowing LLMs to simultaneously handle imagesand videos. Expanding LLMs to additional visualmodalities typically requires pre-alignment, as seenin LLaMA-Adapter (Zhang et al., 2023b; Gao et al.,2023) and ImageBind-LLM (Han et al., 2023).They bind other modalities to the image spacethrough ImageBinds (Girdhar et al., 2023) modal-ity encoder. These models have demonstrated that aunified feature space is advantageous for enhancingLLMs multi-modal reasoning capabilities. Distin-guished from prior work, Video-LLaVA not onlypre-aligns image and video features but also con-ducts joint training of images and videos, facil-itating LLMs in learning multi-modal reasoningcapabilities from a unified visual representation.",
  "As shown in , Video-LLaVA consists ofLanguageBind encoders fV (Zhu et al., 2023a) toextract features from the raw visual signal (im-": "ages or videos), a large language model fL suchas Vicuna, visual projection layers fP and a wordembedding layer fT. We initially obtain visualfeatures using LanguageBind encoders. Language-Bind encoders are capable of mapping differentmodalities into the textual feature space, therebyproviding us with a unified visual representation.Subsequently, the unified visual representation isencoded by shared projection layers, which is thencombined with tokenized textual queries and fedinto a large language model to generate correspond-ing responses.",
  "United Visual Representation": "Our goal is to map images and videos into a sharedfeature space to enable the large language modelto learn from a unified visual representation. Weassume that the same information can be conveyedthrough multiple media. For example, a runningdog can be expressed through language, a imageor a video simultaneously. Therefore, we can com-press information from different modalities into acommon feature space, allowing the model to ex-tract information from a dense feature space, facili-tating modality interactions and complementarity.Hence, we chose the modality encoders from Lan-guageBind (Zhu et al., 2023a), which align imagesand videos with the textual feature space.",
  "Alignment Before Projection": "To validate the performance degradation caused byseparated visual representation, we conduct exper-iments to to explore the performance of the LLMlearning from different visual representations. Wedefine the use of LanguageBind image encoder asunified visual representation while the MAE en-coder (He et al., 2022) use separated visual rep-resentation, which is a well-known and effectiveimage feature extractor. Additionally, since MAEdo not interact with multi-modal inputs during thetraining process, we utilize CLIP-L/14, a model ofthe same size. While CLIP-L/14 exhibits strongmultimodal understanding capabilities, it is not pre-aligned with the video encoder. Consequently, thisresults in a lack of uniformity in the visual featuresprovided to LLM. We only replace the image en-coder of the same scale and keep the LanguageBindvideo encoder.",
  "Understanding Training": "At this stage, the model is required to acquire theability to interpret visual signals within an exten-sive image/video-text pair dataset. Each visualsignal corresponds to a single round of conversa-tion data (Xq, Xa), where XT = Xq and Xa isthe ground truth. The training objective of thisstage is the original auto-regressive loss, where themodel learns the basic ability to view the vision.We freeze the other parameters of the model duringthis process.",
  "Instruction Tuning": "In this stage, the model is required to provideresponses corresponding to different instructions.These instructions often involve more complexvisual comprehension tasks, rather than just de-scribing visual signals. Note that the conversationdataX1q, X1a, , XNq , XNaconsists of multiplerounds. XrT = X1q,r = 1Concat(Xr1q, Xr1A, Xrq),r > 1(3)where r represents the round number. As shown inEquation 3, when r > 1 we concatenate the conver-sations from all previous rounds with the currentinstruction as the input for this round. The train-ing objective remains the same as in the previousstage. After this stage, the model learns to generatecorresponding responses based on different instruc-tions and requests. The LLM are also involved intraining at this stage.",
  "(a) Illustration of Video-LLaVA(b) Performance comparison": ": Training framework and performance. Video-LLaVA exhibits remarkable interactive capabilitiesbetween images and videos, despite the absence of image-video pairs in the dataset. (a) The Video-LLaVAframework demonstrates a data flow that generates corresponding responses based on input instructions. (b) Video-LLaVA achieves superior performances on a broad range of 15 datasets across image and video. LAION-CC-SBU 558k Valley702k LLaVA-mixed 665k Video-ChatGPT 100k",
  "Data Details": "In 3, for the first stage of understanding pretrain-ing, we use a subset of 558K LAION-CC-SBUimage-text pairs with BLIP (Li et al., 2022) cap-tions, which is sourced from CC3M (Sharma et al.,2018) and filtered by LLaVA (Liu et al., 2023b).The video-text pairs are derived from a subset pro-vided by Valley (Luo et al., 2023), and we haveaccess to 702k out of a total of 703k pairs, originat-ing from WebVid (Bain et al., 2021). For the stageof instruction tuning, We gathered instructionaldatasets from two sources, including a 665k image-text instruction dataset from LLaVA 1.5 (Liu et al.,2023a) and a 100k video-text instruction datasetfrom Video-ChatGPT (Maaz et al., 2023).",
  "Training Details": "In the training process, we resize and crop eachimage, resulting in a size of 224224 for each pro-cessed image. We uniformly sample 8 frames fromeach video, and each frame undergoes image pre-processing. The data in each batch is a randomcombination of images and videos. In the firststage, we train for one epoch with a batch size of256, using the AdamW optimizer with a cosinelearning rate schedule. In the second stage, we re-duce the batch size to 128. The initial learning ratefor both stages is set to 1e-3, with a warmup ratioof 0.03. Additional hyper-parameter settings canbe found in the appendix.",
  "Video-LLaVA7B70.73.959.23.570.04.045.33.3": ": Comparison between different LVLMs on image understanding benchmarks. Res., L, Vrespectively represent the input image resolution, LLaMA (Touvron et al., 2023a) and Vicuna (Chiang et al., 2023).Benchmark names are abbreviated due to page limitations. VQA-v2 (Goyal et al., 2017); GQA (Hudson andManning, 2019); VisWiz (Gurari et al., 2018); SQAI: ScienceQA-IMG (Lu et al., 2022); VQAT: TextVQA (Singhet al., 2019); POPE (Li et al., 2023d); MMB: MMBench (Liu et al., 2023c); LLaVAW: LLaVA-Bench (In-the-Wild) (Liu et al., 2023b); MM-Vet (Yu et al., 2023). donates that we reproduce LLaVA-1.5 with LanguageBind-Image encoder to compare fairly. donates that there is some overlap in the training data.",
  "Video-LLaVA V-7B22474.760.348.166.451.884.460.973.132.0": "pabilities of large video-language models on fourdatasets, including MSVD-QA (Chen and Dolan,2011), MSRVTT-QA (Xu et al., 2016), TGIF-QA (Jang et al., 2017) and ActivityNet-QA (Yuet al., 2019). The evaluation pipeline for videounderstanding follows Video-ChatGPT. We reportthe accuracy and score, which is assessed usingGPT-Assistant.Video-LLaVA consistently out-performs Video-ChatGPT in terms of question-answering accuracy, which is an advanced largevideo-language model. Moreover, Video-LLaVAsurpasses the powerful baseline of Video-ChatGPTby 5.8%, 9.9%, 18.6%, and 10.1% on MSRVTT,MSVD, TGIF, and ActivityNet, respectively. Addi-tionally, we conduct comparisons with the recent SOTA model, Chat-UniVi (Jin et al., 2023). De-spite Chat-UniVi utilizing more datasets such asMIMIC-IT (Li et al., 2023a), Video-LLaVA stilldemonstrate competitive results, surpassing Chat-UniVi on MSVD, MSRVTT, and TGIF datasets.In summary, these results validate Video-LLaVAsability to comprehend videos and provide contex-tually appropriate responses based on instructions.",
  "Zero-shot Image Question-answering": "As shown in , we evaluate our approachfor image understanding on five academic imagequestion-answering benchmarks.Compared tothe state-of-the-art model InstructBLIP-7B, Video-LLaVA demonstrates powerful image understand- : Zero-shot object hallucination evaluation results are reported for three POPE evaluation settings. Yesindicates the proportion of positive responses to the given question. donates that we reproduce LLaVA-1.5 withLanguageBind-Image encoder to compare fairly.",
  "Video-LLaVA V-7B81.680.845.885.384.042.186.285.242.0": "ing capabilities, outperforming across all fivequestion-answering benchmarks.Additionally,Video-LLaVA exhibits competitive results com-pared to several more powerful LVLMs, whichare tuned based on 13B or 65B LLM, such as sur-passing InstructBLIP-13B by 14.7% on VisWiz,highlighting its strong understanding ability in nat-ural visual environments. Furthermore, to ensure afair comparison, we replace the image encoder inLLaVA-1.5 with the LanguageBind-Image encoder,called LLaVA-1.5. This demonstrates that the per-formance improvement observed in Video-LLaVAis not solely attributed to a stronger image encoder.Additional details can be found in .3.6. Evaluation under Image Benchmark ToolkitsAdditionally, we evaluate LVLMs using severalbenchmark toolkits for visual instruction tuning.These benchmark toolkits provide a detailed as-sessment of the models capabilities through ro-bust evaluation metrics. Video-LLaVA outperformInstructBLIP-7B by 24.9%, 12.2%, and 5.8% onMMBench, LLaVA-Bench, and MM-Vet, respec-tively. It is worth noting that Video-LLaVA-7Bstill demonstrates advanced performance comparedto larger LLM models, surpassing InstructBLIP-13B by 6.4% on MM-Vet and IDEFICS-80B (Lau-renon et al., 2023) by 6.4% on MMBench. Theseresults demonstrate that Video-LLaVA exhibits astrong understanding of semantic aspects of scenes,enabling it to answer open-ended and free-formnatural language questions about images.",
  "As shown in , we report evaluation resultsfor zero-shot object hallucinations, utilizing a eval-uation pipeline derived from a polling-based querymethod (Li et al., 2023d). Video-LLaVA demon-": "strates competitive performance across three sub-sets: random, popular, and adversarial. Specifically,when compared to the 7B foundation model, Video-LLaVA consistently outperforms MM-GPT (Gonget al., 2023) across all three POPE hallucinationevaluation subsets.Furthermore, when bench-marked against the larger 13B LLM, Video-LLaVAeven surpasses Mini-GPT4 comprehensively. Thesuccessful performance of Video-LLaVA in objecthallucination detection validates the consistencybetween unified visual representations and the gen-eration of textual descriptions.",
  ": Effect of alignment before projection onvideo. We validate and report the accuracy and score onfour video question-answering datasets": "no longer unified during LLMs initial learning ofvisual representations. In , compared toseparated visual representation, the united visualrepresentation significantly improves performanceacross 4 video question-answering datasets. Sepa-rated visual representations not only exhibit loweraccuracy in question-answering, but also demon-strate a similar trend in answer scores. These re-sults demonstrate that the unified visual representa-tion can help the LLM further learn and understandvideos. 4.3.3For Image UnderstandingThe unified visual representation demonstratesstrong performance, surpassing the separated vi-sual representation comprehensively across 5 im-age question-answering datasets and 4 benchmarktoolkits in . Additionally, we observe asignificant margin of performance improvementin the unified visual representation on the MM- VQA V2 GQA VisWiz SQA-IMG TextVQA POPE MMBench LLaVA-Bench MM-Vet",
  ": Effect of joint training on image. donatesthat We reproduce the results of LLaVA-1.5 at a resolu-tion of 224224 with LanguageBind-Image encoder fora fair comparison": "Bench, LLaVA-Bench, and MM-Vet benchmarktoolkits. This highlights that the unified visual rep-resentation not only enhances performance in im-age question-answering but also provides benefitsin other aspects of image understanding, such asreducing object hallucination and improving OCRcapabilities. 4.3.4Joint TrainingThis subsection aims to validate the complemen-tarity of images and videos during joint training,which can mutually enhance the LLMs understand-ing of images and videos based on a unified visualrepresentation.",
  "For comparing performance on video benchmarks,we remove image data during the training of Video-LLaVA, which is called Video-LLaVA. We com-": "pare with Video-LLaVA to assess the performancegains from joint image training on video bench-marks.In , we evaluate our model onfour video question-answering datasets.Com-pared to Video-LLaVA without image in training,the model trained with joint images and videosachieves comprehensive improvements across allfour video datasets.These results demonstratethat joint training of images and videos facilitatesLLMs understanding of visual representations.",
  "For Image Understanding": "When comparing performance on image bench-marks, it is challenging to find a image-basedLVLM with the same configuration as Video-LLaVA. To address this, we replace the image en-coder in LLaVA-1.5 with the LanguageBind-Imageencoder and reproduce the results at a resolutionof 224224 by using the same training configura-tion, called LLaVA-1.5. As shown in ,Compared to LLaVA-1.5, which utilizes the sameimage encoder configuration, we observe perfor-mance improvements in 8 out of 9 benchmarks,demonstrating mutual improvement in visual under-standing. Video-LLaVA outperform LLaVA-1.5 in POPE, indicating that joint training with videosalleviates the object hallucination in images. Thesimilar trend is observed on some other benchmarktoolkits, such as LLaVA-Bench and MMBench,where video data significantly improves LLMsperformance in complex reasoning and image con-versation tasks.",
  "Limitation": "While Video-LLaVA exhibits strong competitive-ness in both images and videos, we still observedsome limitations of Video-LLaVA. To begin with,Video-LLaVA performs moderately in understand-ing long videos. In , Chat-UniVi surpasses0.5 on ActivityNet-QA because Video-LLaVA onlyutilizes uniformly sampled 8 frames to comprehendthe video, which results in the loss of detailed in-formation from long videos. Additionally, trainingVideo-LLaVA is computationally expensive, requir-ing 3-4 days to complete the training process on 8A100-80G GPUs.",
  "Conclusion": "In this work, we introduce Video-LLaVA, a simplebut powerful large visual-language baseline model.We propose a novel framework to address the is-sue of misalignment before projection, utilizing aLanguageBind encoder to pre-bind visual signalsinto the language feature space. To enable a LLMto comprehend both images and videos simulta-neously, we conduct joint training on images andvideos, allowing the LLM to learn multi-modalinteractions from a unified visual representation.Extensive experiments demonstrate that joint train-ing on images and videos mutually benefits per-formance. Furthermore, we validate that aligningvisual representations before projection aids LLMlearning. Remarkably, LLM, after learning from aunified visual representation, exhibits the remark-able ability to simultaneously engage with bothimages and videos, showcasing a powerful com-prehension of unified visual concepts. These re-sults collectively demonstrate the effectiveness ofthe Video-LLaVA training framework. As a uni-fied visual training framework, the performance ofVideo-LLaVA even surpasses that of expert modelsdesigned specifically for images or videos.",
  "This work was supported in part by the Natu-ral Science Foundation of China (No. 62202014,62332002, 62425101), Shenzhen Basic ResearchProgram (No.JCYJ20220813151736001)": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,Antoine Miech, Iain Barr, Yana Hasson, KarelLenc, Arthur Mensch, Katherine Millican, MalcolmReynolds, et al. 2022. Flamingo: a visual languagemodel for few-shot learning. Advances in NeuralInformation Processing Systems, 35:2371623736. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-son, Dmitry Lepikhin, Alexandre Passos, SiamakShakeri, Emanuel Taropa, Paige Bailey, ZhifengChen, et al. 2023. Palm 2 technical report. arXivpreprint arXiv:2305.10403. Max Bain, Arsha Nagrani, Gl Varol, and Andrew Zis-serman. 2021. Frozen in time: A joint video andimage encoder for end-to-end retrieval. In Proceed-ings of the IEEE/CVF International Conference onComputer Vision, pages 17281738. Bin Bi, Chenliang Li, Chen Wu, Ming Yan, Wei Wang,Songfang Huang, Fei Huang, and Luo Si. 2020.Palm: Pre-training an autoencoding&autoregressivelanguage model for context-conditioned generation.arXiv preprint arXiv:2004.07159. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. David Chen and William B Dolan. 2011. Collectinghighly parallel data for paraphrase evaluation. InProceedings of the 49th annual meeting of the associ-ation for computational linguistics: human languagetechnologies, pages 190200. Feilong Chen, Minglun Han, Haozhi Zhao, QingyangZhang, Jing Shi, Shuang Xu, and Bo Xu. 2023. X-llm: Bootstrapping advanced large language modelsby treating multi-modalities as foreign languages.arXiv preprint arXiv:2305.04160. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.2023. Vicuna: An open-source chatbot impressinggpt-4 with 90%* chatgpt quality. See org (accessed 14 April 2023). Wenliang Dai, Junnan Li, Dongxu Li, AnthonyMeng Huat Tiong, Junqi Zhao, Weisheng Wang,BoyangLi,PascaleFung,andStevenHoi.2023. Instructblip: Towards general-purpose vision-language models with instruction tuning. Preprint,arXiv:2305.06500. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng,Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji.2023. Mme: A comprehensive evaluation bench-mark for multimodal large language models. arXivpreprint arXiv:2306.13394. Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, ShijieGeng, Aojun Zhou, Wei Zhang, Pan Lu, ConghuiHe, Xiangyu Yue, et al. 2023. Llama-adapter v2:Parameter-efficient visual instruction model. arXivpreprint arXiv:2304.15010. Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Man-nat Singh, Kalyan Vasudev Alwala, Armand Joulin,and Ishan Misra. 2023.Imagebind: One embed-ding space to bind them all. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 1518015190. Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang,Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang,Ping Luo, and Kai Chen. 2023. Multimodal-gpt: Avision and language model for dialogue with humans.arXiv preprint arXiv:2305.04790. Yash Goyal, Tejas Khot, Douglas Summers-Stay, DhruvBatra, and Devi Parikh. 2017. Making the v in vqamatter: Elevating the role of image understandingin visual question answering. In Proceedings of theIEEE conference on computer vision and patternrecognition, pages 69046913. Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo,Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey PBigham. 2018. Vizwiz grand challenge: Answeringvisual questions from blind people. In Proceedings ofthe IEEE conference on computer vision and patternrecognition, pages 36083617. Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao,Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu,Song Wen, Ziyu Guo, et al. 2023. Imagebind-llm:Multi-modality instruction tuning. arXiv preprintarXiv:2309.03905. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Pi-otr Dollr, and Ross Girshick. 2022. Masked autoen-coders are scalable vision learners. In Proceedingsof the IEEE/CVF conference on computer vision andpattern recognition, pages 1600016009.",
  "Dan Hendrycks and Kevin Gimpel. 2016.Gaus-sian error linear units (gelus).arXiv preprintarXiv:1606.08415": "Drew A Hudson and Christopher D Manning. 2019.Gqa: A new dataset for real-world visual reasoningand compositional question answering. In Proceed-ings of the IEEE/CVF conference on computer visionand pattern recognition, pages 67006709. Gabriel Ilharco, Mitchell Wortsman, Ross Wightman,Cade Gordon, Nicholas Carlini, Rohan Taori, AchalDave, Vaishaal Shankar, Hongseok Namkoong, JohnMiller, Hannaneh Hajishirzi, Ali Farhadi, and Lud-wig Schmidt. 2021. Openclip. If you use this soft-ware, please cite it as below. Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim,and Gunhee Kim. 2017. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. InProceedings of the IEEE conference on computervision and pattern recognition, pages 27582766. Peng Jin, Ryuichi Takanobu, Caiwan Zhang, XiaochunCao, and Li Yuan. 2023. Chat-univi: Unified vi-sual representation empowers large language modelswith image and video understanding. arXiv preprintarXiv:2311.08046.",
  "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.2023b. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large lan-guage models. arXiv preprint arXiv:2301.12597": "Junnan Li, Dongxu Li, Caiming Xiong, and StevenHoi. 2022. Blip: Bootstrapping language-image pre-training for unified vision-language understandingand generation. In International Conference on Ma-chine Learning, pages 1288812900. PMLR. Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,Shafiq Joty, Caiming Xiong, and Steven Chu HongHoi. 2021. Align before fuse: Vision and languagerepresentation learning with momentum distillation.Advances in neural information processing systems,34:96949705.",
  "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong JaeLee. 2023b. Visual instruction tuning. arXiv preprintarXiv:2304.08485": "Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,Songyang Zhang, Wangbo Zhao, Yike Yuan, JiaqiWang, Conghui He, Ziwei Liu, et al. 2023c. Mm-bench: Is your multi-modal model an all-aroundplayer? arXiv preprint arXiv:2307.06281. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, PeterClark, and Ashwin Kalyan. 2022. Learn to explain:Multimodal reasoning via thought chains for sciencequestion answering. Advances in Neural InformationProcessing Systems, 35:25072521. Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong,Minghui Qiu, Pengcheng Lu, Tao Wang, andZhongyu Wei. 2023. Valley: Video assistant withlarge language model enhanced ability.arXivpreprint arXiv:2306.07207. Chenyang Lyu, Minghao Wu, Longyue Wang, XintingHuang, Bingshuai Liu, Zefeng Du, Shuming Shi,and Zhaopeng Tu. 2023. Macaw-llm: Multi-modallanguage modeling with image, audio, video, andtext integration. arXiv preprint arXiv:2306.09093.",
  "OpenAI. 2023.Gpt-4 technical report.Preprint,arXiv:2303.08774": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al.2022. Training language models to follow instruc-tions with human feedback.Advances in NeuralInformation Processing Systems, 35:2773027744. Teven Le Scao, Angela Fan, Christopher Akiki, El-lie Pavlick, Suzana Ilic, Daniel Hesslow, RomanCastagn, Alexandra Sasha Luccioni, Franois Yvon,Matthias Gall, et al. 2022.Bloom:A 176b-parameter open-access multilingual language model.arXiv preprint arXiv:2211.05100. Piyush Sharma, Nan Ding, Sebastian Goodman, andRadu Soricut. 2018. Conceptual captions: A cleaned,hypernymed, image alt-text dataset for automatic im-age captioning. In Proceedings of the 56th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 25562565.",
  "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, YannDubois, Xuechen Li, Carlos Guestrin, Percy Liang,and Tatsunori B Hashimoto. 2023. Stanford alpaca:An instruction-following llama model": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, et al. 2023a.Llama:Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023b.Llama 2: Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288.",
  "Chenfei Wu,Shengming Yin,Weizhen Qi,Xi-aodong Wang, Zecheng Tang, and Nan Duan.2023. Visual chatgpt: Talking, drawing and edit-ing with visual foundation models. arXiv preprintarXiv:2303.04671": "Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr-vtt: A large video description dataset for bridgingvideo and language. In Proceedings of the IEEE con-ference on computer vision and pattern recognition,pages 52885296. Zhengyuan Yang, Linjie Li, Jianfeng Wang, KevinLin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,Ce Liu, Michael Zeng, and Lijuan Wang. 2023. Mm-react: Prompting chatgpt for multimodal reasoningand action. arXiv preprint arXiv:2303.11381. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye,Ming Yan, Yiyang Zhou, Junyang Wang, An-wen Hu, Pengcheng Shi, Yaya Shi, et al. 2023.mplug-owl: Modularization empowers large lan-guage models with multimodality. arXiv preprintarXiv:2304.14178.",
  "Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, XingSun, Tong Xu, and Enhong Chen. 2023. A survey onmultimodal large language models. arXiv preprintarXiv:2306.13549": "Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,Kevin Lin, Zicheng Liu, Xinchao Wang, and LijuanWang. 2023. Mm-vet: Evaluating large multimodalmodels for integrated capabilities. arXiv preprintarXiv:2308.02490. Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yuet-ing Zhuang, and Dacheng Tao. 2019. Activitynet-qa:A dataset for understanding complex web videos viaquestion answering. In Proceedings of the AAAI Con-ference on Artificial Intelligence, volume 33, pages91279134.",
  "Hang Zhang, Xin Li, and Lidong Bing. 2023a. Video-llama: An instruction-tuned audio-visual languagemodel for video understanding.arXiv preprintarXiv:2306.02858": "Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, andYu Qiao. 2023b. Llama-adapter: Efficient fine-tuningof language models with zero-init attention. arXivpreprint arXiv:2303.16199. Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui,HongFa Wang, Yatian Pang, Wenhao Jiang, JunwuZhang, Zongwei Li, et al. 2023a. Languagebind: Ex-tending video-language pretraining to n-modality bylanguage-based semantic alignment. arXiv preprintarXiv:2310.01852.",
  "[Input Instruction] What activities are the woman and the dog engaged inthe video?": "The video shows a person cooking a dish of noodles. They start by boiling water in a pot and adding noodles to it. Then, they add oil and some ingredients to the pot. After that, they mix the noodles and the ingredients together. Finally, they serve the dish on a plate.",
  "[Input Instruction] What is unusual about this video?": "Yes, the baby is playing with a toy truck and a toy car, which could pose a safety risk if they are not supervised. However, the baby is sitting on the floor, so there is less risk of injury compared to if they were playing with the toys on a higher surface.",
  "[Input Instruction] Can you identify any safety hazards in this video?": "Sure, here's a short poem inspired by the video: The flowers bloom, a gentle grace, Their petals soft, their colors trace. Their beauty, a sight to behold, A moment of peace, a moment to hold. Their colors, a symphony of hues, A celebration of life, and all it can do. Their beauty, a reminder to cherish, The simple things, that bring us happiness. So let us take a moment to appreciate, The beauty of flowers, and all they can create.",
  "[Input Instruction] Can you write a short poem inspired from t": "Once upon a time, in a kingdom far away, there lived a beautiful princess named Rose.She was known for her kindness and beauty, and everyone loved her. One day, ahandsome prince named Leo came to visit the kingdom. He was fascinated by Rose'sbeauty and decided to stay in the kingdom. However, Rose's jealous sister, Lily, did notlike the attention Leo was giving her sister. She tried to harm Rose, but Leo saved herand they fell in love. The kingdom celebrated their love and they lived happily ever after.The moral of the story is that true love conquers all and that jealousy can lead to harmfulconsequences.",
  "[Input Instruction] Create a short fairy tale with a moral lesson inspiredby the video": "The video starts with a car covered in snow, and a person is seen usingremove the snow from the car's windows. The person is seen scraping windshield and the side windows of the car. The person continues to scthe car, making sure that all the snow is removed. The video then showwiping the snow off the car's windshield and side windows. The personcloth to wipe the snow off the car's windows, making sure that the car clean. The video ends with the person finishing the job and the car beinOverall, the video shows a person removing snow from a car's windowand then wiping the snow off the car's windows to make sure it is comp",
  "[Input Instruction] Describe the video in detail": "hows a person preparing a dish with peppers and oranges. They slice the oranges and lay them out on a table. The person then proceeds to cut the oranges into strips and lays them out on a piece of wax paper. They use a the peppers and oranges into strips and lay them out on a piece of wax paper. hen uses a spoon to mix the peppers and oranges together. The video ends son cutting the peppers and oranges into strips and laying them out on a piece r.",
  "[Input Instruction] Describe the video": "The main focus of the video is the process of creating a painting of a tree using various techniques and colors. The video shows the artist's hand holding a paintbrush and using it to create the tree, as well as the use of different colors and techniques to create the painting.",
  "[Input Instruction] What does the woman use to split the logs and how doesshe do it?": "In the video, we see a man walking down a track while holding a javelidown the track and throws the javelin. The video then shows the man wthe starting point, and we see the javelin landing on the ground. The maaway from the camera. From the video, we can see that the man is a javhe is practicing his throws. The track appears to be well-maintained, ansurroundings seem to be quiet and peaceful. The man's form and techniimpressive, and it is clear that he has experience in the sport. Overall, tha glimpse into the world of javelin throwing and the dedication and skiin the sport.",
  "on] Can you identify any safety hazards in this video?": "Sure, here's a short poem inspired by the video: The flowers bloom, a gentle grace, Their petals soft, their colors trace. Their beauty, a sight to behold, A moment of peace, a moment to hold. Their colors, a symphony of hues, A celebration of life, and all it can do. Their beauty, a reminder to cherish, The simple things, that bring us happiness. So let us take a moment to appreciate, The beauty of flowers, and all they can create.",
  "[Input Instruction] Can you write a short poem inspired from the video": "a time, in a kingdom far away, there lived a beautiful princess named Rose.own for her kindness and beauty, and everyone loved her. One day, arince named Leo came to visit the kingdom. He was fascinated by Rose'sdecided to stay in the kingdom. However, Rose's jealous sister, Lily, did notntion Leo was giving her sister. She tried to harm Rose, but Leo saved herin love. The kingdom celebrated their love and they lived happily ever after.f the story is that true love conquers all and that jealousy can lead to harmfuls.",
  "ion] Create a short fairy tale with a moral lesson inspired": "The video starts with a car covered in snow, and a person is seen using a scraper to remove the snow from the car's windows. The person is seen scraping the snow off the windshield and the side windows of the car. The person continues to scrape the snow off the car, making sure that all the snow is removed. The video then shows the person wiping the snow off the car's windshield and side windows. The person is seen using a cloth to wipe the snow off the car's windows, making sure that the car is completely clean. The video ends with the person finishing the job and the car being shown off. Overall, the video shows a person removing snow from a car's windows and windshield, and then wiping the snow off the car's windows to make sure it is completely clean.",
  "ion] What does the woman use to split the logs and how does": "In the video, we see a man walking down a track while holding a javelin. He then runs down the track and throws the javelin. The video then shows the man walking back to the starting point, and we see the javelin landing on the ground. The man then walks away from the camera. From the video, we can see that the man is a javelin thrower, and he is practicing his throws. The track appears to be well-maintained, and the surroundings seem to be quiet and peaceful. The man's form and technique are impressive, and it is clear that he has experience in the sport. Overall, the video provides a glimpse into the world of javelin throwing and the dedication and skill required to excel in the sport."
}