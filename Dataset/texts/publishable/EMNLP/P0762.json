{
  "Abstract": "Historically, sign language machine transla-tion has been posed as a sentence-level task:datasets consisting of continuous narrativesare chopped up and presented to the model asisolated clips. In this work, we explore thelimitations of this task framing. First, we sur-vey a number of linguistic phenomena in signlanguages that depend on discourse-level con-text.Then as a case study, we perform therst human baseline for sign language trans-lation that actually substitutes a human intothe machine learning task framing, rather thanprovide the human with the entire documentas context. This human baselinefor ASL toEnglish translation on the How2Sign datasetshows that for 33% of sentences in our sample,our uent Deaf signer annotators were onlyable to understand key parts of the clip in lightof additional discourse-level context. These re-sults underscore the importance of understand-ing and sanity checking examples when adapt-ing machine learning to new domains.",
  "Introduction": "One of the key challenges in sign language pro-cessing is that methods from mainstream naturallanguage processing (NLP) are tailored primarilyto text and secondarily to speech. Much of thework in this space therefore focuses on general-izing these methods to video, in order to capturethis oft-neglected dimension of linguistic diversity(Bragg et al., 2019; Yin et al., 2021).One such carryover is that sign language ma-chine translation (MT) is framed as a sentence-leveltask. Although continuous sign language datasetsare usually derived from long-form signed con-tent (e.g., interpreted news broadcasts), they arepreprocessed into short clips associated with eachsentence in the spoken language transcript (which",
  "* Correspondence to . Work done while at Google": "may not themselves correspond to discrete sen-tences in the continuously translated sign languageversion), and models are trained and evaluated onthese clips in isolation. In this work, we examinethe limitations of this task framing, whichlikemany other sign language modeling decisions (De-sai et al., 2024)was adopted somewhat uncriti-cally, and ask: what is the right unit of translationfor sign language?Machine translation between spoken languagesis typically posed as a sentence-level task, and al-though it largely works, there are known intersen-tential dependencies like anaphora that are impos-sible to resolve in isolation (Bawden et al., 2018;Voita et al., 2019). These dependencies are es-pecially troublesome for language pairs that havemismatches in grammatical features like pronoundropping, tense marking, or gradations of register.The situation is perhaps even more pronouncedfor translation between spoken languages and signlanguages. Sign languages are not just spoken lan-guages produced with the hands: the grammar ofsign languages is shaped by the nature of the visual-spatial modality (Meier et al., 2002). While utter-ances produced by non-native signers tend to re-semble the syntax of the regions spoken language,native signing often expresses concepts in a funda-mentally different way that is richly grounded inspatial world understanding and, more importantlyhere, the discourse context. When deprived of thatcontext, the viewer may catastrophically fail to un-derstand the meaning of an utterance and thereforebe unable to translate it. We describe some linguis-tic phenomena relevant to cross-modal translationin .To the best of our knowledge, no sign languageMT benchmarks provide baselines for human per-formance that actually ask humans to perform thesame task that they expect of the model. Referencetranslations are given in the dataset by construction,either as the source text or by discourse-level trans- lation. Human judgments are used at the discourselevel to quality-check preprocessing or to evaluatemodel-generated outputs, but not to sanity checkthe task framing itself.We therefore provide in the rst suchhuman baseline, for American Sign Language(ASL) to English translation on the How2Signdataset (Duarte et al., 2021), as a case study.How2Sign consists of informal instructional (howto) narratives, which is a particularly illustrativedomain. Before even scoring results against groundtruth references, we nd that for 33.3% of instancesin our sample, our uent Deaf signer annotatorsfelt that they could not fully perform the transla-tion given only the sentence-level clipbut could,given additional discourse-level context. Most ofthese errors were due to features of sign languagesthat lack direct analogues in spoken languages.When we do compute metrics, we get a surprisinglylow score of 19.8 BLEU (56.6 BLEURT) for thesentence-level task, which increases with additionalcontext but only to 21.5 (59.5). We disaggregatethese results for each of ve distinct interpreters inthe How2Sign test set, and nd that sentence-levelresults vary from 5.2 BLEU (45.7 BLEURT) to39.5 (70.0) across individuals. Scores are higherfor interpreters who stay closer to English; contextis more important for those who dont.We hope that these results and analysis will en-courage the sign language MT eld to reconsiderwhether computational benets of the sentence-level task framing outweigh its quality and align-ment limitations, and to continue to pare backunfounded modeling assumptions by understand-ing datasets more deeply and crafting benchmarksmore deliberately.",
  "Sign Languages": "See Bragg et al. (2019), Yin et al. (2021), Costeret al. (2023), and Desai et al. (2024) for excellentsurveys of the social and technical aspects of signlanguage processing.In brief, in contrast to spoken languages, whichare articulated with the vocal tract, sign languagesare articulated with the upper body (including theface). These two modalities impose different con-straints on the grammar of languages within them.Sign languages are minority languages primarilyused by the Deaf/Hard of Hearing communities ofvarious regions; they are natural languages that are genealogically unrelated to but often considerablyinuenced by the dominant spoken language of theregion. Within a single sign language, there is agreat deal of variation due to geographic and socialfactors.For example, in the US and Canada there is adiglossic spectrum from American Sign Language(ASL), a fully-edged independent language; toManually Coded English (MCE), a system used totranscribe spoken English into the sign lexicon ofASL; with Conceptually Accurate Signed English(CASE) vaguely in between (Supalla and McKee,2002; Rendel et al., 2018). Across all of these,there is regional variation in vocabulary, analo-gous to soda vs. pop in American English butperhaps more pronounced (Shroyer and Shroyer,1984). And there is social variation, like BlackASL, analogous to Black English (McCaskill et al.,2011). Less than 6% of deaf children in the USand less than 2% of deaf children worldwide areexposed to a sign language in early childhood (Mur-ray et al., 2019), so there are also different levels ofprociency even among Deaf signers. MT shouldhandle all these dimensions of variation.",
  "Sign Language Translation": "Because the full task involving video to text trans-lation was unapproachable at the time, early workon sign language translation focused on generationcascaded through glosses, which are nonstandard-ized linguistic annotations representing signs. Thisallowed the task to be formulated as a special caseof (sentence-level) text-to-text translation and reusemethods from mainstream MT (Chapman, 1997;Veale et al., 1998; Zhao et al., 2000).Unlike MT for written languages, translationfrom sign language glosses as a source represen-tation is not immediately useful, because signersin general do not use themonly linguists andto some extent students do. Therefore the otherhalf of the cascaded sign language understandingpipeline is sign language recognition (SLR), thetask of predicting glosses from videos of peoplesigning. Isolated SLR classies a single gloss froma short clip (Charayaphan and Marble, 1992; Jozeand Koller, 2019; Li et al., 2020; Desai et al., 2023;Starner et al., 2024), and continuous SLR predictsa sequence of glosses from a clip of an entire sen-tence (Koller et al., 2015; Cui et al., 2017). Thissentence granularity is inherited from translationabove and by analogy to automatic speech recog-nition (ASR), but is not especially harmful here: context is not strictly necessary because the task isto transcribe form, not understand meaning.The modern framing for end-to-end video-to-text sign language MT originates in Camgoz et al.(2018). The paper does not phrase the sentence-level framing as an explicit decision point but ratherinherits it again from mainstream machine trans-lation and continuous SLR. Because videos (andmore generally, long sequences) are computation-ally difcult to work with/learn from, there is alsoan unstated pressure to use shorter clips. The pro-vided dataset, RWTH-PHOENIX-Weather 2014T,is constructed on top of an existing (sentence-level)continuous SLR dataset (Koller et al., 2015) ofweather forecasts interpreted into German SignLanguage. There is no human baseline providedfor the task, but if there were, it would likely beuneventful due to the datasets limited domain andnon-native interpreters.As subsequent datasets have expanded into moresign languages and broader domains (and de-emphasized glosses, because they are a lossy bot-tleneck with limited availability), the datasets haveretained the sentence-level framingdespite be-ing constructed from long video corpora, wherefull discourse context is available and where thereis not necessarily a sentence-level correspondencebetween the speech and sign tracks. Human anno-tations have been used to preprocess/quality checkthe dataset (Camgoz et al., 2021; Albanie et al.,2021; Shi et al., 2022; Joshi et al., 2023; Shen et al.,2023; Uthus et al., 2023) or evaluate model out-puts (Mller et al., 2022, 2023; Duarte et al., 2021),but not to explore the sentence-level framing itself.See Appendix A for a dataset-by-dataset analysis.While surveying gloss-based translation meth-ods, Mller et al. (2022) note that only sentence-level systems had been studied at the time, and theygive spatial indexing as one example of a grammat-ical feature that may be truncated in sentence-levelsystems. We are aware of only one work that hasstudied sign language translation beyond the sen-tence level since then, Sincan et al. (2023). Theirwork examines the empirical gains from provid-ing models with prior text contexteither full sen-tences or sign spottingswithout specic sign lin-guistic motivation. Quality improves signicantlybut is still extremely low in absolute, so it is pos-sible that the context is being used as a shortcutrather than an essential part of the task framing.Our work is complementary in that we analyze awide variety of linguistic phenomena, and study a",
  "Document-Level Translation": "While much work on machine translation has fo-cused on (and has been very successful within)the sentence-level task framing, there is a growingbody of work that highlights the aspects that arelost between sentences. Automatic reference-basedmetrics are relatively insensitive to discourse-levelproblems that stand out to human raters (Hard-meier, 2012; Lubli et al., 2018), such as is-sues with lexical consistency, formality, and gen-der/number agreement (Voita et al., 2019; Fernan-des et al., 2023). Therefore several works createcontrastive test sets where several candidate trans-lations are ranked with respect to each other, ratherthan translations being generated from a blank slate,to measure these properties (Bawden et al., 2018;Mller et al., 2019; Nagata and Morishita, 2020).These works mostly evaluate model outputs ratherthan ideal (human) performance, but, e.g., Mat-suzaki et al. (2015) provides a human baseline forEnglishJapanese translation of short dialogues,in which the rate of correct translations is 18 per-centage points higher given full document contextvs. only an isolated sentence. We extend thisline of work to sign languages by surveying extralinguistic phenomena related to the visual-spatialmodality, then evaluate the empirical importance ofdiscourse-level effects in this domain using a com-bination of automatic metrics and human ratings inthe ideal (human) setting.Historically,thebottleneckfortrainingdocument-level MT was the availability ofdocument-level parallel corpora (Voita et al., 2019;Al Ghussin et al., 2023); only a small fractionof translation data was natively document-level,such as video content with subtitles in multiplelanguages (Lison et al., 2018; Duh, 2018).1 Thesituation is markedly different for sign languages:virtually all sign language corpora are nativelydiscourse-level(withminorexceptionslikeSP-10 (Yin et al., 2022) and WMT-SLT Signsu-isse (Mller et al., 2023), which contain dictionaryexample sentences), but they are preprocessed intoisolated clips. Why not use this extra structure?",
  "THATCAUSEPRESSURECL-[move joystick]WILLCL-[wing flap moves]": ": Example of the interaction between classiers and long-range context. It isnt clear in isolation thatthe st moving back and forth represents a st controlling a joystick, or that the arm represents the planes wingand the hand represents a ap (aileron) on the wing. Interpreters head omitted here for privacy.",
  "Long-Range Linguistic Dependencies": "In this section, we outline a number of long-rangedependencies in the grammar of sign languages, pri-marily ASL, which may be truncated with sentence-level clipping. These features are not necessarilyuniversal to all sign languages, but they are rela-tively common insofar as they are motivated by thevisual-spatial modality (Meier et al., 2002; Aronoffet al., 2005).We create example gures using clips from theHow2Sign dataset (Duarte et al., 2021); we omitthe signers faces in the gures for privacy but notethat facial expressions and mouthing are importantin sign language.",
  "Spatial Referencing": "Perhaps the most salient feature that distinguishessign languages from spoken languages is the abilityto use space in a way that is grammatically struc-tured (as opposed to in co-speech gesture) (Em-morey, 1996). PronounsWhereas spoken languages use third-person pro-nouns to refer to entities that were previously intro-duced in the discourse, sign languages use spatialindexing, i.e., they establish that a locus in spacerefers to a particular entity and then reference thatentity by pointing (Emmorey, 1996; Liddell, 2003).Because spoken languages tend to have a small setof third-person pronouns, they become ambiguousas the number of entities under discussion grows.But the number of unambiguous referents in signlanguages may grow as space and memory permit,especially when using more complex forms of ref-erence than pointing (Ferrara et al., 2023).So it may be the case that a spatial index in a signlanguage should be translated into a named entityin a spoken language (rather than a pronoun), orvice versabut without context, its impossible toknow what name corresponds to that spatial index,or where that named entity lies in space. This islike a more severe version of translation between",
  "languages that have gendered vs. ungendered (oromissible) pronouns (Frank et al., 2004; Savoldiet al., 2021)": "Directional VerbsSome verbs in sign languages are directional, i.e.,their movement is inected to agree with the spatialloci of their arguments (Liddell, 1990). This isanalogous to polypersonal agreement in spokenlanguages (verb agreement with respect to multiplearguments), but more exible (and more context-dependent) for the same reason as pronouns above. ClassiersIn certain spoken languages, the term classiersrefers to words that agree with nouns of differentsemantic categories, and are often obligatory whencounting nouns with numerals (Allan, 1977). Insign languages, classiers are more expansive: likewith spoken classiers, different handshapes rep-resent different categories of objects, but they canalso be inected in classier predicates, where thelocation and movement of the classier take on anextremely exible, iconic predicative meaning (Fr-ishberg, 1975; Liddell, 1980). A classic exampleis the 3 handshape in ASL (extended thumb, in-dex, and middle nger) oriented with the thumb up,which represents a number of vehicles, especiallycars. The classier can be repeated across spaceto describe a packed parking lot, swerved side toside to depict a car driving down a winding road,crumpled to represent a car crash, etc.Because classiers can refer to many objects in aparticular category, and the referent needs only beclear from context (either explicitly introduced orjust implied by the situation), the subject or entiremeaning of a classier predicate may not be clearin isolation. For example, in it is onlyclear from context that the classiers are referringto a joystick & wing aps in an airplane.",
  "Role ShiftWhen describing interactions between two or morecharacters, signers will often role shift, i.e., they": "physically embody and act out the different char-acters (Padden, 1986). This is analogous to quotesin spoken languages, except that turn-taking isnot marked explicitly with words like he said:instead, its marked by shifting the bodys an-gle/position and demeanor. In sentence-level clips,it may not be clear who is referenced by each roleor even that role shift is being used at allbecauseeach turn in the role shift is considered its ownsentence and clipped in isolation.",
  "Out-of-Vocabulary Terms": "With languages in the same modality, it is usu-ally straightforward to translate out-of-vocabularyterms like proper nouns by copying them directlyfrom the source into the target context (perhapswith some phonological tweaks and transliteration,complicated somewhat by acronyms). But this strat-egy breaks down across modalities.Because spoken languages are socially dominantover sign languages, virtually every sign languagecan productively borrow terms from spoken lan-guages, through ngerspelling (spelling the wordwith a manual alphabet) or mouthing (silently say-ing the word while producing a related sign). Butthe reverse isnt true: spoken languages have nomechanism for borrowing signs. Context is impor-tant for strategies that reconcile this mismatch.",
  "Abbreviated Fingerspelling": "When introducing a ngerspelled term for the rsttime in a discourse, signers will spell it clearly tomake sure that it can be understood. But whenreturning to that term later, they may speed throughit amorphously to save time, with the understandingthat the viewer can recognize the shape of the wordin context. For example, in the letters ofthe word basil are ngerspelled simultaneouslyand out of order. This is described as careful vs.rapid ngerspelling in the literature (Patrie andJohnson, 2011; Thumann, 2012; Wager, 2012).2 If the signer anticipates that they will refer to theterm repeatedly, especially for proper nouns, theymay even declare a temporary acronym upfrontand use it for the remainder of the discourse. Forexample, in an instance from the human baselinethe trading card Whalebone Glider is abbrevi-ated WG after its rst mention. Absent context,it is difcult or impossible for someone viewing",
  "L/S": ": Example of the interaction between rapidngerspelling and long-range context. Top is the rstbasil in the narrative (itself spelled slightly out of or-der), and bottom is the version from the test sentence:highly coarticulated, with multiple letters produced si-multaneously. The labels indicate the relevant lettersgiven the ground truth, but without context other letterssuch as Y, X, and T could be perceived. sentence-level clips to know what these abbrevi-ated terms refer to, and copying the abbreviationsdirectly would be unnatural in the target spoken lan-guage. The other translation direction is perhapsless problematic, because one could guess whethera proper noun is being used for the rst time basedon local cues and translate appropriately.",
  "Name Signs": "In American Deaf culture, in addition to their fulllegal names, signers use sign names given to themby other members of the Deaf community. If theirname is short enough, a persons sign name may bea ngerspelled version of their rst name, but oth-erwise it is an idiosyncratic sign based on factorslike their personality, appearance, and interests;name signs are perhaps even more idiosyncraticthan names in spoken languages (Supalla, 1992).When talking to an unfamiliar audience, a signerwill often ngerspell a persons name and give theirname sign, then refer to them using their name signfor the rest of the discourse. Training on isolatedclips that include name signs will encourage themodel to hallucinate. Challenges with name signsare not necessarily universal across sign languages;for example, in Japanese Sign Language, namesigns are often a function of the kanji in a signerslegal name (Nonaka et al., 2015), and thereforecould more easily be translated without context.",
  "certain academic elds (McKee and Vale, 2017).3": "When introducing a nonstandard or niche sign, thesigner will often ngerspell it to ensure that it isunderstood by a less familiar audience. When trans-lating from a sign language into a spoken language,like with name signs the model may be able toguess the meaning but is generally encouraged tohallucinate. When translating from a spoken lan-guage into a sign language, if the model knows mul-tiple nonstandard signs it is unclear how it couldcoordinate their usage across independently trans-lated sentences, as seen with lexical cohesion issuesin MT for written languages (Voita et al., 2019).",
  "Generic Context Dependence": "In addition to the aforementioned features specicto sign languages and the visual-spatial modality,sign languages can be context-dependent in similarways to spoken languages. For example, in termsof grammar: ASL can drop pronouns (Lillo-Martin,1986) and has a variety of strategies for expressingtense (Jacobowitz and Stokoe, 1988) and denite-ness/indeniteness (Irani, 2019). In terms of vocab-ulary: lexical signs can be ambiguous or dialectal,making them harder to understand without context.",
  "Case Study": "In order to explore how these phenomena surface inreal sign language translation datasets, we performa human baseline for ASL to English translationon How2Sign (Duarte et al., 2021) across differ-ent amounts of provided context. To the best ofour knowledge, this is the rst time human perfor-mance has been measured for the sentence-levelsign language machine translation task.How2Sign was constructed by having 11signers5Hearing,4Deaf,and2Hardof Hearingwatch English-captioned instruc-tional how to videos from the earlier How2dataset (Sanabria et al., 2018) a rst time to under-stand the content, then a second time at 0.75x speedwhile performing a live interpretation. The captions(from the original speech track) were manually re-aligned to the signing, with an average sentenceduration of 8.67 seconds.",
  "si1:i, ti1: The previous and current sourceclip, plus the ground truth text for the previousclip.4": "si1:i, t0:i1: The previous and current sourceclip, plus the ground truth captions for theentire video up to this point.Note that each of these settings strictly expandsupon the prior one, so it is valid for a single annota-tor to perform all four in sequence. (Some of thesetranslations may be identical to those for prior set-tings, if the annotator does not want to adjust theirtranslation in light of new context.) However, itis not valid for an annotator to translate multipleclips i within a single video due to leakage. On topof these four translation settings, we also ask theannotators to describe how well they understoodthe sentence in isolation vs. after seeing additionalcontext, and to rate the naturalness of the signingon a scale from 0-2, where higher is more natural.5 To select our human baseline instances we startwith How2Signs test set, which consists of 184ASL translations of 149 How2 narratives, slicedinto 2,322 clips. We discard narratives that aretranslated multiple times by different signers (toavoid cross-instance leakage) and videos that seemgenerally malformed (e.g., large spans of the videolack captions or captions extend beyond the dura-tion of the video). For each remaining narrative,we sample a clip uniformly at random, excludingthe rst clip in each narrative because results forthe context settings would be trivial.6 Some clipswithin narratives are not contiguous because the 4Using the ground truth is slightly unrepresentative of whatis possible at test time; the ideal would have been to translateusing the entire source video up to this point as context, butevaluating this setting would have been prohibitively time-consuming. These settings that condition on previous captionsare more similar to how we expect machine learning prac-titioners to incorporate context in light of sequence lengthconstraints initially, like in Sincan et al. (2023).5Specically, they were asked to answer Is it naturalASL?, with 0=no, 1=eh, and 2=yes as the options.6This means that our metrics will slightly overestimate theeffect of context, because they ignore initial sentences that aremeant to be understood without context. signer made an error between sentences, whichbreaks the si1:i condition; we reject these casesand resample until success. The result is a set of102 test instances, at most one per narrative.Second, we describe the actual execution of thehuman baseline: Our annotators were the two mid-dle authors, who are Deaf signers who use bothASL and English as primary languages;7 the otherauthors set up the test instances. Each annotatorspent several hours performing the translations andratings for a random nonoverlapping split of thedata, leaving additional commentary as they wentfor use in our qualitative analysis. The annotatorswere allowed to slow down or repeat the video, butwere told not to agonize over it frame by frame.See Appendix B.1 for annotator instructions.",
  "Results": "FollowingpriorworksthatevaluateonHow2Sign (lvarez et al., 2022; Lin et al.,2023; Tarrs et al., 2023; Uthus et al., 2023),we report BLEU (Papineni et al., 2002) andBLEURT (Sellam et al., 2020) as our quantitativemetrics. We compute BLEU using SacreBLEU(Post, 2018) version 2 with all default options,and BLEURT using checkpoint BLEURT-20 (Puet al., 2021). See for scores, forratings, and Appendix B.2 for the complete set oftranslations comprising the human baseline. Effect of context.Human performance on thesentence-level translation task is 19.8 BLEU (56.6BLEURT) and increases monotonically with extracontext, but only up to 21.5 BLEU (59.5 BLEURT).This consistent but relatively small difference inautomatic metrics belies the annotators perceptionof the gap: for 33.3% of test instances, the anno-tators judged that they were unable to understandkey details of the signed content from the sentencein isolation which they later understood from addi-tional context (veried with their actual translations 7Note that these annotators are not professional transla-tors, which may harm the quality of the translated outputs(and automated metrics computed on them). However, theEnglish captions in How2Sign (originally from How2) are notespecially polished themselves, since they are transcriptionsof spontaneous speech with disuencies etc., so we expectthis to be less of an issue than if we were comparing to ref-erence translations by professional sign language interpretersof originally signed content. These annotators also know theresearch purpose (and could have inferred it from the sequenceof context settings, even if they hadnt had foreknowledge),which may bias the translations and ratings. We were moreconcerned with getting a good qualitative understanding ofthe data amongst the authors.",
  ": BLEU (top) and BLEURT (bottom) scores ()for the human baseline for ASL to English translation,across different amounts of provided context and differ-ent interpreters featured in the videos": "across settings compared to the ground truth).8 Ofthese failure cases, 47% featured classiers withunclear referents, 38% grammatical features likeprodrop/lack of overt tense markings, 26% rapidngerspelling, 9% acronyms, 6% ambiguous signs,and 6% dialectal sign variation.9 In addition to translations that improved givenpast context, there were several examples wherethe translation was incorrect across all settings be-cause future context was needed to understand thesentence. We did not anticipate this, so there wasno experimental setting to measure it. Variation across interpreters.We observe qual-itatively that there is enormous variation in signingstyle between the ve interpreters (which we labelA-E) featured in the test videos, across the spec-trum from ASL to CASE to MCE. It is hard todisentangle this from the shallow translations that 8These absolute scores are also relatively low compared totypical BLEU/BLEURT scores in MT generally, but this seemsto be a property of the round-trip EnglishASLEnglishconstruction of the references rather than an issue with thezero-shot predicted translations or metrics. For example, thesame kinds of ller words relevant to reduced BLEU in Tar-rs et al. (2023) are often omitted in zero-shot translationsfrom ASL to English, which creates supercial differenceswith the source reference but does not affect meaning.9We didnt come across any How2Sign instances of severallinguistic phenomena described in , for a variety ofpresumed reasons. Spatial indexing, directional verbs, androle shift are relevant when discussing third-person characters(especially multiple ones interacting), but How2Sign is largelyrst-person or second-person given the instructional narrativedomain. Name signs are generally only used in originallyproduced Deaf content. Nonstandard signs are used primarilyby domain experts, so they are unlikely to be introduced incontent translated from English without much preparation. are typical of live interpreting. Inspired by priorwork which disaggregates evals (Buolamwini andGebru, 2018; Raji and Buolamwini, 2019; Baro-cas et al., 2021; Kaplun et al., 2022), we thereforebreak down our results by interpreter.We nd that the human baseline metrics matchour subjective impressions: they vary from 5.2BLEU (45.7 BLEURT) for Interpreter A to 39.5(70.0) for Interpreter D. The interpreters with lowerscores perform deeper translation closer to ASL,and those with higher scores border on MCE(which inates n-gram overlap, because the task ap-proaches sign recognition rather than translation).The interpreters signing with more English inu-ence also tend to mouth more prominently, so some-times the translation is clear from lipreading evenwhen the signing itself is odd and hard to under-stand. The annotators rated the average naturalnessof the content at 1.05 on a scale from 0-2 (), rang-ing from 1.93 for Interpreter A to 0.64 for Inter-preter D; generally, the more natural the content,the worse the sentence-level translation metrics.10 When we look at the other three settings, we seethat context has a proportionally larger effect forinterpreters where the translation metrics were orig-inally lower (and naturalness is rated higher): Inter-preter A increases from 5.2 BLEU (45.7 BLEURT)to 6.3 (48.6) and Interpreter C from 7.4 (47.7) to8.7 (55.0), vs. Interpreter D from 39.5 (70.0) to41.1 (70.3). This bears out in the annotator ratingsas well: translation failed due to missing context73.3% of the time on Interpreter A and 44.0% ofthe time on Interpreter C, but only 13.6% of thetime on Interpreter D. This conrms our suspicionthat the effect of discourse context is obscured byevaluating on live (and especially hearing) inter-preters. Even though there is a clear improvementin metrics due to context, the average effect size isobscured by the fact that we are essentially evaluat-ing on multiple domains at once.",
  "Misalignment.Despite How2Signs use of man-ually realigned captions (and despite us having ex-": "10We emphasize that these naturalness judgments are sub-jective from the perspective of the annotators. This maybe biased by social factors like the perception that a hyper-correct pure form of ASL is the most prestigious, as op-posed to signing with more inuence from Englishor viceversa (Stokoe Jr, 1969; Vicars, 2023). Sign language transla-tion models should still understand this content (especially tothe extent that this reects real variation in how people sign,as opposed to performance effects of live interpreting), but itis important to know what we are actually evaluating so thatwe do not e.g. test on articially easy content and overstateperformance for actual Deaf signers.",
  "Interpreter A73.31.93Interpreter B14.31.00Interpreter C44.01.24Interpreter D13.60.64Interpreter E26.90.73": ": Annotator ratings for the human baseline% of instances where they failed to understand key de-tails from the sentence in isolation but later succeededwith context, and naturalness of the signed content ona scale from 0-2 ()broken down by interpreter. cluded apparently malformed videos earlier), 5% ofthe sentence-level clips in our baseline still do notcontain the relevant content. Even more clips lacksignicant parts of the ground truth translation orhave extra content beyond it. On top of this, the on-set of a sentence usually begins earlier on the facethan the hands, so with even with accurate clip-ping the sentence may either start with a leftoverhandshape from the previous sentence or truncatethe start of the sentence on the face. These all com-bine to make it difcult for annotators (or models)to know which parts of the input clip they shouldand shouldnt translate. In a discourse-level fram-ing, misalignment matters less because the offset isa smaller fraction of the overall contentor thereis no misalignment at all if the entire discourse isin the translation context.",
  "Conclusion": "In this paper, we argued that the costs of thesentence-level sign language MT task framing arehigher than many might expect, with many rele-vant discourse-level phenomena being related tothe visual-spatial modality and cross-modal trans-lation. We supported this with a case study: therst human baseline for sentence-level sign lan-guage MT, from ASL to English on the How2Signdataset. We found that discourse context was nec-essary to fully understand and translate a large frac-tion of sentences (33.3%), and this effect is itselfattenuated by the prevalence of signing data thatdoes not represent the more challenging aspectsof ASL due to its use of non-native or live inter-preters. We hope that this inspires more in-depthanalysis grounded in rsthand experience with signlanguages, to avoid perpetuating systemic bias inthe way we conceptualize sign language tasks (De-sai et al., 2024).",
  "We expect the aforementioned long-range de-pendencies to exist in other sign languages,because they are generally motivated by fea-tures of the visual-spatial modality": "We expect English to ASL translation (trans-lation from a spoken language into a signedlanguage) to suffer similar problems. Some-times, source sentences would not includeenough grounding to perform a natural trans-lation with classiers. And even when sourcesentences do include all necessary informationto perform a faithful translation, even a per-fect sentence-level translation model wouldresult in unnatural discourse-level translationswhen concatenating clips due to inconsistentuse of space and other discourse phenomenaacross sentences. Direct translation between two sign languagesmay be less problematic than translation be-tween a sign language and a spoken language,because similarities in use of space or classi-ers may allow for a shallower translation. Results from How2Sign may not be represen-tative of results on other domains. Informal in-structional narratives are relatively well-suitedto showing the inadequacies of sentence-leveltranslation, because they are grounded in a sin-gle scene for the duration of the narrative anduse relatively short sentences. However, theyare also light on description of multiple third-person entities interacting with each other,which use other context-dependent structuresdescribed above. We expect stories/ASL lit-erature to require more context, and contentwith stronger inuence from English (or the re-spective dominant spoken language for otherregions) to require less.",
  "The ethical considerations of this work are those forsign language processing as a whole. Namely, ma-": "chine understanding of sign languages would im-prove access to information, communication, andother technologies for underserved signing commu-nities. However, there is a risk thatrather thansupplement existing resources to strictly improveaccessentities who currently provide services insign languages might replace a high-quality so-lution that uses human interpreters with a lower-quality automated one. This work tries to exposedecits in the current task framing so that automaticsolutions will be less awed. Inclusion in modernNLP also brings with it a number of well-knownrisks (misinformation, bias, etc. at scale). Futureworks that release trained models should mitigatethese potential harms.",
  "We thank Chris Dyer, Manfred Georg, and anony-mous reviewers for giving feedback on drafts ofthis paper, as well as Caroline Pantofaru for institu-tional support": "Yusser Al Ghussin, Jingyi Zhang, and Josef van Gen-abith. 2023.Exploring paracrawl for document-level neural machine translation.In Proceedingsof the 17th Conference of the European Chapterof the Association for Computational Linguistics,pages 13041310, Dubrovnik, Croatia. Associationfor Computational Linguistics. Samuel Albanie, Gl Varol, Liliane Momeni, HannahBull, Triantafyllos Afouras, Himel Chowdhury, NeilFox, Bencie Woll, Rob Cooper, Andrew McParland,and Andrew Zisserman. 2021. Bbc-oxford britishsign language dataset. arXiv preprint.",
  "J. Keane J. Michaux D. Brentari G. ShakhnarovichB. Shi, A. Martinez Del Rio and K. Livescu. 2018.American sign language ngerspelling recognitionin the wild. SLT": "Solon Barocas, Anhong Guo, Ece Kamar, Jacque-lyn Krones, Meredith Ringel Morris, Jennifer Wort-man Vaughan, Duncan Wadsworth, and Hanna Wal-lach. 2021. Designing disaggregated evaluations ofai systems: Choices, considerations, and tradeoffs.Preprint, arXiv:2103.06076. Rachel Bawden, Rico Sennrich, Alexandra Birch, andBarry Haddow. 2018. Evaluating discourse phenom-ena in neural machine translation. In Proceedings ofthe 2018 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, Volume 1 (Long Pa-pers), pages 13041313, New Orleans, Louisiana.Association for Computational Linguistics. Danielle Bragg, Oscar Koller, Mary Bellard, Lar-wan Berke, Patrick Boudreault, Annelies Braffort,Naomi Caselli, Matt Huenerfauth, Hernisa Ka-corri, Tessa Verhoef, Christian Vogler, and Mered-ith Ringel Morris. 2019. Sign language recognition,generation, and translation: An interdisciplinary per-spective. In Proceedings of the 21st InternationalACM SIGACCESS Conference on Computers andAccessibility, ASSETS 19, page 1631, New York,NY, USA. Association for Computing Machinery. Joy Buolamwini and Timnit Gebru. 2018.Gendershades: Intersectional accuracy disparities in com-mercial gender classication.In Proceedings ofthe 1st Conference on Fairness, Accountability andTransparency, volume 81 of Proceedings of Ma-chine Learning Research, pages 7791. PMLR. Necati Cihan Camgoz, Simon Hadeld, Oscar Koller,Hermann Ney, and Richard Bowden. 2018.Neu-ral sign language translation. In Proceedings of theIEEE Conference on Computer Vision and PatternRecognition (CVPR).",
  "C. Charayaphan and A. E. Marble. 1992. Image pro-cessing system for interpreting motion in AmericanSign Language. J Biomed Eng, 14(5):419425": "Mathieu De Coster, Dimitar Shterionov, Mieke VanHerreweghe, and Joni Dambre. 2023.Machinetranslation from signed to spoken languages: stateof the art and challenges. Universal Access in theInformation Society. Runpeng Cui, Hu Liu, and Changshui Zhang. 2017.Recurrent convolutional neural networks for contin-uous sign language recognition by staged optimiza-tion.In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition (CVPR). Aashaka Desai, Lauren Berger, Fyodor O. Minakov,Vanessa Milan, Chinmay Singh, Kriston Pumphrey,Richard E. Ladner, Hal Daum III au2, Alex X.Lu, Naomi Caselli, and Danielle Bragg. 2023. Aslcitizen: A community-sourced dataset for advanc-ing isolated sign language recognition.Preprint,arXiv:2304.05934. Aashaka Desai, Maartje De Meulder, Julie A. Hochge-sang, Annemarie Kocab, and Alex X. Lu. 2024. Sys-temic biases in sign language ai research: A deaf-led call to reevaluate research agendas.Preprint,arXiv:2403.02563. Amanda Duarte,Shruti Palaskar,Lucas Ventura,Deepti Ghadiyaram,Kenneth DeHaan,FlorianMetze, Jordi Torres, and Xavier Giro-i Nieto. 2021.How2Sign: A Large-scale Multimodal Dataset forContinuous American Sign Language.In Confer-ence on Computer Vision and Pattern Recognition(CVPR).",
  "Gal Kaplun, Nikhil Ghosh, Saurabh Garg, Boaz Barak,and Preetum Nakkiran. 2022.Deconstructing dis-tributions:A pointwise framework of learning.Preprint, arXiv:2202.09931": "Oscar Koller, Jens Forster, and Hermann Ney. 2015.Continuous sign language recognition:Towardslarge vocabulary statistical recognition systems han-dling multiple signers. Computer Vision and ImageUnderstanding, 141:108125. Dongxu Li, Cristian Rodriguez, Xin Yu, and HongdongLi. 2020. Word-level deep sign language recogni-tion from video: A new large-scale dataset and meth-ods comparison. In The IEEE Winter Conference onApplications of Computer Vision, pages 14591469. Scott Liddell. 1990. Four functions of a locus: Reex-amining the structure of space in asl. In Ceil Lucas,editor, Sign Language Research: Theoretical Issues,pages 176198. Gallaudet University Press, Wash-ington D.C.",
  "Has machine translation achieved human parity?a case for document-level evaluation.Preprint,arXiv:1808.07048": "Takuya Matsuzaki, Akira Fujita, Naoya Todo, andNoriko H. Arai. 2015. Evaluating machine transla-tion systems with second language prociency tests.In Proceedings of the 53rd Annual Meeting of theAssociation for Computational Linguistics and the7th International Joint Conference on Natural Lan-guage Processing (Volume 2: Short Papers), pages145149, Beijing, China. Association for Computa-tional Linguistics.",
  "R.P. Meier, K. Cormier, and D. Quinto-Pozos, editors.2002. Modality and Structure in Signed and SpokenLanguages. Cambridge University Press": "MathiasMller,MaliheAlikhani,EleftheriosAvramidis, et al. 2023.Findings of the secondWMT shared task on sign language translation(WMT-SLT23).In Proceedings of the EighthConference on Machine Translation, pages 6894,Singapore.AssociationforComputationalLinguistics. Mathias Mller, Sarah Ebling, Eleftherios Avramidis,Alessia Battisti, Michle Berger, Richard Bowden,Annelies Braffort, Necati Cihan Camgz, CristinaEspaa-bonet, Roman Grundkiewicz, Zifan Jiang,Oscar Koller, Amit Moryossef, Regula Perrollaz,Sabine Reinhard, Annette Rios, Dimitar Shterionov,Sandra Sidler-miserez, and Katja Tissi. 2022. Find-ings of the rst WMT shared task on sign languagetranslation (WMT-SLT22).In Proceedings of theSeventh Conference on Machine Translation (WMT),pages 744772, Abu Dhabi, United Arab Emirates(Hybrid). Association for Computational Linguis-tics.",
  "Mathias Mller, Zifan Jiang, Amit Moryossef, AnnetteRios, and Sarah Ebling. 2022. Considerations formeaningful sign language machine translation basedon glosses. Preprint, arXiv:2211.15464": "Mathias Mller, Annette Rios, Elena Voita, and RicoSennrich. 2019. A large-scale test set for the evalu-ation of context-aware pronoun translation in neuralmachine translation. Preprint, arXiv:1810.02268. Masaaki Nagata and Makoto Morishita. 2020. A testset for discourse translation from Japanese to En-glish. In Proceedings of the Twelfth Language Re-sources and Evaluation Conference, pages 37043709, Marseille, France. European Language Re-sources Association.",
  "Signed names in japanese sign language: Linguis-tic and cultural analyses.Sign Language Studies,16(1):5785": "Carol Padden. 1986. Verbs and role-shifting in amer-ican sign language.In Proceedings of the fourthnational symposium on sign language research andteaching, volume 44, page 57. National Associationof the Deaf Silver Spring, MD. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic eval-uation of machine translation.In Proceedings ofthe 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311318, Philadelphia,Pennsylvania, USA. Association for ComputationalLinguistics.",
  "Amy Pu, Hyung Won Chung, Ankur P Parikh, Sebas-tian Gehrmann, and Thibault Sellam. 2021. Learn-ing compact metrics for mt.In Proceedings ofEMNLP": "Inioluwa Deborah Raji and Joy Buolamwini. 2019. Ac-tionable auditing: Investigating the impact of pub-licly naming biased performance results of com-mercial ai products.In Proceedings of the 2019AAAI/ACM Conference on AI, Ethics, and Society,AIES 19, page 429435, New York, NY, USA. As-sociation for Computing Machinery. Kabian Rendel, Jill Bargones, Britnee Blake, BarbaraLuetke, and Deborah S Stryker. 2018. Signing exactenglish; a simultaneously spoken and signed com-munication option in deaf education.Journal ofEarly Hearing Detection and Intervention, 3(2):1829.",
  "Ozge Mercanoglu Sincan, Necati Cihan Camgoz, andRichard Bowden. 2023.Is context all you need?scaling neural sign language translation to large do-mains of discourse. Preprint, arXiv:2308.09622": "Thad Starner,Sean Forbes,Matthew So,DavidMartin, Rohit Sridhar, Gururaj Deshpande, SamSepah, Sahir Shahryar, Khushi Bhardwaj, TylerKwok, Daksh Sehgal, Saad Hassan, Bill Neubauer,Soa Anandi Vempala, Alec Tan, Jocelyn Heath, Un-nathi Utpal Kumar, Priyanka Vijayaraghavan Mo-sur, Tavenner M. Hall, Rajandeep Singh, Christo-pher Zhang Cui, Glenn Cameron, Sohier Dane, andGarrett Tanzer. 2024. Popsign asl v1.0: an isolatedamerican sign language dataset collected via smart-phones.In Proceedings of the 37th InternationalConference on Neural Information Processing Sys-tems, NIPS 23, Red Hook, NY, USA. Curran Asso-ciates Inc.",
  "Deborah Stocks Wager. 2012. Fingerspelling in ameri-can sign language: A case study of styles and reduc-tion": "Longyue Wang, Chenyang Lyu, Tianbo Ji, ZhiruiZhang, Dian Yu, Shuming Shi, and Zhaopeng Tu.2023.Document-level machine translation withlarge language models. In Proceedings of the 2023Conference on Empirical Methods in Natural Lan-guage Processing, pages 1664616661, Singapore.Association for Computational Linguistics. Aoxiong Yin, Zhou Zhao, Weike Jin, Meng Zhang,Xingshan Zeng, and Xiaofei He. 2022. Mlslt: To-wards multilingual sign language translation.InProceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition (CVPR), pages51095119. Kayo Yin, Amit Moryossef, Julie Hochgesang, YoavGoldberg, and Malihe Alikhani. 2021.Includingsigned languages in natural language processing.In Proceedings of the 59th Annual Meeting of theAssociation for Computational Linguistics and the11th International Joint Conference on Natural Lan-guage Processing (Volume 1: Long Papers), pages73477360, Online. Association for ComputationalLinguistics. Liwei Zhao, Karin Kipper, William Schuler, ChristianVogler, Norman Badler, and Martha Palmer. 2000.A machine translation system from english to ameri-can sign language. In Envisioning Machine Transla-tion in the Information Future, pages 5467, Berlin,Heidelberg. Springer Berlin Heidelberg.",
  "The WMT-SLT datasets (Mller et al., 2022,": "2023) are built on several sources of newsbroadcasts in Swiss German Sign Language,some produced in DSGS and others inter-preted. Competition entries are rated by hu-mans, and the reference translations are scoredin the same human evaluation framework as abaseline, but human translation and refer-ence translation are treated interchangeably.WMT-SLT23 nds that the references in onetest set are rated worse than the others, andraises the possibility that this is related to dis-course context but does not explore it further.",
  "BOBSL (Albanie et al., 2021) is a datasetcomposed of BBC programs interpreted intoBritish Sign Language. Human annotators areused to evaluate preprocessing decisions andclean up the test set": "How2Sign (Duarte et al., 2021) is an Ameri-can Sign Language dataset containing studiotranslations of how to videos. Human anno-tations are used to align captions and evaluatethe intelligibility of skeletons vs. generatedvideos. OpenASL (Shi et al., 2022) is an AmericanSign Language dataset consisting of videosmined from several YouTube channels. Hu-man ratings are only used to evaluate how wellthe caption tracks attached to these videos arealigned to their content. ISLTranslate (Joshi et al., 2023) is built fromchildrens educational content produced in In-dian Sign Language. A signer performs a hu-man baseline given full discourse context tovalidate the quality of the reference captions,not to sanity check the task framing.",
  "YouTube-ASL (Uthus et al., 2023) is a corpusof captioned American Sign Language videosdrawn from YouTube. Human annotators areused only to lter out videos with low-qualitysigning or captions": "JWSign (Gueuwou et al., 2023) is a dataset ofBible translations into many sign languages.No human annotators were used when con-structing the dataset, since it is constructedfrom preexisting clean data. The ngerspelling recognition (not sign lan-guage translation) datasets ChicagoFSWild (B. Shiand Livescu, 2018) and ChicagoFSWild+ (B. Shiand Livescu, 2019), which consist of clips extractedfrom continuous signing data, do provide refer-ences for human performance within the clip-leveltask framing. They observe that the baseline scoresare lower than inter-annotator agreement betweenthe ground truth annotators (who had access tothe surrounding video), meaning that somethingis lost without context. This task has even lesscontext than sentence-level translation, and couldbe seen as a manifestation of rapid ngerspelling,described in .2. However, it is not clearwhether the ground truth annotators had access tocaptions, which could improve results beyond whatis actually possible given the entire video (but onlythe video) as context (like the si1:i, ti1 and si1:i,t0:i1 settings in our How2Sign human baseline).",
  ". Translate from the above clip, but also withthe ground truth English translation for theentire narrative up to that point as context": "Each of those gives strictly more context thanthe previous one, so it should be legitimate for asingle person to do all of them in sequence for asingle sentence. But that means its important thatyou dont see the extra context too soon. This iswhy certain cells are redacted (lled in with black).You can unredact the cell by resetting the ll.So for each sentence/video id, you should do thefollowing:",
  ". Open the rst video link. This is a clip contain-ing only the sentence in question. Translateit into English and write the result in the rstrow under \"your translation goes here\"": "2. Open the second video link. This clip alsoincludes the sentence before the sentence inquestion. Use this extra context to improveyour translation of the sentence in question(if it makes a difference) and write it in thesecond row under \"your translation goes here\",but do not translate the extra sentence includedin the video. Its just for context. 3. Using the same video link (second), reveal thecontents of the rst context cell. This is theEnglish translation of the previous sentence(the one included in the extended video). Usethis extra context to improve your translation(if it makes a difference) and write it in thethird row. 4. Using the same video link, reveal the contentsof the second context cell. This is the En-glish translation of the entire narrative up tothis point. Use this extra context to improveyour translation (if it makes a difference) andwrite it in the fourth row. (In some cases, thenarrative up to this point only consists of theprevious sentence, so #3 and #4 have exactlythe same context. Just copy/paste your trans-lation from above for this case.)",
  "be considered \"no\". PSE might be considered\"meh\".)": "3. Is this an interesting example? You can leavea note here if this sentence might be an inter-esting example for the paper (i.e. it dependson long term context in a way that is interest-ing/exemplary) As a general note: when you translate, if there isambiguity just give your best guess. Pretend thatyoure condent (though you might hedge by usingpronouns, etc.). This is necessary in order to get alike-for-like comparison with machine translationresults.Let me know if you have any questions (or if anyof the clips seem misaligned, links are broken, etc.).",
  "But the most important thing is by using yourlegs, a lot of time you see players come upand shoot their free throw and they stay atfooted and then end up hitting the ball on thefront of the rim": "Sometimes it gets a little stuck, always wipethe edge though of your exacto blade off, thatblade is going to end up tending to be a bladethat your not really going to be able to use forcutting much anymore, so you may want tohave two of the tools available to you so thatin case one of them, you want to just keep thatopen for cutting and the other one you canuse for lifting the materials up when they getstuck.",
  "video idinterpsettingtranslation": "fZgWKh3ENoE-8Cground truthIt helps supplements all this, by discarding Destiny Hero Disk Commanderto the graveyard with Destiny Draw and then drawing cards.siHelp supplement all of this by discarding DH disk on mine. Crosses on agrave with destiny drov, and drawing cards.si1:iHelp supplement all of this by discarding DH disk on my Commander.Crosses on a grave with destiny drov, and drawing cards.si1:i, ti1-si1:i, t0:i1Help supplement all of this by discarding Destiny Hero disk to my grave-yard with destiny drov, and drawing cards. fZgbCwSG3Hc-8Cground truthOnce again. most creatures in most decks, except for blue, will not comewith ying. So, if you are having trouble with ying creatures, you shouldput a couple Whalebone Gliders in your creature deck.siMost creatures - most tiles except blue does not come with ying. Ifyoure struggling with ying creatures you should go ahead and add WGon your creature.si1:iMost creature cards except for blue dont come with ying. If yourestruggling with ying creatures you should go ahead and add WG on yourcreature.si1:i, ti1Most creature cards except for blue dont come with ying. If yourestruggling with ying creatures you should go ahead and add WhaleboneGlider on your creature.si1:i, t0:i1- fzXgYPSnaDs-8Cground truthAll you do, you take your maggot, you can use meal worms, as well, whichare much bigger, which are probably more well suited for this because thisis a rather large hook.siOkay so what are you all doing now? Maggots or mealworms may bemore suited for this since its large.si1:iOkay so what are you all doing now? Maggots - or mealworms may bemore suited for this since its a large hook.si1:i, ti1-si1:i, t0:i1- fzXsxNFczRA-8Cground truthSo that it is a two piece gourd rather than just a simple bowl.siIts missing something cool, rather than just having the bowl.si1:i- parts, very cool. Better than just having the bowl.si1:i, ti1-si1:i, t0:i1-",
  "-g0iPSnQt6w-1Aground truthAnd Im actually going to lock my wrists when I pike.siIntroduce wrist clamp, locked clamp.si1:iUnderneath, leg clamped, locked clamp.si1:i, ti1-si1:i, t0:i1-": "-g0sqksgyc4-2Bground truthIn boxing you always want to be trying to be moving forward, you wantto be trying to be pushed to ght, always trying to be moving forward.siBoxers always want to try to move closer, you want to try to push the ght,try to move closer.si1:i-si1:i, ti1-si1:i, t0:i1- -g45vqccdzI-1Aground truthAnd we can get a little bit of a jump here and here we are on the other sideof that door.siRiding on it, when you arrive, jump into it.si1:i-si1:i, ti1We ride on it, and when we arrive, we jump into the portal.si1:i, t0:i1We ride on the transport plate t oreach the place where we can jump in. 37ZtKNf6Yd8-1Aground truthNow the tuning of this instrument, you have the same string on the topand bottom and then you have a three and a ve of the mayor scale on theinside of the instrument.siHear drums, hear guitar, top and bottom same. You have three to ve ayerbetween scalex inside things.si1:i-si1:i, ti1-si1:i, t0:i1Hear adjustments, listen to a few strums, adjustments at top or bottomhave same effect, three to ve major between scales inside things. 3ddzkmFPEBU-1Aground truthOne would be a string winder, which is used on the tuning machine towind it as youre putting the string on, make it much quicker than turningby hand.siA string winder helps tune machine guitar, it will help adjust tune whileyou listen - will help do it faster than winding at the end, meh.si1:iOne is a string winder that helps machine-tune a guitar, it will help adjusttune while you listen - will help do it faster than hand-winding at theguitars end, no need for that.si1:i, ti1-si1:i, t0:i1-",
  "Fz-N1S0swh8-8Cground truthI loved it, it actually tasted really good.siSpanish oxsi1:i-si1:i, ti1Spanish or Mexicansi1:i, t0:i1-": "FzAIlhumvMA-2Bground truthTheres also information on here about mailing the sample to a laboratoryfor condential conrmation.siAlso the information here - mailed sample to the lab for condentiality -promise.si1:i-si1:i, ti1-si1:i, t0:i1- FzOQMA-CVPc-2Bground truthSo, just try and come up with a budget for your party and you want tohave this much money for food and for decorations and just split it up.siJust try to come rst budget for party you want to have this much moneyfor food and for decorations, split.si1:i-si1:i, ti1Just try to rst come up with a budget for the party - you want to have thismuch money for food and for decorations, split.si1:i, t0:i1- FzQPg4aqNYc-1Aground truthThats how I serve that.siHow I give tea to the customer.si1:iThats how I give tea to the customer.si1:i, ti1-si1:i, t0:i1Thats how I stick in the leaf and then give it to the customer.",
  "FzUdcaxw_vs-2Bground truthCome on; lets get ironing.siCome on, just iron it.si1:i-si1:i, ti1-si1:i, t0:i1-": "FzWvE__PamM-2Bground truthAnother great way to spruce your page is add video to your page and todo that you want to look to the top right.siThe other way to show adding a video to a page. Move your video over toyour page, then we want to look at the top right.si1:i-si1:i, ti1-si1:i, t0:i1- FzaQ-Q5gSmI-1Aground truthAnd this is the base plate.siOn the bottom it has a strip called the base. Its a plate.si1:i-si1:i, ti1The bottom of the saw has a strip called the base. Its a plate.si1:i, t0:i1- Fzj3jz2Imf0-1Aground truthWere going to do some hand vibrato exercises, nger vibrato exercises,as well as arm vibrato exercises so that you can decide for yourself whichtype of vibrato youd like to use and youll have all the information thatyou need to get started.siNotice well discuss this more, like practice hand vibrato - also practicearm vibrato then decide for yourself which you prefer, its important tohave all the information needed to start playing violin.si1:i-si1:i, ti1-si1:i, t0:i1- FzmL8SL6Bow-8Cground truthSo just go in and you can even take this guy here and go in there, atten itdown, real nice.siTake a scoop of it, put it in it, that helps shape it to be a square box andat.si1:iTake a scoop of clay, put it in the bowl, that helps esh out the shape andmake the bottom at.si1:i, ti1-si1:i, t0:i1- FzoUVr98JmQ-8Cground truthSalt, about 1 teaspoon full, add a little bit of chili powder; it depends ifyou want it very spicy, you can go for more.siAround a teaspoon of salt, add some chili akes, if you like it hot, addmore.si1:i-si1:i, ti1-si1:i, t0:i1- G-0gYel1YA8-2Bground truthSome people get kind of confused about the time that it takes for theirpiercings to close up because most people are used to having their eyespierced and theyre used to having them pierced for a long time and thosemost of the time dont close up.siWhy do piercings close up? People are confused about the timing whenyou take piercings off, because people tend to have...si1:iWhy piercings close up. Some people get confused about the timing whenremoving their piercings because people tend to have...si1:i, ti1-si1:i, t0:i1- G05uFub3YFc-2Bground truthWere going to drop that elbow down as we lift the top arm up at least tothe ceiling, and if youre feeling really open and really comfortable withthis pose you can reach it up alongside the ear, but dont let the shoulderscreep up.siKeep one elbow down and bring the other one around above your head,at least try to touch the ceiling. If you feel really comfortable, you canstretch futher, but keep your neck loose, dont squeeze your arm to yourear.si1:iKeeping that elbow down, move the other one around above your head,at least try to touch the ceiling. If you feel really comfortable, you canstretch futher, but keep your neck loose, dont squeeze your arm to yourear.si1:i, ti1-si1:i, t0:i1-6279",
  "G06Irzcwxiw-1Aground truthYou enjoy the moment of what youre doing.siNeed to enjoy that moment.si1:iYou need to enjoy each moment.si1:i, ti1-si1:i, t0:i1-": "G095RWKQ39g-1Aground truthBut as you can see, because of the small size, if Im going to use this reddot nder and Im under six foot, Ive got to get down here and locate myobjects.siTheres a little red divider that says 6ft. Do I still have to bend under it tosee it? I dont know.si1:iTheres a little red dot nder. Im 6 feet tall, so do I still have to benddown to see the crosshairs? Im not sure.si1:i, ti1-si1:i, t0:i1-",
  "G0MjvzT_UqM-2Bground truthReady, inhale.siReady? Breathe in your kee.si1:iReady? Breathe in. Your knee - breathe in.si1:i, ti1-si1:i, t0:i1-": "G0PNAsonBGk-2Bground truthNow were going to turn, instead of bringing the hand up, we leave thehand down, just like this.siNow turn your hands up - like, leave your hands down, like this.si1:i-si1:i, ti1-si1:i, t0:i1- G0Q6AlvH96I-2Bground truthHere, two, three, four, elbow and follow wherever youre going to go, likethe knee to the groin and your elbow.siHere, two, three, four, elbow follows you wherever you go, like your kneeor organs, your elbow.si1:i-si1:i, ti1-si1:i, t0:i1- G19uBylwQww-2Bground truthHi, my name is Robert Segundo and today Im going to teach you how tomake one of my favorite paper airplanes, the simple one.siMy name is Robert Segundo and today is about expert community, myfavorite way to play is making paper airplanes.si1:iMy name is Robert Segundo and today is about the expert community, myfavorite way to play is making paper airplanes.si1:i, ti1-si1:i, t0:i1-",
  "G21Gx_C18IA-2Bground truthOnce again, this is Gabriela Garzon at G.G.siOnce again my name is Gabriel La Garrlon, or G.G.si1:i-si1:i, ti1-si1:i, t0:i1Once again, my name is Gabriela Garzon with G.G": "G23JltC2N8g-5Dground truthBut for safety purposes if thats necessary bring yourself against the wall,and bring yourself right back, and bring your feet up.siCore of your body - the center part of your body, but youre not using itwell, but its for safety.si1:i-si1:i, ti1-si1:i, t0:i1- G2Go6a76xd0-5Dground truthYou need to consider whether the horse has an illness or an injury.siConsider if either of your horses have illness or injury.si1:iConsider whether your horses have illness or injury.si1:i, ti1-si1:i, t0:i1- G2VAlFdgof4-5Dground truthThat is how we do the second line in our heart pulse and monitor design.siHow are we doing the second line in our heart pulse and monitor design.si1:i-si1:i, ti1-si1:i, t0:i1- G2dND014Ps4-5Dground truthThis lever is very important when you want to open up your scooterbecause you cant ride it like this.siReally important - you want to open up your scooter because you cantride like this.si1:iThats really important. So I want you to open up your scooter becauseyou cant ride it like this.si1:i, ti1-si1:i, t0:i1-",
  "G2hnUeetWcc-5Dground truthLook up.siLook up.si1:i-si1:i, ti1-si1:i, t0:i1-": "G2lEchCCRAo-5Dground truthSo you can see in comparison in size they are comfortable so when youare looking at a teapot or a sugar bowl with a set like this you want tomake sure that the sizes are appropriate for what you are buying.siSee that they are comparable in size and that they are comfortable, sowhen you are looking at teapots or sugar bowl sets like this you want tomake sure that the sizes are right for what you are buying.si1:i-si1:i, ti1-si1:i, t0:i1- G2sD7N53ju8-5Dground truthIf you delete the wrong thing, you can always undo it by pressing Apple Zas well.siIf you do the wrong thing you can always undo it by pressing the apple Z.si1:iIf you do the wrong thing you can always undo it by pressing the applebutton and Z.si1:i, ti1-si1:i, t0:i1- G2uKe6hCNSo-5Dground truthYou can get these mostly at a good paper supply or art supply places willcost you a little bit more, so look for a paper supply.siA few paper supply or art supply places will charge you a little more solook for paper supplies.si1:iGot a few good paper supply places - art supply places will charge you alittle bit more so look for paper supply.si1:i, ti1-si1:i, t0:i1- G38DbiHHTW0-5Dground truthSo Im holding it naturally like I was going to do the basic cradle, right,and Im just, Im moving my arms all the way across, so Ive got my rightarm across my body, I turn my stick out so its at and Im going to passthe ball like that, alright?siSo Im holding it naturally like I was going to do with the base handle.Right. And Im going to move it over here. So I have my right arm lowand Im going to raise it so its in front and then turn it over. Im going topass the ball like that, alright?si1:i-si1:i, ti1-si1:i, t0:i1- G3CyVk6dizw-5Dground truthThats basically what we mean when we say were dubbing the body.siThats what we mean when we say we are dubbing the body.si1:i-si1:i, ti1Thats basically what we mean when we say we are dubbing the body.si1:i, t0:i1- G3EE6yhl1vk-5Dground truthYou dont want to hit it to where you restrict it because then, youredenitely going to come up with a cracked cymbal somewhere along theline and if youre paying for them yourselves, youll understand that acouple hundred of dollars a cymbal is not cheap.siBut you dont want to hit where R-B limit. Why? Because then you aredenitely going to come up with a cracked cymbal somewhere on the line.You will understand thats a few hundred dollars of cymbal, its not cheap.si1:iBut you dont want to hit where the rubber restraint is. Why? Because thenyou are denitely going to come up with a cracked cymbal somewhere onthe line. You will understand thats a few hundred dollars of cymbal, itsnot cheap.si1:i, ti1-si1:i, t0:i1-",
  "G3GcPpidwxk-5Dground truthIt always looks like a tuxedo.siAlways looks like a tux.si1:i-si1:i, ti1-si1:i, t0:i1Bow ties will always t the look of a tuxedo": "G3HKHxevpFI-5Dground truthAny facial scrub you dont want to use it more than about three times perweek.siYou dont want to use that other face scrub more than three times a week.si1:iYou dont want to use a face scrub more than three times a week.si1:i, ti1-si1:i, t0:i1- G3IJAoK0uSE-5Dground truthIve used a portion of the back scenery here, a little; just a small clip ofthe city.siHave used a portion of the back scenery here, a small metal movie city.si1:i-si1:i, ti1-si1:i, t0:i1- G3bMqicS4bQ-5Dground truthIts got 102 different classes, and this is where you really need to take yourspecic car, go to the rule book, go to, you know, go online and nd outwhere your car falls in that because thats going to give you your handicap.siHave 102 different categories and this is where you really have to knowyour specic car and the rulebook, and nd out where your car falls in thatbecause thats going to give you your HC.si1:iHave 102 different categories and this is where you really have to knowyour specic car and the rulebook, go online and nd out where your carfalls in that because thats going to give you your HC.si1:i, ti1-si1:i, t0:i1-",
  "G3g0-BeFN3c-5Dground truthAny type of modeling.siAll types of models.si1:iAll kinds of modeling.si1:i, ti1All kinds of modeling.si1:i, t0:i1-": "G3gm_C5UueQ-5Dground truthSo, Im going to turn on my sequencer and Im just going to press playand you can just go to each one and just hear a different presets.siIm going to turn on my sequencer and Im going to push play and youcan say go to each one and listen to different presetssi1:i-si1:i, ti1-si1:i, t0:i1- G3k86AVFwVs-5Dground truthIf the partial which rests on the tooth, is held up by the plastic portion, orthe metal portion, not allowing the partial to completely cede against thetissue.siIf the part which rests on the tooth is held up by the plastic part or themetal part... Not allow the part to be complete ede against the tissue.si1:iIf the part which rests on the tooth is held up by the plastic part or themetal part, it wont allow the part to be completely ceded against thetissue.si1:i, ti1-si1:i, t0:i1- G3qZW-hZXaQ-5Dground truthSo if someone is coming at you with a knife and they stab straight in it isbest to turn out of the way.siIf someone comes close to you with a knife and stabs you directly...si1:i-si1:i, ti1-si1:i, t0:i1-",
  "fzcsY2gm7t0-8Cground truthComposition is what is going to control the ow of the viewers experiencein the space.siWhat controls the ow of the viewing experience.si1:i-si1:i, ti1-si1:i, t0:i1-": "fzncPNr2Sc0-8Cground truthYoull click on that and then theyll want you to sign in and the rsttime that you try to do it, theres a process of signing in, and creating apassword.siTouch the button and a window will pop up for you to sign in. If its therst time youll have to go through the process of setting up a usernameand password.si1:i-si1:i, ti1-si1:i, t0:i1-",
  "g1HvmBOR7Y4-3Eground truthPut it over their head, give them a treat.siPut the collar on then give them treats.si1:i-si1:i, ti1-si1:i, t0:i1-": "g1uA0f9I0Sg-3Eground truthIf you are looking to buy hosiery for open toe shoes, be it if they arepeep toe shoes or if you are looking to wear hosiery with a sandal in thewintertime your best options are to go with hosiery that doesnt have anyhem lines or any type of reinforcements.siIf you want peep toe shoes or sandals in winter, you should still pick hoirwith no lines or reinforcementsi1:iWhether you want to wear open toed shoes like sandals or winter shoes,you should still pick hosery with no lines or reinforcementsi1:i, ti1Whether you want to wear open toed shoes like sandals or winter shoes,you should still pick hosiery with no lines or reinforcementsi1:i, t0:i1-",
  "g2QdwYqm8pg-3Eground truthThis time it is going to be a white face.siNow white face.si1:iNow my face is white.si1:i, ti1-si1:i, t0:i1-": "g2SdWBPoXZ0-3Eground truthSo, for example, if Im going to build up into a backcross pattern, I dontwant to just go here and immediately start throwing backcrosses.siFor example, if Im building to a back cross pattern, I dont want to justgo along and then back cross.si1:i-si1:i, ti1-si1:i, t0:i1- g2eTD-1Jcro-3Eground truthTo do the buttery breath ow, it helps you think about the alignment andalso makes you think about your breath.siStraighten your spine and breathe.si1:i-si1:i, ti1-si1:i, t0:i1-",
  "g2o-GFdGOJE-3Eground truthProbably not going to catch a ush with a three to it.siNot getting a ush with three.si1:i-si1:i, ti1-si1:i, t0:i1-": "g2v-M6EXcUE-3Eground truthBasil is best harvested when theres a lot of leafy stuff right at the tip, butnot a owering stalk yet.siBest to harvest when theres a lot of leaves on the top but not yet anyowers.si1:i-si1:i, ti1-si1:i, t0:i1Basil is best to harvest when theres a lot of leaves on the top but not yetany owers. g38AmwPAYvg-3Eground truthAnd, take them out and take some pictures of them in the sunlight andsee how the sun reects on their skin and how the camera reacts with that,then grab a white piece of paper and hold it up and reect the light backonto their skin.siGo outside and take some pictures with the sun. See how the sun reectson skin and how the camera reacts to that. Then get a white paper, hold itup, and reect light back on the skin.si1:i-si1:i, ti1-si1:i, t0:i1-",
  "g3ZgF8gdfLo-3Eground truthYou then add the top of the condenser, which fastens on with three clips,or clamps.siThen you add the lid and clip the three latches on.si1:i-si1:i, ti1-si1:i, t0:i1-": "g3jQ5ecjGz8-3Eground truthSo Ill just go ahead and use the pistol that we picked up from the gangsterdownstairs, and shoot the chemist.siI picked up this gun from the gangster downstairs. They are shootingchemists!si1:iI pick up this pistol from the gangster downstairs, then I shoot the chemist!si1:i, ti1The pistol that I picked up from the gangster downstairs. Then I shoot thechemist!si1:i, t0:i1-",
  "g3pXM5X3_Xw-3Eground truthNeedle tool.siNeedle tool.si1:i-si1:i, ti1-si1:i, t0:i1-": "g3sLd8JupoQ-3Eground truthIll measure down two inches and put a mark, and then on two inches onthe other side and put a mark.siMeasure 2 inches then mark it. Then 2 inches on the other side and mark.si1:i-si1:i, ti1-si1:i, t0:i1- g3ushtMfLiY-3Eground truthIn order to have your veil in the middle of the choreography, before youget on stage you are going to get your veil and you are going to place it onyour hips like this.siIf you want a veil in the middle of the show/dance, before you arrive onstage, get your veil and put it on your hips like that.si1:i-si1:i, ti1-si1:i, t0:i1-"
}