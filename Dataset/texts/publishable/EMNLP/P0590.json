{
  "Abstract": "The problem of hallucination and omission, along-standing problem in machine translation(MT), is more pronounced when a large lan-guage model (LLM) is used in MT because anLLM itself is susceptible to these phenomena.In this work, we mitigate the problem in anLLM-based MT model by guiding it to betterword alignment. We first study the correlationbetween word alignment and the phenomenaof hallucination and omission in MT. Then wepropose to utilize word alignment as preferenceto optimize the LLM-based MT model. Thepreference data are constructed by selectingchosen and rejected translations from multipleMT tools. Subsequently, direct preference op-timization is used to optimize the LLM-basedmodel towards the preference signal. Given theabsence of evaluators specifically designed forhallucination and omission in MT, we furtherpropose selecting hard instances and utilizingGPT-4 to directly evaluate the performance ofthe models in mitigating these issues. We ver-ify the rationality of these designed evaluationmethods by experiments, followed by exten-sive results demonstrating the effectiveness ofword alignment-based preference optimizationto mitigate hallucination and omission. On theother hand, although it shows promise in miti-gating hallucination and omission, the overallperformance of MT in different language direc-tions remains mixed, with slight increases inBLEU and decreases in COMET.",
  "Introduction": "Large language models (LLMs) have been evolv-ing rapidly and showing predominant perfor-mance in many natural language processing (NLP)tasks (Brown et al., 2020; Achiam et al., 2023; Tou-vron et al., 2023). However, in machine translation(MT), the use of decoder-only LLMs is still lim-ited due to issues such as model size (Xu et al.,2024a) and low-resource languages (Hendy et al.,2023). Conventional encoder-decoder MT models trained on parallel corpora still dominate in prac-tice (Costa-juss et al., 2022). One of the primaryconcerns of applying an LLM to MT is reliabil-ity. Although it does not happen frequently, anLLM is known to hallucinate (Dhuliawala et al.,2023; Zhang et al., 2023a; Bang et al., 2023) asit is pre-trained to predict the next token in verylarge-scale raw texts. Specifically in MT, LLM-based translation systems therefore could have thephenomena of hallucination and omission, whichis also a long-term challenge in the field of MT(Yang et al., 2019; Vamvas and Sennrich, 2022),known as over- and under-translation. In partic-ular, in the very recent WMT-2024 General Ma-chine Translation Task (Kocmi et al., 2024), anewly released LLM-based MT model UnbabelTower (Alves et al., 2024) has achieved the high-est accuracy in most language pairs, demonstratingthe promise of LLM in MT, but also showing thesignificance of the problem of hallucination andomission. As a result, we attempt to mitigate thehallucination and omission in LLM-based MT toimprove its practicality in this work. Hallucination in MT occurs when informationnot present in the source text is generated in thetranslation, and omission occurs when some of theinformation in the source text is missed in the trans-lation. As a related tool that explicitly aligns thesource text and translation at the word level, wordalignment is potentially positive for MT due tothe nature of align and translate (Bahdanau et al.,2015). The degree of coverage of the source textin translation could be a direct signal to identifythe hallucination and omission in MT (Tu et al.,2016). shows the normalized frequencyof the coverage scores predicted by a word aligner.The examples that are annotated as no hallucina-tion or omission tend to have a higher coveragescore, while those in full hallucination or omis-sion are more likely to have an extremely lowcoverage score. small hallucination or omission 0.00.20.40.60.81.0",
  "(b) Coverage distribution of different omission degree": ": A preliminary experiment shows that highercoverage scores correlates to less hallucination andomission.The coverage scores are predicted by aword aligner (Wu et al., 2023a). The human annotationof hallucination and omission is from HalOmi bench-mark (Dale et al., 2023b). Details about the dataset andword alignment model can be found in 5.1. and partial hallucination or omission distribute inthe middle. As the annotations are carefully madeby humans and highly correlates to the coveragescores from the word aligner, this indicates thatword alignment is a simple but promising directionto mitigate these phenomena. Consequently, we propose Word Alignment Pref-erence (WAP) that utilizes word alignment as asignal to optimize LLM-based MT models. WAPconsists of three steps: diverse translation collec-tion, preference data construction, and preferenceoptimization. Specifically, we collect diverse trans-lations with multiple existing translation tools, se-lect chosen and rejected examples with the wordaligner (Wu et al., 2023a), and optimize the model on preference data using direct preference optimiza-tion (DPO) (Rafailov et al., 2024).Furthermore, the evaluation of hallucination andomission is challenging, and there is no existingevaluator specifically designed for this. Improvingthe BLEU and COMET score does not necessarilymean reducing hallucination and omission becausethere are other factors such as mistranslation andfluency. In addition, hallucination is relatively in-frequent, although very severe once it does occur.Hence, to effectively evaluate it, we design exten-sive experiments that include testing on instancesthat potentially have the problem of hallucinationand omission, and using GPT-4 as the evaluatorwith comprehensive analysis. Experimental analy-sis demonstrates the effectiveness of WAP in miti-gating hallucination and omission in MT.In summary, the contributions of this work in-clude the following: We studied the correlation between the cov-erage score by word alignment and the phe-nomena of hallucination and omission in MT.From the preliminary experiments in we found that word alignment is a promisingsignal to mitigate it. In 3 we propose a novel approach, namelyWAP, to construct a word alignment-basedpreference dataset, and use DPO to optimizethe LLM-based MT model. The validity ofthe preference dataset is also demonstrated bydirect fine-tuning on preferred and rejectedtranslations in 6.4. As there is no particular benchmark for evalu-ating the performance of MT models on hallu-cination and omission. We design various ex-periments, including selecting hard instancesand using LLM as an evaluator in 5.2. Theeffectiveness of the evaluation, as well as theproposed WAP has been validated through ex-periments and analysis in 6",
  "Related work": "Hallucination and omission in MT.Hallucina-tions are cases in which the model generates out-put that is partially or completely unrelated to thesource sentence, while omissions are translationsthat do not include some of the input informa-tion (Dale et al., 2023b). Dale et al. (2023a) ex-plore methods that leverage the internal workings of models and external tools, such as cross-lingualsentence similarity and natural language inferencemodels, to detect and mitigate hallucinations inMT. HalOmi (Dale et al., 2023b) introduces anannotated dataset specifically designed to detecthallucinations and omissions. In and 5.2we use HalOmi as a reference to assess how thesetwo phenomena correlate to the coverage output ofthe GPT-4 evaluator and the word aligner, respec-tively. In particular, Yang et al. (2019) introducethe use of word alignment to reduce omission inMT, which partially inspires our idea. Preference tuning for LLMs.LLMs are capa-ble of completing tasks in the zero-shot or few-shotmanner (Radford et al., 2019; Brown et al., 2020).In addition, performance in downstream tasks canalso be enhanced by fine-tuning them with instruc-tion datasets (Wei et al., 2022; Chung et al., 2024;Ouyang et al., 2022). However, acquiring instruc-tion datasets is costly, while obtaining preferencesfor LLM responses is relatively easier (Rafailovet al., 2024). DPO (Rafailov et al., 2024) directlyoptimize LLM with preference data by removingan extra reward model. We utilize DPO in this workdue to the ease of use and effectiveness. A contem-poraneous preference-based method ALMA-R (Xuet al., 2024b), introduces contrastive preferenceoptimization to fine-tune LLMs specifically usingreference-free MT metrics and human annotationas preference. ALMA-R focuses on improving gen-eral LLM-based MT but we attempt to mitigate thehallucination and omission in MT. In addition, ourpreference data are made entirely automatically,which also draws the difference between ALMA-Rand our work. The recently released LLM-basedUnbabel Tower (Alves et al., 2024) has achievedthe best performance in most language pairs inWMT-2024 (Kocmi et al., 2024), which may com-plement our findings in future work. Word alignment.Word-level information hasbeen useful in many NLP tasks such as languagepre-training (Chi et al., 2021; Wu et al., 2021),cross-lingual sentence embedding (Zhang et al.,2023b; Li et al., 2023; Miao et al., 2024), fine-grained visual language grounding (Peng et al.,2023; Wu et al., 2023b, 2024), and particularly inword alignment for MT (Bahdanau et al., 2015; Tuet al., 2016), which aligns the corresponding wordsin translations. Word aligners based on pre-trainedlanguage models (Jalili Sabet et al., 2020; Dou and Neubig, 2021; Nagata et al., 2020; Chousa et al.,2020) have outperformed previous ones based onstatistical MT (Och and Ney, 2003; Dyer et al.,2013).WSPAlign (Wu et al., 2023a) is a pre-trained word aligner that outperforms most of theprevious ones; hence we use it in the experiments.",
  "Gathering translation candidates": "To steer the MT model to avoid hallucination andomission using preference optimization, we firstneed comparable but different translations. Start-ing with a source text x, we utilize K methods toproduce translations, notated as 1, ..., K. Thenwe can get a set of translations Y , in which yk Yis obtained by yk = k(x) and |Y | = K. Details of gathered translationsWe start withthe parallel training data in ALMA (Xu et al.,2024a). This parallel data encompasses five lan-guage pairs with human translations in both direc-tions: cs en, de en, is en, zh en andru en. We employ ISO 639 language codes1 todenote languages. Specifically, cs correspondsto Czech, de to German, is to Icelandic, zhto Chinese and ru and en to Russian and En-glish, respectively. To generate the translations werequire, this dataset is translated in both directionsusing two well-known MT tools, including DeepL2 and ChatGPT (gpt-3.5-turbo-0613)3. Theprompt we use to translate sentences is shown in. The original human-written translationin the training set is also utilized. In particular,Icelandic (is) is not supported by DeepL, therefore,we use Google Translate4 as an alternative.",
  ": The prompt for translating sentences": "alignment of word pairs. Then, the ratio of thesource words, that are aligned with at least oneword, in the translation is taken as the coveragescore, which will be used for the followingpreference annotation.The whole process topredict the coverage score is notated as C(, ).Formally, the coverage score for a translationyk can be calculated by C(x, yk) [0.0, 100.0].Subsequently, the preferred translation and therejected translation are selected as follows:",
  "Filtering": "Note that the whole process of constructing thepreference data is automatic, and the existing MTand word alignment models are not perfect. Evenfor human-annotated translation, quality is also anissue that cannot be ignored (Xu et al., 2024b), and can affect the performance of the model trained onit. Hence, noises are inevitable in both the trans-lated texts and the preference choices. On the otherhand, the MT tools we choose generally have goodperformance, it could happen that the generatedtranslations are not diverse enough, leading to thepreference signal being disrupted. To improve thequality of the constructed preference datasets asmuch as possible, multiple strategies are applied tofilter out potential bad training instances: Remove the instance when the chosen andrejected translations only have a marginal dif-ference in coverage score. The differencethreshold is empirically set as 5.0, that is,(x, yw, yl) is excluded from the dataset ifC(x, yw) C(x, yl) < 5.0. Remove the instance where the chosen andrejected translations are too semantically sim-ilar. Sentence embedding is a widely usedtechnique for sentence similarity with lowcomputation cost (Gao et al., 2021; Wu et al.,2022; Xie et al., 2022; Zhao et al., 2024).In particular, LaBSE (Feng et al., 2022)6 isused in our experiments.We notate it asLB(). The similarity threshold is empiricallyset as 0.9, i.e. (x, yw, yl) is excluded fromthe dataset if sim(LB(yw), LB(yw)) > 0.9.sim(, ) [0.0, 1.0] is cosine similarity.",
  "Details of dataset": "presents the varying proportions of thechosen and rejected preference pairs from threesources: ChatGPT, DeepL, and Human. The fig-ure indicates that most of the chosen translationsoriginate from ChatGPT, while a significant portionof human-written translations are rejected. Thisobservation supports the conclusion that human-written translations can also exhibit quality issues,as discussed in ALMA-R (Xu et al., 2024b). Ex-amples in our constructed preference dataset arepresented in B.1. allcsen deen isen ruen zhen 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 # of examples ChatGPT (Chosen)ChatGPT (Rejected)DeepL (Chosen)DeepL (Rejected)Human (Chosen)Human (Rejected) : This figure illustrates the proportions of cho-sen and rejected preference pairs derived from threesources: ChatGPT, DeepL and Human. all repre-sents the overall proportion for the aggregated dataset.xx en is the subset pair of English and anotherlanguage. Particularly, Google Translate is used foris en as an alternative to DeepL.",
  "Optimization LLM-based MT model": "The final step is to optimize the LLM-based MTmodel on our preference data. Direct preferenceoptimization (DPO) (Rafailov et al., 2024) is a sim-ple but effective approach that directly optimizesthe preference model on a pre-constructed staticdataset. DPO has recently been applied to optimizeLLM in preference data (Tunstall et al., 2023; Xu",
  "Baselines and evaluation datasets": "We choose ALMA-13B9 as the baseline for all ex-periments in this paper, as well as the startingpoint of optimization. ALMA (Xu et al., 2024a)was trained from Llama (Touvron et al., 2023) intwo steps: initial fine-tuning on monolingual dataand subsequent fine-tuning on a small set of high-quality parallel data. For fairly studying the ef-fect of word alignment preference, we use the dataused in the supervised fine-tuning in ALMA asthe source dataset to construct our preference datain 3. Specifically, the source data was collectedfrom WMT17 (Bojar et al., 2017) to WMT20(Barrault et al., 2020), in addition to the devel-opment and text dataset from Flores-200 (Costa-juss et al., 2022). After filtering, we finally make20,074 and 2,226 preference triplets for trainingand development, respectively. For evaluation, thetest set is from WMT22, except that is en isfrom WMT21. The remaining data from WMT21(except is en) is used as the development set.Specifically, 3485, 4021, 2000, 3912, 4053 ex-amples are included in the test set for cs en,de en, is en, zh en, and ru en,respectively. The detailed experimental setup isintroduced in A. HalOmiIn particular, we want to validatewhether our proposed method is capable of mit-igating hallucination and omission in MT. Hence,we also use HalOmi (Dale et al., 2023b) in the ex-periments. HalOmi is an evaluation benchmark forthe detection of hallucination and omission in MT.",
  "N=500": "Easy instancesALMA-BLEU34.3650.8146.9228.5045.1634.6135.2831.7943.9132.1335.13Ours-BLEU35.3350.5947.2527.8244.1633.2534.0730.0042.9231.6734.54ALMA-COMET88.0890.5491.0484.2988.6289.5993.6691.0889.7991.4780.67Ours-COMET87.8090.1090.5083.8688.4088.5592.4889.5788.7990.6180.00 Hard instancesALMA-BLEU21.3135.4628.6613.0825.422.5319.8222.5224.8119.7821.36Ours-BLEU23.0937.9132.6614.0427.3222.8922.3821.3226.5819.7822.82ALMA-COMET73.5678.2481.5567.0774.3972.7476.3880.6173.3875.2967.79Ours-COMET74.7779.7582.4169.5675.6373.2477.3479.1974.1274.9768.60 Overall performance, i.e., N=infinite when all instances are included.ALMA-BLEU30.7344.6836.4624.1540.3731.3731.1226.6739.0528.7630.46Ours-BLEU31.9345.6038.8523.9440.0930.6430.9125.2238.7628.4330.59ALMA-COMET84.4286.2986.3079.7085.0985.4589.4285.8585.7687.5076.83Ours-COMET84.5086.5386.4580.0585.2284.7888.7584.3885.1986.7776.59 : Specific results on 10 translation directions. The size of models are 13B. BLEU and COMET are reported.Cells where the difference is larger than 1.0 are highlighted with colored background. Blue indicates ours modeloutperforms ALMA and red indicates the opposite.",
  "The design of evaluation": "We focus on optimizing LLM-based MT models toavoid hallucination and omission. However, to ourbest knowledge, there is no benchmark measuringMT models specifically for this issue, making theevaluation very challenging. Improving the BLEUor COMET score does not necessarily mean reduc-ing hallucination and omission because there areother factors such as mistranslation and fluency.In addition, hallucination is relatively infrequent,although very severe once it does occur. To intu-itively validate whether our approach is capable ofmitigating hallucination and omission in MT, we",
  "design several evaluation strategies in this section": "Select hard instances.We first select instancesthat the baseline model does not perform well on.This subset of instances is labeled as hard instancesin this work. The subset of the remaining examplesis labeled as easy instances. Specifically, N in-stances with the lowest COMET score are selectedfrom the test set for each translation direction. Ashard examples tend to include more hallucinationand omission, we report the comparison of modelson hard examples and remaining examples, respec-tively. In the experiment, we sample three subsetswhere N = 100, N = 200 and N = 500. The ex-perimental analysis can be found in 6.1. Note thatthe hard instances are only selected for evaluation.We do not differentiate hard or easy instances in thetraining set. Only word alignment signal is used toselect preferred dataset for a fair comparison.",
  ": Prompt to calculate the coverage score": "of hallucination and omission in translation is stillneeded. As we mentioned earlier that improvingthe BLEU and COMET score does not necessarilymean reducing hallucination and omission becausethere are other factors such as mistranslation andfluency, we utilize the generalization and reason-ing ability of LLM (Kojima et al., 2022; Mitchellet al., 2023; Wei et al., 2023) to achieve this di-rect evaluation. We use one of the most powerfulLLM, gpt-4-061310, as an evaluator. LLM isprompted to check whether the given translationhas hallucination or omission referring to the givensource texts. A coverage score between 0 and 100is output as the degree metric. The prompt used isshown in . Is LLM really capable of evaluating halluci-nation and omission in MT?Despite the factthat LLMs have shown impressive zero-shot per-formance in various tasks (Kojima et al., 2022;Mitchell et al., 2023; Wei et al., 2023), the assess-ment of LLM in the evaluation of hallucinationand omission is still important because it has notbeen widely used on this task. We use HalOmidatasets introduced in 5.1 to assess this abilityof GPT-4. The examples in de en, zh en,and ru en are selected, then GPT-4 is used topredict the coverage score for these examples. shows the average score of the degree ofcoverage predicted by GPT-4. The examples fromHalOmi are divided into three subsets according to the labels. We merged the Partial hallucina-tion and omission and Small hallucination andomission in the original because the number ofexamples in these two categories is small. It clearlydemonstrates that examples annotated as No hal-lucination and omission have a higher coveragescore predicted by GPT-4 and those in Full hal-lucination and omission have an extremely lowcoverage score. As a result, using GPT-4 is an ef-fective way to assess whether a translation has theproblem of hallucination or omission.",
  "Evaluation on hard instances": "In 5.2 we introduce how to select hard instancesfrom the test set and explain why hard instances aresuitable to assess hallucination and omission. Inthis section, we evaluate our model on these hardinstances and the remaining examples, respectively. demonstrates the results when the numberof hard instances N = 100, 200, and 500, respec-tively. The following findings can be concluded:",
  "With increasing the number of hard instances,the improvement gained by WAP decreases": "These results indicate that WAP mitigates hallucina-tion and omission to a certain extent, because theseissues are more likely to occur in hard instances.In addition, our model also remains competitiveto the baseline in the remaining easy instances. Itis reasonable that there is no significant differencebecause the compared models are generally good.The challenging part should be in the hard ones.Moreover, it is observed that with increasing N,the improvement gets narrower. The reason is that",
  "Baseline11.33%64.00%21.00%11.33%3.66%56.00%25.33%13.66%4.33%+WAP39.66%75.66%17.33%7.00%0.00%80.00%16.66%5.33%0.00%": ": Human evaluation on zh-en when N=100. Translation quality is the measured by ratio of examples whereWAP beats the baseline. The remaining columns present the ratio of examples in which the corresponding degree ofhallucination or omission occurs. Better model is highlighted with bold fonts. more relatively easy instances are included in thesubset. This is another evidence that WAP providesgains particularly for hallucination and omission inMT. The specific numeric results and the overallresults for the entire test set are shown in C.",
  "Direct evaluation of hallucination andomission by GPT-4": "In addition to improving BLEU and COMET inhard examples prone to hallucination and omission,direct evaluations are also necessary to confirm theeffectiveness of WAP. In 5.2 we have verified theusefulness of GPT-4 as an evaluator with experi-ments. In this section, we prompt GPT-4 to directlypredict a coverage score as a metric for hallucina-tion and omission. The results are demonstratedin . The reported number is the average ofthe coverage scores in hard examples. The resultsshow that WAP outperforms the baseline in all di-rections except en is. In the overall averagescore across all translation directions, WAP outper-forms the baseline model by 4.96, 1.63 and 1.24when N=100, 200 and 500, respectively. The trendis similar to that in 6.1, directly indicating that theLLM-based MT model avoids hallucination andomission with the word-aligned preference.",
  "Although the validity of GPT-4 as evaluator forhallucination and omission has been demonstrated": "in 5.2 and , we conduct a human evalua-tion to further verify our findings, as LLM couldstill be unreliable. The subset of N=100 on zh-en is selected. Three volunteers who speak Chi-nese and English are asked to assess the qualityof the translation and the degree of hallucinationand omission for the baseline and our model, with-out knowing which model generates the transla-tions. demonstrates the results. In general,our model generates better translation in 39.66%of the examples, while the percentage for ALMAis 11.33%. Furthermore, it is observed that withDPO on word-alignment preferred data fine-tuning,the degree of both hallucination and omission de-creases. Specifically, the percentage of no hallu-cination increases from 64% to 75.66%, and thatof small, partial, and full hallucination decreasesaccordingly. The decrease in omission is moredistinct, in which the percentage of no omissionincrease by 24%. Notably, for both hallucinationand omission, the percentage of full hallucinationand omission has decreased to 0 for our model.These results indicate that omission is more fre-quent than hallucination, and WAP can mitigatethem in the LLM-based MT model.",
  "The comparison is demonstrated in": "Does the preferred data truly contribute moreto training?It is observed that FT_prefer sig-nificantly outperforms FT_reject in both hard andeasy instances. This suggests that WAP effectivelyselects samples, improving translation quality. Ithighlights the importance of selecting high-qualitytraining data, as even human-annotated data can beflawed (Xu et al., 2024a). Is DPO preference tuning necessary?The filledarea highlights the necessity of preference tuningwith DPO. While FT_prefer performs competi-tively in hard instances, it significantly underper-forms WAP and ALMA in easy instances, limitingits practicality. The possible reason for the dif-ferent performance in the hard and easy instancescan be the direct fine-tuning, which focusing solely",
  "Conclusion": "The problem of hallucination and omission, a long-standing problem in MT, could become more se-vere when an LLM is used because an LLM it-self could hallucinate or omit in nature. In thispaper, our aim is to mitigate this problem in LLM-based MT by optimizing the model toward a pref-erence for better word alignment. We constructpreference datasets by collecting translations us-ing multiple MT tools and selecting the preferencepair with a higher coverage score output by a wordaligner. DPO is then utilized to optimize the modeltowards the word-aligned preference. As evalua-tion of hallucination and omission is challenging,we design experiments that include selecting hardinstances and using GPT-4 to directly predict cover-age score, ensuring an effective evaluation, whichindicates that the proposed WAP mitigates halluci-nation and omission, especially in hard instances.On the other hand, although WAP shows promise inmitigating hallucination and omission, the overallperformance of MT in different language directionsremains mixed, with slight increases in BLEU anddecreases in COMET.",
  "Limitation": "The first limitation of our method stems from theimperfections of the word alignment model. Withinour approach, it is inevitable to encounter somealignment errors, which we address through a fil-tering method. However, this solution adds com-plexity and clutter to the method. Additionally, theeffectiveness of our method is diminished for low-resource language translations due to the limitednumber of parallel sentences available. From theperspective of experiments, we only evaluate themethods in English-centric translation pairs due tothe lack of Non-English data, in which hallucina-tion and omission could happen more frequently.In particular, the WMT-2024 General MachineTranslation Task (Kocmi et al., 2024) has adoptednon-English language pairs, such as Czech-to-Ukrainian and Japanese-to-Chinese, which couldexpand our work in the future. Moreover, our re-liance on the GPT-4 API to evaluate the resultsintroduces a significant cost factor. In future work,our objective is to find a cost-free alternative to thisevaluation process. Lastly, although WAP showspromise in mitigating hallucination and omission,the overall performance of MT in different lan-guage directions remains mixed, with slight in-creases in BLEU and decreases in COMET.",
  "Ethical Statement": "All datasets and checkpoints used in this paperare copyright-free for research purposes. Previousstudies are properly cited and discussed. This re-search aims to improve LLM-based machine trans-lation models with word alignment preference data,and the preference is made by an automatic wordaligner. We do not introduce additional bias to par-ticular communities. We have obtained the consentof the annotation volunteers for this study. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774. Duarte M. Alves, Jos Pombal, Nuno M. Guerreiro, Pe-dro H. Martins, Joo Alves, Amin Farajian, Ben Pe-ters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal,Pierre Colombo, Jos G. C. de Souza, and Andr F. T.Martins. 2024. Tower: An open multilingual largelanguage model for translation-related tasks. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio. 2015.Neural machine translation by jointlylearning to align and translate. In 3rd InternationalConference on Learning Representations, ICLR 2015,San Diego, CA, USA, May 7-9, 2015, ConferenceTrack Proceedings. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-liang Dai, Dan Su, Bryan Wilie, Holy Lovenia, ZiweiJi, Tiezheng Yu, Willy Chung, et al. 2023. A multi-task, multilingual, multimodal evaluation of chatgpton reasoning, hallucination, and interactivity. In Pro-ceedings of the 13th International Joint Conferenceon Natural Language Processing and the 3rd Confer-ence of the Asia-Pacific Chapter of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 675718. Loc Barrault, Magdalena Biesialska, Ondrej Bo-jar, Marta R. Costa-juss, Christian Federmann,Yvette Graham, Roman Grundkiewicz, Barry Had-dow, Matthias Huck, Eric Joanis, Tom Kocmi,Philipp Koehn, Chi-kiu Lo, Nikola Ljubeic, ChristofMonz, Makoto Morishita, Masaaki Nagata, Toshi-aki Nakazawa, Santanu Pal, Matt Post, and MarcosZampieri. 2020. Findings of the 2020 conference onmachine translation (WMT20). In Proceedings ofthe Fifth Conference on Machine Translation, pages155, Online. Association for Computational Linguis-tics. Ondrej Bojar, Rajen Chatterjee, Christian Federmann,Yvette Graham, Barry Haddow, Shujian Huang,Matthias Huck, Philipp Koehn, Qun Liu, VarvaraLogacheva, Christof Monz, Matteo Negri, Matt Post,Raphael Rubino, Lucia Specia, and Marco Turchi.2017. Findings of the 2017 conference on machinetranslation (WMT17). In Proceedings of the SecondConference on Machine Translation, pages 169214,Copenhagen, Denmark. Association for Computa-tional Linguistics. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020.Language models are few-shot learners.In Ad-vances in Neural Information Processing Systems,volume 33, pages 18771901. Curran Associates,Inc. Zewen Chi, Li Dong, Bo Zheng, Shaohan Huang, Xian-Ling Mao, Heyan Huang, and Furu Wei. 2021. Im-proving pretrained cross-lingual language models viaself-labeled word alignment. In Proceedings of the59th Annual Meeting of the Association for Compu-tational Linguistics and the 11th International Joint",
  "Conference on Natural Language Processing (Vol-ume 1: Long Papers), pages 34183430, Online. As-sociation for Computational Linguistics": "Katsuki Chousa, Masaaki Nagata, and Masaaki Nishino.2020. SpanAlign: Sentence alignment method basedon cross-language span prediction and ILP.InProceedings of the 28th International Conferenceon Computational Linguistics, pages 47504761,Barcelona, Spain (Online). International Committeeon Computational Linguistics. Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, et al.2024. Scaling instruction-finetuned language models.Journal of Machine Learning Research, 25(70):153. Marta R Costa-juss, James Cross, Onur elebi, MahaElbayad, Kenneth Heafield, Kevin Heffernan, ElaheKalbassi, Janice Lam, Daniel Licht, Jean Maillard,et al. 2022.No language left behind: Scalinghuman-centered machine translation. arXiv preprintarXiv:2207.04672. David Dale, Elena Voita, Loic Barrault, and Marta R.Costa-juss. 2023a. Detecting and mitigating hal-lucinations in machine translation: Model internalworkings alone do well, sentence similarity Even bet-ter. In Proceedings of the 61st Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 3650, Toronto, Canada.Association for Computational Linguistics. David Dale, Elena Voita, Janice Lam, PrangthipHansanti, Christophe Ropers, Elahe Kalbassi, Cyn-thia Gao, Loic Barrault, and Marta Costa-juss.2023b. HalOmi: A manually annotated benchmarkfor multilingual hallucination and omission detec-tion in machine translation. In Proceedings of the2023 Conference on Empirical Methods in NaturalLanguage Processing, pages 638653, Singapore. As-sociation for Computational Linguistics. Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu,Roberta Raileanu, Xian Li, Asli Celikyilmaz, andJason Weston. 2023. Chain-of-verification reduceshallucination in large language models.ArXiv,abs/2309.11495. Zi-Yi Dou and Graham Neubig. 2021. Word alignmentby fine-tuning embeddings on parallel corpora. InProceedings of the 16th Conference of the EuropeanChapter of the Association for Computational Lin-guistics: Main Volume, pages 21122128. Chris Dyer, Victor Chahuneau, and Noah A Smith. 2013.A simple, fast, and effective reparameterization ofibm model 2. In Proceedings of the 2013 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, pages 644648.",
  "Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021": "SimCSE: Simple contrastive learning of sentence em-beddings. In Proceedings of the 2021 Conferenceon Empirical Methods in Natural Language Process-ing, pages 68946910, Online and Punta Cana, Do-minican Republic. Association for ComputationalLinguistics. Amr Hendy, Mohamed Abdelrehim, Amr Sharaf,Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita,Young Jin Kim, Mohamed Afify, and Hany HassanAwadalla. 2023. How good are gpt models at ma-chine translation? a comprehensive evaluation. arXivpreprint arXiv:2302.09210. Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,et al. 2021. Lora: Low-rank adaptation of large lan-guage models. In International Conference on Learn-ing Representations. Masoud Jalili Sabet, Philipp Dufter, Franois Yvon,and Hinrich Schtze. 2020. SimAlign: High qual-ity word alignments without parallel training datausing static and contextualized embeddings. In Find-ings of the Association for Computational Linguistics:EMNLP 2020, pages 16271643, Online. Associationfor Computational Linguistics. Tom Kocmi, Eleftherios Avramidis, Rachel Bawden,Ondrej Bojar, Anton Dvorkovich, Christian Feder-mann, Mark Fishel, Markus Freitag, Thamme Gowda,Roman Grundkiewicz, et al. 2024.Preliminarywmt24 ranking of general mt systems and llms. arXivpreprint arXiv:2407.19884. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-guage models are zero-shot reasoners. Advances inneural information processing systems, 35:2219922213. Ziheng Li, Shaohan Huang, Zihan Zhang, Zhi-HongDeng, Qiang Lou, Haizhen Huang, Jian Jiao, FuruWei, Weiwei Deng, and Qi Zhang. 2023.Dual-alignment pre-training for cross-lingual sentence em-bedding. In Proceedings of the 61st Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 34663478, Toronto,Canada. Association for Computational Linguistics.",
  "Franz Josef Och and Hermann Ney. 2003. A systematiccomparison of various statistical alignment models.Computational linguistics, 29(1):1951": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, JohnSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,Maddie Simens, Amanda Askell, Peter Welinder,Paul F Christiano, Jan Leike, and Ryan Lowe. 2022.Training language models to follow instructions withhuman feedback. In Advances in Neural InformationProcessing Systems, volume 35, pages 2773027744.Curran Associates, Inc. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalu-ation of machine translation. In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics, pages 311318, Philadelphia,Pennsylvania, USA. Association for ComputationalLinguistics.",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan,Dario Amodei, Ilya Sutskever, et al. 2019. Languagemodels are unsupervised multitask learners. OpenAIblog, 1(8):9": "Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-pher D Manning, Stefano Ermon, and Chelsea Finn.2024. Direct preference optimization: Your languagemodel is secretly a reward model. Advances in Neu-ral Information Processing Systems, 36. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,and Hang Li. 2016. Modeling coverage for neuralmachine translation. In Proceedings of the 54th An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 7685. Lewis Tunstall, Edward Beeching, Nathan Lambert,Nazneen Rajani, Kashif Rasul, Younes Belkada,Shengyi Huang, Leandro von Werra, ClmentineFourrier, Nathan Habib, Nathan Sarrazin, Omar San-seviero, Alexander M. Rush, and Thomas Wolf. 2023.Zephyr: Direct distillation of lm alignment. ArXiv,abs/2310.16944. Jannis Vamvas and Rico Sennrich. 2022. As little aspossible, as much as necessary: Detecting over- andundertranslations with contrastive conditioning. InProceedings of the 60th Annual Meeting of the As-sociation for Computational Linguistics (Volume 2:Short Papers), pages 490500, Dublin, Ireland. As-sociation for Computational Linguistics. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,Adams Wei Yu, Brian Lester, Nan Du, Andrew M.Dai, and Quoc V Le. 2022. Finetuned language mod-els are zero-shot learners. In International Confer-ence on Learning Representations. Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang,Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu,Yufeng Chen, Meishan Zhang, Yong Jiang, and Wen-juan Han. 2023. Zero-shot information extraction viachatting with chatgpt. ArXiv, abs/2302.10205. Qiyu Wu, Masaaki Nagata, and Yoshimasa Tsuruoka.2023a. WSPAlign: Word alignment pre-training vialarge-scale weakly supervised span prediction. InProceedings of the 61st Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 1108411099, Toronto, Canada.Association for Computational Linguistics. Qiyu Wu, Chongyang Tao, Tao Shen, Can Xu, XiuboGeng, and Daxin Jiang. 2022. PCL: Peer-contrastivelearning with diverse augmentations for unsupervisedsentence embeddings. In Proceedings of the 2022Conference on Empirical Methods in Natural Lan-guage Processing, pages 1205212066, Abu Dhabi,United Arab Emirates. Association for Computa-tional Linguistics.",
  "sentence embeddings with pseudo-siamese mutuallearning. IEEE/ACM Transactions on Audio, Speech,and Language Processing, 30:30463059": "Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Has-san Awadalla. 2024a. A paradigm shift in machinetranslation: Boosting translation performance oflarge language models. In The Twelfth InternationalConference on Learning Representations. Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan,Lingfeng Shen, Benjamin Van Durme, Kenton Mur-ray, and Young Jin Kim. 2024b. Contrastive pref-erence optimization: Pushing the boundaries of llmperformance in machine translation. arXiv preprintarXiv:2401.08417. Zonghan Yang, Yong Cheng, Yang Liu, and MaosongSun. 2019. Reducing word omission errors in neuralmachine translation: A contrastive learning approach.In Proceedings of the 57th Annual Meeting of the As-sociation for Computational Linguistics, pages 61916196, Florence, Italy. Association for ComputationalLinguistics. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,Yulong Chen, Longyue Wang, Anh Tuan Luu, WeiBi, Freda Shi, and Shuming Shi. 2023a. Sirens songin the ai ocean: A survey on hallucination in largelanguage models. ArXiv, abs/2309.01219.",
  "The implementation from alignment-handbook11": "is used for the training of DPO. The learning rateis searched based on performance on developmentset and set to 5e-6. LoRA (Hu et al., 2021) is used.r is set as 16 and is set as 0.1. We train the modelfor 1 epoch and fix the random seed to 42. Themodel is trained on 4 Nvidia A100 80G and thetotal batch size is 64. For evaluation, we use theimplementation of ALMA12 to calculate the BLEUand COMET scores.",
  "B.1Examples of the preference dataset": "includes three examples in our dataset, inwhich the source sentence, the chosen and rejectedtranslations are shown. Refer to 4 for a detailedconstruction of the dataset. Example 1: the re-jected translation is from human annotation, inwhich it repeats the term of I think unnaturally.The possible reason could be the resource of theparallel data, e.g., direct collection from transcrip-tions. Example 2: Fuller is omitted by humanannotation while translated by DeepL. Example3: the chosen translation is from gpt-3.5-turbo thatcompletely translates the source sentence. In con-trast, the translation by DeepL omits the first half.",
  "B.2Translation examples": "shows illustrative comparison betweentranslations from the baseline and our model. Ex-ample 1: in HBOs The Gilded Age\" in thesource sentence is omitted by the baseline.Incontrast, our model successfully translate the cor-responding part into Chinese. Example 2: thebaseline generates (fastening) infinitely intranslation. This type of hallucination also occursin other LLM applications, which emphasizes theneed to address the hallucination issue in LLM-based MT models. Example 3: (when to wait) is omitted by the baseline modelwhile our model translate that into how long Ihave to wait properly.On the other hand, WAP could also fail in somecases. Example 4: Although the baseline omitspictures and box, which our model successfullytranslates, the translation of our model is not fully correct. The source is in the box with frame, butour models translation is (framein the box). Example 5: Although our modeltranslates pot () that is omitted by the base-line, the meaning of the sentence is incorrect. Thesource means \"This pot is a good buy,\" but ourtranslation is \"This pot is worth buying.\" In gen-eral, our model performs well in terms of cover-age, which is more related to hallucination andomission; however, the translation quality does notnecessarily improve accordingly. The study of pref-erence signals for both overall translation qualityand reducing hallucination and omission is worthexploring.",
  "COverall MT Performance": "shows the numeric results in , inwhich boxes on a blue background highlight thecases where our model outperforms the baseline bya margin > 1.0, and the boxes in red are the oppo-site. Boxes without background indicate the caseswhen our model and the baseline have competitiveperformance where the margin < 1.0.In addition to the main findings in 6.1 that ourmodel generally performs better in harder instances,from the results it can also be observed that ourmodel particularly performs worse on en-is thanin other translation directions. The reason could bethat Icelandic is a low-resource language and weused external tools such as WSPAlign and GoogleTranslate to build the training data. Hence, therelatively unreliable performance of external toolson low-resource languages can induce noises in ourtraining data. This could be a future direction forbuilding more reliable word alignment signals andparticular research on low-resource languages. reports the overall performance when wedo not split the dataset into the hard and easy sub-set. The results show that our model and ALMAhave generally competitive performance. Specif-ically, if we only consider the margin larger than1.0, our model outperforms ALMA on de-en andis-en in BLEU while ALMA performs better onen-is in both BLEU and COMET. In particular,a significance test is conducted to investigate nu-meric degradation when all instances are included.We utilize bootstrap sampling from example-wiseCOMET scores with 100,000 iterations and cal-culate the p-value. Based on the results of thesignificance test, there is no statistical significancewhen the margin is greater than 0.25, indicated by"
}