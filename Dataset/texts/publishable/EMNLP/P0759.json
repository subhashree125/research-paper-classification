{
  "Abstract": "One useful application of NLP models is tosupport people in reading complex text fromunfamiliar domains (e.g., scientic articles).Simplifying the entire text makes it under-standable but sometimes removes importantdetails. On the contrary, helping adult read-ers understand difcult concepts in context canenhance their vocabulary and knowledge. Ina preliminary human study, we rst identifythat lack of context and unfamiliarity with dif-cult concepts is a major reason for adult read-ers difculty with domain-specic text. Wethen introduce targeted concept simplication,a simplication task for rewriting text to helpreaders comprehend text containing unfamiliarconcepts. We also introduce WIKIDOMAINS1,a new dataset of 22k denitions from 13 aca-demic domains paired with a difcult conceptwithin each denition.We benchmark theperformance of open-source and commercialLLMs, and a simple dictionary baseline on thistask across human judgments of ease of un-derstanding and meaning preservation. Inter-estingly, our human judges preferred explana-tions about the difcult concept more than sim-plication of the concept phrase. Further, nosingle model achieved superior performanceacross all quality dimensions, and automatedmetrics also show low correlations with humanevaluations of concept simplication ( 0.2),opening up rich avenues for research on per-sonalized human reading comprehension sup-port.",
  "I dont understand what this concept means?": ": An example from the dataset, which consistsof a denition and a potential difcult concept in thetext that a reader may struggle with. The task is torewrite the denition in a way that simplies this con-cept for the reader. (a) Simplies digits of precisionto as many digits as needed, (b) Adds the denitionof digits of precision, (c) Contextually explains thatdigits of precision refers to precision of calculationsand how it relates to memory. LLMs, sentence simplication has made consider-able progress towards generating text at differentreading grade levels (Kew et al., 2023). However,skilled adult readers face more challenges with lackof subject-matter knowledge (Guo et al., 2023).Supporting readers in understanding concepts theynd personally difcult within a larger body oftext not only expands their vocabulary, but alsohelps them develop a broader understanding of thetopic (Kintsch, 1991; Van den Broek, 2010).",
  "For example, in , a person unfamiliarwith the concept digits of precision will not un-": "derstand the denition of arbitrary precision arith-metic. AI tools could help rewrite the text bylexically substituting digits of precision with asmany digits as needed (simplifying) or by elabo-rating on the concept (dening or explaining). (a)Lexical simplication makes the denition under-standable by reducing the overall complexity, per-haps losing some of the meaning. (b) Adding adenition of digits of precision may broaden thereaders vocabulary but does not explain its signif-icance in the context of the overall denition (i.e.its implications for calculations and memory). (c)Providing a contextual explanation about digits ofprecision could more explicitly link the relationbetween memory and preciseness of calculations,enhancing comprehension (Van den Broek, 2010;Srikanth and Li, 2021).In our study, we asked human raters to read def-initions from 13 academic domains and identifythe challenges in understanding them. We foundthat 50% of the reading difculties arose from un-familiar concepts, and annotators expressed theneed for more context around them. Motivated bythis, we present the new task of targeted conceptsimplication for rewriting text to support under-standing of difcult concepts within the denitionscontext. This task focuses on simplifying specicconcepts that users struggle with, allowing for per-sonalized reading support than simply rewritingan entire document at an easier reading level. Per-sonalized support with difcult concepts can helpreaders receive more contextually-relevant informa-tion tailored to their background knowledge. Forinstance, a computer scientist reading a physicsdocument might struggle with physics concepts butunderstand the mathematical terms, while someonewithout a math background might need help withmathematical terms (Guo et al., 2023).To investigate this task, we collect a new dataset,WIKIDOMAINS, consisting of 22k denitions fromWikipedia. We collect denitions using article ti-tles and leading statements from Wikipedia. Ourdenitions span 13 academic domains (e.g., busi-ness, education, etc., see ) improving overexisting datasets that are limited to a single domain(e.g., science) (August et al., 2022). We annotate apotential difcult concept in each denition usingan automated heuristic (Biran et al., 2011).We use this dataset to evaluate the performanceof open-source and commercial LLMs on targetedconcept simplication. We explore three methodsfor rewriting denitions: adding a dictionary de-",
  ": Domains and number of denitions in eachdomain in the WIKIDOMAINS dataset": "nition of the difcult concept, prompting LLMs tosimplify the difcult concept, and prompting LLMsto explain the difcult concept in context. We con-duct human evaluations of all three approachesalong three dimensions: 1) meaning preservation,2) whether a reader who is unfamiliar with the dif-cult concept can understand the rewritten denition,and 3) whether the rewritten denition is easier tounderstand than the original. Our human evalua-tions demonstrate a clear preference towards strate-gies for contextual explanation of the difcult con-cept rather than lexical simplications. However,we also nd that LLMs need to improve further ondimensions of comprehension. Low to mild cor-relations of automated simplication metrics withhuman evaluations of comprehension and mean-ing preservation ( 0.1-0.3) also indicate a needfor better metrics to evaluate nuanced contextualexplanations.In summary, our main contributions include:",
  "Background": "Cognitive Support and Human Reading Com-prehensionSuccessful reading comprehensionis key to integrating new knowledge and fosteringlearning from text (Lorch Jr and van den Broek,1997; Dunietz et al., 2020). Cognitive theoriessuggest that comprehension is a multi-stage pro-cess that primarily involves 1) constructing a localmeaning representation of text such as concepts,facts, and their relations (Graesser et al., 1994),and 2) forming a schema and lling in gaps usingbackground knowledge to create a mental pictureof what the text is about (Kintsch and Van Dijk,1978; Bartlett, 1995). Adult readers lacking domainknowledge can be supported by explicit cues, suchas examples and explanations, to help them con-struct better mental representations of ideas fromthe text (Kintsch, 1991; Van den Broek, 2010). TextSimplicationReducingreading-levelcomplexity and syntax (Garbacea et al., 2021)in text simplication benets specic audienceslike students, second language learners, andindividuals with dyslexia (Paetzold and Specia,2016; Bingel et al., 2018), but may not enhancecomprehension for general adult readers (Garbaceaet al., 2021). Contextual explanations can enhancecomprehension but ndings from studies ofelaborating events in news domains (Srikanth andLi, 2021) may not be the same as difculty withconcepts in academic texts.While Wikipediaand news corpora (Kauchak, 2013; Xu et al.,2015; Zhang and Lapata, 2017) have advancedtext simplication, they focus more on syntaxand discourse difculties than on academicconcepts.Similarly, lexicons are limited tomedicine (Elhadad and Sutaria, 2007; Ong et al.,2007) and science concepts (August et al., 2022),highlighting the need for a multi-domain corpus toadvance personalized simplication for a generalaudience. Complex Terms and JargonLexical simpli-cation systems (Paetzold and Specia, 2016) havebeen shown to benet children, people with lan-guage impairments or medical jargon simplica-tion (Fatima and Strube, 2023; Joseph et al., 2023). Q1 Q2 51.77% 27.59% 27.66% 39.66% 0.00% 2.30% 10.64% 22.99% 0.71% 17.24% 4.96% 7.47% 6.38% 3.45% 10.64% 4.02% Didn't understand a mentioned concept Want *more* details or other context Want *less* detail General writing complexity Want an example or analogy Want visual or audio reference Text was ill-formed General confusion Q1: Your difficulties in understanding? Q2: What would you ask a tutor to change? : Results of annotator study: We asked anno-tators to read complex text for (1) what made the textdifcult for them to understand and (2) how they wouldwant a tutor to edit the text to help their understanding. However, beyond lab studies, it is challenging tospecify reader knowledge in large-scale evalua-tions. Proxies for audience knowledge includespecialized lexicons (Paris, 1988; Elhadad and Su-taria, 2007), coarse indicators such as reading gradelevel (Agrawal and Carpuat, 2023), or binary indi-cators to denote science knowledge audience (Au-gust et al., 2022). Guo et al. (2023) highlightedthe challenge of specifying audience knowledgeat a ner level, suggesting the use of domain as aproxy for concept familiarity. Building on this, weprovide a multi-domain corpus of challenging de-nitions to specify ne-grained audience levels. Un-like prior work on generating denitions (Augustet al., 2022) or simplifying all difcult concepts (Fa-tima and Strube, 2023), we focus on rewriting de-nitions to address specic concept difculties, en-abling readers to leverage their background knowl-edge and improve comprehension (Kintsch, 1991;Rello et al., 2013). While previous tools exploredsimple strategies like adding denitions for com-plex terms (Bingel et al., 2018), we evaluate LLMsthat can provide contextual explanations (Srikanthand Li, 2021).",
  "What AI assistance can benet readingdomain specic text": "To better motivate the scope of this task, we in-vestigate adult readers difculties with domain-specic text and what types of help they wouldwant from an AI-tutor. We randomly selected aset of 900 text examples from Wikipedia-deriveddenitions spanning 13 domains (see for details about denitions and domain selection).For each example, we ask a human annotator to re- spond in free-text to: 1) the reasons for difculty (ifany) when reading the denitions, and 2) what theywould ask a tutor to change in the denition if theyfaced a difculty. In , we report recurringthemes from annotator responses to both questionsfor cases where annotators had difculty readingfrom these examples (categories were agreed on bythe authors, see details in Appendix Section A).These results suggest that specic difcult con-cepts used in the denitions were one of the mostfrequent reasons for reading difculty (52% of def-initions had such difculty), and annotators fre-quently asked for help from a tutor with these con-cepts (28%). This indicates that our proposed task,targeted concept simplication, is an important sub-task for simplication aimed to resolve a key chal-lenge for lay adult readers. When asking a tutorfor help, annotators also explicitly asked for moredetails on the difcult concept (rather than less).This suggests that contextual elaborations (Srikanthand Li, 2021) for these difcult concepts are a bet-ter alternative over lexical simplications to sup-port their comprehension and knowledge. Anno-tators also asked for examples/analogies (17%),visual/audio aids (8%), or identied general issueswith the writing complexity (23%, e.g., an issuewith general reading level or syntactic complexity),though to a lesser degree. The majority of our anno-tators had an above high-school level educationalqualication (see in Appendix), suggestingthat unfamiliar concepts in context is a greater chal-lenge for skilled adult readers than simply difcultwords or syntax.",
  "Task Denition": "We present targeted concept simplication: a textsimplication task focused on specic words orphrases that readers nd difcult to understand.This setup allows for personalized and controlledrewriting of difcult concepts. Our initial userstudy () shows that unfamiliar words orphrases often hinder comprehension.The task of targeted concept simplication is torewrite an input denition containing a concept cto make it understandable to someone unfamiliarwith the concept. For example, showsthe denition of the term arbitrary precision arith-metic. The task is to rewrite the denition to helpsomeone unfamiliar with the difcult concept dig-its of precision. Possible approaches could in-volve replacing digits of precision with a simpler phrase like as many digits as needed, explainingit within the denition, or perhaps even adding ex-amples, analogies, or illustrations. The usefulnessof each strategy will depend on its ability to com-plement the readers existing knowledge about thetopic Kintsch (1991). Unlike other text simpli-cation tasks, our task targets simplifying conceptsdifcult for the reader rather than simplifying theentire text.",
  "The WIKIDOMAINS Dataset": "To support research on targeted concept simplica-tion, we introduce a dataset of 22k denitions from13 academic domains2, where each denition is a12 sentence explanation of a term3. Within eachdenition, we select a difcult concepta wordor phrase that could impede the readers ability tocomprehend the denition as whole. We take inspi-ration from August et al. (2022) who collected de-nitions from Wikipedia science glossaries; however,instead of glossaries, we directly collect denitionsfrom Wikipedia articles of concepts spanning 13domains (see for list of domains).To collect denitions, we start with Johnson (2021)s dataset that contains all English Wikipediaarticles with probabilities of belonging to high-level domains. These domains are broad academictopics (e.g., Physics, Economics) that Wikipediaeditors identied through consensus (Asthana andHalfaker, 2018). We refer to each Wikipedia arti-cle title as a term and take the rst sentence of itslead section as its denition (August et al., 2022).For every domain, we rst select articles with do-main assignment probabilities greater than a thresh-old domain.4 To lter out low-importance articlesthat could be named entities, unimportant placesor things, we also excluded articles having a page-rank percentile score less than pr.5 Finally, wealso excluded articles that were additionally mem-bers of domains related to named entities, events,or things (e.g., Biography). summarizesthe 13 domains and the number of articles in eachdomain that the nal WIKIDOMAINS dataset con-tains (more details in Appendix B). We also provide 2 call the concept being explained by the original de-nition a term to avoid confusion with difcult concepts presentwithin the denitions.4Manual inspection of topic assignments for 50 Wikipediaarticles suggested that a threshold of 0.7 was reasonable toidentify articles belonging to a domain.5We determined the threshold as 0.1 through a manualexamination of 100 articles.",
  ": Statistics on WIKIDOMAINS denitions bro-ken down by split; #tokens and vocabulary size are cal-culated by splitting the denitions on whitespace andremoving punctuation": "each terms lead section in the dataset for futureresearch.We select a training, development, and test splitof 15,873/3,384/3,304 examples (see formore details about the data splits.) We conduct ourexperiments in a zero- or few-shot setting withoutusing the training or development data, but we pub-licly release the full set to facilitate future research.",
  "Difcult Concept Identication": "For each denition, we automatically label a poten-tial difcult concept that could impede a readerscomprehension. Lay readers will be more familiarwith concepts that are popularly mentioned acrossWikipedia (e.g., bacteria) than concepts that onlyoccur in articles of a specic domain (e.g., Phy-tosterol). Thus, following prior work on approx-imating word difculties using specicity-basedmeasures (Biran et al., 2011), we use a domain-specicity measure to score concept difculty for alay audience.First, we identify candidate concepts c men-tioned in each terms denition using Wiki-data (Vrandecic and Krtzsch, 2014)6. We thenorder the candidates by a score of how specicthey are to the terms domain Dt. This is measuredby the ratio of how many articles A the concept cappears in within this domain compared to acrossWikipedia generally:ADt 1[c A]ADall 1[c A](1)",
  "Experiments": "We explore the performance of existing NLP toolson targeted concept simplication and possible av-enues for future improvement. More concretely,we investigate the following research questions:RQ1: What is the performance of out-of-the-box NLP tools in this task?RQ2: Which types of simplication strategiesimprove human understanding of difcult conceptsand the denitions that they appear in?RQ3: For targeted concept simplication, howdo human evaluations compare to automatic met-rics commonly used in text simplication?We perform experiments on the WIKIDOMAINStest data created in .As an addi-tional evaluation set, we also use the scienticdenitions dataset from August et al. (2022)(SCIDEF) that contains denitions of scienceterms extracted from Wikipedia glossaries andMedQuAD (Ben Abacha and Demner-Fushman,2019). We perform the same post-processing onSCIDEF as with WIKIDOMAINS to select a difcultconcept within each denition.",
  "Models": "To explore the benchmark performance on this data,we selected four popular LLMs: GPT-4 (OpenAI,2023), PaLM-2 (Anil et al., 2023), Falcon-40b (Al-mazrouei et al., 2023), and BLOOM-170b (Big-Science Workshop, 2023). For the open-sourcemodels, we selected the instruct versions with thehighest number of parameters available.We also included a baseline approach of dictio-nary look-up (non-LLM) to compare to the LLMs.For this baseline, we looked up a denition of thedifcult concept and simply appended it to the endof the original denition. We retrieved the def-inition from Wikidata (Vrandecic and Krtzsch,2014), falling back on WordNet (Miller, 1994) ifthe term was not found in Wikidata (or stated thatthe difcult concepts denition could not be foundif both sources failed).",
  "Human Eval": "Meaning preservation (HMP)Human evaluation of whether the rewritten denition preserves the meaning of theoriginal denition (on a 5-point Likert scale; 5 = perfectly preserved).Rewrite understanding (HRU)Human evaluation of whether a reader can understand the rewritten denition if theydo not know the difcult concept (1 = yes, 0 = no).Rewrite easier (HRE)Human evaluation of whether the rewritten denition is easier to understand than theoriginal denition (1 = rewrite is easier; 0 = the original is easier or both are similar).",
  "Automatic Eval": "DensityDensity (Grusky et al., 2018) is a measure of how extractive the rewritten denitionis from the original denition.BLEU-4BLEU-4 score (Papineni et al., 2002) of the rewritten denition with respect to theoriginal denition.BERTSCORE (BertSc)BERTSCORE (Zhang* et al., 2020) of the rewritten denition with respect to theoriginal denition.Change in length (Len)Average difference between the lengths (in number of tokens) of the rewritten andthe original denition (positive means the rewritten denition is longer than theoriginal).Change in age of acquisition(AoA)Average difference of the top-10 percentile of the age-of-acquisition (Kupermanet al., 2012) of the words between the rewritten and the original denition (positivemeans the rewritten denition uses less complex words).ChangeinFleschease(Flesch)Average difference of the Flesch reading ease (Flesch, 1948) between the rewrittenand the original denition (positive means the rewritten denition is at an easierreading level than the original).",
  ": Human and automatic metrics used to evaluate LLM rewritten text for concept simplication": "strategies also correspond to familiar approachesfor general text simplication tasks that rely onelaboration (Srikanth and Li, 2021) and lexicalchanges (Paetzold and Specia, 2016), respectively.We chose two different prompts for the LLMsthat reect these two simplication strategies. Inour rst prompt, we show the model the term, de-nition, and difcult concept. We instruct the modelto rewrite the denition, integrating an explana-tion for the difcult concept (explain). The sec-ond prompt is similar, except we instruct the modelto rewrite the denition simplifying the difcultconcept word (simplify).We chose the specic wording of the promptsfor the two strategies after a small scale analysisof results with a few candidate prompts. We de-scribe the candidate prompts, and the full phrasingof the nal selected prompts in the Appendix (Sec-tion C). We report results using 3-shot settings forthe LLMs.8",
  "Human Evaluation": "We asked human raters to rate the rewritten de-nitions along dimensions of meaning preservationand ease of understanding of the rewrites with re-spect to both the difcult concept and the originaldenition. Specically, we asked them about (1)meaning preservation, denoted as HMP: how muchdoes the rewrite preserve the meaning of the orig-",
  "We also experimented with a zero-shot settings with re-sults in the Appendix": "inal denition on a Likert scale of 5; (2) rewriteunderstanding, denoted as HRU: If a reader is un-familiar with the difcult concept, would they beable to understand the rewrite (Yes/No); (3) rewriteeasier, denoted as HRE: Is the rewrite easier tounderstand than the original? These dimensionsare summarized in the rst three rows of .We obtain judgments from 3 human raters per ex-ample for 120 randomly selected examples fromthe WIKIDOMAINS dataset and 60 randomly se-lected examples from the SCIDEF dataset (2880judgments in total). We provide exact wording ofthe question, their rationale and annotator back-ground in the Appendix (Section D). In Appendix we show Krippendorffs alpha agreementscores for each human evaluation dimension.",
  "Automated Metrics": "We investigate the utility of commonly used sim-plication automated metrics for our task and com-pare them to human judgments. Because our dataare reference-less, we cannot use reference-basedmetrics like SARI (Xu et al., 2016). Instead, we es-timate changes in complexity using the differencebetween the rewritten and the original denitionin terms of: (1) age of acquisition (AoA; Kuper-man et al. 2012), (2) Flesch reading ease (Flesch,1948), and (3) token length. We also measure den-sity (Grusky et al., 2018), which scores how ex-tractive the rewritten dention is from the original.Lastly, we use BLEU (Papineni et al., 2002) and",
  "Model Rankings": "summarizes the evaluations of the rewritesbased on human judgment according to the mean-ing preservation (HMP), whether the rewrite iseasier to understand than the original (HRE), andwhether the rewrite can be understood for someoneunfamiliar with the difcult concept (HRU).We observe that no model excels in all dimen-sions, though GPT-4 performs best on average.Different models have distinct strengths; for in-stance, PaLM2 excels in meaning preservation butits rewrites are rarely easier to understand. Addi-tionally, the dictionary-lookup baseline performscomparably well to the LLM models.Weaker scores on the HRU and HRE dimensionscompared to the HMP dimension across all mod-els, indicates opportunities for future research toimprove these scores.",
  ": Pearson correlations between automated met-rics and human evaluations ( : p < 0.005, : p <0.05, : p < 0.01)": "simplify (introduced in .2). shows the comparison of the prompt strategieson human evaluation dimensions averaged acrossthe four LLMs. Human raters clearly preferredrewrites where the model was asked to explainthe difcult concept in both HRE and HRU judg-ments. On WIKIDOMAINS data, human raters alsohad a signicant preference towards the explainstrategy when judging meaning preservation (thedifference in HMP on SCIDEF was not signicant).This aligns with some of our observations fromour initial user study (), which found thathumans preferred adding more context (40%) asopposed to simpler word substitutions (23%). Thishighlights that adding elaborative details is very im-portant towards facilitating human understandingcentered around difcult concepts.",
  "Computing: Prolog is a logic programming languageassociated with articial intelligence and computationallinguistics. (linguistics) [Bloomz] [no change]": ": Examples of concept simplication behaviorfor the simplify 3-shot prompt from three domains: Eco-nomics, Biology and Computing. The difcult conceptis shown in bold at the end of the denition. Deletionsare show in red; additions in ::::blue. automated metrics correlate with either HRE or HRUwhich capture comprehension related to the targetdifcult concept. Even Flesch reading ease, whichis commonly used in text simplication setups, isnot adequate for measuring whether the rewritesare easier understood. This result calls for newmetrics, beyond aggregate similarity measures, toevaluate comprehension at the semantic level ofconcepts. In the Appendix (), we show the fullautomated metric scores for each model, whichmay be useful in characterizing some qualities ofthe outputs. For example, Bloomz and PaLM2make relatively few changes to the text (low Len),and GPT4 chose considerably easier words in therewrite (high AoA). GPT4s low meaning preser-vation rating suggests choosing easier words is notalways desirable (). However, given the lowcorrelations with human scores, we generally keepour observations about relative model rankings tothe human judgment.",
  "We close our paper by discussing the research ques-tions we posed in our experiments and how theymay relate to future improvements on this task": "Can LLMs support Contextual Explanationsof Difcult Text?Despite their instruction-following capabilities, human evaluations indicatethat theres still considerable room for improve-ment at this task. Human judgments () re-veal that no model excels universally, each havingits own strengths and weaknesses. All models tendto perform better at meaning preservation, thoughother dimensions may be more crucial for enhanc-ing broader comprehension (Kintsch, 1991).Our evaluations support prior ndings thatdictionary-based methods for simplication arelimited by availability and their inability to per-sonalize to the readers context and backgroundknowledge (August et al., 2022). showstwo examples from the dictionary baseline that areeither too vague or too complex to be useful to alay reader. However, we nd that LLMs outper-form the deterministic dictionary look-up baselineonly by a small margin depending on the dimen-sion of quality. In examples of output (),we can see failure cases where the models eitherover-simplify text beyond just the difcult conceptor make no changes to the denition at all. LLMshave been found to be useful for reading-gradelevel simplications (Agrawal and Carpuat, 2023),yet they seem to struggle with making ne-grainedsimplications at the level of difcult concepts, call-ing for more careful tooling for targeted simpli-cation. While more custom prompts may elicitdesired simplications from LLMs, we cannot ex-pect lay audience to be familiar with such prompt-ing (Zamrescu-Pereira et al., 2023). Strategies Supporting Readers in Understand-ing Difcult Text.Open-ended human feedbackabout reading difculties () as well ashuman judgment of differences between prompts(.6) support the idea that adult readersmay prefer additional details and context additionto understand difcult concepts. This echoes priorcognitive science work that cues in text (e.g., ex-planations, examples, analogies) enable readersto effectively utilize their background knowledgefor comprehension (Kintsch, 1991; Van den Broek,2010). Better Evaluations to Support Text Under-standing.As shown in prior work (Alva-Manchego et al., 2021), we nd that automatedmetrics cannot capture ne-grained differences insimplication. While there are some correlationsbetween meaning preservation and BLEU-4 andBERTSCORE, we did not observe clear correla-tions of automated metrics with other dimensionsof comprehension, such as alleviating difcultywith an unfamiliar concept. We observe that manyof these metrics rely on brittle lexical scoring (Alva-Manchego et al., 2021), and it may be necessaryfor automated metrics to take more of the underly-ing concept structure of the rewrites into accountin order to adequately judge whether the difcultconcept has been explained sufciently. A separateLLM to score these dimensions more reliably is anoption (Wang et al., 2023; Chen et al., 2023; Gaoet al., 2024); however, human judgment currentlyremains the best standard in this task.",
  "Conclusion": "To support comprehension of domain-specic textfor adult readers, we introduced the task of tar-geted concept simplication to study ne-grainedsimplication of difcult concepts in context. Ourhuman annotation study highlights the importanceof aiding users understanding of these conceptsin domain-specic texts.We also introducedWIKIDOMAINS, a dataset of 22k denitions across13 academic domains, to support this task. Ourndings show a preference for strategies that addexplanatory details over simplifying difcult con-cepts. Human evaluations of LLM rewrites indicateconsiderable room for improvement, especially forpersonalized help with difcult concepts.",
  "Limitations": "Difculty with concepts varies based on personalknowledge. Thus, it is challenging to build large-scale evaluation corpora for domain-specic con-cepts. Our dataset of domain-specic concepts is arst step, providing a foundation for future work tostudy comprehension across domains.While we used popular LLMs at the time of con-ducting the human evaluations, we also acknowl-edge that some of our LLMs may no longer bestate-of-the-art when submitting the work. How-ever, we will release our dataset and have describedour experimental setup to promote reproducibilityof the results with newer LLMs. We evaluated our work by asking human ratersto rate whether they can understand the deni-tions. However, human reading comprehensionis also goal-directed, and different reading goalswill evoke different needs for details (Dunietz et al.,2020). The details needed could differ dependingon using the text for ones own understanding or us-ing it for communicating it with other people aboutspecic aspects. E.g., a lawyer communicatingwith engineers about the risks of a technology mayneed focus on the applications rather than just theunderstanding of technical concepts. Future workcan evaluate how supporting readers with conceptsimplications in documents (e.g., explanations,examples, analogies, and illustrations) help themdevelop a better understanding of the domain inpre-post tests.",
  "Ethical Considerations": "We extract our domain-specic denitions datasetfrom Wikipedia, which is publicly available andaccessible to all. However, Wikipedia content hasa Global North bias because of its editor base, andconcepts in our domain-specic dataset will reectthis bias. We also acknowledge the broader edu-cational implications of making denitions easierto understand, and that using LLMs could intro-duce false information. While in our work we didnot observe instances of hallucinations, LLMs mayintroduce false information when rewriting entiredocuments or narratives, and we need robust mea-sures to validate the faithfulness of rewritten deni-tion in addressing concept difculty and providingcorrect facts. While our evaluations attempt toprovide initial insights into LLMs behavior withdifcult concepts in domain-specic text, we alsoacknowledge that concept difculty is a complexconstruct, and it can be dependent on a readers age,educational, and professional background, whichfuture evaluations should consider.",
  "Acknowledgements": "We thank the anonymous reviewers for their in-sightful feedback and suggestions. Wed also liketo thank Priyanka Agrawal and Slav Petrov fortheir feedback on the paper manuscript, as well asthe other members of the Google DeepMind com-munity for their guidance throughout the project.We also thank the anonymous individuals who par-ticipated in our human evaluations and rated thedenitions. Sweta Agrawal and Marine Carpuat. 2023.Control-ling pre-trained language models for grade-specictext simplication. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1280712819, Singapore. Associ-ation for Computational Linguistics. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-shamsi, Alessandro Cappelli, Ruxandra Cojocaru,Merouane Debbah, Etienne Gofnet, Daniel Hes-low, Julien Launay, Quentin Malartic, BadreddineNoune, Baptiste Pannier, and Guilherme Penedo.2023. Falcon-40B: an open large language modelwith state-of-the-art performance.",
  "Fernando Alva-Manchego, Carolina Scarton, and Lu-cia Specia. 2021. The (Un)Suitability of AutomaticEvaluation Metrics for Text Simplication. Compu-tational Linguistics, 47(4):861889": "Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-son, Dmitry Lepikhin, Alexandre Passos, SiamakShakeri, Emanuel Taropa, Paige Bailey, ZhifengChen, Eric Chu, Jonathan H. Clark, Laurent ElShafey, Yanping Huang, Kathy Meier-Hellstern,Gaurav Mishra, Erica Moreira, Mark Omernick,Kevin Robinson, Sebastian Ruder, Yi Tay, KefanXiao, Yuanzhong Xu, Yujing Zhang, Gustavo Her-nandez Abrego, Junwhan Ahn, Jacob Austin, PaulBarham, Jan Botha, James Bradbury, SiddharthaBrahma, Kevin Brooks, Michele Catasta, YongCheng, Colin Cherry, Christopher A. Choquette-Choo,Aakanksha Chowdhery,Clment Crepy,Shachi Dave, Mostafa Dehghani, Sunipa Dev, Ja-cob Devlin, Mark Daz, Nan Du, Ethan Dyer, VladFeinberg, Fangxiaoyu Feng, Vlad Fienber, MarkusFreitag, Xavier Garcia, Sebastian Gehrmann, Lu-cas Gonzalez, Guy Gur-Ari, Steven Hand, HadiHashemi, Le Hou, Joshua Howland, Andrea Hu, Jef-frey Hui, Jeremy Hurwitz, Michael Isard, Abe Itty-cheriah, Matthew Jagielski, Wenhao Jia, KathleenKenealy, Maxim Krikun, Sneha Kudugunta, ChangLan, Katherine Lee, Benjamin Lee, Eric Li, MusicLi, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim,Hanzhao Lin, Zhongtao Liu, Frederick Liu, Mar-cello Maggioni, Aroma Mahendru, Joshua Maynez,Vedant Misra, Maysam Moussalem, Zachary Nado,John Nham, Eric Ni, Andrew Nystrom, AliciaParrish, Marie Pellat, Martin Polacek, Alex Polo-zov, Reiner Pope, Siyuan Qiao, Emily Reif, BryanRichter, Parker Riley, Alex Castro Ros, Aurko Roy,Brennan Saeta, Rajkumar Samuel, Renee Shelby,Ambrose Slone, Daniel Smilkov, David R. So,Daniel Sohn, Simon Tokumine, Dasha Valter, Vi-jay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pi-dong Wang, Zirui Wang, Tao Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, LintingXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, StevenZheng, Ce Zheng, Weikang Zhou, Denny Zhou, SlavPetrov, and Yonghui Wu. 2023. Palm 2 technical re-port. Preprint, arXiv:2305.10403.",
  "Or Biran, Samuel Brody, and Nomie Elhadad. 2011": "Putting it simply: a context-aware approach to lex-ical simplication. In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages496501, Portland, Oregon, USA. Association forComputational Linguistics. Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, andRuifeng Xu. 2023. Exploring the use of large lan-guage models for reference-free text quality evalu-ation: An empirical study. In Findings of the As-sociation for Computational Linguistics: IJCNLP-AACL 2023 (Findings), pages 361374, Nusa Dua,Bali. Association for Computational Linguistics.",
  "Tim Dettmers, Mike Lewis, Younes Belkada, and LukeZettlemoyer. 2022. Llm. int8 (): 8-bit matrix mul-tiplication for transformers at scale. arXiv preprintarXiv:2208.07339": "Jesse Dunietz, Greg Burnham, Akash Bharadwaj,Owen Rambow, Jennifer Chu-Carroll, and Dave Fer-rucci. 2020. To test machine comprehension, startby dening comprehension. In Proceedings of the58th Annual Meeting of the Association for Compu-tational Linguistics, pages 78397859, Online. As-sociation for Computational Linguistics. Noemie Elhadad and Komal Sutaria. 2007. Mining alexicon of technical terms and lay equivalents. InBiological, translational, and clinical language pro-cessing, pages 4956, Prague, Czech Republic. As-sociation for Computational Linguistics. Mehwish Fatima and Michael Strube. 2023.Cross-lingual science journalism:Select, simplify andrewrite summaries for non-expert readers. In Pro-ceedings of the 61st Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 18431861, Toronto, Canada. Asso-ciation for Computational Linguistics.",
  "Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, and Xiao-jun Wan. 2024. Llm-based nlg evaluation: Currentstatus and challenges. ArXiv, abs/2402.01383": "Cristina Garbacea, Mengtian Guo, Samuel Carton, andQiaozhu Mei. 2021. Explainable prediction of textcomplexity: The missing preliminaries for text sim-plication. In Proceedings of the 59th Annual Meet-ing of the Association for Computational Linguisticsand the 11th International Joint Conference on Nat-ural Language Processing (Volume 1: Long Papers),pages 10861097, Online. Association for Computa-tional Linguistics.",
  "Max Grusky, Mor Naaman, and Yoav Artzi. 2018": "Newsroom: A dataset of 1.3 million summaries withdiverse extractive strategies. In Proceedings of the2018 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, Volume 1 (Long Pa-pers), pages 708719, New Orleans, Louisiana. As-sociation for Computational Linguistics. Yue Guo, Joseph Chee Chang, Maria Antoniak, ErinBransom, Trevor Cohen, Lucy Lu Wang, and TalAugust. 2023. Personalized jargon identication forenhanced interdisciplinary communication. ArXiv,abs/2311.09481.",
  "Isaac Johnson. 2021. Wikipedia Article Topics for AllLanguages (based on article outlinks)": "Sebastian Joseph, Kathryn Kazanas, Keziah Reina,Vishnesh Ramanathan, Wei Xu, Byron Wallace, andJunyi Jessy Li. 2023.Multilingual simplicationof medical texts. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1666216692, Singapore. Associ-ation for Computational Linguistics. David Kauchak. 2013. Improving text simplicationlanguage modeling using unsimplied text data. InProceedings of the 51st Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 15371546, Soa, Bulgaria.Association for Computational Linguistics.",
  "Cecile L. Paris. 1988. Tailoring object descriptions to ausers level of expertise. Computational Linguistics,14(3):6478": "Luz Rello, Ricardo Baeza-Yates, Stefan Bott, and Hora-cio Saggion. 2013. Simplify or help? text simplica-tion strategies for people with dyslexia. In Proceed-ings of the 10th International Cross-DisciplinaryConference on Web Accessibility, W4A 13, NewYork, NY, USA. Association for Computing Machin-ery. Neha Srikanth and Junyi Jessy Li. 2021.Elabora-tive simplication: Content addition and explana-tion generation in text simplication. In Findings ofthe Association for Computational Linguistics: ACL-IJCNLP 2021, pages 51235137, Online. Associa-tion for Computational Linguistics.",
  "AUser study for understandingdifculty with denitions": "As a preliminary study of reading difculty, weasked annotators to read 900 concept denitionsfrom WIKIDOMAINS and describe difculties theyhave in understanding the denition text.As shown in , we asked each participantthe following questions 1) Please tell us the dif-culties that you face in understanding the conceptC from the denition, 2) If you could ask a tutor to make changes to the denition to increase theknowledge and clarity of the concept for you orsomeone else, what would you ask them to change(add/edit/remove). The rst question attempts tounderstand the difculties that lay people may facewith domain specic denitions. The second ques-tion attempts to involve users in the thinking pro-cess of asking a tutor to rewrite the denition. Priorstudies in human-centered research suggest that in-volving users in the task elicits better task-specicchallenges than simply asking about the difcul-ties alone (Nielsen et al., 2002). We keep the taskopen-ended and ask for free-text responses to giveannotators freedom to express any challenges inreading the material.Following the completion of the study, two ofthe authors reviewed a random subset of 900 re-sponses (450 responses to Q1 and 450 responses toQ2). They agreed that when annotators had issueswith the reading material, it could generally t intocategories below:",
  "Editors on Wikipedia have voluntarily come to-gether to form focus groups, called WikiProjects,dedicated to curating and improving articles in": "specic domains or interest areas, such as Eco-nomics, Chemistry, Literature (Asthana and Hal-faker, 2018). Any Wikipedia editor can join differ-ent WikiProjects and participate in editing articlesin that specic WikiProject. As part of the WikiPro-ject effort, Wikipedia editors have annotated a largenumber of articles on Wikipedia with their WikiPro-ject topic assignments and developed a hierarchicaltaxonomy of topics called the Wikiprojects direc-tory9. The dataset contains articles from the entireWikipedia annotated by domains (broad academictopics) derived from Wikiprojects. We use the top-ics in the rst two levels of this categorization asdomains because they represent broad domain cat-egorizations.",
  "B.1Domain selection criteria": "We selected domains where majority of articles re-lated to names of academic concepts or processesin the domain. Thus, we needed to exclude articlesabout people, events, names of things (e.g., musicalbums). For example, the Biography domain con-tains biographical articles of famous personalities,and the Military domain contains articles on histor-ical military conicts. To identify such domains,the lead author manually assessed a random sampleof 100 articles in each domain. If the number ofarticles in each domain that corresponded to namedentities exceeded 50% of the assessed articles, wedropped that domain. This is because our workis focused on academically challenging conceptsand how they are explained in terms of other con-cepts. While articles of named entities may containchallenging concepts, the concepts themselves andtheir explanations in the domain is not the focusof the article. E.g., an article on World War II willlikely contain concepts like diplomacy, but itsexplanation will not be the main focus of the article.We nally excluded the domains: Internet-culture,Literature, Religion, History, Geography, Military-and-warfare, Transportation, Society, Sports, Li-braries and Information, Space, and STEM.STEM*(because this is a superset of the domains: Physics,Chemistry, Mathematics, Biology).",
  "B.3Difcult concept statistics": "In roughly 85% of WIKIDOMAINS examples, weextracted the difcult concept using a ratio of howoften the concept appears within this domain com-pared to Wikipedia overall (Equation 1). For thoseexamples, the average computed ratio for the se-lected difcult concept is 0.9. In the remaining 15%of examples, the difcult concept was chosen us-ing the age of acquisition lexicon (Kuperman et al.,2012). On average, each difcult concept contains1.3 tokens.",
  "CPrompts": "We experimented with 4 candidate prompts forboth explain and simplify prompts categories. outlines these candidate prompts. To iden-tify the best prompt, we applied the prompts toa set of 100 randomly sampled denitions fromthe WIKIDOMAINS and SCIDEF datasets, and thelead author manually assessed the goodness of therewrites, assigning a binary label 1 or 0 to each ofthe rewrites, indicating whether the rewrite success-fully addresses the concept difcult or not respec-tively. The prompts that we use in our experimentalsetup had the highest number of denitions wherethe rewrite was assessed as a good rewrite. details the simplify and explainprompts that we used in our study.",
  "within the denition that we identied, 3) the LLM-rewritten denitions. We ask raters to answer the": "following 1) Please rate on a scale of 1-5 howmuch the REWRITE preserves the meaning of theoriginal, 2) Can someone understand the deni-tion if they do not know the difcult concept: X?(Yes/No), 3) Please rate which of the ORIGINALand REWRITE are easier to understand? (Origi-nal/Rewrite/Both), 4) Please rate your level of fa-miliarity with the concept. Rationale for human evaluation questionsWe cannot control readers familiarity with the con-cept, therefore we rely on their understanding todetermine someones ability to understand the de-nition without knowledge of the difcult concept.How much is a denition understandable to some-one is dependent on their background knowledge.Therefore, by asking whether the REWRITE iseasier to understand or the ORIGINAL denition,we rely on the annotators opinion of whether therewritten denition gives them a better understand-ing of the topic. Each annotator was presented with about 15 def-initions to answer questions about, and the totalannotation time per annotator was about 20-25 min-utes. Before the annotation, we briefed the annota-tors about task and provided two examples to helpthem understand the task of concept simplication. shows screenshot of the annotation task. We displayed a consent form to the participantsdetailing the study and that the risks would be nomore than assessing denitions written by AI andgave them the option to leave the study at any time.We compensated the participants above the hourlyminimum wage based on their demographic loca-tion. The study was approved by the internal ethicsreview team.",
  "Prompt StrategyPrompt text": "simplifyRewrite the denition simplifying the concept: cerebellum.simplifyRewrite the denition making the concept simpler: cerebellum.simplifyRewrite the denition making the concept simpler: cerebellum.simplifyRewrite the denition simplifying difculty with the concept: cerebellum. explainRewrite the denition integrating an explanation for the concept: cerebellum.explainRewrite the denition adding an explanation for the concept: cerebellum.explainRewrite the denition providing an explanation of the concept: cerebellum.explainRewrite the denition to add content that explains the concept: cerebellum.",
  "EInference setting": "For open-source models, we run inference on GPUsusing the Huggingface10 transformers implemen-tation. To t Falcon and Bloom models on theavailable GPUs, we run the models with 8-bit quan-tization (Dettmers et al., 2022). For the commercialmodels, we use the publicly available APIs to querythe models and generate outputs. For all LLMs, weuse top-k sampling11.",
  "GHuman evaluation agreement": "shows the human evaluation agreementscores for our study. The inter-annotator alphascores show weak agreement (in the range between0.2-0.3), which is somewhat expected due to thesubjective nature of some evaluations. In aggre-gating scores we use the majority vote betweenthe three annotators (or the mean in the case ofHMP). The Krippendorffs alpha between individ-ual ratings and the majority vote falls in the rangeof 0.6-0.7, showing that individual ratings are gen-erally closely aligned with the majority rating.",
  "HMP0.310.70HRU0.210.65HRE0.250.62": ": Krippendorffs alpha scores for the humanevaluations of meaning preservation (HMP, an intervalscore out of 5), rewrite understanding (HRU, binaryscore), and rewrite easier (HRE, binary score). We re-port coefcients between pairs of annotators (IAA =inter-annotator agreement) and also the agreement be-tween individual annotation and the majority vote labelfor that example (Ann vs Majority)."
}