{
  "Abstract": "The advent of transformers has fueled progressin machine translation. More recently largelanguage models (LLMs) have come to thespotlight thanks to their generality and strongperformance in a wide range of languagetasks, including translation.Here we showthat open-source LLMs perform on par withor better than some state-of-the-art baselinesin simultaneous machine translation (SiMT)tasks, zero-shot.We also demonstrate thatinjection of minimal background information,which is easy with an LLM, brings furtherperformance gains, especially on challengingtechnical subject-matter.This highlightsLLMs potential for building next generationofmassivelymultilingual,context-awareand terminologically accurate SiMT systemsthatrequirenoresource-intensivetrain-ing or fine-tuning. The code is available at",
  "Introduction": "In simultaneous translation, the translator eithera machine or human is expected to start the trans-lation before the source sentence is finished, oftenmaking strong assumptions about the meaning ofcertain words, phrases, or the intent of the entiremessage. To produce a coherent although notnecessarily accurate translation, human simul-taneous translators routinely use a range of tech-niques, one of which is delaying the translation ofan initially ambiguous word or phrase in the hopethat its meaning will become resolved by later con-text (Ilyukhin, 2001; Chernov, 2004; Setton, 2005;Amos et al., 2022). Perhaps more importantly, hu-man translators reduce this inherent uncertainty byrelying on information from other sources, suchas presentation slides and glossaries of standardterms. This, and the fact that some people insiston using the term \"interpreter\", rather than \"transla- tor\"1, highlights a very different nature of this kindof translation.Despite significant progress in the field of offlinemachine translation, recently enabled by the wideadoption of the transformer architecture (Vaswaniet al., 2017), the practical use of SiMT systemsis still limited due to a range of unsolved prob-lems. One of these problems is that existing SiMTsystems in stark contrast to human simultane-ous translators operate on a sentence level, com-pletely disregarding the context established by pre-vious sentences, or the broader (extralinguistic)context that is implied, but not contained in the textitself. Needless to say, such context-unaware trans-lation is often logically incoherent and is prone toterminological inconsistencies, especially acrosslong discourse. The very fact that human inter-preters even the most experienced professionals routinely prepare for upcoming translation jobs bystudying relevant subject-matter, reviewing or com-piling topic-specific glossaries of terms, names, andjob titles (lvarez Prez and Prez-Luzardo Daz,2022; Gile, 1986, 1985; Chernov, 1978), suggeststhat SiMT systems should have access to additionalinformation needed to make terminologically ap-propriate and accurate translation.Motivated by LLMs strong reasoning (Yao et al., 2023; Huang et al., 2024; Huang and Chang, 2023;Zhou et al., 2024), translation (Xu et al., 2024;Zhu et al., 2024) and in-context learning (Liu et al.,2022; Wei et al., 2022; Brown et al., 2020) capa-bilities, we attempt to address one of the weak-nesses of existing SiMT systems, namely that theirtranslation takes no account of the wider contextand generally cannot respect specific terminolog-ical constraints. Different from previous studieswhich have attempted fine-tuning LLMs for SiMTtasks (Wang et al., 2023; Agostinelli et al., 2024;",
  "Following the practice established in the machine trans-lation community, in this paper we will be using the term\"simultaneous translation\"": "Koshkin et al., 2024), our focus here is on transla-tion in zero-shot mode. In the method we propose,the LLM receives a prompt that contains both thepartial input, partial translation and minimal back-ground information, and generates the next word ofthe translation. At the next step, the prompt is up-dated with the new source and the newly translatedword (see for details). We show empir-ically that such an approach outperforms some ofthe strongest bilingual SiMT baselines and showscompetitive results to a state-of-the-art multilingualSiMT system. Importantly, our approach makesit easy to insert background information (see and ), which helps the LLM to makecontextually appropriate word choices.Our key contributions are as follows: 1. We show that an off-the-shelf instruction-tuned LLM can successfully perform a SiMTtask zero-shot, without a sophisticated seg-mentation policy, with quality and latencymetrics that are competitive with (and in somecases exceeding) the state of the art. 2. We show that instruction-tuned LLMs canbe easily used for contextually-aware SiMT,and that injecting minimal background infor-mation generally improves the quality of thetranslation by a large margin.",
  ". We propose response priming, which consistsin fixing the initial part of the assistants re-sponse, and improves the LLMs zero-shotperformance on SiMT tasks": "The rest of the paper is structured as follows. In we provide an overview of recent SiMTliterature. In we describe our methodand the datasets used for evaluating our method.In we demonstrate the performance ofour approach on the different datasets and languagepairs. We conclude with a discussion of limitationsand future directions and in .",
  "Related work": "Simultaneous machine translation (SiMT) systemsstrive to balance translation quality commonlyevaluated using the BLEU metric (Papineni et al.,2002) with acceptable latency levels. This bal-ance is managed through a \"policy\" that determinesthe timing of translation actions (i.e., a WRITE ac-tion) versus the reception of additional input (i.e.,a READ action). The literature classifies these policies into two main types: fixed and adaptive(Zhang et al., 2020). Fixed policies, such as wait-k(Ma et al., 2019), apply predefined rules for exe-cuting READ and WRITE actions, regardless ofthe textual context. Initially, SiMT models em-ployed chunk-based strategies (Bangalore et al.,2012; Yarmohammadi et al., 2013; Fgen et al.,2007; Sridhar et al., 2013), where the text is di-vided into sub-sentence segments for translationwithout considering the context from precedingchunks, leading to reduced translation accuracy. Inresponse to these drawbacks, Dalvi et al. (2018)introduced an incremental decoding method. Thistechnique enhances chunk translations by integrat-ing preceding contexts via the hidden states of anRNN. Paired with straightforward segmentationtactics, their method surpassed the performance ofprior state-of-the-art systems. Meanwhile, adaptivepolicies, such as \"wait-if\" rules (Cho and Esipova,2016), allow for more flexible WRITE/READ ac-tions by considering parts of the source and/ortarget text. Adaptive policies can be developedusing separately trained agents, often employingreinforcement learning techniques (Alinejad et al.,2018; Satija and Pineau, 2016; Grissom II et al.,2014; Gu et al., 2017). These policies may initiateREAD/WRITE actions based on model attentionmechanisms (Ma et al., 2020; Arivazhagan et al.,2019; Raffel et al., 2017; Chiu and Raffel, 2018)or the stability of output predictions across n steps,a concept referred to as \"local agreement\" (Polket al., 2022; Ko et al., 2023; Liu et al., 2020a). Re-cent research has also investigated policy trainingusing binary search strategies (Guo et al., 2023)to optimize the translation quality improvementper token processed, and has conceptualized thetranslation actions as a hidden Markov transformer(Zhang and Feng, 2023), where hidden events indi-cate optimal translation output times. A promising area of research, related to thisstudy, focuses on adapting encoder-decoder trans-formers like mBART (Liu et al., 2020b), initiallydeveloped for sentence-level translation, to theSiMT task. Significant advances have been madein multilingual translation models (Fan et al., 2020;Tang et al., 2020), with some work focusing oncreating more efficient versions of large models(Mohammadshahi et al., 2022). For instance, Kanoet al. (2022); Fukuda et al. (2023) have appliedfine-tuning techniques using prefix-alignment data,while Zhang et al. (2020) have employed fine- <|begin_of_text|><|start_header_id|>system<|end_header_id|>You are a conference background information: {\"topic\": \"medicine\", \"named_entities\": [{\"entity\": \"PVC\", \"definition\": \"premature ventricular contraction\",",
  "and the newly generated target word": ": Model overview. Chunks of input audio are incrementally processed by WHISPER (1), and the recognizedwords are stored in the buffer. The prompt (2) includes special strings (shown in grey), system message (blue) withbackground information (red) to constrain the space of possible translations, and the models previous translation (ifexists). Given the prompt, the LLMs generates tokens until either a new full word or <|eot_id|> is generated (3).If a new full word is generated, a WRITE action is performed: a new source word from the word buffer and thenewly generated word (\"Vorzeitige\" in this example) are added to the prompt. If <|eot_id|> is generated, a READaction is performed: the prompt is updated only with a new source word from the buffer.",
  "tuning on \"meaningful units\", both demonstratingstrong performance across various language pairs": "More recently, large language models (LLMs)have demonstrated remarkable capabilities acrossa wide range of tasks, including offline machinetranslation (Xu et al., 2024; Zhu et al., 2024). Im-portantly, LLMs ability to learn in-context enablesa range of new capabilities, such as terminology-constrained translation (Moslem et al., 2023) andself-correction of translation errors (Feng et al.,2024). These and other developments raised thequestion whether LLMs can be leveraged for SiMT.Recent works have explored various ways to fine-tune LLMs for SiMT and showed that coupled witha segmentation policy, such as wait-k (Wang et al.,2023) or more sophisticated \"local agreement\"(Agostinelli et al., 2024), it can deliver competi-tive performance on some language pairs. Koshkinet al. (2024) proposed a policy-free approach, inwhich an LLM is fine-tuned on pairs of \"causallyaligned\" source-target sentence pairs to act as boththe translator and segmentation policy at the sametime.",
  "Online ASR": "Similarly to Koshkin et al. (2024), we follow acascaded approach, where an automatic speechrecognition (ASR) model (WHISPER (Radfordet al., 2023)) incrementally converts input au-dio chunks into text which is fed into the LLMfor translation. We found that for English inputwhisper-small.en2 achieved approximately thesame word error rate (WER) of about about 5%as whisper-large-v3, so we chose the smallerversion for faster inference. Although trained onfull sentences, WHISPER can still perform onlineASR with the following simple technique. For each READ action, a new segment of audio, lasting 200ms, is added to any previously read audio chunksand then processed by WHISPER. This windowlength was chosen empirically as a trade-off be-tween, on the one hand, the desire to minimizetranslation latency and word error rate (WER):larger windows typically are likely to result inlower WER, but tend to increase latency metrics.In our online ASR, we discard the last predictedword unless the entire source audio has been readin.Similarly to Koshkin et al. (2024), the out-put of the ASR cascade is fed into the LLM(Llama-3-70B-Instruct3). However, in an im-portant distinction from Koshkin et al. (2024), weinsert the partial target not into the \"user\", but the\"assistant\" part of the prompt (). This sim-ple modification, which we call response priming,effectively limits the space of possible sequencesthat the model can produce and prevents it fromgenerating apologies, explanatory notes or otherundesirable additions to the translation.",
  "Evaluation Data": "For the English-German language pair we usedFLEURS (Conneau et al., 2023) and TED-TST-2023 (Koshkin et al., 2024). However, it is possiblethat those test sets (or the data that they were builtfrom) were leaked into the LLMs pre-training set.For this reason we created another dataset whichwe call TED-TST-2024 similar in size and con-tent type to TED-TST-2023, but only includingtalks posted after the LLM was released.",
  "At the time of writing this paper, Meta had released the8B and 70B versions of the model, but not the correspondingpaper or technical report": "TED-TST-2024 with relevant background infor-mation (Listing 1).We generated this background information withgpt-4-turbo-2024-04-09 by prompting it withthe entire TED talk for which a given sentence wastaken (the full prompt is in Appendix A). The ideahere is to make the translation more realistic byproviding the translator (the LLM in our case) withessential information about the subject-matter athand.Finally, we test our model in a more challeng-ing scenario imitating translation of highly tech-nical subject-matter. Prior to translating complex,technical subject matter, human interpreters com-pile topic-specific glossaries, which typically listterms from the source language along with theirdefinitions and standard translations into the targetlanguage (lvarez Prez and Prez-Luzardo Daz,2022; Gile, 1986, 1985; Chernov, 1978). Thispreparatory work is crucial for effectively convey-ing technical content, as it equips interpreters withthe precise terminology and contextual knowledgeneeded to handle subject-specific nuances. Moti-vated by this, we constructed AMBIEVAL, which isa context-augmented dataset of ambiguous terms,which we describe next. First we collect a list of En-glish words (some of which are acronyms) that canhave very different meanings in different contexts.For example, depending on the context, the word\"MOS\" can mean \"metal oxide semiconductor\" andalso \"military occupational specialty\". Sometimes,the meaning of the word is disambiguated later inthe sentence. Consider the following two exam-ples:One must watch out for kicks, which are danger-ous influxes of formation fluids into the wellbore.One must watch out for kicks, while maintaininga strong defense and executing effective strikes.In these sentences, the meaning of the word\"kicks\" is disambiguated by later context, specifi-cally by the words \"influxes\" and \"strikes\". Unlessbackground information is somehow fed into themodel together with the source, it is difficult forthe SiMT model to immediately translate the word\"kicks\" accurately. We also create examples withwords whose meaning cannot be disambiguatedbased on the information contained within the sen-tence, for example:The CPA recommends holding pharmaceuticalcompanies to stricter standards of accountability.In this sentence, \"CPA\" is never disambiguated and can mean almost anything (e.g. \"ConsumerProtection Act\", \"Canadian Psychiatric Associa-tion\", \"Cerebral Palsy Alliance\"). The source au-dio of AMBIEVAL is generated by Amazons Pollytext-to-speech service.",
  "Inference": "For inference, we follow a similar approach toTRANSLLAMA (Koshkin et al., 2024), but also in-ject background information. Specifically, at timet, the target token yt is conditional on all the sourcetokens xt revealed up to time t, previously gener-ated target tokens x<t and background informationb, which is constant for sentences coming from thesame text (speech).",
  "p(yt|y<t, xt, b)(1)": "Given a prompt () consisting of a systemmessage, partial input and previously translatedpartial target, the LLM greedily generates one ormore new tokens. Once a new full word is gen-erated, a WRITE action is performed. A READaction is performed when an <|eot_id|> token isgenerated. A WRITE action involves adding thenext source word and the newly translated targetword to the prompt. In a READ action, the promptis only updated by inserting the next source wordinto the prompt. WRITE actions are only permittedafter the length of the input audio reaches a certainminimum length. This constraint controls latency-quality trade-off and indirectly the WER: highervalues of this minimum length generally improvethe quality by increasing the average number ofwords the LLM gets at the beginning of translationand decreasing the WER of the ASR4. Except forthe temperature (set to 0 for greedy generation), allthe generation parameters were left at their defaultvalues.After all the source words have been revealed,the input is no longer partial and no new words areadded to it, but the generation process continuesuntil <EOS>. We illustrate the inference process in and Algorithm 1.For fast inference, we use the vllm5 librarywhich implements a range of latest LLM perfor-mance optimizations, most importantly tensor par-allelism. Unless otherwise noted, all the results",
  "Prompt structure": "We follow a similar prompt structure as inKoshkin et al. (2024) (), except thatwe do not instruct the LLM to generate special<WAIT> tokens, but inject background informa-tion as part of the system message.For theSYSTEM_MESSAGE we used the following text: \"Youare a conference interpreter.As you translate,you can use the following background information:BACKGROUND_INFORMATION_JSON. Taking into ac-count the original SRC_LANG text, complete itstranslation into TGT_LANG. Do not add any notes orcomments to the translation.\" This system messageperformed well empirically, and we speculate thatfurther improvements are possible with differentsystem messages. We leave this question to futurework. <|begin_of_text|><|start_header_id|>system<|end_header_id|>SYSTEM_MESSAGEBACKGROUND_INFORMATION_JSONUSER_INSTRUCTION<|eot_id|><|start_header_id|>user<|end_header_id|>Context: PARTIAL_SOURCE<|eot_id|><|start_header_id|>assistant<|end_header_id|>German translation: PARTIAL_TARGET : Prompt structure. <|begin_of_text|>, <|start_header_id|>ROLE_NAME<|end_header_id|>, and<|eot_id|> are special strings used in Llama-3 to flank the system, user and assistant parts of the the prompt.",
  "Benchmarks": "In this section we compare the performance ofour method to SEAMLESSSTREAMING (Barraultet al., 2023), which is a state-of-the-art massivelymultilingual SiMT system on five language pairs(en-{de,es,fr,it,ru}) and additionally to threerecent bilingual SiMT systems, namely: NAIST(Fukuda et al., 2023), FBK (Papi et al., 2023)and TRANSLLAMA7 (Koshkin et al., 2024) onthe en-de pair.We start by examining the quality-latency trade-off on TED-TST-2024 (). Our methodperformed strongly relative to the recent baselines(although not on all language pairs).In all ofthe results presented in this section, we controlledthe translation latency by varying the minimumlength of the audio before allowing WRITE actions(OURS and TRANSLLAMA), attention threshold(SEAMLESSSTREMING and FBK) and source seg-ment size (NAIST).",
  ": Quality and latency results for our approachcompared with state-of-the-art baselines on the en-delanguage pair on AMBIEVAL": "Additional performance tests on TED-TST-2023 () and FLEURS () furtherdemonstrate the performance of our approach.Since TED-TST-2023 and TED-TST-2024 arebuilt from content intended for lay audiences, andtherefore is relatively easy to translate, we also eval-uate our method on another dataset (AMBIEVAL)which models a more challenging scenario wherethe meaning of some technical terms cannot beresolved immediately or without additional contex- BLEU en-fr OursFBKNAISTTransLLaMaSeamlessStreaming en-ru LAAL en-de en-es en-it : Dependence of translation quality (measured by BLEU) on latency (measured by LAAL) for en-{fr, ru,de, es, it} on TED-TST-2024. The latency was controlled by varying the minimum length of the audio beforeallowing WRITE actions (OURS and TRANSLLAMA), attention threshold (SEAMLESSSTREMING and FBK) andsource segment size (NAIST).",
  "Assuming whisper-large-v2 is used for ASR": "TED datasets WHISPER produces a very low WER(< 5%), these errors might still negatively impactthe translation quality. Inspection of the translatedtexts reveals that compared to a state-of-the art of-fline translation model (NLLB-200 (NLLB Teamet al., 2022)) Llama-3 is very good at correctingASR errors, for example:ASR output: I think terrorists like Hamas andhis bala are evil, and there is a bright line betweengroups that aim to kill innocence and those that tryto avoid doing so at all costs.LLM translation: Ich denke , Terroristen wieHamas und Hezbollah sind bse, und es gibt eineklare Grenze zwischen Gruppen, die unschuldigeMenschen tten wollen, und jenen, die alles tun,um dies zu vermeiden.NLLB translation: Ich denke, Terroristen wiedie Hamas und seine Bala sind bse, und es gibteine klare Linie zwischen Gruppen, die Unschuldtten wollen, und denen, die versuchen, dies umjeden Preis zu vermeiden.In the example above, two ASR errors (under-lined in the ASR output) were corrected by theLLM, but not by NLLB-200. For more examples,see Appendix B.",
  "Ablations": "Response priming. shows that removingresponse priming from the prompt results in a smallbut consistent decrease of translation quality. Thismakes sense because response priming constrainsthe space of possible sequences that the LLM cangenerate in response to the prompt. Inspectionof the translations revealed that without responsepriming the translations often begin with unwantednotes, comments and explanations resulting in de-creased quality.",
  ": Disabling response priming consistently de-creases translation quality across all the five languagepairs. The numbers are mean BLEU scores over fiveruns with different latencies on TED-TST-2024": "Background information. The removal of min-imal background information notably decreases thetranslation quality (), highlighting that theLLM can leverage even minimal information forimproved quality. Notably, the smaler version ofLLAMA-3 does not seem to benefit from addedbackground information (), which is likelydue to the fact that smaller LLMs generally haveweaker instruction-following and in-context learn-ing abilities.",
  "no31.1446.0441.7636.3829.11yes36.7649.8144.5740.2631.87": ": Removing background information from theprompt significantly and consistently decreases qualityacross the all the five language pairs. The numbersare mean BLEU scores over five runs with differentlatencies on TED-TST-2024. Smaller LLMs.Is is possible to achievecomparable performance (in terms of qual-ity) with a smaller LLM? Our tests showthat, unfortunately, Meta-Llama-3-8B-Instructsignificantly underperforms its larger version,Meta-Llama-3-70B-Instruct and seems to beunable to benefit from background information (Ta-ble 7). Inspection of the translations suggests thatthe the smaller LLM is much worse at exactly fol-lowing the instruction to only output the translationand nothing else.",
  "Limitations and Future Directions": "Prior work has demonstrated that fine-tuning ona small dataset is sufficient to enable an LLMto perform the challenging task of simultaneoustranslation. However, these existing approachesare potentially limited to one language pair, in-volve constructing a specialized dataset and a non-trivial search for optimal fine-tuning hyperparam-eters. Here we demonstrate the an off-the-shelfinstruction-tuned LLM performs strongly zero-shoton several different datasets and, crucially, can",
  ": A smaller LLM performs significantly worsethan the default 70B version. Results are shown for theTED-TST-2024 dataset": "leverage additional information for improved qual-ity and/or adherence to a predefined list of technicalterms, which is important in translating technicalmaterial.In the future, as stronger and more lightweightmodels become available, the LLM can analyzeits own translations and/or summarize source sen-tences or paragraphs. These summaries could beadded to a vector store or a graph database andretrieved in real time to augment the translation offuture sentences.The big performance gap between the 8B and70B version of LLAMA-3 suggests that even bettertranslation quality could be achieved with largerclosed-source models (such as GPT-4 or CLAUDE)if their APIs allowed response priming.One practical limitation of our approach is thatcurrently, to the best of our knowledge, it cannotbe used with strong closed-source models that areavailable through API. Perhaps as a countermea-sure against model jailbreaking, the APIs throughwhich these instruction-tuned models (e.g. GPT-4,Claude and Gemini) can be accessed enforce a rigidprompt structure that is incompatible with responsepriming specifying a user-specified prefix for the(assistant) models response which is at the coreof our approach.Another significant bottleneck in our LLM-based simultaneous translation system is that itrelies on a separate ASR system that was not de-signed for online operation.Although in gen-eral this cascaded setup works well, hallucinationssometimes occur, especially in low-latency regimeswhen in response to initial silence WHISPER out-puts words that were never said in the audio. Webelieve this limitation can be addressed by imple-menting an end-to-end SiMT system, in which the output embeddings of an ASR system or speechencoder would be directly projected into the LLMsinput embedding space, bypassing a text represen-tation and improving the systems latency overall.In fact, there is already some work in this direction,e.g. by Fathullah et al. (2024) and Huang et al.(2023).It is interesting to explore other ways to improvethe performance and efficiency of our method, suchas local agreement (Polk et al., 2022), efficientweight quantization (e.g. awq (Lin et al., 2024)),and more sophisticated prompting strategies.",
  "The first author acknowledges financial supportfrom KAKENHI grant JP23KJ2131 and Google": "Victor Agostinelli, Max Wild, Matthew Raffel, KaziFuad, and Lizhong Chen. 2024.Simul-LLM: Aframework for exploring high-quality simultaneoustranslation with large language models. In Proceed-ings of the 62nd Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 1053010541, Bangkok, Thailand. As-sociation for Computational Linguistics. Ashkan Alinejad, Maryam Siahbani, and Anoop Sarkar.2018. Prediction improves simultaneous neural ma-chine translation. In Proceedings of the 2018 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 30223027, Brussels, Belgium.Association for Computational Linguistics. Beneharo lvarez Prez and Jessica Mara Prez-Luzardo Daz. 2022. Interpreter preparation in theinterpreting classroom environment. a study on theusefulness of terminological glossaries. InterpretersNewsletter.",
  "Srinivas Bangalore, Vivek Kumar Rangarajan Sridhar,Prakash Kolan, Ladan Golipour, and Aura Jimenez": "2012. Real-time incremental speech-to-speech trans-lation of dialogs. In Proceedings of the 2012 Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 437445. Loc Barrault, Yu-An Chung, Mariano Coria Megli-oli, David Dale, Ning Dong, Mark Duppenthaler,Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar,Justin Haaheim, et al. 2023. Seamless: Multilingualexpressive and streaming speech translation. arXivpreprint arXiv:2312.05187. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Chen Chen, Yuchen Hu, Chao-Han Huck Yang,Sabato Marco Siniscalchi, Pin-Yu Chen, and Eng-Siong Chng. 2023. Hyporadise: An open baselinefor generative speech recognition with large languagemodels. In Advances in Neural Information Process-ing Systems, volume 36, pages 3166531688. CurranAssociates, Inc.",
  "Kyunghyun Cho and Masha Esipova. 2016. Can neu-ral machine translation do simultaneous translation?arXiv preprint arXiv:1606.02012": "Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang,Vera Axelrod, Siddharth Dalmia, Jason Riesa, ClaraRivera, and Ankur Bapna. 2023. Fleurs: Few-shotlearning evaluation of universal representations ofspeech. In 2022 IEEE Spoken Language TechnologyWorkshop (SLT), pages 798805. IEEE. Fahim Dalvi, Nadir Durrani, Hassan Sajjad, and StephanVogel. 2018.Incremental decoding and trainingmethods for simultaneous translation in neural ma-chine translation. In Proceedings of the 2018 Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, Volume 2 (Short Papers), pages493499, New Orleans, Louisiana. Association forComputational Linguistics.",
  "Angela Fan, Shruti Bhosale, Holger Schwenk, ZhiyiMa, Ahmed El-Kishky, Siddharth Goyal, Man-deep Baines, Onur elebi, Guillaume Wenzek,": "Vishrav Chaudhary, Naman Goyal, Tom Birch, Vi-taliy Liptchinsky, Sergey Edunov, Edouard Grave,Michael Auli, and Armand Joulin. 2020. Beyondenglish-centric multilingual machine translation. J.Mach. Learn. Res., 22:107:1107:48. Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Jun-teng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, WenhanXiong, Jay Mahadeokar, Ozlem Kalinli, ChristianFuegen, and Mike Seltzer. 2024. Prompting largelanguage models with speech recognition abilities.In ICASSP 2024 - 2024 IEEE International Confer-ence on Acoustics, Speech and Signal Processing(ICASSP), pages 1335113355.",
  "Daniel Gile. 1986. Le travail terminologique en inter-prtation de confrence": "Alvin Grissom II, He He, Jordan Boyd-Graber, JohnMorgan, and Hal Daum III. 2014. Dont until thefinal verb wait: Reinforcement learning for simul-taneous machine translation. In Proceedings of the2014 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 13421352,Doha, Qatar. Association for Computational Linguis-tics. Jiatao Gu, Graham Neubig, Kyunghyun Cho, and Vic-tor O.K. Li. 2017. Learning to translate in real-timewith neural machine translation. In Proceedings ofthe 15th Conference of the European Chapter of theAssociation for Computational Linguistics: Volume1, Long Papers, pages 10531062, Valencia, Spain.Association for Computational Linguistics.",
  "Shoutao Guo, Shaolei Zhang, and Yang Feng. 2023": "Learning optimal policy for simultaneous machinetranslation via binary search. In Proceedings of the61st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages23182333, Toronto, Canada. Association for Com-putational Linguistics. Yuchen Hu, Chen Chen, Chengwei Qin, Qiushi Zhu,Eng Siong Chng, and Ruizhe Li. 2024. Listen againand choose the right answer: A new paradigm forautomatic speech recognition with large languagemodels. In Annual Meeting of the Association forComputational Linguistics. Jie Huang and Kevin Chen-Chuan Chang. 2023. To-wards reasoning in large language models: A survey.In Findings of the Association for ComputationalLinguistics: ACL 2023, pages 10491065, Toronto,Canada. Association for Computational Linguistics. JieHuang,XinyunChen,SwaroopMishra,Huaixiu Steven Zheng, Adams Wei Yu, Xiny-ing Song, and Denny Zhou. 2024. Large languagemodels cannot self-correct reasoning yet. In TheTwelfth International Conference on LearningRepresentations.",
  "Zhichao Huang, Rong Ye, Tom Ko, Qianqian Dong,Shanbo Cheng, Mingxuan Wang, and Hang Li. 2023.Speech translation with large language models: Anindustrial practice. ArXiv, abs/2312.13585": "Vladimir Mikhailovich Ilyukhin. 2001. Strategies inSimultaneous Interpreting: Based on the Materialof English-Russian and Russian-English Combina-tions. Candidate of philological sciences disserta-tion, Moscow.Specialty 10.02.20: Comparative-Historical, Typological, and Comparative Linguis-tics. Yasumasa Kano, Katsuhito Sudoh, and Satoshi Naka-mura. 2022. Simultaneous neural machine transla-tion with prefix alignment. In Proceedings of the19th International Conference on Spoken LanguageTranslation (IWSLT 2022), pages 2231, Dublin, Ire-land (in-person and online). Association for Compu-tational Linguistics. Yuka Ko, Ryo Fukuda, Yuta Nishikawa, YasumasaKano, Katsuhito Sudoh, and Satoshi Nakamura. 2023.Tagged end-to-end simultaneous speech translationtraining using simultaneous interpretation data. InProceedings of the 20th International Conference onSpoken Language Translation (IWSLT 2023), pages363375, Toronto, Canada (in-person and online).Association for Computational Linguistics.",
  "Danni Liu, Gerasimos Spanakis, and Jan Niehues.2020a. Low-Latency Sequence-to-Sequence SpeechRecognition and Translation by Partial HypothesisSelection. In Proc. Interspeech 2020, pages 36203624": "Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,Lawrence Carin, and Weizhu Chen. 2022.Whatmakes good in-context examples for GPT-3?InProceedings of Deep Learning Inside Out (DeeLIO2022): The 3rd Workshop on Knowledge Extrac-tion and Integration for Deep Learning Architectures,pages 100114, Dublin, Ireland and Online. Associa-tion for Computational Linguistics. Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, SergeyEdunov, Marjan Ghazvininejad, Mike Lewis, andLuke Zettlemoyer. 2020b. Multilingual denoisingpre-training for neural machine translation. Transac-tions of the Association for Computational Linguis-tics, 8:726742. Mingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng,Kaibo Liu, Baigong Zheng, Chuanqiang Zhang,Zhongjun He, Hairong Liu, Xing Li, Hua Wu, andHaifeng Wang. 2019. STACL: Simultaneous trans-lation with implicit anticipation and controllable la-tency using prefix-to-prefix framework. In Proceed-ings of the 57th Annual Meeting of the Association forComputational Linguistics, pages 30253036, Flo-rence, Italy. Association for Computational Linguis-tics.",
  "Xutai Ma, Juan Miguel Pino, James Cross, Liezl Puzon,and Jiatao Gu. 2020. Monotonic multihead attention.In International Conference on Learning Representa-tions": "Alireza Mohammadshahi, Vassilina Nikoulina, Alexan-dre Berard, Caroline Brun, James Henderson, andLaurent Besacier. 2022. SMaLL-100: Introducingshallow multilingual machine translation model forlow-resource languages. In Proceedings of the 2022Conference on Empirical Methods in Natural Lan-guage Processing, pages 83488359, Abu Dhabi,United Arab Emirates. Association for Computa-tional Linguistics. Yasmin Moslem, Rejwanul Haque, John D. Kelleher,and Andy Way. 2023. Adaptive machine translationwith large language models. In Proceedings of the24th Annual Conference of the European Associationfor Machine Translation, pages 227237, Tampere,Finland. European Association for Machine Transla-tion. Team NLLB Team, Marta R. Costa-juss, James Cross,Onur elebi, Maha Elbayad, Kenneth Heafield,Kevin Heffernan, Elahe Kalbassi, Janice Lam,Daniel Licht, Jean Maillard, Anna Sun, SkylerWang, Guillaume Wenzek, Al Youngblood, BapiAkula, Loic Barrault, Gabriel Mejia Gonzalez,Prangthip Hansanti, John Hoffman, Semarley Jar-rett, Kaushik Ram Sadagopan, Dirk Rowe, ShannonSpruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan,Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmn, PhilippKoehn, Alexandre Mourachko, Christophe Ropers,Safiyyah Saleem, Holger Schwenk, and Jeff Wang.2022.No language left behind: Scaling human-centered machine translation. Sara Papi, Marco Gaido, Matteo Negri, and MarcoTurchi. 2022. Over-generation cannot be rewarded:Length-adaptive average lagging for simultaneousspeech translation. In Proceedings of the Third Work-shop on Automatic Simultaneous Translation, pages1217, Online. Association for Computational Lin-guistics. Sara Papi, Matteo Negri, and Marco Turchi. 2023. At-tention as a guide for simultaneous speech translation.In Proceedings of the 61st Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 1334013356, Toronto, Canada.Association for Computational Linguistics. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalu-ation of machine translation. In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics, pages 311318, Philadelphia,Pennsylvania, USA. Association for ComputationalLinguistics. Peter Polk, Ngoc-Quan Pham, Tuan Nam Nguyen,Danni Liu, Carlos Mullov, Jan Niehues, Ondrej Bo-jar, and Alexander Waibel. 2022. CUNI-KIT systemfor simultaneous speech translation task at IWSLT2022. In Proceedings of the 19th International Con-ference on Spoken Language Translation (IWSLT2022), pages 277285, Dublin, Ireland (in-personand online). Association for Computational Linguis-tics. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brock-man, Christine McLeavey, and Ilya Sutskever. 2023.Robust speech recognition via large-scale weak su-pervision. In International Conference on MachineLearning, pages 2849228518. PMLR. Colin Raffel, Minh-Thang Luong, Peter J. Liu, Ron J.Weiss, and Douglas Eck. 2017. Online and linear-time attention by enforcing monotonic alignments.In Proceedings of the 34th International Conferenceon Machine Learning - Volume 70, ICML17, page28372846. JMLR.org.",
  "Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 230238": "Y. Tang, C. Tran, Xian Li, Peng-Jen Chen, NamanGoyal, Vishrav Chaudhary, Jiatao Gu, and AngelaFan. 2020.Multilingual translation with extensi-ble multilingual pretraining and finetuning. ArXiv,abs/2008.00401. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. In Advances in Neural Information Pro-cessing Systems, volume 30. Curran Associates, Inc.",
  "Minghan Wang, Jinming Zhao, Thuy-Trang Vu, Fate-meh Shiri, Ehsan Shareghi, and Gholamreza Haffari.2023. Simultaneous machine translation with largelanguage models. arXiv preprint arXiv:2309.06706": "Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in neuralinformation processing systems, 35:2482424837. Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Has-san Awadalla. 2024. A paradigm shift in machinetranslation: Boosting translation performance oflarge language models. In The Twelfth InternationalConference on Learning Representations. Chao-Han Huck Yang, Yile Gu, Yi-Chieh Liu, ShaliniGhosh, Ivan Bulyko, and Andreas Stolcke. 2023.Generative speech recognition error correction withlarge language models and task-activating prompt-ing. In 2023 IEEE Automatic Speech Recognitionand Understanding Workshop (ASRU). IEEE. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,Tom Griffiths, Yuan Cao, and Karthik Narasimhan.2023. Tree of thoughts: Deliberate problem solvingwith large language models. In Advances in NeuralInformation Processing Systems, volume 36, pages1180911822. Curran Associates, Inc. Mahsa Yarmohammadi, Vivek Kumar Rangarajan Srid-har, Srinivas Bangalore, and Baskaran Sankaran.2013. Incremental segmentation and decoding strate-gies for simultaneous translation. In Proceedings ofthe Sixth International Joint Conference on NaturalLanguage Processing, pages 10321036. Ruiqing Zhang, Chuanqiang Zhang, Zhongjun He, HuaWu, and Haifeng Wang. 2020. Learning adaptivesegmentation policy for simultaneous translation. InProceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 22802289, Online. Association for Computa-tional Linguistics.",
  "Shaolei Zhang and Yang Feng. 2023. Hidden markovtransformer for simultaneous machine translation. InInternational Conference on Learning Representa-tions": "Pei Zhou, Jay Pujara, Xiang Ren, Xinyun Chen,Heng-Tze Cheng, Quoc V. Le, Ed Huai hsin Chi,Denny Zhou, Swaroop Mishra, and Huaixiu StevenZheng. 2024.Self-discover:Large languagemodels self-compose reasoning structures. ArXiv,abs/2402.03620. Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu,Shujian Huang, Lingpeng Kong, Jiajun Chen, andLei Li. 2024. Multilingual machine translation withlarge language models: Empirical results and anal-ysis. In Findings of the Association for Computa-tional Linguistics: NAACL 2024, pages 27652781,Mexico City, Mexico. Association for ComputationalLinguistics.",
  "Example 1": "ASR output: Our mistaken belief that the bestleaders are those who generate the most noise, ac-tion, and sensational activity in the most traumaticcircumstances.LLM translation: Unsere irrige Annahme, dassdie besten Fhrer jene sind, die den meisten Lrm,die meisten Aktionen und die spektakulrsten Ak-tivitten in den dramatischsten Umstnden erzeu-gen.Offline MT (NLLB): Unsere falsche berzeu-gung, dass die besten Fhrungskrfte sind, die diemeisten Lrm, Aktion und sensationelle Aktivittin den traumatischsten Umstnden erzeugen.",
  "Example 3": "ASR output: If Shackleton is the swimmer whorushes recklessly into the water without understand-ing the currents or its own capabilities, amundsenis the swimmer who has spent a lifetime, humblystudying the river before entering the water in justthe right spot, at just the right time and so makes itlook easy.LLM translation:Wenn Shackleton derSchwimmer ist, der sich ohne vorheriges Ver-stndnis der Strmungen oder seiner eigenenFhigkeiten tollkhn ins Wasser strzt, dann istAmundsen der Schwimmer, der ein Leben langbescheiden den Fluss studiert hat, bevor er genauam richtigen Punkt und zur richtigen Zeit insWasser geht und es dadurch leicht aussehen lsst.",
  "CExamples of translation": "English source: I would like to invite you on a littlethought experiment.German translation: Ich mchte Sie einladen zueiner kleinen Gedankenbung.Russian translation:hotelbypri-glasit vas na malenki myslennyksperiment. Italian translation: Vorrei invitarti a una piccolariflessione immaginativa.French translation: Je voudrais vous inviter une petite exprience de pense.Spanish translation: Me gustara invitarle a unapequea experiencia de pensamiento. English source: Lets pretend that were goingon a polar expedition together, all of you and me,and we need to hire a captain.German translation: Lassen Sie uns so tun, alswrden wir gemeinsam eine Polarexpedition un-ternehmen, alle zusammen, Sie und ich, und wirmssen einen Kapitn einstellen.Russian translation: Davate predstavim,qto my otpravlems v polrnu ks-pedici vmeste, vse vy i , i nam nunonant kapitana.Italian translation: Immaginiamo di essere inuna spedizione polare insieme, tutti voi e io, e dob-biamo assumere un capitano.French translation: Imaginons que nous partionsensemble pour une expdition polaire, vous et moi,et que nous devions embaucher un capitaine.Spanish translation: Imaginemos que estamosembarcndonos en una expedicin polar juntos,todos ustedes y yo, y necesitamos contratar a uncapitn. English source: One comes from a man who hasalready successfully achieved all four of the majorpolar goals, the North Pole and the South Pole, andthe Northeast and the Northwest Passage.German translation: Einer stammt von einemMann, der bereits alles vier erfolgreich erreicht hat,was die groen polaren Ziele betrifft, den Nord-pol und den Sdpol sowie die Nordost- und dieNordwestpassage.Russian translation: Odin ot muqiny, ko-tory ue uspexno dostig vse qetyrehpolrnyhcele:Severnogopolsainogopolsa,atakeSevero-vostoqnogoiSevero-Zapadnogopro-hodov.Italian translation: Uno proviene da un uomoche ha gi conseguito con successo tutti e quattrogli obiettivi polari maggiori, il Polo Nord e il PoloSud e il Passaggio del Nordest e il Passaggio delNordovest.French translation: Lun vient de lhomme qui a dj russi tout accomplir dans les quatre grandsobjectifs polaires, le Ple Nord et le Ple Sud, ainsique le Passage du Nord-Est et le Passage du Nord-Ouest.Spanish translation: Uno proviene de un hombreque ya ha logrado con xito los cuatro objetivospolares principales, el Polo Norte y el Polo Sur , ascomo el Paso del Noreste y el Paso del Noroeste. English source: In fact, three of these, he wasthe first person to accomplish.German translation: Tatschlich drei von ihnenwar er der Erste, der dies erreicht hat.Russian translation: Faktiqeski, tri iznih on byl pervym qelovekom, kotoryto soverxil.Italian translation: In realt, tre di questi, fu laprima persona a realizzare.French translation: En ralit, trois dentre eux,il fut le premier accomplir.Spanish translation: De hecho, tres de ellos, fuela primera persona en lograr. English source: Candidate B is a man who setoff for the Antarctic four times, three times as theman in charge, and every time resulted in failure,catastrophe, or death.German translation: Kandidat B ist ein Mann,der aufbrach, um den Sdpol viermal zu erreichen,drei Mal landete er als Leiter und jedes Mal endetees in Misserfolg, Katastrophe oder Tod.Russian translation: Kandidat B - muqina, kotory otpravils v put k Antark-tike qetyre raza, tri raza to byl on, kto rukovodil, i kady raz to za-kanqivalos neudaqe, katastrofo ilismert.Italian translation: Candidato B un uomo chepart per lAntartico quattro volte, tre delle quali fuluomo al comando, e ogni volta il risultato fu unfallimento, una catastrofe o la morte.French translation: Candidat B est un homme quia entrepris une expdition vers lAntarctique qua-tre reprises, trois fois il tait la tte de lexpdition,et chaque fois cela sest sold par un chec, unecatastrophe ou la mort.Spanish translation: El candidato B es un hom-bre que parti hacia la Antrtida cuatro veces, tresveces como hombre a cargo, y cada vez result enun fracaso, una catstrofe o la muerte. English source: But in reality, we often trickourselves into hiring Candidate B or someone likehim.German translation: Aber, in Wirklichkeit, tunwir uns oft selbst einen Gefallen, indem wir Kandi-dat B oder jemanden wie ihn einstellen.Russian translation: No, na samom dele, myqasto obmanyvaem samih seb, nanimakandidata B ili kogo-to vrode nego.Italian translation: Ma, in realt, spesso inganni-amo noi stessi nellassumere candidati come B oqualcuno simile a lui.French translation:Mais, en ralit, noustrompons souvent nous-mmes en embauchant lecandidat B ou quelquun de semblable.Spanish translation: Pero, en realidad, a menudonos engaamos al contratar al candidato B o a al-guien como l. English source: Meanwhile, Candidate A, theNorwegian Roald Amundsen, by any metric, themost successful polar explorer to have ever lived,has been largely forgotten.German translation: Inzwischen, Kandidat A,der Norweger, hnlich wie Amundsen, nach jederMesslatte, der erfolgreichste Polarforscher , der jegelebt hat, wurde grtenteils vergessen.Russian translation:Medutem,kan-didat A, norveec Roald Amundsen,po lbomu kriteri, samy uspexnypolrnyissledovatel,kogda-libosuwestvovavxi , byl v znaqitelnostepeni zabyt.Italian translation: Nel frattempo, il candidatoA, il norvegese Roald Amundsen, secondo ogniparametro, il pi grande esploratore polare di tuttii tempi, stato largamente dimenticato.French translation: Pendant ce temps, le candi-dat A, le Norvgien Roald Amundsen, selon tousles critres, lexplorateur polaire le plus russi detous les temps, est largement tomb dans loubli.Spanish translation: Mientras tanto, el candidatoA, el noruego Amundsen, segn cualquier mtrica,el explorador polar ms exitoso que haya vividojams, ha sido en gran medida olvidado.",
  "English source: I did a quick search in my uni-versitys library catalog before this talk, and I foundno fewer than 26 books that celebrate Shackletonsleadership qualities": "German translation:Ich habe eine schnelleSuche im Bibliothekskatalog meiner Universittdurchgefhrt, bevor ich hierher kam, und fandnicht weniger als 26 Bcher, die ShackletonsFhrungsqualitten feiern.Russian translation: bystro poiskal vbiblioteke naxego universiteta peredtim dokladom, i naxel ni menxe ,qem 26 knig, kotorye proslavlt lid-erstvo Xekltona.Italian translation: Ho fatto una ricerca rapidanel catalogo della biblioteca universitaria prima diquesto intervento e ho trovato non meno di 26 libriche celebrano le qualit di leadership di Shackle-ton.French translation: Jai fait une recherche rapidedans le catalogue de la bibliothque de mon univer-sit avant cette confrence, et jai trouv pas moinsde 26 livres qui clbrent les qualits de leadershipde Shackleton.Spanish translation: Hice una bsqueda rpidaen el catlogo de la biblioteca de mi universidadantes de esta charla y encontr no menos de 26libros que celebran las cualidades de liderazgo deShackleton."
}