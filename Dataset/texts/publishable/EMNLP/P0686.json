{
  "Abstract": "We explore multi-step reasoning in vision-language models (VLMs). The problem is chal-lenging, as reasoning data consisting of mul-tiple steps of visual and language processingare barely available. To overcome the chal-lenge, we first introduce a least-to-most visualreasoning paradigm, which interleaves stepsof decomposing a question into sub-questionsand invoking external tools for resolving sub-questions. Based on the paradigm, we furtherpropose a novel data synthesis approach thatcan automatically create questions and multi-step reasoning paths for an image in a bottom-up manner. Our approach divides the complexsynthesis task into a few simple sub-tasks, and(almost entirely) relies on open-sourced mod-els to accomplish the sub-tasks. Therefore, theentire synthesis process is reproducible andcost-efficient, and the synthesized data is qual-ity guaranteed. With the approach, we con-struct 50k visual reasoning examples. Then, wedevelop a visual reasoner through supervisedfine-tuning, which is capable of generally en-hancing the reasoning abilities of a wide rangeof existing VLMs in a plug-and-play fashion.Extensive experiments indicate that the visualreasoner can consistently and significantly im-prove four VLMs on four VQA benchmarks.Our code and dataset are available at",
  "* Equal Contribution. Corresponding Authors": ": Top Left: An example from TextVQA (Singhet al., 2019) with 2010 as the ground truth. MiddleLeft: Response from LLaVA-NeXT-13B (Liu et al.,2024a). Bottom Left: Response from GPT-4o withthe prompt {question} please think step by step andanswer the question. Right: Response given by theproposed method, which is also the only correct answer. in the development of large multimodal models(LMMs) (Alayrac et al., 2022; Liu et al., 2024b;Team et al., 2023; Dai et al., 2024; Bai et al., 2023;Ormazabal et al., 2024). By adopting a pre-trainingand instruction tuning paradigm, representations ofmultiple modalities are effectively fused in deeparchitectures, bringing substantial advancementsin tasks, such as image captioning (Young et al.,2014), visual question answering (VQA) (Goyalet al., 2017), and optical character recognition(OCR) (Mishra et al., 2019), among others.In this work, we explore vision language models(VLMs) as a typical example of LMMs. Despitethe rapid progress, state-of-the-art VLMs still facechallenges in reasoning over visual content. Asexemplified in , intuitively, the questioncan be correctly solved following a least-to-mostparadigm (Zhou et al., 2022), in which the ques- tion is decomposed into a series of sub-questions,and an answer is deduced by resolving the sub-questions step by step. However, existing VLMsare inept at performing such multi-step reasoningbecause (1) Multi-step reasoning paths, like the oneshown in (right), are rarely included inthe training data (Dai et al., 2024)1. The VLMshave few opportunities to develop the reasoningcapability from the subsequent post-training. And(2) different from reasoning over text, solving ques-tions in a vision-language context may require ma-nipulating the input image (e.g., marking a specificarea) and deducting the next steps from both textualand visual intermediate results. The requirement,however, is difficult to accomplish for most VLMs,whether open-sourced or proprietary.We present least-to-most visual reasoning, a gen-eral paradigm to guide VLMs to decompose a givenquestion into sub-questions and invoke tools to re-solve each sub-question for handling diverse visualreasoning tasks. While there have been extensivestudies for LLMs regarding chain-of-thought rea-soning (Wei et al., 2022b; Yao et al., 2022, 2024;Wang et al., 2022) and tool-invoking (Schick et al.,2024; Qin et al., 2023b), the techniques are less ex-plored in the context of VLMs. Since data scarcityis a major obstacle, we propose a novel data syn-thesis approach, dubbed least-to-most synthesis,to automatically generate a (question, reasoningpath) tuple for a given image in a bottom-up man-ner. Specifically, the pipeline of least-to-most syn-thesis consists of four steps: (1) Entity Recognition:recognizing all entities in an image; (2) Node Con-struction: constructing three types of nodes, eachaggregating an image with a few entities and sometextual features; (3) Reasoning Process Synthesis:synthesizing a reasoning path from a sampled chainof nodes. Based on the nodes, the reasoning path isformed by connecting a sequence of sub-questionsand tool arguments generated by an LLM; and (4)Question Synthesis: generating the main questionby recursively combining the sub-questions in thereasoning path through an LLM. Our approach(almost entirely) relies on open-sourced models.Therefore, it offers several advantages, includingcost-efficiency, reproducibility, and ensured dataquality, over the common practice where data are",
  "A few datasets, such as A-OKVQA (Schwenk et al.,": "2022), VCR (Zellers et al., 2019), and ScienceQA (Lu et al.,2022), contain rationales. However, the rationales merely pro-vide explanations for the answers and thus differ significantlyfrom the multi-step reasoning data we study in the work. obtained by querying powerful proprietary LMMslike GPT-4V (Qi et al., 2024; Li et al., 2024).Based on least-to-most synthesis, we build alarge scale VIsual REasOning dataset (VIREO)with 50k examples, and tailor LLaVA-1.5-7B (Liuet al., 2023a) as a visual reasoner through super-vised fine-tuning on VIREO. The reasoner canbe generally applied to off-the-shelf VLMs in aplug-and-play fashion to enhance their reasoningcapabilities. We conduct experiments with fourrepresentative VLMs as showcases. Evaluation re-sults across four VQA benchmarks indicate that thereasoner can consistently improve all VLMs overall tasks, with absolute performance gains rangingfrom 0.71% to 39%.Our contributions are three-fold:I. We introduce the least-to-most visual reasoningparadigm to synergize question-decomposition andtool-invoking in VLMs for solving complex vision-language tasks.II. We propose least-to-most synthesis, a repro-ducible, cost-efficient, and data quality-assured al-gorithm for automatically creating multi-step visualreasoning data (almost) using open-source models.III. We use least-to-most synthesis to construct theVIREO dataset of 50k examples for fine-tuning areasoner model2. Extensive experiments illustratethat the reasoner can consistently and significantlyenhance existing VLMs in a plug-and-play fashionacross five VQA benchmarks.",
  "Vision-Language Models": "Building LMMs aims to enable foundation modelsto seamlessly handle multimodal signals, such aslanguage, vision, and audio. Among the efforts, sig-nificant attention has been focused on jointly mod-eling vision and language, known as VLMs. Recentwork on VLMs can be broadly categorized into twogroups, according to how visual information is in-corporated into the models. The first line integratesvisual information into LLMs via a vision-languageconnector. For example, LLaVA series (Liu et al.,2023b,a) exploit a linear transformation or an MLPto transform outputs from a vision encoder into in-puts of a language model. BLIP-2 (Li et al., 2023a)and InstructBLIP (Dai et al., 2024) rely on QueryTransformers to achieve vision-language alignment.",
  "We have released a larger version of VIREO comprising500k examples to spur research in the visual reasoning area": "Similarly, Qwen-VL (Bai et al., 2023) and mPLUG-Owl (Ye et al., 2023) use learnable tokens to takevisual information into account. CogVLM (Wanget al., 2023a) maps vision embedding to the spaceof word embedding by an MLP adapter, and thenenables deep vision-language feature alignment viaa visual expert module. The second group strivesto train VLMs natively from image tokens andtextual tokens. Among the representative models,Flamingo (Alayrac et al., 2022) inserts gated cross-attention dense blocks into a pre-trained LM. BEIT-3 (Huang et al., 2024) utilizes Multiway Transform-ers as the backbone to encode various modalities.KOSMOS-1 (Huang et al., 2024) and KOSMOS-2 (Peng et al., 2023) interleave image tokens withtextual tokens in the input sequence through a de-signed format. Gemini family (Team et al., 2023;Reid et al., 2024) take multimodal signals as inputand can natively generate images using discreteimage tokens. Instead of building a new VLM, wetarget enhancing the multi-step reasoning abilityof existing VLMs. Our data synthesis approachenables us to develop a visual reasoner that canbe generally applied to various VLMs, leading toconsistent improvements over various VQA tasks.",
  "Reasoning and Tool Use": "Reasoning is an important emergent ability ofLLMs (Wei et al., 2022a). With appropriate exem-plars or prompts, LLMs can demonstrate chain-of-thought (CoT) behavior and solve problemsthrough multi-step reasoning (Wei et al., 2022b;Kojima et al., 2022). Encouraged by the observa-tion, significant efforts have been made to improvethe reasoning capabilities of LLMs. For example,Zhou et al. (2022) propose least-to-most promptingfor complex reasoning; Yao et al. (2022) extendthe ability of LLMs by interleaving reasoning andacting. Wen et al. (2024) generate code-form plansbefore low-level reasoning. In addition to innova-tions in methodology, reasoning abilities have alsoproven effective in various applications, such as ta-ble understanding (Wang et al., 2023c), math prob-lem solving (Wei et al., 2022b), question answer-ing (Guan et al., 2024), and decision making (Yaoet al., 2022). Very recently, the research communitybegins to investigate the problem in multimodalmodels (Qi et al., 2024; Yang et al., 2023; Wanget al., 2024). Different from existing work, we fo-cus on data synthesis with open-sourced models,and develop a plug-an-play visual reasoner.Our work also relates to the efforts on facili- tating LLMs to leverage tools (Qin et al., 2023b;Schick et al., 2024; Shen et al., 2024; Qin et al.,2023a). The difference is that our tools are spe-cially selected for visual processing, with the goalof enhancing the reasoning capabilities of VLMs.",
  "Task Decomposition for Visual Reasoning": "Various approaches have been proposed to tacklecomplex vision-language tasks through task decom-position and step-by-step reasoning. For exam-ple, Visual Programming (Gupta and Kembhavi,2023) utilizes LLMs to perform visual reasoningby breaking down tasks into subroutines. Similarly,ViperGPT (Surs et al., 2023) and CodeVQA (Sub-ramanian et al., 2023) decompose tasks to generateexecutable code. However, these methods heavilyrely on the strong instruction-following capabilitiesof LLMs, making them less effective with smallermodels. Furthermore, these approaches are sus-ceptible to instability arising from prompt designs,choice and ordering of demonstrations, and LLMselection, even when employing powerful modelslike GPT-3 (Zhao et al., 2021). Our work devi-ates significantly from these methods by focusingon fine-tuning VLMs on large-scale synthesizeddatasets, systematically enhancing the models ca-pability to handle complex tasks. This approach en-ables us to achieve competitive performance usingsmaller, open-source models (e.g., 7B or 13B pa-rameters), making our method more cost-effective,accessible, and suitable for practical deploymentwith fewer computational resources.",
  "Method": "We elaborate on our method for multi-step reason-ing in VLMs. First, we formalize least-to-mostvisual reasoning that delineates how a visual rea-soner solves a complex problem according to animage (3.1). Then, we present details of least-to-most synthesis by which a VLM can be tuned asthe visual reasoner to perform reasoning (3.2).",
  "Least-to-Most Visual Reasoning": "We formalize the least-to-most visual reasoningparadigm as follows: Given an image I and a ques-tion Q, a visual reasoner MR deduces a multi-stepreasoning path R, where each step either performsoperations on I (e.g., marking an area with a redbox) or asks an off-the-shelf VLM M to concludea final answer. To this end, we represent R as achain of invoking tools from a pre-defined poolT = {ti|i = 1, 2, , T} step by step, where each",
  "rk,if rk is an image,Ik,otherwise.(2)": "Particularly, we set I1 to I. If Ik includes a redbox to mark some area smaller than a threshold, and tk intends to infer information from Ik (in-cluding the OCR and Answer tools in ),we automatically crop this area from original Ikand enlarge it to the same size as Ik. The aboveprocess iterates until tk refers to the VLM M, inwhich case we define the final answer A = rk andterminate the reasoning process. In summary, weformally denote R = [(Ik, qk, tk)]Kk=1, Tools.Following human experience, we definefour tools, each targeting a class of atomic prob-lems and outputs either a modified image or a pieceof text. Tab. 1 describes details.Specifically, we implement Grounding andHighlight using GroundingDino (Liu et al., 2023c),OCR using PaddleOCR3, and Answer using theVLM M. The reasoner is trained to invoke thesetools to dive into the details of the given image.By varying M in Answer, the reasoner can adaptto different VLMs in a plug-and-play fashion, andsignificantly enhance their performance across awide range of tasks, as will be shown in 4.",
  "Least-to-Most Synthesis": "To overcome the data barrier, a common practiceis to feed image-question pairs to powerful propri-etary LMMs like GPT-4V, and gather the outputs asa dataset (Qi et al., 2024). The top-down approach,however, suffers from several issues: (1) Even pow-erful proprietary models like GPT-4V still struggleto perform reliable reasoning (Wu et al., 2023),implying that the quality of data obtained in thisway is not guaranteed. (2) Utilizing proprietary models incurs high costs, hindering the scalabilityof the synthesized data. (3) It is difficult for oth-ers to reproduce the method since the behavior ofproprietary models can vary over time.In contrast to the top-down approach, we pro-pose a bottom-up pipeline that can synthesize multi-step visual reasoning data using (almost entirely)open-sourced models while ensuring the quality ofthe synthesized data. The workflow begins with animage, gradually generates sub-questions associ-ated with tools and intermediate results, and ulti-mately synthesizes a question based on the imageand the reasoning path. Specifically, the pipelinecomprises four steps: entity recognition, node con-struction, reasoning path synthesis, and questionsynthesis, where each node aggregates a focused(sub-)image and relevant text-form information. (Left) illustrates the construction process. Entity Recognition.We employ DeformableDETR (Zhu et al., 2020) to recognize entities inan image, which can identify 1,203 types of en-tities. In practice, we discard those entities withconfidence scores 0.5. Node Construction.Given an image, we auto-matically construct nodes based on the recognizedentities. Each node is represented as a pair of animage and a textual profile, where the image isextracted from the given one, and the profile isan attribute-value dictionary about the image. Byconverting the image into a textual profile, data syn-thesis can eliminate the reliance on visual signals,allowing the use of more advanced LLMs insteadof VLMs in subsequent processes. As indicated in, we define three types of nodes spanningvarious granularities: (1) Single-Entity Node. Theimage for such a node refers to one recognized en-tity. We utilize specialized tools to extract accurateand fine-grained attributes from multiple dimen-sions (e.g., color) to form the profile. More detailsare presented in Appendix A. (2) Entity-GroupNode. The image of an entity-group node is theaggregation of multiple recognized entities that areclose to each other. We employ BLIP4 to captionthe image as the corresponding profile of the node.Each caption contains about 10-20 tokens, illustrat-ing the inter-entity relations in detail. (3) Whole-Image Node. This type of node corresponds to the",
  "to caption the image in detail as its profile, typi-cally exceeding 200 tokens in length, which offerscomprehensive but coarse-grained information": "Reasoning Process Synthesis.We sample achain of M nodes from the constructed node set,denoted as (N1, N2, , NM), which will be con-nected in turn to form the reasoning process. Wecraft elaborate rules to ensure nodes can be rea-sonably connected and the last node NM is awhole-image node, as detailed in Appendix A.For every two adjacent nodes Nm and Nm+1 inthe chain (m M 1), based on their profilesand a sampled tool tm, we exploit an LLM as aQuestioner to synthesize one sub-question qm toask about a certain attribute of the head node Nm",
  "qm, tm = Questioner(NPm, NPm+1, tm),(3)": "where NPm and NPm+1 refer to the profiles of Nmand Nm+1, respectively, tm is the argument of thespecified tool tm to solve qm. Iterating the pro-cess w.r.t. m, we obtain M 1 sub-questionsand synthesize the entire reasoning process R as[(IM1, qM1, tM1), . . . , (I1, q1, t1)], where Imis the image of Nm (m = 1, , M 1).",
  ": Tools used in our reasoner": "where qm is an intermediate result for the m-th step.Note that {qm}M2m=2 are excluded from R, as theyare just used for synthesis of Q.We implement Questioner and Combiner byfine-tuning LLaMA-3-8B-Instruct6. Due to limitedresources, we obtain the training data by query-ing GPT-4 (Wang et al., 2023b) using a few high-quality demonstrations as seeds following the self-instruct pipeline (Wang et al., 2023b). Since thetwo tasks are relatively simple, querying GPT-4only 10k times is enough to achieve satisfactoryperformance. More details are in Appendix B.Compared to the top-down approach, our least-to-most synthesis is reproducible and significantlymore cost-efficient. Moreover, every step in thepipeline is atomic, ensuring the performance ofthe open-sourced models on these simple tasks.Therefore, the synthesized data is guaranteed to beof high quality, as will be demonstrated in 4.",
  "Experiments": "To assess the efficacy of the proposed method, wefine-tune an LLaVA-1.5-7B7 as the Reasoner, andplug the Reasoner into four representative VLMswith various sizes and conduct experiments on fourstandard VQA benchmarks. In the implementation,we obtain 10k training examples for Question andCombiner, respectively, by querying GPT-4. Sub-sequently, we synthesize a visual reasoning dataset(VIREO) with 50k examples following the least-to-",
  "Models for the Answer Tool": "To illustrate the versatility of our Reasoner, weemploy several VLMs with different sizes and ar-chitectures as the Answer tool, including (1) BLIP-2 (Li et al., 2023a): It utilize a Q-Former moduleto integrate visual and textual information. Weuse the 2.7B version8, which is the smallest modelin our experiments. (2) InstructBLIP (Dai et al.,2024): It expands BLIP-2 by incorporating instruc-tion prompts into the Q-Former module. We usethe 7B9 and 13B10 versions in our experiments. (3)LLaVA (Liu et al., 2024a): It hinges on a basicprojection layer to align image and text representa-tions. We use the 13B version11.",
  "Evaluation Datasets": "We conduct experiments on the following datasets:(1) GQA (Hudson and Manning, 2019): It is aVQA dataset constructed from knowledge graphs,primarily focusing on inter-entity attribute relation-ships. (2) TextVQA (Singh et al., 2019) and ST-VQA (Biten et al., 2019): The two datasets includetextual information within images, used to evaluate the capability to understand text in pictorial form.(3) TallyQA (Acharya et al., 2019): It is widelyused to assess the counting ability, divided into asimple subset and a complex subset. The complexsubset of TallyQA involves more fine-grained at-tributes of the entities in the images than the simplesubset. In our experiments, we denote these subsetsas Tally-S and Tally-C, respectively.Since the VLMs used for the Answer tool aregeneral-purpose generative models and not fine-tuned on the evaluation datasets, they may pro-duce correct answers but include additional infor-mation (e.g., The answer is) beyond the groundtruth provided by the datasets. Therefore, we useExact Matching (EM) as the metric only for Tal-lyQA. For GQA and TextVQA, we use answerrecall, i.e., whether the output includes the groundtruth, for evaluation. For ST-VQA, we submit re-sults to its official website.",
  "Baselines": "To comprehensively compare our approach withalternative task decomposition strategies, we de-vise a baseline termed Tools inspired by Guptaand Kembhavi (2023). Specifically, we employ afew-shot approach to decompose the initial prob-lem into a sequence of tool-calling operations (e.g.,PaddleOCR and GroundingDINO), which are sub-sequently executed. Notably, we leverage LLaMA-3-8B-Instruct (Dubey et al., 2024) for decomposi-tion, ensuring a comparable parameter count to theLLaVA-7B model used by our Reasoner.",
  "Main Results": "reports evaluation results. We find that(1) The Reasoner consistently improves the per-formance of all VLMs across all datasets. Bydecomposing questions and invoking specializedtools, the Reasoner can improve various off-the-shelf VLMs in a plug-and-play fashion, suggestingits strong generalization ability. (2) The Reasonerhelps better capture complex inter-entity rela-tions. The Reasoner brings clear improvements onGQA, which involves diverse relations among enti-ties in the images. The improvements could resultfrom the least-to-most synthesis algorithm in whichsuch relations are implicitly modeled. The improve-ment for LLaVA (3.63%) is more substantial thanother weaker VLMs (1.17%-1.95%). It is possiblybecause stronger VLMs may better realize their po-tential with the Reasoner considerably alleviatingthe difficulty in locating the target entity. (3) The Reasoner boosts the understanding of words inimages. The OCR tool empowers the Reasonerto effectively enhance the performance on bothTextVQA and ST-VQA. On TextVQA, we noticethat the enhancement is less significant on LLaVAthan on weaker VLMs, possibly because LLaVA isless dependent on external tools for understandingthe words in images. (4) The Reasoner improvesthe counting performance by a large margin.The benefit of the Reasoner is particularly signifi-cant on TallyQA. Without the Reasoner, the baseVLMs are sensitive to irrelevant information in theimages and thus easily miscount. By utilizing theHighlight tool, the Reasoner can reliably mitigatethe impact of such irrelevant information.",
  "Quality Assessment of VIREO": "We conduct a human study to investigate the qual-ity of our synthesized VIREO dataset. Specifically,we randomly sample 200 examples from VIREO,and recruit 3 graduate students majoring in NLP tomanually check the correctness of the synthesizedquestion and reasoning process for each instance.The check focuses on the correctness in three as-pects: (1) Sub-Question. The sub-question mustbe accurately faithful to the profile of the tail nodeand can be resolved using the specified tool. (2)Argument. Based on the current sub-question,the annotators need to assess whether using thegiven argument to invoke the tool can yield the ex-pected output. (3) Main Question. The annotatorsjudge whether the main question covers all sub-questions and whether it can be decomposed intosub-questions in the correct order. Each examplereceives 3 labels from each of the three annota-tors on each aspect. Results show that the threeevaluators consistently approve that all examplesare correct in all aspects12 , suggesting the highlyguaranteed quality of the synthesized data.",
  "Discussions": "Although the main results affirmed the high qualityof VIREO and the general benefits of the Reasonerto enhance VLMs on multiple benchmarks, we arestill curious about the following research questions:(1) RQ1: How does the capability of the Reasonervary w.r.t. the size of instructions for fine-tuning?(2) RQ2: What is the relative impact of each inte-grated tool on the Reasoners overall capability? (3)",
  "+ Reasoner-60.65 (+3.63)59.22 (+2.18)-80.28 (+9.99)68.65 (+38.96)": ": Evaluation results with different VLMs as the Answer tools. Size refers to the number of parameters ofthe language model in each VLM. We do not provide the result of LLaVA on ST-VQA because its output alwaysincludes abundant information, leading to nonsensical low scores using the official evaluation website. We highlightthe best results in bold.",
  "RQ2: Ablation Studies Regarding Tools": "To gauge the significance of the diverse tools em-ployed in our approach, we conducted a thoroughablation study, systematically investigating the im-pact of removing each tool on model performance.The findings, presented in , unequivocallydemonstrate that the exclusion of any single tooladversely affects the models capabilities acrossvarious datasets. This observation underscores themultifaceted nature of visual tasks, which oftennecessitate the synergistic application of multiplesophisticated operations, rendering the reliance ona solitary tool insufficient.",
  "RQ3: Extending to Broader Visual Tasks": "To investigate whether introducing a Reasonerwill hurt the performance on other visual tasks,we conduct experiments on MMMU (Yue et al.,2024) and POPE (Li et al., 2023b) as examples.The MMMU benchmark aggregates massive multi-discipline tasks necessitating abundant knowledgeto address, which is beyond the scope of our Rea-soner. POPE is a discrimination dataset for hallu-cination detection which requires discriminatingwhether a certain coarse-grained object is presentin a given image without the need for multi-stepreasoning. We choose InstructBLIP-7B and -13Bas the base VLMs for discussion.As shown in , introducing the Reasoneron MMMU has a minimal impact on the final per-formance, suggesting that the Reasoner does notinfluence the expression of VLMs inherent knowl-edge. On the other hand, the presence or absence ofthe Reasoner on POPE yields consistent results, in-dicating that additional reasoning over images doesnot increase the likelihood of the VLMs generatinghallucinations.",
  "RQ4: Efficacy on More Advanced VLMs": "Considering that more advanced VLMs such asQwen-VL (Bai et al., 2023) have huge potentialto inherit the function of external tools such asperceiving bounding boxes in images, we convertVIREO into an end-to-end format by sequentiallycombining the input and output of each step inorder to avoid the computation overhead from ex-plicit tool invocation. shows that the Qwen-VL model, fine-tuned on the end-to-end version ofVIREO, significantly improves across all datasets.Notably, the application of our Reasoner does notcause a noticeable increase in time cost comparedto the vanilla model (1.4s v.s. 1.1s per sample).",
  ": The distribution of different error types": "we analyze their error and categorize the errorsinto three types: (1) Reasoning: The Reasoneruses a wrong tool (Tool) or generates wrong ar-guments for the tool (Arguments); (2) Execution:The Grounding, OCR, or Highlight tool returnswrong execution results; and (3) Inference: TheAnswer tool outputs wrong answers (Wrong) orirrelevant answers with the question (Missing). As shown in , the Inference errorhas the largest proportion (42%). Among these,there are even 15% of instances where the Answertool generates irrelevant answers, indicating thehuge room to improve the instruction-followingability of existing VLMs. Additionally, the \"Rea-soning\" error accounts for a significant proportion(39%), and most of the instances (28%) are at-tributed to wrong arguments. This means VLMstool-invoking capability is far from perfect, yet thisability is crucial for VLMs to interact with the en-vironment. The Execution error is less frequent,implying the huge potential of building general andpowerful VLMs based on specialized visual tools.",
  "Conclusions": "We propose a data synthesis approach to enhanc-ing multi-step reasoning capabilities of vision-language models. The approach decomposes thesynthesis task into several simple sub-tasks, and fin-ish the sub-tasks (almost) with open-sourced mod-els. Based on the approach, we build a large-scalevisual reasoning dataset, and develop a visual rea-soner by tuning on the dataset. Evaluation resultsacross four VQA benchmarks indicate that the vi-sual reasoner can generally improve the reasoningcapabilities of a number of existing VLMs.",
  "Limitations": "We select COCO2014 (Lin et al., 2014) as thesource of images for our data synthetic process.However, while COCO2014 is a general-purposeimage dataset, we do not guarantee that its datacan cover all visual tasks. Additionally, our pro-posed method demonstrates consistent improve-ments across four VQA benchmarks, but this doesnot imply that our method will be effective in allvisual datasets and scenarios. Furthermore, duringour experiments, we only selected four VLMs asbase models, and we cannot ensure that our methodis capable of bringing improvements to all models.",
  "Ethical Considerations": "Images may contain sensitive information. Usingor publishing datasets that include such informationcould pose potential ethical risks. In our experi-ments, we strictly control the image sources of ourdata, utilizing only authorized open-source datasets.Furthermore, all the models we used are publiclyaccessible and adhere to established requirements. This work was supported by the National Natu-ral Science Foundation of China (NSFC Grant No.62122089), Beijing Outstanding Young ScientistProgram NO. BJJWZYJH012019100020098, andIntelligent Social Governance Platform, Major In-novation & Planning Interdisciplinary Platform forthe Double-First Class Initiative, Renmin Univer-sity of China, the Fundamental Research Funds forthe Central Universities, and the Research Fundsof Renmin University of China.",
  "Manoj Acharya, Kushal Kafle, and Christopher Kanan.2019. Tallyqa: Answering complex counting ques-tions. In Proceedings of the AAAI conference onartificial intelligence, volume 33, pages 80768084": "Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,Antoine Miech, Iain Barr, Yana Hasson, KarelLenc, Arthur Mensch, Katherine Millican, MalcolmReynolds, et al. 2022. Flamingo: a visual languagemodel for few-shot learning. Advances in neuralinformation processing systems, 35:2371623736. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,and Jingren Zhou. 2023. Qwen-vl: A frontier largevision-language model with versatile abilities. arXivpreprint arXiv:2308.12966. Ali Furkan Biten, Ruben Tito, Andres Mafla, LluisGomez, Maral Rusinol, Ernest Valveny, CV Jawa-har, and Dimosthenis Karatzas. 2019. Scene textvisual question answering. In Proceedings of theIEEE/CVF international conference on computer vi-sion, pages 42914301. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Wenliang Dai, Junnan Li, Dongxu Li, AnthonyMeng Huat Tiong, Junqi Zhao, Weisheng Wang,Boyang Li, Pascale N Fung, and Steven Hoi.2024. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advancesin Neural Information Processing Systems, 36. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,Akhil Mathur, Alan Schelten, Amy Yang, AngelaFan, et al. 2024. The llama 3 herd of models. arXivpreprint arXiv:2407.21783. Yash Goyal, Tejas Khot, Douglas Summers-Stay, DhruvBatra, and Devi Parikh. 2017. Making the v in vqamatter: Elevating the role of image understandingin visual question answering. In Proceedings of theIEEE conference on computer vision and patternrecognition, pages 69046913.",
  "Jian Guan, Wei Wu, Zujie Wen, Peng Xu, Hongn-ing Wang, and Minlie Huang. 2024.Amor: Arecipe for building adaptable modular knowledgeagents through process feedback.arXiv preprintarXiv:2402.01469": "Tanmay Gupta and Aniruddha Kembhavi. 2023. Vi-sual programming: Compositional visual reasoningwithout training. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recog-nition, pages 1495314962. Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,Saksham Singhal, Shuming Ma, Tengchao Lv, LeiCui, Owais Khan Mohammed, Barun Patra, et al.2024. Language is not all you need: Aligning per-ception with language models. Advances in NeuralInformation Processing Systems, 36. Drew A Hudson and Christopher D Manning. 2019.Gqa: A new dataset for real-world visual reasoningand compositional question answering. In Proceed-ings of the IEEE/CVF conference on computer visionand pattern recognition, pages 67006709. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-guage models are zero-shot reasoners. Advances inneural information processing systems, 35:2219922213. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.2023a. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large lan-guage models. In International conference on ma-chine learning, pages 1973019742. PMLR.",
  "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong JaeLee. 2024b. Visual instruction tuning. Advances inneural information processing systems, 36": "Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, HaoZhang, Jie Yang, Chunyuan Li, Jianwei Yang, HangSu, Jun Zhu, et al. 2023c. Grounding dino: Marryingdino with grounded pre-training for open-set objectdetection. arXiv preprint arXiv:2303.05499. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, PeterClark, and Ashwin Kalyan. 2022. Learn to explain:Multimodal reasoning via thought chains for sciencequestion answering. Advances in Neural InformationProcessing Systems, 35:25072521. Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh,and Anirban Chakraborty. 2019. Ocr-vqa: Visualquestion answering by reading text in images. In2019 International Conference on Document Analy-sis and Recognition (ICDAR), pages 947952. IEEEComputer Society. Aitor Ormazabal, Che Zheng, Cyprien de MassondAutume, Dani Yogatama, Deyu Fu, Donovan Ong,Eric Chen, Eugenie Lamprecht, Hai Pham, IsaacOng, et al. 2024. Reka core, flash, and edge: A se-ries of powerful multimodal language models. arXivpreprint arXiv:2404.12387.",
  "Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen,Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang,Chaojun Xiao, Chi Han, et al. 2023a.Toollearning with foundation models.arXiv preprintarXiv:2304.08354": "Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, LanYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,Bill Qian, et al. 2023b. Toolllm: Facilitating largelanguage models to master 16000+ real-world apis.arXiv preprint arXiv:2307.16789. Machel Reid, Nikolay Savinov, Denis Teplyashin,Dmitry Lepikhin, Timothy Lillicrap, Jean-baptisteAlayrac, Radu Soricut, Angeliki Lazaridou, Orhan Fi-rat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Un-locking multimodal understanding across millions oftokens of context. arXiv preprint arXiv:2403.05530. Timo Schick, Jane Dwivedi-Yu, Roberto Dess, RobertaRaileanu, Maria Lomeli, Eric Hambro, Luke Zettle-moyer, Nicola Cancedda, and Thomas Scialom. 2024.Toolformer: Language models can teach themselvesto use tools. Advances in Neural Information Pro-cessing Systems, 36. Dustin Schwenk, Apoorv Khandelwal, ChristopherClark, Kenneth Marino, and Roozbeh Mottaghi. 2022.A-okvqa: A benchmark for visual question answer-ing using world knowledge. In European Conferenceon Computer Vision, pages 146162. Springer. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,Weiming Lu, and Yueting Zhuang. 2024. Hugging-gpt: Solving ai tasks with chatgpt and its friendsin hugging face. Advances in Neural InformationProcessing Systems, 36. Amanpreet Singh,Vivek Natarajan,Meet Shah,Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,and Marcus Rohrbach. 2019. Towards vqa modelsthat can read. In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition,pages 83178326.",
  "Schmid, Andy Zeng, Trevor Darrell, and Dan Klein.2023. Modular visual question answering via codegeneration. arXiv preprint arXiv:2306.05392": "Ddac Surs, Sachit Menon, and Carl Vondrick. 2023.Vipergpt: Visual inference via python execution forreasoning. In Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision, pages 1188811898. Gemini Team, Rohan Anil, Sebastian Borgeaud,Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,Radu Soricut, Johan Schalkwyk, Andrew M Dai,Anja Hauth, et al. 2023.Gemini: a family ofhighly capable multimodal models. arXiv preprintarXiv:2312.11805. Weihan Wang, Qingsong Lv, Wenmeng Yu, WenyiHong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang,Lei Zhao, Xixuan Song, et al. 2023a. Cogvlm: Vi-sual expert for pretrained language models. arXivpreprint arXiv:2311.03079. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,Ed H Chi, Sharan Narang, Aakanksha Chowdhery,and Denny Zhou. 2022. Self-consistency improveschain of thought reasoning in language models. InThe Eleventh International Conference on LearningRepresentations. Yiqi Wang, Wentao Chen, Xiaotian Han, Xudong Lin,Haiteng Zhao, Yongfei Liu, Bohan Zhai, Jianbo Yuan,Quanzeng You, and Hongxia Yang. 2024. Exploringthe reasoning abilities of multimodal large languagemodels (mllms): A comprehensive survey on emerg-ing trends in multimodal reasoning. arXiv preprintarXiv:2401.06805. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, AlisaLiu, Noah A Smith, Daniel Khashabi, and HannanehHajishirzi. 2023b. Self-instruct: Aligning languagemodels with self-generated instructions. In Proceed-ings of the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 1348413508. Zilong Wang, Hao Zhang, Chun-Liang Li, Julian Mar-tin Eisenschlos, Vincent Perot, Zifeng Wang, LeslyMiculicich, Yasuhisa Fujii, Jingbo Shang, Chen-YuLee, et al. 2023c. Chain-of-table: Evolving tablesin the reasoning chain for table understanding. InThe Twelfth International Conference on LearningRepresentations. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,Barret Zoph, Sebastian Borgeaud, Dani Yogatama,Maarten Bosma, Denny Zhou, Donald Metzler, et al.2022a. Emergent abilities of large language models.Transactions on Machine Learning Research. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022b. Chain-of-thought prompting elicits rea-soning in large language models. Advances in neuralinformation processing systems, 35:2482424837.",
  "Yang Wu, Shilong Wang, Hao Yang, Tian Zheng,Hongbo Zhang, Yanyan Zhao, and Bing Qin. 2023.An early evaluation of gpt-4v (ision). arXiv preprintarXiv:2310.16534": "Zhengyuan Yang, Linjie Li, Jianfeng Wang, KevinLin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,Ce Liu, Michael Zeng, and Lijuan Wang. 2023. Mm-react: Prompting chatgpt for multimodal reasoningand action. arXiv preprint arXiv:2303.11381. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,Tom Griffiths, Yuan Cao, and Karthik Narasimhan.2024. Tree of thoughts: Deliberate problem solvingwith large language models. Advances in NeuralInformation Processing Systems, 36. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, IzhakShafran, Karthik R Narasimhan, and Yuan Cao. 2022.React: Synergizing reasoning and acting in languagemodels. In The Eleventh International Conferenceon Learning Representations. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye,Ming Yan, Yiyang Zhou, Junyang Wang, An-wen Hu, Pengcheng Shi, Yaya Shi, et al. 2023.mplug-owl: Modularization empowers large lan-guage models with multimodality. arXiv preprintarXiv:2304.14178. Peter Young, Alice Lai, Micah Hodosh, and Julia Hock-enmaier. 2014. From image descriptions to visualdenotations: New similarity metrics for semantic in-ference over event descriptions. Transactions of theAssociation for Computational Linguistics, 2:6778. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng,Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang,Weiming Ren, Yuxuan Sun, et al. 2024. Mmmu: Amassive multi-discipline multimodal understandingand reasoning benchmark for expert agi. In Pro-ceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 95569567. Rowan Zellers, Yonatan Bisk, Ali Farhadi, and YejinChoi. 2019.From recognition to cognition: Vi-sual commonsense reasoning. In Proceedings of theIEEE/CVF conference on computer vision and pat-tern recognition, pages 67206731. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, andSameer Singh. 2021. Calibrate before use: Improv-ing few-shot performance of language models. InInternational conference on machine learning, pages1269712706. PMLR.",
  "ADetails of Data Construction": "Attributes of Single-Entity Node.Each single-entity node consists of five attributes: Label, Lo-cation, Color, Text, and Size. Specifically, Labeland Location are directly derived from the recogni-tion results of Deformable DETR. Color is deter-mined using ColorThief 13 to analyze the dominantcolor scheme of the subgraph corresponding to thesingle-entity node. Text is obtained by applyingPaddleOCR to recognize textual content within theimage. Size records the nodes proportion in theentire image, considering its width, height, andarea. Construction of Chain.We use tools that pro-duce graphical outputs (e.g., Grounding, Highlight)to connect intermediary nodes and tools that pro-duce textual outputs (e.g., OCR, Answer) to con-nect terminal nodes.We construct the chain by sequentially addingnodes. Specifically, we maintain a queue L =(N1, N2, . . . , Ni) consisting of nodes.To addNi+1, we first specify the tool ti to be used andsample a node in the remaining set N = {Nj / L}that allow the use of this tool. For instance, onlynodes with text in images permit using the OCRtool. If N = or the chain length reaches the limit,the process terminates. When constructing VIREO,we set the maximum chain length to 4. Conse-quently, the dataset includes reasoning paths withlengths of 2, 3, and 4.",
  "ImageSource.WeusetheCOCO2014dataset (Lin et al., 2014) as our source of images": "Questioner.We use LLaMA-3-8B-Instruct as thebase model for the Questioner. To construct thecorresponding training data, we first manually cre-ate 5 seed prompts for each combination of headnode and tail node (3 3 = 9). Then, we generatea total of 10k instances by using GPT-4. Duringthis process, to reduce the bias introduced by the",
  ": The average per-sample time cost (seconds)when using only the VLM and applying the Reasoner": "seed prompts, we gradually add the obtained re-sults into the prompt pool. For each instance, threeprompts are randomly selected from the promptpool as demonstrations. For each instance, the in-put contains 3 fields: the profiles of the head nodeand tail node, and the specified tool. Output in-cludes 2 fields: the question and the arguments ofthe tool. We perform instruction fine-tuning on theQuestioner using LoRA. The rank is set to 8 andthe lora_alpha is set to 8. We adopt AdamW as theoptimizer. We set adam_beta1, adam_beta2, andadam_epslion to 0.9, 0.999, and 1e-8, respectively.We use the cosine schedule to warm up the train-ing and set warmup_steps to 0.1. We set the batchsize to 4 and fine-tune Questioner for 2 epochs,with each epoch taking around 1 hour. The trainingprocess is completed with 8 Nvidia A100 GPUs. Combiner.Similar to Questioner, we also useLLaMA-3-8B-Instruct as the base model for theCombiner. We manually create 20 seed promptsand generate a total of 10k instances by usingGPT-4. The input includes two questions to bemerged, and the output is the merged result. Thefine-tuning settings of Combiner keep the same asQuestioner. Reasoner.We synthesize 50k data examples us-ing the least-to-most synthesis method. Each exam-ple includes the current image I, the main questionQ, and the previous sub-questions {q<k}. The la-bel for each data example comprises the currentsub-question qk and the tool tk that needs to be in-voked. We use this data to train LLaVA-1.5-7B asthe Reasoner. We perform instruction fine-tuningon the Reasoner using LoRA. The rank is set to 8and the lora_alpha is set to 8. We adopt AdamWas the optimizer. We set adam_beta1, adam_beta2,and adam_epslion to 0.9, 0.999, and 1e-8, respec-tively. We use the cosine schedule to warm up thetraining and set warmup_steps to 0.1. We set thebatch size to 8 and fine-tune the Reasoner for 3epochs, with each epoch taking around 3 hours.",
  "CAdditional Time Cost": "While the Visual Reasoner offers significant perfor-mance gains across various tasks, it is crucial to ac-knowledge the computational overhead associatedwith its multi-step reasoning process. As each stepnecessitates additional inference time, the overallinference duration is extended compared to using avanilla VLM. presents a quantitative anal-ysis of the average per-sample time cost using asingle Nvidia A100 GPU."
}