{
  "Abstract": "Comprehensively understanding and accuratelypredicting the performance of large languagemodels across diverse downstream tasks hasemerged as a pivotal challenge in NLP research.The pioneering scaling law on downstreamworks (Hu et al., 2024; Isik et al., 2024) demon-strated intrinsic similarities within model fam-ilies and utilized such similarities for perfor-mance prediction. However, they tend to over-look the similarities between model familiesand only consider design factors listed in theoriginal scaling law. To overcome these limita-tions, we introduce a novel framework, Collab-orative Performance Prediction (CPP), whichsignificantly enhances prediction accuracy byleveraging the historical performance of var-ious models on downstream tasks and otherdesign factors for both model and task. Wealso collect a collaborative data sourced fromonline platforms containing both historical per-formance and additional design factors. Withthe support of the collaborative data, CPP notonly surpasses traditional scaling laws in pre-dicting the performance of scaled LLMs butalso facilitates a detailed analysis of factor im-portance, an area previously overlooked. Ourcode is available here1.",
  "Introduction": "Large Language Models (LLMs) (Brown et al.,2020; Ouyang et al., 2022) have emerged as one ofthe most important AI research powered by large-scale parameters, high computational resources,and massive training data. With the substantialincrease in model sizes, the evaluation cost ofLLMs performance becomes even more signifi-cant. For example, testing a single LLM on cer-tain benchmarks often requires $10K+ and 4K+GPU hours (Liang et al., 2023). Therefore, un-derstanding the behaviors and predicting the capa-bilities of LLMs across scales under various tasks",
  "Co-corresponding Authors1": "becomes a vital question (Ganguli et al., 2022a;Owen, 2024; Finnveden, 2020; Hu et al., 2024) forboth researchers and engineers.Scaling laws (Kaplan et al., 2020; Hoffmannet al., 2022; Hernandez et al., 2022; Gordon et al.,2021; Bahri et al., 2024; Muennighoff et al., 2023)have been powerful tools for predicting the capabil-ities of LLMs. It indicates a power-law correlationbetween the model performance and design factorssuch as computational measure (FLOPs) utilizedduring training. Although the scaling law was orig-inally proposed as a strong intuitive guide for de-signing LLM, researchers (Hu et al., 2024; Ruanet al., 2024; Isik et al., 2024) have extended itsutility into predicting model performances on vari-ous metrics, such as BLEU in Machine Translation,and different tasks. These works can accuratelypredict model performances by utilizing the simi-larity within each model family, e.g., models withineach family are usually trained on the same dataset.However, there are several issues rooted in theirmethods: the performance prediction 1) requirestransparent design factors that consume substantialtraining resources to fit the curve, 2) is only tailoredto a certain model family and a specific task metric,and 3) neglects the connections among differentmodels and tasks.The aforementioned limitations motivate us todesign more effective methods for predicting theperformance of LLMs on downstream tasks. Twoobservations sparked our attention.Firstly, Astrong similarity exists between model families,e.g.LLama-family and GPT family. Models fromdifferent families behave similarly in predictiondistribution (Shrivastava et al., 2023) and emergentphenomenon (Wei et al., 2022). Secondly, withthe emerging LLM models and the increasinglydiverse tasks, the cost of enumerating and bench-marking models with tasks increases exponentially.Therefore, we aim to utilize the similarities acrossmodel families in order to collaboratively predict Academic paper",
  "Linear Layer": ": Framework for Collaborative Performance Prediction of Large Language Models. This schematicdelineates two principal components: (1) Collaborative Data, which encompasses a score matrix illustrating theperformance of various LLMs across downstream tasks, along with external descriptive factors of both modelsand tasks; (2) Collaborative Prediction Method, given the model and task IDs to leverage this collaborative data,enabling accurate score prediction.",
  "the model performance over diverse tasks in anaccurate yet efficient way": "To incorporate the aforementioned intuitions, wepropose a new scheme, Collaborative PerformancePrediction (CPP), to efficiently predict the perfor-mance of LLMs on evaluation tasks. This schemelearns the latent representations of LLMs and tasks,which captures the intrinsic similarity among dif-ferent models and tasks. The interaction (e.g., in-ner product) between the latent representations ofLLMs and tasks can be utilized to predict the per-formance of LLMs on certain tasks. To fulfil theproposed scheme, we collect the LLM performancedata from academic papers, technical reports, andopen leaderboards covering 72 models and 29 tasks.To summarize, our scheme has several advantages:",
  "Low Training Cost:Compared with meth-ods (Hu et al., 2024) that extend scaling law tovarious downstream tasks, no pre-training or fine-tuning of LLM is required in our scheme": "Prediction over proprietary model: Unlike pre-vious methods (Ruan et al., 2024), our schemesupports prediction over proprietary models with-out knowing key design factors, such as compu-tational measures. Prediction from small to large: By utilizingcross-family information, our scheme can accu-rately estimate model performance, e.g., emer-gent ability, of large models on downstream tasksgiven the information from small models.",
  "Under our scheme, multiple customized pre-diction methods (e.g., COLLABORATIVE FITER-": "ING (Koren et al., 2022)) can be incorporated to pre-dict the performance of LLMs, further validatingthe feasibility and generality. Our method enablesmore diverse factors as input, ranging from tradi-tional LLM design factors to task design factors,e.g., targeted ability and few-shot setting. Upon extensive experimentation within the open-released core leaderboard of HELM (Liang et al.,2023) and our collected historical matrix, our pre-dictive performance demonstrated exceptionallywell. Specifically, even without any input of modelfactors or task factors: in HELM, we use 50%of the scores to predict the other 50%, the pre-dictive ranking (derived from predicted scores)achieves Accuracy =10%, and MAE@2 =39%;in our collected matrix (characterized by a 44%sparsity level) achieves an Accuracy =45%, andthe MAE@2 =84%. Notably, the accuracy ofour prediction from small to large LMs signifi-cantly exceeded that predicted by scaling laws.Using an analysis method similar to SHAPLEY-VALUES (Lundberg and Lee, 2017; Shapley, 1952),we elucidate the importance of different factors,which surprisingly does not fully align with scalinglaw (Kaplan et al., 2020). Therefore, our method isundoubtedly more versatile.",
  "Downstream Scaling Law andPerformance Predictability of LLM": "Scaling laws (Kaplan et al., 2020; Hoffmann et al.,2022; Hernandez et al., 2022; Bahri et al., 2024;Muennighoff et al., 2023) for LLMs have increas-ingly become a focal point in understanding andguiding critical design decisions, such as modelsize and the characteristics and volume of pre-training data. Traditionally, most research in thisarea has concentrated on how measures like cross-entropy loss or perplexity scale. Subsequent stud-ies have extended these efforts to the scaling be-havior on translation (Isik et al., 2024; Ghorbaniet al., 2021; Zhuocheng et al., 2023) and otherdownstream tasks modeling (Caballero et al., 2023;Henighan et al., 2020). The high predictabilityin LLMs capability has directly spurred extensiveresearch work (see Survey Anwar et al. (2024))exploring whether LLMs can demonstrate pre-dictability on downstream tasks, which are consid-ered highly unpredictable in traditional ML knowl-edge (Ganguli et al., 2022a).Particularly, theemergence phenomenon (Suzgun et al., 2022; Weiet al., 2022) has challenged predictability, wheremodels suddenly exhibit striking capabilities atspecific training reources. Recent studies (Scha-effer et al., 2023) have made remarkable achieve-ments in breaking the discontinuities in perfor-mance brought about by emergence, and Ganguliet al. (2022a); Owen (2024); Finnveden (2020)demonstrated the predictability on downstreamtasks, for instance, Hu et al. (2024) directly fitsa curve of training resources and downstream taskperformance by repeatedly pretraining a specificmodel. Furthermore, Arora and Goyal (2023) pre-dicted the performance through decomposing thecomplex capabilities of LMs to some base skills. Given that predictability has now been estab-lished, we reassess the underlying premises that en-able this predictability: the prevailing similaritiesacross multiple models and various downstreamtasks (Liu et al., 2023; Perlitz et al., 2024; Poloet al., 2024; Torregrossa et al., 2020; Ilic, 2023).Based on this, we step beyond the limitations de-fined by scaling laws and propose a new methodol-ogy to predict the performance of LLMs on variousdownstream tasks.",
  "Collaborative Filtering": "Collaborative filtering (CF) (Koren et al., 2022) is awidely used technique in recommendation systemsthat predicts users preferences by collecting thehistorical preferences of many other users. The un-derlying assumption of CF is that similar users willshare similar preferences on similar items. A sem-inal method in CF is matrix factorization (Korenet al., 2009) (MF). It reduces the dimensionality ofthe user-item matrix by learning the latent factorsassociated with users and items, respectively. Thisapproach helps handle sparse data and improvesscalability. The factorization of the user-item ma-trix R can be represented as",
  "R P Q ,(1)": "where each column vector in P and Q representsa specific user or item, respectively, with hiddendimension d. The latent representations of usersand items capture the user preferences and itemproperties in the latent space, and the inner product can be utilized to predict the interaction betweenusers and items. To optimize the latent featurevectors, the following loss function is employed:",
  "(u,i)(rui pu qi)2 ,(2)": "which measures the squared differences betweenthe observed ratings rui and the ratings predictedby the model pu qi for each user-item pair (u, i)in the set of observed interactions.Here, Yang et al. (2019) transferred the collabo-rative filtering for ML model selection by predict-ing the cross-valided errors, which demonstratesCFs adaptability and efficiency in diverse applica-tion areas.",
  "Latent Factor=7Latent Factor=10": ": Error Distribution of Predictions (Normalized Score and Rank Derived by Score) Based on the HELM LiteLeaderboard Using Neural Collaborative Filtering: We evaluate the effectiveness of Matrix Factorization (MF) using twolatent factors, 7 and 10, across 2 training/validation split percentages. Accuracy is defined as the percentage of instances wherethe predicted rank equals the actual rank. MAE@2 is defined as the percentage of instances where the absolute differencebetween the predicted rank and the actual rank is 2.",
  "(Sm) f log(Cm) + bf,(4)": "where Sm refers to the normalized downstreamscores of models within the range . How-ever, applying scaling laws across different modelfamilies on various specific tasks presents a trade-off: fitting unique coefficients for each evaluationscenario (e.g., Llama 2 on MMLU) is a resource-intensive endeavor (Hu et al., 2024); alternatively,estimating these coefficients using a limited num-ber (3-5) of models within the same family maycompromise the accuracy of the predictions. More-over, the recent work (Ruan et al., 2024) extendsscaling law by incorporating latent variables to cap-ture the patterns across model families and tasks.",
  "Scaling laws reveal that models from any familyexhibit a similar performance trend as computa-tional measures increase. This insight suggests": "there are commonalities and connections betweendifferent models. These motivate us to employthe MF method to explore more similarities be-yond computational measures, e.g., the relationshipamong the different model families and tasks. We perform the aforementioned MF on thebenchmark matrix to observe the error gap betweenpredicted and truth (normalized) scores. Specifi-cally, we select the core leaderboard provided byHELM for our exploratory experiments with onlymodel name, task name, and performance scores.This leaderboard, 68 models and 16 tasks, pre-sented in a score matrix with a density of 82.5%,which includes both open-source and proprietarymodels, e.g., GPT-4 and Jurassic-2. Our methodtreats all models and tasks as independent enti-ties without introducing any prior similarity factors.We hope to observe whether MF can predict theremaining scores, giving a small part of the matrix,where we evaluate two training/validation sets splitstrategies: 10%/90%, 50%/50%. As illustrated in, MF can accurately predict most of themissing scores within a low error range, whichproves that it can encode the similarity across themodel and the task without regression dependingon explicit computational measures variable.",
  "Motivated by pilot experiments, we introduce theconcept of Collaborative Performance Prediction(CPP) to facilitate the performance prediction ofLLMs": "Definition 1. Let M = {M1, M2, . . . , Mn} bea set of n LLMs, and T = {T1, T2, . . . , Tm} bea suite of m tasks. Define the Score Matrix S,which is an n m matrix where each element sijrepresents the performance score of model Mi ontask Tj. sij is defined as",
  "scoreif tested,unknownotherwise": "Function: Employ an prediction method F to es-timate the unknown elements of S, denoted by sij,based on the known values.Extention: Accommodate model design factorsVm = {V 1m, V 2m, . . . , V Mm }, such as common com-putational meatures, and task design factors Vt ={V 1t , V 2t , . . . , V Tt }, such as targeted capabilitiesand few-shot settings. Based on this definition, our framework consistsof two components: 1) collaborative performancedata, 2) collaborative prediction methods. Weanticipate that an accurate score can be predictedbased on the historical performance of various mod-els on downstream tasks and other design factorsfor both model and task. Moreover, we can incor-porate or solely rely on the factors describing theLLM and the associated downstream tasks.",
  "Unlike the scaling law approach, which requirestraining resource factors to obtain the correlation": "between metric scores and factors at a high train-ing cost, our proposed method makes use of eval-uation results and other design factors reportedfrom existing studies, referred to as collaborativedata. Open-source leaderboards, such as Open-LLM2, HELM, and OpenCompass3, have madetremendous efforts on this issue in fairly evaluat-ing different LLMs. Our efforts extend beyondmerely (Ruan et al., 2024) utilizing data from open-source leaderboards with matrix sparsity of 0%.We also extract test results from different modelspapers, technical reports, and model cards. Ulti-mately, we have collected a score matrix of n = 72,m = 29 with a density of only 56%. Furthermore,we collected 12 and 4 detailed design factors formodels and tasks. These details are listed in Ap-pendix B.1. Our data analysis is shown in and . Data Analysis.Based on the collective data, wecan make the following observations: a) Unevendistribution of testing resources. We observesignificant variability in the deployment of testingefforts, as shown in . For instance, modelsfrom the LLAMA series have undergone extensivetesting across various tasks, in contrast to modelslike GOPHER, where testing has largely stagnated.A similar disparity is also evident among tasks,with MMLU and HELLASWAG receiving consid-erable evaluation, whereas RACE has been rela-tively underexplored. This trend suggests that asLLMs proliferate and tasks evolve, scores acrossthe matrix will increasingly skew. This leads toa pronounced long-tail effect in testing coveragefor many tasks, barring a few that consistently re-ceive comprehensive evaluations. b) Widespreadvariations in the scores. It is noteworthy that iden-tical models yield varying scores on the same tasksacross different studies (Shrivastava et al., 2023;AI@Meta, 2024), a variation often attributed to dif-ferences in prompt settings, model versions, andthe volume of test samples employed. Typically,these score variations range within 0.1, with scoresnormalized between . This phenomenon un-derscores the importance of public leaderboardsand highlights researchers need to articulate theirtesting frameworks when performing customizedevaluations. When conflicted, we prefer the re-sults from the Open-LLM leaderboard in the col-lective data. c) Missing description/model card. We advocate for consistently providing completemodel cards for open-source and proprietary mod-els. Such a phenomenon is shown in and,unsurprisingly, a long-tail distribution is witnessed.While it is understandable that proprietary modelsmight withhold specific details about parameters,they can still divulge information about parameterscale and the extent of pre-training. Furthermore,we recommend a more thorough description of test-ing tasks, including suggested few-shot settingsand detailed descriptions of targeted capabilities.",
  "Prediction Methods": "In .2, classical collaborative filtering meth-ods are inspired to conduct the performance pre-diction. In principle, most collaborative filteringmethods can be applied. Here, in addition to theabovementioned MF, we also leverage neural col-laborative filtering (He et al., 2017) (NCF) meth-ods, which uses a multi-layer perceptron to learnthe model-task interaction function to predict thescore sij for a model i on a task j, providing a wayto learn non-linearities in the data:",
  "= MLP(pi, qj, [evi, evj]),(5)": "where M and T denote the sets of collaborativemodels and tasks, and their descriptive factors Vi,Vj optionally enrich the input data. Here, pj andqj are the latent vectors for model i and task j thatcapture the intrinsic properties of models and tasks,as well as embeddings [evi, evj] derived from theirdescriptive factors, and represents the parametersof NCF.Moreover, we further simplify the model to ver-ify whether it is feasible to predict a score whenonly inputting the descriptive factors Vi, Vj intothe prediction model:",
  "Experiments": "In this section, we evaluate the feasibility of CPPfrom an overall benchmark perspective and a modelperspective in .1 and 5.2, respectively;we then analyze the importance of factors for bothmodels and tasks in .3.Additionally,a substantial amount of ablation and analysis isplaced in the appendix D, such as sparsity, the cor-relations in tasks and models, and which modelsand tasks are more critical for prediction. Experimental Setting.Our validation frame-work utilizes the aforementioned collaborativedataset as the score matrix S. We partition scores{sij} into train and validation set, detailed in Ap-pendix C.2. Evaluation Metric.To accurately evaluate CPP,we adopt two types of metrics: 1) SCORE-LOSSmetrics including MSE LOSS and L1 LOSS be-tween predicted scores and true scores (normalized)on downstream tasks and 2) RANK-ACCURACYmetrics including ACCURACY and MAE@2 be-tween the rank of predicted scores and true scores.We elaborate on these metrics in Appendix C.1.",
  "Evaluation from Benchmark Perspective": "In this study, we select the abovementioned meth-ods, MF and NCF, to verify whether sij can beaccurately predicted based on the input of modeli and task j. To examine whether enhancementsare helpful, we modify NCF to support the input ofdesign factors, detailed in Appendix C.2. Based on and , we can make the followingobservations:First, all methods accurately predicted modelperformance, demonstrating that collaborative fil-tering mechanisms can predict model outcomesbased on collaborative data across different mod-els and tasks. This prediction is achieved withoutexplicit scaling factors or fitting a log-power curve.Second, from MF to NCF, the transformation ininteraction mechanisms further enhances accuracy,suggesting that model improvements can furtheraugment the efficacy of our methodology. Addi-tionally, we further increased accuracy by incorpo-rating factors, such as model scaling variables andtask descriptions, into the NCF framework along-side ID information. This confirms that incorpo-rating explicit factors can enhance model and tasksimilarities. Finally, among all metrics, we particu-larly noted that the accuracy of the predictive rank-",
  "Predicted score": "0.01.00.80.60.40.2 : Comparative visualization of predictive accuracy across various scoring methods. From left to right:MF, NCF, NCF with Factor Enhancement, and NCF based solely on Factors. Each plot displays the regressionbetween predicted and actual scores, where the solid line represents the regression fit and the shaded area denotesthe confidence interval (CI). A line closer to the diagonal indicates perfect prediction and higher prediction accuracy.These plots demonstrate the enhanced performance in score prediction achieved by integrating factors into the NCFmethod.",
  ": Comparison of prediction methods for LLM performance. Bold indicates the best-performed": "ing was acceptable. In other words, researcherscan use our method to accurately predict the rank-ing range of their developed models on test tasks,thereby enhancing model performance on specifictasks. Predictability with Only Description Factors.We validate whether high predictive accuracy canstill be achieved by only inputting the models andtasks design factors. As demonstrated in ,the accuracy of predicted rankings (derived frompredicted scores) remains high, affirming that ourmethod supports predictions based solely on fac-tors. However, the accuracy is lower than othermodels, suggesting that finer-grained latent similar-ities remain encoded as potential factors within theidentity information across different models andtasks.",
  "Evaluation from Model Perspective": "To mimic the utilization of CPP in the real world,this section takes a model perspective to investigatethe predictive accuracy of CPP upon each model.Specifically, we propose two scenarios: (i) pre-diction with no prior testing information and (ii)prediction with prior testing information on 2 tasks.These two scenarios correspond to real-world caseswhen the model has not been developed or is tested",
  "Predicted Score": "0.00.20.40.60.81.0 : Comparison of the predictive performanceof collaborative performance prediction (CPP) versustraditional scaling laws (SL) for LLMs: (a) CPP-0, withno prior testing information, and (b) CPP-2, with priortesting on two tasks. on a few tasks and expects an accurate predictionof its ability on other tasks. In both scenarios, wefocus on larger LLMs, e.g., LLama2-70b, as theyare more computationally expensive to develop andtest, requiring an accurate LLM prediction.We report the results of CPP and SL on both sce-narios in and can draw the following con-clusions. Under the CPP-0 scenario, CPP demon-strated greater adaptability across different taskscompared to SL, with points closely aligned alongthe y = x line (perfect prediction) in (a). This suggests that CPP has effectively capturedtask-specific characteristics, such as value ranges,whereas SL, despite achieving a lower MSE-LOSS,tends to concentrate its predictions around 0.5. Un-der the CPP-2 scenario, the distribution of pointsof CPP is noticeably closer to y = x, as shown in (b), and its MSE-LOSS is also lower thanthat of SL. This indicates that leveraging perfor-mance data from other tasks considerably enhancesthe models cross-task prediction capabilities, un-derscoring a degree of consistency across tasks forthe same model. This approach demonstrates thatpredictions for scaling LLMs on downstream taskscan be dynamically improved by evaluating perfor-mance on less computationally intensive tasks andusing those outcomes to predict scores on subse-quent tasks more accurately.",
  "Factor Importance Analysis viaSHAPLEY-VALUE": "In this section, we aim to analyze each design fac-tors importance over CPP. The Shapley value,a concept derived from cooperative game the-ory (Shapley, 1952), offers a systematic approachto measuring individual factors contribution in pre-dictive models (Lundberg and Lee, 2017; Covertet al., 2021). Appendix C.3 shows a detailed for-mulation of the Shapley value. Visualization forShapley values of each design factor is shown in.Based on (a), we can make the follow-ing observations regarding model factors. First,we have discovered that in addition to tradition-ally important factors such as training data sizeand parameter size mentioned in scaling law (Ka-plan et al., 2020), other design factors significantlyinfluence predictive outcomes. These include themodel family, context window size, and batch size.Second, the importance of the model family cannotbe overlooked, as it may relate to differences in",
  ": Mean Shapley Value on Each Factor": "data quality across models, including proprietarydata or specific architectural details. For instance,using a particular model family might mean adopt-ing architectures or optimization techniques bettersuited to specific tasks. Moreover, the size of thecontext window also significantly affects modelperformance.A larger context window allowsthe model to better understand the context in longtexts, which is particularly crucial for long-contextLLMs (Xiong et al., 2023). Experience (Google,2024) has shown that such models perform betteracross various tasks. Batch size, as another crucialfactor, affects the stability and speed of model train-ing. An appropriate batch size ensures a balancebetween the accuracy of gradient estimation andcomputational efficiency during training. As for the importance of task factors, results in (b) show that the target ability amongall factors is more important. This also impliesthat similarities between the domains of differenttasks can help predict outcomes. This conclusion isconsistent with previous observations (Ruan et al., 2024; Perlitz et al., 2024; Polo et al., 2024)In summary, these findings indicate that LLMsperformance prediction should not rely solely ontraditional design factors limited by scaling law butalso on other key factors that might impact overallmodel performance.",
  "Conclusion and Discussion": "Advancing beyond traditional scaling laws ondownstream tasks, we propose a collaborative per-formance prediction framework for large languagemodels. It offers significant advantages, includingeasy deployment, low training costs, and superiorpredictive accuracy. Uniquely, it enables incorpo-rating additional design factors and supports anin-depth analysis of their impact, including factorimportance and correlations in models and tasks.For prediction, we collect collaborative data con-taining many historical performances and factors.Our methods predictive accuracy is expected toimprove as it benefits from an expanding pool ofcollaborative data. Moreover, this approach high-lights the potential to identify neglected but vitalfactors beyond traditional scaling laws, such as taskdesign factors, thereby enriching our comprehen-sion of LLM performance predictability on down-stream tasks.",
  "Limitations": "Single-source-of-truth.When collecting thecollaborative data, we hypothesize that eachmodels performance on each task is identical.However, in the real world, the detailed testing set-ting, for instance, the testing prompt writing, caninfluence LLMs performance variance. Althoughwe observed this, we only saved one score fromdifferent sources. How to incorporate the settingof testing as an additional dimension remains to besolved in future works. Susceptibility to data quality.The predictionaccuracy of CPP highly depends on the qualityof collaborative data.The current version pas-sively collects collaborative data from online re-sources. Should information from either of thesedata sources be incorrect, the prediction capabilityof CPP would decrease correspondingly. To over-come such a limitation, jointly considering passiveinformation collected from data sources and activeinformation, such as performances of models testedon some tasks by the user, might be a solution.",
  "AI@Meta. 2024. Llama 3 model card": "Usman Anwar, Abulhair Saparov, Javier Rando, DanielPaleka, Miles Turpin, Peter Hase, Ekdeep SinghLubana, Erik Jenner, Stephen Casper, Oliver Sourbut,Benjamin L. Edelman, Zhaowei Zhang, Mario Gn-ther, Anton Korinek, Jose Hernandez-Orallo, LewisHammond, Eric Bigelow, Alexander Pan, Lauro Lan-gosco, Tomasz Korbak, Heidi Zhang, Ruiqi Zhong,Sen higeartaigh, Gabriel Recchia, Giulio Corsi,Alan Chan, Markus Anderljung, Lilian Edwards,Yoshua Bengio, Danqi Chen, Samuel Albanie, TeganMaharaj, Jakob Foerster, Florian Tramer, He He,Atoosa Kasirzadeh, Yejin Choi, and David Krueger.2024. Foundational challenges in assuring alignmentand safety of large language models. In arXiv.",
  "Yasaman Bahri, Ethan Dyer, Jared Kaplan, JaehoonLee, and Utkarsh Sharma. 2024. Explaining neuralscaling laws. In arXiv": "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020.Language models are few-shot learners. In Advances",
  "Ethan Caballero, Kshitij Gupta, Irina Rish, and DavidKrueger. 2023. Broken neural scaling laws. In Inter-national Conference on Learning Representations": "Mark Chen, Jerry Tworek, Heewoo Jun, QimingYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-plan, Harri Edwards, Yuri Burda, Nicholas Joseph,Greg Brockman, Alex Ray, Raul Puri, GretchenKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-try, Pamela Mishkin, Brooke Chan, Scott Gray,Nick Ryder, Mikhail Pavlov, Alethea Power, LukaszKaiser, Mohammad Bavarian, Clemens Winter,Philippe Tillet, Felipe Petroski Such, Dave Cum-mings, Matthias Plappert, Fotios Chantzis, Eliza-beth Barnes, Ariel Herbert-Voss, William HebgenGuss, Alex Nichol, Alex Paino, Nikolas Tezak, JieTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,William Saunders, Christopher Hesse, Andrew N.Carr, Jan Leike, Josh Achiam, Vedant Misra, EvanMorikawa, Alec Radford, Matthew Knight, MilesBrundage, Mira Murati, Katie Mayer, Peter Welinder,Bob McGrew, Dario Amodei, Sam McCandlish, IlyaSutskever, and Wojciech Zaremba. 2021. Evaluatinglarge language models trained on code. In arXiv.",
  "Lukas Finnveden. 2020. Extrapolating gpt-n perfor-mance": "Deep Ganguli,Danny Hernandez,Liane Lovitt,Amanda Askell, Yuntao Bai, Anna Chen, Tom Con-erly, Nova Dassarma, Dawn Drain, Nelson Elhage,Sheer El Showk, Stanislav Fort, Zac Hatfield-Dodds,Tom Henighan, Scott Johnston, Andy Jones, NicholasJoseph, Jackson Kernian, Shauna Kravec, Ben Mann,Neel Nanda, Kamal Ndousse, Catherine Olsson,Daniela Amodei, Tom Brown, Jared Kaplan, SamMcCandlish, Christopher Olah, Dario Amodei, andJack Clark. 2022a. Predictability and surprise inlarge generative models. In Conference on Fairness,Accountability, and Transparency. ACM. Deep Ganguli,Danny Hernandez,Liane Lovitt,Amanda Askell, Yuntao Bai, Anna Chen, Tom Con-erly, Nova Dassarma, Dawn Drain, Nelson Elhage,Sheer El Showk, Stanislav Fort, Zac Hatfield-Dodds,Tom Henighan, Scott Johnston, Andy Jones, NicholasJoseph, Jackson Kernian, Shauna Kravec, Ben Mann, Neel Nanda, Kamal Ndousse, Catherine Olsson,Daniela Amodei, Tom Brown, Jared Kaplan, SamMcCandlish, Christopher Olah, Dario Amodei, andJack Clark. 2022b. Predictability and surprise inlarge generative models. In ACM Conference onFairness, Accountability, and Transparency, pages17471764.",
  "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie,Xia Hu, and Tat-Seng Chua. 2017. Neural collabora-tive filtering. In International Conference on WorldWide Web, pages 173182": "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,Mantas Mazeika, Dawn Song, and Jacob Steinhardt.2021. Measuring massive multitask language under-standing. In International Conference on LearningRepresentations. Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen,Christopher Hesse, Jacob Jackson, Heewoo Jun,Tom B. Brown, Prafulla Dhariwal, Scott Gray, ChrisHallacy, Benjamin Mann, Alec Radford, AdityaRamesh, Nick Ryder, Daniel M. Ziegler, John Schul-man, Dario Amodei, and Sam McCandlish. 2020.Scaling laws for autoregressive generative modeling.In arXiv. Danny Hernandez, Tom Brown, Tom Conerly, NovaDasSarma, Dawn Drain, Sheer El-Showk, NelsonElhage, Zac Hatfield-Dodds, Tom Henighan, Tris-tan Hume, Scott Johnston, Ben Mann, Chris Olah,Catherine Olsson, Dario Amodei, Nicholas Joseph,Jared Kaplan, and Sam McCandlish. 2022. Scalinglaws and interpretability of learning from repeateddata. In arXiv. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,Elena Buchatskaya, Trevor Cai, Eliza Rutherford,Diego de las Casas, Lisa Anne Hendricks, JohannesWelbl, Aidan Clark, Tom Hennigan, Eric Noland,Katherine Millican, George van den Driessche, Bog-dan Damoc, Aurelia Guy, Simon Osindero, KarenSimonyan, Erich Elsen, Oriol Vinyals, Jack WilliamRae, and Laurent Sifre. 2022. An empirical analysisof compute-optimal large language model training.In Advances in Neural Information Processing Sys-tems.",
  "Yehuda Koren, Steffen Rendle, and Robert Bell. 2022.Advances in Collaborative Filtering, pages 91142.Springer": "Percy Liang, Rishi Bommasani, Tony Lee, DimitrisTsipras, Dilara Soylu, Michihiro Yasunaga, YianZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-mar, Benjamin Newman, Binhang Yuan, Bobby Yan,Ce Zhang, Christian Cosgrove, Christopher D. Man-ning, Christopher R, Diana Acosta-Navas, Drew A.Hudson, Eric Zelikman, Esin Durmus, Faisal Lad-hak, Frieda Rong, Hongyu Ren, Huaxiu Yao, JueWang, Keshav Santhanam, Laurel Orr, Lucia Zheng,Mert Yuksekgonul, Mirac Suzgun, Nathan Kim,Neel Guha, Niladri Chatterji, Omar Khattab, PeterHenderson, Qian Huang, Ryan Chi, Sang MichaelXie, Shibani Santurkar, Surya Ganguli, TatsunoriHashimoto, Thomas Icard, Tianyi Zhang, VishravChaudhary, William Wang, Xuechen Li, Yifan Mai,Yuhui Zhang, and Yuta Koreeda. 2023. Holistic eval-uation of language models. In arXiv.",
  "Frank Nielsen. 2016. Hierarchical Clustering, pages195211. Springer": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, JohnSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,Maddie Simens, Amanda Askell, Peter Welinder,Paul F Christiano, Jan Leike, and Ryan Lowe. 2022.Training language models to follow instructions withhuman feedback. In Advances in Neural InformationProcessing Systems, pages 2773027744.",
  "Vaishnavi Shrivastava, Percy Liang, and Ananya Kumar.2023. Llamas know what gpts dont show: Surrogatemodels for confidence estimation. In arXiv": "Mirac Suzgun, Nathan Scales, Nathanael Schrli, Se-bastian Gehrmann, Yi Tay, Hyung Won Chung,Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi,Denny Zhou, and Jason Wei. 2022.Challengingbig-bench tasks and whether chain-of-thought cansolve them. In arXiv. Franois Torregrossa, Vincent Claveau, Nihel Kooli,Guillaume Gravier, and Robin Allesiardo. 2020. Onthe correlation of word embedding evaluation metrics.In Language Resources and Evaluation Conference,pages 47894797. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,Barret Zoph, Sebastian Borgeaud, Dani Yogatama,Maarten Bosma, Denny Zhou, Donald Metzler, Ed H.Chi, Tatsunori Hashimoto, Oriol Vinyals, PercyLiang, Jeff Dean, and William Fedus. 2022. Emer-gent abilities of large language models. In arXiv.",
  "Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, andDenny Zhou. 2023. Chain-of-thought prompting elic-its reasoning in large language models. In arXiv": "Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang,Prajjwal Bhargava, Rui Hou, Louis Martin, RashiRungta, Karthik Abinav Sankararaman, Barlas Oguz,Madian Khabsa, Han Fang, Yashar Mehdad, SharanNarang, Kshitiz Malik, Angela Fan, Shruti Bhosale,Sergey Edunov, Mike Lewis, Sinong Wang, and HaoMa. 2023. Effective long-context scaling of founda-tion models. In arXiv. Chengrun Yang, Yuji Akimoto, Dae Won Kim, andMadeleine Udell. 2019.Oboe: Collaborative fil-tering for automl model selection. In Proceedingsof the 25th ACM SIGKDD International Confer-ence on Knowledge Discovery & Data Mining, page11731183.",
  "List of Models and Tasks.The table 2 containsall the models and tasks we have collected": "Description Factors for Models and TasksWehave collected the characteristics of models andtasks in relevant aspects through model cards, tech-nical reports, and academic papers. We have orga-nized and introduced these characteristics, as wellas the corresponding embedding methods, as listedin .Note that during data collection, not all factorsare available. For these missing factors, such asCO2 and GPU hours, we replace them as zero whenentering data.",
  "B.2Data Analysis": "We conducted a statistical analysis of the data wecollected, specifically examining the number ofmodels tested for each task, the number of taskstested for each model, and the number of modelsdescribed by each factor. Since each task is con-sistently associated with four factors, we did notcreate a distribution chart for this aspect.",
  "C.1Evaluation Metrics": "Apart from visualization, we also evaluate themethod based on two types of metrics: 1) SCORE-LOSS Metric: we calculate MSE LOSS and L1LOSS between predicted scores and true scores(normalized) on downstream tasks; 2) RANK-ACCURACY Metric: researchers are sometimes notconcerned with detailed scores but rather the rank-ings the model is in, so we calculate the accuracyof rank derived from the predicted scores, ACCU- RACY and MAE@2. ACCURACY refers to thepercentage of instances where the predicted rankequals the true rank, and MAE@2 refers to the per-centage of instances where the absolute differencebetween the predicted rank and the true rank is in2, the formulation as below:",
  "In this section, we detail the setup of each experi-ment in 5": "Different Prediction Methods.Due to the 44%sparsity of the collected collaboration matrix, weused 5% of the known data as the validation set,with the remaining data serving as the observedtraining set. We trained each model five timesthrough random splitting, deriving an average per-formance and variance. We configured our modelswith latent factors = 10, learning rate = 0.01, anditeration = 250, 000. The is the resultswhen random_seed = 1.",
  "ModelsTasks": "LLama-2-7B, LLama-2-13B, LLama-2-70B, Llama 3 8B, Llama 3 70B,GLM-130B, LLaMA-7B, LLaMA-13B, LLaMA-33B, LLaMA-65B,GPT-3-175B, PaLM-540B, Claude-V3 Haiku, Claude-V3 Sonnet,Claude-V3 Opus, GPT-4, gpt-3.5, BLOOM-176B, Luminous Base-13B,Luminous Extended-30B, Luminous Supreme-70B, OPT-175B,GPT-NeoX-20B, GPT-J-6B, sheared llama-2.7B, sheared llama-1.3B,INCITE-Base-3B, INCITE-Base-7B, TinyLlama-1.1B, OpenLLaMA-3B-v1,OpenLLaMA-3B-v2, Pythia-1.4B, Pythia-2.8B, Falcon-7B,Falcon-40B, Falcon-180B, Mistral 7B, MPT-30B, MPT-7B,chinchilla, Pythia-70M, Pythia-160M, Pythia-410M, Pythia-1B,Pythia-6.9B, Pythia-12B, Gopher - 280B, Gopher - 44M,Gopher - 117M, Gopher - 417M, Gropher - 1.4B, Gopher - 7.1B,MT-NLG 530B, GLaM, Phi-1.5-1.3B, Phi-2-2.7B, Yi-6b, Yi-9b,Baichuan 1-7B, Baichuan 1-13B-Base, Baichuan 2-7B-Base,Baichuan 2-13B-Base, InternLM2-7B, InternLM2-20B, Skywork-13B,BlueLM-7B, Qwen-7B, Qwen-14B, TigerBot-13b, TigerBot-70b,Gemma-2b, Gemma-7b BoolQ(0-shot), BIG-bench hard(3-shot),WinoGrande(0-shot),WinoGrande(1-shot),Winogrande(5-shot),PIQA(0-shot),SIQA(0-shot),HellaSwag(0-shot),HellaSwag(10-shot),ARC-e,ARC-c(0-shot),ARC-c(25-shot),OBQA(zero-shot),MMLU(5-shot),HumanEval(pass@1),MBPP(3-shot),GSM8K(4-shot),MATH(4-shot),TriviaQA(5-shot),NaturalQuestions(0-shot),NaturalQuestions(1-shot),NaturalQuestions(5-shot),NaturalQuestions(64-shot),LAMBADA(0-shot),AGIEval English (3-5 shot),RACE-m,RACE-h,LogiQA,WSC",
  ": List of Models and Tasks": "across all downstream tasks for comparative analy-sis. However, we observed in the literature (Ruanet al., 2024) that a sigmoidal curve with a singlecoefficient and a single bias value represents thescaling law for downstream tasks. Moreover, thiscurves coefficients and bias values have a generalrange across all tasks, w = [0.5, 2], b = .Consequently, we set this range of coefficients andbias for this curve. Then we used the normalizedscores of smaller models within the same modelfamily and their corresponding parameter sizesto fit the scaling law curve for each task. Thisapproach generally follows a pretrain-finetunemethodology. Additionally, CPP-2 refers to ran-domly selecting two scores from the observedperformances of the model to be included in thetraining data. In this experiment, we use factor-enhanced NCF (setting is same as above).",
  "FactorsDescriptionEmbedding": "Model FamilyType of model family, e.g., LLAMA 2, PYTHIACategorical EmbeddingPretraining Dataset Size (B)Data size in millions of tokensNumerical EmbeddingParameter Size (M)Number of model parameters in millionsNumerical EmbeddingGPUhGPU hours consumedNumerical EmbeddingFLOPsFloating-point operations countNumerical EmbeddingContext WindowMax context size in tokens, e.g., 1024, 2048Categorical EmbeddingBatch Size (M)Size of batches in millions,e.g., 1M, 2MCategorical EmbeddingLayersNumber of layers in the modelNumerical EmbeddingNumber HeadsNumber of attention headsNumerical EmbeddingKey/Value SizeSize of key/value in attention mechanismNumerical EmbeddingBottleneck Activation SizeSize of activation in bottleneck layersNumerical EmbeddingCarbon Emission (tCO2Eq)Carbon footprint of trainingNumerical Embedding",
  "The Shapley value algorithm for analyzing fea-ture (factor) importance is computationally inten-sive, which has led to the development of vari-ous approximation methods (Jethani et al., 2022)": "Fortunately, our predictive model involves a man-ageable number of factors, allowing us to use themost accurate direct computation method of Shap-ley values. Specifically, we apply an enumera-tion approach to compute Shapley values on a pre-trained factor-enhanced neural collaborative filter-ing model during the inference stage. This involvessystematically masking factors to assess their im-pact.For the implementation, we mask factors differ-ently based on their data type as outlined in : numerical factors: we set the input factor valuesto zero; categorical factors: we set the correspondingembedding layer parameters to zero.We then compute the difference in validationloss with and without each factor present, providingus with each factors marginal contribution. Thisdetailed approach allows us to quantify preciselyhow much each factor contributes to the modelspredictive performance, providing valuable insightsinto factor importance and model behavior.",
  "D.1Ablation on Sparsity Threshold": "To ascertain whether matrices composed of col-laborative performance data can accurately predictthe performance of LLMs, it is essential to con-sider the critical variable: the matrix sparsity. Weassessed the impact of sparsity on prediction accu-racy by manipulating the sparsity of the trainingmatrix via masking. This method allowed us toobtain a reliable measure of average accuracy, asillustrated in Figure. 9. It is noteworthy that ourmethod of controlling sparsity only reduces thenumber of training samples. We ensured fairnessin each comparative experiment by maintaining aconsistent validation set throughout. During theexperiment, we maintained the same settings forthe learning rate and number of iterations as inthe main experiment. To ensure the robustness ofour experimental results, each reported outcomerepresents the average score after conducting fiverandom splits.The data we collected inherently has a sparsityof 44%. Hence, we only have the remaining 46%of collaborative data. As sparsity levels range from49.60% to 88.80%(masking 10% to 80% of thecollaborative data), the graph shows a pronouncedincrease in L1 Loss and a decrease in Accuracy,indicating deteriorating model performance with",
  ": Relationship between matrix sparsity and three keyperformance metrics: L1 Loss, Accuracy, and MAE@2": "higher sparsity, especially when sparsity exceeds60%, where there is a significant drop in accuracy.Conversely, MAE@2 remains relatively stable be-fore experiencing fluctuations, suggesting varyingimpacts on this metric. Interestingly, accuracy evenimproves when sparsity reaches 50%. We think thepossible reason for this might be the presence ofan optimal level of information reduction that re-moves redundant or noisy data without significantlycompromising signal integrity. This phenomenonsuggests that a moderate level of sparsity could po-tentially enhance model performance by focusingon more relevant factors.",
  "D.2Ablation on Predicting Performance onComplex Reasoning and CoT Tasks": "From the model perspective, it is crucial for validat-ing the feasibility of predictive methodologies to as-sess the predictive accuracy on special tasks poten-tially exhibiting emergent phenomena (Suzgunet al., 2022; Wei et al., 2022), including complexreasoning and Chain of Thought (CoT) tasks (Weiet al., 2023). Emergent phenomena refers to thechallenges associated with predicting performancefrom smaller models when the scale of a model ex-pands significantly, resulting in discontinuous leapsin model capabilities. The existence of this phe-nomenon is subject to ongoing debate. Nonetheless,recent efforts (Ganguli et al., 2022b; Hu et al., 2024;Owen, 2024; Ruan et al., 2024; Schaeffer et al.,2023) continue to focus on how scaling laws canbe modified to mitigate the gap between smallerand larger models. This may involve modifyingmetrics or incorporating additional data points tolinearize the growth curve or alternatively optingfor a sigmoidal curve.Theoretically, these challenges are not too dif-ficult for our prediction method, as the underly-ing mechanism of emergent abilities reflects atype of similarity. This commonality manifestswhen models exceed a certain scale. By analyzingcross-model similaritieshow other larger mod-els demonstrate emergent capabilities compared to their smaller counterpartswe can enhance ourpredictive accuracy for the current model. Overall,these tasks are pivotal for comprehensive valida-tion processes, e.g., GSM8K (Cobbe et al., 2021),BBH (Suzgun et al., 2022), HUMANEVAL (Chenet al., 2021) and MBPP (Austin et al., 2021).In detail, if we want to evaluate the performanceof predicting a model on these special tasks, thetraining data is the performance information fromother model families, the smaller model of the samefamily, and the randomly selected two non-specialtasks prior to the performance of this model. Inour experiment, we tested the 4 models on thesetasks, and then we plotted the test results on Fig-ure 10. As illustrated in , our predictivescores are more adaptive to each task, where thepoints are close along the perfect prediction line,which means our prediction method captures thesimilarity in the specific task across models. Ourproposed methods MSE Loss is comparable to thescaling law, which shows the feasibility of CPP (inCPP-2).",
  ": Comparison of the predictive performance ofcollaborative performance prediction (CPP) versus traditionalscaling laws (SL) for Large Language Models (LLMs) inComplex Reasoning and CoT Tasks": "Generalization to Completely New Tasks.Aspresented in Tab. 5, CPP-T0 and CPP-T2 havea relative small error, demonstrating our methodCPP shows reliable generalization. When CPP-T2 has the prior performance of two models in thistask, it has a significant drop compared to CPP-T0. These two experimental results inspire us thatprediction and evaluation should be interactive, i.e.,",
  "D.3Correlation between Models": "Experiment.We conducted a leave-one-out ex-periment to test the impact of Model A on the pre-dictive performance of Model B. This involvedmasking Model A and using the performance ofother models to train predictive methods, whichwere then validated on Model B to observe changesin loss. This approach generated a matrix with themasked model names on the X-axis and the vali-dation model names on the Y-axis, with the valuesrepresenting the change in loss.The Leave-one-out experiment is a robustmethod commonly used in statistical analysis. Toassess the impact of different models on the pre-dictive performance of a specific model, we imple-mented a strategy where we systematically maskedeach selected model in the training set. The pro-cedure involved masking each model individuallyand then training and testing the loss on a valida-tion model. This process was repeated across allmodels, culminating in creating a matrix whereaxis=0 represents the masked model ID, and axis=1represents the validation model ID. The values inthe matrix correspond to the loss observed. Thisexperiment was conducted under three differentrandom seeds to ensure the stability and reliabilityof the results.Subsequently, each model was used as a vali-dation set, with the remaining data serving as thetraining set to calculate the loss for each model.This also resulted in a matrix where axis=1 indi-cates the validation model ID, and the columns[:,valid model id] represent the corresponding lossfor that validation model. We derived a delta lossmatrix by calculating the difference between thesetwo matrices.Given that each validation model has its ownrange of loss variations, we normalized the deltaloss matrix. We then performed a row-based corre-lation analysis on this normalized matrix to assesseach models impact on predictive performance.The higher the correlation value between the twomodels, their effects on predictions are more simi-lar.",
  "Analysis.Based on this correlation matrix, wefurther conducted a hierarchical clustering analy-": "sis (Nielsen, 2016). The results indicate that a setof models exists that are similar in their impacton the predictive performance of the model. Othermodels are far away from them. (Details in )This analysis not only helps us understand eachmodels specific contributions to predictive perfor-mance but also reveals the similarities and differ-ences in functionality among the models, providinga crucial basis for model optimization and selec-tion.We performed a row-wise correlation analysis 13 on this matrix and discovered that models fromthe same family tend to have similar impacts onpredictions, as do models of the same size. Af-ter conducting a hierarchical distance analysis, weconcluded that a group of models exists that, whenmore performance data is available, can signifi-cantly enhance the accuracy of the predictive mod-els. There are also what might be termed noisemodel performances in our analysis D.3.",
  "We also conducted leave-one-out experimentson these tasks and created a heatmap figure. 14 of": ": The predictive performance (MSE) of CPP in the predictions of the completely new task. Here, CPP-T0refers to the predictive performance of CPP in the predictions of the completely new task, and CPP-T2 refers to thepredictive performance of CPP in the predictions of the task when we only know two models performance on thistask, indicating CPP has no prior knowledge and few cases.",
  "-Mean Shapley Value": ": Instance Distribution of the task factor Shapleyvalue. X-axis represents the Shapley value, which indicatesthe degree of prediction loss change; Y -axis indicates thefactor names in order of importance from top to bottom. Eachpoint represents an instance. the correlations. Tasks with similar targeted abilitytesting capabilities demonstrated similar influences,such as GSM8K, MATH (Hendrycks et al., 2021),ARC (Chollet, 2019), and HUMANEVAL, whichall require complex reasoning abilities.",
  "Llama 3-70B": "L1 Loss: 0.1287L1 Loss: 0.0918L1 Loss: 0.0842L1 Loss: 0.2131L1 Loss: 0.1023 L1 Loss: 0.1199L1 Loss: 0.1944L1 Loss: 0.1782L1 Loss: 0.1819L1 Loss: 0.1002 : Prediction performance of various scaled Language Models on downstream tasks. This figure illustrates regressionplots comparing the predicted versus actual performance normalized scores for a series of large language models, includingLlama-2-70B, Llama-65B, Falcon-180B, Gopher-280B, Pythia-12B, Gemma-7B, TigerBot-70B, Qwen-14B, Luminous Supreme-70B, and Llama-3-70B. Each subplot displays a regression line with a shaded 95% confidence interval and includes the L1 lossfor each models predictions, highlighting the accuracy and variability of predictive capabilities across different models.",
  "Distance ClusterModels": "LLama-2-7B, LLama-2-13B, LLama-2-70B, Llama 3 8B,LLaMA-7B, LLaMA-65B, Claude-V3 Haiku, Claude-V3 Sonnet,Claude-V3 Opus, GPT-4, BLOOM-176B, Luminous Extended-30B,Luminous Supreme-70B, OPT-175B, GPT-NeoX-20B, sheared llama-2.7B,sheared llama-1.3B, INCITE-Base-3B, INCITE-Base-7B, OpenLLaMA-3B-v1, Pythia-1.4B,Pythia-2.8B, Pythia-70M, Pythia-410M, Pythia-6.9B,Gopher - 280B, Gopher - 44M, Gopher - 117M, MT-NLG 530B, GLaM,Baichuan 1-7B, Baichuan 1-13B-Base, Baichuan 2-7B-Base, Baichuan 2-13B-Base,Skywork-13B, Qwen-7B, Qwen-14B, TigerBot-13b,Gemma-2b, Gemma-7b"
}