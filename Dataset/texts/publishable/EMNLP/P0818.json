{
  "Abstract": "Large Language Models (LLMs) have signif-icantly advanced natural language processing,demonstrating exceptional reasoning, tool us-age, and memory capabilities. As their appli-cations expand into multi-agent environments,there arises a need for a comprehensive evalua-tion framework that captures LLMs reasoning,planning, collaboration, and other social abili-ties. This work introduces a novel competition-based benchmark framework specifically de-signed to assess LLMs within multi-agent set-tings, providing quantitative metrics to evalu-ate their judgment, reasoning, deception, self-awareness, cooperation, coordination, and ratio-nality. We utilize two social deduction gamesalongside three game-theory scenarios to cre-ate diverse environments. Our frame is forti-fied with the probabilistic graphic modeling(PGM) method, enhancing the LLMs capabili-ties in navigating complex social and cognitivedimensions. We evaluate seven LLMs, quan-titatively highlighting a significant capabilitygap of over threefold between the strongest,GPT o1, and the weakest, Llama-2-70B. It alsoconfirms that our PGM enhancement boosts theabilities of all selected models by an averageof 37%. Our data and code can be found here",
  "Area": ": The radar chart depicts LLMs performanceon 7 metrics, with -T for -turbo and +P for+PGM. The bar chart displays the polygons areas,and the red line indicates average game-winning rates.Larger areas correlate with higher winning rates, val-idating the effectiveness of the proposed metrics forassessing LLMs capabilities. For more information,refer to Sec. 5. There is an increasing interest in investigatingLLMs behaviors as agents in single- or multiple-agent systems. Noteworthy examples include Gen-erative Agents (Park et al., 2023), Camel (Li et al.,2023a), Auto-GPT (Richards, 2023), and Voyager(Wang et al., 2023). Meanwhile, quantitative assessment of LLMsas agents is crucial for their advancement. Recentbenchmarks, such as Liu et al. (2023), evaluateLLM-as-Agent in multi-turn contexts, while con-current work by Wu et al. (2023) tests them ingames requiring reasoning and planning. However,these studies focus on understanding and reason-ing in environments, overlooking true interactioncapabilities in multi-agent systems. Other research, including Agashe et al. (2023) social aspects oncoordination, and Fu et al. (2023) on bargaining,explores specific skills in multi-agent scenarios.While these studies provide useful insights intoLLMs certain capabilities, their scope is limitedand lacks quantitative metrics.We have observed three key characteristics in in-teractive multi-agent systems. (1) Agents in thesesystems often operate within the confines of theirlocal perspectives. However, making wise deci-sions typically necessitates a good understandingof global information. To overcome this limitation,agents must adeptly discern contexts and reasonabout the roles or plans of other agents. (2) Con-texts are inherently dynamic in multi-agent systemsdue to the dependent nature of agents decisions.Success hinges on the ability to swiftly adapt strate-gies in response to evolving contexts. (3) Collabo-ration and competition are inevitable when multipleagents try to solve tasks together. The ability topromote cooperation while preserving self-interestis often the ultimate goal of multi-agent systems.Inspired by the above characteristics, we firstpropose a competition-based benchmark to eval-uate the abilities of LLMs as agents by compet-ing with a fixed type of LLM. Besides, sevenquantitative metrics from the competitions areproposed to measure the essential capabilities ofLLMs (Wooldridge, 2009; Minsky, 1988). We de-fine these capabilities from four aspects: cogni-tion, adaptability, rationality, and collaboration: (1)Judgment and reasoning form the core cognitionof agents, crucial for accurate information estima-tion in uncertain scenarios. Judgment evaluatesthe ratio of the final correct decisions. Reason-ing measures the ability to logically analyze otheragents roles and strategy formulation, thus guid-ing agents to make correct decisions in uncertainty.(2) Self-awareness and deception are key to en-hanced adaptability in agents, vital for multi-agentsystem. Self-awareness is an assessment of agentsunderstanding of their capabilities and roles, en-suring the consistency of behaviors towards thetarget. Deception enables agents to subtly manipu-late information in competitive settings, influenc-ing other agents decisions and gaining advantagesin social interactions. (3) Rationality serves as ametric to gauge the efficiency of an agents behav-ior. It directs agents toward making decisions withthe aim of optimizing their benefits by consider-ing the potential actions of other agents rather thanresorting to impulsive or uninformed actions. (4) Cooperation and coordination are two facets ofcollaboration, essential for effective teamwork inmulti-agent systems. Cooperation measures com-munication and agreeability. Coordination indi-cates collaboration facilitation.In light of the essential abilities, we further pro-pose a method to enhance LLMs as agents by in-tegrating Bayesian statistical foundations. Thisnovel approach intertwines the Probabilistic Graph-ical Model(PGM) (Koller and Friedman, 2009)with LLMs, thereby amplifying their capacity tocomprehend intricate scenarios and enabling moreinformed and strategic decision-making in multi-agent environments.In summary, our contributions are as follows: We first propose a competition-based bench-mark environment for LLM-powered multi-agent systems by collecting over 100 cases in5 scenarios and designing 7 metrics to evalu-ate the critical abilities in multi-agent systems. We measure 7 LLMs with our benchmark.The results indicate that GPT o1, GPT-4, andGPT-3.5 remain the superior performers, fol-lowed by other commercial LLMs - PaLM2, Claude 2, and Cohere. Different large lan-guage models (LLMs) exhibit varying perfor-mance levels across different evaluation di-mensions and possess distinct characteristics.For instance, GPT-o1 is more discernible witha good judgment score, GPT-4 tends to bemore rational, whereas GPT-3.5 is generallymore cooperative, as shown in . We design a PGM-aware agent that integratesLLMs and symbolic reasoning to fortify itselfin multi-agent systems. PGM-aware agentsoutperform their vanilla versions by 37% onaverage over these abilities. As shown in Fig-ure 1, GPT o1+PGM has achieved impressiveimprovement over the original GPT o1.",
  "Related Work": "Emergent Capabilities of LLMs. Beyond theircore functions, LLMs have shown diverse emer-gent abilities like reasoning, planning, memory andso on. Recent works like Chain of Thought (Weiet al., 2022), Tree of Thought (Yao et al., 2023a),Graph of Thought (Yao et al., 2023b; Besta et al.,2023), and ReAct (Yao et al.) improve LLM rea-soning. API-bank (Li et al., 2023b) benchmarkstool-augmented LLMs, with ToolLLM (Qin et al., 2023) providing a framework using tools. Reflex-ion (Shinn et al., 2023) enhances LLM decision-making, while Phelps investigates economic goal-like behavior (Phelps and Russell, 2023).LLMs-Powered Agents. Advancements in LLMsignite agents dealing with intricate tasks (Richards,2023; Li et al., 2023a; Wang et al., 2023) and morecomplex scenarios involving multiple agents (Parket al., 2023). Auto-GPT (Richards, 2023) demon-strates GPT-4s capabilities in achieving goalsthrough chained thoughts. Generative Agents (Parket al., 2023) describes a sandbox with 25 AI agentssimulating human actions, recording experiencesfor deeper self-awareness. Meanwhile, recent andconcurrent studies conduct benchmarking for LLM-powered agents. Some studies (Liu et al., 2023;Wu et al., 2023; Gioacchini et al., 2024) evalu-ate the capabilities of LLM-powered single agentswithin games or real-life environments. Othersstudy LLMs social abilities in multi-agent sys-tems. Agashe et al. (2023) explores coordinationability. Abdelnabi et al. (2023) assess the delib-eration ability of LLMs in negotiation games. Aconcurrent work (Huang et al., 2024) tests LLMsin 8 game theory scenarios, analyzing the abili-ties of one LLM in multi-agent playing. However,these works still lack quantifiable measurementsfor social abilities.",
  ": Overview of evaluation setting, scenarios, andproposed metrics": "We propose to measure the abilities of variousLLMs by putting them into competitions of multi-agent scenarios. In this way, we measure the gen-uine capabilities of LLMs when interacting withmultiple other agents. To achieve this, we have con-structed a comprehensive benchmark that incorpo-rates various competition settings and meticulouslydesigned metrics for each scenario. The detailedconstitutes are illustrated in .",
  "Scenarios": "As mentioned in Sec. 1, the evaluation of agentsin multi-agent systems revolves around crucial at-tributes such as cognition, adaptability, rationality,and collaboration. We select scenarios accordingto two scenarios: 1) complex enough, requiringagents to exhibit good global comprehension andinformation manipulation ability; 2) emphasizingboth collaboration and rationality to balance bothglobal and self-interests. In the game of the so-cial deduction games Chameleon and Undercover,quickly grasping global information and takingclever actions are the keys to winning the game.Thus, we mainly measure the cognition and adapt-ability in these two scenarios. Moving to gametheory scenarios, which require the agent to makeoptimal decisions based on the given premise (My-erson, 1991), they are more apt for reflecting ra-tionality and collaboration. As such, we centerour evaluation on these latter two attributes in thecontext of three game theory scenarios.Chameleon is a social deduction game where play-ers are either a chameleon or a non-chameleon.Non-chameleon players give clues about a secretword. The chameleon player tries to blend in with-out knowing the word. Non-chameleons aim toexpose the chameleon without revealing the word.Undercover, as a similar game, divides playersinto civilians and undercovers. The word for under-covers is different from civilians. Players exploretheir roles by telling from their own and othersclues. Civilians need to find undercover, whileundercovers should hide themselves.Cost Sharing involves multiple parties dividingcosts based on their usage of a shared resource.These parties need to propose and negotiate costallocation solutions. Each party are expected toensure fairness to achieve unanimous agreementand meanwhile reduce their own cost to realizelargest interest.Multi-turn Prisoners Dilemma extends the clas-sic Prisoners Dilemma to a multi-round three-player version. Each participant decides to coop-erate or defect in every round, and the scores aredetermined by collective choices. For example, ifonly one player defect while others cooperate, thebetraying player will get highest score. The gametests players ability to strategize, foster trust, andnavigate group decision-making. The player withthe highest total score at the end of the game isdeclared the winner. Public Good explores similar strategies in Pris-oners Dilemma. Players are given fixed initialresources. They can decide how much to investto a common pool at each round. The total invest-ment from all the players is then multiplied anddistributed to each player evenly. The winner is theplayer possessing the most resources at the end.",
  "Competition Settings": "We propose a competition-based evaluation to en-sure genuine multi-agent interactions and compa-rability among different LLMs. In this setting,different LLMs(referred to as challenger LLMs)challenge the same defender agents (powered by afixed LLM), in the same game settings. Then theircapabilities are evaluated based on the meaningfulintermediate game results, and the winning ratesover defender agents. LLM with higher winingrates are more capable, based on which we canrank the ability of different LLMs. As shown byEvaluation Setting in , GPT-4 is used asthe defender LLM, and other LLMs challenge to bethe champion. Weve gathered a collection of casesfor each scenario mentioned above. The detailedcompetition procedures and collection process arein A.1.",
  "siSwsi(1)": "Here S is the set of roles the challenger LLM playsin all the scenarios. In Chameleon and Undercover,the challenger LLM play the Chameleon, Non-Chameleons, Undercovers, and Civilians respec-tively. As for game theory scenarios, the challengerLLM plays one of the players. Thus, the length ofS is 7 in our benchmark. For each role, we havedefined the criteria for winning and denote the winrate as wsi, si S. The detailed definitions forwinning rates of all roles can be found in A.2.Judgement measures the final understanding of theglobal information, essential for assessing LLMsability to distinguish other players identities based",
  "SJ = ncv/nv(2)": ", where ncv and nv are the number of correct votesand total votes when the challenger LLM are play-ing civilians and non-chameleons.Reasoning evaluates the correctness of agentsanalysis about multiple parties, which often re-quires multi-hop logical reasoning based on theglobal settings and partial information from otherplayers. We ask each player deduces other playersroles and also predict a step further about otherplayers deductions. By comparing these deduc-tions with the gold situations and other playerstrue subjective deductions, we can decide whethertheir rightfulness. We denote number of these twotypes of deductions as ngold and ninter. The numberof correct deductions as nc_gold and nc_inter. TheReasoning is defined as:",
  "SD = nwuc/nuc + (nwcg/ncg)(4)": "where nwuc and nuc are the win count and totalcount of games when the LLM plays chameleonand undercover, nwcg is the number of incorrectcode guesses, and ncg is the total number of codeguesses. Here, we assign a weight = 0.25 due tonot all the games trigger code guesses.Self-Awareness measures correct role identifica-tion, ensuring correct and consistent behavior fol-lowing their own roles.",
  "Sself = (ncrc/nrc) + ncru/nru(5)": "where ncrc, nrc is the number of correct and thetotal number of role identifications in chameleon,and ncru, nru. = 0.6 is used because it is mucheasier to identify roles in a chameleon game.Cooperation. The ability to cooperate with otherplayers and achieve a common goal. Our bench-mark measures it in cost-sharing games, showcas-ing an agents effect on the collective efficacy of",
  "A: LLM": "Context: \"$: Player A It's something you drink from.$: Player B It's often used in the morning.$: Player C It's usually made of ceramic.%\"$: Player A It can be filled with hot or cold beverages.%\"$: Player B It can have a handle.%\"$: Player C It's often found in a kitchen.",
  "Scoord = npcs/nwcs(7)": "where npcs is the number of successful collabo-rations proposed by the challenger LLM in thecost-sharing games.Rationality captures the agents ability to act ra-tionally to optimize their own interests accordingto the rules of the game theory scenarios (Myer-son, 1991). Suppose there are Tpd, Tpg rounds ineach competition for Prisoners Dilemma and Pub-lic Good. The Rationality is defined as:",
  "PGM-Aware Agent": "In AI, Bayesian methods embody symbolism,while large language models (LLMs) exemplifyconnectionism. Despite their individual strengths,effectively combining these approaches remains achallenge. LLMs are proficient in complex lan-guage tasks but still struggle with ambiguous rela-tionships and causal reasoning. This shortcoming is especially evident in multi-agent scenarios re-quiring complex inferential analysis. To addressthis, we propose integrating Probabilistic Graphi-cal Models (PGMs), classic Bayesian tools adeptat depicting dependencies between random vari-ables, to enhance LLMs analytical and inferentialcapabilities.",
  "PGM Structure": "We leverage PGM to depict intricate dependencyrelationships among all agents, thereby augmentingthe LLMs comprehension of the global informa-tion. This heightened understanding can subse-quently facilitate informed actions/decisions. ThePGM should be comprehensive and thorough toensure wise decision-making for an agent. Forinstance, considering the prisoners dilemma sce-nario, before deciding to defect or cooperate, it iscrucial to anticipate whether others might defector cooperate and, from others perspectives, howyou will decide. If you anticipate that other play-ers cooperate and they expect the same from you,but you choose to defect, it can lead to a signifi-cant advantage for you. As a result, We design thePGM structure in a two-hop understanding mech-anism in which the agent analyzes from its ownperspective and perspective when it stands in otheragents shoes. This is highly relevant to the psycho-logical concept of Theory of Mind(ToM) (Bakeret al., 2011; Oguntola et al., 2023), which is the ca-pacity to comprehend human actions by predictingtheir unknown beliefs and desires. We employ theProbabilistic Graphical Model (PGM) to provide ageneral formalization of this concept. PGM usesgraphs to illustrate the conditional dependenciesbetween random variables (Koller and Friedman,2009), making it particularly suitable for under-standing interactions among multiple players.Formally, as shown in , suppose there are three players A, B, and C, in one game andtheyve played the game for i 1 turns and formedthe context Ti1 = {a1, b1, ..., ai1, bi1, ci1, }.Here a, b, and c are the decisions from PlayersA, B, and C, respectively. As a PGM-aware player,B manages three distinct random variables, denotedas B1, B2, and B3, representing Bs interpretationsof the global status from A, B, and Cs perspective.We obtain the estimation for these random variablesby prompting LLMs through different prompts aslisted in A.7, Ppgmj, j :",
  "LLM Leaderboard": "We evaluate GPT-3.5-turbo (OpenAI, 2023a), GPT-4 (OpenAI, 2023b), Llama-2-70B (Touvron et al.,2023), PaLM 2 (Anil et al., 2023), Cohere (Cohere,2023) and Claude 2 (Anthropic, 2023) with ourbenchmark. In , we clearly compare the ca-pabilities of different LLMs. The most prominentperformer is the GPT-4-turbo method, showcasingoutstanding overall performance with a remarkablewin rate of 57.2%. This significantly higher winrate underscores its competitive advantage. Follow-ing closely is GPT-4, which achieves a win rate of53.3%, demonstrating its competitiveness.Furthermore, as illustrated by the radar chartin and the corresponding area calcula-tions in the lower bar chart, GPT-4-turbo surpassesLlama-2-70B by more than threefold in overallmulti-agent capabilities. Additionally, GPT-3.5-turbo also demonstrates superior performance com-pared to Llama-2-70B. Our evaluation of other pop-ular commercial LLMs, including PaLM 2, Claude2, and Cohere, shows that their multi-agent abilitiesfall between those of GPT-3.5-turbo and Llama-2-70B. Notably, indicates that the area sizesderived from the proposed abilities values are di-rectly proportional to the winning rates. This corre-lation validates our benchmark as an effective toolfor assessing the capabilities of different LLMs.As demonstrated in , we conducted adetailed comparison by evaluating metrics suchas Judgment, Deception, Reasoning, and Self-Awareness within the Chameleon and Undercoverscenarios. In these contexts, GPT-o1 present im-pressive scores in Judgment, Self-awareness, Co-operation and Coordination. GPT-4 excelled withscores of 90% in Judgment and 75.0% in Decep-tion, solidifying their superiority in these scenarios.The performance gap in reasoning abilities amongthe models was narrow, while deception capabil-ities showed significant disparities. On the otherhand, when assessing metrics related to collabora-tion, coordination, and rationality in game theoryscenarios like Cost Sharing, Prisoners Dilemma,and Public Good, GPT-4 and GPT-4 Turbo contin-ued to shine. GPT-4 achieved 66.7% in Coordina-tion and an optimal performance of 78.1% in Ra-tionality. In contrast, Llama-2-70B, while laggingin overall performance with a win rate of 26.5%,exhibited strengths in specific metrics, such as arelatively high self-awareness score of 53.2%, sur-passing GPT-3.5 Turbos 25.9%.",
  "PGM Enhancement Performance": "As shown in , the green section highlightsthe effectiveness of the PGM-aware approach. Thisenhancement is particularly pronounced in thePaLM, Claude 2, and Llama2 models, as detailed inA.3. Overall, the PGM-aware method has achievedaverage improvements across all capabilities by amargin of 37%, calculated by comparing the radarareas achieved by vanilla in and PGM-aware methods in . PGM-aware meth-ods have increased the win rate in all scenarios by6.57%.For each capability, as illustrated in A.3, PGM-aware methods have achieved an 8.72% increase inJudgement, confirming the methods ability to en-hance analysis in LLMs. Reasoning and Deceptionabilities have seen improvements of approximately5% and 6%, respectively. Notably, the most signif-icant enhancements are observed in Coordinationand Rationality, with improvements of 12.2% and 13%. Weve also done significance tests(t-test) foreach pair of the vanilla LLM and its PGM-awareversion. Most PGM-aware agents significantly out-perform their vanilla counterparts in 3 or 4 out ofthe 7 abilities, as shown in , with p-valueslower than 0.05. GPT-3.5-turbo is also significantlyimproved in rationality and deception, and GPT-4is significantly improved in Cooperation.",
  "Analysis": "The above experimental results prove that the PGM-Aware agent can improve the performance of eachmetric to varying degrees. The discussion partexplores the influence of PGMs from other aspectsin each scenario.RQ1. How PGM of different LLMs help Judge-ment and Reasoning?In , we pro-vide a case of Llama-2-70B, GPT-4, and theirPGM-Aware versions playing as non-chameleonsversus GPT-4 as the chameleon.For Llama-2-",
  "Llama-2-70BGPT-4": "Player 1 => It's juicy.Player 2 => It's sweet.Player 3 => It's red.Moderator => Now vote which of the other players (excluding yourself) is the chameleon Player 1 => I vote for Player 3.Player 2 => I vote for Player 1.Player 3 => I vote for Player 1.Moderator => The most-voted player is Player 1. The accusation is incorrect. Player 2 is the chameleon. Chameleon won 2 credits. Player 1 => It's tropical.Player 2 => It's sweet.Player 3 => It's yellow inside.Moderator => Now vote which of the other players (excluding yourself) is the chameleon Player 1 => I vote for Player 2.Player 2 => I vote for Player 1.Player 3 => I vote for Player 1.Moderator => The most-voted player is Player 1. The accusation is incorrect. Player 2 is the chameleon. Chameleon won 2 credits.",
  "Player 3: [[0.337 0.326 0.337][0.33 0.33 0.341][0.403 0.365 0.232]]": "Moderator => Now vote which of the other players (excluding yourself) is the chameleon.Player 1 => I vote for Player 2.Player 2 => I vote for Player 3.Player 3 => I vote for Player 2.Moderator => The accusation is correct! Player 2 is the chameleon! Now Player 2 can guess the secret code. Player 2 => I guess the code is Apple\".Moderator => Player 2 guessed the code wrong! Non-Chameleon won!",
  "Player 1:[[0.237 0.372 0.391][0.333 0.333 0.333][0.345 0.322 0.333]]": "Moderator => Now vote which of the other players (excluding yourself) is the chameleon.Player 1 => I vote for Player 2.Player 2 => I vote for Player 3.Player 3 => I vote for Player 2.Moderator => The accusation is correct! Player 2 is the chameleon! Now Player 2 can guess the secret code. Player 2 => I guess the code is \"Pineapple\".Moderator => Player 2 guessed the code wrong! Non-Chameleon won!",
  "PGM after all clues:": "Player 1 => As Player 1,I think: Player 2 is more suspicious, because the clue \"It's sweet\" is more specific to the secret word \"Mango\" than my clue.(Opposite conclusion and explanation)Player 3 is no change, because I havent seen .I think now Player 2 thinks: Player 1 is no change, .Player 3 is more suspicious, .I think now Player 3 thinks:Player 1 is no change, because Player 1 hasnt given a clue yet. .(Hallucination)Player 2 is more suspicious, because Player 2's clue is more specific to the secret word than Player 1's clue. (Opposite conclusion and explanation)(omitted PGM text analysis ) : A case study on Chameleon, Llama-2-70B, GPT-4, and their PGM-enhanced versions. The numericalprobabilities are calculated by extracting judgments in the text-based PGM and normalized into scale of 0 to 1. 70B and GPT-4, we can find both LLMs failedto win the game because they voted for the wrongchameleon Player 1. After being equipped withPGM, both models change the game results toNon-chameleons won. If we look into the detailsof the game process, in the first two clues, thePGMs given by LLama-2-70B all indicate wrongchameleons, as highlighted in red in , sincePlayer 2 is the true chameleon. In the third round,Player 1 gave another right analysis which suc-cessfully changed the PGM to the right indicationof the chameleon. However, if we look at the con-tent of the analysis, we find the analysis gave op-posite conclusions and explanations. For exam-ple, more suspicious is more specific to the se-cret word. Besides, LLama-2-70B also presentssome hallucinations in the game, for example, afterPlayer 1 already gave the clue, the analysis stillstates Player 1 hasnt given a clue yet. WhileGPT-4+PGMs analysis aligns the conclusions andexplanations well and has no hallucinations. Ac-cording to the example, we can find PGM couldbe helpful for models to make better judgmentsthrough clear analysis and PGM is affected by theability of LLMs. The more powerful the model, themore accurate its judgment and reasoning.RQ2. Does Collaboration correlate with Costin Cost Sharing?As shown in , welist the win rate(WR) results and several important",
  "Llama-242.837.10.06.050.0139.1Llama-2+P52.437.638.59.864.8109.5GPT-3.5-T57.137.333.39.579.5166.2GPT-3.5-T+P71.434.252.411.657.1139.8GPT-447.630.542.99.9561.9175.3GPT-4+P61.930.376.210.685.7144.1": ": Detailed results in game theory scenarios.Cost, Score, and Payback are the average cost,the final score, and the average payback the challengerLLM got in the Cost sharing, Prisoners Dilemma andPublic Good, respectively. indicators in each game theory scenario. For cost-sharing, we calculated the average final cost thechallenger LLM needs to bear after their negotia-tions. In the negotiation, this is another target theLLM-powered agent should consider when tryingto reach an agreement with other agents. How-ever, these two aspects can contradict each othersometimes. For example, when the player tries toreduce the cost of himself as much as possible, itmight be hard for him to achieve agreement withother players. The LLMs need to make a balancebetween these two aspects. According to the re-sults in , we find that within the modelswithout PGM enhancement, GPT-3.5-turbo won inWin Rate while GPT-4 won in Cost, indicating bothmodels are not well-balanced. If we compare the results with PGM, GPT-4+PGM increases the WinRate and keeps the cost slightly lower. GPT-3.5-turbo+PGM increases the Win Rate and reducesthe cost simultaneously. This proves the effective-ness of PGM enhancement and demonstrates thatGPT-3.5-turbo tends to be more collaborative whileGPT-4 emphasizes the reduction of cost.RQ3. Does Rationality correlate with reward?Similar phenomena happen in Prisoners Dilemmaand Public Good as illustrated in . In thesetwo scenarios, a player is more likely to win whenhe chooses to betray as a prisoner or chooses toreduce contribution to the common pool in the pub-lic good game. The behavior is considered Ra-tional in our metrics. When most of the playersare playing rationally, the scores and payback willbe much lower, thus approaching the well-knownNash Equilibrium (Kreps, 1989). In the PrisonersDilemma, if we compare GPT-3.5+PGM and GPT-4+PGM, GPT-4+PGM won more but got lowerscores, showing that GPT-4+PGM made more ra-tional decisions than GPT-3.5-turbo+PGM. In Pub-lic Good, we found models with PGM all achievedhigher Win Rates but lower payback because theyall performed more rationally in this scenario. Ifwe compare the payback within models with orwithout PGMs, we can observe higher payback forGPT-4 models, which proves that GPT-4 modelsare more strategic in these games.",
  "Discussion: Generalization ofBenchmark": "Beyond the scenarios mentioned in this paper, ourbenchmark can be generalized to more scenariosor tasks. In general, it evaluates agent behaviorsin settings where the participants of a multi-agentsystem are usually local-viewed and need abili-ties involving Cognition, Adaptability, Rationality,and Collaboration to deduce the global informa-tion and make decisions, thus achieving the finalgoal. Judgment evaluates an agents ability to ac-curately assess unknown information, such as rolesin games like Chameleon and Undercover. Rea-soning checks if an agents perspective aligns withthe actual and others views, offering a nuancedunderstanding. Self-awareness adapts to scenarioswith undisclosed roles, while Deception looks athow well an agent can influence others with false in-formation. Cooperation and Coordination gaugethe effectiveness of collaborative efforts, measur-ing agreement and the quality of proposals, respec-",
  "Conclusion": "Our research presents a benchmarking frameworktailored for evaluating LLMs in multi-agent en-vironments. This frameworks incorporation ofdiverse scenarios has enabled a quantitative as-sessment of seven critical abilities for LLMs inmulti-agent systems, including judgment, reason-ing, deception, self-awareness, cooperation, coor-dination, and rationality. The integration of PGMenriches LLMs with structural reasoning abilityin multi-agent scenarios. Our quantitative anal-ysis of 7 different multi-agent systems poweredby various LLMs, including GPT-4-turbo, GPT-4,GPT-3.5-turbo, PaLM 2, Claude 2, Cohere, andLlama2-70B, has revealed their capabilities dispar-ity. Notably, GPT-4-turbo still emerged as the mostcapable, outperforming others by a threefold mar-gin. Moreover, the PGM enhancement amplifiesthe inherent abilities of these models by 37%. Thisshows our benchmarks effectiveness and PGMspotential to enhance LLM capabilities.",
  "Limitation": "This paper proposed a benchmark for measuringthe ability of LLMs in multi-agent systems. Weconclude our limitation as below: Firstly, our in-vestigation of LLMs in multi-agent settings is inits preliminary stage. The scope of game scenar-ios and topic settings needs to be significantly ex-panded. Secondly, the PGM-aware method hasthe potential to enhance LLMs capabilities in theface of complex multi-agent settings. However, theprocess of integrating these incremental abilitiesinto LLMs through methods such as fine-tuningrequires further exploration. Third, whether theproposed metrics help in some of the real-life sce-narios is not explored in this paper.",
  "Ethical Considerations": "Our work introduces 5 multi-agent scenarios toevaluate LLMs. Most of the processes are undercontrol with suitable prompts, so there is a lowpossibility of producing offensive outputs. We alsochecked all the data we use to ensure no personaldata and unsuitable content included. All the dataused in our scenarios are collected from public",
  "Saaket Agashe, Yue Fan, and Xin Eric Wang. 2023.Evaluating multi-agent coordination abilities in largelanguage models. arXiv preprint arXiv:2310.03903": "Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-son, Dmitry Lepikhin, Alexandre Passos, SiamakShakeri, Emanuel Taropa, Paige Bailey, ZhifengChen, Eric Chu, Jonathan H. Clark, Laurent ElShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-rav Mishra, Erica Moreira, Mark Omernick, KevinRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,Yuanzhong Xu, Yujing Zhang, Gustavo HernandezAbrego, Junwhan Ahn, Jacob Austin, Paul Barham,Jan Botha, James Bradbury, Siddhartha Brahma,Kevin Brooks, Michele Catasta, Yong Cheng, ColinCherry, Christopher A. Choquette-Choo, AakankshaChowdhery, Clment Crepy, Shachi Dave, MostafaDehghani, Sunipa Dev, Jacob Devlin, Mark Daz,Nan Du, Ethan Dyer, Vlad Feinberg, FangxiaoyuFeng, Vlad Fienber, Markus Freitag, Xavier Gar-cia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, JoshuaHowland, Andrea Hu, Jeffrey Hui, Jeremy Hur-witz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-ski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,Sneha Kudugunta, Chang Lan, Katherine Lee, Ben-jamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li,Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,Frederick Liu, Marcello Maggioni, Aroma Mahendru,Joshua Maynez, Vedant Misra, Maysam Moussalem,Zachary Nado, John Nham, Eric Ni, Andrew Nys-trom, Alicia Parrish, Marie Pellat, Martin Polacek,Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,Bryan Richter, Parker Riley, Alex Castro Ros, Au-rko Roy, Brennan Saeta, Rajkumar Samuel, ReneeShelby, Ambrose Slone, Daniel Smilkov, David R.So, Daniel Sohn, Simon Tokumine, Dasha Valter,Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang,Pidong Wang, Zirui Wang, Tao Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, LintingXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, StevenZheng, Ce Zheng, Weikang Zhou, Denny Zhou, SlavPetrov, and Yonghui Wu. 2023. Palm 2 technicalreport. arXiv preprint arXiv:2305.10403.",
  "Anthropic. 2023. Claude 2": "Chris Baker, Rebecca Saxe, and Joshua Tenenbaum.2011.Bayesian theory of mind: Modeling jointbelief-desire attribution. In Proceedings of the an-nual meeting of the cognitive science society, vol-ume 33. Maciej Besta, Nils Blach, Ales Kubicek, Robert Ger-stenberger, Lukas Gianinazzi, Joanna Gajda, TomaszLehmann, Michal Podstawski, Hubert Niewiadomski,Piotr Nyczyk, et al. 2023. Graph of thoughts: Solv-ing elaborate problems with large language models.arXiv preprint arXiv:2308.09687.",
  "Noah Shinn, Federico Cassano, Beck Labash, Ash-win Gopinath, Karthik Narasimhan, and ShunyuYao. 2023.Reflexion:Language agents withverbal reinforcement learning.arXiv preprintarXiv:2303.11366, 14": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-dlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, andAnima Anandkumar. 2023. Voyager: An open-endedembodied agent with large language models. arXivpreprint arXiv:2305.16291. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in NeuralInformation Processing Systems, 35:2482424837.",
  "A.1Competition Settings": "Setting DefinitionIn Chameleon and Under-cover, there are clearly two opposite roles, theChameleon versus Non-Chameleons and the Un-dercover versus Civilians. The challenger LLMwill play each role. For example, the challengerLLM plays non-chameleons versus GPT-4 as thechameleon, and the challenger LLM plays thechameleon versus GPT-4 as non-chameleons. Thewin rates of the challenger LLM playing differ-ent roles will be calculated separately, which con-tributes to in total 4 win rates.Cost Sharing has no distinct parties. Therefore,we made the challenger LLM as one player to playwith other GPT-4-powered players. The final ratioof successful negotiations is defined as the win rate,which measures how much the LLM contributes tothe agreement when other players are fixed. Simi-larly, for public good and prisoners dilemma, wealso made the challenger LLM as one of the playersand recorded its win rate in these two games. Thedetailed win rate calculations are presented in A.2.As shown in , we present the number ofsettings, corresponding metrics, and setting sam-ples for each scenario. We build 20 settings forchameleon and undercover, respectively. In eachgame, Chameleon includes one round of clue giv-ing while undercover contains 2 rounds. For eachof the game theory scenarios, we collected 21 set-tings.",
  "Collection ProcessIn the Chameleon and Un-dercover scenarios, weve noticed a consistentbias in competition outcomes.Specifically,the Chameleon team has held an advantage in": "Chameleon, whereas in Undercover, the civilianshave tended to win.To rectify this imbalance,we carried out 200 game simulations involving allthree players as GPT-4 with randomly chosen topicsettings. Through these simulations, we pinpointed20 topic settings that promote a more equitablewin rate between the two roles in both Chameleonand Undercover. In these scenarios, the challengerLLM will play both roles to measure different abil-ities such as judgment and deception, etc.For the Cost-Sharing task, we expect all the par-ticipating airlines to share a fixed fee, with the spe-cific share of each airline determined by its opera-tional frequencies at the airport. These frequenciesencompass various factors such as the number offlights, flight sizes, passenger volumes, and more.To facilitate the task, we asked ChatGPT to createa pool of 20 detailed descriptions of airline opera-tional frequencies. A topic setting with 3 playersis then constructed by three airline operational fre-quency descriptions from the pool, the role, andthe position of the test LLM. Since there are 3 po-sitions, we randomly selected 7 groups of airlineoperational frequency descriptions to form 21 dis-tinct topic settings.Similarly, for the two-game theory scenarios, weadopt a similar topic construction method as CostSharing. In the Prisoner scenario, three playerschoose to defect or cooperate for 5 rounds.Each player will get a different score dependingon the outcomes of defect or cooperate. Theplayer with the highest cumulative score wins thegame. We have devised 7 distinct scoring settings,and the challenger LLM plays the role of eachplayer across these settings, resulting in 21 uniquecompetitions.In the Public Good game, three players deter-mine the number of points to contribute to a com-munal pool for 5 rounds. These invested points aremultiplied by a specified factor (typically greaterthan 1), and the resulting sum is equally distributedamong all players. Each players final score com-prises their remaining points and the payback fromthe communal pool. The player achieving the high-est score is declared the winner. We establish 7 dif-ferent multipliers and assign the challenger LLMto play each of the three players in these settings,thus generating an additional 21 competitions.",
  "In the chameleon, the outcome can be 0: the non-chameleon won, 1: the chameleon won, 2: even": "voting, and 3: the chameleon guessed right. Inthese four situations, credits gained by the rolechameleon and non-chameleon are cchameleon = and cnon-chameleon = , respec-tively. Suppose the outcomes of the n competi-tions are o. The total credits of all the comple-tions are 2n; the win rate defined in Chameleon is",
  "}}": "{\"test_player_name\": \"Player 3\",\"topic\": \"Fixed Airport Fee: $1,000,000Airline Usage Frequency Data: Airline A:Number of Flights: 90/monthNumber of Passengers: 7,000/monthAverage Flight Duration: 1.75 hoursFlight Size: Primarily small and medium aircraftAirline B:Number of Flights: 50/monthNumber of Passengers: 7,500/month\\Average Flight Duration: 1.5 hoursFlight Size: Primarily small aircraftAirline C:Number of Flights: 50/monthNumber of Passengers: 7,500/monthAverage Flight Duration: 1.5 hoursFlight Size: Primarily small aircraft\",",
  "A.4More Case Studies": "DeceptionAnother advanced cognitive abilityof LLMs extends to their proficiency in strategicdeception within a multi-agent framework. In Fig-ure 7, we delve into the dynamics of LLM perfor-mance when assuming an undercover role againstGPT-4. In this scenario, LLMs are expected toblend in with regular civilians and even give mis-leading clues to conceal their actual roles. In thisexample, GPT-3.5-turbo, GPT-3.5-turbo+PGM lost the game, GPT-4 ended with even voting, and GPT-4+PGM won the game. According to their clues,we found models without PGM didnt tend to de-ceive others, and their clues describe their ownwords. Within these models, GPT-4 is more cau-tious when giving clues, while GPT-3.5 often givesvery straightforward clues, like It can be done ata salon or barbershop and It can be washed withshampoo to describe hair cut.",
  "GPT-3.5-turboGPT-4": "Player 1 => It's something you wear on your head.Player 2 => It's something you often get done at a salon or barbershop.Player 3 => It can be used to change your appearance.Moderator => Now we starting another round of giving cluePlayer 1 => It can be of different colors.Player 2 => It's something that involves cutting and styling your hair.Player 3 => It can be synthetic or natural.Player 1 => I vote for Player 2; Player 2 => I vote for Player 3; Player 3 => I vote for Player 2.Moderator => The most-voted player is Player 2. The accusation is correct. Player 2 is the undercover! Civilians won! Player 1 => It's something you wear on your head.Player 2 => It's something that can change your appearance.Player 3 => It's often used in theater productions.Moderator => Now we starting another round of givingPlayer 1 => It can be made of synthetic materials.Player 2 => It's something that can be styled in different ways.Player 3 => It's often used by people who have lost their hair.Player 1 => I vote for Player 2; Player 2 => I vote for Player 3; Player 3 => I vote for Player 1.Moderator => There are even votes. The accusation does not stand. Player 2 is the undercover.",
  "GPT-3.5-turbo+PGMGPT-4+PGM": "Player 2 (pgm) => Based on the clues given, Player 1's word could be \"wig\" and Player 3's word could be \"mask\" .As Player 2, your word is \"haircut\" which does not fit the clues given by Player 1 and Player 3. Therefore, you can conclude that you are the undercover player in this round.2. I think Player 1 thinks Player 3 is the undercover because Player 3s is more vagueand could apply to various items, while Player 1s ...a specific item.3. I think Player 3 thinks Player 1 is the undercover because Player 1's clue is more specific and indicates a .. while Player 3's clue could apply to multiple items.4. I will try..clue as \"It can be made from human or synthetic hair,\" which is similar toboth a hat and a haircut, but still different enough to not give away my identity",
  "Player 1 => I vote for Player 2; Player 2 => I vote for Player 3; Player 3 => I vote for Player 2.Moderator => The accusation is correct! Player 2 is the undercover! Civilians won!": "Player 2 (pgm) => 1. Based on the clues, it seems like Player 1 and Player 3 are describing a wig. Comparing this to my word, \"haircut\", it's clear that I am the undercover in this game. 2. I think Player 1 thinks Player 3 is the undercover, because Player 3's clue about theatrical performances doesn't align as well with the idea of something you wear on your head or something made from synthetic materials.3. I think Player 3 thinks Player 1 is the undercover, because Player 1's clue about synthetic materials doesn't align as well with the idea of something used in theatrical performances.4. I think the word of other players may be \"wig\". I will try to give my next clue as \"It can be used to cover baldness\", which is similar to them to hide myself.",
  "Player 1 (clue) => It's something you wear on your head.Player 2 (clue) => It's something that can change your appearance.Player 3 (clue) => It's often used in theatrical performances": "Moderator => Now we starting another round of giving cluePlayer 1 (clue) => It can be made from synthetic materials.Player 2 (clue) => It can be used to cover baldness.Player 3 (clue) => It comes in different colorsand styles. Player 1 => I vote for Player 3; Player 2 => I vote for Player 3; Player 3 => I vote for Player 1.Moderator => The most-voted player is Player 3. The accusation is incorrect. Player 2 is the undercover. Undercover won!",
  ": A Undercover case study on GPT-3.5-turbo, GPT-4 and their PGM-enhanced version (*+PGM)": "adopted a more sophisticated strategy, utilizingPGM analyses to decide their roles, identify po-tential undercovers, and outline strategic movesfor subsequent rounds. This strategic advantageallowed these models to give fake clues to dis-guise themselves.For instance, as highlightedin , \"Player 2\" portrayed by GPT-3.5-turbo+PGM and GPT-4+PGM introduced decep-tive clues, such as \"It can be made from human andsynthetic hair\" and \"It can be used to cover bald-ness,\" respectively. However, Player 2 played byGPT-3.5-turbo+PGM, still lost the competition be-cause its first clue looked like copying from Player1. Player 2 played by GPT-4+PGM, won thecompetition because of its consistently good cluesin two rounds. Comparing the results of the threeLLMs with PGM, we can conclude that a more ca-pable LLM can benefit more from the PGM struc-ture.",
  "We have designed different prompts to test LLMssability to make PGM analyses.Two kinds ofprompts are used: text-based(w.PGM) and di-": "rect probability matrix(w.ProPGM). In specific,the former lets the LLM analyze global infor-mation in text, as shown by the example in.The latter requires the LLM to di-rectly give a probabilistic matrix to representthe global information, for example, a matrix[[0.3,0.2.0.5],[0.1,0.4,0.5],[0.3,0.3,0.4]]. The threeroles represent B1,B2, and B3 respectively. Eachelement in a role is the probability of a player beingthe undercover or the chameleon for example. Wecompare the vote accuracy and Win rate of thesetwo kinds of prompts in the scenario chameleon,as shown in . We found that more capableLLMs, like GPT-4 and GPT-3.5-turbo, both kindsof prompts work well. However, for Llama-2-70Bthe text PGM analysis performs much better. There-fore, we mainly choose the prompt the LLMs togive text-based PGM analysis to ensure the help ofPGM on all the LLMs.",
  "Player 2 lose !": "Player 2 In my opinion:As Player 2,I think Player 1 will choose to Cooperate again, because in the first round, Player 1 chose to Cooperate and it is generally beneficial to continue cooperating in order to maintain trust and maximize the overall score.I think Player 3 will also choose to Cooperate again, because in the first round, Player 3 chose to Cooperate and it is likely that they will continue to cooperate in order to establish trust and encourage cooperation from the other players.As for other players,I think Player 1 thinks:Player 2 will choose to Cooperate again, because in the first round, Player 2 chose to Cooperate and it is beneficial for both players to continue cooperating.Player 3 will choose to Cooperate again, because in the first round, Player 3 chose to Cooperate and it is advantageous for both players to continue cooperating.I think Player 3 thinks:Player 2 will choose to Cooperate again, because in the first round, Player 2 chose to Cooperate and it is beneficial for both players to continue cooperating.Player 1 will choose to Cooperate again, because in the first round, Player 1 chose to Cooperate and it is advantageous for both players to continue cooperating Player 2 In my opinion:As Player 2,I think Player 1 will cooperate, because in the previous round, Player 1 chose to cooperate and it is likely that they will continue to cooperate in order tomaintain trust and maximize their own points.I think Player 3 will cooperate, because in the previous round, Player 3 also chose to cooperate and they might continue to do so to maintain a cooperative strategy.As for other players,I think Player 1 thinks:Player 2 will defect, because in the previous round, Player 2 chose to defect and they might continue to do so to maximize their own points.Player 3 will cooperate, because in the previous round, Player 3 chose to cooperate and they might continue to do so to maintain a cooperative strategy.I think Player 3 thinks:Player 2 will defect, because in the previous round, Player 2 chose to defect and they might continue to do so to maximize their own points.Player 1 will cooperate, because in the previous round, Player 1 chose to cooperate and they might continue to do so to maintain trust and maximize Player 2 In my opinion:As Player 2,I think Player 1 will cooperate, because in the previous round, Player 1 chose to cooperate and it is likely that they will continue to cooperate in order tomaintain trust and maximize their own points.I think Player 3 will cooperate, because in the previous round, Player 3 also chose to cooperate and they might continue to do so to maintain a cooperative strategy.As for other players,I think Player 1 thinks:Player 2 will defect, because in the previous round, Player 2 chose to defect and they might continue to do so to maximize their own points.Player 3 will cooperate, because in the previous round, Player 3 chose to cooperate and they might continue to do so to maintain a cooperative strategy.I think Player 3 thinks:Player 2 will defect, because in the previous round, Player 2 chose to defect and they might continue to do so to maximize their own points.Player 1 will cooperate, because in the previous round, Player 1 chose to cooperate and they might continue to do so to maintain trust and maximize their own points. Player 2 In my opinion:As Player 2,I think Player 1 will Defect, because in the previous round, Player 1 chose to Defect when both Player 2 and Player 3 chose to Cooperate. This suggests that Player 1 may be more inclined to prioritize their own gain over cooperation.I think Player 3 will Defect, because in the previous round, Player 3 also chose to Defect. This indicates that Player 3 may have a tendency to defect regardless of the choices made by the other players.As for other players,I think Player 1 thinks:Player 2 will Defect, because in the previous round, Player 2 chose to Defect.Player 3 will Defect, because in the previous round, Player 3 also chose to Defect.I think Player 3 thinks:Player 2 will Defect, because in the previous round, Player 2 chose to Defect.Player 1 will Defect, because in the previous round, Player 1 chose to Defect.",
  ": Features in game theory analysis": "ferently when given different topic settings. Forexample, in Public Good, we chose the multipliers[1,1.2,1.5,1.8,2,2.5,3]. We can find in the upperdiagram in , that even though the mul-tiplier is increasing, all the lines do not show atendency to increase investment, which proves thatall the tested challenger LLMs are not sensitive tothe increased possible payback in the communal pool in our current setting. Similar phenomena alsohappen in Prisoners Dilemma. As shown by thebottom diagram in , the x-axis is the scorecriteria, which consists of three digits, represent-ing the score a player can get in situations wherethe player defects when all the other two defect,the player defects when the other two cooperate,and the player defects when one of the other twodefects, respectively.",
  "RoleGlobal PromptPGM PromptPGM Decision Prompt": "Airport FeeYou are participating in the Airport Fee Allocationgame. You will each represent an airline and splitthe fixed cost of the airport. ## Information: - Thetotal cost of the airport is fixed, and all airlines mustcollectively cover this cost. - Airlines have theirunique usage frequencies at the airport, determinedby factors like flight size and passenger volume. Air-lines with higher usage frequencies are responsiblefor a larger portion of the cost. ## Objectives: - Asan airline representative, your goal is to negotiateand agree upon a cost distribution that is both fairand favorable for your airline. ## Rules: The ne-gotiation will continue for {max_turns} rounds. Ineach round: - Proposal: Each airline proposes a costdistribution. - Vote: Each player must vote for a costdistribution they find acceptable and strive to reacha consensus with other players votes. - The gameends successfully when all airlines vote for the sameproposal. If after {max_turns} rounds of voting noconsensus is reached, the game fails. Strive for asuccessful outcome. Please try to analyze how playerswill adjust their proposal from yourperspective and other players in thenext round.You must follow thetemplate below ( make your com-ments concise):As {player_name},I think {oth_player1} will...I think{oth_player2} will...As for otherplayers, I think {oth_player1} thinks:{player_name} will...{oth_player2}will...I think {oth_player2} thinks{player_name}will...{oth_player1}will... According to your own analy-sis of other players possibledecisions, try to adjust yourproposal in the next round sothat other players will all agreewith you.- If some playersstick to their decision and youthink it is fair, you can fol-low their proposals to achieveagreement among you.",
  "PrisonersDilemma": "You are playing the Iterative Three-Player PrisonersDilemma game. Players 1, 2, and 3 are involved ina crime and are interrogated separately. They caneither choose to betray the others (Defect, D) or co-operate with them (Cooperate, C). The rewards andpenalties are distributed based on the combinationof choices made by the three players.The scoring is as follows: - If all three choose tocooperate, they each get {cooperate} points. - If allthree choose to defect, they each get {defect} point.- If one chooses to defect and the others choose tocooperate, the betrayer gets {one_defect} pointsand the collaborators each get 0 points. - If twochoose to defect and the other chooses to cooperate,the betrayers each get {two_defect} points, and thecollaborator gets 0 points. The game is played for 5rounds, and the total score is calculated at the endto determine the winner. \"\"\" Please try to analyze how other playerswill choose to defect or corporate in thenext round from your own perspectiveand others perspectives. You must usethe template:In my opinion: As {player_name}, Ithink {oth_player1} will xx, because...I think {oth_player2} will xx,be-cause...As for other players, I think{oth_player1} thinks:{player_name}will...{oth_player2} will...I think{oth_player2}thinks{player_name}will... {oth_player1} will... Based on your analysis of oth-ers and others thoughts Re-member to be strategic, estab-lishing trust is to defect beforeothers plan to do it to get thehighest score.choose de-fect or cooperate to achievethe highest points for yourself.You must reply with defector cooperate Public GoodWelcome to the Public Good game. Below are thegames settings and rules: - Each player begins with100 monetary points. Over {game_round} rounds,you decide how many points to contribute to a com-munal pool at each round. - After {game_round}rounds of decision-making, the points in the com-munal pool will be multiplied by a factor of {multi-plier} and distributed equally among all players. - Aplayers final points are the sum of their remainingpoints and the shared points from the communalpool. The player who gets the highest final pointswins the game. - Every player must strategicallyinvest their points to the communal pool to get moreshared points and also be the one who invests theleast to win the game. - Usually, you can get morepayback by investing more when the factor is larger. Please try to analyze whether other play-ers will \"reduce\" or \"increase\" their con-tributions in the next round from yourown perspective and others perspective.- Remember, the payment in the com-munal pool is evenly shared by all play-ers at the end of the game, so youneed to make sure you invest the leastmoney to get the highest repayment. Youmust use the template: In my opinion:As {player_name}, I think {oth_player1}will xx, because... I think {oth_player2}will xx,because...As for otherplayers, I think {oth_player1} thinks:{player_name} will...{oth_player2}will...I think {oth_player2} thinks{player_name} will...{oth_player1}will... Based on your analysis of oth-ers and others thoughts, makethe decision about your owncontribution to achieving thehighest repayment for yourself.Remember - Your total invest-ment should be the least to winthe game; - Your target is to getthe highest points and also pro-mote the communal repaymentto get as many points as pos-sible at the end. You must an-swer with the template I con-tribute xx"
}