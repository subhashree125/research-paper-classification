{
  "Abstract": "Natural Language Inference (NLI) evaluationis crucial for assessing language understandingmodels; however, popular datasets suffer fromsystematic spurious correlations that artificiallyinflate actual model performance. To addressthis, we propose a method for the automatedcreation of a challenging test set without rely-ing on the manual construction of artificial andunrealistic examples. We categorize the testset of popular NLI datasets into three difficultylevels by leveraging methods that exploit train-ing dynamics. This categorization significantlyreduces spurious correlation measures, with ex-amples labeled as having the highest difficultyshowing markedly decreased performance andencompassing more realistic and diverse lin-guistic phenomena. When our characterizationmethod is applied to the training set, modelstrained with only a fraction of the data achievecomparable performance to those trained onthe full dataset, surpassing other dataset charac-terization techniques. Our research addresseslimitations in NLI dataset construction, provid-ing a more authentic evaluation of model per-formance with implications for diverse NLUapplications.",
  "Introduction": "Natural Language Inference (NLI), or textual en-tailment (Dagan et al., 2009), has emerged as anenduring challenge in the field of Natural LanguageProcessing for evaluating the Natural Language Un-derstanding (NLU) capabilities of models. Persis-tently, NLI remains a difficult problem, as it impliesreasoning across several linguistic phenomena todetermine the logical relationship (i.e., entailment,contradiction, or neutral) between two documents -a premise and a hypothesis. The capability to accu-rately infer relationships between sentences is cru-cial for a wide range of applications, such as ques-tion answering (Demszky et al., 2018), dialoguesystems (Welleck et al., 2019), and fact-checking(Thorne et al., 2018; Stab et al., 2018). Since its inception (Dagan et al., 2005), sev-eral large-scale benchmark datasets have been pro-posed for NLI (Bowman et al., 2015; Williamset al., 2018; Nie et al., 2019); however, in time, themost ubiquitously used are the Stanford NaturalLanguage Inference (SNLI) (Bowman et al., 2015)and the MultiNLI datasets (Williams et al., 2018),which have played a pivotal role in advancing thestate of the art (Storks et al., 2019).However, multiple works (Liu et al., 2020; Gu-rurangan et al., 2018; Tsuchiya, 2018; Poliak et al.,2018; Naik et al., 2018; Glockner et al., 2018)pointed out several critical limitations in thesedatasets, stemming from systematic annotation er-rors and spurious correlations that impact both thetraining and test sets. A critical consequence ofthese issues is the inflation of model performance,leading to seemingly high results (Naik et al., 2018;Liu et al., 2020) that may not generalize well toreal-world scenarios. For example, a widely usedRoBERTa model (Liu et al., 2019) trained solelyon the hypothesis achieves an unreasonable accu-racy of 71.7% on SNLI and 61.4% on MultiNLI(random chance being 33%), which strongly pointstowards systematic errors in dataset construction. In this work, we aim to address the limitations ofexisting NLI datasets by proposing an automatedconstruction of a more challenging test set. In con-trast to previous approaches, we avoid manuallycreating artificial examples (Naik et al., 2018); in-stead, we leverage existing samples from the testset. To accomplish this, we generalize dataset car-tography (Swayamdipta et al., 2020) to cluster sam-ples in the test set and characterize them into threecategories of increasing difficulty. Our approachleverages 8 measures of training dynamics of eachpremise-hypothesis pair and is inspired by relatedworks in both NLI (Naik et al., 2018; Geiger et al.,2018; Liu et al., 2020) and approaches tackling theproblem of learning with noisy data (Pleiss et al.,2020; Swayamdipta et al., 2020). We show that our method can isolate examples exhibiting spuriouscorrelations and provide a challenging test set. Fur-thermore, our method is general, model-agnostic,and easily extensible to other datasets (e.g., for fact-checking (Thorne et al., 2018)). Our experimentsshow that using the same method on the training setenables the aggressive filtering of uninformativeexamples during training, reducing data quantitybut increasing quality, enabling the model to obtainon-par performance on the NLI stress test proposedby Naik et al. (2018), using only a fraction of data.We make our code publicly available1.This work makes the following contributions: 1. We denote spurious correlations in the test setsfor two popular NLI datasets - SNLI (Bow-man et al., 2015) and MultiNLI (Williamset al., 2018) and a fact-checking dataset, repur-posed for NLI: FEVER (Thorne et al., 2018).We show statistically significant correlationsbetween the performance of models and thepresence of several measures of spurious cor-relations across labels. 2. We propose a general method for creating astrong test set for NLI. Using a multitude oftraining dynamics features of samples in an ex-isting test set, our method automatically char-acterizes examples in the test set into three in-creasing difficulty levels, which strongly cor-relate with decreased model performance. Ourmethod minimizes spurious correlations, pro-viding a more accurate measure of model per-formance in the real world on NLU tasks. Ourmethod is model-independent and the underly-ing difficulty splits generalize across models. 3. The same procedure applied to the trainingdata achieves similar performance on the testset while using only 33% of the available datafor SNLI and 59% for MultiNLI, surpassingother dataset characterization methods (Pleisset al., 2020; Swayamdipta et al., 2020), in-dicating that our approach can be used as astrong method for increasing data quality. The paper is structured as follows. After empha-sizing the shortcomings of existing NLI datasetsand presenting various stress tests, we introduceour method for test set characterization. Then, wepresent the main results, a comparison with a dif-ferent encoder to argue that our approach is model-",
  "Related Work": "Across the development of natural language infer-ence and understanding systems, multiple large-scale training and testing datasets have been de-veloped over different linguistic domains.Ini-tially, progress was driven by the addition of SNLI(Bowman et al., 2015), but several other variantshave been proposed, such as MultiNLI (Williamset al., 2018), containing multiple domains, SciNLI(Sadat and Caragea, 2022) for scientific questionanswering, SQuAD (Rajpurkar et al., 2016) andGLUE (Wang et al., 2018) benchmarks for general-purpose NLU. Moreover, many related problemsin NLU can be cast as an NLI problem; for in-stance, the FEVER (Thorne et al., 2018) dataset forfact-checking can be regarded as an NLI problemin terms of identifying the relationship between astatement and supporting evidence.However, driven by the widespread observationsthat previous popular NLI datasets contain short-cuts (Tsuchiya, 2018; Gururangan et al., 2018),multiple works (Nie et al., 2019; Glockner et al.,2018; Naik et al., 2018; Geiger et al., 2018; Yanakaet al., 2019; Saha et al., 2020) developed \"stresstests\" to benchmark specific linguistic phenomena.For instance, Glockner et al. (2018) proposeda simple test set based on SNLI (Bowman et al.,2015) that involves changing a single word in thepremise sentences. In this setting, performance issubstantially worse than the original SNLI test set,indicating the presence of spurious correlations inthe training dataset construction. Naik et al. (2018)proposed an NLI Stress Test by quantifying thelexical phenomena (e.g., presence of antonyms, nu-merical reasoning) behind common model errors inMultiNLI (Williams et al., 2018). Their proposedstress test involved constructing artificial examplesthat exacerbate common sources of model error,showcasing that some models have markedly re-duced performance. Nie et al. (2019) proposed anew benchmark called Adversarial NLI (ANLI),which leverages an interactive human-and-model-in-the-loop procedure to collect hard examples fornatural language inference, obtaining a challengingstress test for current models.",
  ": Overall diagram of our method to automatically construct a challenging test set for NLI": "cially built sentences based on first-order logic, fix-ing the sentence structure and only varying wordscorresponding to parts of speech at predefined po-sitions. Such a dataset comprises unrealistic sen-tences but provides insight into the (lack-of) expres-sive power of certain neural architectures. Like-wise, Yanaka et al. (2019) proposed the evaluationof monotonicity reasoning for NLI by construct-ing a dataset through curating and manipulatingsentence pairs from the Parallel Meaning Bank(Abzianidze et al., 2017). Saha et al. (2020) iden-tified the lack of conjunctive reasoning examplesin current NLI test sets and estimated that around72% of sentence pairs in SNLI have conjunctionsunchanged between premise and hypothesis. Theauthors proposed CONJNLI, a stress test composedof conjunctive sentence pairs collected automati-cally from Wikipedia and manually verified. In contrast to previous work, we propose amethod to characterize the test set into multipledifficulty levels by using training dynamics of neu-ral networks (Swayamdipta et al., 2020; Pleisset al., 2020), thereby isolating easy and spuriousexamples and keeping only challenging pairs. Ourmethod is general, model-agnostic, utilizes existingdataset samples (avoiding unrealistic artificial sen-tence pairs), is easily extensible to other datasets,and does not require manual verification of humanannotators. Previous approaches (Naik et al., 2018;Saha et al., 2020) aim to develop a stress test forNLI by amplifying spurious correlations and eval-uating model performance under various extremeconditions. Our goal is to minimize spurious cor-relations in existing benchmarks to gain a morerealistic sense of performance under challengingreal-world examples.",
  "Test Set Characterization": "Our goal is to generalize the Data Maps proposedby Swayamdipta et al. (2020) to characterize thetest set. Swayamdipta et al. (2020) proposed anapproach to gauge the contribution of each trainingsample in a dataset by analyzing training dynamics(variability, average confidence of the gold label,and average correctness) across training for a fixedamount of epochs. After training, each exampleis split into one of three categories (i.e., easy-to-learn, ambiguous or hard-to-learn) using a fixedpercentile threshold on one of the features. Forexample, instances regarded as ambiguous are ex-amples for which the variability across 5 epochsis in the top 33% percentiles, disregarding othermeasures.We aim to extend and generalize Data Mapsby employing a Gaussian Mixture Model (GMM)(Reynolds, 2009) to learn the best fitting distribu-tion of data difficulty levels, thus avoiding fixedthresholds.Unlike other clustering techniques,such as KMeans, which outputs spherical clustersand disregards cluster variance, we chose a GMMas a more flexible clustering method. showcases the general methodology used in thiswork. We first characterize the test set by trainingtwo separate models with both premise and hypoth-esis (P+H), and hypothesis only (H); second, wegather 8 measures of training dynamics for eachinstance and cluster them to obtain three difficultylevels (4 for P+H and 4 for H). We found that diffi-culty levels simultaneously align with measures ofspurious correlations and model performance.Incontrasttotheinitialapproachof",
  "Swayamdiptaetal.(2020),weinclude6additional features for a more informative charac-terization across training. In our scenario focused": "on NLI in particular, we gather statistics fortraining dynamics across two types of settings:normal training (i.e., training with P + H) andhypothesis-only (H). Models trained only withthe hypothesis have been shown to produceunreasonably high results (Poliak et al., 2018; Liuet al., 2020), mostly due to artifacts in datasetconstruction. These insights enabled us to gatherstatistics about such examples and improve datacharacterization through more diverse features foreach instance. Different from Swayamdipta et al.(2020) who focused on characterizing the trainingset for increasing data quality, we aim to constructa more challenging test set automatically.As such, in order to construct a data map of thetest set, we trained a model for E epochs on thetest set using premise and hypothesis and, sepa-rately, using only the hypothesis to gather trainingdynamics for each example in the test set. LetDtest = {(x, y)i}Ni=1 be a test dataset containingN instances, where, in our case, xi is comprisedof a premise + hypothesis pair or only a hypoth-esis. We compute the following measures acrosstraining an Encoder model in both scenarios (P +H and H) for each example xi: confidence (i),variability (i), correctness (ci) and Area UnderMargin (AUMi).",
  "where p(e) corresponds to the models prob-ability during training at epoch e.Following": "Swayamdipta et al. (2020), we compute confidence,variability, and correctness concerning the correctlabel yi . Furthermore, we compute AUM (Pleisset al., 2020), which was initially proposed to iden-tify mislabeled examples but yielded a similar typeof characterization as Data Maps.We includeAUM as an additional measure of instance cor-rectness/learnability. Let z(e)y (xi) be the logit (pre-softmax) for class y of the model at epoch e, givenan instance xi. The area under margin (AUM oraverage margin) of xi is computed as:",
  "e=1(z(e)yi (xi) max(y=yi ) z(e)y (xi)) (4)": "In all our experiments, we first fine-tune pre-trained RoBERTa models (Liu et al., 2019), fol-lowed by DeBERTa (He et al., 2022) models dueto their established high performance on a wideset of tasks. In our formulation, any other encoderwould yield similar results, as this method is basedonly on the final classification output and not modelinternals. However, characterization based on fi-nal logits and class confidences is affected by howcalibrated the models predictions are (Guo et al.,2017), as poorly calibrated models have lower logitvariance across classes. For our scope, we are inter-ested in identifying and separating spurious correla-tions in NLI benchmarks and not in benchmarkingdifferent classifiers for this task. We explore theimpact of the underlying encoder in .1.In , we show the results of our RoBERTamodels trained on SNLI, MultiNLI, and FEVERon different configurations of training/testing splitsand using both the premise and the hypothesis oronly the hypothesis. Our reproduction of resultsis on par with other works. For completeness, wealso show results where the model is trained on thetest set, but note that the purpose is only to gathertraining dynamics and not directly use it as a clas-sifier. The model trained on only the hypothesisobtains 71% accuracy on SNLI and 61% accuracyon MultiNLI, while random chance performanceis 33%. These results strongly point toward spu-rious correlations and annotation artifacts on bothdatasets (Tsuchiya, 2018; Gururangan et al., 2018;Poliak et al., 2018; Liu et al., 2020). In the caseof FEVER, the hypothesis-only model achievedclose to random-chance, indicating less spuriouscorrelations found in the hypothesis.",
  "fi = f(P+H)i f(H)i(5)": "Using the feature vectors {fi}Ni=1, we clusterthe test set using a Gaussian Mixture Model intothree clusters. Feature vectors are normalized withstandard scaling by subtracting the mean and divid-ing by the standard deviation of each feature. Theclusters are ranked according to the intra-clusteraverage confidence (P+H), and we interpret themas belonging to three difficulty levels, followingthe terminology introduced by Swayamdipta et al.(2020): easy, ambiguous and hard, in decreasing or-der of the average intra-cluster confidence (P+H).See Appendix A for a high-level overview of ouralgorithm. depicts the distribution of featuresacross difficulty levels for both datasets. Whileeach type of feature captures different aspects ofthe learnability of an instance, their combinationoffers a more diverse view of the learning dynam-ics during training. In the case of both SNLI andMultiNLI, harder examples have consistently loweraverage margin and more variability of the cor-rect class. The effect is not as pronounced in ahypothesis-only setting; however, there is a cleardelimitation of easy examples for average marginand confidence, indicating potential annotation ar-tifacts. For FEVER, since the dataset has reducedspurious correlations, the identified splits corre-spond to difficult-to-learn examples, not necessar-ily examples with annotation artifacts.In the interest of quantifying the number of spu-rious correlations found in the test set, we followNaik et al. (2018) and track several measures thatcorrespond to either shallow statistics between thepremise and the hypothesis, or the presence ofnegations or misspelled words. presentsthe heuristics implemented in our work. We auto-matically compute each measure, avoiding time-consuming manual annotations.Training the pretrained models for dataset char-acterization was performed for 5 epochs each, witha batch size of 32, using the Adam optimizer witha learning rate of 105 following a linear decay",
  "NameExplanation": "Word OverlapNumber of common words between thepremise and hypothesis, normalized bysentence lengthNumber of AntonymsNumber of antonyms of each of thewords in the premise contained in hy-pothesis, based on WordNet (Fellbaum,1998), normalized by sentence length.Length MismatchDifference in length between premiseand hypothesis, normalized by sentencelengthMisspelled WordsTotal number of misspelled words us-ing a spellchecker in the premise andhypothesis, normalized by sentencelength.Contains NegationBoolean flag if either the premise of hy-pothesis contains a negation word (e.g.,no, not, never, none)",
  "Results & Discussion": "shows the performance of a RoBERTamodel trained on each datasets training set andevaluated on our stress test after characterizationusing training dynamics. Easier instances havemore examples annotated with \"contradiction\" and\"entailment\", while harder instances have moreexamples annotated with \"neutral\". Performancemonotonously degrades upon increasing difficultylevels, reaching 56% accuracy on SNLI-hard and53% accuracy on MultiNLI-hard. Performanceon the easy split for both datasets is considerablyhigher compared to the global accuracy with allsplits combined. Furthermore, the accuracy of amodel trained using only the hypothesis degradesto almost random chance on harder splits, indicat-ing that the hard split has fewer annotation artifacts.Compared to Swayamdipta et al. (2020), the dif- : Distributions of feature values across difficulty levels for the test set for SNLI (top), MultiNLI (middle), andFEVER (bottom). In addition to features explored in Data Maps (Swayamdipta et al., 2020), we also incorporated theAverage Margin (Pleiss et al., 2020) and included training dynamics across a model trained only on the hypothesis. : Distributions of the measures of spurious correlations for each level (easy, ambiguous, hard) across thethree labels (entailment, neutral, contradiction) for SNLI (top), MultiNLI (middle) and FEVER (bottom). ficulty levels are not equal in size; the majority(70%) of samples belongs to the easy category,while only around 10% are characterized as beinghard. For FEVER, the performance degradation inthe hard split is more dramatic: 29% accuracy forhard compared to 88% for ambiguous, indicatingtruly difficult examples for the model.",
  "Even though the hard split is relatively small,the subsample is challenging for current models,as it comprises instances with fewer spurious cor-": "relations between premise and hypothesis. Fewerannotation artifacts enable fewer \"correct\" predic-tions from linguistic patterns present only in thehypothesis. In , we show per-class countsrelative to the difficulty levels for both datasets. Itis the case that easier samples contain more contra-dictions and entailments, prone to linguistic com-monalities (word overlap, presence of antonyms).Thus, the hard split has more neutral instances.",
  ": Counts for each class in SNLI, MultiNLI, andFEVER, according to each difficulty level": "SNLI, the easy splits contain unrelated sentenceswhich are sometimes annotated incorrectly as Con-tradiction (e.g., \"a woman running in the park\"versus \"a man cooking at home\" - two unrelatedsentences annotated as contradicting). The modellearns this pattern and incorrectly predicts Contra-diction on some Neutral pairs (e.g., \"... girls chat-ting on the stairwell\" versus \"girls are at school\").For MultiNLI, we found that the easy split usu-ally aligns with simple sentence negations (e.g.,\"it gets it\" versus \"it doesnt get it\") or paraphras-ing (\"I guess history repeats itself\" versus \"his-tory certainly doesnt repeat\"). These observationsstrongly point towards spurious correlations be-tween premise and hypothesis, making the sen-tences easier to classify correctly. We provide se-lected examples in the Appendix A. The ambiguousand hard splits in both datasets contain increasinglymore subtle cues, with little overlap in words be-tween premise and hypothesis (e.g., \"standing on atree log\" versus \"crossing the stream\" / \"wouldnthave mattered\" versus \"would have gotten worse\"),having more natural and challenging sentencepairs.Across SNLI, MultiNLI, and FEVER, wetracked the average amount of each measure be-tween difficulty levels and classes (see ). To rigorously test the difference between theclasses at various difficulty levels, we perform anon-parametric two-sided Mann-Whitney-U test(Mann and Whitney, 1947) with Bonferroni correc-tion to test for statistical significant differences2.We found no evidence for the presence of spuri-ous correlations (Mann-Whitneys U test p > .05)in the hard split between the three classes. Somemeasures are more associated with certain classes.For example, instances annotated with Contradic-tion have a disproportionate amount of antonyms",
  "Significance thresholds: Not Significant (ns): .05 < p,*: .01 p .05, **: .001 p .01, ***: p .001,": "between premise and hypothesis in the easy and am-biguous splits. Similarly, negation is more presentin the Contradiction class for easy splits. For in-stances annotated with Entailment, word overlap ispresent significantly in easy splits. Between SNLIand MultiNLI, MultiNLI has a disproportionatelylarge amount of negations compared to SNLI. Forboth SNLI and MultiNLI, our method yields littleto no significant differences between classes in thehard split across the spurious correlation measures.For FEVER, measures such as the presence of nega-tions, number of antonyms, and word overlap arereduced across difficulty levels. Note that FEVERincludes a small statement as the premise and along text extract containing evidence as the hypoth-esis, which makes the length mismatch negative.",
  ": Comparison between RoBERTa and DeBERTaaccuracy on each difficulty level, across models": "Across datasets and difficulty levels, the perfor-mance sharply drops for the \"hard\" split for bothmodels. DeBERTa achieved higher accuracy for\"hard\" set, most likely due to better overall per-formance compared to RoBERTa. In , weshow that overall heuristic values for \"ContainsNegation\" are maintained across both models. Ex-tended results for are presented in Appendix A.Our proposed methodology is general and inde-pendent of the underlying encoder model since weprocess training dynamics computed from raw logit",
  ": Comparison between the characterizationsobtained by RoBERTa and DeBERTa on the \"ContainsNegation\" heuristic measure": "scores. This characterization procedure may beadapted to using Large Language Models (LLMs)(Lee et al., 2023) in a zero-shot classification set-ting by manipulating the log-likelihood for the to-kens of the correct classes. However, using LLMsrequires a different approach than the one presentedhere since the networks are usually used withoutfurther training, in an in-context-learning manner(Dong et al., 2022). Furthermore, even if the LLMsare fine-tuned (Hu et al., 2021), it is not straightfor-ward how the logits of each of the three classes aretracked across training. We leave this approach forfuture work.",
  "All100%0.72520.75590.27940.59230.77710.72680.7037": ": Results for a RoBERTa model trained on SNLIin various configurations and evaluated on the stresstest by Naik et al. (2018) based on MultiNLI. The bestresults are bold, while the second best are underlined. Our method provides a more challenging testset devoid of shortcuts and spurious correlations.Further, we explore the possibility of using thisapproach to improve data quality for training NLImodels. We employ the same algorithm to char-acterize the training sets for SNLI and MultiNLIand train a RoBERTa model on the different result-ing combinations of difficulty levels. Under each",
  "configuration, the model is trained for 10 epochswith early stopping on the validation set loss, witha learning rate of 105 following a linear decayschedule with a warm-up": "We evaluate each model on the stress test pro-posed by Naik et al. (2018) that is based onMultiNLI. However, we emphasize that the stresstest of Naik et al. (2018) is designed to unrealis-tically amplify spurious correlations to gauge themodel performance under various extreme condi-tions, in contrast to our method, which eliminateslinguistic shortcuts while mimicking real-worldexamples. In Tables 5 and 6, we show the per-formance on the dataset proposed by Naik et al.(2018) for RoBERTa models trained on SNLI andMultiNLI. The authors provided metadata for eachinstance that allows fine-grained evaluation underdifferent linguistic reasoning phenomena. We compared our approach with Data Maps(Swayamdipta et al., 2020) and Area Under Margin(Pleiss et al., 2020), two popular methods for train-ing set characterizations using training dynamics.For Data Maps, we select ambiguous examples bykeeping the instances where average variability isin the top 66% percentile. For AUM, while theauthors did not explicitly propose a threshold forcharacterizing each instance, we follow a similarapproach to Data Maps by considering ambigu-ous examples to have an average margin betweenthe 33% and 66% percentiles. Our method outper-forms Data Maps and AUM across the majority ofsettings and, in some cases, outperforms a modeltrained on the full dataset while using a smalleramount of data but of higher quality. This indicatesthat our method is a viable alternative to AUM orData Maps for increasing dataset quality.",
  "Conclusions": "Our method highlights significant shortcomings inwidely used NLI evaluation datasets (SNLI andMultiNLI) due to spurious correlations in the an-notation process.To address these issues, weproposed an automatic method for constructingmore challenging test sets, effectively filtering outproblematic instances and providing a more re-alistic measure of model performance. Our ap-proach, which categorizes examples in increasingdifficulty levels using a wide range of trainingdynamics features, enhances evaluation reliabil-ity and offers insights into underlying challengesin NLI. Importantly, our methodology is generaland model-agnostic, and can be applied across dif-ferent datasets and models, promising improvedevaluation practices in NLP.Furthermore, we provided evidence that ourmethod can obtain a challenging test set even if thedataset has fewer annotation artifacts; we charac-terized FEVER, a fact-checking dataset repurposedfor NLI, and showed that the identified hard splitis a highly challenging subset of the dataset. Byaggressively filtering uninformative examples, weshow that comparable model performance can beachieved with significantly reduced data require-ments. Our work contributes to advancing NLIevaluation standards, fostering the development ofmore robust NLU models.",
  "Acknowledgements": "The work of Adrian Cosma was supported by amobility project of the Romanian Ministery of Re-search, Innovation and Digitization, CNCS - UE-FISCDI, project number PN-IV-P2-2.2-MC-2024-0641, within PNCDI IV. The work of Stefan Rusetiwas supported by a mobility project of the Roma-nian Ministery of Research, Innovation and Digi-tization, CNCS - UEFISCDI, project number PN-IV-P2-2.2-MC-2024-0585, within PNCDI IV. Thework was also supported by a grant from the Na-tional Science Foundation NSF/IIS #2107518. Lasha Abzianidze, Johannes Bjerva, Kilian Evang, Hes-sel Haagsma, Rik van Noord, Pierre Ludmann, Duc-Duy Nguyen, and Johan Bos. 2017. The ParallelMeaning Bank: Towards a multilingual corpus oftranslations annotated with compositional meaningrepresentations. In Proceedings of EACL, pages 242247, Valencia, Spain. Association for ComputationalLinguistics. Samuel R. Bowman, Gabor Angeli, Christopher Potts,and Christopher D. Manning. 2015. A large anno-tated corpus for learning natural language inference.In Proceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing (EMNLP).Association for Computational Linguistics.",
  "Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Wein-berger. 2017. On calibration of modern neural net-works. In International conference on machine learn-ing, pages 13211330. PMLR": "Suchin Gururangan, Swabha Swayamdipta, Omer Levy,Roy Schwartz, Samuel Bowman, and Noah A. Smith.2018. Annotation artifacts in natural language infer-ence data. In Proceedings of NA-ACL, pages 107112, New Orleans, Louisiana. Association for Com-putational Linguistics. Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2022.Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding shar-ing. In The Eleventh International Conference onLearning Representations.",
  "Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,Jason Weston, and Douwe Kiela. 2019. Adversarialnli: A new benchmark for natural language under-standing. arXiv preprint arXiv:1910.14599": "Geoff Pleiss, Tianyi Zhang, Ethan Elenberg, and Kil-ian Q Weinberger. 2020. Identifying mislabeled datausing the area under the margin ranking. Advances inNeural Information Processing Systems, 33:1704417056. Adam Poliak, Jason Naradowsky, Aparajita Haldar,Rachel Rudinger, and Benjamin Van Durme. 2018.Hypothesis only baselines in natural language infer-ence. In Proceedings of the Seventh Joint Confer-ence on Lexical and Computational Semantics, pages180191, New Orleans, Louisiana. Association forComputational Linguistics. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, andPercy Liang. 2016. SQuAD: 100,000+ questions formachine comprehension of text. In Proceedings ofEMNLP, pages 23832392, Austin, Texas. Associa-tion for Computational Linguistics.",
  "FEVER: a large-scale dataset for fact extraction andVERification.In Proceedings of NA-ACL, pages809819, New Orleans, Louisiana. Association forComputational Linguistics": "Masatoshi Tsuchiya. 2018. Performance impact causedby hidden bias of training data for recognizing textualentailment. In Proceedings of LREC 2018, Miyazaki,Japan. European Language Resources Association(ELRA). Alex Wang, Amanpreet Singh, Julian Michael, FelixHill, Omer Levy, and Samuel Bowman. 2018. GLUE:A multi-task benchmark and analysis platform for nat-ural language understanding. In Proceedings of the2018 EMNLP Workshop BlackboxNLP: Analyzingand Interpreting Neural Networks for NLP, pages353355, Brussels, Belgium. Association for Com-putational Linguistics. Sean Welleck, Jason Weston, Arthur Szlam, andKyunghyun Cho. 2019. Dialogue natural languageinference. In Proceedings of the 57th Annual Meet-ing of the Association for Computational Linguistics,pages 37313741, Florence, Italy. Association forComputational Linguistics. Adina Williams, Nikita Nangia, and Samuel Bowman.2018. A broad-coverage challenge corpus for sen-tence understanding through inference. In Proceed-ings of NA-ACL, pages 11121122. Association forComputational Linguistics. Hitomi Yanaka, Koji Mineshima, Daisuke Bekki, Ken-taro Inui, Satoshi Sekine, Lasha Abzianidze, andJohan Bos. 2019. Help: A dataset for identifyingshortcomings of neural models in monotonicity rea-soning. arXiv preprint arXiv:1904.12166."
}