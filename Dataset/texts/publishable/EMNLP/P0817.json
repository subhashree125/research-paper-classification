{
  "Abstract": "This study evaluates the effectiveness of pre-trained language models in identifying argu-ment structure constructions, important formodeling both first and second language learn-ing.We examine three methodologies: (1)supervised training with RoBERTa using agold-standard ASC treebank, including by-tagaccuracy evaluation for sentences from bothnative and non-native English speakers, (2)prompt-guided annotation with GPT-4, and(3) generating training data through promptswith GPT-4, followed by RoBERTa training.Our findings indicate that RoBERTa trained ongold-standard data shows the best performance.While data generated through GPT-4 enhancestraining, it does not exceed the benchmarks setby gold-standard data.",
  "Introduction": "Argument structure constructions (ASCs) are lexi-cogrammatical patterns at the clausal level. Theyconsist of an argument structure and a main verb,with each argument contributing to the clausesmeaning (Goldberg, 1995). The characteristics ofASC use, such as frequency and/or the strength ofassociation between a verb and its argument struc-ture, have been actively explored in previous stud-ies on first language (L1) and second language (L2)learning and assessment (Tomasello and Brooks,1998; Ellis, 2002; Ninio, 1999; Kyle and Crossley,2017).To effectively model human language learn-ing/development using ASC features, ASCs mustbe reliably identified in target texts. Recent studieshave shifted from manual (e.g., Ellis and Ferreira-Junior, 2009) to automatic ASC analyses (e.g.,Kyle, 2016; Kyle and Sung, 2023; Hwang andKim, 2023). However, building automated ASCannotation systems has proven challenging due tosyntactic ambiguity. For example, while syntacticanalyses would represent the clauses (1) she ran [to",
  ": Distinguishing semantic roles in similar de-pendency structures of two different types of ASCs,visualized by DisplaCy (Honnibal et al., 2020)": "the mountains] and (2) she ran [in the mountains]identically based on the form (i.e., subject-verb-prepositional phrase structures), they imply differ-ent meanings and represent distinct ASC types. Incase (1), the prepositional phrase [to the mountains]is an argument that completes the meaning by spec-ifying the goal of the movement. In contrast, incase (2), the phrase [in the mountains] modifies thelocation of the event, as illustrated in . Onepotential reason for this mismatch is that humanlanguage often employs a pre-built form-meaningschema at the clausal level (Fillmore, 1968; Gold-berg, 1995), which can be challenging to capturefrom a bottom-up perspective. A top-down ap-proach, directly assigning ASC types based on theirclausal contexts, is therefore likely more effectivethan a bottom-up approach.Recent advancements in pre-trained languagemodels (PLMs) may offer a promising solution tothese challenges, given their effectiveness in stor-ing sentence-level contextual knowledge, as wellas part-of-speech and syntactic knowledge withintheir word embeddings (Miaschi and DellOrletta,2020; Hewitt and Manning, 2019). The follow-upempirical question is whether these models can reli-ably capture specific types of ASCs, both with and without top-down annotations provided by trainedhuman annotators focusing on the linguistic charac-teristics of clausal forms. To address this, the cur-rent study explores the use of PLMs for identifyingASCs, evaluating three methodologies: (1) super-vised training with a encoder model (RoBERTa)using a gold-standard ASC treebank, (2) prompt-guided annotation of unlabeled data with a decodermode (GPT-4), and (3) prompt-guided generationof training data with GPT4, followed by trainingwith RoBERTa.",
  "Language learning and ASC use": "Usage-based constructionist theories propose thatlanguage development occurs as learners formform-meaning pairings (i.e., constructions) throughstatistical induction from diverse linguistic inputs.In modeling language learning/development, a keyaspect of this approach involves ASCs, which areclausal level constructions that convey the coreconcepts of a sentence. They are also instrumen-tal in communications as they encapsulate concep-tual archetypes, such as motion or causative events(Bencini and Goldberg, 2000; Goldberg, 1995,2003; OConnor and Kay, 2003; Rappaport Ho-vav and Levin, 1998).Building on theories that emphasize the signif-icance of ASCs, empirical studies in languagelearning (e.g., Ellis, 2002; Ninio, 1999; Tomasello,2005; Gries and Wulff, 2005) have indicated thatthe frequency of ASCs (and of verbs), along withthe strength of their associations, are key factorsin shaping their developmental trajectory. To bespecific, language learners make form-meaningmappings between frequent linguistic forms (e.g.,give-me-the-toy) and their corresponding meanings(Ninio, 1999) early in their learning process. Aslearners encounter more related but varied inputs(e.g., hand-me-the-toy, bring-me-the-toy), they de-velop schematic representations of these formslike VERB-me-the-toy, or more abstractly, VERB-Recipient-Object)1. In short, as they develop, learn-ers adopt a broader range of less frequent ASCs,utilize a wider range of verbs within specific ASCs, 1Research has shown that learners tend to initially over-generalize schematic slots (Ambridge et al., 2013; Goldberget al., 2004; Ninio, 1999). For example, after learning how touse a basic transitive ASC form (e.g., she opened the door), alearner might mistakenly extend this construction to intransi-tive verbs, resulting in ungrammatical sentences (e.g., she sitsthe chair). However, they gradually fine-tune their linguisticsystem through additional input and use. and form stronger associations between verbs andASCs, thus reducing their use of atypical verbs inthese constructions.The use of ASCs has proven to be a useful indi-cator of language proficiency, applicable to NLPapplications such as automatic scoring and model-ing human language development. Kyle and Cross-ley (2017), for example, found that more proficientL2 writers tended to use less frequent but morestrongly associated verb-ASC combinations. Addi-tionally, they found that ASC-based indices werebetter predictors of holistic writing scores thanclassic indices of syntactic complexity (e.g., meanlength of T-unit, mean length of clause), whichfocused on the structural elements of sentenceswithout accounting for the functional relationshipsconveyed by ASCs. Relatedly, scholars have alsofound that the use of particular ASC types indi-cate L2 proficiency. For example, Hwang and Kim(2023) found that more proficient L2 writers tendedto use a wider range of ASC types overall, and alsotended to use a higher proportion of passive andcaused-motion ASCs.",
  "Use of dependency representations": "The advent and popularization of syntactic de-pendency representation in treebanks and parsers(de Marneffe and Manning, 2008; Chen and Man-ning, 2014) provided a helpful starting point forautomated ASC analysis. For example, ODonnelland Ellis (2010) used a dependency parsed ver-sion of the BNC (Andersen et al., 2008) to ex-plore the feasibility of extracting ASCs using de-pendency tags. While this approach allowed forsome target constructions to be accurately ex-tracted (e.g., VERB-preposition-noun construction:[talked about it], Rmer et al., 2014) overall accu-racy was insufficient for broader use.Over time, the introduction of NLP systems uti-lizing neural networks (e.g., Chen and Manning,2014) substantially increased dependency parsingaccuracy, sparking renewed efforts in automatedASC annotation (e.g., Hwang and Kim, 2023; Kyle, 2016; Kyle and Crossley, 2017). In summary, withthese more accurate parsers, researchers attemptedto extract ASCs based on syntactic frames (builtfrom dependency representations) and employedmapping systems to categorize them. However, akey issue with using only syntactic frames to iden-tify ASCs is that current representations lack thesemantic information needed to disambiguate cer-tain ASC types (e.g., between intransitive simpleand intransitive motion constructions, as illustratedin ; also see Kyle and Sung, 2023). Toimprove accuracy, an alternative approach that con-siders the semantics of the clause is necessary.",
  "Use of semantic role labels": "Another approach involves leveraging the semanticinformation in ASCs, as they contain argumentroles that often correspond to traditional semanticroles (e.g., agent, patient, theme, goal). For scal-able and automatic extraction of ASCs, databasesannotated with semantic role labels, such as Prop-Bank (Palmer et al., 2005) or the Universal Propo-sitions (UP) treebank (Akbik et al., 2015), or au-tomated semantic role labeling systems(Gardneret al., 2018; Shi and Lin, 2019), may prove use-ful. This approach mirrors previous efforts whereresearchers extracted ASCs based on syntactic in-formation from dependency treebanks and parsers.However, there are two major obstacles withsolely relying on semantic role labelling systemsat present. First, the accuracy of current automatedsemantic role labeling systems is still not sufficientfor this task2. Second, it is sometimes not straight-forward to map the output of semantic role labelingsystems to ASCs. Typically, these systems use ab-stract semantic role labels (e.g., ARG0, ARG1) thatpose challenges in directly mapping to theoreticalASC categorizations for some complex ASCs3.To address this issue, one potential solution in-volves automatically extracting semantic roles froma clause, grouping them into semantic frames, andmapping each frame to corresponding ASC typesusing linguistic knowledge. Subsequently, these 2To our best understanding, the publicly-available seman-tic role labeling achieved an F1 of 0.86 on argument tagging,and 0.95 on predicate tagging (Gardner et al., 2018; Shi andLin, 2019). Note that these scores are for large-grained argu-ment tags, which do not offer the precision required for ASCidentification.3Particularly, ARG2 and ARG3 cover a number of seman-tic categories. According to Jurafsky and Martin (Chapter24.4), ARG2 includes benefactive, instrumental, attributive,or end-state roles, while ARG3 encompasses start-point, bene-factive, instrumental, or attributive roles. semi-automatically mapped ASCs can be trainedusing a sequential learning model. For example,Kyle and Sung (2023) utilized a combination of UPtreebank (Akbik et al., 2015), VerbNet (Schuler,2005), and FrameNet (Fillmore et al., 2003) to ex-tract semantic roles and corresponding sentencesfrom a subset of the English Web Treebank (Sil-veira et al., 2014). The extracted semantic roleswere grouped into semantic frames, and researchersassigned ASCs to each frame, creating a silver-annotated ASC treebank (ASC Treebank v1). Theythen trained a transformer model using RoBERTaembeddings with the semi-automatically annotatedASC labels and compared its performance againstthree probabilistic models: one based on verb lem-mas, another on syntactic frames using dependencyparsing, and a third combining both verb lemmasand syntactic frames. The results indicated that thetransformer model trained on silver-annotated sen-tences based on the semantic role labels achievedthe highest classification accuracy (F1 = .918), out-performing the other models. Despite this success,there is room to improve the models accuracy (par-ticularly at the ground truth level) by leveraginggold-standard annotations (Kyle and Sung, 2023;Sung and Kyle, 2024). 2.2.3Use of the gold standard ASC treebanksAs discussed, researchers have employed scalabledatabases and systems to extract ASCs. Whilethese approaches have demonstrated effectiveness,they remain limited in their ability to differenti-ate ambiguous cases that cannot be fully capturedby automatically extracted syntactic or semanticframes. A potential alternative is going back to con-struct a treebank from scratch, which echoes earlierefforts to manually identify ASCs (e.g., Ellis andFerreira-Junior, 2009). However, the current goalis to create (1) a training set for supervised learning,specifically designed for sequential named entityrecognition (NER) tasks4, (2) input examples forfew-shot learning in unsupervised tasks, and (3) atest set to evaluate the models accuracy.Recently, Sung and Kyle (2024) released a gold-standard annotated treebank of ASCs (ASC tree-bank v2), which includes sentences from the En-glish Web Treebank (EWT), as well as sentenceswritten by L2 users from ESL-WR (Berzak et al.,2016), and spoken by L2 users from ESL-SP (Kyleet al., 2022) (10,204 sentences; 22,069 ASC to-",
  "Automated linguistic annotation withencoder models": "Recent advancements have underscored the po-tential of PLMs in automated linguistic annota-tion, as encoder models (e.g., BERT [Devlin et al.,2018]; RoBERTa [Liu et al., 2019]) have demon-strated impressive gains in supervised learningtasks.Based on the Transformer architecture(Vaswani et al., 2017), PLMs have been exten-sively pre-trained on large text corpora and adeptlystore morpho-syntactic and sentence-level contex-tual knowledge within their word embeddings (Mi-aschi and DellOrletta, 2020; Hewitt and Manning,2019). One fundamental application, often consid-ered first in linguistic annotation, is dependencytagging and parsing. The performance of thesemodels, specified for English, typically achieves anF1 score above 0.90 (e.g., Honnibal et al., 2020).Beyond syntactic analysis, Shi and Lin (2019)demonstrated that a BERT-LSTM based modelcould attain F1 scores of 0.90 on in-domain test setsand 0.84 on out-domain test sets in semantic rolelabeling. This was accomplished through argumentidentification and classification, without the needfor auxiliary syntactic features like part-of-speechtags or dependency trees.While dependency parsing and semantic role la-beling have focused on word-level linguistic an-notations, a RoBERTa-based model has shownpromising results with discourse-level linguisticannotation. For example, recently Eguchi and Kyle(2023) applied a RoBERTa-based ensemble modelto identify and categorize rhetorical stance featuresin academic English writing. By employing a dis-course analytic framework and manually annotat-ing 4,688 sentences across eight rhetorical stancecategories, they trained an ensemble model com-bining RoBERTa and LSTM. This model achieveda macro-averaged F1 score of 0.72 in span iden-tification of stance-taking expressions, surpassingpre-adjudication human annotator reliability.",
  "To effectively employ encoder models for fine-grained linguistic analyses, it is important to collect": "and precisely annotate a certain amount of trainingdata for the linguistic features of interest. However,data annotation is often a costly process. This costencompasses the labor involved in researchers re-cruiting, training, and managing human annotators,as well as the time spent by annotators in labelingraw data. In this context, recent studies have ex-plored ways to effectively use decoder models (e.g.,GPT) for data annotation with unsupervised learn-ing (Radford et al., 2019). They have demonstratedimpressive zero-shot or few-shot learning abilities,which allow them to perform tasks with minimal orno task-specific training data (Brown et al., 2020). For example, Ding et al. (2022) investigatedleveraging GPT-3 for data annotation in differentNLP tasks, including an NER task. They devel-oped three distinct GPT-3-based data annotationapproaches: (1) prompt-guided unlabeled data an-notation, (2) prompt-guided training data genera-tion, and (3) dictionary-assisted training data gen-eration. Subsequent experiments on both sequence-and token-level NLP tasks were used to evaluatetheir performance. The findings indicated that di-rectly annotating unlabeled data was effective fortasks with a small labelling task, while generation-based methods proved more suitable for tasks witha larger labelling task. Similarly, Yu et al. (2023)investigates the application of GPT models to au-tomate complex pragmatic-discourse features ofapology in zero and few-shot settings. By com-paring the performance of GPT-3.5, GPT-4, andhuman annotations in annotating apology compo-nents, the study demonstrated that GPT-4s accu-racy approached that of human annotators.",
  "On the contrary, the recent study by Ettinger et al": "(2023) found limited success using GPT-3, Chat-GPT, and GPT-4 models for semantic annotations(i.e., abstract meaning representation [Banarescuet al., 2013]). The experiments included zero andfew-shot experiments, as well as an experiment fo-cusing on PLMs ability to handle metalinguisticqueries (e.g., identifying primary sentence eventsand predicates). A comprehensive evaluation ofparse acceptability demonstrated that, even withfew-shot examples, the models almost never suc-ceeded in producing completely accurate parses.The findings indicate that while these models cap-ture some semantic elements, significant challengespersist in achieving precise semantic analyses.",
  "Datasets": "In this study, we utilize two treebanks, namelythe silver (v1) and gold (v2) versions of the ASCtreebank. The first silver version (Kyle and Sung,2023) includes 26,437 ASC tokens that were semi-automatically annotated5. The second gold version(Sung and Kyle, 2024) includes 22,069 manuallyannotated ASC tokens6. The sentences in this tree-bank were sampled from the English Web Treebank(ETW) (Silveira et al., 2014), L2-Written (ESL-WR) (Berzak et al., 2016), and L2-spoken (ESL-SP) (Kyle et al., 2022) treebanks, which are all partof the Universal Dependencies project (Nivre et al.,2020). Given the relatively small representation ofL2-written (ESL-WR) and spoken (ESL-SP) data,training, development, and test sets were resam-pled with a 34/33/33 distribution. The L1 (EWT)sentences retained their original sections and wereroughly distributed at 80/10/10. illustrates the nine ASC tags along withthe most prototypical semantic roles that weremapped in two treebanks (Kyle and Sung, 2023;Sung and Kyle, 2024), accompanied by examplesfrom the annotated dataset. Appendix A showsASC type frequencies in each dataset.",
  "Experiment setup": "The purpose of this study is to explore how toleverage PLMs, specifically RoBERTa (an encodermodel) and GPT-4 (a decoder model), for ASC an-notations which could assist in modeling and mea-suring human language development. To achievethis goal, we designed three different approachesto utilize PLMs to evaluate and compare their per-formance (). 4.2.1Experiment 1The objective of the first experiment is to in-vestigate supervised learning using gold-standarddata applied with RoBERTa embeddings (Liuet al., 2019). To accomplish this, we trained atransformer-based machine learning model, em-ploying the open-access Python library, spaCy (ver-sion 3.7.4; Honnibal et al., 2020) for a multi-classNER task. The model leverages transformer-based 5This dataset (CC-BY-NA-SA 4.0) is publicly available atthe ASC-Treebank GitHub repository( dataset (CC-BY-NA-SA 4.0) is publicly available atthe ASC-Treebank GitHub repository and the osf repository(",
  ": Experiment overview": "RoBERTa embeddings (via the en_core_web_trfpipeline), integrating pre-trained embeddings andfine-tuning them for the NER task by adjusting themodels weights based on the labeled data usedfor training. SpaCys method includes a transition-based parser, which is primarily used for depen-dency parsing but, in this case, provides syntac-tic context that enhances the NER models perfor-mance.To evaluate the performance, we constructedthree comparative models: (1) a model using silver-standard data, (2) a model trained with gold L1data, and (3) a model trained with both gold L1 andL2 data. Considering the necessity for accurate per-formance on L2 data to capture non-native Englishlinguistic structures, we conducted detailed testingon each L1, L2 written, and L2 spoken data. Forspecifics on the hyperparameter settings, refer toAppendix B.",
  "Experiment 2": "The goal of the second experiment is to exploreprompt-guided annotation of unlabeled data. Tothis end, GPT-4 was employed to generate labelsfor a subset of the test set from the gold-standardtreebank. Due to the high processing costs andtime, we streamlined the task by filtering the tag set reducing the number of tags from nine to seven byremoving the ATTR and PASSIVE tags. Moreover,we utilized a random balanced extraction method toselect sentences for annotation, ultimately resultingin a total of 282 sentences.To evaluate performance, we provided GPT-4with three distinct prompts for label generation onthe test set: (1) zero-shot, (2) 3-shot, and (3) 10-shot. In cases of few-shot learning, examples wererandomly selected from the gold-standard ASCtreebank (including both L1 and L2 datasets). Wecompared these results with baseline scores fromthe best model trained under a supervised learn-ing. This comparative model, as described in Ex-",
  "ASC (Annotated tag)Semantic frameExample": "Attributive (ATTR)theme-V-attributeIttheme is now visibleattribute on the streetCaused-motion (CAUS_MOT)agent-V-theme-destinationIagent put ittheme [on the calendars]destinationDitransitive (DITRAN)agent-V-recipient-themeIagent gave himrecipient [the address]themeIntransitive motion (INTRAN_MOT)theme-V-goalItheme wont go [out the door]goalIntransitive resultative (INTRAN_RES)patient-V-resultMoneypatient may become tightresultIntransitive simple (INTRAN_S)agent-VIagent am working from the officePassive (PASSIVE)theme-aux-VpassiveTheytheme were recommendedpassive by himTransitive resultative (TRAN_RES)agent-V-result-resultIagent dont want [my leg]result hurtresultTransitive simple (TRAN_S)agent-V-themeIagent should buy [a new one]theme",
  "Experiment 3": "We explore the use of prompt-guided generationof training data for training RoBERTa. The ex-periment was designed to first train the RoBERTamodel using only the data generated by GPT-4 andthen compare its performance with a model trainedusing gold standard data, as detailed in .The results reveal two key findings: First, increas-ing the number of examples, from 3-shot to 10-shot,enhanced model performance. The F1-scores gen-erally improved with the number of examples pro-vided, with the 10-shot configuration substantiallyoutperforming the 3-shot across most categories. to Appendix C for detailed results and the prompts used.11Among the three models in Experiment 1, the baselinemodel most closely resembles the best model trained on goldL1+L2 data, though it is not exactly the same, as it was onlytrained for 400 iterations. This highlights the role of example-driven guid-ance in enhancing the quality of machine-generatedtraining data; Second, despite the performancegains observed with an increased number of exam-ples, models trained solely with gold data (gold1)consistently outperform those trained with the GPT-4 generated data (both 3-shot and 10-shot), partic-ularly in more complex ASCs (e.g., CAUS_MOT,TRAN_RES). Although machine-generated datashowed some potential in the training process forone ASC type (INTRAN_MOT), it still falls shortof the effectiveness of human-annotated data.",
  "The model was trained for only 400 iterations": "bined these generated sentences with a similarlybalanced selection from the gold-standard datasetto augment the training set. This approach allowedthe integration of artificially generated and golddata into two additional experimental groups: onetrained with 3-shot (i.e., sentences generated from3-shot setting) plus gold data, and another with10-shot plus gold data. The data were convertedto IOB format to train RoBERTa. We then com-pared the performance of these models to baselinescores from a model trained on fewer gold datasentences8. This comparison additionally aimed toevaluate the effectiveness of augmenting trainingsets with machine-generated data versus additionalhuman-annotated data. We ensured consistency inhyperparameters and the number of training epochsto facilitate comparability9. shows an ex-ample of a few-shot learning.",
  "Experiment 1": "We investigated the performance of supervisedlearning using gold-standard data applied withRoBERTa embeddings. The results, detailed in Ta-ble 2, highlight the best performance of the modeltrained using gold-standard data that includes bothL1 and L2 annotations (Gold L1+L2 train model).It demonstrated the highest averaged F1 scoresacross all tested datasets: L1 (F1 = 0.912), L2Written (F1 = 0.915), and L2 Spoken (F1 = 0.928).It also outperformed the other models in individ-ual tag accuracy, securing the highest F1 scoresfor seven out of nine annotation types in both theL2 Written and Spoken datasets. Additionally, the 8This adjustment was made because the GPT-4 generatedsentences typically had fewer ASC types, necessitating a re-duction in the gold training data for a fair comparison.9We used the same hyperparameter settings as the firstexperiment and also did the early stopping of stop at 400iterations of the training data.",
  ": Example of prompting GPT-4 to generateASC labels in a few-shot setting": "model trained on the gold-standard L1 dataset (ex-cluding L2) achieved top F1 scores for four outof nine tags in the L1 dataset. Overall, these re-sults underscore the effectiveness of high-quality,manually annotated gold-standard data, as modelstrained on these datasets consistently outperformedthose trained on silver-standard data in the devel-opment of effective ASC models.",
  ": Comparison of F1-scores for ASC taggingusing different training sets, trained with RoBERTa (Ex-periment 3)": "The second part of the experiment aimed to deter-mine if augmenting the gold-standard training setwith GPT-4-generated data could enhance the per-formance of the supervised learning model. As il-lustrated in , introducing machine-generateddata (both 3-shot and 10-shot) into the gold data setdid not consistently improve performance acrossall ASC tags12. The weighted average F1 scoreindicated that models trained with a combinationof gold and machine-generated data (0.788 for 3-shot+gold and 0.808 for 10-shot+gold) generallyperformed less effectively or, at best, equally com-pared to those trained solely on gold-standard data(0.808).Furthermore, the results demonstrate that themost significant improvement in performance wasobserved when gold data was augmented with ad-ditional gold data (gold1+gold2), achieving thehighest weighted average F1 of 0.877. This under-scores that while machine-generated data can im-prove training effectiveness for certain ASC types(e.g., TRAN_RES in 3-shot+gold, INTRAN_S in 3-",
  "There are some cases where it slightly enhances themodels effectiveness, as seen in the TRAN_RES and IN-TRAN_S tags": "shot+gold and 10-shot+gold), incorporating morehuman-annotated gold data (i.e., gold1+gold2) sig-nificantly enhances model accuracy across all inves-tigated ASC categories. Upon closer examinationof the machine-generated training data, it becameevident that despite the prompts directing GPT-4 togenerate sentences closely resembling the human-produced examples in the 10-shot set, the modelstruggled to capture the nuances present in sen-tences from human sources, such as the web corpusor L2 datasets (See Appendix D). In other words,GPT-4-generated sentences tend to be shorter andless complex, typically lacking multiple clauses,unlike the more elaborate sentences crafted by hu-mans. This limitation likely impacted the qualityof the training data and, consequently, the effective-ness of the training outcomes.",
  "Conclusions": "This study highlights the potential of integrat-ing PLMs into linguistic analysis frameworks,particularly for examining the characteristics ofASCs in the context of modeling L1 and L2 learn-ing/development. RoBERTa, when trained on gold-standard datasets, demonstrated the best perfor-mance in ASC annotations, underscoring the im-portance of comprehensive, high-quality annotateddata. Additionally, the use of GPT-4 for prompt-guided annotation and data generation offered someinsights into the effectiveness of synthetic data inmodel training. While these methods did not sur-pass the F1 scores of the baseline model trainedsolely on gold-standard annotations, they provedeffective in identifying and processing certain typesof ASCs.",
  "Limitations": "The accuracy of ASC annotation was assessedacross three linguistic domainsL1 written, L2written, and L2 spokenbut only a single registerwithin each domain was examined in Experiment 1.Experiments 2 and 3 did not comprehensively ex-plore model performance across different domains.Consequently, the applicability of these models inother registers, such as L2 written narratives or L2argumentative speeches, remains uncertain, partic-ularly with the RoBERTa model. Furthermore, theGPT-4 model should have also included investi-gations into two additional ASC types (PASSIVE,ATTRIBUTE) and comparisons across differentlinguistic domains. Additionally, due to the limitedscope of the L2 datasets, certain ASC types, such astransitive and intransitive resultative constructions,were underrepresented in the test sets. Therefore,the annotation accuracy for these specific ASCsshould be interpreted with caution.",
  "All data and models are available in are licensed under the Creative Com-mons Attribution-NonCommercial-ShareAlike 4.0International License (CC BY-NC-SA 4.0)": "Alan Akbik, Laura Chiticariu, Marina Danilevsky, Yun-yao Li, Shivakumar Vaithyanathan, and Huaiyu Zhu.2015. Generating high quality proposition banks formultilingual semantic role labeling. In Proceedingsof the 53rd Annual Meeting of the Association forComputational Linguistics and the 7th InternationalJoint Conference on Natural Language Processing(Volume 1: Long Papers), pages 397407. Ben Ambridge, Julian M Pine, Caroline F Rowland,Franklin Chang, and Amy Bidgood. 2013. The re-treat from overgeneralization in child language ac-quisition: Word learning, morphology, and verb ar-gument structure. Wiley Interdisciplinary Reviews:Cognitive Science, 4(1):4762.",
  "Giulia ML Bencini and Adele E Goldberg. 2000. Thecontribution of argument structure constructions tosentence meaning.Journal of Memory and Lan-guage, 43(4):640651": "Yevgeni Berzak, Jessica Kenney, Carolyn Spadine,Jing Xian Wang, Lucia Lam, Keiko Sophie Mori,Sebastian Garza, and Boris Katz. 2016. Universaldependencies for learner english.arXiv preprintarXiv:1605.04278. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Danqi Chen and Christopher D Manning. 2014. A fastand accurate dependency parser using neural net-works. In Proceedings of the 2014 conference onempirical methods in natural language processing(EMNLP), pages 740750. Marie-Catherine de Marneffe and Christopher D Man-ning. 2008. The stanford typed dependencies repre-sentation. In Coling 2008: proceedings of the work-shop on cross-framework and cross-domain parserevaluation, pages 18.",
  "Bosheng Ding, Chengwei Qin, Linlin Liu, Yew KenChia, Shafiq Joty, Boyang Li, and Lidong Bing. 2022.Is gpt-3 a good data annotator?arXiv preprintarXiv:2212.10450": "Masaki Eguchi and Kristopher Kyle. 2023. Span identi-fication of epistemic stance-taking in academic writ-ten english. In Proceedings of the 18th Workshop onInnovative Use of NLP for Building Educational Ap-plications (BEA 2023), pages 429442. Associationfor Computational Linguistics (ACL). Nick C Ellis. 2002. Frequency effects in language pro-cessing: A review with implications for theories ofimplicit and explicit language acquisition. Studies insecond language acquisition, 24(2):143188.",
  "Kristopher. Kyle and Scott Crossley. 2017. Assessingsyntactic sophistication in l2 writing: A usage-basedapproach. Language Testing, 34(4):513535": "Kristopher Kyle, Masaki Eguchi, Aaron Miller, andTheodore Sither. 2022. A dependency treebank ofspoken second language english. In Proceedingsof the 17th Workshop on Innovative Use of NLPfor Building Educational Applications (BEA 2022),pages 3945. Kristopher Kyle and Hakyung Sung. 2023. An argu-ment structure construction treebank. In Proceedingsof the First International Workshop on ConstructionGrammars and NLP (CxGs+ NLP, GURT/SyntaxFest2023), pages 5162. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A robustly optimized bert pretraining ap-proach. arXiv preprint arXiv:1907.11692. Alessio Miaschi and Felice DellOrletta. 2020. Con-textual and non-contextual word embeddings: an in-depth linguistic investigation. In Proceedings of the5th Workshop on Representation Learning for NLP,pages 110119.",
  "Peng Shi and Jimmy Lin. 2019. Simple bert models forrelation extraction and semantic role labeling. arXivpreprint arXiv:1904.05255": "Natalia Silveira, Timothy Dozat, Marie-CatherineDe Marneffe, Samuel R Bowman, Miriam Connor,John Bauer, and Christopher D Manning. 2014. Agold standard dependency corpus for english. InLREC, pages 28972904. Citeseer. Hakyung Sung and Kristopher Kyle. 2024. Annotationscheme for English argument structure constructionstreebank. In Proceedings of The 18th Linguistic An-notation Workshop (LAW-XVIII), pages 1218, St.Julians, Malta. Association for Computational Lin-guistics (ACL).",
  "Human-generated Sentences": "1. What if Google expanded on its search-engine (and now e-mail) wares into a full-fledgedoperating system?2. I doubt the very few who actually read my blog have not come across this yet, but I figured Iwould put it out there anyways.3. Click here to view it.4. One of the pictures shows a flag that was found in Fallujah.5. Compare the flags to the Fallujah one.6. Let me join the chorus of annoyance over Googles new toolbar, which, as noted in the linkedarticle, commits just about every sin an online marketer could commit, and makes up a few newones besides.7. You dont need to use their site, you can opt-out of sharing your information, you dont need tosend stuff to anyone with a Gmail account, and if wonder of wonders youre worried that youmight send something to someone who would forward an excerpt to someone who would thenstore it on a Gmail account... you have far, far too much time on your hands.8. On the other hand, it looks pretty cool.9. Keep his cage open and go on your computer, or read a book, etc and maybe he will come out toyou.10. Please let us know if you need anything else.",
  "GPT-4 generated Sentences": "1. I ran to the store to buy milk.2. She was so excited to see her friends at the party.3. The sun set, painting the sky with hues of orange and pink.4. He gave me the book that I wanted.5. The leaves fell from the trees as autumn arrived.6. She baked the cake until it was golden brown.7. He threw the ball to his dog.8. The dog chased the cat up the tree.9. The bird flew out of the cage.10. The child threw the ball into the hoop."
}