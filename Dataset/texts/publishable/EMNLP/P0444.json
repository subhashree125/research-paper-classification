{
  "Abstract": "Language models will inevitably err insituations with which they are unfamiliar.However, by effectively communicating un-certainties, they can still guide humans towardmaking sound decisions in those contexts. Wedemonstrate this idea by developing HEAR,a system that can successfully guide humansin simulated residential environments despitegenerating potentially inaccurate instructions.Diverging from systems that provide userswith only the instructions they generate, HEARwarns users of potential errors in its instructionsand suggests corrections. This rich uncertaintyinformation effectively prevents misguidanceand reduces the search space for users. Evalua-tion with 80 users shows that HEAR achieves a13% increase in success rate and a 29% reduc-tion in final location error distance compared toonly presenting instructions to users. Interest-ingly, we find that offering users possibilities toexplore, HEAR motivates them to make moreattempts at the task, ultimately leading to ahigher success rate. To our best knowledge, thiswork is the first to show the practical benefits ofuncertainty communication in a long-horizonsequential decision-making problem.1",
  "Introduction": "Expecting language models to consistently makeaccurate predictions in a dynamic world is unreal-istic (Kalai and Vempala, 2024; Xu et al., 2024).Evidence shows that these models often falter inunfamiliar situations (Wu et al., 2023; Dziri et al.,2024). Given the inherent fallibility of languagemodels, an important research problem is to enablethese models to successfully assist humans evenwhen they make errors.But how is it possible for a model to guide ahuman toward the right decisions when it cannotprecisely specify what those decisions are? This",
  ":HEAR detects errors in a navigationinstruction and suggests corrections. It enables humansto avoid being misled and efficiently search theenvironment, leading to improved performance": "work demonstrates the feasibility of tackling thisproblem in a language-guided visual navigation set-ting. Concretely, we develop HEAR (HallucinationDEtection And Remedy), a system that aids humannavigation in 3D residential environments using po-tentially erroneous natural language instructions.The key to the success of HEAR is its ability tocommunicate various types of uncertainty infor-mation to users. Specifically, HEAR can identifyand highlight potential errors in an instruction, andsuggest possible corrections. This information pre-vents misdirection and narrows the search spacefor users, enabling them to navigate successfullyeven when given inaccurate instructions. To our best knowledge, our work presents thefirst study on the effects of uncertainty communica-tion on human decision making in a long-horizontask. Although uncertainty communication hasbeen identified as crucial for AI systems, very fewstudies have investigated how uncertainty infor-mation impacts human decisions. Previous stud-ies have primarily focused on classification tasksrather than long-horizon tasks, and on numericaluncertainty (i.e., probability) rather than verbal un- certainties (Vodrahalli et al., 2022; Nizri et al.). Bydemonstrating that presenting uncertainties leads toa substantial performance boost in this navigationtask, we provide strong evidence to support the de-velopment of these features in sequential decision-making AI agents.To build HEAR, we tackle the problem ofdetecting and classifying hallucinated phrases invisually grounded instructions. This problem isparticularly challenging in the environments westudy because of the realisticity and diversity ofthe visual scenes. Our solution involves trainingtwo vision-language models: one for hallucinationdetection and the other for classification (i.e.,deciding whether a phrase should be deleted orreplaced). We combine these models to identifyhallucinations in an instruction, as well as scoreand rank potential corrections. To train each model,we fine-tune a large vision-language model (Guhuret al., 2021) with synthetically created data tooptimize for a contrastive learning objective. Weintroduce a practical methodology for generatingsynthetic data, combining rule-based approacheswith large language models.We conduct an evaluation with 80 human usersto measure the effectiveness of HEAR. Our resultsdemonstrate that incorporating HEAR improvesuser navigation outcomes. Specifically, HEARincreases the likelihood of a user successfullyreaching their destination by 13% and reducesthe average distance to the true destination by29%. Analyzing human behavior reveals that byproviding useful hints, HEAR motivates humansto put more effort into solving a task, leading toa higher success rate.Interestingly, our results suggest that the uncer-tainty communication capabilities of a system donot need to be flawless to boost user performance.The components of HEAR are all imperfect: theerror detection and correction, and the instructiongeneration capabilities are all of reasonable quality,but not faultless. However, because these capa-bilities complement one another, and complementthe knowledge of the human user, they ultimatelyimprove user decisions.",
  "Grounded instruction generation.Grounded in-struction generation involves creating language in-structions for navigation in situated environments,evolving from simple settings (Anderson et al.,": "1991; Goeddel and Olson, 2012; Fried et al., 2018a)to more complex, photo-realistic simulations (Friedet al., 2018b; Kamath et al., 2023; Zhao et al.,2023a). Model-generated instructions can containlandmark errors (e.g., confusing a bathroom with agym) and path errors (e.g., instructing a left turn in-stead of a right turn) (Wang et al., 2022). Zhao et al.(2023a) demonstrate a significant gap between thequality of model- and human-generated instruc-tions. However, their work is not concerned witherror detection. Uncertainty communication for human-AI col-laboration.As AI-assisted decision-making hasbecome the norm, it is imperative to investigate theinfluence of human cognitive biases on their per-ception of model-generated information (Rastogiet al., 2022). Several studies have questioned thenecessity of probabilistic calibration, showing thatpresenting uncalibrated probabilities may improvehuman decisions c(Benz and Rodriguez, 2023; Vo-drahalli et al., 2022; Nizri et al.). Other researchproposes model designs to better calibrate humantrust (Zhang et al., 2020; Ma et al., 2023; Buincaet al., 2021). The experimental settings in all ofthese papers focus on classification tasks ratherthan long-horizon decision-making tasks, as ex-plored in this work.Regardingcomplementaryperformanceinhuman-AI collaboration, Bansal et al. (2021)famously demonstrate that presenting model-generated explanations to humans does not en-able human-AI teams to outperform individual en-tities. We present a contrasting result, showing thata complementary performance boost is possiblewith careful selection and presentation of model-generated information. Hallucination Detection.Neural text generationmodels produce hallucinations in textual domains(Kalai and Vempala, 2024; Mller et al., 2020;Maynez et al., 2020; Durmus et al., 2020; Liu et al.,2022) as well as multimodal domains (Wisemanet al., 2017; Rohrbach et al., 2018; Liu et al., 2024;Chen et al., 2024). Hallucination detection hasbeen explored, but primarily for machine transla-tion (Dale et al., 2023; Xu et al., 2023; Wang andSennrich, 2020; Zhou et al., 2021) or summariza-tion (Falke et al., 2019; Kryscinski et al., 2020;Chen et al., 2021). Closest to our work is Zhaoet al. (2023b), who study this problem in a similarvisual navigation setting. However, their modelcannot provide correction suggestions, nor do they",
  "Problem Setting": "We consider the problem of generating languageinstructions to guide a human to follow an intendedroute in an environment. The concrete goal is tobuild a speaker model S(w | r), which takes an in-tended route r as input and generates a correspond-ing language instruction w as output ().The instruction w = (w1, . . . , wn) is a sequenceof words (e.g., Walk past the couch and turnright. Walk down the hallway and stop in thebedroom.). The route r = (o1, a1, . . . , ol, al)is a sequence of observations and actions, whereeach observation is a collection of RGB imagesthat capture the view at a location, and each actionrepresents a transition from one location to another.The speaker is evaluated through an instruction-following task, in which a human user receives aninstruction generated by the speaker and followsit in the corresponding environment. Success isachieved if the user reaches the final location alongthe intended route.To simulate this problem, we employ the Mat-terport3D simulator and Room-to-Room (R2R)dataset (Anderson et al., 2018) for model trainingand human experiments. Matterport3D is a photo-realistic simulator that features images taken fromvarious real residential buildings. The R2R datasetcontains pairs of route and language instruction.The instructions contain more than 7,000 objectand direction phrases.We follow Zhao et al. (2023a) to train aT5-based (Raffel et al., 2020) speaker model. Theinstructions generated by this model often containobject or directional phrases that are inconsistentwith the scenes along the intended route.Werefer to such phrases as hallucinations.Wecategorize hallucinations into two types: intrinsichallucination is a phrase that needs to be replacedbecause it inaccurately describes an observationor action (e.g., an instruction says Walk past thereception desk and out the door on the right,but on the intended route, the door is on the left);extrinsic hallucination is a phrase that needs to beremoved because it does not have a correspondenceon the input route (e.g., Walk through the officeand out of the office. Walk into the hallway andturn left, where the second sentence describes apath that does not exist in the environment). Upon",
  "HEAR: Hallucination Detection andRemedy": "In this section, we introduce HEAR, whichaugments a speaker model by enabling it to (i)highlight potential hallucinations in an instructionand (ii) produce a list of plausible corrections foreach hallucination. We expect that (i) would helpa user avoid being misled into incorrect regions,while (ii) would reduce the effort required to locatethe correct region. We build two models ( 4.1, 4.2, illustrated in ) to generate thesepieces of information and design an interface toeffectively convey them to users (4.4).",
  "Hallucination Detection": "The hallucination detection model predicts hallu-cinations in an instruction. We adopt the modelfrom Zhao et al. (2023b) but train it on a differenttraining set so that it can detect phrases instead ofjust tokens as in the original work.We frame the hallucination detection problemas a binary classification task: given an input x =(r, w, i, j) consisting of a route r, an instruction w,and token indices i, j {1, , n}(i j), decidewhether the phrase wi:j = (wi, wi+1, ..., wj) is ahallucination (more specifically, whether it shouldbe replaced or removed to make w consistent withr). For example, in the instruction shown in Fig-ure 1, w6:7 is predicted to be a hallucination. Weuse a combination of a POS tagger2 and GPT-3.5-turbo to identify the phrases to be classified.Our model is a classifier PH(y = 1|x =(r, w, i, j)) that is fine-tuned from the Airbertmodel (Guhur et al., 2021)a vision-languagemodel pre-trained on a large corpus of captionedhousehold scenes collected from AirBnB.For each instruction, we wrap the phrasesto be classified between a pair of specialtokens([BH]and[EH]).Forexample,ifwi:jisclassified,theinstructionbecomes[ w1, . . . , [BH], wi, ..., wj, [EH], . . . , wn ].Themodel takes as input this annotated instruction andthe visual route and outputs a score s(x). The hal-lucination confidence is calculated as PH(x) =(s(x)), where is the sigmoid function. The",
  "Language Transformer": ": Our hallucination detection model (top) and hallucination type classification model (bottom). Each modeltakes a language instruction and a visual route as input and predicts a binary label. For hallucination detection, thelabel is whether a phrase is a hallucination. For hallucination-type classification, the label is whether a hallucinationis extrinsic (needed to be replaced) or extrinsic (needed to be removed). Each model is built on top of a pre-trainedvision-language model and is fine-tuned using contrastive learning. The first model is used to decide which phrasesto highlight in an instruction, and the two models are combined to score and rank possible corrections.",
  "Correction Suggestion": "For each phrase wi:j classified as hallucination byPH, we compute the top-K correction suggestions.To do so, we first generate a set of candidatecorrections { wmi:j}Mm=1 (this procedure will bedescribed in 4.3).For example, in ,{ wm6:7} is {turn right, walk straight}. A specialtoken [REMOVE] represents the deletion of thephrase. We train a hallucination-type classificationmodel, which allows us to rank these candidatesand choose the top K. Ranking suggestions.As mentioned in 3, wecategorize hallucinations into two types: intrinsicand extrinsic. Let zx denote the hallucination typeof a phrase x; zx = 1 if x is an intrinsic hallu-cination. We learn a binary classifier to estimatePI(z = 1 | x, yx = 1) where yx = 1 indicatesthat x is a hallucination. Let x = (r, w, i, j) and xbe the corrected version of x obtained by replacingwi:j with a candidate correction wi:j. We computea score R(x) for every candidate (the higher is thebetter). We consider two cases. If x indicates areplacement, we define R(x) as:",
  "PI(z = 1 | x, yx = 1) PH(y = 1 | x)(1)": "where the first term computes how likely xnecessitates a replacement, while the secondterm captures how good the proposed replace-ment x is.If x indicates a deletion, we setR(x) = PI(z = 0 | x, yx = 1), which estimatesthe probability that x is an extrinsic hallucination(thus requiring deletion). Hallucination type classification.The model PIuses the same model architecture and is trained ina similar fashion as the hallucination model PH.However, it solves a different problem: determin-ing the type of a hallucination rather than identi-fying whether a phrase is a hallucination. Thisis achieved by training on a different dataset, asdescribed in 4.3.",
  "Dataset Creation": "To train the models described in previous sections,we construct training datasets with positive andnegative examples, defined by the specific classifi-cation problem. We also create a set of candidatecorrections for each predicted hallucination. Ashuman-labeled training data is costly to obtain, wesynthetically create training data by taking human-generated instructions in the R2R training set andperturbing them using rule-based procedures andGPT models. Training data for hallucination detection.Forthis problem, the negative examples are instructionsfrom the R2R training set (Anderson et al., 2018),which are assumed to contain no hallucinations. Tocreate a positive example from a negative exampledenoted by x = (r, w, i, j)}, we perturb theinstruction w in various ways. Following Zhaoet al. (2023b), we focus on three types of intrinsichallucinations: room, object, and direction. Wecreate a room hallucinations by replacing a roomphrase with another randomly chosen from a pre-composed list, and generate an object hallucinationby replacing an object phrase with another thatappears in the same instruction. For directions,since one can be expressed in various ways (e.g.",
  "go straight is the same as proceed forward), weleverage GPT-3.5-turbo to modify them, using thefollowing prompt (the few-shot examples are notshown for brevity; the full prompt is in A.1):": "SYSTEM: Find a directional word/phrase in theoriginal instruction, and substitute it with a com-pletely different directional word/phrase, so a per-son following the modified instruction would go ina different direction from the original instruction.Craft three modified instructions for each originalinstruction, and utilize the <s></s> tag pair to high-light the directional word/phrase youve modifiedin both the original and modified instructions.Input: Walk out of the bedroom and turn left.Output: <original1> walk <s> out of </s>the bedroom and turn left . </original1><modified1> walk <s> around </s> the bed-room and turn left . </modified1> Meanwhile, an extrinsic hallucination in an in-struction is constructed by inserting a sentencetaken from the same or a different instruction into arandomly selected beginning-of-sentence locationwithin the instruction. Multiple hallucinations are created within an in-struction, but only one is wrapped by the [BH] [EH]tags for classification. We also add hallucinationsto the negative example, but ensure that the spanenclosed by [BH] [EH] is not a hallucination. Training data for hallucination-type classifica-tion.For this dataset, both the positive and neg-ative examples contain hallucinations, but the en-closed spans in the positive examples are intrinsichallucinations, while those in the negative exam-ples are extrinsic hallucinations. We apply the ap-proach used in the detection problem to synthesizehallucinations. Generating sets of candidate corrections.Wegenerate a set of candidate corrections for eachpredicted hallucination. The candidate correctionsfor a room or an object hallucination are all therooms and objects provided by the Matterport3Dsimulator. For directions, we ask GPT-4 to generatecandidates, using the following prompt (the few-shot examples are not shown; the full prompt is inA.1): SYSTEM: Find directional words/phrases in theinstruction and use <original> </original> tags tomark them, and list all the possible substitutions tochange the meaning completely with <modified></modified> tags, so that a person following thesubstituted instruction would go in a different di-rection from the original instruction. Use <sep>to separate each substitution, and do not mark thenouns.Input: Walk out of the bedroom and turn left.Output: walk <original1> out of </original1><modified1>into<sep>around<sep>",
  "Designing Communication Interface": "We build on top of the interface developed by Kuet al. (2021) and Zhao et al. (2023a) which allows ahuman to follow a language instruction to interactwith a Matterport3D environment. We augment theinterface to display highlights and suggestions forpotential hallucinations. This section discusses ourdesign principle; more details and a visualizationof the interface are given in A.5.Our system generates a lot of information thatcan potentially be communicated to users. Decid-ing what piece of information to present and how topresent it is vital to the success of the system. Wechoose not to present model probabilities to usersbecause they can be miscalibrated and even if theyare, different people might interpret them differ-ently (Vodrahalli et al., 2022). Instead, we conveybinary predictions of hallucinations through high-lights. To do so, we select a decision threshold forthe hallucination detection model to maximize itsF-1 score on a manually annotated development set.If all phrases in a clause are highlighted, we simplyhighlight the entire clause and treat the clause as asingle hallucination. For each instruction, we high-light at most three hallucinations predicted by themodel, which is approximately the average numberof hallucinations in an instruction detected by ourhuman annotators.For suggestions, because their presence can beoverwhelming, we display them only when the user deliberately seeks them out. Initially, the user seesonly the instruction (potentially with hallucinationhighlights). We instruct them to click on a high-lighted phrase if they also suspect it to be a hal-lucination and want to view possible corrections.If that happens, a drop-down menu will appear,displaying the top three suggestions in descendingorder by the score produced by our ranking models.The user can click on a suggestion to apply it to theinstruction, which closes the drop-down menu. Weexplicitly instruct users to correct the instruction toencourage them to consider the suggestions.A complication we encounter is to decide howmuch information about the true final locationshould be revealed to the users. If users do notknow the true final locations, they cannot correctthe instructions. However, if the location is com-pletely revealed to them, the influence of the in-structions on their behavior is significantly weak-ened, undermining the purpose of our study. Toaddress this issue, we introduce a Check button,which enables the human to verify whether theyhave reached the final location. The button enablesusers to correct instructions while also retainingtheir reliance on instructions. In addition, analyz-ing user button usage uncovers interesting insightsabout their behavior.",
  "(Q2) Does providing hallucination highlights andsuggesting corrections improve human navi-gation performance?": "(Q3) What are the effects of highlights and sugges-tions on human behavior?To answer Q1, we evaluate HEAR intrinsicallywith human-annotated data. To answer Q2 andQ3, we conduct a human evaluation with varioussystems, including ablated versions of HEAR andan oracle human-based system. Data.To train the hallucination detection model,we synthetically generate a training set with164,939 pairs of positive and negative examples(4.3), which are created from the Room-to-Room(R2R) (Anderson et al., 2018) train set (4,675routes, each route has 3 human-annotated instruc-tions). To train the hallucination type classificationmodel, we generate 117,357 pairs of positive andnegative examples, created from the R2R train set. For both evaluations, we first use a speakermodel ( 3) to generate instructions describingroutes from the R2R validation seen split. Forintrinsic evaluation and model selection, we ran-domly select and annotate 40 routes from the splitas our Dev Set. For human evaluation, we usethe 75 test routes from previous work (Zhao et al.,2023a,b) as our Test Set. There is no overlap be-tween the Dev Set and the Test Set.",
  "Intrinsic Evaluation: HallucinationDetection and Correction Suggestion": "Annotation.We manually annotate hallucina-tions in the instructions generated by the speakermodel, with mutual agreement from two of theauthors. We also annotate corrections for thosespans that we label hallucinations.In the end,we create intrinsic evaluation datasets consistingof 376 examples from the Dev Set for modelselection; and 625 examples from the Test Set fortesting, as well as used by the Oracle system forhuman evaluation (5.2).",
  "(c) One-stage HEAR combines hallucination de-tection and type classification into a singlemodel (more details in A.2). This model candirectly score each correction suggestion": "(d) Random samples a label uniformly at randomamong all possible labels, where the labels are{yes, no} for hallucination detection, and arethe set of all possible 3-element subsets of thecandidate set for correction suggestion. Metrics.We compute macro-averaged F-1 forhallucination detection and compute Recall@3for correction suggestion, which is the empiricalchance that the gold correction appears in the top-3suggestions ranked by a system.",
  "Random42.647.843.850.4HEAR-SameEnvSwap64.875.069.178.7One-stage HEAR62.882.760.986.2HEAR (final)63.488.466.570.6": ": Intrinsic evaluation of HEAR and our baselinesystems.The decision threshold for each systemis selected to maximize the F-1 score on the DevSet. R@3 computes how often the top-3 correctionsuggestions contain the gold annotated correction. the range of 70-90%, showing that they have a highpotential to aid humans.The results in hallucination detection show aclear trend, HEAR-SameEnvSwap is the bestmodel in terms of F-1 score, followed by HEARand finally one-stage HEAR. This indicates that thedata-creation strategy in the HEAR-SameEnvSwaptraining set is beneficial. Meanwhile, the perfor-mance of one stage HEAR is low, possibly becauseit has twice as few parameters as the other two mod-els. The results in correction suggestion recall aremore nuanced: HEAR is best on Dev but one-stageHEAR is superior on Test. HEAR-SameEnvSwapoutperforms others in hallucination detection, butits underperformance in correction suggestion indi-cates that the probabilities output by its hallucina-tion detection module are not reliable.Considering the average of F-1 and R@3, HEARis the best performing model on the Dev set. There-fore, we select it for evaluation with human users.",
  "(a) No communication only tells the user that theinstruction may be imperfect. It does not pro-vide highlights and suggestions, and is similarto the system in Zhao et al. (2023a)": "(b) HEAR (no suggestion) tells the user that theinstructions can be imperfect, highlights po-tential hallucinations, and tells the user thatthose phrases are potential errors. It does notprovide suggestions. This system is similar toZhao et al. (2023b). (c) HEAR is our final system, which adds to (b)the ability to suggest the top three correctionsfor each predicted highlight. We choose topresent the top three suggestions to balancethe systems recall performance with user",
  "(d) Oracle (no suggestion) is similar to (b) buthighlights are annotated by the authors": "(e) Oracle is similar to (c), but highlights andcorrections are annotated by the authors.It displays two instead of three candidatesuggestions: the original phrase and the goldcorrection.We evaluate each system on 18 routes randomlychosen from the Test Set. For each route and eachsystem, we recruit five human users using AmazonMechanical Turk and ask them to follow theinstruction generated by the system to describe theroute. Users are paid $4.10 for each session, whichinvolves performing 7 navigation tasks and takeson average 19 minutes to complete. One of thetasks is a quality-control task that appears in everysession. We analyze only sessions in which the userpasses this task. After completing a session, userscan provide feedback on the system. We ensurethat each user encounters each route only onceto prevent them from memorizing it. In total, werecruit 80 users and evaluate 525 navigation tasks.",
  "Metrics.We evaluate navigation performance us-ing standard metrics of the R2R task:(a) Success rate (SR ): fraction of examples inwhich the users final location is within 3m ofthe true goal;": "(b) Navigation error (DIST ): distance betweenthe users final location and the true goal.After a user has finished navigating, we ask fortheir subjective judgements about the route and theinstruction, specifically:(a) Is the instruction easy to follow?(b) Are you confident the path you followed is theintended path? (c) Is the task mentally demanding?For each question, we use 5-point Likert scale toask for a rating on the affirmative statement (e.g.,I am confident that I traversed the path that theAI system tried to describe). HEAR enhances user navigation performance.As seen in , compared to no communi-cation, simply highlighting potential errors usingHEAR increases user success rate (+6.7%) anddecreases navigation error (-1.9m). These resultsconfirm that error highlights can effectively com-pensate for the deficiencies of the instruction gen-eration model. A user described the effects of high-lights as follows: highlights help me know if theinstructions were going to be wrong. It made it",
  "No communicationHEAR (no suggestion)HEAROracle (no suggestion)Oracle": ": Performance measured by success rate (SR ) and navigation error (DIST ), and the number ofcheck-button clicks recorded when human users perform navigation tasks with different assistant systems. HEARimproves user navigation performance and is competitive with the two Oracle systems. The error bars for SRrepresent 85% confidence intervals. For DIST and Checks, the x marks the mean, the line inside a bar marksthe median, and the box represents the interquartile. shows the corresponding results in table format.",
  "No communication3.73.83.6HEAR (no suggestion)3.53.93.5HEAR4.04.2 3.5Oracle (no suggestion)3.93.83.6Oracle4.14.13.7": ": User subjective ratings of systems after com-pleting navigation sessions. The symbols and indi-cate results that are significantly higher than those of theNo communication system in the first row, with p <0.004 (Bonferroni correction for 12 tests comparing 4systems with No communication) and p < 0.05, re-spectively, as determined by a two-related-sample t-test. easier to know where to go back to and retracesteps in order to go to the right place. User perfor-mance is further improved with suggestions gener-ated by HEAR (+2. 2% in SR and -0.1m in DIST).a shows an example where a user whois provided with both highlights and suggestionssuccessfully reaches the target destination, whereas another user who is shown only highlights does not.Another notable pattern, shown in (mid-dle), is that adding highlights and suggestions sub-stantially decreases the variance of the navigationerror. This indicates that highlights and suggestionseffectively reduce the search space of the users. HEAR receives favorable subjective ratings.As shown in , users find the instructionsgenerated by HEAR (and Oracle systems) easierto follow and report greater confidence in theiractions. Despite being asked to correct errors inthe instructions, users do not report a significantincrease in mental load. HEAR improves user persistence in completingtasks. (rightmost) shows that users, onaverage, use the Check button more often whenprovided with highlights and suggestions. Thisresult suggests that these features incentivizeusers to make more attempts to solve the taskand consequently become more successful. We hypothesize that by suggesting possibilities forexploration,users can avoid blind searches,making them more willing to invest effort.Incontrast, without highlights and suggestions, userslack direction and may give up more quickly. Theymay perceive an entire instruction as incorrect andbelieve that the correct instruction could be entirelydifferent from the current one, leading them to feelthere is no hope in searching without further clues. Better highlights and suggestions further im-prove user performance. shows thatusers benefit from a better hallucination detectionmodel; they achieve a higher success rate (+5.5%)and a smaller navigation error (-1.3 m) when Ora-cle highlights are given, compared to when HEARhighlights are presented.User performance is also enhanced when usingan improved correction suggestion model: +10.0%in success rate and -1.9m in navigation error whenusing Oracle suggestions compared to when usingHEAR suggestions. b illustrates how a useris misled by incorrect highlights and suggestions.",
  "Conclusion": "We present a novel approach to enhance humantask performance by effectively communicatingmodel uncertainties.By encouraging users torefine AI-generated solutions, our approach offersan alternative to the conventional method thatfocuses on directly improving AI autonomouscapabilities while overlooking human capabilities.To fully unlock the potentials of AI technologies,we advocate for viewing AI systems not asindependent problem solvers, but as assistants andcollaborators of humans.Whileourresearchprimarilyaddresseslanguage-guided visual navigation, the insightsgained are broadly applicable to other vision-language tasks. Specifically, we have demonstratedthat:(i) it is feasible to generate meaningfulerror highlights and correction suggestions forvision-language models, and (ii) presenting thesehighlights and suggestions to human users canimprove their decision-making.Moreover, ourmethods for creating synthetic errors and correc-tion suggestions using rules and large languagemodels are generalizable to various contexts.",
  "Due to cost constraints, the scale of our humanevaluation is limited.We prioritize having": "more annotators evaluate each route over havingmore routes.Furthermore, the assessment ofcognitive load in the human evaluation study isnot sufficiently robust; we plan to administer otherschemes, such as the NASA Task Load Index(Hart, 2006), in future work.Before using the navigation interface, userswatch a video tutorial that explains the componentsof the interface and the associated questions. How-ever, this could be improved by incorporating awarm-up practice session to help users becomemore familiar with the interface.Another limitation of our human study isthat we cannot determine how much of theperformance improvement can be attributed tospecific highlights and their associated correctionsuggestions, as task performance is assessed solelybased on how close users are to the true finallocation. Additionally, we do not record the timewhen the Check button is pressed, which preventsus from analyzing the distribution of button pressesthroughout a navigation process.",
  "Acknowledgements": "We thank Hyemi Song, Yue Feng and MingyangXie for providing suggestions on improving humanevaluation interface. We thank Eleftheria Briakou,Connor Baumler, Trista Cao, Navita Goyal andother group members for providing suggestions onhuman evaluation experimental design. Anne H Anderson, Miles Bader, Ellen Gurman Bard,Elizabeth Boyle, Gwyneth Doherty, Simon Garrod,Stephen Isard, Jacqueline Kowtko, Jan McAllister,Jim Miller, et al. 1991. The hcrc map task corpus.Language and speech, 34(4):351366. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce,Mark Johnson, Niko Snderhauf, Ian Reid, StephenGould, and Anton Van Den Hengel. 2018. Vision-and-language navigation:Interpreting visually-grounded navigation instructions in real environ-ments. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 36743683. Gagan Bansal, Tongshuang Wu, Joyce Zhou, Ray-mond Fok, Besmira Nushi, Ece Kamar, Marco TulioRibeiro, and Daniel Weld. 2021. Does the wholeexceed its parts? the effect of ai explanations oncomplementary team performance. In Proceedingsof the 2021 CHI Conference on Human Factors inComputing Systems, pages 116.",
  "Nina L. Corvelo Benz and Manuel Gomez Rodriguez.2023.Human-aligned calibration for AI-assisteddecision making. In Thirty-seventh Conference onNeural Information Processing Systems": "Zana Buinca, Maja Barbara Malaya, and Krzysztof ZGajos. 2021. To trust or to think: cognitive forc-ing functions can reduce overreliance on ai in ai-assisted decision-making. Proceedings of the ACMon Human-Computer Interaction, 5(CSCW1):121. Sihao Chen, Fan Zhang, Kazoo Sone, and Dan Roth.2021.Improving faithfulness in abstractive sum-marization with contrast candidate generation andselection. In Proceedings of the 2021 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 59355941, Online. Association forComputational Linguistics.",
  "Xuweiyi Chen, Ziqiao Ma, Xuejun Zhang, SihanXu, Shengyi Qian, Jianing Yang, David F Fouhey,and Joyce Chai. 2024.Multi-object hallucina-tion in vision-language models.arXiv preprintarXiv:2407.06192": "David Dale, Elena Voita, Loic Barrault, and Marta R.Costa-juss. 2023. Detecting and mitigating halluci-nations in machine translation: Model internal work-ings alone do well, sentence similarity Even better.In Proceedings of the 61st Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 3650, Toronto, Canada. As-sociation for Computational Linguistics. Esin Durmus, He He, and Mona Diab. 2020. FEQA: Aquestion answering evaluation framework for faith-fulness assessment in abstractive summarization. InProceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 50555070, Online. Association for Computational Lin-guistics. Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lor-raine Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck,Peter West, Chandra Bhagavatula, Ronan Le Bras,et al. 2024. Faith and fate: Limits of transformers oncompositionality. Advances in Neural InformationProcessing Systems, 36. Tobias Falke, Leonardo FR Ribeiro, Prasetya AjieUtama, Ido Dagan, and Iryna Gurevych. 2019. Rank-ing generated summaries by correctness: An interest-ing but challenging application for natural languageinference. In Proceedings of the 57th Annual Meet-ing of the Association for Computational Linguistics,pages 22142220.",
  "Daniel Fried, Jacob Andreas, and Dan Klein. 2018a": "Unified pragmatic models for generating and follow-ing instructions. In Proceedings of the 2018 Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, Volume 1 (Long Papers), pages19511963, New Orleans, Louisiana. Association forComputational Linguistics. Daniel Fried, Ronghang Hu, Volkan Cirik, AnnaRohrbach, Jacob Andreas, Louis-Philippe Morency,Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein,and Trevor Darrell. 2018b. Speaker-follower mod-els for vision-and-language navigation. Advances inNeural Information Processing Systems, 31. Robert Goeddel and Edwin Olson. 2012.Dart: Aparticle-based method for generating easy-to-followdirections. In 2012 IEEE/RSJ International Confer-ence on Intelligent Robots and Systems, pages 12131219. IEEE. Pierre-Louis Guhur, Makarand Tapaswi, Shizhe Chen,Ivan Laptev, and Cordelia Schmid. 2021. Airbert: In-domain pretraining for vision-and-language naviga-tion. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 16341643. Sandra G Hart. 2006. Nasa-task load index (nasa-tlx);20 years later. In Proceedings of the human factorsand ergonomics society annual meeting, volume 50,pages 904908. Sage publications Sage CA: Los An-geles, CA.",
  "Adam Tauman Kalai and Santosh S Vempala. 2024.Calibrated language models must hallucinate.InProceedings of the 56th Annual ACM Symposium onTheory of Computing (STOC)": "Aishwarya Kamath, Peter Anderson, Su Wang, Jing YuKoh, Alex Ku, Austin Waters, Yinfei Yang, JasonBaldridge, and Zarana Parekh. 2023. A new path:Scaling vision-and-language navigation with syn-thetic instructions and imitation learning. In CVPR. Wojciech Kryscinski, Bryan McCann, Caiming Xiong,and Richard Socher. 2020. Evaluating the factualconsistency of abstractive text summarization. InProceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 93329346, Online. Association for Computa-tional Linguistics.",
  "Alex Ku, Peter Anderson, Jordi Pont-Tuset, and JasonBaldridge. 2021. Pangea: The panoramic graph envi-ronment annotation toolkit": "Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen,Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li,and Wei Peng. 2024.A survey on hallucinationin large vision-language models.arXiv preprintarXiv:2402.00253. Tianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao,Zhifang Sui, Weizhu Chen, and Bill Dolan. 2022.A token-level reference-free hallucination detectionbenchmark for free-form text generation. In Proceed-ings of the 60th Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 67236737, Dublin, Ireland. Associationfor Computational Linguistics.",
  "ai correctness likelihood to promote appropriate trustin ai-assisted decision-making. In Proceedings of the2023 CHI Conference on Human Factors in Comput-ing Systems, pages 119": "Arjun Majumdar, Ayush Shrivastava, Stefan Lee, PeterAnderson, Devi Parikh, and Dhruv Batra. 2020. Im-proving vision-and-language navigation with image-text pairs from the web. In Computer VisionECCV2020: 16th European Conference, Glasgow, UK, Au-gust 2328, 2020, Proceedings, Part VI 16, pages259274. Springer. Joshua Maynez, Shashi Narayan, Bernd Bohnet, andRyan McDonald. 2020. On faithfulness and factu-ality in abstractive summarization. In Proceedingsof the 58th Annual Meeting of the Association forComputational Linguistics, pages 19061919, On-line. Association for Computational Linguistics.",
  "Meir Nizri, Amos Azaria, Chirag Gupta, and NoamHazon. Does calibration affect human actions?": "Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J Liu. 2020. Exploring the limitsof transfer learning with a unified text-to-text trans-former. The Journal of Machine Learning Research,21(1):54855551. Charvi Rastogi, Yunfeng Zhang, Dennis Wei, Kush RVarshney, Amit Dhurandhar, and Richard Tomsett.2022. Deciding fast and slow: The role of cogni-tive biases in ai-assisted decision-making. Proceed-ings of the ACM on Human-Computer Interaction,6(CSCW1):122. Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns,Trevor Darrell, and Kate Saenko. 2018. Object hallu-cination in image captioning. In Proceedings of the2018 Conference on Empirical Methods in NaturalLanguage Processing, pages 40354045, Brussels,Belgium. Association for Computational Linguistics.",
  "Anderson. 2022. Less is more: Generating groundednavigation instructions from landmarks. In Proceed-ings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 1542815438": "Sam Wiseman, Stuart Shieber, and Alexander Rush.2017. Challenges in data-to-document generation.In Proceedings of the 2017 Conference on Empiri-cal Methods in Natural Language Processing, pages22532263, Copenhagen, Denmark. Association forComputational Linguistics. Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyrek,Boyuan Chen, Bailin Wang, Najoung Kim, Jacob An-dreas, and Yoon Kim. 2023. Reasoning or reciting?exploring the capabilities and limitations of languagemodels through counterfactual tasks. arXiv preprintarXiv:2307.02477. Weijia Xu, Sweta Agrawal, Eleftheria Briakou, Mari-anna J. Martindale, and Marine Carpuat. 2023. Un-derstanding and detecting hallucinations in neuralmachine translation via model introspection. Trans-actions of the Association for Computational Linguis-tics, 11:546564.",
  "Lingjun Zhao, Khanh Nguyen, and Hal Daum III.2023a. Define, evaluate, and improve task-orientedcognitive capabilities for instruction generation mod-els. In Findings of ACL": "Lingjun Zhao, Khanh Nguyen, and Hal Daum III.2023b. Hallucination detection for grounded instruc-tion generation. In Findings of the Empirical Meth-ods in Natural Language Processing: EMNLP 2023,Singapore. Association for Computational Linguis-tics. Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab,Francisco Guzmn, Luke Zettlemoyer, and MarjanGhazvininejad. 2021. Detecting hallucinated contentin conditional neural sequence generation. In Find-ings of the Association for Computational Linguis-tics: ACL-IJCNLP 2021, pages 13931404, Online.Association for Computational Linguistics.",
  "The following prompt is given to GPT-3.5-turbo to create direction hallucinations in instructions (4.3):": "Input: Walk out of the bedroom and turn left. Walk into the kitchen and stop by the counter.Output: (1) <original1> walk out of the bedroom and <s>turn left</s> . walk into the kitchen and stop by the counter .</original1> <modified1> walk out of the bedroom and <s>turn right</s> . walk into the kitchen and stop by the counter .</modified1>(2) <original2> walk <s>out of</s> the bedroom and turn left . walk into the kitchen and stop by the counter . </original2><modified2> walk <s>around</s> the bedroom and turn left . walk into the kitchen and stop by the counter . </modified2>(3) <original3> walk out of the bedroom and turn left . walk <s>into</s> the kitchen and stop by the counter . </original3><modified3> walk out of the bedroom and turn left . walk <s>out of</s> the kitchen and stop by the counter . </modified3>Input: Walk straight and turn left. Walk down the hallway and stop in the first doorway on your left.Output: (1) <original1> walk straight and turn left . walk <s>down</s> the hallway and stop in the first doorway on your left .</original1> <modified1> walk straight and turn left . walk <s>up</s> the hallway and stop in the first doorway on your left .</modified1>(2) <original2> walk straight and turn left . walk down the hallway and stop in the first doorway <s>on your left</s> . </original2><modified2> walk straight and turn left . walk down the hallway and stop in the first doorway <s>to your right</s> . </modified2>(3) <original3> walk straight and turn right . walk down the hallway and stop in the <s>first</s> doorway on your left .</original3> <modified3> walk straight and turn right . walk down the hallway and stop in the <s>second</s> doorway on yourleft . </modified3>Input: Exit the bathroom. Walk forward and go down the stairs. Stop four steps from the bottom.Output: (1) <original1> exit the bathroom . walk <s>forward</s> and go down the stairs . stop four steps from the bottom .</original1> <modified1> exit the bathroom . walk <s>backward</s> and go down the stairs . stop four steps from the bottom .</modified1>(2) <original2> <s>exit</s> the bathroom . walk forward and go down the stairs . stop four steps from the bottom . </original2><modified2> <s>enter</s> the bathroom . walk forward and go down the stairs . stop four steps from the bottom . </modified2>(3) <original3> exit the bathroom . walk forward and go down the stairs . stop four steps from the <s>bottom</s> . </original3><modified3> exit the bathroom . walk forward and go down the stairs . stop four steps from the <s>top</s> . </modified3>Input: walk through open door, turn left, walk toward fireplace turn right, stop outside doorway.Output: (1) <original1> walk through open door , turn left , walk toward fireplace turn right , stop <s>outside</s> doorway .</original1> <modified1> walk through open door , turn left , walk toward fireplace turn right , stop <s>inside</s> doorway .</modified1>(2) <original2> walk through open door , <s>turn left</s> , walk toward fireplace turn right , stop outside doorway . </original2><modified2> walk through open door , <s>go straight</s> , walk toward fireplace turn right , stop outside doorway . </modified2>(3) <original3> walk through open door , turn left , walk <s>toward</s> fireplace turn right , stop outside doorway . </original3><modified3> walk through open door , turn left , walk <s>away from</s> fireplace turn right , stop outside doorway . </modified3>",
  "The following prompt is given to GPT-4 to generate candidate direction corrections (4.3):": "SYSTEM: Find directional words/phrases in the instruction and use <original> </original> tags to mark them, and list all thepossible substitutions to change the meaning completely with <modified> </modified> tags, so that a person following thesubstituted instruction would go in a different direction from the original instruction. Use <sep> to separate each substitution,and do not mark the nouns.Input: Walk out of the bedroom and turn left. Walk into the kitchen and stop by the counter.Output: walk <original1> out of </original1> <modified1> into <sep> around <sep> to the left of <sep> to the right of</modified1> the bedroom and <original2> turn left </original2> <modified2> go straight <sep> turn right <sep> turn around</modified2> . walk <original3> into <original3> <modified3> out of <sep> pass </modified3> the kitchen and <original4> stopby <original4> <modified4> walk pass <sep> walk away from </modified4> the counter .Input: Walk straight and turn left. Walk down the hallway and stop in the first doorway on your left.Output: <original1> walk straight </original1> <modified1> turn left <sep> turn right <sep> turn around </modified1> and<original2> turn left </original2> <modified2> turn right <sep> go straight <sep> turn around </modified2> . <original3>walk down </original3> <modified3> stop in <sep> walk away from </modified3> the hallway and <original4> stop in</original4> <modified4> go into <sep> turn left at <sep> turn right at <sep> walk away from </modified4> the <original5>first </original5> <modified5> second <sep> third <sep> fourth <sep> last </modified5> doorway <original6> on your left</original6> <modified6> on your right <sep> straight ahead </modified6> .Input: Exit the bathroom. Walk forward and go down the stairs. Stop four steps from the bottom.Output: <original1> exit </original1> <modified1> enter </modified1> the bathroom . <original2> walk forward </original2><modified2> go backward <sep> turn left <sep> turn right </modified2> and <original3> go down </original3> <modified3> goup <sep> stop by <sep> walk away from </modified3> the stairs . stop <original4> four </original4> <modified4> one <sep>two <sep> three </modified4> steps from the <original5> bottom </original5> <modified5> top </modified5> .Input: walk through open door, turn left, walk toward fireplace turn right, stop outside doorway.Output: <original1> walk through </original1> <modified1> walk past </modified1> open door , <original2> turn left</original2> <modified2> turn right <sep> turn around <sep> go straight </modified2> , <original3> walk toward </original3><modified3> walk away from </modified3> fireplace <original4> turn right </original4> <modified4> turn left <sep> turnaround <sep> go straight </modified4> , stop <original5> outside </original5> <modified5> inside </modified5> doorway .",
  "No communication68.9 7.16.6 1.62.9 0.6HEAR (no suggestion)75.6 6.64.7 1.23.4 0.7HEAR77.8 6.34.6 1.24.1 0.8Oracle (no suggestion)81.1 6.0 3.4 0.9 3.5 0.7Oracle87.8 5.0 2.7 0.7 3.6 0.6": ": Performance measured by success rate (SR ) and navigation error (DIST ), and the number of check-button clicks recorded when human users perform navigation tasks with different assistant systems. The error barsafter represent 85% confidence intervals. The symbols and indicate results that are significantly higher thanthose of the No communication system in the first row, with p < 0.004 (Bonferroni correction) and p < 0.05,respectively, as determined by a two-related-sample t-test.",
  "A.2Model Variants": "HEAR-SameEnvSwap.This system is identical to HEAR, but the synthetic hallucinations are createdusing different strategies. In the case of object hallucination, rather than swapping two objects within thesame instruction, we replace an object in the instruction with another object randomly selected from thoseencountered along the described route. For room perturbation, instead of replacing a room mentioned inthe instructions with another room from a list, we substitute it with another room that exists in the sameenvironment. One-stage HEAR.This underlying model of this system is similar to the hallucination detection modelof HEAR. But its positive examples contain instructions with an empty token [REMOVE]. For example:Positive: Go forward toward the windows. Exit [BH] [REMOVE] [EH] to living room.Negative: Go forward toward the windows. Exit [BH] exercise room [EH] to living room.Thus, instead of using two models as in HEAR, we can use this single model to score any correction,including deletion corrections. Concretely, with this model, we simply set the score function R(x) =1 P(y = 1 | x) where P(y = 1 | x) is the probability output by the model. The training data of thismodel contain 216,323 pairs of positive and negative examples.",
  "A.3Hyperparameters and Tools": "The hyperparameters and computation cost of the HEARs two models are listed in (they have thesame architecture and are trained in the same way). Other baseline models (A.2) also have the samehyperparameters. We implement our models with Pytorch 1.7.1, Huggingface Transformers 4.5.1, NLTK3.6.7, and use SciPy 1.6.0 for our result analyses.",
  "A.5Human Evaluation": "shows the user interface of the HEAR and the Oracle systems. presents the interfaceof the HEAR (no suggestion) and Oracle (no suggestion) systems. is the interface of Nocommunication. The interfaces are adapted from Zhao et al. (2023a) with the MIT License and Pangea3 with the Apache License v2.0. Before starting a task, we provide the user with a video instruction thatshows them how to use the interface (). After they complete the task, we record their route, thenumber of times they click on the Check button, and their subjective ratings. User participants must be atleast 18 years old and speak English. The intended use of the system is first explained to them, and if theyconsent to perform the task, then they will be taken to the interface.This study has been approved by the Institutional Review Board (IRB). For data anonymization, weremoved the only PII information, the Amazon Mechanical Turk ID, after collecting the data. Thisinformation will also be removed in the future dataset release and replaced with serial numbers that donot reveal the identities of the participants. The dataset will be released under MIT license terms that arecompatible with those of the tools used to create it and will be intended for research usage. We do notidentify any potential risk to participants or the general public in releasing our dataset."
}