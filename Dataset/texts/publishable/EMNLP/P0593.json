{
  "Abstract": "Identifying important neurons for final predic-tions is essential for understanding the mecha-nisms of large language models. Due to com-putational constraints, current attribution tech-niques struggle to operate at neuron level. Inthis paper, we propose a static method forpinpointing significant neurons.Comparedto seven other methods, our approach demon-strates superior performance across three met-rics. Additionally, since most static methodstypically only identify \"value neurons\" directlycontributing to the final prediction, we proposea method for identifying \"query neurons\" whichactivate these \"value neurons\". Finally, we ap-ply our methods to analyze six types of knowl-edge across both attention and feed-forwardnetwork (FFN) layers. Our method and anal-ysis are helpful for understanding the mecha-nisms of knowledge storage and set the stagefor future research in knowledge editing. Thecode is available on",
  "Introduction": "Transformer-based large language models (LLMs)(Brown et al., 2020; Ouyang et al., 2022; Chowd-hery et al., 2023) possess remarkable capabilitiesfor storing factual knowledge, which is importantfor downstream tasks including question answer-ing (Jiang et al., 2021) and reasoning (Rajani et al.,2019). While recent studies (Dai et al., 2021; Menget al., 2022; Geva et al., 2023; Yu et al., 2023; Chenet al., 2024) have made significant progress in un-derstanding knowledge localization and the infor-mation flow from inputs to predictions, it is stillhard to identify exact parameters for knowledgestorage in LLMs due to several reasons. Firstly, ex-isting studies often depend on causal tracing (Pearl,2001; Vig et al., 2020) and integrated gradients(Sundararajan et al., 2017) for knowledge attribu-tion. However, many studies (Stolfo et al., 2023;Zhao et al., 2024; Wu et al., 2024a) point out that the computational complexity of forward and back-ward operations in these methods restricts their ap-plicability to millions of neurons in LLMs, whichare proved as fundamental units for storing knowl-edge (Geva et al., 2020; Dai et al., 2021; Geva et al.,2022; Nanda et al., 2023b). Secondly, while a fewstudies (Dar et al., 2022; Geva et al., 2022) havedevised methods for analyzing neurons, they oftenlack comparisons with other methods. Therefore,how to identify important neurons in LLMs is stillunclear. Lastly, existing methods typically concen-trate on either attention or feed-forward network(FFN) module, often lacking evaluation of the othermodule. It is crucial to quantitatively compare theimportance of both attention and FFN layers.",
  ": (a) Query neurons in shallow FFN layers. (b)Attention query/value neurons in attention heads. (c)Value neurons in deep FFN layers": "In this paper, we focus on neuron-level attribu-tion methods. We analyze the distribution changecaused by each neuron and discover that both theneurons coefficient score and the final predictionsranking, when projecting this neurons subvalueinto vocabulary space, play significant roles. Based on this finding, we employ log probability increaseas importance score, enabling the identification ofneurons that contribute significantly to final pre-dictions. Compared with seven other static meth-ods, our proposed method achieves the best per-formance on three metrics. Furthermore, since theidentified neurons directly contribute to the finalpredictions probability, we also develop a staticmethod to identify \"query neurons\" that aid in ac-tivating these \"value neurons\". Specifically, wecalculate the inner products between the query neu-rons and value neurons as importance scores.Based on our proposed methods, we analyze sixtypes of knowledge in both attention and FFN lay-ers, yielding numerous valuable insights ():1) Both attention and FFN layers can store knowl-edge, and all important neurons directly contributeto knowledge prediction are in deep layers. 2) Inattention layers, knowledge with similar semantics(e.g. language, country, city) tends to be stored inthe same heads. Knowledge with distinct semantics(e.g. country, color) is stored in different heads. 3)While numerous neurons contribute to the final pre-diction, intervening on a few value neurons (300)or query neurons (1000) can significantly influ-ence the final prediction. 4) FFN value neurons aremainly activated by medium-deep attention valueneurons, while these attention neurons are mainlyactivated by shallow/medium FFN query neurons.Overall, our contributions are as follows:a) We design a static method for neuron-levelknowledge attribution in large language models.Compared with seven static methods, our methodachieves the best performance under three metrics.b) As the identified neurons usually directly con-tribute to the final predictions, we design a staticmethod to identify the \"query neurons\" activatingthese \"value neurons\".c) We analyze the localization of six types ofknowledge in both attention and FFN layers. Ouranalysis is helpful for understanding the mecha-nisms of knowledge storage in language models.",
  "Attribution Methods for Transformers": "Determining how to attribute the important pa-rameters for final predictions is a crucial ques-tion. Gradient-based methods (Sundararajan et al.,2017; Kindermans et al., 2019; Miglani et al., 2020;Lundstrom et al., 2022) and causal tracing methods(Pearl, 2001; Vig et al., 2020; Meng et al., 2022; Goldowsky-Dill et al., 2023; Zhang and Nanda,2023; Wu et al., 2024b; Hase et al., 2024) arewidely utilized for this purpose. The core ideais calculating how much an internal module affectsthe final predictions, requiring multiple forwardand/or backward operations (Wu et al., 2024a).Due to the computational overhead, these meth-ods are usually applied on hidden states (Menget al., 2022; Geva et al., 2023; Stolfo et al., 2023),rather than neurons. Another type of studies tendto require only one forward pass for each sentence,typically relying on saliency scores such as atten-tion weights (Vig, 2019; Jaunet et al., 2021; Yehet al., 2023; Wang et al., 2023; Li et al., 2023) andFFN neurons coefficient scores (Geva et al., 2022;Lee et al., 2024). However, the validity of attribu-tions is challenged by many studies (Serrano andSmith, 2019; Jain and Wallace, 2019; Wiegreffeand Pinter, 2019; Mohankumar et al., 2020; Etha-yarajh and Jurafsky, 2021; Bai et al., 2021). Lackof evaluation methods results in an ongoing debateabout the faithfulness of saliency score methods.",
  "Mechanistic Interpretability": "Mechanistic interpretility (Olah, 2022; Nanda et al.,2023a) aims to reverse engineer the circuits frominputs to the final prediction. An essential tech-nology involves projecting internal vectors into thevocabulary space, where numerous studies have dis-covered interpretable results (Nostalgebraist, 2020;Geva et al., 2020, 2022; Dar et al., 2022; Pal et al.,2023). Most studies focus on analyzing the atten-tion heads roles in different cases and tasks (El-hage et al., 2021; Olsson et al., 2022; Wang et al.,2022; Hanna et al., 2023; Lieberum et al., 2023;Conmy et al., 2023; Gould et al., 2023). Also,superposition (Elhage et al., 2022; Nanda et al.,2023b; Gurnee et al., 2023) and dictionary learning(Bricken et al., 2023; He et al., 2024) are importantfor understanding neurons in transformers.",
  "Methodology": "In this section, we aim to locate important neuronsfor specific predictions. We introduce the back-ground in .1, and analyze the distributionchange caused by neurons in .2. Based onthe analysis, we introduce our proposed method forlocating the \"value neurons\" that contribute to thefinal predictions directly in .3, and pro-pose a static method to locate the \"query neurons\"that activate these \"value neurons\" in .4.",
  "Background": "First, we introduce the inference pass from inputsto the final prediction. Given an input sentenceX = [t1, t2, ..., tT ] with T tokens, the model gen-erates the next tokens probability distribution yover B tokens in vocabulary V . The embeddingmatrix E RBd transforms each ti at position iinto a word embedding h0i Rd. Then the wordembeddings are transformed by L + 1 transformerlayers (0th Lth), each has a multi-head self-attention layer (MHSA) and a FFN layer. The layeroutput hli (position i, layer l) is the sum of the layerinput hl1i(previous layers output), the attentionoutput Ali, and the FFN output F li :",
  "mli,k = (fc1lk (hl1i+ Ali))(6)": "The FFN output F li is computed by a weighted sumof fc2 vectors. fc2lk is the kth column of W lfc2(named FFN subvalue), and its coefficient scoremli,k is computed by non-linear on the inner prod-uct between the residual output hl1i+Ali and fc1lk(named FFN subkey), the kth row of W lfc1. Simi-larly, the attention output Ali can be represented asa sum of head outputs, each being a weighted sumof value-output vectors on all positions:",
  "li,j,p = softmax(W qj,lhl1i W kj,lhl1p)(8)": "where W qj,l, W kj,l, W vj,l, W oj,l Rdd/H are thequery, key, value and output matrices of the jthhead in the lth layer. The query and key matricescompute the attention weight li,j,p on the pth posi-tion, then calculate the softmax function across allpositions. The value and output matrices transformthe pth position input vector into the pth value-output vector. Each head output is the weightedsum of value-output vectors on all positions. Definition of \"neuron\".As discussed in Eq.5-6, the kth FFN neuron is the kth subvalue fc2lk,whose coefficient score mli,k is calculated by itscorresponding subkey fc1lk. In Eq.7, each attentionoutput can be represented as a direct addition ofT H vectors, when taking the position value-output vector W oj,l(W vj,lhl1p) as fundamental units.Moreover, the position value-output vector can alsobe regarded as a weighted sum of attention neurons.Similar to the definition of FFN neurons, we regardthe kth column of W oj,l in Eq.7 as the kth attentionsubvalue in this head, whose subkey is the kthrow of W vj,l. When taking the attention neurons asfundamental units, the final output is the sum ofL (T H d/H + N) + 1 vectors.",
  "Distribution Change Caused by Neurons": "The final vector hLT has important information forpredicting the final token. As it is computed bya direct sum of various neuron-level vectors, therelevant information for making the final predictionmust be stored in one or many neurons. The finalvector hlT can be regarded as the sum of one neuronv and another vector x = hlT v. We consider theprobability change p(w|x+v)p(w|x) caused byv for prediction token w. We aim to explore whichcomponents of v are significant for amplifying theprobability change. This allows us to develop staticmethods for locating crucial neurons.As the probability change is nonlinear, analyzingthe exact contribution of neuron v is challenging.For a more concise analysis, we term the score ewxvector xs bs-value (before-softmax value) on tokenw, where ew is the wth row of the unembeddedmatrix Eu. A tokens bs-value directly correspondsto the probability of this token. Bs-values of allvocabulary tokens on vector x are:",
  "bs(x + v) = bs(x) + bs(v)(12)": "Although the probability change is nonlinear,the change on each tokens bs-value is linear. Inorder to analyze which components of v is im-portant, we design several bs(x) and bs(v) andcompute the distribution change. Assume thereare four tokens in vocabulary space, and bs(x) =.The probability distribution of x is[0.03, 0.09, 0.24, 0.64]. We design several v andcompute the probability distribution of p(x + v).The details are shown in .",
  "Existing studies (Geva et al., 2022; Lee et al.,": "2024) state that p(w|x + v) exp(ew v). How-ever, based on the examples provided in ,not only the bs-value of token w, but also the bs-values of all the tokens affect the probability. Forexample, bs(v) = and resultin the same distribution, although the bs-value ofeach token is enlarged.An intuitive observation is that v aids in mag-nifying the token with the largest bs-value. Forinstance, is conducive to increasing theprobability of the last token, and canamplify the probability of the first token. This ob-servation may elucidate why many neurons exhibithuman-interpretable concepts when projected intothe vocabulary space. Given that the vocabularysize B is typically large (often exceeding 30,000),probabilities of tokens with the largest bs-valuesare likely to be augmented. Another significant finding is that both the coef-ficient score and the neurons bs-values play sub-stantial roles. Compared with , can can both amplify and diminish the probabilitychange of . The probability increase ofthe first token is magnified, while the decrease inprobability of the last token is more pronounced.When the coefficient scores sign is changed (e.g.), the effect on the first tokens proba-bility changes from increasing to decreasing.",
  "Importance Score for \"Value Neurons\"": "Based on the analysis in .2, an intu-itive importance score of a neuron mv is |m| |1/rank(w)|, where m is the coefficient score andrank(w) denotes the ranking of the final tokenwhen projecting v into vocabulary space. Anotherintuitive importance score is calculating the prob-ability p(w|mv) on token w. If these scores arelarge, v will contain much information of w.However, these methods have two potential prob-lems. On one hand, they only consider the ef-fect of v, overlooking the varying importanceof v under different x conditions. On the otherhand, we usually hope to analyze the importanceof different modules combination. Therefore, itis better that the importance score Imp satisfiesImp(x + v) Imp(x) + Imp(v).To address these problems, we design log proba-bility increase as importance score for both layer-level and neuron-level vectors. If vl is a vector inlth attention layer, the importance score of vl is: Imp(vl) = log(p(w|vl+hl1))log(p(w|hl1))(13)where the probability of each vector is computedby multiplying the vector with Eu (see Eq.2). Ifvl is a vector in lth FFN layer, we compute theimportance score by replacing hl1 as hl1 +Al inEq.13. In Eq.13, vl is not the only element control-ling the importance score. Also, it is convenient foranalyzing the combination of different modules.",
  "Importance Score for \"Query Neurons\"": "As discussed in .3, the proposed attribu-tion methods can effectively identify the \"valueneurons\" containing crucial information for the fi-nal prediction. However, in addition to these \"valueneurons\", there exist \"query neurons\" that aid inactivating these neurons, even if they may not di-rectly contain information about w. In this section,we propose a static method to identify these \"query neurons\" based on Eq.1, Eq.5, and Eq.6. Since thefc2 vectors do not change, the coefficient scoresare the only varying element in different cases. Foreach \"value neuron\", we can compute the innerproduct between its subkey (see Eq.6) and eachneuron/subvector within the residual output (seeEq.1). Despite the presence of a nonlinear function for computing the coefficient score, it usuallydoes not affect the relative value between differentneurons/subvectors. Therefore, if a \"query\" neu-ron/subvector exhibits a larger inner product withthe subkey compared to another one, it is morehelpful for activating the \"value neuron\".",
  "We compare the proposed method in Eq.13 withseven other methods. For each sentence, we applyevery method to identify top10 FFN neurons, andevaluate the attributed neurons using three metrics": "Dataset.We extract query-answer pairs with sixtypes of answer tokens (language, capital, country,color, number, month) from TriviaQA (Joshi et al.,2017). To explore the mechanism of knowledgestorage, we extract all the sentences where the cor-rect token ranks within the top10 predictions andhigher than other words in the same knowledge inGPT2-large (Radford et al., 2019) and Llama-7B(Touvron et al., 2023). There are 1,350 sentencesfor GPT2-large and 3,141 sentences for Llama-7B. Models.To compare the differences betweenlarge and small models in terms of knowledge stor-age, we conduct experiments on Llama-7B andGPT2-large. Llama-7B consists of 32 layers, witheach attention layer comprising 32 heads, eachhead containing 128 neurons and each FFN layercontaining 11,008 neurons. GPT2-large has 36 lay-ers with 20 heads per attention layer, 64 neuronsper head, and 5,120 neurons per FFN layer. Attribution methods.We compare our methodwith seven static methods. We use each method toattribute the FFN neurons with top10 scores for thecorrect knowledge token w. Similar to Eq.5, eachneuron mv is the product of the coefficient scorem and fc2 vector vl. Here are the methods: a) (proposed method) log probability increase:log(p(w|mvl+Al+hl1))log(p(w|Al+hl1))b) log probability: log(p(w|mvl)), attributing thesame neurons with p(w|mvl). This is similar todirect logit attribution (DLA) in Wang et al. (2022).c) probability increase: p(w|mvl + Al + hl1) p(w|Al + hl1)d) norm: |vl|e) coefficient score: |m|f) ranking in vocabulary space: 1/rank(w)g) |m| |vl|, introduced in Geva et al. (2022).h) |m| 1/rank(w) Metrics.We devise three metrics to evaluate theattribution methods. After attributing the top10FFN neurons by each method, we intervene onthese neurons by setting the top10 neurons param-eters to zero. Subsequently, we rerun the modeland compute the Mean Reciprocal Rank (MRR)score of the correct token w, the probability of w(prob), and the log probability of w (logp). Anattribution method is considered superior when itexhibits greater decreases in these metrics.",
  ": Results of attribution methods on two models": "Results and analysis.The results of the origi-nal model (first line) and eight attribution methodsare shown in . In comparison with the otherseven methods, our attribution method (second line)attributes more important neurons, resulting in themost significant reduction across all metrics in bothGPT2 and Llama. Specifically, when only interven-ing ten FFN neurons, the probability of the cor-rect knowledge token reduces from 7.1% to 3.4%in GPT2, and from 21.5% to 9.2% in Llama-7B.This indicates that there are several neurons storingmuch important information for knowledge storage,and our method can locate these neurons.The attribution methods of norm vl (d) andm|vl| (g) are not useful, which indicates the norm of neurons is not important for attribution. Using|m| 1/rank(w) (h) has good results, which isbetter than 1/rank(w) (f). The ranking of tokensin vocabulary space for projected neurons is a goodsaliency score, and the coefficient score can en-large the distribution change, aligning our analysisin .2. Only using coefficient score (e) isnot helpful for attribution. The role of coefficientscore is to enhance the probability change causedby the neuron, but whether the neuron is usefulfor the selected token depends on the neuron it-self. There are other tokens competing with thecorrect knowledge token, so the neurons with largecoefficient scores may be related to these tokens.",
  ": Neuron distribution on all layers in Llama-7B": "Compared to log probability (b), employing logprobability increase (a) can attribute more impor-tant neurons. This aligns with the analysis in Sec-tion 3.2 and 3.3: not only neuron v, but also xaffects p(w|x+v)p(w|x). Compared with proba-bility increase (c), log probability increase achievesbetter results. We analyze the distribution of neu-rons across all layers in Llama attributed by logprobability increase, log probability, and probabil-ity increase, as depicted in . GPT2 has sim-ilar results, detailed in Appendix A. The neuronsattributed by probability increase are on deepestlayers (23th 31th), while other two methods canattribute neurons among 17th to 31th layers.To delve into the reason of this phenomenon,we analyze the difference of importance scorewhen adding the same vector v on different x.As discussed in Eq.13, the importance score ofv is computed by log(p(w|x + v)) log(p(w|x)).Therefore, the importance score is related to thecurve of F(a) = log(p(w|a)). To analyze thiscurve, we compute the final vector hLT and the 0thlayer input vector h0T on each sentence, and dividehLT h0T into 61 segments, where each segment isSegS = h0T + S(hLT h0T )/60 (S is the segment",
  ": Curves of log probability increase (left) andprobability increase (right) on Llama-7B": "The curve of log probability increase exhibits anapproximately linear shape from 0 to 40 segments,while the curve of probability increase shows a lin-ear trend from 40 to 60 segments. This observationelucidates the findings in : employing prob-ability increase is more inclined to attribute neuronsin the deepest layers, whereas log probability in-crease tends to attribute neurons in medium-deeplayers. Despite the slower slope of the log proba-bility increase curve in very deep layers, it still ef-fectively attributes neurons in very deep layers (asdepicted in ). This maybe because neuronsin very deep layers contain substantial information,and even when the importance score decreases, itremains relatively large. In later sections, we uselog probability increase as importance score forexploration, as this method can identify the impor-tant neurons in both medium-deep layers and verydeep layers, and its experimental results are thebest. Nevertheless, reproducing the importance ofthe deepest layers may be a prospective avenue fordeveloping improved attribution methods.",
  ": Contribution of attention and FFN layers": "layer for each knowledge in , where al andfl means lth attention and FFN layer. For a clearerdisplay, we illustrate the importance (darker colormeans larger importance) of top10 layers in (GPT2) and (Llama).Both attention and FFN layers have ability tostore knowledge, and all the top10 layers are indeep layers. Information with analogous semantics(e.g., language, capital, country) tends to be storedwithin similar layers/modules. For instance, a26,a30, a28, and a22 in GPT2 ranks top for language,capital and country, and a23 in Llama-7B ranksthe first for these knowledge. Data with dissimilarsemantics (e.g., language, color, month) typicallyresides in distinct layers/modules.",
  "top10 important layers": "langa26, a30, a32, a22, a31, a28, a23, a27, a19, a23capia26, a28, a30, a25, a22, f26, f28, a19, f27, f30cntya26, a30, a28, a22, f29, a31, f26, a32, a25, a19cola32, f32, a33, f29, f31, a31, a26, f33, f28, a22numf29, f23, f27, f30, f31, f26, f32, a23, a22, f28mona27, a26, f26, a25, f30, a28, a24, a22, a30, f27 langa23, a21, f21, a19, a18, a31, a25, a16, f20, f19capia23, f21, f22, a18, a25, a21, f19, f20, a16, f24cntya23, a21, a25, f22, a18, a19, a16, f21, f31, a31colf29, a20, f22, f20, a19, a28, a16, a29, a18, f28numf31, f26, f29, f27, a26, f23, f24, a28, f17, f30mona21, a19, f19, a16, f31, a23, a28, f30, f17, f18",
  ": Top10 important \"value layers\" in Llama": "and a2525 rank top5 for these knowledge in Llama.To evaluate how much knowledge the top headsstore, we intervene the top 1% heads (top7 in GPT2and top10 in Llama) by setting the heads parame-ters to zero. Intervening each knowledges heads re-sult in a MRR/probability decrease of 44.5%/53.3%in GPT2, and 32.8%/48.2% in Llama (shown inAppendix B). But semantic-unrelated knowledgeonly reduce 7.1%/9.5% in GPT2 and 3.8%/8.7%in Llama. Therefore, the identified \"knowledgeheads\" contain much semantic-related knowledge.",
  ": Importance of top neurons in attention (firstblock) and FFN (second block) layers in Llama-7B": "Neuron-level knowledge storage.For attentionand FFN layers in Llama, we compute the sumof importance score for all neurons, all positiveneurons (score larger than 0), top100 neurons, andtop200 neurons, as illustrated in . Similarresults of GPT2 is shown in Appendix C.In both models, the sum score of top200 neuronsin attention layers and top100 neurons in FFN lay-ers are similar to that of all neurons. Additionally,we intervene the top neurons to evaluate how muchfinal predictions are affected, detailed in AppendixC. When intervening the top200 attention neuronsand top100 FFN neurons for each sentence, theMRR and probability decreases 96.3%/99.2% inGPT2, and 96.9%/99.6% in Llama. In comparison,randomly intervening the same number of neuronsonly decreases 0.22%/0.14%. Hence, even thoughthere are many neurons contribute to the final pre-diction, intervening a few neurons (300) affects thefinal prediction much. This conclusion holds signif-icance for future studies delving into neuron-levelknowledge editing.",
  "top10 query layers for FFN neurons": "langa26, a22, a19, f26, a17, f25, f27, f23, a25, a23capia26, a22, a19, a17, f25, a23, a18, f26, f21, a28cntya26, a22, a17, a19, a23, f18, a20, a18, f21, f25colf26, f29, a26, f28, f25, a22, f27, a17, a24, f23numf26, f23, f27, a22, f25, a17, f19, f21, a23, f0mona17, a22, f23, a26, f26, f24, a19, a20, f20, f21 langf21, a16, a19, f18, a18, a21, a17, f30, f19, a14capia18, a16, f23, a19, f17, a14, f22, a21, f26, f19cntya16, a18, a21, a19, f21, a14, f19, a17, a31, f20colf20, f21, a15, a17, a18, a20, f19, f22, f17, a16numf19, f21, f22, f16, a18, a22, a24, f14, a12, a25mona16, a19, f18, a21, a17, a14, f29, f19, f17, a18",
  ": Top10 important \"query layers\" in GPT2": "Important query layers for FFN value neurons.The \"value\" FFN neurons are activated by last po-sitions residual stream. We evaluate which \"querylayers\" activate the top100 FFN neurons, shownin . We also illustrate the importance oftop10 important \"query layers\" in (GPT2)and (Llama). The medium-deep atten-tion layers play large rules. Compared -7 with -5, we find that several attention\"query layers\" also contribute to final predictions(e.g. a19, a22, a26 in GPT2 and a16, a18, a19, a21in Llama for country/capital/language).Thesemedium-deep attention layers neurons are veryimportant, working as both \"value\" and \"query\". Important query neurons for attention valueneurons.We compute the important query lay-ers activating the top200 \"value\" attention neurons.finding the shallow and medium FFN layers playmain roles (detailed in Appendix D). To identifythe important query FFN neurons, we weightedsum the inner product between each attention neu-rons subkey and each FFN neuron on every po-sitions residual stream, as query FFN neuronsscores. When intervening top1000 shallow neurons",
  ": Query neuron distribution in GPT2 and Llama": "Then we count the number of queryvalue (bothin top1000 query and top1000 value) and queryonly(only in top1000 query) FFN neurons, shown in. In both models, the number of queryonlyneurons, which is much larger than that of query-value neurons, starts to drop at 60% layer. Thisobservation indicates that the shallow and mediumFFN neurons are important for activating the at-tention \"value neurons\". A difference is that thevery shallow FFN layers play large roles in GPT2,and we defer this exploration to future research.Overall, our analysis learns the information flow atneuron level: features in shallow/medium FFN neu-rons are extracted, then activate the deep attentionand FFN neurons related to final predictions.",
  "Shared Value and query neurons in each knowl-edge.We compute how many \"shared\" query neu-rons and value neurons rank top300 in more than": "50% sentences in each knowledge, shown in Ta-ble 8. On average, there are 27.6% shared valueneurons in GPT2 and 14.1% in Llama. Query neu-rons, with 15.7% shared neurons in GPT2 and 5.2%in Llama, exhibit a more dispersed distributionthan value neurons. To explore the neurons inter-pretability, we project them into vocabulary space.We find most value neurons (first block in )are related to predicted tokens. However, we donot observe much interpretability in query neurons.We only find a few query neurons (second blockin ) related to the final words. Hence, toexplore the interpretability of query neurons maybe a valuable direction in future works.",
  "Limitations": "The first limitation of our study is that it focuses onsix specific types of knowledge, while other typesof knowledge are also important. Secondly, ourexperiments are conducted using GPT2-large andLlama-7B models. It is essential to compare thesimilarities and differences in knowledge storageacross different models. Lastly, our study employsstatic methods for neuron-level knowledge attribu-tion. Although our experiments demonstrate thecorrectness and robustness of our designed method,it is also important to compare static methods withother attribution methods, such as causal mediationanalysis and gradient-based methods. We plan toexplore these areas in future work. A potential risk of our work is that people canutilize our method to identify important neuronsand edit them to change the models outputs. Forinstance, if they identify the toxicity neurons andgender bias neurons and increase these neuronscoefficient scores, the model will be more likelyto generate toxicity and gender bias words. Butthis potential risk depends on how people utilizeour method. Our method can be utilized for reduc-ing hallucinations, toxicity, and bias in LLMs byidentifying and intervening/editing these neurons.",
  "Acknowledgements": "We thank Kailai Yang, Zhiwei Liu, and John Mc-Naught for helpful feedbacks and constructive sug-gestions. This work is supported by the projectJPNP20006 from New Energy and Industrial Tech-nology Development Organization (NEDO). Thiswork is supported by the computational shared fa-cility and the studentship from the Department ofComputer Science at the University of Manchester. Bing Bai, Jian Liang, Guanhua Zhang, Hao Li, KunBai, and Fei Wang. 2021. Why attentions may notbe interpretable? In Proceedings of the 27th ACMSIGKDD conference on knowledge discovery & datamining, pages 2534. Trenton Bricken, Adly Templeton, Joshua Batson,Brian Chen, Adam Jermyn, Tom Conerly, NickTurner, Cem Anil, Carson Denison, Amanda Askell,Robert Lasenby, Yifan Wu, Shauna Kravec, NicholasSchiefer, Tim Maxwell, Nicholas Joseph, ZacHatfield-Dodds, Alex Tamkin, Karina Nguyen,Brayden McLean, Josiah E Burke, Tristan Hume,Shan Carter, Tom Henighan, and ChristopherOlah. 2023.Towards monosemanticity: Decom-posing language models with dictionary learning.Transformer Circuits Thread. Https://transformer-circuits.pub/2023/monosemantic-features/index.html. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shotlearners. Advances in neural information processingsystems, 33:18771901. Yuheng Chen, Pengfei Cao, Yubo Chen, Kang Liu, andJun Zhao. 2024. Journey to the center of the knowl-edge neurons: Discoveries of language-independentknowledge neurons and degenerate knowledge neu-rons. In Proceedings of the AAAI Conference on Ar-tificial Intelligence, volume 38, pages 1781717825. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts, PaulBarham, Hyung Won Chung, Charles Sutton, Sebas-tian Gehrmann, et al. 2023. Palm: Scaling languagemodeling with pathways. Journal of Machine Learn-ing Research, 24(240):1113. Arthur Conmy, Augustine Mavor-Parker, Aengus Lynch,Stefan Heimersheim, and Adri Garriga-Alonso.2023. Towards automated circuit discovery for mech-anistic interpretability. Advances in Neural Informa-tion Processing Systems, 36:1631816352.",
  "Guy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant.2022. Analyzing transformers in embedding space.arXiv preprint arXiv:2209.02535": "Nelson Elhage, Tristan Hume, Catherine Olsson,Nicholas Schiefer, Tom Henighan, Shauna Kravec,Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain,Carol Chen, et al. 2022. Toy models of superposition.arXiv preprint arXiv:2209.10652. Nelson Elhage, Neel Nanda, Catherine Olsson, TomHenighan, Nicholas Joseph, Ben Mann, AmandaAskell, Yuntao Bai, Anna Chen, Tom Conerly,Nova DasSarma, Dawn Drain, Deep Ganguli, ZacHatfield-Dodds, Danny Hernandez, Andy Jones,Jackson Kernion, Liane Lovitt, Kamal Ndousse,Dario Amodei, Tom Brown, Jack Clark, Jared Ka-plan, Sam McCandlish, and Chris Olah. 2021. Amathematical framework for transformer circuits.Transformer Circuits Thread. Https://transformer-circuits.pub/2021/framework/index.html.",
  "Rhys Gould, Euan Ong, George Ogden, and ArthurConmy. 2023. Successor heads: Recurring, inter-pretable attention heads in the wild. arXiv preprintarXiv:2312.09230": "Wes Gurnee, Neel Nanda, Matthew Pauly, Kather-ine Harvey, Dmitrii Troitskii, and Dimitris Bert-simas. 2023.Finding neurons in a haystack:Case studies with sparse probing. arXiv preprintarXiv:2305.01610. Michael Hanna, Ollie Liu, and Alexandre Variengien.2023. How does gpt-2 compute greater-than?: In-terpreting mathematical abilities in a pre-trained lan-guage model. In Thirty-seventh Conference on Neu-ral Information Processing Systems. Peter Hase, Mohit Bansal, Been Kim, and Asma Ghan-deharioun. 2024. Does localization inform editing?surprising differences in causality-based localizationvs. knowledge editing in language models. Advancesin Neural Information Processing Systems, 36. Zhengfu He, Xuyang Ge, Qiong Tang, Tianxiang Sun,Qinyuan Cheng, and Xipeng Qiu. 2024.Dictio-nary learning improves patch-free circuit discovery inmechanistic interpretability: A case study on othello-gpt. arXiv preprint arXiv:2402.12201.",
  "Sarthak Jain and Byron C Wallace. 2019. Attention isnot explanation. arXiv preprint arXiv:1902.10186": "Theo Jaunet, Corentin Kervadec, Romain Vuillemot,Grigory Antipov, Moez Baccouche, and ChristianWolf. 2021. Visqa: X-raying vision and languagereasoning in transformers. IEEE Transactions on Vi-sualization and Computer Graphics, 28(1):976986. Zhengbao Jiang, Jun Araki, Haibo Ding, and GrahamNeubig. 2021. How can we know when languagemodels know? on the calibration of language modelsfor question answering. Transactions of the Associa-tion for Computational Linguistics, 9:962977.",
  "Mandar Joshi, Eunsol Choi, Daniel S Weld, and LukeZettlemoyer. 2017. Triviaqa: A large scale distantlysupervised challenge dataset for reading comprehen-sion. arXiv preprint arXiv:1705.03551": "Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo,Maximilian Alber, Kristof T Schtt, Sven Dhne,Dumitru Erhan, and Been Kim. 2019. The (un) relia-bility of saliency methods. Explainable AI: Interpret-ing, explaining and visualizing deep learning, pages267280. Andrew Lee, Xiaoyan Bai, Itamar Pres, Martin Watten-berg, Jonathan K Kummerfeld, and Rada Mihalcea.2024. A mechanistic understanding of alignment al-gorithms: A case study on dpo and toxicity. arXivpreprint arXiv:2401.01967.",
  "Zongxia Li, Paiheng Xu, Fuxiao Liu, and Hyemi Song.2023. Towards understanding in-context learningwith contrastive demonstrations and saliency maps.arXiv preprint arXiv:2307.05052": "Tom Lieberum, Matthew Rahtz, Jnos Kramr, GeoffreyIrving, Rohin Shah, and Vladimir Mikulik. 2023.Does circuit analysis interpretability scale? evidencefrom multiple choice capabilities in chinchilla. arXivpreprint arXiv:2307.09458. Daniel D Lundstrom, Tianjian Huang, and MeisamRazaviyayn. 2022. A rigorous study of integratedgradients method and extensions to internal neuronattributions. In International Conference on MachineLearning, pages 1448514508. PMLR.",
  "Chris Olah. 2022. Mechanistic interpretability, vari-ables, and the importance of interpretable bases. InTransformer Circuits Thread": "Catherine Olsson, Nelson Elhage, Neel Nanda, NicholasJoseph, Nova DasSarma, Tom Henighan, Ben Mann,Amanda Askell, Yuntao Bai, Anna Chen, et al. 2022.In-context learning and induction heads.arXivpreprint arXiv:2209.11895. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al.2022. Training language models to follow instruc-tions with human feedback. Advances in neural in-formation processing systems, 35:2773027744.",
  "Jesse Vig. 2019. Bertviz: A tool for visualizing mul-tihead self-attention in the bert model.In ICLRworkshop: Debugging machine learning models, vol-ume 3": "Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov,Sharon Qian, Daniel Nevo, Yaron Singer, and StuartShieber. 2020. Investigating gender bias in languagemodels using causal mediation analysis. Advancesin neural information processing systems, 33:1238812401. Kevin Wang, Alexandre Variengien, Arthur Conmy,Buck Shlegeris, and Jacob Steinhardt. 2022.In-terpretability in the wild: a circuit for indirect ob-ject identification in gpt-2 small.arXiv preprintarXiv:2211.00593. Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou,Fandong Meng, Jie Zhou, and Xu Sun. 2023. Labelwords are anchors: An information flow perspectivefor understanding in-context learning. arXiv preprintarXiv:2305.14160.",
  "Sarah Wiegreffe and Yuval Pinter. 2019. Attention is notnot explanation. arXiv preprint arXiv:1908.04626": "Xuansheng Wu, Haiyan Zhao, Yaochen Zhu, YuchengShi, Fan Yang, Tianming Liu, Xiaoming Zhai, WenlinYao, Jundong Li, Mengnan Du, et al. 2024a. Usablexai: 10 strategies towards exploiting explainability inthe llm era. arXiv preprint arXiv:2403.08946. Zhengxuan Wu, Atticus Geiger, Thomas Icard, Christo-pher Potts, and Noah Goodman. 2024b. Interpretabil-ity at scale: Identifying causal mechanisms in alpaca.Advances in Neural Information Processing Systems,36. Catherine Yeh, Yida Chen, Aoyu Wu, Cynthia Chen,Fernanda Vigas, and Martin Wattenberg. 2023. At-tentionviz: A global view of transformer attention.IEEE Transactions on Visualization and ComputerGraphics.",
  "typetop10 heads": "langa630, a1726, a726, a1132, a019, a931, a1325, a1722, a1328, a229capia726, a630, a1726, a1722, a1325, a1328, a019, a1019, a229, a1132cntya726, a630, a1722, a1328, a1726, a1132, a019, a1325, a931, a1019cola533, a134, a726, a1924, a1823, a1332, a130, a822, a1432, a228numa1822, a317, a823, a219, a330, a1925, a320, a030, a212, a325mona227, a726, a1125, a1019, a230, a428, a1823, a1717, a133, a317 langa1223, a3119, a2531, a2525, a516, a118, a921, a2229, a1721, a2318capia1223, a2229, a2525, a2531, a3119, a118, a1516, a516, a921, a2318cntya1223, a3119, a2525, a921, a2531, a1516, a118, a516, a2229, a1928cola2229, a1928, a2720, a1516, a2717, a2128, a1425, a2818, a124, a314numa1928, a2426, a1023, a1330, a2921, a2413, a2418, a2229, a2317, a119mona1021, a016, a2221, a1823, a1628, a2019, a631, a119, a314, a1320",
  "top10 query layers for attention neurons": "langf0, f1, a0, f2, f19, f20, f3, f17, f18, f21capif0, f1, a0, f2, f3, f20, f5, f4, f19, f17cntyf0, f1, f19, a0, f18, f2, f3, f21, f20, f17colf0, f1, f2, f23, f20, f21, f22, f24, a0, f3numf0, f18, f1, f19, f22, f16, f21, f2, f12, f20monf0, f1, f19, f2, f9, f22, f10, f21, a0, f18 langf20, f19, a16, f16, f15, f18, f14, f21, f12, f21capif20, f24, f22, f23, f19, a16, a23, f28, f18, f25cntyf18, f21, f19, a18, f22, a14, a16, f17, a21, a19colf15, f18, f20, f16, f19, f13, f17, f22, f24, f14numf24, f17, f19, f23, f22, f20, f18, f2, f25, f21monf14, a16, f19, a19, f18, f20, f13, f17, f15, f22"
}