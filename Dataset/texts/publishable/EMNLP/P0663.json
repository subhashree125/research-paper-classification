{
  "Abstract": "In this paper, we address the data scarcityproblem in automatic data-driven glossing forlow-resource languages by coordinating mul-tiple sources of linguistic expertise. We en-hance models by incorporating both token-level and sentence-level translations, utilizingthe extensive linguistic capabilities of modernLLMs, and incorporating available dictionaryresources. Our enhancements lead to an av-erage absolute improvement of 5%-points inword-level accuracy over the previous stateof the art on a typologically diverse datasetspanning six low-resource languages. The im-provements are particularly noticeable for thelowest-resourced language Gitksan, where weachieve a 10%-point improvement. Further-more, in a simulated ultra-low resource settingfor the same six languages, training on fewerthan 100 glossed sentences, we establish anaverage 10%-point improvement in word-levelaccuracy over the previous state-of-the-art sys-tem.",
  "Introduction": "The extinction rate of languages is alarminglyhigh, with an estimated 90% of the worlds lan-guages at risk of disappearing within the next cen-tury (Krauss, 1992). As speech communities dwin-dle, linguists are urgently prioritizing the docu-mentation of these languages. This is a multi-stepprocess involving: 1. phonetic and orthographictranscription, 2. translation into a so-called matrixlanguage like English or Spanish, which providesa common frame of reference for all annotations,3. morpheme segmentation, and 4. grammaticalannotation (Crowley, 2007). The end-result is rep-resented as Interlinear Glossed Text (IGT) like theGitksan example below (see Appendix A for addi-tional details):",
  "Orthography:Ii hahlalsdiy goohl IBMSegmentation:ii hahlalst-y goo-hl IBMGloss:CCNJ work-1SG.II LOC-CN IBMTranslation:And I worked for IBM": "The traditional manual approach to languagedocumentation, while thorough, is notably labor-intensive. This has spurred the development ofautomated tools leveraging machine learning fortasks such as word segmentation and glossing. Forexample, Moeller and Hulden (2018) train neuralmodels for automatic glossing of Lezgi, a Nakh-Daghestanian language. Their models deliver rea-sonable performance when trained on a small train-ing set of 3,000 glossed tokens of Lezgi text. How-ever, neural models are data-hungry and the smalltraining set prevents the models from reaching theirfull potential. The most straightforward way to im-prove model performance would be to manuallygloss more training data. However, as stated above,manual glossing is a very time-consuming process.Therefore, additional data sources should be con-sidered. : When glossing input such as the French sen-tence Le chien aboie, our system utilizes multiple infor-mation sources: an English sentence-level translation,general linguistic knowledge provided by an LLM anddictionary definitions for the input tokens.",
  "Many recent glossing approaches (Girrbach,": "2023; Moeller and Hulden, 2018) exclusively trainon glossed source language transcripts. However,we often have access to additional helpful knowl-edge sources. One option is to augment the datausing translations of the training examples intothe matrix language.1 These provide an impor-tant source of lexical information because the glossof nouns and verbs can often be found within thetranslation.2 Because translation is a part of thelanguage documentation process, these are oftenreadily available and, thus, represent a quick andcost-effective way to provide an additional sourceof supervision. Our system incorporates transla-tions as an added information source.Unfortunately, the availability of translations forIGT data is necessarily limited simply because thequantity of IGT data itself is limited. As an ad-ditional source of lexical information, our systemincorporates external dictionaries which provideword-level translations of target language lexemesinto the matrix language. This helps the systemgeneralize to words missing from the training data.Recently, powerful pretrained models haveemerged as a viable approach to strengthen andsupplement the training signal for NLP tasks inlow-resource settings (Ogueji et al., 2021; Bhat-tacharjee et al., 2021; Hangya et al., 2022). Ad-vancements in large language models (LLMs) alsopresent new opportunities for enhancing the lan-guage documentation process. Pretrained languagemodels such as BERT (Devlin et al., 2018) andLLMs like GPT-4 (Achiam et al., 2023), trained onbillions of tokens of text, encode extensive lexicaland linguistic knowledge in the matrix language,and their incorporation has improved the bench-marks in many natural language tasks (Zhao et al.,2023; Bommasani et al., 2021; Zhou et al., 2023).We integrate LLMs into our glossing pipeline asa post-correction step through in-context learning.It is worth noting that our approach does not re-quire fine-tuning and is, therefore, appropriate inlow-resource settings where compute capacity islimited.By leveraging three external sources of informa-tion (see ): utterance translations, exter-nal dictionaries and LLMs, our glossing pipeline 1Frequently, the matrix language will be English but canalso be another language like Spanish or Russian.2For the French sentence Le chien aboie, the correct glossof both chien dog and aboyer bark can be found in itsEnglish translation: The dog barks. achieves an average absolute improvement of 5%-points over the previous state-of-the-art on datasetsfrom the SIGMORPHON 2023 Shared Task onInterlinear Glossing (Ginn et al., 2023). In par-ticular, the incorporation of dictionaries leads tosignificant advancements for ultra-low resource lan-guages such as Gitksan, resulting in a 10%-pointsincrease in word-level accuracy. Our key contribu-tions are:1. We enhance the training of glossing systemsinaddition to plain glossed training examples, we in-troduce additional supervision in the form of inputtranslations which are encoded using a pre-trainedlanguage model.2. We utilize external dictionaries which improveglossing performance, particularly for the lowest-resourced languages.3. We pioneer the use of LLM prompting and in-context learning techniques as a post-correctionstep in the glossing pipeline. To our knowledge,this is the first time LLMs have been applied to theautomatic glossing task. Our findings show thatin-context prompting results in substantial improve-ments, especially when very limited training datais available.",
  "Related Work": "Interlinear GlossingResearch into automaticglossing starts with rule-based analysis (Benderet al., 2014; Snoek et al., 2014) followed by data-driven neural models (Moeller and Hulden, 2018;Girrbach, 2023; Ginn and Palmer, 2023; Zhao et al.,2020). More recently, the integration of pre-trainedmultilingual models (Ginn et al., 2024; Sheikhet al., 2024) has shown great potential to aid doc-umentation projects. Our work is inspired by thesuccess of these powerful models and aims to buildupon their strengths. Integrating Translation into the Glossing TaskWe are not unique in incorporating translation in-formation into a glossing system in the presenceof small training datasets. The system presentedby Okabe and Yvon (2023) is based on CRFs (Sut-ton et al., 2012), and also employs translations.However, in contrast to our approach, they heavilyrely on source and target word alignments derivedfrom an unsupervised alignment system (Jalili Sa-bet et al., 2020). In low-resource settings, it is hardto learn an accurate alignment model. 3",
  "Pioneering studies by Zoph and Knight (2016),": "Anastasopoulos and Chiang (2018) and Zhao et al.(2020), show that leveraging translations can en-hance the performance of a neural glossing system.A notable limitation in all of these approaches isthe scarcity of available English translations fortraining models. Therefore, only modest improve-ments in glossing accuracy are observed.Ourwork, in contrast, incorporates translation infor-mation through large pre-trained language models,which leads to greater improvements in glossingperformance. This strategy has lately become in-creasingly popular in low-resource NLP and showspromise across various language processing tasks(Ogueji et al., 2021; Hangya et al., 2022).Similarly to our approach, Okabe and Yvon (2023) also take advantage of the BERT model intheir study, but only utilize BERT representationsfor translation alignment. In contrast, we directlyincorporate encoded translations into our glossingmodel. He et al. (2023) also use pre-trained lan-guage models, namely, XLM-Roberta (Conneauet al., 2020), mT5 (Xue et al., 2021) and ByT5(Xue et al., 2022), as part of their glossing model.However, they do not incorporate IGT translationinformation.4 Instead, they directly fine-tune thepre-trained models for glossing. LLM PromptingIn recent years, the applicationof LLMs for various NLP tasks has expanded sig-nificantly, demonstrating remarkable potential infew-shot and in-context learning. This approachleverages the inherent knowledge and adaptabil-ity of LLMs like GPT-4 (Achiam et al., 2023) andLLaMA-3 (Touvron et al., 2023), allowing them toperform tasks based on a few examples providedas context, without requiring further fine-tuning.Margatina et al. (2023) introduce a novel perspec-tive by applying active learning (AL) principlesto in-context learning with LLMs. Their studyframes the selection of in-context examples as apool-based AL problem conducted over a singleiteration. Various AL algorithms, including uncer-tainty, diversity, and similarity-based sampling, isexplored to identify the most informative examplesfor in-context learning. The findings consistentlyindicate that selecting examples semantically sim-ilar to the test instances significantly outperforms simplifies the glossing task. We instead address the muchharder task of predicting glosses without segmentationinformation.4Though He et al. (2023) do use external dictionary infor-mation for post-correction of glosses.",
  ": 2023 Sigmorphon Shared Task Dataset Infor-mation (Ginn et al., 2023)": "other methods, including random sampling and tra-ditional uncertainty-based approaches .Building on these insights, our proposed workaims to enhance the task of automatic glossing inlow-resource settings by integrating LLM prompt-ing and active learning principles. Our approachapplies the strategies outlined by (Margatina et al.,2023) by focusing on similarity-based methods forselecting in-context examples. This ensures thatthe most relevant and informative examples areutilized, enhancing the models ability to generateaccurate glosses. Additionally, we explore the ef-fectiveness of various active learning methods suchas BERT-similarity, word overlapping, longest com-mon subsequence, and random sampling, tailoringthese approaches to the specific needs of the gloss-ing task.",
  "Data": "We conduct experiments on data from the 2023SIGMORPHON shared task on interlinear gloss-ing (Ginn et al., 2023). The shared task providestwo distinct tracks: an open track, where the inputis morphologically segmented, and a closed track,where no segmentations are provided. Our anal-ysis focuses on data from the closed track. Thissetting is substantially more challenging becausemorphological segmentation now, effectively, be-comes a part of the glossing task. The closed-tracklanguages are Arapaho (arp), Gitksan (git), Lezgi(lez), Natgu (ntu), Tsez (ddo), and Uspanteko(usp).5 Data details are shown as in . Withmost languages, except Arapaho, comprising fewerthan training 10,000 sentences, our datasets can becalled low-resourced. For all languages, the dataincludes translations in a matrix language which isEnglish, except from Uspanteko, where it is Span-ish.",
  "Baseline Model": "Our glossing system is based upon a neural gloss-ing model developed by Girrbach (2023). This isthe winning system of the 2023 SIGMORPHONshared task on internlinear glossing. As shown in, the model accomplishes glossing of mor-phological segments through a three-stage process:input encoding, unsupervised morpheme segmen-tation, and morpheme classification. Input encoderThe model input consists of acharacter-sequence s = s1, ..., sN, representinga sentence. A bidirectional long short-term mem-ory network (BiLSTM) encodes the input into asequence of contextualized embeddings hi, one forevery character in s. Morpheme SegmenterNext, the model per-forms unsupervised morphological segmentationusing the forward-backward algorithm (Kim et al.,2016). In a first step, an MLP is used to predict thenumber of morphemes Jw for each word w in inputsentence s. For each character si, the model appliesa linear layer with Sigmoid activation function toits character encoding hi to get the probability psegithat indicates whether si is the last character of themorpheme segment. Then the forward and back-ward scores ( and , respectively) for each inputposition i and target morpheme j can be computedas follows:",
  "Our code is publicly available:": "the training data, because it treats glossing as a mor-pheme classification task with a closed set of po-tential gloss labels. This deficiency is particularlyharmful when predicting glosses for lexical mor-phemes (i.e. word stems) which represent a muchlarger inventory than grammatical morphemes (i.e.inflectional and derivational affixes). A character-based decoder can enhance the models capabilityto use words from a translation of the input ex-ample. Following Kann and Schtze (2016), weimplemented a LSTM decoder. However, we adaptit to function at the character level for lexical mor-phemes and at the morpheme level for grammaticalmorphemes. 7",
  "Translation Encoder": "We then extend the model of Girrbach (2023) byincorporating matrix-language translations. We en-code the English or Spanish (in the case of Uspan-teko) translations in the shared task datasets usinga deep encoder. We experiment with three differentencoders: a character-based BiLSTM (Hochreiterand Schmidhuber, 1997) and pre-trained transform-ers BERT-base (Kenton and Toutanova, 2019) andT5-large (Raffel et al., 2020).8 To represent trans-lations, we then either use the final hidden statefrom the translation encoder, or attend over thetranslation hidden states.When attending over the hidden states, we applyBahdanau attention (Bahdanau et al., 2014) scoringthe association between each encoder hidden statesand the previous decoder state di1. We separatelyattend to the encoded morpheme representations ejin the input example (morphemes are discovered byour baseline model in an unsupervised manner asexplained above) and the encoded subword-tokenstk in the translation. This gives us a morphemerepresentation ei = Jj=1 wejej and a translation",
  "For instance, if the word gloss is \"dog-FOC\", the decoderwill generate it as \"d-o-g-FOC\".8See Appendix B for details concerning the encoders": "leveraging LLM prompting. We enhance the accu-racy and reliability of glosses through an in-contextlearning approach.For each language, we generate conservative sil-ver glosses (requiring correction) using a BERT-based model with attention (BERT+attn+chr) toprevent excessive corrections, as the baselinemodel (Girrbach, 2023) already provides a reason-ably accurate starting point. We use one-quarter ofthe training data to produce silver glosses for theremaining training data, fine-tuning the model onthe original development split. To reduce noise, weapply an edit distance constraint, retaining exam-ples where the gloss edit distance from the goldgloss is limited to 4-8 characters.9 The initial one-quarter of data is then reintroduced into the trainingset, ensuring completeness and accuracy, as theseglosses match the original training data.Here we prepare a prompt which asks the LLMto correct the lexical morphemes in a glossed inputsentence. A prompt is generated by selecting twotraining examples as in-context learning examplesfor each test example. Each in-context learningexample includes the source language transcript,morpheme/word translations based on the trainingdata, the English translation of the sentence, thesilver gloss, and the gold gloss. Additional wordtranslations will also be added if a bilingual dic-tionary between the target language and matrixlanguage is available. The test example is struc-tured similarly but omits the gold gloss, promptingthe language model to generate the corrected gloss.The prompting pipeline is illustrated in .When using an external dictionary, we additionallyprovide word translations in the prompt. Follow-ing the in-context paradigm, we do not performany further training or fine-tuning of the LLM. Thetemplate used for the prompting is detailed in Ap-pendix F. We experiment with two models in thisscenario: GPT-4 (Achiam et al., 2023) and LLaMA-3 (Touvron et al., 2023).In-context Learning Examples Selection Tech-niques In our experiment, we compare three tech-niques to optimize the selection of in-context learn-ing examples. We evaluate these techniques againstrandom selection. BERT Similarity (BERT-Sim)We first embed the translated test sentence fromthe IGT using BERT (we use multilingual BERTfor Uspanteko). We then find the two training sen-",
  ": The procedure of selecting in-context learning examples to generate components for LLM prompting": "tences with the lowest embedded cosine distancefrom the test case, and use them as our in-contextexamples. Overlapping Words (Overlap) We cal-culate the number of overlapping words betweensource sentences in the test and training datasets.In-context examples are selected to maximize thenumber of overlapping words between the test caseand the training sentences. Longest Common Sub-strings (LCS) We select in-context examples fromthe training sentences that maximize the LCS withthe test case.",
  "Translation Enriched Model Results": "shows the glossing accuracy across dif-ferent model settings and languages.10We re-port performance separately for original sharedtask datasets and our simulated ultra low-resourcedatasets spanning 100 training sentences. We groupthe Gitksan shared task dataset in the ultra low-resource category because it only has 30 trainingexamples.11 Shared Task DataWhen only integrating trans-lations through the final state of a bidirectionalLSTM, we observe an improvement in averageglossing accuracy, but performance is reduced fortwo languages (Arapaho and Uspanteko).Augmenting translations via an attentional mech-anism (LSTM+attn) does not confer consistent im-provements. In contrast, translation informationincorporated via a pre-trained model (BERT+attn)",
  "We additionally present edit distance in Appendix C.11Apart from the baseline, all systems apply majority votingfrom 10 independently trained models. Its impact is discussedin Appendix D": "renders consistent improvements in glossing ac-curacy across all languages and we see notablegains in average glossing accuracy over the base-line. Incorporating a character-based decoder leadsto further improvements in average glossing ac-curacy and for all individual languages. The T5model (T5+attn+chr) attains the highest averageperformance: 82.56%, which represents a 3.97%-points improvement over the baseline.It alsodelivers the highest performance for three outof our five test languages (Arapaho, Lezgi andTsez), while the BERT-based model with attention(BERT+attn+chr) delivers the best performancefor the remaining two (Natgu and Uspanteko).Among all languages, we see improvements overthe baseline model ranging from 2.32%-points to5.95%-points.12 Ultra Low-Resource DataIn order to investi-gate the performance of our model in ultra low-resource settings, we additionally form smallertraining sets by sampling 100 sentences from theoriginal shared task training data. We use the origi-nal shared task development and test sets for vali-dation and testing, respectively.Translations integrated through the final state ofa randomly initialized bidirectional LSTM (LSTMand LSTM+attn), lead to an average 6%-pointsimprovement in accuracy over the baseline. Weachieve particularly impressive gains for Uspan-teko, surpassing the baseline accuracy by over15%-points.Incorporating pre-trained models(BERT+attn) exhibits a slight increase in accuracyfor certain languages. However, when we incor-porate both pre-trained models and the character-based decoder (BERT+attn+chr and T5+attn+chr),we see larger gains in accuracy across the board.",
  "Prompting Model Results": "The prompting experiments aim to further improvethe output of the T5/BERT+attn+chr model by post-correcting its glossed output using an LLM. Weonly allow the LLM to change the gloss of lexi-cal morphemes because preliminary experimentsdemonstrated that post-processing tends to worsenperformance on grammatical morphemes.Theword-level accuracy shown in highlightsthe performance of various training data selectiontechniques across multiple languages.13 We furtherselect the best setting to compare with the baseline",
  "We additionally present lexical morpheme accuracy inAppendix G": "model and translation enriched models. The com-parison demonstrates that using in-context learn-ing continues to boost glossing accuracy. This ap-proach delivers further improvements for Arapaho,Lezgi, Natgu, and Gitksan. It presents the high-est accuracy for Lezgi , showing a 2.33%-pointsincrease over the highest-performing translationenriched model T5/BERT+attn+chr.When applying GPT-4 for post-correction, theOverlapping Words selection technique emergesas the most effective, achieving the highest accu-racy for Arapaho at 81.57% and maintaining strongperformance across other languages. The BERTsimilarity and LCS techniques also provide sub-stantial improvements over random selection, withnotable improvements for Lezgi at 84.70% andNatgu at 86.38% accuracy, respectively. Addition-ally, the LLaMA-3 model using the OverlappingWords method shows competitive results, particu-larly excelling in the low-resource language Gitk-san at 30.11%, indicating its potential utility insuch challenging settings.We further examine Lezgi predictions from theprompting model to better understand how prompt-ing help correcting the glosses. One such exampleincludes a sentence whose translation is \"She waslonely\". The pre-corrected gloss from our encoder-decoder model (T5/BERT+attn+chr) contains in-correct lexical morpheme glosses, including pieand he. It is evident that the prompting modelsuccessfully changed these lexical morphemes ac-cording to the words in the translation line of theIGT14. Results are as shown below:",
  "External Dictionaries": "We also assess the impact of introducing additionalword translations into the in-context prompts toenhance accuracy. We expand the prompt by alsoinjecting word translations from available exter-nal dictionaries for Arapaho, Lezgi, and Gitksan.The sources and detailed information about eachdictionary are shown in Appendix I.The word-level results, as presented in illustrate that the integration of out-of-domain dic-tionary resources is highly beneficial, especiallyfor languages with limited training data like Gitk-san and Lezgi. Dictionary translations consistentlyboost the performance of our best models, enhanc-ing benefits obtained solely through prompting.As shown in an Lezgi example below: The En-glish translation of this sentence is: All this (sic)were real stories. Initially, the prompting modelproduces the nonsense gloss stoply.Addingthe gloss translation allows the model to producestory, which is closer to the meaning of the orig-inal sentence. However, the translation employssome creative license, and dictionary definition iscloser to work. Only after adding the dictionarytranslation does the glossing model generate thecorrect output.",
  "Learning Curves": "The learning curves in illustrate the impactof prompting on model performance when usingvarying amounts of IGT training data. This compar-ison includes models with and without prompting,focusing on both word-level and lexical morphemeaccuracy. We focuse on the Arapaho language,which has the largest number of manually glossedtraining examples: 39,501 training sentences, intotal.",
  ": Lexical morpheme and word-level accu-racy on Arapaho. We incorporate prompting with theencoder-decoder model which is enriched with transla-tion": "The bar chart represents the word-level accu-racy for models trained with varying amounts ofdata (100 sentences, 25% data, 50% data, and100% data). The results clearly demonstrate that in-context post-correction greatly improves glossingaccuracy. In ultra-low data conditions, the post-corrected model is more than twice as accurate asthe uncorrected model. As the amount of trainingdata increases, the benefits gained through prompt-ing diminish.The line chart maps the accuracy of lexicalmorphemes prior- and post-correction. Similarlyto the word-level accuracy, the accuracy of lex-ical morphemes benefits greatly from in-contextpost-correction.The most significant improve-ments are again observed when training data isrestricted. With only 100 training sentences, thepost-corrective model achieves a lexical morphemeaccuracy that is nearly as high as that obtainedusing the full dataset.",
  "This paper offers a promising and efficient solu-tion by introducing multiple resources to aid in": "the glossing task, particularly in linguistically di-verse and data-sparse environments. The currentstudy demonstrates the effectiveness of incorporat-ing translation information at both the token andsentence level, alongside LLM prompting in au-tomatic glossing for low-resource languages. Theproposed system, based on a modified version ofGirrbachs model (Girrbach, 2023), shows signif-icant performance enhancements, particularly inlow-resource settings. By leveraging translationdata and integrating a character-based decoder, ourapproach provides a robust solution for unobservedlexical morphemes (stems).This research pioneers the application of LLMprompting to the glossing task. By employing var-ious in-context example selection strategies andadding extra dictionary words as a resource, wehave shown that LLM prompting can substantiallyrefine lexical morpheme glosses, leading to higherword-level accuracy. This approach is also partic-ularly beneficial in scenarios with limited trainingdata, as it maximizes the potential of minimal dataresources.In all, the integration of translation information,additional dictionary resources, along with LLMprompting, sets a new benchmark in automaticglossing.",
  "Limitations": "The limitations of our study primarily pertain to theextent of our experimentation and the models wehave chosen. Firstly, our investigation relies solelyon an LSTM decoder. This decision was influ-enced by time constraints, which limited our abilityto explore more complex decoders. Additionally,our experimentation is confined to the T5-largemodel. While this model has shown promising re-sults in our study, we acknowledge the existenceof other large language models in the field of natu-ral language processing. Although we did exploreother large language models such as LLaMA-2(Touvron et al., 2023), our preliminary experimentsyielded unsatisfactory results compared to T5. Con-sequently, we made the decision not to includeLLaMA-2 in our paper due to its inferior perfor-mance. These limitations underscore the need forfuture research to explore a wider range of decod-ing architectures and incorporate various large lan-guage models to enhance our understanding of thesubject matter. However, using large language mod-els requires significant computational resources,",
  "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio. 2014.Neural machine translation by jointlylearning to align and translate.arXiv preprintarXiv:1409.0473": "Emily M. Bender, Joshua Crowgey, Michael WayneGoodman, and Fei Xia. 2014. Learning grammarspecifications from IGT: A case study of chintang.In Proceedings of the 2014 Workshop on the Use ofComputational Methods in the Study of EndangeredLanguages, pages 4353, Baltimore, Maryland, USA.Association for Computational Linguistics. Abhik Bhattacharjee, Tahmid Hasan, Wasi UddinAhmad, Kazi Samin, Md Saiful Islam, AnindyaIqbal, M Sohel Rahman, and Rifat Shahriyar. 2021.Banglabert: Language model pretraining and bench-marks for low-resource language understanding eval-uation in bangla. arXiv preprint arXiv:2101.00204. Rishi Bommasani, Drew A Hudson, Ehsan Adeli,Russ Altman, Simran Arora, Sydney von Arx,Michael S Bernstein, Jeannette Bohg, Antoine Bosse-lut, Emma Brunskill, et al. 2021. On the opportuni-ties and risks of foundation models. arXiv preprintarXiv:2108.07258. Alexis Conneau, Kartikay Khandelwal, Naman Goyal,Vishrav Chaudhary, Guillaume Wenzek, FranciscoGuzmn, douard Grave, Myle Ott, Luke Zettle-moyer, and Veselin Stoyanov. 2020. Unsupervisedcross-lingual representation learning at scale. In Pro-ceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 84408451.",
  "Michael Ginn and Alexis Palmer. 2023. Taxonomicloss for morphological glossing of low-resource lan-guages. arXiv preprint arXiv:2308.15055": "Michael Ginn, Lindia Tjuatja, Taiqi He, Enora Rice,Graham Neubig, Alexis Palmer, and Lori Levin.2024.Glosslm:Multilingual pretraining forlow-resource interlinear glossing.arXiv preprintarXiv:2403.06399. Leander Girrbach. 2023. T-CL at SIGMORPHON2023: Straight-through gradient estimation for hardattention. In Proceedings of the 20th SIGMORPHONworkshop on Computational Research in Phonet-ics, Phonology, and Morphology, pages 151165,Toronto, Canada. Association for Computational Lin-guistics. Viktor Hangya, Hossain Shaikh Saadi, and AlexanderFraser. 2022. Improving low-resource languages inpre-trained multilingual language models. In Pro-ceedings of the 2022 Conference on Empirical Meth-ods in Natural Language Processing, pages 1199312006. Taiqi He, Lindia Tjuatja, Nathaniel Robinson, ShinjiWatanabe, David R Mortensen, Graham Neubig, andLori Levin. 2023. SigMoreFun submission to theSIGMORPHON shared task on interlinear glossing.In Proceedings of the 20th SIGMORPHON workshopon Computational Research in Phonetics, Phonology,and Morphology, pages 209216.",
  "Sepp Hochreiter and Jrgen Schmidhuber. 1997. Longshort-term memory. Neural computation, 9(8):17351780": "Masoud Jalili Sabet, Philipp Dufter, Franois Yvon,and Hinrich Schtze. 2020. SimAlign: High qual-ity word alignments without parallel training datausing static and contextualized embeddings. In Find-ings of the Association for Computational Linguistics:EMNLP 2020, pages 16271643, Online. Associationfor Computational Linguistics. Katharina Kann and Hinrich Schtze. 2016. MED: TheLMU system for the SIGMORPHON 2016 sharedtask on morphological reinflection. In Proceedingsof the 14th SIGMORPHON Workshop on Computa-tional Research in Phonetics, Phonology, and Mor-phology, pages 6270.",
  "Ilya Loshchilov and Frank Hutter. 2017.Decou-pled weight decay regularization.arXiv preprintarXiv:1711.05101": "Katerina Margatina, Timo Schick, Nikolaos Aletras, andJane Dwivedi-Yu. 2023. Active learning principlesfor in-context learning with large language models.In Findings of the Association for Computational Lin-guistics: EMNLP 2023, pages 50115034, Singapore.Association for Computational Linguistics. Sarah Moeller and Mans Hulden. 2018. Automaticglossing in a low-resource setting for language doc-umentation.In Proceedings of the Workshop onComputational Modeling of Polysynthetic Languages,pages 8493, Santa Fe, New Mexico, USA. Associa-tion for Computational Linguistics. Kelechi Ogueji, Yuxin Zhu, and Jimmy Lin. 2021.Small data? no problem! exploring the viabilityof pretrained multilingual language models for low-resourced languages. In Proceedings of the 1st Work-shop on Multilingual Representation Learning, pages116126. Shu Okabe and Franois Yvon. 2023. Towards multi-lingual interlinear morphological glossing. In Find-ings of the Association for Computational Linguis-tics: EMNLP 2023, pages 59585971, Singapore.Association for Computational Linguistics. Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J Liu. 2020. Exploring the limitsof transfer learning with a unified text-to-text trans-former. The Journal of Machine Learning Research,21(1):54855551. Zaid Sheikh, Antonios Anastasopoulos, Shruti Rijhwani,Lindia Tjuatja, Robbie Jimerson, and Graham Neu-big. 2024. Cmulab: An open-source framework fortraining and deployment of natural language process-ing models. arXiv preprint arXiv:2404.02408. Conor Snoek, Dorothy Thunder, Kaidi Lo, Antti Arppe,Jordan Lachler, Sjur Moshagen, and Trond Trosterud.2014. Modeling the noun morphology of Plains Cree.In Proceedings of the 2014 Workshop on the Use ofComputational Methods in the Study of EndangeredLanguages, pages 3442, Baltimore, Maryland, USA.Association for Computational Linguistics.",
  "future with pre-trained byte-to-byte models. Transac-tions of the Association for Computational Linguis-tics, 10:291306": "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale,Rami Al-Rfou, Aditya Siddhant, Aditya Barua, andColin Raffel. 2021. mT5: A massively multilingualpre-trained text-to-text transformer. In Proceedingsof the 2021 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 483498. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,Xiaolei Wang, Yupeng Hou, Yingqian Min, BeichenZhang, Junjie Zhang, Zican Dong, et al. 2023. Asurvey of large language models.arXiv preprintarXiv:2303.18223. Xingyuan Zhao, Satoru Ozaki, Antonios Anastasopou-los, Graham Neubig, and Lori Levin. 2020. Auto-matic interlinear glossing for under-resourced lan-guages leveraging translations. In Proceedings ofthe 28th International Conference on ComputationalLinguistics, pages 53975408, Barcelona, Spain (On-line). International Committee on Computational Lin-guistics. Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu,Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan,Lifang He, et al. 2023. A comprehensive survey onpretrained foundation models: A history from bert tochatgpt. arXiv preprint arXiv:2302.09419.",
  "AIGT Information": "In the IGT data, the second line includes segmen-tations with morphemes normalized to a canonicalorthographic form. The third line has an abbrevi-ated gloss for each segmented morpheme. Lexicalmorphemes typically correspond to the stems ofwords. The morpheme glosses usually have two cat-egories: Lexical and Grammatical morphemes. Forexample, in glossing labels such as work-1SG.II,work\" would be considered a Lexical morpheme,representing the core semantic unit. On the otherhand, Grammatical morphemes like 1SG.II\" areoften denoted by uppercase glosses and generallysignify grammatical functions, such as tense, as-pect, or case, rather than specific lexical content.",
  "BModel Settings": "Our experimental framework and hyperparame-ters draw inspiration from Girrbachs methodology,with a focus on organizing and optimizing the tech-nical setup. For model optimization, we employ theAdamW optimizer (Loshchilov and Hutter, 2017),excluding weight decay, and set the learning rate at 0.001. Except for this specific adjustment, wemaintain PyTorchs default settings for all otherparameters.Our configuration is structured to allow a rangeof experiments, varying from 1 to 2 LSTM layers,with hidden sizes spanning from 64 to 512, anddropout rates fluctuating between 0.0 and 0.5. Thescheduler is adjusted within a range of 0.9 to1.0, and batch sizes are diversified, ranging from 2to 64. This versatile approach is designed to thor-oughly evaluate the models performance across aspectrum of hyperparameter configurations.Departing from the original model which wastrained for 25 epochs, our approach extends thetraining duration to 300 epochs when using largepretrained models. In cases where the BERT modelis utilized, we sometime apply a 0.5 dropout rateduring the BERT training phase. We exclusivelyemploy the multilingual BERT model for Uspan-teko, while we utilize the standard BERT modelfor all other languages. This comprehensive andmeticulously organized setup is aimed at enhanc-ing the effectiveness and efficiency of our modeltraining process.To prevent coincidences, for each proposedmodel configuration, we train the model for 10iterations, and the final prediction is determinedthrough majority voting.",
  "EAttention Distribution": "To assess whether our model is able to success-fully incorporate translation information, we visu-alize attention patterns (from the BERT+attn+chrmodel) over the English translation representations. presents an example for Natgu. Atten-tion weights are displayed in a heat map, whereeach cell indicates difference from mean attention:a 1/(n + 2). Here n is the length of the trans-lation in tokens (+2 here because of the start-of-sequence and end-of-sequence tokens [CLS] and",
  "BERT/T5+attn+chr-average79.3279.4980.7681.0074.9279.1025.4323.9554.2857.1832.4128.7737.00BERT/T5+attn+chr-majority81.1182.3785.4185.9179.3482.8328.8228.1157.3362.8239.9735.8442.14": ": Word-level accuracy of languages in the 2023 Sigmorphon Shared Task (Ginn et al., 2023) and low-resourcesettings. We compute the average across 10 models and also utilized majority voting accuracy results. Languageabbreviations were used, with arp representing Arapaho, git for Gitksan, lez for Lezgi, ntu for Natgu, ddofor Tsez, and usp for Uspanteko. Model specifics are elaborated in . [SEP] which are concatenated to the translation).Positive red cells inidicate high attention and neg-ative blue cells low attention. The visualizationclearly indicates that the model attends to the rele-vant tokens in the translation when predicting thestems people, mankind and kill. -shows randomly picked heat maps for the rest of thelanguages. We can see that attention weights for thelarger shared task datasets tend to express relevantassociations, while attention weights for the ultralow-resource training sets largely represent noise.- also displays attention distri-butions when translations are incorporated using arandomly initialized LSTM instead of a pre-trainedlanguage model. These distributions also largelyrepresent noise indicating that pre-trained modelsconfer an advantage.",
  "FPrompt template": "You are a linguistic annotator for the Gitksan lan-guage, tasked with correcting errors in glossingbased on translation details and morpheme transla-tions. Your task is to adjust errors in the stems (inlowercase) without changing the total number ofmorphemes or words in the gloss. Each gloss ele-ment is separated by hyphens within morphemesand spaces between words.",
  "Example 2: Gitksan sentence is {example[train2-raw-sentence]}.Youareprovidedwithmorpheme translations according to the dic-tionary:{example[train2-word/morpheme-": ": Difference from mean attention weights of glossed output tokens (y-axis) with respect to encoded transla-tion tokens (x-axis) for an Arapaho example (attention weights are derived from the model BERT+attn+chr (left) andthe model LSTM+attm (right)). The gold-standard glosses for this sentence: IC.it.is-2S IC.be.had.as.father.by.all-2S. : Difference from mean attention weights of glossed output tokens (y-axis) with respect to encodedtranslation tokens (x-axis) for a Gitksan example (attention weights are derived from the model BERT+attn+chr(left) and the model LSTM+attm (right)). The gold-standard glosses for this sentence: CCNJ want-3.II PROSP-3.Itell-T-3.II OBL-1PL.II MANR LVB-3.II. : Difference from mean attention weights of glossed output tokens (y-axis) with respect to encodedtranslation tokens (x-axis) for a Lezgi example (attention weights are derived from the model BERT+attn+chr (left)and the model LSTM+attm (right)). The gold-standard glosses for this sentence: 1pl.abs return-AOR this one therevillage-ERG-DAT. : Difference from mean attention weights of glossed output tokens (y-axis) with respect to encodedtranslation tokens (x-axis) for a Natgu example (attention weights are derived from the model BERT+attn+chr(left) and the model LSTM+attm (right)). The gold-standard glosses for this sentence: but mankind MID-kill-COS-3MINIS people SUBR PAS-see-INTS-just. : Difference from mean attention weights of glossed output tokens (y-axis) with respect to encodedtranslation tokens (x-axis) for a Tsez example (attention weights are derived from the model BERT+attn+chr(left) and the model LSTM+attm (right)). The gold-standard glosses for this sentence: DEM2.ISG.OBL-LATvillage-IN.ESS beautiful girl give-PST.UNW : Difference from mean attention weights of glossed output tokens (y-axis) with respect to encodedtranslation tokens (x-axis) for a Uspanteko example (attention weights are derived from the model BERT+attn+chr(left) and the model LSTM+attm (right)). The gold-standard glosses for this sentence: CONJ INC-ir PREP rbol.",
  "Model settingarplezntuddouspgit": "T5/BERT+attn+chr83.6881.2981.5192.7982.7512.83+GPT4-random84.7885.1283.1990.5270.5426.79+GPT4-BERT-Sim85.1386.3583.3391.2373.2827.13+GPT4-Overlap86.5486.2084.1791.7674.9127.17+GPT4-LCS85.9785.8684.8790.8773.6526.98+LLaMA3-Overlap85.2384.0583.8889.5471.4329.81 : Lexical morpheme accuracy across languagesin the 2023 Sigmorphon Shared Task (Ginn et al., 2023)with arp representing Arapaho, git for Gitksan, lezfor Lezgi, ntu for Natgu, ddo for Tsez, and usp forUspanteko. Model specifics are elaborated in . translation]}.The English translation forthissentenceis:{example[train2-sentence-translation]}. The glossing pending to be revisedis: {example[train2-silver-gloss]}. The correctedgloss is {example[train2-gold-gloss]}. Now, heres the gloss you need to correct:Gitksansentenceis{example[test-raw-sentence]}.You are provided with mor-pheme translations according to the dictio-nary:{example[test-word/morpheme-gloss]}.The English translation for this sentence is:{example[test-translation]}. The glossing pend-ing to be revised is: {example[test-silver-gloss]}.What is the corrected gloss for this sentence? Youshould answer in this format: The corrected glossis: (your generated answer). Note, dont changethe total number of words or morphemes in thegloss."
}