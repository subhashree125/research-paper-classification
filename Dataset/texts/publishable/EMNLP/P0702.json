{
  "Abstract": "As the scale of training corpora for large lan-guage models (LLMs) grows, model devel-opers become increasingly reluctant to dis-close details on their data. This lack of trans-parency poses challenges to scientific evalu-ation and ethical deployment. Recently, pre-training data detection approaches, which inferwhether a given text was part of an LLMs train-ing data through black-box access, have beenexplored. The Min-K% Prob method, whichhas achieved state-of-the-art results, assumesthat a non-training example tends to containa few outlier words with low token probabil-ities. However, the effectiveness may be lim-ited as it tends to misclassify non-training textsthat contain many common words with highprobabilities predicted by LLMs. To addressthis issue, we introduce a divergence-based cal-ibration method, inspired by the divergence-from-randomness concept, to calibrate tokenprobabilities for pretraining data detection.We compute the cross-entropy (i.e., the diver-gence) between the token probability distri-bution and the token frequency distributionto derive a detection score. We have devel-oped a Chinese-language benchmark, Patent-MIA, to assess the performance of detectionapproaches for LLMs on Chinese text. Ex-perimental results on English-language bench-marks and PatentMIA demonstrate that our pro-posed method significantly outperforms exist-ing methods. Our code and PatentMIA bench-mark are available at",
  "*Corresponding author": "2023; Bai et al., 2023; Brown et al., 2020; Tou-vron et al., 2023b; Yang et al., 2023). This lackof transparency complicates the assurance that allethical and legal standards are met. The pretrainingcorpus may contain unauthorized private informa-tion or copyrighted content (Chang et al., 2023;Mozes et al., 2023). Indeed, OpenAI and NVIDIAface lawsuits over copyright issues related to theirtraining data (Grynbaum and Mac, 2023; Stempel,2024). Moreover, a lack of transparency aroundthe pretraining data used prevents us from prop-erly addressing the data contamination problem(Cao et al., 2024; Dong et al., 2024) and, hence,from determining whether an LLMs performanceis due to genuine task understanding or to priorexposure to test data. We focus on the followingkey question: How can we detect if a black-boxLLM was pretrained on a given text, consideringthat its training data is undisclosed?The pretraining data detection problem can beviewed as an instance of the membership inferenceattack (MIA) task (Shokri et al., 2017), where theprimary objective is to determine if a particular textwas part of a target LLMs training corpus. Prevail-ing methods to tackle this problem are based onthe idea that a texts token probability distributioncan reveal its inclusion in the training set. E.g., theMin-K% Prob method (Shi et al., 2024) is basedon the hypothesis that non-training examples tendto have more tokens assigned lower probabilitiesthan training examples do. Min-K% Prob relieson the assumption that data with higher probabil-ity is more likely to be training data. Languagemodels trained with a cross-entropy loss functiontend to favor high-frequency tokens when conduct-ing next-token prediction, which will also lead toLLMs generally predicting higher probabilities forhigh-frequency tokens (Jiang et al., 2019). As theconceptual example shown in figure 1, x1 is a non-training text and x2 is a training text. We can seethat the lowest raw token probabilities for x1 are : A conceptual example: Let x1 represent a non-training text and x2 a training text. (a) Min-K% Probdirectly selects the k% of tokens with the lowest probabilities for detection. (b) DC-PDD computes the divergencebetween the token probability distribution and the token frequency distribution for detection. higher than those for x2, which may be becausethe words in x1 (e.g., \"boys\", \"great\") are generallymore common than the words in x2 (e.g., \"erudite\",\"conundrum\"). Therefore, Min-k% Prob will cal-culate a detection score of -0.88 for x1 and -2.94for x1, which means that x1 is more likely to beconsidered a training text than. This is contrary tothe actual situation.Inspired by the divergence-from-randomness the-ory (Amati and van Rijsbergen, 2002), we in-troduce a divergence-based calibration method,named DC-PDD, to calibrate the token probabili-ties for pretraining data detection. The basic ideaunderlying divergence-from-randomness is that thehigher the divergence of the within-document term-frequency of a word in a document from its fre-quency within the collection, the more informa-tion the word carries. In our scenario, the within-document term-frequency can be interpreted as thetarget LLMs predicted probability for each tokenwith regard to the text to be detected, to which werefer as the token probability distribution. The fre-quency of a word within the collection refers to thefrequency of each token in the target LLMs pre-training corpus, to which we refer as the token fre-quency distribution. According to the divergence-from-randomness theory, the higher the divergencebetween these two distributions, the more infor-mative the tokens are in indicating that the text",
  "was part of the models training corpus, rather thansolely relying on token probabilities as the indica-tor for detection.Like prior works (Duan et al., 2024; Shi et al.,": "2024), we assume that we only have access to thetarget LLM as a black box: we can compute tokenprobabilities for the text to be detected but have noaccess to the internals of the LLM (e.g., weightsand activations). We first obtain the token proba-bility distribution by querying the LLM with thetext. Next, we use a large-scale publicly availablecorpus as a reference corpus to obtain an estimationof the token frequency distribution since an LLMspretraining corpus is not accessible usually. Wethen calibrate the token probabilities by compar-ing the token probability distribution to the tokenfrequency distribution. Based on the calibrated to-ken probabilities, we derive a score for pretrainingdata detection. Finally, a predefined threshold isapplied to the score to determine whether the textwas included in the LLMs pretraining corpus.(b) illustrates that DC-PDD assigns ascore to text that better reflects whether it is train-ing data or non-training data (i.e., a training textshould have a higher score than a non-training text).In contrast to other calibration methods (Carliniet al., 2021; Zhang et al., 2024), DC-PDD neitherrequires additional reference models nor extra ac-cess requirements on the target LLM.",
  "PatentMIA (Ours)GooglePatent Chinese51210,000 Open-source Chinese LLMs released betweenJanuary 1, 2023 and March 1, 2024": ": Benchmark summary statistics: Each benchmark has an equal split of training and non-training examples.Text Length refers to the number of words contained in each text example of the benchmark. #Examples denotesthe number of text examples in the benchmark. To facilitate this study and the evaluation of pre-training data detection for LLMs, we introducea new benchmark named PatentMIA, specificallydesigned for Chinese-language pretraining datadetection.PatentMIA is sourced from Google-Patents (Google, 2006) and constructed followingShi et al. (2024), who distinguish between train-ing and non-training data based on cut-off dates ofthe target LLM, where training data precedes, andnon-training data follows, the cut-off date.We conduct experiments on two English-language benchmarks (Shi et al., 2024) and onPatentMIA against a range of representative, state-of-the-art methods. Our experiments show that theproposed DC-PDD significantly outperforms priormethods. E.g., in the commonly used detection per-formance metrics, AUC and TPR@5%FPR, DC-PDD surpasses Min-K% Prob by 8.6% and 13.3%,respectively, on existing BookMIA benchmark.",
  "Task Description": "Formally, given a piece of text x and an LLM Mwith no knowledge of its pretraining corpus D, thepretraining data detection task aims to design amethod to determine if x was included in D. Thus,given x and M as input, a method A for the pre-training data detection task returns 1 if it predictsthat x is included in D and 0 if it is not:",
  "Benchmark Construction": "Unlike traditional membership inference attacks(Carlini et al., 2022; Jagannatha et al., 2021; Yeomet al., 2018), which are conducted on locally trainedmodels where the training and non-training dataare explicitly known, the pretraining data detectionfor LLMs poses a new challenge as the pretrain-ing corpus of LLMs is not disclosed. Here, weintroduce existing benchmarks and our newly con-structed benchmark that are specifically designedfor LLMs. shows their overall statistics.Pre-existing datasets. Shi et al. (2024) proposed abenchmark construction method by distinguishingbetween the training and non-training data basedon the knowledge cut-off date of the target LLM,where training data precedes and non-training datafollows the cut-off date. This method has been usedto construct two English-language benchmarks:WikiMIA and BookMIA. In this paper, we con-duct experiments on these benchmarks.A Chinese-language benchmark: PatentMIA.Existing benchmarks for the pretraining data de-tection task are exclusively in English.Otherlanguages exhibit unique grammatical character-istics such as flexible spacing and case insensitiv-ity compared to English, potentially influencingthe effectiveness of methods for the detection task.These differences warrant specific benchmarks toassess the performance of detection methods in lan-guages other than English. We propose a Chinese-language benchmark for that reason. Next, we de-tail the construction of the PatentMIA benchmark.Data source.We collect data from Google-Patents (Google, 2006) as (i) it contains a largevolume of high-quality, publicly available Chinesepatent texts and some publicly available large-scaleChinese corpora like ChineseWebText (Chen et al.,2023) explicitly incorporate data from this web-site, which indicates that existing LLMs are highlylikely to have used such data for pretraining; and (ii) if the priority date of a patent is after the releasedate of the LLM, there is a guarantee that the patenttext was not present during LLMs pretraining.Data collection. Based on Google-Patents, weconstruct a Chinese-language benchmark calledPatentMIA as follows. (i) Data crawling. Werandomly crawl 5,000 Chinese patent pages witha priority date after March 1, 2024 and 5,000patent pages with a publication date before Jan-uary 1, 2023 respectively. (ii) Data preprocessing.These pages then undergo several preprocessingand cleaning steps similar to those used in Chi-neseWebText to ensure the data format matchesthe pretraining data format of LLMs. (iii) Snippetextraction. For each page, we randomly extracta snippet of 512 words from the original content,creating a balanced set of 10,000 examples. Weuse jieba1 to segment Chinese texts into words.",
  "Overview": "Given a piece of text x = x1x2 . . . xn, where xirepresent the tokens after tokenizing x, and a targetLLM M, we compute a detection score by measur-ing the divergence between the token probabilitydistribution of x and the token frequency distri-bution in pretraining corpus, without any modeltraining processes. Our method consists of foursteps: (i) Token probability distribution computa-tion, by querying M with x (.2). (ii) To-ken frequency distribution computation, by usinga large-scale publicly available corpus D as a ref-erence corpus to obtain an estimation of the tokenfrequency distribution since Ms pretraining cor-pus is not assumed to be accessible (.3).(iii) Score calculation via comparison, by com-paring the above two distributions to calibrate thetoken probability for each token xi in x, and derivea score for pretraining data detection based on thecalibrated token probabilities (.4). (iv) bi-nary decision, by applying a predefined thresholdto the score, we predict whether x was included inMs pretraining corpus or not (.5).We summarize our method in Algorithm 1.",
  "Token Frequency DistributionComputation": "According to the divergence-from-randomness the-ory, after obtaining the token probability distribu-tion for x from M, we also need to calculate thefrequency of xi appearing in the pretraining corpusD of M to get the token frequency distribution.However, since D is not accessible, we cannot di-rectly calculate these terms. To address this, weuse a large-scale publicly available corpus D to",
  "i,if i < aa,if i a.(7)": "Typically, for a word that appears multiple times ina text, LLMs predict a higher probability for thatword in subsequent occurrences since the modelhas seen the word earlier in the text. Therefore, weadopt a simple countermeasure that only uses icorresponding to the first occurrence of xi in x tocalculate the final score :",
  "Experimental Settings": "Benchmarks and models. To evaluate the perfor-mance of DC-PDD, we conduct experiments onthree benchmarks mentioned in . Specifi-cally, for WikiMIA, we consider OPT-6.7B (Zhanget al., 2022), Pythia-6.9B (Biderman et al., 2023),Llama-13B (Touvron et al., 2023a), and GPT-NeoX-20B (Black et al., 2022), since they werereleased after 2017 and before 2023, and are well-known for incorporating Wikipedia dumps intotheir pretraining data. For BookMIA, we considerGPT-3,2 since its an OpenAI model released be-fore 2023. These settings are akin to Shi et al.(2024). For our benchmark PatentMIA, we selectBaichuan-13B (Yang et al., 2023) and Qwen1.5-14B (Team, 2024), since they are representativemodels in Chinese text generation and are releasedbetween January 1, 2023 and March 1, 2024.Baselines. We consider the following methods asour baselines, each predicting whether an examplewas included in training set based on: (i) PPL:The perplexity of the example. (ii) Lowercase:The ratio of the examples perplexity to that ofthe lowercased example. (iii) Zlib: The ratio ofthe examples perplexity against its zlib entropy.(iv) Small Ref: The ratio of an examples per-plexity to the examples perplexity under a smallermodel pretrained on the same data. (v) Min-K%Prob (Shi et al., 2024): The average log-likelihoodof the k% of tokens with the lowest probabilities.(vi) Min-K%++ Prob (Zhang et al., 2024): Theaverage normalized log-likelihood of the k% oftokens with the lowest normalized probabilities,where the normalization is based on the statisticsof the categorical distribution over the entire vo-cabulary. Note that the first four baselines wereintroduced in (Carlini et al., 2021). For more de-tails on our baselines, please refer to Appendix A.1.Evaluation metrics.Following most existingworks (Duan et al., 2024; Shi et al., 2024; Zhanget al., 2024), we use AUC score (area under ROCcurve) and TPR (true positive rate) at a low FPR(false positive rate) (TPR@5%FPR) as our metrics.For more details on these metrics, please refer toAppendix A.2.Implementation details. For the start-of-sentencetoken x0 to prepend, we use <|endoftext|> in",
  ": TPR@5%FPR scores for detecting pretraining texts. Bold indicates the best performing method. Two-tailedt-tests show that DC-PDD significantly improves over Min-K% Prob ( * indicates p 0.05)": "Pythia, Qwen1.5, GPT-NeoX and GPT-3, <s> inOPT and Llama, and </s> in Baichuan. For the ref-erence corpus D to compute the token frequencydistribution, we take a subset of C4 (Raffel et al.,2020) ( 15Gb) for English text detection and takea subset of ChineseWebText (Chen et al., 2023)( 15Gb) for Chinese text detection. For hyperpa-rameter a settings, we set it to 0.01 for WikiMIAand PatentMIA detection tasks, and to 10 for Book-MIA. Since we take the AUC score as our evalua-tion metric, we do not need to determine a specificthreshold in our method. For the baseline imple-mentation, we set k = 20 to achieve the optimalperformance of Min-K% Prob following Shi et al.(2024). Correspondingly, the hyperparameter k inMin-K%++ Prob is also set to 20 for fair compar-ison. For the smaller reference model setting, weemploy OPT-350M as the smaller model for OPT-6.7B, Pythia-70M for Pythia-6.9B, Llama-7B forLlama-13B, GPT-Neo-125M for GPT-NeoX-20B,Baichuan-7B for Baichuan-13B and Qwen1.5-7Bfor Qwen1.5-14B.",
  "Main Results": "Our results can be found in and 3. Weobserve that: (i) DC-PDD surpasses most base-lines across three benchmarks and various tar-get models. For instance, on existing BookMIAbenchmark, DC-PDD exceeds the best baselineLowercase 5.4% and 9.6% in terms of AUC andTPR@5%FPR. On our PatentMIA benchmark, DC-PDD exceeds the best baseline Min-K% Prob 5.4%and 13.2% in terms of AUC and TPR@5%FPR.(ii) Compared to Min-K% Prob, the AUC improve-ment of DC-PDD on the WikiMIA benchmark isless than that of Min-K%++ Prob, possibly becauseWikiMIA has only 250 examples, with fewer casesshown in we aim to optimize. While Min-K%++ Prob calibrates token probabilities fromother points, which might suit these examples bet-ter. This indicates that token probabilities are im-pacted by various factors and are unreliable for de-tection. Hence, we plan to explore better detectionsignals in the future. (iii) The superior performanceof DC-PDD is more agnostic to data and models, incomparison to other methods. For example, whileMin-K% Prob and Min-K%++ Prob perform wellon models using the WikiMIA benchmark, they donot do as well on models using the PatentMIAbenchmark. A similar phenomenon can be ob-served with the Zlib method. (iv) Additionally, the",
  ": Ablation studies of DC-PDD": "Small Ref method are not applicable to GPT-3, asclosed-source models lack corresponding smallermodels in the same series. The Min-K%++ Probis also not applicable to GPT-3 since GPT-3 donot provide the access to the next-token predictionprobability distribution across the models entirevocabulary. The Lowercase method is unsuitablefor detecting Chinese text, as Chinese charactersdo not have case distinctions. (v) By evaluatingperformance on the PatentMIA benchmark, exceptfor the Lowercase method, it is evident that exist-ing methods are still effective for Chinese-languagepretraining data detection, with our method consis-tently achieving the best results.",
  "Ablation Studies": "DC-PDD employs two strategies before using thecalibrated token probabilities to compute the score for x for detection. They are (i) LUP: Limitingthe UPper bound of each calibrated token proba-bility, w.r.t. Eq. (7), and (ii) SFO: only Selectingthe calibrated token probabilities corresponding totokens with the First Occurrence in x to compute, w.r.t. Eq. (8). We conduct ablation studies toexplore the effect of these strategies using the fol-lowing three method variants: CLD: It serves as the initialization of DC-PDDby averaging all the CaLibrateD token probabili-ties to compute a score for detection.",
  "+LUP: Based on CLD, it incorporates the LUPstrategy to compute": "+SFO: Based on +LUP, it further incorporatesthe SFO strategy to compute .Results are shown in . For Baichuan-13B and Qwen1.5-14B, both strategies contributeto the effectiveness of DE-CPP. However, for GPT-3, we found that the LUP strategy did not resultin a significant performance improvement. Wespeculate that this may be related to the setting ofthe hyperparameter a involved in the LUP strategy.Therefore, we discuss the impact of a on DC-PDDin detail in .3.",
  "Impact of Different Factors": "This section explores several factors that may in-fluence the performance of DC-PDD, includingtwo method-independent factors (model size andtext length) and two method-dependent factor (thereference corpus D and hyperparameter a).Model size. To investigate the impact of model sizeon the performance of DC-PDD, we analyze theQwen1.5 family with models of 1.8B, 4B, 7B, and14B versions to determine if larger models demon-strate improved results. As illustrated in (a), DC-PDD consistently achieves the best re-sults across all model sizes, and like other methods,the AUC score increases as the model size grows,confirming findings from prior research (Liu et al.,2024; Shi et al., 2024). The reason for this trend isprobably because larger models, having more pa-rameters, are better at memorizing the pre-trainingdata.Text length. We further explore the potential im-pact of text length on the performance of DC-PDD.For this purpose, we perform assessments usingfour different length settings (64, 128, 256, 512) inour PatentMIA benchmark to determine whethershort texts are more challenging than longer texts.(b) illustrates that DC-PDD still consis-tently outperforms other baselines across all textlength settings, and the AUC score also improves",
  ": AUC scores of DC-PDD in different referencecorpus settings": "with increasing length in Chinese-language pre-training data detection. This trend may be due tothe fact that longer texts carry more informationthat the target model has memorized, making themeasier to differentiate from non-training texts.Reference corpus D. Recall that we use a ref-erence corpus D to estimate the token frequencydistribution of the LLMs pretraining corpus, w.r.t.Eq. (4). To analyze the effect of different referencecorpora on the efficacy of the method, we com-pare the performance of DC-PDD under variousreference corpus settings across different scales anddomains. Specifically, when detecting WikiMIA-128 from pythia-6.9B, we employee 1Gb of C4corpus, 10Gb of C4 corpus, 1Gb of Case-lawcorpus, and 10Gb of Case-law corpus as thereference corpus respectively. Note that the Case-law (Louis Brul Naudet, 2024) is a corpus in thelegal domain. As shown in , We observethat the performance of DC-PDD does not exhibitsignificant differences across the various referencecorpora, indicating that DC-PDD is not sensitiveto the selection of a reference corpus. Notably,when the reference corpus is chosen as the 10Gbof C4 corpus, the performance of DC-PDD is thebest. This enhancement may be attributed to thegreater diversity of the C4 corpus compared to the 10Gb of Case-law corpus, as well as the richerdata compared to the 1Gb of C4 corpus, whichallow for a more accurate estimation of the tokenfrequency distribution in the LLMs pretrainingcorpus, thereby resulting in better performance.Hyperparameter a. Recall that we set a hyper-parameter a to prevent the final score from beingdominated by a few tokens, w.r.t. Eq. (7). We evalu-ate DC-PDD with different a settings to investigatetheir impact on detection performance. As shownin , performance varies significantly witha set to 0.001, 0.01, 0.1, 1, and 10. Actually, if ais set too high, it does not effectively limit the cal-ibrated token probabilities. Conversely, if set toolow, it will result in nearly equal calibrated tokenprobabilities, causing scores for training and non-training text to be similar and thus, ineffective fordetection. From the , we can see that the op-",
  ": AUC scores of DC-PDD in different a settings": "timal a setting varies across different target modelsand benchmarks. For instance, the optimal a is 10in detecting BookMIA from GPT-3 while it is 0.01in detecting PatentMIA from Qwen1.5-14B. Whena is set to 0.01, the overall performance for all mod-els is optimal. Therefore, we recommend setting ato 0.01 when using DC-PDD for pretraining datadetection in practical scenarios. In future work, wewill explore more flexible methods for setting a toachieve better performance of DC-PDD.",
  "Related Work": "Membership inference attack (MIA). MIA is thede-facto threat model when evaluating privacy con-cerns in machine learning models. First introducedby Shokri et al. (2017), MIAs objective is to ascer-tain whether a specific sample was part of a modelstraining dataset. Prior MIA research has focusedon traditional deep learning models (Sablayrolleset al., 2019; Song and Shmatikov, 2019) and fine-tuning language models (Hisamoto et al., 2020;Jagannatha et al., 2021; Mattern et al., 2023). Butrecently, MIA on LLMs has attracted growing at-tention with various applications, including exam-ination of training data memorization (Nasr et al.,2023), data contamination (Oren et al., 2023), andcopyright infringement (Duarte et al., 2024; Meeuset al., 2023). We consider a different type of MIA:pretraining data detection.Pretraining data detection for LLMs. Here, theMIA problem centers on identifying whether apiece of text was used by an LLM for pretraining.According to the access conditions to LLMs, cur-rent pretraining data detection methods for LLMscan be divided into two categories: (i) The white-box setting: assuming one has access to internalsof LLMs, such as weights and activations. (ii) Theblack-box setting: assuming one can only queryLLMs to compute token probabilities for the text.There is limited research on the white-box set-ting since the internals of LLMs are typically notdisclosed, rendering detection methods in white-box scenarios impractical. Liu et al. (2024) proposeto use the probing technique for pretraining data detection, based on the assumption that texts en-countered during the LLMs pretraining phase arerepresented differently in its internal activationscompared to unseen texts.Most research focuses on the black-box setting,assuming that the token probability distribution of atext can provide crucial information about whetherthe text was included in the training set. Carliniet al. (2021) considered the models perplexity fora text as an indicator to detect pretraining data fromGPT-2 (Radford et al., 2019). They further intro-duced three methods, Zlib, Lowercase, and SmallerRef, that take into account the intrinsic complex-ity of the target text. More recently, Shi et al.(2024) have proposed a straightforward yet well-performing method called Min-K% Prob. Min-K%Prob tends to classify a non-training text composedof common words as training data. A concurrentstudy Min-K%++ Prob (Zhang et al., 2024) im-proves Min-K% Prob by normalizing token prob-abilities, but requires access to the next-token pre-diction probability distribution across the LLMsentire vocabulary, which is unavailable in closed-source LLMs like GPT-3 (Brown et al., 2020).We consider the black-box setting and calibratethe token probabilities before using them for de-tection. What distinguishes our approach is that itneither requires additional reference models (unlikeSmall Ref) nor does it have extra access require-ments on the LLM (unlike Min-K%++ Prob).",
  "Conclusion": "In this work, we proposed DC-PDD to improvemethods that directly rely on token probabilities forpretraining data detection, which tend to misclas-sify non-training texts containing many commonwords as training texts. The key idea of DC-PDDis to calibrate the token probabilities and therebymake them more informative signals for detection.The calibration process is achieved by computingthe cross-entropy (i.e., the divergence) between thetoken probability distribution and the token fre-quency distribution. Experiments demonstrate thesuperior performances of DC-PDD compared tovarious baselines. In future work, we want to de-tect whether an LLM was pretrained on a givencorpus (corpus-level detection), rather than just ona piece of text (sample-level detection).",
  "DC-PDD, while showing promising results in pre-training data detection from LLMs, has several": "limitations. (i) DC-PDD utilizes a reference cor-pus to calculate the token frequency distributionto estimate that of the training corpus. Althoughworking, the similarity between these two distribu-tions remains uncertain. Additionally, the languageof reference corpus should be the same as that oftext to be detected. (ii) Secondly, an important hy-perparameter in DC-PDD is the upper bound of cal-ibrated token probabilities. We have demonstratedits significant impact on method performance, butnot how the optimal value should be set. We leavethis issue to future work. (iii) Thirdly, DC-PDDis specific to textual data. While some detectionmethods can be applied universally across differentdata modalities by relying on sample-level loss val-ues obtained from models, our method is based ontoken-level probability. This specificity hinders itsdirect application to other types of data, such as im-ages. (iv) Fourthly, DC-PDD requires access to to-ken probabilities, and therefore is not applicable tosome closed-source models. In the future, we willexplore detection methods based solely on modeloutput to design more generalizable detection meth-ods. (v) Lastly, except for the closed-source modelGPT-3 (Brown et al., 2020), our research primarilyfocused on models with up to 20 billion parametersdue to hardware constraints. Further studies repli-cating our work using larger-scale models will beessential to validate the effectiveness of DC-PDDin scenarios involving larger models.",
  "Ethical Considerations": "Although DC-PDD aims to address issues suchas copyright infringement or data contaminationthrough pretraining data detection, it can also beused to compromise the privacy of individualswhose data has been used to train models, as pre-training data detection problem is an instance ofMembership Inference Attacks (MIAs). Recogniz-ing the potential risks associated with MIAs, we areextremely cautious with the data we use to ensurethere is limited risk of any exposure of confiden-tial data. For example, the PatentMIA benchmarkis collected from the publicly available Google-Patents website and does not involve personal pri-vacy data. Additionally, the other benchmarks weuse have also been employed in prior research anddo not pose any privacy risks.",
  "Acknowledgements": "This work was funded by the National Natural Sci-ence Foundation of China (NSFC) under GrantsNo. 62472408 and 62372431, the Strategic Prior-ity Research Program of the CAS under GrantsNo.XDB0680102 and XDB0680301, the Na-tional Key Research and Development Programof China under Grants No. 2023YFA1011602 and2021QY1701, the Youth Innovation Promotion As-sociation CAS under Grants No. 2021100, theLenovo-CAS Joint Lab Youth Scientist Project, andthe project under Grants No. JCKY2022130C039.This work was also (partially) funded by theDutch Research Council (NWO), under projectnumbers 024.004.022, NWA.1389.20.183, andKICH3.LTP.20.006, and the European UnionsHorizon Europe program under grant agreementNo 101070212.All content represents the opinionof the authors, which is not necessarily shared orendorsed by their respective employers and/or spon-sors. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. GPT-4 technical re-port. arXiv preprint arXiv:2303.08774. Gianni Amati and Cornelis Joost van Rijsbergen. 2002.Probabilistic models of information retrieval basedon measuring the divergence from randomness.ACM Transactions on Information Systems (TOIS),20(4):357389.",
  "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, FeiHuang, et al. 2023. Qwen technical report. arXivpreprint arXiv:2309.16609": "Stella Biderman, Hailey Schoelkopf, Quentin GregoryAnthony, Herbie Bradley, Kyle OBrien, Eric Hal-lahan, Mohammad Aflah Khan, Shivanshu Purohit,USVSN Sai Prashanth, Edward Raff, et al. 2023.Pythia: A suite for analyzing large language mod-els across training and scaling.In InternationalConference on Machine Learning, pages 23972430.PMLR. Sid Black, Stella Biderman, Eric Hallahan, Quentin An-thony, Leo Gao, Laurence Golding, Horace He, Con-nor Leahy, Kyle McDonell, Jason Phang, et al. 2022.GPT-NeoX-20B: An open-source autoregressive lan-guage model. arXiv preprint arXiv:2204.06745.",
  "Jialun Cao, Wuqi Zhang, and Shing-Chi Cheung. 2024.Concerned with data contamination? Assessing coun-termeasures in code language model. arXiv preprintarXiv:2403.16898": "Nicholas Carlini, Steve Chien, Milad Nasr, ShuangSong, Andreas Terzis, and Florian Tramer. 2022.Membership inference attacks from first principles.In 2022 IEEE Symposium on Security and Privacy(SP), pages 18971914. IEEE. Nicholas Carlini,Florian Tramer,Eric Wallace,Matthew Jagielski, Ariel Herbert-Voss, KatherineLee, Adam Roberts, Tom Brown, Dawn Song, UlfarErlingsson, et al. 2021. Extracting training data fromlarge language models. In 30th USENIX SecuritySymposium (USENIX Security 21), pages 26332650.",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan,Dario Amodei, Ilya Sutskever, et al. 2019. Languagemodels are unsupervised multitask learners. OpenAIblog, 1(8):9": "Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J Liu. 2020. Exploring the lim-its of transfer learning with a unified text-to-texttransformer. Journal of Machine Learning Research,21(140):167. Alexandre Sablayrolles, Matthijs Douze, CordeliaSchmid, Yann Ollivier, and Herv Jgou. 2019.White-box vs black-box: Bayes optimal strategies formembership inference. In International Conferenceon Machine Learning, pages 55585567. PMLR. Weijia Shi, Anirudh Ajith, Mengzhou Xia, YangsiboHuang, Daogao Liu, Terra Blevins, Danqi Chen, andLuke Zettlemoyer. 2024. Detecting pretraining datafrom large language models. In The Twelfth Interna-tional Conference on Learning Representations. Reza Shokri, Marco Stronati, Congzheng Song, and Vi-taly Shmatikov. 2017. Membership inference attacksagainst machine learning models. In 2017 IEEE sym-posium on security and privacy (SP), pages 318.IEEE. Congzheng Song and Vitaly Shmatikov. 2019. Audit-ing data provenance in text-generation models. InProceedings of the 25th ACM SIGKDD InternationalConference on Knowledge Discovery & Data Mining,pages 196206.",
  "Qwen Team. 2024. Introducing qwen1.5": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, et al. 2023a.Llama:Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023b.Llama 2: Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288.",
  "Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang,Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang,Dong Yan, et al. 2023. Baichuan 2: Open large-scalelanguage models. arXiv preprint arXiv:2309.10305": "Samuel Yeom, Irene Giacomelli, Matt Fredrikson, andSomesh Jha. 2018. Privacy risk in machine learn-ing: Analyzing the connection to overfitting. In 2018IEEE 31st Computer Security Foundations Sympo-sium (CSF), pages 268282. IEEE. Jingyang Zhang, Jingwei Sun, Eric Yeats, Yang Ouyang,Martin Kuo, Jianyi Zhang, Hao Yang, and Hai Li.2024. Min-K%++: Improved baseline for detectingpre-training data from large language models. arXivpreprint arXiv:2404.02936. Susan Zhang, Stephen Roller, Naman Goyal, MikelArtetxe, Moya Chen, Shuohui Chen, Christopher De-wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.OPT: Open pre-trained transformer language models.arXiv preprint arXiv:2205.01068.",
  "A.1Baseline details": "The baselines are all based on a detection score todetermine a text x whether was included in the per-training corpus of an LLM M. Followings are thedetails of how they calculate the detection score.PPL. (Carlini et al., 2021) This is an instance ofLoss Attack proposed by (Yeom et al., 2018). Inthe context of LLMs, this loss corresponds to per-plexity. Thus, the detection score is the perplexityof x. A low score suggests that x was likely part ofthe pretraining data.Small Ref. (Carlini et al., 2021) This method ex-actly follows the approach described by Watsonet al. (2021), which assumes access to a referencemodel, Mref, trained on a disjoint set of trainingdata drawn from a similar distribution and positsthat the intrinsic complexity of x can be quantifiedas Mrefs perplexity for x. Since the assumptionis impractical, the Small Ref method employs asmaller model from the same family of M as a sub-stitute for Mref, and then calibrate Ms perplexityfor x using a difficulty estimate through the smallermodels perplexity for x. Consequently, the detec-tion score is calculated as the ratio of xs perplexityunder M to xs perplexity under a smaller modelpre-trained on the same data. A low score suggeststhat x was likely part of the pretraining data.Zlib. (Carlini et al., 2021) Similar to the SmallRef method, but uses the zlib entropy of x in placeof the smaller models perplexity for x. The zlibentropy is the entropy in bits when the sequence iscompressed using zlib.3 The detection score is thendetermined by the ratio of Ms perplexity for x tothe zlib entropy of x. A low score suggests that xwas likely part of the pretraining data.Lowercase. (Carlini et al., 2021) Similarly to theSmall Ref method, but uses Ms perplexity forthe lowercase of x to replace the smaller modelsperplexity for x. The detection score is then de-termined by the ratio of Ms perplexity for x toMs perplexity for the lowercase of x. A low scoresuggests that x was likely part of the pretrainingdata.Min-K% Prob. (Shi et al., 2024) Min-K% Prob isbased on the intuition that non-member examplestend to have more tokens assigned lower probabili-ties than member examples do. Thus, it begins bycalculating the probability of each token in x, then selects the k% of tokens with the lowest probabili-ties to compute their average log-likelihood as thedetection score. A high score suggests that x waslikely part of the pretraining data.Min-K%++ Prob. (Zhang et al., 2024) The un-derlying idea of Min-K%++ Prob is that if theprobability of the current input token surpassesthe probabilities of other tokens in the vocabulary,it is probable that the input has been seen duringtraining, irrespective of the actual probability valueof the input token. Therefore, it first calculates theprobability of each token in x, then normalizes thetoken probability using the statistics of the cate-gorical distribution over the entire vocabulary, andfinally selects the k% of tokens with the lowestnormalized probabilities to compute their averageas the detection score. A high score suggests thatx was likely part of the pretraining data.",
  "A.2Metrics": "Area Under the ROC Curve (AUC). The AUCscore quantifies the overall performance of a clas-sification method. To calculate the AUC score fora method, we need to compute the True PositiveRates (TPRs) and False Positive Rates (FPRs) at allclassification thresholds and plot a TPR vs. FPRcurve, known as the ROC curve. The AUC is thendefined as the Area Under the ROC curve, pro-viding an aggregate measure of the effect of allpossible classification thresholds. Therefore, AUCprovides a comprehensive, threshold-independentscore that reflects the methods ability to distin-guish between positive and negative cases effec-tively.TPR (true positive rate) at a low FPR (false pos-itive rate). We report TPR at a low FPR by adjust-ing the threshold value, Specifically, we choose 5%as our target FPR value, and report the correspond-ing TPR value."
}