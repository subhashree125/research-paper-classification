{
  "Abstract": "Simultaneous Machine Translation (SiMT) re-quires target tokens to be generated in real-time as streaming source tokens are consumed.Traditional approaches to SiMT typically re-quire sophisticated architectures and extensiveparameter configurations for training adaptiveread/write policies, which in turn demand con-siderable computational power and memory.We propose PsFuture, the first zero-shot adap-tive read/write policy for SiMT, enabling thetranslation model to independently determineread/write actions without the necessity for ad-ditional training. Furthermore, we introducea novel training strategy, Prefix-to-Full (P2F),specifically tailored to adjust offline translationmodels for SiMT applications, exploiting theadvantages of the bidirectional attention mech-anism inherent in offline models. Experimentsacross multiple benchmarks demonstrate thatour zero-shot policy attains performance on parwith strong baselines and the P2F method canfurther enhance performance, achieving an out-standing trade-off between translation qualityand latency.1",
  "Introduction": "Simultaneous Machine Translation (SiMT) (Guet al., 2017) is required to generate target tokensconcurrently as it processes incoming source to-kens. Differing from traditional machine transla-tion (MT) methods (Bahdanau et al., 2015; Vaswaniet al., 2017; Pang et al., 2024) that access the fullsource text, SiMT necessitates a read/write (R/W)policy to decide between emitting target tokensor awaiting more source input, coupled with theability to translate from source prefixes to targetprefixes (P2P) (Ma et al., 2018). Typically, theread/write policy is integrated with the translation",
  "* Corresponding author.1Thecodeisavailableat": "mechanism: either employing a fixed wait-k pol-icy alongside a corresponding translation model(Ma et al., 2018; Elbayad et al., 2020; Zhang et al.,2021b), or utilizing an adaptive policy (Gu et al.,2017; Dalvi et al., 2018; Zheng et al., 2019, 2020;Ma et al., 2020; Zhang and Feng, 2022b; Guo et al.,2023a; Zhao and Zeng, 2024; Chen et al., 2024)that dynamically adjusts read/write decisions basedon the context, in conjunction with a model trainedto translate policy-defined prefixes. This adaptivemethod has led to superior performance (Zhangand Feng, 2022a, 2023), yet it demands special-ized architectural solutions and multitask learningframeworks for concurrent training of the closelylinked adaptive policy and translation model, com-plicating component optimization and increasingcomputational demands.On the other hand, DaP-SiMT (Zhao et al., 2023)introduces a novel framework that separates theadaptive read/write policies from the translationmodel, offering greater versatility in simultaneoustranslation. This approach demonstrates that trans-lation models, when directed by an effective adap-tive read/write policy, even if initially trained onfixed policies, can balance quality and latency well,achieving state-of-the-art (SOTA) outcomes. How-ever, akin to other adaptive policies, it requiresintricate designs and a significant parameter set fortraining the adaptive read/write policy, often de-manding substantial computational resources andmemory.We introduce PsFuture, a zero-shot adaptiveread/write policy based on pseudo-future informa-tion. This policy utilizes the inherent capabilitiesof the translation model itself to make read/writedecisions without additional training. Similar to thepolicy in DaP-SiMT (Zhao et al., 2023), we drawinspiration from human simultaneous translation(Al-Khanji et al., 2000; Liu, 2008), where inter-preters shift from listening to translating upon antic-ipating that further future words would not impact",
  "toaan": ": An ZhEn example demonstrating an ideal timing for predicting the next token \"to\". Even when providedwith additional possible future information, the probability distribution of the predicted next token does not changesignificantly, remaining dominated by the token \"to\". Therefore, based on the current source prefix \"\" andthe current target prefix \"I want,\" a write operation can be executed to predict the next token as \"to\". their current decisions. As illustrated in ,this behavior implies a minor divergence betweentranslation predictions based on partial versus morecomplete source context. However, in simultaneoustranslation tasks, previewing future source informa-tion is not feasible. Our method, PsFuture, over-comes this by utilizing pseudo-future information,which is a token suffix in the source language thatcan be fixed or dynamically predicted by languagemodels. By quantifying the divergence betweenthe predicted next target token distributions with orwithout pseudo-future information, and comparingit to a predefined threshold, a read/write decisioncan be made. The proposed PsFuture method can be directlyapplied to most existing simultaneous translationmodels, such as the multi-path wait-k model, whichdemonstrates superior performance when directedby effective adaptive policies (Zhao et al., 2023).Additionally, we investigate the application of thePsFuture method to offline translation models. Pre-vious SiMT models (Elbayad et al., 2020; Zhangand Feng, 2022a) conventionally employ a unidi-rectional attention encoder with tailored masked-cross-attention for prefix-to-prefix training. Thisapproach, while efficient, limits the models abil-ity to extract features, making it less adept inhigh-latency scenarios compared to offline mod-els that utilize bidirectional attention mechanisms.To leverage the benefits of bidirectional attentionin SiMT, we introduce a novel and effective train-ing technique, Prefix-to-Full (P2F), designed to en-hance the performance of offline translation modelsunder diverse latency conditions. Our main contri-butions can be summarized as follows. 1. We propose the first zero-shot adaptiveread/write policy in SiMT, PsFuture, whichutilizes the inherent capabilities of the transla-tion model to make read/write decisions with-out any additional training. To our knowledge,PsFuture is the only adaptive method in thecurrent SiMT field that offers such flexibility.",
  "Related Work": "SiMT policies are broadly categorized into fixedand adaptive schemes. Fixed policies (Ma et al.,2018; Elbayad et al., 2020; Zhang et al., 2021b)execute read/write actions following predefinedrules, such as the wait-k policy (Ma et al., 2018),which after reading k source tokens, alternates be-tween reading and writing one token. Conversely,adaptive policies dynamically determine read/writeactions based on the evolving source and targetcontext, enhancing the balance between translationaccuracy and latency.Adaptive approaches employ methods like re-inforcement learning within a Neural MachineTranslation (NMT) framework (Gu et al., 2017),incremental decoding for variable target tokenoutput (Dalvi et al., 2018), and attention-basedmethods (Arivazhagan et al., 2019; Ma et al.,",
  "Read / Write": ": An overall schematic of the PsFuture policy. Based on the current source prefixes (x1, x2), target prefixes(y1, y2), and pseudo future information (x3, x4) (tokens highlighted in red), the simultaneous translation model candirectly perform adaptive read/write decisions. 2020). Additionally, the wait-info policy (Zhanget al., 2022) and ITST (Zhang and Feng, 2022a)quantify the waiting latency and informationweight respectively for adaptive policy formula-tion. HMT (Zhang and Feng, 2023) optimizesread/write decisions by enhancing the target se-quences marginal likelihood across various transla-tion initiation points. Kim and Cho (2023) employsa word-level policy to enhance SiMT. Furthermore,Ma et al. (2023) introduces a non-autoregressivestreaming Transformer (NAST) to mitigate the chal-lenges of nonmonotonicity and source-informationleakage present in conventional autoregressiveSiMT frameworks. Guo et al. (2023b) proposeto provide a tailored reference for the improvementof SiMT model training.Sharing a similar inspiration with PsFuture, DaP-SiMT (Zhao et al., 2023) autonomously generatesread/write supervisions by leveraging future infor-mation divergence for training a decision-makingnetwork. In contrast, our approach harnesses themodels inherent translation capability to attain animmediate, zero-shot read/write policy.",
  "Full-sentence MT and SiMT": "In full sentence translation tasks, an encoder-decoder architecture like the Transformer (Vaswaniet al., 2017) transforms a translation pair x =(x1, x2, ..., xN) and y = (y1, y2, ..., yT ) by encod-ing x into latent representations, followed by the au-toregressive generation of target tokens from theserepresentations. Generally, the model is optimized",
  "g(t; k) = min{t + k 1, N}.(3)": "Multi-path Wait-k (Elbayad et al., 2020) is an ef-ficient technique for wait-k training. It randomlysamples different k values between batches dur-ing model optimization. By employing a unidi-rectional attention encoder with a tailored uppertriangular masked cross-attention mechanism, themulti-path wait-k model achieves efficient prefix-to-prefix training. Zhao et al. (2023) demonstratesthat the multi-path wait-k model can attain SOTAperformance under the guidance of effective adap-tive policies.",
  "The Pseudo-Future-based Zero-ShotAdaptive Policy": "In simultaneous translation, skilled human transla-tors execute read/write decisions grounded in theevolving contexts of source and target texts. Con-ceptualizing a well-trained translation model as anintelligent agent like a human, our objective is to de-lineate a zero-shot adaptive read/write policy. Thisapproach enables decision-making based purely onthe models inherent linguistic comprehension andtranslation proficiency, facilitating adaptive poli-cies without necessitating further training.Zooming in on the details of the read/writedecision-making process, interpreters transitionfrom listening to translating when they have ac-quired sufficient source context xg(t) to decideon extending the partial translation y<t with thenext target word yt. This decision is based on theanticipation that additional source information willnot alter their current translation choice, which im-plies a slight divergence Dppartt, pmoretbetweenthe interpreters estimation of the translation dis-tribution with partial source context ppartt, and thetranslation distribution considering the more com-plete source context pmoret. Interpreters opt to waitfor more source words if this divergence becomessubstantial.",
  "pmoret= p(yt = |xmore, y<t),(5)": "where xmore represents the more complete sourcecontext by appending additional source tokens(xg(t)+1, xg(t)+2, ...) to the current source textsxg(t) and the distributions can be computed byany SiMT translation models.However, previewing future source informationis not feasible during inferring in simultaneoustranslation. Our proposed PsFuture method, as thename implies, overcomes this by utilizing pseudo-future information xps-suffix, which is a token suffixin the source language. It should be noted thatpseudo-future information here does not merelyrefer to the predicted next few source tokens adher-ing to human natural language patterns, but rathera broader concept representing additional informa-tion beyond current source input. When such infor-mation minimally impacts the subsequent target to-ken prediction of the translation agent, it indicateslow ambiguity in the translating process, which suggests that the translation of the current sourceprefix remains incomplete, thereby signaling anappropriate moment for a WRITE operation. Con-versely, it indicates an opportune moment for aREAD operation. In this work, we explore vari-ous forms of pseudo-future information, includingboth predefined fixed suffixes and adaptive onesthat are dynamically predicted by language models(detailed in .2).As shown in Equation 6 and 7, we utilize co-sine distance, which has been validated as effec-tive in DaP-SiMT (Zhao et al., 2023), to quantifythe divergence Dppartt, ppseudotbetween the pre-dicted next target token distributions with or with-out pseudo-future information.",
  "write if Dt,g(t) < , else read(8)": "shows an example divergence matrixbased on the PsFuture method and a highlightedread/write path, in which we only employ a <eos>token as the pseudo-future suffix. It can be ob-served that comparing with a suitable thresholdallows for the easy identification of a potentialread/write path.Following (Zhao et al., 2023), we also introduceanother hyperparameter in the read/write decision-making process to limit the maximum number ofcontinuous READ operations for certain languages,thereby enhancing their performance. The infer-ence process is summarized in Algorithm 1.",
  ": Example of a ZhEn divergence matrix D,where Dt,g(t) = Dppartt, ppseudot. The red elementsin the matrix denote a potential read/write path, deter-mined by a predefined threshold (0.2 in this case)": "2017)). Offline translation models have demon-strated substantial potential for simultaneous trans-lation, as evidenced by their efficacy in speechtranslation (Papi et al., 2022). However, the lackof Prefix-to-Prefix (P2P) training in offline modelsleads to lower translation quality under low-latencyconditions compared to SiMT models (Ma et al.,2018). On the other hand, the bidirectional atten-tion mechanism of offline models significantly en-hances feature extraction, surpassing the unidirec-tional attention mechanism typically used in SiMTmodels to facilitate P2P training. Thus, in high-latency scenarios, offline models usually achievebetter translation quality, as shown in .To harness the benefits of the bidirectional at-tention mechanism in real-time contexts, we in-troduces a simple yet effective training strategyfor offline translation models named Prefix-to-Full(P2F). This method aims to preserve the models su-",
  ":Comparison of case-insensitive BLEUin offline scenario among the standard Transformermodel(Vaswani et al., 2017),multi-path wait-kmodel(Elbayad et al., 2020) and ITST(Zhang and Feng,2022a)": "perior performance in high-latency scenarios whileimproving its effectiveness in mid-to-low latencysituations. The training regimen not only utilizesthe conventional translation loss as Equation 1, butalso integrates an innovative loss function, Prefix-to-Full (P2F) loss. P2F loss is designed to translatea source prefix into a complete sentence, with theprefix length l being uniformly distributed and ran-domly chosen. The overall loss is computed asfollows.",
  "Bernoulli(r),(12)": "where r is a hyperparamete to control the pro-portion of the P2F loss. L is the candidate setof the prefix length l, or more specifically, L ={1, 2, ..., |x|}.The P2F loss endows offline translation mod-els with the capability to translate prefixes. Al-though translating prefixes into full target sentencesincreases the risk of hallucinations during the si-multaneous translation process, we posit that aneffective read/write policy can mitigate such occur-rences. For a detailed analysis of experiments onthis, please refer to .1.",
  "Datasets": "WMT2022 ZhEn3. We use a subset with 25Msentence pairs for training4, from which 1500unique sentence pairs are extracted as the valida-tion set. We first tokenize the Chinese and Englishdata using the Jieba Chinese Segmentation Tool5 and Moses6, respectively, and then apply BPE with32000 merge operations. We employ the dev setof 956 sentence pairs from BSTC (Zhang et al.,2021a) as the test set.WMT15 DeEn7. All 4.5M sentence pairs fromthis dataset are used for training, and are tok-enized using 32K BPE merge operations. We usenewstest2013 (3000 sentence pairs) for validationand report results on newstest2015 (2169 sentencepairs).",
  "(c) EnVi, Transformer-Small": ": Comparison of BLUE vs. AL curves between multi-path (abbreviated as Mp) wait-k, ITST, DaP-SiMT,and our proposed PsFuture approach on three language pairs. PsFuture-W and PsFuture-O denote the multi-pathwait-k model based PsFuture method and the offline model (P2F-enhanced) based PsFuture method, respectively. IWSLT15 EnVi8. All 133K sentence pairs fromthis dataset (Luong and Manning, 2015) are usedfor training. We use TED tst2012 (1553 sentencepairs) for validation and TED tst2013 (1268 sen-tence pairs) as the test set. Following the settingsin (Ma et al., 2020), we adopt word-level tokeniza-tion and replace rare tokens (frequency < 5) with<unk>. The vocabulary sizes are 17K for Englishand 7.7K for Vietnamese, respectively.",
  "Settings": "The Pseudo-Future Suffix . In this study, we in-vestigate various pseudo-future suffixes, denotedas xps-suffix, as detailed in . These suffixescan be divided into two categories: fixed and adap-tive. A primary criterion for selecting a fixed suffixis its richness in information. For instance, the<eos> token, often encountered in training trans-lation models, effectively indicates sentence termi-nation. Consequently, all chosen suffixes concludewith <eos> to guarantee an essential incrementof information.Specifically, the fixed suffixes range from the ba-",
  "8nlp.stanford.edu/projects/nmt": "sic <eos> token (suffix 1) to more complex struc-tures involving special tokens (<unk> <eos>, suf-fix 2) and natural sentence extensions (suffixes 3and 4), which simulate ellipsis and ellipsis with sig-nals of information discontinuity. We also conductan experiment with random suffixes to investigatethe sensitivity of the PsFuture method to suffix con-tent. These random suffixes consist of four tokens,each randomly selected from the top 200 most fre-quent tokens in the vocabulary, ensuring adequateinformation. Furthermore, the suffix is resampledrandomly each time a read/write decision occurs.The adaptive suffix is dynamically generatedby large language models, based on the currentsource prefix for pseudo-future information predic-tion. For ZhEn, we employ the Chinese-Llama-2-7b model9, while the Llama-2-7b-chat (Touvronet al., 2023) is used for DeEn and EnVi exper-iments. In the main results (5.3), we empiricallydetermine the optimal suffix through performanceevaluation. .1 delves into the effects ofvarious suffixes on the experimental outcomes, pro-viding a thorough assessment. The Prefix-to-Full Loss Ratio. The hyperparame-ter P2F ratio r is employed to control the propor-tion of the P2F loss. The most effective configura-tions are identified as 0.5, 0.8, and 0.5 for ZhEn,DeEn, and EnVi, respectively. Detailed infor-mation on the ablation studies concerning hyperpa-rameter r is referred to .1.Other Settings. The proposed PsFuture policy un-dergoes empirical experiments based on the multi-path wait-k model and the P2F-enhanced offlinemodel as mentioned in .2, comparing itsperformance with two leading models in the SiMTdomain, ITST (Zhang and Feng, 2022a) and DaP-SiMT (Zhao et al., 2023). All our implementationsare based on the Transformer (Vaswani et al., 2017)architecture and adapted from the Fairseq Library(Ott et al., 2019). For the ZhEn experiments, weutilize the transformer big architecture, while thebase and small architectures are used for DeEnand EnVi experiments respectively.For evaluation, following ITST and DaP-SiMT,we report case-insensitive BLEU (Papineni et al.,2002) scores to assess translation quality and Av-erage Lagging (AL/token) (Ma et al., 2018) tomeasure latency. Regarding the maximum num-ber of continuous read actions in our method, weempirically select the best-performing configura-tions, which are no constraint, 4, no constraint forZhEn, DeEn, EnVi respectively. Further-more, to achieve more robust inference results, theinitial length of the source prefix during the real-time translation process is set to 2.",
  "We compare the proposed PsFuture method againstprevious approaches for three language pairs in": ". PsFuture-W and PsFuture-O refer to themulti-path wait-k model-based PsFuture approachand the offline model (P2F enhanced) based PsFu-ture method, respectively.Firstly, the PsFuture-W experiment significantlysurpasses traditional multi-path wait-k models, ben-efiting from the proposed PsFuture policy over thefixed wait-k policy. Notably, the performance ofPsFuture-W often matches or exceeds the SiMTleading model ITST, which is specifically trainedwith a complicated adaptive read/write policy. Thishighlights the capability of SiMT translation mod-els to make adaptive decisions themselves. Al-though trained with a fixed strategy, the multi-pathwait-k model, when coupled with the zero-shotPsFuture policy, significantly outperforms its coun-terparts and rivals strong SiMT baselines.Secondly,the performance of PsFuture-Odemonstrates improvements over PsFuture-W tovarying extents across all language pairs, especiallyin the ZhEn experiment where it outdoes the for-mer SiMT SOTA method, DaP-SiMT. As antici-pated, the offline translation model, endowed withsuperior feature extraction capabilities, achievesbetter performance at moderate to high latencies,while the introduction of the Prefix-to-Full lossensures the model maintains comparable effective-ness at lower latencies.",
  "Effect of the pseudo-future suffix": "This part investigates the influence of variouspseudo-future suffixes () on the experimentresults. As shown in , the majority of suf-fixes tested can achieve a desirable equilibrium be-tween translation quality and latency, which show-cases the tolerance of the proposed method to thechoice of suffixes. Through the comparison of vari-ous experimental results, it is also feasible to iden-tify specific suffixes for particular language pairsto optimize performance. Adaptive suffixes, gen-erated by large language models, consistently per-form well across various corpora. However, due toa lack of extensive experimentation with differentadaptive suffixes, their effectiveness does not sur-pass that of the best fixed suffixes. We believe thata large-scale exploration of adaptive suffix experi-ments could potentially yield superior outcomes.Additionally, it is surprising that the random suf-fix experiment exhibits unexpectedly strong perfor-mance. Although there are fluctuations in specificareas, the overall result is comparable to that ofother meticulously crafted suffixes. This indicatesthat PsFutures effectiveness is not significantly af-fected by suffix content. This finding indicates thatthe proposed method possesses a substantial lowerbound, emphasizing its robustness and straightfor-ward applicability. These qualities align with themethods key features: simple yet effective.Furthermore, experiments with ground truth suf-fixes are conducted to ascertain the upper bound ofthe PsFuture method. The results indicate that thereremains potential for enhancement. Future effortswill focus on incrementally approaching this upperlimit by exploring and refining suffixes.",
  "Effect of the P2F loss": "illustrates the impact of different Prefix-to-Full (P2F) loss ratios on the performance of ourexperiments. Setting the P2F ratio r to 0 corre-sponds to conventional offline translation modeltraining. This configuration, when applied directlyto SiMT tasks, yields less than ideal results, espe-cially at lower to medium latencies. Incorporatingany level of P2F loss markedly improves perfor-mance, effectively tailoring the offline model forSiMT applications. Moreover, the experimental re-sults reveal a noticeable sensitivity to the P2F ratior, indicating that an optimal r can enhance the bal-ance between translation accuracy and latency.",
  ": Hallucination Rate (HR) vs. Average Lagging(AL) curves comparing PsFuture-O with other methods": "In the PsFuture-O experiment, the additional in-troduction of the Prefix-to-Full (P2F) loss aimsto enhance the models capability to translate asource prefix into a full target sentence, therebyadapting it for SiMT tasks. However, this approachmay increase the risk of hallucinations during thetranslation process. A hallucination is defined asa generated token that cannot be aligned with anysource word. To illustrate this potential issue, wecompare the hallucination rate (Chen et al., 2021)of hypotheses generated by PsFuture-O with thoseproduced by other methods. The comparative re-sults are depicted in .It is evident that, overall, the PsFuture-O ex-periment achieves the lowest hallucination rate,surpassing not only the DaP-SiMT and PsFuture-W methods, which rely on the multi-path wait-kmodel, but also outperforming the meticulouslytrained ITST model. This indicates that the pro-posed PsFuture policy effectively mitigates the oc-currence of hallucinations during the simultaneoustranslation inference process.",
  "Conclusion": "In this paper, we propose the first zero-shot adap-tive read/write policy for SiMT, PsFuture. It em-powers the translation model to autonomously de-cide on read/write actions without requiring ad-ditional training and can attain effectiveness onpar with previously meticulously trained adaptivepolicies. Moreover, we introduce a novel trainingstrategy, Prefix-to-Full (P2F), specifically tailoredto adjust offline translation models for SiMT appli-cations, exploiting the benefits of the bidirectionalattention mechanism inherent in offline models.",
  "Limitations": "In this work, the proposed PsFuture policy con-ducts two forward computations for each read/writedecision-making, which may increase the totalcomputational load when inferring. However, itsimportant to note that while other adaptive pol-icy methods may require only one forward com-putation for each decision, they also necessitateadditional computations, which are also not neg-ligible when compared to single forward comput-ing. Overall, despite the increased computationalrequirement for inference, the PsFuture methodeliminates the need for additional learnable param-eters and training to obtain a read/write decisionmaker, which also significantly reduces computa-tional demands during training.",
  "Acknowledgements": "This work was supported by the National Natu-ral Science Foundation of China (No. 62406114),the Guangzhou Basic and Applied Basic ResearchFoundation (No. 2023A04J1687), the FundamentalResearch Funds for the Central Universities (No.2024ZYGXZR074), the NSFC Young ScientistsFund (No. 62006203), the Research Grants Coun-cil of the Hong Kong Special Administrative Re-gion (No. PolyU/25200821), the Innovation andTechnology Fund (No. PRP/047/22FX), PolyUResearch Centre on Data Science and ArtificialIntelligence (No. 1-CE1E) and a gift fund fromMicroware (No. N-ZDG2).",
  "Dzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Ben-gio. 2015.Neural machine translation by jointlylearning to align and translate. In 3rd InternationalConference on Learning Representations, ICLR2015": "Junkun Chen, Renjie Zheng, Atsuhito Kita, MingboMa, and Liang Huang. 2021. Improving simultane-ous translation by incorporating pseudo-referenceswith fewer reorderings. In Proceedings of the 2021Conference on Empirical Methods in Natural Lan-guage Processing, pages 58575864, Online andPunta Cana, Dominican Republic. Association forComputational Linguistics. Xinjie Chen, Kai Fan, Wei Luo, Linlin Zhang, LiboZhao, Xinggao Liu, and Zhongqiang Huang. 2024.Divergence-guided simultaneous speech translation.In Proceedings of the AAAI Conference on ArtificialIntelligence, pages 1779917807. Fahim Dalvi, Nadir Durrani, Hassan Sajjad, and StephanVogel. 2018.Incremental decoding and trainingmethods for simultaneous translation in neural ma-chine translation. In Proceedings of the 2018 Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, Volume 2 (Short Papers), pages493499, New Orleans, Louisiana. Association forComputational Linguistics.",
  "Minhua Liu. 2008. How do experts interpret. Implica-tions from research in interpreting studies and cogni-tive": "Minh-Thang Luong and Christopher D Manning. 2015.Stanford neural machine translation systems for spo-ken language domains. In Proceedings of the 12thInternational Workshop on Spoken Language Trans-lation: Evaluation Campaign. Mingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng,Kaibo Liu, Baigong Zheng, Chuanqiang Zhang,Zhongjun He, Hairong Liu, Xing Li, et al. 2018.Stacl: Simultaneous translation with implicit antici-pation and controllable latency using prefix-to-prefixframework. arXiv preprint arXiv:1810.08398.",
  "Zhengrui Ma, Shaolei Zhang, Shoutao Guo, ChenzeShao, Min Zhang, and Yang Feng. 2023.Non-autoregressive streaming transformer for simultane-ous translation. arXiv preprint arXiv:2310.14883": "Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,Sam Gross, Nathan Ng, David Grangier, and MichaelAuli. 2019. fairseq: A fast, extensible toolkit forsequence modeling. In Proceedings of the 2019 Con-ference of the North American Chapter of the Associa-tion for Computational Linguistics (Demonstrations),pages 4853, Minneapolis, Minnesota. Associationfor Computational Linguistics. Jianhui Pang, Baosong Yang*, Derek Fai Wong*,Yu Wan, Dayiheng Liu, Lidia Sam Chao, and JunXie. 2024. Rethinking the exploitation of monolin-gual data for low-resource neural machine translation.Computational Linguistics, 50(1):2547.",
  "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.Bleu: A method for automaticevaluation of machine translation. ACL 02, page311318, USA. Association for Computational Lin-guistics": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processingsystems, 30. Ruiqing Zhang, Xiyang Wang, Chuanqiang Zhang,Zhongjun He, Hua Wu, Zhi Li, Haifeng Wang, YingChen, and Qinfei Li. 2021a. Bstc: A large-scalechinese-english speech translation dataset.arXivpreprint arXiv:2104.03575. Shaolei Zhang and Yang Feng. 2022a. Information-transport-based policy for simultaneous translation.In Proceedings of the 2022 Conference on EmpiricalMethods in Natural Language Processing, Onlineand Abu Dhabi. Association for Computational Lin-guistics.",
  "AEffect of The Max Continuous READConstraint": "Following DaP-SiMT (Zhao et al., 2023), we seta constraint on the maximum consecutive reads al-lowed during inference, necessitating a write actiononce this limit is reached. demonstratesthe influence of this hyperparameter on various lan-guage pairings. Consistent with DaP-SiMT, wenote that this parameter exerts little or even neg-ative impact on the ZhEn and EnVi experi-ments, yet it proves substantially advantageous forthe DeEn pair. Thus, it is advisable to identifythe optimal maximum number of continuous readson the validation set before the practical implemen-tation of this approach.",
  "BCase Study": "Here, we present specific cases to demonstrate theeffectiveness of the proposed method, as illustratedin and . It is evident that the Ps-Future policy can effectively align the source andtarget tokens. Even in instances where there is asignificant difference in word order between sourceand target, the PsFuture method can still make cor-rect decisions, waiting for more source informationto proceed with the accurate translation.",
  "CDiscussion on The Extra Cost Causedby Bi-directional Encoders": "During the decoding process, the use of a unidi-rectional encoder allows for incremental decoding,which reduces computational requirements. How-ever, this is not feasible with bidirectional encoders.Compared to unidirectional encoders, predictingeach target token necessitates the additional com-putation of g(t) 1 encoder hidden states (g(t)represents the current number of source tokens).While the extra computational load is affordable forshorter texts, it becomes considerably burdensomefor longer texts, potentially imposing untenablecost. If users cannot accommodate the substantialcomputational demand, they can opt for a unidi-rectional encoder with the PsFuture method, akinto the PsFuture-W experiment mentioned in thispaper which also demonstrates performance com-parable to previous top non-zero-shot read/writepolicies.",
  ": Case No.226 in BSTC ZhEn test set, evaluated with = 0.08": ": Case No.85 in BSTC ZhEn test set, evaluated with = 0.08. While there is a significant difference inword order between source and target, the PsFuture method can still make correct decisions. Specifically, at step 9,the PsFuture Policy reads an additional six tokens in sequence to ensure the accuracy of the translation."
}