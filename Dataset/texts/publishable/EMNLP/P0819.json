{
  "Abstract": "The performance of large language models(LLMs) is significantly influenced by the qual-ity of the prompts provided. In response, re-searchers have developed enormous promptengineering strategies aimed at modifying theprompt text to enhance task performance. Inthis paper, we introduce a novel techniquetermed position engineering, which offers amore efficient way to guide large language mod-els. Unlike prompt engineering, which requiressubstantial effort to modify the text provided toLLMs, position engineering merely involves al-tering the positional information in the promptwithout modifying the text itself. We have eval-uated position engineering in two widely-usedLLM scenarios: retrieval-augmented genera-tion (RAG) and in-context learning (ICL). Ourfindings show that position engineering sub-stantially improves upon the baseline in bothcases. Position engineering thus represents apromising new strategy for exploiting the capa-bilities of large language models.",
  "Introduction": "Recent advancements in Large Language Mod-els (LLMs) have demonstrated significant stridestowards achieving artificial general intelligence.These models exhibit a wide range of capabilities,such as in-context learning (Brown et al., 2020),answering questions based on documents (Lewiset al., 2020; Guu et al., 2020), solving complexmathematical problems (Frieder et al., 2024), andgenerating code (Romera-Paredes et al., 2024; Maet al., 2023).When utilizing LLMs, user prompts are inputted,converted into sequences of tokens, and then pro-cessed through multiple attention layers (Vaswaniet al., 2017). These attention layers employ twotypes of information derived from the token se-quences: (i) Semantic information, where the to-kens are converted into text embeddings, and (ii)Positional information, where the indices of the tokens are converted into positional embeddings(Vaswani et al., 2017; Su et al., 2024). The atten-tion mechanism then combines the semantic andpositional information to predict the distribution ofthe next token in the sequence. Para 1 Sent 1Sent 2 Para 2 Rephrased Para 1 Sent 2 Para 2 Sent 1Sent 3",
  "(b) Position engineering": ": Comparison of prompt engineering and posi-tion engineering. \"Para\" refers to paragraphs, and \"Sent\"to sentences in prompts. Prompt engineering involveseither adding, replacing, or removing paragraphs andsentences from prompts. In contrast, the proposed posi-tion engineering maintains the original prompt text butincorporates placeholder tokens instead. These place-holders are not involved in the computation of attentionscores, thus the computation overhead is not increased.However, they do hold position indices, thereby affect-ing the position information of other tokens in the text. Extensive research has been conducted on mod-ifying prompt text to alter semantic information,aiming to boost task performances. For instance,few-shot prompting is introduced, enabling LLMsto learn new tasks in an in-context manner (Brownet al., 2020).Moreover, the Chain-of-Thoughtmethodology has been introduced to enhanceLLMs reasoning abilities by prompting them toproduce intermediate tokens (Wei et al., 2022; Ko-jima et al., 2022). Additionally, Automatic PromptEngineer has been developed to autonomously de-sign the prompting text for better task-specific per-formance (Zhou et al., 2022).In this study, we investigate the potential ofimproving performance by solely modifying po- sitional information, without any semantic infor-mation change. For the first time, we reveal thatdownstream task performance can be significantlyenhanced by simply adjusting the positional indicesof tokens, without modifying the text itself.As illustrated in , our approach involvesthe introduction of placeholder tokens to modifypositional information. These placeholder tokensdo not contribute to the computation of attentionscores; however, they do occupy token indices.Consequently, the relative position of other to-kens is altered, which could optimize the atten-tion weights among different segments within theprompts. We refer to this approach as positionengineering, highlighting the exclusive focus onmanipulating positional information.We propose a simple yet effective method basedon brutal force to discover the optimal placeholdertoken number for each downstream task, and exper-iment it within two prevalent scenarios of LLMs:Retrieval-Augmented Generation (RAG) and In-Context Learning (ICL). Our method significantlyenhances performance in both tasks, achieving upto a 15.4% absolute increase in accuracy for RAGand a 3.6% absolute increase for ICL. We also dis-cover that the same placeholder number can con-sistently improves the RAGs performance for dif-ferent datasets and models.In all, our contributions can be summarized asfollows:",
  "(2)": "where am,n is a scalar capturing the attention scorebetween m-th token in the query and n-th token inthe value and key sets. d denotes the dimension ofthe attention layer, and om indicates the output forthe m-th query token.Absolute positioning is initially introduced byincorporating a positional embedding vector pn,which is related to m and n (Vaswani et al., 2017):",
  "Altering Position Information in Prompts": "The performance of LLMs is significantly influ-enced by the quality of the prompts used.Toenhance the effectiveness of these prompts, re-searchers have developed a wide range of promptengineering strategies. This refinement process in-volves transforming the initial input tokens {ti}Ni=1into revised inputs {tj} Nj=1, which necessitatesmodifications to the text. For instance, the Zero-shot chain-of-thought technique enhances the rea-soning abilities of LLMs by appending the sentence\"Lets think step by step.\" to the prompts (Kojimaet al., 2022).In this paper, we propose a novel methodologytermed \"position engineering\" to further exploit thecapabilities of LLMs. Unlike prompt engineering,position engineering requires no modification to theinput tokens themselves. Instead, it solely modifiesthe position information utilized in Equation (1).Through empirical experiments, we have discov-ered that such adjustments to position informationcan significantly improve performance. Formally,we aim at discovering a position editing function,() : N N, that boosts LLM performance. Thisfunction changes the token position information,which is incorporated into the model as shown be-low:",
  "(7)": "We impose a condition on that i > j, (i) >(j). This requirement ensures that: (1) No twodistinct tokens are assigned the same new positionindex, and (2) The causality in language modelingremains intact, meaning only query vectors with alarger index can access the key and value vectorswith an equal or smaller index.The concept of position engineering can be alsoexplained through placeholder tokens. Placeholdertokens are defined as tokens that are excluded thecomputation of attention scores, yet they are al-located position indices. To elaborate, when thecalculation of am,n is undertaken as described inthe Equation (2), and either the m-th or n-th tokenis identified as a placeholder, the conventional com-putation is bypassed, and am,n is set to 0. Whileplaceholder tokens do not directly influence theattention scores at their positions, they do alterthe position indices of other input tokens. As de-picted in b, the insertion of placeholder tokens between sentences 1 and 2 affects the rela-tive positional information between them, which inturn influences the calculation of attention scoresbetween tokens of the two sentences. The con-nection between the position editing function andthe placeholder tokens can be described as follows:Employing a position editing function translatesto adding (i + 1) (i) 1 placeholder tokensafter the i-th token, and specifically, adding (0)placeholder tokens before the 0-th token.",
  "Position Engineering": "Consider a particular task defined by (Q, A), forwhich a training set {(Qi, Ai)}Ni=1 has been sam-pled according to the task distribution . We trans-form each question Qi into its corresponding textprompt Pi. A large language model M is utilized,which operates based on the prompt Pi, and itsoutput is evaluated through a scoring function r,denoted as r(M, Pi). To potentially enhance theperformance, a position editing function might beapplied to each question prompt. This function isassumed to be parameterized by a vector , and isdenoted as Pi;. After the application of the po-sitional editing function, a new score is generated,formulated as r(M, Pi, Pi;).For instance, in retrieval-augmented generation(RAG) tasks, the prompt Pi is typically composedof three segments: the instruction, the documents,and the question.It can be possible to define = , while 1 translates to inserting 1placeholder tokens between the instruction and thedocument segment, and 2 translates to insertingplaceholder tokens between the document segmentand the question.Formally, prompt engineering is framed as an op-timization problem. We aim at finding the optimal that maximizes the score:",
  "i=1r(M, Pi, Pi;)(8)": "In this research, we utilize a basic algorithm fortackling the optimization problem by initially defin-ing a limited number of candidates for and assess-ing each candidates score via brute force. Notably,since is a numeric vector, the search processcan be accelerated by adopting various optimizers,such as Gaussian processes of Bayesian optimiza-tion (Srinivas et al., 2010). The exploration of moresophisticated optimization methods will be consid-ered in future works.",
  "Position Engineering for RAG": "Datasets: To explore the effectiveness of posi-tion engineering on RAG tasks, we utilize fouropen-domain QA datasets: NQ open (Lee et al.,2019), EntityQuestions (Sciavolino et al., 2021),TrivialQA (Joshi et al., 2017), and WebQuestions(Berant et al., 2013). These datasets each include atraining and an evaluation (or test) set, with each setcomprising a series of question-and-answer pairs.From the original training set of each dataset, werandomly select 300 QA pairs to serve as our train-ing set for position engineering. Similarly, we ran-domly select 2,000 pairs from their original test setsto constitute our test set. In cases where a datasetlack a test set, we utilize its evaluation set instead.The Contriever model, which has been fine-tunedon the MS-MARCO dataset, is employed as theretrieval model (Izacard et al., 2021). We employdocument passages from Wikipedia as our sourcefor retrieval, with each passage containing a totalof 100 words (Karpukhin et al., 2020). k documentpassages, specifically k = 1, 3, 5, are retrieved, andsubsequently concatenated and fed into LLMs. Ourevaluation metric is the best exact match accuracy,judging whether any correct answer is in the out-put, which is a common practice in previous works(Kandpal et al., 2023; Mallen et al., 2023). Search Space: We adopt the following prompttemplate for all RAG experiments. The prompttemplate is divided into three segments. The firstsegment provides instructions for the task; the sec-ond segment presents a list of retrieved documents,each accompanied by its title and a passage; andthe third segment combines the instruction with aspecific question. These segments are referred toas the instruction segment, the document segment,and the question segment for convenience.",
  "PH Tokens": ": Position Engineering for ICL. In the figure,the term \"PH tokens\" refers to the placeholder tokens in-troduced in .2. We investigate a defined searchspace, with inserting A placeholder tokens between theinstruction and document segments, B placeholder to-kens between the document and question segments, andmid placeholder tokens among the examples. The can-didate value set of A and B is set to {0, 100, ..., 600},and while mid is set to {0, 20, ..., 100}. Results: The results for ICL are presented in Ta-ble 4, indicating an enhancement in performanceacross both datasets, with an absolute 3.6% im-provement observed on the TREC dataset and anabsolute 1.9% improvement on the SST2 dataset.The optimal position settings, represented as A,B, and mid, vary between datasets. Specifically,TREC requires adjusting mid to 40, with A andB set to 0, whereas SST2 requires setting B to100, with A and mid to 0. We observe a significant performance drop whenB is set within the {200, 300, ..., 600} range, mir-roring the trends observed in RAG tasks wherea high B value leads to poor outcomes.Bcan be interpreted as a parameter to adjust theimpact of the example segment. In the case ofSST2, which involves classifying sentiments of re- viewsa domain that LLMs might have commonknowledgethe choice of B = 100 is intendedto slightly reduce the example segments influence.For TREC, which requires LLMs to learn ques-tion types from examples, maintaining B = 0 isoptimal.",
  "Answer the question based on the givendocuments (some of which might beirrelevant). Only give me the answer anddo not output any other words.Question: {question}Answer:": "As presented in , our study exploresthe methodology of position engineering for RAGby strategically inserting A placeholder tokensbetween the instruction and document segments,and B placeholder tokens between the documentand question segments. To narrow down the searchspace, the values of A and B are limited to apredefined set {0, 100, ..., 2500}. Additionally, weimpose a restriction that A + B 2500, dueto the constraints of the context window size. Weevaluate the performance of all combinations onthe training set with the Llama2-13B-chat model,and then apply the best configuration to the test set.Results: displays the results for RAG, in-dicating that position engineering substantially en-",
  "WebQuestions10.3190.473+15.4%1,900500WebQuestions30.4100.507+9.7%2,100400WebQuestions50.4340.514+8.1%1,600800": ": The test results for RAG. We initially examine all possible combinations to determine the optimalconfiguration on the training set, which is denoted as A and B. This optimal configuration is then applied on thetest set, and the results are presented in the table. The baseline is A = B = 0. The term \"Abs Impr.\" representsabsolute accuracy improvement in percentage. The Llama2-13B-chat model is utilized for the experimentation. hance the RAGs performance across all settings.The most notable improvement is 15.4%, observedin the WebQuestions dataset with a single retrieveddocument. The best-performing parameters, Aand B, reveal a consistent trend: A tends to be alarge number, usually in the range of 1,000 to 2,000,while B is a smaller figure, ranging between 200and 600.",
  "It has been observed that the most effective positionconfigurations, represented as A and B in Section": "3.1, demonstrate a consistent trend across all exam-ined datasets. In this section, we aim to determinea single position setting that can enhance RAG per-formance universally across different datasets andvarious numbers of retrieved documents.Given that absolute accuracy scores vary acrossdatasets, we adopt the percentile value of the accu-racy score as a metric to assess each position setting.In this context, we define \"experiment setting\" asthe combination of one dataset and a specific num-ber of retrieved documents, and \"position setting\"as a specific pair of A and B. For every exper-iment setting, we accumulate the scores from allposition settings. The effectiveness of each posi-tion setting is then evaluated based on its percentileranking, which varies from 0 to 100, within theexperiment setting. Finally, The overall efficacyof a position setting is determined by averaging itspercentile rankings across all experiment settings.",
  "B": ": We visualize the average percentile values foreach positional configuration (A, B). These valuesare initially obtained by aggregating all accuracy scoresfor a given dataset and a specific number of retrieveddocuments, and calculate the percentile scores. Subse-quently, they are averaged across all configurations, asdetailed in .2. The baseline configuration without position en-gineering (A = B = 0) achieves an average per-centile of 31.6. This suggests that approximately68% of configurations can surpass the baseline per-formance by simply adjusting positional informa-tion. The visualization of averaged percentiles forall position settings is provided in .Generally, it is advantageous to select a A valuewithin the range of 1300 to 2000, and set B withinthe range of 300 to 500. Setting B to an exces- sively high figure (for instance, more than 1500)significantly deteriorates performance, possibly be-cause it leads to the neglect of document informa-tion in prompts. Moreover, for each specified B,an increase in A is generally associated with betterperformance.On the training set, A = 1900, B = 400 ex-hibits the highest percentile value of 92.9. Weapply this configuration to the test set across alldatasets and retrieved document numbers. Resultspresented in demonstrate that it leads to auniversal performance improvement. In AppendixA.1, we also demonstrate that such configurationremains effective for other models.",
  "Without the instruction segment": "From , it is observed that a larger A ispreferred for optimal performance. A representsthe gap between the instruction segment and thedocument segment. A larger A reduces the in-struction segments impact. This raises the ques-tion of whether eliminating the instruction segmententirely could further enhance performance. Toexplore this, we conduct tests, and the outcomesare presented in . It is discovered that theperformance of removing the instruction segmentis comparable to the baseline setting. The most sig-nificant improvement, a 2% increase, is observedwith the WebQuestions dataset when one retrieveddocument is utilized. However, the enhancement",
  "WebQuestions10.3190.335WebQuestions30.4100.410WebQuestions50.4340.440": ": We test the RAG performance without the in-struction segment on the Llama2-13B-chat model. Theresults are comparable to the baseline, with a slight im-provement ranging from 1% to 2% on the NQ Openand WebQuestions datasets when a single document isretrieved. from position engineering in the same experimentsetting is 15.4%. Thus, to achieve the best perfor-mance, it is essential to lessen but not eliminate theeffect of the instruction segment, a goal that is easyfor position engineering, but difficult to accomplishby prompt engineering.",
  "Position Engineering for ICL": "Datasets: To explore the impact of positional en-gineering on ICL tasks, we employ two datasets:TREC (Li and Roth, 2002; Hovy et al., 2001) andSST2 (Socher et al., 2013). The TREC dataset in-cludes a variety of questions, with the aim beingto categorize these questions into 6 coarse and 50fine-grained question types. We focus on the 6coarse question types. The SST2 dataset containsmovie reviews, with the objective being to cate-gorize these reviews as either positive or negative.For our training set, we randomly choose 300 sam-ples from the original training sets of TREC andSST2. For our test set, we utilize TRECs entire500-sample test set. For the SST2 dataset, due tothe lack of labels in its original test set, we useall 842 samples from its validation set as our testset. For each sample tested, we randomly select 3examples of each label from the training set as thein-context demonstrations, leading to 18 examplesfor TREC and 6 for SST2. The exact match scoreis adopted as the evaluation metric.",
  "TREC0.6920.728+3.6%0400SST20.9150.935+1.9%00100": ": The test results for ICL. We initially examine all possible combinations to determine the optimal configura-tion on the training set, which is denoted as (A, mid, B). This optimal configuration is then applied on the test set,and the results are presented in the table. The baseline is A = mid = B = 0. The term \"Abs Impr.\" representsabsolute accuracy improvement in percentage. The Llama2-13B-chat model is utilized for this experimentation. Search Space: The prompt template provided be-low is designed for evaluating performance on theSST2 dataset and is divided into three sections: aninitial instruction segment that outlines the task,a middle segment that provides examples demon-strating the task, and a final segment that com-bines the instruction with a query. These segmentsare referred to as the instruction segment, the ex-ample segment, and the query segment, respec-tively. For the TREC dataset, we employ a similarprompt template, altering only the terms \"Review\"to \"Question\" and \"Sentiment\" to \"Question Type\"with Llama2-13B-chat.To investigate the impact of position engineer-ing, we conduct experiments by inserting A place-holder tokens between the instruction and examplesegments, B placeholder tokens between the ex-ample segment and the query segment, and midplaceholder tokens among the examples, as de-picted in . The candidate value set ofA and B is set to {0, 100, ..., 600}, and whilemid is set to {0, 20, ..., 100}. We evaluate the per-formance of all possible combinations within thetraining set and apply the optimal configuration tothe test set.",
  "Discussion": "We hypothesize that position engineering serves asa technique to finely adjust the attention weightsassigned to different segments within prompts. Byextending the positional gap between two segments,the interaction between them is lessened, therebyincreasing the attention allocated to other segments.For example, in RAG experiments, an increasedvalue of A could potentially reduce the impact ofthe instruction segment while amplifying the at-tention allocated to the retrieved documents. It isimportant to note, however, that the initial instruc-tion remains essential, as evidenced in .3.Position engineering offers a nuanced approach toadjusting the weights of different blocks withoutthe need for direct addition or removal of text.Position engineering offers several advantages:(i) It is easier to optimize due to its numericalsearch space {}, in contrast to prompt engineer-ing, which requires searching over a more complextext space. (ii) It is computationally efficient, as al-tering position information merely involves updat-ing the position indices input into LLMs, withoutincreasing the overall computational overhead. (iii)It is orthogonal to prompt engineering, meaningthe two approaches can be effectively combined.Future works may advance in the following di-rections. Firstly, investigating the internal dynam-ics of LLMs can enhance our understanding of po-sition engineerings underlying mechanisms. Sec-ondly, employing more sophisticated optimizers,such as Gaussian processes or multi-armed ban-dits, could reduce the search time and discovermore refine-grained position editing functions. Fi-nally, the exploration of merging position engineer-ing with prompt engineering could harness the fullpower of LLMs.",
  "Related Works": "Prompt engineering: Prompt engineering hasemerged as a technique to enhance the performanceof LLMs by modifying the instructions given tothem. For instance, few-shot prompting allowsLLMs to learn from demonstrations, a process also known as in-context learning (Brown et al., 2020).Additionally, Chain-of-Thought prompting encour-ages LLMs to produce intermediate tokens, therebyimproving their reasoning capabilities (Wei et al.,2022; Kojima et al., 2022). Another technique,Retrieval-Augmented Generation (RAG), involvesretrieving relevant document passages and incorpo-rating them into the prompts (Lewis et al., 2020). Ithas been discovered that the RAG performance canbe improved by adding random documents to themix of relevant documents (Cuconasu et al., 2024),a technique that is relevant to our study. How-ever, this approach demands significant additionalcomputational resources. In contrast, our proposedmethod does not require extra computation. Positional Information in LLMs: Positional em-bedding has been introduced to integrate the po-sition information of tokens within the attentionlayers (Vaswani et al., 2017). Initially, this con-cept relied on absolute position indices. However,subsequent developments have introduced methodsbased on relative positions, such as the relative po-sitional encodings in Transformer-XL (Dai et al.,2019), and RoPE (Su et al., 2024). ALiBi is a differ-ent method for integrating positional informationinto LLMs (Press et al., 2022), which does not uti-lize embeddings but introduces a fixed bias basedon relative positions during the computation of at-tention scores. More recent studies have focused onmodifying positional embeddings to increase thecontext window size in LLMs (Ding et al., 2024;Peng et al., 2024). Apart from positional embed-dings, the performance of LLMs has been found tocorrelate with document positions in prompts. InRAG tasks, documents that are positioned in themiddle are often more neglected than those at thebeginning or the end (Liu et al., 2024). However,to the best of our knowledge, there has been nosimilar effort on improving task performance bymodifying positional indices.",
  "Limitations": "Our method needs an explicit search process todiscover the optimal position setting for a giventask. Such search process will cost computationresource and time. Sometimes, the search processcan be omitted if a universal good positional set-ting exists, e.g. the universal setting for RAG taskswith Llama2-13B-chat model. Besides, the inter-nal mechanism of position engineering remainsunclear. We hypothesize that position engineer-ing serves as a technique to finely adjust the atten-tion weights assigned to different segments withinprompts. Future efforts can be made to furtherinvestigate it. Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang. 2013. Semantic parsing on Freebase fromquestion-answer pairs. In Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing, pages 15331544, Seattle, Wash-ington, USA. Association for Computational Linguis-tics. Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish,Alec Radford, Ilya Sutskever, and Dario Amodei.2020. Language models are few-shot learners. In Ad-vances in Neural Information Processing Systems 33:Annual Conference on Neural Information Process-ing Systems 2020, NeurIPS 2020, December 6-12,2020, virtual. Florin Cuconasu, Giovanni Trappolini, Federico Sicil-iano, Simone Filice, Cesare Campagnano, YoelleMaarek, Nicola Tonellotto, and Fabrizio Silvestri.2024. The power of noise: Redefining retrieval forrag systems. In Proceedings of the 47th Interna-tional ACM SIGIR Conference on Research and De-velopment in Information Retrieval, SIGIR 24, page719729, New York, NY, USA. Association for Com-puting Machinery. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-bonell, Quoc Le, and Ruslan Salakhutdinov. 2019.Transformer-XL: Attentive language models beyonda fixed-length context. In Proceedings of the 57thAnnual Meeting of the Association for ComputationalLinguistics, pages 29782988, Florence, Italy. Asso-ciation for Computational Linguistics. Yiran Ding, Li Lyna Zhang, Chengruidong Zhang,Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang,and Mao Yang. 2024. Longrope: Extending llm con-text window beyond 2 million tokens. ArXiv preprint,abs/2402.13753. Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths,Tommaso Salvatori, Thomas Lukasiewicz, PhilippPetersen, and Julius Berner. 2024. Mathematical ca-pabilities of chatgpt. Advances in Neural InformationProcessing Systems, 36. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,and Ming-Wei Chang. 2020. Retrieval augmentedlanguage model pre-training. In Proceedings of the37th International Conference on Machine Learning,ICML 2020, 13-18 July 2020, Virtual Event, volume119 of Proceedings of Machine Learning Research,pages 39293938. PMLR. Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, and Deepak Ravichandran. 2001. Towardsemantics-based answer pinpointing. In Proceedingsof the First International Conference on Human Lan-guage Technology Research. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-bastian Riedel, Piotr Bojanowski, Armand Joulin,and Edouard Grave. 2021. Unsupervised dense in-formation retrieval with contrastive learning. ArXivpreprint, abs/2112.09118. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023. Mistral7b. ArXiv preprint, abs/2310.06825. Mandar Joshi, Eunsol Choi, Daniel Weld, and LukeZettlemoyer. 2017. TriviaQA: A large scale distantlysupervised challenge dataset for reading comprehen-sion. In Proceedings of the 55th Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 16011611, Vancouver,Canada. Association for Computational Linguistics. Nikhil Kandpal, Haikang Deng, Adam Roberts, EricWallace, and Colin Raffel. 2023. Large languagemodels struggle to learn long-tail knowledge. In In-ternational Conference on Machine Learning, pages1569615707. PMLR. Vladimir Karpukhin, Barlas Oguz, Sewon Min, PatrickLewis, Ledell Wu, Sergey Edunov, Danqi Chen, andWen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the2020 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 67696781,Online. Association for Computational Linguistics. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-guage models are zero-shot reasoners. Advances inneural information processing systems, 35:2219922213. Teven Le Scao, Angela Fan, Christopher Akiki, El-lie Pavlick, Suzana Ilic, Daniel Hesslow, RomanCastagn, Alexandra Sasha Luccioni, Franois Yvon,Matthias Gall, et al. 2023.Bloom:A 176b-parameter open-access multilingual language model. Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.2019. Latent retrieval for weakly supervised opendomain question answering. In Proceedings of the57th Annual Meeting of the Association for Computa-tional Linguistics, pages 60866096, Florence, Italy.Association for Computational Linguistics. Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik-tus, Fabio Petroni, Vladimir Karpukhin, NamanGoyal, Heinrich Kttler, Mike Lewis, Wen-tau Yih,Tim Rocktschel, Sebastian Riedel, and DouweKiela. 2020.Retrieval-augmented generation forknowledge-intensive NLP tasks. In Advances in Neu-ral Information Processing Systems 33: Annual Con-ference on Neural Information Processing Systems2020, NeurIPS 2020, December 6-12, 2020, virtual.",
  "Xin Li and Dan Roth. 2002. Learning question clas-sifiers. In COLING 2002: The 19th InternationalConference on Computational Linguistics": "Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-jape, Michele Bevilacqua, Fabio Petroni, and PercyLiang. 2024. Lost in the middle: How language mod-els use long contexts. Transactions of the Associationfor Computational Linguistics, 12:157173. Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, YukeZhu, Linxi Fan, and Anima Anandkumar. 2023. Eu-reka: Human-level reward design via coding largelanguage models. ArXiv preprint, abs/2310.12931. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,Daniel Khashabi, and Hannaneh Hajishirzi. 2023.When not to trust language models: Investigatingeffectiveness of parametric and non-parametric mem-ories. In Proceedings of the 61st Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 98029822, Toronto,Canada. Association for Computational Linguistics. Niklas Muennighoff, Thomas Wang, Lintang Sutawika,Adam Roberts, Stella Biderman, Teven Le Scao,M Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai-ley Schoelkopf, Xiangru Tang, Dragomir Radev,Alham Fikri Aji, Khalid Almubarak, Samuel Al-banie, Zaid Alyafeai, Albert Webson, Edward Raff,and Colin Raffel. 2023.Crosslingual generaliza-tion through multitask finetuning. In Proceedingsof the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 1599116111, Toronto, Canada. Associationfor Computational Linguistics. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and EnricoShippole. 2024. YaRN: Efficient context window ex-tension of large language models. In The TwelfthInternational Conference on Learning Representa-tions. Ofir Press, Noah A. Smith, and Mike Lewis. 2022. Trainshort, test long: Attention with linear biases enablesinput length extrapolation. In The Tenth InternationalConference on Learning Representations, ICLR 2022,Virtual Event, April 25-29, 2022. OpenReview.net. BernardinoRomera-Paredes,MohammadaminBarekatain,Alexander Novikov,Matej Balog,M Pawan Kumar, Emilien Dupont, Francisco JRRuiz, Jordan S Ellenberg, Pengming Wang, OmarFawzi, et al. 2024. Mathematical discoveries fromprogram search with large language models. Nature,625(7995):468475. Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee,and Danqi Chen. 2021. Simple entity-centric ques-tions challenge dense retrievers. In Proceedings ofthe 2021 Conference on Empirical Methods in Natu-ral Language Processing, pages 61386148, Onlineand Punta Cana, Dominican Republic. Associationfor Computational Linguistics. Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Ng, andChristopher Potts. 2013. Recursive deep models forsemantic compositionality over a sentiment treebank.In Proceedings of the 2013 Conference on Empiri-cal Methods in Natural Language Processing, pages16311642, Seattle, Washington, USA. Associationfor Computational Linguistics. Niranjan Srinivas, Andreas Krause, Sham M. Kakade,and Matthias W. Seeger. 2010. Gaussian process op-timization in the bandit setting: No regret and exper-imental design. In Proceedings of the 27th Interna-tional Conference on Machine Learning (ICML-10),June 21-24, 2010, Haifa, Israel, pages 10151022.Omnipress.",
  "Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan,Wen Bo, and Yunfeng Liu. 2024. Roformer: En-hanced transformer with rotary position embedding.Neurocomputing, 568:127063": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, et al. 2023a.Llama:Open and effi-cient foundation language models. ArXiv preprint,abs/2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023b.Llama 2: Open founda-tion and fine-tuned chat models.ArXiv preprint,abs/2307.09288. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N. Gomez, LukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. In Advances in Neural Information Pro-cessing Systems 30: Annual Conference on NeuralInformation Processing Systems 2017, December 4-9,2017, Long Beach, CA, USA, pages 59986008. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in neuralinformation processing systems, 35:2482424837. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,Keiran Paster, Silviu Pitis, Harris Chan, and JimmyBa. 2022. Large language models are human-levelprompt engineers. In NeurIPS 2022 Foundation Mod-els for Decision Making Workshop.",
  "WebQuestions1+18.5%+3.8%WebQuestions3+10.0%+2.2%WebQuestions5+5.9%0.0%": ": We evaluate the universal position configuration for RAG, as identified in .2 with A = 1900 andB = 400, across the test splits of all datasets employing the Llama2-7B-chat and Mistral-7B-instruct-v0.2 models.The results showcase the absolute accuracy improvements over the baseline configuration, where A and B areboth set to 0. In .2, we identified a universal position configuration,A = 1900 and B = 400 , on RAG tasksfor the Llama2-13B-chat model. In this section, we further investigate whether such configuration remainseffective for other models by applying it to the Llama2-7B-chat (Touvron et al., 2023b) and Mistral-7B-instruct-v0.2 (Jiang et al., 2023) model. The configuration is evaluated on the test splits across all datasets,with the results presented in . The findings indicate a consistent enhancement in the performancewith the Llama2-7B-chat model under the universal position configuration. It is noteworthy that thisconfiguration is initially identified with the Llama2-13B-chat model, suggesting that the Llama2-7B-chatmodel exhibits similar positional characteristics with Llama2-13B-chat. Furthermore, the Mistral-7B-instruct-v0.2 model also demonstrates consistent performance improvements when utilizing a singleretrieved document. However, the performance gains become inconsistent with the use of multipleretrieved documents, indicating a potential need for model-specific adjustments.",
  "A.2Applying Position Engineering to Non-RoPE Models": "In our previous evaluation section, Llama2-13B-chat was utilized as the primary model for testing. Thismodel employs RoPE (Su et al., 2024) to integrate positional information. Furthermore, in this section,we aim to assess the effectiveness of position engineering using models with a different method forincorporating positional information. To this end, we apply position engineering to BLOOMZ-7b1(Muennighoff et al., 2023) under the same experimental settings for the ICL tasks. BLOOMZ-7b1 isan instruction-fined version of BLOOM (Le Scao et al., 2023), which incorporates position informationusing ALiBi (Press et al., 2022). Unlike RoPE, ALiBi introduces a fixed position-related bias term duringthe computation of attention scores.Specifically, we follow the search space in for ICL tasks. We determine the optimal positionconfiguration on the training dataset by evaluating all configuration candidates in the search space,subsequently applying this configuration to the test set. Both the training and test sets remain the samewith the previous settings. The results are presented in . Notably, there is a significant improvementin ICL tasks, with the SST2 dataset showing an absolute improvement of 11.0%. It demonstrates thatposition engineering can be also effective in non-RoPE models.",
  "TREC0.7240.782+5.8%00200SST20.8360.946+11.0%020500": ": We apply position engineering to the BLOOMZ-7b1 model on ICL tasks. The same search space setting isemployed as shown in . A, mid, and B is the optimal configuration identified in the training set, whichis then applied on the test set. The baseline is A = mid = B = 0. The term \"Abs Impr.\" represents absoluteaccuracy improvement in percentage compared to the baseline."
}