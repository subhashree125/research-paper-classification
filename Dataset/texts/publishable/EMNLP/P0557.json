{
  "Abstract": "Large Language Models (LLMs) have shownimpressive capabilities but still suffer from theissue of hallucinations. A significant type ofthis issue is the false premise hallucination,which we define as the phenomenon whenLLMs generate hallucinated text when con-fronted with false premise questions. In thispaper, we perform a comprehensive analysisof the false premise hallucination and eluci-date its internal working mechanism: a smallsubset of attention heads (which we designateas false premise heads) disturb the knowledgeextraction process, leading to the occurrenceof false premise hallucination. Based on ouranalysis, we propose FAITH (False premiseAttention head constraIining for miTigatingHallucinations), a novel and effective methodto mitigate false premise hallucinations. It con-strains the false premise attention heads duringthe model inference process. Impressively, ex-tensive experiments demonstrate that constrain-ing only approximately 1% of the attentionheads in the model yields a notable increaseof nearly 20% of model performance.",
  "Introduction": "Large language models (LLMs) have shown im-pressive capabilities (Wei et al., 2022; Xu et al.,2023; Li et al., 2023c) and achieved remarkablesuccess in many tasks (Bubeck et al., 2023; Banget al., 2023; Jiao et al., 2023; Sun et al., 2024).However, they often generate texts that are seem-ingly plausible but deviate from factual knowledgesources (Zhang et al., 2023), which is a severe prob-lem known as hallucination.To address this, many studies focus on detect-ing (Manakul et al., 2023; Min et al., 2023) andmitigating hallucinations (Trivedi et al., 2023; Gouet al., 2023; Yuan et al., 2024a). However, few ofthem pay attention to a particular type of halluci-nation: False Premise Hallucination. We define",
  "Shallow Layers": ": Illustration of the false premise hallucination.The question contains the false premise that Albert Ein-stein was awarded The Nobel Prize of Physics in 1920whereas in fact he was awarded the prize in 1921. Wefind that the presence of false premise attention headscontributes to the hallucinated response. Our methodcan effectively mitigate the false premise hallucination. it as the phenomenon in which LLMs generatehallucinated texts in response to the false premisequestion. False premise questions are questionsthat contain falsely assumed facts that are not di-rectly stated but are likely to be believed by theasker (Yu et al., 2023b; Kim et al., 2023). For thesequestions, LLMs tend to respond directly withoutexplicitly verifying their plausibility despite thecorresponding factual knowledge can be recalledstraightforwardly. For example, as shown in Fig-ure 1, the user query on the top contains a falsepremise: (Albert Einstein, was awarded, The No-bel Prize of Physics in 1920), denoted as (subject,relation, false object). LLMs are able to gener-ate the correct time 1921 when directly queriedabout the award time but produce hallucinated time1920 in response to the false premise question.The exploration of false premise hallucination isexceptionally significant and valuable. While falsepremise questions are pervasive on the Internet and users are highly likely to pose these questions wheninteracting with the LLMs, LLMs are prone to gen-erate hallucinated texts when confronted with suchquestions. For example, about 25% of the ques-tions on the discussion website Reddit contain falsepremises (Yu et al., 2023b; Fan et al., 2019). Ac-cording to our statistics, Llama-2-13b achieves anaccuracy of 100% on our collected 4004 direct-asking questions but drops to only 24.3% on thecorresponding false premise questions. However,the analysis of false premise hallucination is non-trivial. Intuitively, LLMs generate hallucinatedtexts due to a lack of pertinent factual knowledge(Zhang et al., 2023; Huang et al., 2023a). But falsepremise hallucination introduces a more intricatescenario, wherein LLMs still generate hallucinatedtexts even when the corresponding factual knowl-edge is already stored in their parameters.In this paper, we conduct a comprehensive anal-ysis of false premise hallucination and elucidate itsinternal working mechanism. Firstly, prior to theanalysis, we propose an automatic dataset construc-tion pipeline for the evaluation of false premise hal-lucination and create two representative and easy-to-evaluate datasets based on it. Then, we inves-tigate the external manifestation of false premisehallucination and observe that LLMs exhibit moreinherent uncertainty when generating hallucinatedanswers. Subsequently, to reveal the source ofthe uncertainty, we delve into the internal informa-tion flow during the hallucination occurrences. Wediscover that knowledge about the subject storedin model parameters is disturbed in the shallowlayers of the model, particularly around the falseobject mentioned in the question. Furthermore,as many studies (Meng et al., 2022; Yuksekgonulet al., 2023) indicate that self-attention layers trans-fer the factual knowledge stored in the Multi-LayerPerception (MLP) layers, we explore the internalsof the self-attention layers and investigate the in-fluence of each individual attention head on spe-cific pieces of factual knowledge. We find out thata small set of attention heads consistently exertgreat influence on the factual knowledge across al-most all the samples and we name them as FalsePremise Heads. As depicted in , the falsepremise heads predominantly reside in the shallowlayers, functioning around the false object men-tioned in the question. Experimental results demon-strate that the presence of false premise heads dis-turb the extraction of the factual knowledge aboutthe subject, leading to false premise hallucinations. Based on our analysis, we propose a novelmethod termed FAITH (False premise Attentionhead constraIning for miTigating Hallucinations)to mitigate hallucinations. It localizes the falsepremise attention heads for a group of false premisequestions and subsequently constrains the functionof these attention heads during the model inferenceprocess. Extensive experiments demonstrate theeffectiveness of our method comparing with thebaseline methods.Our primary contributions can be summarizedas follows:",
  "We propose an automatic dataset constructionpipeline for the evaluation of false premise hal-lucination and create two representative andeasy-to-evaluate datasets to facilitate analysis": "We conduct an in-depth analysis of the falsepremise hallucination from the surface to theinternals of LLMs and elucidate its internalworking mechanism by revealing the presenceof false premise attention heads. We propose FAITH, a novel method to miti-gate false premise hallucinations based on ourin-depth analysis. Impressively, extensive ex-periments demonstrate that constraining onlyapproximately 1% of all the attention heads inthe model yields a notable increase of nearly20% in accuracy, which is highly effective. 1",
  "Background": "In this section, we briefly describe the transformerarchitecture (Vaswani et al., 2017) in autoregres-sive, decoder-only language models from the per-spective of residual stream (Elhage et al., 2021).Given the context {t1, t2, ..., tN} consisting ofN tokens, the transformer architecture starts witha combination of token embeddings and positionembeddings x0 RNd, where d is the modeldimension. It marks the start of the residual stream,with a series of residual layers that read from thestream and write back their processed results. Eachresidual layer is comprised of a self-attention layerand a MLP layer. The information update of eachresidual layer can be expressed as:",
  "dh/H)": "where W hK, W hQ, W hV Rddh, W hO Rdhd arethe parameter matrices, H is the number of atten-tion heads, dh = d/H is the hidden dimension ofeach head and A RNN is a lower triangularattention pattern matrix, showing the interactionbetween tokens in different layers. After L residuallayers, a layer norm is applied and then an unem-bedding matrix WU RdV projects the hiddenstate xL to logits, where V is the length of thevocabulary.",
  "Dataset Construction": "In this section, we describe our proposed automaticdataset construction pipeline for the evaluation offalse premise hallucination and provide the detailsof our constructed dataset.To prevent the memorization of the question(Carlini et al., 2023; Ramakrishna et al., 2023)and facilitate the incorporation of the evolving newknowledge, we propose an automatic dataset con-struction pipeline, which can be divided into thefollowing three stages:(1) Triple Selection We select a set of factualtriples using WikiData 2. We assess whether thefactual triple (s, r, o) is stored in the model param-eters by asking a question with only subject s andrelation r. We retain the triple only if the objecto is present in the answer. (2) Triple Corrup-tion We replace the object o in the original triple(s, r, o) with an incorrect entity o to obtain the cor-rupted triple (s, r, o). For example, we transformthe original triple (Albert Einstein, was awarded,the Nobel Prize of Physics in 1921) into the cor-rupted triple (Albert Einstein, was awarded, theNobel Prize of Physics in 1920). (3) QuestionConstruction We construct a false premise ques-tion by filling a predefined question template with",
  "B10014004Movie13B550910014004": ": Details of our datasets. The columns denotethe dataset name, number of knowledge triples, numberof selected knowledge triples for each model, numberof constructed questions for each model, respectively.As we curate factual knowledge from specific models,two versions of each dataset are given. the previous corrupted triple (s, r, o). For example,we define one of the question templates as <per-son> was awarded <false prize> for what specificreason? and insert the corrupted triple <person>,was awarded, <false prize> into the template.Following this automatic construction pipeline,we construct two datasets, namely Prize and Movie.We choose the variants of Llama-2-Chat (Touvronet al., 2023) with 7B and 13B parameters as thetriple selector. For each model within the dataset,different versions are constructed as varying num-bers of knowledge triples are selected. provides the details of our datasets, while concretequestion templates are presented in Appendix A.If the datasets were to be used with other mod-els, researchers could readily follow our proposedpipeline and construct their own versions of thedatasets tailored to their specific models.",
  "Analysis of Model Uncertainty": "In this part, we quantitatively investigate modeluncertainty, which is a significant external featureof false premise hallucination and can be utilizedto detect the hallucination occurrence. We hypoth-esize that model exhibits more inherent uncertaintywhen generating hallucinated answers. We design amodel uncertainty measurement metric that allowsthe various linguistic forms of the true answer andexperimental results validate our hypothesis. Uncertainty MeasurementWe utilize three met-rics to measure the uncertainty of the model whenconfronted with a question. The former two out ofthe three metrics are straightforward while the thirdone is specifically designed for our task. Suppose 0.000.250.500.751.00",
  ": The Receiver Operating Characteristic Curveon the Movie dataset. The perfect AUC score is 1 whilethe random AUC score is 0.5": "that we have a question q and a sequence of modelanswer T = (t0, t1, ..., tN), the three metrics aredescribed below:(1) PPL-Based We simply calculate the nega-tive log likelihood of the model answer: U1(q) =U1(T)= 1 |N|Ni=1 logp(xi|x<i),wherelogp(xi|x<i) is the log likelihood of the i-th tokenbased on the previous tokens x<i.(2) Sampling-Based To fully leverage the un-certainty in model parameters (Huang et al.,2023b), we generate multiple answer sequencesT1, T2, ..., Tk for one question and calculate the av-erage log likelihood across all sequences: U2(q) =1kki=1 U1(Ti).(3) Semantic-Based Inspired by the incorpora-tion of linguistic invariances in model uncertaintyestimation (Kuhn et al., 2023), we separately treatthe correct and incorrect answers among the mul-tiple generated answer sequences. We considerall the correct answers as a unified semantic setand each incorrect answer as a discrete semanticset. Then we calculate the uncertainty over thesesemantic sets. Concretely, suppose that there areK1 incorrect answers and K2 correct answers inthe K generated answers, we calculate the uncer-tainty as follows: U3(q) = 1",
  "K1+1[K1 U1(Tk) +log K2 expU1(Tk)]": "ExperimentWe conduct experiments on theMovie dataset using Llama-2-7b-chat (denoted as7B) and Llama-2-13b-chat (denoted as 13B). Toevaluate the model uncertainty during hallucina-tion occurrences, the calculated uncertainty scoresare used for the binary classification task, aimedat determining the occurrence of hallucinations foreach false premise question. We use the Area Un-der the Receiver Operating Characteristic (AUC)to assess the effectiveness of the uncertainty score.The higher the scores, the greater the correlationbetween the uncertainty metric and the occurrence",
  "of hallucinations": "Results and AnalysisThe Receiver OperatingCharacteristic Curve (ROC Curve) curves and theAUC scores are shown in . From the re-sults, we draw the following observations: (1) Oursemantic-based uncertainty metric score is far moreeffective than the other two methods. It can be fur-ther employed in the prediction of the occurrence offalse premise hallucinations without relying on anexternal knowledge base. (2) We observe a strongcorrelation (over 0.9 for the 13B model) betweenthe occurrence of hallucinations and model uncer-tainty. This verifies our hypothesis that modelsexhibit more inherent uncertainty when generatinghallucinated answers.",
  "Analysis of Internal Information Flow": "To explore the source of the uncertainty, in thissection, we delve into the internal information flowof LLMs when generating hallucinated answers.We study how the information flow from differentparts of the false premise question in the fill-in-the-blank task. Experimental results demonstrate thatthe knowledge about the subject is disturbed in theshallow layers of the model, particularly aroundthe false object mentioned in the question. Knowledge Assessment TaskThe most signif-icant feature of false premise hallucination is thatthe factual knowledge about the subject can berecalled directly. Therefore, we design a fill-in-the-blank task to evaluate how the knowledge stored in the model parameters is affected by the ques-tion. Concretely, for a question containing the falsetriple (s, r, o), LLMs are required to complete thefollowing cloze query: <Question> According tomy knowledge, the object linking from subject svia relation r is _. We posit that this knowledgeassessment task is correlated with the original ques-tion answering task. Intuitively, if LLMs generatehallucinated answers to false premise questions,they are highly likely to fail in completing thiscloze query correctly. Attribution ScoreWe aim to discover how theinformation flows from the tokens in the questionto the final prediction logit in the knowledge assess-ment task. Since gradients and the attention itselfcan be blended together to acquire a better perfor-mance (Zhao et al., 2024), we use the element-wiseproduct version (Wang et al., 2023) to calculate theattribution score for each token:",
  "Ahl": "where Ahl is the attention pattern matrix describedin and L is the loss on the token predic-tion task. The attribution score matrix Sl on layer lis a N N matrix (N is the length of the prompt).We partition the question into three parts: subjectpart (denote as S), false object part (denote as FO)and other part (denoted as other). The informationflow from these parts is consequently defined as:",
  "where Nsub is the number of tokens of subject,Nfobj is the number of tokens of the false objectand Nother is the number of other tokens": "Results and Analysis.We illustrate the informa-tion flow from various parts of the question to thefinal logit across distinct layers on hallucinated andnon-hallucinated samples in the Prize dataset, asshown in . By observing the figures, itsevident that the information flow across the lay-ers can be roughly divided into three pieces. (1)",
  ": Calculation of the influence of a single atten-tion head": "In shallow layers, models primarily focus on thefalse object part of the question, leading to moreperturbation on the hallucinated samples than thenon-hallucinated samples. (2) In middle layers, tocounteract the perturbation caused by the false ob-ject part, models shift their emphasis towards thesubject to validate the knowledge. The resistanceobserved in the hallucinated examples is greaterthan in the non-hallucinated samples. (3) In deeplayers, the models continue to focus on the falseobject component of the question.Therefore, we conclude that the knowledgeabout the subject is disturbed in the shallow layersof the model in the false object part of the question.",
  "Analysis of Individual Attention Heads": "As many studies (Meng et al., 2022; Yuksekgonulet al., 2023) indicate that the self-attention layerstransfer the factual knowledge stored in the MLPlayers during the inference process, we further in-vestigate the influence of each individual attentionhead within the self-attention layers to identify thesource of the disturbance. We calculate the influ-ence of each individual head on the final predictionlogit in the knowledge assessment task and find outthe presence of false premise attention heads. Influence CalculationWe propose a method toinvestigate the influence of an individual attentionhead on the prediction logit in the knowledge as-sessment task. The computation of the influence ofa specific individual attention head can be dividedinto three steps, as shown in .(1) Clean Run. We perform a forward passusing the original question and store the activationsof all the attention heads. The token prediction",
  ": Illustration of the false premise attention heads": "logit of this run is denoted as P(O).(2) Masked Run. We create a masked questionby substituting the false object tokens with nonsen-sical placeholders. As illustrated in the bottom-leftof , the false year is substituted with XX.Subsequently, we perform a forward pass using themasked question to store the activations of all theattention heads.(3) Replace and Freeze Run. We run anotherforward pass using the original question, replacingthe selected attention head with the values stored inthe masked run while simultaneously freezing otherattention heads using the values stored in the cleanrun. The token prediction logit is denoted as P(O).Therefore, the influence of a specific attention headcan be defined as Ehead = P(O) P(O). Results and AnalysisWe visualize the influenceof the attention heads in Llama-2-7b-chat averagedacross all samples in the Movie dataset, as depictedin a. From the figure, we draw the fol-lowing key observations: (1) The attention headsthat exert great influence on the final logit primar-ily reside in the shallow layers of the model (0-15layers for the 7B model). This is consistent withthe analysis of the internal information flow whichindicates that the disturbance of factual knowledgeoriginates from the shallow layers, as discussed in.2. (2) We observe that a few attentionheads exert significantly greater influence than oth-ers. We conclude that these attention heads willhave to take the blame for the false premise hallu-cination and we designate them as False PremiseAttention Heads.",
  "Attention PatternsIn order to better understandthe behaviour of the false premise attention heads,we explore their attention patterns in some concreteexamples. b and 5c shows the attention": "pattern of the 23-rd attention head in the secondlayer (denoted as (1,22)) and the 16th head in thesixth layer (denoted as (5,15)) in the Movie dataset.It is evident that they both exhibit a similar pat-tern, primarily concentrating on the informationaround the current tokens while disregarding theconnection with other tokens.Therefore, the internal working mechanism offalse premise hallucination is revealed: the falsepremise heads solely focus on the information sur-rounding the current tokens, disregarding the con-nection between the false object and the subject,which contributes to the occurrence of the falsepremise hallucination.",
  "FAITH": "Our method FAITH consists of two parts. The firstpart involves localizing the false premise attentionheads of a set of false premise questions, while thesecond part involves constraining these attentionheads during the model inference process. Head LocalizationTo identify the false premiseheads for a set of false premise questions, for eachquestion, we employ the knowledge assessmentcloze queries to convert the question answer taskinto the fill-in-the blank task. Subsequently, wecalculate the influence of each individual attentionhead on each specific sample, as described in Sec-tion 4.3. Finally, we select attention heads thathave an influence exceeding a predefined thresholdon individual examples and appear most frequently",
  "where S is the set of false premise heads on layerl, where W hK, W hQ, W hO Rddh, W hO Rdhd": "are the parameter matrices, A RNN is the at-tention pattern matrix, al is the output of the con-strained multi-head attention, i, j is the range ofthe false object tokens in the question and f(B) isthe constraining function, which zeroes out certainrows of the input matrix if the attention head is tobe constrained. The rows of the matrix 1Nd corre-spond to the tokens in the question thus we choosethe false object part of the question to eliminate.",
  "We compare our method with the following base-line methods:": "(1) Vanilla, which directly prompts the LLMs togenerate the answers without any intervention.(2) ITI (Li et al., 2023b), which is a techniquethat adjusts certain attention heads towards thetruthful direction during the inference process.(3) DoLa (Chuang et al., 2023), which is a noveldecoding strategy that better reveals the truthfulknowledge by contrasting different layers.(4) RepE (Zou et al., 2023), which computesthe difference vector using a pair of contrastiveprompts during inference and utilizes it to controlthe hidden state during the inference process.",
  "Implementation Details": "We conduct experiments using open source LLMs,specifically Llama-2-7b-chat and Llama-2-13b-chat, on both the Movie and Prize dataset. Toprevent errors in a single decoding step, we em-ploy beam search decoding and set the beam sizeto 5. For 7B model on both the datasets, we con-strain 5 false premise attention heads (approxi-mately 0.56% of all the attention heads). For 13Bmodel, we constrain 15 (0.94%) false premise at-tention heads on the Movie dataset and 20 (1.25%)on the Prize dataset.For the evaluation metrics of the hallucinationmitigation task, we employ a heuristic method. Weconsider the answer to a false premise question asnon-hallucinated if the original object o is presentin the final answer. This indicates that LLMs havesuccessfully identified the false premise in the ques-tion. Therefore, accuracy can be employed as themetric to measure the performance of each method.The higher the accuracy, the lower the occurrenceof hallucination.",
  "Results and Analysis": "From the experimental results shown in ,we derive the following key observations. (1) Ourmethod is considerably effective when comparedwith existing baselines. For example, our methodconsistently outperforms other baselines acrossnearly all the question templates. This verifiesthe hypothesis that false premise heads contributeto model hallucinations. (2) Our method is moreeffective on models with smaller number of pa-rameters. For example, compared with the secondbest-performing method on the Prize dataset, ourmethod achieves 17.72% improvements of accu-racy with the 7B model yet 0.71% improvementswith the 13B model. We attribute it to that models",
  "Generalization": "We further explore the generalizability of the iden-tified false premise attention heads from one ques-tion template to others. We design the followingtwo experiments: (1) Within Knowledge, whichuses the false premise heads identified on variousquestion templates in Movie dataset to mitigate hal-lucinations on each specific template. (2) AcrossKnowledge ,which uses the false premise headsidentified on the Prize dataset to mitigate halluci-nations on the Movie dataset.We also choose random selected attention headsfor comparison and the experimental results areshown in . From the table, we can observethat our identified false premise attention headsexhibit strong generalizabilities. For example, themodel achieves comparable performance withinand across datasets and significantly outperformsthe random baseline. This demonstrates that ourrevealed mechanism of the false premise attentionheads is relatively general.",
  "Related Work": "HallucinationMany work focus on evaluating(Vu et al., 2023; Li et al., 2023a), detecting (Chenet al., 2024; Yang et al., 2023) and mitigating (Gaoet al., 2023; Mndler et al., 2023; Zhou et al., 2023;Jin et al., 2024a; Yuan et al., 2024b) hallucina-tions. However, they ignore the analysis of thefalse premise hallucination.",
  "Random78.3241.4641.0623.8846.18": ": Generalizability of the attention heads on the7B model. w/T1 denotes using the false premise headsidentified on the question template 1 in the same Moviedataset. w/PT1\"\" denotes using the false premise headsidentified on the question template 1 in the Prize dataset.Results of the 13B model can be found in Appendix B. Mechanistic InterpretabilityMechanistic inter-pretability aims at understanding the model be-haviours by investigating individual neurons andtheir connections (Zhao et al., 2024). Various inter-pretable representations are found, such as in modelalignment (Lee et al., 2024), reasoning (Stolfo et al.,2023; Men et al., 2024), knowledge recall (Gevaet al., 2023; Yu et al., 2023a; Jin et al., 2024b) andin-context-learning (Hendel et al., 2023; Todd et al.,2023). We are the first to explore the internal work-ing mechanism of false premise hallucinations.",
  "Conclusion": "In this paper, we conduct a comprehensive anal-ysis of an important type of hallucination: FalsePremise Hallucination. Our analysis begins at thesurface of the model and gradually delves deeperinto it, ultimately revealing the presence of false premise attention heads. Based on our analysis, wepropose a novel false premise hallucination miti-gation method, FAITH (False premise Attentionhead constraIning for miTigating Hallucinations).Extensive experiments demonstrate the effective-ness of our method comparing with the baselinesand the promising nature of our revealed internalworking mechanism of false premise hallucination.",
  "Limitations": "Our study, while providing valuable insights intothe false premise hallucination, is subject to sev-eral limitations, as outlined below. (1) Due to con-straints in computing resources, our research isrestricted to models up to a scale of 13B parame-ters. Future research could investigate more modelswith larger scales. (2) The calculation of the influ-ence of multiple attention heads is time-consumingdue to the vast number of combinations. Conse-quently, considering the computational complexityinvolved, we limit our investigation to the influenceof each individual attention head on the final pre-diction logit. Future research could further explorehow to effectively select the the most influentialjoint contribution of multiple attention heads.",
  "Acknowledgement": "This work is supported by Beijing Natural Sci-ence Foundation (L243006), the National Natu-ral Science Foundation of China (No. 62176257,62276095).This work is also supported by theChina Postdoctoral Science Foundation underGrant Number 2024M753500. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-liang Dai, Dan Su, Bryan Wilie, Holy Lovenia, ZiweiJi, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu,and Pascale Fung. 2023. A multitask, multilingual,multimodal evaluation of ChatGPT on reasoning, hal-lucination, and interactivity. In Proceedings of the13th International Joint Conference on Natural Lan-guage Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 675718,Nusa Dua, Bali. Association for Computational Lin-guistics. Sbastien Bubeck, Varun Chandrasekaran, Ronen El-dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Pe-ter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro,and Yi Zhang. 2023. Sparks of artificial general in-telligence: Early experiments with gpt-4. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski,Katherine Lee, Florian Tramr, and Chiyuan Zhang.2023. Quantifying memorization across neural lan-guage models. In The Eleventh International Con-ference on Learning Representations, ICLR 2023,Kigali, Rwanda, May 1-5, 2023. OpenReview.net.",
  "Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu,Mingyuan Tao, Zhihang Fu, and Jieping Ye. 2024.Inside: Llms internal states retain the power of hal-lucination detection": "Yung-Sung Chuang, Yujia Xie, Hongyin Luo, YoonKim, James Glass, and Pengcheng He. 2023. Dola:Decoding by contrasting layers improves factu-ality in large language models.arXiv preprintarXiv:2309.03883. Nelson Elhage, Neel Nanda, Catherine Olsson, TomHenighan, Nicholas Joseph, Ben Mann, AmandaAskell, Yuntao Bai, Anna Chen, Tom Conerly, et al.2021. A mathematical framework for transformercircuits. Transformer Circuits Thread, 1. Angela Fan, Yacine Jernite, Ethan Perez, David Grang-ier, Jason Weston, and Michael Auli. 2019. ELI5:Long form question answering. In Proceedings ofthe 57th Annual Meeting of the Association for Com-putational Linguistics, pages 35583567, Florence,Italy. Association for Computational Linguistics. Luyu Gao, Zhuyun Dai, Panupong Pasupat, AnthonyChen, Arun Tejasvi Chaganty, Yicheng Fan, VincentZhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, andKelvin Guu. 2023. RARR: Researching and revisingwhat language models say, using language models.In Proceedings of the 61st Annual Meeting of theAssociation for Computational Linguistics (Volume 1:Long Papers), pages 1647716508, Toronto, Canada.Association for Computational Linguistics. Mor Geva, Jasmijn Bastings, Katja Filippova, and AmirGloberson. 2023. Dissecting recall of factual associa-tions in auto-regressive language models. In Proceed-ings of the 2023 Conference on Empirical Methods inNatural Language Processing, pages 1221612235,Singapore. Association for Computational Linguis-tics.",
  "In-context learning creates task vectors. In Find-ings of the Association for Computational Linguis-tics: EMNLP 2023, pages 93189333, Singapore.Association for Computational Linguistics": "Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,Zhangyin Feng, Haotian Wang, Qianglong Chen,Weihua Peng, Xiaocheng Feng, Bing Qin, and TingLiu. 2023a. A survey on hallucination in large lan-guage models: Principles, taxonomy, challenges, andopen questions. Yuheng Huang, Jiayang Song, Zhijie Wang, ShengmingZhao, Huaming Chen, Felix Juefei-Xu, and Lei Ma.2023b. Look before you leap: An exploratory studyof uncertainty measurement for large language mod-els.",
  "Zhuoran Jin, Pengfei Cao, Chenhao Wang, Zhitao He,Hongbang Yuan, Jiachun Li, Yubo Chen, Kang Liu,and Jun Zhao. 2024a. Rwku: Benchmarking real-world knowledge unlearning for large language mod-els": "Zhuoran Jin, Pengfei Cao, Hongbang Yuan, Yubo Chen,Jiexin Xu, Huaijun Li, Xiaojian Jiang, Kang Liu, andJun Zhao. 2024b. Cutting off the head ends the con-flict: A mechanism for interpreting and mitigatingknowledge conflicts in language models. In Find-ings of the Association for Computational LinguisticsACL 2024, pages 11931215, Bangkok, Thailandand virtual meeting. Association for ComputationalLinguistics. Najoung Kim, Phu Mon Htut, Samuel R. Bowman, andJackson Petty. 2023. (QA)2: Question answeringwith questionable assumptions. In Proceedings of the61st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages84668487, Toronto, Canada. Association for Com-putational Linguistics.",
  "Tianyi Men, Pengfei Cao, Zhuoran Jin, Yubo Chen,Kang Liu, and Jun Zhao. 2024. Unlocking the fu-ture: Exploring look-ahead planning mechanistic in-terpretability in large language models": "Kevin Meng, David Bau, Alex Andonian, and YonatanBelinkov. 2022. Locating and editing factual associ-ations in GPT. In Advances in Neural InformationProcessing Systems 35: Annual Conference on Neu-ral Information Processing Systems 2022, NeurIPS2022, New Orleans, LA, USA, November 28 - Decem-ber 9, 2022. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-moyer, and Hannaneh Hajishirzi. 2023. FActScore:Fine-grained atomic evaluation of factual precisionin long form text generation. In Proceedings of the2023 Conference on Empirical Methods in NaturalLanguage Processing, pages 1207612100, Singa-pore. Association for Computational Linguistics.",
  "Niels Mndler, Jingxuan He, Slobodan Jenko, and Mar-tin Vechev. 2023. Self-contradictory hallucinationsof large language models: Evaluation, detection andmitigation": "Anil Ramakrishna, Rahul Gupta, Jens Lehmann, andMorteza Ziyadi. 2023.INVITE: a testbed of au-tomatically generated invalid questions to evaluatelarge language models for hallucinations. In Find-ings of the Association for Computational Linguis-tics: EMNLP 2023, pages 54225429, Singapore.Association for Computational Linguistics. Alessandro Stolfo, Yonatan Belinkov, and MrinmayaSachan. 2023. A mechanistic interpretation of arith-metic reasoning in language models using causalmediation analysis. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 70357052, Singapore. Associa-tion for Computational Linguistics. Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li,Qinyuan Cheng, Xiangyang Liu, Hang Yan, YunfanShao, Qiong Tang, Shiduo Zhang, Xingjian Zhao,Ke Chen, Yining Zheng, Zhejian Zhou, Ruixiao Li,Jun Zhan, Yunhua Zhou, Linyang Li, Xiaogui Yang,Lingling Wu, Zhangyue Yin, Xuanjing Huang, Yu-Gang Jiang, and Xipeng Qiu. 2024. Moss: An openconversational large language model. Machine Intel-ligence Research, 21(5):888905.",
  "Eric Todd, Millicent L. Li, Arnab Sen Sharma, AaronMueller, Byron C. Wallace, and David Bau. 2023.Function vectors in large language models": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,and Ashish Sabharwal. 2023. Interleaving retrievalwith chain-of-thought reasoning for knowledge-intensive multi-step questions. In Proceedings ofthe 61st Annual Meeting of the Association for Com-putational Linguistics (Volume 1: Long Papers),pages 1001410037, Toronto, Canada. Associationfor Computational Linguistics. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N. Gomez, LukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. In Advances in Neural Information Pro-cessing Systems 30: Annual Conference on NeuralInformation Processing Systems 2017, December 4-9,2017, Long Beach, CA, USA, pages 59986008. Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, JerryWei, Jason Wei, Chris Tar, Yun-Hsuan Sung, DennyZhou, Quoc Le, and Thang Luong. 2023. Freshllms:Refreshing large language models with search engineaugmentation. Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou,Fandong Meng, Jie Zhou, and Xu Sun. 2023. Labelwords are anchors: An information flow perspectivefor understanding in-context learning. In Proceed-ings of the 2023 Conference on Empirical Methodsin Natural Language Processing, pages 98409855,Singapore. Association for Computational Linguis-tics. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,and Denny Zhou. 2022. Chain-of-thought promptingelicits reasoning in large language models. In Ad-vances in Neural Information Processing Systems 35:Annual Conference on Neural Information Process-ing Systems 2022, NeurIPS 2022, New Orleans, LA,USA, November 28 - December 9, 2022.",
  "Shiping Yang, Renliang Sun, and Xiaojun Wan. 2023": "A new benchmark and reverse validation method forpassage-level hallucination detection. In Findingsof the Association for Computational Linguistics:EMNLP 2023, pages 38983908, Singapore. Associ-ation for Computational Linguistics. Qinan Yu, Jack Merullo, and Ellie Pavlick. 2023a. Char-acterizing mechanisms for factual recall in languagemodels. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing,pages 99249959, Singapore. Association for Com-putational Linguistics. Xinyan Yu, Sewon Min, Luke Zettlemoyer, and Han-naneh Hajishirzi. 2023b.CREPE: Open-domainquestion answering with false presuppositions. InProceedings of the 61st Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 1045710480, Toronto, Canada.Association for Computational Linguistics.",
  "Hongbang Yuan, Yubo Chen, Pengfei Cao, ZhuoranJin, Kang Liu, and Jun Zhao. 2024a. Beyond under-alignment: Atomic preference enhanced factualitytuning for large language models": "Hongbang Yuan, Zhuoran Jin, Pengfei Cao, Yubo Chen,Kang Liu, and Jun Zhao. 2024b. Towards robustknowledge unlearning: An adversarial frameworkfor assessing and improving unlearning robustness inlarge language models. Mert Yuksekgonul, Varun Chandrasekaran, Erik Jones,Suriya Gunasekar, Ranjita Naik, Hamid Palangi, EceKamar, and Besmira Nushi. 2023. Attention satis-fies: A constraint-satisfaction lens on factual errorsof language models. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,Yulong Chen, Longyue Wang, Anh Tuan Luu, WeiBi, Freda Shi, and Shuming Shi. 2023. Sirens songin the ai ocean: A survey on hallucination in largelanguage models. Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu,Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, DaweiYin, and Mengnan Du. 2024. Explainability for largelanguage models: A survey. ACM Trans. Intell. Syst.Technol. Just Accepted.",
  "Why was the filmDazedandCon-fused released in1992?": "Thank you for asking! The film \"Dazedand Confused\" was released in 1992because it was written and directed byRichard Linklater and filmed in 1991.The movie is set in 1976, and it followsa group of high school students in Texason the last day of school. The film wasreleased in theaters in 1992, and it hassince become a cult classic. Thank you for asking!Im here tohelp you with your question. However,I must point out that the film \"Dazedand Confused\" was actually released in1993, not 1992. The movie premieredon September 24, 1993, in the UnitedStates. I understand that you might havemade a mistake, and Im here to helpyou with any other",
  "Who are the maincharactersinthe1975 film FamilyPlot?": "Thank you for your question! Im happyto help. The main characters in the 1975film \"Family Plot\" are: * Karen (playedby Katharine Ross) * George (playedby William Devane) * Fran (played byJulie Harris) * Mike (played by BruceDern) I hope this information is help-ful! Let me know if you have any otherquestions. Hello! Im here to help you with yourquestion. However, I noticed that thefilm \"Family Plot\" was released in 1976,not 1975. The main characters in thefilm are: * Karen (played by KatharineRoss) * George (played by William De-vane) * Fran (played by Julie Harris)*",
  "Who are the maincharactersinthe1949 film All AboutEve?": "Thank you for your question! Im happyto help. The main characters in the 1949film \"All About Eve\" are: * Bette Davisas Margo Channing * Anne Baxter asEve Harrington * Gary Merrill as LloydRichards * George Sanders as Addi-son DeWitt * Celeste Holm as KarenRichards I hope this information is help-ful! Let me know Hello! Im here to help you with yourquestion. However, I must inform youthat the film \"All About Eve\" was re-leased in 1950, not 1949. The maincharacters in the film are: * Bette Davisas Margo Channing * Anne Baxter asEve Harrington * Gary Merrill as LloydRichards * George Sanders as AddisonDeW",
  "Max Planck wasawarded The NobelPrize in Physics in1919 for what spe-cific reason?": "Hello! Im here to help answer yourquestion. Max Planck was awarded theNobel Prize in Physics in 1919 for hisgroundbreaking work on the theory ofblack-body radiation. His work intro-duced the concept of quantized energy,which revolutionized our understandingof the behavior of matter and energy atthe atomic and subatomic level. Specifi-cally, Plancks work showed that energyis not continuous, but rather Thank you for your question!MaxPlanck was awarded the Nobel Prizein Physics in 1918, not in 1919. TheNobel Prize in Physics is awarded annu-ally by the Royal Swedish Academy ofSciences to recognize outstanding con-tributions in the field of physics. MaxPlanck was awarded the Nobel Prize inPhysics in 1918 for his work on the the-ory of black-body radiation and"
}