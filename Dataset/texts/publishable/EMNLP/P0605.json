{
  "Abstract": "Decompilation aims to convert binary code tohigh-level source code, but traditional tools likeGhidra often produce results that are difficultto read and execute. Motivated by the advance-ments in Large Language Models (LLMs),we propose LLM4Decompile, the first andlargest open-source LLM series (1.3B to 33B)trained to decompile binary code. We opti-mize the LLM training process and introducethe LLM4Decompile-End models to decompilebinary directly. The resulting models signifi-cantly outperform GPT-4o and Ghidra on theHumanEval and ExeBench benchmarks over100% in terms of re-executability rate. Addi-tionally, we improve the standard refinementapproach to fine-tune the LLM4Decompile-Ref models, enabling them to effectively re-fine the decompiled code from Ghidra andachieve a further 16.2% improvement overthe LLM4Decompile-End. LLM4Decompile1 demonstrates the potential of LLMs to revolu-tionize binary code decompilation, deliveringremarkable improvements in readability and ex-ecutability while complementing conventionaltools for optimal results.",
  "Introduction": "Decompilation, the reverse process of convertingmachine code or binary code into a high-levelprogramming language, facilitates various reverseengineering tasks such as vulnerability identifica-tion, malware research, and legacy software mi-gration (Brumley et al., 2013; Katz et al., 2018;Hosseini and Dolan-Gavitt, 2022; Xu et al., 2023;Armengol-Estap et al., 2023; Jiang et al., 2023;Wong et al., 2023; Hu et al., 2024). Decompilationis challenging due to the loss of information inher-ent in the compilation process, particularly finer de-tails such as variable names (Lacomis et al., 2019)",
  ": Illustration of compiling source code to binary,disassembling binary to assembly code (ASM), anddecompiling ASM to pseudo-code with Ghidra. Thepseudo-code is hard to read and not executable": "and fundamental structures like loops and condi-tionals (Wei et al., 2007). To address these chal-lenges, numerous tools have been developed for de-compilation, with Ghidra (Ghidra, 2024a) and IDAPro (Hex-Rays, 2024) being the most commonlyused. Although these tools have the capability to re-vert binary code to high-level pseudo-code, the out-puts often lack readability and re-executability (Liuand Wang, 2020a; Wang et al., 2017), which areessential for applications like legacy software mi-gration and security instrumentation tasks (Wonget al., 2023; Dinesh et al., 2020). illustrates the transformation from thesource C code to a binary file, assembly code(ASM), and pseudo-code decompiled from Ghidra.In this pseudo-code, the original nested for struc-ture is replaced with a less intuitive combination ofa do-while loop inside another while loop. Fur-thermore, array indexing like num[i] is decom-piled into complicated pointer arithmetic such as*(float *)(param_2 + (long)local_24 * 4).The decompiled output also exhibits syntactical er-rors, with the function return type being convertedto undefined4. Overall, traditional decompilationtools often strip away the syntactic clarity provided by high-level languages and do not ensure the cor-rectness of syntax, posing significant challengeseven for skilled developers to reconstruct the algo-rithmic logic (Wong et al., 2023; Hu et al., 2024).Recent advancements in Large Language Mod-els (LLMs) have greatly improved the processof decompiling code. There are two primary ap-proaches to LLM-based decompilationRefined-Decompile and End2end-Decompile. In particular,Refined-Decompile prompts LLMs to refine the re-sults from traditional decompilation tools (Hu et al.,2024; Wong et al., 2023; Xu et al., 2023). However,LLMs are primarily optimized for high-level pro-gramming languages and may not be as effectivewith binary data. End2end-Decompile fine-tunesLLMs to decompile binaries directly. Nevertheless,previous open-source applications of this approachwere limited by the use of smaller models withonly around 200 million parameters and restrictedtraining corpus (Hosseini and Dolan-Gavitt, 2022;Armengol-Estap et al., 2023; Jiang et al., 2023), Incontrast, utilizing larger models trained on broaderdatasets has proven to substantially improve theperformance (Hoffmann et al., 2024; Kaplan et al.,2020; Rozire et al., 2023; OpenAI, 2023).To address the limitations of previous studies,we propose LLM4Decompile, the first and largestopen-source LLM series with sizes ranging from1.3B to 33B parameters specifically trained to de-compile binary code. To the best of our knowl-edge, theres no previous study attempts to im-prove the capability of LLM-based decompila-tion in such depth or incorporate such large-scaleLLMs.Based on the End2end-Decompile ap-proach, we introduce three critical steps: data aug-mentation, data cleaning, and two-stage training, tooptimize the LLM training process and introducethe LLM4Decompile-End models to decompile bi-nary directly. Specifically, our LLM4Decompile-End-6.7B model demonstrates a successful decom-pilation rate of 45.4% on HumanEval (Chen et al.,2021) and 18.0% on ExeBench (Armengol-Estapet al., 2022), far exceeding Ghidra (Ghidra, 2024a)or GPT-4o (OpenAI, 2023) by over 100%. Addi-tionally, we improve the Refined-Decompile strat-egy by examining the efficiency of Ghidras decom-pilation process, augmenting and filtering data tofine-tune the LLM4Decompile-Ref models, whichexcel at refining Ghidras output. Experiments sug-gest a higher performance ceiling for the enhancedRefined-Decompile approach, with 16.2% improve-ment over LLM4Decompile-End. Additionally, we assess the risks associated with the potential misuseof our model under obfuscation conditions com-monly used in software protection. Our findingsindicate that neither our approach nor Ghidra caneffectively decompile obfuscated code, mitigatingconcerns about unauthorized use for infringementof intellectual property.In summary, our contributions are as follows:",
  "We introduce the LLM4Decompile series, thefirst and largest open-source LLMs (ranging from1.3B to 33B parameters) fine-tuned on 15 billiontokens for decompilation": "We optimize the LLM training process and in-troduce LLM4Decompile-End models, which seta new performance standard of direct binary de-compilation, significantly surpassing GPT-4o andGhidra by over 100% in terms of re-executabilityon the HumanEval and ExeBench benchmarks. We improve the Refined-Decompile approachto fine-tune the LLM4Decompile-Ref models,enabling them to effectively refine the decom-piled results from Ghidra and achieve fur-ther 16.2% re-executability enhancements overLLM4Decompile-End.",
  "Related Work": "The practice of reversing executable binaries totheir source code form, known as decompilation,has been researched for decades (Miecznikowskiand Hendren, 2002; Nolan, 2012; Katz et al., 2019).Traditional decompilation relies on analyzing thecontrol and data flows of program (Brumley et al.,2013), and employing pattern matching, as seen intools like Hex-Rays Ida pro (Hex-Rays, 2024) andGhidra (Ghidra, 2024a). These systems attemptto identify patterns within a programs control-flow graph (CFG) that corresponding to standardprogramming constructs such as conditional state-ments or loops. However, the output from suchdecompilation processes tends to be a source-code-like representation of assembly code, includingdirect translations of variables to registers, use ofgotos, and other low-level operations instead ofthe original high-level language constructs. Thisoutput, while often functionally similar to the orig-inal code, is difficult to understand and may not bere-executable (Liu and Wang, 2020b; Wong et al.,2023). Drawing inspiration from neural machinetranslation, researchers have reformulated decompi-lation as a translation exercise, converting machine- level instructions into readable source code (Katzet al., 2019). Initial attempts in this area utilizedrecurrent neural networks (RNNs) (Katz et al.,2018) for decompilation, complemented by error-correction techniques to enhance the outcomes. Motivated by the success of Large LanguageModels (Li et al., 2023; Rozire et al., 2023; Guoet al., 2024), researchers have employed LLMs fordecompilation, primarily through two approachesRefined-Decompile and End2end-Decompile. Inparticular, Refined-Decompile prompts the LLMsto refine results from traditional decompilationtools like Ghidra or IDA Pro.For instance,DeGPT (Hu et al., 2024) enhances Ghidras read-ability by reducing cognitive load by 24.4%, whileDecGPT (Wong et al., 2023) increases IDA Prosre-executability rate to over 75% by integrating er-ror messages into its refinement process. Theseapproaches, however, largely ignore the fact thatLLMs are designed primarily for high-level pro-gramming languages (Li et al., 2023; Rozire et al.,2023; Guo et al., 2024), and their effectivenesswith binary files is not well-established. End2end-Decompile, on the other hand, fine-tunes LLMsto decompile binaries directly. Early open-sourcemodels like BTC (Hosseini and Dolan-Gavitt,2022) and recent development Slade (Armengol-Estap et al., 2023) adopt the language model witharound 200 million parameters (Lewis et al., 2020a)to fine-tune for decompilation. While Nova (Jianget al., 2023), which is not open-sourced, devel-ops a binary LLM with 1 billion parameters andfine-tunes it for decompilation. Consequently, thelargest open-source model in this domain is limitedto 200M. Whereas utilizing larger models trainedon broader datasets has proven to substantially im-prove the performance (Hoffmann et al., 2024; Ka-plan et al., 2020; Rozire et al., 2023). Therefore, our objective is to present the firstand most extensive open-source LLM4Decompileseries, aiming at comprehensively advancing thedecompilation capability of LLMs. Initially, weoptimize the End2end-Decompile approach to trainthe LLM4Decompile-End, demonstrating its effec-tiveness in directly decompiling binary files. Subse-quently, we enhance the Refined-Decompile frame-works to integrate LLMs with Ghidra, augmentingtraditional tools for optimal effectiveness.",
  "In this section, we describe the general End2end-Decompileframework,andpresentdetailson our strategy to optimize the training ofLLM4Decompile-End models": "3.1.1The End2End-Decompile Framework illustrates the End2end-Decompile frame-work from compilation to decompilation processes.During compilation, the Preprocessor processes thesource code (SRC) to eliminate comments and ex-pand macros or includes. The cleaned code is thenforwarded to the Compiler, which converts it intoassembly code (ASM). This ASM is transformedinto binary code (0s and 1s) by the Assembler.The Linker finalizes the process by linking func-tion calls to create an executable file. Decompila-tion, on the other hand, involves converting binarycode back into a source file. LLMs, being trainedon text, lack the ability to process binary data di-rectly. Therefore, binaries must be disassembledby Objdump into assembly language (ASM) first.It should be noted that binary and disassembledASM are equivalent, they can be interconverted,",
  "We optimize the training of LLM4Decompile-EndModels through three key steps: 1) augmentingthe training corpus, 2) improving the quality of thedata, 3) and incorporating two-state training": "Training Corpus.As indicated by the Scaling-Law (Hoffmann et al., 2024; Kaplan et al., 2020),the effectiveness of an LLM heavily relies on thesize of the training corpus. Consequently, our ini-tial step in training optimization involves incorpo-rating a large training corpus. We construct asm-source pairs based on ExeBench (Armengol-Estapet al., 2022), which is the largest public collectionof five million C functions. To further expand thetraining data, we consider the compilation opti-mization states frequently used by developers. Thecompilation optimization involves techniques likeeliminating redundant instructions, better registerallocation, and loop transformations (Muchnick,1997), which perfectly acts as data augmentationfor decompilation. The key optimization levels areO0 (default, no optimization) to O3 (aggressiveoptimizations). We compile the source code intoall four stages, i.e., O0, O1, O2, and O3, and paireach of them with the source code. Data Quality.Data quality is critical in trainingan effective model (Li et al., 2023). Therefore, oursecond step is to clean our training set. We followthe guidelines of StarCoder (Li et al., 2023) bycomputing MinHash (Broder, 2000) for the codeand utilizing Locally Sensitive Hashing (LSH) toremove duplicates. We also exclude samples thatare less than 10 tokens. Two-Stage Training.Our final step for trainingoptimization aims to educate the model with bi-nary knowledge, and includes two-stage training.In the first stage, we train the model with a largecorpus of compilable but not linkable (executable)data. Note that its significantly easier to extract Ccode that is compilable but not linkable (da Silvaet al., 2021; Armengol-Estap et al., 2022). Suchnot-executable binary object code will closely re-semble its executable version except it lacks linkedaddresses for external symbols. Therefore, in thefirst stage, we use the extensive compilable codesto ground our model in binary knowledge. In the",
  "LLM4Decompile-Ref": "We now examine how the conventional decompi-lation tool, Ghidra, can be significantly improvedby integrating it with LLMs. Note that our ap-proach aims at refining entire outputs from Ghidra,offering a broader strategy than merely recover-ing names or types (Nitin et al., 2021; Xu et al.,2024). We begin by detailing the general Refined-Decompile framework, and discuss our strategy toenhance Ghidras output by LLM4Decompile-Ref.",
  "The Refined-Decompile Framework": "The Refined-Decompile approach is shown in Fig-ure 3. This approach differs from that in only in terms of the LLMs input, which in thecase of Refined-Decompile comes from Ghidrasdecompilation output. Specifically, Ghidra is usedto decompile the binary, and then the LLM is fine-tuned to enhance Ghidras output. While Ghidraproduces high-level pseudo-code that may sufferfrom readability issues and syntax errors, it effec-tively preserves the underlying logic. Refining thispseudo-code significantly mitigates the challengesassociated with understanding the obscure ASM.",
  "Decompiling using Ghidra.Decompiling theexecutable code with Ghidra () is time-consuming due to the complex nature of the ex-ecutables in ExeBench, which include numerous": "external functions and IO wrappers. Ghidra Head-less (Ghidra, 2024b) requires 2 seconds per sampleusing 128-core multiprocessing. Given such a highcomputational load, and the high similarities be-tween non-executable and executable binaries, wechoose to decompile the non-executable files usingGhidra. This choice significantly reduces the timeto 0.2 seconds per sample, enabling us to efficientlygather large amounts of training data. OptimizationStrategies.SimilartoSec-tion 3.1.2, we augment our dataset by compilingwith optimization levels O0, O1, O2, and O3. Wefurther filter the dataset using LSH to removeduplicates. As shown in , Ghidra oftengenerates overly long pseudo-code. Consequently,we filter out any samples that exceed the maximumlength accepted by our model.",
  "Experimental Setups": "Experimental Datasets.The training data isconstructed using ExeBench, with Ghidra Head-less (Ghidra, 2024b) employed to decompile thebinary object file. Due to constraints in computa-tional resources, only 400K functions each withoptimization levels from O0 to O3 (1.6M samples,1B tokens) are used for training and the evaluationis conducted on HumanEval-Decompile. The mod-els are trained using the same template describedin .1.1. In addition, following previouswork (Hosseini and Dolan-Gavitt, 2022; Armengol-Estap et al., 2023), we access the readability of de-compiled results in terms of Edit Similarity score.",
  "Where the loss is calculated only for the outputsequence xi...xj, or the source code": "Baselines.We selected two key baselines forcomparison. First, GPT-4o (OpenAI, 2023) rep-resents the most capable LLMs, providing an upperbound on LLM performance. Second, DeepSeek-Coder (Guo et al., 2024) is selected as the cur-rent SOTA open-source Code LLM. It representsthe forefront of publicly available models specifi-cally tailored for coding tasks. While recent work 2Exebench provides comparison of the test set against theGitHub population using nine distinct code complexity met-rics, confirming that the characteristics of the testing functionsare aligned with functions in larger real-world projects.",
  ": Ablation study on training dataset. The Compilable models are trained on 7.2M non-executable functions,while the Executable models are trained on 1.6M executable functions": "Slade (Armengol-Estap et al., 2023) fine-tuneslanguage model for decompilation, it relies on in-termediate compiler outputs, specifically, the *.sfiles. In practice, however, such intermediate filesare rarely released by developers. Therefore, wefocus on a more realistic approach, and considerdecompilation only from the binaries, for furtherdiscussions please refer to Appendix B. Implementation.We use the DeepSeek-Codermodels obtained from Hugging Face (Wolf et al.,2019).We train our models using LLaMA-Factory library (Zheng et al., 2024). For 1.3Band 6.7B models, we set a batch size = 2048and learning rate = 2e5 and train the mod-els for 2 epochs (15B tokens). Experiments areperformed on NVIDIA A100-80GB GPU clusters.Fine-tuning the 1.3B and 6.7B LLM4Decompile-End takes 12 and 61 days on 8A100 respectively.Limited by the resources, for the 33B model weonly train for 200M tokens. For evaluation, weuse the vllm (Kwon et al., 2023) to accelerate thegeneration (decompilation) process. We employgreedy decoding to minimize randomness.",
  "Experimental Results": "TheresultsforthebaselinesandRefined-Decompile approaches are summarized in .For the pseudo-code decompiled by Ghidra, whichis not optimized for re-execution, only an averageof 20.1% of them pass the test cases. GPT-4o as-sists in refining this pseudo-code and enhancingits quality. The LLM4Decompile-Ref models offersubstantial improvements over Ghidras outputs,with the 6.7B model yielding a 160% increase inre-executability. Similar to the discussion in Sec-tion 4.1.2, the 33B model outperforms the 1.3Bmodel even though it used considerably less train-ing data. And it achieves performance that is only3.6% below the 6.7B model, which benefited fromten times more training data. When compared toLLM4Decompile-End-6.7B, the LLM4Decompile-Ref-6.7B model, though trained on just 10% ofthe data in LLM4Decompile-Ref models, shows a16.2% performance increase, suggesting a greaterpotential for the Refined-Decompile approach. Wepresent further analysis in Appendix D.",
  "LLM4Decompile-End-6.7B68.0539.5136.7137.2045.3715.5712.9212.9312.6913.53": "GhidraBase34.7616.4615.2414.0220.126.996.136.195.476.20+GPT-4o46.9534.1528.6631.1035.226.605.635.674.995.72+LLM4Decompile-Ref-1.3B68.9037.2040.8537.2046.0415.1713.2512.9212.6713.50+LLM4Decompile-Ref-6.7B74.3946.9547.5642.0752.7415.5913.5313.4212.7313.82+LLM4Decompile-Ref-33B*70.7347.5643.9041.4650.9115.4013.7913.6313.0713.97+LLM4Decompile-Ref-22B*80.4958.5459.7657.9364.1815.1914.0413.5813.4013.85 :Main comparison of Refined-Decompile approaches for re-executability rate and Edit Similar-ity on HumanEval-Decompile benchmark.+GPT-4o refers to enhance the Ghidra results with GPT-4o,+LLM4Decompile-Ref means refining Ghidra results with the fine-tuned LLM4Decompile-Ref models. Note thatthe 33B model was trained using only 200M tokens, which is just 10% of the tokens used for the 1.3B/6.7B/22Bmodel. For the 22B model, Please refer to Appendix D. function calls, which is similar in text distribu-tion to the HumanEval-Decompile data, consistingof single functions dependent only on standard Clibraries. Consequently, the 6.7B model trainedonly on compilable data successfully decompiled37.5% of HumanEval-Decompile functions, butonly 6.8% on ExeBench, which features real func-tions with extensive user-defined functions. Onthe other hand, the 6.7B model trained solely onexecutable data achieved a 26.7% re-executabilityrate on the ExeBench test set but faced challengeswith single functions, with only a 23.8% successrate on HumanEval-Decompile due to the smallersize of the training corpus. Limited by the space,we present further analysis in Appendix C.",
  "Implementation.Configuration settings for themodel are consistent with those in .1.1.For the 1.3B and 6.7B models, the fine-tuningprocess involves 2B tokens in 2 epochs, and re-": "quires 2 and 8 days respectively on 8 A100.Limited by the resource, for 33B model we onlytrain for 200M tokens. For evaluation, we firstaccess the re-executability rate of Ghidra to estab-lish a baseline. Subsequently, GPT-4o is used toenhance Ghidras decompilation result with theprompt, GeneratelinuxcompilableC/C++codeofthemainandotherfunctionsinthesuppliedsnippetwithoutusinggoto, fix any missing headers.Do notexplain anything., following DecGPT (Wonget al., 2023). Finally, we use LLM4Decompile-Refmodels to refine the Ghidras output.",
  "GPT-4o": ": Decompilation results of different approaches.GPT-4o output is plausible yet fail to recover the arraydimension (incorrect 2D array arr[outer][inner]).Ghidras pseudo-code is notably less readable asdiscussed in .GPT-refined Ghidra re-sult (Ghidra+GPT-4o) marginally enhances readabil-ity but fails to correctly render for loops and ar-ray indexing. Conversely, LLM4Decompile-End andLLM4Decompile-Ref produce accurate and easy-to-read outputs. An analysis of readability across different meth-ods is also conducted and presented in with illustrative examples presented in .For text similarity, all decompiled outputs divergefrom the original source code, with Edit Similarityranging from 5.7% to 14.0%, primarily becausethe compilation process removes variable namesand optimizes the logic structure. Ghidra generatespseudo-code that is particularly less readable with6.2% Edit Similarity on average. Interestingly, withrefinement from GPT (Ghidra+GPT-4o), there is amarginal decrease in Edit Similarity. GPT assistsin refining type errors like undefined4 and ulong(). However, it struggles to accurately re-construct for loops and array indexing. In contrast,both LLM4Decompile-End and LLM4Decompile-Ref generate outputs that are more aligned with theformat of the source code and easier to comprehend. To summarize, domain-specific fine-tuning is cru-cial for enhancing re-executability and readabilityof decompilation outputs.We further employed GPT-4o to evaluate read-ability (Wang et al., 2023; Liu et al., 2023). Specif-ically, we guide GPT to assess syntax similarity(variables, loops, conditions) and structural in-tegrity (logic flow, structure) using a structuredtemplate. We then summarize readability with ascore from 1 (Poor) to 5 (Excellent), based on de-tailed comparisons between original and decom-piled code. The template is available on our GitHubrepository3. summarizes our readabilityassessments on HumanEval-Decompile across var-ious models and optimization levels.",
  ": Evaluation by GPT-4o on the readability ofdecompiled results from various methods": "Compared with the results in , it indicatesthat Edit Similarity (ES) follows a trend similar toGPT evaluation. Although ES is mathematicallybased, its values can be difficult to interpret. For in-stance, a 15 ES score obtained by LLM4Decompilemodel may seem low, yet the decompiled functionand the source code are highly aligned. In contrast,GPT evaluation, which measures readability con-ceptually, is more intuitive. A score of 4 on theGPT scale suggests that the decompiled code isnearly identical to the original. Nonetheless, thesescores are derived from GPTs \"subjective\" judg-ments. Combining insights from both ES and GPT-Eval could lead to a more thorough assessment ofcode readability.",
  "Obfuscation Discussion": "The process of decompilation aims at revealing thesource code from binaries distributed by develop-ers, presenting a potential threat to the protectionof intellectual property. To resolve the ethical con-cerns, this section accesses the risks of the possiblemisuse of our decompilation models.In software development, engineers typically im-plement obfuscation techniques before releasingbinary files to the public (Lachaux et al., 2021;Junod et al., 2015). This is done to protect the",
  ": Re-executability rates of different approaches on the HumanEval-Decompile benchmark under obfuscations.Compared to , the decompilation success rates significantly drop for over 70%": "software from unauthorized analysis or modifica-tion. In our study, we focus on two fundamentalobfuscation techniques as suggested in Obfuscator-LLVM (Junod et al., 2015): Control Flow Flatten-ing (CFF) and Bogus Control Flow (BCF). Thesetechniques are designed to disguise the true logic ofthe software, thereby making decompilation morechallenging to protect the softwares intellectualproperty. We present the details of these two tech-niques in the Appendix E.Results summarized in demonstrate thatbasic conventional obfuscation techniques are suffi-cient to prevent both Ghidra and LLM4Decompilefrom decoding obfuscated binaries. For example,the decompilation success rate for the most ad-vanced model, LLM4Decompile-Ref-6.7B, dropssignificantly for 90.2% (0.5274 to 0.0519) underCFF and 78.0% (0.5274 to 0.1159) under BCF.Considering the industry standard of employingseveral complex obfuscation methods prior to soft-ware release, experimental results in mit-igate the concerns about unauthorized use for in-fringement of intellectual property.",
  "Conclusions": "We propose LLM4Decompile, the first and largestopen-source LLM series with sizes ranging from1.3B to 33B trained to decompile binary code.Based on the End2end-Decompile approach, weoptimize the LLM training process and introducethe LLM4Decompile-End models to decompile bi-nary directly. The resulting 6.7B model shows adecompilation accuracy of 45.4% on HumanEvaland 18.0% on ExeBench, surpassing existing toolslike Ghidra and GPT-4o over 100%. Addition-ally, we improve the Refined-Decompile strategy tofine-tune the LLM4Decompile-Ref models, whichexcel at refining the Ghidras output, with 16.2%improvement over LLM4Decompile-End. Finally,we conduct obfuscation experiments and addressconcerns regarding the misuse of LLM4Decompilemodels for infringement of intellectual property.",
  "Limitations": "The scope of this research is limited to the com-pilation and decompilation of C language target-ing the x86 platform. While we are confident thatthe methodologies developed here could be eas-ily adapted to other programming languages andplatforms, these potential extensions have been re-served for future investigation. Furthermore, Ourresearch is limited by financial constraints, with abudget equivalent to using 8 A100 GPUs for oneyear, which includes all trials and iterations. Asa result, we have only managed to fully fine-tunemodels up to 6.7B, and conducted initial explo-rations on the 33B models with a small dataset,leaving the exploration of 70B and larger modelsto future studies. Nonetheless, our preliminarytests confirm the potential advantages of scaling upmodel sizes and suggest a promising direction forfuture decompilation research into larger models.",
  "Ethic Statement": "We have evaluated the risks of the possible mis-use of our decompilation models in .Basic obfuscation methods such as Control FlowFlattening and Bogus Control Flow have beenempirically tested and proven to protect againstunauthorized decompilation by both traditionaltools like Ghidra and advanced models likeLLM4Decompile. This built-in limitation ensuresthat while LLM4Decompile is a powerful tool forlegitimate uses, it does not facilitate the infringe-ment of intellectual property.In practical applications in the industry, softwaredevelopers typically employ a series of complex ob-fuscation methods before releasing their software.This practice adds an additional layer of securityand intellectual property protection against decom-pilation. LLM4Decompiles design and intendeduse respect these measures, ensuring that it servesas an aid in legal and ethical scenarios, such as un-derstanding legacy code or enhancing cybersecurity defenses, rather than undermining them.ThedevelopmentanddeploymentofLLM4Decompileareguidedbystrictethi-cal standards. The model is primarily intended foruse in scenarios where permission has been grantedor where the software is not protected by copyright.This includes academic research,debugging,learning, and situations where companies seek torecover lost source code of their own software. This work is partially supported by the Na-tional Natural Science Foundation of China (No.62372220), the Research Grants Council of theHong Kong Special Administrative Region, China(Project No. PolyU/25200821), the NSFC YoungScientists Fund (Project No.62006203), theInnovation and Technology Fund (Project No.PRP/047/22FX), and PolyU Internal Fund fromRC-DSAI (Project No. 1-CE1E).",
  "Andrei Z Broder. 2000. Identifying and filtering near-duplicate documents. In Annual symposium on com-binatorial pattern matching, pages 110. Springer": "David Brumley, JongHyup Lee, Edward J. Schwartz,and Maverick Woo. 2013. Native x86 decompila-tion using semantics-preserving structural analysisand iterative control-flow structuring. In Proceedingsof the 22th USENIX Security Symposium, Washing-ton, DC, USA, August 14-16, 2013, pages 353368.USENIX Association. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,Henrique Pond de Oliveira Pinto, Jared Kaplan,Harrison Edwards, Yuri Burda, Nicholas Joseph,Greg Brockman, Alex Ray, Raul Puri, GretchenKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-try, Pamela Mishkin, Brooke Chan, Scott Gray,Nick Ryder, Mikhail Pavlov, Alethea Power, LukaszKaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cum-mings, Matthias Plappert, Fotios Chantzis, Eliza-beth Barnes, Ariel Herbert-Voss, William HebgenGuss, Alex Nichol, Alex Paino, Nikolas Tezak, JieTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,William Saunders, Christopher Hesse, Andrew N.Carr, Jan Leike, Joshua Achiam, Vedant Misra, EvanMorikawa, Alec Radford, Matthew Knight, MilesBrundage, Mira Murati, Katie Mayer, Peter Welinder,Bob McGrew, Dario Amodei, Sam McCandlish, IlyaSutskever, and Wojciech Zaremba. 2021. Evaluat-ing large language models trained on code. CoRR,abs/2107.03374. Chris Cummins, Volker Seeker, Dejan Grubisic, Bap-tiste Roziere, Jonas Gehring, Gabriel Synnaeve, andHugh Leather. 2024. Meta large language modelcompiler: Foundation models of compiler optimiza-tion. arXiv preprint arXiv:2407.02524. Anderson Faustino da Silva, Bruno Conde Kind,Jos Wesley de Souza Magalhes, Jernimo NunesRocha, Breno Campos Ferreira Guimares, andFernando Magno Quinto Pereira. 2021.ANG-HABENCH: A suite with one million compilable Cbenchmarks for code-size reduction. In IEEE/ACMInternational Symposium on Code Generation andOptimization, CGO 2021, Seoul, South Korea, Febru-ary 27 - March 3, 2021, pages 378390. IEEE. Sushant Dinesh, Nathan Burow, Dongyan Xu, and Math-ias Payer. 2020. Retrowrite: Statically instrument-ing cots binaries for fuzzing and sanitization. In2020 IEEE Symposium on Security and Privacy (SP),pages 14971511.",
  "Hex-Rays. 2024.Ida pro: a cross-platform multi-processor disassembler and debugger": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,Elena Buchatskaya, Trevor Cai, Eliza Rutherford,Diego de Las Casas, Lisa Anne Hendricks, JohannesWelbl, Aidan Clark, Tom Hennigan, Eric Noland,Katie Millican, George van den Driessche, BogdanDamoc, Aurelia Guy, Simon Osindero, Karen Si-monyan, Erich Elsen, Oriol Vinyals, Jack W. Rae,and Laurent Sifre. 2024. Training compute-optimallarge language models. In Proceedings of the 36thInternational Conference on Neural Information Pro-cessing Systems, NIPS 22, Red Hook, NY, USA.Curran Associates Inc.",
  "Nan Jiang, Chengxiao Wang, Kevin Liu, XiangzheXu, Lin Tan, and Xiangyu Zhang. 2023. Nova+:Generative language models for binaries.CoRR,abs/2311.13721": "Pascal Junod, Julien Rinaldini, Johan Wehrli, and JulieMichielin. 2015.Obfuscator-LLVM softwareprotection for the masses.In Proceedings of theIEEE/ACM 1st International Workshop on SoftwareProtection, SPRO15, Firenze, Italy, May 19th, 2015,pages 39. IEEE. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.Brown, Benjamin Chess, Rewon Child, Scott Gray,Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.Scaling laws for neural language models. Preprint,arXiv:2001.08361. Deborah S. Katz, Jason Ruchti, and Eric M. Schulte.2018. Using recurrent neural networks for decompi-lation. In 25th International Conference on SoftwareAnalysis, Evolution and Reengineering, SANER 2018,Campobasso, Italy, March 20-23, 2018, pages 346356. IEEE Computer Society.",
  "Omer Katz, Yuval Olshaker, Yoav Goldberg, and EranYahav. 2019. Towards neural decompilation. ArXiv,abs/1905.08325": "Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, YingSheng, Lianmin Zheng, Cody Hao Yu, Joseph E.Gonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-cient memory management for large language modelserving with pagedattention. In Proceedings of theACM SIGOPS 29th Symposium on Operating SystemsPrinciples. Marie-AnneLachaux,BaptisteRoziere,MarcSzafraniec, and Guillaume Lample. 2021. Dobf: Adeobfuscation pre-training objective for program-ming languages. Advances in Neural InformationProcessing Systems, 34:1496714979. Jeremy Lacomis, Pengcheng Yin, Edward J. Schwartz,Miltiadis Allamanis, Claire Le Goues, Graham Neu-big, and Bogdan Vasilescu. 2019. DIRE: A neuralapproach to decompiled identifier naming. In 34thIEEE/ACM International Conference on AutomatedSoftware Engineering, ASE 2019, San Diego, CA,USA, November 11-15, 2019, pages 628639. IEEE. Chris Lattner and Vikram Adve. 2004. Llvm: A com-pilation framework for lifelong program analysis& transformation. In International symposium oncode generation and optimization, 2004. CGO 2004.,pages 7586. IEEE. Mike Lewis, Yinhan Liu, Naman Goyal, MarjanGhazvininejad, Abdelrahman Mohamed, Omer Levy,Veselin Stoyanov, and Luke Zettlemoyer. 2020a.BART: Denoising sequence-to-sequence pre-trainingfor natural language generation, translation, and com-prehension. In Proceedings of the 58th Annual Meet-ing of the Association for Computational Linguistics,pages 78717880, Online. Association for Computa-tional Linguistics. Patrick Lewis, Ethan Perez, Aleksandra Piktus, FabioPetroni, Vladimir Karpukhin, Naman Goyal, Hein-rich Kttler, Mike Lewis, Wen-tau Yih, Tim Rock-tschel, et al. 2020b. Retrieval-augmented generationfor knowledge-intensive nlp tasks. Advances in Neu-ral Information Processing Systems, 33:94599474. Raymond Li, Loubna Ben Allal, Yangtian Zi, NiklasMuennighoff, Denis Kocetkov, Chenghao Mou, MarcMarone, Christopher Akiki, Jia Li, Jenny Chim,Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo,Thomas Wang, Olivier Dehaene, Mishig Davaadorj,Joel Lamy-Poirier, Joo Monteiro, Oleh Shliazhko,Nicolas Gontier, Nicholas Meade, Armel Zebaze,Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu,Benjamin Lipkin, Muhtasham Oblokulov, ZhiruoWang, Rudra Murthy, Jason Stillerman, Siva SankalpPatel, Dmitry Abulkhanov, Marco Zocca, Manan Dey,Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya,Wenhao Yu, Swayam Singh, Sasha Luccioni, PauloVillegas, Maxim Kunakov, Fedor Zhdanov, ManuelRomero, Tony Lee, Nadav Timor, Jennifer Ding,Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, TriDao, Mayank Mishra, Alex Gu, Jennifer Robinson,Carolyn Jane Anderson, Brendan Dolan-Gavitt, Dan-ish Contractor, Siva Reddy, Daniel Fried, DzmitryBahdanau, Yacine Jernite, Carlos Muoz Ferrandis,Sean Hughes, Thomas Wolf, Arjun Guha, Leandrovon Werra, and Harm de Vries. 2023. Starcoder: maythe source be with you! Preprint, arXiv:2305.06161. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,Ruochen Xu, and Chenguang Zhu. 2023. G-eval:NLG evaluation using gpt-4 with better human align-ment. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing,pages 25112522, Singapore. Association for Com-putational Linguistics. Zhibo Liu and Shuai Wang. 2020a. How far we havecome: testing decompilation correctness of c decom-pilers. In Proceedings of the 29th ACM SIGSOFTInternational Symposium on Software Testing andAnalysis, ISSTA 2020, page 475487, New York,NY, USA. Association for Computing Machinery. Zhibo Liu and Shuai Wang. 2020b. How far we havecome: testing decompilation correctness of C decom-pilers. In ISSTA 20: 29th ACM SIGSOFT Interna-tional Symposium on Software Testing and Analysis,Virtual Event, USA, July 18-22, 2020, pages 475487.ACM.",
  "OpenAI. 2023.GPT-4 technical report.CoRR,abs/2303.08774": "Baptiste Rozire, Jonas Gehring, Fabian Gloeckle, StenSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,Jingyu Liu, Tal Remez, Jrmy Rapin, ArtyomKozhevnikov, Ivan Evtimov, Joanna Bitton, Man-ish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori,Wenhan Xiong, Alexandre Dfossez, Jade Copet,Faisal Azhar, Hugo Touvron, Louis Martin, Nico-las Usunier, Thomas Scialom, and Gabriel Synnaeve.2023. Code llama: Open foundation models for code.CoRR, abs/2308.12950.",
  "Ruoyu Wang, Yan Shoshitaishvili, Antonio Bianchi,Aravind Machiry, John Grosen, Paul Grosen, Christo-pher Kruegel, and Giovanni Vigna. 2017. Ramblr:Making reassembly great again. In NDSS": "Tao Wei, Jian Mao, Wei Zou, and Yu Chen. 2007. Anew algorithm for identifying loops in decompilation.In Static Analysis, 14th International Symposium,SAS 2007, Kongens Lyngby, Denmark, August 22-24,2007, Proceedings, volume 4634 of Lecture Notes inComputer Science, pages 170183. Springer. Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz,and Jamie Brew. 2019. Huggingfaces transformers:State-of-the-art natural language processing. CoRR,abs/1910.03771.",
  "Wai Kin Wong, Huaijin Wang, Zongjie Li, Zhibo Liu,Shuai Wang, Qiyi Tang, Sen Nie, and Shi Wu. 2023.Refining decompiled C code with large languagemodels. CoRR, abs/2310.06530": "Xiangzhe Xu, Zhuo Zhang, Shiwei Feng, Yapeng Ye,Zian Su, Nan Jiang, Siyuan Cheng, Lin Tan, andXiangyu Zhang. 2023. Lmpa: Improving decompila-tion by synergy of large language model and programanalysis. CoRR, abs/2306.02546. Xiangzhe Xu, Zhuo Zhang, Zian Su, Ziyang Huang,Shiwei Feng, Yapeng Ye, Nan Jiang, DanningXie, Siyuan Cheng, Lin Tan, and Xiangyu Zhang.2024.Leveraging generative models to recovervariable names from stripped binary.Preprint,arXiv:2306.02546.",
  "AExeBench Setups": "For every sample in ExeBenchs executable splits,assembly code from *.s filea compilers interme-diate output as discussed in .1 and Fig-ure 1is required to compile the sample into abinary. The specific compilation settings and pro-cessing details, however, are not provided by theirauthors. Consequently, we choose to compile thecode in a standard way and manage to compile onlyhalf of the samples. This leaves us with 443K outof 797K samples for the executable training set and2621 out of 5000 samples for the executable testset. Accordingly, we train our model on the 443Ksamples and conduct the re-executability evalua-tion on these 2621 samples, the results are shownin .TheresearchersfromSlade(Armengol-Estapetal.,2023),whoalsodevelopedExeBench(Armengol-Estapetal.,2022),have published their decompilation findingson ExeBench.They chose to decompile theintermediate output, or assembly code from *.sfile, directly without further compilation intobinaries, where in practice, such intermediateoutput is rarely released by software developers.Their reported results, as seen in , showa significant difference from ours. Their versionof ChatGPT achieved a re-executability rate of22.2% and an edit similarity of 44.0% under O0optimization.On the other hand, our GPT-4omodel only reached a 4.4% re-executability rateand a 7.9% edit similarity. The approach taken bySlade involves settings not commonly available inpractical decompilation scenarios, which explainswhy their results vary significantly from ours. Weadhere to a more realistic setting, decompiling",
  ":Re-executability and Edit Similarity onExebench": "To further illustrate our settings, of-fers an example where the source function includesspecific user-defined types like Ltc4151State,Ltc4151, and device. However, these types arecompletely lost after compilation, i.e., no informa-tion related to these user-definitions can be foundin the binary (disassembled ASM code). Conse-quently, GPT-4o is unable to reconstruct these typesbased purely on the ASM (the realistic setting), butconverting them to default types int or pointer,producing non-executable code. This issue waspervasive across the ExeBench test set, leading tothe failure of GPT-4o models in decompiling theExeBench samples in a realistic setting.",
  ": Re-executability rate with the growth of inputlength. The 6.7B model is more robust against inputlength": "illustrates that the re-executability ratedecreases as the input length increases, and there isa marked decline in performance at higher levels ofcode optimization, highlighting the difficulties indecompiling long and highly optimized sequences.Importantly, the performance difference betweenthe 1.3B and 6.7B models showcased in the figureemphasizes the advantages of larger models in such",
  ": Types of errors identified in the two bench-marks: LLM4Decomile-End-6.7B faces issues with log-ical errors in HumanEval-Decompile and user-definedcomponents in ExeBench": "tasks. Larger models, with their expanded compu-tational resources and deeper learning capabilities,are inherently better at resolving the challengesposed by complex decompilations.The error analysis presented in forLLM4Decompile-End-6.7B indicates that logicalerrors are prevalent in the HumanEval-Decompilescenarios, with 64% of errors due to assertions thatthe decompiled codes do not pass. In the ExeBenchdataset, which features real functions with user-defined structures and types, the major challengesare related to reclaiming these user-specific com-ponents. Where 50% of the errors come from un-declared functions, and 28% from improper use ofstructures. Given that these user-defined details aretypically lost during the compilation process, re-constructing them can be particularly challenging.Integrating techniques like Retrieval AugmentedGeneration (Lewis et al., 2020b) might supplementthe decompilation process with necessary externalinformation.",
  "DData Quality, Volume and Model": "Data QualityIn this project, we intentionallylimited our data preprocessing to classical tech-niques such as filtering short texts and removingduplicates. This approach was chosen to estab-lish a fair baseline model for decompilation thatminimizes potential biases, aiming to provide abroad, unrefined baseline model that reflects di-verse scenarios. We acknowledge that selectivedata removal, specifically excluding data incompat-ible with standard C libraries (Free Software Foun-dation, 2024), can enhance performance, as evi-denced in with Decompile-Eval, whichonly relies on standard C libraries. While refiningthe dataset can lead to improved performance, ourprimary goal in this study was to set a foundationalbaseline for the community. We believe this base-",
  ": Performance of LLM4Decompile-Ref-1.3B onHumanEval-Decompile w.r.t. different training epochs": "Moreover, we have included results from the6.7B model to address scaling issues and providedcomparison with the 1.3B and 33B models in Ta-ble 10. Notably, the 6.7B model achieves com-parable performance to the 1.3B model with only20% of the data, and the 33B model reaches similaroutcomes to the 6.7B with just 10% of the data, al-though these ratios may differ with varying datasets.",
  "ModelChoosing the right base model for de-compilation training significantly influences perfor-": "mance. Our first choice, Deepseek-Coder-6.7B, de-livered an encouraging average re-executability rateof 52.74% on the HumanEval-Decompile bench-mark. Conversely, LLM-Compiler-7B (Cumminset al., 2024), trained to compile source code intoLLVM IR (Lattner and Adve, 2004)the oppositeof decompilationserved as a more effective foun-dation, enhancing performance by 3.5% comparedto Deepseek-Coder-6.7B. Additionally, Yi-Coder-9B (01-AI, 2024), introduced in September 2024as the current state-of-the-art model, markedly im-proved decompilation training results by 23.1%.Furthermore, CodeStral-22B (Mistral-AI, 2024),benefiting from its larger architecture, provided a21.7% improvement over smaller models.",
  "We provide the details of two classic obfuscationtechniques suggested in Obfuscator-LLVM (Junodet al., 2015)": "Control Flow Flattening.It enhances the secu-rity of software by transforming its straightforward,hierarchical control flow into a more complex, flat-tened structure. The workflow involves breaking afunction into basic blocks, arranging these blocksat the same level, and encapsulating them within aswitch statement inside a loop. Bogus Control Flow.It modifies a functions ex-ecution sequence by inserting an additional basicblock prior to the existing one. This added blockincludes an opaque predicate, followed by a con-ditional jump that leads back to the original block.Additionally, the original basic block is pollutedwith randomly selected, meaningless instructions."
}