{
  "Introduction": "Vision-Language Models (VLMs) have shownemergent capabilities through large-scale trainingthat have made them gain popularity in recent years.VLMs show promising results across various visionand language tasks, from image captioning to vi-sual question answering and cross-modal retrievaland grounding. A key component contributing totheir strong performance across the board is thescale of their pre-training datasets. However, theselarge-scale datasets tend to predominantly containimages from Western cultures (Shankar et al., 2017;Ananthram et al., 2024). The underrepresentationof certain cultures in the data translates into perfor-mance disparities across cultures. (De Vries et al.,2019; Gustafson et al., 2023).",
  "Several benchmarks and datasets have been pro-": "posed to test the cultural inclusivity of VLMs.These include testing the models performance onquestions pertaining to images from certain cul-tures (Liu et al., 2021a; Yin et al., 2021), on theirability to adapt images from one culture to another(Khanuja et al., 2024), or on stereotypical depictionof various cultures (Jha et al., 2024). Nonetheless,existing benchmarks address a limited set of cul-tures (5-7), leaving a substantial representationalgap. Moreover, current benchmarks leave out acrucial aspect: assessing the cultural diversity inthe representation of universal concepts.",
  "The rst task, retrieval across universals, cov-": "ers images from 50 countries across 10 regions. Itassesses the ability of VLMs to retrieve culturally-diverse images pertaining to textual prompts ofuniversal concepts such as breakfast and wed-ding. In addition to the standard precision@kmetric, which veries that the retrieved imagescorrectly depict the target concept, we also pro-pose a new metric, diversity@k, that measures thecultural-diversity among the retrieved images, al-lowing us to identify models bias towards speciccountries or regions.",
  "Extensive evaluation on 7 models for the re-": "trieval task and 5 models for the grounding taskreveals discrepancies across cultures, reassessingndings by prior work (e.g., Liu et al., 2021a; Yinet al., 2021). We further analyze whether VLMs ex-hibit biases towards certain cultures. In the ground-ing task, the performance on North America andEurope is substantially higher than on East Asiaand South East Asia. This preference is inconsis-tent across universals in the retrieval task, e.g., amodel may retrieve European images of funeralsbut African images of farming. A closer look re-veals that when models retrieve seemingly diverseimages, they often share Western elements, such aseggs for breakfast, white dresses at weddings.",
  "The Geo-Diversity Problem.Existing large-": "scale vision and language datasets are imbalancedin their representation of different regions, over-representing the West (Shankar et al., 2017). Asa result, models trained on these datasets mayexhibit discrepancies in performance when intro-duced with inputs concerning various demographicand geographic factors (e.g. Gustafson et al., 2023; De Vries et al., 2019). For instance, image gen-eration modelswhen asked to generate imagesof universal concepts such as house, tend to de-pict the concept as it appears in the US or India,cultures that are more prominently featured in thetraining data (Basu et al., 2023).",
  "To serve users from diverse cultures fairly, it is": "imperative to collect large-scale datasets from di-verse data sources (Kim et al., 2021; Goyal et al.,2022). Two recent geo-diverse image datasets thatare popular for training geo-diverse VLMs, Dol-lar Street (Rojas et al., 2022) and GeoDE (Ra-maswamy et al., 2024), focus on common house-hold items, lacking coverage of more abstract andculture-specic concepts. Finally, to make cross-cultural data collection more feasible, researchersproposed to apply domain adaptation (Kalluri et al.,2023) and active learning (Ignat et al., 2024) basedon visual similarity.",
  "Geo-Diverse Benchmarks.With the understand-": "ing that language has a social function, there hasbeen growing interest in the NLP community inmaking models more culturally inclusive (e.g., Her-shcovich et al., 2022; Nguyen et al., 2023; Bha-tia and Shwartz, 2023). Several benchmarks havebeen developed to test language models culturalawareness with respect to values and social norms(Durmus et al., 2023), culinary norms (Palta andRudinger, 2023), gurative language (Kabra et al.,2023), and more.",
  "In the multimodal domain, benchmarks have": "been developed to test VLMs on visual question an-swering and reasoning (Liu et al., 2021a; Yin et al.,2021; Zhou et al., 2022; Nayak et al., 2024), image-text retrieval and visual grounding (Zhou et al.,2022), image captioning (Ye et al., 2023; Burda-Lassen et al., 2024; Karamolegkou et al., 2024),and cultural adaptation (Khanuja et al., 2024; Caoet al., 2024). Despite these efforts, current bench-marks typically cover an incredibly small numberof cultures (5-7). To bridge this gap, we introducea benchmark with two tasks covering 50 and 15cultures respectively. Moreover, our benchmarktests models both on their familiarity with culture-specic concepts and on the diversity of their rep-resentation of universal concepts.",
  ": Human universals used as textual queries inour retrieval dataset": "retrieval benchmarks such as COCO (Lin et al.,2014), Flicker30K (Plummer et al., 2015), Image-CoDe (Krojer et al., 2022), and CIRR (Liu et al.,2021b) contain images predominantly from NorthAmerica and Europe. To develop globally effectiveretrieval systems, it is crucial to evaluate modelson culturally heterogeneous datasets. In this work,we present a dataset containing images from 50cultures (). We introduce the novel task ofRetrieval across Universals, aimed at retrievingculturally-diverse images for universal conceptssuch as wedding. We describe the dataset collec-tion in Sec 3.1.",
  "Image-text retrieval is typically evaluated using": "precision. Beyond measuring the correctness ofthe retrieved images, this metric overlooks a signif-icant aspect of retrieval systems: cultural diversity.We thus propose an additional evaluation metricto measure the cultural diversity of the retrievedimages (Sec 3.2). We evaluate an extensive numberof VLMs on the retrieval task (Sec 3.3) and reportthe results in Sec 3.4.",
  "Textual Queries.The queries in our dataset are": "human universalsconcepts common across cul-tures worldwide, such as clothing and dance. presents the list of 20 human universalsused as textual queries in our dataset. The list wasadapted from an extensive list of 369 human uni-versals by Brown (2004) and Pinker (2004). We manually selected human universals that can bedepicted in images. For example, universals likeclothing are associated with tangible objects, anddance is a ritual that can be visually depicted. Inboth cases, these universal concepts are expectedto be visually represented differently across diversecultures.1",
  "Images.To obtain culturally diverse images cor-": "responding to the textual queries, we rst usedCANDLE (Nguyen et al., 2023), a comprehensivecorpus of cultural knowledge, to extract 3 sentencescorresponding to each universal concept and eachculture. For example, for wedding and India,CANDLE contains the sentence The mehendi cere-mony holds signicance in Indian tradition. Thesesentences provide context and cultural specicityfor each universal. We use these sentences to scrapeimages from Google Images. To ensure the qualityof the images, one of the authors manually veri-ed each image in the dataset, ltering out low-resolution images, images with text, and imagesdepicting multiple scenes (i.e., grid images). The -nal dataset includes a total of 3,000 visually-diverseimages (50 cultures 20 universals 3 images).",
  ": Average performance of various VLMs on the the retrieval across universals task, in terms of Relevanceand Diversity": "where pi is the proportion of images from the i-th culture in the top k retrieved images R(q), andm is the total number of cultures in the top k. Ahigh normalized entropy value ( 1) indicates highdiversity, meaning the retrieved images are well-distributed across different cultures. Conversely,a low entropy value ( 0) indicates low diversity,suggesting that the retrieved images are biased to-wards specic cultures. We report diversity withrespect to both the country and the region.",
  "Models": "We benchmark a series of models on our groundingtask, considering both specialist models, designedexplicitly for visual grounding tasks, and gener-alist models, which can handle a wide range ofvision-language tasks, such as captioning, questionanswering, and grounding. These models are listedin , along with their training data, vision andlanguage backbones, and training methodology.",
  "Similarly, in dual-encoder models, OpenCLIP": "demonstrates superior cultural diversity, benetingfrom its large training dataset of 2 billion images.CLIP, which uses the same dual-encoder architec-ture and contrastive loss objectives as OpenCLIPbut is trained on a dataset ve times smaller, ex-hibits lower performance across all metrics. Nat-urally, pre-training on a larger-scale dataset in-creases the chances that the model was exposedto more culturally diverse images. In contrast, re-gional diversity scores are notably lower across theboard. At the same time, for country diversity@5,BLIP-2 stands out as having the highest culturaldiversity, leveraging frozen pre-trained encoders(ViT-G (Fang et al., 2023) as the vision encoderand instruction-tuned FlanT5 (Chung et al., 2024)as the language model) and a QFormer architecture.",
  "A particularly surprising nding is the robust": "performance of TCL with respect to both rele-vance and diversity despite being trained on a thesmallest dataset among all models (4M images).TCL incorporates a unique uni-modal objective tomake the model invariant to data modications,which likely benets the cross-modal alignmentand joint multi-modal embedding learning. Thismay suggest that well-designed training objectivescan sometimes compensate for smaller datasets,highlighting the signicance of pre-training objec-tives alongside data scale.",
  "With respect to farming, CLIP and BLIP-2": "mostly retrieve images from Western countries de-picting technologically advanced farming tools andlarge green elds, whereas CoCA retrieves imagesfrom Africa and the Middle East of people workingin the elds. Finally, the images for wedding arediverse across models, although CLIP focuses onWestern images whereas BLIP-2 prefers the MiddleEast (yet still retrieving images of white dresses).",
  "We also present a visualization in , illus-": "trating the geographical biases across three VLMs:CLIP, CoCA, and BLIP-2. The heatmap shows theratio of how frequently a region appears in the top10 retrieved images, normalized by how often thatregion is represented in the retrieval dataset. Thisis calculated across all 20 universal concepts. Con-sistently, across all three models, there is a clearoverrepresentation of images from North Americaand Europe in the top retrievals. In contrast, imagesfrom regions like Africa, Southeast Asia, and EastAsia are retrieved far less frequently, despite beingcandidates for retrieval. This disparity highlightssignicant biases in the models retrieval mech-anisms, emphasizing the need for better trainingstrategies to ensure fairer and more balanced globalrepresentation in VLMs.",
  "(C) BLIP-2": ": Performance disparity across regions on the retrieval task, depicted across three VLMs, (a) CLIP, (b)CoCA, (c) BLIP-2. The plot shows the ratio of how frequently a region appears in the top 10 retrieved images,normalized by how often that region is represented in the retrieval dataset. tures. Finally, typical pre-training objectives are de-signed to maximize general image-text alignmentand do not specically target cultural diversity,leading models to associate for example breakfastwith eggs and weddings with white dresses.",
  "Task 2: Cultural Visual Grounding": "Visual grounding is essential for human-AI interac-tions, enabling users to reference regions using spa-tial cues and models to respond with precise visualanswers, such as bounding boxes. Existing ground-ing datasets such as RefCOCO and its variants(Kazemzadeh et al., 2014; Yu et al., 2016), FlickrEntities (Plummer et al., 2015), Visual Genome(Krishna et al., 2017), and GRIT (Gupta et al.,2022) tend to focus on generic concepts and theirimages lack cultural contexts.",
  "restricted the task to countries based on the availability ofannotators": "Cloud Research.4 We instructed annotators to ndan image depicting the target cultural concept usingGoogle Images. We emphasized that the imagesshould be of high quality and do not solely depictthe target concept but also include other visuals, tomake sure the grounding task is not trivial. For in-stance, an image for the Korean sauce gochujangmay contain gochujang along with other dishes.",
  "Verication.We perform an additional analysis": "step to verify that the cultural concept is not themain focus of the image. We do so by ensuringthat the bbox-to-image ratio is less than 0.3. Wealso used an off-the-shelf object detection model,YOLOv5, to assess the number of objects in the im-age, ltering out images with fewer than 3 objects.5 Additionally, annotators were asked whether theconcept was prevalent in their culture, and 1.3% ofthe concepts were marked as not prevalent. Thisprocess resulted in the collection of 591 images.More detailed statistics of the collected data areprovided in .",
  "Task Denition and Evaluation Setup": "Given an image I and a query q describing a cul-tural keyword, the goal is to predict a boundingbox R around the region in I that corresponds toq. We evaluate models based on the overlap be-tween the gold standard and predicted regions ofinterest, using Intersection over Union (IoU) as themetric: IoU = |R\\Rgold|",
  "DINO (Liu et al., 2023), a zero-shot object de-": "tection model that combines a Transformer-baseddetector (DINO; Zhang et al., 2022) with phrasegrounding pre-training (GLIP; Li et al., 2022).The generalist models are multimodal large lan-guage models (MLLMs). MLLMs encode visualpatches as tokens that a language model can under-stand. They perform visual grounding by generat-ing bounding boxes in textual format, typically inthe format of hXleftihYtopihXrightihYbottomi, denot-ing the coordinates of the top-left and bottom-rightcorners of the generated bounding box.",
  ": Country-level Accuracy of each model on theCultural Visual Grounding task": "Vietnam. We hypothesize that this discrepancystems from a number of factors. First, there maybe insufcient coverage of Vietnamese conceptsin the pre-training data. Second, the Vietnameseconcepts have distinct English spellings and dia- critical marks. For example, wonton is spelled ashonh thnh, which can impact the models famil-iarity with the concept and consequently its accu-racy. This highlights the need for future researchto focus on developing multilingual, geo-diversebenchmarks to assess performance across a broaderrange of cultural contexts.",
  "RQ3: What challenges do VLMs face in ground-ing culture-specic concepts? presents": "some failure cases of the VLMs in the groundingtask. We can categorize the errors into two primarytypes. In the rst type, models draw a boundingbox around an unrelated object. For example, inthe image depicting a bayong, a type of bag fromthe Philippines, the models frequently misidentifypeople as the bayong. This suggests the modelis unfamiliar with the term bayong and its vi-sual representation. The other error type occurswhen models draw the bounding box around an-other object with a shape similar to the target object.For instance, for ogene, a double-bell instrumentfrom Nigeria, some models incorrectly identied apersons arm as the ogene, which may be due toshape similarity. This may suggest limited famil-iarity with the concept and its visual form.",
  "Our extensive experiments across a variety of": "VLMs uncover specic cultural biases. For in-stance, in the retrieval task, models consistentlyshow varying diversity scores, with a noticeablepreference for Western-style elements. Likewise,VLMs demonstrate higher accuracy in grounding images from Western cultures, which underscoresthe imbalances in their training data. These resultsreveal an urgent need for more culturally diverse,large-scale datasets and for developing training ob-jectives that explicitly consider a wide range ofcultural contexts. Addressing these challenges willpave the way for more inclusive and equitable mod-els that better capture global cultural diversity andmitigate biases in downstream applications.",
  "Metric for diversity.We currently employ a di-": "versity metric based on entropy to evaluate the cul-tural diversity of retrieved images. While this met-ric provides insights into the distribution of imagesacross different cultures, it may not fully capturethe nuanced variations in cultural representation.Our approach to regional diversity assessment maylack granularity, potentially overlooking ner dis-tinctions in cultural diversity within regions.",
  "Mapping from countries to regions.For the pur-": "pose of our tasks, we mapped countries to broadregional categories as specied in . We ac-knowledge that cultures do not follow geographicboundaries and that this variation occurs at an in-dividual level, shaped by ones own life experi-ences. Despite this, we used our mapping as a prac-tical starting point. This approach is a preliminarystep, with the ultimate goal of developing systems",
  "Annotator selection and compensationAnno-": "tators hired from Cloud Research were predom-inately based in USA, Canada, Australia, NewZealand, United Kingdom and Ireland. Participa-tion was strictly limited to those who met speciccriteria to maintain the relevance of the annotationprocess. Annotators were required to belong to achosen ethnicity and to have lived in the designatedcountries for at least 5 of the past 15 years. Thiscriterion ensured that participants had sufcientcultural context and lived experience relevant tothe annotation tasks. We employed a second roundof annotators for the human evaluation phase, en-suring none were repeated from the rst round.",
  "Inadvertent stereotypes in collect images.We": "recognize that some images used to capture culturalconcepts might inadvertently perpetuate stereo-types. While our goal was to gather authentic cul-tural representations, we are aware of the ethicalimplications of including such content. We ap-proached this task with the intention of collectingmeaningful cultural data while being mindful ofthe potential for reinforcing harmful stereotypes.",
  "Daniel Hershcovich, Stella Frank, Heather Lent,": "Miryam de Lhoneux, Mostafa Abdou, StephanieBrandl, Emanuele Bugliarello, Laura Cabello Pi-queras, Ilias Chalkidis, Ruixiang Cui, ConstanzaFierro, Katerina Margatina, Phillip Rust, and AndersSgaard. 2022. Challenges and strategies in cross-cultural NLP. In Proceedings of the 60th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 69977013,Dublin, Ireland. Association for Computational Lin-guistics.",
  "CountryCultural Concepts": "Argentinaalfajor, alpargatas, asado, bandoneon, bifes a la criolla, boina, bolero, bombilla, carbonada, chimichurri, chipa, chocotorta, choripan, churrosrellenos, dulce de batata, dulce de leche, dulce de membrillo, empanada, facturas, faina, gaucho knife, humita, locro, lomito sandwich, malbec,matambre, mate, medialuna, milanesa, morcilla, parrilla, pascualina, pastel de papa, pebete, picada, provoleta, rabanito, ravioles, rosca de pascua,sandwich de miga, torta frita, vino patero, yerba Brazilacai, acaraje, alfajor, baiao, bombacha, bumba-meu-boi, brigadeiro, cachaca, caipirinha, carimbo, chimarrao, churrasco, cocar, cuica, empada,espetinho, farofa, feijoada, frescobol, moqueca, pacoca, pao de queijo, rapadura, requeijao, rosca, romeu e julieta, samba, sarongue, tapioca,tucupi, vatapa Canadabagel, bannock, beavertail pastry, blueberry grunt, butter tart, caribou, cipaille, caesar cocktail, cretons, date squares, donair, ipper pie, garlicngers, inukshuk, jiggs dinner, maple taffy, nanaimo bar, peameal bacon, pemmican, persian roll, poutine, rappie pie, sugar pie, toboggan, toque,tourtiere Chinabaozi, bianlian, bianzhong, biang biang noodles, chinese knot, chinese lantern, chinese seal, cong you bing, doufu, dragon beard candy, erhu,fenghuang crown, gongbi, guzheng, hongbao, hotpot, huanghuali furniture, hulusi, jiaozi, jinghu, laziji, liuli, longjing tea, luo han guo, malatang, mahjong tiles, mooncake, paper cutting, peking opera mask, pipa, qipao, shengjianbao, suzhou embroidery, wushu sword, xiao long bao,xun, yuanyang hotpot, zongzi Indiaaarti thali, achaar, bangles, bhang, bhatura, bharatanatyam, bindi, biryani, chapati, chai, diya, dosa, dhoti, gajra, ganesha, idli, jalebi, jhumka,kathakali, kul, kurta, kumkum, laddu, lassi, lehenga, lungi, mangalsutra, mehndi, mojaris, mridangam, murukku, namaste, pani puri, papadum,paratha, payal, rasam, rasgulla, rangoli, raita, salwar kameez, sari, shehnai, sherwani, sitar, tabla, tanpura, tandoor, tikka, turban, veena, vada Israelbaba ganoush, baklava, bourekas, challah, chamsa, chuppah, eshet chayil candlesticks, fattoush, falafel, galabeya, hamentashen, halva, hatzilim,hamsa, jachnun, kibbeh, kippah, krembo, ketubah, knafeh, kubbeh, kiddush cup, knafeh, labaneh, malabi, matbucha, matzah, menorah,muhammara, matkot, ptitim, rugelach, sabich, sambusak, sefer torah, shakshuka, shofar, skhug, stuffed grape leaves, sufganiyah, tallit, tellin,tembel hat, tabbouleh, tzatziki, tzitzit, yemenite kudu horn Mexicoaguas frescas, alebrije, banderita, barbacoa, calavera, cantarito, carnitas, cemitas, ceviche, chalupa, chapulines, chicharrones, churro, cochinitapibil, enchilada, gordita, huarache, huipil, menudo, metate, mole, molinillo, nopal, ofrenda, panucho, papel picado, pinata, pozole, pulque,quesadilla, quexquemitl, rebozo, salbute, sarape, sopes, taco, talavera, tamale, teponaztli, tlayuda, torta, vihuela, zarape Nigeriaabacha, abeti aja, agbada, agidi, akara, amala, aso oke, asoke, buba, chin chin, danfo, dodo, edikang ikong, egusi soup, ekwe, ewedu, la, fufu,gbegiri, gele, garri, isi ewu, jollof rice, keke napep, kilishi, kuli kuli, moi moi, ogene, okapi, oha soup, pounded yam, sakara, suya, talking drum,zobo Pakistanachaar, ajrak, balochi sajji, banarasi saree, balti, biryani, chapli kabab, chitrali cap, cobalt pottery, dholki, falooda, gulab jamun, gilgit cap,haleem, henna, hunza cap, karahi, kheer, khadi, khussa, kul, lacha paratha, lehnga choli, miswak, moti choor ladoo, multani sohan halwa, nankhatai, nihari, paan, pakol, pathani suit, peshawari chappal, saag, samosa, sharbat, sheermal, shalwar kameez, sindhi topi. Philippinesadobo, anting-anting, arnis sticks, bahay kubo, balangay, balisong, balut, bangus, barong tagalog, bayong, bulul, calamansi, carabao, dinuguan,durian, guling, halo-halo, ifugao hut, jeepney, kalesa, kinilaw, kulintang, lechon, malong, maranao gong, pamaypay, pan de regla, pandesal,palabok, pinya fabric, puto bumbong, salakot, santol, sinigang, singkaban, tarsier, tapsilog, terno, tinikling. Polandbarszcz, basolia, bigos, bryndza, chrzan, aki, faworki, golonka, kasza gryczana, kaszanka, kabanos, kartacze, kielbasa, kiszka, knysza,kogel mogel, kompot, kotlet schabowy, kluski slaskie, makowiec, mizeria, oscypek, pasztecik szczecinski, paczek, pierogi, pierniki, plackiziemniaczane, ptasie mleczko, rosol, rogalswietomarcinski, ryba po grecku, sledz, smalec, ser bialy, sekacz, szarlotka, tatar, zrazy, zurek. Russiababushka, balalaika, bayan, blini, borshch, budyonovka, caviar, chak-chak, domra, dymkovo toys, faberg eggs, garmon, gusli, gzhel,khokhloma, kasha, kokoshnik, kvass, lapti, matryoshka, okroshka, pelmeni, podstakannik, pryanik, pirozhki, russian blue, samovar, sarafan,shchi, soljanka, sushki, syrniki, telnyashka, treshchotka, ushanka, valenki, varenyky. South Africaamarula, amagwinya, beadwork, biltong, boeremusiek instruments, boerewors, braai, bunny chow, chakalaka, djembe, dompas, fufu, geelbek,hadeda ibis, kaross, knobkerrie, makarapa, malva pudding, marula fruit, melktert, mopane worms, pap, potjiekos, protea, rooibos, rondavel,shweshwe, sosatie, spaza shop, txalaparta, umqombothi, vetkoek, vuvuzela. South Koreabibimbap, bokjumeoni, bossam, bulgogi, buchaechum, dduk, ddukbokki, dongchimi, galbi, gat, gayageum, geomungo, gochujang, gimbap,haeguem, hahoetal, hanbok, hangwa, hanji, hwagwan, jeogori, jeon, jokduri, janggu, jeotgal, kimchi, makgeolli, naengmyeon, norigae,pyeongyeong, samgyeopsal, samulnori, seonji, sikhye, sotdae, sundubu-jjigae, soju, tteok, tteokguk, tteokbokki, yeot. Vietnamao ba ba, ao dai, ao thu than, banh bao, banh canh, banh chung, banh cuon, banh gio, banh giay, banh khuc, banh mi, banh pia, banh xeo, ca phetrung, cao lau, cafe sua da, canh chua, chao long, che, dua mon, gio lua, gio lua, goi cuon, hoanh thanh, kem xoi, keo dua, khanh ran, my quang,non la, nuoc mam, pho, sinh to, thit kho tau, thit heo quay, trong com, trung vit lon, thung chai boat, bun cha, bun bo hue, bun thit nuong, comtam."
}