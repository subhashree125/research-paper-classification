{
  "Abstract": "Authorship obfuscation, rewriting a text to in-tentionally obscure the identity of the author,is an important but challenging task. Currentmethods using large language models (LLMs)lack interpretability and controllability, oftenignoring author-specific stylistic features, re-sulting in less robust performance overall. To address this, we develop STYLEREMIX, anadaptive and interpretable obfuscation methodthat perturbs specific, fine-grained style ele-ments of the original input text. STYLEREMIXuses pre-trained Low Rank Adaptation (LoRA)modules to rewrite an input specifically alongvarious stylistic axes (e.g., formality andlength) while maintaining low computationalcost. STYLEREMIX outperforms state-of-the-art baselines and much larger LLMs in a varietyof domains as assessed by both automatic andhuman evaluation. Additionally, we release AUTHORMIX, a largeset of 30K high-quality, long-form texts froma diverse set of 14 authors and 4 domains, andDISC, a parallel corpus of 1,500 texts spanningseven style axes in 16 unique directions1.",
  "Introduction": "Authorship obfuscation, the act of rewriting text toconceal the author, is an important method for pre-serving the privacy of authors in sensitive contexts,e.g., anonymous discussion forums, double-blindreviews, and health services. However, it is inher-ently complex, requiring a substantial change inwriting style to obscure the authors identity whilealso preserving the original content and fluency.Historically, authorship obfuscation methodshave manipulated aspects of an authors style to",
  ": Overview of": "STYLEREMIX.In pre-obfuscation, distinct style elements are distilled froman LM into individual training sets, which are used totrain specialized LoRA adapters. During obfuscation,the user can automatically or manually select the styleadapter(s) which, when combined with the base LM,will best steer generations away from the original style. obfuscate the original text (Karadzhov et al., 2017;Shetty et al., 2017; Bevendorff et al., 2019). Thesetechniques typically use style aspects that are easyto automatically evaluate, such as text length, capi-talization frequency, and punctuation, to alter theoriginal text. However, these rule-based methodsare often too rigid and lead to a degradation offluency and grammaticality (Fisher et al., 2024).Recent work demonstrates strong obfuscationperformance using LLMs (Mahmood et al., 2019;Haroon et al., 2021; Weggenmann et al., 2022;Fisher et al., 2024), but the common challengeamong these is a relative lack of interpretabilityand controllability on the obfuscation; these ap-proaches do not incorporate any author-specificstylometric characteristics of the original author,leading to more generalized and ineffective obfus-cations. For example, a method that relies solelyon increasing language model fluency might ef-fectively obfuscate more informal writing, but notformal writing.Toaddressthisgap,weintroduce STYLEREMIX, an interpretable, inference-time, author-specific obfuscation method thatcombines the fluency and steerability of LLMs withauthor-specific style information. STYLEREMIXfirst detects unique stylistic elements of the author,either through automatic processes or manually.It then uses this information during obfuscationby integrating style-specific adapters with a baselanguage model (an LLM) to guide the generatedtext away from the authors original style.STYLEREMIX avoids high computational costsby utilizing pre-trained Low Rank Adaptation mod-ules (LoRA; Hu et al., 2021), which we train torewrite inputs towards specific directions on vari-ous stylistic axes (e.g., more/less length, more/lessformality, higher/lower grade level). Drawing inspi-ration from the process of creating a remix, wheremusical elements of a song, such as tempo, key,and instrumentation are adjusted to form an en-tirely new track, in this work we seek to identifyand manipulate different elements of an authorshipstyle, and propose a simple yet effective approachto steer different components of the text with LoRAadapters. Our results show that STYLEREMIX out-performs state-of-the-art authorship obfuscationmethods and instruction-based models of similarand larger sizes. Additionally, our method has theadded benefit of explainability and is customizableto any unique authorship style.We make the following contributions: (I) We introduce STYLEREMIX, an interpretable,inference-time algorithm designed for author-ship obfuscation. This method offers the per-sonalization and flexibility required for effec-tiveness across various styles and text types. (II) We release two datasets:(1) AUTHORMIX,a comprehensive au-thorship dataset with over 30K paragraphsspanning four diverse domains (presidentialspeeches, novels, scholarly articles, andblogs) and 14 author styles, encompassingmany more domains and styles than anyprevious work to our knowledge.(2)DISTILLEDSTYLECOMPONENTSDATASET (DISC), a high-quality, validated,parallel dataset over 7 style axes. It featuresn = 1,500 texts rewritten towards 16 distinctstyle directions for a total corpus size of 24K.",
  "Methods": "STYLEREMIX is an obfuscation method that lever-ages style elements to adapatively rewrite texts.Specifically, it incorporates information about thestyle of the original author to guide the obfusca-tion process. illustrates this new approach,which consists of two phases.The pre-obfuscation phase, conducted only onceregardless of the number of authors, involves creat-ing a diverse training set for each style axis we aimto modify (e.g., length variations, formality levels,grade level adjustments, etc.). These style-specificdatasets are then used to train Low-Rank Adap-tation (LoRA) adapters, which are low-parametermodules that can be seamlessly integrated with alarger base model to guide text generation alongspecific style axes.In the obfuscation phase, users can choose thestyle axes that most effectively disguise the originalauthors style, either automatically or manually.The selected pre-trained LoRA adapters are thenused to steer the obfuscated text generation.",
  "Function words are words that express grammatical rela-tionships among other words (if, up, would, etc.)": ": We compare generations from rewriting a text from AUTHORMIX-Speech using each of the style axisadapters individually. This demonstrates the distinct transformation capabilities of each adapter, highlightingvariations in tone, formality, and other linguistic features. We choose the direction of the style axes based on theautomatic style selection method described in .2. (Peng and Hengartner, 2002). Additionally, we in-corporate \"grade level,\" which primarily measuresdiscrete features like the number of syllables andsentence and word lengths. Since this measure canvary slightly, we averaged three similar metrics: theFlesch-Kincaid (FK; Flesch, 1948), Linsear Write(L; OHayre), and the Gunning Fog Index (GF;Gunning, 1952) metrics. For the exact formulas,see Appendix C.1.Beyond formula-based properties, we also ex-plore more abstract style axes such as the use ofsarcasm, formality, voice (passive or active), andwriting type (persuasive, descriptive, narrative, andexpository). Due to the lack of existing formulas,we train model-based classifiers to measure theseproperties. More details on the training of thesemodels can be found in Appendix C.1.In total, we identify seven style axes, each withtwo directions (\"higher\" or \"lower\"), except writingstyle, which has four options. This results in 16distinct style elements. We acknowledge that thisis not an exhaustive list of all author invariants, butwe observed noticeable differentiation among theauthors in our experimentation using these metrics.For more details, see Appendix B.5. Adapter Training DataWith the above styleaxes chosen, we create DISC, a 16-style-elementparallel dataset which distills each style elementfrom a large LLM. To standardize the style adapterand minimize content dependencies, we create asingle base training set and used instruction prompt-ing with a LLM to generate rewrites along the cho-sen style axes. The base dataset comprises a diverserange of domains to encompass different writing types. Specifically, we randomly sample 500 para-graphs from sources including Wikipedia, booksand plays (Kryscinski et al., 2021), and diary-styleblogs (Schler et al., 2006a). Each paragraph iscleaned and standardized, resulting in paragraphsof 2-5 sentences each. Using GPT-4 Turbo (Ope-nAI, 2023), we then generate new versions of theseparagraphs along different style axes and direc-tions (\"higher\" or \"lower\") using detailed instruc-tion prompt tuning (see Appendix C.2). This resultsin 16 parallel datasets written in different style axisand directions.Next, we evaluate the generated paragraphs toensure that they accurately reflect the intended styleaxis and direction. presents the evaluationresults, both automatic and human, for the styletraining datasets created. The results demonstratethat our datasets effectively capture the desiredstyles. See Appendix C.2 for more details. Train LoRA AdaptersNext, our goal is to trainthe models to generate text along the chosen styleaxes. To minimize computational cost (Strubellet al., 2019), we bypass model fine-tuning and in-stead employ Low Rank Adapation (LoRA; Huet al., 2021) adapters for each of the style axes.By freezing the base model and tuning only asmall portion of injected features, LoRA guar-antees lightweight training (Rebuffi et al., 2017;Houlsby et al., 2019) while also incurring no ad-ditional inference latency, ensuring both efficienttraining and deployment.We use Llama-3 8B(AI@Meta, 2024) as our base model, and trainLoRA adapters on top of them for each directionon the style axes. See Appendix C.3 for more train-",
  "Sarcasm97.7Voice93.7Writing Intent (4 classes)77.7": ": Evaluation of the parallel style training datasets.Automatic evaluation (top) is shown for the originalscore, as well as the score for the dataset that had instruc-tion to increase (More) or decrease (Less) the given styleaxis. The highest value is bolded and the lowest valueis underlined. Other style axes required human evalu-ation (below). For this we randomly combine 10% ofthe high and low datasets (or all four types for WritingType) and ask three NLP experts to label whether thestyle axis was high or low; average accuracy is shown.",
  "Stage 2: Obfuscation": "Style Axes and Weights SelectionDuring theobfuscation phase, a text or set of texts is pre-sented for obfuscation. If a user has a clear ideaof which style axes to adjust, they can input theirdesired styles and the corresponding weights of theadapters to control the strength of the generation.However, since this information is often unavail-able, we develop a straightforward yet effectivemethod for selecting which style axes to modifyand the magnitude of the weights of these adapters.For the given m authors in some genre (e.g.speech, novel), we first create an author vectorxi R7 for each author, which is composed of theautomatic evaluation of the seven style axes. Afternormalizing with respect to all m authors, we cal-culate the difference vector between each authorand the average, defined as xi = xi 1 mmj=1 xj.Using the absolute values in this difference vector|xi|, users can select the top k style axes where thespecific author deviates most from the average.Next, the user needs to specify the weight foreach chosen style adapter to merge with the basemodel. This procedure could be manual, but wealso provide a heuristic to determine the weightsautomatically. Building on prior work, we find thatLoRA adapters perform well with values in therange [-1.5, 1.5] (Huang et al., 2023). Next, we usethe number of standard deviations an author vectordeviates from the average to map each style axis to",
  "For detailed implementation, see Appendix C.5": "Generation TechniquesDuring generation, weuse the adapters corresponding to the selected styleaxes to rewrite the given text, steering these promi-nent styles toward the average. In addition, weexperiment with multiple methods for combiningthese LoRA adapters. Sequential: We pass in the text through asequence of adapters iteratively; the outputfrom one adapter serves as the input for thenext. This method provides additional inter-pretability by revealing how the text becomesobfuscated at different stages after alteringspecific style axes. However, it increases com-putation time, as it requires a forward pass foreach chosen style axis. Adapter Merging (AM): We merge theweights of all the adapters before combiningthem with the base model by concatenatingtheir weights (Yadav et al., 2023a; Yu et al.,2024). See Appendix C.4 for more details. LoraHub+: LoraHub is a framework de-signed to assemble multiple LoRA adapterswith the goal of maximizing performance onspecific tasks (Huang et al., 2023). It adjuststhe weights of the given adapters to optimizethe specified objective through gradient-freeoptimization. We extend this with LoraHub+,which defines a new objective function L de-signed to optimize for obfuscation by sum-ming up the automatic evaluations of the se-lected style axes across a small set of test ex-amples. We also add the fluency score to en-courage more fluent text:",
  "Size7B13B8B70B7BSeq.AMAM + LoraHub+": "AUTHORMIX SpeechDrop Rate18.224.017.616.823.124.110.315.129.234.941.231.4Grammar67.867.167.170.267.871.254.937.856.761.766.563.9Content83.880.880.880.278.683.989.189.556.471.377.373.9Overall10.313.09.59.512.314.45.15.19.415.321.214.8 AUTHORMIX NovelsDrop Rate12.213.79.211.313.310.87.013.524.919.328.635.6Grammar71.873.873.175.470.068.346.336.861.272.668.163.5Content82.980.783.181.581.981.385.288.158.683.776.172.9Overall7.38.25.66.97.66.02.84.48.911.814.816.5 AUTHORMIX ScholarDrop Rate0.81.51.62.50.00.81.54.66.11.89.211.5Grammar64.364.964.166.665.369.154.531.062.365.848.644.7Content91.789.788.984.088.991.392.885.860.678.075.368.8Overall0.50.90.91.40.00.50.81.22.30.93.43.5 AUTHORMIX BlogDrop Rate17.721.321.818.927.522.29.412.156.434.441.042.0Grammar68.469.171.374.069.069.841.929.160.666.764.965.3Content82.579.078.177.877.880.483.785.845.172.173.774.2Overall10.011.612.110.914.812.53.33.015.416.519.620.4 : Comparison of obfuscation methods measured by mean drop rate, grammar, meaning similarity, and overall(the mean product of the metrics), across STYLEREMIX and comparatively sized or larger baselines on each subsetof AUTHORMIX. Bold and underline denote the highest and the second-highest score respectively in each row. Allmetrics displayed in the table are multiplied by 100 for easier viewing of significant figures.",
  "Datasets": "We aimed to test how authorship obfuscation meth-ods perform on a diverse array of author styles anddomains. To this end, we develop a new benchmarkdataset called AUTHORMIX, covering four distinctdomains: presidential speeches, early-1900s fictionnovels, scholarly articles, and diary-style blogs.Together, AUTHORMIX contains more than 30 khigh-quality paragraphs from 14 authors.For the presidential domain, we curate and cleanspeeches from George W. Bush, Barack Obama,and Donald Trump3. For novel domain, we choosea collection of early 1900s fiction writers withstrong writing styles: Ernest Hemingway, F. ScottFitzgerald, and Virginia Woolf. We choose thesespecific writers in an effort to limit the topic biasin the evaluation metrics.Lastly, we alter two existing datasets to matchthe formality of our new domains: the Extended-Brennan Greenstad (Brennan et al., 2012), a col-lection of scholarly short (500-word) paragraphs",
  "STYLEREMIX Configurations": "We compare three versions of STYLEREMIX: se-quential, adapter merging, and LoraHub+. Forsequential, to account for the order of the styles,we average over n = 3 shuffled orders. The adaptermerging method uses the static standard deviationmapping method described in .2. Forthese two methods, we select the best method perdomain (based on the overall score) using the topk = 1, 2, 3, 4 changed styles. Lastly, we run ourcustomized LoraHub method (LoraHub+), match-ing the best styles per domain as the base adaptermerging method for direct comparison.",
  "Paraphraser (Paraphrase)We use the T5-Largeparaphraser introduced by Jung et al. (2024) whichiteratively improves through self-distillation": "JAMDEC (JD)This method (Fisher et al., 2024)relies on a smaller LLM, GPT2-XL (Radford et al.,2019) to overgenerate many new rewrites given thekeywords from the original text. It then uses a filterto select the best new rewrite. We run this methodusing the default settings, and a beam width of 10. Instruction-tuned LLMsWe compare against asuite of instruction-tuned LLMs including Llama-2-Chat (7B, 13B) (Touvron et al., 2023), Llama-3-Instruct (8B, 70B) (AI@Meta, 2024), and Gemma-Instruct (7B) (Team et al., 2024). For each model,we provide instruction to rewrite the given text.More comparisons of different models can be foundin the Appendix B.2. Exact instructions used forgeneration can be found in Appendix D.",
  "In line with previous work, we evaluate authorshipobfuscation on four main criteria: obfuscation, con-tent preservation, grammaticality, and overall4. SeeAppendix D.5 for more details": "ObfuscationClassifiers with various machinelearning architectures have been used to measureobfuscation (Mahmood et al., 2019; Haroon et al.,2021; Fisher et al., 2024). Recent work in au-thorship obfuscation and style transfer often usesRoBERTa (Liu et al., 2019) classifiers (Xing et al.,2024a; Uchendu et al., 2021; Liu and May, 2024;Hallinan et al., 2023).In line with this previous work, we fine-tunefour RoBERTa large (355M) models, one for eachdomain in AUTHORMIX; classifiers achieve onaverage 94.0% accuracy in the evaluation set of 4All metrics are bounded between 0 and 1, which ensuresthe product has the same bounds. Although similarity scoresare theoretically bounded from -1 to 1, we observe empiricallyacross all datasets and methods that they are bounded morestrictly between 0 and 1; see Appendix D.5 for more details each respective domain (for full results, see Ap-pendix D.5). Using these classifiers, we calculatethe drop rate, which is the normalized decrease inthe classifiers accuracy when comparing its perfor-mance on the original texts to the obfuscated texts.The drop rate can be expressed as:",
  "Content PreservationWe use the embeddingsimilarity of the inputs and their obfuscations inSentence Transformers (Reimers and Gurevych,2019) to gauge semantic similarity": "Language QualityWe measure grammaticalityvia the probability of being grammatically accept-able from TextAttack (Morris et al., 2020), a binaryRoBERTa-large classifier fine-tuned on the Corpusof Linguistic Acceptability (Warstadt et al., 2018). Overall Task ScoreThe overall success of eachobfuscation is measured by the product of the abovethree metrics: drop rate, similarity score, and CoLAscore. This product ensures a high overall taskscore accurately reflects high scores in all threecategories; it is also used in prior work in textrewriting (Krishna et al., 2020a; Hallinan et al.,2023; Patel et al., 2023; Xu et al., 2018)",
  "Human Evaluation": "We also conduct human evaluation to verifythe quality of the obfuscations from the bestSTYLEREMIX variant and comparably sized base-lines; we omit . We randomly select n = 20 textsfrom each author in AUTHORMIX for annotationvia Amazon Mechanical Turk by three workerseach. Following the setup of Fisher et al. (2024),we instruct each annotator to read both the originaland obfuscated text, then respond to five questionsrated on a three-point Likert scale (0, 0.5, or 1),measuring grammar, fluency, high content preser-vation, low content addition, and obfuscation. Wediscard evaluations where all annotators disagreeon the label5. Lastly, we calculate an overall scoreusing the weighted product of all five metrics. Fur-ther details can be found in Appendix E.",
  "Pairwise agreement is greater than 93% for all questions": "10098.0 99.0 97.3 98.7 97.8 86.7 98.2 Grammar () 10094.8 95.4 95.0 96.1 95.5 81.9 95.6 Fluency () 88.3 89.7 90.3 83.6 81.2 57.9 89.9 Content Preserved () 77.2 79.2 87.0 83.5 88.4 41.6 80.2 Less Content Added () 72.5 80.2 75.7 80.6 76.3 78.6 83.0 Obfuscation () 57.3 66.3 65.5 65.8 62.0 32.2 69.9 Overall () Llama-2-7bLlama-3-8b Llama-3-70bGemma-7b ParaphraserJamDec StyleRemix : Human evaluation results for mean grammar,fluency, content preserved, less content added, and ob-fuscation. For each of the metrics, higher is better. Wealso compute the mean overall score, the product ofgrammar, content preserved, and less style similarity.",
  "Main Results": "STYLEREMIX has the highest overall task scoreand obfuscation drop rate comparesSTYLEREMIX to LLMs of all sizes and the fourSOTA authorship obfuscation methods on obfus-cation, content preservation, grammar, and overalltask score. STYLEREMIX has on average 5.5%higher overall task score and up to 12% higher ob-fuscation drop rate on all domains, with similarscores on content preservation and grammar. Im-pressively, STYLEREMIX is also more effective atobfuscation than LLMs of much larger magnitudes such as Llama-2-Chat 13B and Llama-3-Instruct70B especially on the Blog domain with a 10%higher overall task score. This shows how a moretargeted approach can outperform larger but moregeneric methods. Human evaluation shows STYLEREMIX outputsthe best overall and most obfuscated generations shows the result of our human evalua-tion. All methods have strong grammar and fluencyscores, with only slight differences between them.STYLEREMIX beats all other baselines in obfus-cation with a score of 83.0%, and ranks secondfor content preservation, trailing by just 0.4 pointsbehind the significantly larger LLama-3-Instruct70B. Notably, STYLEREMIX even surpasses thelarger Llama-3-Instruct 70B in obfuscation, contentpreservation, and grammar. In terms of less contentadded, STYLEREMIX still outperforms strong base-lines like Llama-3-Instruct 8B, but loses slightly to",
  "%! I was surprised, but not complaining lol. But yeah Mr. Curpheysnow on crutches, hobbling around the classroom and still teaching us likehis usual self": "MT100 per cent! I was surprised, but I dont complain lol. But yes, Mr. Curpheyis now in the gardens to walk through the classrooms and still teach us ashis ordinary self.Stylo.One hundred percent; i was surprised, but not complaining ll, as but yeahMr. murphy s now on crutches, sobbing around the classroom and alwaysteaching us like his usual self.JD100%! I was surprised, but not complaining lol. But yeah Mr. Curpheysnow on crutches, hobbling around the classroom and still teaching us likehis usual self.",
  "Original(Obama)": "I recognize that times have changed since I first spoke to this convention.The times have changed, and so have I. Im no longer just a candidate. Imthe President.MTI recognize that the times have changed since I first spoke to this convention.The times have changed, and thats how I have. Im not just a candidateanymore. I am the President.Stylo.I acknowledge that times have changed since I 1st spoke to this convention,and the times have changed, and so have I. I am no longer just a candidate;and myself m the President.JDIts important to remember where we came from, where were going, andwhat were trying to do,\" he said, adding, I think its important for all of usto be. We are the people who built this country, and we have a responsibilityto make sure that we preserve it for our children and our childrens children,and thats what were going to do,.Para.Since I first addressed this convention, I have seen significant changes. As aresult, I have evolved from a candidate to the current President. Llama-2Chat (7B)Since my initial conversation with this convention, time has passed andchanges have occurred. As a result, my perspective and circumstances haveevolved. I am no longer just a candidate, but the actual President.Llama-3Inst (8B)Since my initial address to this esteemed convention, I have come to ac-knowledge that the times have indeed changed. So too have I. Gone are thedays when I stood before you as a mere candidate, eager to make my case.Today, I stand before you as the President, with a wealth of experience andinsight gathered during my time in office.GemmaInst (7B)Since my initial address to this esteemed convention, the world has shifted,and I have evolved alongside it. Today, I stand before you not as a candidate,but as the President, ready to tackle the challenges that lie ahead.",
  "Oh, how the world has transformed since I first addressed you all here!Indeed, the world has shifted, and so have I. Its not just about being acandidate anymoreits about being the President": ": Examples of obfuscations from baseline meth-ods and STYLEREMIX from texts from AUTHORMIX-Blog and AUTHORMIX-Speech respectively. Greenregions highlight where the method obfuscates well andred areas show grammar, content, or obfuscation issues. methods like Gemma-Instruct 7B and Paraphrase;though these methods may be good abstaining fromadding new content, this is likely a byproduct oftheir generations being too succinct and failing topreserve information or obfuscate, as shown bytheir low human evaluations on these two metrics.For overall score, which captures aspects offluency, content preservation, and obfuscation,STYLEREMIX performs the best, achieving an over-all score of 69.9%; the next highest scoring methodis Llama-3-Instruct 8B with a score of 66.3%, a sig-nificant dropoff. Each individual metric must behigh to achieve a high product; this indicates thatour method on averages produces the obfuscations",
  "with the best overall quality, balancing betweengrammar, content preservation, and obfuscation,rather than optimizing for just one dimension": "Qualitatively, STYLEREMIX generates moreflexible, directed obfuscations compared toother methodsQualitative results demonstratethat, as designed, STYLEREMIX provides a strong,personalized obfuscation compared to the moregeneral results of other methods and models. Ta-ble 3 presents two randomly6 selected texts alongwith the generations from various models and meth-ods. Consistent with previous work (Fisher et al.,2024), the rule-based methods (MT and Stylo resultin poor grammar or loss of content. Conversely,methods based on LLMs tend to maintain grammarand content preservation more effectively.The most significant difference is evident in thestyle of the generated text. Other methods some-times struggle to clearly obfuscate and instead gen-erally mimic the original authors style or defaultto a more formal model-like writing style. Incontrast, STYLEREMIX stands out by providing amore personalized and targeted obfuscation. Forinstance, in the Blog example (top), STYLEREMIXgenerates text that is more formal, uses higher-grade level language, and is longer compared to theoriginal text. Meanwhile, in the Speech example(bottom), it adopts a more sarcastic, less formaltone, and incorporates more function words.We also find that this multi-style mixture ap-proach often results in noticeably different sentencestructures and punctuation. For example, in thespeech text (bottom), the order of the first sentenceis reversed compared to the original, a feature notobserved in any other generation. Additional gen-erations are available in Appendix B.6.TofurtherhighlightthesteerabilityofSTYLEREMIX, we display a randomly selectedtext from AUTHORMIX-Speech and randomgenerations created using an adapter in the optimalsteering direction7 for each of the seven style axesin . Each generation demonstrates how thechoice of adapter significantly transforms the textand influences the type of obfuscation.",
  ": Overall task score on the base adapter mergingmethod using different number of style adapters. Wecompare the overall task score using 17 style adapters.For all automatic evaluations see": "Although STYLEREMIX can be used with any arbi-trary method of choosing the style axes to change,we do find that choosing based on difference be-tween the average style vector and the author vectorimproves obfuscation on average by 6% over ran-dom selection of the same number of weights. Wenote that the grammar and content remained aboutequal. More details can be found in Appendix B.1 ShufflingstyleadapterswhenusingSTYLEREMIX-SequentialleadstosomevariationForSTYLEREMIX-Sequentialweexperiment with shuffling the order of the chosenstyle adapters over n = 3 random shuffling. Wefound that the order of the styles does have someeffect on the obfuscation drop rate (standarddeviation of3% 6%) but little effect onthe grammar or content preservation (standarddeviation of 1% 2%). This was seen stronglywhen choosing 3+ styles and in domains withstrong style differences among the authors (Speechand Blog). More details are in Appendix B.3 Changing 5+ style axes decreases grammati-cality shows how the overall task scorechanges the number of styles chosen to use theadapter merging method increases. At first, bothobfuscation drop rate and overall score steadily in-crease as we increase the number of style adapters,which corresponds with changing more elementsof the original text. However, for AUTHORMIXSpeech, Scholar, and Blog, we see a sudden de-crease in overall task score when using 5 styles.Investigating this, we found that using 5+ styleadapters leads to an average of 16% decreasein grammar and a 5% decrease in overall score.More details can be found in Appendix B.4.",
  "Authorship Obfuscation MethodsTraditionalauthorship obfuscation methods leverage stylo-metric insights, such as author invariant features,": "to obfuscate texts (Karadzhov et al., 2017; Man-soorizadeh et al., 2016; Xing et al., 2024b). How-ever, these methods have been shown to have issueswith grammar and fluency due to their strict rule-based approach (Fisher et al., 2024).To reduce this behavior,model-based ap-proaches have been developed, such as Mutant-X, a genetic algorithm which utilizes an internalclassifier to iteratively \"mutate\" a sentence (Mah-mood et al., 2019). Later work improves on thiswith an ensemble of classifiers rather than a sin-gle one (Haroon et al., 2021) or via variationalautoencoders as the base model to generate defer-entially private generations (Weggenmann et al.,2022). Most recently, Fisher et al. (2024) demon-strate the efficacy of smaller LLMs for authorshipobfuscation through over-generation and filtering.However, this methods reliance on a heavy decod-ing algorithm to generate diverse candidates makesit impractical. Some obfuscation methods have alsoincorporated authorship information (Jones et al.,2022; Shetty et al., 2017). Although these bothshowed promise, they require extensive trainingand are only applicable in specific use cases. ParameterEfficientLearningParameter-efficient adapters, small modules tuned on top of afrozen large model for effective transfer learning,have been proposed for vision (Rebuffi et al.,2017) and NLP (Houlsby et al., 2019). Othershave extended these methods by tuning specificlayers and embeddings (Li and Liang, 2021; Lesteret al., 2021), or by making the adapter matrices anaddition to the original model weights themselvesrather than additional injected layers (Hu et al.,2021; Lu et al., 2023).Adjacent to parameter-efficient training strate-gies are model merging techniques, which seekto integrate model knowledge by combining theirweights (Matena and Raffel, 2021); this is efficientand prevents additional inference cost. Merging hasbeen explored extensively in previous work, to com-bine diverse, targeted domain models (Jang et al.,2023; Ram et al., 2023), or over the same modeltrained with different seeds or hyperparameters toimprove robustness (Wortsman et al., 2022; Ramet al., 2022). Model merging has even been ex-plored with parameter-efficient adapters like LoRA(Huang et al., 2023). Other lines of work expandon merging techniques, creating strategies beyondsimply averaging model weights. (Yadav et al.,2023b; Stoica et al., 2023; Yu et al., 2023). Controllable GenerationPrevious work intro-duces methods to control the content of a genera-tion (Lu et al., 2021) or steer the style of the gener-ation (Liu et al., 2021; Lu et al., 2023). Howeverthese types of controllable generation are less prac-tical for authorship obfuscation, which requires asteerability of the content and the style. Style TransferStyle transfer techniques have uti-lized both simple models (Hu et al., 2022) and moreadvanced machine learning models (Jin et al., 2022;Hallinan et al., 2023). Most approaches depend ontraining generation models with a dataset, whichcan be natural or synthetic (Jin et al., 2022). Thesemethods also interpret \"style\" in various ways, rang-ing from comprehensive notions like specific au-thors (e.g., Shakespeare and Hemingway) (Krishnaet al., 2020b) to particular stylistic elements (e.g.,formality and sentiment) (Fu et al., 2018). Notably,recent, effective style transfer techniques resemblethe approach we use, which involves fine-tuning aLLM (Hallinan et al., 2023; Krishna et al., 2020b).",
  "Conclusion": "In this work, we introduce STYLEREMIX, a noveland interpretable method for authorship obfusca-tion. By targeting specific fine-grained stylisticelements and leveraging Low Rank Adaptation(LoRA) modules, STYLEREMIX provides a moreinterpretable and controllable approach than exist-ing methods based on large language models orother state-of-the-art techniques while still main-taining excellent performance. We show our newmethod outperforms a suite of strong, state-of-the-art baselines in four diverse domains overall in bothautomatic and human evaluation.Additionally, as part of this work, we release anew authorship attribution dataset, AUTHORMIX.This dataset includes two new domains: presi-dential speeches and fiction books, which werecarefully selected to ensure a high degree of topicmatching, thereby enriching the datasets applica-bility and depth. Furthermore, we develop DISC, acollection of 16 parallel, human-validated datasetsspanning various stylistic dimensions, which canbe employed in future research to further exploreand refine the nuances of stylistic text manipulation.These resources aim to advance the field of author-ship obfuscation and provide a solid foundation forsubsequent studies.",
  "Limitations and Ethical Considerations": "One limitation of STYLEREMIX is the requirementof needing trained LoRA adapters and the corre-sponding style datasets for their training. This ne-cessitates an additional pre-obfuscation step involv-ing separate style corpi and computational trainingtime. However, this is a one-time expense, andthe same style adapters can be utilized for multi-ple authors. In return, users benefit from a moreinterpretable method for authorship obfuscation.During obfuscation, STYLEREMIX does requirea slightly higher computational time and memorydue to the extra style LoRA adapters comparedto using a finetuned model with instructions. Forthe sequential version of STYLEREMIX, the com-putational time is multiplied by the number ofstyles. The base adapter merging variation out-performs the sequential version and is also moreefficient: the time is only increased by a smallamount from merging adapters then adding themto the base model, rather than requiring multiplemodel forward-passes. In inference, no extra timeis added, since LoRA weights are seamlessly addedto the original model (Hu et al., 2021); see Ap-pendix C.4 for further analysis. Finally, we notethat for the adapter merging with LoraHub, there isalso additional time over the base adapter mergingto identify the optimal weights.An additional limitation is that STYLEREMIX isdeveloped and tested only on English. However,we believe that the framework (identifying styleaxes, training LoRA adapters, evaluating originaltext on these axes, and perturbing based on theseevaluations) could be generalized to any language.As such, we believe our pipeline can be effectivelyadapted to obfuscate other languages; we leave theexploration of these adaptations to future work.Lastly, our work also has some potential risks.Though the intention of authorship obfuscation isto protect identities in sensitive situations, there isa possibility that malicious users could misuse ourmethod. We acknowledge this as a potential riskfor any authorship obfuscation method, which isinherent when creating these methods.",
  "Janek Bevendorff, Martin Potthast, Matthias Hagen, andBenno Stein. 2019. Heuristic authorship obfuscation.In Annual Meeting of the Association for Computa-tional Linguistics": "Michael Brennan, Sadia Afroz, and Rachel Greenstadt.2012. Adversarial stylometry: Circumventing author-ship recognition to preserve privacy and anonymity.ACM Transactions on Information and System Secu-rity (TISSEC), 15. Angela Fan, Shruti Bhosale, Holger Schwenk, ZhiyiMa, Ahmed El-Kishky, Siddharth Goyal, Man-deep Baines, Onur Celebi, Guillaume Wenzek,Vishrav Chaudhary, Naman Goyal, Tom Birch, Vi-taliy Liptchinsky, Sergey Edunov, Edouard Grave,Michael Auli, and Armand Joulin. 2020.Be-yond english-centric multilingual machine transla-tion. arXiv preprint. Jillian Fisher, Ximing Lu, Jaehun Jung, Liwei Jiang,Zaid Harchaoui, and Yejin Choi. 2024. JAMDEC:Unsupervised authorship obfuscation using con-strained decoding over small language models. InProceedings of the 2024 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies(Volume 1: Long Papers), pages 15521581, MexicoCity, Mexico. Association for Computational Lin-guistics.",
  "Chengsong Huang, Qian Liu, Bill Yuchen Lin, TianyuPang, Chao Du, and Min Lin. 2023. Lorahub: Effi-cient cross-task generalization via dynamic lora com-position. ArXiv, abs/2307.13269": "Joel Jang, Seungone Kim, Bill Yuchen Lin, YizhongWang, Jack Hessel, Luke Zettlemoyer, HannanehHajishirzi, Yejin Choi, and Prithviraj Ammanabrolu.2023. Personalized soups: Personalized large lan-guage model alignment via post-hoc parameter merg-ing. ArXiv, abs/2310.11564. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, Llio Renard Lavaud,Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,Thibaut Lavril, Thomas Wang, Timothe Lacroix,and William El Sayed. 2023. Mistral 7b. Preprint,arXiv:2310.06825.",
  "Are you robert or roberta? deceiving online author-ship attribution models using neural text generators.Preprint, arXiv:2203.09813": "Jaehun Jung, Peter West, Liwei Jiang, Faeze Brah-man, Ximing Lu, Jillian Fisher, Taylor Sorensen,and Yejin Choi. 2024. Impossible distillation: fromlow-quality model to high-quality dataset & modelfor summarization and paraphrasing.Preprint,arXiv:2305.16635. Georgi Karadzhov, Tsvetomila Mihaylova, YasenKiprov, Georgi Georgiev, Ivan Koychev, and PreslavNakov. 2017. The case for being average: A medi-ocrity approach to style masking and author ob-fuscation. International Conference of the Cross-Language Evaluation Forum for European Lan-guages, pages 173185.",
  "The power of scale for parameter-efficient prompttuning. In Conference on Empirical Methods in Nat-ural Language Processing": "Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:Optimizing continuous prompts for generation. Pro-ceedings of the 59th Annual Meeting of the Associa-tion for Computational Linguistics and the 11th Inter-national Joint Conference on Natural Language Pro-cessing (Volume 1: Long Papers), abs/2101.00190. Alisa Liu,Maarten Sap,Ximing Lu,SwabhaSwayamdipta, Chandra Bhagavatula, Noah A. Smith,and Yejin Choi. 2021. DExperts: Decoding-time con-trolled text generation with experts and anti-experts.In Proceedings of the 59th Annual Meeting of theAssociation for Computational Linguistics and the11th International Joint Conference on Natural Lan-guage Processing (Volume 1: Long Papers), pages66916706, Online. Association for ComputationalLinguistics.",
  "Shuai Liu and Jonathan May. 2024.Style transferwith multi-iteration preference optimization. ArXiv,abs/2406.11581": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A robustly optimized bert pretraining ap-proach. ArXiv, abs/1907.11692. Ximing Lu, Faeze Brahman, Peter West, Jaehun Jang,Khyathi Raghavi Chandu, Abhilasha Ravichander,Lianhui Qin, Prithviraj Ammanabrolu, Liwei Jiang,Sahana Ramnath, Nouha Dziri, Jillian R. Fisher,Bill Yuchen Lin, Skyler Hallinan, Xiang Ren, SeanWelleck, and Yejin Choi. 2023. Inference-time policyadapters (ipa): Tailoring extreme-scale lms withoutfine-tuning. In Conference on Empirical Methods inNatural Language Processing. Ximing Lu, Peter West, Rowan Zellers, Ronan Le Bras,Chandra Bhagavatula, and Yejin Choi. 2021. Neuro-Logic decoding: (un)supervised neural text genera-tion with predicate logic constraints. In Proceedings",
  "Alec Radford, Jeff Wu, Rewon Child, David Luan,Dario Amodei, and Ilya Sutskever. 2019. Languagemodels are unsupervised multitask learners": "Colin Raffel, Noam Shazeer, Adam Roberts, Kather-ine Lee, Sharan Narang, Michael Matena, YanqiZhou, Wei Li, and Peter J. Liu. 2020. Exploring thelimits of transfer learning with a unified text-to-texttransformer. Journal of Machine Learning Research,21(140):167. Alexandre Ram, Guillaume Couairon, Mustafa Shukor,Corentin Dancette,Jean-Baptiste Gaya,LaureSoulier, and Matthieu Cord. 2023. Rewarded soups:towards pareto-optimal alignment by interpolatingweights fine-tuned on diverse rewards.ArXiv,abs/2306.04488. Alexandre Ram, Matthieu Kirchmeyer, ThibaudRahier, Alain Rakotomamonjy, Patrick Gallinari,and Matthieu Cord. 2022. Diverse weight averag-ing for out-of-distribution generalization.ArXiv,abs/2205.09739.",
  "Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:Sentence embeddings using siamese bert-networks.In Conference on Empirical Methods in Natural Lan-guage Processing": "Jonathan Schler, Moshe Koppel, Shlomo Argamon, andJames W Pennebaker. 2006a. Effects of age and gen-der on blogging. In AAAI spring symposium: Compu-tational approaches to analyzing weblogs, volume 6,pages 199205. Jonathan Schler, Moshe Koppel, Shlomo Argamon, andJames W Pennebaker. 2006b. Effects of age and gen-der on blogging. In AAAI spring symposium: Compu-tational approaches to analyzing weblogs, volume 6,pages 199205.",
  "Emma Strubell, Ananya Ganesh, and Andrew McCal-lum. 2019. Energy and policy considerations fordeep learning in nlp. ArXiv, abs/1906.02243": "Gemma Team, Thomas Mesnard, Cassidy Hardin,Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,Laurent Sifre, Morgane Rivire, Mihir SanjayKale, Juliette Love, Pouya Tafti, Lonard Hussenot,Pier Giuseppe Sessa, Aakanksha Chowdhery, AdamRoberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amlie Hliou, Andrea Tac-chetti, Anna Bulanova, Antonia Paterson, BethTsai, Bobak Shahriari, Charline Le Lan, Christo-pher A. Choquette-Choo, Clment Crepy, Daniel Cer,Daphne Ippolito, David Reid, Elena Buchatskaya,Eric Ni, Eric Noland, Geng Yan, George Tucker,George-Christian Muraru, Grigory Rozhdestvenskiy,Henryk Michalewski, Ian Tenney, Ivan Grishchenko,Jacob Austin, James Keeling, Jane Labanowski,Jean-Baptiste Lespiau, Jeff Stanway, Jenny Bren-nan, Jeremy Chen, Johan Ferret, Justin Chiu, JustinMao-Jones, Katherine Lee, Kathy Yu, Katie Milli-can, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikua, Mateo Wirth, MichaelSharman, Nikolai Chinaev, Nithum Thain, OlivierBachem, Oscar Chang, Oscar Wahltinez, Paige Bai-ley, Paul Michel, Petko Yotov, Rahma Chaabouni,Ramona Comanescu, Reena Jana, Rohan Anil, RossMcIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith,Sebastian Borgeaud, Sertan Girgin, Sholto Douglas,Shree Pandya, Siamak Shakeri, Soham De, Ted Kli-menko, Tom Hennigan, Vlad Feinberg, WojciechStokowiec, Yu hui Chen, Zafarali Ahmed, ZhitaoGong, Tris Warkentin, Ludovic Peran, Minh Giang,Clment Farabet, Oriol Vinyals, Jeff Dean, KorayKavukcuoglu, Demis Hassabis, Zoubin Ghahramani,Douglas Eck, Joelle Barral, Fernando Pereira, EliCollins, Armand Joulin, Noah Fiedel, Evan Senter,Alek Andreev, and Kathleen Kenealy. 2024. Gemma:Open models based on gemini research and technol-ogy. Preprint, arXiv:2403.08295. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288. Adaku Uchendu, Zeyu Ma, Thai Le, Rui Zhang, andDongwon Lee. 2021. Turingbench: A benchmarkenvironment for turing test in the age of neural textgeneration. In Conference on Empirical Methods inNatural Language Processing.",
  "Mitchell Wortsman, Gabriel Ilharco, Samir YitzhakGadre, Rebecca Roelofs, Raphael Gontijo-Lopes,": "Ari S. Morcos, Hongseok Namkoong, Ali Farhadi,Yair Carmon, Simon Kornblith, and Ludwig Schmidt.2022. Model soups: averaging weights of multiplefine-tuned models improves accuracy without increas-ing inference time. ArXiv, abs/2203.05482. Eric Xing, Saranya Venkatraman, Thai Le, and Dong-won Lee. 2024a. Alison: Fast and effective stylo-metric authorship obfuscation. In Proceedings ofthe AAAI Conference on Artificial Intelligence, vol-ume 38, pages 1931519322. Eric Xing, Saranya Venkatraman, Thai Le, and Dong-won Lee. 2024b. Alison: Fast and effective stylo-metric authorship obfuscation. Proceedings of theAAAI Conference on Artificial Intelligence, 38:1931519322. Jingjing Xu, Xu Sun, Qi Zeng, Xiaodong Zhang, Xu-ancheng Ren, Houfeng Wang, and Wenjie Li. 2018.Unpaired sentiment-to-sentiment translation: A cy-cled reinforcement learning approach. In Proceed-ings of the 56th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 979988, Melbourne, Australia. Associationfor Computational Linguistics.",
  "B.1Random Selection of Styles": "In .2, we describe a simple automaticmethod to select the style axes to change for eachauthor. It requires creating an author vector, whichis composed of the ten style axes automatic evalu-ations, and finding the difference for each authorcompared to the average vector of all authors in adomain. In order to test the efficacy of our styleaxes selection method, we compare the results ofSTYLEREMIX when selecting the styles axes in thisway and randomly (over n = 3 different seeds). shows the average and standard devia-tion of the drop rate, grammar score, content preser-vation score and overall task score for each domainrandomly choosing 1 4 styles (circles) and us-ing our automatic method of style axes selection(stars). First, we notice that overall, the grammarand content preservation is mostly similar for bothrandom and the automatic method. However, wedo see a large difference in obfuscation drop rate,especially in speech ( 18% average) and Scholar( 8 average). These datasets have more modern,similar styles, which might need a more targetedobfuscation rather than the novels (which are writ-ten in older English) and the blog (which are veryinformal).",
  "B.3Shuffling Styles using the SequentialMethod": "One version of STYLEREMIX described in Sec-tion 2.2 is the sequential method, which runs theoriginal text through each adapter sequentially. Wehypothesized that the order in which the styles wererewritten might affect the final outcome. To testthis, we randomly shuffled the order of the adaptersof the styles axes over n = 3 different seeds whenchanging 2 4 styles and tested automatic evalua-tions as we did in the main experiment. shows the average and standard devia-tion for all the automatic evaluations for each do-main and different number of styles changed. Wefirst note that grammar and content preservation re-mains similar, with very low standard deviation.However, for specific domains, the obfuscationdrop rate has a large variation between the threerandom shuffles. This most diverse obfuscationdrop rates seen in Speech ( 14% standard devi-ation) and Blog ( 9% standard deviation). Thisindicates that the order of adapter in the sequentialmethod could contribute to the overall efficacy ofthe method. Future work could experiment morewith these findings. shows the automatic evaluations whenwe shuffle 2 4 style axes adapters.",
  "B.4Number of Styles Change": "In STYLEREMIX the user can decide how manystyle adapters to use during obfuscation. We testedhow obfuscation drop rate, grammar, and contentpreservation is affected when more style adapterare added. For this experiment, we used the basemodel adapter method and selected 1 7 stylesusing the difference from the author vector to theaverage domain vector. shows all the automatic evaluations foreach number of style. At first, we see a stead in-crease in both obfuscation drop rate and overallscore as we increase style adapters. This corre-sponds with changing more elements of the origi-nal text. However, as mentioned in our main paper,we see on average a 5% decrease in overall taskscore when using 4 to 5 style adapters. Then, as thenumber of style adapter increase, we see a stead de-crease in content preservation and grammar. Thiscorrelates with a qualitative decrease in generationsseen as we increase the styles over 5.",
  "B.5Author Style Vector Analysis": "In the pre-obfuscation phase, we choose 7 specificstyle axes to train the LoRA adapters; length, use offunction words, grade level, voice, use of sarcasm,formality, and writing intent. Some of these styleaxes have rule-based evaluations, and others haveclassifier-based evaluations. We used these auto-matic evaluations to create a unique author vectorfor each author in a domain and use the differencein this vector compared to other authors in the same",
  ": PCA, clustering analysis": "domain to choose the styles axes to change duringobfuscation. Although these selected style axes arejust a subsample of suitable options, we wanted toexplore how well these author vectors separate theauthors in our test data set.To analyze this, we first created an author vectorfor each author by taking the average of each auto-matic evaluation over the paragraphs in the authorstest set. This resulted in 14 (authors), vectors with7 (style axes) entries each. We then performed aprinciple component analysis (PCA) to reduce thesize of the vector dimension to explain at least 90%of the variance in the data (it went from 7 to 4 di-mensions). We note that the first two dimensionsaccount for 70% of the variation. compares all the authors (across all do-mains) using the first and second component of thePCA. First, we notice that the Scholar (triangles)and Speech (circle) domains have distinct clustersaway from the other two domains. The most spreadout domain is Blog (square) with one author quitedifferent from the rest. Lastly, we see that thenovel (start) dataset is closely clustered together,but are quite similar to 4 of the blog authors. Wenote that four of the blog authors have more story-telling writing styles, while the last one has a morediary-like, very informal writing style. This seemsconsistent then that it would cluster similarly asnovels.Overall, this analysis showed starting evidencethat our style axes vectors were able to separatethe diverse writing styles. Future research couldcontinue to explore the types of style axes that aremost important when obfuscating.",
  "In Appendix G we provide more examples fromeach author in the AUTHORMIX. We note that we": "selected these samples by randomly selecting 3,paragraphs of less than 45 words for each authorand then selecting the example from these three.For STYLEREMIX, we used the base model adaptermethod with 3 style adapters. From these examples,especially the Blog and Novels, we see the qualita-tive benefits of STYLEREMIX and its flexibility toadapt to different original author styles.",
  "B.7Tradeoff between Obfuscation, ContentPreservation, and Grammar": "We want to note that there is a natural trade-offbetween authorship obfuscation, content preserva-tion, and grammar. For example, a naive copyingbaseline would have high grammar and perfect con-tent preservation but low obfuscation. On the otherextreme, a complete gibberish output would scorevery low on grammar and content but high on ob-fuscation.This phenomenon is well-documented in the con-text of style transfer. To assess the overall qual-ity of generations where there are multiple objec-tives, previous work in style transfer (Krishna et al.,2020b; Hallinan et al., 2023; Xu et al., 2018; Patelet al., 2023) proposes taking the product (or geo-metric mean) of the metrics (instead of drop rate,for style transfer we have target style strength, stillbounded). The intuition is that these style transfersystems should jointly optimize all metrics ratherthan just one or two; this is reflected in taking aproduct.In line with this past style transfer work, wechoose to use the product, as we think high-qualityobfuscations should jointly prioritize the three met-rics of fluency, meaning similarity, and obfuscation,and so that we do not encourage systems that onlyoptimize one or two of these metrics. However, wenote that certain aspects might be more importantfor users than others, and they might not want touse an equally weighted total.However, as an alternative overall metric, wepresent the overall score as an equally weightedaverage of the drop rate, grammar, and contentscore below, rather than a product. See these re-sults in . We note that this was used in otherauthorship obfuscation papers as an overall totalmetric as well (Fisher et al., 2024). Here, we againsee STYLEREMIX performs best overall in 3 ofthe 4 datasets. Note that the decrease in perfor-mance on Scholar split of AUTHORMIX is due tothe very low obfuscation rate among all methods,which results in only a difference of 5% between",
  "C.1Style Axes Selection and Evaluation": "We choose seven different style axes. The first threestyle axes have rule-based evaluation; length, useof function words, and grade level. For length, weevaluate using the average words per sentence andfor function words we use the number of functionwords. Additionally, we incorporated \"grade level,\"which primarily measures the number of syllables.Since this measure can vary slightly, we averagedthree similar metrics: the Flesch-Kincaid (FK;Flesch, 1948), Linsear Write (L; OHayre), andthe Gunning Fog Index (GF; Gunning, 1952) met-rics. The exact formulas are given below; for moredetails, see",
  "where complex words are words with three ormore syllables": "The next four style axes have model-based eval-uation; use of sarcasm, voice (active or passive),formality, and writing intent (descriptive, expos-itory, narrative, and persuasive). Although thesewere chosen arbitrarily, we believe they do reflectsome unique aspects of authorship style. How-ever, these styles do require a unique classifierto automatically evaluate a text.For formal-ity we used a RoBERTa-base (Liu et al., 2019)based formality classifier (Babakov et al., 2023), found at for the other three axes (voice, sar-casm, and writing intent) there was not a reliable,existing model, so we trained our own classifiers.We follow the same procedure to make DISC de-tailed in .1, but 1) with different basetraining data, to ensure that there is no overlap be-tween the classifier and adapter data and 2) only forthe following style elements: voice passive, voiceactive, sarcasm less, sarcasm more, and persuasive,expository, narrative, and descriptive. With thenew datasets of length 1500 for each style element,we then train RoBERTA-large (Liu et al., 2019)discriminators for the voice, sarcasm, and writingintent categories, splitting the train into 85% trainand 15% dev set. We set the seed to 0 and trainwith a batch size of 128, learning rate of 5e-5, andfor 5 epochs.For all models, we choose the checkpoint withthe best evaluation accuracy product (to ensurehigh accuracy for all classes); this correspondedto 100%, 99.1%, 45.5% for sarcasm, voice, andtype respectively. Each model took approximately1 hour to train using 1 NVIDIA A100 GPU with80 GB of VRAM.",
  "C.2DISC Training Data and Evaluations": "We use GPT4-Turbo (OpenAI, 2023) to distill thestyle axes into 16 parallel training sets. We col-lect 1500 paragraphs from Wikipedia, books andplays, and blogs, then prompt GPT4 with the fol-lowing: Rewrite the following paragraph to in-clude the same content but specific prompt\\n Para-graph: paragraph \\n Rewrite: where paragraph isthe original data. Depending on the target style, wechange the specific prompt to:",
  "Descriptive writing style: with descriptivewriting style": "We use sampling with a temperature of 1.0. As aresult of this prompting, we achieve 1500 16 =24000 generations spanning 16 unique style direc-tions from GPT-4.We then validate the quality of this data. Foraxes with available automatic metrics, specificallylength, function words, grade level, and formality,we run their respective metrics on the original texts,and on the GPT-4 generations in both directions,ie., we run the formality classifier on the originaltexts, and on both the more and less formal GPT-4generations. For the axes without automatic evalu-ation, we instead randomly evaluate 10% of them.Specifically, we randomly combine generated datafrom the same style axis but different directions(such as more and less sarcasm), and ask annota-tors (three NLP experts) to label if the style axis ishigh or low (or the specific type for Writing Type),then compute the accuracy. shows the results. For the metrics thatwe can automatically evaluate, our generated data captures the desired axes and directions well; forexample, the texts steered towards higher lengthhave the highest average number of words per sen-tence. For sarcasm and voice, human evaluationsof 97.7% and 93.7% respectively indicate that thegenerations match the targeted directions. For writ-ing intent, the human evaluation accuracy is 77.7%which is still a good number as the task of discrim-inating between four classes is inherently morecomplex.",
  "C.3Style Adapter Training": "We train LoRA adapters (Hu et al., 2021) usingeach of the 16 generated parallel datasets. Specif-ically, we train LLama-3 8B (base model) on thefollowing prompt for each of the datasets:<bos>###Original:{original}\\n###Rewrite: {rewrite} <eos>where original and rewrite denote the originaltext and rewrite is the text we generated from GPT-4. Note that the format we train on is the same forall parallel datasets to make future model mergingmore effective.We train the 16 LoRA modules each for 5 epochswith a seed of 0, batch size of 6, and a max se-quence length of 512; we choose the checkpointwith the best eval loss and have an early stoppingcriteria of 5. For LoRA parameters, we use user = 32, the rank of the matrix, and the alpha anddropout values of 32 and 0.01 respectively. Overall,each LoRA adapter involves training 13 million pa-rameters each, about 0.16% of the total parametersin LLama-3 8B.All of our models train well over time on bothtrain and eval loss; please see our repository forexact training curves and loss numbers for the 16models. We train each of the models on a singleA100 80GB GPU for about 2 hours each.",
  "Given a model with weights W0 of dimension ddLoRA freezes W0 and trains two matrices: A of": "size r d and B of size d r. At inference, weuse the new weights of W = W0 + BA.In the situation when we have n LoRA adapa-ters, parameterized by A1...An and B1...Bn andwant to ensemble them for inference, we use con-catenation. Specifically, we concatenate each ofthe A1...An matrices resulting in a matrix A1...nof size nr d. Similarily, we concatenate each ofB1...Bn resulting in a matrix B1...n of size d nr.We then can combine the matrices the same way toget new weights of W = W0 + B1...nA1...n. No-tably, we have no additional inference latency byconcatenating the vectors, only a slightly increasedfixed matrix multiplication cost.",
  "C.5Style and Weight Selection": "As described in our paper, we developed an auto-matic method for selecting the style axes to change,direction, and weights of the adapters. First, wecreate an author vector for each author in a domain,which is a vector with 10 automatic evaluations;average words/sentences, average number of func-tion words, average grade level (using FK, L, GF)(Flesch, 1948; OHayre; Gunning, 1952), averagelikelihood score from formality classifier (Babakovet al., 2023), average likelihood score from sarcasmclassifier (see Appendix C.1 for more details), av-erage likelihood from a voice classifier (see Ap-pendix C.1 for more details), average classificationinto each of the four writing intents. We label thisvector for author i as xi R10.In order to select the k number of styles axesto change, we use the other authors in the samedomain as a baseline. Specifically, we average thevalues from all authors in the domain and find thestyles of author i that are furthest from this averagevector. More specifically, we use the followingformula:",
  ",": "where we have m total authors in the domain andtopk(y) is a function which selects the rows of ywith the highest values. Similarly, we use the signof this difference to decide on the direction of thechange. For example, if the sign of the differenceis negative, then the authors style value is lowerthan the average and we will implement a higherdirection (driving the style up to average).Once the styles axes are selected, we use dif-ferent methods for choosing the adapter weights for each style axes. First, we also use the authordifference vector to select the weight of the adapter.To do this, we calculate the number of standarddeviation the authors value is from the averagevector. We then use this metric to map to a staticweight; see . We note that these weightswere selected in line with past work (Huang et al.,2023).Second, we employ a non-gradient based op-timization method called LoraHub (Huang et al.,2023). This method uses a few validation examplesto optimize the values. For this method, we devel-oped our our loss function which is the sum of thechosen style axes automatic evaluations as well asthe grammar. Specifically,",
  "Style Adapters": "Trump[length, grade level, persuasive, sarcasm][0.9, 0.9, 0.9, 0.9][1.27, 1.15, 0.88, 0.85]Obama[grade level, sarcasm, persuasive, length][0.7, 0.7, 0.7, 0.7][0.70, 0.70, 0.70, 0.70]Bush[formal, descriptive, grade level, sarcasm][0.7, 0.9, 0.7, 0.7][0.32, 0.07, 0.34, 1.05]Hemingway[sarcasm, grade level, expository, length][0.9, 0.9, 0.7, 0.9][0.98, 0.80, 0.66, 0.96]Fitzgerald[sarcasm, descriptive, grade level, length][0.7, 0.7, 0.7, 0.7][0.73, 0.70, 0.67, 0.72]Woolf[length, grade level, formal, narrative][0.7, 0.7, 0.7, 0.9][0.06, 0.30, 0.77, 0.24]Scholar-H[sarcasm, expository, voice, formal][0.9, 1.5, 0.7, 0.7][1.44, 1.36, 0.60, 0.74]Scholar-PP[formal, grade level, descriptive, voice][0.9, 0.7, 1.5, 0.9][1.24, 0.55, 1.47, 0.59]Scholar-QQ[length, narrative, formal, grade level][0.9, 1.5, 0.7, 0.9][0.91, 1.25, 0.70, 0.90]Blog-1[formal, narrative, length, grade level][0.7, 0.9, 0.9, 0.9][1.07, 1.16, 0.80, 0.67]Blog-2[expository, length, formal, sarcasm][0.7, 0.7, 0.7, 0.7][0.76, 0.70, 0.71, 0.66]Blog-3[formal, grade level, sarcasm, descriptive][0.9, 0.9, 0.9, 0.7][1.15, 0.90, 0.90, 0.70]Blog-4[narrative, formal, sarcasm, length][0.7, 0.7, 0.7, 0.7][0.58, 0.28, 0.46, 0.95]Blog-5[descriptive, voice, grade level, formal][0.7, 0.9, 0.7, 0.7][0.69, 0.70, 0.58, 0.59] : Comparison of the initial base weights, chosen using the standard deviation to static mapping, and theoptimized LoraHub weights, found using our customized loss function. We show the style axes changed, the baseweights and the LoraHub weights for each author in each domain.",
  ": Details of AUTHORMIX, including the num-ber of samples for the test/eval/train for each domain": "As mentioned, we wanted to use a test datasetwhich had a wide range of diverse authorship stylesas well as domains. For this reason, we decidedto create a new data set on authorship obfuscationcalled AUTHORMIX. This dataset is composed forfour domains; presidential speeches, early 1900sfiction novels, scholarly articles, and dairy-styleblog entries. Altogether, AUTHORMIX containsover 30,000 high-quality paragraphs from 14 au-thors.For the presidential domain, we curate andclean a novel collection of high-quality presidentialspeeches from George W. Bush (n = 38), BarackObama (n = 29), and Donald Trump (n = 26)8,transcribed by the Miller Center (Miller Center ofPublic Affairs)9 at the University of Virginia. Webroke the speeches naturally into paragraphs andthen selected all paragraphs between 2 5 sen-tences. This resulted in a total of n = 13K para-graphs.Similarly, we also decided to develop a newcollection of early 1900s fiction writers from thewith strong writing styles, therefore we choose textfrom books by Ernest Hemingway, F. Scott Fitzger-ald, and Virginia Woolf which were collected fromProject Gutenberg (Gutenberg). We selected thetop 4 most popular books on Project Gutenberg",
  "These presidents were selected due to their diverse stylesbut similar time periods, which minimizes content discrepan-cies.9": "for each author and then again, used the naturalparagraphs from each author. We selected all para-graphs between 2 5 sentences. This resulted in atotal of n = 9K paragraphs.Lastly, we altered the existing data from twocurrent datasets, the Extended-Brennan Greenstad(Brennan et al., 2012) which is a collection ofscholarly short (500-word) paragraphs gatheredfrom Amazon Mechanical Turk (AMT) and theBlog Authorship corpus (Schler et al., 2006b), acollection of blogs (diary-style entries) that wereposted to blog.com. We note, these datasets matchthose used in (Haroon et al., 2021), (Mahmoodet al., 2019), and (Fisher et al., 2024). For the AMTdataset, we used authors \"h\", \"pp\", and \"qq\" and weartificially created paragraphs by chunking the textinto a random collection of 2-5 sentences (as thetext is not naturally broken into paragraphs). Forthe Blog dataset, we used authors \"5546\", \"11518\",\"25872\", \"30102\", \"30407\", we used the naturalparagraphs. Then, to match the speech and noveldomains, we edited to include all paragraphs be-tween 2 5 sentences and 3 words. This resultedin n = 500 and n = 8K paragraphs for the AMTand Blog accordingly.",
  "Stylometric (Stylo)We used (Karadzhov et al.,": "2017) method for AO using stylometric methods,which was originally proposed in the PAN-2016Author Masking Shared Task competition (Mi-haylova et al., 2016). This method calculates met-rics for 12 features that are indicative of style, thenmodifies the text, so these metrics align with an\"average\" value. The \"averages\" were calculatedusing a combination of training sets including thePAN-2016 Author Obfuscation task (Mihaylovaet al., 2016) and public domain books from ProjectGutenberg (Gutenberg). Examples of the metricsthis method uses include the average number ofwords per sentence, word frequency, and the useof uppercase letters. Changes employed includeactions such as sentence splitting and merging, sub- stitution of words with synonyms, and alterationsin spelling. For a full list of metrics and proposedchanges, see the (Karadzhov et al., 2017). To fur-ther enhance the obfuscation process, the methodintroduces \"noise\" by modifying words that dif-fer between English and British English and intro-ducing additional functional words. We make nochanges to the hyperparameters used in the originalmethod. Machine Translation (MT)We used a round-trip machine translation method proposed byKeswani et al. (2016). In this method, they translatethe original text from English to German, Germanto French, and then French back to English. Weenhanced their method by use of the new M2Mtranslation model (Fan et al., 2020), which doesnot rely on English as an intermediate language.",
  "JAMDEC (JD)This method was proposed by": "Fisher et al. (2024) and uses a small languagemodel, GPT2-XL (Radford et al., 2019), as the basemodel. For this method, they use a three stage ap-proach where they extract the keywords of text (toguide generation to have the same content), over-generate using diverse constrained beam search,and then filter based on grammar and content over-lap. We used this models default parameters, witha beam width of 10, and only using the likelihoodkeyword extractors, which was recommended to bejust as effective but take less time. More details ofthis methods implementation can be found (Fisheret al., 2024). ParaphrasingWe used the paraphrasing modelfrom Jung et al. (2024). This model uses GoogleT5 (Raffel et al., 2020) as the base and is finetunedon the dataset DIMPLE, which is a dataset of 4Mhigh-quality pairs of paraphrases. Instruction LLMsLastly, we wanted to comparewith LLMs of similar and bigger sizes. For these,we opted to use instruction tuned models whichcould easily follow instruction to rewrite the text.For each model, we used a temperature of 1.0 anda top-p of 0.9. shows the exact promptsused to generate the generations from each of thebaseline LLMs.",
  "that almost all values were less than 3 standarddeviations, with the majority between 0 2": "Adapter MergingWe used three different abla-tion of our methods; sequential, adapter mergingbase, and adapter merging LoraHub+. For the se-quential method, we averaged results over n = 3random shuffling of style axes orders. For theadapter merging base method we used the weightadapter found from mapping using the standarddeviations.For the adapter merging LoraHub+, we buildon the prior LoraHub method (Huang et al., 2023).We used the weights selected using our mappingmethod as the initial values and then used a non-gradient based optimization (Liu et al., 2020) overa new customized loss function. The loss functionadds together the automatic evaluations from theauthor vector (described in Appendix C.5) for thespecific style axes that are being considered formerging. Note, that since we are optimizing byfinding the lowest loss, if the direction of the styleaxes is \"higher\" we take 1 value and if the di-rection is \"lower\" we just add the value. Lastly,we also add the grammar score into the loss tomaintain good fluency. Then, a non-gradient basedoptimization method is use (Liu et al., 2020). Note,we use non-gradient based due to the large numberof parameters of the model. We provide a compari-son of the base weights chosen and the optimizedLoraHub weights in .",
  "D.5Evaluation Methodology and OtherDetails": "Obfuscation: ClassifierWe train classifiers overeach of the four domains in AUTHORMIX to mea-sure obfuscation during evaluation using their re-spective training and development sets. Specifi-cally, for each of AUTHORMIX- { speech, novels,scholar, blog }, we train a RoBERTa-Large classi-fier (Liu et al., 2019) with a learning rate of 5e-5,batch size of 64, seed of 0, a max length of 256, andfor 10 epochs. We set an early stopping thresholdof 5, and choose the best checkpoint based on thebest evaluation accuracy product (to ensure highaccuracy for all classes).Overall, our final evaluation accuracy productsfor AUTHORMIX- {speech, novels, scholar, blog }",
  "ModelInstruction": "Llama 2\"[INST] SYS\\n You are a helpful assistant.\\n \\n /SYS \\nPlease rewrite the following: <paragraph>[/INST] Rewrite: \"Llama 3\"[INST] SYS \\n You are a helpful assistant.\\n \\n/SYS \\nPlease rewrite the following: <paragraph>[/INST] Rewrite: \"Gemma\"You are a helpful assistant.\\n \\nPlease rewrite the following: <paragraph> Rewrite: \"Mistral\"<s>[INST] You are a helpful assistant.\\n \\nPlease rewrite the following: <paragraph> [/INST] Rewrite: \"",
  "-10.71-20.92-31.23+1.5": ": This shows the static mapping used in thebase adapter merging method. We use the number ofstandard of deviations an authors automatic style scoreis from the average style score of all authors in thatdomain. The static values were chosen base on pastwork (Huang et al., 2023) are 74.5%, 85.6%, 100%, and 70.8%, while the av-erage overall accuracies are 90.6%, 95.8%, 100%,and 93.3%. Further training details including lossfunctions can be found in our repository. We traineach of these models with a single NVIDIA A10080 GB GPU for approximately 2 hours. Content Preservation: Cosine SimilarityWecompute neural text embeddings on the inputsand their obfuscations in Sentence Transformers(Reimers and Gurevych, 2019). Next, we use thecosine similarity between the two vectors to gaugesemantic similarity and get a approximation of con-tent preservatinon. Note that though the cosinesimilarity can output values from -1 to 1, we findon all of our validation dataset (across all datasetsand methods) all similarities between inputs andtheir obfuscations are non-negative, with a boundof 0 to 1. If the similarity metric were to, in a veryrare case, have a negative value, we would set thevalue to 0 so that we could have a still meaning-ful overall product of metrics; however, we neverobserve this. Grammar: CoLATo ensure both fluency andgrammaticality, we use TextAttack (Morris et al.,2020), a RoBERTa-large model (Liu et al., 2019)fine-tuned on the Corpus of Linguistic Acceptabil-ity (CoLA; Warstadt et al., 2018) which includes10,600 sentences with binary annotations for lin-guistic acceptability.",
  "EHuman Evaluation": "We omit the MT and Stylo methods from humanevaluation as Fisher et al. (2024) show that JamDecoutperforms them in previous work for both auto-matic and human evaluation. We also omit 13Band 70B models for fair comparison. Finally, wereport human evaluation for the best performingSTYLEREMIX. We used workers from Amazon Mechanical Turk(AMT) who voluntarily opt-in to the task to anno-tate n = 20 text from each author. Each text wasannotated by n = 3 authors, who were paid at arate of 15$/hour. Our annotators are from English-speaking countries. A screenshot of the interface isshown in",
  "Our agreement numbers for the five metrics wecollect, grammaticality, fluency, content preserva-tion, low content addition, and obfsucation are 99.8,98.2, 95.3, 93.8, and 93.5% respectively": "To compute the overall score, it would be unfairto take the product of our metrics (grammaticality,fluency, content preservation, low content addition,and obfuscation), since content preservation andfluency have two metrics representing their cat-egory, while obfuscation has only one; rewritesthat simply output fluent, content-preserving textswould score higher than more balanced obfusca-tions which sacrifice small amounts of contentpreservation. Instead, we take a weighted prod-uct. Our overall product is computed as:",
  "FAlternative Obfuscation EvaluationMetrics": "ToverifytheobfuscationeffectivenessofSTYLEREMIX, we run an alternative evaluationto measure drop rate using the method from Learn-ing Universal Authorship Representations (LUAR;Soto et al., 2021) and train models to learn au-thorship embeddings for each of the four do-mains in our AuthorMix dataset (speech, novels,scholar, and blog) using the training data. Weuse the default hyperparameters from the code-base (ie, training for 20 epochs, using sentence-transformers/paraphrase-distilroberta-base-v1 asthe base model, etc).Next, we create authorship embeddings for allauthors by passing their validation data into thetrained models with their respective domain wherethey are aggregated, resulting in a single embed-ding for each author. To perform authorship attri-bution and obtain predictions for a set of input dataof size N over some domain (such as speech), wefirst pass the input data through the trained modelto extract individual embeddings before they areaggregated, resulting in N input embeddings. Foreach of these input embeddings, we calculate thecosine similarity with each validation authorshipembedding in the specified domain. The predictedauthorship style is the one with the highest cosinesimilarity.As with our RoBERTa models, we can computethe drop rate for the LUAR method. Recall that thedrop rate is the drop in accuracy of the classifierevaluated on the original text and the obfuscatedtext, where accuracy represents how many of thetext the classifier correctly identified the author.For each domain in AuthorMix, we obtain the ini-tial classification accuracy with the LUAR methodusing the test set. Then, we calculate the LUARdrop rate for our StyleRemix methods and for thebaselines.In , we display from the main ob-fuscation results, but now add two new rows whichwe compute: [NEW] Drop Rate w/ LUAR, the droprate with LUAR, and [NEW] Overall (using DropRate w/ LUAR), the overall obfuscation quality,equivalent to the product of the grammar, contentand Drop Rate w/ LUAR which we compute.Across all datasets, the new authorship attribu-tion results with LUAR aligns with our RoBERTabased results and reinforces the strong obfusca-tion results of StyleRemix over baselines. Specifi- cally, across all datasets, StyleRemix has the high-est LUAR drop rate on the speech, novels, andscholar datasets, and the second-highest LUARdrop rate on the blog dataset, beating much largerbaselines like Llama-3-70b-Inst. This is the sameas the previous obfuscation results and underlinesthe effectiveness of StyleRemix for obfuscation.Furthermore, the new overall metric with LUARdrop rate confirms the results from the original over-all metric: over all datasets, StyleRemix generatesthe best overall obfuscations, beating all baselines.Overall, our additional evaluation using LUARauthorship attribution confirm the previous resultobtained with the RoBERTa classifiers and demon-strates the excellent anonymization capabilities ofStyleRemix.",
  "Size7B13B8B70B7BSeq.AMAM + LoraHub": "AUTHORMIX SpeechDrop Rate18.224.017.616.823.124.110.315.129.234.941.231.4[NEW] Drop Rate w/ LUAR8.37.44.76.85.33.20.07.79.23.323.912.2Grammar67.867.167.170.267.871.254.937.856.761.766.563.9Content83.880.880.880.278.683.989.189.556.471.377.373.9Overall10.313.09.59.512.314.45.15.19.415.321.214.8[NEW] Overall (using Drop Rate w/ LUAR)4.74.02.53.82.81.50.02.62.91.512.35.8 AUTHORMIX NovelsDrop Rate12.213.79.211.313.310.87.013.524.919.328.635.6[NEW] Drop Rate w/ LUAR3.65.05.73.66.45.32.210.416.58.817.931.7Grammar71.873.873.175.470.068.346.336.861.272.668.163.5Content82.980.783.181.581.981.385.288.158.683.776.172.9Overall7.38.25.66.97.66.02.84.48.911.814.816.5[NEW] Overall (using Drop Rate w/ LUAR)2.13.03.52.23.72.90.93.45.95.39.314.7 AUTHORMIX ScholarDrop Rate0.81.51.62.50.00.81.54.66.11.89.211.5[NEW] Drop Rate w/ LUAR6.12.65.26.16.90.90.01.85.210.410.413.0Grammar64.364.964.166.665.369.154.531.062.365.848.644.7Content91.789.788.984.088.991.392.885.860.678.075.368.8Overall0.50.90.91.40.00.50.81.22.30.93.43.5[NEW] Overall (using Drop Rate w/ LUAR)3.61.53.03.44.00.60.00.52.05.33.84.0 AUTHORMIX BlogDrop Rate17.721.321.818.927.522.29.412.156.434.441.042.0[NEW] Drop Rate w/ LUAR7.29.47.55.912.314.26.09.119.412.716.214.2Grammar68.469.171.374.069.069.841.929.160.666.764.965.3Content82.579.078.177.877.880.483.785.845.172.173.774.2Overall10.011.612.110.914.812.53.33.015.416.519.620.4[NEW] Overall (using Drop Rate w/ LUAR)4.15.14.23.46.68.02.12.35.36.17.76.9 : Comparison of obfuscation methods measured by 1) mean drop rate, grammar, meaning similarity, andoverall (the same metrics and results as in ) and 2) additionally with an alternative, LUAR-based droprate metric and a new overall score computed with this drop rate. Bold and underline denote the highest and thesecond-highest score respectively in each row. All metrics displayed in the table are multiplied by 100 for easierviewing of significant figures.",
  "AUTHORMIX-Speech": "AuthorTrumpOriginalAmericans know that in a world where others seek conquest and domination, ournation must be strong in wealth, in might, and in spirit. That is why the United Statesvigorously defends the traditions and customs that have made us who we are.MTThe Americans know that in a world where the other is seeking conquest anddomination, our people must be strong in wealth, power and spirit. Thats why theUnited States defends the traditions and habits that have made us know who we are.StyloHowever, Americans know that in a world where others seek conquest and domina-tion, our nation must be strong in wealth, in might, and in spirit. Lastly, That is whythe United States vigorously depends the traditions and customs that have made uswho we are.JAMDECAmericans know that in a world where others seek conquest and domination, ournation must be strong in wealth, in might, and in spirit. That is why the United Statesvigorously defends the traditions and customs that have made us who we are.ParaphrIn a world where others strive for dominance and control, Americans recognize theimportance of being strong in wealth, strength, and spirit. As a result, the UnitedStates vigorously protects the cultural and historical heritage that has contributed toour identity.Llama2 (7B)As Americans, we recognize the importance of a strong and resilient nation in aworld where power-hungry individuals and nations seek to conquer and dominate.To protect our identity and values, the United States fiercely guards the traditionsand customs that define us as a nation. These beliefs are the bedrock upon whichour great country was built, and they continue to serve as the foundation for ourcontinued prosperity and influence in the world today.Llama3 (8B)Americans understand that in a world where some nations strive for dominance, itis crucial that our own nation remains robust in its economic, military, and moralstrength. This is why the United States actively preserves and upholds the values,customs, and traditions that have shaped our identity and character.Mistral V2Americans recognize the importance of maintaining strength in wealth, militarypower, and cultural values in a global landscape where others may strive for conquestand dominance. Consequently, the United States is committed to safeguarding itsheritage and customs that define its identity.Gemma (7B)In a world where others strive for dominance, Americans believe their nation mustmaintain strength in wealth, military power, and morale. To uphold their heritageand identity, the United States stands firm in protecting its cherished traditions andcustoms. STYLEREMIXAmericans hold the conviction that, in a universe where many nations pursue ex-pansion and hegemony, our country must maintain its robustness in wealth, militaryprowess, and unwavering resolve. Hence, the United States zealously protects thevery traditions and values that have shaped our distinct identity. AuthorObamaOriginalThank you. God bless you. And God bless this country we love.MTThanks for you. God blessed you. And God blesses this country that we love.StyloThank you, god bless you; and and God bless this country we love. JAMDECAnd because we are on the cusp of a new era in human history, one that will bedefined by our ability to boldly go where no one has gone before, to reach for newfrontiers, to seek out new life and new civilizations, and to learn from the mistakesand successes of the past so that we might better prepare ourselves for the challengesof And because, in the words of President Kennedy, we choose to go to the moonin this decade and do the other things, not because they were easy, but because theyare hard, because that challenge is one that we are willing to accept, one we areunwilling to postpone, and one which we intend to win, and the others, too.\"ParaphrThank you, God bless you, and God bless this nation we cherish.Llama2 (7B)Thank you. May God bless you and our country.Llama3 (8B)I would like to express my gratitude to you. May God grant you His blessings. MayGod also continue to bless our beloved nation.Mistral V2I appreciate your kind words. May Gods blessings be upon you. And may Godcontinue to bless the country we cherish deeply.Gemma (7B)Thank you. May God bless you and this beloved nation.",
  "STYLEREMIXThank you. May God bless you, and may He continue to bless this nation that wehold dear": "AuthorBushOriginalHis tax his policies of tax and spend of expanding government rather thanexpanding opportunity are the policies of the past. We are on the path to the future and were not turning back.MTHis taxes his tax and expenditure policy the extension of the government ratherthan the extension of possibilities are the policies of the past. We are on the way tothe future and we will not return.StyloHis tax his policies in tax and spend in expanding government rather than expand-ing opportunity are the policies in the past. We are on the path to the future andwe re not turning back.JAMDECHis tax his policies of tax and spend of expanding government rather thanexpanding opportunity are the policies of the past. We are on the path to the future and were not turning back.ParaphrThe current tax policies, which prioritize government expansion over promotingeconomic growth, are a reflection of the past. However, we are on the path to thefuture and we will not retreat.Llama2 (7B)His tax and spend policies, which focused on expanding the government rather thancreating opportunities for growth, are a relic of the past. We are moving forwardwith a renewed commitment to innovation and progress, leaving behind the stagnantpolicies of the past.Llama3 (8B)His tax philosophy of spend and tax, which prioritizes government expansion overcreating opportunities, is a relic of the past. Were now on a trajectory towards abrighter future and we wont be reversing course.Mistral V2His outdated tax and spending policies, which focus more on expanding governmentrather than creating opportunities, are no longer relevant. We are moving forwardtowards the future and will not revert to these old ways.Gemma (7B)His tax and spending policies, which aim to expand government rather than fosteropportunity, reflect outdated ideologies. We are committed to embracing a forward-looking vision that prioritizes progress and innovation. STYLEREMIXHes all about taxing up a storm and spending it all, plus, hes all about growinggovernment big instead of creating more chances for everyone. Thats really old-school thinking. Were all about moving forward and not looking back.",
  "AuthorHemingway": "Original\"Dont worry about money,\" Mike said. \"You can pay for the car, Jake, and Ill sendyou my share.\"MT\"They dont worry about money,\"Mike said. \"You can pay for the car, Jake, and Illtell you my part.\"Stylo\" Don t worry about money, \"Mike said, and \" You can pay for the wagon, Jake,and I ll send you my party, and \"JAMDEC\"We can pay the family and move on with our lives, and we wont have to worryabout where were going to live or whos going to take care of us when were gone.ParaphrMike advised Jake not to worry about finances. He suggested that Jake could pay forthe car himself, and he would then send him his share of the cost.Llama2 (7B)\"Dont worry about money,\" Mike said. \"Ill cover my share of the cost, and you canpay for Jakes.\"Llama3 (8B)Mike reassured Jake, \"Dont stress about the cost, Jake. You take care of buying thecar, and Ill cover my portion of the expense.\"Mistral V2Mike assured Jake, \"You dont need to be concerned about the cost of the car. Illcover my portion and send it to you later.\"Gemma (7B)\"Dont worry about the cost,\"Mike said. \"Ill cover the car, Jake, and Ill transfermy share to you later.\" STYLEREMIXMike addressed the financial concerns with a seemingly casual remark. \"Dont stressover finances,\" he assured, \"Jake, you handle the car expenses, and Ill cover myportion.\" This approach, seemingly nonchalant about the financial management, wasdelivered with a casual air. AuthorFitzgeraldOriginalShe liked him immediately reddish-brown mustache under watery blue eyes thathad something in them that these other eyes lacked, some quality of appreciation.They exchanged stray sentences through dinner, and she made up her mind to seehim again.MTShe loved her immediately a red-brown moustache under the blue eyes of water thathad something in them, which was missing in those other eyes, a kind of appreciation.They exchanged stray for dinner, and she awakened her mind to see him again.StyloShe liked her immediately a reddish-brown mustache, under watery blue eyes thathad something of them that these other eyes lacked, some quality of appreciation.They exchanged stray sentences through supper, and she made up her mind to see itagain.JAMDECParaphrShe immediately fell in love with him, noticing his distinctive reddish-brown mus-tache beneath his watery blue eyes, which had a unique quality that set him apartfrom other eyes. They exchanged brief sentences during dinner, and she made up hermind to see him again.Llama2 (7B)She was immediately taken by him, noticing the reddish-brown mustache framinghis watery blue eyes. There was something unique about those eyes, a quality ofthoughtfulness that set them apart from the other eyes she had seen. They exchangedbrief words throughout dinner, and she found herself making plans to see him again.Llama3 (8B)As soon as she met him, she was drawn to him - the distinctive reddish-brownmustache and watery blue eyes that seemed to hold a certain depth of understanding.Throughout dinner, they exchanged brief, scattered phrases, and she found herselfresolved to reconnect with him again in the future.Mistral V2From the first moment, she was drawn to him. His reddish-brown mustache framedwatery blue eyes that held a unique quality of appreciation. During dinner, theyengaged in brief conversations, and she found herself looking forward to anotherencounter with him. Gemma (7B)She instantly admired his reddish-brown mustache framing his watery blue eyes,which held a depth that set them apart from the others. They exchanged fleetingwords throughout dinner, and she resolved to meet him again. STYLEREMIXShe felt an instant connection with him. His reddish-brown mustache framed hiswatery blue eyes, which seemed to hold a special kind of appreciation, different fromothers she had seen. During dinner, they shared a few casual conversations, and shedecided she wanted to meet him again. AuthorWoolfOriginal\"But I must look so queer!\"she cried, running over to the glass and looking first thisside then that. Then she snatched it off again, for there was a tap at the door. Couldit be Sir William Bradshaw? Had he sent already?MT\"But I need to look so beautiful!\"she cried, blown to the glass and looked first onthat side, then on that. Then she pushed him, because there was a ball at the door.Can he be Sir William Bradshaw? Has he already sent?Stylo\" But I must listen so queer; and \"she cried, running over to the drink and lookingfirst this side then that, then she snatched it off once again, for there was a tap at thedoor; could it be Sir William Bradshaw, had he sent already?JAMDECshe cried, looking at herself in the mirror. \"I dont know whats the matter with me.\"What is the matter with me?\" she asked, looking at herself in the glass. She lookedagain, and there he was, sitting in a chair by the table.ParaphrShe exclaimed, \"I must appear unusual!\" as she ran towards the glass and carefullyexamined it, first on one side and then the other. Then she quickly snatched it offagain, as there was a tap at the door. Could it be Sir William Bradshaw? Had healready sent something?Llama2 (7B)\"But I must look so strange!\" she exclaimed, rushing over to the mirror and examiningherself from different angles. She then quickly removed the mirror from the wall, asshe heard a knock at the door. \"Could it be Sir William Bradshaw? Has he arrivedalready?\" she wondered aloud.Llama3 (8B)But I must look so peculiar! she exclaimed, hurrying to the mirror and scrutinizingherself from every angle. She then hastily removed the mirror, her attention distractedby a knock at the door. Could it be Sir William Bradshaw? Had he already arrived,sent by her aunt?Mistral V2\"Oh dear, I must look quite strange!\" she exclaimed, hurrying towards the mirror.She glanced at her reflection from various angles before hurriedly removing themirror from the wall. A knock at the door interrupted her, and she wondered if itcould be Sir William Bradshaw. Had he arrived already?Gemma (7B)In a sudden burst of anxiety, she exclaimed, \"Oh, I must look so strange!\" and rushedover to the glass. She peered cautiously through, casting her gaze first one wayand then the other. Moments later, she snatched the glass away, for a tap at thedoor startled her. The unfamiliar sound made her wonder if it could be Sir WilliamBradshaw, and if he had already arrived. STYLEREMIXShe suddenly felt awkward when she noticed herself in the mirror, turning this wayand that to check her appearance. Just as she was about to examine herself further,someone knocked on the door. She wondered if it might be Sir William Bradshaw,wondering if he had already arrived.",
  "AuthorScholar-hOriginalSo what are the takeaways from the Nepal case study?MTSo what are the Takeaways of the Nepal case study?StyloSo what are the takeaways from the Nepal case study?": "JAMDECFirst of all, it is important to note that this is not the first time that the Nepalesegovernment has had.ParaphrThe Nepal case study has provided valuable insights.Llama2 (7B)What are the key takeaways from the Nepal case study?Llama3 (8B)Here are the key lessons learned from the Nepal case study: Please let me know ifthis meets your expectations or if you would like me to make any changes!Mistral V2What key insights can be gained from the Nepal case study?Gemma (7B)What are the key lessons learned from the Nepal case study? STYLEREMIXImagine yourself in the heart of a Nepal case studywhat exactly are the lessonsyoure likely to glean? The narrative unfolds there, rich with nuances and insights,each experience potentially etched into the global lexicon. AuthorScholar-ppOriginalEven if you cant eliminate identity theft, you can get instant fraud notifications anytime your identity is used without your knowledge. Prevent identity theft with MyID.MTEven if you cannot eliminate identity theft, you can receive instant fraud messages atany time if your identity is used without your knowledge. Avoid identity theft withMyID.StyloConversely, Even if you can t eliminate identity theft, you can obtain instant fraudmodifications any time your identity is used without your knowledge; prevent identitytheft with MyID.JAMDECIf you have questions about your identity, or if you think someone is trying to useit without your permission, click here to get instant alerts from the Identity TheftResource Center (ITRC) and the Federal Trade Commission (FTC) about identitytheft and how to protect. Identity theft is the theft of your identity by someone whouses your identity without your permission, such as by using your Social Securitynumber to open a bank account or apply for a credit card, or by opening up a newbank account in your name and using it to receive.ParaphrDespite the fact that it may not be possible to completely prevent identity theft, youcan receive instant notifications of any fraudulent activity involving your identitywithout your knowledge. This helps to protect your identity and prevent any potentialthreats.Llama2 (7B)Although identity theft cannot be completely eliminated, MyID offers instant fraudnotifications whenever your identity is used without your knowledge. Protect yourselffrom identity theft with MyID.Llama3 (8B)While its impossible to completely eliminate the risk of identity theft, you canreceive immediate alerts whenever your identity is used without your consent. Stayprotected with MyID, your trusted identity monitoring solution.Mistral V2MyID allows you to receive immediate notifications if your identity is used withoutyour consent, providing an extra layer of protection against identity theft. Althoughits impossible to completely prevent identity theft, having this alert system can helpyou take swift action to minimize potential damage.Gemma (7B)Stay vigilant against identity theft with MyID. Receive instant fraud notificationswhenever your identity is used without your knowledge, even if you cant completelyeliminate the risk. STYLEREMIXAmidst the complexities of digital security, the safeguarding of your identity might bechallenged beyond your control, yet, it is assured that you will be alerted to instancesof suspected fraud almost instantaneously whenever your identity is invoked withoutyour awareness. This protection is diligently offered through the mechanism of MyID,a strategic layer of defense, designed to help you actively prevent the encroachmentof identity theft.",
  "AuthorScholar-qq": "OriginalThey further stated that an independent Angola under the MPLA would be dominatedby urban mesticos and assimilados, effectively transferring power from one class ofcolonial masters to another.MTThey continued to explain that an independent Angola under the MPLA is dominatedby urban mesticos and assimilados, which effectively transfer power from one classof colonial masters to another.StyloThey further stated that an independent Angola under the MPLA would be dominatedby urban mesticos and assimilados, effectively transferring power from one class ofcolonial masters to another.JAMDECThis was further stated in an article entitled \"What would happen if Angolas urbanelite became mestisos?\" which was published in La Repblica, the official mouthpieceof the Communist Party of Angola (MPLA).ParaphrAccording to the text, an independent Angola under the MPLA would be primarilygoverned by urban elites and assimilationists, effectively transferring power fromone class of colonial leaders to another.Llama2 (7B)They further emphasized that an independent Angola under the MPLA would bedominated by urban mixed-race and assimilated individuals, effectively transferringpower from one group of colonial oppressors to another.Llama3 (8B)According to them, if Angola became an independent country under the MPLAs lead-ership, it would likely be controlled by urban mesticos and assimilados, essentiallyreplacing one group of colonial rulers with another.Mistral V2The MPLAs proposed independent Angola was predicted to be controlled by urbanmesticos and assimilados, implying a shift in power from one colonial ruling class toanother.Gemma (7B)According to their statement, an independent Angola under the MPLA would likelybe controlled by urban dwellers who had mixed ancestry and those who had assimi-lated into Angolan society, effectively replacing one class of colonial masters withanother. STYLEREMIXThey explained that if Angola became independent, led by the MPLA, it mightend up being mostly ruled by people who were born in cities and considered eithermesticos or assimilados. This would mean that the power might just shift from onegroup of people who were once in charge, to another group who were also outsiders,but from a different background.",
  "AUTHORMIX-Blog": "AuthorBlog-1OriginalCostco is a phenomenon you have to visit to believe. This is impulse buying taken toa new level.MTCostco is a phenomenon that you need to visit to believe. This is an impulse for thepurchase to a new level.Styloosteo s a phenomenon you have to visit to believe, that is impulse buying adopted toa new level.JAMDECCostco is a phenomenon you have to visit to believe. This is impulse buying taken toa new level.ParaphrCostco is a unique and remarkable experience that requires a visit to fully appreciate.Its like a shopping experience that takes the concept of impulse buying to a wholenew level. Llama2 (7B)Costco is a truly unique and unforgettable experience that will leave you amazed andsurprised at every turn. Known for its unparalleled impulse buying opportunities,this retail giant has taken the concept of spontaneous shopping to a whole new level.From its vast array of products to its unbeatable prices, a visit to Costco is an eventthat should not be missed. Whether youre a seasoned shopper or just looking for afun day out, this incredible store is sure to delight and surprise you in ways you neverthought possible. So come and experience it for yourself you wont be disappointed!Llama3 (8B)\"Costco is an extraordinary experience that must be seen to be fully appreciated. Itsa retailer that takes impulse buying to a whole new level, making it easy to get caughtup in the excitement of discovery and purchase.\"Mistral V2Visiting Costco is an experience unlike any other, showcasing impulse buying at itspeak.Gemma (7B)Costcos sprawling warehouses and endless aisles entice customers with an unpar-alleled shopping experience, urging them to explore the realm of bulk buying andspontaneous purchases.",
  "STYLEREMIXCostco is a place you really need to see to understand. Its a spot where you mightjust end up buying more than you planned": "AuthorBlog-2OriginalIf not, then not. \"How long ago since you ate anything proper, hmm?\"MTIf not, then not. How long have you spent since youve eaten something good?StyloIf not, then not. How a lengthy time ago since you ate anything proper, him?JAMDECAnd if not, well, then, I guess Ill just have to go back to the drawing board andfigure out what to. I dont know what else to say, other than that Im really happythat I got to be a part of it and.Paraphr\"How long has it been since you last consumed a nutritious meal, hmm?\"Llama2 (7B)If not, then not. How long has it been since youve had a proper meal?Llama3 (8B)It seems you havent eaten a substantial meal recently, have you?Mistral V2If you havent eaten a proper meal for some time now, hmm?Gemma (7B)If not, then not. How long ago was it since you ate something proper, hmmm?",
  "STYLEREMIXIf not, then indeed, not. \"How long has it been since you last consumed a propermeal?\"": "AuthorBlog-3OriginalQuicklunch.....IwantedJasperstodaythough.Theirfoodisstartintoreallybegood......mmmmmperogiesandquesadillas....andpizza....mmmmmmmmmmmm.....................foooood...Bio class was prettylame.MTBreakfast, but I wanted Jaspers today. Your diet begins to be really good...mmmmperogies and quesadillas....and pizza....mmmmmm.............foooood... the biologicalclass was pretty lame.StyloQuick lunch ..... myself wanted paper s today though; and their food is starting toreally be good ...... mamma perogies also quesadillas .... and penza .... mmmmmmm-mmmmm ..................... foooood... mio class was pretty lame.JAMDECI dont know if Ill be able to get it, but I wanted to make sure I got it in time forlunch, so I made a quick lunch out of some of the things I had left over from the lasttime I made lunch, and a few things I found in the pantry that I thought would begood for lunch that I didnt have time to make lunch with, and then. I was going tobe honest and say that I didnt really care for the food here, but it was Jaspers so Ihad to give it a try! I ordered the chicken and waffles and it was pretty good! Thewaffles were crispy on the outside and soft on the inside, and the chicken was cookedperfectly!! I would definitely come back here if I was in the area again! ParaphrThe author wanted to have a quick lunch at Jaspers today, as their food is starting tobe quite good. They are particularly fond of their perogies and quesadillas, as wellas their pizza. The author found the bio class to be quite uninteresting.Llama2 (7B)\"Ah, a quick lunch... I was actually hoping to visit Jaspers today, their food hasreally been impressing me lately. I cant get enough of their perogies, quesadillas,and pizza... mm mm mm... delicious! Unfortunately, bio class was quite dull today. \"Llama3 (8B)I was craving Jaspers for lunch today, but unfortunately, I didnt get a chance to go.Their food has been getting better and better, and Ive been eyeing their perogies,quesadillas, and pizza. Mmm, just thinking about it is making me hungry! Too bad Imissed out. On a different note, bio class was pretty underwhelming today.Mistral V2I was considering having a quick lunch, but I had a craving for Jaspers today. Theirfood has significantly improved recently, and I find myself longing for their perogies,quesadillas, and pizza - oh, the deliciousness! However, my biology class left mefeeling less than satisfied.Gemma (7B)My quick lunch today was at Jaspers. Their food is seriously good! I highlyrecommend their perogies, quesadillas, and pizza. Bio class was unfortunately prettyboring. STYLEREMIXTodays lunch session was brief and I had a particular desire to visit Jaspers. Theirculinary offerings are notably improving, notably including their distinctive perogies,quesadillas, and pizza, which are truly exceptional. The experience was quiteunderwhelming during my bio class. AuthorBlog-4OriginalNowadays, this phase I have been going through is making me face all these quitebrutally. Because I choose it to be so. I have never been a woman who takes thingsswiftly (which is, yet another choice).MTToday, this phase Ive done faces me with all this quite brutal. Because I chose to belike that. Ive never been a woman who takes things quickly (i.e. another choice).StyloAfterwards, Nowadays this: step I have been going through is making me face allthese quite brutally; and because I choose it to be so, i have never been a wife whichtakes things swiftly ( which is yet another choice).JAMDECI dont know what to do about it, but Im going to try my best to get through thisphase as best as I can, so that I can get on with my life. I dont know if Ill ever beable to look at myself in the mirror and be proud of what I look like, but at least Iknow that Im not a. I choose to live my life as a woman, even though I know its nota choice Im allowed to make, its a choice my body makes for me, and.ParaphrThe current phase I am experiencing is causing me to confront these challenges witha brutal intensity, as I have never been a person who takes things lightly (which isanother choice).Llama2 (7B)In recent times, I have been grappling with a phase that is proving particularlychallenging. Rather than navigating it with ease, I find myself facing these challengeshead-on with a sense of brutal directness. This is a choice I have made, one that Ihave always been hesitant to embrace. As someone who prefers to take things slowlyand carefully, I have never been one to rush into situations without fully consideringthe consequences. This cautious approach has led to a certain degree of struggle, butit is a choice that I stand by nonetheless.Llama3 (8B)Lately, Ive been confronting some harsh realities, and I have to admit that its adeliberate choice Ive made. Ive never been one to rush through lifes challenges,and thats a choice Ive always made for myself.Mistral V2Currently, Im experiencing this period in life quite intensely. Ive decided to face itschallenges head-on, although Ive never been one to act quickly in the past. Gemma (7B)Currently, I am navigating through a phase that has forced me to confront variouschallenges with blunt honesty. This stems from my conscious decision to approachthings at a deliberate pace, a characteristic that aligns with my intrinsic personality. STYLEREMIXThese days, Im really laying it all out there, hitting the tough stuff head-on, becausethats just how I roll. Its definitely not my style to breeze through life; Ive alwaysbeen more of the slow-and-thorough typeanother deliberate choice of mine. AuthorBlog-5OriginalAnyway, it looks great. I painted the same color, but in flat instead of eggshell, andits a big improvement. I love the way flat paint looks.MTHowever, it seems great. I made the same color, but in flat instead of garlic, and itsa great improvement. I like how Flat Paint looks like.StyloAnyway, it looks awesome, and i painted the same color colour, but in apartmentinstead in eggshell, and it s a big improved; and i love the way apartment flat paintlooks.JAMDECIt looks like its going to be a really good game. I cant wait to see what you comeup with for the rest of the series, and Im really looking forward to seeing what youdo with the other characters in the series! Im going to try to get a few more of thesein the.ParaphrThe paint job turned out great. I used the same color, but in a flat finish instead ofeggshell, and its a significant improvement. I love the way flat paint looks.Llama2 (7B)\"Ah, I see! Painting the same color but in a flat finish instead of eggshell has madea significant difference. I completely agree, flat paint can give a room a sleek andmodern look thats simply stunning. Its great to hear that youre enjoying the newfinish!\"Llama3 (8B)To be honest, Im really pleased with how it turned out. I opted to use the samecolour, but in a flat finish instead of eggshell, and its made a significant difference.Im a big fan of the flat paint look, and I think its a real improvement.Mistral V2The new look is excellent. I opted for flat paint instead of eggshell for the same color,and Im thrilled with the outcome. Flat paint really enhances the appearance.Gemma (7B)In any case, it looks fantastic! I opted for flat paint instead of eggshell in the samecolor, and the difference is remarkable. I adore the seamless and sophisticated lookof flat paint. STYLEREMIXThe appearance has been significantly improved. The same color was chosen, but theapplication was altered from eggshell to flat, which has been found to significantlyenhance the aesthetic. There is a preference for the appearance of flat paint."
}