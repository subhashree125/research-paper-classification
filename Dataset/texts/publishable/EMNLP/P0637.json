{
  "Abstract": "Neural networks without hierarchical biases of-ten struggle to learn linguistic rules that comenaturally to humans. However, neural networksare trained primarily on form alone, while chil-dren acquiring language additionally receivedata about meaning. Would neural networksgeneralize more like humans when trained onboth form and meaning? We investigate this byexamining if Transformersneural networkswithout a hierarchical biasbetter achieve hi-erarchical generalization when trained on bothform and meaning compared to when trainedon form alone. Our results show that Trans-formers trained on form and meaning do favorthe hierarchical generalization more than thosetrained on form alone, suggesting that statis-tical learners without hierarchical biases canleverage semantic training signals to bootstraphierarchical syntactic generalization.",
  "Introduction": "Language learners encounter sentences throughtheir surface forms: linear sequences of words.However, syntactic rules are sensitive to sentencesunderlying hierarchical structure. What evidencelets learners determine that syntactic rules operateon hierarchical structure, rather than linear order?Some contend that the evidence children receive isinsufficient for a learner without a hierarchical biasto generalize hierarchically (Chomsky, 1968, 1971,1980; Berwick et al., 2011). An alternative hypoth-esis (e.g., Lewis and Elman, 2001) is that learnersrequire no innate hierarchical bias: the input chil-dren get includes sufficient cues for hierarchicalgeneralization. Both sides, however, tacitly assumethat the data relevant for hierarchical generalizationis form alone (i.e., words and their linear order),rather than form and meaning. Since meaningsinvolve hierarchical dependencies which often cor-respond to syntactic structure (Partee et al., 1984),they may provide additional cues to the hierarchicalsyntactic generalization.",
  ": Two possible rules for English yes/no questionformation. Modified from McCoy et al. (2020)": "The rise of neural networks seems to suggest thatthe focus on form is warranted: Networks trainedon form alone perform well on syntactic evalua-tions (e.g., Gulordava et al., 2018; Wilcox et al.,2018; Warstadt et al., 2020; Hu et al., 2020; Hueb-ner et al., 2021). However, when networks inputaligns more closely with the sentences children get,models fail to generalize hierarchically, suggestingthat attaining hierarchical generalization from formalone requires stronger priors than those of stan-dard neural architectures (Yedetore et al., 2023).In this work, we test the hypothesis that learnerswithout a hierarchical bias can generalize hierarchi-cally when trained on form and meaning. We trainTransformers (Vaswani et al., 2017), an architec-ture known to prefer linear rules (Petty and Frank,2021), to translate form to meaning, then test forhierarchical generalization. Following McCoy et al.(2020), our testbed for hierarchical generalizationis yes/no question formation, exemplified by therelationship between declarative sentence (1a) andyes/no question (1b). We train models on questionformation data like (1) which is consistent with ahierarchical and a linear rule (see ).",
  "Background": "Meaning representations are not observable in themind. Forms, however, are observable. This asym-metry makes reasoning about the effect of form ongeneralization simpler than reasoning about mean-ings effect by lessening the need to make assump-tions about unobservable representations. Focusingon forms, Chomsky (1971) observes that althoughEnglish speaking adults acquire the hierarchicalrule for yes/no question formation, childrens inputlikely lacks the evidence ruling out the linear rule.Chomsky (1971) conjectures that even a child whonever encounters such disambiguating examples(e.g., (2a)) would generalize hierarchically, and ar-gues that a innate hierarchical bias is thus necessary.Empirically, Crain and Nakayama (1987) find thatchildren do behave consistently with the hierarchi-cal rule, and rarely with the linear rule (Ambridgeet al., 2008), while disambiguating evidence is veryuncommon in childrens input (Pullum and Scholz,2002; Legate and Yang, 2002).Though such work makes a hierarchical biasseem necessary for child-like generalization, do-main general biases may suffice for hierarchicalgeneralization. To explore this possibility, severalstudies have investigated how artificial learners gen-eralize from form alone. Some argue that their re-sults support an innate hierarchical bias (McCoyet al., 2018; Yedetore et al., 2023), while othersargue against this conclusion (Lewis and Elman,2001; Reali and Christiansen, 2005; Perfors et al.,2011; Bod and Smets, 2012), and still others do nottake a strong stance (Frank and Mathis, 2007; Linet al., 2019; Warstadt and Bowman, 2020).The direct precursors to our work also vary intheir conclusions. McCoy et al. (2020) and Pettyand Frank (2021) show that neural networks with-",
  "GitHub repo with data and code:": "out hierarchical biases trained on form alone in asequence-to-sequence setup generalize to the linearrule of question formation. These results supportthe claim that hierarchical generalization requiresa hierarchical bias. However, Murty et al. (2023a)and Ahuja et al. (2024) find that models trainedin a language modeling setup on the McCoy et al.(2020) and Petty and Franks (2021) data gener-alize linearly early on, but grok the hierarchicalgeneralization after training far beyond saturationon in-domain performance.Though the role of semantic information in theacquisition of syntax has long been theorized (cf.Chomsky (1965); Pinker (1979)), fewer studieshave explored semantic signals effect on hierarchi-cal generalization. Studying children, Crain andNakayama (1987) find evidence against Stemmers(1981) hypothesis about how meanings aid hierar-chical generalization, but leave open the possibilitythat meanings help in other ways. Morgan andNewport (1981) find that visual context aided theacquisition of constituent structure in adult learnersof an artificial language, though no more so thanadding explicit cues to constituent structure to theforms. This finding with adults, however, does notaddress how children generalize hierarchically dur-ing first language acquisition. Using computationalmodeling, Fitz and Chang (2017) show that net-works with built-in linguistic knowledge general-ize hierarchically when trained to map meaning toform, and Abend et al. (2017) explore how seman-tic training signals help a learner acquire syntacticrules but use statistical modeling techniques thatpresuppose that syntactic structures must be hier-archical. It is an open question if learners withoutsuch built-in knowledge generalize hierarchicallywhen trained on both form and meaning.The hypothesis we test in this work is in thespirit of the semantic bootstrapping hypothesis:that children leverage sentences paired with struc-tured meaning representations to acquire syntacticrules (Abend et al., 2017). In this work, we gen-eralize semantic bootstrapping to the problem ofdetermining that syntactic rules must be sensitiveto hierarchical structure rather than linear order.",
  "In this work, we train models to form yes/no ques-tions in two ways. In Exp. 1 (), wetrain neural networks in a sequence-to-sequencesetup on the objective of translating declarative": "sentences to their yes/no question counterparts. Weuse this setup to enable comparison to Petty andFrank (2021) and McCoy et al. (2020). We thentest if models generalization behavior is more con-sistent with the linear or the hierarchical rule.In Exp. 2 () we explore grokking: Wetrain models longer and track how training on formand meaning changes models training dynamics.We additionally train neural networks on the taskof language modeling (predicting the next word atevery point in a sentence), since grokking may de-pend on this training objective (Ahuja et al., 2024).In Exp. 3 (Sections 6 and 7) we vary the rep-resentation of meaning and the translation task toinvestigate several possible causes of the benefit oftraining models to map form to meaning.",
  "Framing of the Task": "In this experiment, we compare the generalizationof sequence-to-sequence networks trained on formalone with those additionally trained to translateforms to meanings. Following McCoy et al. (2020),models trained on form alone are tasked with map-ping from declarative sentences to themselves asin (3a), or to their yes/no question forms as in (3b),where the inputs final token specifies the task.",
  "Crucially, all the training instances for the questionformation task are consistent with both the hierar-": "2We do not claim that the exact structured logical represen-tations of meaning that we use in this work is a part of the inputthat children explicitly receive. Rather, it is likely that seman-tic cues available to children derive from language-externalmodalities, such as visual input. The meaning representationswe provide in our experiments correspond to a conservativeupper bound to what the child could determine about themeaning of the sentence that they heard, possibly leveraginglanguage-external cues. We seek to explore if under this ide-alized scenario there are benefits to syntactic generalization,which is a precondition to expecting benefits of noisier, morerealistic semantic signals. chical and the linear generalizations (). Toevaluate what models learn from the training data,we test models on examples like (2), for which thehierarchical rule produces well-formed questions,like (2a), while the linear rule produces ill-formedquestions, like (2b). shows the distributionof the training and evaluation data.",
  "Datasets": "For these datasets, we use the same grammar as inExperiment 1 and 2. Our first dataset, IDENTIFYMAIN AUXILIARY, as shown in (12a), explores thepossibility that a translation task in which modelsneed to identify the main auxiliary aids hierarchi-cal generalization. Our second dataset, IDENTIFYMAIN VERB, is exemplified in (12b).Another possible reason for the benefit of themeanings is that the meaning representation con-tains hierarchical structure that is similar to thesyntactic structure underlying the sentence. Trans-lating declarative to such hierarchical structuresmight be the source of the hierarchical general-ization. To explore this possibility, we introduceCONSTITUENCY PARSING, shown in (12c).",
  "The newts who dont fly do see the yak. See(x.Newt(x) Fly(x), y.Yak(y))": ": White cells () indicate the type of data in the FORM ALONE training set and the in-distribution test set.Light gray cells () indicate the additional data in the FORM & MEANING training set. Dark gray cells () indicatethe generalization data. RC stands for relative clause. To save space, this table uses some words not present in thevocabulary used to generate the training instances. For instance, fly is not in the vocabulary, though see is. a patience of 5. We implement models with Open-NMT (Klein et al., 2017).5 See Appendix A formore hyperparameter details. We tokenize declara-tives and questions by splitting at whitespaces, andtokenize meaning representations as in (4).",
  "Evaluation": "Following McCoy et al. (2020), we use two evalua-tion metrics: full sentence accuracy on the test set(consisting of held out examples similar to thoseseen in training), and first word accuracy on thegeneralization set. Full sentence accuracy mea-sures if the models output is exactly correct giventhe input. The first word metric evaluates whetherthe first word of the question is correct, abstract-ing away from extraneous errors irrelevant to thechoice between hierarchical and linear generaliza-tions (e.g., the model may incorrectly replace theverb sleep with giggle). Crucially, the first wordof the question is sufficient to disambiguate thelinear and hierarchical rules. For instance, whengiven (2) as an input, a model that has learned thehierarchical rule would choose doesnt as the firstword of the output, as in (2a), while a model thathas learned the linear rule would choose does asthe first word of the output, as in (2b).",
  "Across ten random reruns, Transformers trainedon FORM ALONE and those trained on FORM &MEANING achieve perfect performance on the test": "set: they always produce the full question correctly.This indicates that models successfully learned tohandle questions like those seen during training.On the generalization set, models seldom producethe full sentence correctly, replicating prior find-ings (Petty and Frank, 2021). Turning to the morelenient measure of first word accuracy, Transform-ers trained on FORM ALONE generalize linearly(choosing the linear option for 95% of the gen-eralization sentences, and the hierarchical optionfor 5%), again replicating prior findings of Pettyand Frank (2021). On the other hand, Transform-ers trained additionally on meaning prefer the hi-erarchical generalization (60% hierarchical, 40%linear).6 See for a summary.",
  "Experiment 2: Grokking": "Recent work suggests that Transformer models dis-play structural grokking: when trained past satura-tion on in-domain accuracy, out-of-domain gener-alization continues to improve, and eventually hi-erarchical generalization is achieved (Murty et al.,2023a). This raises the possibility that early stop-ping caused the lack of hierarchical generalizationin Experiment 1. To explore the interaction be-tween grokking and semantic training signals, wetrain models on the datasets from Experiment 1 6We also train models on the original (form alone) datafrom McCoy et al. (2020), which includes some and has 8%more DECL than QUEST examples, to ensure that our findingsare not due to our minor modifications to the sampling pro-cess. We find similar results to those for FORM ALONE inExperiment 1. See Appendix D.",
  "Architecture and Training Setup": "Since prior work (Ahuja et al., 2024) has foundthat grokking varies according to the choice ofa sequence-to-sequence or a language modelingsetup, we use both in this experiment. For bothsetups, we use hyperparameters following those inAhuja et al. (2024): 8 heads, embedding size of512, 6 layers, word-level tokenization, and batchsize 8. We use the Adam optimizer (Kingma andBa, 2014) with a learning rate of 0.0001, and trainmodels for 300k steps, without early stopping.7",
  "Results": "See . When trained on FORM[agr], eightof the ten transformers generalize linearly fromstep 50k on (100% linear, 0% hierarchical), whiletwo models generalize partially to the hierarchicalrule (43% and 67% hierarchical after 300k trainingsteps). This suggests that the source of structuralgrokking in the language modeling setup is thesubject-auxiliary agreement, serving as a cue toidentify the main auxiliary.On MEANING[agr], models generalize hierar-chically. These results are similar to the results",
  "First word consistency withthe hierarchical generalization": ": First word consistency with the hierarchical generalization. Top: models trained in a language modelingsetup. Bottom: models trained in a sequence to sequence setup. The black line indicates the average across 10random reruns. Light gray shaded areas are the minimum and maximum values for any model at each training stepacross the 10 random reruns.",
  "Specifics of the Meaning Representation?": "One reason that the data in Experiments 1 and 2may lead to hierarchical generalization is that therelationship between the main auxiliary and thefirst word of the question strongly parallels the re-lationship between the negations location in thedeclarative and in the meaning representation. Forinstance, though models do not receive questionslike (6a), they do receive meanings like (6b), wherethe relationship between the main auxiliary (dontin (6)) and the negation ( in (6b)) closely corre-sponds to the relationship between that main aux-iliary in the declarative and the first word in thequestion in (6a). See for an illustration.",
  "(6)the newts who do fly dont see the yak .a. dont the newts that do fly see the yak ?b. See(x.Newt(x) Fly(x), y.Yak(y))": "To test whether this close parallel is respon-sible for the models preference for hierarchicalgeneralization, we remove the negation from thedataset, transition to an event semantic representa-tion where the element of meaning correspondingto the auxiliary is no longer directly at the frontof the meaning representation, and introduce thenecessary variability in the auxiliary using tense.8",
  "(7)a. the newt did fly .b. e : Past(e) Fly(e, x.Newt(x))": "Now the element of meaning corresponding to theauxiliary is no longer directly at the front of themeaning representation, due to e :. However,the relationship between (7a) and (7b) still bears aclose correspondence to the hierarchical questionformation rule. As (8) shows, the tense predicate(Past) corresponding to the main auxiliary (did) ap-pears near the front of the meaning representation. 8To use the first word evaluation for hierarchical general-ization, we require at least two distinct auxiliaries that sharethe same number agreement marking (e.g., does and doesnt).Thus, we cannot remove negation without adding additionalauxiliaries. Here we add did.",
  "(8)a. the newts who do fly did see the yak .b. e : (Past(e) See(e, x.Newt(x) e :Pres(e) Fly(e, x), y.Yak(y)))": "For this reason, we explore an equivalent semanticsin which the tense predicate corresponding to themain auxiliary is located at the end of the mean-ing representation, as demonstrated in (9) and (10),which are translations of (7a) and (8a), respectively.If we see an equivalent boost in preference for hi-erarchical generalization using this representationscheme, this suggests that the similarity of the hier-archical question formation rule and the placementof negation at the front of the meaning representa-tion is not the source of hierarchical generalizationin Experiments 1 and 2.",
  "(10) e : (See(e, x.Newt(x) e : Fly(e, x) Pres(e), y.Yak(y)) Past(e))": "6.1.1DatasetsWe rename the datasets from Experiments 1and 2 FORM[+neg] and MEANING[+neg] todistinguish them from the datasets introducedhere. We introduce FORM[+tense], which is likeFORM[+neg] but differentiates auxiliaries withtense. MEANING[+tensefirst] includes the exam-ples in FORM[+tense] plus translations into a mean-ing representation like (8b). MEANING[+tenselast]is like MEANING[+tensefirst] but with representa-tions as in (9) and (10). 6.1.2Results shows the results with the alternativemeaning representations. Here, models trained onFORM[+tense] in a language modeling setup varyin their generalization patterns, four of ten showinga preference for the hierarchical generalization (af-ter grokking), and six of ten showing a preferencefor the linear generalization. Models trained inthe sequence-to-sequence setup on FORM[+tense]display no hierarchical generalization.Now we compare these results to those fromExperiment 2 (). Though models trainedon FORM[+tense] in the the sequence-to-sequencesetup behave similarly to models trained onFORM[+neg], the corresponding results in the lan-guage modeling setup show differences. Specifi-cally, there is more variability in the generalizationof the models trained on FORM[+tense] than themodels on FORM[+neg], with one choosing thelinear generalization even after 300k training steps.",
  "The relative difficulty models have withgeneralizinghierarchicallywhentrainedonFORM[+tense] makes the models generalizationto the hierarchical rule when trained on MEAN-": "ING[+tensefirst] and MEANING[+tenselast] morestriking. Models in both the language modelingand sequence-to-sequence setups trained on eitherMEANING[+tensefirst] or MEANING[+tenselast]display stronger hierarchical generalization thanwhen trained FORM[+tense]. These results suggestthat the similarity between the hierarchical ques-tion formation rule and negations location in theforms versus meanings does not account for Trans-formers behavior in Experiments 1 and 2.",
  "do ] sleep": "We hypothesize that the presence of subject-auxiliary agreement drives the grokking of thehierarchical generalization in models trained onFORM[+neg]. We test this by removing the subject-auxiliary agreement from the models trainingdata. Importantly, this ablation of subject-auxiliaryagreement does not fundamentally change the gen-eralization problem the models face: the trainingdata is still ambiguous between the linear and hier-archical rules shown in .",
  "Discussion": "Across three experiments and several training se-tups, we find that training models to translate fromform to meaning enables stronger hierarchical gen-eralization than training on form alone. These re-sults suggest that one avenue to hierarchical gener-alization in learners without a hierarchical bias isleveraging semantic signals in the training data.For Transformers specifically, our results showthat Transformers generalize more like humanswhen trained to translate forms to meanings thanwhen trained on form alone. In practice, this take-away must be tempered by the possibility that thelarge quantities of form that large language modelsreceive make the benefits of training to translate form to meaning inconsequential. However, re-cent results suggest that translations from formsto meaning-like representations may provide bene-fits even to language models trained at scale: Kimet al. (2024) find that large language models trainedadditionally on code performed better on a entitytracking task. This benefit may be due to the thepresence of translations from natural language sen-tences to code in the training data. Further work isnecessary to disentangle this possibility from otherpossible contributing factors.For child acquisition, our results suggest thatone possible source of hierarchical generalizationis the relationship between forms and meanings, solong as children can construct logical meaning rep-resentations either innately or develop this capacitysometime before making hierarchical generaliza-tions. The early development of such logical capa-bilities is consistent with recent work on the logicin infants (e.g., Cesana-Arlotti et al. 2018), thoughthis line of research is still in its early stages.Though in this work we focus on meaning, itmay be that other language-external cues also fa-cilitate hierarchical generalization. For instance,prosody (Morgan and Demuth, 2014) may alsoprovide a hierarchical signal of a similar natureto meanings, and visual information (Shi et al.,2019; Wang et al., 2023) may provide informa-tion about lexical semantics that is useful to deter-mine how meanings must combine. Future workshould also better align the input with what chil-dren get, perhaps following the lead of Yedetoreet al. (2023) and using a corpus of child-directedspeech as model training data, to strengthen theinferences about the innate biases necessary forchildren to acquire language.",
  "Limitations": "We view our behavioral analysis in this work as astrong starting point for understanding how seman-tic training signals affect generalization to hierar-chical syntactic rules in Transformers. However,we see it as critical that future work look into theinternal mechanisms of these models to determinethe computations underlying model behavior. Thiswill allow the determination of whether hierarchi-cal generalization corresponds to hierarchical rep-resentation, or if neural networks that generalizehierarchically employ shortcut mechanisms that donot involve hierarchical representation.With respect to child acquisition, the connectionbetween our work and the acquisition problem chil-dren face hinges on our assumption that the childcan recover a structured representation of the mean-ing of a sentence from utterances and their contexts.If children cannot construct structured representa-tions of meaning given a sentence and its context,our work may not bear on the language acquisitionproblem. Future work is needed to determine thenature of the meaning representations children canrecover from context. We are grateful to Hayley Ross, Liz Coppock, theaudience at the 2024 New England Natural Lan-guage Processing (NENLP) meetup, the MIT Com-putational Psycholinguistics Lab, tinlab at BU, andthe audience at Harvards Language and Cognition,for helpful comments and feedback. Any errors areour own. We are also grateful to the Boston Univer-sity Shared Computing Cluster (SCC) for providingthe computing resources used in our experiments.",
  "Robert Frank and Donald Mathis. 2007. Transforma-tional networks. Models of Human Language Acqui-sition, pages 2227": "Kristina Gulordava, Piotr Bojanowski, Edouard Grave,Tal Linzen, and Marco Baroni. 2018. Colorless greenrecurrent networks dream hierarchically. In Proceed-ings of the 2018 Conference of the North AmericanChapter of the Association for Computational Lin-guistics: Human Language Technologies, Volume1 (Long Papers), pages 11951205, New Orleans,Louisiana. Association for Computational Linguis-tics. Jennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox,and Roger Levy. 2020. A systematic assessmentof syntactic generalization in neural language mod-els. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics, pages17251744, Online. Association for ComputationalLinguistics.",
  "James L Morgan and Elissa L Newport. 1981. Therole of constituent structure in the induction of anartificial language. Journal of verbal learning andverbal behavior, 20(1):6785": "Shikhar Murty, Pratyusha Sharma, Jacob Andreas, andChristopher Manning. 2023a. Grokking of hierarchi-cal structure in vanilla transformers. In Proceedingsof the 61st Annual Meeting of the Association forComputational Linguistics (Volume 2: Short Papers),pages 439448, Toronto, Canada. Association forComputational Linguistics. Shikhar Murty, Pratyusha Sharma, Jacob Andreas, andChristopher D Manning. 2023b. Characterizing in-trinsic compositionality in transformers with tree pro-jections. In The Eleventh International Conferenceon Learning Representations.",
  "Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-hananey, Wei Peng, Sheng-Fu Wang, and Samuel R": "Bowman. 2020. BLiMP: A benchmark of linguis-tic minimal pairs for English. In Proceedings of theSociety for Computation in Linguistics 2020, pages409410, New York, New York. Association for Com-putational Linguistics. Ethan Wilcox, Roger Levy, Takashi Morita, and RichardFutrell. 2018. What do RNN language models learnabout fillergap dependencies? In Proceedings ofthe 2018 EMNLP Workshop BlackboxNLP: Analyz-ing and Interpreting Neural Networks for NLP, pages211221, Brussels, Belgium. Association for Com-putational Linguistics. Aditya Yedetore, Tal Linzen, Robert Frank, andR. Thomas McCoy. 2023. How poor is the stim-ulus? evaluating hierarchical generalization in neu-ral networks trained on child-directed speech. InProceedings of the 61st Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 93709393, Toronto, Canada.Association for Computational Linguistics.",
  "AHyperparameters": "For our experiments in both the language model-ing and sequence-to-sequence setups, we addition-ally use the following hyperparameters: AdamW(1: 0.9, 2: 0.999, : 1e-7), and linear warmupscheduling for 10k steps. We clip gradients to havea max L2 norm of 10, and tie input and outputembeddings for the encoder and decoder in thesequence-to-sequence setup.",
  "BModel implementation": "OpenNMT (Klein et al., 2017), used in this workfor the sequence-to-sequence setup, has a MIT Li-cense. Though the codebase developed in Murtyet al. (2023a), used in this work for the languagemodeling setup, does not specify a license, thatcode is built upon the codebase of Murty et al.(2023b), which has a MIT License.Models with trained using these codebases withthe hyperparameters from Appendix A have 18Mtrainable parameters in the language modeling case,and 25M in the sequence-to-sequence case, andtake 3 hours to train on Nvidia k80 GPUs. In-cluding all models trained, our experiments takeapproximately 1000 GPU hours.",
  "the := f.x.f(x)": "my := f.x.(f(x) Poss(Speaker, x))your := f.x.(f(x) Poss(Addressee, x))her := f.x.(f(x) Poss(y, x) Female(y))our:=f.x.(f(x) Poss(Speaker, x) Poss(Addressee, x))newt := x.Newt(x)orangutan := x.Orangutan(x)peacock := x.Peacock(x)quail := x.Quail(x)raven := x.Raven(x)salamander := x.Salamander(x)vulture := x.Vulture(x)walrus := x.Walrus(x)yak := x.Yak(x)zebra := x.Zebra(x)xylophone := x.Xylophone(x)unicorn := x.Unicorn(x)tyrannosaurus := x.Tyrannosaurus(x)around := x.y.Around(y, x)near := x.y.Near(y, x)beside := x.y.Beside(y, x)upon := x.y.Upon(y, x)by := x.y.By(y, x)above := x.y.Above(y, x)behind := x.y.Behind(y, x)below := x.y.Below(y, x)giggle := x.Giggle(x)smile := x.Smile(x)sleep := x.Sleep(x)swim := x.Swim(x)wait := x.Wait(x)move := x.Move(x)change := x.Change(x)read := x.Read(x)eat := x.Eat(x)entertain := x.y.Entertain(y, x)amuse := x.y.Amuse(y, x)highfive := x.y.Highfive(y, x)applaud := x.y.Applaud(y, x)confuse := x.y.Confuse(y, x)admire := x.y.Admire(y, x)accept := x.y.Accept(y, x)remember := x.y.Remember(y, x)comfort := x.y.Comfort(y, x)does := P.x.P(x)do := P.x.P(x)doesnt := P.x.P(x)dont := P.x.P(x)",
  "DResults on McCoy et al.s (2020) Data": "Here we compare the results when using the datain McCoy et al. (2020), which we label ORIGINAL,with the results reported in Experiment 1 and ex-periment 2. (corresponding to in the main text) compares the results for the datagenerated for Experiment 1 with the results usingthe data in McCoy et al. (2020). Overall, modelstrained on ORIGINAL and FORM ALONE general-ized similarly on the in distribution test set and onthe generalization set. (corresponding to in the maintext) compares the results on ORIGINAL, on FORMALONE, and on FORM & MEANING using theevaluation setup reported for Experiment 2. Ourresults here differ from those reported in Murtyet al. (2023a). This difference is due to choice ofrandom seeds. The 10 random seeds chosen inMurty et al. (2023a) display hierarchical grokking(namely, generalization to the hierarchical ruleafter many training steps), but a few seedsexcluded from the results display systematicgeneralization to the linear rule (see, e.g., seed222in"
}