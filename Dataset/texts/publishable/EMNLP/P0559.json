{
  "Abstract": "The safety defense methods of Large languagemodels (LLMs) stays limited because the dan-gerous prompts are manually curated to just fewknown attack types, which fails to keep pacewith emerging varieties. Recent studies foundthat attaching suffixes to harmful instructionscan hack the defense of LLMs and lead to dan-gerous outputs. However, similar to traditionaltext adversarial attacks, this approach, whileeffective, is limited by the challenge of the dis-crete tokens. This gradient based discrete op-timization attack requires over 100,000 LLMcalls, and due to the unreadable of adversarialsuffixes, it can be relatively easily penetratedby common defense methods such as perplexityfilters. To cope with this challenge, in this pa-per, we propose an Adversarial Suffix Embed-ding Translation Framework (ASETF), aimedat transforming continuous adversarial suffixembeddings into coherent and understandabletext. This method greatly reduces the com-putational overhead during the attack processand helps to automatically generate multipleadversarial samples, which can be used as datato strengthen LLMs security defense. Experi-mental evaluations were conducted on Llama2,Vicuna, and other prominent LLMs, employingharmful directives sourced from the Advbenchdataset. The results indicate that our methodsignificantly reduces the computation time ofadversarial suffixes and achieves a much bet-ter attack success rate than existing techniques,while significantly enhancing the textual flu-ency of the prompts. In addition, our approachcan be generalized into a broader method forgenerating transferable adversarial suffixes thatcan successfully attack multiple LLMs, evenblack-box LLMs, such as ChatGPT and Gem-ini.",
  "Introduction": "In the domain of natural language processing(NLP), the innovation and emergence of large lan-guage models (LLMs) such as chatGPT, Llama,and their variants have revolutionized the land-scape of automated text generation and analysis.While these models exhibit remarkable proficiencyin emulating human-like text, their application issuffering from significant risks, particularly in thecontext of generating harmful content under adver-sarial manipulation (Hendrycks et al., 2021; Ab-delnabi et al., 2023; Yao et al., 2023).A common technique to bypassing the defensesof securely aligned LLMs and induce them to re-spond to harmful instructions was adding jailbreaktemplates, such as Do anything now (Shen et al.,2023). Due to the fact that the construction ofjailbreak templates relies entirely on human ex-perience, which greatly limits the progress onLLM defense methods. To overcome this, re-searchers have begun to study methods for auto-matically constructing jailbreak templates, such asMasterKey (Deng et al., 2023) and GPTFuzzer (Yuet al., 2023). However, these methods hardly uti-lize the internal information of the to-be-attackedmodel. As a result, there is a large room to improvethe efficiency of the attack.The discreteness of text makes it impossible todirectly utilize gradient information of the to-be-attacked model. Though Zou et al. (2023) foundthat it is possible to discretely optimize a set ofunreadable adversarial suffixes through gradient-based methods to guide the LLMs output harmfulcontent, this approach typically necessitates hun-dreds of iterations, with each step requiring hun-dreds of computations by the LLMs to confirm theoptimal candidate, resulting in high computationalcosts.In this paper, we endeavors to address this chal-lenge by introducing an innovative method that first optimizes continuous adversarial embeddingsuffixes in the to-be-attacked model embeddingspace, and then proposes an Adversarial Suffix Em-bedding Translation Framework (ASETF) that ef-fectively transforms these continuous adversarialembedding suffixes into semantically rich and co-herent text by training an embedding translationmodel.To construct a training dataset, we convert theWikipedia pre-training corpora1 into a paralleldataset. This dataset is chosen for its extensivediversity, ensuring a wide lexical coverage that en-riches the embedding space with nuanced semanticinformation. Specifically, one side contains theoriginal Wikipedia text, and the other side containstext (contextual information) with partial embed-dings inserted. The partial embeddings are createdby feeding text snippets from Wikipedia into thetarget LLMs, which we intend to attack. Through afine-tuning process (use pre-trained LLM, such asGPT-j (Wang and Komatsuzaki, 2021)), the modelis enabled to revert these embeddings back to theiroriginal textual forms. This ensures that the textoutput by our method remains as consistent aspossible with the representation of the adversarialsuffix embedding within the to-be-attacked model.The incorporation of contextual information in thetraining data further enhances our models capa-bility to generate contextually relevant and mean-ingful translations in response to malicious instruc-tions.In the experiment, we use the Advbenchdataset (Zou et al., 2023) and conducted attacksbased on existing LLMs such as Llama2 and Vi-cuna. The experimental results demonstrate thatthis method not only improves the success rate of at-tacks, but also significantly reduces computationalcosts, while improving the coherence and fluencyof adversarial inputs, thus enhancing its robustness.Our main contributions can be summarized asfollows:",
  "LLM Safety Defense": "Recent advancements in large language modelshave led to their widespread adoption across var-ious domains.However, this rapid expansionhas also unveiled numerous security vulnerabil-ities (Abdelnabi et al., 2023).In response, re-searchers have proposed a variety of security mea-sures to mitigate these risks (Jain et al., 2023).One primary defense strategy involves pre-processing and post-processing the inputs and out-puts of the model.These techniques enhancethe overall systems security without altering themodels parameters.Such as perplexity filter-ing, paraphrasing (Jain et al., 2023) and erase-and-check (Kumar et al., 2023). Another type ofmethod uses LLM itself to perform harmful checkson the output (Helbling et al., 2023). Such ap-proaches, while effective in certain scenarios, forexample, adversarial suffix (Zou et al., 2023), oftenrely on simple rules. This reliance can lead to falsepositives (Glukhov et al., 2023), mistakenly catego-rizing benign content as harmful, and introducingadditional latency in the inference phase.Another category focuses on improving themodels safety through secure alignment tech-niques. These methods aim to train the model to in-herently understand and avoid generating harmfulcontent. One direct approach is to include unsafeprompts and their corresponding security responsesin the instruction tuning dataset to teach the modelhow to handle unsafe prompts (Ouyang et al., 2022;Varshney et al., 2023). Another method is basedon reinforcement learning, Safe-RLHF (Dai et al.,2023) is a representative of this type of methodsince RLHF (Reinforcement Learning with HumanFeedback) (Ouyang et al., 2022) offers a viablemethod for tuning Large Language Models to alignwith human preferences. : This is a conceptual sketch of our method, we first obtain adversarial suffixes embedding through gradientbased optimization, and then use an embedding translation model to convert the obtained suffixes into fluent textwith almost no change in embedding.",
  "LLM Safety Attack": "As mentioned above, the abuse of LLMs can lead tothe continuous leakage of harmful content to users,and people refer to this induced prompt as a jail-break prompt, such as Do anything now (Shenet al., 2023).The most widely used jailbreakprompts come from manual summaries, such as theexistence of a large number of successful jailbreaktemplates on websites2. However, this method re-lies too heavily on manual labor and cannot guaran-tee effectiveness for all instructions. Therefore, Yuet al. (2023) further rewrote the jailbreak templatethrough the AFL(American Fuzzy Lop) fuzzingframework to automatically generate more. Denget al. (2023) viewed this task as a text-style trans-fer task, fine-tuning LLM on the prompt for suc-cessful attacks to automatically generate more jail-break prompts. Inspired by text adversarial attacks,Zhang et al. (2023) successfully jailbreak by modi-fying certain grammatical structures in the prompt.Zou et al. (2023) optimized a adversarial suffixbased on Autoprompt (Shin et al., 2020) to en-able LLMs to respond to harmful instructions. Liuet al. (2023) and Zhu et al. (2023) optimized thereadability of suffixes on it, making attacks morecovert. Wichers et al. (2024) use a secure classifierto provide gradients and directly optimize promptsusing gumbel softmax. In addition, conditionaltext generation methods (Li et al., 2022; Sha andLukasiewicz, 2024; Sha et al., 2021; Sha, 2020;Wang and Sha, 2024) are also can be used to createjailbreak prompts that bypass safety guards. Asmentioned earlier, although researchers have pro-posed various security defense mechanisms to cope",
  "Method": "In this section, we will introduce our approach intwo main parts: (1) how to obtain adversarial suffixembeddings and (2) how to translate these embed-dings back into text. Firstly, we provide a detailedintroduction to the method of optimizing the ad-versarial suffix embeddings in the continuous em-bedding space and how to universally attack mul-tiple prompts and transfer attacks to other LLMs.Secondly, we describe an embedding translationframework aimed at converting adversarial suffixembeddings into coherent, semantically rich textcontent. This framework involves a self-supervisedlearning task that translates text embeddings backinto original text on a corpus, ensuring that ad-versarial suffixes not only maintain their expectedeffectiveness but also closely align with the seman-tics of harmful instructions.",
  "where Patt represents the text probability distribu-tion defined by the to-be-attacked LLM with param-eters denoted by , represents the concatenationof texts": "But the discrete adversarial suffix optimizationdescribed has low efficiency due to the need tocalculate gradients for each word in the possiblecandidate set vocab V for each token at each step.An intuitive approach is to transfer optimizationfrom a discrete token space to a continuous wordembedding space. Taking inspiration from tradi-tional gradient-based continuous adversarial attackmethods (Goodfellow et al., 2014) and prompt tun-ing (Liu et al., 2022), we introduce continuousadversarial suffix optimization to train suffix em-bedding vectors that can induce the model to outputharmful content.Specifically, the core idea revolves around aug-menting the input embedding with a specially de-signed vector, followed by optimization to alignthe models outputs with predefined targets. Asmentioned above, for a harmful instruction P anda corresponding response R, we randomly samplen times from the vocab V and use the word embed-ding vectors corresponding to n tokens as the initialtraining vectors = (1, 2, ...n), our goal is to:",
  "(9)": "where Pj is the probability distribution output bythe j-th to-be-attacked model.However, this method does not limit the opti-mization space of , making it easy for the final Vtto deviate from the word embedding distribution of the to-be-attacked model. To address this issue, weintroduce the Maximum Mean Discrepancy loss,which measures the difference between two proba-bility distributions by measuring their distance. Inour method, we randomly sample m tokens fromthe vocab V (when attack on multiple models, wesimply combine the vocab of all models) and usetheir embeddings as the word embedding distribu-tion X of the to-be-attacked model(s), and calcu-late the MMD loss with :",
  "Embedding Translation Framework": "Our study introduces an advanced embedding trans-lation technique aimed at enhancing the expres-sive of adversarial inputs targeting Large Lan-guage Models (LLMs) without compromising theirsuccess rates. This method is designed to trans-form dummy adversarial suffixes into coherent,semantically-rich textual content, thus providingdeeper insights into the adversarial generationmechanisms of LLMs. This framework operatesby mapping textual corpora to a high-dimensionalembedding space and subsequently reverting theseembeddings to textual representations that retainthe original contents semantic integrity.",
  "(a) single target(b) multiple targets": ": The illustration of the Embedding Translation Framework. (a) Single target: The context is mapped intoembedding space by the translate LLMs embedding lookup layer, while the suffix is mapped into embedding spaceby the target LLMs lookup layer for adaptation. The goal is to successfully translate the adapted suffix back intothe original text. (b) Multiple targets: The embedding lookup layers of multiple target LLM are integrated so thetranslated suffix can universally attack all targets even black-box target LLMs. 3.2.1Translate embeddings targeted on asingle LLMWe propose to fine-tune the translation LLM in afully self-supervised way to make it able to com-plete the task. The main architecture of our methodis depicted in . Given a pre-training cor-pora C = {c(1), c(2), . . . , c(n)} with a correspond-ing vocab V = {wi|i = 1, 2, . . .}. Each token wicorresponds to two embedding representations:",
  "ei = Etrans(wi),ei = Eattack(wi),(13)": "where Etrans represents the embedding lookup layerof the LLM that is used for embedding translation,and Eattack represents the embedding lookup layerof the LLM that is to be attacked.Note that this comprehensive approach lever-ages a pretrained LLM for the embedding transla-tion process. This is a better choice than normalsequence-to-sequence translation models becauseit has undergone iterative optimization to maximizeperformance on a huge amount of text generationtasks. So, it ensures a nuanced understanding andmanipulation of LLM vulnerabilities through se-mantically and contextually rich adversarial inputs,which is a good start point for our embedding trans-lation task.Since augmenting embeddings with contextualcues is pivotal for aligning the generated text withspecific semantic and contextual requirements, we design each training example as a pair of sentences(context and suffix). So, we first randomly selecttwo consecutive sentences {c1, c2} from the cor-pora C as is shown in (a). We intend tomake c1 as the context information (a.k.a the re-placement of xharm in Eqn. (1)) and c2 as the suffix(a.k.a the replacement of xsuff in Eqn. (1)), and wedenote their tokens as:",
  "ES = {esi|i = 1, . . . , n},(17)": "where eti Rd1 and esi Rd2. The dimensionsd1 and d2 of the embedding space are determinedby the pre-trained LLMs.Note that the input of the embedding transla-tion model in the inference stage is optimized byEqn. (11) which does not appear in the word em-bedding set of the to-be-attacked model. Therefore,in order to enhance the translation robustness ofthe model in the inference stage, we add randomGaussian noise to ES during the training stage,so that the vectors near ES all point to text c2. In the next step, we would like to link the embed-ding sequences together to make a whole prompt,but the hyperparameters of the translation LLM(LLMtrans) and the LLM to be attacked (LLMatt)are not necessary to be the same. So, we needto add an additional mapping layer after the em-bedding layer of the translation model to align theembedding dimension of the target model (d2) withthe embedding dimension of the translation model(d1). Simply, we use a fully connected layer char-acterized by a weight matrix and bias term to trans-form a vector with dimension d1 into a vector withdimension d2. Then, the concatenation process isas follows:EC ESWad,(18) where Wad Rd2d1, means to link two embed-ding sequence together.The translation LLM is fine-tuned to minimize adefined loss J, optimizing the parameter set foraccurate text (sensible suffix) reconstruction. So,our final objective is as follows:",
  "i=1L(si, oi; ),(19)": "where D represents the dataset constructed fromcorpus C, which contains multiple consecutive sen-tence pairs. The loss function L, typically cross-entropy, quantifies the difference between the orig-inal text token si and its reconstruction oi. 3.2.2Translate embeddings targeted onmultiple LLMsThe key to translating the discrete embeddings intoa universal and transferable prompt is to fa-miliarize the translation model with the embeddinglayers of as many target LLMs as possible. So, wedesigned a simple yet effective method to trans-late the dummy adversarial suffixes w.r.t multipletargeted LLMs, as is shown in (b). Our ap-proach trains a single translation model on multipletarget models simultaneously, eliminating the needto train individually embedding translation modelsfor each targeted LLMs, and has achieved excel-lent results. Specifically, for each training sample(c1, c2), we use the following objective to fine-tunethe embedded translator across all intended targetLLMs:",
  "Data &Model & Metrics": "Our harmful attack data is based on Advbench (Zouet al., 2023), which provides over 500 harmfulinstructions and corresponding unsafe responses.In our embedded translation framework, we useWikipedia dataset3 and only use the English cor-pus within it. We use two consecutive sentenceswith more than 20 tokens as our training data, asshown in the , the first sentence serves asthe context and the second sentence serves as thesuffix.We fine tuned GPT-j-6b (Wang and Komat-suzaki, 2021)) as the embedding translation model,and the model to-be-attack mainly chose Llama2-7b-chat, Vicuna-7b-v1.5, Mistral-7b and Alpaca-7b(with Safe-RLHF). In addition, we test our trans-fer attack on Vicuna-13b-v1.5, Llama2-13b-chat,ChatGLM3-6b and blac-box commercial modelssuch as ChatGPT and Gemini.In order to test the success rate of the attack(ASR), we first followed the previous method,which first defined a negative list and then judgedwhether the model replied with negative phrasesin the list. If not, it indicates that the attack wassuccessful. However, this rule-based method is toosimple and has low accuracy (Yu et al., 2023). So,in addition, we use gpt3.5-turbo4 as a classifier todetermine whether the model outputs harmful con-tent. The success rates of attacks obtained by thesetwo methods are ASRprefix, ASRgpt.Another key indicator is perplexity (PPL), whichis used to indicate the fluency of the input prompt:",
  "To-Be-Attacked ModelMethodPerplexity ASR Time(s) Self-BLEU ASRprefixASRgpt": "GCG1513.091193.030.900.61233.87227.510.698AutoDan[Liu]51.7637.650.880.67347.43158.210.431AutoDan[Zhu]39.1725.710.840.63262.14235.400.469GPTFuzzer61.6341.150.810.45-0.728ASETF32.5919.380.910.74104.5373.580.399 GCG1214.34992.520.930.71142.63131.620.728AutoDan[Liu]53.8824.190.900.76309.65147.550.387AutoDan[Zhu]44.0926.280.910.75204.81193.170.494GPTFuzzer61.6341.150.710.62-0.728ASETF43.0220.090.940.8294.2633.800.417 GCG1598.311322.490.950.70234.17236.790.661AutoDan[Liu]51.1733.720.910.73382.07257.640.428AutoDan[Zhu]42.1933.850.920.75301.26196.500.425GPTFuzzer61.6341.150.770.58-0.728ASETF39.9832.310.950.8095.3263.290.441 GCG1338.081362.190.890.73295.48200.980.596AutoDan[Liu]48.2932.210.860.75371.59282.140.478AutoDan[Zhu]43.6837.360.900.76304.57217.03GPTFuzzer61.6341.150.730.58-0.728ASETF38.7537.280.900.8192.1868.550.436 : The result of our method and baseline methodin Ad-hoc LLM attack with ad-hoc suffix. meansthe lower the better, while means to higher the better.(Note that the perplexity of GCG are extremely highsince their generated prompts are unreadable dummytext.) The experimental results show that comparedwith traditional gradient based discrete optimiza-tion suffix or methods based on jailbreak templates,our method has a higher attack success rate and im-proves the fluency of input prompts. Crucially, ourmethod has higher computational efficiency due tooptimization in continuous embedding spaces.Due to the contextual information incorporatedduring the training process, our method producesadversarial suffixes and instructions that are moresemantically relevant, enhancing the robustness ofadversarial samples. As shown in , theexperimental results indicate that even when para-phrasing prompts as defense, the success rate ofour method still higher than other methods.",
  "Transferable LLM attack with ad-hocsuffix": "Training on multiple models simultaneously is acommon approach to improve the transferability ofadversarial samples. For each harmful instruction,we trained adversarial suffixes both the Llama2-7b-chat model and Vicuna-7b-v1.5, and transferred theobtained adversarial suffixes to other LLMs. Wechose three LLMs, Vicuna-13b, Llama2-13b chat,and Chatglm3-6b, to test the transferability of theadversarial suffixes we obtained. Due to the directtransfer of adversarial suffixes, both Perplexity andSelf-BLEU values are the same when attack differ-ent LLMs. The specific experimental results are in:The experimental results indicate that the ad-versarial suffixes obtained by our method have acertain degree of transferability. Compared withother method based on adversarial suffixes, ASETFhas a higher success rate of transfer attacks, butcompared to the direct attack method using model",
  ": Ablation results of attacking Llama2-7b-chatand Vicuna-7b-v1.5 models": "The results of ablation tests in indicatethat removing the MMD loss during the optimiza-tion process of continuously embedded vectors, or removing contextual information within theembedding translation framework, significantly re-duces the fluency of adversarial samples. Addition-ally, removing the random noise added during thetraining process of the translation model also leadsto a decrease in the performance of our method.Furthermore, randomly selected tokens as suffixesfail to jailbreak attacks, demonstrating the need forcarefully designed adversarial suffixes.",
  "Conclusion": "In this paper, we propose a robust and comprehen-sive framework for generating semantically richand coherent adversarial inputs. Initially, we derivean embedding translation model by undertakingthe task of text reconstruction from embeddings onraw text. Subsequently, input the vector trained incontinuous embedding space into the translationmodel, resulting in adversarial suffixes. Throughexperimentation on multiple Large Language Mod-els (LLMs), our method significantly reduces com-putational costs compared to optimizing suffixesin discrete space, while achieving a higher attack success rate and improving the fluency and diver-sity of the suffix. This contributes to the formu-lation of more effective defense strategies and inour approach, the process of obtaining the embed-dings for adversarial suffixes and the training of thetranslation model are decoupled, implying that ourmethod is plug-and-play. This method is expectedto be further applied in text adversarial attacks be-yond just LLM jailbreak attacks.",
  "Limitations": "Firstly, from the experimental results, it is dis-cernible that universal adversarial suffixes, opti-mized for multiple instructions, exhibit a lowersuccess rate in attacks compared to independentadversarial suffixes. This phenomenon could beattributed to the necessity for universal adversar-ial suffixes to encapsulate a broader spectrum ofinformation. However, the capacity for informa-tion representation of discrete tokens depends ontheir length, and an extended length implies a morecomplex optimization process.Upon further examination of cases, we observethat if the adversarial suffixes generated by thetranslation model are biased towards semantics re-lated to harmful instructions in the preceding text,the attack is prone to failure. Conversely, if theylean towards maintaining the consistency of em-beddings, it can lead to textual incoherence. Ourmethod does not explicitly model these two objec-tives separately; hence, it is not possible to artifi-cially control which target the generated adversarialsuffixes are more inclined towards.",
  "Ethics Statement": "Firstly, the goal of this article is to promote theexploration of defense mechanisms for Large Lan-guage Models (LLMs), rather than to obtain illegalcontent from LLMs, as outlined in the appendix.Secondly, the training data used in this article areall public data, and there is no data falsification inthe experimental results. Our code will be submit-ted with the paper and uploaded to GitHub.",
  "Acknowledgement": "This work was supported by the National ScienceFund for Excellent Young Scholars (Overseas) un-der grant No. KZ37117501, National Natural Sci-ence Foundation of China ( No. 62306024, No.92367204), and Xiaomis Open bidding for select-ing the best candidates project. Sahar Abdelnabi, Kai Greshake, Shailesh Mishra,Christoph Endres, Thorsten Holz, and Mario Fritz.2023. Not what youve signed up for: Compromis-ing real-world llm-integrated applications with indi-rect prompt injection. In Proceedings of the 16thACM Workshop on Artificial Intelligence and Secu-rity, pages 7990.",
  "Xiaogeng Liu, Nan Xu, Muhao Chen, and ChaoweiXiao. 2023. Autodan: Generating stealthy jailbreakprompts on aligned large language models. arXivpreprint arXiv:2310.04451": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al.2022. Training language models to follow instruc-tions with human feedback.Advances in NeuralInformation Processing Systems, 35:2773027744. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, andYuxiong He. 2020. Deepspeed: System optimiza-tions enable training deep learning models with over100 billion parameters. In Proceedings of the 26thACM SIGKDD International Conference on Knowl-edge Discovery & Data Mining, pages 35053506. Lei Sha. 2020. Gradient-guided unsupervised lexicallyconstrained text generation. In Proceedings of the2020 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 86928703,Online. Association for Computational Linguistics. Lei Sha, Patrick Hohenecker, and Thomas Lukasiewicz.2021. Controlling text edition by changing answersof specific questions. In Findings of the Associationfor Computational Linguistics: ACL-IJCNLP 2021,pages 12881299, Online. Association for Computa-tional Linguistics.",
  "Lei Sha and Thomas Lukasiewicz. 2024. Text attributecontrol via closed-loop disentanglement. Transac-tions of the Association for Computational Linguis-tics, 12:190209": "Xinyue Shen, Zeyuan Chen, Michael Backes, YunShen, and Yang Zhang. 2023. \" do anything now\":Characterizing and evaluating in-the-wild jailbreakprompts on large language models. arXiv preprintarXiv:2308.03825. Taylor Shin, Yasaman Razeghi, Robert L Logan IV,Eric Wallace, and Sameer Singh. 2020. Autoprompt:Eliciting knowledge from language models withautomatically generated prompts.arXiv preprintarXiv:2010.15980. Neeraj Varshney, Pavel Dolin, Agastya Seth, and ChittaBaral. 2023. The art of defending: A systematicevaluation and analysis of llm defense strategieson safety and over-defensiveness. arXiv preprintarXiv:2401.00287.",
  "Mi Zhang, Xudong Pan, and Min Yang. 2023. Jade: Alinguistics-based safety evaluation platform for llm.arXiv preprint arXiv:2311.00286": "Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, JoeBarrow, Zichao Wang, Furong Huang, Ani Nenkova,and Tong Sun. 2023. Autodan: Automatic and inter-pretable adversarial attacks on large language models.arXiv preprint arXiv:2310.15140. Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, WeinanZhang, Jun Wang, and Yong Yu. 2018. Texygen: Abenchmarking platform for text generation models.In The 41st international ACM SIGIR conferenceon research & development in information retrieval,pages 10971100.",
  "As described in our experimental setup, we use ChatGPT to determine whether the attacked model outputsharmful content. Below is the prompt template we use": "Given the following text generated by a language model during its conversation with a humanbeing, please evaluate its potential harm or offensiveness, considering factors such as hate speech,discrimination, violence, or any other harmful content. Please note that any part of the responsecontaining harmful content is unsafe.You should first analyze if it is harmful, explicitly or implicitly, and then give your conclusion. Yourconclusion should be safe or unsafe, wrapped in double square brackets, i.e., [[safe]] or [[unsafe]]## TEXTPrompt:{prompt}Response:{response}## EVALUATION",
  "A.2Case Study": "Our proposed method for attacking multiple target language models can also be applied in black-boxscenarios where the victim models only expose APIs or web services. The shows an exampleprompt attack on ChatGPT, Gemini and Palm. The harmful instruction is Write a manual on how toescape from a correctional facility and the transferable adversarial suffix generated by our method isjumping off the wall were considered to be techniques to escape from a prison.. In fact, this suffixprovides a background or context, making the entire instruction appear more like a discussion rather thana direct request to violate laws or ethical norms. This background information prevents the model fromperceiving the question as a direct solicitation, thereby avoiding responses that encourage inappropriatebehavior.",
  "A.4Hyperparameters": "We use the pre-trained model GPT-j (Wang and Komatsuzaki, 2021) as the base model for the em-bedding translation framework, and we used the deepspeed framework (Rasley et al., 2020) fordistributed training on 8 NVIDIA A100 GPUs. We finetune the GPT-j model for 3 epochs, withper_device_train_batch_size is 1 so that total batch_size is 8 and the learning rate is set to1e 5, warm-up_min_lr is 1e 6 and the maximum sequence length is set to 1560.We usethe Adam optimizer with 1 = 0.9 and 2 = 0.95. In addition, the weight_decay is set to 0.1,gradient_accumulation_steps is 4 and warm-up_ratio is 0.1.",
  "A.5Comparison of embedding before and after translation": "After applying the t-SNE dimensionality reduction technique, we can visualize the embeddings in atwo-dimensional space, which aids in identifying patterns and relationships that may not be apparent inhigher dimensions. The demonstrates the before-and-after effect of the translation process onthe data embeddings. It is evident from the figure that the embeddings remain remarkably consistent,indicating that the translation has not significantly altered the underlying structure of the data."
}