{
  "Abstract": "Information Extraction (IE), aiming to extractstructured information from unstructured nat-ural language texts, can significantly benefitfrom pre-trained language models. However,existing pre-training methods solely focus onexploiting the textual knowledge, relying exten-sively on annotated large-scale datasets, whichis labor-intensive and thus limits the scalabilityand versatility of the resulting models. To ad-dress these issues, we propose SKIE, a novelpre-training framework tailored for IE that inte-grates structural semantic knowledge via con-trastive learning, effectively alleviating the an-notation burden. Specifically, SKIE utilizes Ab-stract Meaning Representation (AMR) as a low-cost supervision source to boost model perfor-mance without human intervention. By enhanc-ing the topology of AMR graphs, SKIE deriveshigh-quality cohesive subgraphs as additionaltraining samples, providing diverse multi-levelstructural semantic knowledge. Furthermore,SKIE refines the graph encoder to better cap-ture cohesive information and edge relation in-formation, thereby improving the pre-trainingefficacy. Extensive experimental results demon-strate that SKIE outperforms state-of-the-artbaselines across multiple IE tasks and show-cases exceptional performance in few-shot andzero-shot settings.",
  "Introduction": "Information Extraction (IE) aims to extract struc-tured information from unstructured natural lan-guage texts (Grishman and Sundheim, 1996; Gr-ishman, 2019), which encompasses several sub-tasks such as Named Entity Recognition (NER)(Shen et al., 2023; Ghosh et al., 2023), RelationExtraction (RE) (Sun et al., 2023; Wu et al., 2023),and Event Extraction (EE) (Guzman Nateras et al.,2023; Liu et al., 2023). Considering the inherentconnections among these subtasks, recent methods",
  ":time": ": An example from the WikiEvents dataset.The AMR graph (left) highlights key elements: theentities driver and house are connected by the relation\"source\". The event trigger is come out, with the entitiesthemselves serving as the arguments for the event. propose to jointly resolve them within a unifiedframework, capitalizing on the generalization ver-satility of pre-trained language models (PLMs).For instance, UIE (Lu et al., 2022) embedsschema-based prompts into the corpora to pre-train a text-to-structure generative PLM, enablingit to generate uniform representations. USM (Louet al., 2023) utilizes three kinds of superviseddatasets and employs unified token linking to struc-ture information during pre-training. Mirror (Zhuet al., 2023) designs a unified data interface toreorganize datasets into multi-slot tuples for pre-training. MetaRetriever (Cong et al., 2023) in-troduces a Meta-Pretraining Algorithm to retrievetask-specific knowledge from PLMs for IE tasks.However, existing pre-training methods sufferfrom two major challenges. First, the high cost ofannotation restricts existing datasets for IE tasks toa few predefined categories and small data volumes(Lou et al., 2023), limiting the amount of super-vised datasets available for pre-training. Second,these methods are constrained to solely utilizingannotated textual knowledge, neglecting the poten-tial structural semantic knowledge inherent in texts,which hinders their ability to leverage complexstructural knowledge. A feasible solution is to generate self-supervisedsignals from extensive unsupervised data by lever-aging their structural semantic knowledge, insteadof relying on limited supervised data. AbstractMeaning Representation (AMR) (Banarescu et al.,2013), which has demonstrated its ability to cap-ture structural semantic knowledge within textswithout additional human effort (Bai et al., 2022;Wang et al., 2015), stands out as a fitting choice. illustrates an example of a text segmentand its corresponding AMR graph. The text is con-verted into an AMR graph through AMR parsing,where nodes represent basic semantic units suchas entities and predicates, while edges denote theirsemantic relations (Bai et al., 2022).Armed with this insight, we propose SKIE, anovel pre-training method that integrates Structuralsemantic Knowledge to enhance the models ver-satility across multiple IE tasks. SKIE leveragesAMR parsing to generate self-supervised signals,offering a flexible and general approach to semanticrepresentations. To capture more diverse semanticstructures, SKIE introduces cohesive subgraphs,which are densely connected subsets of pivotalnodes within the graph. Then, contrastive learn-ing is employed to bridge the associations betweentexts and graphs.Specifically, SKIE comprises three key modules:the topology enhancement module, the encodingcohesion module, and the contrastive learning mod-ule. Capitalizing on the cohesion-guided topologyenhancement, we extract cohesive subgraphs fromAMR graphs to acquire diverse multi-level struc-tural semantic knowledge. To preserve edge rela-tion information and cohesive information in thegraphs, we propose a topology-aware encoder forcohesive encoding. By learning from correlationsand distinctions between texts and graphs via con-trastive learning, SKIE can comprehend semanticassociations and intrinsic patterns within the texts,enabling better adaptation to IE tasks.Our contributions are as follows: We propose a novel pre-training method,which incorporates structural semantic knowl-edge from AMR graphs into the training pro-cess to enhance the capability and versatilityof the resulting models, without additionalannotation needs.",
  "Information Extraction": "IE can be formulated as a text-to-structure task,with different IE subtasks corresponding to dif-ferent target structures. OneIE (Lin et al., 2020)extracts optimal global information from inputtexts through global graph searching. Addition-ally, TANL (Paolini et al., 2021) translates struc-tured prediction language tasks into IE processesthrough enhanced translation tasks between naturallanguages.Recently, researchers have delved into univer-sal frameworks for IE tasks. UIE (Lu et al., 2022)achieves generic modeling and adaptive structuregeneration for various IE tasks through structuredlanguage extraction and pattern-based promptingmechanisms. USM (Lou et al., 2023) decouplesIE tasks and employs a unified semantic matchingframework alongside unified token linking opera-tions. UniEX (Ping et al., 2023) and UTC-IE (Yanet al., 2023) transform text-based IE tasks into a uni-fied token-pair problem. UniEX leverages pattern-based cues and text information encoding, whereasUTC-IE achieves unified IE through axis-aware in-teractions and local interactions on the token-pairfeature matrix.",
  "Abstract Meaning Representation": "AMR graph is a single-rooted directed graph usedto represent the meaning of texts. AMR parsingtranslates texts into corresponding AMR graphs(Cai and Lam, 2020; Hoang et al., 2021; Wanget al., 2022; Vasylenko et al., 2023). With thecontinuous development of deep learning, therehas been a gradual emergence of neural transition-based parsers, sequence-to-graph parsers, andsequence-to-sequence parsers (Bai et al., 2022).The neural transition-based parsers (Fernan-dez Astudillo et al., 2020; Zhou et al., 2021; Droz-dov et al., 2022) incrementally construct AMRgraphs by applying basic operations (e.g., SHIFT,",
  "Maximize": ": The overall framework of SKIE, which comprises three key modules: topology enhancement, encoding co-hesion, and contrastive learning. The topology enhancement constructs cohesive subgraphs using both deterministicand probabilistic topology enhancement strategies. The encoding cohesion independently extracts features fromtexts and graphs. Finally, the contrastive learning analyzes the semantic correspondences between texts and graphs. LEFT-ARC, RIGHT-ARC) in transition-based pars-ing. Sequence-to-graph parsers (Zhang et al., 2019;Cai and Lam, 2020; Xia et al., 2021) directly gener-ate AMR graphs from texts. In addition, sequence-to-sequence parsers (Bevilacqua et al., 2021; Yuand Gildea, 2022; Gao et al., 2023) are employedto transform AMR parsing into a \"linearized\" se-quence generation task.",
  "Cohesive Subgraphs": "After obtaining AMR graphs, we introduce co-hesive subgraphs to derive semantic representa-tions at different levels. These cohesive subgraphsaim to capture tight structures and semantic cor-relations within AMR graphs, revealing multi-level structural cohesion contained in the texts.We primarily focus on the k-core (Kong et al.,2019) due to its effectiveness in identifying corestructures within graphs and its applicability tolarge-scale networks (King et al., 2023).Foran AMR graph G = (V, E), we extract a setof k-core cohesive subgraphs, denoted as G =Gk|k = kmin, kmin+1, . . . , kmax.First, we utilize a deterministic topological en-hancement strategy, which employs deterministicrules to generate subgraphs and select them basedon predefined conditions. To further improve co-hesion during graph diffusion, we strategically as-sign higher weights to edges within these cohesivesubgraphs, thereby emphasizing key relations andenhancing overall structure connectivity.",
  "SPPR = (I (1 )D1/2AD1/2)1(3)": "where (0, 1) is the teleport probability, I is thedegree matrix of nodes, D is the diagonal degreematrix, and A Ai,j is the adjacency matrix withAi,j = we(eij).Second, to compensate for the potential knowl-edge missing in deterministic topological enhance-ment, we incorporate a probabilistic topologicalenhancement strategy. This method typically gen-erates subgraphs based on certain probability distri-butions or random processes, introducing random-ness in subgraph generation, resulting in potentiallydifferent subgraphs each time.Specifically, we introduce probabilistic mech-anisms with the original dropping probability Pto modify edges or nodes in the graph, therebyaltering its topological structure. To maintain co-hesion in the subgraphs generated by probabilistictopological enhancement, we employ a decay fac-tor (0, 1) to limit the probability P, which isinitialized based on the node weights. The proba-bilities P(vi) for nodes vi and P(eij) for edgeseij are:",
  "Encoding Cohesion Module": "3.2.1Text EncoderTo ensure a direct correspondence between AMRgraphs and the original texts, we utilize Roberta-Large (Liu et al., 2019) as the text encoder, aligningits encoding structure with that of the AMR parser.Roberta-Large is a PLM based on Transformer ar-chitecture, known for its ability to effectively modeldiverse information across extensive unsupervisedcorpora. Given a text s, it is encoded through mul-tiple layers of self-attention mechanisms to obtainthe resulting vector from the last hidden layer. Thevector encapsulate not only superficial information(such as vocabularies and phrases) but also contex-tual nuances.",
  "Graph Encoder": "We employ a graph encoder to extract correspond-ing vectors from AMR graphs and their cohesivesubgraphs. However, traditional graph encodersexhibit shortcomings in handling such graphs intwo primary aspects. First, they operate within themessage-passing neural network framework, whichconcentrates on integrating neighbor informationinto a comprehensive representation, leading to theloss of substructure details. Second, since cohesivesubgraphs are extracted based on node connectivityand feature-dense relations among nodes, it is cru-cial to preserve such semantic knowledge duringthe encoding process. Therefore, the graph encodermust prioritize maintaining both detailed substruc-ture information and dynamic node relations.In light of these limitations, we propose aTopology-aware Graph Substructure Network (T-GSN) that incorporates structural topology intoGSN (Bouritsas et al., 2023). Specifically, we in-troduce relation-specific transformations to handleinformation uniquely according to the type of rela-tions, allowing for tailored information processingfrom neighbors to derive the aggregated feature foreach node. The update equation is articulated asfollows:",
  "(6)where h(l)idenotes the feature of node vi at layer l. represents the activation function, such as ReLU.N ri represents the set of neighbor nodes of node": "vi under relation r. W(l)rdenotes the weight ma-trix for relation r at layer l, and W(l)orepresentsthe self-connection weight matrix to maintain ownfeatures of the nodes. ni,r is the normalization con-stant, typically chosen as |N ri |, which representsthe number of neighbors of node vi associated withrelation r.Then, the updated feature h(l+1)iand the en-coded feature xi of node vi in the AMR graphare fed into T-GSN:",
  "T-GSN(vi) = AGG(h(l+1)i, h(l+1)j, xi, xj)(7)": "where AGG denotes a neighborhood aggregationfunction, which may involve utilizing a multi-layerperceptron to aggregate features of node vi fromits neighbors j N ri .Ultimately, we encode the graphs not only basedon their topological structures but also by consid-ering the semantic relations within substructures.T-GSN significantly amplifies the expressive capa-bilities of GNNs, allowing them to more accuratelycapture and comprehend the intricate structures andrepresentations of AMR graphs and their cohesivesubgraphs.",
  "Contrastive Learning Module": "Contrastive learning has made significant stridesin the field of representation learning, particu-larly demonstrating outstanding results in self-supervised training. It learns the intrinsic represen-tation of data by maximizing the distance amongnegative sample pairs and minimizing the distanceamong positive sample pairs. During pre-training,contrastive learning can help models distinguishbetween graph-text pairs with different similari-ties, enabling them to more accurately capture thecorrespondence between texts and graphs.Specifically, we employ triplet loss (Schroffet al., 2015) as our contrastive learning loss func-tion, structured in triplets <anchor, positive, nega-tive>. For a given text s, we designate it as an an-chor, forming positive pairs with its correspondingAMR graph and AMR cohesive subgraphs, whilegenerating negative pairs with non-matching AMRgraphs and AMR cohesive subgraphs. Our coreconcept revolves around ensuring a certain mar-gin separation between positive and negative pairs.This optimization ensures that samples of the samecategory in the embedding space are sufficientlyclose, while samples of different categories are ad-equately distant. In essence, the distance between",
  "L = max(0, |s g+|2 |s g|2 + m)(8)": "here, s, g+, and g respectively represent the vec-tors that map the text s, the positive graphs corre-sponding to the text, and the negative graphs intothe embedding space. || denotes the Euclidean dis-tance (or other distance metric), and m is a positivenumber defining the minimum separation betweenpositive and negative sample pairs.SKIE can better comprehend the semantic corre-spondence between texts and graphs by minimiz-ing loss to enhance performance in downstream IEtasks. This effect is particularly pronounced whendealing with semantically complex texts.",
  "Task-specific Fine-tuning": "Since the focus of this paper is on pre-trainingrather than fine-tuning, we adopt fine-tuning tech-niques from previous work (Zhu et al., 2023) toefficiently adapt the PLM to different IE tasks andsettings. First, we convert the text s into an inputti in a unified data format by embedding an in-struction and a schema label. The instruction startswith a leading token [I] and includes a sentence toprompt the model with a specific task (e.g., \"Pleaseextract event information from the given text, in-cluding triggers and arguments\"). The schemalabel serves as guidance for different IE tasks (e.g.,[LM] for entities or event types and [LR] for rela-tions or argument roles).Then, we use the PLM to convert ti into thevector zi Rdz and obtain the adjacency matrixB of the multi-span cyclic graph through biaffineattention (Dozat and Manning, 2017). The multi-span cyclic graph includes three types of connec-tions: consecutive connections within the sameentity span, jump connections linking differentslots within tuples, and tail-to-head connectionsmarking the boundaries of the graph. The con-nection probability pcij (c {continuous, jump,tail-to-head}) between ti and tj in the matrix B(pcij > 0.5, Bcij = 1, otherwise Bcij = 0) is formu-lated as:",
  "CASIE-Arg-61.3062.9163.26-61.2760.3763.96": ": Overall F1-scores on 8 IE benchmarks (-Tgg. and -Arg. denote event trigger and arguments, respectively).These datasets are excluded from the pre-training phase. The results of UIE are reported based on the UIE-Largemodel proposed by Lu et al. (2022). The best results are shown in bold. connections. FFNN is a feedforward neural net-work incorporating rotary positional embeddings,as introduced in RoFormer (Su et al., 2024).Finally, we use Circle Loss (Su et al., 2022) asthe downstream loss function:",
  "Datasets": "We collect a large amount of datasets and transformthem into unified unsupervised corpora for pre-training. A detailed list of the pre-training corporacan be found in Appendix A.We conduct experiments on NER, RE, andEE tasks, including 8 IE benchmarks: ACE04(Mitchell et al., 2005), ACE05 (Walker et al.,2006), CoNLL03 (Tjong Kim Sang and De Meul-der, 2003), CoNLL04 (Roth and Yih, 2004), Sci-ERC (Luan et al., 2018), and CASIE (Satyapanichet al., 2020). Additionally, we employ five subsetsof CrossNER (AI, literature, music, politics, andscience) (Liu et al., 2021) to evaluate the zero-shotcapabilities of SKIE. All extraction tasks adopt anend-to-end setting, taking texts as input and directlygenerating the target structure.",
  "Baselines": "We compare SKIE with generation-based TANL(Paolini et al., 2021), UIE (Lu et al., 2022), MetaRe-triever (Cong et al., 2023), and extraction-basedUniEX (Ping et al., 2023), USM (Lou et al., 2023),UTC-IE (Yan et al., 2023), Mirror (Zhu et al.,2023), respectively. Among them, UIE, USM, Mir-ror, and MetaRetriever are all pre-training methodsfor IE tasks. During pre-training, we tune the graph encodinglayers, the margin in the triplet loss, and the decayfactor in the topology enhancement strategy to im-prove training outcomes. The implementation de-tails of the pre-training and fine-tuning phases areincluded in Appendix B. The code is available at",
  "Evaluation": "We employ span-based offset Micro-F1 as the pri-mary metric to evaluate methods for different IEtasks: For NER tasks, an entity is considered cor-rect if its offset and type are correct. For RE tasks,under strict matching, a relation is correct if therelation type, the offsets, and the types of relatedentities are correct. For event trigger extractiontasks, an event trigger is considered correct if itsoffset and event type match the reference trigger.For event argument extraction tasks, an event ar-gument is correct if its offset, role type, and eventtype match the reference argument.",
  "Main Results": "shows the performance of all methods onthe aforementioned IE benchmarks. Compared toother baselines, SKIE outperforms them acrossall datasets, achieving an average F1-score im-provement of 1.49, 6.75, and 3.42 in NER, RE,and EE tasks, respectively. This strongly demon-strates the effectiveness of our pre-training method,which leverages structural semantic knowledge toenhance the performance on IE tasks.Meanwhile, our pre-training corpora supplementmore RE and EE datasets compared to pre-trainingmethods such as Mirror (Zhu et al., 2023), result-ing in a significant improvement in downstreamRE and EE tasks. SKIE can rapidly adapt to down-stream IE tasks, enabling efficient and targeted ex-tractions. Notably, our pre-training corpora do notrequire manually setting prompts or annotations,substantially reducing labor costs and facilitatingthe integration of new corpora in the future. Addi-tionally, it avoids the impact of label errors or labeldrift on training.",
  "We focus on the performance of SKIE in low-resource settings.To validate its rapid adap-tation capability, we conduct few-shot experi-": "ments. Specifically, we sample 1/5/10 texts perentity/relation/event type in the training set, follow-ing the experimental setup of previous work (Louet al., 2023). To mitigate the impact of randomsampling, each experiment is repeated 10 timeswith different samples, and the average F1-score isused to represent performances.As shown in , SKIE achieves excel-lent results on CoNLL03, CoNLL04, and ACE05.Among the four tasks, the NER task is relativelyeasier to handle and can achieve satisfactory resultswith minimal fine-tuning. However, for tasks asso-ciated with other datasets, there is a significant gapbetween the few-shot fine-tuning results and thefull fine-tuning results, highlighting the difficultyof these tasks and the effectiveness of fine-tuning.Additionally, SKIE can learn deeper structural se-mantic knowledge during pre-training, rather thancapturing information specific to a particular task.Therefore, compared to baselines with limited sam-ples, SKIE performs better on these tasks even withonly a few samples.",
  "shows the zero-shot results of SKIE on5 NER datasets, which are eliminated during pre-training. SKIE outperforms USM and Mirror on": "most of datasets, achieving a superior average F1-score of 58.03, notably exceeding USM. Empha-sized that USM trains on the same datasets andevaluates using the provided labels, while SKIE isnot exposed to these datasets before testing.Among the above datasets, SKIE achieves themost outstanding performance enhancement on lit-erature, with an average F1-score improvement of10.43, due to using a dataset containing academiccontent during pre-training. However, SKIE fallsshort of Mirror in the politics dataset, with a lowerF1-score of 3.77, suggesting that enhancing the pre-training with more diverse and comprehensive datacould potentially improve SKIEs performance.",
  "Ablation Results": "To validate the effectiveness of SKIE, we explorethe impact of modifications to the graph encoderand topological enhancement strategies. As shownin , GSN can capture local substructure fea-tures in graphs more precisely, rather than GCNfocusing solely on global features. However, GSNperforms worse than T-GSN, resulting in an aver-age F1-score drop of 7.69, proving that modifyingthe graph encoder enables better capture of the co-hesive information in AMR graphs and preservesedge relation information.Additionally, the results reveal that a single de-terministic topological enhancement may lead toknowledge missing, while a single probabilistictopological enhancement may shift the cohesivecenter, thereby affecting the quality of the gener-ated cohesive subgraphs. Meanwhile, the averageF1-score without cohesive subgraphs decreases by6.97. This indicates that cohesive subgraphs in-troduce multi-level structural semantic knowledgeduring pre-training, markedly enhancing the ef- fectiveness and generalization versatility of SKIE.When removing both cohesive subgraphs and T-GSN, there are expressive declines in performanceacross IE tasks, underscoring the essential roles ofthese components.",
  "Corpora Validity Results": "To evaluate the quality of the corpora used forpre-training, we conduct an ablation study on dif-ferent types of pre-training data, as shown in Ta-ble 5. The results indicate that removing any partof pre-training data negatively impacts the perfor-mance, demonstrating the effectiveness of our pre-training corpora. Additionally, it can be seen thatpre-training with a combination of different typesof pre-training data, rather than relying solely on asingle type, improves the performance of the cor-responding downstream tasks. This suggests thatthere is indeed a correlation between different IEtasks. Therefore, it is beneficial to perform joint IEtasks, as this facilitates mutual learning among dif-ferent IE tasks, leading to better extraction results.",
  "Language Adaptation Results": "To verify language adaptability, we evaluate SKIEon Multiconel (Malmasi et al., 2022), which isa common multilingual dataset for NER. shows the results of our English AMR parser basedmodel compared to ChatGPT and GLiNER (Zara-tiana et al., 2024). GLiNER-En and GLiNER-Multiare two variants of GLiNER, utilizing two versionsof deBERTa-v3: GLiNER-En uses deBERTa-v3-Large, while GLiNER-Multi employs mdeBERTa-v3-base, which is the multilingual version ofdeBERTa-v3. It can be seen that even using anEnglish AMR parser and pre-training on Englishcorpora, our model can still achieve satisfactory IEperformance on other languages, demonstrating thegeneralizability of SKIE.",
  "Conclusion": "In this paper, we propose SKIE, a contrastive pre-training method designed to enhance IE modelswith structural semantic knowledge. Specifically,SKIE leverages AMR graphs generated from unsu-pervised texts as self-supervised signals and furtherextracts cohesive subgraphs to provide multi-levelstructural semantic knowledge. Additionally, SKIEintegrates edge relation information and cohesioninformation for the encoder, effectively enhancing the learning process of PLMs. Compared to exist-ing methods, SKIE enables the training on unsuper-vised datasets in a self-supervised manner, signifi-cantly reducing the annotation burden. The result-ing models demonstrate proficiency in handling IEtasks on complex texts by utilizing the structuralsemantic knowledge. Experimental results showthat SKIE achieves state-of-the-art performancesacross multiple IE tasks and excels in few-shot andzero-shot settings. Our future work will focus onrefining SKIE to alleviate noise in AMR graphsand extending its application to broader NLP tasks.",
  "Limitations": "Although SKIE has shown outstanding perfor-mance in IE tasks, it still has some limitations.Firstly, we use nearly a million datasets duringpre-training, and the need to encode both textsand graphs separately resulted in lengthy runtime.Secondly, due to the constraints of existing publicdatasets, the NER, RE, and EE pre-training datasetswe found are imbalanced, with the EE dataset beingmuch smaller in scale compared to NER and RE,limiting the performance of the EE task. Finally,the Roberta-Large model we used has a maximuminput sequence length of 512 tokens. However, IEtasks often require processing longer texts. In thefuture, we will consider using a sliding windowapproach to handle the input or exploring othermodels capable of processing longer sequences,such as Longformer or BigBird.",
  "Ethics Statement": "In the development of our pre-training framework,we acknowledge several ethical considerations.Our method requires large-scale corpora collectedfrom the Internet, which may exhibit common do-main biases (e.g., in the news domain). Such biasescan lead to errors in domain-specific IE tasks, po-tentially causing inaccuracies in real-world appli-cations and affecting the reliability of the resultingmodel. It is crucial to recognize and address thesepotential ethical issues to ensure that our method isused responsibly and ethically in real-world appli-cations.",
  "Xuefeng Bai, Yulong Chen, and Yue Zhang. 2022": "Graph pre-training for AMR parsing and generation.In Proceedings of the 60th Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 60016015, Dublin, Ireland.Association for Computational Linguistics. Laura Banarescu, Claire Bonial, Shu Cai, MadalinaGeorgescu, Kira Griffitt, Ulf Hermjakob, KevinKnight, Philipp Koehn, Martha Palmer, and NathanSchneider. 2013. Abstract Meaning Representationfor sembanking. In Proceedings of the 7th LinguisticAnnotation Workshop and Interoperability with Dis-course, pages 178186, Sofia, Bulgaria. Associationfor Computational Linguistics. Michele Bevilacqua, Rexhina Blloshmi, and RobertoNavigli. 2021. One spring to rule them both: Sym-metric amr semantic parsing and generation withouta complex pipeline. Proceedings of the AAAI Confer-ence on Artificial Intelligence, 35(14):1256412573. Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou,and Michael M. Bronstein. 2023. Improving graphneural network expressivity via subgraph isomor-phism counting. IEEE Transactions on Pattern Anal-ysis and Machine Intelligence, 45(1):657668. Deng Cai and Wai Lam. 2020. AMR parsing via graph-sequence iterative inference. In Proceedings of the58th Annual Meeting of the Association for Compu-tational Linguistics, pages 12901301, Online. Asso-ciation for Computational Linguistics. Xin Cong, Bowen Yu, Mengcheng Fang, Tingwen Liu,Haiyang Yu, Zhongkai Hu, Fei Huang, Yongbin Li,and Bin Wang. 2023. Universal information extrac-tion with meta-pretrained self-retrieval. In Findingsof the Association for Computational Linguistics:ACL 2023, pages 40844100, Toronto, Canada. As-sociation for Computational Linguistics.",
  "Deep biaffine attention for neural dependency pars-ing. In International Conference on Learning Repre-sentations": "Andrew Drozdov, Jiawei Zhou, Radu Florian, AndrewMcCallum, Tahira Naseem, Yoon Kim, and RamnAstudillo. 2022. Inducing and using alignments fortransition-based AMR parsing. In Proceedings ofthe 2022 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 10861098,Seattle, United States. Association for ComputationalLinguistics. Ramn Fernandez Astudillo, Miguel Ballesteros, TahiraNaseem, Austin Blodgett, and Radu Florian. 2020.Transition-based parsing with stack-transformers. InFindings of the Association for Computational Lin-guistics: EMNLP 2020, pages 10011007, Online.Association for Computational Linguistics. Bofei Gao, Liang Chen, Peiyi Wang, Zhifang Sui, andBaobao Chang. 2023. Guiding AMR parsing withreverse graph linearization. In Findings of the Associ-ation for Computational Linguistics: EMNLP 2023,pages 1326, Singapore. Association for Computa-tional Linguistics. Sreyan Ghosh, Utkarsh Tyagi, Manan Suri, Sonal Ku-mar, Ramaneswaran S, and Dinesh Manocha. 2023.ACLM: A selective-denoising based generative dataaugmentation approach for low-resource complexNER. In Proceedings of the 61st Annual Meetingof the Association for Computational Linguistics(Volume 1: Long Papers), pages 104125, Toronto,Canada. Association for Computational Linguistics.",
  "Valerie King, Alex Thomo, and Quinton Yong. 2023": "Computing (1+epsilon)-approximate degeneracy insublinear time. In Proceedings of the Thirty-SecondInternational Joint Conference on Artificial Intel-ligence, IJCAI-23, pages 21602168. InternationalJoint Conferences on Artificial Intelligence Organi-zation. Main Track. Johannes Klicpera, Stefan Weienberger, and StephanGnnemann. 2019. Diffusion improves graph learn-ing. In Advances in Neural Information ProcessingSystems 32: Annual Conference on Neural Informa-tion Processing Systems 2019, NeurIPS 2019, De-cember 8-14, 2019, Vancouver, BC, Canada, pages1333313345.",
  "Yi-Xiu Kong, Gui-Yuan Shi, Rui-Jie Wu, and Yi-ChengZhang. 2019.k-core: Theories and applications.Physics Reports, 832:132. K-core: Theories andApplications": "Viet Dac Lai, Nghia Trung Ngo, Amir Pouran BenVeyseh, Hieu Man, Franck Dernoncourt, Trung Bui,and Thien Huu Nguyen. 2023. Chatgpt beyond en-glish: Towards a comprehensive evaluation of largelanguage models in multilingual learning. In Find-ings of the Association for Computational Linguis-tics: EMNLP 2023, Singapore, December 6-10, 2023,pages 1317113189. Association for ComputationalLinguistics.",
  "Ying Lin, Heng Ji, Fei Huang, and Lingfei Wu. 2020": "A joint neural model for information extraction withglobal features. In Proceedings of the 58th AnnualMeeting of the Association for Computational Lin-guistics, pages 79998009, Online. Association forComputational Linguistics. Jian Liu, Dianbo Sui, Kang Liu, Haoyan Liu, and ZheZhao. 2023. Learning with partial annotations forevent detection. In Proceedings of the 61st AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 508523,Toronto, Canada. Association for Computational Lin-guistics. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A robustly optimized BERT pretrainingapproach. CoRR, abs/1907.11692. Zihan Liu, Yan Xu, Tiezheng Yu, Wenliang Dai, ZiweiJi, Samuel Cahyawijaya, Andrea Madotto, and Pas-cale Fung. 2021. Crossner: Evaluating cross-domainnamed entity recognition. In Thirty-Fifth AAAI Con-ference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Arti-ficial Intelligence, IAAI 2021, The Eleventh Sympo-sium on Educational Advances in Artificial Intelli-gence, EAAI 2021, Virtual Event, February 2-9, 2021,pages 1345213460. AAAI Press. Jie Lou, Yaojie Lu, Dai Dai, Wei Jia, Hongyu Lin,Xianpei Han, Le Sun, and Hua Wu. 2023.Uni-versal information extraction as unified semanticmatching.In Proceedings of the Thirty-SeventhAAAI Conference on Artificial Intelligence andThirty-Fifth Conference on Innovative Applicationsof Artificial Intelligence and Thirteenth Symposiumon Educational Advances in Artificial Intelligence,AAAI23/IAAI23/EAAI23. AAAI Press. Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, HongyuLin, Xianpei Han, Le Sun, and Hua Wu. 2022. Uni-fied structure generation for universal informationextraction. In Proceedings of the 60th Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 57555772, Dublin,Ireland. Association for Computational Linguistics. Yi Luan, Luheng He, Mari Ostendorf, and HannanehHajishirzi. 2018. Multi-task identification of entities,relations, and coreference for scientific knowledgegraph construction. In Proceedings of the 2018 Con-ference on Empirical Methods in Natural Language",
  "Alexis Mitchell, Stephanie Strassel, Shudong Huang,and Ramez Zakhary. 2005. ACE 2004 MultilingualTraining Corpus": "Giovanni Paolini, Ben Athiwaratkun, Jason Krone, JieMa, Alessandro Achille, RISHITA ANUBHAI, Ci-cero Nogueira dos Santos, Bing Xiang, and StefanoSoatto. 2021. Structured prediction as translation be-tween augmented natural languages. In InternationalConference on Learning Representations. Yang Ping, JunYu Lu, Ruyi Gan, Junjie Wang, Yuxi-ang Zhang, Pingjian Zhang, and Jiaxing Zhang. 2023.UniEX: An effective and efficient framework for uni-fied information extraction via a span-extractive per-spective. In Proceedings of the 61st Annual Meetingof the Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 1642416440, Toronto,Canada. Association for Computational Linguistics. Dan Roth and Wen-tau Yih. 2004. A linear program-ming formulation for global inference in natural lan-guage tasks. In Proceedings of the Eighth Confer-ence on Computational Natural Language Learn-ing (CoNLL-2004) at HLT-NAACL 2004, pages 18,Boston, Massachusetts, USA. Association for Com-putational Linguistics. Oscar Sainz, Iker Garca-Ferrero, Rodrigo Agerri,Oier Lopez de Lacalle, German Rigau, and EnekoAgirre. 2024. Gollie: Annotation guidelines improvezero-shot information-extraction. In The Twelfth In-ternational Conference on Learning Representations,ICLR 2024, Vienna, Austria, May 7-11, 2024. Open-Review.net.",
  "Taneeya Satyapanich, Francis Ferraro, and Tim Finin.2020. Casie: Extracting cybersecurity event informa-tion from text. Proceedings of the AAAI Conferenceon Artificial Intelligence, 34(05):87498757": "Florian Schroff, Dmitry Kalenichenko, and JamesPhilbin. 2015. Facenet: A unified embedding forface recognition and clustering. 2015 IEEE Confer-ence on Computer Vision and Pattern Recognition(CVPR), pages 815823. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,Weiming Lu, and Yueting Zhuang. 2023. Diffusion-NER: Boundary diffusion for named entity recogni-tion. In Proceedings of the 61st Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 38753890, Toronto,Canada. Association for Computational Linguistics.",
  "Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan,Wen Bo, and Yunfeng Liu. 2024. Roformer: En-hanced transformer with rotary position embedding.Neurocomputing, 568:127063": "Jianlin Su, Ahmed Murtadha, Shengfeng Pan, Jing Hou,Jun Sun, Wanwei Huang, Bo Wen, and Yunfeng Liu.2022. Global pointer: Novel efficient span-basedapproach for named entity recognition.Preprint,arXiv:2208.03054. Qi Sun, Kun Huang, Xiaocui Yang, Pengfei Hong,Kun Zhang, and Soujanya Poria. 2023. Uncertaintyguided label denoising for document-level distantrelation extraction. In Proceedings of the 61st An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 1596015973, Toronto, Canada. Association for Computa-tional Linguistics. Erik F. Tjong Kim Sang and Fien De Meulder.2003. Introduction to the CoNLL-2003 shared task:Language-independent named entity recognition. InProceedings of the Seventh Conference on NaturalLanguage Learning at HLT-NAACL 2003, pages 142147. PavloVasylenko,PereLlusHuguetCabot,Abelardo Carlos Martnez Lorenzo, and RobertoNavigli. 2023.Incorporating graph informationin transformer-based AMR parsing.In Findingsof the Association for Computational Linguistics:ACL 2023, pages 19952011, Toronto, Canada.Association for Computational Linguistics.",
  "Chuan Wang, Nianwen Xue, and Sameer Pradhan. 2015": "Boosting transition-based AMR parsing with refinedactions and auxiliary analyzers. In Proceedings ofthe 53rd Annual Meeting of the Association for Com-putational Linguistics and the 7th International JointConference on Natural Language Processing (Vol-ume 2: Short Papers), pages 857862, Beijing, China.Association for Computational Linguistics. Peiyi Wang, Liang Chen, Tianyu Liu, Damai Dai, YunboCao, Baobao Chang, and Zhifang Sui. 2022. Hier-archical curriculum learning for AMR parsing. InProceedings of the 60th Annual Meeting of the As-sociation for Computational Linguistics (Volume 2:Short Papers), pages 333339, Dublin, Ireland. As-sociation for Computational Linguistics. Shengqiong Wu, Hao Fei, Yixin Cao, Lidong Bing, andTat-Seng Chua. 2023. Information screening whilstexploiting! multimodal relation extraction with fea-ture denoising and multimodal topic modeling. InProceedings of the 61st Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 1473414751, Toronto, Canada.Association for Computational Linguistics. Yucheng Wu, Leye Wang, Xiao Han, and Han-Jia Ye.2024. Graph contrastive learning with cohesive sub-graph awareness. In Proceedings of the ACM onWeb Conference 2024, WWW 24, page 629640,New York, NY, USA. Association for ComputingMachinery. Qingrong Xia, Zhenghua Li, Rui Wang, and Min Zhang.2021. Stacked AMR parsing with silver data. InFindings of the Association for Computational Lin-guistics: EMNLP 2021, pages 47294738, PuntaCana, Dominican Republic. Association for Compu-tational Linguistics. Hang Yan, Yu Sun, Xiaonan Li, Yunhua Zhou, XuanjingHuang, and Xipeng Qiu. 2023. UTC-IE: A unifiedtoken-pair classification architecture for informationextraction. In Proceedings of the 61st Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 40964122, Toronto,Canada. Association for Computational Linguistics. Chen Yu and Daniel Gildea. 2022.Sequence-to-sequence AMR parsing with ancestor information.In Proceedings of the 60th Annual Meeting of theAssociation for Computational Linguistics (Volume2: Short Papers), pages 571577, Dublin, Ireland.Association for Computational Linguistics. Urchade Zaratiana, Nadi Tomeh, Pierre Holat, andThierry Charnois. 2024. Gliner: Generalist model fornamed entity recognition using bidirectional trans-former. In Proceedings of the 2024 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies (Volume 1: Long Papers), NAACL 2024,Mexico City, Mexico, June 16-21, 2024, pages 53645376. Association for Computational Linguistics. Sheng Zhang, Xutai Ma, Kevin Duh, and BenjaminVan Durme. 2019. AMR parsing as sequence-to-graph transduction. In Proceedings of the 57th An-nual Meeting of the Association for ComputationalLinguistics, pages 8094, Florence, Italy. Associationfor Computational Linguistics. Jiawei Zhou, Tahira Naseem, Ramn Fernandez As-tudillo, and Radu Florian. 2021. AMR parsing withaction-pointer transformer. In Proceedings of the2021 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 55855598, On-line. Association for Computational Linguistics. Wenxuan Zhou, Sheng Zhang, Yu Gu, Muhao Chen,and Hoifung Poon. 2024. Universalner: Targeted dis-tillation from large language models for open namedentity recognition. In The Twelfth International Con-ference on Learning Representations, ICLR 2024,Vienna, Austria, May 7-11, 2024. OpenReview.net. Tong Zhu, Junfei Ren, Zijian Yu, Mengsong Wu, Guo-liang Zhang, Xiaoye Qu, Wenliang Chen, ZhefengWang, Baoxing Huai, and Min Zhang. 2023. Mirror:A universal framework for various information ex-traction tasks. In Proceedings of the 2023 Conference",
  "BImplementation Details": "We conduct experiments on the same NVIDIATesla A100 GPU. The hyper-parameter configu-rations for pre-training and fine-tuning are detailedin and 11, respectively. Figures 3, 4,and 5 present the loss trends over 30 epochs dur-ing pre-training across different hyper-parametersettings. examines the impact of decay factorsset at 0.1, 0.2, and 0.3, showing that the decayfactor of 0.2 leads to optimal loss reduction overtime. illustrates the influence of vary-ing the number of graph encoder layers (2, 3, and4 layers). Here, the configuration with 3 layersdemonstrates the most effective learning. explores the effects of different margin values (0.1,0.2, and 0.3). The results indicate that the mar-gin of 0.1 achieves the most consistent reductionin loss. These analyses confirm that the optimal",
  "CK-core": "A k-core cohesive subgraph is a maximal subgraphin which every node is connected to at least k othernodes. The core idea of the k-core algorithm isto identify core nodes in the graph by iterativelyremoving nodes with degrees less than k and theirassociated edges. The steps are as follows:Step 1-Initialization: Calculate the degree ofeach node in the graph and store the degrees in adictionary.Step 2-Iterative pruning: Remove all nodeswith degrees less than k and their incident edgesfrom the graph, resulting in a new subgraph. Then,",
  "DSupplementary Zero-shot Results": "To facilitate a fair comparison,we replacethe DeBERTa-large-v3 model in Mirror withRoBERTa-large, re-pretrain and fine-tune it, andthen compare the performance under the zero-shotsetting. The comparison results between SKIE andMirror-RoBERTa-Large are shown in . Itcan be observed that under the same base modelRoBERTa-large, SKIE is still superior to Mirror, demonstrating the effectiveness of our approach.Whats more, we have also included the resultsof UniNER (Zhou et al., 2024), GoLLIE (Sainzet al., 2024), and GLiNER (Zaratiana et al., 2024)in for more comprehensive comparisons.As our pre-training task employs self-supervisedcontrastive learning, it can support a broader rangeof unsupervised corpora, but this also inevitablycreates a discrepancy with downstream IE tasks.Consequently, the results of SKIE are inferior tothese three baselines on 5 NER datasets.",
  "We conduct a comprehensive error identificationanalysis in NER experiment on ACE05 from threeaspects": "Error identification using GSN in SKIE. Thesentence is \"Thousands more may have been ig-nored over the last decade.That is the Bushrecord in policing surgeons, why should we trusthim now?\" After removing the T-GSN module,SKIE misclassifies \"surgeons\" as a location, failingto grasp the deep semantic meaning of the sen-tence. The T-GSN module enhances the modelscontextual understanding and reasoning abilitythrough additional relational and cohesive informa-tion. Without it, the model might not fully exploitthe potential relationships between entities.Error identification without cohesive sub-graphs in SKIE. The sentence is \"The ax fell heav-ily on government and non-profit workers as manystate and local governments face severe budgetcrunches.\" Without the cohesive subgraph mod-ule, SKIE struggles to accurately identify longspans. The cohesive subgraphs, with their multi-level nodes and connections, provide rich semanticand logical structures. By leveraging these, themodel can better comprehend complex relation-ships and concepts in text. Its absence may hinderthe models ability to recognize long or complexentities.Error identification in other methods but notin SKIE. The sentence is \"An automotive tire shop,and someone noticed him, recognized him.\" WhileMirror incorrectly identifies the facility as \"shop\",SKIEs results are flawless. We attribute this toour methods ability to capture more structural andsemantic knowledge, granting it an advantage inlong sentences.",
  "FLayers Change Analysis": "We conduct layers change of the text encoder analy-sis using a sentence from ACE05, \"Sergeant ChuckHagel was seriously wounded twice in Vietnam,\"and input it into our pre-trained model. shows the following observations:Lower layers (1-5): The attention distributionis relatively even, with minimal gaps in attentionscores between words, indicating that the modelprimarily focuses on basic vocabulary and gram-matical structures within the sentence.Middle layers (6-10): The attention scores be-tween the last word and the preceding text increase,suggesting that the model is beginning to analyzethe contextual information of the sentence.Middle layers (11-15): The attention scores forentities begin to rise significantly, such as \"Chuck Hagel\" and \"Vietnam\", indicating that the model isgradually comprehending the roles of these entitieswithin the statement.Upper layers (16-24): The attention scores forrelationships between entities notably increase. Forinstance, \"was wounded\" receives a marked boostin attention, demonstrating that the model is en-hancing its understanding of the relationships be-tween different entities and the overall semanticswithin the sentence."
}