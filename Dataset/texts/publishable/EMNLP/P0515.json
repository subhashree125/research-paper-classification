{
  "Abstract": "This paper investigates the potential benefits oflanguage-specific fact-checking models, focus-ing on the case of Chinese using CHEF dataset.To better reflect real-world fact-checking, wefirst develop a novel Chinese document-levelevidence retriever, achieving state-of-the-artperformance. We then demonstrate the limi-tations of translation-based methods and multi-lingual language models, highlighting the needfor language-specific systems. To better ana-lyze token-level biases in different systems, weconstruct an adversarial dataset based on theCHEF dataset, where each instance has a largeword overlap with the original one but holdsthe opposite veracity label. Experimental re-sults on the CHEF dataset and our adversarialdataset show that our proposed method out-performs translation-based methods and mul-tilingual language models and is more robusttoward biases, emphasizing the importance oflanguage-specific fact-checking systems.1",
  "Introduction": "Since manual fact-checking requires significanttime and resources, there has been growing interestin automated fact-checking in recent years (Graves,2018; Nakov et al., 2021).While misinforma-tion exists in various languages, studies have pre-dominantly focused on claims and evidence in En-glish (Guo et al., 2022; Akhtar et al., 2023). Cur-rent research in fact-checking in other languagesoften lacks grounding in real-world claims (Changet al., 2023) or is constrained to a single domain,like COVID-19 (Shahi and Nandini, 2020).In this paper, we raise the question: Should wedevelop language-specific fact-checking models, orcan we effectively utilize existing English modelsby translating claims and evidence into English?We present a case study focused on Mandarin Chi-nese to investigate it for two reasons. Firstly, Chi-",
  ": Upper section: the challenge in accurate translation(Red: Incorrect, Blue: Correct); Lower section: the bias ofmultilingual LLMs towards certain claims": "nese is widely spoken by over a billion people andpossesses unique linguistic characteristics differentfrom English (Yang et al., 2017; Fei, 2023). Sec-ondly, Chinese is the only language other than En-glish that has a real-world evidence-based datasetannotated manually, i.e., CHEF (Hu et al., 2022)).This is in contrast to other popular datasets suchas X-FACT (Gupta and Srikumar, 2021), whichdepend on Google search results without evidenceannotation, and DanFEVER (Nrregaard and Der-czynski, 2021), a Danish translation of FEVER,which do not address real-life fact-checking needs.To study the question proposed, we first developa novel Chinese document-level evidence retriever(DLR), which outperforms state-of-the-art mod-els by 10% in terms of accuracy and Macro F1 inCHEF. Paired with either DLR-retrieved or anno-tated gold evidence, we then demonstrate the limita-tions of translation-based methods (i.e., first trans-lating Chinese claims and evidence into Englishand then applying English fact-checking models ontranslated data) or multilingual language modelsin fact-checking. We further identify the culturalbiases in CHEF and create an adversarial dataset.Experiments on our newly proposed dataset show asignificant decrease in both accuracy and F1 score.Overall, our study highlights the necessity of devis-ing language-specific fact-checking models.",
  "Language-specific models vs translationand multilingual models": "To construct a Chinese fact-checking system, twostraightforward approaches are direct translationfrom Chinese to English and the application ofmultilingual LLMs. However, as demonstrated in, translation from Chinese to English may re-sult in inaccuracies, particularly with idiomatic ex-pressions or language-specific phrases (Shao et al.,2018). Furthermore, LLMs like ChatGPT, predom-inantly trained on English data (Lai et al., 2023;Hu et al., 2023), tend to reflect Western norms, val-ues and biases (Naous et al., 2023; Masoud et al.,2023; Wang et al., 2023), making them less effec-tive for fact-checking in other languages. More-over, LLMs also suffer from hallucination problem(Zhang et al., 2023). shows examples ofscientifically refuted claims that GPTs incorrectlyaccept, alongside the evidence retrieved. To exam-ine the abovementioned limitations in a systematicway, we conduct experiments on a large scale Chi-nese evidence-based dataset, CHEF. RetrieversEvidence retrieval is crucial in thefact-checking pipeline (Augenstein et al., 2019a;Jiang et al., 2020). To align our work with real-world fact-checking, we develop a state-of-the-art Chinese evidence retrieval component. Ournovel Document-level Retriever (DLR) enhancesevidence retrieval by considering the context ofevidence sentences, unlike prior approaches thatisolate evidence selection to pairwise sentence clas-sification (Hu et al., 2022). Inspired by Stammbach(2021), we train a retriever to assign a score to eachChinese token within an evidence document andthen aggregate these token scores at the sentencelevel. In particular, we fine-tune a BigBird (Zaheeret al., 2020) to assign a value of 1 to tokens that belong to annotated evidence for a claim, whileassigning a value of 0 to all other tokens. Duringinference, we compute the average scores for all to-kens within each sentence. If the resulting averagescore exceeds 0.5, we classify the sentence as evi-dence. We compare our proposed document-levelretriever with the annotated gold evidence, and Se-mantic Ranker (Nie et al., 2019; Liu et al., 2020)used in CHEF. To eliminate the effects of evidenceretrieval, we also utilize the gold evidence for theideal case. More training details of DLR can befound in Appendix A. VerifiersFor the verifiers in , we mainlyinclude the baselines in Hu et al. (2022) with someof our variations: (1) For the translation mod-els, we first translate the evidence and claims viaGoogle Translator (GT) and GPT-4, then apply theDeBERTa-large verifier. (2) The cross-lingual ap-proach GPT-4 + mDeBERTa involves training mul-tilingual DeBERTa-large on data translated fromChinese to English, followed by evaluation of theoriginal CHEF. For the GPT models,2 we use 5shots for in-context learning. Other models are alltrained and tested on original CHEF. We providedetailed experimental settings in the Appendix B. Results on CHEFAs shown in , our sys-tem that combines DLR and Chinese DeBERTa,yields the best results with an accuracy of 74.50%and a Macro F1 score of 74.46%.There isan improvement of over 10% compared to thebest translation-based result (GPT-4+DeBERTa).For the multilingual LMs, the cross-lingual ap-proach (GPT-4+mDeBERTa) show poor perfor-mance. This is probably because of the disparity be-tween the languages of the training and testing sets.",
  ": Top 10 LMI-ranked phrases in the train set of CHEF for SUPPORTED (left) and REFUTED (right)": "On the contrary, mDeBERTa, trained on ChineseCHEF, achieves competitive performance on parwith Chinese RoBERTa and outperforming severalbaseline models. Overall, the results suggest thattraining a model specifically on Chinese, whetherit be a multilingual or monolingual model, is morebeneficial than relying on English-centric ones. Evidence RetrievalThe DLR, paired with dif-ferent verifiers, improves accuracy and Macro F1by about 5% over the Semantic Ranker. Regardingthe recall of human-annotated gold evidence, DLRleads to 10% higher Recall@5 (). We alsofind that our new retriever can retrieve evidencepieces which, when considered individually can-not verify the claim but, when combined they can. gives a detailed example in the Appendix C.",
  "Biases in CHEF": "To explore the reasons behind the deficiency oftranslation services and multilingual LMs, we in-vestigate the biases present in the CHEF dataset inthis section. Prior research has demonstrated thatfact-checking datasets, such as FEVER (Thorneet al., 2018) and MultiFC (Augenstein et al.,2019b), result in training models that rely on heuris-tics such as surface-level patterns within claims, po-tentially impeding their ability to generalize effec-tively (Schuster et al., 2019; Thorne et al., 2019). Inthis section, we show that while biases are presentas in the English language datasets and models,they are specific to the Chinese culture. Domain BiasFirst, in CHEF, claims are catego-rized into domains such as politics, society, health,and culture and we find a significant skew in the dis-tribution: 64% of social and 66% of health claimsare REFUTED, while 55% in politics and 72% inculture are SUPPORTED. Notably, there is an imbal-ance in the proportion of social and health claims,",
  "which collectively constitute 68% of the total. Fig-ure 2 in the Appendix details the label distributionacross domains": "Cultural BiasWe then examine the correlationbetween phrases within the claims and the corre-sponding labels. The word distribution within thetraining set is analyzed for this purpose. Initially,all claims in the training set are tokenized by Chi-nese text segmentation tool, jieba.3 The averagelength of the words is 2.39 characters. Then, twometrics are employed to assess the correlation be-tween phrases and labels. Following Schuster et al.(2019), first we use p(l|w) to calculate the proba-bility of a label l given the presence of a specificphrase w in the claim. As this metric tends to ex-hibit bias towards low-frequency words, the secondmetric utilizes Local Mutual Information (LMI; Ev-ert 2005) to identify high-frequency n-grams thatdisplay a strong correlation with a particular label.The p(l|w) and LMI between phrase w and label lis defined as follows:",
  ": Performance comparison of models on the adversarial dataset. The original 250 pairs refers to pairsdirectly extracted from CHEF, while generated 750 pairs denotes pairs generated using GPT-4": "a strong correlation between politician names (e.g.,Barack Obama and Donald Trump) and refutedclaims, however, our research identifies a distinctcultural bias within CHEF. In CHEF, claims aboutbiomedical and health issues frequently exhibit astrong association with negative labels. Terms suchas (virus), (vaccine), (carcino-genic), and (coronavirus) are more com-monly encountered in refuted claims. Conversely,financial terms like (finance), (RMB),and (Peoples Bank of China), as well as polit-ical terms such as (China), (Ministryof Foreign Affairs), tend to carry positive labels.One possible reason behind this is that fact-checking in China tends to avoid criticism of hard-core public issues, such as politics, economics, andother current affairs (Liu and Zhou, 2022). Onthe contrary, it focuses more on providing refer-ences for everyday decision-making, such as inhealth.Another political reason could be thatthe Cyberspace Administration of China keepsa close watch on online news services (Liu andZhou, 2022). Non-state enterprises are not per-mitted to criticize politics, economics, and othercurrent affairs. Private companies are only autho-rized to distribute and curate news produced bystate-owned media. Furthermore, in CHEF, cer-tain regions such as (Taiwan), (Japan),and (United States) are commonly associatedwith the REFUTED label. This may also reflect thecontentious nature of international relations withinthe realm of Chinese fact-checking.",
  "Our analysis revealed the presence of labels andcultural biases specific to the Chinese context ( 3).These biases can significantly impact the perfor-mance and fairness of fact-checking models when": "applied to Chinese-language claims.We therefore introduce an adversarial dataset de-rived from the CHEF dataset for a better evaluationof the models. Inspired by Schuster et al. (2019), tocreate it we pair each claim-evidence instance witha synthetic counterpart where claim and evidencehave high word overlap with the original ones butthe opposite veracity label (). Under thissetting, determining veracity from the claim alonewould be equivalent to a random guess. Instead ofinvolving human annotators, we opt for the utiliza-tion of GPT-4 to generate the dataset. To control thequality, we invited two Chinese native speakers toannotate randomly sampled 25% of claim-evidencepairs with SUPPORTED, REFUTED or NOT ENOUGH INFO. The results demonstrated strong agreementbetween humans and GPT-4. They agreed withthe dataset labels in 89% of cases, with a Cohen of 0.80 (Cohen, 1960). Our approach overcomeslabor-intensive manual annotation and rigid rule-based generation, advocating for automated samplegeneration using LLMs. This new test set nulli-fies the benefit of relying exclusively on cues fromclaims. Details of the dataset construction and theprompt we use can be found in Appendix D.",
  "Experiments on Adversarial CHEF": "Results on Adversarial CHEF comparesmodel performance on adversarial versus originaldata from CHEF.4 Since our adversarial datasetwas constructed using GPT-4, including it as a veri-fier will lead to a risk for data leakage, resulting inbiased comparisons. Therefore, we have excludedGPT-4 from the evaluation results and have pro-vided its performance metrics for reference only.",
  "Note that here the original 250 pairs\" use gold evidence,and a binary classification approach, contributing to higherperformance metrics than in": "All models perform worse on adversarial exam-ples compared to the original CHEF. Specifically,we highlight the following findings: (1) ChineseDeBERTa drops from 86.69% accuracy on originalpairs to 57.84% and 65.01% on adversarial sub-sets. Baselines similarly see over 37% decreasesin both accuracy and F1 scores. This underscoresthe models reliance on surface features and re-veals the aforementioned biases. (2) Compare thetranslation-based GPT-4+DeBERTa (F1 53.04%),the multilingual mDeBERTa (F1 60.56%), and theChinese DeBERTa (F1 63.74%), we observe a per-formance increase with more language-specificmodels, highlighting the benefits of incorporatingChinese data during pre-training. (3) GPT-3.5-Turbo, which has not been fine-tuned on the CHEFdataset, demonstrates better robustness but still un-derperforms compared to the language-specific Chi-nese DeBERTa. Overall, we recommend future re-search to use both the original and our adversarialCHEF dataset for a comprehensive evaluation. Exposing adversarial examples to the modelscan improve robustness. DeBERTas performancedeclines less than that of the baselines includingBERT, Attention, and Graph-based models whenfaced with adversarial examples, about 30% com-pared to over 37%, suggesting a higher sensitivityto evidence changes. To investigate the reasonsbehind the decrease in the models performanceand improve models robustness, we employ theinoculation fine-tuning method (Liu et al., 2019a).Results show the performance decline observed inthe baselines primarily stems from inherent weak-nesses within the model family. In contrast, for theDeBERTa model, gradually exposing it to more ad-versarial samples leads to a gradual reduction in theperformance gap. Inoculation results by fine-tuningthe model with different sizes of adversarial exam-ples are provided in in the Appendix F.",
  "Conclusion": "Our study reveals the shortcomings of English-centric fact-checking systems when applied to Chi-nese claims, highlighting the failure of translation-based methods due to linguistic and cultural nu-ances. We introduce a novel system that achievesbest-reported results on CHEF and provides an ad-versarial dataset for continued research, underscor-ing the need for specialized fact-checking models.",
  "Limitations": "This study has several notable limitations. Firstly,the performance of our document-level retriever,despite showing improvements over the semanticranker, still exhibits a relatively low recall rate.This underscores the ongoing challenges in evi-dence retrieval, which necessitate further researchand refinement to enhance the accuracy and relia-bility of the system.Secondly, the scope of our analysis is limited toEnglish- and Chinese-language datasets. This isdue to the scarcity of real-world, evidence-basedfact-checking datasets annotated by humans inother languages. The inclusion of the Chinesedataset in this study is not arbitrary, rather, itsbased on two main factors. First, there is a lack ofsuitable datasets in other languages. Second, sev-eral authors of this study are native Chinese speak-ers, which gives us a distinct advantage in terms ofunderstanding and appreciating the linguistic fea-tures and nuances of the Chinese language. Thisexpertise has allowed us to conduct a more thor-ough and in-depth case study.Moving forward, we are actively seekingdatasets in other languages that align with our re-search requirements. One potential approach is toconstruct new datasets based on the recently estab-lished fact-checking website, Elections24Check5.This platform gathers and categorizes fact-checkedinformation for European countries and citizensahead of the 2024 European Elections. It offers arich source of political fact-checks, disinformationdebunks, prebunking articles, and narrative reportsin various European languages, which could beinstrumental in expanding the breadth of our re-search.",
  "Ethics Statement": "The CHEF dataset employed in our research is ac-cessible to the scientific community, and its usein our experiments presents no conflict of inter-est. Although the adversarial dataset used in thisstudy was developed with a GPT-4 model, to en-sure its integrity and safety, we conducted an exten-sive manual review to eliminate sensitive or poten-tially harmful information. This review receivedapproval from our institutions ethics committee.Furthermore, the hourly salary for annotators sur-passed the national minimum wage, and all annota-tors consented to the use of the data.",
  "Andreas Vlachos is supported by the ERC grantAVeriTeC (GA 865958). Caiqi Zhang is funded byAmazon Studentship. We thank Yulong Chen forhis great support on this work": "Mubashara Akhtar, Michael Schlichtkrull, Zhijiang Guo,Oana Cocarascu, Elena Simperl, and Andreas Vla-chos. 2023. Multimodal automated fact-checking: Asurvey. In Findings of the Association for Computa-tional Linguistics: EMNLP 2023, pages 54305448,Singapore. Association for Computational Linguis-tics. Isabelle Augenstein, Christina Lioma, DongshengWang, Lucas Chaves Lima, Casper Hansen, Chris-tian Hansen, and Jakob Grue Simonsen. 2019a. Mul-tiFC: A real-world multi-domain dataset for evidence-based fact checking of claims. In Proceedings ofthe 2019 Conference on Empirical Methods in Natu-ral Language Processing and the 9th InternationalJoint Conference on Natural Language Processing(EMNLP-IJCNLP), pages 46854697, Hong Kong,China. Association for Computational Linguistics. Isabelle Augenstein, Christina Lioma, DongshengWang, Lucas Chaves Lima, Casper Hansen, Chris-tian Hansen, and Jakob Grue Simonsen. 2019b. Mul-tiFC: A real-world multi-domain dataset for evidence-based fact checking of claims. In Proceedings ofthe 2019 Conference on Empirical Methods in Natu-ral Language Processing and the 9th InternationalJoint Conference on Natural Language Processing(EMNLP-IJCNLP), pages 46854697, Hong Kong,China. Association for Computational Linguistics. Yi-Chen Chang, Canasai Kruengkrai, and Junichi Yam-agishi. 2023. XFEVER: Exploring fact verificationacross languages. In Proceedings of the 35th Confer-ence on Computational Linguistics and Speech Pro-cessing (ROCLING 2023), pages 111, Taipei City,Taiwan. The Association for Computational Linguis-tics and Chinese Language Processing (ACLCLP).",
  "Jacob Cohen. 1960. A coefficient of agreement fornominal scales. Educational and psychological mea-surement, 20(1):3746": "Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, ShijinWang, and Guoping Hu. 2020. Revisiting pre-trainedmodels for Chinese natural language processing. InFindings of the Association for Computational Lin-guistics: EMNLP 2020, pages 657668, Online. As-sociation for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for",
  "Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023": "Debertav3: Improving deberta using electra-stylepre-training with gradient-disentangled embeddingsharing. In The Eleventh International Conferenceon Learning Representations, ICLR 2023, Kigali,Rwanda, May 1-5, 2023. OpenReview.net. Pengcheng He, Xiaodong Liu, Jianfeng Gao, andWeizhu Chen. 2021. Deberta: decoding-enhancedbert with disentangled attention. In 9th InternationalConference on Learning Representations, ICLR 2021,Virtual Event, Austria, May 3-7, 2021. OpenRe-view.net.",
  "Nelson F. Liu, Roy Schwartz, and Noah A. Smith. 2019a": "Inoculation by fine-tuning: A method for analyz-ing challenge datasets. In Proceedings of the 2019Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, Volume 1 (Long and ShortPapers), pages 21712179, Minneapolis, Minnesota.Association for Computational Linguistics. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019b.Roberta: A robustly optimized bert pretraining ap-proach. ArXiv preprint, abs/1907.11692.",
  "Parth Patwa, Shivam Sharma, Srinivas PYKL, VineethGuptha, Gitanjali Kumari, Md Shad Akhtar, AsifEkbal, Amitava Das, and Tanmoy Chakraborty. 2020.Fighting an infodemic: Covid-19 fake news dataset": "Michael Schlichtkrull, Zhijiang Guo, and Andreas Vla-chos. 2023. Averitec: A dataset for real-world claimverification with evidence from the web. In Advancesin Neural Information Processing Systems 36: An-nual Conference on Neural Information ProcessingSystems 2023, NeurIPS 2023, New Orleans, LA, USA,December 10 - 16, 2023. Tal Schuster, Darsh Shah, Yun Jie Serene Yeo, DanielRoberto Filizzola Ortiz, Enrico Santus, and ReginaBarzilay. 2019. Towards debiasing fact verificationmodels. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processingand the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), pages34193425, Hong Kong, China. Association for Com-putational Linguistics.",
  "FakeCovid a multilingual cross-domain fact checknews dataset for covid-19. In Workshop Proceedingsof the 14th International AAAI Conference on Weband Social Media": "Yutong Shao, Rico Sennrich, Bonnie Webber, and Fed-erico Fancellu. 2018. Evaluating machine transla-tion performance on Chinese idioms with a blacklistmethod. In Proceedings of the Eleventh InternationalConference on Language Resources and Evaluation(LREC 2018), Miyazaki, Japan. European LanguageResources Association (ELRA). Dominik Stammbach. 2021. Evidence selection as atoken-level prediction task. In Proceedings of theFourth Workshop on Fact Extraction and VERifica-tion (FEVER), pages 1420, Dominican Republic.Association for Computational Linguistics. James Thorne, Andreas Vlachos, Oana Cocarascu,Christos Christodoulopoulos, and Arpit Mittal. 2018.The fact extraction and VERification (FEVER)shared task. In Proceedings of the First Workshop onFact Extraction and VERification (FEVER), pages 19, Brussels, Belgium. Association for ComputationalLinguistics. James Thorne, Andreas Vlachos, Oana Cocarascu,Christos Christodoulopoulos, and Arpit Mittal. 2019.The FEVER2.0 shared task. In Proceedings of theSecond Workshop on Fact Extraction and VERifica-tion (FEVER), pages 16, Hong Kong, China. Asso-ciation for Computational Linguistics. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N. Gomez, LukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. In Advances in Neural Information Pro-cessing Systems 30: Annual Conference on NeuralInformation Processing Systems 2017, December 4-9,2017, Long Beach, CA, USA, pages 59986008. Junjie Wang, Yuxiang Zhang, Lin Zhang, Ping Yang,Xinyu Gao, Ziwei Wu, Xiaoqun Dong, Junqing He,Jianheng Zhuo, Qi Yang, Yongfeng Huang, Xiayu Li,Yanghan Wu, Junyu Lu, Xinyu Zhu, Weifeng Chen,Ting Han, Kunhao Pan, Rui Wang, Hao Wang, Xiao-jun Wu, Zhongshen Zeng, Chongpei Chen, Ruyi Gan,and Jiaxing Zhang. 2022. Fengshenbang 1.0: Be-ing the foundation of chinese cognitive intelligence.ArXiv preprint, abs/2209.02970.",
  "Wenxuan Wang, Wenxiang Jiao, Jingyuan Huang, RuyiDai, Jen tse Huang, Zhaopeng Tu, and Michael R.Lyu. 2023. Not all countries celebrate thanksgiving:On the cultural dominance in large language models": "Man Yang, North Cooc, and Li Sheng. 2017. An inves-tigation of cross-linguistic transfer between chineseand english: a meta-analysis. Asian-Pacific Journalof Second and Foreign Language Education, 2:121. Manzil Zaheer, Guru Guruganesh, Kumar AvinavaDubey, Joshua Ainslie, Chris Alberti, Santiago On-tan, Philip Pham, Anirudh Ravula, Qifan Wang,Li Yang, and Amr Ahmed. 2020. Big bird: Trans-formers for longer sequences. In Advances in NeuralInformation Processing Systems 33: Annual Confer-ence on Neural Information Processing Systems 2020,NeurIPS 2020, December 6-12, 2020, virtual. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,Yulong Chen, Longyue Wang, Anh Tuan Luu, WeiBi, Freda Shi, and Shuming Shi. 2023. Sirens songin the ai ocean: A survey on hallucination in largelanguage models.",
  "ADocument-level Retriever (DLR)": "In our approach, we recognize the significance ofcontext in determining whether a given sentencecan be considered as evidence. To leverage thiscontextual information, we assign a score to eachtoken within a document and then aggregate thesetoken scores at the sentence level. Subsequently,we fine-tune a transformer model to assign a valueof 1 to tokens that belong to annotated evidence fora claim, while assigning a value of 0 to all othertokens. During testing, we compute the averagescores for all tokens within a sentence. If the re-sulting average score exceeds 0.5, we classify thesentence as evidence. illustrate this proce-dure.Vanilla Transformer-based architectures, as in-troduced by Vaswani et al. (2017), utilize a com-plete attention matrix that captures all pairwiseinteractions among tokens within a given sentence.However, the quadratic time and memory complex-ity associated with the complete attention matriximposes limitations on the processing capabilitiesof these systems, often confining them to mod-erately long inputs with a typical cutoff point of512 subwords. Notably, in fact-checking area, aretrieved potential evidence passage can easily sur-pass this length. For example in CHEF, the averagelength of an evidence document is 866, which ex-ceeds the maximum length supported by BERT.To overcome this limitation, we turn to BigBird(Zaheer et al., 2020), which incorporates sparseattention patterns. This adaptation enables BigBirdto handle sequence lengths of up to 4096 tokens.Zaheer et al. (2020) also demonstrate that BigBirdsattention pattern exhibits comparable performanceto BERT for short sequence lengths, while out-performing BERT in tasks that involve longer se-quences. illustrates our framework, high-lighting the utilization of BigBird in our approach.The models are trained on A100-SXM-80GBGPUs. We fine-tuned the Chinese BigBird6. Itemploys a custom tokenizer, merging jieba tok-enizer with BertTokenizer, for processing Chinesetext. Optimization leverages the AdamW optimizer(learning rate of 2e-5, epsilon of 1e-8), with noweight decay for specific parameters and a linearlearning rate scheduler initiating at a warm-up stepcount of 0. Training and evaluation phases bothutilize a batch size of 16 across 5 epochs.",
  "BExperiment Setup": "In the results presented in , the transla-tion models initially employ Google/GPT-4 to con-vert all claims and evidence within the CHEFdataset to English.Subsequently, an EnglishRoBERTa-large is fine-tuned to assess the ve-racity of these claims using the CHEF trainingset. For multilingual LLMs, we apply a five-shotin-context learning approach with both GPT-3.5-Turbo and GPT-4-Turbo. Regarding the baselinemodelsBERT-base, attention-based, and graph-based modelswe adhere to the default hyperpa-rameters as delineated in the CHEF study (Hu et al.,2022). We run our experiments on A100-SXM-80GB GPUs. For each pipeline system, we conductthree independent experiments and report the meanvalues.Verifiers We utilize DeBERTa (He et al., 2021)to verify a claim given the selected evidence, us-ing the Chinese version pretrained on the WuDaoCorpora (Wang et al., 2022). We also compare ourresults with the baselines in Hu et al. (2022), in-cluding BERT-base (Devlin et al., 2019), Attention-based (Gupta and Srikumar, 2021), and Graph-based (Liu et al., 2020) methods. We also incorpo-rate the RoBERTa-based model (Liu et al., 2019b),GPT-3.5-Turbo (OpenAI, 2022) and GPT-4-Turbo(OpenAI, 2023) for a more comprehensive com-parison. For the GPT models, we use 5 shots forin-context learning..",
  "CComparison of Different Retrievers": "compares the performance of Seman-tic Ranker and Document-level Retriever.TheDocument-level Retriever leads to better Recall@5and Marco F1. Recall@5 measures the propor-tion of gold evidence that are successfully retrievedamong the top 5 retrieved evidence sentences.Although outperforming the Semantic Ranker,the Document-level Retriever only attains a 33.58%Recall@5, indicating the difficulty of evidence re-trieval, yet remarkably leads to a 74.46% MacroF1 score in claim verification. This may be dueto the CHEFs gold evidence annotation not beingexhaustive, a known issue in datasets with evidenceretrieved from the Web (Schlichtkrull et al., 2023),and thus the retriever can return correct evidencethat was not annotated. Additionally, the modelmight leverage surface-level patterns in claims toinform verification, which allows for high accuracyeven when the available evidence is insufficient.",
  "Drinking milk can raise cholesterol level": "Many people do not consume milk because they think that drinking it will raise their cholesterol. Studies have found that milk does not contain high levels of cholesterol and that certain ingredients in milk have cholesterol-suppressing properties. Medical research has also confirmed that drinking milk can also help reduce coronary heart disease and treat high blood pressure. However, milk should not be consumed on an empty stomach as it is not conducive to the absorption and utilization of nutrients.",
  ":Comparison of Semantic Ranker andDocument-level Retriever for evidence sentence re-trieval with DeBERTa-large": "is an example where leveragingdocument-level information can help with the ev-idence retrieval. To verify the claim: (The principle that red wine contains antho-cyanins allows for a straightforward authenticitytest.)\", each retriever collects five pieces of evi-dence. Without additional context, it is not possibleto retrieve the sentences highlighted in red throughsemantic matching alone. None of these sentences,when considered individually, can be used to ver-ify the claim. However, when taken together, theyprovide a comprehensive explanation of why an-thocyanins can be utilized to test red wine. Havingaccess to the entire document makes it much easierto accurately predict similar examples.",
  "To further detect and eliminate bias in CHEF, wepropose to generate a new Chinese adversarialdataset for it. We adopt the methodology presented": "by (Schuster et al., 2019) as our primary frameworkfor constructing a symmetrical dataset for CHEF, asillustrated in . Our approach involves gen-erating synthetic claim-evidence pairs that main-tain the same relationship (e.g., SUPPORTS orREFUTES) while conveying contrasting factual in-formation. Moreover, we ensure that each sentencein the new pair exhibits the inverse relationshipwith its corresponding sentence in the original pair.Some new rules have been devised to bet-ter suit the Chinese context. More specifically,when rewriting the given claim 20205\" (Chen Dawen, announcedthat the 2020 edition of the fifth series of 5-yuanbanknotes will be issued, with improved anti-counterfeiting features, in Beijing.), in our frame-work, the following rewriting strategies are al-lowed: Important nouns that appear in both the claimand the evidence can be modified. These in-clude key information such as time, place, per-son, and number. Changing these essentialterms can alter the original meaning of thesentence. For example, substituting the nameChen Dawen\" with Li Xiaoming,\" revisingthe year 2020\" to 2023,\" replacing the lo-cation Beijing\" with Shanghai,\" and trans-forming the denomination 5 yuan\" to 10yuan.\"",
  ": Evidence sentences retrieved by Semantic Ranker and Document-leverl Retriever for the claim: (The principle that red wine contains anthocyanins allows for astraightforward authenticity test.).\"": "Verbs or phrases indicating degrees in boththe claim and the evidence can be replacedwith their opposites. For instance, substitut-ing rise\" with fall,\" changing increase\" todecrease,\" converting helpful\" to unhelp-ful,\" replacing substantiated\" with unsub-stantiated,\" and transforming no evidence\"to evidence not found.\" Note that these methods do not constitute an ex-haustive set of legal rewrite methods. They serveas heuristics for the model, which may also employsimilar modifications automatically. Similarly, theevidence undergoes a comparable rewriting pro-cess. For additional examples of these methods,please refer to . To rewrite the sentences, weemploy the state-of-the-art GPT-4 (OpenAI, 2023)model ,which has demonstrated human-level per-formance in various NLP tasks. By leveraging theGPT-4 model, we eliminate the laborious task ofhuman annotation and enhance the diversity of gen-eration through handcrafted rules.",
  "EPrompt Engineering": "Given the importance of prompt engineering for thequality of the generated data, as well as the scarcityof relevant literature, it is imperative to carefullycraft our prompt. To address this challenge, wesought guidance from the empirical findings of the open source community 7, which provided valuableinsights into prompt design practices. Furthermore,we consult the recently published prompt designguideline by (Fulford and Ng, 2023) to ensure ourapproach aligns with the newest recommendations.We conducted extensive experiments to iterativelyrefine our prompt, culminating in the developmentof an innovative prompt that not only enhancesthe quality of generated results but also exhibitsversatility, enabling its seamless adaptation to awide range of tasks. According to Fulford and Ng (2023), the effec-tiveness of a prompt relies on two key principles.Principle 1 emphasizes the significance of provid-ing clear and specific instructions to the model. Toachieve this, the prompt should employ delimiters(such as backticks) to clearly demarcate distinctparts of the input. Furthermore, the provision ofexamples helps the model formulate a few-shot\"prompt, allowing it to generate responses basedon limited examples. Principle 2 focuses on opti-mizing the models processing by breaking downthe full task into several subtasks. This approachguides the model to think step by step, enhancingits performance. The structure of our prompt isoutlined in .",
  "E.1Quality Control": "Following the data generation process, we gener-ated 250 new claim and evidence pairs. By per-muting them under the symmetric setting Schusteret al. (2019), we obtained an adversarial datasetconsisting of 1000 pairs. We then enlisted the par-ticipation of two Chinese native speakers to per-form annotations on a randomly selected subsetof 300 claim-evidence pairs removing their labels,which accounted for 30% of the total pairs withinthe symmetric adversarial dataset. These annota-tions involved assigning one of two labels, namelySUPPORTS, and REFUTES, while also flagginginstances of nongrammatical cases. The averageagreement between the annotators and the pre-existing dataset labels reached 89% of the cases,resulting in a Cohen coefficient of 0.80 (Cohen,1960). It demonstrates that the new claim-evidencepairs generated by GPT-4 mostly remain in theiroriginal relation, proving the effectiveness of ourmethod. Additionally, approximately 4% of thecases exhibited minor grammatical errors or typos.",
  "After manually examining the wrongly predictedcases for the DeBERTa-large model following theinoculation process, we have identified three pri-mary challenges that current models struggle toaddress:": "Subtle modifications can induce a dramaticchange in sentence meaning. In the adver-sarial CHEF dataset, a large number of state-ments exhibit slight differences before andafter modifications, often differing by onlyone or two Chinese characters. Given the richsemantic nature of Chinese characters, evena single-word alteration can reverse the en-tire sentences meaning. For instance, in thefirst example of and the first exampleof , minor changes involving a singlecharacter completely alter the original mean-ing. These nuanced distinctions pose difficul-ties for models to accurately capture. Further-more, even if these changes are encoded inthe models parameters, they may not receivesignificant weighting during veracity assess-ment. Adversarial CHEF includes numerical reason-ing challenges that lack a dedicated mecha-nism. While the original CHEF dataset con-tains extensive instances of numbers, thereare relatively few statements that necessitateinference from numerical information.Incontrast, the adversarial CHEF dataset intro-duces numerous modifications associated withnumbers, requiring the model to determine",
  "REFUTE": "ORIGINAL201820.36During the 2018 Spring Fes-tival season, the total box office revenue reached2.036 billion RMB, making the cinemas even morepopular with the \"celebrate the Lunar New Yearlocally\" trend. 201820.36201715.06In terms of box office performance, the2018 Spring Festival season achieved a record-breakingbox office revenue of 2.036 billion RMB, surpassing theprevious record of 1.506 billion RMB set in the 2017Spring Festival season and establishing a new record forthe Spring Festival box office.",
  ": This table outlines the purpose of each snippet in the prompt, explaining the role of each section accordingto the prompt design principles": "to a small portion of challenging dataset data to seehow the performance changes.Post-inoculation, we anticipate three possibleoutcomes:Outcome 1: A narrowing of the performancediscrepancy between the original and challenge testsets suggests that the challenge data didnt exposemodel weaknesses but rather a lack of diversity inthe original dataset.Outcome 2: No change in performance on ei-ther test set indicates that the challenge dataset haspinpointed a fundamental model flaw, as the modelfails to adjust even when familiarized with the chal-lenge data.Outcome 3: A performance drop on the originaltest set suggests the fine-tuning skewed the modelto suit the challenge data, highlighting a deviationfrom the original data characteristics. This could bedue to differences in label distribution or annotationartifacts that are dataset-specific. shows results from fine-tuning withvarious amounts of adversarial data. We observethe \"performance gap\" as the difference in modelperformance on the original versus adversarial testsets pre-inoculation. For BERT-base, attention-based, and graph-based models, we observe minor performancechangesOutcome 2signifying that fine-tuningdoes not close the performance gap significantly,pointing to a core weakness in adapting to adver-sarial data distributions.In contrast, the DeBERTa-large model shows areduced performance gap post-inoculation, cuttingit down by 53% after fine-tuning with 800 adversar-ial examples. Its strong performance on the originaldataset persists, suggesting DeBERTas architec-ture, with its nuanced attention to content, relative,and absolute positions in sentences, equips it tohandle slight alterations in claim or evidence moreadeptly.",
  "SUPPORT": "GENERATED202178.45During the 2021 Spring Fes-tival season, the total box office revenue reached7.845 billion RMB, making the cinemas even morepopular with the \"celebrate the Lunar New Yearlocally\" trend. 202178.45201959.06In terms of box office performance, the2021 Spring Festival season achieved a record-breakingbox office revenue of 7.845 billion RMB, surpassing theprevious record of 5.906 billion RMB set in the 2019Spring Festival season and establishing a new record forthe Spring Festival box office."
}