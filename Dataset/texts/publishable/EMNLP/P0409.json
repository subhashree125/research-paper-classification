{
  "Abstract": "This paper explores the open research prob-lem of understanding the social behaviors ofLLM-based agents. Using Avalon as a testbed,we employ system prompts to guide LLMagents in gameplay. While previous studieshave touched on gameplay with LLM agents,research on their social behaviors is lacking.We propose a novel framework, tailored forAvalon, features a multi-agent system facil-itating efficient communication and interac-tion. We evaluate its performance based ongame success and analyze LLM agents so-cial behaviors. Results affirm the frameworkseffectiveness in creating adaptive agents andsuggest LLM-based agents potential in nav-igating dynamic social interactions. By ex-amining collaboration and confrontation be-haviors, we offer insights into this fields re-search and applications.Our code is pub-licly available at",
  "Introduction": "Artificial intelligence (AI) agents (Xi et al., 2023;Park et al., 2023) exhibit human-like behaviors,from perceiving and analyzing the environment todecision-making and action-taking.Advances in large language models (LLMs)(Kasneci et al., 2023; Peng et al., 2023; Touvronet al., 2023; Vaswani et al., 2017) offer new avenuesfor creating AI agents in complex environments, po-tentially simulating human society. Various works(Gao et al., 2023; Qian et al., 2023; Park et al.,2023; Ghaffarzadegan et al., 2023) simulate differ-ent aspects of human society. For instance, Qianet al. (Qian et al., 2023) simulate a software devel-opment company with agents representing diversesocial identities. Park et al. (Park et al., 2023)assign varied social roles to agents within a sand-box environment. However, prior studies mostly",
  "Both authors contributed equally to this research.The corresponding author": "examine positive social behaviors like honesty andcollaboration, leaving research on negative socialbehaviors of LLM agents relatively scarce.Previous research on human society has high-lighted issues like misinformation and online con-flicts, leading to efforts to address these problems(Song and Jiang, 2022; Levy et al., 2022; Chenet al., 2022). To delve deeper into the social behav-iors of LLM agents, we intend to comprehensivelyinvestigate both positive and negative aspects oftheir conduct. To achieve this, we employ Avalonas the environment to illustrate collaboration andconfrontation among agents. Avalon, a represen-tative social deduction game, assigns players hid-den roles and divides them into opposing teams.Throughout gameplay, players partake in discus-sions, debates, and strategic maneuvers.LLM agents face a challenging task in winningthe incomplete information game of Avalon. Theyneed to share and obtain information via communi-cation and analysis, deducing other players roles,building trust among allies, and deceiving oppo-nents. Success requires technical abilities like nat-ural language understanding, incomplete informa-tion analysis, and strategy learning. Additionally,social behaviors such as teamwork, persuasion, andcamouflage are crucial for success in Avalon game-play.To investigate the LLM-based agent society, wepropose a novel framework for the agents to playAvalon. Specifically, we adopt ChatGPT as theplayers and assign various roles to agents. Weadopt system prompts to guide LLM agents to playAvalon automatically.Following humans thinking methodology, weincorporate multiple modules, including memorystorage and summarization, analysis and planning,game action and response generation, and experi-ence learning. We utilize a competitive baselineapproach (Xu et al., 2023a), to elaborate the effi-cacy of our proposed framework. We also carefully",
  "analyze the social behaviors of LLM agents, andobserve clear collaboration and confrontation be-tween agents during the gameplay.Our contributions can be summarized as:": "We explore the social behaviors exhibited byLLM-based agents in the context of Avalongameplay. We reveal the various aspects ofthese behaviors, including teamwork, leader-ship, persuasion, camouflage, and confronta-tion. We design an effective framework to playAvalon, which presents superior performancecompared with the baseline method. We alsocarefully analyse the relationship between themodule design and agents social behaviors,providing comprehensive experiment discus-sions. Our findings have the potential to contributeto a better understanding of the role of LLM-based agents in social and strategic contexts,and shed light on the implications of thesebehaviors in such environments.",
  "Social Deduction Game Agent": "The emergence of communication among agents insocial deduction games (SDG) has garnered signif-icant attention in the research community. Hirataet al. (2016) introduces an AI-based agent for theWerewolf game, aiming to advance intelligenceand communication skills in AI systems. Naka-mura et al. (2016) proposes a psychological modelconsidering multiple perspectives to simulate hu-man gameplay in The Werewolf. Wang and Kaneko(2018) addresses decision-making challenges in theWerewolf game using deep reinforcement learn-ing techniques. Furthermore, Wiseman and Lewis(2019) explores player decision-making in socialdeduction games, focusing on sources of infor-mation influencing player strategies. Examiningthe broader context of multi-agent communication,",
  "LLM-Based Gameplay": "The rapid development of LLM-based agents hasresulted in significant advancements in problem-solving across various domains.These agents,known for their quick and strategic processing,have improved the effectiveness and robustness ofsolving tasks (Lin et al., 2023; Wang et al., 2023b;Tsai et al., 2023; Zhou et al., 2023; Park et al.,2023; Qian et al., 2023; Fu et al., 2023).LLMs have recently been utilized in vari-ous gaming environments, including task-basedgames like Minecraft and multiplayer strategygames (Yuan et al., 2023; Zhu et al., 2023; Wanget al., 2023a; Akata et al., 2023; Xu et al., 2023a;Wang et al., 2023c). In multiplayer strategy gamessuch as the Prisoners Dilemma and Battle of theSexes, LLMs model strategic interactions (Akataet al., 2023). Theyre also employed in social de-duction games like Werewolf and Avalon (Xu et al.,2023a; Wang et al., 2023c; Shi et al., 2023; Xuet al., 2023b), where they exhibit strategic behav-iors. To combat misinformation, recursive contem-plation has been proposed (Wang et al., 2023c).However, previous works have only partially an-alyzed behaviors and designed agent frameworksbased on limited game characteristics. Thus, wepropose a comprehensive social deduction gameagent framework based on LLMs and conduct athorough behavior analysis. illustrates thedistinctions between our work and others.",
  "LLMs Impact on Society": "The growing influence of Large Language Mod-els (LLMs) on society has spurred significant re-search (Movva et al., 2023). Innovations includeusing LLMs for virtual social network simulationsto advance social science research (Gao et al., 2023)and enrich human social experiences in virtual spaces (Kaiya et al., 2023). However, concernsarise regarding validity, privacy, and ethics in LLM-driven social computing. Ghaffarzadegan et al. pro-pose feedback mechanisms to address these con-cerns (Ghaffarzadegan et al., 2023). Additionally,LLMs fuel advancements in social robot develop-ment (Yang and Menczer, 2023), posing challengeslike social bot detection and misinformation spread.Ongoing research aims to align LLMs with ethicalstandards, mitigate biases and errors, and ensuretheir reliable and ethical use across diverse applica-tions (Wang et al., 2023d; Liu et al., 2023).",
  "Background": "In our study, we chose Avalon, also known as TheResistance, instead of Werewolf as our environ-ment. Unlike Werewolf, where players are grad-ually eliminated, Avalon ensures that all playersremain engaged throughout the game, promotingsocial cohesion.Avalon accommodates 5 to 10 players, focusingon the 6-player variant herein. Players receive se-cret roles in either the good or evil faction. Thegood faction includes Merlin, Percival, and LoyalServants, while the evil faction comprises Morganaand Assassin. Morgana and Assassin know eachothers identities, Percival can identify Merlin andMorgana, and Merlin recognizes all evil players.The game spans 3-5 rounds. Players discuss andvote to form a quest team of 2-3 members. Ap-proval requires a majority vote; otherwise, leader-ship shifts. Each round allows up to five votingcycles before the leader selects the team. Questsuccess hinges on cards submitted by team mem-bers. Good players submit success cards, whileevil players can choose success or failure cards. Aquest fails if it receives a failure card. The gameconcludes with victory for good players if threequests succeed, or for evil players if three questsfail. Evil players can also win by correctly identi-fying Merlin at the games end.",
  "Social Behaviors in Avalon": "Teamwork. Good players must collaborate to com-plete quests for winning. They should build trustwith teammates while being wary of evil players.Leadership. Each player has the chance to lead thediscussion for forming the quest team. The leadercan guide the conversation and build trust amongplayers. Effective leadership is crucial for victory.Persuasion. Players must use their communication skills to persuade others to believe their claims,trust their judgments, and support their decisions.Camouflage. Evil players pretend to be good play-ers, using deceptive tactics and concealing infor-mation to mislead others.Confrontation. Disagreements and conflicts willarise during the game. Players must tackle theseconfrontations and work towards resolving them.Sharing. Each role has unique clues. Sharingthese clues promotes collaboration and builds trustamong players, but risks exposing ones identity.",
  "Setup": "shows the proposed framework.Allprompts used are shown in Appendix . Tostart the game, system prompts are used to assigndifferent roles to LLM agents. Each system promptfor a role pi includes several important components:Role Information RIpi (Role Name and Role In-troduction), Goal Gpi (Winning Conditions), andAbstracted Strategy Spi for gameplay. The RoleName and Role Introduction provide informationabout the assigned role to the LLM agent, whilethe Goal (Winning Conditions) offers insights intohow to achieve victory. Additionally, the InitialPlaying Strategy outlines the high-level planningfor the LLM agent to take specific actions duringgameplay.Below is a specific example of a system promptfor the role of Margana:Role: Morgana.Role Introduction: In identification phase, youcan identify teammates and the Assassin.Goal: Win the game by intentionally causing queststo fail for three rounds, alone or with teammates.Initial Strategy: You always pretend to be a loyalservant and recommend yourself as a candidate forquests, and let the quests fail.",
  "Memory Storage": "Analyzing game history is vital for agents to graspthe current situation and make decisions. Yet, inAvalon, LLM agents history responses are oftentoo lengthy, surpassing input limits and potentiallylowering performance. To tackle this, a memorystorage system is introduced to record conversa-tions among LLM agents, enabling subsequentanalysis and decision-making.Memory Storage. Memory storage is vital forrecording agents conversation history in the cur- : Our framework has six modules: summary, analysis, planning, action, response, and experiential learning.This design follows human thinking, helps LLM agents play Avalon effectively, and reveals their social behaviors. rent game round. It comprises structured memoryobjects containing key details like role name, de-tailed natural language responses, round number,and a flag indicating public or private status. Publicinformation is visible to all roles, while private in-formation pertains to each roles conversation. Weassign separate memory pools to each agent forclarity in information processing. By storing thisdata, memory storage enables agents to access andreview past conversations, improving their under-standing of the games progress.",
  "Planning": "Agents need to understand the game progress andnecessary strategies to win. Thus, a planning mod-ule is designed to create a strategic plan. The planis based on the memory and information from thecurrent round of the game, as described below: Ppit= PLANMt, Hpit , Ppit1, RIpi, Gpi, Spi,(3)where Ppitrepresents the strategic plan of agentpi at round t. Gpi and Spi are goals and initialstrategies. By creating a strategic plan, the agentscan have a flexible strategy for different situations.This foresight helps them make better decisionsabout collaborating with teammates, deceiving op-ponents, taking on the opposing factions identity,and, if needed, sacrificing teammates or oneself tosecure winning in the game.",
  "Action": "In the action module, agents decide their next ac-tion based on memory information, situation anal-ysis, and the strategic plan. There are five typesof actions: selecting players, voting (agree or dis-agree), completing quests (succeed or fail), usingnon-verbal signals (raising hands, putting handsdown, opening or closing eyes), and choosing toremain silent. The process of choosing the nextaction is as follows: Apit pA|Mt, Hpit , Ppit , RIpi, Gpi, Spi, It.(4)The subsequent action depends on the memory,the comprehensive analysis, the strategic plan, andthe instruction from the host. The details of theseaction decisions are confidential and only knownto the respective agent. The host and other playerscannot see these decisions.",
  "Self-Role Strategy Learning": "In Step 1, agents generate three strategic recom-mendations for a players role-specific gameplay inAvalon games based on the game history. Agentsavoid mentioning specific players and instead userole names to make the suggestions applicable infuture games. In Step 2, agents enhance their strate-gies by incorporating the gathered suggestionswhile maintaining the original strategys strengths.",
  "Implementation Details": "We developed the Avalon game program in Python,using the gpt-3.5-turbo-16k model as both our back-end and the baselines. In all experiments, we setthe agent models temperature to 0.3 and the LLMextractors to 0. The number of suggestions gener-ated for updating strategies is 3. Game rules androle descriptions were set according to the base-line template (Xu et al., 2023a), which leverageshistorical context, enhances agent reasoning, andlearns from past mistakes. Detailed descriptionsare provided in Section A.2.",
  "We evaluate the performance of our frameworkbased on metrics from two perspectives": "5.2.1Gameplay Outcome and Strategy.From this perspective, we use metrics associatedwith the gameplay outcome and strategies to quan-titatively evaluate the performance of the proposedagents and the baseline agents.Winning Rate (WR). The winning rate is the per-centage of games won out of the total played, cal-culated by dividing the number of wins by the totalgames played:",
  ": Results of the gameplay between ours andbaseline. We present the winning rates (WR) of ourmethod being good and evil sides": ": (a): Comparison of the engaging quests ratewhen playing evil side. Higher engaging quests ratemeans more opportunities for the player to influence theoutcome of the game. (b): Comparison of the failurevote rate when playing evil side. Baseline is worse. leader votes across 20 Avalon games. It reflectsconsensus among players on proposed quest teams.Persuasion. To evaluate LLM agents persuasion,we track two metrics: self-recommendation rate(proposing oneself for quests) and success rate(self-recommendation for quest participation).Camouflage. Detecting camouflage in AI agentsis challenging. We focus on identifying instanceswhere agents assume different identities in the ini-tial round of each game. Behaviors include Self-Disclosure, Camouflage, and Withholding Identity.Teamwork and Confrontation.We use ChatGPTto analyze role responses, aiming to identify in-stances of collaboration or confrontation. Chat-GPT prompts with a players response and evalu-ates trust (teamwork), lack of trust (confrontation),or ambivalence towards others.Sharing. Sharing reflects how often agents dis-close valuable information, crucial for team coop-eration. Using ChatGPT, we analyze agents di-alogues to identify instances of sharing behavior,aiming to quantify their willingness to share for theteams benefit.",
  "Experiment Results": "To validate the efficacy of Avalon AI agents, werepurposed Werewolf AI agents (Xu et al., 2023a)as baselines. Across two sets of 10 consecutiveAvalon games, our agents faced off against thebaselines, with Evil versus Good and vice versa. After the matches, we compared the winning ratesof our Avalon AI agents to the baselines. As de-picted in , our method demonstrated a 90%winning rate in 10 games when playing the goodside. Conversely, when playing the evil side, thewinning rate was 100% over the same number ofgames.Ablation studies reveal the importance of keymodules in our AI agents. Removing the analy-sis module lowered winning rates to 60% for bothsides, showing its impact on understanding anddecision-making. Excluding the planning modulereduced the good sides winning rate to 80%, high-lighting its role in devising strategies. Without theaction module, the good side won 100% while theevil side dropped to 80%, indicating its importancefor the evil sides success. Removal of the strategylearning module led to winning rates decreasing to50% and 60% for good and evil respectively, em-phasizing its role in enhancing strategies. In con-clusion, the analysis and strategy learning modulessignificantly influence game outcomes, affectingboth sides winning rates. Additionally, the plan-ning and action modules are crucial for success,given their impact on gameplay.To better grasp the strategies employed by ourAvalon Agents and the baseline agent, we com-pared quest engagement and failure voting rateswhen different AI agents acted as the evil side.Both rates significantly impact game outcomes. Ahigher quest engagement rate allows more chancesfor players to influence the game, while a higherfailure voting rate suggests a greater chance forthe evil side to win but also increases the risk ofexposure, indicating an aggressive gameplay ap-proach. illustrates the outcomes for questengagement and failure voting rates. Our AI agents,particularly when playing as Morgana and Assas-sin, show assertiveness, with a 40.3% quest en-gagement rate and 84.0% failure voting rate. Incomparison, baseline agents have lower rates at33.1% and 36.5% respectively. As a result, ourproposed Avalon AI agents achieve a 100% winrate against the baseline agents when playing as theevil side.",
  "To evaluate if AI agents replicate human social be-haviors in Avalon, we conduct a thorough analysis.This involves assessing the agents execution ofteamwork, leadership, persuasion, camouflage, and": ": (a): The leadership behavior. Players withhigher Leader Approval Rate get more agreements fromother players when deciding a quest team. (b) and (c):The persuasion behavior. Self-recommendation Rate:players with higher Self-recommendation Rate are morewill to engage in quests. Self-recommendation SuccessRate: players more likely to gain the trust of other play-ers has higher Self-recommendation Success Rate.",
  "Leadership": "Leadership skills come into play when players takecharge of discussions and decision-making pro-cesses. A good leader can steer the conversation,guide suspicions, and rally the loyal servants tomake informed decisions. Leadership abilities arecrucial for the good side to effectively counter thedeceptive tactics employed by the evil side. (a) illustrates the Leader Approval Ratewhen agents assume various roles. It is evidentthat our agents, playing on the good side, attainremarkably high Leader Approval Rates when serv-ing as leaders. Notably, the AI agents achieve aLeader Approval Rate exceeding 80% averagelywhile undertaking roles associated with the goodside. This signifies their robust leadership qual- ities and their proactive approach to steering thegameplay towards victory. However, the baselineagents could propose good side players to the questteam to achieve high Leader Approval Rate but lowgame win rate.",
  "Persuasion": "As the Loyal Servant, I would like to propose player1, player 3, and myself, player 5, as candidates for the third mission. Player 1 Player 3 As for myself, I have been actively involved in the previous missions and have consistently emphasized myloyalty and dedication to the good side's victory. Loyal Servant:",
  "Camouflage": "Camouflage is central to Avalon. Evil roles mustdeceive loyal servants while subtly sabotaging mis-sions. Skilled players create elaborate lies andmisdirection. Loyal servants also engage in cam-ouflage to conceal their identities, especially whenunder suspicion.In , the rates of various behaviors ex-hibited by AI agents are displayed. Notably, theagents display a notably high tendency to revealtheir identities at the commencement of the game,particularly among the roles associated with thegood side. Intriguingly, in the roles of Morgana andAssassin, agents opt to either conceal or assume dif-ferent identities without explicit instructions to doso in the initial strategy. Specifically, Morgana andthe Assassin display rates of assuming alternateidentities of 10% and 15%, respectively, a strat-egy akin to that observed in human players, wherePercival perceives both Merlin and Morgana butlacks precise knowledge of their identities. Thisspontaneous adoption of deceptive behaviors by AIagents stands out as a captivating observation, un-derscoring their adaptability and strategic acumenin the pursuit of game victory. : The teamwork and confrontation behaviors when playing different roles. Each subfigure shows the attitudedistribution of the player portraying specific role (on the top) towards players in other roles (on the left).",
  "Teamwork and Confrontation": "Teamwork is vital for loyal servants to identifyeach other and succeed in missions by strategizing,discussing assignments, and sharing informationto uncover evil roles. Confrontations arise whensuspicions lead to accusations, resulting in intenseexchanges where accusers present reasoning andthe accused offer defenses or deflect suspicion ontoothers.In (a), teamwork and confrontation ratesof good side roles are depicted. Loyal Servantstend to avoid confrontation due to their lack ofspecific identity information. However, Merlin,aware of Morgana and Assassin, confronts themfrequently. Percival, aware of Merlin and Morganawithout knowing their exact identities, confrontsboth. These observations highlight the adaptivestrategies of AI agents, mirroring the social dynam-ics of human players in Avalon. (b) shows teamwork and confrontationrates of baseline agents. Rates remain consistentacross roles, suggesting they do not adjust strate-gies based on role assumptions.",
  "Sharing is essential for Percival and Merlin. Theypossess more information than other good roles,and sharing their insights aids in winning the game": "However, excessive sharing of known informationmay also benefit the opposing side, as discussionsare public to all players. Therefore, strategic shar-ing of information is necessary to win the game. (a) depicts the proportion of knowninformation shared with other players by differentagents playing the roles of Merlin and Percival inthe first round of the game. It is observed that boththe agents designed by us and the baseline agentsexhibit an excessive level of sharing behaviors.",
  "Vacillation": "At the games onset, some players possess identityclues, like Percival knowing Morgana and Mer-lin without distinction, while others, like LoyalServants, lack such info. Both situations requireplayers to deduce identities for their camps bene-fit. Analyzing teamwork proportions across roundsreveals players ability to discern allies and foes. (b) illustrates Loyal Servants team-work tendencies, while (c) shows Percivals tenden-cies towards Morgana and Merlin. Throughout thegame, players increasingly collaborate with team-mates and less with enemies. However, Loyal Ser-vants face greater challenges inferring roles, lead-ing to higher teamwork with potential foes.",
  "Conclusion": "This paper explores the social behaviors of LLM-based agents in the Avalon game. We introduce amulti-agent framework facilitating efficient com-munication and interaction. This framework in-cludes memory, analysis, planning, action, and re-sponse modules capable of learning from experi-ence. Unlike prior studies, our research delves intothe social dynamics of these agents in gameplayscenarios. Our evaluation showcases the successof our framework in achieving winning strategiesand the adaptability of LLM agents in complexsocial interactions. Future work involves optimiz-ing our approach, exploring its applicability in di-verse game environments, and further understand-ing LLM agents potential in dynamic social inter-actions.",
  "Limitations": "Although the LLM agent framework we proposedhas performed well in the Avalon game, there arealso limitations of high cost and slow interactionspeed, due to multiple accesses to the model re-quired for each interaction. Additionally, from thebehaviors exhibited by the agent, there are also in-stances of unreasonable behavior distribution, suchas excessive self-disclosure actions. In the future,we will explore and improve these aspects.",
  "Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata.2023. Improving language model negotiation withself-play and in-context learning from ai feedback": "Chen Gao, Xiaochong Lan, Zhi jie Lu, Jinzhu Mao,Jing Piao, Huandong Wang, Depeng Jin, and YongLi. 2023.S3: Social-network simulation systemwith large language model-empowered agents. ArXiv,abs/2307.14984. NavidGhaffarzadegan,AritraMajumdar,RossWilliams, and Niyousha Hosseinichimeh. 2023. Gen-erative agent-based modeling: Unveiling social sys-tem dynamics through coupling mechanistic mod-els with generative artificial intelligence.ArXiv,abs/2309.11456. Yuya Hirata, Michimasa Inaba, Kenichi Takahashi, Fu-jio Toriumi, Hirotaka Osawa, Daisuke Katagami, andKousuke Shinoda. 2016. Werewolf game modelingusing action probabilities based on play log analysis.In Computers and Games.",
  "Paul Pu Liang, Jeffrey Chen, Ruslan Salakhutdinov,Louis-Philippe Morency, and Satwik Kottur. 2020.On emergent communication in competitive multi-agent teams. ArXiv, abs/2003.01848": "Bill Yuchen Lin, Yicheng Fu, Karina Yang, Prithvi-raj Ammanabrolu, Faeze Brahman, Shiyu Huang,Chandra Bhagavatula, Yejin Choi, and Xiang Ren.2023. Swiftsage: A generative agent with fast andslow thinking for complex interactive tasks. ArXiv,abs/2305.17390. Yang Liu, Yuanshun Yao, Jean-Francois Ton, XiaoyingZhang, Ruocheng Guo, Hao Cheng, Yegor Klochkov,Muhammad Faaiz Taufiq, and Hanguang Li. 2023.Trustworthy llms: a survey and guideline for eval-uating large language models alignment.ArXiv,abs/2308.05374. Rajiv Movva, S. Balachandar, Kenny Peng, GabrielAgostini, Nikhil Garg, and Emma Pierson. 2023.Large language models shape and are shaped by so-ciety: A survey of arxiv publication patterns. ArXiv,abs/2307.10700. Noritsugu Nakamura, Michimasa Inaba, Kenichi Taka-hashi, Fujio Toriumi, Hirotaka Osawa, DaisukeKatagami, and Kousuke Shinoda. 2016. Construct-ing a human-like agent for the werewolf game usinga psychological model based multiple perspectives.2016 IEEE Symposium Series on Computational In-telligence (SSCI), pages 18.",
  "Qiurong Song and Jiepu Jiang. 2022. How misinfor-mation density affects health information search. InProceedings of the ACM Web Conference 2022, pages26682677": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288. Chen Feng Tsai, Xiaochen Zhou, Sierra S Liu, JingLi, Mo Yu, and Hongyuan Mei. 2023. Can largelanguage models play text games well?currentstate-of-the-art and open questions. arXiv preprintarXiv:2304.02868. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processingsystems, 30.",
  "Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-dlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and An-ima Anandkumar. 2023a. Voyager: An open-endedembodied agent with large language models": "Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, HaoYang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang,Xu Chen, Yankai Lin, et al. 2023b. A survey on largelanguage model based autonomous agents. arXivpreprint arXiv:2308.11432. Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi,Shuo Chen, Qisen Yang, Andrew Zhao, ChaofeiWang, Shiji Song, and Gao Huang. 2023c. Avalonsgame of thoughts: Battle against deception throughrecursive contemplation. Tianhe Wang and Tomoyuki Kaneko. 2018. Applica-tion of deep reinforcement learning in werewolf gameagents. 2018 Conference on Technologies and Appli-cations of Artificial Intelligence (TAAI), pages 2833.",
  "Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi,Xingshan Zeng, Wenyong Huang, Lifeng Shang,Xin Jiang, and Qun Liu. 2023d.Aligning largelanguage models with human: A survey.ArXiv,abs/2307.12966": "Sarah Wiseman and Kevin B. Lewis. 2019.Whatdata do players rely on in social deduction games?Extended Abstracts of the Annual Symposium onComputer-Human Interaction in Play CompanionExtended Abstracts. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, YiwenDing, Boyang Hong, Ming Zhang, Junzhe Wang,Senjie Jin, Enyu Zhou, et al. 2023. The rise andpotential of large language model based agents: Asurvey. arXiv preprint arXiv:2309.07864.",
  "A.1Avalon Introduction": "Avalon is designed for 5 to 10 players. Specifically,we focus on the 6-player variant of the game.Player roles. Roles including Merlin, Percival,Morgana, Assassin, and two Loyal Servants, aredivided into good and evil sides. Merlin, Percival,and loyal servants are on the good side, while Mor-gana and Assassin are on the evil side. Players areassigned roles secretly, with some having specialabilities. Morgana and Assassin are initially awareof each other. Percival is able to see Merlin andMorgana but does not know their exact identities.Merlin is aware of the identities on the evil side.Quest team assignment. After receiving roles,players engage in 3-5 rounds of discussion andvoting for a certain number of players to form aquest team. At the start of each round, a leader isassigned in rotation. The leader hosts a discussion,followed by a public vote on quest team members.If more than half of the votes agree, the team forms;otherwise, leadership rotates to the next player forfurther discussion and voting. Each round allowsup to five discussion and voting cycles, with theleader directly assigning team members after thefifth round.Quest phase. The quest outcome is determinedby the cards submitted by the quest team. Goodplayers can only submit success cards, while evilplayers can choose to submit either success or fail-ure cards. A quest is successful if all team membersvote for success, and fails if one or more membersvote for failure.End of the game. The game ends when threequests succeed (good side wins) or three quests fail(evil side wins). Additionally, the evil players canwin by correctly identifying Merlin at the end.",
  "A.4Heuristic Rules for LLM Gameplay": "In the gameplay, we used LLM to extract infor-mation from the responses of the agents. For ex-ample, when the agent selects a player, it extractsthe player number, and when voting, it extracts theplayers voting result. With several demonstrationsof how to extract corresponding information, LLM can extract information very accurately to help thegame proceed smoothly. shows some casesof extraction.It is observed agents sometimes may fail to an-swer questions correctly, such as voting with un-clear attitudes. In order to allow the game to pro-ceed smoothly, we design the following heuristicrules. When voting for quest candidates, if theagents answer is unclear, we assume that it agrees.When voting the quest for success or failure, if theagents answer is unclear, we default to it voting forfailure. When agents select an excessive numberof players, we truncate the selection to meet thequests requirements. In cases where the agentschoose too few players, the host will repeat ques-tion to the agent. If the required player count isstill not met even after multiple retries, the programsteps in to assist by making a random selection onbehalf of the agent.",
  "A.5Ablation Study": "To validate the efficacy of the proposed modules,we conducted an ablation study under both withand without learning from experience setting. Ini-tially, we assessed the effectiveness of the Improv-ing Strategy Module (IS), the Analysis of OthersStrategies Module (AO), and the Analysis Module(AM) within the context of the learning from ex-perience setting, wherein strategies were updatedbased on accumulated gameplay for both our agentsand the baseline agents. In this evaluation, theproposed agents engaged in ten games, assumingevil side roles, against the baseline agents for eachmodule. Following these games, the wining rate(WR), quest engagement rate (QER), and the fail-ure voting rate (FVR) were measured and reportedfor analysis. presents the outcomes of theablation study conducted within the learning-from-experience setting. It is discernible that in the ab-sence of the Improving Strategy module, where thestrategy remains static but the agent can still gleaninsights from other players strategies, the winningrate decreases by 20%. Additionally, the agents ex-hibit reduced aggression, indicated by lower questengagement rates and failure voting rates. Further-more, the absence of the Analysis of Others Strate-gies module and the Analysis Module also leads toa decline in the winning rate. In these scenarios,the agents adopt a cautious gameplay approach,resulting in significantly lower quest engagementrates but higher failure voting rates.",
  "Summarization:": "Within the context of the Avalon game, please assist {Player i} insummarizing the conversations known to him from the current phase.Theseconversations are structured in JSON format, with message signifyingthe content of the conversation, \"name\" identifying the speaker, andmessage_type indicating the type of message relevant to {Player i}.Specifically,public implies that all players have access to the message,while private implies that only {Player i} has access to it.",
  "Action:": "Your objective is to make decisions based on your role, your game goaland the current game state.There are five types of actions you can take:choosing players, voting (agree or disagree), performing missions (makemissions succeed or fail), using non-verbal signals (raise hands up, puthands down, open eyes, or close eyes), and choosing to remain silent.Onlyone action type can be selected at a time.If you decide to choose players,you can choose multiple players according to Hosts question.",
  "Previous suggestions:{suggestions from last game}": "Give your suggestions, No more than two sentences per suggestion and thesuggestions should be general for future games (This implies that you shouldavoid referencing player x directly and instead use the respective role nameswhen making your suggestion.)and effectively help him achieve his game goalin future games.Self-Role Strategy Learning (Step 2)",
  ": Module Ablation: under the setting withoutlearning from experience": "Following the initial evaluation, we proceededto assess the effectiveness of the Analysis Mod-ule, Planning Module, and Action Module underconditions where learning from experience was notincorporated. In this scenario, strategies were notupdated for both our agents and the baseline agent.It is essential to note that the games were conductedindependently, with no influence from previousgames on future gameplay. presents theresults from the module ablation study conductedwithout incorporating learning from experience. Itis discernible that the absence of the planning mod-ule results in a notable 20% decrease in the winningrate. Additionally, the Assassin exhibits a signif-icantly lower quest engagement rate, indicating atendency to overlook the mission objective withoutthe guidance of a strategic plan. This underscoresthe critical importance of the planning module inensuring that agents consistently progress towardwinning the game.Furthermore, in the absence ofboth the analysis and action modules, the agentsexhibit a slightly lower quest engagement rate. De-spite this, they manage to maintain an impressive80% winning rate.In the final phase of our evaluation, we scruti-",
  ": Camouflage example": "nized the impact of analysis on all players, team-mates and adversaries. In each configuration, ouragents assumed the roles of the evil side in tengames, facing off against baseline agents aided bycorresponding analysis information. The results,encompassing winning rate, quest engagement rate,and failure voting rate, are tabulated in .It becomes apparent that when analysis informa-tion is restricted solely to teammates, the winningrate declines by 10%. In response, our proposedAI agents adopt a less aggressive approach, evi-dent in reduced quest engagement rates and failurevoting ratings. However, when analysis informa-tion pertains exclusively to adversaries, there is adecrease in quest engagement rates while retain-ing the winning rate and failure voting rate. Thisphenomenon can be attributed to the strategic ad-vantage gained by the Assassin, who can identifyMerlin with the aid of analysis information on ad-versaries. Consequently, the analysis of adversariesproves to be paramount for the evil sides victoryin Avalon games for AI agents.",
  ": Leadership example": "token limitations. Preliminary exploration withoutfurther analysis is discussed below. presents the performance of agents basedon LLaMA2 in the Avalon game, where we mea-sure their performance using Valid Response Rate(defined in equation 8).Compared to GPT3.5,LLaMA shows a decrease of 25.1% in this met-ric. This could be attributed to LLaMAs poorerlanguage comprehension abilities compared toGPT3.5, resulting in its inability to grasp the com-plex content of the Avalon game.Valid Response Rate (VRR). Agents are requiredto engage in discussion, select players, and vote. AValid Response is defined as a response that adheresto these requirements. the VRR is calculated asfollows:",
  "DTeamwork and Confrontation": "and illustrate the differencesin teamwork and confrontation behaviors of agentsunder conditions with and without experience learn-ing. shows that, without strategic learning,evil-side players (e.g., Morgana) overly confront,while good-side players confront less, with mini- mal variation. This contrasts with , de-picting agents with strategic learning. Here, theintroduction of strategic learning mitigates exces-sive confrontation by evil-side players, who strate-gically engage in more teamwork.Conversely,good-side players strategically increase confronta-tion with potential enemies while reducing it withpotential teammates. : The teamwork and confrontation behaviors when playing different roles: each subfigure shows theattitude distribution of the player portraying specific role (on the top) towards players in other roles (on the left)."
}