{
  "Abstract": "Recent advances in Vision-Language Models(VLMs) and the scarcity of high-quality multi-modal alignment data have inspired numer-ous researches on synthetic VLM data gener-ation. The conventional norm in VLM dataconstruction uses a mixture of specialists incaption and OCR, or stronger VLM APIs andexpensive human annotation. In this paper, wepresent World to Code (W2C), a meticulouslycurated multi-modal data construction pipelinethat organizes the final generation output into aPython code format. The pipeline leverages theVLM itself to extract cross-modal informationvia different prompts and filter the generatedoutputs again via a consistency filtering strat-egy. Experiments have demonstrated the highquality of W2C by improving various existingvisual question answering and visual groundingbenchmarks across different VLMs. Furtheranalysis also demonstrates that the new codeparsing ability of VLMs presents better cross-modal equivalence than the commonly useddetail caption ability. Our code is availableat",
  "Introduction": "Fueled by the rapid development of Vision-Language Models (VLMs) (Zhu et al., 2023;Liu et al., 2024b; Team et al., 2023; Liu et al.,2024a; Dong et al., 2024b) and Diffusion Models(DMs) (Betker et al., 2023), collecting detailed andconcrete high-quality captions for each image be-comes more and more urging. However, expensiveand tedious human labeling for high-quality image-text pairs further incurs the necessity of a cheap andreliable data construction pipeline without humanintervention.Related works on image-text data curation canbe divided into two main streams. Distillation-",
  "* These authors contributed equally to this work. Corresponding author": "based methods leverage closed-source commer-cial products (e.g., GPT-4V (Achiam et al., 2023))with the state-of-the-art performance for image cap-tion (Chen et al., 2023a; Li et al., 2024; Chenet al., 2024a). Another line of work curates animage caption pipeline with existing VLMs to fil-ter high-quality image-text for the training of bet-ter VLMs. These methods usually combine open-source LLMs (Touvron et al., 2023a,b; Chianget al., 2023) and different visual specialists (Liet al., 2023a; Huang et al., 2023b; Zong et al., 2023;Zhang et al., 2024a; Fang et al., 2023; Mindereret al., 2022; Ren et al., 2024; Zhang et al., 2023b)to endow existing VLMs with new abilities, e.g.,pixel grounding in GLaMM (Rasheed et al., 2023).However, the dependency on a mixture of special-ists and human feedback in filtering noisy genera-tions (Wang et al., 2023b) makes it difficult to scalethe generated data and automate the process.Recent progress shows that generated results ofLLMs (Wang et al., 2022; Li et al., 2023c) andVLMs (Zhang et al., 2024b) for prompts with simi-lar meanings should be alike and we can help filterout noisy generated texts and captions by consis-tency checking among multiple prompt instructedresults. In light of the above evidence, we presenta self-instructed data construction pipeline, coinedW2C . W2C autonomously extracts and articulatesspecific content from images, and enhances thereliability of the generated image captions by em-ploying consistency filtering by assessing the out-puts through multiple instructed prompt consisten-cies. The overall pipeline reduces requested spe-cialists and frees off expensive human feedback asshown in . In addition, we leverage theidea from human-machine interaction and organizethe model-generated responses into a Python codeformat, following Eureka (Ma et al., 2023) andText2Reward (Xie et al., 2023a). Experiments haveshown that our proposed W2C can improve VLMson various visual question-answering benchmarks. : Overview of W2C and comparison of existing data construction pipelines. W2C differs from existingworks by reducing the need for a mixture of specialists and expensive human annotations via self-instruct. To be specific, W2C performs the best in 7 out of 9VQA benchmarks on LLaVA-NeXT-7B, and 6 outof 9 VQA benchmarks on LLaVA-NeXT-13B. Fur-thermore, W2C also improves few-shot evaluationson two widely used VQA benchmarks includingGQA and MME. Especially, on the 2-shot evalua-tion of GQA, the method achieves over 5 accuracygains across different VLMs.Our contribution is summarized in threefold: We present the data pipeline of W2C , whichproposes to generate and filter data all by ex-isting VLMs themselves via self-instruct, sig-nificantly reducing the need for a mixture ofspecialists or expensive human annotations inconventional pipelines.",
  "Related Work": "Vision Language ModelsWith the emergenceof LLMs (OpenAI, 2023; Achiam et al., 2023; Tou-vron et al., 2023a; Team et al., 2023; Jiang et al.,2024), VLMs (Zhu et al., 2023; Zhang et al., 2023a;Team et al., 2023) have demonstrated exceptionalcapabilities in visual recognition and understand-ing, achieving remarkable results on various VLMbenchmarks (Singh et al., 2019; Tito et al., 2021; Zhang et al., 2024b; Liu et al., 2023b; Ying et al.,2024; Fu et al., 2024). The seminal BLIP2 (Liet al., 2023a) firstly introduces Q-Former to adaptencoded image features as potential language to-kens for LLM-based caption prediction. Followingworks (Liu et al., 2024a; Team et al., 2023; Donget al., 2024c) improve the visual component by re-placing VIT (Dosovitskiy et al., 2020) or scalingthe input image resolution, while Zhu et al. (Zhuet al., 2023) extends BLIP2 by employing emergentopen-source LLMs (Touvron et al., 2023a; Chianget al., 2023), endowing current VLMs with signif-icantly better instruction following and problemsolving abilities. LLaVA/LLaVA-1.5 (Liu et al.,2024b, 2023a) further remove Q-Former and pointout that simple MLP projection layers present im-pressive performance in aligning image represen-tation with LLMs. Some works also highlight theimportance of collecting high-quality cross-modalalignment data for improving the consistently scal-ing VLMs (Bai et al., 2023; Wang et al., 2023b; Liet al., 2023b). Multi-modalDatasetConstructionThescarcityofhigh-qualityhuman-labeleddatainspires the synthesis of cross-modal data (Wanget al., 2024; Chen et al., 2023a; Rasheed et al.,2023; Wang et al., 2023a; Li et al., 2024; Luet al., 2023; Dong et al., 2024a; Chen et al.,2024c).Among them, Wang et al. (2023b)propose the AS-1B data generation pipeline andopen-sourced high-quality dense captions on 1Bimages. GLaMM (Rasheed et al., 2023) furtherextends AS-1B by introducing about 10 specialistsof different functionalities including grounding,tagging, and in-context learning. These specialistsenable pixel-wise grounded dense captions for each image.However, the expensive humanannotation required in AS-1B and the complicatedconstruction pipeline in GLaMM have greatlylimited the potential of data scaling. In this work,we try to answer whether synthetic data canimprove VLMs on classical VQA benchmarks (Fuet al., 2024; Ying et al., 2024; Chen et al., 2024b)to avoid tedious data collection.Recent progress in synthetic data generation forLLMs (Huang et al., 2023a; Li et al., 2023c; Wanget al., 2022, 2023c) shed light on the possibility ofMulti-modal data construction by leveraging con-sistency in generation to filter invalid data. Wanget al. (2022) presents the consistent reasoning pathgeneration demonstrating better performance inCOT. Li et al. (2023c) uses the generator-validatorconsistent data for training and can effectively im-prove LLMs on various tasks. Zhang et al. (2024b)further shows that the generator-validator consis-tency in most VLMs is prone to be correct. Code Representation for Visual TasksCoderepresentations can formally encode various struc-ture information in a scene. Eureka (Ma et al.,2023) and Text2Reward (Xie et al., 2023a)parse a scene into Python codes and encourageLLMs to generate programmable dense rewards.ViStruct (Chen et al., 2023b) takes the first step invisual code intelligence by decomposing the code-visual representation into multiple components in-cluding object recognition, object grounding, at-tribute detection, relation detection, and event de-tection. Chen et al. (2023b) further introduces acurriculum learning approach to endow VLMs withthe aforementioned four abilities. However, theheavy dependency on supervised human-labeleddatasets and the complicated curriculum learningpipeline limits its potential. This work investigatesan effective data-constructing pipeline based oncode-vision representation.",
  "Method": "Our data construction pipeline shares some similar-ities with GLaMM (Rasheed et al., 2023), whereboth methods focus on the region-level caption ofthe whole image. W2C further extend GLaMM tosupport generation-validation consistency filteringby exploring different organization formations ofthe labeled elements and present how VLMs boostthemselves on basic multi-modal understandingtasks.To make a comprehensive and systematic exposi- tion of our W2C entire pipeline, the following willbe divided into three parts for discussion:(1) Visual Concepts Extraction in .1,(2) Self-Instructed Information Extraction in Sec-tion 3.2, (3) Information Filtering via Self Con-sistency in .3, (4) Structured formattingin .4. The overview of our constructionpipeline is shown in and all the used in-struct prompts are shown in Appendix A.1.",
  "Visual Concepts Extraction": "To build a fully covered concept list for each imageI in images dataset Draw, we prompt VLMs to gen-erate both general captions (for a concise overviewof the image) and detail captions (to bootstrap asmany visual concepts as possible in the caption)using specific instruct prompts pg and pd. We usebeam search to encourage the VLMs to provide asmany visual concepts as possible to improve gen-eration diversity. The general captions and detailcaptions are obtained as follows:",
  "og, od = fVLM(I, pg), fVLM(I, pd).(1)": "Since visual concepts are mainly composed of nounphrases, we employ the NLTK toolkit (Bird, 2006)to extract all noun phrases denoted as N from ogand od.We use Grounding DINO to map extracted nounphrases to bounding boxes of the current image,where part of the false positive noun phrases are fil-tered as they fail to be mapped with correspondingareas in the image. Here we denote the filtered vi-sual concepts as C, and their corresponding bound-ing boxes as B, which is formulated as follows:",
  "After obtaining visual concepts, we extract region-level captions and OCR information for croppedimages of each concept bounding box, respectively": "Region-level CaptionsWe crop image I for eachvisual concept ci C with its correspondingbounding box bi B to obtain a detailed cap-tion and prompt the VLMs to provide a generalcaption centered on ci. Additionally, to encouragethe VLMs to offer more concrete details about theproperties of ci, we instruct the VLMs to includethe color and material of ci in the caption. Denotethe description prompt for the region-level captionas pdesc(ci) and the image cropped by bi as I(bi). : The data construction pipeline for W2C . Our pipeline utilizes both VLM and an object detector modelto furnish structured data with region-specific awareness, detailed entity captions, and comprehensive globalinformation. The VLM is iteratively invoked to generate the caption and perform consistency filtering to obtainhigh-quality data. The visual concepts set is obtained from the captions by the NLTK toolkit, ci here represents avisual concept from the set. The instruction prompts are all predefined templates.",
  "odesc(ci) = fVLM(pdesc(ci), I(bi))(3)": "OCR informationPrevious methods mainly useOCR tools (PaddleOCR, 2023; JaidedAI, 2023) toenhance the OCR capabilities. In contrast, W2C ac-quire the OCR information via an instructed promptto guide VLMs for existing VLMs have the bettercapability in reading text in complex natural sce-narios. Given the OCR instruct prompt pocr(ci),the OCR information in each cropped image bybounding box area bi with concept ci is formulatedas follows:",
  "Information Filtering via Self Consistency": "Our consistency filtering strategy is inspired by thesimilar generator-validator consistency findings inConBench (Zhang et al., 2024b), where different in-struct prompts may lead to in-consistent captions ofvisual concepts, and the highly consistent genera-tions are prone to be correct ones. In this paper, wepropose to filter the visual concepts via generation-validation consistency, where we change the region-level captions into multiple visual question answer-ing problems for both counting filtering and captionreranking.",
  "Counting Filtering via ConsistencyDifferentfrom AS-1B, we introduce Grounding DINO in": "our construction process, which can naturally filterpart of the plausible visual concepts as these con-cepts usually fail to find corresponding boundingboxes in the image. However, Grounding DINOintroduces new challenges for counting problems,as visual concepts ci might be mapped to multipleboxes that have a large overlap due to inappropri-ately designed hyper-parameters. To prevent theeffect by plausibly mapped (bi, ci), we group allthe ci that have the same name into C and calculatethe existing times for each ci C as ni. We thenmerge all the boxes for each ci (which might con-tain multiple visual concepts with the same name)into B, for a box bi B we crop the image andprompt the VLMs to check whether the group el-ement ci exist ni times in the image via instructprompt pcivalid-g:",
  "ovalid-g(ci) = fVLM(pvalid-g(ci, ni), I(bi)).(5)": "Caption Re-ranking via ConsistencyTo pro-vide better region-level captions for a given visualconcept, we use beam search to bootstrap mul-tiple caption candidates. To select the best can-didate, we again leverage the generator-validatorconsistency. Specifically, for each given visualconcept ci, we get a list of caption candidate[o1desc(ci), o2desc(ci), ..., obdesc(ci)]. We use NLTK toparse these captions and collect all the visual con-cepts that are contained in these captions. Taking nas the total number of extracted concepts in the cap-tions of ci, we get a new visual concept list denoted",
  "ovalid-c(cki ) = fVLM(pvalid-c(cki ), I(bi))(6)": "We then manually design a scoring mechanismbased on the validation result ovalid-c(cki ). Specif-ically, for each caption that contains multiple ex-tracted visual concepts, we assign each correct vi-sual concept ovalid-c(cki ) = \"Yes\" to score 1 andeach hallucinated visual concept ovalid-c(cki ) =\"No\" to -1. By accumulating the scores in each cap-tion, we select the caption with the highest score inone beam as the final caption odesc(ci) for the givenvisual concept ci, which is supposed to be the mostdiverse and correct caption.",
  "Structured Formatting and Filtering": "As shown in , we organize the structuredinformation into code format to fully represent theregion-level information of an image. Inspired byEureka (Ma et al., 2023) and Text2Reward (Xieet al., 2023b), we organize the information as astructured representation into the Python formatdue to its generality and conciseness. The organi-zation is achieved by the following three rules.",
  "Experimental Setup": "DatasetsFor the data construction pipeline, westrictly use the images in the ShareGPT4V datasetfor our self-instructed approach validation in afair comparison. Since the original ShareGPT4Vdataset contains duplicate images, We remove therepeated images in the original 102K data and getabout 87K original images. We follow the practiceof LLaVA-1.5 (Liu et al., 2023a) to adopt a two-stage training approach consisting of prompt tuning(PT) and instruct tuning (IT). For the experimentson low resolution setting, we follow the LLaVA-1.5 to use training dataset LLaVA558k for PT stageand LLaVA665k for IT stage on LLaVA-1.5 trainingstages. As the specific mixture ratio details of theLLaVA-NeXT data were omitted, we directly uti-lized the entire training set from each of the follow-ing datasets in the IT stage, forming a mixture ofdatasets including: LLaVA665k (Liu et al., 2023a),DocVQA (Tito et al., 2021), ChartQA (Masry et al.,2022) and ShareGPT4V (Chen et al., 2023a) onhigh resolution setting.To comprehensively assess the effectiveness ofour constructed dataset, we evaluate the modelon widely adopted multi-modal benchmarks andgrouding benchmarks, including TextVQA (Singhet al., 2019) (without providing OCR tokens),DocVQA (Tito et al., 2021), ChartQA (Masryet al., 2022), MME (Fu et al., 2024), MMTBench (Ying et al., 2024), MMStar (Chen et al.,2024b), ScienceQA (Lu et al., 2022), POPE (Liet al., 2023d), GQA (Hudson and Manning,",
  "LLaVA-NeXT-13B65.3154587.170.137.250.667.678.166.2+ShareGPT4V65.3157487.170.137.550.467.078.463.8+W2C65.5159787.570.737.151.465.279.165.6": ": Visual Question Answering benchmarks of W2C on LLaVA1.5 and LLaVA-NeXT under differentcombination of IT datasets. The best results are bold and the second results are underlined. : our reproduction ofLLaVA-1.5 and LLaVA-Next, which achieves comparable performance with the original papers. : LLaVA-1.5 doesnot support benchmarks that requires high input resolution. Abbreviations: SQAI(ScienceQA), MMS.(MMStar),MMT.(MMT-Bench), Text.(TextVQA), Doc.(DocVQA), Chart.(ChartQA). 2019), RefCOCO (Kazemzadeh et al., 2014), Ref-COCO+ (Mao et al., 2016) and RefCOCOg (Maoet al., 2016). These benchmarks provide a com-prehensive assessment of multiple perspectives onmulti-modal VLM performance. Implementation DetailsIn this paper, we em-ploy two types of leading methods:LLaVA-1.5 (Liu et al., 2023a) uses a CLIP-pretrained ViT-L/14 (Radford et al., 2021) as a vision encoder,a projector and an LLM, and LLaVA-NeXT (Liuet al., 2024a) increases the input image resolutionby applying an adaptive image cropping strategy toconcatenate all vision tokens. To ensure a fair andcomprehensive comparison and present results both excluding and including theShareGPT4V dataset, as well as results from theincorporation of our dataset. We have re-produced LLaVA-NeXT with a learning rate of ViTto 1/10 of the base learning rate for the reason thatLLaVA-NeXT only publishes their evaluation code.The learning rate for the PT stage is set to 1e3 and the IT stage is set to 2e5 for both Vicuna-7Band Vicuna-13B backbone LLM. We use 16 A100for experiments on VLM training. We freeze thevision encoder during training on the LLaVA-1.5and only freeze the vision encoder on the PT stageduring training on the LLaVA-NEXT following theoriginal paper. We show more training details inthe Appendix C.1 Data Processing DetailsDuring the data con-struction pipeline, we employ NLTK (Bird, 2006)tool to extract noun phrases from the captions, andthe resulting set of phrases is then post-processedusing WordNet (Miller, 1995) to remove duplicatesand filter out inaccurately named entities. The totalamount of final data after consistency filtering willnot be completely consistent for different VLMsand we show the details in Appendix C.1. Thecheckpoints of the VLM we used in our data pro-cessing are the original checkpoints of the officialrelease. For LLaVA-1.5, which is not trained withthe ShareGPT4V dataset, LLaVA-NEXT is trainedwith part of the ShareGPT4V dataset. The detailedGPU hours can be found in Appendix C.2 and weshow the visualization of our W2C samples in Ap-pendix C.3.",
  "Main Results": "Effectiveness of W2C data improve variousVLMs in Visual Question Answering bench-marksWe show a quantitative comparison re-sults of the trained VLMs with and without theShareGPT4V dataset, as well as W2C for replace-ment of the ShareGPT4V during the IT trainingstage in . W2C consistently improves theperformance on different settings in both LLaVA-1.5 and LLaVA-NeXT. Especially, in the high reso-lution setting, our W2C presents impressive perfor-mance improvement on multi-modal visual under-standing benchmarks such as MMT Bench, MM-",
  "+Monkey76.462.587.535.749.685.077.478.2+W2C76.563.087.535.850.186.479.580.5": ": Visual Question Answering benchmarks and Grouding benchmarks on LLaVA-NeXT-7B under morecombination of SOTA IT dataset methods. The best results are bold and the second results are underlined. : ourreproduction of LLaVA-Next, which achieves comparable performance with the original papers. To ensure a faircomparison, we randomly selected an equal amount of corresponding data from each dataset for this analysis. Star, and MME. Specifically, W2C can bring im-provement in 7 out of 9 benchmarks on LLaVA-NeXT-7B and 6 out of 9 on LLaVA-NeXT-13B.Especially, on LLaVA-NeXT-13B, W2C improvesDocVQA by 0.7 ANLS, ChartQA by 1.8 accu-racy, MMT Bench by 0.8 accuracy and MME by23 points compared to the reproduction results ofLLaVA-NeXT. More benchmarks results are shownin B.1. W2C data show impressive performance onGrounding benchmarksWe present the perfor-mance of the VLMs on Grounding benchmarks in. The task of referential expression com-prehension necessitates that the model accuratelyidentifies and localizes the object described. Ourmodels demonstrate their exceptional capabilityfor detailed image recognition and localization byundergoing evaluation across various referentialexpression comprehension benchmarks, includingRefCOCO, RefCOCO+, and RefCOCOg. Benefitfrom the entity-enteric generation of local captionsand the presence of local bounding box informa-",
  "tion, our model achieved an average improvementof 1.5/1.6 average IoU on LLaVA-1.5 7B/13B and3.5/1.3 average IoU on LLaVA-NeXT-7B/13B": "Comparison results of more data generationmethods and W2C on LLaVA-NeXT-7B modelunder different benchmarks.We show morequantitative results on the LLaVA-NeXT-7B base-line, employing more data generation methods(ALLaVA and Monkey) that utilize the GPT APIfor data annotation. To ensure a fair comparison,we randomly selected an equal amount of corre-sponding data from each dataset.We reportedon representative Visual Question Answering andGrounding benchmarks and achieved the best out-comes in 7 out of 8 benchmarks. W2C still getscomparable results compared to ALLaVA and getsbetter results on Grounding benchmarks.",
  ": Comparison between detail caption and codeparsing ability in few-shot evaluations on MME andGQA without referring to the image": "pability on LLaVA-1.5. We proceed with furtherablation studies on LLaVA-Next-7B for the con-straints on resources, which optimally demonstratethe full benefits of our pipeline and consistencyfiltering in a comprehensive manner. Organizing data into the python code formatpresents better performanceWe discussed in.2 the strengths of choosing the code for-mat for the representation of structured data. In, we quantitatively compare our data formatwith a single-round dialogue format and a multi-round dialogue format. By using the python codeas data construction format, we observe improvedperformance in both visual grounding benchmarksand visual question answer benchmarks on LLaVA-NeXT-7B. Especially, we improved the MMT-Bench by 0.9/1.3 accuracy and DocVQA by 1.1/4.5",
  "ANLS compared to the single/multi data format": "Filtering introduces better downstream bench-marks performanceWe show the ablation ofdifferent consistency filtering choices in .Similarly, the performance of LLaVA-NeXT-7Bon the both visual grounding benchmarks and vi-sual question answering benchmarks highlights theeffectiveness and necessity of our consistency fil-tering approaches. When two filtering strategiesare combined, we achieve the best performanceby improving DocVQA with 1.0 ANLS, TextVQAwith 1.0 accuracy, RefCOCO+val with 0.5 IOU andRefCOCOgval with 0.8 IOU. We also achieve com-parable results on MMT-Bench and RefCOCOvalwith little performance degradation.",
  "Code Parsing Ability Evaluation": "We further present better cross-modality equiva-lence between image and text brought by the newcode parsing ability. An ideal caption of the im-age should enable the ability to question withoutreferring to the image. Therefore, we compare thequality of the code output and widely used detailcaption output in the ability to handle downstreamtasks via in-context learning on the same LargeLanguage Model. Experimental SettingWe conduct experimentson both LLaVA-1.5-7B/13B and LLaVA-NeXT-7B/13B on two widely used Visual Question An-swering benchmarks, including GQA and the per-ception subset of MME. Due to the support of32k long context and satisfying performance inthe open-source community, we use Qwen-1.5-14B (Bai et al., 2023; Team, 2024) as the problem-solving LLM, and prompt it with few shot inputs. Each shot can be represented as a combinationof {description, question, answer}. For the detailcaption output, we use the models trained with boththe original dataset and the ShareGPT4V dataset toimprove their detail caption abilities. For the codeparsing output, we replace ShareGPT4V with ourproposed W2C dataset. The code parsing ability of VLMs presents muchbetter few-shot performance.From , thecode parsing output shows significant improvementwhen compared with using the detail caption out-put. On the binary classification task for the visualperception subset of MME, the code parsing abil-ity achieves comparable or better performance invarious settings. On the free generation VQA task,GQA, using the code parsing output can bring clearaccuracy gain across different model size and ar-chitectures. Especially, on the 2-shot evaluationof GQA on LLaVA-NEXT-13B, the code parsingoutput by model trained with W2C achieves 8.2 ac-curacy improvement compared to baseline, indicat-ing that the code-parsing ability present improvedperformance in presenting the details of one image.More benchmarks results are shown in B.2.",
  "Conclusion": "This paper presents W2C , an enhanced dataconstruction pipeline that only leverages existingVLMs themselves for detail and compositionalcaptions for an image, which is further organizedin Python code format. We present that existingVLMs can improve themselves on the understand-ing benchmarks in various scenarios, significantlyreducing the need for a mix of visual specialistsand heavy human annotations. Moreover, addi-tional experiments show that the new code parsingability of VLMs presents better capability in fullydescribing the image, with notable improvement inthe few-shot evaluation on downstream tasks whenthe raw images are not provided. Our proposedW2C not only enhances the original capabilities onthe widely used multi-modal understanding bench-marks but also endows existing VLMs with detailedand executable multi-modal parsing ability.",
  "Despite the advancements in improved multi-modalunderstanding benchmarks and new code parsingability, W2C can be further improved in some as-pects": "In this paper, we directly use the ShareGPT4Vdataset images for a fair comparison withShareGPT4V. However, it contains fewerOCR-centric images, limiting the final perfor-mance. Further investigation could be takenin studying the performance of W2C on moredistribution of unlabeled datasets. The experiments are mainly conducted onthe SOTA open-source VLM structures, i.e.,the LLaVA series which use MLP projectorsfor multi-modal alignment. The effectivenessof W2C can be further investigated on otherVLM structures.",
  "Given the promising performance of W2C onevaluation benchmarks, we would like to explorea more high-quality and diverse data generationpipeline in future investigations": "Acknowledgments.This work is partially sup-ported by the National Natural Science Founda-tion of China (U21A20515, 62476262, 62102393,62206263,62271467,2306297,62306296),Beijing Natural Science Foundation (4242053,L242096), China Postdoctoral Science Founda-tion (2022T150639) and the Fundamental ResearchFunds for the Central Universities. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774.",
  "Steven Bird. 2006. Nltk: the natural language toolkit.In Proceedings of the COLING/ACL 2006 InteractivePresentation Sessions, pages 6972": "Guiming Hardy Chen, Shunian Chen, Ruifei Zhang,Junying Chen, Xiangbo Wu, Zhiyi Zhang, ZhihongChen, Jianquan Li, Xiang Wan, and Benyou Wang.2024a. Allava: Harnessing gpt4v-synthesized datafor a lite vision-language model.arXiv preprintarXiv:2402.11684. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, YuhangZang, Zehui Chen, Haodong Duan, Jiaqi Wang,Yu Qiao, Dahua Lin, et al. 2024b. Are we on theright way for evaluating large vision-language mod-els? arXiv preprint arXiv:2403.20330. Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Con-ghui He, Jiaqi Wang, Feng Zhao, and DahuaLin. 2023a.Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprintarXiv:2311.12793. Yangyi Chen, Xingyao Wang, Manling Li, DerekHoiem, and Heng Ji. 2023b. Vistruct: Visual struc-tural knowledge extraction via curriculum guidedcode-vision representation. In Proceedings of the2023 Conference on Empirical Methods in NaturalLanguage Processing, pages 1334213357.",
  "Hongyuan Dong, Jiawen Li, Bohong Wu, Jiacong Wang,Yuan Zhang, and Haoyuan Guo. 2024a.Bench-marking and improving detail image caption. arXivpreprint arXiv:2405.19092": "Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao,Bin Wang, Linke Ouyang, Xilin Wei, SongyangZhang, Haodong Duan, Maosong Cao, et al.2024b.Internlm-xcomposer2:Mastering free-form text-image composition and comprehensionin vision-language large model.arXiv preprintarXiv:2401.16420. Xiaoyi Dong, Pan Zhang, Yuhang Zang, YuhangCao, Bin Wang, Linke Ouyang, Songyang Zhang,Haodong Duan, Wenwei Zhang, Yining Li, et al.2024c.Internlm-xcomposer2-4khd:A pioneer-ing large vision-language model handling resolu-tions from 336 pixels to 4k hd.arXiv preprintarXiv:2404.06512. AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,Dirk Weissenborn,Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, MatthiasMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.An image is worth 16x16 words: Transformersfor image recognition at scale.In InternationalConference on Learning Representations.",
  "Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang,Xinlong Wang, and Yue Cao. 2023. Eva-02: A vi-sual representation for neon genesis. arXiv preprintarXiv:2303.11331": "Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng,Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji.2024. Mme: A comprehensive evaluation benchmarkfor multimodal large language models.Preprint,arXiv:2306.13394. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu,Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2023a.Large language models can self-improve. In The2023 Conference on Empirical Methods in NaturalLanguage Processing. Xinyu Huang, Youcai Zhang, Jinyu Ma, Weiwei Tian,Rui Feng, Yuejie Zhang, Yaqian Li, Yandong Guo,and Lei Zhang. 2023b. Tag2text: Guiding vision-language model via image tagging. In The TwelfthInternational Conference on Learning Representa-tions. Drew A Hudson and Christopher D Manning. 2019.Gqa: A new dataset for real-world visual reasoningand compositional question answering. In Proceed-ings of the IEEE/CVF conference on computer visionand pattern recognition, pages 67006709.",
  "JaidedAI. 2023. Easy-ocr [software]": "Albert Q Jiang, Alexandre Sablayrolles, AntoineRoux, Arthur Mensch, Blanche Savary, Chris Bam-ford, Devendra Singh Chaplot, Diego de las Casas,Emma Bou Hanna, Florian Bressand, et al. 2024.Mixtral of experts. arXiv preprint arXiv:2401.04088. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten,and Tamara Berg. 2014. Referitgame: Referring toobjects in photographs of natural scenes. In Proceed-ings of the 2014 conference on empirical methods innatural language processing (EMNLP), pages 787798. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.2023a. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large lan-guage models. In International conference on ma-chine learning, pages 1973019742. PMLR. Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, PeiyiWang, Shuhuai Ren, Mukai Li, Yazheng Yang,Jingjing Xu, Xu Sun, et al. 2023b. M3it: A large-scale dataset towards multi-modal multilingual in-struction tuning. arXiv preprint arXiv:2306.04387. Xiang Lisa Li, Vaishnavi Shrivastava, Siyan Li, Tat-sunori Hashimoto, and Percy Liang. 2023c. Bench-marking and improving generator-validator consis-tency of language models. In The Twelfth Interna-tional Conference on Learning Representations.",
  "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong JaeLee. 2024b. Visual instruction tuning. Advances inneural information processing systems, 36": "Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu,Mingxin Huang, Dezhi Peng, Mingyu Liu, MingruiChen, Chunyuan Li, Lianwen Jin, et al. 2023b. Onthe hidden mystery of ocr in large multimodal models.arXiv preprint arXiv:2305.07895. Jianqiao Lu, Wanjun Zhong, Wenyong Huang, YufeiWang, Fei Mi, Baojun Wang, Weichao Wang, LifengShang, and Qun Liu. 2023. Self: Language-drivenself-evolution for large language model.arXivpreprint arXiv:2310.00533. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, PeterClark, and Ashwin Kalyan. 2022. Learn to explain:Multimodal reasoning via thought chains for sciencequestion answering. Advances in Neural InformationProcessing Systems, 35:25072521. Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, YukeZhu, Linxi Fan, and Anima Anandkumar. 2023. Eu-reka: Human-level reward design via coding largelanguage models. In The Twelfth International Con-ference on Learning Representations. Junhua Mao, Jonathan Huang, Alexander Toshev, OanaCamburu, Alan L Yuille, and Kevin Murphy. 2016.Generation and comprehension of unambiguous ob-ject descriptions. In Proceedings of the IEEE con-ference on computer vision and pattern recognition,pages 1120.",
  "PaddleOCR. 2023. Awesome multilingual ocr toolk-its based on paddlepaddle": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,et al. 2021. Learning transferable visual models fromnatural language supervision. In International confer-ence on machine learning, pages 87488763. PMLR. Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Ab-delrahman Shaker, Salman Khan, Hisham Cholakkal,Rao M Anwer, Erix Xing, Ming-Hsuan Yang, andFahad S Khan. 2023.Glamm:Pixel ground-ing large multimodal model.arXiv preprintarXiv:2311.03356. Shuhuai Ren, Aston Zhang, Yi Zhu, Shuai Zhang, ShuaiZheng, Mu Li, Alexander J Smola, and Xu Sun. 2024.Prompt pre-training with twenty-thousand classes foropen-vocabulary visual recognition. Advances inNeural Information Processing Systems, 36. Amanpreet Singh,Vivek Natarajan,Meet Shah,Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,and Marcus Rohrbach. 2019. Towards vqa modelsthat can read. In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition,pages 83178326. Gemini Team, Rohan Anil, Sebastian Borgeaud,Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,Radu Soricut, Johan Schalkwyk, Andrew M Dai,Anja Hauth, et al. 2023.Gemini: a family ofhighly capable multimodal models. arXiv preprintarXiv:2312.11805.",
  "Qwen Team. 2024. Introducing qwen1.5": "Rubn Tito, Dimosthenis Karatzas, and Ernest Val-veny. 2021. Document collection visual questionanswering. In Document Analysis and RecognitionICDAR 2021: 16th International Conference, Lau-sanne, Switzerland, September 510, 2021, Proceed-ings, Part II 16, pages 778792. Springer. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, et al. 2023a.Llama:Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971.",
  "Bhosale, et al. 2023b.Llama 2: Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288": "Teng Wang, Jinrui Zhang, Junjie Fei, Yixiao Ge, HaoZheng, Yunlong Tang, Zhe Li, Mingqi Gao, ShanshanZhao, Ying Shan, et al. 2023a. Caption anything: In-teractive image description with diverse multimodalcontrols. arXiv preprint arXiv:2305.02677. Weiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li,Chenxiang Yan, Zhe Chen, Wenhai Wang, QingyunLi, Lewei Lu, Xizhou Zhu, et al. 2024. The all-seeingproject v2: Towards general relation comprehensionof the open world. arXiv preprint arXiv:2402.19474. Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang,Zhenhang Huang, Linjie Xing, Zhe Chen, Hao Li,Xizhou Zhu, Zhiguo Cao, et al. 2023b. The all-seeingproject: Towards panoptic visual recognition and un-derstanding of the open world. In The Twelfth Inter-national Conference on Learning Representations. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,Ed H Chi, Sharan Narang, Aakanksha Chowdhery,and Denny Zhou. 2022. Self-consistency improveschain of thought reasoning in language models. InThe Eleventh International Conference on LearningRepresentations. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, AlisaLiu, Noah A Smith, Daniel Khashabi, and HannanehHajishirzi. 2023c. Self-instruct: Aligning languagemodels with self-generated instructions. In Proceed-ings of the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 1348413508. Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu,Qian Luo, Victor Zhong, Yanchao Yang, and TaoYu. 2023a. Text2reward: Automated dense rewardfunction generation for reinforcement learning. arXivpreprint arXiv:2309.11489. Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu,Qian Luo, Victor Zhong, Yanchao Yang, and TaoYu. 2023b. Text2reward: Automated dense rewardfunction generation for reinforcement learning. arXivpreprint arXiv:2309.11489. Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li,Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, YuqiLin, Shuo Liu, et al. 2024. Mmt-bench: A compre-hensive multimodal benchmark for evaluating largevision-language models towards multitask agi. arXivpreprint arXiv:2404.16006. Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, ChaoXu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding,Songyang Zhang, Haodong Duan, Hang Yan, et al.2023a.Internlm-xcomposer: A vision-languagelarge model for advanced text-image comprehensionand composition. arXiv preprint arXiv:2309.15112.",
  "Luo. 2023b. Gpt4roi: Instruction tuning large lan-guage model on region-of-interest. arXiv preprintarXiv:2307.03601": "Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li,Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, TongLuo, Yaqian Li, Shilong Liu, et al. 2024a. Recognizeanything: A strong image tagging model. In Pro-ceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 17241732. Yuan Zhang, Fei Xiao, Tao Huang, Chun-Kai Fan,Hongyuan Dong, Jiawen Li, Jiacong Wang, KuanCheng, Shanghang Zhang, and Haoyuan Guo. 2024b.Unveiling the tapestry of consistency in large vision-language models. arXiv preprint arXiv:2405.14156. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, andMohamed Elhoseiny. 2023. Minigpt-4: Enhancingvision-language understanding with advanced largelanguage models. In The Twelfth International Con-ference on Learning Representations.",
  "B.2Code Parsing Ability Evaluation": "We have added an analysis of in-context learningfor two representative datasets in : MM-Star and RefCOCOg. Its important to note that al-though we report the in-context learning results onRefCOCOg val set under the same settings, compar-ing these two types of outputs for grounding tasksis not practically meaningful. This is because whenwe instruct the W2C -trained model to output indetailed caption format, the captions do not usuallycontain specific box information like [x1,y1,x2,y2].This leads to a low IoU score for in-context learn-ing with 2/4 shot detailed captions. However, whenoutputting in code format, the model does predictbox information, which accounts for the significantdifference in results on RefCOCOg.",
  "Data Construction Pipeline DetailsWe incor-porate images from the open-source ShareGPT4Vdataset, totaling approximately 87K images. For": "the VLMs in our data construction pipeline, we di-rectly use the official release checkpoints includingLLaVA-1.5 and LLaVA-NeXT.For the cost of our data construction pipeline,we use about 1/1.5 day on 32 A100s GPU forLLaVA-1.5 and about 2/3 days on 48 A100s GPUfor LLaVA-NeXT. For the data obtained by W2Cpipeline, we get 34K from LLaVA-1.5-7B, 33Kfrom LLaVA-1.5-13B, 37K from LLaVA-NeXT-7B, and 29K from LLaVA-NeXT-13B. The reasonsfor the inconsistency in the amount of data are mul-tifaceted. On the one hand, a minor portion of thedata was discarded due to improper handling ofanomalous data throughout the processing stage.On the other hand, a significant amount of data waseliminated during the consistency filtering stageowing to inconsistencies detected by the VLMs.Additionally, the generative capabilities of variousVLMs vary, and the inherent randomness withinVLMs themselves also contributes to these incon-sistencies. Training DetailsDuring the training of VLMs,we use different dataset combinations. We uti-lize the original papers open-source dataset duringboth the PT and IT training stages for LLaVA-1.5.In contrast, for the training of LLaVA-NeXT, thelack of disclosure regarding the specific detailsof the IT stage, we trained using all training setfrom LLaVA665k (Liu et al., 2023a), DocVQA (Titoet al., 2021), ChartQA (Masry et al., 2022) andShareGPT4V (Chen et al., 2023a). Furthermore, byaligning our dataset with that of the original study,we achieved comparable experimental results. Weuse the CLIP-pretrained ViT-L/14 (Radford et al.,2021) as a vision encoder, which input resolutionis 336336. We freeze the vision encoder duringtraining on the LLaVA-1.5 and only freeze the vi-sion encoder on the PT stage during training on theLLaVA-NEXT following the original paper. Theexperiments of VLM training are all conducted on16 A100 GPUs.",
  "C.2Implementation Details of our Pipeline": "We employ beam search to fully leverage the power-ful language generation capabilities and extensiveknowledge base of VLM. This approach enablesthe generation of an increased number of captions,assisting us in acquiring a broader set of visual con-cept candidates. Due to the limitation of GPU mem-ory, we set the generation beam to 8 on LLaVA-1.5and 4 on LLaVA-Next. The learning rate for the",
  "Prompt for Self-Instructed Concept-targeted Captions": "Compositional Caption pdescFrom the image, provide one sentence that describes {e} (you should try your best toinclude attributes like shape, color or material), especially, using {e} as the beginningof your answer.OCR Extract pocrList all the text in the image, answer with the ocr tokens only, and answer No withone word if there isnt any.",
  ": Comparison between detail caption and codeparsing ability in few-shot evaluations on MMStar andRefCOCOg without referring to the image on LLaVA-NeXT-7B": "PT stage is set to 1e3 and the IT stage is set to2e5 for both Vicuna-7B and Vicuna-13B back-bone LLM. We set the warmup ratio to 0.03, thePT stage batch size is set to 256 and the IT stagebatch size is set to 128. We use model max length2048 on LLaVA-1.5 and 4096 on LLaVA-Next forits high resolution setting.",
  "C.3Data Example": "In and , we present images fromthe ShareGPT4V dataset alongside the correspond-ing annotations we constructed by W2C . As shownin these images, the annotations generated entirelyby the VLMs accurately describe both the globalcaptions and the detailed captions of local entitieswithin specific areas. Additionally, the OCR text isalso encapsulated within the corresponding frames.For multiple entities present in the images, a dis-play of group merging is also conducted."
}