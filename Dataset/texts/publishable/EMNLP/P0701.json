{
  "Abstract": "Large Language Models (LLMs) have demon-strated remarkable capability in a variety ofNLP tasks. However, LLMs are also proneto generate nonfactual content. UncertaintyQuantification (UQ) is pivotal in enhancingour understanding of a models confidence onits generation, thereby aiding in the mitigationof nonfactual outputs. Existing research onUQ predominantly targets short text genera-tion, typically yielding brief, word-limited re-sponses. However, real-world applications fre-quently necessitate much longer responses. Ourstudy first highlights the limitations of currentUQ methods in handling long text generation.We then introduce LUQ with its two variations:LUQ-ATOMIC and LUQ-PAIR, a series of novelsampling-based UQ approaches specifically de-signed for long text. Our findings reveal thatLUQ outperforms existing baseline methods incorrelating with the models factuality scores(negative coefficient of -0.85 observed for Gem-ini Pro). To further improve the factuality ofLLM responses, we propose LUQ-ENSEMBLE,a method that ensembles responses from mul-tiple models and selects the response with thelowest uncertainty. The ensembling methodgreatly improves the response factuality uponthe best standalone LLM.1",
  "Introduction": "Large Language Models (LLMs) have demon-strated significant prowess across a wide range ofNLP tasks and are increasingly being used in vari-ous downstream applications (Zhao et al., 2023;Chang et al., 2023).However, existing LLMsare susceptible to hallucination, often resulting inthe generation of nonfactual or fabricated content(Manakul et al., 2023; Zhang et al., 2023). One wayto predict the factuality of an LLMs output with-out resorting to resource-intensive fact-checking",
  "*Now at Google DeepMind.Work done outside of Amazon.1": "procedures is by examining its uncertainty over auser query. Moreover, accurate measurement ofa models confidence in its generated responsescan enable the rejection of answers with high un-certainty, potentially reducing hallucinations andimproving the factuality of the output (Geng et al.,2023; Wang et al., 2023).Although Uncertainty Quantification (UQ) is awell-researched area in machine learning (Gaw-likowski et al., 2023), its application in the contextof LLMs remains under-explored. One primarylimitation is that previous studies on UQ mostlyrequire access to a models internal states (e.g.,logits) (Murray and Chiang, 2018; Kuhn et al.,2023; Vazhentsev et al., 2023; Duan et al., 2023).However, many best-performing LLMs, such asGPT-4 (OpenAI, 2023), Gemini 1.0 Pro (GeminiTeam, 2023), and Claude 2.1 (Anthropic, 2023),are closed-source and only accessible via API calls.This limits the ability to directly analyze their in-ternal processes. Another challenge is that existingresearch on modeling uncertainty predominantlyfocuses on short responses, typically less than 10words in length (Kuhn et al., 2023; Duan et al.,2023; Lin et al., 2023). This is in stark contrastto the more common use cases of LLMs, whereresponses to queries often far exceed this length,sometimes reaching hundreds of words. Such dis-parity points to a need for new UQ methods tailoredfor long-form text generated by LLMs. Therefore,in this study we aim to answer the following re-search questions: RQ1: Are existing UQ methodsstill effective in the context of long-text genera-tion? RQ2: If not, how can we effectively quantifyLLMs uncertainty for long-form answers? RQ3:In what ways can uncertainty scores be utilized toenhance the factuality of model outputs?We explore UQ for long-text generation (at least100 words), with an emphasis on using factualityas the key metric of the models performance. Themain contributions of this paper are:",
  "Tell me a bio of Ramesses IV": "Ramesses IV (reigned 1153-1147 BC) was was the son of Ramesses III and his Great Royal Wife, Iset Ta-Hemdjert. He was the third pharaoh of the 20th Dynasty of Ancient Egypt ... His tomb was discovered in 1898 by Victor Loret in KV35. Ramesses IV (also spelled Ramses IV), was the third pharaoh of the 20th Dynasty of Egypt. He reigned for six years, from 1150 to 1143 BC. ... His tomb in the Valley of the Kings (KV2) was discovered in 1898 by Victor Loret. Ramesses IV, also known as Ramesses Menpehtyre, was the third pharaoh of the 20th Dynasty of Ancient Egypt. His reign lasted from 1155 to 1149 BC. Ramesses IV was the son of Ramesses III and Queen Tyti. ... His tomb was discovered in 1898 by Howard Carter in KV35.",
  "Uncertainty Calculation:": ": The illustration of the LUQ and LUQ-ENSEMBLE framework. Given a question, various LLMs exhibitdiffering levels of uncertainty. We generate n sample responses from each LLM and then assess the uncertaintybased on the diversity of these samples (the LUQ metric). Green highlights indicate consistency across responses(low uncertainty) and red highlights discrepancies (high uncertainty). The LUQ-ENSEMBLE method selects theresponse from the LLM with the lowest uncertainty score as the final answer. We first highlight the limitations of existingUQ methods for long text generation andthen propose LUQ (Long-text UncertaintyQuantification; pronounced asluck), a novelUQ method that computes sentence-level con-sistency in long text scenarios. Through extensive experiments on the origi-nal FACTSCORE dataset and our newly pro-posed FACTSCORE-DIS dataset in medicaldomain, we demonstrate that LUQ consis-tently shows strong negative correlations withthe responses factuality over 6 popular LLMs,outperforming all the baseline methods. We propose an ensemble modeling approachthat selects responses from the model exhibit-ing the lowest LUQ uncertainty score, observ-ing an improvement of up to 5% in the overallfactuality scores. Additionally, we enhancethe models uncertainty awareness by imple-menting a selective answering strategy.",
  "Uncertainty and Confidence": "Confidence and uncertainty in the context of ma-chine learning models pertain to the level of assur-ance or certainty associated with a prediction ordecision (Geng et al., 2023). While many studiestreat confidence and uncertainty as antonyms anduse them interchangeably (Xiao et al., 2022; Chenand Mueller, 2023), Lin et al. (2023) provide a cleardistinction: uncertainty denotes the dispersion ofpotential predictions for a given input, whereas con-fidence pertains to the degree of confidence in a specific prediction or output. We will adopt thisterminology in the following sections.Currently, a formal and universally accepted def-inition of uncertainty levels in language generationtasks remains elusive. Common practice in existingliterature measures uncertainty through the entropyof predictions, akin to approaches in classificationtasks (Kuhn et al., 2023; Lin et al., 2023). Predic-tive entropy is formally expressed as:",
  "rp(r | x) log(p(r | x))": "In classification tasks, confidence for a specificprediction y is quantified using the predicted prob-ability, represented as p(Y = y | x) (Geifmanand El-Yaniv, 2017; Hendrycks and Gimpel, 2017).Similarly, in the context of NLG, the confidencescore for a given response r is represented by thejoint probability of the tokens in the response:",
  "In our study, we adopt a more flexible approachto defining uncertainty and confidence in long text": "generation. Similar to Huang et al. (2024a), we fo-cus on the ability of UQ methods to effectively rankresponses, differentiating between correct and in-correct predictions. This approach also aligns withthe concept of relative confidence as discussed byGeng et al. (2023). Our objective diverges from theorthogonal research direction about models cali-bration, which requires models to precisely reflecttheir true accuracy in practical scenarios (Lin et al.,2023). We argue that while short-answer questionsmay be straightforwardly assessed using metricssuch as accuracy or exact match, these standardsare often unrealistic for long text generation, giventhe complexities of real-life probabilities.From a practical perspective, we aim for the un-certainty score to serve as a reliable indicator of themodels performance. This performance encom-passes several dimensions of generation quality,including factuality, coherence, and creativity. Ourstudy prioritizes factuality and the truthfulness ofresponses, adopting these as our primary metrics.The factuality of the responses R given a specificquery x is denoted as F (R | x). Considering twoinputs xi and xj, we explore the relationship be-tween the models uncertainty, denoted as U (x),and the factuality. Our goal is to have:",
  "In this section, we introduce our LUQ method andits two variations (LUQ-ATOMIC and LUQ-PAIR)to estimate uncertainty in long text generation. Theoverall framework is illustrated in": "Motivation. Our underlying assumption positsthat the greater the models uncertainty regarding agiven question x, the more diverse its responses toquestion x will be. For instance, as shown in Fig-ure 1, the term third pharaoh of the 20th Dynastyof Egypt is frequently supported by other sampleresponses, indicating the models high confidencein this information. However, the samples suggestdifferent reign periods for Ramesses IV; the incon-sistency shows the models higher uncertainty. Following the generation of n responses, tradi-tional UQ methods for short text commonly calcu-late the pairwise similarity among the responses(Kuhn et al., 2023; Lin et al., 2023). These pairwisesimilarity scores indicate the consistency betweena pair of responses and play a vital role in subse-quent uncertainty estimation. However, answers tocertain questions such as Give me an introductionof ... and Tell me something about ... may ex-tend to hundreds of words. Longer text leads to anunexpected high similarity across all response pairswhen applying previous methods. To address thisissue and achieve a more nuanced similarity assess-ment, we propose the LUQ uncertainty measure-ment with sentence-level similarity computation.Inspired by the hallucination detection method inManakul et al. (2023), we split each response tosentences, and check whether each sentence can besupported by other samples. Notation. Let ra represent the response gener-ated by a LLM to a user query x. We generatean additional n stochastic LLM sample responsesR = {r1, r2, . . . , rn} using the same query. Theset R = {ra, r1, r2, . . . , rn} encompasses all out-puts from the model.For any given response ri R, the first objec-tive is to determine how often it is supported (orentailed) by other samples. To this end, we employan NLI classifier to assess the similarity between riand each r R \\ {ri}. The output from an NLIclassifier normally includes classifications of entail-ment, neutral, and contradiction, along with theirrespective logit values. It is important to note thatwe focus exclusively on the entailment and con-tradiction classes, as sentences labeled as neutralgenerally do not impact the overall factuality of aresponse. We calculate the NLI score for each sen-tence sj within a response r, and then average thesescores. Formally, the similarity score S(ri, r) be-tween ri and r is defined as:",
  "riR(1 C(x, ri))": "Unlike Kuhn et al. (2023)s method of apply-ing an off-the-shelf DeBERTa model, we applythe DeBERTa-v3-large model (He et al., 2023),fine-tuned on the MultiNLI (Williams et al., 2018)dataset. This choice is due to our input being a con-catenation of short hypothesis (sentence s) and acomparatively longer premises (reference responser). The format of our input aligns with the task inMultiNLI dataset, ensuring an effective assessmentof consistency among the responses. LUQ-ATOMIC. To check the consistency of thegenerated responses in a more fine-grained man-ner, we implement LUQ-ATOMIC, a variation ofthe original LUQ. The key difference is that it firstuses ChatGPT to break a response r into atomicfact pieces {a1, a2, ..., aj}. LUQ-ATOMIC then cal-culates the uncertainty scores bases on atomic factpiece level (aj) instead of sentence level (sj). LUQ-PAIR. The performance of our NLI classifiermay be constrained by the length of the premisesand hypotheses. To address this, we propose LUQ-PAIR to calculate the entailment score sj for eachsentence sj in r and select the maximum value.Formally, we define this as:",
  "Dataset, Metric, and LLM Selection": "Dataset. When selecting the dataset, we consid-ered three main criteria: (1) The dataset shouldbe a long-form QA dataset. (2) There should be awell-designed and widely-accepted automatic eval-uation tool. (3) The questions should be clear, spe-cific, and have definite answers for objective eval-uation. We therefore employ FACTSCORE (Minet al., 2023) to evaluate the factuality of our gen-erated text. It offers automated assessment with alow error rate (below 2%), enabling scalable appli-cation to diverse LLMs without requiring manualannotation. To supplement the extensive reliability testing of FACTSCORE conducted by its creators,we performed a smaller-scale human annotationstudy. Our findings demonstrate a strong Pearsoncorrelation of 0.88 between FACTSCORE ratingsand human factuality judgments, confirming it be-ing a reliable reference for factuality. Please referto Appendix B for more information about the dis-cussion of this dataset and our validation process.The original FACTSCORE dataset (denotedas FACTSCORE-BIO) includes 500 individualsbiographies from Wikidata with correspondingWikipedia entries. To evaluate the applicabilityof UQ methods across different domains, we ad-ditionally developed a dataset, FACTSCORE-DIS,focusing on disease entities. Details of this datasetcan be found in Appendix C. Metrics.Foreachgeneratedresponse,FACTSCOREcalculatesafactualityscore(FS). We apply FACTSCORE for the first generatedresponse (ra).As the LLMs may sometimerefuse to answer certain questions, to have a faircomparison, we introduce a penalized factualityscore (PFS) and penalized uncertainty score (PUS).To calculate PFS and PUS, we assign a factualityscore of zero and uncertainty score of one toquestions that models opt not to answer.We then proceed to calculate both the PearsonCorrelation Coefficient (PCC) and Spearman Cor-relation Coefficient (SCC) between the factualityscores and uncertainty scores. Following the cri-teria proposed by Schober et al. (2018), we clas-sify the correlation coefficients into five categoriesbased on their absolute values: over 0.9 indicates avery strong correlation; 0.7 to 0.9 signifies strong;0.5 to 0.7 suggests moderate; 0.3 to 0.5 denotesweak; 0.1 to 0.3 implies very weak; and below 0.1means negligible correlation. LLMs. We selected six top-performing LLMsfrom the Arena Leaderboard (Zheng et al., 2023)for our experiments. Within our access rights, wechose three closed-sourced models: GPT-4 (Ope-nAI, 2023), GPT-3.5 (OpenAI, 2022), and Gem-ini 1.0 Pro (Gemini Team, 2023); and three open-sourced models: Yi-34B-Chat (01.ai, 2023), Tulu-2-70B (Ivison et al., 2023), and Vicuna-33B (Zhenget al., 2023). For each LLM, we include the follow-ing baseline UQ methods for comparison. Our im-plementation is based on the LM-Polygraph frame-work as proposed by Fadeeva et al. (2023). Moredetails are provided in Appendix D.",
  "GPT-480.872.420.829.086.6GPT-3.568.368.325.725.7100Yi-34B-Chat55.755.741.341.3100Tulu-2-70B47.247.255.855.8100Gemini 1.0 Pro43.242.761.762.298.9Vicuna-33B42.542.555.355.3100": ": Results on the FACTSCORE-BIO: FS and PFSare average and penalized factuality scores; US and PUSare average and penalized uncertainty scores by LUQ;RR is the response rate. All values are percentages. box UQ methods as baselines: Lexical similarity(LexSim) (Fomicheva et al., 2020), Number of se-mantic sets (NumSets) (Lin et al., 2023), Sum ofeigenvalues of the graph Laplacian (EigV) (Linet al., 2023), Degree matrix (Deg) (Lin et al., 2023),Eccentricity (Ecc) (Lin et al., 2023), SelfCheckNLI(SCN) (Manakul et al., 2023). We also includethree white-box methods for comparison: Maxi-mum Sequence Probability (MSP), Monte CarloSequence Entropy (MCSE) (Malinin and Gales,2021), and Semantic Entropy (SE) (Kuhn et al.,2023). More details can be found in Appendix E.",
  "Uncertainty Quantification Results": "Effectiveness of LUQ. and illus-trate the correlation between factuality scores anduncertainty scores. The results highlight LUQseffectiveness as an indicator of model factualityin long text generation tasks. LUQ demonstrates astrong negative correlation for GPT-3.5, Gemini 1.0 Pro, Yi-34B-Chat, Vicuna-33B, and Tulu-2-70B,with the strongest Pearson correlation being -0.851.For the baseline methods, LexSim emerges as arobust baseline offering lower computational de-mands. The confidence-based SCN method demon-strates the best Spearman correlation in modelssuch as Gemini 1.0 Pro and Tulu-2-70B. Otherbaselines such as Ecc, NumSets and Deg yield un-satisfactory results, occasionally exhibiting evenpositive correlations. Case studies of our proposedLUQ method can be found in Appendix J. Variations of LUQ. We compare the original LUQwith its two variations, LUQ-ATOMIC and LUQ-PAIR, in . We find that, with more fine-grained entailment checking, both consistently out-perform the original LUQ. Further discussion onthe pros and cons of these variations, along withusage guidelines, can be found in Appendix A. LUQ for GPT-4. We also observe that LUQ is bet-ter suited for models with relatively lower factualityand a lack of self-expressiveness regarding uncer-tainty. For models with high factuality capabilities,such as GPT-4, LUQ only demonstrates a moder-ate correlation with factuality scores. As shown in, among all models, GPT-4 exhibits the high-est overall factuality scores and the lowest averageuncertainty scores. a also shows that thedata points of GPT-4 are tightly clustered with onlyfew instances of high uncertainty. This is becauseGPT-4 tends to abstain from answering questions 0.20.40.60.81.0",
  "(f) Vicuna-33B": ": Scatter plot illustrating the relationship between factuality scores (x-axis) and uncertainty scores (y-axis)for different LLMs. Each point symbolizes an item in the FactScore dataset, with a red line highlighting the Pearsoncorrelation. The distribution suggests a pattern where higher factuality correlates with lower uncertainty. more often compared to other models, highlightingimproved uncertainty self-detection. However, thisobservation does not influence the effectivenessof our method, as in real life models with lowerfactuality and unable to express uncertainty are ingreater need of external uncertainty measurements. LUQ in FACTSCORE-DIS. We test one closed-source LLM, GPT-3.5, and one open-source LLM,Yi-34B-Chat in our newly proposed FACTSCORE-DIS. Our LUQ model consistently surpasses theperformance of baseline models, thereby demon-strating its effectiveness on the newly proposeddataset within the medical domain. Higher frequency leads to higher factuality andlower uncertainty. In , we compare thefactuality and uncertainty scores across different en-tity frequencies. The original FACTSCORE datasetprovides the frequency of each entity in Wikipedia,categorizing them based on page views and co-occurrence within the training set (Min et al., 2023).Frequencies are classified into five categories, rang-ing from very rare to very frequent. Our ob-servations suggest that questions associated withhigher entity frequencies tend to yield more fac-tual responses, alongside decreased model uncer-tainty. Notably, GPT-4 demonstrates consistentperformance regarding uncertainty and factualityacross varying frequencies, potentially attributable to its selective response strategy. Although it an-swers all the questions in the very frequent, fre-quent, and medium categories, it refuses to an-swer around 25% of rare questions and 30% ofvery rare questions. 0.0 0.2 0.4 0.6 0.8 1.0",
  ": Results of different ensemble strategies onFACTSCORE-BIO (expressed as percentage). The An-swer Distribution (AD) indicates the percentage of finalanswers generated by each component model": "a specific question. After obtaining outputs fromdifferent LLMs, the challenge now is to choosethe best one without fact-checking each answer(which is both time-consuming and costly (Guoet al., 2022; Zhang et al., 2024)). Utilizing theLUQ uncertainty score as a reliable indicator of fac-tuality, we enhance overall performance throughan ensemble approach. In this method, the modelexhibiting the lowest LUQ score for a given ques-tion is chosen as the final answer. Experimentalresults () affirm the superiority of the LUQ-ENSEMBLE over its constituent counterparts. Ensembling models with similar factualityscores can notably enhance performance. Ourfindings suggest that ensembling models with simi-lar factuality scores can significantly enhance per-formance. For instance, in the combination ofTulu-2-70B, Gemini 1.0 Pro, and Vicuna-33B, thePFS increases by 5% compared to the originally top-performing Tulu-2-70B, which scored 47.19%.Additionally, ensembling models with compara-ble performance leads to a more balanced distribu-tion of answers. In contrast, integrating a modelwith substantially superior performance, as seen inthe combination of GPT-3.5, Gemini 1.0 Pro, andVicuna-33B, predominantly favors answers fromGPT-3.5 (92.35%), leading to marginal improve-ment (0.06%) in the ensemble method. Ensembling does not guarantee better perfor-mance. While ensembling always reduces uncer-tainty scores (as we select the model with the leastuncertainty), it does not necessarily improve factu-ality scores. Ensembling LLMs according to poorUQ methods may result in overall performance thatis worse than that of its individual components. Ta-ble 10 in Appendix G compares the effectivenessof using LUQ as the ensemble indicator with othermethods. The results indicate that ensembling doesnot inherently enhance performance. For example,with UQ method Ecc, the ensemble factuality scorecan be lower than that of its best-performing com-ponent (47.2% vs 43.3%). In contrast, using LUQas the ensembling indicator yields the best overallperformance.",
  "Selective Question Answering": "From , it is observed that while GPT-4opts not to respond to some queries, other mod-els generally attempt to answer all questions. Thelimited refusal by Gemini 1.0 Pro primarily stemsfrom considerations of sensitive content and regula-tory constraints, rather than uncertainty. Therefore,we investigate the application of the LUQ score toequip these models with the capability for selec-tive question answeringthat is, to enable themto decline responses when uncertain. Contrary tothe traditional aim of responding correctly to everyquestion, the objective in a selective question an-swering framework is to preserve accuracy while maximizing the number of questions answered (Ka-math et al., 2020; Cole et al., 2023; Yang et al.,2023; Dong et al., 2024). presents the results of selective questionanswering. The models are permitted to refrainfrom answering questions with high uncertainty.The percentiles indicate the proportion of ques-tions each model abstained from answering. Thefindings demonstrate that adopting a selective an-swering approach enhances the models factualityby allowing for more question rejections. By de-clining to answer a similar proportion of questions(approximately 15%) as GPT-4, the models typi-cally achieve an improvement of over 5% in overallfactuality scores. In Appendix H, we provide a de-tailed discussion on how practitioners can use LUQto implement selective answering strategies, includ-ing setting and adjusting uncertainty thresholds.",
  "Related Work": "UQ in Machine Learning. Prior to LLMs, UQhas been extensively explored within the field ofmachine learning (Gawlikowski et al., 2023). Ac-cording to the source of uncertainty, it is typi-cally categorized into two types: aleatoric andepistemic uncertainty(Hora, 1996; Der Kiureghianand Ditlevsen, 2009). Aleatoric uncertainty, alsoknown as statistical uncertainty, pertains to the in-herent randomness in experimental outcomes dueto stochastic effects (Hllermeier and Waegeman,2021). In contrast, epistemic uncertainty stemsfrom incomplete knowledge, potentially includinguncertainties in a machine learning models param-eters or the lack of certain training data (Hller-meier and Waegeman, 2021; Huang et al., 2023).Our focus is primarily on epistemic uncertainty. UQ in LLMs. In contrast to discriminative mod-els, which readily provide probability scores forspecific categories, uncertainty estimation in gener-ative LLMs presents unique challenges: (1) Thereis an exponential increase in the output space assentence length grows, rendering the evaluation ofall possible predictions impractical (Geng et al.,2023; Wang et al., 2023). (2) The significance ofsemantic nuances and their inherent uncertainties,which diverges from the fixed category labels typi-cal of discriminative models, complicates mattersfurther (Kuhn et al., 2023). Generally, UQ methodsfor LLMs can be categorized based on the acces-sibility of the models internal states, distinguish-ing between black-box and white-box approaches. White-box LLMs often rely on logit-based evalua-tions, assessing sentence uncertainty through token-level probabilities or entropy (Murray and Chiang,2018; Kuhn et al., 2023; Vazhentsev et al., 2023;Duan et al., 2023).However, as access to LLMs increasingly relieson API calls, research has pivoted towards black-box methods. These can be further categorizedinto: (i) verbalized methods, which prompt LLMsto articulate their uncertainty in the output, usingphrases like I am sure\" or I do not know\" (Mielkeet al., 2022). Nonetheless, a practical mismatchbetween the expressed and actual uncertainty lev-els has been noted (Lin et al., 2022; Xiong et al.,2023). Xiong et al. (2023) highlight that LLMs of-ten display excessive confidence when verbalizingtheir certainty. (ii) Consistency-based (sampling-based) estimation premises on the assumption thatincreased uncertainty in a model corresponds togreater diversity in its outputs, frequently result-ing in hallucinatory outputs (Manakul et al., 2023;Lin et al., 2023). Our proposed method, LUQ, fol-lows this consistency-based approach. There arealso efforts on integrating verbalized methods withconsistency-based approaches (Xiong et al., 2023;Rivera et al., 2024). Understanding uncertainty inLLMs can enhance in-context learning (Zhou et al.,2023; Li et al., 2023), selective question answer-ing (Yang et al., 2023), LLM cascading (Huanget al., 2024b), adaptive retrieval (Ding et al., 2024),language agents (Han et al., 2024), and model self-refinement (Yao et al., 2024; Chen et al., 2024).",
  "Conclusion": "In this work, we first identify that existing UQmethods are ineffective on long text generation.We therefore introduce LUQ, a novel UQ methodtailored for long-form text generation in LLMs. Itovercomes the limitation of previous methods bycalculating sentence level consistency. We con-duct extensive experiments over six popular LLMs,such as GPT-4 and Gemini 1.0 Pro. We extend theexisting FACTSCORE dataset with human valida-tion and annotations for additional disease domain.Our findings demonstrate that LUQ significantlyimproves the correlation with models factualityscores over previous methods across various differ-ent setups and domains. LUQ serves as a reliableindicator of models factuality performance. Ad-ditionally, we present LUQ-ENSEMBLE, a modelensembling and selective question answering strat- egy, which showcases a promising avenue for en-hancing the factual accuracy of LLM outputs. Thisresearch not only advances our understanding ofUQ in the context of LLMs but also offers practicaltools for improving the reliability and trustworthi-ness of AI-generated content.",
  "Limitation": "The limitations of this study include the following:(1) A primary challenge in studying uncertaintyquantification for long text generation lies in thedifficulty of evaluating the generated text. Unlikeclassification tasks and short-answer QA, there isno straightforward metric for assessing the qualityof generated text. In this study, we employ thefactuality score as the primary evaluation metric,thereby leaving other text aspects, such as coher-ence, cohesion, and creativity, under-explored. Fu-ture work could investigate uncertainty scores us-ing more comprehensive evaluation metrics. (2) Inthis study, we do not investigate the performance ofUQ methods under ambiguous and unanswerablequestions, such as ASQA (Stelmakh et al., 2022)and SelfAware (Yin et al., 2023). Previous uncer-tainty metrics for short-answer questions are testedon answerable questions with clear intentions. Thisis because clearly defined questions with definiteanswers provide a straightforward framework forevaluating model accuracy. In contrast, unanswer-able or ambiguous questions lack clear groundtruths, complicating the assessment of uncertaintyestimates. We advocate researchers to explore morein this area in the future. (3) We focus on assess-ing the overall uncertainty of a model, rather thanmodel uncertainty on individual instances. The rel-ative uncertainty equation in .2 representsan ideal scenario. If a model learns a significantamount of non-factual data over factual data for aparticular entity/instance, the aforementioned equa-tion can be inaccurate for that case. Future workcould investigate the causes of this special caseand develop strategies to address it during the pre-training stage.",
  "Anthropic. 2023. Introducing claude 2.1. Availablefrom Anthropic:": "Joris Baan, Nico Daheim, Evgenia Ilia, Dennis Ulmer,Haau-Sing Li, Raquel Fernndez, Barbara Plank,Rico Sennrich, Chrysoula Zerva, and Wilker Aziz.2023. Uncertainty in natural language generation:From theory to applications. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,Cunxiang Wang, Yidong Wang, et al. 2023. A sur-vey on evaluation of large language models. ArXivpreprint, abs/2307.03109.",
  "Jiuhai Chen and Jonas Mueller. 2023. Quantifying un-certainty in answers from any language model andenhancing their trustworthiness": "Yulong Chen, Yang Liu, Jianhao Yan, Xuefeng Bai,Ming Zhong, Yinghao Yang, Ziyi Yang, ChenguangZhu, and Yue Zhang. 2024. See what llms cannotanswer: A self-challenge framework for uncoveringllm weaknesses. Jeremy Cole, Michael Zhang, Daniel Gillick, JulianEisenschlos, Bhuwan Dhingra, and Jacob Eisenstein.2023. Selectively answering ambiguous questions.In Proceedings of the 2023 Conference on Empiri-cal Methods in Natural Language Processing, pages530543, Singapore. Association for ComputationalLinguistics.",
  "Kaidi Xu. 2023. Shifting attention to relevance: To-wards the uncertainty estimation of large languagemodels. ArXiv preprint, abs/2307.01379": "Ekaterina Fadeeva, Roman Vashurin, Akim Tsvigun,Artem Vazhentsev, Sergey Petrakov, Kirill Fedyanin,Daniil Vasilev, Elizaveta Goncharova, AlexanderPanchenko, Maxim Panov, Timothy Baldwin, andArtem Shelmanov. 2023.LM-polygraph: Uncer-tainty estimation for language models. In Proceed-ings of the 2023 Conference on Empirical Methodsin Natural Language Processing: System Demon-strations, pages 446461, Singapore. Association forComputational Linguistics. Angela Fan, Yacine Jernite, Ethan Perez, David Grang-ier, Jason Weston, and Michael Auli. 2019. ELI5:Long form question answering. In Proceedings ofthe 57th Annual Meeting of the Association for Com-putational Linguistics, pages 35583567, Florence,Italy. Association for Computational Linguistics. Marina Fomicheva, Shuo Sun, Lisa Yankovskaya,Frdric Blain, Francisco Guzmn, Mark Fishel,Nikolaos Aletras, Vishrav Chaudhary, and Lucia Spe-cia. 2020. Unsupervised quality estimation for neuralmachine translation. Transactions of the Associationfor Computational Linguistics, 8:539555. Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi,Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxi-ang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung,Ribana Roscher, et al. 2023. A survey of uncertaintyin deep neural networks. Artificial Intelligence Re-view, 56(Suppl 1):15131589. Yonatan Geifman and Ran El-Yaniv. 2017. Selectiveclassification for deep neural networks. In Advancesin Neural Information Processing Systems 30: An-nual Conference on Neural Information ProcessingSystems 2017, December 4-9, 2017, Long Beach, CA,USA, pages 48784887.",
  "Aleatoric and epistemic uncertainty in machine learn-ing: An introduction to concepts and methods. Ma-chine Learning, 110:457506": "Hamish Ivison, Yizhong Wang, Valentina Pyatkin,Nathan Lambert, Matthew Peters, Pradeep Dasigi,Joel Jang, David Wadden, Noah A. Smith, Iz Belt-agy, and Hannaneh Hajishirzi. 2023. Camels in achanging climate: Enhancing lm adaptation with tulu2. Amita Kamath, Robin Jia, and Percy Liang. 2020. Se-lective question answering under domain shift. InProceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 56845696, Online. Association for Computational Lin-guistics.",
  "Potsawee Manakul, Adian Liusie, and Mark Gales. 2023": "SelfCheckGPT: Zero-resource black-box hallucina-tion detection for generative large language models.In Proceedings of the 2023 Conference on Empiri-cal Methods in Natural Language Processing, pages90049017, Singapore. Association for Computa-tional Linguistics. Sabrina J. Mielke, Arthur Szlam, Emily Dinan, and Y-Lan Boureau. 2022. Reducing conversational agentsoverconfidence through linguistic calibration. Trans-actions of the Association for Computational Linguis-tics, 10:857872. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-moyer, and Hannaneh Hajishirzi. 2023. FActScore:Fine-grained atomic evaluation of factual precisionin long form text generation. In Proceedings of the2023 Conference on Empirical Methods in NaturalLanguage Processing, pages 1207612100, Singa-pore. Association for Computational Linguistics. Kenton Murray and David Chiang. 2018. Correctinglength bias in neural machine translation. In Proceed-ings of the Third Conference on Machine Translation:Research Papers, pages 212223, Brussels, Belgium.Association for Computational Linguistics.",
  "Patrick Schober, Christa Boer, and Lothar A Schwarte.2018. Correlation coefficients: appropriate use andinterpretation. Anesthesia & analgesia, 126(5):17631768": "Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. 2022. ASQA: Factoid questions meetlong-form answers. In Proceedings of the 2022 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 82738288, Abu Dhabi, UnitedArab Emirates. Association for Computational Lin-guistics. Artem Vazhentsev, Akim Tsvigun, Roman Vashurin,Sergey Petrakov, Daniil Vasilev, Maxim Panov,Alexander Panchenko, and Artem Shelmanov. 2023.Efficient out-of-domain detection for sequence to se-quence models. In Findings of the Association forComputational Linguistics: ACL 2023, pages 14301454, Toronto, Canada. Association for Computa-tional Linguistics. Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, XiangruTang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao,Wenyang Gao, Xuming Hu, Zehan Qi, Yidong Wang,Linyi Yang, Jindong Wang, Xing Xie, Zheng Zhang,and Yue Zhang. 2023. Survey on factuality in largelanguage models: Knowledge, retrieval and domain-specificity. Adina Williams, Nikita Nangia, and Samuel Bowman.2018. A broad-coverage challenge corpus for sen-tence understanding through inference. In Proceed-ings of the 2018 Conference of the North AmericanChapter of the Association for Computational Lin-guistics: Human Language Technologies, Volume1 (Long Papers), pages 11121122, New Orleans,Louisiana. Association for Computational Linguis-tics. Yuxin Xiao, Paul Pu Liang, Umang Bhatt, WillieNeiswanger, Ruslan Salakhutdinov, and Louis-Philippe Morency. 2022. Uncertainty quantificationwith pre-trained language models: A large-scale em-pirical analysis. In Findings of the Association forComputational Linguistics: EMNLP 2022, pages72737284, Abu Dhabi, United Arab Emirates. As-sociation for Computational Linguistics.",
  "Do we need language-specific fact-checking models?the case of chinese": "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.Weinberger, and Yoav Artzi. 2020. Bertscore: Evalu-ating text generation with BERT. In 8th InternationalConference on Learning Representations, ICLR 2020,Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-view.net. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,Yulong Chen, et al. 2023. Sirens song in the ai ocean:A survey on hallucination in large language models.ArXiv preprint, abs/2309.01219. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,Xiaolei Wang, Yupeng Hou, Yingqian Min, BeichenZhang, Junjie Zhang, Zican Dong, Yifan Du, ChenYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.A survey of large language models. ArXiv preprint,abs/2303.18223. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,Joseph E. Gonzalez, and Ion Stoica. 2023. Judgingllm-as-a-judge with mt-bench and chatbot arena. InAdvances in Neural Information Processing Systems36: Annual Conference on Neural Information Pro-cessing Systems 2023, NeurIPS 2023, New Orleans,LA, USA, December 10 - 16, 2023.",
  "ALUQ variations": "compares LUQ, LUQ-PAIR, and LUQ-ATOMIC. Our findings indicate that LUQ-PAIRand LUQ-ATOMIC outperform the original LUQacross all models.The superiority of LUQ-PAIR stems from its useof shorter premises for NLI (sentence sj instead ofr), which leads to higher NLI accuracy. However,this improvement comes at the cost of increasedcomputational requirements. For N samples withM sentences each, the original LUQ requires M M NLI computations, whereas LUQ-PAIR requiresN M2 computations.In LUQ-ATOMIC, we first break down the textinto atomic sentences using ChatGPT before pro-ceeding with further steps. The main concern of this variation is about evaluation fairness. BothLUQ-ATOMIC and FACTSCORE use ChatGPT tobreak long texts into atomic sentences, potentiallycreating an unfair comparison with other UQ meth-ods that do not involve this step. Further thor-ough investigation is needed to determine if thisapproach is universally beneficial, regardless of theatomic fact producer/converter used. This wouldrequire a new study and could be a valuable follow-up work. Notably, our original LUQ can still out-perform existing baselines without this step.Regarding the choice of LUQ and its variations,we recommend the following:",
  "B.1Dataset for Long-form Uncertainty": "As mentioned in .1, when selecting thedataset, we considered three main criteria: (1) Thedataset should be a long-form QA dataset with rela-tively lengthy answers. (2) There should be a well-designed and widely-accepted automatic evaluationtool. (3) The questions should be clear, specific,and have definite answers for objective evaluation.Evaluating long-form QA is a long-standingchallenge, making the last criterion especially im-portant to mitigate factors that could affect evalu-ation quality. According to Hu et al. (2023) andBaan et al. (2023), uncertainty in NLG systemscan be disentangled into three main sources: input,model, and output. Due to the intrinsic ambiguityof language and unknown queries, the input itselfcontains uncertainty (Baan et al., 2023). To con-duct a more controlled study, we focus primarilyon output uncertainty, assuming all questions aregenerally answerable and clearly stated.Among all the datasets, FACTSCORE advancesthe field by using LLMs for human-level evalua-tion, addressing the limitations of traditional met-rics like BLEU, ROUGE-L, and BERTScore. Otherlong-form QA benchmarks fall short in at least onecriterion. For example, ELI5 (Fan et al., 2019)questions are very general (e.g., How can differ-ent animals perceive different colors?) and can beanswered in many ways, making it hard to define",
  "B.2Human Evaluation on FACTSCORE": "We also engaged human annotators to assess thefactuality of the generated passages. AlthoughMin et al. (2023) conducted comprehensive ex-periments to demonstrate the effectiveness of theFACTSCORE framework, we perform a sanitycheck by directly correlating the annotated passagefactuality with uncertainty scores. We recruitedthree students with Masters degrees in ComputerScience from our university to conduct the humanannotations. We ensured the annotators were notinvolved in our project and had not discussed it.We used Fleiss Kappa to measure inter-annotatoragreement, achieving a score of 0.793, indicatingsubstantial agreement (close to the almost per-fect\" standard of 0.8-1.0) according to Landis andKoch (1977). Annotators are compensated abovethe local minimum hourly wage standard. The in-structions provided to the annotators are listed in.We randomly selected 50 passages from the re-sponses generated by the Yi-34B-Chat model. Weobserved a Pearson correlation coefficient of 0.88between the FACTSCORE factuality score and thehuman-annotated factuality score. This findingaligns with the results reported by Min et al. (2023),demonstrating that FACTSCORE is a reliable toolin our experiments. compares the resultsof different UQ methods with those obtained usingFACTSCORE and human annotation.",
  "as FACTSCORE-DIS. The detailed information ofFACTSCORE-DIS dataset is as follows:": "Data CollectionFollowing FACTSCORE-Bio,we use Wikipedia as our main knowledge source.We first select all the diseases names using the fol-lowing SPARQL codes calling the wiki API. Wethen removed those diseases with empty Wikipediapages.FollowingFACTSCORE-BIO,weutilizedWikipedia as our primary knowledge source. Ini-tially, we extracted all disease names using thefollowing SPARQL queries to call the WikidataAPI. Subsequently, we removed those diseases withempty Wikipedia pages.",
  "}": "FrequencyFor each entity retrieved, we adhereto the methodology described by Min et al. (2023)to assign a frequency label ranging from VeryRare\" to Very Frequent\" based on an entityspageviews. Its crucial to acknowledge that in thecontext of diseases, the number of diagnosed casesis commonly used as a metric. However, we optednot to use this metric because our goal is to simulatethe distribution of these diseases within the train-ing corpus of LLMs. Relying solely on diagnosedcase numbers may underrepresent the prominenceof a disease within the corpus. Diseases like Amy-otrophic Lateral Sclerosis (ALS), despite their lowincidence rate in the population, attract significantglobal interest and impact. As a result, LLMs maydemonstrate extensive knowledge about such dis-eases, reflecting their visibility in the data on whichthey are trained, rather than their actual morbidityrates.After determining the frequencies, we sampled36 disease entities for each category, amassing atotal of 180 data points. Subsequently, we con-ducted a human evaluation to validate the selected",
  "DExperiment Setup": "ForGPT-4andGPT-3.5,weusetheOpenAIAPI,withspecificver-siongpt-4-turbo-0125-previewandgpt-3.5-turbo-0613.For Gemini 1.0 Pro,we call the API for developers. For Yi-34B-Chat,Tulu-2-70B (tulu-2-dpo-70b), and Vicuna-33B(vicuna-33b-v1.3), we use them off-the-shelfand only for inference. The temperature is setto 0.7.We run our uncertainty measurementexperiments on A100-SXM-80GB GPUs. For ourexperiments, we use the following prompt: Tell me a short bio of the person <entity>.Begin with their birth, significant life events,achievements, and contributions.Includetheir education, career milestones, any no-table awards or recognitions received, andtheir impact on their field or society.En-sure the biography is concise, factual, andengaging, covering key aspects of their lifeand work.",
  "EBaselines": "We mainly use the library LM-Polygraph (Fadeevaet al., 2023) for the UQ methods. Here we providea brief introduction for each method:LexicalSimilarity (Fomicheva et al., 2020): it com-putes the similarity between two phrases using met-rics like ROUGE scores and BLEU. For our exper-iment, we utilize BERTScore (Zhang et al., 2020)to enhance performance, computing the averagesimilarity score with other answers.NumSemSets (Lin et al., 2023): it clusters seman-tically equivalent answers into the same sets. Ini-tially, the number of semantic sets equals the totalnumber of generated answers. Then it sequentiallyexamines responses, making pairwise comparisonsbetween them, and combines different answers.One of the limitation of this method is that theuncertainty score UNumSemSets can only take inte- ger values. EigValLaplacian is therefore designedto overcome this problem.EigValLaplacian (Lin et al., 2023): For a similar-ity matrix S, it calculates the Normalized GraphLaplacian of S using L = I D 1",
  "SD 1": "2 , whereD is a diagonal matrix and Dii = mj=1 Sij(m is the number of responses). Consequently,the uncertainty score is defined as UEigV=mk=1 max (0, 1 k). This value is a continu-ous analogue of UNumSemSets. In extreme case ifadjacency matrix S is binary these two measureswill coincide.DegMat (Lin et al., 2023): it is based on the ideathat the total uncertainty of the answers might bemeasured as a corrected trace of the diagonal ma-trix D. This is because elements on the diagonal ofmatrix D are sums of similarities between the givenanswer and other answers. We thus define uncer-tainty estimate UDeg (x) = trace(m D)/m2.Eccentricity (Lin et al., 2023): A drawback of pre-viously considered methods is the limited knowl-edge of the actual embedding space for the dif-ferent answers since we only have measures oftheir similarities. The graph Laplacian, however,can provide us with coordinates for the responses.Denote u1, . . . , uk Rm as the eigenvectorsof L that correspond to k smallest eigenvalues.We can efficiently construct an informative em-bedding vj = [u1,j, . . . , uk,j] for an answer yj.Then it uses the average distance from center asthe uncertainty measure, defined as : UEcc =vT1 , . . . , vTm2, where vj = vj 1",
  "FResponse Statistics": "Average Response Length.In our study, we de-fine long text as the models output consistingof at least 100 words. For our experiments, thetexts are even longer, averaging more than 200words. The average word count is calculated andshown in the following table. In contrast, com-monly used datasets for existing UQ methods havemuch shorter texts, with an average of 1.95 wordsfor Trivia QA dataset and 3.37 words for NaturalQuestions dataset.",
  ": The average response length for each LLM": "Number of Facts in a Response showsthe average atomic facts provided by various AImodels for the FACTSCORE dataset. GPT-4 has thehighest average number of atomic facts at 52.24,indicating it provides the most detailed factual re-sponses. Tulu-2-70B follows with an average of52.17, nearly matching GPT-4 in factual details.GPT-3.5 has an AF of 50.67, showing it also deliv-ers a high level of factual details in its responses.Yi-34B-Chat and Gemini 1.0 Pro have compara-tively lower averages, at 45.80 and 42.72 respec-tively. Vicuna-33B has the lowest AF at 36.20,indicating it offers the least amount of factual in-formation in its responses. Generally, these modelsprovide similar number of atomic facts in their re-sponses. GPT-4 Tulu-2-70B GPT-3.5 Yi-34B-Chat",
  "In this section, we discuss more about the nov-elty, motivation, and effectiveness of our LUQ-ENSEMBLE method": "The Motivation of LUQ-ENSEMBLE. After get-ting multiple answers from different LLMs, thechallenge is now to choose which one as the finaloutput. Traditional aggregation methods like ma-jority vote and weighted vote are ineffective for long text generation because finding the majorityanswer is difficult when all the responses are some-what different, and methods like boosting or bag-ging require additional training. Our uncertaintymeasure thus serves as an effective indicator formodel ensembling. The Effectiveness of LUQ-ENSEMBLE. Mean-while, it is not guaranteed that ensembling willlead to better performance. It is true that the un-certainty scores will by definition decrease (as weselect the model with the least uncertainty as the fi-nal response), but the factuality score may not. Theeffectiveness of LUQ-ENSEMBLE largely relies onthe reliability of the UQ method. compares the effectiveness of usingLUQ as the ensemble indicator with other meth-ods. The results show that ensembling does notinherently improve performance. With a poor UQmethod (such as Ecc), the ensemble factuality scorecan be lower than that of its highest component(47.2 vs 43.3). In contrast, using LUQ as the indi-cator for ensembling yields the best overall perfor-mance.",
  "HSelective QA Strategy": "When implementing a selective answering strategyin practical applications, it is essential for practi-tioners to tailor the uncertainty thresholds to thespecific models and tasks at hand. In our exper-iment, as shown in , we find that GPT-4tends to refuse to answer around 15% of the ques-tions. To simulate a GPT-4-like answering strategy,for each model, we set different thresholds to en-sure they refuse to answer between 0 and 15% ofthe questions. Our experiments indicate that dif-ferent LLMs may have varying average absolute uncertainty values, making a universal uncertaintythreshold unsuitable for all models. Additionally,the inherent nature of the tasks may influence prac-titioners decisions to make the model more conser-vative or more willing to attempt answering usersquestions.We advise practitioners to implement selectiveQA strategies using the following practical steps:",
  "IAblation Study": "TemperatureAs the diversity of content gen-erated by LLMs may be influenced by the tem-perature setting, we adjust the temperature to testthe robustness of our methods. Due to limitationsin computational resources and API budget con-straints, we selected GPT-3.5, Yi-34B-Chat, andVicuna-33B for our experiments (refer to ). Our findings indicate that a lower temperatureleads to a weaker correlation score, likely becausethe generated responses are more uniform, pro-viding limited information for the self-consistencytest. As the temperature increases, we observe astrengthening in correlation. However, beyond acertain point, further increases in temperature leadto diminishing improvements and can even resultin a weaker correlation. We hypothesize that exces-sively diverse responses may complicate the NLIprocess, as a greater number of sentences fail to besupported by other samples. Number of SamplesPrevious research on shortanswer generation (Kuhn et al., 2023; Lin et al.,2023) has demonstrated that an increase in thenumber of samples correlates with enhanced per-formance. We investigate whether it also applies 0.10.30.50.70.9",
  ": The effect of different temperatures (upper)and the number of samples (lower) on the PCC withLUQ": "to long-text generation and find that with moresamples, LUQ shows better performance and PCCscores, which corroborates with previous observa-tions in short-text generation, as depicted in Fig-ure5. Providing a greater number of samples en-ables the NLI process to predict sentence factualitywith higher accuracy. However, a notable drawbackof increasing the sample size is the associated risein computational costs.",
  "JCase Study": "In this section, we present two case studies illustrating the performance of LUQ. For simplicity, we onlyshow three samples for each question using Yi-34B-Chat. In both cases, the baseline UQ methods indicatelow uncertainty. However, the first case is with higher factuality, while the second case includes manynon-factual information. Unlike baseline UQ methods that can only calculate similarity at the passagelevel, our method captures more nuanced differences.In the first case study, LUQ demonstrates lower uncertainty. As shown in , we use green torepresent sentences that can be cross-validated by other samples. It is evident that most sentences areconsistent across different samples, thus leading to lower uncertainty.In the second case study, we showcase an example with higher LUQ uncertainty. In , red isused to indicate contradictory information. It is apparent that the samples are not consistent with eachother. The samples even state different nationalities of Jonathan Haagensen. Chadwick Boseman was an American actor, playwright, and screenwriter who rose to prominence for his portrayal of historical figures, most notably as T'Challa in the Marvel Cinematic Universe films \"Black Panther,\" \"Captain America: Civil War,\" \"Avengers: Infinity War,\" and \"Avengers: Endgame. Boseman was born on November 29, 1976, in Anderson, South Carolina. He graduated from Howard University and began his acting career in theater, appearing in various stage productions. His first major film role was as baseball player Jackie Robinson in the biopic \"42\" (2013). He went on to play other notable figures, such as James Brown in \"Get on Up\" (2014) and Thurgood Marshall in \"Marshall\" (2017). Boseman's portrayal of T'Challa, the Black Panther, in the Marvel Cinematic Universe, beginning with \"Captain America: Civil War\" in 2016, brought him widespread recognition and critical acclaim. \"Black Panther,\" which was released in 2018, was a cultural phenomenon, breaking box office records and earning him a NAACP Image Award and a Screen Actors Guild Award for his performance. Off-screen, Boseman was known for his philanthropy and commitment to education, often visiting schools and speaking about the importance of representation in media. He was also an advocate for cancer awareness, having battled colon cancer for several years, which he kept private. Sadly, Chadwick Boseman passed away on August 28, 2020, at the age of 43, due to complications from colon cancer. His death was a great loss to the entertainment industry and his fans around the world. His legacy lives on through his powerful performances and the impact he had on diversity and representation in film. Chadwick Boseman was an American actor, playwright, and screenwriter who rose to prominence for his portrayals of real-life historical figures, most notably Jackie Robinson in \"42\" (2013) and James Brown in \"Get on Up\" (2014). He was born on November 29, 1977, in Anderson, South Carolina. Boseman graduated from Howard University with a degree in Fine Arts in 2000. After a few years of performing in theater and making appearances in television series, he landed his breakthrough role as Jackie Robinson in \"42,\" which earned him critical acclaim. He followed this with a star-turning performance as James Brown in \"Get on Up,\" further establishing himself as a talent to watch. Boseman's career reached new heights when he was cast as T'Challa, the Black Panther, in the Marvel Cinematic Universe, beginning with \"Captain America: Civil War\" (2016). The role catapulted him to international stardom, and he reprised the character in \"Black Panther\" (2018), which he also executive produced. \"Black Panther\" was a cultural phenomenon, breaking box office records and becoming the first superhero film to be nominated for the Academy Award for Best Picture. He was also an advocate for education and often spoke about the importance of literacy and education in his interviews. Sadly, Boseman passed away on August 28, 2020, at the age of 43, following a private four-year battle with colon cancer. His death sent shockwaves through the entertainment industry and beyond, with tributes pouring in from all over the world. His legacy as an artist and a role model continues to inspire, and his impact on cinema and culture is profound. Chadwick Boseman was an American actor, playwright, and filmmaker, best known for his groundbreaking role as T'Challa in the Marvel Cinematic Universe film \"Black Panther.\" Born on November 29, 1977, in Anderson, South Carolina, Boseman graduated from Howard University and began his acting career in theater, appearing in various stage productions. Boseman's breakthrough came with his portrayal of legendary baseball player Jackie Robinson in the 2013 film \"42.\" He went on to play other notable figures, such as James Brown in \"Get on Up\" (2014) and Thurgood Marshall in \"Marshall\" (2017). However, it was his role as the Black Panther that catapulted him to international fame. As T'Challa, Boseman became an inspiration to many, representing a powerful and complex superhero who was also the king of the fictional African nation of Wakanda. \"Black Panther\" was not only a critical and commercial success but also a cultural phenomenon, breaking barriers and inspiring a new generation of superhero fans. Sadly, Boseman passed away on August 28, 2020, at the age of 43, after a private four-year battle with colon cancer. His death was a shock to the world, and his legacy as an artist and a role model continues to be celebrated and honored. Chadwick Boseman's life and work have left an indelible mark on the entertainment industry and society at large.",
  ": Example with low uncertainty by LUQ. Sentences that can be cross-validated by other samples are shownin green": "Jonathan Haagensen is a Brazilian actor, model, and television personality. He was born on January 28, 1983, in Rio de Janeiro, Brazil. Haagensen began his career as a model, working for various fashion brands and appearing in campaigns for companies such as Calvin Klein and Versace. He transitioned into acting, making his debut in the Brazilian television series \"Malhao\" in 2003. Haagensen gained widespread recognition for his role in the telenovela \"Pginas da Vida\" (2006), where he portrayed the character of Henrique. This role earned him a nomination for the Best Actor award at the 2007 International Emmy Awards. Haagensen has since appeared in several other Brazilian telenovelas, including \"Caminho das ndias\" (2009), \"A Favorita\" (2011), and \"Amor Vida\" (2013). He has also ventured into film, with roles in movies like \"O Cheiro do Ralo\" (2006) and \"As Cariocas\" (2010). Apart from his acting career, Haagensen has also hosted various television shows and events. He is known for his good looks and has been considered one of the most handsome Brazilian actors. Haagensen continues to work in the entertainment industry, both in Brazil and internationally. Jonathan Haagensen is a Danish actor, model, and singer who was born on October 27, 1979, in Copenhagen, Denmark. He is best known for his role as Marcus Brostrup in the Danish television series \"Borgen,\" which aired from 2010 to 2013. Haagensen began his career as a model, working for various fashion brands and appearing in magazines. He later transitioned into acting, landing his breakthrough role in \"Borgen,\" where he played the role of the Prime Minister's press secretary. His performance in the series was well-received, and it helped to establish him as a prominent figure in the Danish entertainment industry. Aside from \"Borgen,\" Haagensen has appeared in several other Danish television shows and films. He has also ventured into music, releasing his debut album in 2014. In addition to his work in entertainment, Haagensen is also involved in various philanthropic activities. He has supported organizations that work towards environmental conservation and has been an ambassador for causes related to children's rights and education. Jonathan Haagensen continues to act, model, and pursue his musical career while maintaining a low profile personal life. Jonathan Haagensen is a Brazilian actor, model, and television host. He was born on June 25, 1974, in Rio de Janeiro, Brazil. Haagensen began his career as a model and quickly gained popularity in the fashion industry, working for various international brands and gracing the covers of several magazines. In addition to his modeling career, Haagensen ventured into acting, making his debut in the telenovela \"Corao de Estudante\" in 2002. He has since appeared in several Brazilian television series and films, establishing himself as a prominent figure in the country's entertainment industry. Haagensen is also known for his work as a television host. He has hosted various shows, including the Brazilian version of the reality show \"Big Brother. Throughout his career, Haagensen has been recognized with several awards and nominations for his performances in television and film. He continues to be an influential figure in Brazilian media, known for his talent and good looks."
}