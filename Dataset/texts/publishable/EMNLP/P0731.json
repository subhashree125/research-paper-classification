{
  "Abstract": "Foundation models and vision-language pre-training have notably advanced Vision Lan-guage Models (VLMs), enabling multimodalprocessing of visual and linguistic data. How-ever, their performance has been typicallyassessed on general scene understanding recognizing objects, attributes, and actions rather than cultural comprehension.Thisstudy introduces CULTURALVQA, a visualquestion-answering benchmark aimed at assess-ing VLMs geo-diverse cultural understanding.We curate a collection of 2,378 image - ques-tion pairs with 1-5 answers per question rep-resenting cultures from 11 countries across 5continents. The questions probe understandingof various facets of culture such as clothing,food, drinks, rituals, and traditions. Bench-marking VLMs on CULTURALVQA, includingGPT-4o and Gemini, reveals disparity in theirlevel of cultural understanding across regions,with strong cultural understanding capabilitiesfor North America while significantly lowerperformance for Africa. We observe dispar-ity in their performance across cultural facetstoo, with clothing, rituals, and traditions see-ing higher performances than food and drink.These disparities help us identify areas whereVLMs lack cultural understanding and demon-strate the potential of CULTURALVQA as acomprehensive evaluation set for gauging VLMprogress in understanding diverse cultures.",
  "Introduction": "Recentmultimodalvision-languagemodels(VLMs) (Radford et al., 2021; Liu et al., 2023;Peng et al., 2023; Chen et al., 2024; Lu et al.,2024) have shown impressive performance in taskssuch as image-to-text generation, visual questionanswering, and image captioning, inter alia.These tasks predominantly focus on general sceneunderstanding capabilities such as recognizing Apr, 2023 Jan, 2024 Apr, 2024 May, 2024 Jun, 2024",
  "Country": "BrazilCanadaGermanyChinaEthiopiaIndiaIran NigeriaRwandaTurkeyUSAWestern TrendNon-Western Trend : The performance of VLMs over time, seg-mented by non-Western (red) and Western (blue) coun-tries, with model release dates annotated (bottom).Dashed and solid lines differentiate trends for non-Western and Western countries respectively. VLMsunderstanding of non-Western cultures has been in asteep upward trend since Jan 24. attributes, objects, and actions in scenes containingobjects in their common context (Lin et al., 2014).However, given the advancing capabilities ofVLMs, we believe the time is now ripe to holdVLMs to higher standards.Indeed, to supportincreasingly global digital interactions, VLMsmust also be capable of understanding the culturalvalues (Liu et al., 2021) such as beliefs, rituals, andtraditions, for a variety of cultures in the world.In order to adequately assess whether the currentstate-of-the-art VLMs including proprietary mod-els such as GPT-4O (OpenAI, 2023) and GEMINI(Gemini Team et al., 2023) encode cultural knowl-edge, we need systematic benchmarks. However,evaluating cultural understanding is a challengingtask since culture is a multifaceted concept con-sisting of both tangible (e.g., clothing, and food)as well as intangible elements (e.g., ritual prac-tices). Current benchmarks in this domain, includ-ing MaRVL (Liu et al., 2021) and GD-VCR (Yinet al., 2021), while offering foundational insights,",
  "India": ": Samples from CULTURALVQA. Our dataset is comprised of images presenting cultural concepts from11 countries across five facets: traditions, rituals, food, drink, and clothing. It further includes questions probingcultural understanding of the concepts presented in the images and answers to these questions. have critical shortcomings. MaRVL primarily fo-cuses on visual reasoning tasks (e.g., counting, spa-tial reasoning) on top of images sourced from var-ious cultures, and lacks probing cultural commonsense the knowledge base shared by the membersof a cultural group (see 3). While GD-VCR doesconsider commonsense to a degree, it primarilyconsiders movie scenes, which do not encompassthe broader spectrum of everyday cultural contexts. In response to the above challenges, we proposeCULTURALVQA, a novel benchmark specificallydesigned to assess cultural understanding of VLMs.CULTURALVQA is based on Visual Question An-swering (VQA), requiring models to integrate bothvisual and textual information, which permits theformulation of diverse questions, thereby enablingthe evaluation of a models understanding of cul-tural nuances. The CULTURALVQA benchmark ex-tends the language-only CANDLE dataset (Nguyenet al., 2023), which provides a comprehensive col-lection of cultural commonsense knowledge asser-tions. We expand this dataset by automaticallycollecting images that depict the cultural conceptdescribed by the assertions. On top of these im-ages, we collect questions and answers by employ-ing annotators from different cultures who wouldbe familiar with the different cultural concepts de-picted in the images. See for some examplesof questions and answers. Our benchmark con- sists of 2,378 questions collected on top of 2,328unique images with 1-5 answers per question (total7,206 answers) from 11 countries.1 We also presentseveral analyses to better understand the nature ofquestions and answers in our benchmark.Further, we systematically evaluate severalstate-of-the-art VLMs on CULTURALVQA. Ourevaluation reveals a distinct performance gapbetween proprietary and open-source models, withopen-source models significantly underperformingin comparison (e.g., there is a 29.78% gap betweenthe highest-performing closed-source model and itsopen-source counterpart in the country for whichthe models perform the worst, Ethiopia). Addition-ally, we observe a significant disparity in modelperformance across countries. For instance, thehighest-performing proprietary model, GPT-4O,achieves 67% and 72% accuracy on North Ameri-can cultural concepts while only between 43% and56% accuracy on concepts from Africa. VLMs alsoshow varying degrees of proficiency across culturalfacets, with closed-source VLMs performing betteron questions about rituals and traditions while scor-ing worse on those related to clothing, food, anddrink. We develop CULTURALVQA as a compre-hensive evaluation set for gauging VLMs progressin understanding diverse cultures and highlighting",
  "Related work": "Cultural understanding is closely related to geo-diverse understanding. For instance, the DollarStreet dataset (Gaviria Rojas et al., 2022) includes38,479 images of everyday household items fromhomes around the world, while the GLDv2 dataset(Weyand et al., 2020) contains 5 million imagesand 200k distinct instance labels of natural andhuman-made landmarks, but both only test recogni-tion capabilities as opposed to cultural understand-ing. Burda-Lassen et al. (2024) introduce MOSAIC-1.5k, a culture-specific captioning dataset that in-cludes images from various regions. Bhatia et al.(2024) propose GLOBALRG, which aims to eval-uate retrieval and grounding capabilities in VLMsacross 15 and 50 countries respectively. Anotherrelated line of work focuses on multilingual under-standing. For instance, Bugliarello et al. (2022)unify five datasets across a number of tasks in 20languages. However, their focus lies in multilingualunderstanding as opposed to cultural understand-ing. Additionally, the XM3600 dataset (Thapliyalet al., 2022), includes image captions from 36 re-gions and languages, but lacks depth in culturalconcepts, making it insufficient for evaluating cul-tural diversity in VLMs (Pouget et al., 2024).Closest to our work are the following bench-marks:MaXM (Changpinyo et al., 2023),GD-VCR (Yin et al., 2021), MaRVL (Liu et al.,2021) and the concurrent work CVQA (Romeroet al., 2024).MaXM lacks depth in culturalconcepts, as it builds on XM3600 images. Also,its questions focus more on reasoning and generalimage understanding rather than cultural under-standing 2. The GD-VCR dataset probes culturalunderstanding, but its reliance on cinematic scenes 2We manually annotated 100 random questions from theEnglish subset of the MaXM and found the following distri-bution: color - 3.7%, spatial understanding - 12.9%, sceneunderstanding - 42.6%, Yes/No - 20.4%, counting - 20.4% limits the diversity of real-world cultural contexts itcan have. Moreover, they rely on a multiple-choiceevaluation format, which can be influenced bythe difficulty of answer choices. We believe anopen-ended evaluation provides a more faithfulassessment of the models underlying capabilities.Similarly, while MaRVL tests visually groundedreasoning across multiple languages and cultures,it does not assess cultural common sense related torituals and traditions and also employs a True/Falseevaluation style. CVQA studies cultural questionsin a multilingual setup.However, their focusdiverges from ours as they allocate a much smallerproportion of their dataset to traditions and rituals(13% as compared to 44.1 % in CULTURALVQA)and use a multiple-choice evalaution format. Acomprehensive comparison of different datasetsacross various dimensions is presented in Tab. 1.CULTURALVQA uniquely emphasizes open-endedevaluation, includes culturally diverse images (i.e.,images from multiple cultures), and its questionsprobe for cultural understanding by design. Thecombination of these characteristics sets CUL-",
  "CULTURALVQA: Dataset Creation": "Cultural TaxonomyCulture is a multifacetedconcept that describes the way of life of a col-lective group of people, distinguishing them fromother groups with different cultures (Hofstede et al.,2010; Hershcovich et al., 2022). In this paper, weuse the concept of a country as a proxy for a cul-tural group (Adilazuarda et al., 2024)3. Our workassumes common ground within a cultural group byprobing culturally relevant concepts that are collec-tively understood, as well as shared cultural com-mon sense employed in reasoning (Hershcovichet al., 2022). For instance, lavash a traditionalPersian bread (see ) is an example of a cul-turally relevant concept, while the common prac-",
  "See 7 for a discussion of these choices": "tice of waltzing at weddings exemplifies the cul-tural common sense among Germans.Building on these definitions, we introduce abenchmark that evaluates both the tangible aspectsof culture through culturally relevant concepts,such as food, drink, and clothing, as well as the in-tangible facets via shared common sense embeddedin rituals and traditions.4 We frame this evaluationas a VQA task assessing models cultural under-standing. Starting with a pool of countries, wecollect images and use culturally knowledgeableannotators to frame questions. Finally, we collectthe ground truth answers. Selection of CountriesTo build a benchmarkthat reflects cultural diversity, we aimed to achievebroad geographical coverage. Our final datasetspans 11 countries and 5 continents. These coun-tries were specifically selected to cover differentcultural categories from the World Values Sur-vey (Haerpfer et al., 2022) and include Confucian(China), African-Islamic (Turkey, Iran, Ethiopia,Nigeria, Rwanda), Protestant Europe (Germany),English-speaking (USA, Canada), Latin America(Brazil), and South Asian (India) cultures. We optfor an intentional overrepresentation of African-Islamic countries to address their typical scarcityin geo-diverse datasets. Selection of ImagesWe use the CANDLEdataset (Nguyen et al., 2023) for our image sourcewhich contains 1.1 million entries of Cultural Com-monsense Knowledge (CCSK) along with URLs tocorresponding webpages from the C4 corpus (Raf-fel et al., 2020). The CANDLE dataset representscultural concepts from approximately 196 coun-tries and 80% of web pages in this corpus containimages related to the text (Zhu et al., 2023). Thisallows us to begin with a culturally relevant poolof images.We apply filters for aspect ratio, size, and spe-cific keywords to refine the image dataset. Further,we use CLIP similarity (Hessel et al., 2021) to fil-ter images for cultural relevance, discarding thosewith a CLIP score below a threshold determinedthrough qualitative evaluation of sample images.5 Since our initial pool already contains culturallyrelevant images, there is minimal risk of introduc-ing western-centric biases through the use of CLIP,despite potential biases in its pretraining data. To",
  "Herein, the term concepts is used to encompass bothcultural concepts and common sense.5Threshold of 23 (precision = 0.92, recall = 0.96)": "further ensure quality, we apply an additional roundof human filtering (detailed in the next section).Thus, our multi-stage filtering ensures that the finalset of images is appropriate for cultural annotations.Further details of the image filtering process areprovided in App. B. Question CollectionFollowing the conceptualculture framework by Hofstede et al. (2010), wedirect annotators to create questions that are easilyanswerable by someone from their own culture butchallenging for outsiders. To elicit such questions,we provide annotators the instructions shown inApp. N as well as images and additional context forthe cultural concepts present in the image (retrievedfrom CANDLE). We encourage them to createquestions based on their own cultural knowledge,using the additional context (accessible behind aclick-to-expand box) only when absolutely nec-essary. We also advise annotators to skip imagesif they found them culturally irrelevant or wereunfamiliar with the depicted content. This adds anadditional layer of filtering, resulting in annotatorsdiscarding 19.64% of the images shown to them. Answer CollectionNext, we ask the annotatorsto write answers to the questions created in theprevious step, while ensuring that the answersreflected common agreement within their culture(see instructions in App. O). Here we prompt themto use English for universal concepts like catsor apples and use widely recognized and agreedupon local terms for concepts like festivals or localcuisine, rather than translating them into English.For example, the annotators should write the termNaan instead of Indian bread.This approachpreserves the cultural specificity of the collectedanswers. Further, we instruct annotators to be asprecise as possible in their answers (e.g., sushiinstead of food and Oolong tea instead of tea) andto keep their responses concise, ideally betweenone to three words.Further details about the rationale behindthe data curation process and the challengesencountered are provided in App. I.",
  "TURALVQAs composition and characteristics in-cluding analysis of images, questions, answers, andcultural concepts contained in it": ": Comparative analysis of data by country. The figure presents three aspects: (A) unique counts of images,questions, and answers, (B) average lengths of questions and answers, and (C) average number of answers per ques-tion and inter-annotator agreement scores across countries, showcasing variations and trends in CULTURALVQA. : Word clouds representing the answers in CULTURALVQA across five facets of culture: clothing, drink,food, rituals, and traditions. In the bottom right, a breakdown of cultural facets in data is depicted. ImagesCULTURALVQAcomprises2,328unique images. In we show representativesamples. We choose images to ensure significantcultural representation across 11 different coun-tries. The distribution of unique image count percountry is detailed in (A). QuestionsWe collect 2,378 questions in total.In (A), we present the number of uniquequestions per country.The questions have anaverage length of 10.98 words (see (B) forcountry-wise breakdown). Most frequent questiontypes include What(51.3%), Which(11.2%), In(5.6%), and Why (3.4%) questions. For example,What questions often relate to identifying culturalentities like saree or Dirndl (traditional Indian andGerman dresses, respectively) in the clothing cate- gory, or festivals like Spring Festival (celebrated inChina) among rituals. Where questions inquireabout locations significant to specific foods, such asthe origins of Quebec chicken. Finally, we analyzewhether the collected questions contain stereotypesand found that they are largely absent (see App. C). AnswersCULTURALVQA consists of 7,206manually curated answers in total.6 The average an-swer length is 1.73 words (see (B) for coun-try wise breakdown). We assess whether answerspredominantly feature terms from local languages.To this end, we verified how many answers havecorresponding English Wikipedia titles; for 80%of the answers at least one of the answer words is",
  "contained in at least one Wikipedia title. Thus ourbenchmark is still suitable for English VLMs": "Cultural ConceptsAccording to the pie chartin , food-related questions are most preva-lent, accounting for 37.3% of the dataset, followedclosely by traditions and rituals, which represent26.1% and 18% respectively. Thus, roughly 44%of the questions in our dataset probe for culturalunderstanding of the intangible aspects of culture(rituals and traditions). The word clouds generatedfrom the collected answers in illustrate the di-versity of expressions, such as hamam (Turkey) andmeskel (Ethiopia) for rituals and traditions, and fei-joada (Brazil), fufu (Nigeria), and vada (India) forfood, indicating a geographically diverse culinaryand cultural scope. While the clothing categoryis the least prevalent in the dataset, it shows thehighest variety in terms of collected answers. Thedrink category is notably one of the smallest, bothin terms of the size and number of unique answers.",
  "Evaluating VLMs on CULTURALVQA": "Evaluation MetricEvaluating open-ended VQAis challenging. Traditionally, string matching hasbeen used but it is known to underestimate modelperformance. Based on findings from Maas et al.(2024), which demonstrate the effectiveness ofreference-based LLM evaluation for open-endedVQA tasks, we adopt LAVE, their proposed metric,as our evaluation metric with GPT-4 as the LLM(see App. L for the LLM prompt used). We vali-dated the effectiveness of LAVE for our use caseby computing correlation with human judgments.LAVE judgment agrees with human judgment 79%of the times for GPT-4, 73% of the times for GEM-",
  "VLMs used for evaluationWe benchmark sev-eral state-of-the-art VLMs on the proposed CUL-": "TURALVQA dataset, ranging from closed-sourcemodels like GPT-4 (GPT-4O), CLAUDE (CLAUDE3.5) and GEMINI PRO (GEMINI-PRO-VISION1.0) to a wide variety of open-source models,ranging from 7 to 25 billion parameter count:BLIP2 (Li et al., 2023), INSTRUCTBLIP (Daiet al., 2024), MBLIP (Geigle et al., 2023) PAL- LIGEMMA (Beyer et al., 2024) LLAVA1.5 (Liuet al., 2023), LLAVA_NEXT (Liu et al., 2024),IDEFICS2 (Laurenon et al., 2024), and INTERN-VL 1.5 (Chen et al., 2024). See App. D for adetailed discussion of the selected models.",
  ": Baseline evaluation of the degree of visualunderstanding required in CULTURALVQA: LLM-only,LLM with a country-specific context, LLM with GoogleLens entities, and GPT-4V": "What degree of visual understanding is requiredto answer the questions in CULTURALVQA?To investigate this, we employ the following base-lines. LLM-only: This baseline uses an LLM toanswer questions based solely on the question in-put. It helps gauge how well the questions can beaddressed without visual context, relying only onthe cultural knowledge encoded in the LLM. LLM+ Country: It introduces country-specific contextinto the LLM prompts to determine if knowing thecountry along with the question can already elicitthe correct answer. LLM + Lens: This baselineuses image entity names extracted by Google Lens(Google, 2017) along with the question as input, un-like the other baselines that lack visual context. Ithelps assess whether coarse-level visual knowledgeis sufficient to answer the questions.We evaluate the baselines using GPT-4 as theunderlying LLM. The LAVE accuracies for thesebaselines, as well as for the GPT-4 VLM (whichalso incorporates an image as input in addition tothe question), are presented in . We see thatalthough the country information and the coarsevisual entities help improve the performance on topof the LLM-only baseline, the performance of thestrongest baseline (LLM + Lens) is still far fromthat of the VLM. This verifies that the questions inour dataset require sufficient visual understandingto answer them accurately.",
  "Average24.5032.8136.3738.0341.5146.1852.9751.6661.36": ": LAVE accuracies of open- and closed-source models on CULTURALVQA. Best-performing results percountry are highlighted in green, and best-performing results among open-source models are highlighted in blue. : Performance gap between the best open-source (one of INTERNVL, IDEFICS2, BLIP2) andclosed-source models (GPT-4O) compared to humanperformance. Negative values indicate where modelsunderperform relative to humans. uation of VLMs on the proposed CULTURALVQAbenchmark in Tab. 2 and Tab. 4. The averageLAVE accuracy for the highest-performing model,GPT-4, is approximately 61%, with performancevarying across countries from 43% to 72%. Wesee substantial disparity in cultural understandingacross different VLMs, with the best-performingopen-source model (INTERN-VL for most coun-tries) achieving an average LAVE accuracy of only46%, and performance ranging across countriesfrom 26% to 71%. This result indicates a consid-erable performance gap between closed-sourcemodels and the best-performing open-sourcemodel. It is particularly pronounced in countrieswithin the African-Islamic culture (Ethiopia,Nigeria, Iran, and Turkey), with a 29.78% gap forEthiopia, the country for which the models performthe worst. We also conduct few-shot evaluationof VLMs but find that it does not significantlyimpact performance (see App. E for more details).",
  "Hence, the subsequent analyses in this section areconducted on top of zero-shot results": "Are VLMs better at understanding culturesfrom some countries than others?A country-level (see Tab. 2) analysis of the models revealsstark variance in performance across different re-gions.Generally, open-source models performwell for high-resource countries such as the USA,Canada, Brazil, and India while achieving infe-rior performance in underrepresented countries(Ethiopia, Iran, and Rwanda). This trend holdstrue even for open-source models with large param-eter sizes, such as INTERN-VL, indicating that datadiversity is more crucial for cultural understandingthan model size. Although closed-source modelsshowcase less drastic performance discrepanciesacross countries, their performance also degradessignificantly for African-Islamic countries.",
  "Are VLMs better at understanding some cul-tural concepts than others?In , we re-port the model performance across 5 cultural facets.Generally, we find that proprietary models tend to": "perform better on intangible concepts rituals, andtraditions, compared to drink and food. Indeed, thehighest performance of GPT-4 is achieved in therituals facet ( 63%), whereas in the clothing facet,it achieves a lower performance of 55%. Referto App. F for a more detailed discussion. Do multilingual VLMs perform better in cul-turally diverse settings?One might expect thatmultilingual VLMs may demonstrate superior per-formance due to their exposure to culturally di-verse data.However, our analysis of multilin-gual models, mBLIP and PaliGemma, on CUL- TURALVQA reveals a more nuanced picture. FromTab. 2, mBLIP, built on top of monolingual BLIP2,consistently underperforms it despite multilingualtraining. This could be due to the quality of themachine-translated data used in mBLIP and theLLM backbone used (mT0 (Muennighoff et al.,2023) in mBLIP vs. FlanT5 (Chung et al., 2022)in BLIP2). Also, from Tab. 4 we observe thatPaliGemma shows significant disparities acrosscountries despite large-scale multilingual training.This is possibly due to its smaller size (3B) whichsuggests that multilingual data exposure alone isinsufficient for cultural understanding. How do culturally knowledgeable people per-form on CULTURALVQA?We calculate humanperformance for 1,455 questions for which we havethree or more answers using the LAVE metric. Foreach question, we compute the accuracy of one ofthe human answers against the remaining humananswers using LAVE. We then average the scoresacross all answers. Since all these answers are writ-ten by annotators who are familiar with the cultureprobed in the question, this human performancetells us how well culturally knowledgeable peopleperform on CULTURALVQA.Based on the results reported in (C), hu-man performance is notable and ranges from 55%-85%, with certain countries, such as Iran, show-ing particularly high scores (> 80%). In contrast,Rwanda and Nigeria had the lowest scores (56.05%and 61.76%, respectively). These lower scorescan be partially attributed to the cultural diversitywithin these countries, where using a country asa proxy for a cultural group may not accuratelycapture the nuances of subcultural variations. Thesame concept may hold different meanings acrosssubcultures, leading to varied interpretations andinconsistencies in responses. Further qualitativeinsights are provided in the App. G. We also calculate the Pearson correlation be-tween human and model performance across coun-tries. For open-source models, we observe a rela-tively low correlation, ranging from 0.1 to 0.4. In-terestingly, for closed-source models like GEMINIand GPT-4, we find a stronger correlation of 0.69and 0.75, respectively. This suggests that the fac-tors affecting human performance similarly influ-ence the performance of these closed-source mod-els. However, from , when comparing humanand model performance using the same metric, wefind that closed-source models still lag behind hu-mans for all countries indicating that while thesemodels follow human performance trends, there isstill a marked gap in their cultural understandingcompared to humans. This gap is even more pro-nounced for open-source models, which show aneven larger discrepancy across all countries.Further, in , we observe a larger gap fornon-Western countries such as Iran, Nigeria, India,Turkey, and Ethiopia (> 13%). Conversely, thesmaller gap for Canada and the USA (< 7.0%)indicates a closer alignment between modelsand human performance, likely due to a betterrepresentation of Western cultural concepts inthe training data. Interestingly, GPT-4 shows arelatively low gap for Brazil ( 2%), possiblybecause the questions for Brazil often probecoarse visual understanding. This trend is furthersupported by LLM + Lens baseline in which performs exceptionally well for Brazil. How much does varying question difficultyand varying answer counts affect model perfor-mance disparity across countries?Since wesourced questions from different annotator groupsacross countries, it is imperative to ask if thedisparity in model performance across countriesis due to differences in inherent question difficultyacross countries. To investigate this, we analyzethe Spearman rank correlation 7 between the modelperformance and the average question length (see (B) for average question length across coun-tries). We use average question length as a proxyfor question difficulty - assuming shorter questionsprobe more direct knowledge, while longer onesrequire nuanced cultural understanding. We founda weak correlation between question length andmodel performance (-0.3 to 0.3) for most models,with the exception of GPT-4 and GEMINI, which",
  "We use this metric as the variance in lengths is small,making rank-based analysis more meaningful": "Q: This image depict or give thesign of what in Nigerianculture?Human: sign or symbol ofroyaltyGPT-4 Coral Beads Q: Which planet is the above animalcompared with?Human: EarthGPT-4 Jupiter Q:In which type of glass do Turkishpeople consume the thing depicted inthe image?Human: Slim-waisted glassGPT-4 Tulip glass Q:What is the traditional occupationname of this person?Human:NaghaliGPT-4 Dervish Q:What does the animal in the imagedepict?Human: GarudaGPT-4 Tibetan Snow Lion",
  ": Qualitative failure examples of GPT-4 predictions": "showed a moderate negative correlation of -0.52 onaverage. As illustrated in , for most coun-tries except Brazil, Canada, and the USA, the vari-ance in question lengths is small, suggesting thatquestion length is not a significant factor behind thedisparity in model performance across countries.Another factor potentially affecting the dispar-ity in model performance is the variable numberof human answers per question across countries(see (C)). These human answers are used asthe reference answers in the LAVE metric makingit more rigid for countries with fewer referencesand vice-versa. To investigate this, we compute theSpearman correlation7 between model performanceand the average number of answers per questionacross countries. We find a very low correlationranging between -0.3 and 0 across models, indi-cating that the disparity in the number of humananswers does not meaningfully affect the disparityin model performance across countries. Human judgment of model performanceWeevaluate responses from the three best-performingmodels, GPT-4, GEMINI, and INTERN-VL to ques-tions from India, with each answer rated by 5 hu-mans on a scale of 1 to 5, from completely correctto completely incorrect. See App. H for detailson the human evaluation study. shows thepercentage of questions that fall into each of thefive scales.The models scores closely align withhuman judgments for case 1 scores, suggesting thatour metric predicts answers to be correct only ifthey are both precise and culturally specific. Wenote that humans tend to rate model predictionshigher than the LAVE metric. Finally, the evalua-tion shows that humans tend to choose the extremeratings, considering most model responses as eitherfully accurate or entirely wrong. Qualitative examples of model failuresOurqualitative evaluation of the best-performingmodel,GPT-4,highlights its limitations inrecognizing and interpreting cultural nuances. Forinstance, GPT-4 overlooks the cultural significance of intangible cultural concepts like coral beads inNigeria, which symbolize wealth and heritage butare treated merely as decorative objects, as wellas it fails to recognize the symbolic connectionbetween cows and planet Earth in Indian culture(see ). Focusing on tangible cultural conceptsin , the models shortcomings are evidentas it inaccurately recognizes cultural entities andobjects. For instance, it mislabels Naghali, a tradi-tional Iranian storyteller as a Dervish and mistakesa traditional Turkish tea glass for a tulip glass,commonly used for serving beer. These examplesreveal how GPT-4 has difficulties distinguishingbetween visually similar but culturally distinct enti-ties and objects, and it lacks a deep understandingof cultural beliefs and symbolic meanings.",
  "Conclusions": "In this paper, we introduce CULTURALVQA, anovel VQA benchmark for assessing VLMs ontheir cultural understanding. By curating a diversecollection of images from 11 countries across 5continents and collecting 2,378 hand-crafted ques-tions and 7,206 answers about cultural conceptspresented in these images, written by annotators,we ensured a broad representation of cultural con-cepts pertinent to diverse cultural groups.Benchmarking state-of-the-art models on CUL- TURALVQA reveals notable disparities in their per-formance across regions. Models perform muchbetter on North American cultures compared toAfrican-Islamic ones.Further, we find a starkperformance disparity between closed- and open-source models, with a 29.78% gap between thehighest-performing closed-source and open-sourcemodels for the lowest-performing country. VLMsalso show varying proficiency across cultural facets,excelling in questions about clothing, rituals, andtraditions but struggling with food and drink. Ourresults underscore the current limitations of VLMsin achieving uniform cultural comprehension andpinpoint specific areas that require improvement.",
  "Limitations": "Our study faces limitations due to our data collec-tion methods, the scope of the CULTURALVQAdataset, and our focus on the English language. Weapproximated cultural groups using geographicalregions for annotator recruitment, potentiallyoversimplifying cultural identities and conflatingculture with nationality due to practical constraintslike annotator availability. We acknowledge thatsome cultural concepts may lack local terms thatcan be effectively represented in English letters8. Hence, our use of English-only data may alsomiss key cultural nuances available only in nativelanguages. In such cases, collecting annotationsin native languages would help mitigate this issue.However, we emphasize that our benchmark,despite being in English, is already challengingenough for the models, as evidenced by thesignificant disparity in model performance acrosscultures. Although our dataset aims for culturaldiversity, it does not capture the full spectrum ofglobal cultural diversity. Future work will expandthe dataset to represent diverse cultures and regionsmore broadly and develop multilingual datasets forgreater inclusivity. Challenges in collecting culturally informativedataCollecting culturally rich content from di-verse annotators proved challenging, particularlybecause the images and concepts were limited tothose available on English-language websites. Thisrestriction likely omits important cultural details.Allowing annotators to skip inadequate images didnot fully overcome the drawbacks of limited im-age quality, impacting the depth of the questionscreated.",
  "Ethical Considerations": "Our CULTURALVQA benchmark involves cultur-ally specific questions and answers, developed byprofessional annotators from the relevant countries.We sought wide cultural representation by engag-ing with three different communities, compensat-ing annotators at $10-15 per hour for both includedand excluded contributions after pilot testing. Thisreflects our best effort to maintain fairness and in-clusivity in our data collection process.Despite these efforts, we recognize our ap-proachs limitation in equating cultural groupswith national borders, potentially overlooking the complex realities of minority and diasporacommunities. We urge future research to explorefiner distinctions within cultural groups to enhancerepresentation.Although we have rigorouslytried to remove biases, some subjective contentmay persist; however, a substantial portion of thedataset has been verified as unbiased (see App. C).We acknowledge these constraints but are hopefulthat our work will advance the understanding ofcultural nuances in VLMs.",
  "Acknowledgements": "We would like to extend our gratitude to David Ife-oluwa Adelani for connecting us with Masakhane,Frat ncel for assisting with annotators in Turkey,Saba Ahamadi for securing annotators from Iran,and Qian Yang for sourcing annotators from China.We also appreciate the valuable feedback providedby Ibrahim Alabdulmohsin on the early draft. Thetechnical support from the Mila IDT team in man-aging the computational infrastructure is greatlyappreciated. We would also like to thank ChrisEmezue for his assistance in helping with pay-ments for the annotators. Additionally, AishwaryaAgrawal received support from the Canada CIFARAI Chair award throughout this project. KarolinaStanczak was supported by the Mila P2v5 grantand the Mila-Samsung grant. This project was gen-erously funded by a research grant from Google. Muhammad Farid Adilazuarda, Sagnik Mukherjee, Prad-hyumna Lavania, Siddhant Singh, Ashutosh Dwivedi, Al-ham Fikri Aji, Jacki ONeill, Ashutosh Modi, and MonojitChoudhury. 2024. Towards measuring and modeling cul-ture in LLMs: A survey. Preprint, arXiv:2403.15412.",
  "Anthropic. 2024.Claude 3.5 sonnet": "Yujin Baek, ChaeHun Park, Jaeseok Kim, Yu-Jung Heo, Du-Seong Chang, and Jaegul Choo. 2024. Evaluating visualand cultural interpretation: The k-viscuit benchmark withhuman-vlm collaboration. Preprint, arXiv:2406.16469. Emily M. Bender and Batya Friedman. 2018. Data statementsfor natural language processing: Toward mitigating sys-tem bias and enabling better science. Transactions of theAssociation for Computational Linguistics, 6:587604. Lucas Beyer, Andreas Steiner, Andr Susano Pinto, AlexanderKolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann,Ibrahim Alabdulmohsin, Michael Tschannen, EmanueleBugliarello, Thomas Unterthiner, Daniel Keysers, SkandaKoppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko,Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisen-schlos, Rishabh Kabra, Matthias Bauer, Matko Bonjak,Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi,Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harm-sen, and Xiaohua Zhai. 2024. Paligemma: A versatile 3bvlm for transfer. Preprint, arXiv:2407.07726. Mehar Bhatia, Sahithya Ravi, Aditya Chinchure, EunjeongHwang, and Vered Shwartz. 2024. From local concepts touniversals: Evaluating the multicultural understanding ofvision-language models. Preprint, arXiv:2407.00263. Emanuele Bugliarello, Fangyu Liu, Jonas Pfeiffer, Siva Reddy,Desmond Elliott, Edoardo Maria Ponti, and Ivan Vulic.2022. IGLUE: A benchmark for transfer learning acrossmodalities, tasks, and languages. In Proceedings of the39th International Conference on Machine Learning, vol-ume 162 of Proceedings of Machine Learning Research,pages 23702392. PMLR.",
  "Olena Burda-Lassen, Aman Chadha, Shashank Goswami,and Vinija Jain. 2024. How culturally aware are vision-language models? Preprint, arXiv:2405.17475": "Soravit Changpinyo, Linting Xue, Michal Yarom, AshishThapliyal, Idan Szpektor, Julien Amelot, Xi Chen, andRadu Soricut. 2023. MaXM: Towards multilingual visualquestion answering. In Findings of the Association forComputational Linguistics: EMNLP 2023, pages 26672682, Singapore. Association for Computational Linguis-tics. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhang-wei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, JiapengLuo, Zheng Ma, et al. 2024. How far are we to GPT-4V?Closing the gap to commercial multimodal models withopen-source suites. arXiv preprint arXiv:2404.16821. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph,Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, MostafaDehghani, Siddhartha Brahma, Albert Webson, Shixi-ang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen,Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat,Kevin Robinson, Dasha Valter, Sharan Narang, GauravMishra, Adams Yu, Vincent Zhao, Yanping Huang, An-drew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean,Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,and Jason Wei. 2022. Scaling instruction-finetuned lan-guage models. Preprint, arXiv:2210.11416. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng HuatTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pas-cale N Fung, and Steven Hoi. 2024. InstructBLIP: Towardsgeneral-purpose vision-language models with instructiontuning. Advances in Neural Information Processing Sys-tems, 36. William Gaviria Rojas, Sudnya Diamos, Keertan Kini, DavidKanter, Vijay Janapa Reddi, and Cody Coleman. 2022. Thedollar street dataset: Images representing the geographicand socioeconomic diversity of the world. In Advances inNeural Information Processing Systems, volume 35, pages1297912990. Curran Associates, Inc.",
  "Google. 2017. Google lens api. Accessed: 2017-10-4": "Christian Haerpfer, Ronald Inglehart, Alejandro Moreno,Christian Welzel, Kseniya Kizilova, Marta Lagos, JuanDiez-Medrano, Pippa Norris, Eduard Ponarin, and Bi Pura-nen. 2022. World Values Survey: Round seven - country-pooled datafile version 3.0. Madrid, Spain & Vienna, Aus-tria: JD Systems Institute & WVSA Secretariat. Daniel Hershcovich, Stella Frank, Heather Lent, Miryamde Lhoneux, Mostafa Abdou, Stephanie Brandl, EmanueleBugliarello, Laura Cabello Piqueras, Ilias Chalkidis, Ruix-iang Cui, Constanza Fierro, Katerina Margatina, PhillipRust, and Anders Sgaard. 2022. Challenges and strategiesin cross-cultural NLP. In Proceedings of the 60th AnnualMeeting of the Association for Computational Linguistics(Volume 1: Long Papers), pages 69977013, Dublin, Ire-land. Association for Computational Linguistics. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,and Yejin Choi. 2021. CLIPScore: A reference-free evalu-ation metric for image captioning. In Proceedings of the2021 Conference on Empirical Methods in Natural Lan-guage Processing, pages 75147528, Online and PuntaCana, Dominican Republic. Association for ComputationalLinguistics.",
  "BLIP-2: Bootstrapping language-image pre-training withfrozen image encoders and large language models. In Inter-national Conference on Machine Learning, pages 1973019742": "Wenyan Li, Xinyu Zhang, Jiaang Li, Qiwei Peng, RaphaelTang, Li Zhou, Weijia Zhang, Guimin Hu, Yifei Yuan,Anders Sgaard, Daniel Hershcovich, and Desmond El-liott. 2024.Foodieqa: A multimodal dataset for fine-grained understanding of chinese food culture. Preprint,arXiv:2406.11030. Tsung-Yi Lin, Michael Maire, Serge Belongie, JamesHays, Pietro Perona, Deva Ramanan, Piotr Dollr, andC. Lawrence Zitnick. 2014. Microsoft COCO: Commonobjects in context. In Computer Vision ECCV 2014,pages 740755, Cham. Springer International Publishing. Fangyu Liu, Emanuele Bugliarello, Edoardo Maria Ponti, SivaReddy, Nigel Collier, and Desmond Elliott. 2021. Visu-ally grounded reasoning across languages and cultures. InProceedings of the 2021 Conference on Empirical Meth-ods in Natural Language Processing, pages 1046710485,Online and Punta Cana, Dominican Republic. Associationfor Computational Linguistics.",
  "Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, ShaohanHuang, Shuming Ma, and Furu Wei. 2023. Kosmos-2:Grounding multimodal large language models to the world.arXiv preprint arXiv:2306.14824": "Angline Pouget, Lucas Beyer, Emanuele Bugliarello, XiaoWang, Andreas Peter Steiner, Xiaohua Zhai, and IbrahimAlabdulmohsin. 2024.No filter: Cultural and socioe-conomic diversity in contrastive vision-language models.arXiv preprint arXiv:22405.13777. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,Gabriel Goh, Sandhini Agarwal, Girish Sastry, AmandaAskell, Pamela Mishkin, Jack Clark, Gretchen Krueger,and Ilya Sutskever. 2021. Learning transferable visualmodels from natural language supervision. In Proceedingsof the 38th International Conference on Machine Learning,volume 139 of Proceedings of Machine Learning Research,pages 87488763. PMLR. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, andPeter J. Liu. 2020. Exploring the limits of transfer learningwith a unified text-to-text transformer. J. Mach. Learn.Res., 21(1). David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo,Teresa Lynn, Injy Hamed, Aditya Nanda Kishore, AishikMandal, Alina Dragonetti, Artem Abzaliev, Atnafu Lam-bebo Tonja, Bontu Fufa Balcha, Chenxi Whitehouse, Chris-tian Salamea, Dan John Velasco, David Ifeoluwa Ade-lani, David Le Meur, Emilio Villa-Cueva, Fajri Koto,Fauzan Farooqui, Frederico Belcavello, Ganzorig Bat-nasan, Gisela Vallejo, Grainne Caulfield, Guido Ivetta,Haiyue Song, Henok Biadglign Ademtew, Hernn Maina,Holy Lovenia, Israel Abebe Azime, Jan Christian BlaiseCruz, Jay Gala, Jiahui Geng, Jesus-German Ortiz-Barajas,Jinheon Baek, Jocelyn Dunstan, Laura Alonso Ale-many, Kumaranage Ravindu Yasas Nagasinghe, LucianaBenotti, Luis Fernando DHaro, Marcelo Viridiano, Mar-cos Estecha-Garitagoitia, Maria Camila Buitrago Cabr-era, Mario Rodrguez-Cantelar, Mlanie Jouitteau, Mi-hail Mihaylov, Mohamed Fazli Mohamed Imam, Muham-mad Farid Adilazuarda, Munkhjargal Gochoo, Munkh-Erdene Otgonbold, Naome Etori, Olivier Niyomugisha, Paula Mnica Silva, Pranjal Chitale, Raj Dabre, RendiChevi, Ruochen Zhang, Ryandito Diandaru, SamuelCahyawijaya, Santiago Gngora, Soyeong Jeong, Sukan-nya Purkayastha, Tatsuki Kuribayashi, Thanmay Jayaku-mar, Tiago Timponi Torrent, Toqeer Ehsan, VladimirAraujo, Yova Kementchedjhieva, Zara Burzo, Zheng WeiLim, Zheng Xin Yong, Oana Ignat, Joan Nwatu, RadaMihalcea, Thamar Solorio, and Alham Fikri Aji. 2024.CVQA: Culturally-diverse multilingual visual question an-swering benchmark. arXiv preprint arXiv:22406.05967,arXiv:2406.05967. Ashish V. Thapliyal, Jordi Pont Tuset, Xi Chen, and Radu Sori-cut. 2022. Crossmodal-3600: A massively multilingualmultimodal evaluation dataset. In Proceedings of the 2022Conference on Empirical Methods in Natural LanguageProcessing, pages 715729, Abu Dhabi, United Arab Emi-rates. Association for Computational Linguistics. T. Weyand, A. Araujo, B. Cao, and J. Sim. 2020. Google land-marks dataset v2 a large-scale benchmark for instance-level recognition and retrieval. In 2020 IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR),pages 25722581, Los Alamitos, CA, USA. IEEE Com-puter Society. Da Yin, Liunian Harold Li, Ziniu Hu, Nanyun Peng, andKai-Wei Chang. 2021. Broaden the vision: Geo-diversevisual commonsense reasoning.In Proceedings of the2021 Conference on Empirical Methods in Natural Lan-guage Processing, pages 21152129, Online and PuntaCana, Dominican Republic. Association for ComputationalLinguistics. Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir YitzhakGadre, Jesse Dodge, Alex Fang, Youngjae Yu, LudwigSchmidt, William Yang Wang, and Yejin Choi. 2023. Mul-timodal C4: An open, billion-scale corpus of images inter-leaved with text. arXiv preprint arXiv:2304.06939.",
  "We provide a data statement (Bender and Friedman,2018) to document the generation and provenanceof CULTURALVQA": "Curation RationaleCULTURALVQA bench-mark is designed to evaluate VLMs cultural under-standing capacities across various cultures. Theimages are sourced from the CANDLE dataset(Nguyen et al., 2023), which offers a comprehen-sive collection of Cultural Commonsense Knowl-edge (CCSK) from the C4 corpus (Raffel et al.,2020), consisting of 1.1 million entries each linkedto relevant CCSK data via URLs to webpages. An-notators writing questions and answers for thisproject are recruited through the MTurk platform,an African NLP organization, and an internationalacademic AI research institute.",
  "questions and answers. We build our dataset inEnglish to disentangle multicultural understandingfrom multilingual comprehension": "Annotator DemographicsAll annotators comefrom the following 11 countries: China, Turkey,Iran, Ethiopia, Nigeria, Rwanda, Germany, USA,Canada, Brazil, and India. Initially, we attemptedto engage professional annotators from the AmazonMechanical Turk (MTurk) platform. However, weencountered challenges in finding sufficient pres-ence of annotators from some of the targeted coun-tries. Therefore, we expanded our search to othercommunities with a broad cultural representation,including Masakhane, an African NLP organiza-tion, and Mila, an international academic AI re-search institute. All annotators are either natives ofthe country they annotated for or have resided therefor at least 18 years, ensuring they have sufficientcultural context and lived experiences required forthe task. We conducted multiple pilot rounds toensure that annotators adhere to our guidelines andare fluent in English. Other demographics such asage and gender are unknown. All annotators werecompensated at an hourly rate of 10-15$ per hourdepending on a task and the number of completedHITs. The number of unique annotators from eachcountry can be found in Tab. 3.",
  "BImage Filtering": "Given the potential noise inherent in an imagedataset derived from web scraping, we implementheuristic filters to refine our selection. First, weapply aspect ratio filtering, retaining only imageswith an aspect ratio between 0.5 and 2, effectivelyremoving many banner-like advertisements. Next,we discard any image smaller than 100 pixels dueto their inadequate detail for analysis. We also ex-clude images containing specific keywords suchas logo and social, which typically denote non-relevant graphics or branding content. To guarantee the high quality of images includedin our benchmark, we first employed CLIP simi-larity (Hessel et al., 2021) to rank the remainingimages for cultural relevance. Based on a man-ual annotation of images for 200 CCSK assertions,to assess their relevance to the CCSK, we set athreshold of 23 to ensure culturally relevant images(precision = 0.92, recall = 0.96). Images below thisscore were discarded. Higher-scoring images weremore likely to be selected for question creation.",
  "CStereotypes and Biases": "To ascertain the representational fairness of ourdataset, we implemented a Sentence-Level Stereo-type Classifier,9 a transformer-based model, fordetecting stereotypical content within the datasetsquestions. This models efficacy in classifying sen-tences based on the presence of stereotypes or anti-stereotypes was evaluated across various dimen-sions including race, gender, religion, and profes-sion. The classifier identified relatively few stereo-typical instances: 69 cases pertained to race, 44 togender, 22 to religion, and 8 to profession. In con-trast, anti-stereotypical content was more prevalent,with 169 cases for race, 25 for religion, 23 for gen-der, and 7 for profession. A significant portion ofthe data, 923 instances, did not correlate with anystereotypical or anti-stereotypical categories, un-derscoring the minimal presence of biased contentin the dataset. These findings support the datasetsutility in facilitating unbiased and culturally com-prehensive studies.",
  "STRUCTBLIP (Dai et al., 2024), MBLIP (Geigleet al., 2023) PALLIGEMMA (Beyer et al., 2024)": "LLAVA1.5 (Liu et al., 2023), LLAVA_NEXT (Liuet al., 2024), IDEFICS2 (Laurenon et al., 2024),and INTERN-VL 1.5 (Chen et al., 2024). Thesemodels were selected based on their release yearand parameter size (3 to 25 billion) to test how theseaspects affect cultural understanding. INSTRUCT-BLIP, fine-tuned with instruction tuning, is com-pared to BLIP2 to see if instruction tuning enhancescultural understanding. IDEFICS2, with 8 billionparameters, is evaluated for its performance onopen datasets, surpassing larger models. INTERN-VL 1.5, with 25 billion parameters, bridges thegap between open-source and proprietary mod-els, showing strong multimodal benchmark per-formance, even outperforming proprietary modelson some benchmarks. For each model, we usethe default text-generation parameters as found intheir HuggingFace code repository which includea greedy decoding strategy with the temperatureset to 1. Finally, we also evaluate closed-sourcemodels GPT-4 (GPT-4o), GEMINI (Gemini-Pro-Vision 1.0) and CLAUDE (Claude 3.5 (Anthropic,2024)) using their API endpoints.",
  "We conduct a few-shot evaluation of GPT-4 (bestperforming model) to determine whether the CUL-": "TURALVQA benchmark can be solved by guidingthe models with a few examples. In this setup, weinclude one example per country (11 examples to-tal). The few-shot prompt is detailed in App. J.Our analysis () reveals that few-shot prompt-ing does not consistently improve performanceover zero-shot, despite examples from all countries.While some countries like Germany, Ethiopia, andNigeria showed improvements (3-8%), others suchas Brazil, China, India, and Rwanda experiencedperformance drops or minimal gains. This suggests that few-shot prompting may not be uniformly ben-eficial across cultural contexts and that GPT-4sperformance on culturally nuanced tasks largelydepends on its pre-existing knowledge. These re-sults highlight the challenging nature of CULTUR-",
  "FAnalysis of Performance Across CulturalFacets": "To better understand the performance disparities be-tween the different facets, we categorise the image-question-answer triplets in our dataset into morefine-grained categories based on the aspect of thefacet being probed in the question. More specifi-cally, the sub-categories include Type / Name, Lo-cation / Region, Customs associated, Ingredients,Taste, Other (for food, clothing, and drinks facets),and Beliefs and Customs, Location / Landmarks,Celebration, Music / Instruments, Sports, People /Historical Figures, Other (for traditions and ritualsfacets). These fine-grained categories are inspiredby the categorization in MaRVL (Liu et al., 2021)for the traditions and rituals facets and FoodieQA(Li et al., 2024) for the food, drink, and clothingfacets. We prompt GPT-4 with question, answer,and the original facet along with a list of fine-grained categories and several in-context examplesto perform this categorization. A few examplesfrom this exercise are shown in . We re-port the number of image-question-answer tripletsbelonging to each fine-grained category in .While the most popular fine-grained category forthe food, drink, and clothing facets corresponds toidentifying the type or name of the entity, a signif-icant proportion of the samples (61.8% for Food,61.3% for Drink, 52.7% for Clothing) require moredetailed knowledge such as associated customs.The samples from traditions and rituals requiremore diverse knowledge, with the sub-categoriesof Beliefs and Customs, Location / Landmarks, andCelebration being the most prevalent.We summarize the results obtained for differentsubcategories for GPT-4 and InternVL in Tab. 6and Tab. 7. From these results, we observe thatamong the food, clothing, and drink facets, on av-erage, models tend to perform better on questionsthat involve identifying the name, location, and in-gredients (only applicable to food and drink facets)of the concept. However, they perform relativelypoorly on questions probing associated customs",
  "TraditionsQ: What is the name of the national anthem related to this flag?A: Oh CanadaMusic/Instruments": ": Examples of subcategories assigned by GPT-4 for different question-answer pairs from each facet. \"Q\"represents the question, and \"A\" represents the answer in the \"Qustion/Answer input to GPT-4 \" column. : Breakdown of facets into various subcategories. The plot on the left illustrates the detailed subcategoriesfor the Food, Drink, and Clothing facets, while the plot on the right presents the corresponding breakdown for theRituals and Traditions facets.",
  "Why do certain models perform better on spe-cific cultural facets?We analyze three factorsthat could lead to disparity in a models perfor-mance across facets. From Tab. 6 and Tab. 7, we": "observe that there are stark differences in perfor-mance across different fine-grained categories. Forinstance, the relatively better performance of GPT-4 on questions about traditions and beliefs can beattributed to a couple of fine-grained categories,such as celebrations (Q: For which holiday seasonare these items in the image popular?, A: Christ-mas), landmarks (Q: What is the name of thisfamous Hindu temple shown above?, A: Janakitemple), and sports (Q: What are the people in",
  ": Scatter plot of GPT-4V performance versusaverage question length across different countries": "the picture practicing?, A: Wushu).Another source of disparity in the performancecould be the disparity in inherent difficulty levelsof questions belonging to each facet. To inves-tigate this, we calculate human performance foreach facet and observe minimal differences (per-formance for food - 74%, clothing - 72.7%, drinks - 74.5%, rituals - 71.1%, traditions - 73.7%), sug-gesting that this is unlikely to be the case.Finally, we investigate if the disparity in modelperformance across facets is correlated with the rep-resentation of each facet in the models pre-trainingdata. We conduct this study for the best-performingopen-source model InternVL. We randomly sam-ple 1.3 million data points from LAION (the pre-training data for Intern-VL) and check how manysamples in the pretraining data contain at least oneanswer string from our benchmark correspondingto a given facet. Once we get the counts, we normal-ize them by the total number of answers within eachfacet, since facets with more number of answerswill naturally have more matches in the pretrainingdata. The relative percentages for each facet areas follows: Food (46.6%), Clothing (6.5%), Drink(7.8%), Rituals (25.1%), Traditions (13.8%), andOthers (0.2%). We observe that the food facet hasthe highest representation, followed by rituals andtraditions. However, this does not align with theperformance trends observed for Intern-VL, wherethe highest performance is seen for clothing, fol-lowed by traditions, rituals, food, and drink. Thissuggests that factors beyond the occurrence of con-cepts in the pre-training data contribute to the dis-parity in model performance across different facets.Understanding these factors presents an intriguingarea for future research.",
  "GQualitative Analysis of HumanPerformance": "We qualitatively investigate why countries likeNigeria and Rwanda exhibit relatively lower humanperformance. We identify two major contributingfactors. First, we have used country as a proxyfor a cultural group, which might be particularlyinaccurate for these countries. There may be sub- cultures within these countries where the same con-cept holds different meanings, leading to variedinterpretations. This is especially relevant for visu-ally similar items. For example, for the question:Whats the item that the people are beating calledin the local parlance?, the answers received arel, Igba, and drums. The first two are also types ofdrums, with the former used in Yoruba culture andthe latter in Igbo culture. Depending on the respon-dents cultural background, their answers may havevaried. Secondly, we also found that annotators of-ten disagreed on questions that required identifyinggeographical locations. For example, for the ques-tion: What part of Rwanda are the crops shown inthe image grown more? the answers are Gisagara,Gicumbi District, and Nyamagabe. These typesof questions, especially for Rwanda, might havecontributed to the lower performance",
  "HHuman Judgment of Model Predictions": "We perform human evaluation of model responsesfor questions from India. Five human annotatorsrate each answer on a scale of 1 to 5: 1 (completelycorrect), 2 (correct but not culturally specific), 3(correct but not precise), 4 (correct but neither cul-turally specific nor precise), and 5 (completely in-correct). The instructions given to the annotatorscan be found in .",
  "IBehind the scenes: Journey of howCULTURALVQA came into place": "The journey of creating the CulturalVQA datasetwas shaped by various design decisions, challenges,and lessons learned. This section aims to outlineour motivations, initial ideas, and the obstacles weencountered, with the hope of guiding others whoare interested in building similar datasets. Motivation and Initial IdeaThe project wasprimarily motivated by the lack of comprehensivebenchmarks to evaluate cultural understanding invision-language models (VLMs) across a broadset of countries. We wanted to create a resourcethat would holistically test these models culturalknowledge. We were looking into a source forobtaining culturally diverse images. The initialspark for the dataset came from the CCSK (Nguyenet al., 2023) and MMC4 papers (Zhu et al., 2023),inspiring exploration into leveraging the images inthe C4 corpus (Raffel et al., 2020).",
  "Early Efforts and ChallengesIn December2023 and January 2024, we focused on scraping,": "filtering, and conducting quality analysis on the im-ages from the C4 corpus filtered using cultural com-monsense knowledge assertions from CANDLE.Initially, our goal was to create a large-scale datasetsemi-automatically, covering about 100 countries.We wanted to leverage LLM-based question gener-ation methods to achieve this. By March 2024, wehad built an early version of CULTURALVQA thatincluded 12 countries. We used GPT-4 to generatecultural questions based on the CCSK informa-tion and metadata like captions, object informationand entity tags from Google Lens (Google, 2017).However, we soon found several issues with thisdataset. For instance, GPT-4 performed exception-ally well on the dataset achieving results above 90%for countries like India, Germany, and Poland. Theopen-source models like LLaVA-Next (Liu et al.,2024) were not very far behind. These results echothe observations by Baek et al. (2024), who build adataset using a similar method for Korean cultureand observe that models like GPT-4 and Geminisurpass human performance on their dataset. Onfurther analysing the questions, we found that theyrequired only a coarse-grained understanding ofvisual content and did not adequately probe forcultural nuance. This highlighted the limitationsof building such geo-diverse and cultural datasetsusing existing LLMs. Hence, we reevaluated ourapproach, and we decided to involve human anno-tators to enhance cultural depth and authenticity. Note on Filtering ImagesWe aimed to use au-tomated methods to create an image corpus forbuilding the CULTURALVQA benchmark. Thisidea originated from the need for a large-scaledataset, which would be impractical to gather solelythrough human efforts. The internet, as a vast anddiverse source of imagery, provided an opportu-nity to build a culturally rich image corpus. How-ever, since we decided to involve human evalua-tors, our final approach was not entirely automated;it incorporated human input to further refine thedataset. This human refinement led to the removalof 19.64% of the images, highlighting that auto-mated methods alone are still insufficient for con-structing such high-quality datasets. Future workcould explore methods to bridge this gap.Even though we obtain a culturally relevant cor-pus from our image selection method, leveragingonly the English portion of Common Crawl has itslimitations, as it predominantly contains popularconcepts from well-represented cultures. We hy-",
  "pothesize that utilizing the multilingual segmentsof Common Crawl could help uncover more rarecultural concepts and corresponding images, lead-ing to a more diverse and inclusive dataset": "Annotator Selection and Pilot StudiesWe thenexplored crowdsourcing platforms and ultimatelychose Amazon Mechanical Turk (MTurk) due toits easy-to-use interface. We also considered Pro-lific, but its lack of interface customization led usback to MTurk. Our initial pilot began with Indiawhere we spent about a month conducting pilotsto debate and fine-tune the guidelines. We believethis is a very important step to collect high-qualitydata and it is worth spending a lot of time on this.Once we were satisfied by the guidelines we aimedfor larger-scale annotation for multiple countries.Unfortunately, we quickly discovered a major chal-lenge: MTurk had almost no active annotators forcountries outside the US, Canada, India, and Brazil.We tried to collect data from the Philippines, In-donesia, Japan, Germany, France, China, Iran, andMorocco, but found almost no willing annotators.This taught us the difficulty of recruiting diverse an-notators through traditional platforms. This is alsoan important bottleneck for building representativedatasets required to build inclusive models. Shifting to Community InvolvementTo ad-dress the limitations of cultural representation, weturned to more diverse communities by partneringwith Mila and Masakhane for annotations. We con-ducted several workshops and maintained ongoingcommunication with annotators through extensiveemail threads to provide consistent feedback. How-ever, we faced challenges with providing timelyfeedback to MTurk participants compared to ourdirect community engagements, which resulted indiscarding a significant amount of data from MTurkdue to poor adherence to guidelines.Managing a large group of annotators across dif-ferent time zones added further complexity, empha-sizing the need for scalable platforms or outsourc-ing to enhance efficiency. After completing thepaper, we discovered platforms like CloudConnect,which have been used in works such as (Bhatiaet al., 2024) to collect data from a larger number ofcountries. However, they also faced similar chal-lenges in obtaining high-quality data, with poorcommunication with annotators leading to the re-jection of numerous data points. This highlightsthe common struggle of balancing scale and qualityin annotation processes across diverse regions. Key TakeawaysBuilding the CulturalVQAdataset was a challenging yet rewarding journey.What began as an automated, LLM-driven ap-proach evolved into one deeply rooted in humanannotation. Our biggest takeaway is that humaninput remains irreplaceable in creating culturallyrich datasetsat least for now. Additionally, lever-aging a scalable platform with a dedicated, diversepool of annotators, combined with effective andtimely communication, is essential for achievinghigh-quality results. Choosing the right annotatorsis critical, as their contributions directly impact thedatasets quality. Conducting multiple pilot stud-ies was invaluable in helping us identify the bestannotators and refine our process. By sharing our experiencesfrom initial ideasto refining our annotation methodswe hope toprovide guidance to others facing similar chal-lenges in creating culturally diverse benchmarks forVLMs. We believe that our journey offers usefulinsights for building more inclusive, high-qualitydatasets in the future.",
  "System prompt used for the LAVE evalu-ation metric": "You are an expert cultural anthropologisttasked with evaluating the correctness ofcandidate answers for cultural visual ques-tion answering. Given a question, a set ofreference answers by an expert, and a can-didate answer by a model, please rate thecandidate answers correctness. Use a scaleof 1-2, where 1 indicates an incorrect, irrel-evant, or imprecise answer, and 2 indicatesa correct and precise answer. Specify therating in the format rating=X, where X iseither 1 or 2. Also, provide the rationale foryour rating.",
  "the instructions, ensuring that annotators adheredto the criteria required for writing answers. The in-structions provided to the annotators for collectinganswers are detailed in": ": The instructions given to annotators to evaluate answers generated by various models. To assist withwriting, we provide clear guidelines and offer multiple examples showcasing both good and poor practices. : Instructions given to annotators from India to write questions and answers for images. Similar instructions,with different examples, were given to annotators from other countries. To assist with writing, we provide a briefvideo detailing our task and guidelines, along with multiple examples showcasing both good and poor practices(examples not included here). : The instructions given to annotators from India to write answers for questions collected for images.Similar instructions, with different examples, were given to annotators from other countries. To assist with writing,we provide clear guidelines and offer multiple examples showcasing both good and poor practices."
}