{
  "Abstract": "This study explores the proactive ability ofLLMs to seek user support. We propose met-rics to evaluate the trade-off between perfor-mance improvements and user burden, and in-vestigate whether LLMs can determine whento request help under varying information avail-ability. Our experiments show that withoutexternal feedback, many LLMs struggle torecognize their need for user support.Thefindings highlight the importance of exter-nal signals and provide insights for future re-search on improving support-seeking strate-gies.Source code:",
  "Introduction": "The impressive instruction-following (Wei et al.,2021) abilities of large language models (LLMs)have enabled their out-of-the-box usage to solveproblems. However, these models generate hallu-cinated content (Rawte et al., 2023) or incorrectpredictions in their efforts to fulfill user instruc-tions, which undermines their reliability.When LLMs generate incorrect outputs for agiven instruction, the issue can be examined frommultiple perspectives. One is that the model simplylacks the competence to satisfy the instruction, sug-gesting a straightforward solution: enhancing themodels capabilities, which is the focus of most pre-vious research. Another is that the model could ac-tually solve the task with additional support. For in-stance, Pourreza and Rafiei (2023) found that mod-els often fail due to underspecified natural languagequeries. Similarly, Li et al. (2024) showed thatwhile GPT-4 struggles initially, its performance canimprove by up to 20.01% with human-annotatedexternal knowledge. In such cases, models shouldproactively seek help rather than attempting to sat-isfy instructions with insufficient information.",
  "*Equal contributionEqual advisorship": "Direct AskWrite then Ask Need Help? Instruction ( ) Need Help? Instruction ( ) Execute then Ask DB I Need Help! Instruction ( ) Output ( ) Results ( ) : Overview of our experiments on text-to-SQL.LLMs struggle to determine when they need help basedsolely on the instruction (x) or their output (y). Theyrequire external feedback, such as the execution results(r) from the database, to outperform random baselines. Motivated by these considerations, we aim toinvestigate whether LLMs can identify when toask for user support. Since providing such sup-port requires additional effort from users, there isan inherent trade-off between LLM performanceimprovement from user support and user bur-den. Therefore, we seek to answer the followingresearch questions: RQ1: How can we design eval-uation metrics to quantify this trade-off? RQ2:How effectively do LLMs manage this trade-off,and what strategies are effective in improving it?In this work, we focus on the text-to-SQL taskas a case study to empirically investigate the afore-mentioned research questions. We chose the text-to-SQL task for several reasons: (1) Its promis-ing applicability, empowering lay users to retrievedata with natural language queries. (2) The inher-ent ambiguity in some natural language queries,leading to uncertainty in the generation of SQLcode (Pourreza and Rafiei, 2023), making it suit-able for scenarios where additional user supportis beneficial. (3) There exists a large-scale BIRDdataset (Li et al., 2024) with human-annotated ex-ternal knowledge, providing a valuable source ofuser support for our empirical investigation.",
  "General Setup": "Consider an LLM f parameterized by , along witha prompt template p(). Given a natural languageinstruction x, we use z to represent support, whichshould enhance the LLMs ability to fulfill x. For-mally, yz = f(p(x, z) | ) is more likely to satisfyx compared to y = f(p(x) | ). We denote the\"ask for support\" signal emitted by the LLM as a,defined as a confidence score in the range ,where 1 indicates an absolute need for support. Athreshold is then used to determine whether torequest z. In practice, a could also be a natural lan-guage request specifying the type of support neededby the LLM, which we leave for future work.",
  "i=1(h(yi, yi,z) h(yi, yi))": "User Burden (%) Execution Accuracy (%) Delta-Burden Curve (DBC) RandomDirect AskWrite then AskExecute then Ask Recall of Asking for Support (%) Precision of Asking for Support (%) PR Curve of Asking for Support RandomDirect AskWrite then AskExecute then Ask User Burden (%) Flip Rate (%) Flip Rate Curve (FRC) RandomDirect AskWrite then AskExecute then Ask",
  "Methods for Seeking Support": "We design a prompt template pask() to enableLLMs to request support by a = s(f(pask(w) | )).Here, w represents the textual information that theLLM f uses to determine whether it needs to seeksupport, and s is the scoring function that convertsthe probability distribution of output tokens into aconfidence score a . We propose methodswith varying compositions of w to explore the in-formation LLMs require to achieve better trade-offunder DBC. Note that pask remains the same acrossall methods to minimize prompt engineering. Anoverview of these methods is shown in .Direct Ask (DA): w = (db, x), composed ofdatabase schema db and user data requirement x.Write then Ask (WA): w = (db, x, y), where theLLM generates the SQL code y = f(p(db, x) | )first and then use this self-generated output as theadditional information in w.Execute then Ask (EA): w = (db, x, y, r), wherethe execution results r is returned by the databaseby executing LLM-generated SQL y.",
  "Dataset": "We use BIRD (Li et al., 2024), which includeshuman-annotated external knowledge that servesas z. For example, z might be domain-specificknowledge, such as how to calculate financial in-dicators from database values. The instruction xrepresents the users data requirements, paired withthe ground truth SQL y. It uses Execution Accu-racy (EX) as the evaluation metric, where h(yi, yi)is defined as 1(ri = ri). Here, ri is the SQL ex-ecution result of yi, and ri is the execution resultof yi. Simply put, EX is the proportion of testinginstances where ri and ri are identical.",
  "Implementation": "For open-weight LLMs, we use WizardCoder-34B(Luoetal.,2023),Llama-3-70b-chat,DeepSeek-Coder-33B (Guo et al., 2024), andMixtral-8x22B (Jiang et al., 2024) for diversity ofdifferent LLM families. For closed-source LLMs,we use gpt-3.5-turbo-0125, gpt-4-turbo-2024-04-09, and gpt-4o-2024-05-13 (OpenAI, 2023). Theprompt pask(w) (included in Appendix A) instructsthe model to output a single token Yes/No to in-dicate whether it needs support. We define thescoring function s as the softmax of Yes over logprobabilities of Yes and No to derive a .",
  "Main Results": "Using the formulation in .2, we quantifythe performance of different methods with the AreaUnder Delta-Burden Curve (AUDBC) in .Visualized DBCs are available in the leftmost sub-plots in . Note that AUDBC should only becompared between methods under the same LLM,as it is normalized to the range of by dividingthe area under the curve by the maximum squarearea, which depends on the scale of EX and dif-fers across LLMs, as shown in .There are three major findings: (1) Executionthen Ask consistently improves the performance-burden trade-off for LLMs, although Llama-3-70b-chat fails to outperform the random baseline. (2)The leftmost four LLMs in do not surpassthe random baseline without the assistance of r,indicating that many current LLMs still struggle todetermine the need for support based on x and yalone. (3) Despite this, the rightmost three LLMsoutperform the random baseline with the Writethen Ask (x, y) or even Direct Ask (x) methods.Nevertheless, the inclusion of r remains beneficialfor further enhancing the trade-off between perfor-mance improvement and user burden. Practicalimplications of the third point include the potentialfor cost savings by trading off the execution of y toobtain r in certain resource-constrained scenarios.",
  "#OriginallyWrong": "PR Curve of Asking for Support Similar to howDBC is plotted, one can also adjust the threshold from high to low along the x-axis to plotthe Precision-Recall Curve of Asking for Support.2. For the second ability, we introduce Flip Rate:Flip Rate: This metric is calculated as the propor-tion of instances where the LLMs initially incor-rect answers were corrected after receiving support,divided by the total number of instances wheresupport was requested. Formally, it is defined as:",
  ": Area Under Delta-Burden Curve (AUDBC) with the verbalized token log probabilities approach. Text inbold denotes the method with the best performance, while underlined text means better than random": "Different from defined in .2, this metricemphasizes the efficiency of leveraging support in-stead of the total improvement on the test set. LikeDBC, one may adjust the threshold to plot theFlip Rate Curve (FRC). With the definition of thesetwo abilities, we plot the DBC, PR Curve, and FRCon . Although the Write then Ask methodshows near-random performance in DBC, the PRCurve indicates it achieves better-than-random per-formance in identifying when support is needed.However, its lower Flip Rate suggests it is less ef-ficient in utilizing the support to correct mistakes.These two abilities, represented by the PR Curveand FRC, respectively, balance each other out, re-sulting in near-random performance on the DBC.This finding shows that the ability to identify theneed for support and the ability to utilize that sup-port are distinct. In future work, it is worth explor-ing how to further enhance each of these abilities.",
  "LLMs without Access to Log Probabilities": "Given that not all LLMs provide access to token logprobabilities, we discuss how our method can beadapted for these black-box models. We modifythe prompt template pask to pverb, which instructsthe LLM to output the verbalized confidence scorea directly by specifying the range and meaningof a in pverb (attached in Appendix A.2).In addition to the seven LLMs mentioned in Sec-tion 3.2, we also include two black-box models:gemini-1.0-pro-001 and claude-3-haiku-20240307.The results, shown in , indicate that using",
  "Related Work": "The ability of LLMs to identify the need for supportrelies on their well-calibratedness (Kadavath et al.,2022), which refers to their capacity to recognizeuncertainty. Previous studies focus on enhancingthe calibration of predictions (Xiao et al., 2022;Kuhn et al., 2023), or using verbalized token prob-abilities to achieve better calibration (Tian et al.,2023). Our work extends this line of research byexploring how LLMs can effectively seek user sup-port by leveraging their well-calibrated property.The major distinction between this and existingcalibration studies lies in extending the focus fromidentifying the uncertainty to utilizing support.",
  "Conclusion": "We propose a framework for LLMs to seek support,and evaluate methods on Text-to-SQL generation.Our findings suggest the importance of externalsignals, such as SQL execution results, in helpingLLMs better manage performance-burden trade-off.We further decompose DBC into the ability of iden-tify the need for support and the ability to utilizethe support. Future works may explore a broaderrange of tasks or develop methods to improve boththe identification and utilization of support.",
  "Task Coverage": "The scope of our experiments is limited to the Text-to-SQL task. While this task provides a usefulcase study for evaluating LLMs ability to seek andutilize support, it does not encompass the full rangeof potential applications for LLMs. Future workshould extend the evaluation to a broader set oftasks to ensure the generalizability of our findings.",
  "Dependence on External Feedback": "Our findings indicate that LLMs significantly ben-efit from external signals, such as SQL executionresults. However, this reliance on external feedbackmay not always be feasible in practical applications,where immediate execution or access to externaldata might be limited. Developing methods that en-able LLMs to better manage without such feedbackremains an important area for future exploration. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, KaiDong, Wentao Zhang, Guanting Chen, Xiao Bi,Yu Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wen-feng Liang. 2024. Deepseek-coder: When the largelanguage model meets programming - the rise of codeintelligence. ArXiv, abs/2401.14196. Albert Q. Jiang, Alexandre Sablayrolles, AntoineRoux, Arthur Mensch, Blanche Savary, Chris Bam-ford, Devendra Singh Chaplot, Diego de Las Casas,Emma Bou Hanna, Florian Bressand, GiannaLengyel,Guillaume Bour,Guillaume Lample,Lelio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,Sophia Yang, Szymon Antoniak, Teven Le Scao,Thophile Gervet, Thibaut Lavril, Thomas Wang,Timothe Lacroix, and William El Sayed. 2024. Mix-tral of experts. ArXiv, abs/2401.04088. Saurav Kadavath, Tom Conerly, Amanda Askell, TomHenighan, Dawn Drain, Ethan Perez, NicholasSchiefer, Zac Hatfield-Dodds, Nova DasSarma, EliTran-Johnson, et al. 2022.Language models(mostly) know what they know.arXiv preprintarXiv:2207.05221.",
  "Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023.Semantic uncertainty: Linguistic invariances for un-certainty estimation in natural language generation.arXiv preprint arXiv:2302.09664": "Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, BinhuaLi, Bowen Li, Bailin Wang, Bowen Qin, RuiyingGeng, Nan Huo, et al. 2024. Can llm already serveas a database interface? a big bench for large-scaledatabase grounded text-to-sqls. Advances in NeuralInformation Processing Systems, 36. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xi-ubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma,Qingwei Lin, and Daxin Jiang. 2023. Wizardcoder:Empowering code large language models with evol-instruct. ArXiv, abs/2306.08568.",
  "Vipula Rawte, Amit Sheth, and Amitava Das. 2023. Asurvey of hallucination in large foundation models.arXiv preprint arXiv:2309.05922": "Katherine Tian, Eric Mitchell, Allan Zhou, ArchitSharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn,and Christopher D Manning. 2023. Just ask for cali-bration: Strategies for eliciting calibrated confidencescores from language models fine-tuned with humanfeedback. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing,pages 54335442. Jason Wei, Maarten Bosma, Vincent Y Zhao, KelvinGuu, Adams Wei Yu, Brian Lester, Nan Du, An-drew M Dai, and Quoc V Le. 2021. Finetuned lan-guage models are zero-shot learners. arXiv preprintarXiv:2109.01652. Yuxin Xiao, Paul Pu Liang, Umang Bhatt, WillieNeiswanger, Ruslan Salakhutdinov, and Louis-Philippe Morency. 2022. Uncertainty quantificationwith pre-trained language models: A large-scale em-pirical analysis. arXiv preprint arXiv:2210.04714.",
  "The prompt template pask(w) used to instructLLMs for seeking support is as follows:": "You are currently doing the text-to-SQL task.Based on the information provided ({items}),you have to determine whether additionalhints are required for you to generate the SQLcorrectly to answer the users question. Youshould only ask for additional hints when youactually need them, since you will also be eval-uated based on the number of times you askfor hints, which would be provided by the user.",
  "In this template, the actual contents of {items}": "and {information} depend on the method used. Thecontents are summarized in . For example,w = (db, x, y, r) in Execute then Ask (EA), so{items} will be filled with the four item names and{information} will be replaced by actual informa-tion of the four items. Similarly for Write then Ask(w = (db, x, y)) and Direct Ask (w = (db, x)).",
  "The prompt template for generating verbalizedprobabilities in LLMs without access to token logprobabilities (e.g., Gemini and Claude families):": "You are currently doing the text-to-SQL task.Based on the information provided ({items}),you have to determine whether additionalhints are required for you to generate the SQLcorrectly to answer the users question. Youshould only ask for additional hints when youactually need them, since you will also be eval-uated based on the number of times you askfor hints, which would be provided by the user.",
  "We present visualizations of all performance curvesin , 4, 5, 6, 7, 8, and 9": "User Burden (%) Execution Accuracy (%) Delta-Burden Curve (DBC) RandomDirect AskWrite then AskExecute then Ask Recall of Asking for Support (%) Precision of Asking for Support (%) PR Curve of Asking for Support RandomDirect AskWrite then AskExecute then Ask User Burden (%) Flip Rate (%) Flip Rate Curve (FRC) RandomDirect AskWrite then AskExecute then Ask",
  ": Performance curves of WizardCoder-Python-34B-V1.0": "User Burden (%) 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 Execution Accuracy (%) Delta-Burden Curve (DBC) RandomDirect AskWrite then AskExecute then Ask Recall of Asking for Support (%) 80.0 82.5 85.0 87.5 90.0 92.5 95.0 97.5 100.0 Precision of Asking for Support (%) PR Curve of Asking for Support RandomDirect AskWrite then AskExecute then Ask User Burden (%) Flip Rate (%) Flip Rate Curve (FRC) RandomDirect AskWrite then AskExecute then Ask",
  ": Performance curves of Llama-3-70b-chat-hf": "User Burden (%) 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 Execution Accuracy (%) Delta-Burden Curve (DBC) RandomDirect AskWrite then AskExecute then Ask Recall of Asking for Support (%) Precision of Asking for Support (%) PR Curve of Asking for Support RandomDirect AskWrite then AskExecute then Ask User Burden (%) Flip Rate (%) Flip Rate Curve (FRC) RandomDirect AskWrite then AskExecute then Ask",
  ": Performance curves of deepseek-coder-33b-instruct": "User Burden (%) Execution Accuracy (%) Delta-Burden Curve (DBC) RandomDirect AskWrite then AskExecute then Ask Recall of Asking for Support (%) Precision of Asking for Support (%) PR Curve of Asking for Support RandomDirect AskWrite then AskExecute then Ask User Burden (%) Flip Rate (%) Flip Rate Curve (FRC) RandomDirect AskWrite then AskExecute then Ask",
  ": Performance curves of gpt-3.5-turbo-0125": "User Burden (%) 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 Execution Accuracy (%) Delta-Burden Curve (DBC) RandomDirect AskWrite then AskExecute then Ask Recall of Asking for Support (%) Precision of Asking for Support (%) PR Curve of Asking for Support RandomDirect AskWrite then AskExecute then Ask User Burden (%) Flip Rate (%) Flip Rate Curve (FRC) RandomDirect AskWrite then AskExecute then Ask",
  ": Performance curves of Mixtral-8x22B-Instruct-v0.1": "User Burden (%) 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 Execution Accuracy (%) Delta-Burden Curve (DBC) RandomDirect AskWrite then AskExecute then Ask Recall of Asking for Support (%) Precision of Asking for Support (%) PR Curve of Asking for Support RandomDirect AskWrite then AskExecute then Ask User Burden (%) Flip Rate (%) Flip Rate Curve (FRC) RandomDirect AskWrite then AskExecute then Ask",
  ": Performance curves of gpt-4-turbo-2024-04-09": "User Burden (%) 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Execution Accuracy (%) Delta-Burden Curve (DBC) RandomDirect AskWrite then AskExecute then Ask Recall of Asking for Support (%) Precision of Asking for Support (%) PR Curve of Asking for Support RandomDirect AskWrite then AskExecute then Ask User Burden (%) Flip Rate (%) Flip Rate Curve (FRC) RandomDirect AskWrite then AskExecute then Ask"
}