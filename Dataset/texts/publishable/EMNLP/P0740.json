{
  "Abstract": "Long sequence modeling has gained broad in-terest as large language models (LLMs) con-tinue to advance. Recent research has identi-fied that a large portion of hidden states withinthe key-value caches of Transformer modelscan be discarded (also termed evicted) withoutaffecting the perplexity performance in gen-erating long sequences. However, we showthat these methods, despite preserving perplex-ity performance, often drop information thatis important for solving downstream tasks, aproblem which we call information neglect.To address this issue, we introduce ChunkedInstruction-aware State Eviction (CItruS), anovel modeling technique that integrates theattention preferences useful for a downstreamtask into the eviction process of hidden states.In addition, we design a method for chunkedsequence processing to further improve effi-ciency. Our training-free method exhibits supe-rior performance on long sequence comprehen-sion and retrieval tasks over several strong base-lines under the same memory budget, whilepreserving language modeling perplexity. Thecode and data have been released at",
  "Introduction": "Recent advances in large language models (LLMs)have raised interest in long sequence modeling (Qinet al., 2023; Xiao et al., 2023). Several studies havefound that information relevant to the next tokenprediction task often accumulates in the hiddenrepresentations of just a few tokens, and the atten-tion distributions tend to focus sparsely on thesetokens (Liu et al., 2024; Bai et al., 2024; Wanget al., 2023b). This observation has resulted inmethods that model longer sequences by evictingunnecessary key-value caches during the language",
  "Equal contribution. Work done during the internship atMila Quebec Artificial Intelligence Institute.Corresponding author": ": One sample from attention distributions inthe 16th layer of the Mistral 7B Instruct model appliedto the Qasper dataset. The attention distributions arecalculated from a document context and an instructiontext to the key-value cache. The x-axis represents dif-ferent positions within the key-value cache, while they-axis represents the attention weights. The positionsare reordered by descending attention weights from thecontext, and positions with low attention weights areomitted for clarity.",
  "modeling process (Zhang et al., 2024b; Oren et al.,2024), mostly based on the attention weights eachtoken receives from the following context": "However, these methods achieve limited perfor-mance on downstream tasks that require specificinformation from long documents (e.g., questionanswering), suggesting that they struggle to retainthe detailed information necessary for such tasks.We refer to this condition as the information ne-glect problem. This issue arises because the cacheacquired through state eviction is based only on thelocal document context. There is no explicit signalfor the model to ensure that it is useful for solvingdownstream tasks. Consider , which showstwo attention distributionsone from a documentcontext and one from an instruction promptwhenapplying the Mistral 7B Instruct model to a samplefrom the Qasper dataset. Note that the two differsubstantially in their weighting of positions, sug-gesting that the document context-derived attentionweights may not capture well the task specified by",
  "the instructions.1": "In this paper, we propose to address this in-formation neglect issue by incorporating the in-struction text into the state eviction process. Ourmethod, Chunked Instruction-aware State Evic-tion (CItruS), decomposes long sequence process-ing into two different subprocesses: language mod-eling and task solving. For the language model-ing process, we propose chunked state eviction,splitting the long sequence input into large chunkswhile maintaining a cache that only stores the mostimportant key-value states, which we show allowsthe model to efficiently and effectively encode longdocuments. As for the task-solving process, wepropose an instruction-aware cache, either indepen-dent of or shared with the language modeling cache,which maintains the specific detailed informationrequired to generate responses in downstream set-tings. The instruction-aware cache is then used togenerate the final response for solving the task. Ourapproach can be seen as analogous to ideas fromcognitive science that language and thought canbe disentangled in human language processing (Fe-dorenko and Varley, 2016),We evaluate CItruS on three tasks: long docu-ment reading comprehension, knowledge retrieval,and language modeling. Our approach improvesdownstream task performance over several strongbaselines by large margins and enables the retrievalof desired information hidden within a long doc-ument of up to one million tokens. Furthermore,the model maintains high language modeling per-formance with a low perplexity. Notably, CItruSis applicable to all the transformer-based decoder-only model without any further training, improvingthe models ability to conduct downstream tasksfor input sequences with arbitrary lengths.Overall, our contributions are summarized asfollows: 1) We define and demonstrate the infor-mation neglect problem in state-eviction methods.2) We propose CItruS, a state eviction method de-signed for long sequence downstream tasks, whichincorporates an instruction-aware cache for task-solving and a chunked state eviction process forefficient language modeling. 3) Experiments onlong document reading comprehension, knowledgeretrieval, and language modeling show that CIt-ruS improves performance on downstream tasksinvolving long sequence by large margins whilemaintaining low language modeling perplexity.",
  "Long Sequence Processing": "Long sequence processing has long been a key re-search area in natural language processing (Tiezziet al., 2024). Various approaches have been ex-plored to address this challenge, including Long-former and State Space Models (Beltagy et al.,2020; Gu et al., 2022; Gu and Dao, 2023). Ad-ditionally, memory-augmented models use exter-nal memory to handle long sequences (Kuhn andDe Mori, 1990; Wu et al., 2022; Bertsch et al.,2024; Lu et al., 2024), while recurrent-based trans-formers have been designed for long-sequencetasks (Dai et al., 2019; Li et al., 2023; Peng et al.,2023). More related work about long sequences isfurther provided in Appendix M.Except for LONGHEADS, a memory-augmentedmethod which requires storing all the past key-value states, all the above methods require fur-ther training of the model to handle long sequenceprocessing.Our approach is an inference-timemethod and eliminates the need for further training,working directly with any open-source transformer-based language model and requiring significantlyfewer resources than the methods mentioned.Our work is also similar to retrieval-augmentedgeneration (RAG) methods (Gao et al., 2023; Zhaoet al., 2024), which incorporates knowledge fromexternal databases to enhance the generation. How-ever, RAG research mainly focuses on the retrievalprocess in order to better leverage the documentsthat could support the response generation, whereasCItruS is a method that more generally focuses onperforming various long sequence tasks. It couldbe a good option to be applied to the RAG process.In fact, our testing includes long-document ques-tion answering and retrieval as primary tasks. Wefurther discuss the difference between our methodand RAG in Appendix M.",
  "State Eviction for Large Language Models": "Liu et al. (2024) explore the persistence of impor-tance hypothesis for the key-value cache of largelanguage models, which states that the positionof the cache that are useful for language model-ing tend to remain consistent over time. Based onthis, various methods that evict the key-value cacheduring language modeling have been proposed forimproving the efficiency of LLM inference. Zhanget al. (2024b) use accumulative attention scoresto evict unnecessary key-value cache states. Oren Context 1Document Instruction text Context 2 Instruction text Top-k states Top-k states Calculate intersection Context 2 Context 1 AttendSelect SelectAttend",
  ": The illustration of our experiments that applyintersection calculation to explore the information ne-glect problem in state eviction models": "et al. (2024) use the attention of the last token as ametric for evicting hidden states. Ge et al. (2023)profile all the attention heads and maintain differ-ent hidden states for different heads. Ren and Zhu(2024) propose determining the eviction scope byevaluating the standard variance of the attentionweights received by individual tokens, and they testthe efficiency improvement of state eviction meth-ods using small text chunks of size 16, which wescale up to 768 in our work. Yang and Hua (2024)bring the preference of future tokens into the stateeviction process. Xiao et al. (2023) propose thatattention sinks exist during LLM sequence pro-cessing. By keeping the key-value states of theinitial tokens, and evicting the key-value states outof a sliding window maintained for recent tokens,their model could maintain the perplexity whileprocessing 4 million tokens.We propose that these previous methods sufferfrom the information neglect problem; that is, theyfail to preserve specific information related to theinstruction text, therefore might lower the perfor-mance on down-stream tasks.",
  "The Information Neglect Problem": "In this section, we demonstrate the information ne-glect problem of existing state eviction methods.State eviction methods have two basic elements:a key-value cache C that maintains the most im-portant hidden states for language modeling and astrategy S to evict unnecessary states from the key-value cache, thereby making room for new states.By iteratively evicting the most unnecessary tokensfrom the cache, the model achieves the capabilityto model long sequences of arbitrary lengths. S isusually based on the attention weight a cache statereceives from tokens later in the sequence.The information neglect problem stems from : The difference between the top-k hidden statesselected by the instruction text and the document contextwith the k set as 20, conducted with Mistral 7B Instruct.Context-instruction intersection represents the overlapbetween the top-k hidden states selected by the attentiondistribution from one piece of the context in the longdocument and the instruction text to a key-value cache. the observation that the preserved states useful forlanguage modeling are not necessarily the onesfor a downstream task (e.g., answering a specificquestion). We demonstrate this by measuring thedifference between the top-k states selected by adocument context compared to those selected by aspecific instruction text (). Specifically, weselect one context and encode it to acquire a cachethat could be evicted (i.e, Context 1 in ).Then, we use another piece of context (i.e., Context2 in ) and the instruction text, both with thesame length, to evict the cache separately, retainingthe top-k most important hidden states. By com-puting the overlap of the differently evicted caches,we draw conclusions about the information neglectscenarios during the eviction-based language mod-eling process. More experimental setup for theseexperiments is shown in Appendix A. We use thesame setting to acquire the results in .We conduct this experiment on the full test setof the Qasper dataset (Dasigi et al., 2021). Weuse the averaged attention score of all the tokensfrom one piece of text to the cache to select themost important states, which is further describedin .1. As shown in , the hiddenstates focused on by the document context and thedownstream instruction text are remarkably differ-ent, reflected by an intersection ratio lower than 0.2in the middle layers.Supported by the above experiments, we claimthat if only the attention distribution of the con-text chunk is used to select the key-value statesrelevant to language modeling, some informationspecifically related to the final instruction text willbe discarded during the encoding of the document,possibly decrease the task performance.",
  ": The illustration of our proposed differentsubprocesses for task-specific long sequence modeling.Each process serves as different roles": "A similar line of work that models long sequencewith sliding-window-based methods (Xiao et al.,2023; Han et al., 2023) also suffers from informa-tion neglect problems, where we provide detaileddescription in Appendix D.Additionally, we conduct another set of experi-ments that demonstrate the performance degrada-tion of the standard state eviction models comparedto the standard models that use the full text as in-put. Results supporting the presence of informationneglect problem are presented in Appendix E.",
  "Methods": "To address the problem of information neglect, wepropose to decompose the inference procedure oflarge language models into two different subpro-cesses: the language modeling process and the tasksolving process, shown in . For the lan-guage modeling process, we propose to use chun-ked state eviction methods to make the modelingprocess more efficient. For the task solving process,we propose instruction-aware state eviction, us-ing the hidden states of the final instruction promptas an additional instruction-aware query to extractand preserve the task-related information in a key-value cache. Then, we utilize this key-value cacheto generate a task-specific response.For downstream tasks with a long document in-put D and a final instruction I (a piece of textthat prompt the model to conduct the downstreamtasks), our proposed method generates a corre-sponding response R according to I.",
  "encoding the long document D more efficiently": "Overall process:Given a document D, we di-vide it into chunks D = {s1, s2, . . . , sn}, where ndenotes the number of chunks. Each chunk s has alength of ls except for the final chunk sn. As illus-trated in (a), the Standard Chunked StateEviction (Standard CSE) process includes threesteps: 1) given a cache C, we encode the currenttext chunk s with an LLM; 2) evict the unimportanthidden states in C according to the attention distri-bution from s to C; 3) put all the new hidden statesof s into the cache. This iterative process startswith putting the first text chunk into the cache Cand ends when the document has been fully pro-cessed. After the whole encoding process, the finalchunk (maybe shorter than the length of ls) is putinto the cache, which leads to possible informationbias towards this chunk. To alleviate this bias, weuse the instruction text as a new text chunk to evictthe cache C one more time. The resulting cacheC is then used to encode the instruction text andgenerate the final response. State eviction based on chunked averaged atten-tion score:For state eviction, we use the atten-tion score from all the tokens in the current textchunk s to a state c in the cache C as a metric ofthe states importance:",
  "(1)": "where Imp(s, c) represents the importance score ofstate c with chunk s, dk is the dimensionality of thekey vector, Qt and Kc is the query vector of tokent and the key vector of state c, respectively.We preserve the states with the k highest impor-tance scores while evicting the other states:",
  "C = {c C | I(c) = 1}(3)": "where Setselection is the hidden states with the khighest importance scores Imp(s, c) from the cur-rent chunk s, and C represents the cache after theeviction. We execute the eviction in a layer-wisemanner, which means that the hidden states re-tained in different layers could belong to differenttokens. This design allows more flexibility sincedifferent layers could be responsible for differentfunctions and semantics. We choose to not apply afiner-grained head-wise eviction to our model sinceit performed worse in our initial experiments.",
  "Instruction-aware State Eviction": "Next, we introduce chunked instruction-aware stateeviction (CItruS) that aims to preserve informationrelevant to the task-solving process. We proposetwo kinds of cache design to achieve this goal. First,we propose to maintain a separate individual in-struction cache CI during the standard chunkedstate eviction process, which retains informationrelated to the instruction text. Second, we proposea variant with a common shared cache for CI andC to reduce the computational cost. Illustrations ofthe two proposed methods are shown in . Individual cacheWe use an individual instruc-tion cache CI to specifically store the hidden statesrelated to the instruction text, in addition to C.Specifically, after the eviction on C, we conductanother eviction process on CI with the final in-struction text, and then put the key-value statesof the current text chunk s into CI. The evictionprocess is shown as follows:",
  "where SetIselection is the key-value cache states withk highest importance scores of Imp(I, cI)": "Shared cacheUsing individual caches will dou-ble the memory usage for a fixed cache size.Guided by the persistence of importance hypothe-sis (Liu et al., 2024), where the hidden states use-ful for maintaining the perplexity are attended bymost of the following tokens, we hypothesize thatthe intersection between states selected by con-text and instruction texts, mentioned in ,could be responsible for maintaining the perplex-ity. Hence, we suppose that we could further re-duce the memory cost of CI by sharing it withthe language modeling process. Specifically, thetop-k state Setselection of the shared cache is de-termined based on the attention-based importancescore Imp(I, c), which measures the attention fromthe final instruction I to a cache state c. Shown in(c), we directly use this key-value cacheevicted by Imp(I, c) to encode the current chunk s.The rest of the eviction process follows the sameprocedure as described in Eq. (2) and (3).",
  "Overall Process": "In this section, we summarize the overall processfor applying CItruS to downstream tasks. As de-scribed in .1, the model starts by itera-tively encoding the chunked document D. Unlikethe Standard CSE model, CItruS introduces the in-struction text to evict either an individual or sharedinstruction-aware cache. As mentioned, we use theinstruction text to evict these caches again after pro-cessing the entire document, selecting the k mostimportant key-value states for each layer. We usethese k states to encode the final instruction andgenerate the response, thereby setting the size ofeach cache for all models to k during this period2.",
  "The cache size of our standard CSE and shared cacheCItruS during the encoding is ls + k while the individualcache CItruS requires a cache size of 2 (ls + k)": "et al., 2021), MultifieldQA-en (Bai et al., 2023),HotpotQA (Yang et al., 2018), and TriviaQA (Joshiet al., 2017). We also include two other long few-shot tasks, Trec (Li and Roth, 2002) and Sam-Sum (Gliwa et al., 2019), which focus on classifica-tion and dialogue summarization, respectively. Wefollow Bai et al. (2023) to adapt these datasets intolong-document tasks. Instead of reporting the aver-age scores in the main paper, we choose to reportthe average rank each model performs to avoid thevariance differences among the datasets. Detailedresults on each dataset is provided in Appendix C. Long document knowledge retrievalWe usetwo tasks to test if the model could preserve theimportant information during the whole languagemodeling process: passkey retrieval3 (Mohtashamiand Jaggi, 2023) and needle-in-a-haystack 4 tasks.The passkey retrieval task tests if the model canretrieve a single passkey (e.g., a five-digit number)inserted in a synthetic long document made up byrepetitive simple sentences. We conduct this taskon the documents with lengths up to 1 million to-kens. The needle-in-a-haystack task replaces thepasskey with a more general text fact and insertsthem in real long documents. An example of thefact and the information of the documents can befound in Appendix G. The maximum length of doc-uments for needle-in-a-haystack is set to 12,000.We use accuracy in the passkey retrieval task andthe ROUGE metric (Lin, 2004) for the needle-in-a-haystack task to award partial correctness. Ad-ditional experiments using BABILong (Kuratovet al., 2024), a dataset design for the long-contextneedle-in-a-haystack task, are also conducted inAppendix I. Long-range language modelingWe report per-plexity scores on long-range language modeling toestimate how well our models maintain fluency ingeneration (Xiao et al., 2023). We used PG19 (Raeet al., 2019) dataset for this task.",
  "Streaming LLM2.833.173.502.503.004.171.673.173.83TOVA2.673.002.673.674.003.503.834.004.33RoCo3.672.672.833.003.172.004.001.332.33H2O4.003.504.174.172.502.673.333.504.83": "Standard CSE3.333.673.003.674.174.175.003.502.00+ Individual Cache7.178.007.006.507.006.676.507.336.33+ Shared Cache6.506.677.006.837.337.336.507.336.33H2O + Shared Cache5.175.175.505.334.835.504.675.175.83 : The averaged reversed rank results among allthe 8 models on six different reading comprehensiontasks, where 8 is the highest score and 1 is the low-est score. Results are presented by grouping text withlengths of 0-4k, 4k-8k, and 8k+. Best results are bolded. TOVAframes transformers as multi-state RNNsby using the attention distribution of the last tokento identify which token should be evicted (Orenet al., 2024). This model could be seen as a specialcase of our standard CSE model with the ls as 1.",
  "RoCouses averaged attention probability fromfuture tokens and determines the eviction scope byevaluating the standard variance of the attentionweights one token receives (Ren and Zhu, 2024)": "LONGHEAD (Lu et al., 2024) is another methodthat does not require further training. However,it requires large excessive memory cost (althoughcould be offloaded to cpu memory, but that wouldcost more time) compared to our methods. Hencewe choose to omit this model from our baselines tomaintain a fair comparison.Note that our proposed chunked instruction-aware state eviction is uncoupled with the evictionstrategies used by the above models, hence it couldbe applied to all the above methods to achieve evenbetter results. Due to the limitation of the compu-tational cost, we only experiment the instruction-aware state eviction with our proposed chunkedaverage attention score strategy and the accumula-tive attention score strategy used by H2O (denotedas H2O + Shared Cache) in our paper. All baselinesare reimplemented with public repositories56. Forall baseline models, we apply the same encodingand generation process described in .3 forfair comparison.",
  "Hyperparameters": "We applied the position shift mechanism leveragedby Xiao et al. (2023), which always use the samepositional embeddings for the caches containingdifferent hidden states, to make the models processlong documents better. We also apply this tech-nique to all the baselines to enhance their abilityof processing long sequences. We use the Llama2 Chat model (Touvron et al., 2023) with 7 billionand 13 billion parameters and the 7 billion param-eter Mistral Instruct model (Jiang et al., 2023) asthe backbone models. Additionally, we include ex-periments using Llama 3 8B Instruct model, shownin Appendix H. k is set as 768 and ls is set as 256,resulting a cache size of 1,024 during modeling thedocument. This setting is also applied to all thebaseline models. We apply 8 bit quantization on the13 billion parameter model. Results are inferred onone A100 80G GPU. All the hyperparameters areselected using the validation sets.",
  ": The language modeling results on the Llama2 7B chat and Mistral 7B Instruct model. The line chartis smoothed with a window size of 4096 for clarity": "basic framework. Both variants of CItruS consis-tently outperform all baselines and Standard CSE.As mentioned in .2, our method could alsobe applied on different eviction policies. Hence, wefurther included a variant of the H2O model (H2O+ Shared Cache) and show that it achieves betterperformance over the H2O model in all cases.We find models with a shared cache achieve thesame level of performance as their correspondingmodel with separate caches. This suggests that theoverlapping tokens between the context and theinstruction text might be sufficient to support lan-guage modeling, while the shared cache also main-tains the information useful for the downstreamtasks. We will further discuss this in .3.",
  "Long document knowledge retrieval": "The main results of long document knowledge re-trieval are shown in and . Ourproposed CItruS retrieves all the passkeys usingLlama 2 7B and Mistral 7B while still outperform-ing the Standard CSE for Llama 2 13B7, whichshows the superiority of CItruS for long documentknowledge retrieval. For the needle-in-a-haystacktask, our method outperforms the standard stateeviction methods across different large languagemodels and lengths.",
  "Long-range language modeling": "We compare our model with the long-range lan-guage modeling model, Streaming LLM. Specifi-cally, we evaluate the standard CSE as well as theshared cache version of our proposed CItruS. ForCItruS with a shared cache, we randomly sample10 different instructions including different ques-tions from Qasper and HotpotQA dataset. We showthe results using one instruction here and appendthe rest of the results in the Appendix J. Results in show that our standard CSE could main-tain the perplexity when processing long sequencesas low as the Streaming LLM. Meanwhile, al-though showing a slight increase in perplexity withthe Llama 2 7B Chat model, CSE with a sharedcache achieves consistent perplexity results with-out exploding as described by Xiao et al. (2023).This shows that introducing the instruction text asthe query to evict hidden states would not affectthe text perplexity of the large language models.A more detailed discussion about the roles of thestandard cache and the instruction-aware cache inour model is provided in Appendix K.",
  "Analysis": "In this section, we provide analyses on the hyper-parameters of our model, the effect of chunk size,and the position bias in the knowledge retrievaltasks. We also provide an analysis on the effectof the initial tokens in Appendix L. We report theaveraged results in this section since all the modelsperform similarly in the analyses across all thedatasets. The full results are shown in Appendix C.",
  ": Results of the different chunk size ls. Bestresults are bolded. Param. stands for hyperparameters": ": The position-wise results from CItruS withshared cache (ls = 1,024, k = 1,024) on needle-in-the-haystack using Mistral 7B Instruct. The x-axis repre-sents the position where the needle is inserted, whilethe y-axis represents the length of the documents. Thecolor of the grid represents the ROUGE-1 score. this section, we probe our model by adjusting dif-ferent hyperparameters to demonstrate that our pro-posed CItruS is insensitive to them. shows that with a fixed budget, CItruSconsistently outperforms the Standard CSE models,showing that our method is not sensitive to thechoices of k and ls, and the instruction-aware cachemethods are the best when considering both theefficiency and the down-stream task performance.",
  "Analysis of the chunk size": "We provide a comparison of models using chunksizes ranging from 64 to 768. The inference timeof each model decreases linearly as ls increases.As shown in , the performance fluctua-tion when using different chunk sizes is very lim-ited, while the efficiency is significantly improved.Our CItruS model extends the chunk size beyondthat of previous methods and demonstrates a sub-stantial improvement in efficiency for conductinglong-sequence downstream tasks.",
  "Position bias in knowledge retrieval": "Liu et al. (2023) propose that large language mod-els tend to pay less attention to the middle parts oflong documents. In this section, we test our modelto determine if this issue persists with our proposedinstruction-aware cache method.We use the needle-in-a-haystack task as the ba-sic task and evaluate the ROUGE results when thefact is inserted at different positions in the docu-ment. As shown in , we demonstrate thatthe CItruS model still prefers to attend to the in-formation at the beginning and the end, leavingfuture work to address this lost-in-the-middle issuein eviction-based long-sequence methods.",
  "Conclusion": "We have proposed CItruS, an inference-time stateeviction method for large language models (LLMs)that improves their performance on long sequencedownstream tasks. It features a large chunked se-quence processing procedure and an instruction-aware cache that helps with solving downstreamtasks.Experiments on long document readingcomprehension, knowledge retrieval, and languagemodeling show the utility of our method comparedto strong baselines.Our work demonstrates the possibility of gener-alizing standard LLMs trained on text constrainedto certain lengths to processing longer sequenceswithout any parameter adjustments. Our evalua-tion mainly focuses on retrieving task-related infor-mation from a long document. Future work mayconsider extending more high-level abilities (e.g.,multi-hop and compositional reasoning) to the longsequence regime. Moreover, trainable componentscan be further introduced to facilitate this process.",
  "Acknowledgements": "The authors thank all the reviewers for their sug-gestions and comments. This work is supported byNational Natural Science Foundation of China (No.U21B2009) and McGill Science Undergraduate Re-search Award (SURA). Jackie Chi Kit Cheung issupported by Canada CIFAR AI Chair program.The authors also acknowledge the material supportof NVIDIA in the form of computational resources.",
  "We only tested our methods with Llama 2 and Mis-tral models, leaving performance on other datasetsto be evaluated. The instruction-aware cache is": "only applied to our Standard CSE and the H2Omodels, it could be further applied to models usingother state eviction policies to possibly further en-hance the performance. Our work only uses oneinstruction for each task to conduct all the exper-iments. It would be interesting to show whetherbetter instruction texts exist that are specifically de-signed for conducting long sequence down-streamtasks. Future work might consider optimizing thequery, or even use soft prompt optimization tech-nique to select the hidden states.",
  "Ethical Considerations": "The associated risks with this work include using amodel trained on vast amounts of text, which likelycontains gender, racial, and cultural bias. Anotherconcern is the potential misuse of the model forgenerating misleading or harmful content when ap-plying our method to generate text. Meanwhile,cache-based methods could be more effective formalicious applications like jailbreaking or reveal-ing private information, since it breaks the standardusage of the hidden states in large language models. Sotiris Anagnostidis, Dario Pavllo, Luca Biggio,Lorenzo Noci, Aurelien Lucchi, and Thomas Hof-mann. 2024. Dynamic context pruning for efficientand interpretable autoregressive transformers. Ad-vances in Neural Information Processing Systems,36. Yu Bai, Heyan Huang, Cesare Spinoso-Di Piano, Marc-Antoine Rondeau, Sanxing Chen, Yang Gao, andJackie Chi Kit Cheung. 2024.Analyzing task-encoding tokens in large language models. arXivpreprint arXiv: 2401.11323. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,Jiankai Tang, Zhidian Huang, Zhengxiao Du, XiaoLiu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,and Juanzi Li. 2023. Longbench: A bilingual, mul-titask benchmark for long context understanding.arXiv preprint arXiv: 2308.14508.",
  "Shouyuan Chen, Sherman Wong, Liangjian Chen, andYuandong Tian. 2023. Extending context window oflarge language models via positional interpolation.arXiv preprint arXiv: 2306.15595": "Zihang Dai, Zhilin Yang, Yiming Yang, J. Car-bonell, Quoc V. Le, and R. Salakhutdinov. 2019.Transformer-xl: Attentive language models beyond afixed-length context. Annual Meeting of the Associa-tion for Computational Linguistics. Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan,Noah A. Smith, and Matt Gardner. 2021. A dataset ofinformation-seeking questions and answers anchoredin research papers. North American Chapter of theAssociation for Computational Linguistics.",
  "Dongseong Hwang, Weiran Wang, Zhuoyuan Huo,Khe Chai Sim, and Pedro Moreno Mengibar. 2024.Transformerfam:Feedback attention is workingmemory. arXiv preprint arXiv:2404.09173": "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, Llio Renard Lavaud,Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,Thibaut Lavril, Thomas Wang, Timothe Lacroix,and William El Sayed. 2023.Mistral 7b.arXivpreprint arXiv: 2310.06825. Mandar Joshi, Eunsol Choi, Daniel S. Weld, and LukeZettlemoyer. 2017. Triviaqa: A large scale distantlysupervised challenge dataset for reading comprehen-sion. Annual Meeting of the Association for Compu-tational Linguistics. Sehoon Kim, Sheng Shen, David Thorsley, Amir Gho-lami, Woosuk Kwon, Joseph Hassoun, and KurtKeutzer. 2022. Learned token pruning for transform-ers. In Proceedings of the 28th ACM SIGKDD Con-ference on Knowledge Discovery and Data Mining,pages 784794.",
  "Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, JohnCanny, and Ian Fischer. 2024. A human-inspiredreading agent with gist memory of very long contexts.arXiv preprint arXiv: 2402.09727": "Jiawei Li, Yizhe Yang, Yu Bai, Xiaofeng Zhou, YinghaoLi, Huashan Sun, Yuhang Liu, Xingpeng Si, YuhaoYe, Yixiao Wu, et al. 2024a. Fundamental capabili-ties of large language models and their applicationsin domain scenarios: A survey. In Proceedings of the62nd Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages1111611141. Xianming Li, Zongxi Li, Xiaotian Luo, Haoran Xie,Xing Lee, Yingbin Zhao, Fu Lee Wang, and QingLi. 2023. Recurrent attention networks for long-textmodeling. In Findings of the Association for Com-putational Linguistics: ACL 2023, pages 30063019,Toronto, Canada. Association for Computational Lin-guistics.",
  "Chin-Yew Lin. 2004. ROUGE: A package for auto-matic evaluation of summaries. In Text Summariza-tion Branches Out, pages 7481, Barcelona, Spain.Association for Computational Linguistics": "Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-jape, Michele Bevilacqua, F. Petroni, and PercyLiang. 2023. Lost in the middle: How languagemodels use long contexts. Transactions of the Asso-ciation for Computational Linguistics. Zichang Liu, Aditya Desai, Fangshuo Liao, WeitaoWang, Victor Xie, Zhaozhuo Xu, Anastasios Kyril-lidis, and Anshumali Shrivastava. 2024.Scis-sorhands: Exploiting the persistence of importancehypothesis for llm kv cache compression at test time.Advances in Neural Information Processing Systems,36.",
  "Matanel Oren, Michael Hassid, Yossi Adi, and RoySchwartz. 2024. Transformers are multi-state rnns.arXiv preprint arXiv:2401.06104": "Bo Peng, Eric Alcaide, Quentin G. Anthony, Alon Al-balak, Samuel Arcadinho, Stella Biderman, HuanqiCao, Xin Cheng, Michael Chung, Matteo Grella,G. Kranthikiran, Xuming He, Haowen Hou, Przemys-law Kazienko, Jan Kocon, Jiaming Kong, BartlomiejKoptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Fer-dinand Mom, Atsushi Saito, Xiangru Tang, BolunWang, J. S. Wind, Stansilaw Wozniak, RuichongZhang, Zhenyuan Zhang, Qihang Zhao, P. Zhou, JianZhu, and Rui Zhu. 2023. Rwkv: Reinventing rnnsfor the transformer era. Conference on EmpiricalMethods in Natural Language Processing. Guanghui Qin, Yukun Feng, and Benjamin Van Durme.2023.The NLP task effectiveness of long-rangetransformers. In Proceedings of the 17th Conferenceof the European Chapter of the Association for Com-putational Linguistics, pages 37743790, Dubrovnik,Croatia. Association for Computational Linguistics.",
  "Eric Todd, Millicent L. Li, Arnab Sen Sharma, AaronMueller, Byron C. Wallace, and David Bau. 2023.Function vectors in large language models": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schel-ten, Ruan Silva, Eric Michael Smith, Ranjan Sub-ramanian, Xiaoqing Ellen Tan, Binh Tang, RossTaylor, Adina Williams, Jian Xiang Kuan, PuxinXu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, An-gela Fan, Melanie Kambadur, Sharan Narang, Aure-lien Rodriguez, Robert Stojnic, Sergey Edunov, andThomas Scialom. 2023. Llama 2: Open foundationand fine-tuned chat models. arXiv preprint arXiv:2307.09288.",
  "Guangxuan Xiao, Yuandong Tian, Beidi Chen, SongHan, and Mike Lewis. 2023. Efficient streaminglanguage models with attention sinks. arXiv preprintarXiv:2309.17453": "Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang,Prajjwal Bhargava, Rui Hou, Louis Martin, RashiRungta, Karthik Abinav Sankararaman, Barlas Oguz,et al. 2023. Effective long-context scaling of founda-tion models. arXiv preprint arXiv:2309.16039. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,William W. Cohen, R. Salakhutdinov, and Christo-pher D. Manning. 2018. Hotpotqa: A dataset fordiverse, explainable multi-hop question answering.Conference on Empirical Methods in Natural Lan-guage Processing.",
  "Jungmin Yun, Mihyeon Kim, and Youngbin Kim. 2023": "Focus on the core: Efficient attention via prunedtoken compression for document classification. InFindings of the Association for Computational Lin-guistics: EMNLP 2023, pages 1361713628, Singa-pore. Association for Computational Linguistics. Zhenyu Zhang, Runjin Chen, Shiwei Liu, Zhewei Yao,Olatunji Ruwase, Beidi Chen, Xiaoxia Wu, andZhangyang Wang. 2024a.Found in the middle:How language models use long contexts better viaplug-and-play positional encoding. arXiv preprintarXiv:2403.04797. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, TianlongChen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuan-dong Tian, Christopher R, Clark Barrett, et al. 2024b.H2o: Heavy-hitter oracle for efficient generative in-ference of large language models. Advances in Neu-ral Information Processing Systems, 36.",
  "Jing Zhao, Junwei Bao, Yifan Wang, Yongwei Zhou,Youzheng Wu, Xiaodong He, and Bowen Zhou.2021.Ror: Read-over-read for long documentmachine reading comprehension.arXiv preprintarXiv:2109.04780": "Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhen-gren Wang, Yunteng Geng, Fangcheng Fu, LingYang, Wentao Zhang, Jie Jiang, and Bin Cui. 2024.Retrieval-augmented generation for ai-generated con-tent: A survey. arXiv preprint arXiv: 2402.19473. Yimeng Zhuang and Huadong Wang. 2019. Token-leveldynamic self-attention network for multi-passagereading comprehension. In Proceedings of the 57thAnnual Meeting of the Association for ComputationalLinguistics, pages 22522262, Florence, Italy. Asso-ciation for Computational Linguistics.",
  "ADetails for the Intersection ProbingExperiments": "The goal of the intersection probing experiment isto determine whether the document context selectsa different set of top-k states with the highest at-tention scores within the cache compared to theinstruction text. This difference could lead to thedocument context overlooking crucial informationrequired by the final instruction.For this purpose, we use all the 416 documentsin the test split of the Qasper dataset (Dasigi et al.,2021). For each document, we randomly select achunk, referred to as Context 1, consisting of 200tokens from the first 1",
  "as Context 1. Then, we ran-domly select a second chunk, referred to as Context2, from the final 1": "4 document to ensure sufficientdistance between Context 1 and Context 2, avoid-ing recency bias and placing Context 2 close to thefinal instruction text. To ensure a fair comparison,we also make sure that the length of Context 2 isthe same as that of the instruction text for eachdocument.We send the concatenation of Context 1 and Con-text 2 to the Mistral 7B Instruct model to obtain thesimulated cache C, which consists of all the key-value states of Context 1. We could also acquirethe attention distribution from Context 2 to Con-text 1 through this step. At each model layer l, wedefine the importance of the jth state in Context 1as the average of alij, the attention score from eachposition i in Context 2 to the jth state in Context1. We keep the top-k states in Context 1 with thehighest average attention scores as Setselection, andcompute the final evicted cache Ccontext 2 followingEqu. (2) and (3). Similarly, we use the same modelto encode the concatenation of Context 1 and the in-struction text to get the attention distribution fromthe instruction text to Context 1, and follow thesame steps as described above to obtain the finalevicted cache Cinstruction from the instruction text.In this experiment, we set k to 20, which is 1",
  "oflength of the first context.We compute the intersection ratio between theCcontext 2 and Cinstruction as | Ccontext 2 Cinstruction|": "| Cinstruction|, andaverage the intersection ratio over all the 416 doc-uments for each layer. As shown in , theintersection ratio is particularly low in the middlelayers of the model, supporting our hypothesis that the document context neglects a significant amountof information considered important by the finalinstruction. This discrepancy may be attributed tothe remarkably different semantics of the instruc-tion text and the document context, despite theirclose proximity.",
  "BStatistics for Each Dataset": "Qasper(Dasigi et al., 2021) consists of 5049questions from 1585 NLP research papers. Thequestions are created by practitioners who readonly the title and abstract, and answered by an-other group, who also provide supporting evidence.We use all available questions for each of the 224documents selected by (Bai et al., 2023) from thisdataset to evaluate model performance. When do-ing the intersection probing experiments, we useall 416 documents from the test split of Qasper. Werandomly choose one question as the instructiontext for each document. MultifieldQA(Bai et al., 2023) consists of longarticles from about 10 sources, including Latex pa-pers, judicial documents, government work reports,and PDF documents indexed by Google. For eachlong article, several PhD and master students areinvited to annotate. Each annotator is asked to pro-pose questions with definitive answers as much aspossible. We only use the English version of thisdataset in our experiments. It contains 150 longdocuments. HotpotQA(Yang et al., 2018) is a dataset with113,000 question-answer pairs based on Wikipedia.This dataset requires multi-document reasoning toanswer questions, and the questions are quite di-verse and not tied to specific knowledge bases. Hot-potQA has been adapted by (Bai et al., 2023) forlong context evaluation, by concatenating the evi-dence text containing the answer along with severaldistracting articles. We use all 150 documents fromthe adapted HotpotQA for our experiments. TriviaQA(Joshi et al., 2017) is a reading com-prehension dataset containing over 650K question-answer-evidence triples. Averagely, six evidencedocuments are collected for each question. We useall 300 document-question pairs selected by (Baiet al., 2023), where each document consists of theconcatenation of all available evidence documentsfor that question.",
  ": The detailed results on six different long sequence tasks, where ls = 256, k = 768 for all methods. Resultsare separately presented by grouping text with different source lengths": "classification dataset collected from 4500 Englishquestions published by USC (Hovy et al., 2001)together with 500 manually constructed questionsfor a few rare question types. This dataset has alsobeen adapted for long context evaluation (Bai et al.,2023). This is achieved by sampling several casesfrom the training set to create few-shot examplesas long context. We use all 300 examples from theadapted TREC. SamSum(Gliwa et al., 2019) includes around16K messenger-like conversations with summaries,created by English-fluent linguists. These conver-sations mirror the topics and styles of real-life mes-senger interactions, ranging from informal to for-mal, and may include slang, emoticons, and typos.Each conversation is annotated with a third-personsummary, providing a concise overview of the dis-cussion. This dataset has been adapted for longcontext evaluation as well in the same manner as",
  "the TREC dataset, and we use all 300 examplesfrom this adaptation": "PG19(Rae et al., 2019) includes a set of booksextracted from the Project Gutenberg books library,that were published before 1919. We concatenateseveral selected books from this dataset to form asuper long document and test the language model-ing ability of our proposed methods on this docu-ment to up to 400K tokens in length.",
  "DInformation Neglect of the SlidingWindow Methods": "As pointed out by Jiang et al. (2023), the slidingwindow method with a window size of w wouldmake the ith token representation hli in a specificlayer l access tokens from the input layer at a dis-tance of up to l w. This is due to the inherentdesign of attention mechanism, where the represen-tations of a former token in one layer could only beaggregated to the representations of a following to-ken in the next layer. We describe this phenomenonmore specifically by analyzing the equation of thesliding window attention mechanism for the tokenti with index i in a specific layer l,",
  "j=iwalij vlj(7)": "where dk is the dimension of the hidden states, iand j are the the indexes of the query token andthe tokens whose information are aggregated, re-spectively. As all the tokens are processed paral-lelly in one layer, the hidden states vj and kj couldonly contain their aggregated information from theprevious layer, acquired by Attentionl1j. Consid-ering qi could only attend to {viw, . . . , vi} and",
  "i=i(ll)wAttentionli": "(8)Hence, the information of token ti in the layer0 (i.e., the embedding layer) would completely dis-appear in layer l after lw time steps. Consideringthe effect that LLM would use specific layers toprocess the specifc information (e.g., syntax, taskvector, etc) (Hendel et al., 2023; Todd et al., 2023),the specific information for one token might disap-pear merely after a few window lengths.",
  "ESupplemental Experiments forInformation Neglect Problem": "We discussed the issue of information neglect in. In this section, we present a straightfor-ward experiment to further demonstrate the exis-tence of this problem. Specifically, we comparethe performance of models that read the full con-text of the document with those employing stateeviction techniques. This experiment utilizes theLlama 3 8B Instruct model across the six readingcomprehension datasets mentioned in our paper.As most long documents exceed the models pro-cessing capacity, we limited our tests to exampleswith fewer than 4096 tokens. Additionally, we",
  ": The detailed results on six different long sequence tasks, where ls = 768, k = 768 for all methods.Results are separately presented by grouping text with different source lengths": "applied 8-bit quantization for efficiency. Along-side the previously discussed state eviction models,we also include our proposed CItruS model. Weset k = 256, ls = 256 for these experiments tosimulate scenarios with small caches and long doc-uments. The results are shown in :Results show: 1) There is a large gap between theperformance of the previous cache eviction meth-ods and the model that could read the full text. 2)We would like to point out that this is not the idealcase for our proposed CItruS, which is designed forprocessing long sequences beyond the capacity ofLLMs. However, even with the short context, theproposed method approaches the performance offull-context models better than the baseline mod-els. Notably, in the TriviaQA and Qasper datasets,CItruS outperforms the models with the full text.We hypothesize that it is because some noisy infor-mation is eliminated during the eviction process.",
  "GSetup for the Needle-in-a-HaystackTask": "Due to the computational cost limitation, we usedone fact to conduct this task. The fact is Thebest thing to do in San Francisco is eat a sandwichand sit in Dolores Park on a sunny day. and thequestion input is What is the best thing to do inSan Francisco?. The document is concatenatedfrom documents from Paul Graham Essays. We cut",
  "IExperimental Results with BABILongDataset": "We conduct supplementary experiments with BA-BILong (Kuratov et al., 2024), a newly proposeddataset which contains long sequence needle-in-the-haystack tasks involving multiple supportingfacts and requires the model to generate answersusing multi-hop reasoning and temporal dynamics.We test our models and the baselines on the qa1,qa2, and qa3 subsets of BABILong with a maxi-mum length of 128k tokens. All results were ob-tained using the Llama 3 8B Instruct model. Theresults are shown in , , and Ta-ble 18, where the 0k, 1k, and 64k representthe context length of the subset.Results show that our method performs betteron these tasks, especially when the context lengthis longer. However, we want to point out that it isnot guaranteed that our method could enhance thereasoning ability of LLMs. We are only claiming",
  "DatasetsPrompt": "QasperYou are given a scientific article and a question. Answer thequestion as concisely as you can, using a single phrase or sentenceif possible. If the question cannot be answered based on theinformation in the article, write unanswerable. If the questionis a yes/no question, answer yes, no, or unanswerable. Donot provide any explanation.\\n\\nArticle: {context}\\n\\nAnswerthe question based on the above article as concisely as you can,using a single phrase or sentence if possible. If the questioncannot be answered based on the information in the article, writeunanswerable. If the question is a yes/no question, answer yes,no, or unanswerable. Do not provide any explanation.\\n\\nQuestion: {input}\\n\\n Answer: MultifieldQARead the following text and answer briefly.\\n\\n{context}\\n\\nNow,answer the following question based on the above text, only giveme the answer and do not output any other words.\\n\\nQuestion:{input}\\n Answer: HotpotQAAnswer the question based on the given passages. Only give methe answer and do not output any other words.\\n\\nThe followingare given passages.\\n\\n{context}\\n\\n Answer the question basedon the given passages. Only give me the answer and do not outputany other words.\\n\\nQuestion: {input}\\n Answer: TriviaQAAnswer the question based on the given passage. Only giveme the answer and do not output any other words. The follow-ing are some examples.\\n\\n\\n\\n{context}\\n\\n\\n\\n Question: {in-put}\\n\\n\\n\\nAnswer:",
  "SamSumSummarize the dialogue into a few short sentences. The followingare some examples.\\n\\n\\n\\n{context}\\n\\n\\n\\n{input}": "Passkey RetrievalThere is an important info hidden inside a lot of irrelevant text.Find it and memorize them. I will quiz you about the importantinformation there.{context}\\n\\n\\n\\nWhat is the pass key? The passkey is needle-in-a-haystacksystem: You are a helpful AI bot that answers questions for a user.Keep your response short and direct \\n\\n user: {context}\\n\\nuser:{Question} Dont give information outside the document or repeatyour findings\\n\\n system:",
  "Instruction 2Answer the question based on the given passages. Only give methe answer and do not output any other words.\\n\\nQuestion: Whatarchitecture does the encoder have?\\nAnswer:": "Instruction 3Answer the question based on the given passages. Only giveme the answer and do not output any other words.\\n\\nQuestion:Which case was brought to court first Miller v. California or Gatesv. Collier ?\\nAnswer: Instruction 4Answer the question based on the given passages. Only giveme the answer and do not output any other words.\\n\\nQuestion:What occupation is shared by both Marge Piercy and RichardAldington?\\nAnswer:",
  "Instruction 7Answer the question based on the given passages. Only give methe answer and do not output any other words.\\n\\nQuestion: Whatsentiment classification dataset is used?\\nAnswer:": "Instruction 8Answer the question based on the given passages. Only give methe answer and do not output any other words.\\n\\nQuestion: Thehistorical Nimavar school in the Nimavar Bazaar, or bazar, islocated in which country?\\nAnswer: Instruction 9Answer the question based on the given passages. Only give methe answer and do not output any other words.\\n\\nQuestion: Forwhat type of work is the production company for The Year Withouta Santa Claus best known?\\nAnswer: Instruction 10Answer the question based on the given passages. Only give methe answer and do not output any other words.\\n\\nQuestion: Thephysicist who is responsible for identifying the Rabi cycle wonwhat award?\\nAnswer:",
  "JResults of Perplexity with OtherInstructions": "We used 10 different instructions, shown in Ta-ble 13. We show the perplexity of models of CItruSwith Shared cache when using these ten differentinstructions in , , , and. As these results demonstrate, the per-plexity of our Shared Cache CSE remains consis-tent across a wide variety of instructions, similar tothe standard CSE and streaming LLM methods.",
  "Cache Type0-4k4-8k8k+": "Streaming LLM39.7538.8234.20TOVA41.0939.8838.65Roco45.1745.1042.41H2O45.4542.2439.13H2O + Shared Cache50.8551.8648.24Standard CSE44.8241.4837.17+ Individual Cache50.6652.1053.03+ Shared Cache51.1753.2051.66 : The averaged results among all the 8 mod-els on six different reading comprehension tasks withLlama 3 8B Instruct, where 8 is the highest score and 1is the lowest score. Results are presented by groupingtext with lengths of 0-4k, 4k-8k, and 8k+. Best resultsare bolded.",
  "KDiscussion": "In this paper, we argue that the cache used in stan-dard chunked state eviction (CSE) is primarily re-sponsible for maintaining the perplexity of lan-guage models, whereas an instruction-aware cacheoffers advantages for long-sequence downstreamtasks. This claim is supported by the followingobservations from our experiments: (1) perplexityevaluations and previous work on state evictionmethods (Zhang et al., 2024b; Oren et al., 2024)indicate that the basic cache effectively maintainslanguage model perplexity; (2) performance im-provements are observed when using an instruction-aware cache, which is only information that themodel could access when generating the responseduring the task-solving thread. It is important tonote that it is not solely the case that the standardcache only impacts perplexity while the instruction-aware cache solely affects task performance; thereis potential overlapping, as demonstrated in ourintersection calculation experiments discussed in",
  "LAnalysis on initial tokens": "Xiao et al. (2023) show that the initial tokens playa critical role in long-sequence language model-ing by serving as attention sinks. Although ourproposed method does not specifically process theinitial tokens, we assert that it can adaptively re-tain the hidden states of these tokens because theyconsistently receive a large proportion of attentionweights. In this section, we conduct experimentsthat always preserve the first 4 initial tokens duringthe eviction process.Shown in and , we demonstratethat the difference between our methods with andwithout the initial tokens are limited, showing thecapability of keeping the attention sink tokensusing our method.",
  "M.1Long Sequence Processing": "Long sequence language modeling have attractedmore and more research interests in recentyears (Tiezzi et al., 2024), as large language mod-els continue to advance (Li et al., 2024a). Vari-ous long document processing tasks are proposedto evaluate the long sequence modeling of lan-guage models (Zhao et al., 2021; Luo et al., 2021;Bai et al., 2023). Longformer, leveraging sparseself-attention pattern, save the memory cost tomake the model process long document (Beltagyet al., 2020). Memorizing transformer uses a ex-ternal memory to save the information during thelong sequence modeling process (Wu et al., 2022).Mistral applied Pre-fill and chunking sliding win-dow methods to model longer sequences (Jianget al., 2023). State space models and their vari-ations are also popular recently (Gu et al., 2022;Gu and Dao, 2023; Wang et al., 2022). Unlimit-edformer wraps pretrained encoder-decoder trans-former, and offloads the cross-attention computa-tion to a single k-nearest-neighbor index, whilethe returned kNN distances are the attention dot-product scores (Bertsch et al., 2024). Nawrot et al.(2024) propose to compress the key-value cache tomake the model process longer sequences. Xionget al. (2023) conduct continual pretraining fromLlama 2 (Touvron et al., 2023) with longer trainingsequences and on a dataset where long texts areupsampled. Rotary Position Embedding and the",
  ": The results on BABILong qa2 subset. Results are evaluated on test examples with different lengths. Bestresults are bolded": "positional interpolation based on it are also usedenable the model process longer sequences (Suet al., 2024; Chen et al., 2023). Text summariza-tion has also been known by its relation with longsequence processing area (Du and Gao, 2023; Gaoet al., 2024; Li et al., 2024b). ReadAgent are pro-posed by using a large language model agent toprocess long sequences (Lee et al., 2024). LONG-HEADS enhances the long-context processing oflarge language models by allowing multi-headattention to attend to selected important contextchunks within the trained length (Lu et al., 2024).Infini-Transformer leverage a compressive mem-ory between different context segment to achievemodeling long range text (Munkhdalai et al., 2024).Hwang et al. (2024) propose TransformerFAM, anovel architecture with a feedback loop for attend-ing to latent representations, enables Transformersto process indefinitely long sequences without addi-tional weights. Zhang et al. (2024a) leverage plug-and-play positional encoding to make the modelbetter collect the information in the middle of thedocument. Except LONGHEADS which requires storing allthe past key-value states, all the above needs furthertraining to make the model able to handle the longsequence processing task. Our work do not needany training and can be applied directly to any open-source transformer-based large language models. Retrieval-augmented generation techniques alsoshare similar aspects with our methods. RAG tech-niques usually involve two steps: first, retrievingrelevant information (usually a document) froma large database, and second, concatenating thedocument and the user query to enhance the perfor-mance of generating the response text. The similar-ity between our method and RAG methods mainlylies in the fact that our method can be applied tolong document question-answering tasks, whichis the typical form of the final step of the RAGmethods. In this sense, our method is orthogonal tothem, as it aims to improve the LLMs themselvesand can handle documents that exceed the lengthlimitations of LLMs in the RAG process. Hence, itis not appropriate to directly compare our methodsto RAG techniques.",
  "M.2State Eviction for Large LanguageModels": "Liu et al. (2024) explore the persistence of impor-tance hypothesis for the key-value cache of largelanguage models. They establish that the key-valuecache that useful for large language modeling areconsistent for all the following text. Based on this,various methods that evicts the key-value cacheduring the language modeling has been proposedfor improving the efficiency of the LLM inference.Xiao et al. (2023) propose that attention sink ex-",
  ": Results of the different start sizes averagedon six different long sequence tasks. Best results arebolded. Param. stands for hyperparameters": "ists during the sequence processing of large lan-guage models. By keeping the key-value states ofthe initial tokens, and evict the key-value states outof a sliding window maintained for recent tokens,the model could maintain the perplexity while pro-cessing 1 million tokens. Zhang et al. (2024b) useaccumulative attention scores to evict the unnec-essary key-value cache states. Oren et al. (2024)uses the attention of the last token as a metric toevict the hidden states. Ge et al. (2023) profile allthe attention heads and maintain different hiddenstates for different heads. Attendre (Yang and Hua,2024) brings the preference of future tokens intothe state eviction process.Besides inference-only state-eviction, a lot ofmethods also explore to learn to prune tokens dur-ing the training process in computer vision (Wanget al., 2023a; Kim et al., 2022; Ye et al., 2021) ornatural language processing (Zhuang and Wang,2019; Frantar and Alistarh, 2023; Yun et al., 2023;Anagnostidis et al., 2024). There is also work thatdelete tokens from the discrete prompt (Weston andSukhbaatar, 2023).Compared to this paper, the previous work rarelyfocuses the state eviction technique on the longsequence modeling scenario and does not relatedto the specific optimization for the down-streamtasks.",
  "Mistral7B Instruct": "Standard CSE25.9121.999.2541.6028.4719.1732.0428.1721.8144.0047.0034.0029.5530.2227.0415.5215.2215.35+ Individual Cache30.3727.0915.3056.3240.2847.2147.7445.9833.8049.0064.0056.0061.9959.6966.1428.7330.2634.87+ Shared Cache30.3925.5919.4954.8540.9044.9244.8844.6336.7251.0062.0051.0057.4347.1635.1926.7830.1728.97 : The detailed results on six different long sequence tasks, where the start size is set to 4 and ls = 256, k =768 for all methods. Results are separately presented by grouping text with different source lengths. : The language modeling results on the Llama 2 7B chat model. The instructions 1 to 5 listed in table 13 areused for the Shared Cache CSE method, respectively. The line chart is smoothed with a window size of 4096 forbetter visibility. : The language modeling results on the Llama 2 7B chat model. The instructions 6 to 10 listed in table 13are used for the Shared Cache CSE method, respectively. The line chart is smoothed with a window size of 4096 forbetter visibility. : The language modeling results on the Mistral 7B Instruct model. The instructions 1 to 5 listed in table 13are used for the Shared Cache CSE method, respectively. The line chart is smoothed with a window size of 4096 forbetter visibility. : The language modeling results on the Mistral 7B Instruct model. The instructions 6 to 10 listed in table13 are used for the Shared Cache CSE method, respectively. The line chart is smoothed with a window size of 4096for better visibility."
}