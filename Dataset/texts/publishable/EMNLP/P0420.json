{
  "Abstract": "Language Models (LMs) have shown promis-ing performance in natural language genera-tion. However, as LMs often generate incorrector hallucinated responses, it is crucial to cor-rectly quantify their uncertainty in respondingto given inputs. In addition to verbalized confi-dence elicited via prompting, many uncertaintymeasures (e.g., semantic entropy and affinity-graph-based measures) have been proposed.However, these measures can differ greatly, andit is unclear how to compare them, partly be-cause they take values over different ranges(e.g., [0, ) or ). In this work, we ad-dress this issue by developing a novel and prac-tical framework, termed Rank-Calibration, toassess uncertainty and confidence measures forLMs. Our key tenet is that higher uncertainty(or lower confidence) should imply lower gen-eration quality, on average. Rank-calibrationquantifies deviations from this ideal relation-ship in a principled manner, without requiringad hoc binary thresholding of the correctnessscore (e.g., ROUGE or METEOR). The broadapplicability and the granular interpretability ofour methods are demonstrated empirically. Thecode to replicate our experiments is here.",
  "Introduction": "Language Models (LMs), especially Large Lan-guage Models (LLMs), have shown promising per-formance in Natural Language Generation (NLG).These models, fitted on huge text corpora, can pro-duce responses resembling those of humans (Tou-vron et al., 2023b; OpenAI, 2023).However,since LMs often generate wrong or hallucinatedresponses (Weidinger et al., 2021; Xiao and Wang,2021; Huang et al., 2024), it is crucial to correctly *The first two authors are listed alphabetically. Correspon-dence to: Xinmeng Huang <> andShuo Li <>.University of Pennsylvania, Philadelphia (PA), USUniversity of Southern California, Los Angeles (CA), USCollaborative advising.",
  "quantify their level of uncertainty in responding toparticular inputs": "Percentage of UNLL (%) Percentage of Correctness (%) CDF( [A|U]) Percentage of UEcc (%) Percentage of Correctness (%) CDF( [A|U]) : Indication diagrams comparing two uncer-tainty measures, UNLL (negative log-likelihood) andUEcc (eccentricity), for the GPT-3.5-turbo model on theTriviaQA benchmark. The red bars indicate the aver-age correctness of different outputs, as a function ofthe corresponding relative uncertainty levels. The blueand shallow red areasdeviating from the anti-diagonallineindicate where the uncertainty measures are over-optimistic and pessimistic, respectively. Their sum isour rank-miscalibration metric (i.e., RCE), which hereis lower for UNLL than UEcc. See Sec. 4.3 for details. Uncertainty quantification is well-explored in su-pervised learning, specifically in classification (e.g.,Lichtenstein et al., 1977; Gal and Ghahramani,2016; Lakshminarayanan et al., 2017, etc). In clas-sification, a confidence measure is an estimate ofthe probability that the predicted class Y matchesthe true class label Y (Lichtenstein et al., 1977;Lee et al., 2023). A confidence measure C is con-sidered calibrated if it reflects the probability ofcorrect prediction, i.e., P(Y = Y | C) = C, forall values in Cs range. The Expected CalibrationError (ECE) measures the miscalibration of a confi-dence measure (Harrell, 2015; Naeini et al., 2015):",
  "ECP(Y =Y |C)C.(ECE)": "In classification, confidence measures are pre-dominantly built on model logits (Guo et al., 2017;Kull et al., 2019). However, these methods are lesssuitable for NLG tasks. First, the label space is of- ten too large to assess correctness via Y = Y , sinceLMs produce potentially long textual responses Yfor any given input. Second, for LMs, logits en-code the likelihood of selecting the next token anddo not necessarily capture linguistic sense (Mielkeet al., 2022). Third, even hand-crafted prompts in-tended to make LMs express confidence explicitlymay not lead to reliable confidence values becauseelicitation is heavily tied to prompt formats (Zhaoet al., 2021; Xiong et al., 2024).Recent works have studied uncertainty measuresas an alternative to confidence measures. Thesecapture the dispersion of an LMs potential out-puts for a fixed input. Kuhn et al. (2023) introducesemantic entropy, which incorporates linguistic in-variances arising from the shared meaning of gen-erated responses. Lin et al. (2023) extend semanticentropy by leveraging the affinity matrices inducedby entailment scores of generated outputs. Further,Chen et al. (2024) characterize differential entropyin the embedding space with EigenScore, via thecovariance of embeddings of potential responses.Uncertainty measures are more general and ar-guably more principled than confidence measuresfor LMs, but they lack a universal assessment met-ric such as ECE. A key issue is that uncertaintymeasures are not necessarily commensurate. Forinstance, the semantic entropy (Kuhn et al., 2023)can take arbitrarily large positive values, whereasthe EigV measure of Lin et al. (2023) depends onthe number of responses generated. This makesit difficult to understand, evaluate, and compareuncertainty measures via a unified lens.This paper develops a principled framework toassess the quality of uncertainty and confidencemeasures for LMs. We provide a novel and practi-cal framework, termed Rank-Calibration. Specifi-cally, our contributions are as follows."
}