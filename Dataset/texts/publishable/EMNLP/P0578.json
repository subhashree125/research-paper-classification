{
  "Abstract": "Grammatical Error Detection (GED) methodsrely heavily on human annotated error corpora.However, these annotations are unavailable inmany low-resource languages. In this paper, weinvestigate GED in this context. Leveraging thezero-shot cross-lingual transfer capabilities ofmultilingual pre-trained language models, wetrain a model using data from a diverse set oflanguages to generate synthetic errors in otherlanguages. These synthetic error corpora arethen used to train a GED model. Specificallywe propose a two-stage fine-tuning pipelinewhere the GED model is first fine-tuned on mul-tilingual synthetic data from target languagesfollowed by fine-tuning on human-annotatedGED corpora from source languages.Thisapproach outperforms current state-of-the-artannotation-free GED methods. We also analysethe errors produced by our method and otherstrong baselines, finding that our approach pro-duces errors that are more diverse and moresimilar to human errors.",
  "Introduction": "Grammatical Error Detection (GED) refers to theautomated process of detecting errors in text. Itis often framed as a binary sequence labeling taskwhere each token is classified as either correct orerroneous (Volodina et al., 2023; Kasewa et al.,2018). GED is widely used in language learningapplications and contributes to the performance ofgrammatical error correction (GEC) systems (Yuanet al., 2021; Zhou et al., 2023; Sutter Pessurno deCarvalho, 2024).Prior research in multilingual GED has primar-ily operated in supervised settings (Volodina et al.,2023; Colla et al., 2023; Yuan et al., 2021), relyingon human annotated data for training. Despite re-cent efforts to obtain annotated corpora (Nplavaet al., 2022; Alhafni et al., 2023) many languagesstill lack these resources, motivating research onmethods operating without GED annotations. To overcome the absence of human annota-tions, researchers have explored two primary ap-proaches. The first involves language-agnostic ar-tificial error generation (AEG). This is achievedusing rules (Rothe et al., 2021; Grundkiewiczand Junczys-Dowmunt, 2019), non-autoregressivetranslation (Sun et al., 2022), or round-trip trans-lation (Lichtarge et al., 2019). These methods arenot trained to replicate human errors and compareunfavorably to supervised techniques like back-translation (Kasewa et al., 2018; Stahlberg andKumar, 2021; Kiyono et al., 2019; Luhtaru et al.,2024) which train models to learn to generate hu-man errors.The second approach leverages the cross-lingualtransfer (CLT) capabilities of BERT-like (Devlinet al., 2019) multilingual pre-trained language mod-els (mPLMs). This involves fine-tuning a GEDmodel on languages with abundant human annota-tions (termed as source languages) and evaluatingtheir performance on languages devoid of humanannotations (referred to as target languages). Whilecertain languages exhibit unique error types, mostadhere to shared linguistic rules, which mPLMscan exploit to detect errors across languages.In this paper, we hypothesize that error genera-tion also share linguistic similarities across lan-guages. We propose a novel approach to zero-shot CLT in GED by combining back-translationwith the CLT capabilities of mPLMs to performAEG in various target languages. Our method-ology involves a two-stage fine-tuning pipeline:first, a GED model is fine-tuned on multilingualsynthetic data produced by our language-agnosticback-translation approach; second, the model un-dergoes further fine-tuning on human-annotatedGED corpora from the source languages.We experiment on 6 source and 5 target lan-guages and show that our technique surpasses previ-ous state-of-the-art annotation-free GED methods.In addition, we provide a detailed error analysis",
  "Related Work": "GED Originally addressed through statistical (Ga-mon, 2011) and neural models (Rei and Yan-nakoudakis, 2016), GED is now tackled using pre-trained language models (Kaneko and Komachi,2019; Bell et al., 2019; Yuan et al., 2021; Collaet al., 2023; Le-Hong et al., 2023).Historically, most research in GED has been con-centrated on the English language. However, re-cently, Volodina et al. (2023) organised the firstshared task on multilingual GED in which Collaet al. (2023) set state-of-the-art in all non-Englishdatasets by fine-tuning a XLM-RoBERTa largemodel on human annotated data in a monolingualsetting. While we follow their methodology to trainour GED model, we complement prior research byexploring GED for languages lacking annotations.ArtificialErrorGenerationCurrentmeth-odsforAEGcanbebroadlycategorizedinto language-agnostic and language-specific ap-proaches. Language-specific methods focus onreplicating the error patterns found in a specificGEC corpora.This can involve heuristic ap-proaches tailored to mimic the linguistic errorsidentified in GEC corpora (Awasthi et al., 2019;Cao et al., 2023a; Nplava et al., 2022), or employ-ing techniques such as back-translation (Kasewaet al., 2018; Stahlberg and Kumar, 2021; Kiyonoet al., 2019; Luhtaru et al., 2024). While effectivefor languages with annotated corpora, these meth-ods are not suitable for languages lacking suchresources. In contrast, there are few language-agnosticmethods for generating artificial errors. Grund-kiewicz and Junczys-Dowmunt (2019) introduceerrors in a corpus by deleting, swapping, insertingand replacing words and characters. Replacementsrely on confusion sets obtained from an invertedspellchecker.Lichtarge et al. (2019) introducenoise via round-trip translation using a bridge lan-guage. Finally, Sun et al. (2022) corrupt sentencesby performing non-autoregressive translation usinga pre-trained cross-lingual language model. Allthese error generation techniques have primarilybeen applied to GEC, and to the best of our knowl-edge, their performance has not been evaluated onGED.",
  "Our work advances existing synthetic data gen-eration methods by exploring a language-agnosticvariant of back-translation": "Unsupervised GEC Unlike GED, GEC without hu-man annotations has been explored in several stud-ies (Alikaniotis and Raheja, 2019; Yasunaga et al.,2021; Cao et al., 2023b). State-of-the-art unsuper-vised GEC systems (Yasunaga et al., 2021; Caoet al., 2023b) typically begin with the developmentof a GED model trained on erroneous sentencesgenerated through rule-based methods (Awasthiet al., 2019) or masked language models (Cao et al.,2023b). This GED model is subsequently usedwith the Break-It-Fix-It (BIFI) method to create anunsupervised GEC system.However, the methods used by Yasunaga et al. (2021); Cao et al. (2023b) for creating the GEDmodel are not language-agnostic, as they rely ona thorough analysis of language-specific error pat-terns, making them difficult to apply to languageslacking such annotations.Cross-lingual transfer Previous studies haveshown the capacity of mPLMs to generalize to lan-guages unseen during fine-tuning for both NLU(Conneau et al., 2020; Chi et al., 2021; Latoucheet al., 2024) and generative tasks (Xue et al., 2021;Chirkova and Nikoulina, 2024; Shaham et al.,2024). Close to our work, Yamashita et al. (2020)explored cross-lingual transfer in GEC, a closely re-lated topic. Their findings indicate that pre-trainingwith Masked Language Modeling and TranslationLanguage Modeling enhances cross-lingual trans-fer. Additionally, they show that fine-tuning on acombination of a high and a low-resource languageimproves the performance of GEC models on thelow-resource language. Legend : Step 3Step 4Step 2Step 1 mAEG Fine-tuning GED Data Generation GED Fine-tuning on artificial data GED Fine-tuning on human-annotated data",
  ": Overview of our proposed method": "In contrast to Yamashita et al. (2020) our re-search focuses on zero-shot cross-lingual transfer,specifically for GED and AEG, without relyingon target language annotations. Additionally, weadvance previous work on zero-shot cross-lingualtransfer by demonstrating its effectiveness in im-proving downstream task performance. Investigat-ing zero-shot CLT in GED is particularly signif-icant because the \"translate-train\" baseline (Con-neau et al., 2018; Wu et al., 2024), which involvestraining a GED model on a translated dataset, isinfeasible. This arises because machine translationsystems tend to correct the errors that the GEDmodel is intended to detect.",
  "Method": "Our proposed GED method is developed througha four-step process, as illustrated in . Ini-tially, we train a multilingual AEG model usingGEC datasets from the source languages. ThisAEG model is subsequently employed to produce aGED dataset encompassing both target and sourcelanguages. In the third step, we fine-tune a GEDmodel on this multilingual artificially generateddataset. Finally, we perform an additional fine-tuning of the GED model using human-annotatedGED data from the source languages. The resultantGED model is capable of detecting errors acrossany target language.Data Our method necessitates three types of cor-pora. First, the AEG model is trained using GECdatasets in a collection of source languages, Ds,which include pairs of ungrammatical sentencesand their corrected versions. Additionally, mono-lingual corpora in the source languages Ds and inthe target low-resource languages Dt, consisting ofraw sentences, are required.Step 1: mAEG Fine-tuning A generative mPLMis fine-tuned to generate errors from a dataset Dscombining all source languages. The model learnsto generate errors by using corrected text as inputand ungrammatical text as output. We refer to theresulting model as our multilingual Artificial Error Generator (mAEG). Post-training, the mAEG canintroduce errors in any language supported by themPLM, leveraging the inherent zero-shot cross-lingual transfer capabilities of generative mPLMs.Step 2: GED Data Generation Using our mAEGsystem we obtain a multilingual dataset Dsynthof raw sentences and their corresponding syntheti-cally generated ungrammatical versions by corrupt-ing sentences from Ds and Dt. We obtain GEDtoken-level annotation from Dsynth by tokenizingusing language-specific tokenizers, and aligningboth sentence versions using Levenshtein distancewith minimal alignment following Kasewa et al.(2018). We follow the labeling methodology ofVolodina et al. (2023); Kasewa et al. (2018). Wedesignate tokens that are not aligned with them-selves or tokens following a gap as incorrect, whileremaining tokens are labeled as correct.Step 3: GED Fine-tuning on artificial data GEDmodel training begins with the fine-tuning of anmPLM such as XLM-R (Conneau et al., 2020)on our synthetically generated multilingual GEDdatasets created in step 2.Step 4: GED Fine-tuning on human-annotateddata The mPLM from Step 3 is further fine-tunedusing human-annotated GED data from all oursource languages, Ds. This final model is usedto detect errors in our target languages.",
  "Datasets & Evaluation Metric": "We use English, German, Estonian, Russian, Ice-landic, and Spanish as our source languages andSwedish, Italian, Czech, Arabic, and Chinese as ourtarget languages. For each dataset, when multiplesubsets are available we use the L2 learners cor-pora and the annotations for minimal correctionsfor grammaticality.Training set The English, German, Estonian, Rus-sian, Icelandic, and Spanish datasets are taken fromthe FCE corpus (Yannakoudakis et al., 2011), theFalko-MERLIN GEC corpus (Boyd, 2018), UT-L2 GEC (Rummo and Praakli, 2017), RULEC-",
  ": Comparison of F0.5 between our proposed method, previous synthetic data generation techniques, and thezero-shot cross-lingual transfer baseline on L2 corpora": "GEC (Rozovskaya and Roth, 2019), the Icelandiclanguage learners section of the Icelandic ErrorCorpus (Arnardttir et al., 2021), and COWS-L2H(Davidson et al., 2020), respectively. We use thetraining set of each of these GEC datasets to trainour generative mPLM. Additionally, for the sec-ond stage of our multilingual two-stage fine-tuningpipeline, we use the GED version of each GECtraining dataset. For English and German, we usethe GED dataset of Volodina et al. (2023). ForRussian, we convert the M2 files (Dahlmeier andNg, 2012) to a GED dataset following the approachused by Volodina et al. (2023); for the remaininglanguages, we obtain GED annotations from GECcorpora as detailed in 3.Evaluation set The Swedish, Italian and Czechdatasets originate from the Swell corpus (Volod-ina et al., 2019), MERLIN (Boyd et al., 2014) andGECCC (Nplava et al., 2022) respectively. Weemploy the processed version of those datasets pro-vided in the Multi-GED Shared task 2023 (Volod-ina et al., 2023). For Arabic, we use both develop-ment and test data of the QALB-2015 shared tasks(Rozovskaya et al., 2015) provided by Alhafni et al.(2023). Finally, the Chinese GED data is derivedfrom two GEC corpora: MuCGEC-Dev (Zhanget al., 2022) as development set and NLPCC18-Test (Zhao et al., 2018) as test set. We apply thepost-processing method described in 3 to producethe GED versions.Monolingual corpora Our monolingual text datacomes from the CC100 dataset (Conneau et al.,2020) in which we sample 200 thousand error-freeinstances for each language.Evaluation Metric Following previous work inGED, we report the token-based F0.5 (Kaneko andKomachi, 2019; Yuan et al., 2021; Volodina et al.,2023). For finer-grained analysis we also report the",
  "Baselines": "We evaluate the proposed artificial error gener-ation method against strong baselines that donot require human-annotated datasets in the tar-get language.We chose methods representa-tive of different family of artificial error genera-tion in GEC: Rules (Grundkiewicz and Junczys-Dowmunt, 2019), Round-trip translation (RT trans-lation) (Lichtarge et al., 2019), Non auto-regressivetranslation (NAT) (Sun et al., 2022). Addition-ally, we compare our approach with a zero-shotCLT baseline, which involves directly fine-tuningthe GED model on GED datasets from all sourcelanguages. We refer to this technique as Direct-CLT to distinguish it from our method, which usesthe cross-lingual transfer capabilities of generativemPLMs to generate errors in any target language.More information on the implementations of ourbaselines in Appendix A.1.",
  "Models and Fine-tuning setups": "Synthetic Data Generation We use the No Lan-guage Left Behind (NLLB-200) model (Team et al.,2022) which supports 202 languages as our gen-erative mPLM. Specifically, we use NLLB 1.3B-distilled for all our experiments. Following Luhtaruet al. (2024), we train the model on non-tokenizedtext or detokenized if the non tokenized format isnot available. Details regarding our hyperparame-ters can be found in Appendix A.2.Grammatical Error Detection In line with Collaet al. (2023), we use XLM-RoBERTa-large, a mul-tilingual pre-trained encoder with strong cross-lingual abilities (Conneau et al., 2020) as our GEDmodel. We evaluate two versions of our method:(1) A Monolingual version, where the GED model",
  ": Comparison of F0.5 between the monolingual version of our method and previous synthetic data generationtechniques on L2 corpora": "is exclusively trained on synthetic data from thetarget language, enabling direct comparison withexisting synthetic data generation techniques. (2)A Multilingual version using our two-stage fine-tuning procedure to compare against DirectCLT. Postprocessing The postprocessing steps outlinedin 3, which transform synthetic corpora into GEDcorpora, necessitate tokenized text. To achieve this,we use Stanza (Qi et al., 2020) for Czech and Spacy(Honnibal et al., 2020) for Swedish and Italian. Fol-lowing previous works on Arabic GEC (Belkebirand Habash, 2021; Alhafni et al., 2023), we useCAMeL Tools (Obeid et al., 2020). Lastly, for Chi-nese, we use the PKUNLP word segmentation toolprovided in the NLPCC 2018 shared task (Zhaoet al., 2018).",
  "We posit that our superior performance can beattributed to the quality and diversity of the errorsgenerated by our AEG. This hypothesis is furtherexamined in": "It is worth mentioning that while our results rep-resent a significant advancement, they still fall shortof the state-of-the-art supervised settings. This re-sult is expected and aligns with the existing liter-ature in GED, which highlights notable discrep-ancies when evaluating supervised models without-of-domain data, even if it originates from thesame language as the training data (Volodina et al.,2023; Colla et al., 2023).",
  "Evaluation of AEG": "As all previous work using AEG for GED has beenin monolingual settings, we introduce a monolin-gual variant of our approach. Here, the GED modelis exclusively fine-tuned on synthetic data from thetarget language. shows that our synthetic data generationtechnique achieves the best performance amongannotation-free synthetic data generation methodsapplied to GED. Given that rule-based methods ap-ply a set of transformations without considering thesentence context, the average improvement of 9.2points of F0.5 over these methods highlights the sig-nificance of learning to generate context-dependenterrors in synthetic data generation. Additionally,given that NAT is not trained to generate errors butto produce translations, outperforming this methodby 8.3 points of F0.5 highlights the advantage oflearning to generate errors from authentic instances,even when these instances originate from differentlanguages.We hypothesize that the ability to synthesizecontext-dependent errors combined with the acqui-sition of error-generation insights from authenticinstances empower our method to yield errors moreakin to human errors, thus leading to better perfor-mance. We further analyze this hypothesis in 6.1.Additionally, our monolingual setup outper-forms DirectCLT in four out of five languages.This is a notable achievement given other syntheticdata generation methods inability to meet thisbenchmark. Both approaches leverage the CLTof mPLMs, albeit differently: ours uses it for ar-tificial error generation in target languages with agenerative mPLM, while DirectCLT leverages itdirectly to perform error detection across target lan-guages. This comparison suggests that our methodcreates tailored error patterns in target languagesthat a GED model trained only on source languageannotations cannot detect, indicating that our ap-",
  "Language Ablation": "We study the effect of changing the language con-figuration of the synthetic data. We compare fine-tuning the GED model using synthetic data com-prising different language sets: exclusively sourcelanguages, exclusively target languages, and a com-bination of both source and target languages.Results in show that any first stage fine-tuning language configuration improves the GEDperformance of our method over the DirectCLTbaseline, highlighting the robustness of our two-stage fine-tuning pipeline. Notably, including syn-thetic data from the target language results in amore significant improvement which emphasizethe importance of using a language-agnostic artifi-cial error generation method capable of generatingerrors in any target language.Furthermore, results from suggest thatfirst-stage fine-tuning exclusively on synthetic datafrom target languages outperforms fine-tuning on acombination of source and target languages. How-ever, comparing F0.5 scores does not reveal the bigpicture and can lead to false conclusion. The F0.5score is computed at an operation point that is usu-ally arbitrarily set to 0.5 in the literature (Kasewaet al., 2018; Colla et al., 2023; Le-Hong et al.,2023).For a more comprehensive comparison of performance, presents the Precision-Recall curves for each method. It shows that fine-tuning on either both synthetic data from sourceand target languages simultaneously or target lan-guages alone yields similar results and outperformsfine-tuning on synthetic data from source languagesonly. We can conclude that the determining fac-tor is the inclusion of synthetic data in the targetlanguage. We can also see that our method outper-forms other baseline in the curves too. We encour-age practitioners to use such figures to compareGED models for more meaningful conclusions thanthreshold dependent metrics such as F scores. We experimented with reversing our fine-tuningpipeline by initially training on human annotationsfrom our source languages followed by fine-tuningon synthetic data. However, this approach em-pirically yielded inferior performance. The factthat ending the fine-tuning process with human-annotated data, even in source languages, is moreeffective than using target language synthetic dataindicates that artificial errors still do not reach thequality of authentic corpora. Otherwise it wouldmake sense to end the training with errors specificto the target language. We hypothesize that im-proved synthetic error generation techniques wouldlead to opposite conclusions regarding the fine-tuning order.",
  "Scalability": "Here we investigate how our synthetic data gen-eration method scales as new languages corporabecome available. We fine-tune the AEG model byprogressively incorporating new languages in dif-ferent orders to an English-only fine-tuned baseline.We follow the protocol of Shaham et al. (2024). Wereport average scores per target language of a GEDmodel fine-tuned on monolingual synthetic data. shows that on average, performance in-creases with the number of source languages. Thissuggests that our synthetic data generation methodapplied to GED might continue to improve as newGED corpora become available.",
  "Generalization to out-of-domain errors": "Errors vary between different populations. For in-stance native speakers (L1) do not commit the sametype of errors than second language learners (L2).We investigate the robustness of our method to dif-ferent error distributions. Our method is trainedon L2 learner corpora and we evaluate it on L1data. We found available GED annotated data ofL1 speakers for Arabic and Czech: QALB 2014(Mohit et al., 2014) and the Native Formal sectionof GECCC (Nplava et al., 2022). presents the results. Our method sur-passes all other baselines, demonstrating its con-tinued suitability for out-of-domain corpora in thetarget language. A comparison between and further illustrates that, unlike the otherbaselines, our method achieves approximately simi-lar performance on both L1 and L2 Arabic corpora.",
  ": Performance of a binary classifier trained todistinguish between human errors and errors producedby a synthetic data generation technique. We report thePrecision, Recall and F1 score": "However, for Czech, all methods show a signifi-cant decrease in performance. We hypothesize thatthis is due to the unique stringent rules regardingthe use of commas in Czech. This results in thepredominance of \"Punctuation\" errors in the L1Czech corpora, which are less common in manyother languages, and therefore amplify the differ-ence between domains.",
  "Analysis of synthetic errors": "We compare the errors produced by the AEG meth-ods. We first study Czech using a Czech extension(Nplava et al., 2022) of the ERRANT (Bryantet al., 2017) error annotation tool and an artificialvs human error discriminator. We then extend ouranalysis to many languages using GPT-4 (OpenAIet al., 2024) to classify error types.",
  "Czech Case Study": "Similarity Analysis with Human Errors To as-sess if the synthetic instances are realistic andhuman-like, we train a binary classifier (one persynthetic data generation technique) to distinguishbetween errors generated by a particular syntheticdata generation method and human errors. Weconstructed a development set comprising approx-imately equal numbers of authentic and syntheticdata and assessed performance using the F1 score.More information on how we train the classifiercan be found in A.3. Results are presented in .Our classifier achieves an F1 score of 83.4%for the proposed method, indicating a moderateability to differentiate between synthetic and hu-man errors. This supports our hypothesis that oursynthetic data generation method does not fullyreplicate the quality of authentic sentences. In con-trast, the classifier achieves an F1 score exceeding95% for other synthetic data generation methods,suggesting a higher degree of differentiation. Over-all, this suggests that our method produces errorsthat are more human-like, translating into better",
  ": Normalized Entropy comparison of authenticand synthetic errors aggregated over different datasets": "downstream performance.Error Distribution We use the Czech extension(Nplava et al., 2022) of ERRANT to categorizethe errors made by different systems. presents the distribution of the top 10 error typesfor the various synthetic data generation methodsstudied. Our method produces a more diverse setof errors compared to NAT (Sun et al., 2022) andrule-based approaches (Grundkiewicz and Junczys-Dowmunt, 2019). Notably, while other methodspredominantly yield Other and Spell error types,our method features a more balanced distributionof error types, indicating that our method is moreeffective in mimicking the complexity and range ofhuman language errors.Additionally, our method generates a higher per-centage of DIACR errors compared to other tech-niques. Since DIACR errors are the most com-mon among L2 learners of Czech, this could ex-plain the performance improvements of our method.Given that DIACR errors are specific to Czech(Nplava et al., 2022) in the set of languages westudy, this indicates that our method can produceerror types not encountered during the fine-tuningon source languages of our generative mPLM.",
  "Multilingual Extension": "We want to extend our previous findings by assess-ing if our synthetic data generation method effec-tively captures a variety of error types across alllanguages. For this, we need a language-agnosticclassifier. We use GPT-4 to classify errors fromvarious sources across all the languages under in- vestigation. Prior studies have shown that GPT-4sjudgments align closely with human evaluations(Wang et al., 2023; Fu et al., 2023) and exhibitpromising error correction capabilities (Fang et al.,2023; Davis et al., 2024; Wu et al., 2023). Al-though a thorough assessment of GPT-4 for errorclassification is beyond the scope of the study, weperformed a limited qualitative analysis of GPT-4saccuracy in Italian, Swedish, Spanish, and Englishwith native speakers. We found that it is suitablefor our application. For each type of error classifiedby GPT-4 we compute its frequency distributionacross data and compute the entropy of this distribu-tion. Further details on our evaluation methodologyare provided in Appendix A.4. validates our previous findings that ourmethod generates a more diverse set of errors com-pared to NAT. However, the range of error typesgenerated by our method is narrower than that pro-duced by humans. Moreover, the variability in thediversity of error types is significantly higher withour method than with human errors across differentlanguages. This suggests that our method does notconsistently perform across languages.",
  "Conclusion": "We introduced a novel zero-shot approach for GEDwith low-resource languages. Our method com-bines back-translation with the CLT capabilitiesof mPLMs to perform AEG across various targetlanguages. Then, we fine-tune the GED modelin two steps: first on multilingual synthetic datafrom source and target languages, then on human-annotated source language corpora. This methodachieves state-of-the-art performance in annotation-free GED. Our error analysis shows that we pro-duce errors that are more diverse and human-likethan the baselines.In future work, we intend to explore the potentialof our GED models to enhance unsupervised GECmethods.",
  "Limitations": "Our approach relies on the CLT capabilities ob-tained during the multilingual unsupervised pre-training of mPLMs. Consequently, the applica-bility of our method is restricted to the languagessupported by the mPLM. Furthermore, its perfor-mance on each language may vary depending onthe amount of pre-training data available for thatlanguage. This limitation is inherent to all studies leveraging mPLMs.Additionally, our study primarily focuses on theerrors made by second language learners. Whilewe have analyzed the performance of our methodon native language corpora, it would be valuable toevaluate its generalizability to other domains withina language. For instance, this includes errors madein casual text messaging or by machine translationsystems.Compared to the direct application of CLT inGED, our method involves additional steps suchas training a generative mPLM and generating asubstantial amount of synthetic data. These re-quirements may pose challenges for researcherswith limited computational resources and couldlimit the practicality of developing this approachin resource-constrained environments. To addressthis constraint, we have made available a syntheticGED corpus encompassing more than 5 millionsamples across 11 languages.",
  "Ethics Statement": "Our research is driven by a commitment to sup-porting and preserving linguistic diversity. Low-resource languages often face marginalization inthe realm of technological advancements. By de-veloping GED models for these languages, we aimto enhance their digital presence and usability, thuspromoting linguistic equity.However, it is important to acknowledge poten-tial ethical concerns. The use of CLT to generatesynthetic data, while beneficial for training GEDmodels, carries the risk of misuse. Such systemscould potentially be exploited to create false infor-mation or propaganda in low-resource languages.Additionally, while GED systems are crucial forregions with a shortage of language teachers, thereis a risk that their widespread use could lead toan over-reliance on these tools. This dependencymight result in a decline in the linguistic and gram-matical skills of native speakers, as they becomemore reliant on technology for language correctionand validation.It is essential for future users to use these tech-nologies judiciously. Balancing the use of GEDtools with a genuine effort to improve ones linguis-tic abilities is crucial. Building on the research byFei et al. (2023) could provide a valuable advance-ment by incorporating explainability into our GEDsystems. Bashar Alhafni, Go Inoue, Christian Khairallah, andNizar Habash. 2023. Advancements in Arabic gram-matical error detection and correction: An empiricalinvestigation. In Proceedings of the 2023 Conferenceon Empirical Methods in Natural Language Process-ing, pages 64306448, Singapore. Association forComputational Linguistics. Dimitris Alikaniotis and Vipul Raheja. 2019. The unrea-sonable effectiveness of transformer language modelsin grammatical error correction. In Proceedings ofthe Fourteenth Workshop on Innovative Use of NLPfor Building Educational Applications, pages 127133, Florence, Italy. Association for ComputationalLinguistics. runn Arnardttir, Xindan Xu, Dagbjrt Gumunds-dttir, Lilja Bjrk Stefnsdttir, and Anton Karl In-gason. 2021. Creating an error corpus: Annotationand applicability. In Proceedings of CLARIN AnnualConference, pages 5963. Abhijeet Awasthi, Sunita Sarawagi, Rasna Goyal,Sabyasachi Ghosh, and Vihari Piratla. 2019. Par-allel iterative edit models for local sequence trans-duction. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processingand the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), pages42604270, Hong Kong, China. Association for Com-putational Linguistics. Riadh Belkebir and Nizar Habash. 2021. Automaticerror type annotation for Arabic. In Proceedings ofthe 25th Conference on Computational Natural Lan-guage Learning, pages 596606, Online. Associationfor Computational Linguistics. Samuel Bell, Helen Yannakoudakis, and Marek Rei.2019. Context is key: Grammatical error detectionwith contextual word representations. In Proceedingsof the Fourteenth Workshop on Innovative Use of NLPfor Building Educational Applications, pages 103115, Florence, Italy. Association for ComputationalLinguistics. Adriane Boyd. 2018. Using Wikipedia edits in lowresource grammatical error correction. In Proceed-ings of the 2018 EMNLP Workshop W-NUT: The4th Workshop on Noisy User-generated Text, pages7984, Brussels, Belgium. Association for Computa-tional Linguistics. Adriane Boyd, Jirka Hana, Lionel Nicolas, DetmarMeurers, Katrin Wisniewski, Andrea Abel, KarinSchne, Barbora tindlov, and Chiara Vettori. 2014.The MERLIN corpus: Learner language and theCEFR. In Proceedings of the Ninth InternationalConference on Language Resources and Evaluation(LREC14), pages 12811288, Reykjavik, Iceland.European Language Resources Association (ELRA).",
  "Hannan Cao, Wenmian Yang, and Hwee Tou Ng. 2023a": "Mitigating exposure bias in grammatical error cor-rection with data augmentation and reweighting. InProceedings of the 17th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 21232135, Dubrovnik, Croatia. As-sociation for Computational Linguistics. Hannan Cao, Liping Yuan, Yuchen Zhang, andHwee Tou Ng. 2023b. Unsupervised grammaticalerror correction rivaling supervised methods. In Pro-ceedings of the 2023 Conference on Empirical Meth-ods in Natural Language Processing, pages 30723088, Singapore. Association for Computational Lin-guistics. Zewen Chi, Li Dong, Furu Wei, Nan Yang, SakshamSinghal, Wenhui Wang, Xia Song, Xian-Ling Mao,Heyan Huang, and Ming Zhou. 2021. InfoXLM: Aninformation-theoretic framework for cross-linguallanguage model pre-training. In Proceedings of the2021 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 35763588, On-line. Association for Computational Linguistics.",
  "Nadezhda Chirkova and Vassilina Nikoulina. 2024.Key ingredients for effective zero-shot cross-lingualknowledge transfer in generative tasks.arXivpreprint arXiv:2402.12279": "Davide Colla, Matteo Delsanto, and Elisa Di Nuovo.2023. EliCoDe at MultiGED2023: fine-tuning XLM-RoBERTa for multilingual grammatical error detec-tion. In Proceedings of the 12th Workshop on NLPfor Computer Assisted Language Learning, pages 2434, Trshavn, Faroe Islands. LiU Electronic Press. Alexis Conneau, Kartikay Khandelwal, Naman Goyal,Vishrav Chaudhary, Guillaume Wenzek, FranciscoGuzmn, Edouard Grave, Myle Ott, Luke Zettle-moyer, and Veselin Stoyanov. 2020. Unsupervisedcross-lingual representation learning at scale. In Pro-ceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 84408451, Online. Association for Computational Lin-guistics. Alexis Conneau, Ruty Rinott, Guillaume Lample, AdinaWilliams, Samuel Bowman, Holger Schwenk, andVeselin Stoyanov. 2018. XNLI: Evaluating cross-lingual sentence representations. In Proceedings ofthe 2018 Conference on Empirical Methods in Nat-ural Language Processing, pages 24752485, Brus-sels, Belgium. Association for Computational Lin-guistics.",
  "Daniel Dahlmeier and Hwee Tou Ng. 2012.Betterevaluation for grammatical error correction. In Pro-": "ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages568572, Montral, Canada. Association for Compu-tational Linguistics. Sam Davidson, Aaron Yamada, Paloma Fernandez Mira,Agustina Carando, Claudia H. Sanchez Gutierrez,and Kenji Sagae. 2020. Developing NLP tools with anew corpus of learner Spanish. In Proceedings of theTwelfth Language Resources and Evaluation Confer-ence, pages 72387243, Marseille, France. EuropeanLanguage Resources Association. Christopher Davis, Andrew Caines, istein Andersen,Shiva Taslimipoor, Helen Yannakoudakis, ZhengYuan, Christopher Bryant, Marek Rei, and Paula But-tery. 2024. Prompting open-source and commerciallanguage models for grammatical error correction ofenglish learner text. Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics.",
  "Masahiro Kaneko and Mamoru Komachi. 2019. Multi-head multi-layer attention to deep language represen-tations for grammatical error detection. Computaciny Sistemas, 23(3):883891": "Sudhanshu Kasewa, Pontus Stenetorp, and SebastianRiedel. 2018. Wronging a right: Generating bet-ter errors to improve grammatical error detection.In Proceedings of the 2018 Conference on Empiri-cal Methods in Natural Language Processing, pages49774983, Brussels, Belgium. Association for Com-putational Linguistics. Shun Kiyono, Jun Suzuki, Masato Mita, Tomoya Mizu-moto, and Kentaro Inui. 2019. An empirical study ofincorporating pseudo data into grammatical error cor-rection. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processingand the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), pages12361242, Hong Kong, China. Association for Com-putational Linguistics.",
  "Philipp Koehn. 2005. Europarl: A parallel corpus forstatistical machine translation. In Proceedings ofMachine Translation Summit X: Papers, pages 7986,Phuket, Thailand": "Gaetan Latouche, Marc-Andr Carbonneau, and Ben-jamin Swanson. 2024. BinaryAlign: Word alignmentas binary sequence labeling. In Proceedings of the62nd Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages1027710288, Bangkok, Thailand. Association forComputational Linguistics. Phuong Le-Hong, The Quyen Ngo, and Thi Minh HuyenNguyen. 2023. Two neural models for multilingualgrammatical error detection. In Proceedings of the12th Workshop on NLP for Computer Assisted Lan-guage Learning, pages 4044, Trshavn, Faroe Is-lands. LiU Electronic Press. Yinghao Li, Xuebo Liu, Shuo Wang, Peiyuan Gong,Derek F. Wong, Yang Gao, Heyan Huang, and MinZhang. 2023. TemplateGEC: Improving grammaticalerror correction with detection template. In Proceed-ings of the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 68786892, Toronto, Canada. Association forComputational Linguistics. Jared Lichtarge, Chris Alberti, Shankar Kumar, NoamShazeer, Niki Parmar, and Simon Tong. 2019. Cor-pora generation for grammatical error correction. InProceedings of the 2019 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,Volume 1 (Long and Short Papers), pages 32913301,Minneapolis, Minnesota. Association for Computa-tional Linguistics.",
  "Agnes Luhtaru, Taido Purason, Martin Vainikko,Maksym Del, and Mark Fishel. 2024.To err ishuman, but llamas can learn it too. arXiv preprintarXiv:2403.05493": "Behrang Mohit, Alla Rozovskaya, Nizar Habash, Wa-jdi Zaghouani, and Ossama Obeid. 2014. The firstQALB shared task on automatic text correctionfor Arabic.In Proceedings of the EMNLP 2014Workshop on Arabic Natural Language Processing(ANLP), pages 3947, Doha, Qatar. Association forComputational Linguistics. Jakub Nplava, Milan Straka, Jana Strakov, andAlexandr Rosen. 2022. Czech grammar error cor-rection with a large and diverse corpus. Transac-tions of the Association for Computational Linguis-tics, 10:452467. Ossama Obeid, Nasser Zalmout, Salam Khalifa, DimaTaji, Mai Oudah, Bashar Alhafni, Go Inoue, FadhlEryani, Alexander Erdmann, and Nizar Habash. 2020.CAMeL tools: An open source python toolkit forArabic natural language processing. In Proceedingsof the Twelfth Language Resources and EvaluationConference, pages 70227032, Marseille, France. Eu-ropean Language Resources Association. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-man, Diogo Almeida, Janko Altenschmidt, Sam Alt-man, Shyamal Anadkat, Red Avila, Igor Babuschkin,Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-ing Bao, Mohammad Bavarian, Jeff Belgum, Ir-wan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,Christopher Berner, Lenny Bogdonoff, Oleg Boiko,Madelaine Boyd, Anna-Luisa Brakman, Greg Brock-man, Tim Brooks, Miles Brundage, Kevin Button,Trevor Cai, Rosie Campbell, Andrew Cann, BrittanyCarey, Chelsea Carlson, Rory Carmichael, BrookeChan, Che Chang, Fotis Chantzis, Derek Chen, SullyChen, Ruby Chen, Jason Chen, Mark Chen, BenChess, Chester Cho, Casey Chu, Hyung Won Chung,Dave Cummings, Jeremiah Currier, Yunxing Dai,Cory Decareaux, Thomas Degry, Noah Deutsch,Damien Deville, Arka Dhar, David Dohan, SteveDowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,Simn Posada Fishman, Juston Forte, Isabella Ful-ford, Leo Gao, Elie Georges, Christian Gibson, VikGoel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, ScottGray, Ryan Greene, Joshua Gross, Shixiang ShaneGu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,Yuchen He, Mike Heaton, Johannes Heidecke, ChrisHesse, Alan Hickey, Wade Hickey, Peter Hoeschele,Brandon Houghton, Kenny Hsu, Shengli Hu, XinHu, Joost Huizinga, Shantanu Jain, Shawn Jain,Joanne Jang, Angela Jiang, Roger Jiang, HaozhunJin, Denny Jin, Shino Jomoto, Billie Jonn, Hee-woo Jun, Tomer Kaftan, ukasz Kaiser, Ali Ka-mali, Ingmar Kanitscheider, Nitish Shirish Keskar,Tabarak Khan, Logan Kilpatrick, Jong Wook Kim,Christina Kim, Yongjik Kim, Jan Hendrik Kirch-ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,ukasz Kondraciuk, Andrew Kondrich, Aris Kon-stantinidis, Kyle Kosic, Gretchen Krueger, VishalKuo, Michael Lampe, Ikai Lan, Teddy Lee, JanLeike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, MateuszLitwin, Theresa Lopez, Ryan Lowe, Patricia Lue,Anna Makanju, Kim Malfacini, Sam Manning, TodorMarkov, Yaniv Markovski, Bianca Martin, KatieMayer, Andrew Mayne, Bob McGrew, Scott MayerMcKinney, Christine McLeavey, Paul McMillan,Jake McNeil, David Medina, Aalok Mehta, JacobMenick, Luke Metz, Andrey Mishchenko, PamelaMishkin, Vinnie Monaco, Evan Morikawa, DanielMossing, Tong Mu, Mira Murati, Oleg Murk, DavidMly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh,Long Ouyang, Cullen OKeefe, Jakub Pachocki, AlexPaino, Joe Palermo, Ashley Pantuliano, Giambat-tista Parascandolo, Joel Parish, Emy Parparita, AlexPassos, Mikhail Pavlov, Andrew Peng, Adam Perel-man, Filipe de Avila Belbute Peres, Michael Petrov,Henrique Ponde de Oliveira Pinto, Michael, Poko-rny, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-ell, Alethea Power, Boris Power, Elizabeth Proehl,Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,Cameron Raymond, Francis Real, Kendra Rimbach,Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-der, Mario Saltarelli, Ted Sanders, Shibani Santurkar,Girish Sastry, Heather Schmidt, David Schnurr, JohnSchulman, Daniel Selsam, Kyla Sheppard, TokiSherbakov, Jessica Shieh, Sarah Shoker, PranavShyam, Szymon Sidor, Eric Sigler, Maddie Simens,Jordan Sitkin, Katarina Slama, Ian Sohl, BenjaminSokolowsky, Yang Song, Natalie Staudacher, Fe-lipe Petroski Such, Natalie Summers, Ilya Sutskever,Jie Tang, Nikolas Tezak, Madeleine B. Thompson,Phil Tillet, Amin Tootoonchian, Elizabeth Tseng,Preston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-lipe Cern Uribe, Andrea Vallone, Arun Vijayvergiya,Chelsea Voss, Carroll Wainwright, Justin Jay Wang,Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,CJ Weinmann, Akila Welihinda, Peter Welinder, Ji-ayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,Clemens Winter, Samuel Wolrich, Hannah Wong,Lauren Workman, Sherwin Wu, Jeff Wu, MichaelWu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-ing Yuan, Wojciech Zaremba, Rowan Zellers, ChongZhang, Marvin Zhang, Shengjia Zhao, TianhaoZheng, Juntang Zhuang, William Zhuk, and BarretZoph. 2024. Gpt-4 technical report. Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, andChristopher D. Manning. 2020. Stanza: A Pythonnatural language processing toolkit for many humanlanguages. In Proceedings of the 58th Annual Meet-ing of the Association for Computational Linguistics:System Demonstrations. Marek Rei and Helen Yannakoudakis. 2016. Composi-tional sequence labeling models for error detectionin learner writing. In Proceedings of the 54th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 11811191,Berlin, Germany. Association for Computational Lin-guistics.",
  "In Proceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing. Associa-tion for Computational Linguistics": "Sascha Rothe, Jonathan Mallinson, Eric Malmi, Sebas-tian Krause, and Aliaksei Severyn. 2021. A simplerecipe for multilingual grammatical error correction.In Proceedings of the 59th Annual Meeting of the As-sociation for Computational Linguistics and the 11thInternational Joint Conference on Natural LanguageProcessing (Volume 2: Short Papers), pages 702707,Online. Association for Computational Linguistics. Alla Rozovskaya, Houda Bouamor, Nizar Habash, Wa-jdi Zaghouani, Ossama Obeid, and Behrang Mohit.2015. The second QALB shared task on automatictext correction for Arabic. In Proceedings of theSecond Workshop on Arabic Natural Language Pro-cessing, pages 2635, Beijing, China. Associationfor Computational Linguistics.",
  "Uri Shaham, Jonathan Herzig, Roee Aharoni, IdanSzpektor, Reut Tsarfaty, and Matan Eyal. 2024. Mul-tilingual instruction tuning with just a pinch of multi-linguality. arXiv preprint arXiv:2401.01854": "Felix Stahlberg and Shankar Kumar. 2021. Syntheticdata generation for grammatical error correction withtagged corruption models. In Proceedings of the16th Workshop on Innovative Use of NLP for Build-ing Educational Applications, pages 3747, Online.Association for Computational Linguistics. Xin Sun, Tao Ge, Shuming Ma, Jingjing Li, Furu Wei,and Houfeng Wang. 2022. A unified strategy formultilingual grammatical error correction with pre-trained cross-lingual language model. arXiv preprintarXiv:2201.10707.",
  "pages 479480, Lisboa, Portugal. European Associa-tion for Machine Translation": "Elena Volodina, Christopher Bryant, Andrew Caines,Orphe De Clercq, Jennifer-Carmen Frey, ElizavetaErshova, Alexandr Rosen, and Olga Vinogradova.2023. MultiGED-2023 shared task at NLP4CALL:Multilingual grammatical error detection. In Pro-ceedings of the 12th Workshop on NLP for ComputerAssisted Language Learning, pages 116, Trshavn,Faroe Islands. LiU Electronic Press. Elena Volodina, Lena Granstedt, Arild Matsson, BetaMegyesi, Ildik Piln, Julia Prentice, Dan Rosn,Lisa Rudebeck, Carl-Johan Schenstrm, GunlgSundberg, et al. 2019. The swell language learnercorpus: From design to annotation. Northern Eu-ropean Journal of Language Technology (NEJLT),6:67104. Jiaan Wang, Yunlong Liang, Fandong Meng, ZengkuiSun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu,and Jie Zhou. 2023. Is ChatGPT a good NLG evalua-tor? a preliminary study. In Proceedings of the 4thNew Frontiers in Summarization Workshop, pages111, Singapore. Association for Computational Lin-guistics.",
  "Zhaofeng Wu, Ananth Balashankar, Yoon Kim, JacobEisenstein, and Ahmad Beirami. 2024. Reuse yourrewards: Reward model transfer for zero-shot cross-lingual alignment. arXiv preprint arXiv:2404.12318": "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale,Rami Al-Rfou, Aditya Siddhant, Aditya Barua, andColin Raffel. 2021. mT5: A massively multilingualpre-trained text-to-text transformer. In Proceedingsof the 2021 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 483498, On-line. Association for Computational Linguistics. Ikumi Yamashita, Satoru Katsumata, Masahiro Kaneko,Aizhan Imankulova, and Mamoru Komachi. 2020.Cross-lingual transfer learning for grammatical er-ror correction.In Proceedings of the 28th Inter-national Conference on Computational Linguistics,pages 47044715, Barcelona, Spain (Online). Inter-national Committee on Computational Linguistics. Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011. A new dataset and method for automaticallygrading ESOL texts.In Proceedings of the 49thAnnual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages180189, Portland, Oregon, USA. Association forComputational Linguistics.",
  "Michihiro Yasunaga, Jure Leskovec, and Percy Liang.2021. LM-critic: Language models for unsupervised": "grammatical error correction. In Proceedings of the2021 Conference on Empirical Methods in NaturalLanguage Processing, pages 77527763, Online andPunta Cana, Dominican Republic. Association forComputational Linguistics. Zheng Yuan, Shiva Taslimipoor, Christopher Davis, andChristopher Bryant. 2021. Multi-class grammaticalerror detection for correction: A tale of two systems.In Proceedings of the 2021 Conference on Empiri-cal Methods in Natural Language Processing, pages87228736, Online and Punta Cana, Dominican Re-public. Association for Computational Linguistics. Yue Zhang, Zhenghua Li, Zuyi Bao, Jiacheng Li,Bo Zhang, Chen Li, Fei Huang, and Min Zhang. 2022.MuCGEC: a multi-reference multi-source evaluationdataset for Chinese grammatical error correction. InProceedings of the 2022 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 31183130, Seattle, United States. Associationfor Computational Linguistics. Yuanyuan Zhao, Nan Jiang, Weiwei Sun, and XiaojunWan. 2018. Overview of the nlpcc 2018 shared task:Grammatical error correction. In Natural LanguageProcessing and Chinese Computing: 7th CCF Inter-national Conference, NLPCC 2018, Hohhot, China,August 2630, 2018, Proceedings, Part II 7, pages439445. Springer. Houquan Zhou, Yumeng Liu, Zhenghua Li, Min Zhang,Bo Zhang, Chen Li, Ji Zhang, and Fei Huang. 2023.Improving Seq2Seq grammatical error correction viadecoding interventions. In Findings of the Associa-tion for Computational Linguistics: EMNLP 2023,pages 73937405, Singapore. Association for Com-putational Linguistics. Michal Ziemski, Marcin Junczys-Dowmunt, and BrunoPouliquen. 2016. The United Nations parallel cor-pus v1.0. In Proceedings of the Tenth InternationalConference on Language Resources and Evaluation(LREC16), pages 35303534, Portoro, Slovenia.European Language Resources Association (ELRA).",
  "A.1Baselines": "Rules We re-implemented Grundkiewicz andJunczys-Dowmunt (2019) using Aspell dictionar-ies2 for the replacement operation.NAT We replicated the NAT model using InfoXLM(Chi et al., 2021) and English as source language,following (Sun et al., 2022) methodology. Fornon-autoregressive translation generation, we usedEuroparl (Koehn, 2005) for Italian, Swedish andCzech and the UN Parallel Corpus v1.0 (Ziemskiet al., 2016) for Arabic and Chinese. We conductedhyper-parameter tuning for the NAT-based data con-struction by exploring the parameter set specifiedin (Sun et al., 2022) and selected the optimal pa-rameters for each language based on performanceon the development set.RT translation We use OPUS-MT (Tiedemannand Thottingal, 2020) as our translation model andEnglish as the bridge language.",
  "A.2Implementation details": "Artificial error generation We use two distinctAEG models to generate errors in target and sourcelanguages, both based on NLL 1.3B-distilled buttrained with different hyper-parameters.For synthetic data generation in target lan-guages, we conduct preliminary grid searches onthe Swedish development set to determine the opti-mal hyperparameters. We select the learning ratefrom {1e-4, 5e-4, 1e-5, 5e-5} and the number ofepochs from {3, 5, 10, 15, 20}. Ultimately, we setthe learning rate to 1e-5 and fine-tune for 3 epochswith a batch size of 24 and a linear scheduler.For synthetic data generation in source lan-guages, we use a different set of hyper-parametersbased on grid searches on the English developmentset. The learning rate is set to 1e-4, and we fine-tune for 10 epochs with a batch size of 24 and alinear scheduler.Grammatical error detection Based on initial ex-periments with the Swedish development set, weuse a learning rate of 1e-5, a batch size of 24, andtrain for 5 epochs with a linear scheduler. In oursecond-stage experiments, we maintain the samesetup but fine-tune for only 1 epoch.Monolingual corpora: As mentioned in .1, our monolingual text data is sourced from theCC100 dataset (Conneau et al., 2020), from which we sample 200,000 error-free instances for eachlanguage. To ensure the text is error-free, we usethe DirectCLT baseline for error detection, includ-ing only sentences verified to be error-free.For all our trainings, we use 3*A6000 GPUswith 48 GB of VRAM.",
  "A.3Similarity Analysis details": "To distinguish between authentic and synthetic in-stances, we train a binary classifier. The classifierprocesses a pair of sentences: a grammatical sen-tence and its corresponding ungrammatical versionseparated by a separator token. Its task is to identifywhether the ungrammatical sentence is syntheticor authentic. We train separate binary classifiersfor each synthetic data generation method, usingmdeberta-v3-base (He et al., 2023) as our back-bone.",
  "A.4GPT-4 analysis details": "To evaluate the linguistic diversity of errors acrossdifferent languages, we employed GPT-4 as an er-ror classifier. Specifically, we used GPT-4 to de-scribe the nature of the errors in sentences. Withoutconstraining GPT-4 to a predetermined set of errortypes, it generated a diverse range of error descrip-tions for similar errors.We then categorized these errors into distinctclusters using a clustering method based on thesentence embeddings generated using sentence-transformers (Reimers and Gurevych, 2019). Inparticular, we applied KMeans clustering with fourdifferent values of K (16, 32, 64, 128). This ap-proach produced multiple sets of clusters, each rep-resenting distinct error patterns within the dataset.For each value of K, we computed the frequencydistribution of errors across the clusters and sub-sequently calculated the entropy of these distribu-tions. To enable comparison across different valuesof K, we normalized the entropy values, ensuringcomparability and eliminating bias from the num-ber of clusters chosen.Finally, to derive a comprehensive measure ofnormalized entropy for each language under study,we averaged the normalized entropy values ob-tained across all K settings. The resulting normal-ized entropy metric provides a robust indicator ofthe diversity of error patterns observed across dif-ferent languages, as illustrated in ."
}