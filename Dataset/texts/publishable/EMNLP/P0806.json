{
  "Abstract": "Knights and knaves problems represent a clas-sic genre of logical puzzles where characterseither tell the truth or lie. The objective is tologically deduce each characters identity basedon their statements. The challenge arises fromthe truth-telling or lying behavior, which in-fluences the logical implications of each state-ment. Solving these puzzles requires not onlydirect deductions from individual statements,but the ability to assess the truthfulness ofstatements by reasoning through various hy-pothetical scenarios.As such, knights andknaves puzzles serve as compelling examplesof suppositional reasoning. In this paper, weintroduce TruthQuest, a benchmark for suppo-sitional reasoning based on the principles ofknights and knaves puzzles. Our benchmarkpresents problems of varying complexity, con-sidering both the number of characters and thetypes of logical statements involved. Evalua-tions on TruthQuest show that large languagemodels like Llama 3 and Mixtral-8x7B exhibitsignificant difficulties solving these tasks. Adetailed error analysis of the models outputreveals that lower-performing models exhibita diverse range of reasoning errors, frequentlyfailing to grasp the concept of truth and lies. Incomparison, more proficient models primarilystruggle with accurately inferring the logicalimplications of potentially false statements.",
  "Introduction": "Well-designed logic puzzles can serve as a valuabletool for gaining deeper insights into the capabilitiesof large language models (LLMs) (Giadikiaroglouet al., 2024; Li et al., 2024; Del and Fishel, 2023).By challenging models to navigate sophisticatedlogic problems, these puzzles can reveal how LLMsidentify patterns, recognize relationships, and em-ploy logical principles (Tong et al., 2023; Dinget al., 2024). In his book \"What is the Name of ThisBook?\", Smullyan (1978) introduced a series of",
  ": An instance of the knights & knaves puzzle.By reasoning about the characters statements and theirtruthfulness, it is possible to deduce that Greeny andBluey must be knights, while Pinky is a knave": "knights and knaves puzzles, where characters areeither knights who always tell the truth or knaveswho always lie.1 The goal is to deduce the identityof each character based on their statements (see). Unlike other deductive reasoning tasks,where premises are typically assumed to be true(Han et al., 2024; Dalvi et al., 2021; Clark et al.,2021), these puzzles require the reasoner to assessthe truthfulness of statements by exploring differenthypothetical scenarios. For instance, if the state-ment of Pinky in were true, Greeny mustbe a knight, thus telling the truth. However, Greenystates that Pinky is lying, which contradicts the ini-tial truth assumption of Pinkys statement. Hence,Pinky must be a knave. If Pinky is a knave, thenGreenys statement is true, and thus Greeny will be",
  "Note: puzzles of this kind have existed before under dif-ferent variations and names (Maurice, 1953; Goodman, 1972)": "a knight. Based on Pinkys false statement, it thenfollows that Bluey must also be a knight. This formof suppositional reasoning, i.e. the ability to reasonconditionally, is essential in scenarios where thelogical ramifications of different possibilities needto be considered, such as in planning or everydayreasoning (Byrne and Handley, 1997).In this paper, we introduce TruthQuest,2 a bench-mark designed to evaluate the suppositional reason-ing capabilities of large language models throughknights and knaves puzzles. We present 2,400 prob-lems of varying complexity, depending on the num-ber of characters and types of logical statementsinvolved (see ). We assess the reasoningbehavior of three model families: Llama 2 (Tou-vron et al., 2023), Llama 3 (Meta AI, 2024), andMixtral-8x7B (Mistral AI, 2023). In addition toevaluating the models task performance, we con-duct an in-depth analysis of their outputs to gaininsights into the types of errors encountered duringreasoning. This is done through both comprehen-sive human inspection and AI-assisted evaluation.Our findings reveal that:",
  "Related Work": "Deductive Reasoning with LLMs.Severalstudies evaluate LLMs on deductive reasoningtasks (Saparov and He, 2023; Dziri et al., 2023;Wan et al., 2024). In line with our research, some ofthese works employ logical puzzles to analyze thereasoning behaviors of LLMs (Ishay et al., 2023;Yao et al., 2023; Jiang et al., 2023). However, tothe best of our knowledge, TruthQuest is the firstdeductive reasoning benchmark that evaluates theability of LLMs to infer both the truthfulness ofstatements and their logical implications.",
  "Dataset": "Various versions of knights and knaves puzzlesexist (Smullyan, 1978; Johnson-Laird and Byrne,1990). In this work, we focus on the most popularvariant, which features only two types of characters:knights, who always tell the truth, and knaves, whoalways lie, as illustrated in . To constructvalid instances of knights and knaves problems,we formalize the puzzle using a two-valued logic.Specifically, knights are assigned the truth valuetrue, while knaves are mapped to false. For a givenpuzzle with n characters, where P denotes the truthvalue of a character and Q is the characters logi-cal claim, the puzzle can be expressed as a singleconjunction using the bi-conditional operator:",
  "= (P1 P3) P2 (P3 (P1 P2))": "where P1, P2, and P3 correspond to the truth val-ues of Greeny, Bluey, and Pinky, respectively. Inthis example, the characters statements are givenas follows: Q1 = P3 (Pinky is a knave!), Q2 =P2 (I am a knight!), and Q3 = (P1 P2)(Greeny is a knight and Bluey is a knave!). Thebi-conditional operator (Pi Qi) reflects that acharacter is either lying or telling the truth. Specifi-cally, it indicates that the statement Qi holds true ifand only if the character is a knight (denoted by Pi).Conversely, if the character is a knave, Pi, thenthe corresponding statement is false, Qi. To de-rive all m possible solutions for such a puzzle, theexpression can be transformed into disjunctivenormal form using Boolean algebra:",
  "E": ": Character statements in TruthQuest. Each type is represented by an example expressed both in naturallanguage and boolean logic. The final column indicates the types of statements included in each statement set. Forinstance, S is the only set that includes self-referential statements alongside accusations and conjunctions. and knaves puzzles are generated. Specifically,instances are created by randomly sampling thestatement of each character from the respective set,Qi C {S, I, E}. Each puzzle is solved byconverting the problem (Equation 1) into disjunc-tive normal form, as shown in Equation 2. WhileBoolean algebra is used to construct the dataset, allstatements and solutions are ultimately presented innatural language. Each dataset sample comprisesthe characters statements and the correspondingsolution, representing the logically valid identity ofeach character (as illustrated in ). For ourbenchmark, we include only instances that havea single, unique solution, i.e., m = 1. Further-more, we consider varying numbers of charactersfor each statement set, specifically: n = 3, 4, 5, 6.This yields 3 4 = 12 data subsets. For each sub-set, 200 problems are generated, resulting in a totalof 2,400 unique instances.",
  "3huggingface.co/meta-llama4huggingface.co/mistralai/Mixtral-8x77B-Instruct-v0.1": "an additional language model, specifically LLaMA-3-8B, which extracts the conclusion in the desiredformat (for the full evaluator prompt, see in the appendix). A schematic overview of thisapproach is presented in in the appendix.Once a conclusion is successfully parsed, the mod-els accuracy is determined by an exact match be-tween their prediction and the correct puzzle solu-tion. For a prediction to be considered accurate,the models must correctly deduce the identity ofall characters.Beyond assessing task performance, we analyzethe models reasoning errors. We manually inspecta subset of the responses from LLaMA-3-8B (zero-shot) and LLaMA-3-70B (zero-shot and four-shotchain-of-thought prompting). Specifically, we eval-uate 10 responses from each of the 12 data subsetsfor each model and setup, totaling 360 responses.This involves parsing the models conclusion andassessing its reasoning against six common errorcategories previously devised, as outlined in in the appendix. This comprehensive manual eval-uation is conducted independently by two hired stu-dents with expertise in data annotation. To assessthe quality of the annotations, we report an overallCohens Kappa value of = 0.70. For a detaileddescription of the manual evaluation procedure andan overview of the inter-annotator agreement foreach error type, please refer to Section C.1 in theappendix. To complement our manual evaluationand assess all model responses with respect to theerror categories devised, we leverage GPT-4 (Ope-nAI et al., 2024) using few-shot prompting. For thecomplete prompt with all few-shot examples, seeFigures 10 to 16 in the appendix. Meta-Evaluation.We assess the quality of ourevaluation procedures by comparing the results ob-tained via automatic evaluation with our manualassessment. Respective results are reported in Sec-tion 5 and Appendix C.2.",
  "Results": "Task Performance provides an overviewof the models task performance on TruthQuest.The table includes results for all models whenprompted in a zero-shot setting, with additionalresults for LLaMA-3-70B using various promptingtechniques (detailed results for other models can befound in in the appendix). We observe thatunder zero-shot prompting, all models exhibit rel-atively poor performance across the different datasubsets, often performing at or below chance level.Although LLaMA-3-70B generally outperformsother models, its accuracy significantly declinesas the number of charactersand consequently,the number of inference stepsincreases. Whenguided via chain-of-thought prompting, LLaMA-3-70B shows performance improvements for prob-lems involving fewer characters, particularly withstatements sampled from set S. Other promptingtechniques, such as few-shot prompting or zero-shot CoT (Kojima et al., 2022), do not substantiallyenhance LLaMA-3-70Bs task performance. Content Effects.For our analysis, we replace theterms knights and knaves with the pseudo-wordsjabbas and tettes to reduce the likelihood that mod-els have been exposed to similar problems duringtraining. Interestingly, we find that the choice ofterms for knights and knaves seems to have nosubstantial impact on the models performance, asshown in in the appendix.",
  "(b) LLaMA-3-70B (four-shot CoT)": ": Relative occurrences of the reasoning errorsdisplayed by LLaMA-3-8B and LLaMA-3-70B whenprompted via four-shot chain-of-thought. Values are ob-tained from GPT-4-based evaluations. Error categoriesare abbreviated for a more comprehensive overview:(SR) False statement reproduction, (TS) Assuming state-ments to be true, (TL) Misunderstanding the concept oftruth and lies, (LO) Misunderstanding logical operators,(UC) Unjustified conclusion, and (UF) Unfaithfulness.",
  "Conclusion": "In this paper we introduce TruthQuest, a bench-mark for suppositional reasoning based on knightsand knaves puzzles. We demonstrate that LLMsexhibit significant difficulties solving these tasks.Our error analysis reveals that less proficient LLMsexhibit diverse errors, often failing to grasp the con-cept of truth and lies. In contrast, more proficientmodels primarily struggle with logical deductionsfrom potentially false statements.",
  "Limitations": "While we introduce TruthQuest, a novel benchmarkdesigned to evaluate the suppositional reasoningcapabilities of large language models, several lim-itations remain that could be addressed in futureresearch. Task SetupCurrently, TruthQuest includes onlyknights and knaves puzzles with a single, uniquesolution. Future work could expand this restrictionto examine the impact of none or several solutionson model performance and behavior. Additionally,the benchmark is limited to simple propositionalstatements, as outlined in . Future iterationscould incorporate more complex statement typesthat require more advanced inferences. Variationsof knights and knaves puzzles, which consider addi-tional characters or altered character attributes, alsopresent opportunities for further exploration. Forinstance, Johnson-Laird and Byrne (1990) proposeproblems involving two types of persons: logicians,who always make valid deductions, and politicians,who never make valid deductions. An exampleproblem states: A says that either B is telling thetruth or else is a politician (but not both). B saysthat A is lying. C deduces that B is a politician. Is Ca logician? Such variations represent compellingdirections for future research. Evaluation FrameworkOur manual evaluationframework is constrained by the number of annota-tors and the volume of annotations provided. De-spite our efforts to optimize available resources,these constraints may impact the scalability andgeneralizability of our results. While our automaticevaluation procedure offers a promising alternative,we found that error annotations obtained throughthis method exhibit only fair overall agreementwith human annotations at the instance level (seeSection C.2 in the appendix for further details). Ad-ditionally, although we consider various prompting techniques in our study, future research could ex-plore the impact of more advanced methods, suchas Tree-of-Thoughts (Yao et al., 2023) or Graph-of-Thoughts (Besta et al., 2024), on model perfor-mance. We express our gratitude to the members of theMaiNLP lab for their invaluable and insightful feed-back. We specifically appreciate the suggestions ofDiego Frassinelli, Michael Hedderich, Siyao Peng,Robert Litschko, Beiduo Chen, Jian Lan, XinpengWang, Verena Blaschke, Elena Senger, and Robvan der Goot. Additionally, we would like to thankHuangyan Shan and Sebastian Loftus for their ex-ceptional work and support in data annotation. Weare also grateful to the anonymous reviewers fortheir constructive feedback. The emojis featuredin are designed by OpenMoji the open-source emoji and icon project (License: CC BY-SA 4.0). Lastly, we recognize the support for BPthrough the ERC Consolidator Grant 101043235. Maciej Besta, Nils Blach, Ales Kubicek, Robert Ger-stenberger, Michal Podstawski, Lukas Gianinazzi,Joanna Gajda, Tomasz Lehmann, Hubert Niewiadom-ski, Piotr Nyczyk, and Torsten Hoefler. 2024. Graphof thoughts: Solving elaborate problems with largelanguage models. Proceedings of the AAAI Confer-ence on Artificial Intelligence, 38(16):1768217690.",
  "Ruth M.J Byrne and Simon J Handley. 1997. Reasoningstrategies for suppositional deductions. Cognition,62(1):149": "Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2021.Transformers as soft reasoners over language. InProceedings of the Twenty-Ninth International JointConference on Artificial Intelligence, IJCAI20. Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, ZhengnanXie, Hannah Smith, Leighanna Pipatanangkura, andPeter Clark. 2021. Explaining answers with entail-ment trees. In Proceedings of the 2021 Conferenceon Empirical Methods in Natural Language Process-ing, pages 73587370, Online and Punta Cana, Do-minican Republic. Association for ComputationalLinguistics. Maksym Del and Mark Fishel. 2023. True detective: Adeep abductive reasoning benchmark undoable forGPT-3 and challenging for GPT-4. In Proceedingsof the 12th Joint Conference on Lexical and Com-putational Semantics (*SEM 2023), pages 314322,Toronto, Canada. Association for Computational Lin-guistics. Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu,Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan,Qingwei Lin, and Dongmei Zhang. 2024. Everythingof thoughts: Defying the law of penrose triangle forthought generation. Preprint, arXiv:2311.04254. Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang (Lor-raine) Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck,Peter West, Chandra Bhagavatula, Ronan Le Bras,Jena Hwang, Soumya Sanyal, Xiang Ren, Allyson Et-tinger, Zaid Harchaoui, and Yejin Choi. 2023. Faithand fate: Limits of transformers on compositional-ity. In Advances in Neural Information ProcessingSystems, volume 36, pages 7029370332. Curran As-sociates, Inc.",
  "Nelson Goodman. 1972. Problems and Projects. Bobbs-Merrill, Indianapolis": "Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhent-ing Qi, Martin Riddell, Wenfei Zhou, James Coady,David Peng, Yujie Qiao, Luke Benson, Lucy Sun,Alex Wardle-Solano, Hannah Szabo, EkaterinaZubova, Matthew Burtell, Jonathan Fan, Yixin Liu,Brian Wong, Malcolm Sailor, Ansong Ni, LinyongNan, Jungo Kasai, Tao Yu, Rui Zhang, Alexander R.Fabbri, Wojciech Kryscinski, Semih Yavuz, Ye Liu,Xi Victoria Lin, Shafiq Joty, Yingbo Zhou, Caim-ing Xiong, Rex Ying, Arman Cohan, and DragomirRadev. 2024. Folio: Natural language reasoning withfirst-order logic. Preprint, arXiv:2209.00840. Adam Ishay, Zhun Yang, and Joohyung Lee. 2023.Leveraging large language models to generate an-swer set programs. In Proceedings of the 20th In-ternational Conference on Principles of KnowledgeRepresentation and Reasoning, KR 2023, Proceed-ings of the International Conference on KnowledgeRepresentation and Reasoning, pages 374383. As-sociation for the Advancement of Artificial Intelli-gence. Publisher Copyright: 2023 Proceedings ofthe International Conference on Knowledge Repre-sentation and Reasoning. All rights reserved; 20thInternational Conference on Principles of KnowledgeRepresentation and Reasoning, KR 2023 ; Confer-ence date: 02-09-2023 Through 08-09-2023.",
  "Mistral AI. 2023. Mixtral of experts: A high qualitysparse mixture-of-experts. Accessed: 2024-06-01": "OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-man, Diogo Almeida, Janko Altenschmidt, Sam Alt-man, Shyamal Anadkat, Red Avila, Igor Babuschkin,Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-ing Bao, Mohammad Bavarian, Jeff Belgum, Ir-wan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,Christopher Berner, Lenny Bogdonoff, Oleg Boiko,Madelaine Boyd, Anna-Luisa Brakman, Greg Brock-man, Tim Brooks, Miles Brundage, Kevin Button,Trevor Cai, Rosie Campbell, Andrew Cann, BrittanyCarey, Chelsea Carlson, Rory Carmichael, BrookeChan, Che Chang, Fotis Chantzis, Derek Chen, SullyChen, Ruby Chen, Jason Chen, Mark Chen, BenChess, Chester Cho, Casey Chu, Hyung Won Chung,Dave Cummings, Jeremiah Currier, Yunxing Dai,Cory Decareaux, Thomas Degry, Noah Deutsch,Damien Deville, Arka Dhar, David Dohan, SteveDowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,Simn Posada Fishman, Juston Forte, Isabella Ful-ford, Leo Gao, Elie Georges, Christian Gibson, VikGoel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, ScottGray, Ryan Greene, Joshua Gross, Shixiang ShaneGu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,Yuchen He, Mike Heaton, Johannes Heidecke, ChrisHesse, Alan Hickey, Wade Hickey, Peter Hoeschele,Brandon Houghton, Kenny Hsu, Shengli Hu, XinHu, Joost Huizinga, Shantanu Jain, Shawn Jain,Joanne Jang, Angela Jiang, Roger Jiang, HaozhunJin, Denny Jin, Shino Jomoto, Billie Jonn, Hee-woo Jun, Tomer Kaftan, ukasz Kaiser, Ali Ka-mali, Ingmar Kanitscheider, Nitish Shirish Keskar,Tabarak Khan, Logan Kilpatrick, Jong Wook Kim,Christina Kim, Yongjik Kim, Jan Hendrik Kirch-ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,ukasz Kondraciuk, Andrew Kondrich, Aris Kon-stantinidis, Kyle Kosic, Gretchen Krueger, VishalKuo, Michael Lampe, Ikai Lan, Teddy Lee, JanLeike, Jade Leung, Daniel Levy, Chak Ming Li,Rachel Lim, Molly Lin, Stephanie Lin, MateuszLitwin, Theresa Lopez, Ryan Lowe, Patricia Lue,Anna Makanju, Kim Malfacini, Sam Manning, TodorMarkov, Yaniv Markovski, Bianca Martin, KatieMayer, Andrew Mayne, Bob McGrew, Scott MayerMcKinney, Christine McLeavey, Paul McMillan,Jake McNeil, David Medina, Aalok Mehta, JacobMenick, Luke Metz, Andrey Mishchenko, PamelaMishkin, Vinnie Monaco, Evan Morikawa, DanielMossing, Tong Mu, Mira Murati, Oleg Murk, David Mly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh,Long Ouyang, Cullen OKeefe, Jakub Pachocki, AlexPaino, Joe Palermo, Ashley Pantuliano, Giambat-tista Parascandolo, Joel Parish, Emy Parparita, AlexPassos, Mikhail Pavlov, Andrew Peng, Adam Perel-man, Filipe de Avila Belbute Peres, Michael Petrov,Henrique Ponde de Oliveira Pinto, Michael, Poko-rny, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-ell, Alethea Power, Boris Power, Elizabeth Proehl,Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,Cameron Raymond, Francis Real, Kendra Rimbach,Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-der, Mario Saltarelli, Ted Sanders, Shibani Santurkar,Girish Sastry, Heather Schmidt, David Schnurr, JohnSchulman, Daniel Selsam, Kyla Sheppard, TokiSherbakov, Jessica Shieh, Sarah Shoker, PranavShyam, Szymon Sidor, Eric Sigler, Maddie Simens,Jordan Sitkin, Katarina Slama, Ian Sohl, BenjaminSokolowsky, Yang Song, Natalie Staudacher, Fe-lipe Petroski Such, Natalie Summers, Ilya Sutskever,Jie Tang, Nikolas Tezak, Madeleine B. Thompson,Phil Tillet, Amin Tootoonchian, Elizabeth Tseng,Preston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-lipe Cern Uribe, Andrea Vallone, Arun Vijayvergiya,Chelsea Voss, Carroll Wainwright, Justin Jay Wang,Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,CJ Weinmann, Akila Welihinda, Peter Welinder, Ji-ayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,Clemens Winter, Samuel Wolrich, Hannah Wong,Lauren Workman, Sherwin Wu, Jeff Wu, MichaelWu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-ing Yuan, Wojciech Zaremba, Rowan Zellers, ChongZhang, Marvin Zhang, Shengjia Zhao, TianhaoZheng, Juntang Zhuang, William Zhuk, and Bar-ret Zoph. 2024. Gpt-4 technical report. Preprint,arXiv:2303.08774.",
  "Raymond M. Smullyan. 1978. What is the Name of ThisBook?: The Riddle of Dracula and Other LogicalPuzzles. Prentice-Hall, Englewood Cliffs, N.J": "Yongqi Tong, Yifan Wang, Dawei Li, Sizhe Wang,Zi Lin, Simeng Han, and Jingbo Shang. 2023. Elim-inating reasoning via inferring with planning: Anew framework to guide llms non-linear thinking.Preprint, arXiv:2310.12342. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar- tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288. Yuxuan Wan, Wenxuan Wang, Yiliu Yang, YouliangYuan, Jen tse Huang, Pinjia He, Wenxiang Jiao, andMichael R. Lyu. 2024. A & b == b & a: Triggeringlogical reasoning failures in large language models.arXiv preprint arXiv:2401.00757. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,and Denny Zhou. 2022. Chain-of-thought prompt-ing elicits reasoning in large language models. InAdvances in Neural Information Processing Systems,volume 35, pages 2482424837. Curran Associates,Inc. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,Tom Griffiths, Yuan Cao, and Karthik Narasimhan.2023. Tree of thoughts: Deliberate problem solvingwith large language models. In Advances in NeuralInformation Processing Systems, volume 36, pages1180911822. Curran Associates, Inc.",
  "A.1Language Models": "As outlined in , six distinct large languagemodels from three open-access model familiesare evaluated in this study. Detailed information,including the number of parameters and the contextlength for each model, is provided in . Eachmodel is prompted with a system message thatoffers context about the task setup and specifiesthe required response format. Following this, auser prompt containing the task description isgiven. The complete prompt is depicted in . For few-shot setups, examples are presentedin dialogue format, with the desired responseindicated using the assistants special tokens.Model responses are generated using nucleussampling, utilizing the models default values as specified on the Huggingface Platform (top-p = 0.9, temperature T = 0.6).5. All few-shotprompts as well as the code for prompting andevaluating models on TruthQuest are publicly ac-cessible at errors (see ), are available at",
  "A.2Error Categorization": "To gain a deeper understanding of the models rea-soning behavior, we develop six different error cat-egories that encompass common errors observed inthe models reasoning. These categories are estab-lished through a preliminary manual examinationof the models responses. Detailed descriptions ofeach error category are provided in . It isimportant to note that these error categories are notmeant to be exhaustive. Instead, they are intendedto offer practical insights into the models frequentfailure modes. For clarity, an example for eacherror category is provided below: False statement reproduction (SR)Smaller,less advanced models, such as LLaMA-2-7B, oftenfail to accurately reproduce statements from theproblem description. For example, a model mayincorrectly reproduce Pinkys statement Greenyis a knight and Bluey is a knave as Bluey statesthat both Pinky and Greeny are knaves. Assuming statements to be true (TS)Modelssometimes overlook the possibility that statementsmay be false and instead assume the truth of eachstatement. For example, a model might directlyconclude from Pinkys statement Greeny is aknight and Bluey is a knave that Greeny is indeeda knight and Bluey a knave, without consideringthe potential falsity of the statement. Misunderstanding the concept of truth and lies(TL)Some models demonstrate a lack of un-derstanding regarding truth and falsehood. Forinstance, a model may incorrectly assume thatknights lie or knaves tell the truth.Addition-ally, models sometimes erroneously assume thatknights only make statements about other knightsand knaves only about other knaves.",
  "Mixtral-8x7B-InstructMixtral-8x7B46.7B32K tokens---SFT, DPO": ": Details about the models used in this study. Tokens refer to the number of tokens in the pre-training data.Similarly, the context length, GPU hours and carbon emissions relate to the base models pre-training. Carbonemissions are reported as tCO2eq. We use the following abbreviations for fine-tuning: supervised fine-tuning (SFT),reinforcement learning with human feedback (RLHF), direct preference optimization (DPO). Information aboutLlama 2 is taken from Touvron et al. (2023), while properties of Llama 3 are reported by Meta AI (2024). ForMixtral-8x7B, we consider the blog post of Mistral AI (2023). Dashes denote unavailable information. Misunderstandinglogicaloperators(LO)When processing false statements, such as Pinkysassertion that Greeny is a knight and Bluey is aknave (when Pinky is actually a knave), modelsoften fail to infer the logical implications of thelie. Specifically, the model may not deduce thatthe possibilities are: (i) both Greeny and Bluey areknaves, (ii) both are knights, or (iii) Greeny is aknave and Bluey is a knight. Unjustified Conclusion (UC)Models occasion-ally draw conclusions without providing a valid jus-tification. For example, a model may erroneouslyassert that Greeny is a knave without offering anyreasoning to support this conclusion. Unfaithfulness (UF)Models are sometimes in-consistent in their reasoning. For instance, a modelmight first deduce that Greeny is a knight, but latercontradict this by asserting that Greeny is a knave,without addressing the discrepancy in its reasoning.",
  "plexity, i.e.fewer number of characters. Simi-larly, other prompting techniques such as few-shotprompting or zero-shot CoT do not seem to in-crease model performance": "B.1.1Content EffectsIn conventional knights and knaves puzzles, knightsare characters who always tell the truth, whileknaves always lie. However, this setup can be mod-ified by assigning different terms to these charac-ters. It is likely that the models evaluated in thisstudy have encountered conventional knights andknaves puzzles during their training procedure, assuch examples are readily available on the inter-net (Smullyan, 1978). By altering the terms usedfor truth-tellers and liars, we can significantly re-duce the likelihood that the models have been ex-posed to similar samples in our benchmark. Conse-quently, we analyze the impact of the terminologyused for knights and knaves on model performance.Specifically, we examine three different formula-tions: (i) the conventional knights and knaves, (ii)neutral descriptions such as truth-tellers and liars,and (iii) pseudo-terms such as jabbas and tettes. illustrates the zero-shot performance ofall models for each terminology setup across thedifferent subsets of TruthQuest. Surprisingly, wefind no substantial impact of the choice of termson the models task performance. We hypothesizethat this may be because the specific instances gen-erated for TruthQuest have not yet been exposed tothe internet. Consequently, the models might haveencountered only a negligible fraction of instancesby chance during their training process.",
  "B.2Error Analysis": "We examine the models reasoning errors throughboth comprehensive manual inspections and AI-based evaluations of their rationales. The follow-ing sections present additional results from bothevaluation methods. B.2.1Human EvaluationAs outlined in , we manually evaluate asubset of the models responses to asses their errorsencountered during reasoning (for a detailed expla-nation of the evaluation procedure, please refer toSection C.1). To supplement our findings summa-rized in , , we report the common errors ex-hibited by LLaMA-3-70B when prompted in a zero-shot setting (see ). We observe that, similarto LLaMA-3-8B (zero-shot) and LLaMA-3-70B(four-shot CoT), LLaMA-3-70B (zero-shot) fre-quently displays errors when deducing the logicalimplications of potentially false statements (LO).Additionally, we find that while the model strugglesless with understanding the concept of truth andlies (TL) compared to LLaMA-3-8B (zero-shot),it still exhibits this error category more frequentlythan LLaMA-3-70B (four-shot CoT). This trend,indicating that more proficient models better graspthe concept of truth and lies than lower-performingones, is also reflected in our analysis of all model re-sponses conducted via automatic LLM-based eval-uation (for details, refer to Section B.2.2). B.2.2AI-Assisted EvaluationHigh-quality human annotations are typicallycostly to obtain. In our study, we manually in-spect 360 model responses from three differentLLMs, where each instance is evaluated twice in-dependently by two annotators (for details on theevaluation procedure, please refer to Section C.1). However, as our benchmark comprises 2,400 differ-ent instances, this evaluation procedure only coversa small subset of the models responses. To com-plement our manual evaluation, we employ GPT-4(OpenAI et al., 2024)6 to assess all 2,400 responsesof a model with respect to the reasoning errors out-lined in (for details about the exact promptswe employ, or the alignment between human andAI-based evaluation, please refer to Section C.2).The respective results are illustrated in .We present the relative occurrences of each errorcategory for LLaMA-2-7B (zero-shot), LLaMA-3-8B (zero-shot), LLaMA-3-70B (zero-shot), andLLaMA-3-70B (four-shot CoT). All values are av-",
  "Error": "0000.033 0.130.170.170.13 0.570.370.530.47 0.830.9310.9 00.0670.0670.2 0.0330.0330.0670 0.0 0.2 0.4 0.6 0.8 1.0 : Relative occurrences of the reasoning errorsdisplayed by LLaMA-3-70B when prompted in a zero-shot setting. Values are obtained from human evaluation.Error categories are abbreviated for a more comprehen-sive overview: (SR) False statement reproduction, (TS)Assuming statements to be true, (TL) Misunderstand-ing the concept of truth and lies, (LO) Misunderstand-ing logical operators, (UC) Unjustified conclusion, and(UF) Unfaithfulness. C=3C=4C=5C=6 Number of characters SR TS TL LO UC UF",
  "(d) LLaMA-3-70B (four-shot CoT)": ": Relative occurrences of the reasoning errors displayed by LLaMA-2-7B, LLaMA-3-8B, and LLaMA-3-70B in zero-shot prompting, as well as LLaMA-3-70B when prompted via four-shot chain-of-thought. Values areobtained from GPT-4-based evaluations. Error categories are abbreviated for a more comprehensive overview: (SR)False statement reproduction, (TS) Assuming statements to be true, (TL) Misunderstanding the concept of truth andlies, (LO) Misunderstanding logical operators, (UC) Unjustified conclusion, and (UF) Unfaithfulness. eraged across the different statement sets for eachnumber of characters. Consistent with the resultsobtained through human evaluation, we observe astrong trend for higher-performing models to con-verge on errors related to deducing the correct log-ical implications of statements (LO). In contrast,lower-performing models such as LLaMA-2-7B(zero-shot) or LLaMA-3-8B (zero-shot), displaydiverse errors, ranging from misconceptions abouttruth and lies (TL) to unjustified conclusions (UC).Notably, LLaMA-2-7B is the only model that fre-quently fails to consider statements as lies (TS).",
  "CEvaluation Procedures": "In this study, we utilize two types of evaluationmethods: human evaluations and AI-assisted evalu-ations. Below, we provide further details on eachmethod, including the instructions given to humanannotators and the process by which large language models are employed to generate similar annota-tions automatically. Finally, we assess the qualityof our automatic evaluation procedures by compar-ing the results to the results obtained via manualassessment.",
  "LLaMA-3-70B four CoT-0.900.510.740.040.34": ": Cohens Kappa values to assess the human inter-annotator agreement across different models, promptsetups and and error categories. Error categories are abbreviated for a more comprehensive overview: (SR) Falsestatement reproduction, (TS) Assuming statements to be true, (TL) Misunderstanding the concept of truth and lies,(LO) Misunderstanding logical operators, (UC) Unjustified conclusion, and (UF) Unfaithfulness. the annotators to knights and knaves puzzles, ask-ing them to solve these puzzles by hand to famil-iarize themselves with the task structure. Once theannotators are confident in solving puzzles of thisstyle, we present exemplary responses from themodels evaluated in this study. Together, we dis-cuss notable behaviors and errors exhibited by themodels. Next, we introduce the annotators to thesix error categories outlined in . We proceedonly when both annotators confirm their full un-derstanding of each error type and have no furtherquestions. The annotators are then tasked with in-dependently annotating model responses. For eachresponse, they parse the models conclusion and as-sign a binary label (yes/no) to each error category,indicating its presence or absence in the modelsreasoning. Initially, the annotators work with prac-tice examples to highlight and address any ambigu-ities in the annotation process. They only move onto labeling the actual model responses when theyare confident in their understanding of the labelingprocess. To maintain high annotation quality, weask both annotators to review their annotations, en-suring any potential errors in their annotations areaccounted for.",
  "C.1.2Inter-Annotator Agreement": "To assess the quality of our manual annotations, wecalculate the inter-annotator agreement, reportingan overall Cohens kappa value of = 0.70,which indicates substantial agreement betweenthe two annotators. presents Cohenskappa values for each model and error type. Weobserve that the agreement rate varies acrossdifferent categories, ranging from none to perfectagreement. Notably, the values for False statementreproduction (FS) in LLaMA-3-70B (zero-shot)and Unjustified conclusion (FS) in LLaMA-3-70B(four-shot CoT) are almost zero. This is likely dueto a strong bias in the label distribution towards nolabels, as these errors rarely occur in these models.All human annotations are publicly available at",
  "C.2AI-Assisted Evaluation": "In addition to the human evaluation, we employGPT-4 to assess the models reasoning errors.Similar to the human annotators, GPT-4 is taskedwith assigning binary labels (yes/no) to eacherror category described in , indicatingthe presence or absence of the error type in themodels reasoning. Additionally, GPT-4 is requiredto provide a justification for each label assigned.To ensure GPT-4s comprehension of each errorcategory, we provide detailed descriptions in themodel input. The full prompt can be found in. Furthermore, we present six few-shotexamplesillustratingthedesiredannotationbehavior (see Figures 11 to 16). To assess thequality of the annotations, we compute thePearson correlation for the error distributionsof LLaMA-3-8B (zero-shot),LLaMA-3-70B(zero-shot), and LLaMA-3-70B (four-shot CoT)between the automatically obtained labels and thehuman annotations. All correlation coefficientsand their corresponding p-values are reported in. Overall, we find that the error distributionobtained through GPT-4 strongly correlates withthe error distribution obtained via manual labeling.However, on an instance level, we observe only fairagreement, with an overall Cohens kappa value of = 0.34. For our automatic evaluation of 9,600model responses (4 models 2,400 responses),the cost was approximately $250. All annotationsobtained through GPT-4 are publicly available at Task performance.To assess the quality of ourperformance evaluation procedure depicted in Fig-ure 6, we compute the proportion of instanceswhere the final conclusions derived from our two-step method match those reported by manual as-sessment. We find an alignment of 100%.",
  "DPrompts": "We present all prompts used in this study. Thetask prompt is shown in . il-lustrates the prompt for the two-step conclusionevaluator.Additionally, the system prompt forGPT-4 is provided in , along with thefew-shot examples in Figures 11 to 16. Few-shotexamples are designed to demonstrate the desiredassistant behavior (highlighted in orange) in re-sponse to the corresponding user request (high-lighted in blue). All prompts are publicly accessi-ble at",
  "(i) jabbas and tettes": ": Zero-shot performance of all models across the data subsets of TruthQuest, focusing on the terminologyused for characters who always tell the truth, and those who always lie. Three different formulations are compared:(i) the conventional knights and knaves, (ii) neutral descriptions such as truth-tellers and liars, and (iii) pseudo-termssuch as jabbas and tettes.",
  "A: knight/knaveB: knight/knaveC: knight/knave... [/INST]": ": The task prompt. Placeholders such as <number of characters> and <statements> are replaced by thecorresponding input of the problem description. Note that the arrangement and usage of special tokens may varydepending on the specific language model employed. [INST] SYSYour task is to extract information from a given piece of text. You are given a piece of text that reasons about the identity of a set of characters. You mustextract the final answer from the given text by clearly stating the deduced identity of each character in the following format:A: ...B: ...C: ......You must strictly follow this format. Do not generate any additional text. You must only extract information that is present in the text provided. Do not addinformation that goes beyond the one contained in the text at hand./SYS### Instruction ###You are given a piece of TEXT that reasons about the identity of a set of characters. You must extract the FINAL ANSWER from the TEXT by clearlystating the deduced identity of each character in the following format:### FINAL ANSWER:A: ...B: ...C: ......You must strictly follow this format. Do not generate any additional text. You must only extract information that is present in the text provided. Do not addinformation that goes beyond the one contained in the text at hand. If \"not a\" is placed in front of the identity, flip the result of that character, e.g. \"not aknight\" becomes \"knave\", while \"not a knave\" becomes \"knight\". You should simplify results like \"knight/knight\" to \"knight\" and \"knave/knave\" to \"knave\".However, do not simplify \"knight/knave\", or \"knave/knight\".### TEXT:Great! Lets apply logical reasoning to deduce the identity of each character.A: Based on statement A, we know that C is a knave and D is a knave. This means that C and D are both knaves.B: Statement B tells us that C is a knave and D is a knave. Since C and D are both knaves, we can conclude that B is a knight.C: Statement C tells us that D is a knave. Since D is already identified as a knave in statements A and B, we can conclude that C is a knight.D: Statement D tells us that B is a knight. Since B has already been identified as a knight in statement B, we can confirm that D is a knave.Therefore, the identities of each character are:A: not a knightB: knight/knaveC: knight/knightD: knave [/INST]",
  "<|assistant|> ### FINAL ANSWER:A: knaveB: knight/knaveC: knightD: knave </|assistant|>": "[INST] ### TEXT:Sure! Lets start by analyzing the statements one by one.1. A: B is a knave.Since A says that B is a knave, we can deduce that B is a knave.2. B: If D is a knight, then C is a knave.This statement implies that if D is a knight, then C is a knave. However, since B is a knave, we can deduce that D is not a knight. Therefore, C must be aknight.3. C: B is a knight.Since C says that B is a knight, we can deduce that B is a knight.4. D: A is a knave and C is a knave.This statement is not helpful in determining the identity of D, as D is simply repeating what A and C have said.Now, lets summarize our conclusions:A: knaveB: knaveC: knightD: unable to determineHowever, based on the statements, we can deduce that B and C are knaves, and A is a knight. Therefore, the correct identities of the characters are: A:knightB: knaveC: knaveD: unable to determine [/INST]",
  "<|assistant|> ### FINAL ANSWER:A: knightB: knaveC: knaveD: unable to determine </|assistant|>": "[INST] ### TEXT:Okay, I understand the problem. Heres my reasoning:From statement A, if B is a knave, then C is a knight. This means that if B is a knave, then C must be a knight.From statement B, if A is a knave, then C is a knight. This means that if A is a knave, then C must be a knight.Now, lets examine the third statementC: B is a knave. If B is a knave, then C is a knave. But we know that knaves always lie, so we cant trust what C says. Therefore, we cant determine theidentity of C based on this statement alone.However, we can use the first two statements to deduce the identity of C. If A is a knave, then C is a knight, and if B is a knave, then C is a knight. Sincewe dont know the identity of A and B, we cant determine the identity of C.CONCLUSION:A: knave/knight (cannot be determined)B: knave/knight (cannot be determined)C: knave [/INST]",
  ": The three-shot conclusion evaluator prompt. Examples are presented in chat form, where the task promptis depicted in blue, and the desired answer is exemplified in orange": "<|system|>Your task is to classify whether a set of errors are present in the response of a large language model. You are given the initial PROBLEM STATEMENTand the models RESPONSE.You must consider the following error types: - False statement reproduction (A statement of the problem description is repeated incorrectly.)- Assuming statements to be true (The possibility that a characters statement is a lie is not considered. The model does not question the premises truth butassumes it as given.)- Misunderstanding the concept of truth and lies (The model does not understand that a knights always tells the truth and a knave always lies. For instance,the model mistakenly assumes that knights lie, while knaves tell the truth. Alternatively, the model assumes that knights can only talk about other knightsand knaves can only talk about knaves.)- Misunderstanding logical operators (Logical operators and their implications are not properly understood. The model fails at inferring the logical impli-cations of a statement. For instance, if character As statement is a logical conjunction (e.g. B and C) and A is a knave, the model does not consider allpossible implications of that lie (not B and C, B and not C, not B and not C).)- Unjustified conclusion (A conclusion about the identity of a characters such as X is a knights/knave is presented without ANY justification. Note thatthis error does not address the logical validity of a justification, but checks whether a justification is present or not. As an example of this error, the modelmight simply state: \"Since A is a knights/knave...\" without prior justification.)- Unfaithfulness (A conclusion is drawn that explicitly contradicts a conclusion previously inferred by the model, without the model reflecting on this contra-diction.) For each error type, you must classify whether the error is present in the models response (yes) or not (no). Start with a short justification for your deci-sion (max. 1-2 sentences) and then clearly state whether the error type is present (yes) or not (no).Strictly adhere to the following output format:",
  ": The system message of the six-shot error evaluator prompt": "<|user|>### Instruction ###You are given a PROBLEM STATEMENT and the RESPONSE of a large language model, respectively. Your task is to classify whether a set of errors arepresent in the models response. For each of the following error types, classify whether the error is present in the models response (yes) or not (no). - False statement reproduction (A statement of the problem description is repeated incorrectly.)- Assuming statements to be true (The possibility that a characters statement is a lie is not considered. The model does not question the premises truth butassumes it as given.)- Misunderstanding the concept of truth and lies (The model does not understand that a truth-teller always tells the truth and a liar always lies. For instance,the model mistakenly assumes that truth-tellers lie, while liars tell the truth. Alternatively, the model assumes that truth-tellers can only talk about othertruth-tellers and liars can only talk about liars.)- Misunderstanding logical operators (Logical operators and their implications are not properly understood. The model fails at inferring the logical implica-tions of a statement. For instance, if character As statement is a logical conjunction (e.g. B and C) and A is a liar, the model does not consider all possibleimplications of that lie (not B and C, B and not C, not B and not C).)- Unjustified conclusion (A conclusion about the identity of a characters such as X is a truth-teller/liar is presented without ANY justification. Note thatthis error does not address the logical validity of a justification, but checks whether a justification is present or not. As an example of this error, the modelmight simply state: \"Since A is a truth-teller/liar...\" without prior justification.)- Unfaithfulness (A conclusion is drawn that explicitly contradicts a conclusion previously inferred by the model, without the model reflecting on this contra-diction.)",
  "### PROBLEM STATEMENTA: If C is a truth-teller, then D is a truth-teller.B: C is a liar and A is a liar.C: D is a liar and B is a liar.D: B is a truth-teller and C is a liar": "### RESPONSE1. A: If C is a truth-teller, then D is a truth-teller.This statement is made by A, who is either a truth-teller or a liar. If A is a truth-teller, then the statement is true, and D is indeed a truth-teller. But if A isa liar, then the statement is false, and D is not a truth-teller. So, As statement is true only if A is a truth-teller. 2. B: C is a liar and A is a liar.This statement is made by B, who is either a truth-teller or a liar. If B is a truth-teller, then the statement is false, as C is not a liar. But if B is a liar, thenthe statement is true, as C is indeed a liar. So, Bs statement is true only if B is a liar. 3. C: D is a liar and B is a liar.This statement is made by C, who is either a truth-teller or a liar. If C is a truth-teller, then the statement is false, as B is not a liar. But if C is a liar, thenthe statement is true, as D is indeed a liar. So, Cs statement is true only if C is a liar. 4. D: B is a truth-teller and C is a liar. This statement is made by D, who is either a truth-teller or a liar. If D is a truth-teller, then the statement is true, as B is indeed a truth-teller. But if D is aliar, then the statement is false, as C is not a liar. So, Ds statement is true only if D is a truth-teller. Now, lets combine the information:* As statement is true only if A is a truth-teller.* Bs statement is true only if B is a liar.* Cs statement is true only if C is a liar.* Ds statement is true only if D is a truth-teller. From statement 2, we know B is a liar. From statement 4, we know D is a truth-teller.From statement 1, we know that if A is a truth-teller, then D is a truth-teller. Since we already know D is a truth-teller, A must be a truth-teller.From statement 3, we know C is a liar.",
  "### Assuming statements to be true- Justification: The possibility that the characters statements are lies is considered by the model.- Label: no": "### Misunderstanding the concept of truth and lies- Justification: When discussing Bs statement, the model says: \"So, Bs statement is true only if B is a liar\". However, if B were a liar, its statement wouldnot be true. This shows a model failure to understand the concept of a liar.- Label: yes ### Misunderstanding logical operators- Justification: The model fails to understand the logical implications of As statement: \"If C is a truth-teller, then D is a truth-teller.\" It draws conclusionsabout D based on the identity of A, ignoring character C, who is the antecedent of the logical implication.- Label: yes",
  ": The first few-shot example of the error evaluator six-shot prompt. Examples are presented in chat form,where the task prompt is depicted in blue, and the desired answer is exemplified in orange": "<|user|>### Instruction ###You are given a PROBLEM STATEMENT and the RESPONSE of a large language model, respectively. Your task is to classify whether a set of errors arepresent in the models response. For each of the following error types, classify whether the error is present in the models response (yes) or not (no). - False statement reproduction (A statement of the problem description is repeated incorrectly.)- Assuming statements to be true (The possibility that a characters statement is a lie is not considered. The model does not question the premises truth butassumes it as given.)- Misunderstanding the concept of truth and lies (The model does not understand that a truth-teller always tells the truth and a liar always lies. For instance,the model mistakenly assumes that truth-tellers lie, while liars tell the truth. Alternatively, the model assumes that truth-tellers can only talk about othertruth-tellers and liars can only talk about liars.)- Misunderstanding logical operators (Logical operators and their implications are not properly understood. The model fails at inferring the logical implica-tions of a statement. For instance, if character As statement is a logical conjunction (e.g. B and C) and A is a liar, the model does not consider all possibleimplications of that lie (not B and C, B and not C, not B and not C).)- Unjustified conclusion (A conclusion about the identity of a characters such as X is a truth-teller/liar is presented without ANY justification. Note thatthis error does not address the logical validity of a justification, but checks whether a justification is present or not. As an example of this error, the modelmight simply state: \"Since A is a truth-teller/liar...\" without prior justification.)- Unfaithfulness (A conclusion is drawn that explicitly contradicts a conclusion previously inferred by the model, without the model reflecting on this contra-diction.)",
  "### PROBLEM STATEMENTA: C is a liar and B is a truth-teller.B: I am a truth-teller.C: A is a truth-teller": "### RESPONSEGreat, lets dive into the problem!To solve this problem, we need to use logical reasoning and deduction. We are given three statements from three different characters, and our goal is todetermine who is a truth-teller and who is a liar based on their statements.Statement A tells us that C is a liar and B is a truth-teller. This means that B is telling the truth, since he is a truth-teller.Statement B tells us that I (B) am a truth-teller. This means that I am telling the truth, since I am a truth-teller.Statement C tells us that A is a truth-teller. This means that A is telling the truth, since A is a truth-teller.Now, lets use these statements to deduce the identity of each character:",
  "<|assistant|>### False statement reproduction- Justification: No occurrence of this error in the models response.- Label: no": "### Assuming statements to be true- Justification: The model treats each characters statement as a true premise and disregards the possibility that statements might be lies. See for instance:\"A tells us that C is a liar and B is a truth-teller. This means that B is telling the truth, since he is a truth-teller.\"- Label: yes",
  ": The second few-shot example of the error evaluator six-shot prompt. Examples are presented in chatform, where the task prompt is depicted in blue, and the desired answer is exemplified in orange": "<|user|>### Instruction ###You are given a PROBLEM STATEMENT and the RESPONSE of a large language model, respectively. Your task is to classify whether a set of errors arepresent in the models response. For each of the following error types, classify whether the error is present in the models response (yes) or not (no). - False statement reproduction (A statement of the problem description is repeated incorrectly.)- Assuming statements to be true (The possibility that a characters statement is a lie is not considered. The model does not question the premises truth butassumes it as given.)- Misunderstanding the concept of truth and lies (The model does not understand that a truth-teller always tells the truth and a liar always lies. For instance,the model mistakenly assumes that truth-tellers lie, while liars tell the truth. Alternatively, the model assumes that truth-tellers can only talk about othertruth-tellers and liars can only talk about liars.)- Misunderstanding logical operators (Logical operators and their implications are not properly understood. The model fails at inferring the logical implica-tions of a statement. For instance, if character As statement is a logical conjunction (e.g. B and C) and A is a liar, the model does not consider all possibleimplications of that lie (not B and C, B and not C, not B and not C).)- Unjustified conclusion (A conclusion about the identity of a characters such as X is a truth-teller/liar is presented without ANY justification. Note thatthis error does not address the logical validity of a justification, but checks whether a justification is present or not. As an example of this error, the modelmight simply state: \"Since A is a truth-teller/liar...\" without prior justification.)- Unfaithfulness (A conclusion is drawn that explicitly contradicts a conclusion previously inferred by the model, without the model reflecting on this contra-diction.)",
  "A: C is a truth-teller and B is a liar.B: If A is a truth-teller, then C is a truth-teller.C: B is a liar and A is a liar": "If A is a truth-teller, then As statement must be true, so C is a truth-teller and B is a liar.If C is a truth-teller, then Cs statement must be false, so either B is not a liar or A is not a liar.This contradicts our assumption that A is a truth-teller, so A must be a liar. If A is a liar, then As statement must be false, so either C is not a truth-teller or B is not a liar.Since As statement is false, we cant conclude anything about C, but we know B is not a liar.If B is a truth-teller, then Bs statement must be true, so A is a liar and C is a truth-teller.This is consistent with our previous conclusion that A is a liar.If C is a truth-teller, then Cs statement must be false, so either B is not a liar or A is not a liar.We know A is a liar, so B must be a truth-teller.",
  "### Assuming statements to be true- Justification: The model considers the possibility that statements might be false.- Label: no": "### Misunderstanding the concept of truth and lies- Justification: When discussing Cs statement, the model says: \"This statement is made by C, who according to statement B, is a liar. Therefore, thisstatement is true...\". However, if C were a liar, its statement could not be true. This shows a failure of the model to understand the concept of truth.- Label: yes",
  ": The third few-shot example of the error evaluator six-shot prompt. Examples are presented in chat form,where the task prompt is depicted in blue, and the desired answer is exemplified in orange": "<|user|>### Instruction ###You are given a PROBLEM STATEMENT and the RESPONSE of a large language model, respectively. Your task is to classify whether a set of errors arepresent in the models response. For each of the following error types, classify whether the error is present in the models response (yes) or not (no). - False statement reproduction (A statement of the problem description is repeated incorrectly.)- Assuming statements to be true (The possibility that a characters statement is a lie is not considered. The model does not question the premises truth butassumes it as given.)- Misunderstanding the concept of truth and lies (The model does not understand that a truth-teller always tells the truth and a liar always lies. For instance,the model mistakenly assumes that truth-tellers lie, while liars tell the truth. Alternatively, the model assumes that truth-tellers can only talk about othertruth-tellers and liars can only talk about liars.)- Misunderstanding logical operators (Logical operators and their implications are not properly understood. The model fails at inferring the logical implica-tions of a statement. For instance, if character As statement is a logical conjunction (e.g. B and C) and A is a liar, the model does not consider all possibleimplications of that lie (not B and C, B and not C, not B and not C).)- Unjustified conclusion (A conclusion about the identity of a characters such as X is a truth-teller/liar is presented without ANY justification. Note thatthis error does not address the logical validity of a justification, but checks whether a justification is present or not. As an example of this error, the modelmight simply state: \"Since A is a truth-teller/liar...\" without prior justification.)- Unfaithfulness (A conclusion is drawn that explicitly contradicts a conclusion previously inferred by the model, without the model reflecting on this contra-diction.)",
  "Start with a short justification for your decision (max. 1-2 sentences). Then, clearly state whether the error type is present (yes) or not (no)": "### PROBLEM STATEMENTAssume that there exist only two types of people: truth-tellers and liars. truth-tellers always tell the truth, while liars always lie.You are given the statements from a set of characters. Based on their statements, infer who is a truth-teller and who is a liar:A: D is a truth-teller.B: C is a liar and A is a liar.C: A is a liar.D: A is a liar and B is a truth-teller. ### RESPONSELets analyze the statements given:1. A: D is a truth-teller.This statement is made by A, who according to statement D, is a liar. Therefore, this statement is a lie, and D is not a truth-teller.2. B: C is a liar and A is a liar.This statement is made by B, who according to statement D, is a truth-teller. Therefore, this statement is true, and C and A are both liars.3. C: A is a liar.This statement is made by C, who according to statement B, is a liar. Therefore, this statement is true, and A is a liar.4. D: A is a liar and B is a truth-teller.This statement is made by D, who according to statement A, is a truth-teller. Therefore, this statement is true, and A is a liar and B is a truth-teller. Now, lets summarize our findings:* A is a liar (according to statements B and C).* B is a truth-teller (according to statement D).* C is a liar (according to statement B).* D is a truth-teller (according to statement A).",
  "### Misunderstanding logical operators- Justification: The model does not show any failures at understanding logical operators.- Label: no": "### Unjustified conclusion- Justification: When discussion the implications of As statement, the model defines Ds statement as truth without properly justifying this assumption:\"This statement is made by A, who according to statement D, is a liar. Therefore, this statement is a lie, and D is not a truth-teller.\"- Label: yes ### Unfaithfulness- Justification: The model first concludes that \"D is not a truth-teller\", but then follows up \"This statement is made by D, who according to statement A, is atruth-teller. Therefore, this statement is true.\" This contradicts the initial conclusion.- Label: yes</|assistant|>",
  ": The fourth few-shot example of the error evaluator six-shot prompt. Examples are presented in chatform, where the task prompt is depicted in blue, and the desired answer is exemplified in orange": "<|user|>### Instruction ###You are given a PROBLEM STATEMENT and the RESPONSE of a large language model, respectively. Your task is to classify whether a set of errors arepresent in the models response. For each of the following error types, classify whether the error is present in the models response (yes) or not (no). - False statement reproduction (A statement of the problem description is repeated incorrectly.)- Assuming statements to be true (The possibility that a characters statement is a lie is not considered. The model does not question the premises truth butassumes it as given.)- Misunderstanding the concept of truth and lies (The model does not understand that a truth-teller always tells the truth and a liar always lies. For instance,the model mistakenly assumes that truth-tellers lie, while liars tell the truth. Alternatively, the model assumes that truth-tellers can only talk about othertruth-tellers and liars can only talk about liars.)- Misunderstanding logical operators (Logical operators and their implications are not properly understood. The model fails at inferring the logical implica-tions of a statement. For instance, if character As statement is a logical conjunction (e.g. B and C) and A is a liar, the model does not consider all possibleimplications of that lie (not B and C, B and not C, not B and not C).)- Unjustified conclusion (A conclusion about the identity of a characters such as X is a truth-teller/liar is presented without ANY justification. Note thatthis error does not address the logical validity of a justification, but checks whether a justification is present or not. As an example of this error, the modelmight simply state: \"Since A is a truth-teller/liar...\" without prior justification.)- Unfaithfulness (A conclusion is drawn that explicitly contradicts a conclusion previously inferred by the model, without the model reflecting on this contra-diction.)",
  "### RESPONSELets start by analyzing the statements:": "1. A: F is a liar.If A is a truth-teller, then F is indeed a liar, which means A is a truth-teller. But if A is a liar, then F is not a liar, which is a contradiction. Therefore, Amust be a truth-teller.2. B: A is a truth-teller.Since A is a truth-teller, this statement is true. Therefore, B is a truth-teller.3. C: D is a truth-teller and B is a truth-teller.Since C is a truth-teller, this statement is true. Therefore, D is a truth-teller, and B is indeed a truth-teller.4. D: E is a truth-teller.Since D is a truth-teller, this statement is true. Therefore, E is a truth-teller.5. E: D is a liar and B is a truth-teller.Since E is a liar, this statement is false. However, we know that D is a truth-teller from statement 3, and B is a truth-teller from statements 2 and 3. There-fore, E must be a truth-teller.6. F: I am a truth-teller.Since F is a liar, this statement is false. Therefore, F is a liar.",
  ": The fifth few-shot example of the error evaluator six-shot prompt. Examples are presented in chat form,where the task prompt is depicted in blue, and the desired answer is exemplified in orange": "<|user|>### Instruction ###You are given a PROBLEM STATEMENT and the RESPONSE of a large language model, respectively. Your task is to classify whether a set of errors arepresent in the models response. For each of the following error types, classify whether the error is present in the models response (yes) or not (no). - False statement reproduction (A statement of the problem description is repeated incorrectly.)- Assuming statements to be true (The possibility that a characters statement is a lie is not considered. The model does not question the premises truth butassumes it as given.)- Misunderstanding the concept of truth and lies (The model does not understand that a truth-teller always tells the truth and a liar always lies. For instance,the model mistakenly assumes that truth-tellers lie, while liars tell the truth. Alternatively, the model assumes that truth-tellers can only talk about othertruth-tellers and liars can only talk about liars.)- Misunderstanding logical operators (Logical operators and their implications are not properly understood. The model fails at inferring the logical implica-tions of a statement. For instance, if character As statement is a logical conjunction (e.g. B and C) and A is a liar, the model does not consider all possibleimplications of that lie (not B and C, B and not C, not B and not C).)- Unjustified conclusion (A conclusion about the identity of a characters such as X is a truth-teller/liar is presented without ANY justification. Note thatthis error does not address the logical validity of a justification, but checks whether a justification is present or not. As an example of this error, the modelmight simply state: \"Since A is a truth-teller/liar...\" without prior justification.)- Unfaithfulness (A conclusion is drawn that explicitly contradicts a conclusion previously inferred by the model, without the model reflecting on this contra-diction.)",
  "### PROBLEM STATEMENTA: B is a liar if and only if C is a liar.B: C is a liar.C: A is a liar": "### RESPONSEREASONING:Lets assume B is a truth-teller.If B is a truth-teller, Bs statement is true and C is a liar.If C is a liar, Cs statement is false and A is a truth-teller.If A is a truth-teller, As statement is true, so either B is a liar and C is a liar or B is a truth-teller and C is a truth-teller.Since we know C is a liar, B must be a liar.This leads to a contradiction, so B must be a liar.If B is a liar, Bs statement is false and C is a truth-teller.If C is a truth-teller, Cs statement is true and A is a liar.If A is a liar, As statement is false, so either B is a truth-teller and C is a truth-teller or B is a liar and C is a liar.Since we know B is a liar, C must be a truth-teller.Therefore, we can conclude:",
  "### Misunderstanding the concept of truth and lies- Justification: The model seem to capture the concept of truth and lies.- Label: no": "### Misunderstanding logical operators- Justification: The model fails to infer the logical implications of As false statement. Although the model identifies As statement as a lie, it does notsuccessfully build the negation of the logical equivalence: \"If A is a liar, As statement is false, so either B is a truth-teller and C is a truth-teller or B is aliar and C is a liar.\"- Label: yes"
}