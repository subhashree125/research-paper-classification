{
  "Abstract": "Speech Language Models (SLMs) aim to learnlanguage from raw audio, without textual re-sources. Despite significant advances, our cur-rent models exhibit weak syntax and semanticabilities. However, if the scaling properties ofneural language models hold for the speechmodality, these abilities will improve as theamount of compute used for training increases.In this paper, we use models of this scalingbehavior to estimate the scale at which our cur-rent methods will yield a SLM with the Englishproficiency of text-based Large Language Mod-els (LLMs). We establish a strong correlationbetween pre-training loss and downstream syn-tactic and semantic performance in SLMs andLLMs, which results in predictable scaling oflinguistic performance. We show that the lin-guistic performance of SLMs scales up to threeorders of magnitude more slowly than that oftext-based LLMs. Additionally, we study thebenefits of synthetic data designed to boost se-mantic understanding and the effects of coarserspeech tokenization.",
  "Introduction": "Inspired by the remarkable ability of preschoolchildren to learn language from raw sensory in-puts, Lakhotia et al. (2021) introduced in their sem-inal paper the textless NLP (Natural Language Pro-cessing) project. The project aimed to leverageadvances in self-supervised speech representationlearning for unsupervised unit discovery (Hsu et al.,2021; Chung et al., 2021) and generative neurallanguage models (Brown et al., 2020) to jointlylearn the acoustic and linguistic characteristics ofa language from audio alone, without access totextual supervision (e.g. lexicon or transcriptions).They formalized this goal in the task of Genera-tive Spoken Language Modeling (GSLM), in whicha language model is trained on sequences of self-supervised learned speech units.Beyond bridging the gap between human and C (FLOPS) 2 100 3 100 4 100 6 100 Test loss L = 4.83 C0.02R2 = 0.98",
  ": Speech Language Models test loss curves forall our single-epoch runs. Axes are in logarithmic scale.The envelope of minimal loss per FLOP (black dots)follows a power law (dashed line)": "machine language acquisition, the textless NLPproject hoped to democratize access to NLP tech-nologies by extending them to the millions of usersof languages with little or no textual resources (e.g.due to a lack of standardized orthography). Theselanguages are unlikely to be supported by currenttechnologies, which are heavily dependent on mas-sive volumes of text data. In todays landscape,where NLP-based AI systems are becoming in-creasingly relevant and pervasive, it is all the morepressing to expand their inclusivity by buildingspeech-based systems that can match the capabili-ties of their text-based counterparts. Despite a significant body of research on theseSpeech-based Language Models (SLMs) (Lakhotiaet al., 2021; Kharitonov et al., 2022; Borsos et al.,2023; Hassid et al., 2023), they are still far frommatching the syntactic and semantic abilities oftext-based systems (Hassid et al., 2023). Therefore,the promise of textless NLP is yet to be realized.However, if the scaling behavior of text-based neu- ral language models (Brown et al., 2020; Kaplanet al., 2020) holds for the speech modality, we canreasonably expect those abilities to improve as theamount of compute used for training increases.In this work, we apply recently proposed modelsof the scaling behavior of neural language modelsto SLMs, and use them to estimate the scale atwhich our current methods will match the linguisticperformance of Large Language Models (LLMs),generative text-based systems that have achievedremarkably strong performance across a wide rangeof NLP applications (Brown et al., 2020). The maincontributions of this work are: We trained over 50 SLMs with different num-ber of parameters and data budgets. We showthat the test loss of SLMs follows scalingpower laws as those observed in text-basedLLMs (), and use the methods fromHoffmann et al. (2022) and Muennighoff et al.(2023) to model the scaling behavior of SLMs. We establish a strong correlation between thetest loss of neural LMs and the downstreammetrics commonly used to evaluate their syn-tactic and semantic abilities. Therefore, thelinguistic performance of LMs follows similarscaling laws (). We leverage this in-sight to determine the relative efficiency withscale of SLMs relative to LLMs. We speculate that SLMs require more contextthan fits in their context window to acquirefrom commonly used speech datasets the se-mantic understanding measured by our met-rics. Accordingly, we propose a new speechdataset to boost semantic understanding inSLMs. Specifically, we synthesized a spo-ken version of the Tiny Stories dataset (Eldanand Li, 2023), and show that its use duringpre-training improves downstream semanticperformance. On the basis of our previous observation, westudied the use of unigram tokenization toshorten sequences and pack more informationin the context window of SLMs. However,our results suggest that a coarser tokenizationis detrimental to downstream performance.",
  "Generative spoken language modeling": "We follow the GSLM framework from Lakhotiaet al. (2021). The general GSLM pipeline is com-posed of three separately trained models: (i) aspeech tokenizer, (ii) a language model, and (iii) avocoder (token-to-waveform) module. In the fol-lowing, we provide background for the speech tok-enizer and LM, as these are the components we usein this work. For details about the vocoder pleaserefer to Lakhotia et al. (2021).Speech tokenizers transform raw speech wave-forms into discrete representations. A speech en-coder is used to extract continuous representa-tions that are then transformed into discrete se-quences through vector quantization. Formally,let X R denote the domain of audio sam-ples, a waveform is therefore a sequence of sam-ples x = (x1, . . . , xT ), where xt X for all1 t T. An encoder F : X m Rd trans-forms windows of samples of width m into d di-mensional continuous frame representations. Ap-plying F to x yields a sequence of frame represen-tations z = (z1, . . . , zT ), where usually T < T.Subsequently, a k-means algorithm is applied tothe encoder output to generate a sequence of dis-crete speech tokens u = (u1, . . . , uT ), whereui {1, . . . , K} for 1 i T , and K is thesize of the vocabulary.Language models aim to learn the joint proba-bility of token sequences P(w1, . . . , wn). By thechain rule of probability, the probability of a se-quence can be computed as a product of its condi-tional probabilities:",
  "L(C) C,L(N) N,L(D) D (3)": "Where C is the amount of compute (in FLOPS), Nis the number of parameters of the model, and D isthe number of training tokens.Building upon their work, Hoffmann et al. (2022)proposed a parametric function to model the finalloss of neural LMs trained for a single epoch as afunction of N and D:",
  "D(4)": "Where the first term is the loss for an ideal LM, andshould correspond to the entropy of the distributionof token sequences. The second term captures theapproximation error that results from using a neuralnetwork with N parameters to approximate theideal generative process. The final term reflectsthat the model is not trained to convergence, as afinite number of optimization steps are performedon a sample of size D from the real distribution. Hoffmann et al. (2022) aimed to solve the prob-lem of optimal allocation of resources given a fixedcompute budget Cavail. They proposed to approx-imate the compute needed to train a transformerLM with N parameters on D tokens as C 6ND.",
  "D(7)": "Where D D is the number of effective trainingtokens, assuming that the value of repeated tokensdecays exponentially. Similarly, they note that over-sized models offer diminishing returns per param-eter, as excess parameters learn the same featuresand do not add value (in the extreme). They pro-pose an exponential decay model for them, yieldinga number of effective parameters N N. Theyderived the expressions for D and N as:",
  ": Models description": "Where UD is the number of unique tokens used,RD =DUD 1 is the number of repetitions (0 fora single epoch), UN is the number of parametersneeded to optimally fit UD according to Equation 6,RN =Nand RUN 1 is the number of excess parameters,D and RN are constants.The constants E, A, B, , , RD and RN canbe estimated empirically by fitting Equation 4 or7 to a set of tuples (N, D, RN, RD, L) obtainedfrom training experiments with different budgets.",
  "Models and training": "We adhere to the framework described in .1. For the speech tokenizer, we use a pre-trainedHuBERT model (Hsu et al., 2021) with frame-rateof 25 Hz as the speech encoder F, and a vocabularysize of K = 500. This setup reports the best per-formance among publicly available models (Hassidet al., 2023). For the SLMs we use the Llama archi-tecture (Touvron et al., 2023) with context windowof 2050 tokens. describes the model sizesused in our experiments. For the LLMs, we use thePythia suite of pre-trained LLMs (Biderman et al.,2023), ranging in size from 14M to 6.9B param-eters (we do not use the largest 12B model), andtrained with 300B tokens.AllSLMsareoptimizedusingAdamW(Loshchilov and Hutter, 2019) with weight decayof 0.1, maximum learning rate of 5e-4, half-cyclecosine decay learning rate schedule to 5e-5, anda warm-up initial stage of max(100, 0.01 niters)steps, where niters is the number of training steps,which varies for each experiment according to thedata budget. We use batch sizes of 64, 128, 256and 512 for the models with 20M, 85M, 155M and309M, and 828M parameters, respectively.To fit the constants in Equations 4 and 7, weadopt the approaches of Hoffmann et al. (2022)and Muennighoff et al. (2023), utilizing the Huberloss with = 0.03 as the error function and L-BFGS as optimizer. Following Muennighoff et al.(2023), we first fit the parameters E, A, B, , and",
  "Evaluation": "For upstream performance, we report and use theaverage loss (Equation 2) on the test set in all casesincluding the parametric fits. For downstream eval-uation we rely on the zero-shot metrics used inthe textless NLP literature, which evaluate LMslinguistic knowledge by comparing likelihoods ofpositive and negative speech samples. We focus onmetrics evaluating syntax and semantic knowledge.In all cases, performance is measured as the bi-nary accuracy with which the model assigns higherlikelihood to the positive samples.Syntax: We use the SBLIMP task from the ZeroResource Speech Challenge (Nguyen et al., 2020).In SBLIMP, the model is presented with mini-mal pairs of sentences, where one is grammaticallycorrect (positive) and the other is not (negative),targeting specific syntactic contrasts.Semantics: To evaluate semantic understandingwe use the spoken Story Cloze benchmark fromHassid et al. (2023), a spoken version of the Sto-ryCloze textual benchmark (Mostafazadeh et al.,2016), which consists of 4k five-sentence common-sense stories. In StoryCloze, the model receives asinput the first four sentences of a story, and has toassign higher probability to the correct final sen-tence than to an adversarial negative sample.The spoken Story Cloze benchmark comes intwo versions: sStoryCloze and tStoryCloze. Thedifference between them lies in how the negativesample is generated. sStoryCloze uses the samenegative samples as the textual benchmark, whichare carefully constructed to evaluate models abilityto grasp causal and temporal commonsense rela-tions. In tStoryCloze, the negatives are randomlysampled from the whole dataset, and therefore mea-sures the ability of the model to stay on topic. Sincein tStoryCloze the negatives are randomly sampled,they are not specifically designed to violate causalor temporal logic. Instead, they are more likely tobe incoherent or irrelevant in a more obvious way,making it an easier task than sStoryCloze.",
  ": Datasets statistics. The UNIGRAM column cor-responds to the dataset of HuBERT tokens compressedthrough unigram tokenization": "SWC (Baumann et al., 2019), Tedlium (Hernandezet al., 2018), Peoples Speech (Galvez et al., 2021),and Vox Populi (Wang et al., 2021b); and a noveldataset: STINYSTORIES, a spoken version of theTiny Stories dataset (Eldan and Li, 2023) that wesynthesized using the single-speaker TTS systemprovided by Wang et al. (2021a). Tiny Stories isa synthetic text corpus of short stories designedto boost commonsense reasoning in neural LMs.We propose STINYSTORIES because we hypoth-esize that the semantic understanding that taskssuch as sStoryCloze measure is hard to acquirefrom commonly used speech datasets. Considerfor instance the audiobooks in LibriLight. Thedata has long-range dependencies spanning multi-ple pages, whereas our SLMs can ingest roughly adozen sentences of spoken text in their context win-dow. Other datasets, which were mainly designedto serve as training data for automatic speech recog-nition systems, consist of too small fragments of au-dio that lack meaningful causal structure. STINYS- TORIES consists of full stories with causal structurethat fit within the context window of our SLMs.We do not include samples from STINYSTORIESin our test set, as we intend to use our test loss asmeasure of the quality with which SLMs model nat-ural language, not synthetic one. For other datasetswe use the defined held-out sets for testing. In caseswhere a held-out set is not defined, we randomlysampled 1% of the data to serve as test set. See for dataset sizes.",
  "Data budgets": "In order to have a representative set of sam-ples to fit Equations 4 and 7, for each modelsize, we performed training runs with a ratio oftraining tokens D to parameters N: D/N{2, 4, 8, 10, 20, 32, 64, 100}.This setup yields 0.68 0.70 0.72 tStoryCloze 0.52 0.54 sStoryCloze Parameters (millions) Pre-training dataset (Speaker) Libri(Multiple human speakers)sTinyStories(FastSpeech2 LJSpeech) Test speaker FastSpeech2 LJSpeechAvg. across 10 Bark speakers : Gains from synthetic data on downstreamsemantic performance of SLMs. Pre-training on sTinyS-tories yields consistent improvements on semantic un-derstanding relative to pre-training on audiobooks (Lib-riSpeech plus LibriLight). Performance gains hold formismatched train and test speakers. single-epoch and multi-epoch runs for the largermodels but not for the smaller models (e.g. for themodel with 85M parameters the maximum numberof training tokens corresponds to 0.99 epochs). Tobetter fit Equation 7, we performed additional ex-periments so that for each model size there wereruns with training epochs in {2, 4, 8, 10}, with theexception of the 828M parameter model, for whichthe maximum was 8 epochs.",
  "Gains from sTinyStories": "In order to determine if STINYSTORIES meaning-fully contributes to the semantic understandingof SLMs, we compare the performance on tSto-ryCloze and sStoryCloze of models trained on oneepoch of the union of LibriSpeech and LibriLight,against models trained on an equivalent amountof STINYSTORIES tokens. shows the ob-tained results. Models trained on STINYSTORIESconsistently outperform those trained on audio-books across all model scales. A factor that couldcontribute to the observed performance gain is thematch between training and evaluation speakers, asboth STINYSTORIES and Story Cloze were synthe-sized using the single-sepaker TTS from Wang et al.(2021a). However, we believe this to be unlikely as the speech tokenizer we use likely captures littlespeaker-specific information (Nguyen et al., 2023).To isolate the potential impact of speaker mismatchbetween training and evaluation data, we createda multi-speaker version of the sStoryCloze bench-mark using Bark TTS 1, and repeat the evaluations.The results, also shown in , indicate thateven with mismatched train and test speakers train-ing on STINYSTORIES yields performance gains.",
  "Benchmarking our setup": "To validate our setup, we compared our best per-forming model with other models in the SLM lit-erature in . Our model outperformed allother speech-only LMs on the semantic tasks, andperformed second best in general, even relativeto hybrid speech-text LMs. Notably, our modeloutperformed models with a larger compute bud-get. Considering that the models from Hassid et al.(2023) and Nguyen et al. (2024) use similar hyper-parameters (same speech tokenizer and the Llamaarchitecture for LMs); the most likely factor to ex-plain the performance difference is the data used.We believe these results further illustrate the bene-fits from using STINYSTORIES.",
  "Scaling laws": "We trained multiple SLMs for each model size withdifferent data budgets as described in .3.2.The resulting learning curves for single-epoch runsare presented in as a function of compute,and show that the envelope of minimal loss perFLOP follows a power law. 4.3.1Downstream scaling with computeWe analyzed the relationship between the upstreamand linguistic downstream performance in SLMsand LLMs. shows the obtained results.Downstream linguistic metrics before saturationare strongly correlated with the upstream test lossin both LLMs and SLMs. Therefore, the envelopeof maximum downstream performance per FLOPalso follows a power law, i.e. for a downstream per-formance function Q, Q Cq. The power lawsfor the different performance metrics are presentedin and the exponents in .These results allow us to compare the efficiencywith scale of LLMs and SLMs. For each metric,we can interpret the ratio between the q exponentsof the power laws of LLMs and SLMs as the rel-ative efficiency with scale. For BLIMP, the ratio",
  "is 0.066": "0.021 = 3.14, indicating that for an increase incompute C yielding a Q in LLMs syntacticperformance, SLMs require 103.14C to get thesame Q. Similarly, for tStoryCloze and sSto-ryCloze the ratios are 1.56 and 2.7, respectively. 4.3.2Scaling with parameters and tokensWe fitted the functions from Equations 4 and 7 toour data using the procedure described in .1. We present the empirically fitted scaling lawparameters and compare them to the ones obtainedfor text by Muennighoff et al. (2023) in .From Equation 6, Nopt Ca and Dopt Cb. Forboth modalities a b 0.5, suggesting that ascompute increases, model size and data should bescaled equally for optimal performance. Contraryto text, RN > RD, indicating that repeated tokensdecay faster than excess parameters (albeit bothslower than in text). Therefore, in SLMs, computeallocated to parameters should scale faster thancompute allocated for epochs.",
  "Unigram tokenization": "As mentioned in .3, we believe that thelimited context window of SLMs could cripple theirability to model the long-range dependencies inlanguage required for causal reasoning. Seekingto mitigate this limitation, we apply unigram to-kenization to shorten the length of speech tokensequences. We use the SentencePiece tokenizer(Kudo and Richardson, 2018) with a vocabularysize of 5000. We choose the vocabulary size onthe scale of previous works that have used simi-lar tokenization strategies for speech applications(Chang et al., 2023). The resulting dataset sizesafter compression are presented in .We train a set of Speech LMs on the compresseddatasets, with model sizes up to 309M parame-ters and data budgets ranging from 740M to 6.31Btokens. We analyze the scaling behavior of theupstream and downstream metrics and compareit with SLMs trained on raw HuBERT speech to-kens in . SLMs trained on unigram com-pressed speech tokens show similar upstream scal-ing with compute, but worse downstream scaling.Notably, the performance on the StoryCloze bench-mark does not seem to scale with compute.We fitted the function from Equation 4 to theresults obtained on the compressed dataset. presents the resulting scaling law parameters. Sim-ilar to the previous findings, for a given computebudget, scaling model size and training data equally",
  "Related work": "Previous works have studied the scaling behaviorof neural networks on speech applications. Droppoand Elibol (2021) showed that acoustic modelstrained with an auto-predictive coding loss followsimilar power laws to those observed in neural LMs.Aghajanyan et al. (2023) used the scaling laws fromHoffmann et al. (2022) to model the scaling behav-ior of the upstream loss of neural LMs on multiple",
  ": Scaling law parameters fit to Equations 4 and 7for different language tokenizations": "modalities, including speech. They used a speechtokenizer with higher framerate (50 Hz) and vo-cabulary size (K = 2000) than the one we used(.1). Such fine-grained tokenizers capturea lot of the paralinguistic information in speech(Nguyen et al., 2023). Therefore, their speech to-kens can be considered almost a different modalitydue to the acoustic variance. Furthermore, they donot study the behavior with scale of downstreamperformance. In this work, we focus on the linguis-tic content of the signal. As reported by Hassid C (FLOPS) 1.90 1.95 2.00 2.05 2.10 2.15 2.20 Test loss LUNI = 4.75 C0.021, R2 = 0.99 L = 4.83 C0.020, R2 = 0.98 UnigramHuBERT C (FLOPS) 0.52 0.54 0.56 0.58 0.60 0.62",
  "BLIMP": "BLIMPUNI = 0.29 C0.016, R2 = 0.97 BLIMP = 0.23 C0.021, R2 = 0.99 UnigramHuBERT C (FLOPS) 0.64 0.66 0.68 0.70 0.72 0.74 0.76 tStoryCloze tClozeUNI = 0.32 C0.018, R2 = 0.92 tCloze = 0.24 C0.025, R2 = 0.99 UnigramHuBERT C (FLOPS) 0.50 0.52 0.54 0.56 0.58 0.60 sStoryCloze sClozeBPE = 0.25 C0.019, R2 = 1.000 sCloze = 0.26 C0.017, R2 = 0.970 UnigramUnigram sub optimalHuBERT : Comparison of the scaling behavior of SLMs trained on raw speech tokens and unigram compressedtokens. Axes are in logarithmic scale. The upstream loss of SLMs trained on unigram tokens scales better withcompute, but downstream performance scales worse. Notably, the sStoryCloze metric for SLMs trained on unigramtokens does not seem to improve with increased compute. et al. (2023), our speech tokenizer performs beston downstream linguistic applications, and is there-fore a more suitable choice to study the scalingbehavior of the linguistic performance of SLMs.This paper is most closely related to the workof Hassid et al. (2023). We largely follow theirsetup in terms of hyperparameters and evaluationmetrics. They reported improved linguistic down-stream performance with scale in SLMs, but didnot characterize their scaling behavior. Our scalinglaws allow practitioners to determine the computeneeded to attain a specific loss, syntactic and/or se-mantic performance; and its optimal allocation withrespect to parameters and tokens. To the best of ourknowledge, we are the first to model the scalingproperties of downstream linguistic performance inSLMs, and to study the scaling of the considereddownstream metrics on text-based LLMs. This en-ables a comparison between the two modalities interms of scaling efficiency.",
  "Discussion": "Our work showed that the upstream and down-stream linguistic performance of our current meth-ods for GSLM scales predictably with com-pute. This suggests that, with sufficient compu-tational resources, the goal of the textless NLPprojectachieving neural LMs trained exclusivelyon speech, and matching the linguistic proficiencyof their text-based counterpartsis achievable.However, the cost of such models could be pro-hibitive, as we estimate that they will require upto three orders of magnitude more compute than atext-based LLM to achieve equivalent performance.We believe this points to the need for leveragingthe rich language representations already learnedby text LLMs. This seems to be the current trendin the community, as several recent works havesought to improve SLMs through transfer learn- ing from text-based models (Hassid et al., 2023;Zhang et al., 2023; Nguyen et al., 2024). However,considering one of the grand goals of the textlessNLP projectextending the benefits of large-scalelanguage modeling to low-resource or non-writtenlanguageswe will have to address the questionof how knowledge transfer from text LLMs per-forms when the speech data is in a different lan-guage than the one the text LLM was trained on.If cross-lingual knowledge transfer between textand speech modalities proves to be unfeasible, thenpurely speech-based SLMs, such as the ones stud-ied here, could still offer a compelling solution forlow-resource languages.We explored the use of synthetic data and coarsertokenization to increase the semantic abilities ofSLMs.Our synthetic dataset improved seman-tic performance, but using a coarser tokenizationled to overall degradation of downstream perfor-mance. We do not have yet an hypothesis for whycoarser tokens degrade performance, as this seemscounter-intuitive, and contradicts the findings onother speech applications (Chang et al., 2023). Weleave this as an interesting issue to address in fu-ture work. Moreover, we believe that working onmethods that allow to increase the information den-sity per context-window of SLMs holds promise toimprove their scaling behavior.",
  "Limitations": "Any extrapolation from our models of the scal-ing behavior of SLMs should be considered opti-mistic for the following reasons: 1) Our modelsfor downstream performance ignore the fact thatthe metrics saturate. As observed in text LLMs,the improvements with scale slow down as perfor-mance approaches the saturation value. It is likelythat, due to saturation, the compute required toyield a particular performance will be larger than predicted. Moreover, due to the lower density oflinguistic information per context window in SLMsrelative to LLMs, the saturation values of the met-rics may be lower for SLMs. 2) The LLMs fromthe Pythia suite that we used in this study are likelyovertrained (all models were trained with 300Btokens). Optimally trained LLMs (according toEquation 6) should show better performance withscale, and therefore widen the gap with the scalingefficiency of SLMs. 3) The envelope of minimalloss per FLOP () might show a slight neg-ative curvature at larger scale (Hoffmann et al.,2022), reducing the scaling efficiency. Muennighoff et al. (2023) note that the scalinglaw coefficients for text LLMs, and consequentlythe optimal compute allocation, can vary depend-ing on the training datasets used in the scalingstudy. Commonly used text datasets are signifi-cantly larger and more diverse than the academicspeech datasets typically used for GSLM, suchas those in this study. As a result, these speechdatasets represent a more biased sample of the over-all distribution of speech data, making scaling lawsderived from them less likely to generalize. There-fore, we cannot guarantee that the scaling laws wehave developed will be universally applicable toother datasets. However, we do not expect signif-icant deviations that affect the conclusions herepresented. Future research could explore validat-ing the predictions from this study on larger andmore diverse datasets, such as the recently releasedYodas (Li et al., 2023).",
  "Conclusions": "We have trained a large set of SLMs with differentcompute budgets and studied the scaling propertiesof their upstream and downstream performance us-ing recently proposed models of scaling laws forneural LMs. The obtained models allow practition-ers to optimally allocate compute to attain a spe-cific loss, syntactic, and/or semantic performance.We showed that the pre-training loss and down-stream linguistic performance of SLMs and LLMsis highly correlated, and both scale predictably ac-cording to power laws. This allowed us to comparethe scaling properties of SLMs and LLMs, fromwhich we established that the linguistic abilities ofSLMs scale up to three orders of magnitude moreslowly. Additionally, we proposed a new speechdataset, STINYSTORIES, and showed that its useduring pre-training improves downstream seman-",
  "Acknowledgements": "We are grateful to the French National ResearchAgency for their support through the ANR-20-CE23-0012-01 (MIM) grant, and the Institute ofConvergence ILCB, supported by grants fromFrance 2030 (ANR-16-CONV-0002) and the Ex-cellence Initiative of Aix-Marseille University(A*MIDEX). This work was granted access to theHPC resources of GENCI-IDRIS under the alloca-tion AD011014044. Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-NingHsu, Karen Hambardzumyan, Susan Zhang, StephenRoller, Naman Goyal, Omer Levy, and Luke Zettle-moyer. 2023. Scaling laws for generative mixed-modal language models.In Proceedings of the40th International Conference on Machine Learn-ing, ICML23. JMLR.org.",
  "The spoken wikipedia corpus collection: Harvesting,alignment and an application to hyperlistening. Lang.Resour. Eval., 53(2):303329": "Stella Biderman, Hailey Schoelkopf, Quentin Anthony,Herbie Bradley, Kyle OBrien, Eric Hallahan, Mo-hammad Aflah Khan, Shivanshu Purohit, USVSN SaiPrashanth, Edward Raff, Aviya Skowron, LintangSutawika, and Oskar Van Der Wal. 2023. Pythia:a suite for analyzing large language models acrosstraining and scaling. In Proceedings of the 40th Inter-national Conference on Machine Learning, ICML23.JMLR.org. Zaln Borsos, Raphal Marinier, Damien Vincent, Eu-gene Kharitonov, Olivier Pietquin, Matt Sharifi,Dominik Roblek, Olivier Teboul, David Grangier,Marco Tagliasacchi, and Neil Zeghidour. 2023. Au-diolm: A language modeling approach to audio gen-eration. IEEE/ACM Transactions on Audio, Speech,and Language Processing, 31:25232533. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020.",
  "Language models are few-shot learners.In Ad-vances in Neural Information Processing Systems,volume 33, pages 18771901": "Xuankai Chang, Brian Yan, Kwanghee Choi, JeeweonJung, Yichen Lu, Soumi Maiti, Roshan Sharma, Ji-atong Shi, Jinchuan Tian, Shinji Watanabe, YuyaFujita, Takashi Maekaku, Pengcheng Guo, Yao-FeiCheng, Pavel Denisov, Kohei Saijo, and Hsiu-HsuanWang. 2023. Exploring speech recognition, transla-tion, and understanding with discrete speech units: Acomparative study. Yu-An Chung, Yu Zhang, Wei Han, Chung-ChengChiu, James Qin, Ruoming Pang, and Yonghui Wu.2021.w2v-bert: Combining contrastive learningand masked language modeling for self-supervisedspeech pre-training. In 2021 IEEE Automatic SpeechRecognition and Understanding Workshop (ASRU),pages 244250.",
  "Ronen Eldan and Yuanzhi Li. 2023. Tinystories: Howsmall can language models be and still speak coherentenglish?": "Daniel Galvez, Greg Diamos, Juan Manuel Ciro Tor-res, Juan Felipe Cern, Keith Achorn, Anjali Gopi,David Kanter, Max Lam, Mark Mazumder, and Vi-jay Janapa Reddi. 2021. The peoples speech: Alarge-scale diverse english speech recognition datasetfor commercial usage. In Thirty-fifth Conference onNeural Information Processing Systems Datasets andBenchmarks Track (Round 1). Michael Hassid, Tal Remez, Tu Anh Nguyen, Itai Gat,Alexis Conneau, Felix Kreuk, Jade Copet, Alexan-dre Dfossez, Gabriel Synnaeve, Emmanuel Dupoux,Roy Schwartz, and Yossi Adi. 2023. Textually pre-trained speech language models. In Thirty-seventhConference on Neural Information Processing Sys-tems. Franois Hernandez, Vincent Nguyen, Sahar Ghannay,Natalia Tomashenko, and Yannick Estve. 2018. Ted-lium 3: Twice as much data and corpus repartition forexperiments on speaker adaptation. In Speech andComputer, pages 198208, Cham. Springer Interna-tional Publishing. Joel Hestness, Sharan Narang, Newsha Ardalani, Gre-gory F. Diamos, Heewoo Jun, Hassan Kianinejad,Md. Mostofa Ali Patwary, Yang Yang, and YanqiZhou. 2017. Deep learning scaling is predictable,empirically. CoRR, abs/1712.00409. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,Elena Buchatskaya, Trevor Cai, Eliza Rutherford,Diego de Las Casas, Lisa Anne Hendricks, JohannesWelbl, Aidan Clark, Tom Hennigan, Eric Noland,Katie Millican, George van den Driessche, BogdanDamoc, Aurelia Guy, Simon Osindero, Karen Si-monyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,",
  "and Laurent Sifre. 2022. Training compute-optimallarge language models": "Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,Kushal Lakhotia, Ruslan Salakhutdinov, and Abdel-rahman Mohamed. 2021. HuBERT: Self-supervisedspeech representation learning by masked predictionof hidden units. IEEE/ACM Trans. Audio SpeechLang., 29:34513460. J. Kahn, M. Rivire, W. Zheng, E. Kharitonov, Q. Xu,P.E. Mazar, J. Karadayi, V. Liptchinsky, R. Col-lobert, C. Fuegen, T. Likhomanenko, G. Synnaeve,A. Joulin, A. Mohamed, and E. Dupoux. 2020. Libri-light: A benchmark for asr with limited or no super-vision. In ICASSP 2020 - 2020 IEEE InternationalConference on Acoustics, Speech and Signal Process-ing (ICASSP), pages 76697673. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.Brown, Benjamin Chess, Rewon Child, Scott Gray,Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.Scaling laws for neural language models. CoRR,abs/2001.08361. Eugene Kharitonov, Ann Lee, Adam Polyak, YossiAdi, Jade Copet, Kushal Lakhotia, Tu Anh Nguyen,Morgane Riviere, Abdelrahman Mohamed, Em-manuel Dupoux, and Wei-Ning Hsu. 2022. Text-freeprosody-aware generative spoken language modeling.In Proceedings of the 60th Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 86668681, Dublin, Ireland.Association for Computational Linguistics. Taku Kudo and John Richardson. 2018. SentencePiece:A simple and language independent subword tok-enizer and detokenizer for neural text processing. InProceedings of the 2018 Conference on EmpiricalMethods in Natural Language Processing: SystemDemonstrations, pages 6671, Brussels, Belgium.Association for Computational Linguistics. Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu,Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-AnhNguyen, Jade Copet, Alexei Baevski, AbdelrahmanMohamed, and Emmanuel Dupoux. 2021. On gen-erative spoken language modeling from raw audio.Transactions of the Association for ComputationalLinguistics, 9:13361354. Xinjian Li, Shinnosuke Takamichi, Takaaki Saeki,William Chen, Sayaka Shiota, and Shinji Watanabe0001. 2023. Yodas: Youtube-oriented dataset foraudio and speech. In IEEE Automatic Speech Recog-nition and Understanding Workshop, ASRU 2023,Taipei, Taiwan, December 16-20, 2023, pages 18.IEEE.",
  "Nasrin Mostafazadeh, Nathanael Chambers, XiaodongHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,": "Pushmeet Kohli, and James Allen. 2016. A corpusand cloze evaluation for deeper understanding ofcommonsense stories. In Proceedings of the 2016Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 839849, San Diego,California. Association for Computational Linguis-tics. Niklas Muennighoff, Alexander M Rush, Boaz Barak,Teven Le Scao, Nouamane Tazi, Aleksandra Piktus,Sampo Pyysalo, Thomas Wolf, and Colin Raffel.2023.Scaling data-constrained language models.In Thirty-seventh Conference on Neural InformationProcessing Systems. Tu Anh Nguyen,Maureen de Seyssel,PatriciaRoz, Morgane Rivire, Evgeny Kharitonov, AlexeiBaevski, Ewan Dunbar, and Emmanuel Dupoux.2020. The zero resource speech benchmark 2021:Metrics and baselines for unsupervised spoken lan-guage modeling. CoRR, abs/2011.11588. Tu Anh Nguyen, Wei-Ning Hsu, Antony DAvirro,Bowen Shi, Itai Gat, Maryam Fazel-Zarani, Tal Re-mez, Jade Copet, Gabriel Synnaeve, Michael Has-sid, Felix Kreuk, Yossi Adi, and Emmanuel Dupoux.2023. Expresso: A Benchmark and Analysis of Dis-crete Expressive Speech Resynthesis. In Proc. IN-TERSPEECH 2023, pages 48234827. Tu Anh Nguyen, Benjamin Muller, Bokai Yu, Marta R.Costa-jussa, Maha Elbayad, Sravya Popuri, Paul-Ambroise Duquenne, Robin Algayres, Ruslan Mav-lyutov, Itai Gat, Gabriel Synnaeve, Juan Pino, BenoitSagot, and Emmanuel Dupoux. 2024. SpiRit-LM:Interleaved Spoken and Written Language Model. Vassil Panayotov, Guoguo Chen, Daniel Povey, and San-jeev Khudanpur. 2015. Librispeech: An asr corpusbased on public domain audio books. In IEEE Inter-national Conference on Acoustics, Speech and SignalProcessing (ICASSP), pages 52065210. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Aurelien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. 2023. Llama: Openand efficient foundation language models. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is Allyou Need. In Advances in Neural Information Pro-cessing Systems, volume 30. Changhan Wang, Wei-Ning Hsu, Yossi Adi, AdamPolyak, Ann Lee, Peng-Jen Chen, Jiatao Gu, andJuan Pino. 2021a. fairseq s2: A scalable and inte-grable speech synthesis toolkit. In Proceedings ofthe 2021 Conference on Empirical Methods in Nat-ural Language Processing: System Demonstrations,pages 143152, Online and Punta Cana, DominicanRepublic. Association for Computational Linguistics. Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu,Chaitanya Talnikar, Daniel Haziza, Mary Williamson,Juan Pino, and Emmanuel Dupoux. 2021b. VoxPop-uli: A large-scale multilingual speech corpus for rep-resentation learning, semi-supervised learning andinterpretation. In Proceedings of the 59th AnnualMeeting of the Association for Computational Lin-guistics and the 11th International Joint Conferenceon Natural Language Processing (Volume 1: LongPapers), pages 9931003, Online. Association forComputational Linguistics. Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan,Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. 2023.SpeechGPT: Empowering large language modelswith intrinsic cross-modal conversational abilities.In Findings of the Association for ComputationalLinguistics: EMNLP 2023, pages 1575715773, Sin-gapore. Association for Computational Linguistics."
}