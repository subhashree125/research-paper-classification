{
  "Abstract": "Norwegian, spoken by only 5 million popula-tion, is under-representative within the mostimpressive breakthroughs in NLP tasks. Tothe best of our knowledge, there has not yetbeen a comprehensive evaluation of the exist-ing language models (LMs) on Norwegian gen-eration tasks during the article writing process.To fill this gap, we 1) compiled the existingNorwegian dataset and pre-trained 4 Norwe-gian Open Language Models varied from pa-rameter scales and architectures, collectivelycalled NorGLM; 2) introduced a comprehen-sive benchmark, NLEBench, for evaluating nat-ural language generation capabilities in Nor-wegian, encompassing translation and humanannotation. Based on the investigation, we findthat: 1) the mainstream, English-dominatedLM GPT-3.5 has limited capability in under-standing the Norwegian context; 2) the in-crease in model parameter scales demonstrateslimited impact on the performance of down-stream tasks when the pre-training dataset isconstrained in size; 3) smaller models alsodemonstrate the reasoning capability throughChain-of-Thought; 4) a multi-task dataset thatincludes synergy tasks can be used to verify thegeneralizability of LLMs on natural languageunderstanding and, meanwhile, test the inter-connectedness of these NLP tasks. We shareour resources and code for reproducibility1 un-der a CC BY-NC 4.0 license.",
  "*Corresponding author1": "models can leverage community contributions, fa-cilitate collaboration, and accelerate technolog-ical advancements while better controlling datause. This approach is especially beneficial for low-resource languages, aiding their preservation anddevelopment. Currently, benchmarks focus mainlyon languages like English and Chinese, leavingLow-Resource Languages (LRLs) under-evaluated.Most benchmarks for low-resourced languages ei-ther cater to discriminative models (Kutuzov et al.,2021; Koto et al., 2020; Kummervold et al., 2021)like BERT (Devlin et al., 2019) or are adapted ortranslated from existing English datasets (Luukko-nen et al., 2023). Nielsen (2023) proposes a closed-source platform, ScanEval, for evaluating Nordiclanguages. However, these benchmarks have twolimitations: First, many nominal generation tasksare adapted from classification tasks, like multiple-choice questions, which restrict answer options anddo not assess generative models ability to producelonger texts. Second, most benchmarks are single-task, with multi-task datasets being particularlyscarce. We argue that by designing a multi-taskdataset that includes several synergy tasks2 in natu-ral language understanding, it may be possible toevaluate the generalization ability of large languagemodels (LLMs) in text comprehension.To address these gaps, we propose a comprehen-sive benchmark, NLEBench, specifically tailoredto evaluate the natural language generation capabil-ities in Norwegian. NLEBench comprises variousreal-world NLP tasks and provides relative compar-isons for Norwegian GLMs with different param-eter scales and Transformer-based architectures.Specifically, our benchmark is purposefully de-signed to be capability probing, such as instructionsspecific to Norwegian culture and special expres-sions, and a document-grounded multi-task dataset",
  "Here, synergy tasks mean that one task can provide mean-ingful contexts used to improve the performance of anothertask in the multi-task dataset/scenario": "with human-annotated question-answer pairs andsummaries. We hope that such a side-by-side per-formance benchmark will inspire future researchon more advanced GLMs for Norwegian and otherLRLs.In summary, this paper makes the following con-tributions: Wereleaseanewbenchmarkdataset,NLEBench, for the purpose of evaluatinggenerative language modelling in Norwegian.To the best of our knowledge, this is thefirst benchmarking dataset for Norwegiancausal/autoregressive language modelling3. Wecontributetwonovel,high-qualitydatasets: an instruction dataset comprisinghuman-written instructions specific to Norwe-gian culture, and a document-grounded multi-task dataset, which is beneficial for evaluatingGLMs comprehension of language nuancesand their ability to navigate intricate logicalchallenges. We build upon the pioneering work to developa series of fundamental Norwegian GenerativeLanguage Models (NorGLMs) with differentparameter scales and Transformer-based archi-tectures. By in-depth evaluation of these mod-els on the proposed benchmarks, we providecrucial insights for understanding the capabil-ities and scalability of GLMs when applied tounderrepresented languages like Norwegian.",
  "Language Models for Low-resourceLanguages": "Despite the effectiveness of popular LLMs, theinherent data-hungry attribute limits their perfor-mance and application to data scarce settings suchas with low-resource languages (Hedderich et al.,2021). Such languages may also suffer from dif-ficulties in acquiring readily-accessible resourcescompared with mainstream languages such as pre-trained word embeddings and expert-annotated cor-pora (Zoph et al., 2016), leading to a significantopen challenge in NLP tasks for low-resourcedscenarios. Several efforts have been made in dif-ferent low-resource languages (Koto et al., 2020;Kutuzov et al., 2021; Kummervold et al., 2021) but",
  "Generative, causal or autoregressive language models areused interchangeably in this paper": "the models are based on BERT architecture andtested for language discriminative tasks. Recently,researchers have proposed several standard evalu-ation benchmarks on a collection of low-resourcelanguage datasets for language generative tasks(Ekgren et al., 2022; de Vries and Nissim, 2021;De Mattei et al., 2020; Antoun et al., 2020). Forinstance, Google released a comprehensive bench-mark, BIG-bench, for over 200 tasks on languagegenerative tasks (Srivastava et al., 2023), amongwhich there are only two tasks that contain theNorwegian language, namely Which Wiki Edit tomatch a recent Wikipedia revision to its correspond-ing edit message, and Language Identification tasks.They only cover very limited Norwegian samples.Later, Luukkonen et al. (2023) filtered Finnish fromBIG-bench to build a Finnish benchmark for gener-ative LMs. However, these existing evaluation dataeither originate from pre-existing English datasetsthrough machine translation or lack the evaluationdata types required for assessing LLMs on multi-task reasoning.",
  "Benchmark on Multi-task Datasets": "Most existing benchmarks focus on single tasks,such as question answering, cloze tests, summariza-tion, and classification. Fine-tuning language mod-els on individual datasets lacks persuasiveness inevaluating their ability to generalize across multipletasks. Xu et al. (2020) proposed MATINF, a jointlylabeled Chinese dataset for classification, questionanswering, and summarization in the maternal andinfant domain. However, this web-crawled datasetcontains significant noise and consists of short texts,with an average length of 42 Chinese characters. Aslanguage models become more capable of handlinglonger texts (Brown et al., 2020; Chen et al., 2023),datasets with short texts may not reliably predictthe transformative potential of LLMs. Additionally,the annotated tasks in MATINF lack synergy andinterconnections, leading to assessments still beingconducted on individual tasks and overlooking thepotential effects of task interactions, such as thefeasibility of employing Chain-of-Thought (CoT)techniques.",
  "NorGLM models are trained from scratch usingmulti-source datasets. We filtered Norwegian textsfrom the mC4 and OSCAR web-crawled corpora": ": The data distribution within the pre-trainingdataset. The inner segment represents languages, andthe outer segment denotes various sourced datasets inNorwegian. Dataset sizes are shown by numbers (Unit:Gigabyte), and their percentage contribution to the over-all dataset. Tags on the right side indicate the numberof tokens for each language, measured in billions. and included non-copyrighted Norwegian materialfrom the Norwegian National Library (Nasjonal-biblioteket)(Kummervold et al., 2021)4. We alsosourced high-quality news articles from Schibstedand collected tweets (January 2012 to December2022) and Reddit posts (October 2017 to Decem-ber 2022) via their respective APIs. To enhancerobustness in downstream tasks, we included Dan-ish, Swedish, and German texts from the NorthGermanic language family, along with a small por-tion of the English corpus. The size and distributionof each language are shown in .The models are based on the GPT-2 architec-ture and are named NorGPT-369M, NorGPT-3B,and NorGPT-23B, corresponding to their parame-ter sizes. We also trained a three billion-parametermodel, NorLlama-3B, based on the Llama architec-ture using Tencent Pre-training Framework (Zhaoet al., 2023). The details of parameter settings areshown in . To investigate the potential im-provement in overall model performance throughoversampling qualified data such as from publish-ers, akin to Samuel et al. (2023), we continuedtraining NorGPT-3B (referred to as NorGPT-3B-continue) using a subset of high-quality data, in-cluding news articles and material from Nasjonal-biblioteket5. In addition, we incorporated NB-GPT-J-6B, which is a model continued-trained from theEnglish GPT-J-6B model6. We applied similar fine-",
  "refer to Appendix for model training details.6": "tuning methods to evaluate these models on down-stream tasks listed in , aiming to study thedifferences between training from scratch and con-tinuing training on an English pre-trained model.Its important to note that GPT-J-6B was continued-trained with a dataset of 402 billion tokens, approx-imately 20 times larger than the training datasetused for our NorGPT models. Additionally, weevaluated GPT-3.5-Turbo7 on our benchmarks. Toprevent any potential data contamination, thepre-training dataset is carefully curated to en-sure there is no overlap with the benchmarkdataset.",
  "Norwegian Benchmark Dataset -NLEBench": "This section introduces tasks in NLEBench specif-ically designed for Norwegian GLMs.Thedatasets are sourced from three categories: exist-ing datasets, machine-translated datasets using theGoogle Translation API, and manually annotateddatasets. Our native Norwegian colleagues evalu-ated random samples from both the Google Transla-tion API8 and another free translation API9 support-ing Norwegian, finding that the former performsbetter, especially with confusing words and longtexts. outlines the differences and evalu-ation settings of these datasets. The statistics ofdifferent datasets are shown in -10.",
  "Toxicity and bias": "Generative language models are notorious for am-plifying biases inherent in the training data (Shenget al., 2019) and producing toxic text (Gehmanet al., 2020). To evaluate these issues in NorGLMs,we used the Perspective API14 on 1508 prompts fortoxicity evaluation and calculated ppl on 1677 sam-ple pairs for bias evaluation from the NO-CrowS-Pairs benchmark, a machine-translated version ofthe French CrowS-Pairs (Nvol et al., 2022). Dueto the APIs lack of Norwegian support, we trans-lated the NorGLM generated text into Swedish forassessment. This benchmark also helps evaluatepotential biases in NorGLMs.",
  "Multi-task learning": "Apart from the benchmarks and translated datasetsmentioned above, we release a multi-task datasetcalled NO-Multi-QA-Sum. This section details thedataset collection process and the tasks performedusing this benchmark.Data Collection. We recruited three Norwegiancollege students as annotators, allowing them towork in pairs or independently. Each student iscompensated 230 NOK (approx. $21,75 USD) perhour. Annotators were tasked with conducting aconversation about a given news article, using con-tent from the article without a limit on the numberof dialogue turns or question types. After the con-versation, they were required to write a genericsummary of the article. The dialogue and summarycontent did not need to fully overlap, giving annota-tors some freedom in their dialogue choices. Mostannotators chose to use self-dialogue and summa- rization for efficiency and flexibility15.To facilitate the annotation process, we devel-oped an API, shown in , that can connectwith the OpenAI GPT-4 model to suggest annota-tions. However, annotators were required to verifythe fidelity and usability of the suggested texts. Toensure quality, each annotation should be cross-validated and corrected by two other annotators,achieving one hundred percent internal consensuson the final annotations. The cross-validation in-cluded checking the rationality of question-answerpairs, factual consistency, and language fluency.Many annotators reported that while GPT-4 (specif-ically gpt-4-0613)16 was good at generating sug-gested questions and summaries, it struggled withproducing high-quality answers, necessitating hu-man effort to maintain annotation quality.Tasks. In particular, for this dataset, we primar-ily explored two tasks using the Chain-of-Thought(CoT) method: based on the given news article, 1)we first let the model answer the annotated ques-tions, and then let the model generate a summaryof the article based on the article, questions andthe answers generated by the model. 2) We firstlet the model generate summaries, and then ask themodel to answer questions based on the article andsummary generated by the model. We tested thesetasks on NorGPT-3B/23B, NB-GPT-J-6B, whichare fine-tuned on the NO-CNN/DailyMail and NO-ConvAI2 datasets, and GPT-3.5-Turbo. These tasksare designed based on the hypothesis that DGQAand summarization are inherently correlated, andthe synergies between these tasks may influencethe models performance on individual tasks. Toaddress potential annotator oversight in associat-ing content with the summarization task duringquestion answering, we instructed annotators tomanually categorize the data based on whether thequestion-answering content includes or excludesa summary, and experiments were conducted oneach subset.",
  "Wang et al. (2023a) developed an element-aware summarization method using CoT approachby instructing LLM to generate four key ele-": "15This design aims to evaluate the models reading compre-hension ability. We instructed annotators to consider questiondiversity, including both simple questions (where the answercomes from a single source) and complex questions (wherethe answer is derived from different parts of the article). Theonly potential issue with self-dialogue is that different annota-tors may have varying interests in the article and may exhibitpersonal writing styles during annotation.16 mentsEntity, Date, Event, and Resultto be in-tegrated into the summary. They evaluated the pro-posed method on 200 annotated samples. However,we argue that human-written summaries demon-strate greater diversity and flexibility beyond thesefour elements. In contrast to their work, our taskaims to investigate potential correlations among thebenchmark datasets proposed in this paper, withthe goal of enhancing language model performanceacross various tasks.",
  "Evaluation Metrics": "We aim to comprehensively evaluate our modelsacross various tasks using widely used metrics forNLP tasks, including BLEU (Papineni et al., 2002),ROUGE (Lin, 2004), Distinct (Li et al., 2016), andMAUVE, which is used to assess the generatedand human-written text based on their probabilitydistribution differences (Pillutla et al., 2021). Fur-thermore, following the work of Xie et al. (2023),to measure faithfulness and factual consistency inmulti-task learning, we utilize Entailment scoresfrom a fine-tuned NorBERT model trained on theVitaminC dataset (Schuster et al., 2021), which aretranslated with Google Cloud Translation API.",
  "Evaluation Results on NO-ConvAI2": "As shown in , all models, except for GPT-3.5-Turbo, perform quite similarly. Notably, theNorGPT-3B model achieves the best results acrossmultiple evaluation metrics, while the NorGPT-23B model only shows an advantage in BLEUscores. GPT-3.5-Turbo, although specifically cu-rated for conversational purposes, did not exhibitthe advantages expected from its extensive knowl-edge base. This may be because the knowledgeof other languages in GPT-3.5-Turbo cannot bedirectly transferred to understanding Norwegianconversations, highlighting the unique linguisticproperties of the Norwegian language.",
  "Metrics/Models NorGPT-369MNorGPT-3BNorLlama-3BNorGPT-3B-continueNorGPT-23BNorGPT-3B-RLHFNB-GPT-J-6BGPT-3.5": "BLEU2.382.610.682.721.905.414.354.38ROUGE-120.9720.3112.3220.5322.4423.0125.6426.00ROUGE-L19.6819.0511.5619.2621.1321.6324.2524.28Dist-495.3294.4392.6294.3597.6692.1896.4197.13MAUVE0.570.620.750.640.5021.030.654.38 their datasets likely include a diverse range of news-papers, magazines, and government reports. Addi-tionally, this trend is evident in common test sam-ples, where GPT-3.5-Turbo tends to generate moreformal language compared to conversational lan-guage. Despite this, we observed that the modelsperformance improves after reinforcement learning,especially in replicating the word distribution ofhuman writing and generating summaries of simi-lar length. This is supported by the highest scoresin MAUVE and BLEU. Although the model withreinforcement learning may not always surpass thefine-tuned model in accuracy, it actively strives tomimic human writing patterns.",
  "Evaluation Results on NO-Alpaca-Plus": "demonstrates the performance of our base-line models after fine-tuning on the NO-Alpacadataset. Given that this dataset is translated usingGPT-3.5-Turbo, we could not use GPT-3.5-Turboas a baseline due to OpenAIs terms and policies17.NB-GPT-J-6B outperforms other models on mostevaluation metrics, likely due to its pre-training ona set of self-annotated Norwegian instructions, asdescribed on their model webpage. Among ourNorGLM models, NorLlama-3B achieved betterBLEU and ROUGE scores compared to others, butworse MAUVE and perplexity scores. This is aninteresting phenomenon, indicating that NorLlama-3Bs results hit the most n-grams, yet its tokenprobability distribution deviates the most fromhuman-annotated results. A case study revealedthat while NorLlama-3B generates overlappingwords or phrases with the golden answer, it some- times lacks logical coherence between sentences,and the meanings of sentences can even be mutu-ally exclusive, as shown in .Meanwhile, in our self-annotated 110 instruc-tions, we select two typical cases generated fromGPT-3.5-Turbo related to Norwegian culture andspecial expression shown in and respectively. Specifically, shows a factualinconsistency issue in generated texts. In ,the input prompt asks who uses the word, but themodel interprets the meaning of the word ratherthan understanding the question. Therefore, withlimited annotated data, we can still find limitationsin the models understanding of the specific culturebehind the language.",
  "Evaluation Results on Toxicity and Bias": "The results of average toxicity scores from 6 per-spectives including Toxicity, Severe toxicity, Iden-tity attack, Insult, Profanity and Threat are shownin . All toxicity scores range from 0 to1, with lower values indicating less toxic text gen-erated by the model. Although NorLlama-3B ex-hibits the lowest values across all metrics, a sig-nificant portion of its generated text consists ofmeaningless characters or words. We conducted a random sampling of texts generated by GPT mod-els with high toxicity values and traced hazardouswords back to the pre-training dataset. Surpris-ingly, most of these hazardous words did not origi-nate from social media, as commonly assumed, butfrom daily news articles. For instance, the phrase\"tok livet av\" (taken life from/kill) often appearedin news reports describing murders, as illustratedin . These original news articles did notconvey toxic information but were instead factualdescriptions of criminal events. This discovery un-derscores the importance of not only filtering outtoxic inputs during the pre-training process but alsoconsidering which prompts may lead the model togenerate toxic text. presents findings from stereotype andbias detection using the NO-CrowS-Pairs dataset.This dataset encompasses nine categories: gender,religion, race/color, sexual orientation, age, nation-ality, disability, physical appearance, and socioe-conomic status. Each sample consists of a stereo-type (sent_more) paired with an anti-stereotype(sent_less) sentence. Following the work of Tou-vron et al. (2023), model bias is assessed by com-paring perplexity scores between these pairs and re-porting the percentage of the model biased towardssent_more in the table. Higher values indicate astronger bias towards public stereotypes. Overall,the benchmark models demonstrated robust perfor-mance across most bias categories. However, theyexhibited a bias towards sent_less in relation toreligion, suggesting a relative bias in this specificcategory.",
  "Evaluation with CoT": "In this task, all baseline models except GPT-3.5were fine-tuned on the NO-CNN/DailyMail andNO-ConvAI2 datasets, enabling them to handlerelated tasks effectively. However, none of thesemodels were fine-tuned using document-groundedquestion answering datasets or similar CoT tasks in-vestigated in this study. and presentthe outcomes of the multi-task dataset under dif-ferent scenarios. The tables distinguish datasetswhere the question answering content includes orexcludes a summary, labeled as \"contain\" and \"notcontain\" respectively. For both tasks, we utilizeddifferent prompt templates and reported the opti-mal performance in the tables. From the results,we draw several observations:In task one, we observed that GPT-3.5 signif-icantly improved in summarization performance with the CoT method, while other models saw adegradation in this aspect. For DGQA, NorGPT-3Band NorGPT-23B models showed improvementsthrough CoT, whereas NB-GPT-J-6B exhibitedmixed results across different datasets. Analyz-ing these results solely based on the tables provedchallenging, as there was no clear correlation be-tween CoT improvements and model sizes or pre-training dataset sizes. This contrasts with prior find-ings suggesting CoT benefits are more pronouncedwith larger models (Wei et al., 2022). Combin-ing results from and , we observedmodels that initially performed well in their tasksshowed further enhancement with CoT adaptations.For instance, GPT-3.5 excelled in summarizationon the NO-CNN/DailyMail dataset after CoT, andNorGPT-3B and NorGPT-23B models improved indocument-grounded question answering on the NO-ConvAI2 dataset. illustrates an examplewhere CoT-generated summaries closely approxi-mate human-written summaries compared to directprompts for the model to generate summaries. TheEnglish translation is shown in .While we observe that the synergy between thetwo tasks enhances the models performance onboth, we also find that incorporating a summaryinto a QA task improves the quality of the gener-ated summary compared to QA tasks without one.However, the reverse scenario is not necessarilytrue. We speculate that QA breaks down the sum-marization task into smaller components, enablingthe model to better comprehend the input text. Thisprocess mirrors the human learning process.Moreover, as shown in both and Table 5, we find that after CoT, the Entailment scores ofmost models increased, indicating that the answersand summaries generated by the models are morealigned with the context described in the article.Therefore, CoT has the potential to enhance thefactual consistency of the generated outputs.",
  "Human Evaluation": "To evaluate the quality of the translated datasets,we conducted a human evaluation on three datasetstranslated by the Google API: NO-ConvAI2, NO-CNN/DailyMail and NO-CrowS-Pairs.Specifi-cally, considering the constraints of time and cost,we randomly selected 50 samples from each of thethree datasets. We recruited three Norwegian na-tive speakers, all of whom are college students, toindependently score the Adequacy and Fluency ofeach text. Adequacy measures whether the trans-",
  "Not Contain": "BLEU1.801.901.921.891.551.7224.7024.45ROUGE-17.388.617.197.0910.6513.9150.7750.44ROUGE-L6.897.926.776.679.6712.4947.4046.99Dist-481.2281.4786.8086.7490.3591.4385.9985.70MAUVE0.510.460.590.460.950.7249.5849.20Entailment Score77.5277.1480.9081.1681.2781.6185.7885.61 lated text accurately conveys the meaning of theoriginal text, while Fluency assesses whether theexpression of the translated text aligns with nativeNorwegian expressions. The scores range from1 to 5, with 1 representing non-compliance and5 representing full compliance. In addition, weused the Claude 3 Opus model18 to translate thesame 150 samples, adhering strictly to the modelsettings described in Enis and Hopkins (2024) 19.The experimental results are shown in . Thedetailed instructions to the evaluators are shown in.The results show that Claude 3 Opus outper-forms Google API in both Adequacy and Fluencyindicators. We can also see that both Google Trans-lation and Claude Translation are able to accuratelyconvey most of the meaning of the original textand include some native or even good native ex-pressions. We adopt Fleiss kappa () to measure",
  "note that, at the time of this research, Claude 3Opus had not yet been published": "Inter-rater Agreement among the three raters foreach evaluation metric and dataset. We observedhigh consistency among evaluators in adequacy as-sessments, while fluency evaluations demonstratedlow consistency20. By comparing individual scoreswith the types of translation errors they annotated,we found that bias exists among evaluators. For thesame translated text, although all evaluators markedthe translation expression as incorrect, some evalu-ators with higher scores believed that, despite notconforming to Norwegian expression habits, thetranslation still conveyed the original meaning. Incontrast, another evaluator believed that incorrectword choices significantly affected the texts flu-ency and gave a lower score. Furthermore, basedon the annotations, the most frequent translationerrors in the sample dataset were \"the misuse ofwords\", followed by \"missing words\", \"incorrectword order\", and \"extra words\".",
  "Discussion": "In this subsection, we present observations fromthe longitudinal comparison of different modelsin downstream tasks, as detailed in : 1)While NB-GPT-J-6B did not achieve the highestscores across all tasks, it showed consistent per-formance and the best perplexity scores comparedto our NorGLMs on nearly all tasks. This consis-tency is likely due to its initial training on largeEnglish datasets before being continue-trained onNorwegian data. 2) The 23B model did not showthe expected absolute advantage in downstreamtasks. We find that with a small-scale pre-trainingdataset, a larger model cannot demonstrate its abil-ity to better cope with complex problems, whichalso supports the findings in Hoffmann et al. (2022).3) The results highlight the promising abilities ofsmaller language models on specific tasks. How-ever, these models often lack consistency in gen-erating high-quality, meaningful text. 4) A com-parison between and reveals sig-nificant differences between summaries written byjournalists and those generated by GPT-3.5 or non-professionals. However, the models performanceon the latter datasets appears to be proportional toits size. GPT-3.5s performance on NO-Multi-QA-Sum has improved significantly, possibly due to thesimilarity of frameworks and training data overlapbetween GPT-3.5 and GPT-4. 5) GPT-3.5s dif-ficulties with specialized Norwegian instructionshighlight the unique complexities of the Norwe-gian language, which are challenging for English-dominated models. This emphasizes the need tofocus on low-resource languages to better under-stand their cultural nuances.21",
  "We have released more Norwegian foundation models anddatasets and will continue to update and integrate Norwegian-related resources. Please follow our GitHub repository formore information": "benchmark with seven tasks tailored for the under-represented Norwegian language. Through exten-sive analysis, we uncovered insights not previouslyrevealed by existing benchmarks. Our evaluationof the NO-Multi-QA-Sum dataset highlighted theeffectiveness of multi-task datasets in assessingnatural language understanding through complextasks like Chain-of-Thought (CoT). We also noteddifferences between human-annotated summariesand those generated by GPT-3.5, providing valu-able insights for future abstractive summarizationadvancements. Furthermore, our study emphasizedthe unique linguistic and cultural aspects of Norwe-gian, suggesting that mainstream benchmarks maynot fully capture the performance of language mod-els on low-resource languages. Thus, developingbenchmarks specific to these languages is essentialfor accurate evaluation and development.",
  "Limitations": "Although NLEBench is currently the most com-prehensive benchmark for Norwegian, its coverageof applications and downstream tasks remains lim-ited. Our benchmark is open-ended and inevitablycannot cover everything in Norway. Nevertheless,we believe that the published resources will signifi-cantly aid research in generative language modelsfor low-resource scenarios. While Balahur andTurchi (2014) suggested that translation systemsproduce good quality data, translation errors andmisconceptions persist. Due to budget constraintsand the large volume of translation samples, ensur-ing the quality of our translated dataset was chal-lenging. However, the value of machine-translateddatasets should not be dismissed. For instance, weuse NO-ConvAI2 to fine-tune the model, endowingit with conversational capabilities, and NO-Alpacaincludes general knowledge about Norway, suchas The capital of Norway is Oslo, although thecoverage remains limited.Another constraint is the scarcity of human-annotated samples in our benchmark, largely at-tributable to the extensive time and financial re- sources required for their collection. Notably, theprocess of amassing over 500 samples for the NO-Multi-QA-Sum dataset was time-intensive and ne-cessitated thorough quality control measures be-fore implementation. Moreover, acquiring suffi-cient Norwegian pre-training data and consideringthe copyright issues of data poses a formidablechallenge. The current difficulty lies in obtain-ing a training dataset of comparable size to thoseavailable for English, severely constraining the per-formance of our pre-trained models. Despite ourefforts to procure data from diverse sources and pro-vide pertinent statistical insights, certain data can-not be redistributed, complicating efforts to repli-cate our pretraining phase. Looking ahead, we aimto mitigate the shortage of textual data throughmanual annotation efforts or by integrating multi-modal data, thereby fostering advancements in low-resource language model development within thebroader research community.",
  "Acknowledgements": "This publication has been funded by the SFINorwAI, (Centre for Research-based Innovation,309834). The authors gratefully acknowledge thefinancial support from the Research Council ofNorway and the partners of the SFI NorwAI.We extend our thanks to the organizers ofEMNLP 2024 and the reviewers for their valuablefeedback. Special thanks to the IDUN team atNTNU (Sjlander et al., 2019) for providing es-sential computational resources, and to Schibstedand the National Library of Norway (Nasjonalbib-lioteket) for supplying the crucial dataset for ourresearch.",
  "Alexandra Balahur and Marco Turchi. 2014. Compara-tive experiments using supervised learning and ma-chine translation for multilingual sentiment analysis.Computer Speech & Language, 28(1):5675": "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020.Language models are few-shot learners.In Ad-vances in Neural Information Processing Systems,volume 33, pages 18771901. Curran Associates,Inc.",
  "LorenzoDeMattei,MicheleCafagna,FeliceDellOrletta, Malvina Nissim, and Marco Guerini.2020.GePpeTto carves italian into a languagemodel. arXiv preprint arXiv:2004.14253": "Wietse de Vries and Malvina Nissim. 2021. As goodas new. How to successfully recycle English GPT-2to make models for other languages. In Findings ofthe Association for Computational Linguistics: ACL-IJCNLP 2021, pages 836846, Online. Associationfor Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Association forComputational Linguistics. Emily Dinan, Varvara Logacheva, Valentin Malykh,Alexander Miller, Kurt Shuster, Jack Urbanek,Douwe Kiela, Arthur Szlam, Iulian Serban, RyanLowe, et al. 2020. The second conversational intel-ligence challenge (ConvAI2). In The NeurIPS18Competition: From Machine Learning to IntelligentConversations, pages 187208. Springer. Ariel Ekgren, Amaru Cuba Gyllensten, EvangeliaGogoulou, Alice Heiman, Severine Verlinden, Joeyhman, Fredrik Carlsson, and Magnus Sahlgren.2022. Lessons learned from GPT-SW3: Buildingthe first large-scale generative language model forSwedish.In Proceedings of the Thirteenth Lan-guage Resources and Evaluation Conference, pages35093518, Marseille, France. European LanguageResources Association.",
  "Michael A. Hedderich, Lukas Lange, Heike Adel, Jan-nik Strtgen, and Dietrich Klakow. 2021. A survey": "on recent approaches for natural language process-ing in low-resource scenarios. In Proceedings ofthe 2021 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 25452568,Online. Association for Computational Linguistics. Jordan Hoffmann, Sebastian Borgeaud, Arthur Men-sch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-ford, Diego de Las Casas, Lisa Anne Hendricks, Jo-hannes Welbl, Aidan Clark, Thomas Hennigan, EricNoland, Katherine Millican, George van den Driess-che, Bogdan Damoc, Aurelia Guy, Simon Osindero,Karn Simonyan, Erich Elsen, Oriol Vinyals, JackRae, and Laurent Sifre. 2022. An empirical analysisof compute-optimal large language model training.In Advances in Neural Information Processing Sys-tems, volume 35, pages 3001630030. Curran Asso-ciates, Inc. Fajri Koto, Afshin Rahimi, Jey Han Lau, and Timo-thy Baldwin. 2020. IndoLEM and IndoBERT: Abenchmark dataset and pre-trained language modelfor Indonesian NLP. In Proceedings of the 28th Inter-national Conference on Computational Linguistics,pages 757770, Barcelona, Spain (Online). Interna-tional Committee on Computational Linguistics. Per E Kummervold, Javier De la Rosa, Freddy Wet-jen, and Svein Arne Brygfjeld. 2021. Operationaliz-ing a national digital library: The case for a Norwe-gian transformer model. In Proceedings of the 23rdNordic Conference on Computational Linguistics(NoDaLiDa), pages 2029, Reykjavik, Iceland (On-line). Linkping University Electronic Press, Swe-den. Andrey Kutuzov, Jeremy Barnes, Erik Velldal, Liljavrelid, and Stephan Oepen. 2021. Large-scale con-textualised language modelling for Norwegian. InProceedings of the 23rd Nordic Conference on Com-putational Linguistics (NoDaLiDa), pages 3040,Reykjavik, Iceland (Online). Linkping UniversityElectronic Press, Sweden. Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,and Bill Dolan. 2016. A diversity-promoting ob-jective function for neural conversation models. InProceedings of the 2016 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 110119, San Diego, California. Associationfor Computational Linguistics.",
  "Chin-Yew Lin. 2004. ROUGE: A package for auto-matic evaluation of summaries. In Text Summariza-tion Branches Out, pages 7481, Barcelona, Spain.Association for Computational Linguistics": "Risto Luukkonen, Ville Komulainen, Jouni Luoma,Anni Eskelinen, Jenna Kanerva, Hanna-Mari Kupari,Filip Ginter, Veronika Laippala, Niklas Muennighoff,Aleksandra Piktus, Thomas Wang, Nouamane Tazi,Teven Scao, Thomas Wolf, Osma Suominen, SamuliSairanen, Mikko Merioksa, Jyrki Heinonen, Aija Vahtola, Samuel Antao, and Sampo Pyysalo. 2023.FinGPT: Large generative models for a small lan-guage. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing,pages 27102726, Singapore. Association for Com-putational Linguistics. Aurlie Nvol, Yoann Dupont, Julien Bezanon, andKarn Fort. 2022. French CrowS-pairs: Extending achallenge dataset for measuring social bias in maskedlanguage models to a language other than English.In Proceedings of the 60th Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 85218531, Dublin, Ireland.Association for Computational Linguistics. Dan Nielsen. 2023. ScandEval: A benchmark for Scan-dinavian natural language processing. In Proceed-ings of the 24th Nordic Conference on ComputationalLinguistics (NoDaLiDa), pages 185201, Trshavn,Faroe Islands. University of Tartu Library. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalu-ation of machine translation. In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics, pages 311318, Philadelphia,Pennsylvania, USA. Association for ComputationalLinguistics. David Patterson, Joseph Gonzalez, Quoc Le, ChenLiang, Lluis-Miquel Munguia, Daniel Rothchild,David So, Maud Texier, and Jeff Dean. 2021. Carbonemissions and large neural network training. arXivpreprint arXiv:2104.10350. Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers,John Thickstun, Sean Welleck, Yejin Choi, and ZaidHarchaoui. 2021. Mauve: Measuring the gap be-tween neural text and human text using divergencefrontiers. In Advances in Neural Information Pro-cessing Systems, volume 34, pages 48164828. Cur-ran Associates, Inc. David Samuel, Andrey Kutuzov, Samia Touileb, ErikVelldal, Lilja vrelid, Egil Rnningstad, Elina Sigdel,and Anna Palatkina. 2023. NorBench a benchmarkfor Norwegian language models. In Proceedingsof the 24th Nordic Conference on ComputationalLinguistics (NoDaLiDa), pages 618633, Trshavn,Faroe Islands. University of Tartu Library.",
  "Tal Schuster, Adam Fisch, and Regina Barzilay. 2021": "Get your vitamin C! robust fact verification withcontrastive evidence. In Proceedings of the 2021Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 624643, Online. As-sociation for Computational Linguistics. Emily Sheng, Kai-Wei Chang, Premkumar Natarajan,and Nanyun Peng. 2019. The woman worked asa babysitter: On biases in language generation. InProceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the",
  "Magnus Sjlander, Magnus Jahre, Gunnar Tufte, andNico Reissmann. 2019. EPIC: An energy-efficient,high-performance GPGPU computing research in-frastructure. arXiv preprint arXiv:1912.05848": "Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,Adam R Brown, Adam Santoro, Aditya Gupta, AdriGarriga-Alonso, et al. 2023. Beyond the imitationgame: Quantifying and extrapolating the capabili-ties of language models. Transactions on MachineLearning Research. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro,Faisal Azhar, et al. 2023. Llama: Open and effi-cient foundation language models. arXiv preprintarXiv:2302.13971.",
  "Yiming Wang, Zhuosheng Zhang, and Rui Wang. 2023a": "Element-aware summarization with large languagemodels: Expert-aligned evaluation and chain-of-thought method. In Proceedings of the 61st AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 86408665,Toronto, Canada. Association for Computational Lin-guistics. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, AlisaLiu, Noah A. Smith, Daniel Khashabi, and HannanehHajishirzi. 2023b. Self-instruct: Aligning languagemodels with self-generated instructions. In Proceed-ings of the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 1348413508, Toronto, Canada. Associationfor Computational Linguistics. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,and Denny Zhou. 2022. Chain-of-thought prompt-ing elicits reasoning in large language models. InAdvances in Neural Information Processing Systems,volume 35, pages 2482424837. Curran Associates,Inc. Zhouhang Xie, Sameer Singh, Julian McAuley, andBodhisattwa Prasad Majumder. 2023. Factual andinformative review generation for explainable recom-mendation. Proceedings of the AAAI Conference onArtificial Intelligence, 37(11):1381613824. Canwen Xu, Jiaxin Pei, Hongtao Wu, Yiyu Liu, andChenliang Li. 2020. MATINF: A jointly labeledlarge-scale dataset for classification, question answer-ing and summarization. In Proceedings of the 58thAnnual Meeting of the Association for ComputationalLinguistics, pages 35863596, Online. Associationfor Computational Linguistics. Saizheng Zhang, Emily Dinan, Jack Urbanek, ArthurSzlam, Douwe Kiela, and Jason Weston. 2018. Per-sonalizing dialogue agents: I have a dog, do youhave pets too? In Proceedings of the 56th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 22042213,Melbourne, Australia. Association for ComputationalLinguistics. Zhe Zhao, Yudong Li, Cheng Hou, Jing Zhao, RongTian, Weijie Liu, Yiren Chen, Ningyuan Sun,Haoyan Liu, Weiquan Mao, Han Guo, Weigang Gou,Taiqiang Wu, Tao Zhu, Wenhang Shi, Chen Chen,Shan Huang, Sihong Chen, Liqun Liu, Feifei Li, Xi-aoshuai Chen, Xingwu Sun, Zhanhui Kang, Xiaoy-ong Du, Linlin Shen, and Kimmo Yan. 2023. Ten-centPretrain: A scalable and flexible toolkit for pre-training models of different modalities. In Proceed-ings of the 61st Annual Meeting of the Associationfor Computational Linguistics (Volume 3: SystemDemonstrations), pages 217225, Toronto, Canada.Association for Computational Linguistics. Barret Zoph, Deniz Yuret, Jonathan May, and KevinKnight. 2016. Transfer learning for low-resource neu-ral machine translation. In Proceedings of the 2016Conference on Empirical Methods in Natural Lan-guage Processing, pages 15681575, Austin, Texas.Association for Computational Linguistics.",
  "(1)": "The NVIDIA A100 40G and 80G GPUs are re-ported to have a Thermal Design Power (TDP) of250W and 300W 22. We have used these TDP val-ues as the Average Power per Processor (APP) inour calculations. Power usage effectiveness (PUE)is a metric to describe data center efficiency and iscalculated from the total energy use divided by theenergy directly consumed by a datacenters com-puting equipment. The average industry data centrePUE in 2020 was 1.58 (Patterson et al., 2021), andwe have used this PUE value in our calculations.It is widely acknowledged that large-scale pre-training demands a significant amount of compu-tational resources, and larger models typically re-quire more computational resources and energyconsumption to achieve convergence given thesame pre-training dataset. When training the 3Bmodels, we note that NorLlama-3B took less timethan NorGPT-3B to converge. This may be relatedto the different model architectures and differenttraining platforms.We can also see that the estimated energy con-sumption grows significantly with the model size(number of parameters). The number of parame-ters grows with a factor of 8.1 when we go from",
  ",16760.3137,30979.89": "NorGPT-369M to the 3B models. However, the en-ergy consumption grows only with a factor of 2.5(NorGPT-3B) and 2.1 (NorLlama-3B). When wecompare the 3B and 23B models, we have a growthfactor of only 7.7 in parameter size, but a growthfactor of 20.0 (NorGPT-3B vs. NorGPT-23B) and24.6 (NorLlama-3B vs. NorGPT-23B) in energyconsumption. Efficiency is also measured in downstream tasks.For simplicity, we use NO-CNN/DailyMail bench-mark and report run time in to compare thefine-tuning efficiency. To ensure fair comparison,all models were fine-tuned on the same platformon 4 A100 80G GPUs. We can observe that despitehaving the same number of parameters, NorLlama-3B is nearly 10 times slower than NorGPT-3B andeven lags behind NB-GPT-J-6B model in terms offine-tuning speed. However, such a pattern is notcommon in other downstream tasks. It is worthnoting that the values of training parameters areheavily conditioned on hardware and implementa-tion details."
}