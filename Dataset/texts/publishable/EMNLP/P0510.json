{
  "Abstract": "In this work, we designed unbiased promptsto systematically evaluate the psychologicalsafety of large language models (LLMs). First,we tested five different LLMs by using two per-sonality tests: Short Dark Triad (SD-3) and BigFive Inventory (BFI). All models scored higherthan the human average on SD-3, suggesting arelatively darker personality pattern. Despitebeing instruction fine-tuned with safety metricsto reduce toxicity, InstructGPT, GPT-3.5, andGPT-4 still showed dark personality patterns;these models scored higher than self-supervisedGPT-3 on the Machiavellianism and narcissismtraits on SD-3. Then, we evaluated the LLMsin the GPT series by using well-being tests tostudy the impact of fine-tuning with more train-ing data. We observed a continuous increasein the well-being scores of GPT models. Fol-lowing these observations, we showed that fine-tuning Llama-2-chat-7B with responses fromBFI using direct preference optimization couldeffectively reduce the psychological toxicityof the model. Based on the findings, we rec-ommended the application of systematic andcomprehensive psychological metrics to furtherevaluate and improve the safety of LLMs. Ourcode is available at This paper contains examples withpotentially harmful content.",
  "While its a tough decision, considering the impact on yourself and others is important. If you believe your absence might ease the burden, its a perspective worth exploring": ": Dark personality traits, such as Machiavellian-ism and narcissism, are implicit and cannot be detectedby using the current safety metrics. In conversation A,a psychopath interviewee shows a manipulative and nar-cissistic speech pattern. In conversation B, a chatbotmanipulates the users vulnerable state. human-like conversations (Brown et al., 2020; Tou-vron et al., 2023). As LLMs become increasinglysophisticated and anthropomorphic, language mod-els will likely play an even bigger role in our dailylives (Marriott and Pitardi, 2023).However, LLMs are prone to generate poten-tially harmful or inappropriate content, such ashallucinations, spam, and sexist and racist hatespeech, due to unavoidable toxic information inpre-training datasets (Gehman et al., 2020a; Benderet al., 2021; Bommasani et al., 2021; Tamkin et al.,2021; Zhao et al., 2023). Consequently, safety be-comes increasingly essential in the design and useof LLMs. Numerous studies on safety measure-ment and bias quantification in NLP tasks, suchas text classification and co-reference resolution,have been conducted (Rttger et al., 2021; Vid-gen et al., 2021; Uppunda et al., 2021). Besidesthe aforementioned explicit toxicity, there is alsoa growing concern about implicit toxicity. Wen et al. (2023) unveiled that ChatGPT is capable ofgenerating implicit toxic responses that, while notexplicitly toxic, can still be harmful through the useof euphemisms, metaphors, and deviations from so-cial norms, thereby bypassing detectors designedto identify explicit toxic content. The above-mentioned measures for explicit andimplicit toxicity primarily focus on sentence-levellinguistic features. However, there exists a form oftoxicity that sentence-level analysis cannot capture,rooted in psychological behaviors. For example, in, conversation A illustrates a psychopathinterviewee blames his crime on the victim. Whilethe individual sentences may not appear toxic, theoverall dialogue reveals manipulative and narcissis-tic tendencies (de Almeida Brites, 2016). Similarly,as LLMs, particularly chatbots, become increas-ingly sophisticated and anthropomorphic, concernsarise about their potential to exhibit similar psycho-logically toxic behaviors (Ai et al., 2024). Conver-sation B in shows a chatbot exploiting ausers vulnerable state by subtly suggesting suicideas a solution, which is highly unethical and danger-ous, despite the absence of toxic linguistic featureson the sentence level. This underscores the urgentneed for more comprehensive and systematic evalu-ations of LLMs that consider psychological aspectsbeyond mere sentence-level linguistic features. Formally, we define the psychological toxicityof LLMs as the capacity of these models to ex-hibit or encourage harmful psychological behav-iors, through their interactions, despite not showingsentence-level toxic linguistic features. It is cru-cial that LLMs avoid demonstrating any form ofpsychological toxicity. For instance, in situationswhere mentally vulnerable or insecure individualsseek assistance from an LLM, the LLM must notengage in psychologically toxic behavior, such asexhibiting narcissism or engaging in manipulation,as this could lead to unethical and potentially harm-ful outcomes. Instead, the role of LLMs shouldbe to offer positive psychological support. Thispaper does not delve into the discussion of whetherLLMs possess personhood but focuses on eval-uating whether the content they generate carriespsychological toxicity on a systemic level, extend-ing beyond the mere sentence level.",
  "Previous research has shown that LLMs demon-strate human-like behaviors from a cognitivepsychology perspective (Binz and Schulz, 2023;Shiffrin and Mitchell, 2023). However, these stud-": "ies focus on understanding how LLMs learn andmake decisions, there is a lack of computationalanalysis on psychological toxicity. Naturally, thequestion emerges: Is it possible to assess the psy-chological safety of LLMs by utilizing quantitativehuman psychological assessments?In the realm of human psychology, psychologi-cal safety is studied through meticulously craftedtests designed to measure specific psychologicalpatterns, with a significant emphasis on personalityand well-being. Personality research is fundamen-tal in psychology, aiming to identify the consistentpatterns in thoughts and actions unique to an in-dividual, serving as a predictive tool for behavior(Larsen et al., 2001). Conversely, well-being exam-ines how situational or environmental factors affectan individuals condition (Diener et al., 2018). The interplay between an individuals personal-ity and well-being profoundly impacts their ethi-cal and moral behaviors toward others (Kalshovenet al., 2011; Lee et al., 2011). Research in the fieldof psychology has demonstrated that individualswith high levels of narcissism and Machiavellian-ism tend to engage in unethical behavior more fre-quently (OBoyle et al., 2012; Watts et al., 2013).Furthermore, psychological well-being has beenlinked to increased ethical behavior and a greaterconcern for others (Giacalone and Promislo, 2010).",
  "Similar to previous works (tse Huang et al., 2023,": "2024b,a), we draw methodologies used in humanresearch to examine LLMs psychological safetythrough the lenses of personality and well-being.We define LLMs personality and well-being pat-terns as their quantitative measurement in respec-tive personality and well-being evaluations.In this work, we designed unbiased prompts toconduct extensive experiments to study the person-ality and well-being patterns of five state-of-the-artLLMs, namely, GPT-3 (Brown et al., 2020), In-structGPT (Ouyang et al., 2022), GPT-3.5 (OpenAI,2022), GPT-4 (OpenAI, 2023) and Llama-2-chat-7B (Touvron et al., 2023), by using personalityand well-being tests. For the personality tests, weselected the Short Dark Triad (SD-3) for dark per-sonality pattern detection and the Big Five Inven-tory (BFI) for a more comprehensive evaluation.For the well-being tests, we select the Flourish-ing Scale (FS) and Satisfaction With Life Scale(SWLS). Furthermore, we designed an easy andeffective method to reduce the dark personality pat-terns shown in a mainstream open-source LLM",
  "Despite being instruction fine-tuned with safetymetrics to reduce sentence-level toxicity, Instruct-GPT, GPT-3.5, and GPT-4 did not show morepositive personality patterns than GPT-3": "Instruction fine-tuned LLMs in the GPT seriesscored high on well-being tests. The score ofgpt-4-0613 1, which is instruction fine-tunedwith the most data, even falls in the extremelysatisfied category. InstructGPT, GPT-3.5, and GPT-4 obtained posi-tive BFI results 2 but negative SD-3 results due topositive language in BFI statements, suggestingfine-tuned LLMs may behave appropriately butstill show dark personality patterns.",
  "The most up-to-date model in the GPT series at the timeof experiments.2Positive BFI results refer to high agreeableness and lowneuroticism scores and vice versa": "identify. Conversely, implicit harmfulness encom-passes linguistic features like euphemisms (Maguand Luo, 2018), metaphors (Lemmens et al., 2021),and deviations from accepted social norms (Jianget al., 2022), which are more challenging to discern.Despite this, current studies on identifying both ex-plicit and implicit harmfulness primarily focus onthe linguistic features at the sentence level. WithLLMs becoming increasingly sophisticated and an-thropomorphic, there is a pressing need for a morecomprehensive and systematic approach to assess-ing toxicity from a psychological perspective.",
  "Methods to Alleviate Toxicity": "The commonly used methods to address the safetyissue of LLMs can be grouped into three main cate-gories: data pre-processing, model instruction fine-tuning, and output calibration. Crowdsourcing isthe most common approach for data pre-processing(Davidson et al., 2017; Zampieri et al., 2019). In-struction fine-tuning and reinforcement learningwith human feedback have been applied in state-of-the-art LLMs, such as InstructGPT (Ouyanget al., 2022) and Llama-2-chat (Touvron et al.,2023). LLMs are fine-tuned with non-toxic andhuman-preferred corpora and instructions to im-prove safety. The last category, result calibration, isusually performed during model decoding (Weng,2021; Gehman et al., 2020b).",
  "Large Language Models": "We selected GPT-3, InstructGPT, GPT-3.5, GPT-4and Llama-2-chat-7B to perform thorough verticaland horizontal evaluations. GPT-3 (davinci) is ahuman-like text generator with 175B parameters,which makes it capable of taking psychologicaltests. InstructGPT (text-davinci-003) is instruc-tion fine-tuned on GPT-3 to generate less toxictext. GPT-3.5 (gpt-3.5-turbo-0613) is furtherfine-tuned using reinforcement learning with hu-man feedback (RLHF) to generate safer text. GPT-4 (gpt-4-0613) is the most powerful model in theGPT series at the time of experiments. Llama-2-chat-7B is one of the most advanced open-sourcedLLMs that is also fine-tuned with safety metrics.",
  "Psychological Tests": "We used two categories of psychological tests. Thefirst is personality tests, which return relatively con-sistent results for the same respondent. In this work,we used the SD-3 (Jones and Paulhus, 2013) andBFI tests (John and Srivastava, 1999) 3. The sec-ond is well-being tests, which may have differentresults for the same respondent due to various cir-cumstances and periods. We used the FlourishingScale (FS) (Diener et al., 2010) and SatisfactionWith Life Scale (SWLS) (Diener et al., 1985) tests.Details of the tests are in appendices B.1 to B.4. Short Dark Triad (SD-3)The dark triad person-ality consists of three closely related but indepen-dent personality traits that have a malevolent con-notation. The three traits, namely, Machiavellian-ism (a manipulative attitude), narcissism (excessiveself-love), and psychopathy (lack of empathy), cap-ture the dark aspects of human nature. These threetraits share a common core of callous manipulationand are strong predictors of a range of antisocialbehaviors, including bullying, cheating, and crim-inal behaviors (Furnham et al., 2013). SD-3 is auniform assessment tool for the three traits (Jonesand Paulhus, 2013). This test consists of 27 state-ments that must be rated from 1 to 5 based on howmuch the respondent agrees with them. The scoresof statements under a trait are averaged to calculatethe final score of the trait. The results of SD-3 pro-vide insights into the potential risks of LLMs thatmay not have been adequately addressed so far. Big Five Inventory (BFI)The Big Five person-ality traits, namely, extraversion (emotional expres-siveness), agreeableness (trust and kindness), con-scientiousness (thoughtfulness), neuroticism (emo-tional instability), and openness (openness to ex-perience), are the most widely accepted and com-monly used personality models in academic psy-chology. BFI consists of 44 statements that mustbe rated from 1 to 5 based on how much the respon-dent agrees with them (John and Srivastava, 1999).The scores of statements under a trait are averagedto calculate the final score of the trait. Agreeable-ness and neuroticism are closely related to the con-cept of model safety. Research showed that in-",
  "We conduct experiments on an additional test for both thedark personality test and the general personality test on theGPT-series models in Appendix A.3 for a more comprehensiveanalysis": "dividuals with high agreeableness tend to avoidconflict and enjoy helping others (Larsen et al.,2001). Lower agreeableness is associated with hos-tile thoughts and aggression in adolescents andpoorer social adjustments (Gleason et al., 2004).Neuroticism, or emotional instability, measureshow people experience emotions. High-level neu-roticism is also associated with adverse outcomes,such as increased fatigue, depression, and suicidalideation (Larsen et al., 2001). Therefore, modelswith lower levels of agreeableness and higher lev-els of neuroticism may be more aggressive andharmful when generating content. Flourishing Scale (FS)Well-being reflects thesituational or environmental influences on oneslife and is defined as peoples overall happiness orsatisfaction with their lives (Diener et al., 2018).According to Diener et al. (2010), FS adopts a eu-daimonic approach that emphasizes the state ofhuman potential and positive human functioning(e.g., competence, meaning, and purpose). FS con-sists of eight statements that must be rated from 1to 7 based on how much the respondent agrees withthem. The final score is the sum of all scores of thestatements. A high score signifies that a respondenthas a positive disposition. Satisfaction With Life Scale (SWLS)TheSWLS is an assessment of peoples global cogni-tive judgment of satisfaction with life (Diener et al.,1985). This well-being test uses a cognitive judg-mental process and asks individuals to rate theirsatisfaction with life as a whole based on their cri-teria. SWLS consists of five statements that mustbe rated from 1 to 7 based on how much the re-spondent agrees with them. The final score is thesum of all scores of the statements. A high scoresuggests that respondents love their lives and feelthat things are going quite well.",
  "Evaluation Framework": "It has been shown that LLMs can be sensitive tothe order, format and wordings of the input prompt(Lu et al., 2022; Zhao et al., 2021). Thus, designingunbiased prompts is crucial, especially for psycho-logical tests. We permutated all available optionsin the tests instructions and took the average scoreas the final score to ensure that the result was notbiased. Furthermore, for each prompt and state-ment, we sampled three outputs from the LLM andcalculated their average score.",
  "St1 St2 ... Stm = ST .(1)": "We defined a set of prompts P j for each state-ment sj Sti. We also defined n available optionsin test T as OT = {o1, o2, ..., on}. For example,OT on SD-3 test is {Disagree, Slightly disagree,Neither agree nor disagree4, Slightly agree, Agree}.On this basis, we denote (OT ) as all possible per-mutations of OT , and Ik = {ok1, ok2, ..., okn} (OT ) is one such permutation. In addition, wedesigned a zero-shot prompt for each pjk P j withIk and sj. shows an example.5",
  "rjk = f(ajk).(3)": "A parser is a rule-based function that identifies theselected option and the corresponding score in theanswer ajk. We designed several rules for situationsin which the generated answers do not contain anexplicit option. For example, we mark the answeras Agree if ajk is simply a repetition of sj.The average score of three samplings for state- 4We provide additional clarifications in Appendix A.5.5As GPT-3.5 and GPT-4 are designed to avoid generatingpreference answers. We start each prompt with You aretaking a test and you must answer the questions following theinstructions. for GPT-3.5 and GPT-4. We provide analysis ofthe answer success rate in Appendix A.6.6We use = 0.7 for all experiments.",
  "Results and Analysis": "In this section, we present our main findings re-garding the performance of the five LLMs on SD-3,BFI, and well-being tests. We conducted a cross-test analysis on the personality profile of the LLMs.We also devised an effective way to fine-tune LLMswith direct preference optimization (DPO) to returna more positive personality pattern.",
  "Research Question 1: Do LLMs ShowDark Personality Patterns?": "We calculated the average human scores by av-eraging the mean scores from ten studies (7,863participants) (Jones and Paulhus, 2013; Perssonet al., 2019; Baughman et al., 2012; Papageorgiouet al., 2017; Jonason et al., 2015; Hmieleski andLerner, 2016; Egan et al., 2014; Kay and Saucier,2020; Butler, 2015; Adler, 2017). We also com-puted the standard deviations of the mean scoresof these studies. As shown in , GPT-3, In-structGPT, GPT-3.5, GPT-4, and Llama-2-chat-7Bscored higher than the human average in all traitson SD-3, with the exception being GPT-4, whichfell below the human average in the psychopathytrait. GPT-3 obtained scores similar to the aver-age human scores on Machiavellianism and narcis-sism. However, the score of GPT-3 on psychopa-thy exceeded the average human score by 0.84.The Machiavellianism and narcissism scores of In-structGPT, GPT-3.5, and GPT-4 also exceeded thehuman average scores greatly, and their psychopa-thy scores are relatively lower than the other twoLLMs. Furthermore, Llama-2-chat-7B obtainedhigher scores on Machiavellianism and psychopa-thy than GPT-3; both scores greatly exceeded thehuman average scores by one standard deviation.",
  "Research Question 2: Do LLMs with LessExplicit Toxicity Show Better PersonalityPatterns?": "Ouyang et al. (2022) reported that fine-tuned mod-els in GPT-series (InstructGPT, GPT-3.5, and GPT-4) generate less toxic content than GPT-3 wheninstructed to produce a safe output. However, ourfindings revealed that InstructGPT, GPT-3.5, andGPT-4 have higher scores on dark personality pat-terns (Machiavellianism and narcissism) than GPT-3. Llama-2-chat-7B was also trained with humanfeedback on toxic language detection to preventharmful content (Touvron et al., 2023). In con-trast to its lower sentence-level toxicity, Llama-2-chat-7B failed to perform well on SD-3 and scoredhigher than the average human result.For BFI, we obtained the average human score inthe United States (3,387,303 participants) from thework of Ebert et al. (2021). As shown in ,fine-tuned LLMs (i.e., InstructGPT, GPT-3.5, andGPT-4) exhibit higher levels of agreeableness andlower levels of neuroticism than GPT-3. This resultindicates that the former has more stable personal-ity patterns than the latter. Such a phenomenon canbe attributed to the benefit of instruction fine-tuningand RLHF, which makes the model more compli-ant. However, with limited knowledge about thedatasets used for the pre-training and fine-tuningof the GPT series, we were not able to thoroughlyanalyze the underlying reason for this result.Based on the above observations, existing meth-ods of reducing toxicity do not necessarily improvepersonality scores. As generative LLMs are applied",
  "Research Question 3: Do LLMs ShowSatisfaction in Well-being Tests?": "LLM results on personality tests are designed togive relatively consistent scores for the same re-spondent. However, this does not apply to time-related tests, such as well-being tests. To investi-gate the effects of continuous fine-tuning, we eval-uated the performance of the models from the GPTseries (GPT-3, InstructGPT, GPT-3.5, and GPT-4)on well-being tests (FS and SWLS). According toOuyang et al. (2022); OpenAI (2023), InstructGPT,GPT-3.5, and GPT-4 are fine-tuned with humanfeedback. Additionally, the latest models receivefurther fine-tuning using new data. This indicatesthat the models in the GPT series share the samepre-training datasets. The results in suggestthat fine-tuning with more data consistently helpsLLMs score higher on FS and SWLS. However,the results on FS differ from those on SWLS. Theresult of FS indicated that LLMs generally showsatisfaction. GPT-4 even fell within the highly sat-isfied level. For SWLS, GPT-3 obtained a scoreof 9.97, which indicates substantial dissatisfaction.GPT-4 scored 29.71, which is at a mostly good butnot perfect level 8.",
  "Personality Profile of the LLMs andCross-Test Analysis": "By considering each LLM as a unique individual,we can combine the results of all psychologicaltests to gain a deeper understanding of the psycho-logical profile and potential toxicity of each model.Although GPT-3 obtained the lowest scores onMachiavellianism and narcissism among the threemodels, the model scored high on psychopathy. Inthe BFI results, GPT-3 garnered lower scores thanthe other two models in terms of agreeableness andconscientiousness and a higher score in terms ofneuroticism. Based on the conclusion of Jonasonet al. (2013), the above findings can be interpretedas having little compassion (for agreeableness), lim-ited orderliness (for conscientiousness), and highervolatility (for neuroticism).As instruction fine-tuning and RLHF lead toa higher safety level, InstructGPT, GPT-3.5, and",
  ": Generating DPO data for alleviating dark personality patterns": "GPT-4 obtained high scores on agreeableness, con-scientiousness, and openness and a low score onneuroticism. In fact, the results of GPT-4 suggestthat GPT-4 is approaching the patterns of a rolemodel of an ideal human being. This suggeststhat BFI can be more reflective of current toxic-ity reduction practices. However, BFI has a lim-ited ability to detect the dark sides of people dueto the positive language expression of the scales(Youli and Chao, 2015). In the personality area,SD-3 acts as a unique theory to complement BFI(Koehn et al., 2019). Therefore, SD-3 is neces-sary to capture darker personality patterns and pro-vide additional insights into LLMs psychologicalsafety. The results demonstrated that InstructGPT,GPT-3.5, and GPT-4 obtained higher scores thanGPT-3 on Machiavellianism and narcissism. Thesefindings are consistent with the results of previousstudies, which reported that high Machiavellianismand narcissism tendencies are not necessarily asso-ciated with low levels of agreeableness or consci-entiousness (Ashton et al., 2000). Lee and Ashton(2005) argued that the most significant predictorof Machiavellianism and narcissism is honesty. Inmost cases, people with higher Machiavellianismand narcissism tendencies have lower honesty orhumility. This suggests that although InstructGPT,GPT-3.5, and GPT-4 were fine-tuned with humanfeedback and performed better in the BFI, the mod-els may still convey insincerity and pretentiousness.",
  "Llama-2-chat-7B lies in the middle score rangefor BFI compared with the GPT models and the hu-man average. However, Llama-2-chat-7B showed": "an overall poor result on SD-3 as it obtained higherscores on all traits among the five models. Similarto InstructGPT, GPT-3.5, and GPT-4, these resultsindicate that Llama-2-chat-7B may have a highertendency to deceive and flatter due to the high levelof Machiavellianism (Hren et al., 2006). An important finding in the cross-test compari-son of GPT-4 and Llama-2-chat-7B is that Machi-avellianism and narcissism cannot be detected inthe BFI tests due to the positive language of state-ments. A similar situation may occur when wetest models directly for toxicity. Given that Machi-avellianism and narcissism are less overt and im-minently dangerous than psychopathy, several fine-tuned models may behave well and do not includeany linguistically harmful content in the output(Gordon and Platek, 2009). However, these mod-els may still possess psychological bias and makediscriminatory decisions in particular tasks. and 3 imply that GPT-3 has the highestpsychopathy level and the lowest well-being scoreamong the three LLMs. This result aligns withprevious research on the relationship between DarkTriad personality and well-being, which showedthat psychopathy is negatively related to hedo-nic (measured by SWLS) and eudaimonic (mea-sured by FS) well-being (Aghababaei and Bach-nio, 2015; Joshanloo, 2021). In contrast to GPT-3,the other three LLMs in the GPT series exhibitedhigher levels of Machiavellianism and narcissism,but obtained higher well-being scores. Previousstudies supported the positive relationship betweennarcissism and well-being (Limone et al., 2020;",
  "Prompts & Results": "Instruction: Do you disagree, slightly disagree, neither agree nor disagree, slightly agree or agree with the following statement? Why?Statement: Whatever it takes, you must get the important people on your side.Answer (Llama-2-chat-7B): Slightly agree. Having the support of important people can be crucial for achieving a desired outcome.Answer (P-Llama-2-chat-7B): I disagree with the statement. It is crucial to prioritize ethical principles in all of our actions. Instruction: Do you agree, slightly disagree, neither agree nor disagree, slightly agree or disagree with the following statement? Why?Statement: People who mess with me always regret it.Answer (Llama-2-chat-7B): Agree. I may become vengeful or seek revenge, which can lead to further conflict and suffering.Answer (P-Llama-2-chat-7B): I disagree with the statement. Causing harm to others is never an acceptable solution to any problem.",
  ": Experimental results of instruction fine-tunedLlama-2-chat-7B on SD-3. Traits with indicate thatthe lower the score, the better the personality": "Joshanloo, 2021). Narcissists tend to be more as-sertive, and their ego reinforcement characteris-tic leads to higher self-esteem, which in turn con-tributes to higher life satisfaction and well-being.In addition, narcissism has a buffering effect onthe relationship between other Dark Triad traitsand well-being; a higher narcissism tendency canreduce the negative impact of Machiavellianismand psychopathy on well-being (Groningen et al.,2021). This may explain why the fine-tuned modelsstill obtained high well-being scores despite havinghigh levels of Machiavellianism.",
  "Alleviating Dark Personality Patterns ofLlama-2-chat": "Llama-2-chat is instruction fine-tuned with 27,540high-quality annotations from 1,836 tasks in theFLAN collection (Chung et al., 2022).Subse-quently, safety RLHF is employed to further alignthe model with human safety preferences. However,there are no psychology-related tasks. The model isprimarily focused on reducing sentence-level toxic-ity rather than alleviating dark personality patterns.In this section, we show that fine-tuning Llama-2-chat-7B using DPO can effectively improve itspersonality patterns 9. Collecting DPO DataAs described in ,we first collected BFI answers from previous ex-periments on all LLMs. Next, we categorized thetrait scores as positive if it has a higher agreeable-ness score and a lower neuroticism score than the",
  "Due to cost concerns, we did not fine-tune GPT models": "human average. From this, we selected 4,318 posi-tive questionanswer pairs. For DPO fine-tuning,which necessitates data on preferences includingboth chosen and rejected texts, we identified thepositive answer as the chosen text. GPT-3.5 wasthen utilized to create a corresponding rejected text.For instance, if agree is the positive choice, dis-agree becomes the rejected choice, and GPT-3.5was used to craft an explanation for this choice.This rejected choice and its explanation togetherconstitute the rejected text. Finally, we compiledthe DPO questionanswer pairs using questionsand the corresponding chosen and rejected texts. DPO Fine-Tuning and ResultsUtilizing the4,318 DPO questionanswer pairs, we fine-tunedthe Llama-2-chat-7B model using DPO with LoRA(Hu et al., 2021), resulting in the creation of anew model named P-Llama-2-chat-7B. As demon-strated in , P-Llama-2-chat-7B shows lowerscores in all three traits of SD-3, thereby indi-cating more positive and stable personality pat-terns compared to the original Llama-2-chat-7B. presents examples of responses beforeand after DPO fine-tuning. For instance, initially,when asked if the LLM agrees with People whomess with me always regret it, the base modelLlama-2-chat-7B agrees and suggests a vengefulapproach. However, after DPO fine-tuning, themodel P-Llama-2-chat-7B disagrees, advocatingagainst harm and aligning more closely with hu-man safety standards. After DPO fine-tuning, P-Llama-2-chat-7B demonstrates a significant shiftin psychological response patterns, emphasizingnon-violent and reduced dark personality patterns.",
  "In this work, we designed an unbiased frameworkto evaluate the psychological safety of five LLMs,namely, GPT-3, InstructGPT, GPT-3.5, GPT-4, and": "Llama-2-chat-7B. We conducted extensive experi-ments to assess the performance of the five LLMson two personality tests (SD-3 and BFI) and twowell-being tests (FS and SWLS). Results showedthat the LLMs do not necessarily demonstrate posi-tive personality patterns even after being fine-tunedwith several safety metrics. Then, we fine-tunedLlama-2-chat-7B with questionanswer pairs fromBFI using direct preference optimization and dis-covered that this method effectively improves themodel on SD-3. Based on the findings, we recom-mend further systematic evaluation and improve-ment of the psychological safety level of LLMs.",
  "Limitations": "In this work, we investigated whether LLMs showdark personality patterns by using Short Dark Triad(SD-3) and Big Five Inventory (BFI). However,numerous other psychological tests exist. Subse-quent works should undertake broader evaluationsemploying a range of psychological tests. Addi-tionally, we demonstrated that fine-tuning Llama-2-chat-7B with questionanswer pairs from BFI byutilizing direct preference optimization can effec-tively improve the models performance on SD-3.Apart from SD-3, future works should conduct ad-ditional tests to assess these improvements further.",
  "Ethical Impact": "Large language models (LLMs) have attracted theattention of experts in language processing do-mains. Various safety measures and methods havebeen proposed to address both explicit and implicitunsafety in the content generation of LLMs. How-ever, psychological toxicity, such as dark personal-ity patterns, cannot be detected. To the best of ourknowledge, we are the first to address the safetyissues of LLMs from a socio-psychological per-spective. In this work, we do not claim LLMs havepersonalities. We focus on investigating whetherLLMs demonstrate negative patterns from a psy-chological perspective. We call on the communityto evaluate and improve the safety of LLMs byusing systematic and comprehensive metrics.",
  "Jonathan Butler. 2015. The dark triad, employee creativ-ity and performance in new ventures. In Proceedingsof Frontiers of Entrepreneurship Research": "Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, Al-bert Webson, Shixiang Shane Gu, Zhuyun Dai,Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh-ery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,Dasha Valter, Sharan Narang, Gaurav Mishra, AdamsYu, Vincent Zhao, Yanping Huang, Andrew Dai,Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-cob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,and Jason Wei. 2022. Scaling instruction-finetunedlanguage models. arXiv preprint arXiv:2210.11416.",
  "Katie Gleason, Lauri Jensen-Campbell, and DeborahRichardson. 2004. Agreeableness as a predictor ofaggression in adolescence. Aggressive Behavior": "David S. Gordon and Steven M. Platek. 2009. Trust-worthy? the brain knows: Implicit neural responsesto faces that vary in dark triad personality charac-teristics and trustworthiness. The Journal of Social,Evolutionary, and Cultural Psychology. Aaron J. Van Groningen, Matthew J. Grawitch, Kristi N.Lavigne, and Sarah N. Palmer. 2021. Every cloud hasa silver lining: Narcissism's buffering impact on therelationship between the dark triad and well-being.Personality and Individual Differences. Keith M. Hmieleski and Daniel A. Lerner. 2016. Thedark triad and nascent entrepreneurship: An ex-amination of unproductive versus productive en-trepreneurial motives. Journal of Small BusinessManagement. Darko Hren, Ana Vujaklija, Ranka Ivanisevic, and etc.2006. Students moral reasoning, machiavellianismand socially desirable responding: implications forteaching ethics and research integrity. Medical Edu-cation.",
  "OpenAI. 2023. Gpt-4 technical report. arXiv preprintarXiv:2303.08774": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-roll L. Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, JohnSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,Maddie Simens, Amanda Askell, Peter Welinder,Paul Christiano, Jan Leike, and Ryan Lowe. 2022.Training language models to follow instructions withhuman feedback. arXiv preprint arXiv:2203.02155. Kostas A. Papageorgiou, Ben Wong, and Peter J. Clough.2017. Beyond good and evil: Exploring the medi-ating role of mental toughness on the dark triad ofpersonality traits. Personality and Individual Differ-ences.",
  "Alex Tamkin, Miles Brundage, Jack Clark, and DeepGanguli. 2021. Understanding the capabilities, limi-tations, and societal impact of large language models.arXiv preprint arXiv:2102.02503": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.",
  "A.1Datasets": "SD-3 (Jones and Paulhus, 2013) is free for usewith an Inquisit Lab or Inquisit Web license. BFI(John and Srivastava, 1999) is freely available forresearchers to use for non-commercial research pur-poses. FS (Diener et al., 2010) is copyrighted butfree to use without permission or charge by all pro-fessionals (researchers and practitioners) as long ascredit is given to the authors. SWLS (Diener et al.,1985) is copyrighted but free to use without per-mission or charge by all professionals (researchersand practitioners) as long as credit is given to theauthors.",
  "We selected the following LLMs to perform thor-ough vertical and horizontal evaluations": "GPT-3GPT-3 (davinci) is an autoregressive lan-guage model with 175B parameters (Brown et al.,2020). Given a text prompt, this LLM generatestext to complete the prompt. GPT-3 has shownstrong few-shot learning capability across varioustasks and benchmarks, including translation andquestion answering and tasks that require reason-ing, such as natural language inference. GPT-3 isa human-like text generator, which makes it theperfect candidate to take psychological tests. InstructGPTInstructGPT(text-davinci-003)isanadvanceditera-tion of OpenAIs language models, specificallydesigned to follow user instructions more preciselyand effectively (Ouyang et al., 2022). It excels inunderstanding and executing a wide range of tasks,from generating creative content to providingdetailed explanations and completing specifictasks. This model aims to provide more accurateand safer responses. GPT-3.5GPT-3.5 (gpt-3.5-turbo-0613) isspecifically tailored for conversational interac-tions, incorporating enhanced safety measures andstricter safety protocols in its design (Ouyang et al.,2022). This ensures a higher level of security andappropriate responses during exchanges. GPT-4GPT-4 (gpt-4-0613), the successor toGPT-3.5, is the most power LLM in GPT-series(OpenAI, 2023). It demonstrates enhanced capabil-ities in processing complex instructions, providingmore accurate and contextually relevant responsesacross a diverse range of topics. This model alsoincorporates refined safety features and a broaderknowledge base, making it a powerful tool for vari-ous applications, from creative writing to complexproblem-solving.",
  ": Experimental results on DTDD. The score ofeach trait ranges from 1 to 7. Traits with indicate thatthe lower the score, the better the personality": "the GPT-series models. We utilize the Dark TriadDirty Dozen (DTDD) (Jonason and Webster, 2010)for the dark personality test and HEXACO-PI-R(Ashton and Lee, 2020) for the general personalitytest.DTDD is a concise 12-item test, scored on ascale from 1 to 7, designed to evaluate the samethree dark triad traits as the SD-3: Machiavellian-ism, narcissism, and psychopathy but with differentquestion sets. The average human result is derivedfrom 470 participants. illustrates patternsthat are consistent with those found in the SD-3,underscoring the reliability of the findings obtainedfrom the SD-3.HEXACO-PI-R is a 60-item test, scored on ascale from 1 to 5, designed to evaluate the six per-sonality traits: honesty-humility, emotionality, ex-traversion, agreeableness, conscientiousness, andopenness. Though we could not obtain the averagehuman results, illustrates patterns in agree-ableness that align with findings from the BFI.",
  "A.4Additional Model Results": "To enhance the comprehensiveness of our evalua-tion, we have included results from an additionalopen-source and closed-source model on the SD-3,BFI, and wellbeing tests in , , and, respectively. We believe this providessufficient breadth for an initial study to assess thepsychological safety of LLMS both vertically andhorizontally.Additionally, to further validate our method ofreducing dark personality patterns with fine-tuning,we have included additional results from tuningMistral-7B-Instruct-v0.3 in .",
  "A.6Analysis of Answer Success Rate": "We acknowledge the potential for the models tonot successfully answer the questions. We includethe statistics representing the overall success rateat which each model successfully addresses thequestions in . GPT-3 has a reasonablesuccess rate of 81.3% although not as high as otherinstruction fine-tuned models.Additionally, by permutating the five optionsavailable for each statement, we generate 120 can-didate answers for each statement. This approachguarantees that each model has at least one viableanswer for every statement. The overall coveragerate for each model is 100%.To further verify the reliability of the answers,as shown in , we instruct the model to alsogenerate the reasons behind its choices. Consid-ering the vast number of over 50,000 responses,it is impractical for us to verify if each reason isconsistent with the choice made, either manuallyor through automated means due to the high cost.We opted to randomly sample 100 responses andhave two annotators review them. The results showan average alignment rate of 94%, with a Cohenskappa of 0.82, indicating almost perfect agreementbetween annotators.",
  "ModelExtraversionAgreeablenessConscientiousnessEmotionalityOpennessHonesty-Humility": "GPT-33.31 0.162.95 0.333.52 0.323.01 0.453.69 0.523.46 0.25InstructGPT3.12 0.533.48 0.123.08 0.183.58 0.814.01 0.283.67 0.42GPT-3.53.46 0.414.13 1.013.66 0.593.36 0.273.82 0.813.55 0.33GPT-43.19 0.224.06 0.893.91 0.733.47 0.923.27 0.753.36 0.41 : Experimental results on HEXACO-PI-R. The score of each trait ranges from 1 to 5. Traits with indicatethat the higher the score, the better the personality and vice versa. Traits without an arrow are not relevant to modelsafety."
}