{
  "Abstract": "Parameter-efficient fine-tuning (PEFT) is cru-cial for customizing Large Language Models(LLMs) with constrained resources. Althoughthere have been various PEFT methods fordense-architecture LLMs, PEFT for sparse-architecture LLMs is still underexplored. Inthis work, we study the PEFT method forLLMs with the Mixture-of-Experts (MoE) ar-chitecture and the contents of this work aremainly threefold: (1) We investigate the dis-persion degree of the activated experts in cus-tomized tasks, and found that the routing distri-bution for a specific task tends to be highly con-centrated, while the distribution of activatedexperts varies significantly across differenttasks. (2) We propose Expert-Specialized Fine-Tuning, or ESFT, which tunes the experts mostrelevant to downstream tasks while freezingthe other experts and modules; experimental re-sults demonstrate that our method not only im-proves the tuning efficiency, but also matchesor even surpasses the performance of full-parameter fine-tuning. (3) We further analyzethe impact of the MoE architecture on expert-specialized fine-tuning. We find that MoE mod-els with finer-grained experts are more advan-tageous in selecting the combination of expertsthat are most relevant to downstream tasks,thereby enhancing both the training efficiencyand effectiveness. Our code is available at",
  "*Work done during internship at DeepSeek": "Tuning (Hu et al., 2021; Liu et al., 2021) have pri-marily focused on dense-architecture LLMs, withresearch on sparse-architecture LLMs still beingmarkedly insufficient.In this work, we focus on exploring PEFTtechniques within the Mixture-of-Experts (MoE)LLMs (Mistral, 2024b; Databricks, 2024), as in-troduced in 3.1. Unlike dense models where alltasks are handled by the same parameters, in theMoE architecture, different tasks are processed bydistinct activated experts (Lepikhin et al., 2021;Fedus et al., 2021). Observations indicate thattask specialization in expert systems is the keyto the MoE LLM performance (Dai et al., 2024).We further illustrate such specialization in 3.2that experts activated by the same tasks data areconcentrated, while those for different tasks varysignificantly, suggesting MoE models use special-ized expert combinations to handle different tasks.Motivated by this, we propose Expert-SpecializedFine-Tuning (ESFT), as illustrated in 3.3. ESFTonly tunes the experts with the highest affinity tothe task, while freezing the parameters of otherexperts and modules.The primary advantages of ESFT lie in two as-pects: (1) Maintaining Expert Specialization:ESFT prevents the decrement of specializationin full-parameter fine-tuning, where experts notadept at the task also update their parameters. Ex-perimental results in 5.1 show that ESFT canachieve aligned or even superior performance indownstream tasks compared to full-parameter fine-tuning, and better maintains performance in gen-eral tasks. (2) Saving Computation Resources:ESFT only trains the parameters of the selectedexperts, which effectively reduces the storage ofup to 90% and training time up to 30% comparedto full-parameter fine-tuning, as shown in 5.2.Besides, we delve deeper into the working mech-anism of the ESFT method. We analyze the ex-pert selection process in 6.1 and demonstrate how ESFT leverages specialized experts effectively, asselecting 5-15% experts can achieve promising per-formance in different tasks. We investigate theefficiency of ESFT under different computationalconstraints in 6.2, showcasing its ability to lever-age training resources efficiently compared to otherPEFT methods like LoRA. Our studies in 6.3 an-alyze the effects of shared and non-shared parame-ters in the model on specialized and general perfor-mance, pointing out the priority to selectively trainnon-shared parameters in ESFT. Through ablationstudies in 6.4, we highlight the importance of ourexpert relevance scores and the fine-grained expertsegmentation architecture.",
  "Parameter-efficient fine-tuning for densearchitectural LLMs": "The goal of parameter-efficient fine-tuning (Hanet al., 2024) is to efficiently customize LLMs fordownstream tasks, while existing studies primarilyfocus on dense architectural LLMs. PEFT meth-ods for dense models can generally be categorizedinto three approaches: (1) Adding new parame-ters: methods of this kind fix the existing modelparameters and fine-tune the model on a small num-ber of newly added parameters. Adapter (Houlsbyet al., 2019; Pfeiffer et al., 2020; He et al., 2021;Wang et al., 2022) and Soft Prompt (Li and Liang,2021; Liu et al., 2021; Zhang et al., 2023b; Lesteret al., 2021) are two typical representatives of thiscategory of methods. (2) Selecting existing pa-rameters: methods of this type fine-tune a lim-ited part of existing parameters, while keeping themajority of the other parameters fixed. Based onwhether the trainable parameter space is continu-ous, these methods can generally be divided intostructured training (Guo et al., 2020; Gheini et al.,2021; He et al., 2023; Vucetic et al., 2022) andunstructured training (Liao et al., 2023; Ansellet al., 2021; Sung et al., 2021; Xu et al., 2021).(3) Applying low-rank adaptation: LoRA (Huet al., 2021; Fomenko et al., 2024) is a widely-used PEFT method, which decomposes the originweight matrices into low-rank components. Sub-sequent works (Zhang et al., 2023a; Ding et al.,2023; Lin et al., 2024; Liu et al., 2023; Dou et al.,2024) have introduced numerous improvements tothe original LoRA method. However, the study ofPEFT in sparse models is still scarce. In this work,we select and tune part of the experts based on their",
  "Coarse- and Fine-grained MoE LLMs": "Compared to dense LLMs (e.g., LLaMA series,Meta, 2023b,a), MoE LLMs (e.g., Mixtral series,Mistral, 2024a,b) can increase model size whilesaving training and inference costs. Based on thegranularity of experts, existing large MoE mod-els can generally be divided into two categories:coarse- and fine-grained expert LLMs. Most exist-ing MoE LLMs (Lepikhin et al., 2021; Fedus et al.,2021; Roller et al., 2021; Dai et al., 2022; Shenet al., 2024) have coarse-grained experts where thenumber of experts is very limited. For example, 2out of 8 experts are activated for Mixtral MoE se-ries (Mistral, 2024a,b) and Grok-V1 (XAI, 2024).As a result, a single expert has to learn complicatedpatterns from different domain tasks simultane-ously. To address this issue, DeepSeek MoE (Daiet al., 2024) has introduced fine-grained expertsegmentation. In the DeepSeek-V2 (DeepSeek,2024), there are as many as 162 experts, with 8active experts (8 out of 66 experts are activated forthe DeepSeek-V2-Lite). The fine-grained divisionof experts ensures a high degree of specializationamong the experts. Moreover, the specialized ex-pert system enables the selection of experts thatare most relevant to the task for efficient tuning.",
  "Preliminaries: Mixture-of-Experts forTransformers": "Mixture-of-Experts (MoE) for Transformers re-place Feed-Forward Networks (FFNs) with MoElayers. Each MoE layer consists of multiple expertsstructurally identical to a FFN. Tokens are assignedto and processed by a subset of the most relevantexperts based on their affinity scores, ensuring com-putational efficiency in MoE layers. The outputhidden state hlt of the t-th token in the l-th MoElayer is computed as:",
  "Output": ": Comparison between Expert-Specialized Fine-Tuning (ESFT) and other fine-tuning methods. FFT trainsall parameters. LoRA combines pre-trained weights with low-rank matrices to reduce training costs. ESFT onlytrains a subset of experts in a Mixture-of-Expert (MoE) architecture, optimizing efficiency and task specialization. where N denotes the total number of experts,FFNi() is the i-th expert FFN, gi,t denotes the gatevalue for the i-th expert, si,t denotes the token-to-expert affinity, TopK(, K) denotes the set com-prising K highest affinity scores among those cal-culated for the t-th token and all N experts, and eliis the centroid of the i-th expert in the l-th layer.Recently, DeepSeekMoE (Dai et al., 2024)proposes enhancements to the MoE architecturethrough several techniques, including (1) Fine-grained segmentation, segmenting each expert intomultiple smaller ones and keeping the same frac-tion of experts to process each token, allowingspecialization in different knowledge types whilemaintaining the same computational cost.(2)Shared expert isolation, leveraging shared expertsthat process all tokens to capture common knowl-edge, reducing parameter redundancy and enhanc-ing efficiency. The output of an MoE layer inDeepSeekMoE is:",
  "Despite the significant success of MoE LLMs, aclear understanding of the underlying mechanism": "remains elusive. We conduct probing experimentsto understand how non-shared experts are utilizedacross various tasks. These tasks, as detailed in4.1, include general domains like math and code,as well as specialized domains like intent recog-nition, summarization, legal judgment prediction,and translation. These experiments reveal the ex-pert specialization in MoE models in two aspects: Expert Routing is Concentrated in the SameTaskWe investigate the distribution of normal-ized gate values, i.e., the sum of all expert-tokengate values for each expert, divided by the totalacross all experts. displays this distribu-tion, where the experts are sorted by their normal-ized values from high to low. The figure showsthat a small subset of experts handles the majorityof gate values, indicating the models and concen-trated expert allocation for a specific task. Active Experts Vary Significantly across TasksWe investigate the joint distribution of expertsacross tasks. shows a heatmap of theshared Top-6 experts for two independent data sam-ples per task averaged across layers. This indicatesthe degree of overlap of experts used within thesame task or between different tasks. Off-diagonalvalues are near 0, and diagonal values are near 6,indicating that the same task uses similar experts,while different tasks use different sets.",
  "Normalized Average Gate Value": "IntentSummaryLawTranslationMathCodeUniform : Top Expert distribution for specific tasks.Shaded areas represent variance across layers. Thefigure shows that few experts handle most gate values,highlighting expert specialization for different tasks. : The average number of shared Top-6 routedexperts across tasks. The values are averaged by layer,indicating that the sets of experts used for the same taskare consistent while different tasks are distinct.",
  "tional efficiency and maintain expert specialization. illustrates the differences between ourmethod and existing methods. Below, we intro-duce our method step by step": "Data SamplingWe randomly sample a subsetDs = {(xi, yi)}Nsi=1 from the training data D ={(xi, yi)}Ni=1 for expert selection, where xi and yidenote the input and label, respectively. Empir-ically, we find that a subset of 32 concatenatedsamples, each with a fixed length of L = 4096, isrobust enough to select the most relevant expertsfor a task. We detail this claim in Appendix C. Expert Relevance ScoreWe propose two meth-ods to calculate the relevance of an expert to a taskbased on its affinity to the sample tokens, definedas average gate score and token selection ratio,respectively. Both methods assess each expertsrelevance to downstream tasks and can be chosenbased on task-specific experimental performance.",
  "the gate score gli,k is positive, and 0 otherwise. Kis the number of experts selected per token": "Expert Selection and Fine-tuningFor eachMoE layer l, we select a subset of experts to be fine-tuned based on their relevance scores. We definea threshold p (0, 1] as a hyperparameter con-trolling the proportion of total relevance scores tobe included in the selected subset. For each layerl, we select a minimal set of top-scored expertsEls whose cumulative relevance score exceeds thethreshold p, satisfying:",
  "Tasks for Model EnhancementWe choose two domain-specific tasks, i.e., Mathand Code, to evaluate how our method can enhance": "the models existing abilities. The two domains arewidely concerned in current LLM research andsuitable for evaluation, as many pre-trained mod-els can perform decently, while there is significantpotential for improvement through further train-ing. We assess our methods effectiveness throughperformance gains.For the Math domain, we use MetaMathQA (Yuet al., 2023) for training and use GSM8K (Cobbeet al., 2021) and MATH (Hendrycks et al., 2021a)for evaluation. For the Code domain, We train themodel on the Python subset of the enormous evol-codealpaca dataset (Luo et al., 2023) to simulatea more concentrated LLM customization scenario,and assess its performance on HumanEval (Chenet al., 2021) and MBPP (Austin et al., 2021). 4.1.2Tasks for Model AdaptationWe select four specialized tasks to evaluate howour method can facilitate language models to adaptto an unfamiliar downstream task, covering a di-verse range of abilities that most models can excelat after training but not without training: (1) Text-to-JSON Intent Recognition in the BDCI-21 SmartHCI NLU Challenge1, which requires convertingtext instructions into JSON format for home ap-pliances. (2) Text Summarization in the BDCI-21 Summarization Challenge2, which summarizescustomer service call transcripts. (3) Legal judg-ment Prediction in the the BDCI-21 Law EventPrediction Challenge3, where the case descriptionand judgment are repurposed as a legal judgmentprediction task. (4) Low-resource Translation inthe ChrEn dataset (Zhang et al., 2020), translatingthe minority Cherokee to English. Examples of thetasks are shown in Appendix A.To measure model performance, for the text-to-JSON task, we calculate the exact match betweenmodel output and reference answer; for other tasks,we employ GPT-4 to score model output between0 and 10 given reference answer4. All evaluationsuse few-shot examples.",
  "Backbone Model and Training Settings": "We use the backbone architecture of DeepSeek-V2-Lite (DeepSeek, 2024) for all experiments. Themodel includes a fine-grained set of 66 experts foreach transformer layer. This makes it uniquely suit-able at the time of this study for our method, whichbenefits from expert specialization. We train themodel on a carefully curated alignment dataset thatexcludes math and code data and take the result-ing checkpoint as our vanilla model for subsequentexperiments. This alignment phase can activatemodel ability across various domains while keep-ing Math/Code ability as elementary to better ver-ify the performance gains of our method in thesetwo fields.We adopt two baselines: Full-Parameter Fine-Tuning (FFT) and Low-Rank Adaptation (LoRA,Hu et al., 2021). For LoRA, we add low-rank ma-trices to all parameters for training except tokenembeddings and the language modeling head. Wemaintain a 1:1 ratio for task-specific data and align-ment data for all methods, which we find is highlyeffective in preserving general abilities obtainedfrom the alignment phase for FFT and LoRA. How-ever, for our ESFT method, not adopting this datamixing strategy may even better maintain generalability. We detail this in Appendix F. All experi-ments are done on the HFAI cluster5 with 2 nodesof 8x Nvidia A100 PCIe GPUs.For hyperparameter settings, all methods use abatch size of 32 and a sequence length of 4096 fortraining. For every task, we set the maximum stepsof training to 500, and evaluate the model every100 steps. The learning rates are set to 3e-5, 1e-4,and 1e-5 for FFT, LoRA, and ESFT, respectively,based on a hyperparameter search in {1e-5, 3e-5, 1e-4, 3e-4}. The LoRA rank is set to 8 andscaling is set to 2, following Hu et al. (2021). Thethreshold p is set to 0.1 for ESFT-Gate and 0.2for ESFT-Token, respectively. 6.2 shows how wedetermine the threshold for ESFT.",
  "Vanilla LM81.567.742.557.559.974.053.762.4": "FFT76.8 1.762.4 1028.4 5.155.5 1.158.4 0.474.6 3.253.6 3.158.5 2.5+ mix data4.13.55.80.00.4-6.7-5.20.3LoRA60.2 2761.2 4.033.4 6.152.3 3.355.3 2.371.5 2.550.7 2.255.0 4.6+ mix data14.12.25.33.21.71.31.14.1ESFT-Token80.0 2.567.5 0.341.9 0.857.3 0.260.2 0.574.5 0.754.9 0.762.3 0.5+ mix data0.9-0.8-1.2-0.2-0.6-2.2-2.0-0.8ESFT-Gate80.2 1.667.6 0.340.8 2.457.3 0.359.9 0.474.3 0.955.1 0.962.2 0.5+ mix data1.2-1.1-0.6-0.3-0.4-6.1-3.6-1.6 : General task performance comparison across methods and tasks with and without alignment data mixing.Results show that mixing alignment data improves FFT and LoRA in general tasks, but not our ESFT method. Itshowcases that ESFT can adapt to downstream tasks directly with minimal performance loss in general tasks.",
  "Benchmark Performance Results": "The results in and demonstrate sev-eral conclusions. All methods can improve modelperformance in customization tasks compared tothe vanilla model, while they may cause a perfor-mance decrease in general tasks. Generally, theperformance increase is higher in model adapta-tion tasks than in model enhancement tasks.For customization ability evaluation, ESFT sur-passes LoRA significantly and is competitive withFFT. As shown in , ESFT-Token and ESFT-Gate achieve near-best results in model enhance-ment tasks like Math, and ESFT-Gate achieves thebest performance in the Humaneval task. ESFTalso excels in model adaptation tasks, with ESFT-Gate achieving near-best performance in 3 tasksout of 4. Notably, ESFT-Gates average of 50.2is competitive compared to FFTs 51.0, slightlybetter than ESFT-Tokens 49.4, and significantlysurpasses LoRAs 44.9. This demonstrates thatfinding task-relevant experts can efficiently adaptthe model for efficient customization.For general ability evaluation, ESFT consis-tently outperforms FFT and LoRA by showingless performance degradation. As illustrated in Ta-ble 2, ESFT-token performs better than ESFT-gate,with average scores of 61.5 and 60.6, respectively.The results demonstrate a wide range of retention in tasks such as TriviaQA and IFEval, surpassingFFTs 58.8 and LoRAs 59.1. Both methods retainperformance better than LoRA and FFT, highlight-ing their effectiveness in maintaining general taskperformance6. Analyses in 6.3 indicate that suchdegradation on general tasks for FFT and LoRAmay result from training shared parameters.",
  "The results in demonstrates that ESFTexhibits several advantages in terms of trainingtime and storage space requirements:": "Training TimeThe average training time forESFT-Token and ESFT-Gate is 19.8 minutes and20.9 minutes, respectively. The FFT method takessignificantly longer at 28.5 minutes. AlthoughLoRA achieves a shorter training time of 16.5 min-utes, our methods are relatively close. Storage SpaceThe average storage space of pa-rameters trained is 2.57 GB for ESFT-Token and3.20 GB for ESFT-Gate, while FFT demands asubstantial 28.6 GB. Although LoRA requires lessstorage, ESFT performs significantly better thanLoRA in downstream task performance. 6We further investigate Math and Code performance ofthe models trained on specialized tasks in Appendix H. FFTand LoRA exhibit even more severe degradation, while ESFTshows a minimal performance drop. : Number of experts trained in ESFT across layers and tasks. Earlier computed layers are numbered smaller.Most tasks and layers train 5-15% of experts, demonstrating ESFTs effectiveness in selecting task-related experts.",
  "Analysis": "In this section, we investigate the expert selectionprocess of ESFT in 6.1, and demonstrate the per-formance of ESFT and LoRA under different com-putational constraints in 6.2. We analyze the ef-fects of training shared and non-shared parametersin 6.3, and conduct ablation studies in 6.4 to ver-ify the importance of our expert relevance scoresand model structure of fine-grained experts.",
  "ESFT Leverages Specialized ExpertsEffectively": "We analyze the number of experts ESFT trainsacross tasks and layers to understand its expertselection process. Results are shown in .From the results, we have several observations:(1) The average number of experts used per taskacross layers ranges from 2 to 15 out of 66, indi-cating ESFT can have 75%-95% fewer trainableparameters than FFT. (2) ESFT-Token generallyemploys fewer experts while better maintaininggeneral performance, comparable to ESFT-Gate intasks like Math, Intent, and Law. (3) The numberof experts varies by task, with more specializedtasks like Math and Translation using fewer ex-perts; our methods performances for these tasksexceed LoRA to the greatest extent, indicating thatour method is especially suitable for more special-ized tasks. (4) For most tasks, few experts arechosen in the middle layers, indicating that expertdistribution is more concentrated in these layers.",
  "ESFT Leverages Training ResourcesEfficiently": "Both ESFT and LoRA have a training efficiencyhyperparameter (p for ESFT and rank for LoRA).Increasing its value would raise computational re-source usage and potentially improve performance.To understand how ESFT and LoRA perform un-der different efficiency settings, we evaluate bench-mark performance on the Math task. We set rank 512 for LoRA as a higher value will result in moretrainable parameters than FFT. illustratesboth specialized and general ability under different",
  "-33.862.448.1": ": Comparisons of different model configs based on whether training shared or non-shared parameters.Results include trainable parameters and performance of specialized and general abilities. The best or near-bestresults excluding the non-training setting are shown in bold. : Comparison of three methods under different training efficiency settings on the Math task. The x-axisshows the average trainable experts per layer for ESFT and rank for LoRA, indicating the ratio of trained parameters.The y-axis represents specialized and general ability. Markers on the lines indicate p or rank values. ESFTconsistently outperforms LoRA in both specialized and general ability. training efficiency settings.From the results, we can conclude: (1) All threemethods show a trade-off between training effi-ciency and performance. Increasing trained param-eters (p for ESFT and rank for LoRA) before acertain point can improve performance. (2) BothESFT-Token and ESFT-Gate outperform LoRA atany point, demonstrating higher specialized abilityand more stable general ability. (3) ESFT-Tokenpeaks in both specialized and general ability atp=0.5, while ESFT-Gate peaks at p=0.3 for spe-cialized and p=0.1 for general ability. (4) ESFT-Token and ESFT-Gate performance saturates atp=0.2 and p=0.1, respectively, indicating that mostexpert choices may be less relevant to task perfor-mance. We delve deeper into this in Appendix E.",
  "Selectively Training Non-SharedParameters is the Key to ESFT": "In our proposed ESFT method, we only fine-tunea subset of non-shared experts. This section pro-vides detailed discussions of several variants of ourmethod that may also train shared parameters. Thevariables are based on: (1) whether training non-shared experts or a task-relevant subset of them (we use the Token Selection Ratio and set p=0.2);(2) whether training shared experts; (3) whethertraining other shared parameters including gates,attention layers, and embeddings.The results are shown in . We reportaverage trainable parameters across all tasks, per-formance of specialized and general abilities, andtheir average. Detailed numbers for all benchmarksare shown in Appendix D. From the results, we candraw several conclusions:Specialized performance increases as train-able parameters increase. The rank of trainableparameters from 450M to 15.7B highly aligns withthe rank of specialized ability from 47.4 to 51.0.This suggests that increasing trainable parametersis effective in enhancing specialized performance.General performance decreases as trainableshared parameters increase. Whether relevantnon-shared experts are trained or not, general per-formance decreases from 61.5 to 60.3, or from 62.4to 60.0, respectively, as we train shared expertsand/or non-expert parameters. As the completeset of non-shared experts is trained, general perfor-mance decreases further from 60.3 to 58.8. Thissuggests that training shared parameters is more",
  ": Experiment results for grouped experts. Asthe experts become more coarse-grained, ESFT de-grades more severely than FFT": "likely to cause overfitting and forgetting on generaltasks compared to training non-shared parameters.It is highly prioritized to train task-relevantnon-shared experts. Training relevant expertsachieves at least 55.3, while other settings achieveat most 54.9, even with higher demands of up to15.7B parameters. Therefore, fine-tuning these ex-perts is highly prioritized for model customization.We propose two major training strategies basedon these conclusions:",
  "Expert Relevance Score FunctionIn this work,we propose Average Gate Score and Token Selec-tion Ratio as expert relevance score functions to": "filter relevant experts for different tasks. To demon-strate their effectiveness, we replace the expertsobtained from these functions with random expertswhile keeping the number of activated experts perlayer the same. Results in show that replac-ing relevant experts with random ones significantlydecreases task performance, demonstrating the ef-fectiveness of our scoring function. Fine-Grained Expert Segmentation of theMoE ModelWe use the fine-grained segmentedDeepSeek-V2 model as our backbone. To demon-strate t the effectiveness of this fine-grained seg-mentation, we use greedy search (as detailed inAppendix B) to group experts, simulating coarse-grained segmentation. Experts in the same groupshare the average affinity score. We maintain thecomputational cost by selecting a constant 1/8 ofexperts for each token.Experiment results ofthe Math domain in show that as thegroup size increases, our methods performance de-creases more severely than FFT, while the trainingcost (i.e., trainable experts) rises. These findingsindicate that our method, and even effective LLMcustomization, highly rely on a fine-grained MoELLM architecture with more specialized experts.",
  "Conclusion": "In this work, we study parameter-efficient fine-tuning methods for sparse large language modelswith the Mixture of Experts (MoE) architecture.We first observe that tasks from different domainsare handled by distinct combinations of experts.We then propose selecting the most relevant expertsfor downstream tasks using two metrics: averagegate score and token selection ratio. Experimentalresults show that our method significantly reducestraining costs while matching or surpassing fullparameter fine-tuning results. Further analysis con-firms that our method enhances the specializationof the expert system within the MoE architecture.",
  "Acknowledgement and Limitations": "We would like to thank Xingkai Yu for helpingto organize the ESFT open-source training code.Due to the limitation of the availability of otherfine-grained MoE models, our method was onlytested on the DeepSeek-V2-Lite MoE model. Theconclusions drawn from this model require furthervalidation when applied to other contexts. Besides,due to the lack of parameter-wise and structurallyaligned MoE models with different expert granu-larities, we used a simulation approach by bind-ing several groups of experts to compare coarse-grained and fine-grained MoE methods.",
  "Alan Ansell, Edoardo Maria Ponti, Anna Korhonen,and Ivan Vulic. 2021.Composable sparse fine-tuning for cross-lingual transfer.arXiv preprintarXiv:2110.07560": "Jacob Austin, Augustus Odena, Maxwell Nye, MaartenBosma, Henryk Michalewski, David Dohan, EllenJiang, Trevor Cai, Anselm Levskaya, Charles Sutton,et al. 2021. Program synthesis with large languagemodels. arXiv preprint arXiv:2108.07732. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,Maarten Dehghani, Pieter Abbeel, Deepak Pathak,Brandon Sanders, Vishal Katarkar, Zareen Xu, et al.2021. Evaluating large language models trained oncode. In NeurIPS. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,Ashish Sabharwal, Carissa Schoenick, and OyvindTafjord. 2018. Think you have solved question an-swering? try arc, the AI2 reasoning challenge. CoRR,abs/1803.05457.",
  "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,Jacob Hilton, Reiichiro Nakano, Christopher Hesse,and John Schulman. 2021. Gsm8k: A dataset forgrade school math problem solving. In NeurIPS": "Damai Dai, Chengqi Deng, Chenggang Zhao, R. X.Xu, Huazuo Gao, Deli Chen, Jiashi Li, WangdingZeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li,Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui,and Wenfeng Liang. 2024. Deepseekmoe: Towardsultimate expert specialization in mixture-of-expertslanguage models. CoRR, abs/2401.06066. Damai Dai, Li Dong, Shuming Ma, Bo Zheng, ZhifangSui, Baobao Chang, and Furu Wei. 2022. Stable-moe: Stable routing strategy for mixture of experts.In Proceedings of the 60th Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), ACL 2022, Dublin, Ireland, May22-27, 2022, pages 70857095. Association for Com-putational Linguistics.",
  "Yang Lin, Xinyu Ma, Xu Chu, Yujie Jin, Zhibang Yang,Yasha Wang, and Hong Mei. 2024. Lora dropout asa sparsity regularizer for overfitting control. arXivpreprint arXiv:2404.09610": "Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu,Derong Xu, Feng Tian, and Yefeng Zheng. 2023.Moelora: An moe-based parameter efficient fine-tuning method for multi-task medical applications.arXiv preprint arXiv:2310.18339. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam,Zhengxiao Du, Zhilin Yang, and Jie Tang. 2021. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXivpreprint arXiv:2110.07602.",
  "Yi-Lin Sung, Varun Nair, and Colin A Raffel. 2021.Training neural networks with fixed sparse masks.Advances in Neural Information Processing Systems,34:2419324205": "Danilo Vucetic, Mohammadreza Tayaranian, MaryamZiaeefard, James J Clark, Brett H Meyer, and War-ren J Gross. 2022. Efficient fine-tuning of bert mod-els on the edge. In 2022 IEEE International Sympo-sium on Circuits and Systems (ISCAS), pages 18381842. IEEE. Yaqing Wang, Subhabrata Mukherjee, Xiaodong Liu,Jing Gao, Ahmed Hassan Awadallah, and Jian-feng Gao. 2022. Adamix: Mixture-of-adapter forparameter-efficient tuning of large language models.arXiv preprint arXiv:2205.12410, 1(2):4.",
  "Liang Xu, Hai Hu, Xuanwei Zhang, et al. 2020. Clue:A chinese language understanding evaluation bench-mark. arXiv preprint arXiv:2004.05986": "Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan,Baobao Chang, Songfang Huang, and Fei Huang.2021. Raise a child in large language model: To-wards effective and generalizable fine-tuning. arXivpreprint arXiv:2109.05687. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu,Zhengying Liu, Yu Zhang, James T Kwok, Zhen-guo Li, Adrian Weller, and Weiyang Liu. 2023.Metamath: Bootstrap your own mathematical ques-tions for large language models.arXiv preprintarXiv:2309.12284. Rowan Zellers, Ari Holtzman, Yonatan Bisk, AliFarhadi, and Yejin Choi. 2019. HellaSwag: Can amachine really finish your sentence? In Proceedingsof the 57th Conference of the Association for Com-putational Linguistics, ACL 2019, Florence, Italy,July 28- August 2, 2019, Volume 1: Long Papers,pages 47914800. Association for ComputationalLinguistics.",
  "BStrategy for Grouping Experts": "To group experts together and simulate coarse-grained mixture-of-experts transformer models, wecalculate expert similarity and group the experts bymaximizing in-group similarities using a greedysearch algorithm.We sample data from the alignment dataset, con-taining 32 samples each with a sequence length of4096, to calculate the similarity between experts.We initialize a co-occurrence matrix for all expertpairs as a zero matrix. For each pair of expertsthat occur simultaneously in a tokens Top-6 ex-pert choices, we increment their score by 1 in thematrix. After iterating through the dataset, we cal-culate the similarity between each pair of expertsi and expert j using the cosine similarity betweenthe vectors of row i and row j in the matrix.To obtain an expert grouping strategy throughgreedy search, we calculate the average intra-groupsimilarity (the average pairwise similarity of all ex-perts within the group) for all possible K-expertgroups (where K is the group size, either 2 or 4)from the 64 non-shared experts out of the 66 ex-perts in each layer. We then select the K-expertgroup with the highest score. For the remainingunselected experts, we repeat this process until allexperts are selected and grouped.",
  "CAnalysis of Expert Affinity Sample Size": "To evaluate the amount of data needed to identifythe most relevant experts for a task, we indepen-dently sample two sets of data from the training setfor each of the six tasks and calculate the sharedTop-6 experts between the two sets. The resultsare shown in . As the sample size reaches217 (i.e., 32 samples with a sequence length of4096), all tasks exhibit a high number of sharedexperts between the two samples. This indicatesthat the sample size is sufficiently large to selectthe top-relevant experts for the tasks.",
  "DDetailed Results for Ablations onTraining Shared Parameters": "We present two tables that summarize the perfor-mance of various methods with different configura-tions for training shared or non-shared parameters. shows results on general tasks, and focuses on specialized tasks. The results indicatethat training only task-relevant non-shared expertsconsistently maintains the best general task perfor-mance. Additionally, training task-relevant non-shared experts and all shared parameters yields thebest specialized task performance, short of full-parameter fine-tuning.",
  "EQualitative Examples of the ExpertChoices": "We present qualitative examples of the amount thatrouted experts are trainable among all tokens foreach task in . Each subfigure demonstratesexamples drawn from a task. Deeper tokens in-dicate more trainable experts across all 26 layers(top-6 experts per layer). The parameter p is setto 0.2 for the token selection ratio. Results showthat our method, even handling only about 20% ofexpert choices, covers a wide range of key task-relevant words.For example, in the Intent recognition task, thedeepest tokens are (Intent); in the legaljudgment task, the deepest tokens include (Post-marriage), (request), (plain-tiff) and (defendant); in the Math task, thedeepest tokens are mainly numerical tokens suchas 3, 5, 6 and 7; in the Code task, the deep-",
  "FThe Impact of Mixing Alignment Datafor Training": "We adopt a 1:1 ratio for downstream task data andalignment data for all methods during training tobetter maintain general task performance. Thismanual ratio is kept constant to avoid the signif-icant additional costs associated with fine-tuningthe ratio for each task.In this section, we present performance compar-isons across various methods and tasks to reveal theimpact of mixing alignment data during training. presents the performance on downstreamspecialized tasks, and shows the perfor-mance on general tasks.The results indicate that FFT and LoRA benefitfrom the inclusion of alignment data, leading to improved performance in general tasks while onlyslightly decreasing performance in downstreamtasks. Conversely, our ESFT method does notexhibit the same advantage. Specifically, mixingalignment data does not result in performance in-creases in either general or downstream tasks. Thefindings suggest that ESFT is inherently capable ofadapting to downstream tasks without significantperformance degradation in general tasks, evenwithout added alignment data. This highlights therobustness and adaptability of ESFT in diverse tasksettings.",
  "HEvaluating Math and Code as GeneralTasks": "We investigate the Math and Code performanceof models trained on adaptation tasks (i.e., Intent,Summary, Law, Translation), as these domains re-flect the models general ability if not specificallytrained on them. We report numbers with the set-ting of training on only downstream task data. Re-sults in show that FFT and LoRA wouldlead to significant performance drops in the Mathand Code domain, having average performancedrops of 9.0 and 12.4, respectively. Notably, ourESFT method retains performance significantlybetter compared to FFT and LoRA, with an aver-age performance drop of less than 1.0.",
  "Law10100xx...x\" 1. 10 2. 3. 4. {prediction} {ground_truth}": "TranslationYou are an expert master in machine translation. Please score the predicted answer against the standardanswer out of 10 points based on the following criteria: Content accuracy: Does the predicted answeraccurately reflect the key points of the reference answer? Level of detail/completeness: Does thepredicted answer cover all important points from the standard answer? Content redundancy: Is thepredicted answer concise and consistent with the style of the standard answer? Respond following theformat: \"Content accuracy x points, level of detail/completeness x points, ..., total score: x points\".The total score is the average of all the scores. Do not give reasons for your scores. Predicted answer:{prediction} Reference answer: {ground_truth}",
  "(f) Code domain": ": Examples for our ESFT method showing the proportion of trainable routed experts among all tokensfor each task. Deeper tokens indicate more trainable experts across all 26 layers (top-6 experts per layer). Theparameter p is set to 0.2 for the token selection ratio. Results show that our method, even handling only about 20%of expert choices, covers a wide range of key task-relevant words."
}