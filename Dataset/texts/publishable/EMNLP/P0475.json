{
  "Abstract": "Legal case retrieval for sourcing similar casesis critical in upholding judicial fairness. Dif-ferent from general web search, legal case re-trieval involves processing lengthy, complex,and highly specialized legal documents. Ex-isting methods in this domain often overlookthe incorporation of legal expert knowledge,which is crucial for accurately understandingand modeling legal cases, leading to unsatis-factory retrieval performance. This paper in-troduces KELLER, a legal knowledge-guidedcase reformulation approach based on large lan-guage models (LLMs) for effective and inter-pretable legal case retrieval. By incorporatingprofessional legal knowledge about crimes andlaw articles, we enable large language modelsto accurately reformulate the original legal caseinto concise sub-facts of crimes, which containthe essential information of the case. Exten-sive experiments on two legal case retrievalbenchmarks demonstrate superior retrieval per-formance and robustness on complex legal casequeries of KELLER over existing methods.",
  "Introduction": "Legal case retrieval is vital for legal experts tomake informed decisions by thoroughly analyz-ing relevant precedents, which upholds justice andfairness (Hamann, 2019). This practice is crucialin both common law and civil law systems glob-ally (Lastres, 2015; Harris, 2002). In civil law,although following past cases (known as \"stare de-cisis\") is not mandatory, judges are still highly ad-vised to consider previous cases to improve theaccuracy and trustworthiness of their judgments.In legal case retrieval, both the query and thedocument are structured legal cases, distinguish-ing the task from other information retrieval (IR)tasks. Specifically, as shown in , a legalcase document comprises several sections, such as",
  "*Corresponding author": "Criminal Judgment of the Peoples Court of Tieli City, Heilongjiang Province (2019)...The Peoples Procuratorate of Tieli City charged that ... The defendants actions violated Article 114 of the Criminal Law of the Peoples Republic of China. The defendant should be held criminally responsible for the crime of arson. The defendant Yan, has no objections to the criminal facts and charges brought by the public prosecution and offers no defense.",
  "Procedure": "After trial and investigation, it has been established that Mu is the paternal uncle of the defendant Yan. The two parties had developed conflicts over inheritance issues, and prior to the incident, they had a quarrel over a trivial matter. In a bid to vent personal spite, Yan took advantage of Mu's absence and set fire to Mu's house...",
  "Fact": "The Court finds that the defendant Yan intentionally set fire to and destroyed a house, endangering public safety. Such conduct constitutes the crime of arson... In accordance with Article 114 and Paragraph 1 of Article 67 of the Criminal Law of the People's Republic of China, the judgment is as follows:",
  "Document Case": ": The query case and candidate document caseexamples. The query case typically contains only partialcontent since it has not been adjudicated. Extractablecrimes and law articles are highlighted in red. procedure, facts, and the courts decision, makingit much longer than typical queries and passagesin the standard ad-hoc search tasks. Its averagetext length often exceeds the maximum input limitsof popular retrievers, such as 512 tokens (Devlinet al., 2019). Moreover, a legal case may encom-pass multiple, distinct criminal behaviors (Denget al., 2024b). Comprehensively considering allcriminal behaviors of a legal case is important indetermining its matching relevance with a querycase. However, these key criminal descriptions areusually dispersed throughout the lengthy contents,which can significantly affect the effectiveness oftraditional long document modeling strategies likeFirstP and MaxP (Dai and Callan, 2019) in the legal domain.To tackle the challenge of comprehending longand complex legal cases, previous works mainlyfall into two categories. The first approach focuseson expanding the context window size (Xiao et al.,2021) or splitting legal cases into passages (Shaoet al., 2020). However, given the specialized andcomplex nature of legal texts, merely increasing thecontext window size still proves insufficient for sig-nificantly improving the retrieval performance. Incontrast, the second approach performs direct textsummarization (Askari and Verberne, 2021; Tanget al., 2023) or embedding-level summarization (Yuet al., 2022) on the legal case, aiming to only keepthe most crucial information for assessing the rele-vance between legal cases. However, they typicallyonly rely on heuristic rules or the models inher-ent knowledge for summarization. As the legaldomain is highly specialized, existing approachesthat overlook professional legal knowledge (e.g.,law articles) are likely to perform inaccurate sum-marization.In this paper, we present a Knowledge-guidEdcase reformuLation approach for LEgal case Re-trieval, named KELLER. Our main idea is to lever-age professional legal knowledge to guide largelanguage models (LLMs) to summarize the corre-sponding key sub-facts for the crimes of the legalcases, and then directly learn to model case rele-vance based on these crucial and concise sub-facts.Due to the specialization and complexity of thelegal case, it is quite challenging to directly sum-marize the corresponding key sub-facts for all thecrimes from the legal case, even using advancedLLMs (Tang et al., 2023). To address this problem,we propose a two-step legal knowledge-guidedprompting method, as illustrated in the left sideof . In the initial step, we prompt LLM toextract all of the crimes and law articles containedin the legal case and then perform post-processingon them to establish correct mappings betweenthe crimes and law articles by referring to the le-gal expert database. In the next step, we promptLLM with the extracted crime-article pairs tosummarize the sub-fact of the crime from the le-gal case. The intermediate law articles, servingas high-level abstractions of the actual criminalevents, can largely reduce the difficulty of identi-fying the corresponding sub-fact for the crime andimprove accuracy. shows an example ofthree summarized sub-facts from a legal case.Then, we directly model the case relevance based on these sub-facts because they are not onlythe most crucial information for relevance judg-ment in legal case retrieval but are also conciseenough to meet the text length limitations of popu-lar pre-trained retrieval models. For the comprehen-sive consideration of effectiveness, efficiency, andinteroperability, we adopt the simple MaxSim andSum operators to aggregate the relevance scoresbetween query and document sub-facts to get the fi-nal case relevance score. The model is trained withdual-level contrastive learning to comprehensivelycapture the matching signals at the case level andthe sub-fact level. On two widely-used datasets, weshow that KELLER achieves new state-of-the-artresults in both zero-shot and fine-tuning settings.Remarkably, KELLER demonstrates substantialimprovements in handling complex queries.Our main contributions can be summarized as:(1) We propose to leverage professional legalknowledge about crimes and law articles to equipLLM with much-improved capabilities for summa-rizing essential sub-facts from complex cases.(2) We suggest performing simple MaxSim andSum aggregation directly on those refined sub-factsto achieve effective and interpretable legal retrieval.(3) We introduce dual-level contrastive learningthat enables the model to capture multi-granularitymatching signals from both case-level and sub-fact-level for enhanced retrieval performance.",
  "Related Work": "Legal case retrieval. Existing legal case retrievalmethods are categorized into statistical and neuralmodels. Statistical models, notably the BM25 algo-rithm, can be enhanced by incorporating legal ex-pert knowledge such as legal summarization (Tranet al., 2020; Askari and Verberne, 2021), issue ele-ments (Zeng et al., 2005) and ontology (Saravananet al., 2009). Neural models have been advancedthrough deep learning and the use of pre-trainedlanguage models (Devlin et al., 2019; Zhong et al.,2019; Chalkidis et al., 2020; Zhang et al., 2023).Recent advancements in this domain include thedesign of specialized pre-training tasks tailored forlegal case retrieval, which yields remarkable im-provements in retrieval metrics (Li et al., 2023a;Ma et al., 2023b; Deng et al., 2024a).Due to the limitations of neural models inhandling long texts, researchers mainly focus onprocessing lengthy legal documents by isolatingthe \"fact description\" section and truncating it to fit the models input constraints (Ma et al.,2021; Yao et al., 2022; Ma et al., 2023b; Li et al.,2023a). To overcome the long-text problem, someother strategies include segmenting texts intoparagraphs for interaction modeling (Shao et al.,2020), employing architectures like Longformerfor extensive pre-training on legal texts (Xiao et al.,2021), and transforming token-level inputs intosentence-level encoding (Yu et al., 2022). Query rewriting with LLMs.Recently, re-searchers naturally employ LLMs to enhance theeffectiveness of query rewriting and intent un-derstanding (Zhu et al., 2023; Mao et al., 2023;Ma et al., 2023a; Wang et al., 2023; Jagermanet al., 2023; Mao et al., 2024).For instance,HyDE (Gao et al., 2023) creates pseudo passagesfor better query answers, integrating them intoa vector for retrieval, while Query2Doc (Wanget al., 2023) employs few-shot methods to gen-erate precise responses. Furthermore, Jagermanet al. (2023) explores LLMs reasoning capacitiesto develop \"Chain-of-Thoughts\" responses for com-plex queries. However, the above methods strugglewith legal case retrieval, where both queries anddocuments are lengthy cases. In the legal domain,PromptCase (Tang et al., 2023) attempts to addressthis by summarizing case facts within 50 words,but this approach often misses important details asmany cases feature multiple independent facts.",
  "Preliminaries": "In legal case retrieval, both queries and candidatedocuments are real structured legal cases that canextend to thousands of tokens in length. shows an illustration of the typical case structure.Specifically, a case usually contains several sec-tions, including procedure, fact, reasoning, deci-sion, and tail. Notably, the candidate documentsare completed legal cases that have been throughthe adjudication process and therefore contain allsections. In contrast, the query cases are not yetadjudicated, so they usually only include the proce-dure and fact sections. Formally, given a query case q and a set of docu-ment cases D, the objective of legal case retrieval isto calculate a relevance score s between the querycase and each document case in D, and then rankthe document cases accordingly.",
  "Knowledge-Guided Case Reformulation": "When assessing the relevance between two legalcases, the key facts of their crimes are the mostcrucial things for consideration. Therefore, giventhe complexity of the original legal cases whichmakes direct learning challenging, we try to firstrefine the legal cases into shorter but more essentialcrime-fact snippets. For example, we can get sucha snippet from the case shown in , whosecrime is the crime of arson and the fact is Yantook advantage of Mus absence and set fire ....However, the description of a crime and itscorresponding facts are often scattered throughoutthe lengthy case, and a single case may containmultiple crimes and facts, significantly com-plicating the extraction process. To tackle thisproblem, we propose a two-step prompting methodleveraging professional legal knowledge to guideLLM to achieve accurate extraction. Crime and law article extraction.First, weprompt LLM to extract all crimes and all lawarticles from the case.This step is relativelystraightforward for LLM, as each crime and lawarticle is a distinct, identifiable element within thetext. For example, the extracted crime and lawarticle for the case shown in are thecrime of arson and Article 114 and Paragraph 1of Article 67 of the Criminal Law of the PeoplesRepublic of China, respectively. Our extractionprompt is shown in Appendix B. Post-Processing. The extracted law articles mayjust be the titles. We then expand these titles intofull articles by gathering their detailed provisioncontent from the Web based on the titles. Then,we establish a mapping between each crime andits relevant law articles by referring to a databasebuilt by our legal experts. Note that the correlationbetween specific crimes and their correspondinglegal articles is objective, as it is clearly defined bylaw. After post-processing, we can obtain all thecrime-articles pairs for a legal case.",
  "Annotation": ": Overview of KELLER. We first perform legal knowledge-guided prompting to reformulate the legal casesinto a series of crucial and concise sub-facts. Then, we directly model the case relevance based on the sub-facts. Themodel is trained at both the coarse-grained case level and the fine-grained sub-fact level via contrastive learning. guide LLM in summarizing the specific facts ofeach crime from the original legal case. The lawarticles, serving as high-level abstractions of theactual criminal events, can considerably simplifythe task of identifying the corresponding specificfacts. The prompt for fact summarization is shownin Appendix B.2. Through our legal knowledge-guided reformu-lation, we can accurately distill a series of crimesand their corresponding specific facts from the orig-inally lengthy legal cases. Finally, we form a sub-fact snippet, with the crime as the title and its factsas the main body. These refined sub-facts are notonly the most crucial information for relevancejudgment in legal case retrieval but are also con-cise enough to meet the text length limitations ofpopular pre-trained retrieval models. Please notethat, since the required legal knowledge is presentin criminal case documents from mainstream coun-tries (e.g., China and the United States), our ap-proach is actually internationally applicable. Ourmaterials in Appendix D further prove this.",
  "Relevance Modeling": "We directly model the relevance of legal cases us-ing the refined sub-facts, rather than relying on thefull text of the original legal cases. Specifically,given a query case q = {q1, ..., qm} and a candi-date case d = {d1, ..., dn}, where qi represents thei-th sub-fact of q and dj represents the j-th sub-fact of d. We utilize a pre-trained text encoder to",
  "Edj = Pool[CLS] (Encoder(dj)) ,(1)": "where Pool[CLS] means extracting the embeddingoutput at the [CLS] token position. Then, we com-pute the similarity matrix Mmn using the L2-norm dot product. Each element Mi,j of M is thesimilarity calculated between the normalized em-beddings of the i-th sub-fact in the reformulatedquery case and j-th sub-fact in the reformulateddocument case:",
  "i=1Maxnj=1Mi,j,(3)": "where sq,d is the final predicted relevance score.We choose these two operators because of theiradvantages in effectiveness, efficiency, and inter-pretability over the other aggregation approachesfor our scenario:(1) Effectiveness: Typically, each querys sub-fact qi matches one document sub-fact dj at most in practice, which is well-suited for MaxSim of apply-ing the Max operation across all documents sub-facts for a given querys sub-fact. For instance, con-sidering a query sub-fact about drug trafficking,and the document sub-facts about drug traffickingand the discovery of privately stored guns andammunition, only the drug trafficking sub-factof the document is relevant for providing matchingevidence. In contrast, using soft aggregation meth-ods (e.g., kernel pooling (Xiong et al., 2017)) mayintroduce additional noise in this scenario.(2) Efficiency: Maxsim and Sum operationson tensors are quite efficient for both re-rankingand large-scale top-k retrieval supported by multi-vector-based Approximate Nearest Neighbor algo-rithms (Khattab and Zaharia, 2020). This highefficiency is important for meeting the low-latencyrequirements of the practical use.(3) Interpretability: MaxSim provides clear in-terpretability by revealing the quantitative contribu-tion of each query and document sub-fact towardsthe final relevance score, which can aid in under-standing the ranking strategies and justifying theretrieval results. We further illustrate this advan-tage by studying a real case in .6.",
  "(4)where d+ is the positive document of the query qand each d is from the in-batch negatives. is atemperature parameter": "Sub-fact-level contrastive learning. At the sub-fact level, we incorporate intermediate relevancesignals among sub-facts to fine-grainedly enhancethe models effectiveness in understanding sub-facts content and their matching relationships. However, only the case-level relevance labels areavailable in the dataset. Naively considering all thesub-fact pairs between the query and the positivedocuments as positives and all the sub-fact pairs be-tween the query and the negative documents as neg-atives will introduce substantial false positive andnegative noise. To mitigate this issue, we proposea heuristic strategy to obtain high-quality relevancelabels for the querys sub-facts {q1, ..., qm}. Thecore idea of this strategy is to combine the case-level relevance and the charges of each sub-fact toaccurately identify true positive and negative sam-ples. We introduce the details of this strategy inAppendix C due to the space limitation.After getting the sub-fact level relevance labels,we also adopt the ranking loss function for sub-factlevel contrastive learning:",
  "Experimental Setup": "Dataset and evaluation metrics.We conductextensive experiments on two widely-used datasets:LeCaRD (Ma et al., 2021) and LeCaRDv2 (Liet al., 2023b), whose statistics are listed inAppendix A.1. Considering the limited number ofqueries in LeCaRD, we directly evaluate all thequeries of LeCaRD using the best model trainedon LeCaRDv2, thereby avoiding the need fordataset split. Following the previous studies (Liet al., 2023a,b), we regard label=3 in LeCaRD andlabel2 in LeCaRDv2 as positive. For the querywhose candidate documents are all annotated aspositive, we supplement the candidate pool bysampling 10 document cases from the top 100-150BM25 results. To exclude the effect of unlabeledpotential positives in the corpus, we rank thecandidate pools and adopt MAP, P@k (k=3), and",
  "NDCG@k (k=3, 5, 10) as our evaluation metrics": "Baselines.We compare KELLER against thefollowing baselines across three categories. Thefirst is traditional probabilistic models, includingTF-IDF and BM25. The second is ranking methodsbased on pre-trained language models, includingBERT (Devlin et al., 2019), RoBERTa (Liu et al.,2019), BGE (Xiao et al., 2023) and SAILER (Liet al., 2023a).The third is ranking methodsdesigned for handling long (legal) text, includingBERT-PLI (Shao et al., 2020), Lawformer (Xiaoet al., 2021), and PromptCase (Tang et al., 2023).",
  "Main Results": "The main results are as shown in and wehave the following observations:(1) KELLER outperforms all baseline meth-ods across all metrics on both datasets. Com-pared with previous methods tailored for thelong-text problem, KELLER employs knowledge-guided case reformulation to address the challengeof long-text comprehension. This demonstratesthe effectiveness of separating comprehension andmatching tasks in the domain of legal case retrieval.(2) After fine-tuning on legal case retrievaldatasets, the performance gap between general-purpose and retrieval-oriented PLMs becomesless distinct. This observation may stem from tworeasons. First, the scarcity of training data in thelegal case retrieval task can induce overfitting toannotation signals, which hampers the models gen-eralization capabilities. Second, Naive truncationof lengthy texts can make the models inputs losesufficient matching signals, leading to inconsisten-cies between relevance annotations and matchingevidence.(3) We observe that these long-text-orientedbaseline methods do not show significant ad-vantages. Despite BERT-PLI and Lawformer pro-cessing more text than other methods, their inputcapacity was still insufficient for the average lengthof legal cases. Handling both long-text processingand complex semantic understanding within oneretriever presents a significant challenge. To ad-dress this issue, our approach offloads a portion ofthe long-text comprehension task via knowledge-guided case reformulation and improves the rank-",
  "Zero-shot Evaluation": "Considering the inherent data scarcity problem inlegal case retrieval, we evaluate the zero-shot per-formance (i.e., without fine-tuning on the trainingset of LeCaRDv2) of models on LeCaRDv2.Results are shown in and we find thatKELLER consistently outperforms baselines inboth zero-shot and fine-tuning settings. Upon com-paring the performance of each method under zero-shot and fine-tuned settings, we observe that mostmethods benefit from fine-tuning except SAILER.Intuitively, models trained in a general domain ortask could be enhanced through fine-tuning. Inspecific domains, continued fine-tuning of modelsgenerally does not lead to a significant decreasein performance. We posit that the unexpected out-comes in the SAILER model primarily arise fromoverfitting the limited data used for fine-tuning,which impairs the generalization capabilities estab-lished in the pre-training phase.",
  "Ablation Study": "We design the following six ablations:(1)KGCRNS: We replace our Knowledge-GuidedCase Reformulation (KGCR) with a Naive Sum-marization (NS), which produces case summarieswithout hierarchical structure. We subsequently op-timize the dual encoders with this text as the input.(2)MS Mean: We replace MaxSim and Sum (MS)with Mean to capture the average relevance of eachsub-fact in the candidate cases to the query. (3)MS NC: We Naively Concatenate (NC) all thereformulated sub-facts into a text sequence and sub-sequently optimize the dual-encoders. (4) MS KP: We employ kernel pooling (Xiong et al., 2017)on the score matrix to capture relevance signals. (5)w/o sfCL: Training without the sub-fact-level con-trastive learning. (6) w/o SfCL: Training withoutthe case-level contrastive learning.Results are shown in and we can observe:(1) Every ablation strategy results in a decline inthe models performance, demonstrating the effec-tiveness of each module within KELLER. This out-come indicates that KELLERs architecture is bothcomprehensive and synergistic, with each modulecontributing to the models overall performance.(2) The replacement of the KGCR module ex-hibits the most significant impact on performance.This highlights the pivotal role of the KGCR mod-ule in KELLER. The KGCR module decomposes",
  "KELLER64.1757.7880.4781.4384.3665.8761.6783.3383.7586.06": "cases into structured sub-facts, which are crucialfor the models learning process.(3) Among different aggregation strategies, MS Mean demonstrates the least performance degra-dation. This is primarily because the dataset mainlyconsists of simple cases with single charges, whereMean and MS become essentially equivalent. Con-versely, MS NC exhibits the most notable perfor-mance decline. This is mainly because the modelno longer maintains a cross-matching architectureafter the concatenation operation. Merging mul-tiple facts into a single representation negativelyimpacts representation learning.",
  ": An example of the interpretability of KELLER. We can observe that each sub-fact of the query finds acorrect match in the candidate document (in red)": "Original Case Description:The Defendant Gong hid methamphetamine in a cylinder and instructed the defendant He tomail it. Gong texted He the address details. The package was shipped, but intercepted atShenzhen airport on January 15th with the drugs inside. Gong was arrested on March 23rd,with police finding red pills, an air rifle, 68 bullets, (omit many drug-related items here). Knowledge-guided Case Reformulation:Transporting drugs: Gong intended to transport methamphetamine elsewhere, placedthe drugs in a gas cylinder, and texted He the mailing address and recipient information,asking He to help mail it. The package was seized at Shenzhen airport, containing 975.8grams of drugs.",
  "Illegal possession of drugs: After Gong was arrested for mailing drugs, the police founda large quantity of drugs including meth, heroin, and marijuana in his residence": "Illegal possession of firearms and ammunition: After Gong was arrested for mailingdrugs, the police found a long air gun and 68 bullets at his residence, 23 of which wereidentified as ammunition, suspecting illegal possession of firearms and ammunition. Naive Summarization:On January 14th, Gong instructed He to hide methamphetamine in a mechanical cylinderand arrange for its delivery via courier. The next day, this batch of drugs was seized at asecurity checkpoint at Shenzhen Airport. Gong was captured in an industrial area, wheremore drugs were found, including meth, heroin, and cannabis, in significant quantities. : Comparison of the original text, naive sum-marization, and our proposed knowledge-guided casereformulation. The original text is manually abbreviateddue to its length. Important sentences are marked in red.",
  "declined after fine-tuning, so we included its zero-shot results for comparison, alongside the fine-tuned outcomes of other models. Results as shownin and we find:": "(1) KELLER outperformed other models on bothquery types, showing more substantial gains in con-troversial queries with improvements of 24.04%and 13.41% in the LeCaRD and LeCaRDv2datasets, respectively. This enhanced performanceis credited to KELLERs novel case reformulation,which simplifies complex scenarios into sub-facts,aiding in better comprehension and matching. (2) In the LeCaRD dataset, lexical-based mod-els showed consistent performance across differ-ent queries, unlike representation-based modelswhich varied significantly. For example, BERToutperformed BM25 on common queries but wasless effective on controversial ones, a differenceattributed to the models limited ability to handlemultifaceted cases. KELLERs cross-matching ar-chitecture successfully addresses this limitation.",
  "Case Studies": "Case reformulation. We provide an illustrativecomparison between the original case description,naive summarization, and our knowledge-guidedcase reformulation in .The case cen-ters on complex issues of drug transport andfirearm possession. Most details focus on drugtransportation, with brief mentions of firearmsfound at the defendants residence towards theend. Given the 512-token limit of most retrievers,crucial information about the firearms is ofteninaccessible. While naive summarization capturesthe main points, it overlooks specifics aboutthe firearms in the context of drug offenses. Incontrast, our KGCR method segments the caseinto three topicsdrug transportation, illegal drugpossession, and illegal firearms possessionthusdetailing each criminal aspect comprehensively. Interpretability. In KELLER, each sub-fact ina query represents a specific intent of the query,with the highest match score from a candidate caseindicating how well this intent is met. KELLERallows users to see which sub-fact in a candidatecase matches their intent. For example, in a caseinvolving robbery and harboring crimes shown in, KELLER accurately matches sub-factsin the query to those in the candidate case, demon-strating the alignment of KELLERs scoring withthe underlying legal facts of the case. The matchingis shown in a matrix, where the positions (q1, d1)and (q2, d2) highlight the defendants actions in thequery and the candidate case, respectively, estab-lishing a direct correlation between the computedscores and the case ranking.",
  "Limitations": "External Knowledge base Construction. Ourmethod requires constructing a legal knowledgebase to assist in case reformulation, which intro-duces an extra step compared to the out-of-the-boxdense retrievers. This issue is common in mostdomain-specific knowledge-enhanced methods. Computing Efficiency. Our approach needs tocall large language models when processing thequery case, which may bring additional computa-tional costs. In our experiments, we have employedtechniques such as vLLM to achieve high-speed in-ference. Furthermore, we believe that with ongoingadvancements in techniques in both hardware andalgorithms, the computational of utilizing LLMsfor processing individual query cases online will beacceptable. For example, Llama3-8B can achieve aspeed exceeding 800 tokens per second on the Groqplatform, while recent inference services providedby Qwen and DeepSeek require less than $0.0001per 1,000 tokens.",
  "Ethical Discussion": "The application of artificial intelligence in the legaldomain is sensitive, requiring careful examinationand clarification of the associated ethical implica-tions. The two datasets utilized in our experimentalanalysis have undergone anonymization processes,particularly with regard to personally identifiableinformation such as names. Although KELLER demonstrates superior per-formance on two human-annotated datasets, its rec-ommendations for similar cases may sometimes beimprecise when dealing with intricate real-worldqueries. Additionally, the case databases in ex-isting systems may not consistently include casesthat fully satisfy user requirements. The choice toreference the retrieved cases should remain at thediscretion of the experts.",
  "Acknowledgement": "This work was supported by the NationalScience and Technology Major Project No.2022ZD0120103, National Natural Science Foun-dation of China No.62272467, the fund for build-ing world-class universities (disciplines) of RenminUniversity of China, Public Computing Cloud ofRUC. The work was partially done at the Engineer-ing Research Center of Next-Generation IntelligentSearch and Recommendation, MOE, and School ofInterdisciplinary Studies of RUC. Arian Askari and Suzan Verberne. 2021. Combininglexical and neural retrieval with longformer-basedsummarization for effective case law retrieval. InProceedings of the Second International Conferenceon Design of Experimental Search & InformationREtrieval Systems, Padova, Italy, September 15-18,2021, volume 2950 of CEUR Workshop Proceedings,pages 162170. CEUR-WS.org.",
  "Ilias Chalkidis, Manos Fergadiotis, Prodromos Malaka-siotis, Nikolaos Aletras, and Ion Androutsopoulos.2020. LEGAL-BERT: the muppets straight out oflaw school. CoRR, abs/2010.02559": "Zhuyun Dai and Jamie Callan. 2019. Deeper text un-derstanding for ir with contextual neural languagemodeling. In Proceedings of the 42nd internationalACM SIGIR conference on research and developmentin information retrieval, pages 985988. Chenlong Deng, Zhicheng Dou, Yujia Zhou, PeitianZhang, and Kelong Mao. 2024a. An element is wortha thousand words: Enhancing legal case retrieval byincorporating legal elements. In Findings of the As-sociation for Computational Linguistics, ACL 2024,Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 23542365. Association for Compu-tational Linguistics.",
  "Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao,and Nan Duan. 2023a.Query rewriting forretrieval-augmented large language models. CoRR,abs/2305.14283": "Yixiao Ma, Yunqiu Shao, Yueyue Wu, Yiqun Liu,Ruizhe Zhang, Min Zhang, and Shaoping Ma. 2021.Lecard: a legal case retrieval dataset for chinese lawsystem. In Proceedings of the 44th internationalACM SIGIR conference on research and developmentin information retrieval, pages 23422348. Yixiao Ma, Yueyue Wu, Weihang Su, Qingyao Ai,and Yiqun Liu. 2023b. Caseencoder: A knowledge-enhanced pre-trained model for legal case encoding.In Proceedings of the 2023 Conference on EmpiricalMethods in Natural Language Processing, EMNLP2023, Singapore, December 6-10, 2023, pages 71347143. Association for Computational Linguistics. Kelong Mao, Chenlong Deng, Haonan Chen, FengranMo, Zheng Liu, Tetsuya Sakai, and Zhicheng Dou.2024. Chatretriever: Adapting large language mod-els for generalized and robust conversational denseretrieval. CoRR, abs/2404.13556. Kelong Mao, Zhicheng Dou, Fengran Mo, Jiewen Hou,Haonan Chen, and Hongjin Qian. 2023. Large lan-guage models know your contextual search intent: Aprompting framework for conversational search. InFindings of the Association for Computational Lin-guistics: EMNLP 2023, Singapore, December 6-10,2023, pages 12111225. Association for Computa-tional Linguistics.",
  "Yunqiu Shao, Jiaxin Mao, Yiqun Liu, Weizhi Ma, KenSatoh, Min Zhang, and Shaoping Ma. 2020. Bert-pli:Modeling paragraph-level interactions for legal caseretrieval. In IJCAI, pages 35013507": "Yanran Tang, Ruihong Qiu, and Xue Li. 2023. Prompt-based effective input reformulation for legal caseretrieval. In Databases Theory and Applications -34th Australasian Database Conference, ADC 2023,Melbourne, VIC, Australia, November 1-3, 2023, Pro-ceedings, volume 14386 of Lecture Notes in Com-puter Science, pages 87100. Springer. Vu Tran, Minh Le Nguyen, Satoshi Tojo, and Ken Satoh.2020. Encoded summarization: summarizing doc-uments into continuous vector space for legal caseretrieval. Artificial Intelligence and Law, 28:441467.",
  "Shitao Xiao, Zheng Liu, Peitian Zhang, and NiklasMuennighof. 2023.C-pack: Packaged resourcesto advance general chinese embedding.CoRR,abs/2309.07597": "Chenyan Xiong, Zhuyun Dai, Jamie Callan, ZhiyuanLiu, and Russell Power. 2017. End-to-end neuralad-hoc ranking with kernel pooling. In Proceedingsof the 40th International ACM SIGIR Conference onResearch and Development in Information Retrieval,Shinjuku, Tokyo, Japan, August 7-11, 2017, pages5564. ACM. Feng Yao, Chaojun Xiao, Xiaozhi Wang, Zhiyuan Liu,Lei Hou, Cunchao Tu, Juanzi Li, Yun Liu, WeixingShen, and Maosong Sun. 2022. LEVEN: A large-scale chinese legal event detection dataset. In Find-ings of the Association for Computational Linguistics:ACL 2022, Dublin, Ireland, May 22-27, 2022, pages183201. Association for Computational Linguistics. Weijie Yu, Zhongxiang Sun, Jun Xu, Zhenhua Dong,Xu Chen, Hongteng Xu, and Ji-Rong Wen. 2022.Explainable legal case matching via inverse optimaltransport-based rationale extraction. In SIGIR 22:The 45th International ACM SIGIR Conference onResearch and Development in Information Retrieval,Madrid, Spain, July 11 - 15, 2022, pages 657668.ACM. Yiming Zeng, Ruili Wang, John Zeleznikow, and Eliz-abeth A. Kemp. 2005. Knowledge representationfor the intelligent legal case retrieval. In Knowledge-Based Intelligent Information and Engineering Sys-tems, 9th International Conference, KES 2005, Mel-bourne, Australia, September 14-16, 2005, Proceed-ings, Part I, volume 3681 of Lecture Notes in Com-puter Science, pages 339345. Springer. Kun Zhang, Chong Chen, Yuanzhuo Wang, Qi Tian, andLong Bai. 2023. Cfgl-lcr: A counterfactual graphlearning framework for legal case retrieval. In Pro-ceedings of the 29th ACM SIGKDD Conference onKnowledge Discovery and Data Mining, pages 33323341.",
  "# Train queries-640# Test queries107160# Documents9,19555,192Average query length4454,499Average doc length7,4464,768Average golden docs / query10.3913.65": "of BM25. For ranking methods based on PLMs,a uniform learning rate of 1e-5 and a batch sizeof 128 are consistently applied. In BERT-PLI, thenumbers of queries and candidate case segmentsare set to 3 and 4, respectively, with a maximumsegment length of 256. For Lawformer, the max-imum text input length is set to 3,072, optimizedusing a learning rate of 1e-5 and a batch size of 64. In KELLER, we employ the Qwen-72B-Chat (Bai et al., 2023), which is currently one ofthe best open-source Chinese LLMs, to performcase reformulation. We do not choose OpenAI APIdue to concerns about reproducibility and high cost.All prompts, except for the case description, areinput as system prompts. In the ranking model, themaximum number of crimes per case is capped at 4,which meets the needs of most cases. We adopt thepre-trained retriever SAILER as the text encoder.The in the contrastive learning is 0.01, and the in the final loss function is 0.9. We conduct modeltraining with a learning rate of 1e-5 and a batchsize of 128. All experiments are conducted on fourNvidia Tesla A100-40G GPUs.",
  "B.1Extraction Prompt": "Extraction Prompt: You are now a legal ex-pert, and your task is to find all the crimes andlaw articles in the procuratorates charges (orcourt judgments) from the provided case. Theoutput format is one line each for crimes andlaw articles, two lines in total. Multiple crimes(law articles) are separated by semicolons.",
  "B.2Summarization Prompt": "Summarization Prompt: You are now a legalexpert, and you are good at analyzing lengthy le-gal case texts containing multiple circumstancesof crime. Your task is to concisely summarizethe causes, procedures, and outcomes associ-ated with a specified crime, ensuring each partdoes not exceed 100 words.[Crime]: the specific crime name[Law Articles]: the specific provisions of lawarticles",
  "Specifically, for a positive document d+ of queryq, we first check whether any of the document sub-facts share the same crimes as any of the querysub-facts:": "If it exists, as shown in (a), for a querysub-fact qi, we treat the document sub-facts thatshare the same crime as the positives (e.g., thegreen rectangles in columns d+1 , d+2 , and d+3 ),and all the other document sub-facts as negatives(e.g., the red rectangles in columns d+1 , d+2 , andd+3 ). If the crime of qi is different from any of",
  "the document sub-facts, we will not include qifor training (e.g., the gray rectangles in row q3)": "If not, as shown in (b), we select the(qi, d+j ) which has the highest similarity score asa positive training pair (e.g., the green rectangle),and retain any (qi, d+k (k = j)) as negatives (e.g.,the red rectangles in columns d+2 and d+3 ). Allthe other query and document sub-fact pairs arediscarded (e.g., the gray rectangles in columnsd+1 , d+2 , and d+3 ).",
  "If it exists, we further check whether one of itsdocument sub-facts dj shares the same crime asa qi": "1. Both dj and qi are implicated to the samecrime. we will include all (qi, dk (k = j))as negatives (e.g., the red rectangles of col-umn d1 and d2 in (a) and (b)). Allthe other sub-facts are discarded to avoidintroducing false negatives (e.g., the grayrectangles of (q1, d1 ) in (a) and(b)).",
  "DCase Format of Other Regions": "To demonstrate the international applicability ofour method, we use U.S. legal documents as ex-amples. and depict the formatsof a U.S. indictment and a judgment document,respectively. It is evident that the legal knowl-edge required by our method (a combination ofcharges and law articles in this paper) is commonlypresent in the body sections of these documents.our method can be applied to reformulate legaltexts in documents from other jurisdictions simi-larly, thereby enhancing their performance of legalcase retrieval."
}