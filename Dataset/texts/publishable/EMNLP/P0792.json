{
  "Abstract": "In the ten years since the development of theAbstract Meaning Representation (AMR) for-malism, substantial progress has been made onAMR-related tasks such as parsing and align-ment. Still, the engineering applications ofAMR are not fully understood. In this survey,we categorize and characterize more than 100papers which use AMR for downstream tasksthe first survey of this kind for AMR. Specifi-cally, we highlight (1) the range of applicationsfor which AMR has been harnessed, and (2) thetechniques for incorporating AMR into thoseapplications. We also detect broader AMR en-gineering patterns and outline areas of futurework that seem ripe for AMR incorporation.We hope that this survey will be useful to thoseinterested in using AMR and that it sparks dis-cussion on the role of symbolic representationsin the age of neural-focused NLP research.",
  "Introduction": "Abstract Meaning Representation (AMR; Ba-narescu et al., 2013) is a semantic representationthat takes the form of a rooted, directed graph.Since the release of AMR in 2013, a full AMR-ecosystem has emerged, with substantial researchactivity on AMR annotation, text-to-AMR pars-ing, AMR-to-text generation, and domain- andlanguage-based extensions of AMR.1 In particular,the progress on text-to-AMR parsing and AMR-to-text generation has propelled work using AMRfor various NLP applications. To date, downstreamapplications of AMR have been spread across nu-merous tasks and have found varying degrees ofsuccess.Thus, given the recent advancements for andwith AMR, this survey addresses the pressing ques-tion: how can AMR be used for engineering pur-poses and downstream applications? Our main",
  ": The AMR for the sentence After 3 days andmuch deliberation, the jury rendered a guilty verdict,as a graph (top) and as a string in PENMAN notation(bottom)": "goals of this investigation include (1) providing anoverview of the many application areas and taskswhere AMR has been applied, (2) examining whattechniques have been used to leverage AMR forNLP systems, and (3) detecting new avenues forfuture applications of AMR in NLP research.Our investigation is also motivated by the preva-lence of large language models (LLMs) that seemto be able to generalize across a large suite of NLPtasks, prompting consideration of how semanticrepresentations can remain useful. We hope that",
  "AMR Formalism": "Semantic representations such as AMR aim to con-vey the meaning of a text and can be designed to fo-cus on specific aspects of meaning. AMR is specif-ically designed to reflect who does what to whomas the schema centers on predicate-argument rela-tions. By abstracting away from the surface form,two sentences with equivalent meaning and contentwords should be represented by the same AMRgraph. Among semantic representations, AMR isparticularly popular and well-resourced (Sadeddineet al., 2024).AMRs are rooted and directed, and can be repre-sented in graph form or in the text-based PENMANnotation (Kasper, 1989) (an example AMR in bothforms is shown in ); text-based AMRsare also called linearized, and often appear con-densed onto one line for ease of neural encoding.Concepts correspond to nodes in the graph, andedges denote relationships between those concepts.These concepts can occupy core argument roles(i.e. :argN) or non-core roles (e.g. :time and:domain). AMR makes use of PropBank framefiles (Palmer et al., 2005) to indicate the sense ofeach concept in the graph, as well as to specifythe arguments associated with each concept. AMRannotation is unanchored, so individual tokens donot necessarily align with specific concepts in thegraph. Coreferent concepts are reflected in AMRgraphs as re-entrant graph nodes.AMR also notably does not represent morphol-ogy or tense, meaning that annotation is fairlylightweight.Inter-annotator agreement is typi-cally measured quantitatively using Smatch (Caiand Knight, 2013), which calculates graph over-lap via hill climbing. Embedding-based metricswhich measure AMR graph overlap include mono-lingual S2match (Opitz et al., 2020) and multilin-gual XS2match (Wein and Schneider, 2022).AMR was originally designed for English andwas not intended to serve as an interlingua (Ba-narescu et al., 2013), but the schema has since beenconsidered for or adapted to numerous other lan-guages: Czech (Ureov et al., 2014), Chinese (Xueet al., 2014; Li et al., 2016), Spanish (Migueles-Abraira et al., 2018; Wein et al., 2022), Vietnamese (Linh and Nguyen, 2019), Korean (Choe et al.,2020), Portuguese (Sobrevilla Cabezudo and Pardo,2019; Anchita and Pardo, 2018; Incio et al., 2022;Baptista et al., 2024), Turkish (Azin and Eryigit,2019; Oral et al., 2022), Persian (Takhshid et al.,2022), and German (Otto et al., 2024).Multilingual adaptations of AMR which are notspecific to one individual language include Uni-form Meaning Representation (UMR; Van Gyselet al., 2021) and BabelNet (Martnez Lorenzo et al.,2022). Some extensions of the AMR schema in-corporate tense and aspect (Donatelli et al., 2018;Bakal, 2021), while others move beyond the sen-tence level (OGorman et al., 2018; Moreda et al.,2018; Naseem et al., 2022). Many engineering ap-plications of AMR have focused on English, likelydue to English AMR tools currently being the mostwidely available and accurate.",
  "AMR Parsing and Text Generation": "Two crucial AMR-intrinsic tasks are text-to-AMRparsing and AMR-to-text generation. Both tasksare actively researched, monolingually and multi-lingually. Substantial efforts towards highly accu-rate parsing and generation contribute further tothe interest in using AMR for downstream applica-tions.Parsing and generation models now tend to lever-age pre-trained Transformers that are fine-tuned onlinearized AMR graphs (Bevilacqua et al., 2021).Current models can parse and generate quite accu-rately, reporting Smatch scores upwards of 86% forparsing (Lee et al., 2022b; Vasylenko et al., 2023)and more than 50 BLEU (Papineni et al., 2002)points for generation (Cheng et al., 2022).Thus, while AMR parsing and generation are notyet solved (Opitz and Frank, 2022a; Groschwitzet al., 2023), model performance is quite high, andsuccess towards semantically consistent parsingand generation (Kachwala et al., 2024) has led to aspike in the downstream utility of AMR.",
  "Explainable Semantic Similarity": ": Bar chart of the number of papers using AMR in downstream applications per year, from 2014 to 2024(year to date). The 5 most common application areas are individually shown, with all other areas grouped into theOther category. cidating the core elements of meaning. Broadly,the meaning-focused tasks which have seen AMRleveraged fall under information extraction, ques-tion answering, and summarization; these areasoverlap, particularly information retrieval and ques-tion answering, as the former can be an importantstep for the latter.Information Retrieval/Extraction.Early AMRinvestigations for information retrieval/extractionfocused on the biomedical domain. Biomolecularinteractions elicited via AMR have been used inclassifiers to outperform state-of-the-art interactionmodels (Garg et al., 2016; Wang et al., 2017). No-tably, Rao et al. (2017) showed that biomedicalevents are subgraphs of full AMR graphs and deve-loped an LSTM model (Hochreiter and Schmidhu-ber, 1997) to identify those event subgraphs. Zhanget al. (2021) performed biomedical informationextraction by creating an AMR graph enhancedwith information from an external knowledge base.These enhanced AMR graphs were then encodedinto a graph attention network, leading to an im-provement over state-of-the-art methods.Models for event extraction have also incorpo-rated AMR, occasionally outperforming state-of-the-art models. First, Li et al. (2015) added AMRfeatures in the form of node-relation unigrams andbigrams to an event detection model. More recently,Xu et al. (2023) created new event extraction la-bels by using an existing event extraction modeland an AMR parser to compute a compatibility score between the event and an argument. Yanget al. (2023) performed event structure extractionby identifying whether there is an edge connectingthe event and the argument, parsing an AMR andthen using a Graph Neural Network (GNN) to pre-dict whether there is an edge. Again outperformingstate-of-the-art event extraction models, Hsu et al.(2023) produced a linearized AMR, encoded it witha neural network, and prepended the encoding tothe neural text embedding.More specifically than general event extraction, Zhang and Ji (2021) performed entity and rela-tion extraction by training an AMR encoder andusing AMR parses in order to determine the or-der of decoded events. This work outperformedprior state-of-the-art models on information extrac-tion by multiple F1 points. Gururaja et al. (2023)compared the utility of different sorts of linguisticgraphs for Transformer-based models for relationextraction, finding that AMRs were most usefulin few-shot settings. Pan et al. (2015); Steinmetz(2023) performed entity linking by mapping namedentities onto concepts in AMR graphs.At the document level, Xu et al. (2022) per-formed event extraction using text embeddingscombined with document-level and sentence-levelAMR graphs, outperforming prior state-of-the-artsystems, and Zhao et al. (2023) encoded document-level AMR graphs in a GNN for relation extraction.Finally, Mller and Kuwertz (2022) extractedrelevant information from remote sensing database",
  "management systems, using AMR graph overlapmetrics to measure semantic relevance": "Question Answering and Knowledge Graphs.When incorporating AMRs into question answer-ing models, prior approaches have combined AMRgraphs with a formal reasoning layer (Mitra andBaral, 2016) and sentence embeddings (Park et al.,2024). On the other hand, Bonial et al. (2020b)used AMR graphs directly, parsing medical ques-tions (about COVID-19) into AMRs and compar-ing them against AMR graphs of possible answers.The answers were then ranked by similarity andthe most similar response AMR was returned asthe answer. Regan et al. (2024) created multilingual AMRgraphs of questions and developed a joint AMR-SPARQL parsing model for hallucination de-tection in knowledge base question answering(KBQA). For direct use in KBQA, AMR graphshave been converted into SPARQL queries (Borneaet al., 2021; Kapanipathi et al., 2021; Shivashankaret al., 2022). Similarly, AMRs have been used toproduce Resource Description Framework (RDF)knowledge graphs (Burns et al., 2016; Meloni et al.,2017; Gangemi et al., 2023), and to semantic rolesfor a climate-focused knowledge base (Islam et al.,2022).For multi-hop question answering (questionswhich require multiple steps to reach the answer),Xu et al. (2021) parsed AMR graphs of the hy-pothesis and the relevant facts and merged them,while Deng et al. (2022) segmented AMR parsesof the question into subgraphs, and generated sub-questions via AMR-to-text generation of the sub-graphs. Similarly, for open domain question an-swering (ODQA), Wang et al. (2023) integratedAMR graphs of the relevant facts from a text byappending a single token embedding of each con-cept or relation in the AMR graph to the text em-bedding. Shi et al. (2024) performed ODQA viaretrieval augmented generation (RAG), using anAMR-based algorithm to compress textual infor-mation into individual concepts. Pham et al. (2024)conditioned QA systems on AMR graphs, findingthe approach works best with small models, whichthen outperformed very large LLMs such as Chat-GPT.The task of machine comprehension, whichinvolves systems producing answers about a text,has also benefited from comparison between textand AMR graphs (Galitsky, 2020), with Sachan and Xing (2016) framing machine comprehensionas a graph entailment problem.Towards ques-tion answer dataset creation, Rakshit and Flani-gan (2021) parsed AMRs of sentences to generatequestion-answer pairs. Summarization.AMR use in summarization hastaken various approaches, often by parsing andjoining AMR graphs of the sentences determinedto be the most important. For instance, Dohare et al.(2017); Liao et al. (2018) picked the most importantsentences from a text and created a single AMRgraph from those sentences, then generated a shortsummary from key subgraphs.Rather than parsing only the most important sen-tences, Liu et al. (2015) parsed individual AMRgraphs of a text, combining them into one sum-marization graph by collapsing multiple conceptsinto single nodes with new concept labels, and thengenerating text from the summarized AMR. Varia-tions of this approach have been proposed by Hardyand Vlachos (2018); Kouris et al. (2022).Numerous approaches to genre-specific sum-marization, such as opinion summarization (In-cio and Pardo, 2021), TV transcript summariza-tion (Hua et al., 2022), timeline generation (Man-souri et al., 2023), long dialogue summarization(Hua et al., 2023), and abstractive summarizationof biomedical documents (Frisoni et al., 2023) haveall seen the incorporation of AMR. Some of theseworks have leveraged AMRs by parsing AMRs ofthe text and then incorporating them into an LLMvia an attention mechanism (Hua et al., 2022, 2023;Frisoni et al., 2023). In a non-English setting, Sev-erina and Khodra (2019) used AMR for Indonesianmulti-document summarization.As a post-hoc refinement step for text sum-marization, Ming et al. (2018) used AMR andWordNet (Miller, 1995; Fellbaum, 1998) to filterout redundant information.",
  "Style Transfer.Jangra et al. (2022) leveragedAMR as an intermediate representation to generatea paraphrase in a different style, using a fine-tuned": "Transformer-based AMR parser (encoder) and mul-tiple Transformer-based text generators (decoders)for various text styles. Shi et al. (2023) also usedAMR as an intermediary, parsing an AMR graphfrom the text and performing concept-level stylerewriting on the AMR graph (modifying the wordsto a different genre), achieving state-of-the-art re-sults. Paraphrase Generation.Prior work has utilizedAMR for paraphrase generation by producing para-phrases directly from AMR graphs (Huang et al.,2023; Bao et al., 2023), occasionally altered withadditional information (Lee et al., 2022a; Tu et al.,2024), or by injecting an embedding of an AMRinto a Transformer model (Huang et al., 2022).As AMR inherently preserves semantic simi-larity, Huang et al. (2023) used AMR directly asan intermediary to generate syntactically diverseparaphrase sets.They changed the root of theparsed AMR in order to be able to produce multi-ple AMRs,2 and thus multiple sentences via AMR-to-text generation. Similarly, Shou et al. (2022);Ghosh et al. (2024) tackled data augmentation byparsing AMR graphs of the text, then editing themand generating new text.",
  "Grammatical Error Correction.Cao and Zhao": "(2023) constructed denoised AMR graphs of sen-tences with grammatical errors and incorporatedthem into a sequence-to-sequence model as addi-tional knowledge, achieving significantly higherprecision and recall than the text-only baseline. Machine Translation.Multiple approaches toneural machine translation have seen performanceimprovements when incorporating AMR as addi-tional knowledge (Song et al., 2019; Nguyen et al.,2021). Li and Flanigan (2022) in particular ob-served performance gains when integrating AMRgraphs into both the encoder and decoder of aTransformer model. Jin et al. (2024) on the otherhand found that feeding an AMR in a zero-shotprompting setting with LLMs did not improveoreven hurtperformance.AMR can also be used as an intermediary for atranslation post-processing step in order to reducethe presence of translation artifacts (translationese,Wein and Schneider (2024b)). 2In AMR, the root indicates the linguistic focus of a sen-tence. Thus, changing the root of the AMR of the sentencethe cat drinks water from drink to water, will yield a para-phrase such as it is water that the cat drinks.",
  "Given that some syntactic content is not included inAMR graphs, the AMR schema has been adaptedas necessary for specific domains": "Math.Two recent works addressed how formulasconveyed in text should be accommodated withinAMR graphs. Iordan (2021) developed an AMRparser with an added coreference detection featureto parse AMR graphs from descriptions of geome-try problems, and Mansouri et al. (2022) incorpo-rated embeddings of altered AMRs into an LLMfor extracting formulas. Legal Reasoning.As is the case for math,domain-specific language in legal documents ne-cessitates alteration of the AMR formalism. Vuand Nguyen (2019) evaluated the (generally poor)performance of AMR parsers on legal documents;further, Schrack et al. (2022) showed that neuro-symbolic methods which include linearized AMRgraphs do not outperform text-only methods onmultiple choice question answering for legal rea-soning, but do offer a complementary signal. Toaddress these challenges, Vu et al. (2022) intro-duced a human-annotated dataset of AMRs in thelegal domain. Spatial/Situated Dialogue.A fruitful line ofAMR application research has focused on spatial/situated dialogue, in particular on human-robot in-teraction.3 Numerous datasets of AMR graphs ofhuman-robot interactions have been created (Bas-tianelli et al., 2014; Shichman et al., 2023). An al-tered AMR schema called Dialogue-AMR (con-taining information on tense, aspect, and speechacts) supports the representation of human-robot in-teractions (Bonial et al., 2019; Abrams et al., 2020;Bonial et al., 2020a, 2021, 2023). Ultimately, thishas enabled grounded natural language understand-ing for human-robot interactions.Other work in spatial and situated AMR (un-related to human-robot interaction) has also ac-counted for the necessity of altering AMR to in-clude grounding language.Datasets of AMRgraphs for multimodal dialogue have incorporatedgestures (Donatelli et al., 2022; Lai et al., 2024)and spatial information (Bonn et al., 2020; Danet al., 2020) into the AMR schema. Martin et al. 3While AMR-based dialogue understanding work has beenprimarily focused on human-robot dialogue, Bai et al. (2022)achieved state-of-the-art performance on general dialogue un-derstanding by using AMR to continuously pre-train a Trans-former encoder.",
  "AMR for Image and Speech": "As a semantic representation, AMR has been con-verted into other types of text-based formalisms(such as SPARQL in 3.1 and UMR (Post et al.,2024)), as well as leveraged in support of non-text-based forms of media such as images and speech.Images.Recent work has investigated the use ofAMR for scene graph parsing, which is the pro-duction of a graph-based representation of objectboundaries in images. Choi et al. (2022a,b) con-verted AMR graphs into scene graphs, while Ab-delsalam et al. (2022) explored the use of AMR asan alternative to scene graphs (via image-to-AMRparsing).Image captioning has employed AMR in orderto focus on specific aspects of meaning or task-specific difficulties. Neto et al. (2020) used AMRto produce descriptions of specific regions of animage, and Kim et al. (2024) used the relationshipbetween the sentence and object (via AMR) forcaption debiasing. Finally, Bhattacharyya et al.(2024) and Chen et al. (2024) leveraged semanticrelations from AMR and the image to guide captiongeneration.Speech.Little work has investigated the utility ofAMR for speech systems, though Addlesee and Da-monte (2023) addressed nonstandard speech as anaccessibility issue for voice assistants by producinga corpus of AMR graphs of disrupted speech, andtraining models on this data.",
  "AMR for NLG Evaluation": "AMRs nature as an interpretable semantic repre-sentation lends itself to evaluation-based tasks.Dialogue Evaluation.Ghazarian et al. (2022) de-veloped a robust dialogue coherence measure bytraining on negative text examples that are gener-ated from AMRs which were manipulated in con-trolled ways, e.g., introducing contradictions by changing node labels to antonyms. The resultingmodel achieves significantly better correlation tohumans than other text- and graph-based baselines.On the same task, Yang et al. (2024) showed thatperformance improvements can also be achievedby fusing text and AMR in a model using a dual-encoder, thus more directly using the AMR.Summary Evaluation.Ribeiro et al. (2022)trained a model that leverages AMR as auxiliary in-formation in a dual encoder, outperforming strongQA-based and NLI-based summary factualitymodels.Addressing the same task, Qiu et al. (2024) produced training examples with manip-ulated AMRs, resulting in a state-of-the-art factu-ality prediction model. Tackling the second pillarof summary quality, being summary relevance,Nawrath et al. (2024) split AMR graphs of sum-maries into subgraphs with the aim to generateSummary Content Units (clauses that identify sub-sentential content in summaries (Nenkova and Pas-sonneau, 2004)). In this case, the results weremore mixed and the authors noted that develop-ment of advanced splitting methods is necessaryfor improved results. As a general summary inter-pretability method, Landes and Di Eugenio (2024)developed an AMR-alignment tool for the inspec-tion of summaries, aligning parts of the summarywith the evidence in the source document. General Evaluation and Diagnostics.Opitz andFrank (2021) used AMR metrics to compare AMRgraphs of candidates and references, enabling mea-surement of fine-grained text quality aspects likepolarity or coreference faithfulness. Using AMRmetrics to evaluate NLG quality is limited by cur-rent parsing inaccuracies (Manning and Schneider,2021).",
  "AMR for Language Studies": "AMR is a linguistic tool which has been utilizedfor language-focused research and teaching.Linguistic Research.Sawai et al. (2015) usedAMR to build a model that answers statistical re-search questions about the semantic structure ofnoun phrases. Teaching.The investigation of the meaning of atext emerges as an intriguing and interesting class-room exercise. In particular, given its linguisticspecificity and interpretability, AMR can help stu-dents learn about linguistic structures, as exempli-fied in the lesson and exercise on AMR in Eisen-stein (2021).",
  "AMR for Explainable Semantic Similarity": "AMR-based metrics are of wider interest in mea-suring semantic similarity and relatedness, beyondNLG evaluation (c.f. 3.5). Intuitively, we canparse two input texts and calculate AMR similarity,providing an additional layer of interpretability andexplainability via AMR.AMR metrics have been used for detecting para-phrases (Issa et al., 2018), evaluating the answersprovided by language learners on reading compre-hension questions (Dellert, 2020), judging argu-ment and text similarity (Opitz et al., 2021b), andmatching local knowledge graphs (Kachwala et al.,2024). Furthermore, assessing structural graphisomorphism in the AMRs of multilingual textsachieves finer-grained semantic equivalence judg-ments than neural methods (Wein et al., 2023). In-corporating AMR graphs and AMR metrics intoneural models for natural language inference hasalso been of value (Opitz et al., 2023; Feng andHunter, 2024; Bao et al., 2023).Neural text embedding models such as SBERT(Reimers and Gurevych, 2019) and SimCSE (Gaoet al., 2021) have been retro-fitted by encodingAMR graphs (Cai et al., 2022). Alternatively, se-mantic embedding interpretability has been in-duced by binding parts of embeddings to semanticfeatures such as negation, semantic roles, or namedentities, that can be measured with AMR metrics(Opitz and Frank, 2022b). In the same direction,Fodor et al. (2024) found that state-of-the-art trans-formers poorly capture the pattern of human se-mantic similarity judgments, and AMR can beused to build simple methods that combine seman-tic compontents into an improved hybrid model.",
  "Miscellanea": "Finally, we discuss miscellanea, which are eitherapplications where the impetus behind the use ofAMR may be less obvious, or applications that es-cape a categorization into the above classes. Firstly,AMRs have been employed for commonsense rea-soning, using different strategies: tracing reasoningpaths through AMRs (Lim et al., 2020), enrich-ing AMRs with relations from a CommonsenseKnowledge Graph (Oh et al., 2022), and withina neuro-symbolic approach where AMR is con-verted into first-order logic (Chanin and Hunter,2023). AMR has also been used for sentiment an-alysis (Ma et al., 2023) and to generate feedbackfor reinforcement learning in text-based games (Chaudhury et al., 2023). Elbasani and Kim (2022)parsed AMRs of the text and then used that as inputto a convolutional neural network for toxic contentdetection. For a similar taskfake news detec-tionGupta et al. (2023) used text-based features inconjunction with AMR graphs to classify whethera tweet is fake news. Finally, AMR has been usedto perform general text classification (Ogawa andSaga, 2023).",
  "AMR Preparation": "As an initial step in working with AMR, many ap-plications conduct operations on the AMR graph.We observe frequent use of the following threetypes of operations:pre-processing, splitting/merging, and encoding.AMR pre-processing can range from simplestring changes to more elaborate graph transfor-mations. Examples of simple string changes in-clude lower-casing or truncating the concept labels.Graph transformations that preserve the equiva-lency of AMRs can include reification (Opitz et al.,2021a; Shou and Lin, 2023), where, with the helpof a dictionary, we generalize binary edge labelsto n-ary structures. Alternatively conversion to aLevi Graph (Beck et al., 2018; Lim et al., 2020)which is a bipartite graph without edge labels, alle-viates the need to handle edge labels in some spe-cific way other than node labels (see Appendix Afor examples of these transformations).AMR splitting and merging can also come inhandy. For example, AMRs are split to find thelargest common sub-structures in question answerpairs (Deng et al., 2022), or to extract subgraphsthat elicit specific aspects of meaning such as po-larity or semantic roles (Opitz and Frank, 2022b;Opitz, 2023).Merging can be applied by firstmatching concepts or named entities from twographs, and then connecting or fusing nodes thatrepresent the same entities (Liu et al., 2015; Leeet al., 2021), possibly leveraging advanced corefer-",
  "ence resolution within AMR (Fu et al., 2021). Inthe simplest case, merging is conducted by con-necting multiple graphs at their roots (Kouris et al.,2022; Bai et al., 2022)": "Bai et al. (2022) also exemplifies the possibilityof AMR enrichment with task-specific informa-tion (here: edges labeled with the speaker in adialogue). Other examples of additional informa-tion used to enrich AMR graphs include VerbNetevent structure (Tu et al., 2024) and links fromknowledge graphs (Zhang et al., 2021).These line of work on AMR merging and enrich-ment may profit from the ongoing research into theAMR-intrinsic tasks of AMR coreference reso-lution (Fu et al., 2021; Li et al., 2022) and AMR-to-text alignment (Blodgett and Schneider, 2021;Martnez Lorenzo et al., 2023).Many approaches have required that AMR some-how be encoded into an external model. Synergiz-ing well with the strong NLU inductive bias oftext language models, one successful paradigm forAMR encoding is to simply feed the linearizedgraph as a string, where string pre-processing tricks(such as those described) can increase performance(Ribeiro et al., 2021b,a).AMR encoding can also involve constructingfeature vectors (/embeddings) of the full AMRgraphs (Wang et al., 2017) or targeted semanticparts (Fodor et al., 2024).Prior work on AMR-to-text generation has foundsuccess encoding AMRs using Graph RNNs (Songet al., 2018) and Graph Transformers (Song et al.,2020; Yao et al., 2020), and the same or similar en-coding mechanisms are also found when encodingAMRs for downstream applications (Song et al.,2019).",
  "Two Processing Paradigms": "We observe two major processing paradigms inAMR applications: the neuro-symbolic model, andthe use of AMR as an intermediate representation.The first approach has been consistently popular;the second approach has grown in popularity morerecently. Fusing Text and AMR: the Neuro-symbolicModel.The abundant recent interest in neuro-symbolic approaches for NLP (Besold et al., 2021;Hamilton et al., 2022; Yu et al., 2023) has bled intoAMR applications.A common way of leveraging AMR informationis merging information from the AMR modality with information from the text modality, typicallywith an auxiliary motive (e.g., AMR is used to helprefine the extracted information from the text toimprove a model accuracy by some points). Toaccomplish this, a prominent strategy has been toconstruct an AMR parse from the text and thenfeed both this parse and text into one neural model.Sometimes, a joint encoder is employed, whereAMR and text are simply concatenated and fusedat the lowest processing layers (Huang et al., 2022;Hsu et al., 2023). The two modalities (text andAMR) can also be first processed separately, usingtwo individual encoders, to create disjoint higher-level representations that are then fused later suchas by adding or concatenating. This fusing canhappen in intermediate layers (Dai et al., 2022; Maet al., 2023), or at the final decision layer (Cai et al.,2022; Opitz et al., 2023). AMR as an Intermediate Representation.Us-ing AMR as an intermediate representation meanstypically operating on and with the AMR X as fol-lows: parse X generate, interlinking parsingand generation models.One appealing aspect of this technique is theincreased interpretility and linguistic control, asto induce controlled changes in meaning. Forexample, the AMR graph can be transformed togenerate (1) paraphrases (e.g., by swapping theroot (Huang et al., 2023), or swapping out a conceptwith a synonym, and then generating text (Shi et al.,2023)), or to generate (2) contradictions (e.g., byinserting a targeted negation to a predicate andthen generating text from the manipulated structure(Ghazarian et al., 2022)).On the other hand, the AMR graph can instead re-main unaltered while the input text, parsing method,or generation method are varied, such as in thecases of Jangra et al. (2022) for style transfer andWein and Schneider (2024b) for translationese re-duction. As another example, Dohare et al. (2017)compile a summary AMR, by finding AMR nodesfocused on important entities, and selecting the sub-tree hanging from that verb as the summary AMR.Text is then generated from the specified subtree.This highlights that the splitting and merging tech-niques highlighted in 4.1 can be part of workingwith AMR as intermediate representation.",
  "Surveying the vast number of tasks and techniquesutilized throughout the last decade, we observe": "three notable areas for future work on AMR appli-cations.First, a technique which has shown great promisefor incorporating AMR into neural or non-neuraldownstream applications is as an intermediate rep-resentation. This intuitively leverages both AMRsdesign as a graph-based semantic representationas well as the progress on text-to-AMR parsingand AMR-to-text generation.4 Using AMR as anintermediary provides us linguistic control and in-terpretability, which are increasingly desirable inthe age of black box neural models. Numerousrecent studies have successfully exploited AMRas an intermediary (4.2), indicating that this maybe a promising path forward, particularly in low-resource settings or for data augmentation.Second, recent work has shown the benefits ofincorporating AMR in few-shot or low-resourcesettings (Nguyen et al., 2021; Gururaja et al., 2023;Hua et al., 2023; Ghosh et al., 2024). This indicatesthat, regardless of the technique of incorporation,AMR is positioned to be especially well suited forengineering gains in these settings.Finally, an area which has been largely under-studied (3.6) but directly follows from the designof AMR as semantic representation, is the use ofAMR for linguistic analysis and text statistics. Po-tential applications include language learning orstudying predicate-argument patterns in L1 or L2texts. Additional uses of AMR for linguistic analy-sis include linguistically-focused evaluations andfiner statistics of NLG systems (prior work in thisdirection discussed in 3.5).5",
  "Related Work on Applications of OtherMeaning Representations": "Other surveys have considered the engineering util-ity of semantic representations. Regarding spe-cific tasks, Verrev (2023) tested out the benefitsof various meaning representations for knowledge-base question answering (KBQA), and Prange et al.(2022) for next-word prediction in conjunction withneural models.Related work has also compared the designs andfeatures of semantic representations (Abend andRappoport, 2017), with Pavlova et al. (2023) specif- 4This approach also hearkens back to one of the classic ap-proaches towards a fundamental NLP problem, being machinetranslation: interlingual machine translation (Dorr, 1993).5For mining large unstructured text data, AMR offers se-mantic triplets that await to be sensibly aggregated, e.g., tocraft an automatic knowledge graph.",
  "Conclusion": "In this survey, we provided a thorough overview ofthe tasks where Abstract Meaning Representationgraphs have been used and the techniques involvedin using AMR for engineering purposes. Giventhe availability of strong parsing systems and theincreased interest in AMR, we expect that we areon the precipice of exciting progress employingAMR in downstream applications. As our synthe-sis of AMR engineering patterns indicates, thereare numerous methods, techniques and possibleapplications that await further exploration and con-tinued improvement.",
  "Limitations": "In this survey, we direct our attention exclusivelytowards the AMR formalism given the recent abun-dance of work incorporating AMR, and the fact thatthere are still few surveys addressing AMR. Whileother semantic representations have been consid-ered as engineering tools (6), AMR is currentlyunique in the breadth with which it has been usedand studied.As discussed in 2.1, applications of AMRhave been primarily focused on English; recentwork (discussed throughout this survey) has demon-strated the cross-lingual and non-English utility ofAMR (Wein and Schneider, 2024a), which contin-ues to increase given advancements in multilingualAMR parsing and generation.We have incorporated the full breadth of existingAMR application work to the best of our knowl-edge; the bar chart in 3 serves as a lower boundas there may be papers that were missed.",
  "We thank anonymous reviewers for their helpfulfeedback and recommendations": "Mohamed Ashraf Abdelsalam, Zhan Shi, FedericoFancellu, Kalliopi Basioti, Dhaivat Bhatt, VladimirPavlovic, and Afsaneh Fazly. 2022. Visual semanticparsing: From images to Abstract Meaning Repre-sentation. In Proceedings of the 26th Conference onComputational Natural Language Learning (CoNLL),pages 282300, Abu Dhabi, United Arab Emirates(Hybrid). Association for Computational Linguistics. Omri Abend and Ari Rappoport. 2017. The state ofthe art in semantic representation. In Proceedingsof the 55th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 7789, Vancouver, Canada. Association forComputational Linguistics. Mitchell Abrams, Claire Bonial, and Lucia Donatelli.2020. Graph-to-graph meaning representation trans-formations for human-robot dialogue. In Proceed-ings of the Society for Computation in Linguistics2020, pages 250253, New York, New York. Associ-ation for Computational Linguistics.",
  "Angus Addlesee and Marco Damonte. 2023. Under-standing disrupted sentences using underspecified ab-stract meaning representation. In Interspeech 2023": "Rafael Anchita and Thiago Pardo. 2018.TowardsAMR-BR: A SemBank for Brazilian Portuguese lan-guage. In Proceedings of the Eleventh InternationalConference on Language Resources and Evaluation(LREC 2018), Miyazaki, Japan. European LanguageResources Association (ELRA). Zahra Azin and Glsen Eryigit. 2019. Towards TurkishAbstract Meaning Representation. In Proceedingsof the 57th Annual Meeting of the Association forComputational Linguistics: Student Research Work-shop, pages 4347, Florence, Italy. Association forComputational Linguistics.",
  "Mollie Bakal. 2021.Graph-to-graph translations toaugment abstract meaning representation tense andaspect. Bachelors thesis, University of Michigan": "Laura Banarescu, Claire Bonial, Shu Cai, MadalinaGeorgescu, Kira Griffitt, Ulf Hermjakob, KevinKnight, Philipp Koehn, Martha Palmer, and NathanSchneider. 2013. Abstract Meaning Representationfor sembanking. In Proceedings of the 7th LinguisticAnnotation Workshop and Interoperability with Dis-course, pages 178186, Sofia, Bulgaria. Associationfor Computational Linguistics. Laura Banarescu, Claire Bonial, Shu Cai, MadalinaGeorgescu, Kira Griffitt, Ulf Hermjakob, KevinKnight, Philipp Koehn, Martha Palmer, and NathanSchneider. 2019. Abstract Meaning Representation(AMR) 1.2.6 specification. Qiming Bao, Alex Yuxuan Peng, Zhenyun Deng, Wan-jun Zhong, Neset Tan, Nathan Young, Yang Chen,Yonghua Zhu, Michael Witbrock, and Jiamou Liu.2023. Contrastive learning with logic-driven dataaugmentation for logical reasoning over text. arXivpreprint arXiv:2305.12599. Jorge Baptista, Snia Reis, Joo Dias, and Pedro Santos.2024. Lexicalized meaning representation (LMR).In Proceedings of the Fifth International Workshopon Designing Meaning Representations @ LREC-COLING 2024, pages 101111, Torino, Italia. ELRAand ICCL. Emanuele Bastianelli, Giuseppe Castellucci, DaniloCroce, Luca Iocchi, Roberto Basili, and DanieleNardi. 2014.HuRIC: a human robot interactioncorpus. In Proceedings of the Ninth InternationalConference on Language Resources and Evaluation(LREC14), pages 45194526, Reykjavik, Iceland.European Language Resources Association (ELRA). Daniel Beck, Gholamreza Haffari, and Trevor Cohn.2018. Graph-to-sequence learning using gated graphneural networks. In Proceedings of the 56th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 273283,Melbourne, Australia. Association for ComputationalLinguistics. Tarek R Besold, Artur dAvila Garcez, Sebastian Bader,Howard Bowman, Pedro Domingos, Pascal Hit-zler, Kai-Uwe Khnberger, Luis C Lamb, PriscilaMachado Vieira Lima, Leo de Penning, et al. 2021.Neural-symbolic learning and reasoning: A surveyand interpretation 1. In Neuro-Symbolic ArtificialIntelligence: The State of the Art, pages 151. IOSpress. Michele Bevilacqua, Rexhina Blloshmi, and RobertoNavigli. 2021. One SPRING to rule them both: Sym-metric AMR semantic parsing and generation withouta complex pipeline. Proceedings of the AAAI Confer-ence on Artificial Intelligence, 35(14):1256412573. Abhidip Bhattacharyya, Martha Palmer, and ChristofferHeckman. 2024. ReCAP: Semantic role enhancedcaption generation. In Proceedings of the 2024 JointInternational Conference on Computational Linguis-tics, Language Resources and Evaluation (LREC-COLING 2024), pages 1363313649, Torino, Italy.ELRA and ICCL. Austin Blodgett and Nathan Schneider. 2021. Prob-abilistic, structure-aware algorithms for improvedvariety, accuracy, and coverage of AMR alignments.In Proceedings of the 59th Annual Meeting of theAssociation for Computational Linguistics and the11th International Joint Conference on Natural Lan-guage Processing (Volume 1: Long Papers), pages33103321, Online. Association for ComputationalLinguistics. Claire Bonial, Mitchell Abrams, David Traum, andClare Voss. 2021. Builder, we have done it: Eval-uating & extending dialogue-AMR NLU pipelinefor two collaborative domains. In Proceedings ofthe 14th International Conference on ComputationalSemantics (IWCS), pages 173183, Groningen, TheNetherlands (online). Association for ComputationalLinguistics. Claire Bonial, Lucia Donatelli, Mitchell Abrams,Stephanie M. Lukin, Stephen Tratz, Matthew Marge,Ron Artstein, David Traum, and Clare Voss. 2020a.Dialogue-AMR: Abstract Meaning Representationfor dialogue.In Proceedings of the Twelfth Lan-guage Resources and Evaluation Conference, pages684695, Marseille, France. European Language Re-sources Association. Claire Bonial, Lucia Donatelli, Stephanie M. Lukin,Stephen Tratz, Ron Artstein, David Traum, and ClareVoss. 2019. Augmenting Abstract Meaning Repre-sentation for human-robot dialogue. In Proceedingsof the First International Workshop on DesigningMeaning Representations, pages 199210, Florence,Italy. Association for Computational Linguistics. Claire Bonial, Julie Foresta, Nicholas C. Fung, Cory J.Hayes, Philip Osteen, Jacob Arkin, Benned Hede-gaard, and Thomas Howard. 2023. Abstract MeaningRepresentation for grounded human-robot commu-nication. In Proceedings of the Fourth InternationalWorkshop on Designing Meaning Representations,pages 3444, Nancy, France. Association for Com-putational Linguistics. Claire Bonial, Stephanie M. Lukin, David Doughty,Steven Hill, and Clare Voss. 2020b. InfoForager:Leveraging semantic search with AMR for COVID-19 research. In Proceedings of the Second Interna-tional Workshop on Designing Meaning Representa-tions, pages 6777, Barcelona Spain (online). Asso-ciation for Computational Linguistics. Julia Bonn, Martha Palmer, Zheng Cai, and KristinWright-Bettner. 2020. Spatial AMR: Expanded spa-tial annotation in the context of a grounded Minecraftcorpus.In Proceedings of the Twelfth LanguageResources and Evaluation Conference, pages 48834892, Marseille, France. European Language Re-sources Association. Mihaela Bornea, Ramon Fernandez Astudillo, TahiraNaseem, Nandana Mihindukulasooriya, Ibrahim Ab-delaziz, Pavan Kapanipathi, Radu Florian, and SalimRoukos. 2021.Learning to transpile AMR intoSPARQL. arXiv preprint arXiv:2112.07877.",
  "David Chanin and Anthony Hunter. 2023.Neuro-symbolic commonsense social reasoning.arXivpreprint arXiv:2303.08264": "Subhajit Chaudhury, Sarathkrishna Swaminathan, DaikiKimura, Prithviraj Sen, Keerthiram Murugesan,Rosario Uceda-Sosa, Michiaki Tatsubori, AchilleFokoue, Pavan Kapanipathi, Asim Munawar, andAlexander Gray. 2023. Learning symbolic rules overAbstract Meaning Representations for textual rein-forcement learning. In Proceedings of the 61st An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 67646776, Toronto, Canada. Association for Computa-tional Linguistics. Feng Chen, Xinyi Li, Jintao Tang, Shasha Li, and TingWang. 2024. Benefit from AMR: Image captioningwithexplicit relations and endogenous knowledge.In Web and Big Data, pages 363376, Singapore.Springer Nature Singapore. Ziming Cheng, Zuchao Li, and Hai Zhao. 2022. BiBL:AMR parsing and generation with bidirectionalBayesian learning. In Proceedings of the 29th Inter-national Conference on Computational Linguistics,pages 54615475, Gyeongju, Republic of Korea. In-ternational Committee on Computational Linguistics. Hyonsu Choe, Jiyoon Han, Hyejin Park, Tae Hwan Oh,and Hansaem Kim. 2020. Building Korean AbstractMeaning Representation corpus. In Proceedings ofthe Second International Workshop on DesigningMeaning Representations, pages 2129, BarcelonaSpain (online). Association for Computational Lin-guistics. Woo Suk Choi, Yu-Jung Heo, Dharani Punithan, andByoung-Tak Zhang. 2022a. Scene graph parsing viaAbstract Meaning Representation in pre-trained lan-guage models. In Proceedings of the 2nd Workshopon Deep Learning on Graphs for Natural LanguageProcessing (DLG4NLP 2022), pages 3035, Seattle,Washington. Association for Computational Linguis-tics.",
  "Shibhansh Dohare, Harish Karnick, and Vivek Gupta.2017. Text summarization using abstract meaningrepresentation. arXiv preprint arXiv:1706.01678": "Lucia Donatelli, Kenneth Lai, Richard Brutti, and JamesPustejovsky. 2022. Towards situated AMR: Creatinga corpus of gesture AMR. In International Confer-ence on Human-Computer Interaction, pages 293312. Springer. Lucia Donatelli, Michael Regan, William Croft, andNathan Schneider. 2018. Annotation of tense and as-pect semantics for sentential AMR. In Proceedingsof the Joint Workshop on Linguistic Annotation, Mul-tiword Expressions and Constructions (LAW-MWE-CxG-2018), pages 96108, Santa Fe, New Mexico,USA. Association for Computational Linguistics.",
  "Xuyao Feng and Anthony Hunter. 2024. Identificationof entailment and contradiction relations between nat-ural language sentences: A neurosymbolic approach.arXiv preprint arXiv:2405.01259": "James Fodor, Simon De Deyne, and Shinsuke Suzuki.2024.Compositionality and Sentence Meaning:Comparing Semantic Parsing and Transformers on aChallenging Sentence Similarity Dataset. Computa-tional Linguistics, pages 152. Giacomo Frisoni, Paolo Italiani, Stefano Salvatori, andGianluca Moro. 2023. Cogito ergo summ: Abstrac-tive summarization of biomedical papers via semanticparsing graphs and consistency rewards. Proceed-ings of the AAAI Conference on Artificial Intelligence,37(11):1278112789. Qiankun Fu, Linfeng Song, Wenyu Du, and Yue Zhang.2021. End-to-end AMR coreference resolution. InProceedings of the 59th Annual Meeting of the Asso-ciation for Computational Linguistics and the 11thInternational Joint Conference on Natural LanguageProcessing (Volume 1: Long Papers), pages 42044214, Online. Association for Computational Lin-guistics.",
  "Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021": "SimCSE: Simple contrastive learning of sentence em-beddings. In Proceedings of the 2021 Conferenceon Empirical Methods in Natural Language Process-ing, pages 68946910, Online and Punta Cana, Do-minican Republic. Association for ComputationalLinguistics. Sahil Garg, Aram Galstyan, Ulf Hermjakob, and DanielMarcu. 2016. Extracting biomolecular interactionsusing semantic parsing of biomedical text. In Pro-ceedings of the AAAI Conference on Artificial Intelli-gence, volume 30. Sarik Ghazarian, Nuan Wen, Aram Galstyan, andNanyun Peng. 2022. DEAM: Dialogue coherenceevaluation using AMR-based semantic manipula-tions. In Proceedings of the 60th Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 771785, Dublin, Ire-land. Association for Computational Linguistics. Sreyan Ghosh, Utkarsh Tyagi, Sonal Kumar, Chan-dra Kiran Evuru, Ramaneswaran S, S Sakshi, andDinesh Manocha. 2024.ABEX: Data augmenta-tion for low-resource NLU via expanding abstractdescriptions. In Proceedings of the 62nd AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 726748,Bangkok, Thailand. Association for ComputationalLinguistics. Jonas Groschwitz, Shay Cohen, Lucia Donatelli, andMeaghan Fowlie. 2023. AMR parsing is far fromsolved: GrAPES, the granular AMR parsing evalu-ation suite. In Proceedings of the 2023 Conferenceon Empirical Methods in Natural Language Process-ing, pages 1072810752, Singapore. Association forComputational Linguistics. Shubham Gupta, Narendra Yadav, Suman Kundu, andSainathreddy Sankepally. 2023. FakEDAMR: Fakenews detection using abstract meaning representationnetwork. In International Conference on ComplexNetworks and Their Applications, pages 308319.Springer. Sireesh Gururaja, Ritam Dutt, Tinglong Liao, and Car-olyn Ros. 2023.Linguistic representations forfewer-shot relation extraction across domains. InProceedings of the 61st Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 75027514, Toronto, Canada.Association for Computational Linguistics.",
  "Sepp Hochreiter and Jrgen Schmidhuber. 1997. Longshort-term memory. Neural computation, 9(8):17351780": "I-Hung Hsu, Zhiyu Xie, Kuan-Hao Huang, Prem Natara-jan, and Nanyun Peng. 2023. AMPERE: AMR-awareprefix for generation-based event argument extractionmodel. In Proceedings of the 61st Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 1097610993, Toronto,Canada. Association for Computational Linguistics. Yilun Hua, Zhaoyuan Deng, and Kathleen McKeown.2023. Improving long dialogue summarization withsemantic graph representation. In Findings of the As-sociation for Computational Linguistics: ACL 2023,pages 1385113883, Toronto, Canada. Associationfor Computational Linguistics. Yilun Hua, Zhaoyuan Deng, and Zhijie Xu. 2022. AM-RTVSumm: AMR-augmented hierarchical networkfor TV transcript summarization. In Proceedings ofThe Workshop on Automatic Summarization for Cre-ative Writing, pages 3643, Gyeongju, Republic ofKorea. Association for Computational Linguistics.",
  "Kuan-Hao Huang, Varun Iyer, I-Hung Hsu, AnoopKumar, Kai-Wei Chang, and Aram Galstyan. 2023": "ParaAMR: A large-scale syntactically diverse para-phrase dataset by AMR back-translation. In Proceed-ings of the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 80478061, Toronto, Canada. Association forComputational Linguistics. Kuan-Hao Huang, Varun Iyer, Anoop Kumar, SriramVenkatapathy, Kai-Wei Chang, and Aram Galstyan.2022. Unsupervised syntactically controlled para-phrase generation with Abstract Meaning Represen-tations. In Findings of the Association for Computa-tional Linguistics: EMNLP 2022, pages 15471554,Abu Dhabi, United Arab Emirates. Association forComputational Linguistics. Marcio Incio and Thiago Pardo. 2021. Semantic-basedopinion summarization. In Proceedings of the Inter-national Conference on Recent Advances in NaturalLanguage Processing (RANLP 2021), pages 619628,Held Online. INCOMA Ltd. Marcio Lima Incio,Marco Antonio SobrevillaCabezudo, Renata Ramisch, Ariani Di Felippo, andThiago Alexandre Salgueiro Pardo. 2022. The AMR-PT corpus and the semantic annotation of challengingsentences from journalistic and opinion texts. Sci-ELO Preprints.",
  "Anca-Elena Iordan. 2021. Automatic comprehensionof geometry problems using AMR parser. In Inter-national Conference on Software Engineering andKnowledge Engineering (SEKE), pages 628631": "MdSaifulIslam,AdibaProma,YilinZhou,Syeda Nahida Akter, Caleb Wohn, and EhsanHoque. 2022. KnowUREnvironment: An automatedknowledge graph for climate change and environ-mental issues. In AAAI 2022 Fall Symposium: TheRole of AI in Responding to Climate Challenges. Fuad Issa, Marco Damonte, Shay B. Cohen, XiaohuiYan, and Yi Chang. 2018. Abstract Meaning Repre-sentation for paraphrase detection. In Proceedingsof the 2018 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, Volume 1 (Long Pa-pers), pages 442452, New Orleans, Louisiana. As-sociation for Computational Linguistics. Anubhav Jangra, Preksha Nema, and Aravindan Raghu-veer. 2022. T-STAR: Truthful style transfer usingAMR graph as intermediate representation. In Pro-ceedings of the 2022 Conference on Empirical Meth-ods in Natural Language Processing, pages 88058825, Abu Dhabi, United Arab Emirates. Associationfor Computational Linguistics. Zhijing Jin, Yuen Chen, Fernando Gonzalez Adauto,Jiarui Liu, Jiayi Zhang, Julian Michael, BernhardSchlkopf, and Mona Diab. 2024. Analyzing therole of semantic representations in the era of largelanguage models. In Proceedings of the 2024 Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies (Volume 1: Long Papers), pages",
  ", Mexico City, Mexico. Association forComputational Linguistics": "Zoher Kachwala, Jisun An, Haewoon Kwak, and Fil-ippo Menczer. 2024. REMATCH: Robust and effi-cient matching of local knowledge graphs to improvestructural and semantic similarity. In Findings of theAssociation for Computational Linguistics: NAACL2024, pages 10181028, Mexico City, Mexico. Asso-ciation for Computational Linguistics. Pavan Kapanipathi, Ibrahim Abdelaziz, Srinivas Ravis-hankar, Salim Roukos, Alexander Gray, Ramn Fer-nandez Astudillo, Maria Chang, Cristina Cornelio,Saswati Dana, Achille Fokoue, Dinesh Garg, AlfioGliozzo, Sairam Gurajada, Hima Karanam, NaweedKhan, Dinesh Khandelwal, Young-Suk Lee, YunyaoLi, Francois Luus, Ndivhuwo Makondo, NandanaMihindukulasooriya, Tahira Naseem, Sumit Neelam,Lucian Popa, Revanth Gangi Reddy, Ryan Riegel,Gaetano Rossiello, Udit Sharma, G P Shrivatsa Bhar-gav, and Mo Yu. 2021. Leveraging Abstract Mean-ing Representation for knowledge base question an-swering. In Findings of the Association for Com-putational Linguistics: ACL-IJCNLP 2021, pages38843894, Online. Association for ComputationalLinguistics. Robert T. Kasper. 1989. A flexible interface for link-ing applications to Penmans sentence generator. InSpeech and Natural Language: Proceedings of aWorkshop Held at Philadelphia, Pennsylvania, Febru-ary 21-23, 1989. Jungeun Kim, Jangyeong Jeon, Seungjin Jung, and Jun-yeong Kim. 2024. De-bias using abstract meaningrepresentation for image captioning. In 2024 IEEEInternational Conference on Consumer Electronics(ICCE), pages 14. IEEE.",
  "Aurlien Lamercerie and Annie Foret. 2021.Fromrequirements as AMR-like graphs to automata-basedreasoning. Research report, Universit de Rennes 1": "Paul Landes and Barbara Di Eugenio. 2024. CALAMR:Component ALignment for Abstract Meaning Rep-resentation. In Proceedings of the 2024 Joint In-ternational Conference on Computational Linguis-tics, Language Resources and Evaluation (LREC-COLING 2024), pages 26222637, Torino, Italia.ELRA and ICCL. Fei-Tzin Lee, Miguel Ballesteros, Feng Nan, and Kath-leen McKeown. 2022a. Using structured contentplans for fine-grained syntactic control in pretrainedlanguage model generation. In Proceedings of the29th International Conference on Computational Lin-guistics, pages 58825895, Gyeongju, Republic ofKorea. International Committee on ComputationalLinguistics.",
  "Fei-Tzin Lee, Chris Kedzie, Nakul Verma, and Kath-leen McKeown. 2021.An analysis of documentgraph construction methods for AMR summarization.arXiv preprint arXiv:2111.13993": "Young-Suk Lee, Ramn Astudillo, Hoang Thanh Lam,Tahira Naseem, Radu Florian, and Salim Roukos.2022b. Maximum Bayes Smatch ensemble distilla-tion for AMR parsing. In Proceedings of the 2022Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 53795392, Seattle,United States. Association for Computational Lin-guistics. Bin Li, Yuan Wen, Weiguang Qu, Lijun Bu, and Ni-anwen Xue. 2016. Annotating the little prince withChinese AMRs. In Proceedings of the 10th Linguis-tic Annotation Workshop held in conjunction withACL 2016 (LAW-X 2016), pages 715, Berlin, Ger-many. Association for Computational Linguistics. Changmao Li and Jeffrey Flanigan. 2022. Improvingneural machine translation with the Abstract Mean-ing Representation by combining graph and sequencetransformers. In Proceedings of the 2nd Workshopon Deep Learning on Graphs for Natural LanguageProcessing (DLG4NLP 2022), pages 1221, Seattle,Washington. Association for Computational Linguis-tics.",
  "Irene Li, Linfeng Song, Kun Xu, and Dong Yu. 2022": "Variational graph autoencoding as cheap supervisionfor AMR coreference resolution. In Proceedingsof the 60th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 27902800, Dublin, Ireland. Association forComputational Linguistics. Xiang Li, Thien Huu Nguyen, Kai Cao, and Ralph Gr-ishman. 2015. Improving event detection with Ab-stract Meaning Representation. In Proceedings ofthe First Workshop on Computing News Storylines,pages 1115, Beijing, China. Association for Com-putational Linguistics. Kexin Liao, Logan Lebanoff, and Fei Liu. 2018. Ab-stract Meaning Representation for multi-documentsummarization. In Proceedings of the 27th Inter-national Conference on Computational Linguistics,pages 11781190, Santa Fe, New Mexico, USA. As-sociation for Computational Linguistics.",
  "reasoning. In Proceedings of the 28th InternationalConference on Computational Linguistics, pages24592471, Barcelona, Spain (Online). InternationalCommittee on Computational Linguistics": "Ha Linh and Huyen Nguyen. 2019. A case study onmeaning representation for Vietnamese. In Proceed-ings of the First International Workshop on Design-ing Meaning Representations, pages 148153, Flo-rence, Italy. Association for Computational Linguis-tics. Fei Liu, Jeffrey Flanigan, Sam Thomson, NormanSadeh, and Noah A. Smith. 2015. Toward abstrac-tive summarization using semantic representations.In Proceedings of the 2015 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 10771086, Denver, Colorado. Association forComputational Linguistics. Fukun Ma, Xuming Hu, Aiwei Liu, Yawen Yang,Shuang Li, Philip S. Yu, and Lijie Wen. 2023. AMR-based network for aspect-based sentiment analysis.In Proceedings of the 61st Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 322337, Toronto, Canada.Association for Computational Linguistics. Emma Manning and Nathan Schneider. 2021.Ref-erenceless parsing-based evaluation of AMR-to-English generation. In Proceedings of the 2nd Work-shop on Evaluation and Comparison of NLP Systems,pages 114122, Punta Cana, Dominican Republic.Association for Computational Linguistics. Behrooz Mansouri, Ricardo Campos, and Adam Jatowt.2023.Towards timeline generation with abstractmeaning representation. In Companion Proceedingsof the ACM Web Conference 2023, WWW 23 Com-panion, page 12041207, New York, NY, USA. As-sociation for Computing Machinery. Behrooz Mansouri, Douglas W. Oard, and RichardZanibbi. 2022. Contextualized formula search usingmath abstract meaning representation. In Proceed-ings of the 31st ACM International Conference onInformation & Knowledge Management, CIKM 22,page 43294333, New York, NY, USA. Associationfor Computing Machinery. Mary Martin, Cecilia Mauceri, Martha Palmer, andChristoffer Heckman. 2020.Leveraging non-specialists for accurate and time efficient AMR anno-tation. In Proceedings of the LREC 2020 Workshopon Citizen Linguistics in Language Resource Devel-opment, pages 3539, Marseille, France. EuropeanLanguage Resources Association. AbelardoCarlosMartnezLorenzo,PereLlusHuguet Cabot, and Roberto Navigli. 2023. Cross-lingual AMR aligner: Paying attention to cross-attention. In Findings of the Association for Compu-tational Linguistics: ACL 2023, pages 17261742,Toronto, Canada. Association for Computational Lin-guistics. Abelardo Carlos Martnez Lorenzo, Marco Maru, andRoberto Navigli. 2022. Fully-Semantic Parsing andGeneration: the BabelNet Meaning Representation.In Proceedings of the 60th Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 17271741, Dublin, Ireland.Association for Computational Linguistics. Antonello Meloni, Diego Reforgiato Recupero, andAldo Gangemi. 2017. AMR2FRED, a tool for trans-lating abstract meaning representation to motif-basedlinguistic knowledge graphs. In The Semantic Web:ESWC 2017 Satellite Events, pages 4347, Cham.Springer International Publishing. Noelia Migueles-Abraira, Rodrigo Agerri, and ArantzaDiaz de Ilarraza. 2018. Annotating Abstract Mean-ing Representations for Spanish. In Proceedings ofthe Eleventh International Conference on LanguageResources and Evaluation (LREC 2018), Miyazaki,Japan. European Language Resources Association(ELRA).",
  "Arindam Mitra and Chitta Baral. 2016. Addressing aquestion answering challenge by combining statisti-cal methods with inductive rule learning and reason-ing. volume 30": "Paloma Moreda, Armando Surez, Elena Lloret, EstelaSaquete, and Isabel Moreno. 2018. From sentencesto documents: Extending abstract meaning represen-tation for understanding documents. Procesamientodel Lenguaje Natural, 60:6168. Almuth Mller and Achim Kuwertz. 2022. Evaluationof a semantic search approach based on AMR forinformation retrieval in image exploitation. In 2022Sensor Data Fusion: Trends, Solutions, Applications(SDF), pages 16. Tahira Naseem, Austin Blodgett, Sadhana Kumaravel,Tim OGorman, Young-Suk Lee, Jeffrey Flanigan,Ramn Astudillo, Radu Florian, Salim Roukos, andNathan Schneider. 2022. DocAMR: Multi-sentenceAMR representation and evaluation. In Proceedingsof the 2022 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 34963505,Seattle, United States. Association for ComputationalLinguistics.",
  "Marcel Nawrath, Agnieszka Nowak, Tristan Ratz,Danilo Walenta, Juri Opitz, Leonardo Ribeiro, JooSedoc, Daniel Deutsch, Simon Mille, Yixin Liu, Se-bastian Gehrmann, Lining Zhang, Saad Mahamood,": "Miruna Clinciu, Khyathi Chandu, and Yufang Hou.2024. On the role of summary content units in textsummarization evaluation. In Proceedings of the2024 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies (Volume 2: Short Pa-pers), pages 272281, Mexico City, Mexico. Associ-ation for Computational Linguistics. Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-ing content selection in summarization: The pyramidmethod. In Proceedings of the Human LanguageTechnology Conference of the North American Chap-ter of the Association for Computational Linguistics:HLT-NAACL 2004, pages 145152, Boston, Mas-sachusetts, USA. Association for Computational Lin-guistics. Antonio MS Almeida Neto, Helena M Caseli, andTiago A Almeida. 2020. Dense captioning usingabstract meaning representation. In Intelligent Sys-tems: 9th Brazilian Conference, BRACIS 2020, RioGrande, Brazil, October 2023, 2020, Proceedings,Part I 9, pages 450465. Springer.",
  "Improving neural machine translation with AMR se-mantic graphs. Mathematical Problems in Engineer-ing, 2021": "Takuro Ogawa and Ryosuke Saga. 2023.Inductivemodel using abstract meaning representation for textclassification via graph neural networks. In Interna-tional Conference on Human-Computer Interaction,pages 258271. Springer. Tim OGorman, Michael Regan, Kira Griffitt, Ulf Her-mjakob, Kevin Knight, and Martha Palmer. 2018.AMR beyond the sentence: the multi-sentence AMRcorpus. In Proceedings of the 27th International Con-ference on Computational Linguistics, pages 36933702, Santa Fe, New Mexico, USA. Association forComputational Linguistics.",
  "for Computational Linguistics: Main Volume, pages15041518, Online. Association for ComputationalLinguistics": "Juri Opitz and Anette Frank. 2022a. Better Smatch= better parser? AMR evaluation is not so simpleanymore. In Proceedings of the 3rd Workshop onEvaluation and Comparison of NLP Systems, pages3243, Online. Association for Computational Lin-guistics. Juri Opitz and Anette Frank. 2022b. SBERT studiesmeaning representations: Decomposing sentence em-beddings into explainable semantic features. In Pro-ceedings of the 2nd Conference of the Asia-PacificChapter of the Association for Computational Lin-guistics and the 12th International Joint Conferenceon Natural Language Processing (Volume 1: LongPapers), pages 625638, Online only. Association forComputational Linguistics. Juri Opitz, Philipp Heinisch, Philipp Wiesenbach,Philipp Cimiano, and Anette Frank. 2021b. Explain-able unsupervised argument similarity rating with Ab-stract Meaning Representation and conclusion gener-ation. In Proceedings of the 8th Workshop on Argu-ment Mining, pages 2435, Punta Cana, DominicanRepublic. Association for Computational Linguistics.",
  "Martha Palmer, Paul Kingsbury, and Daniel Gildea.2005. The proposition bank: An annotated corpusof semantic roles. Computational Linguistics, 31:71106": "Xiaoman Pan, Taylor Cassidy, Ulf Hermjakob, HengJi, and Kevin Knight. 2015. Unsupervised entitylinking with Abstract Meaning Representation. InProceedings of the 2015 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 11301139, Denver, Colorado. Association forComputational Linguistics. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalu-ation of machine translation. In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics, pages 311318, Philadelphia,Pennsylvania, USA. Association for ComputationalLinguistics. Jinwoo Park, Hosoo Shin, Dahee Jeong, and JunyeongKim. 2024. Improving the representation of sen-tences with reinforcement learning and AMR graph.In 2024 IEEE International Conference on ConsumerElectronics (ICCE), pages 14. IEEE. Siyana Pavlova, Maxime Amblard, and Bruno Guil-laume. 2023. Structural and global features for com-paring semantic representation formalisms. In Pro-ceedings of the Fourth International Workshop onDesigning Meaning Representations, pages 112,Nancy, France. Association for Computational Lin-guistics. Hai Pham, Isma Hadji, Xinnuo Xu, Ziedune Degutyte,Jay Rainey, Evangelos Kazakos, Afsaneh Fazly,Georgios Tzimiropoulos, and Brais Martinez. 2024.Graph guided question answer generation for pro-cedural question-answering. In Proceedings of the18th Conference of the European Chapter of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 25012525, St. Julians, Malta.Association for Computational Linguistics. Claire Benet Post, Marie C. McGregor, Maria LeonorPacheco, and Alexis Palmer. 2024.AcceleratingUMR adoption: Neuro-symbolic conversion fromAMR-to-UMR with low supervision. In Proceedingsof the Fifth International Workshop on DesigningMeaning Representations @ LREC-COLING 2024,pages 140150, Torino, Italia. ELRA and ICCL. Jakob Prange, Nathan Schneider, and Lingpeng Kong.2022. Linguistic frameworks go toe-to-toe at neuro-symbolic language modeling. In Proceedings of the2022 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 43754391, Seat-tle, United States. Association for ComputationalLinguistics. Haoyi Qiu, Kung-Hsiang Huang, Jingnong Qu, andNanyun Peng. 2024. AMRFact: Enhancing summa-rization factuality evaluation with AMR-driven nega-tive samples generation. In Proceedings of the 2024Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies (Volume 1: Long Papers),pages 594608, Mexico City, Mexico. Associationfor Computational Linguistics.",
  "Abstract Meaning Representation. In BioNLP 2017,pages 126135, Vancouver, Canada,. Association forComputational Linguistics": "Michael Regan, Shira Wein, George Baker, and EmilioMonti. 2024. MASSIVE multilingual Abstract Mean-ing Representation: A dataset and baselines for hallu-cination detection. In Proceedings of the 13th JointConference on Lexical and Computational Seman-tics (*SEM 2024), pages 117, Mexico City, Mexico.Association for Computational Linguistics. Nils Reimers and Iryna Gurevych. 2019.Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processingand the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), pages39823992, Hong Kong, China. Association for Com-putational Linguistics. Leonardo F. R. Ribeiro, Mengwen Liu, Iryna Gurevych,Markus Dreyer, and Mohit Bansal. 2022. FactGraph:Evaluating factuality in summarization with semanticgraph representations. In Proceedings of the 2022Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 32383253, Seattle,United States. Association for Computational Lin-guistics. Leonardo F. R. Ribeiro, Jonas Pfeiffer, Yue Zhang, andIryna Gurevych. 2021a. Smelting gold and silver forimproved multilingual AMR-to-Text generation. InProceedings of the 2021 Conference on EmpiricalMethods in Natural Language Processing, pages 742750, Online and Punta Cana, Dominican Republic.Association for Computational Linguistics. Leonardo F. R. Ribeiro, Martin Schmitt, HinrichSchtze, and Iryna Gurevych. 2021b. Investigatingpretrained language models for graph-to-text genera-tion. In Proceedings of the 3rd Workshop on NaturalLanguage Processing for Conversational AI, pages211227, Online. Association for Computational Lin-guistics. Mrinmaya Sachan and Eric Xing. 2016. Machine com-prehension using rich semantic representations. InProceedings of the 54th Annual Meeting of the As-sociation for Computational Linguistics (Volume 2:Short Papers), pages 486492, Berlin, Germany. As-sociation for Computational Linguistics. Zacchary Sadeddine, Juri Opitz, and Fabian Suchanek.2024. A survey of meaning representations fromtheory to practical utility. In Proceedings of the 2024Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies (Volume 1: Long Papers),pages 28772892, Mexico City, Mexico. Associationfor Computational Linguistics.",
  "Yuichiro Sawai, Hiroyuki Shindo, and Yuji Matsumoto.2015. Semantic structure analysis of noun phrases": "using Abstract Meaning Representation. In Proceed-ings of the 53rd Annual Meeting of the Associationfor Computational Linguistics and the 7th Interna-tional Joint Conference on Natural Language Pro-cessing (Volume 2: Short Papers), pages 851856,Beijing, China. Association for Computational Lin-guistics. Nikolaus Schrack, Ruixiang Cui, Hugo Lpez, andDaniel Hershcovich. 2022. Can AMR assist legaland logical reasoning? In Findings of the Associa-tion for Computational Linguistics: EMNLP 2022,pages 15551568, Abu Dhabi, United Arab Emirates.Association for Computational Linguistics. Verena Severina and Masayu Leylia Khodra. 2019. Mul-tidocument abstractive summarization using abstractmeaning representation for indonesian language. In2019 International Conference of Advanced Informat-ics: Concepts, Theory and Applications (ICAICTA),pages 16. Kaize Shi, Xueyao Sun, Li He, Dingxian Wang, Qing Li,and Guandong Xu. 2023. AMR-TST: Abstract Mean-ing Representation-based text style transfer. In Find-ings of the Association for Computational Linguis-tics: ACL 2023, pages 42314243, Toronto, Canada.Association for Computational Linguistics.",
  "Kanchan Shivashankar, Khaoula Benmaarouf, and Na-dine Steinmetz. 2022. From graph to graph: Amr tosparql": "Ziyi Shou, Yuxin Jiang, and Fangzhen Lin. 2022. AMR-DA: Data augmentation by Abstract Meaning Rep-resentation. In Findings of the Association for Com-putational Linguistics: ACL 2022, pages 30823098,Dublin, Ireland. Association for Computational Lin-guistics. Ziyi Shou and Fangzhen Lin. 2023. Evaluate AMRgraph similarity via self-supervised learning. In Pro-ceedings of the 61st Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 1611216123, Toronto, Canada. As-sociation for Computational Linguistics. Marco Antonio Sobrevilla Cabezudo and Thiago Pardo.2019. Towards a general Abstract Meaning Repre-sentation corpus for Brazilian Portuguese. In Pro-ceedings of the 13th Linguistic Annotation Workshop,pages 236244, Florence, Italy. Association for Com-putational Linguistics.",
  "Linfeng Song, Daniel Gildea, Yue Zhang, Zhiguo Wang,and Jinsong Su. 2019.Semantic neural machinetranslation using AMR. Transactions of the Associa-tion for Computational Linguistics, 7:1931": "Linfeng Song, Ante Wang, Jinsong Su, Yue Zhang, KunXu, Yubin Ge, and Dong Yu. 2020. Structural infor-mation preserving for graph-to-text generation. InProceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 79877998, Online. Association for Computational Lin-guistics. Linfeng Song, Yue Zhang, Zhiguo Wang, and DanielGildea. 2018. A graph-to-sequence model for AMR-to-text generation. In Proceedings of the 56th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 16161626,Melbourne, Australia. Association for ComputationalLinguistics. Katharina Stein, Lucia Donatelli, and Alexander Koller.2023.From sentence to action: Splitting AMRgraphs for recipe instructions. In Proceedings of theFourth International Workshop on Designing Mean-ing Representations, pages 5267, Nancy, France.Association for Computational Linguistics.",
  "Reza Takhshid, Razieh Shojaei, Zahra Azin, and Mo-hammad Bahrani. 2022. Persian Abstract MeaningRepresentation. arXiv preprint arXiv:2205.07712": "Christopher Tam, Richard Brutti, Kenneth Lai, andJames Pustejovsky. 2023. Annotating situated ac-tions in dialogue. In Proceedings of the Fourth Inter-national Workshop on Designing Meaning Represen-tations, pages 4551, Nancy, France. Association forComputational Linguistics. Jingxuan Tu, Timothy Obiso, Bingyang Ye, Kyeong-min Rim, Keer Xu, Liulu Yue, Susan WindischBrown, Martha Palmer, and James Pustejovsky. 2024.GLAMR: Augmenting AMR with GL-VerbNet eventstructure.In Proceedings of the 2024 Joint In-ternational Conference on Computational Linguis-tics, Language Resources and Evaluation (LREC-COLING 2024), pages 77467759, Torino, Italy.ELRA and ICCL.",
  "Zdenka Ureov, Jan Hajic, and Ondrej Bojar. 2014": "Comparing Czech and English AMRs.In Pro-ceedings of Workshop on Lexical and GrammaticalResources for Language Processing, pages 5564,Dublin, Ireland. Association for Computational Lin-guistics and Dublin City University. Jens E. Van Gysel, Meagan Vigus, Jayeol Chun, Ken-neth Lai, Sarah Moeller, Jiarui Yao, Tim OGorman,Andrew Cowell, William Croft, Chu-Ren Huang,Jan Hajic, James H. Martin, Stephan Oepen, MarthaPalmer, James Pustejovsky, Rosa Vallejos, and Ni-anwen Xue. 2021. Designing a uniform meaning",
  "representation for natural language processing. KI -Knstliche Intelligenz": "PavloVasylenko,PereLlusHuguetCabot,Abelardo Carlos Martnez Lorenzo, and RobertoNavigli. 2023.Incorporating graph informationin transformer-based AMR parsing.In Findingsof the Association for Computational Linguistics:ACL 2023, pages 19952011, Toronto, Canada.Association for Computational Linguistics. Martin Verrev. 2023. Evaluation of semantic parsingframeworks for automated knowledge base construc-tion. In Intelligent Systems Design and Applications,pages 554563, Cham. Springer Nature Switzerland. Trong Sinh Vu and Minh Le Nguyen. 2019. An empiri-cal evaluation of AMR parsing for legal documents.In New Frontiers in Artificial Intelligence: JSAI-isAI 2018 Workshops, JURISIN, AI-Biz, SKL, LENLS,IDAA, Yokohama, Japan, November 1214, 2018,Revised Selected Papers, pages 131145. Springer.",
  "Abstract meaning representation for legal documents:an empirical research on a human-annotated dataset.Artificial Intelligence and Law, 30(2):221243": "Cunxiang Wang, Zhikun Xu, Qipeng Guo, XiangkunHu, Xuefeng Bai, Zheng Zhang, and Yue Zhang.2023. Exploiting Abstract Meaning Representationfor open-domain question answering. In Findings ofthe Association for Computational Linguistics: ACL2023, pages 20832096, Toronto, Canada. Associa-tion for Computational Linguistics. Yanshan Wang, Sijia Liu, Majid Rastegar-Mojarad, Li-wei Wang, Feichen Shen, Fei Liu, and HongfangLiu. 2017. Dependency and AMR embeddings fordrug-drug interaction extraction from biomedical lit-erature. In Proceedings of the 8th ACM InternationalConference on Bioinformatics, Computational Biol-ogy,and Health Informatics, ACM-BCB 17, page3643, New York, NY, USA. Association for Com-puting Machinery. Shira Wein, Lucia Donatelli, Ethan Ricker, Calvin En-gstrom, Alex Nelson, Leonie Harter, and NathanSchneider. 2022. Spanish Abstract Meaning Repre-sentation: Annotation of a general corpus. In North-ern European Journal of Language Technology, Vol-ume 8, Copenhagen, Denmark. Northern EuropeanAssociation of Language Technology. Shira Wein and Nathan Schneider. 2022. Accountingfor language effect in the evaluation of cross-lingualAMR parsers.In Proceedings of the 29th Inter-national Conference on Computational Linguistics,pages 38243834, Gyeongju, Republic of Korea. In-ternational Committee on Computational Linguistics.",
  "Shira Wein, Zhuxin Wang, and Nathan Schneider. 2023": "Measuring fine-grained semantic equivalence withAbstract Meaning Representation. In Proceedings ofthe 15th International Conference on ComputationalSemantics, pages 144154, Nancy, France. Associa-tion for Computational Linguistics. Runxin Xu, Peiyi Wang, Tianyu Liu, Shuang Zeng,Baobao Chang, and Zhifang Sui. 2022. A two-streamAMR-enhanced model for document-level event ar-gument extraction. In Proceedings of the 2022 Con-ference of the North American Chapter of the As-sociation for Computational Linguistics: HumanLanguage Technologies, pages 50255036, Seattle,United States. Association for Computational Lin-guistics. Weiwen Xu, Huihui Zhang, Deng Cai, and Wai Lam.2021. Dynamic semantic graph construction and rea-soning for explainable multi-hop science questionanswering. In Findings of the Association for Com-putational Linguistics: ACL-IJCNLP 2021, pages10441056, Online. Association for ComputationalLinguistics.",
  "Zhiyang Xu, Jay Yoon Lee, and Lifu Huang. 2023": "Learning from a friend: Improving event extrac-tion via self-training with feedback from AbstractMeaning Representation. In Findings of the Asso-ciation for Computational Linguistics: ACL 2023,pages 1042110437, Toronto, Canada. Associationfor Computational Linguistics. Nianwen Xue, Ondrej Bojar, Jan Hajic, Martha Palmer,Zdenka Ureov, and Xiuhong Zhang. 2014. Not aninterlingua, but close: Comparison of English AMRsto Chinese and Czech. In Proceedings of the Ninth In-ternational Conference on Language Resources andEvaluation (LREC14), pages 17651772, Reykjavik,Iceland. European Language Resources Association(ELRA). Bohao Yang, Kun Zhao, Chen Tang, Liang Zhan, andChenghua Lin. 2024. Structured information matters:Incorporating abstract meaning representation intollms for improved open-domain dialogue evaluation.arXiv preprint arXiv:2404.01129. Yuqing Yang, Qipeng Guo, Xiangkun Hu, Yue Zhang,Xipeng Qiu, and Zheng Zhang. 2023. An AMR-based link prediction approach for document-levelevent argument extraction. In Proceedings of the 61stAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 1287612889, Toronto, Canada. Association for Computa-tional Linguistics. Shaowei Yao, Tianming Wang, and Xiaojun Wan.2020. Heterogeneous graph transformer for graph-to-sequence learning. In Proceedings of the 58th AnnualMeeting of the Association for Computational Lin-guistics, pages 71457154, Online. Association forComputational Linguistics.",
  "Dongran Yu, Bo Yang, Dayou Liu, Hui Wang, andShirui Pan. 2023. A survey on neural-symbolic learn-ing systems. Neural Networks, 166:105126": "Zixuan Zhang and Heng Ji. 2021. Abstract MeaningRepresentation guided graph encoding and decodingfor joint information extraction. In Proceedings ofthe 2021 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 3949, Online.Association for Computational Linguistics. Zixuan Zhang, Nikolaus Parulian, Heng Ji, AhmedElsayed, Skatje Myers, and Martha Palmer. 2021.Fine-grained information extraction from biomedi-cal literature based on knowledge-enriched AbstractMeaning Representation. In Proceedings of the 59thAnnual Meeting of the Association for ComputationalLinguistics and the 11th International Joint Confer-ence on Natural Language Processing (Volume 1:Long Papers), pages 62616270, Online. Associationfor Computational Linguistics. Qihui Zhao, Tianhan Gao, and Nan Guo. 2023.Document-level relation extraction based on sememeknowledge-enhanced abstract meaning representa-tion and reasoning. Complex & Intelligent Systems,9(6):65536566.",
  "(r / render-01(a1 / ARG0(j / jury))(a2 / ARG1(v / verdict(m / mod(g / guilty)))))": ": Three equivalency-preserving AMR transfor-mations for The jury rendered a guilty verdict. Left/-Top: Standard AMR. Middle/Middle: Reification withAMR rules. Right/Bottom: Bipartite Levi Graph withunlabeled edges. While Levi Graphs are not AMR-specific, reification is. Per the AMR guidelines (Ba-narescu et al., 2019), any labeled edge not in a stan-dardized set (:opN, :argN, etc.) is generalized to a newstructure, where the old edge assumes the position of anode linked with :opN/:argN to the original structure(in the example, :mod triggers the node have-mod-91with arguments :ARG1 and :ARG2)."
}