{
  "Abstract": "Personalization in large language models(LLMs) is increasingly important, aiming toalign the LLMs interactions, content, and rec-ommendations with individual user preferences.Recent advances have highlighted effectiveprompt design by enriching user queries withnon-parametric knowledge through behaviorhistory retrieval and textual profiles. However,these methods faced limitations due to a lackof model ownership, resulting in constrainedcustomization and privacy issues, and oftenfailed to capture complex, dynamic user behav-ior patterns. To address these shortcomings, weintroduce One PEFT Per User (OPPU)1, em-ploying personalized parameter-efficient fine-tuning (PEFT) modules to store user-specificbehavior patterns and preferences. By pluggingin personal PEFT parameters, users can ownand use their LLMs individually. OPPU inte-grates parametric user knowledge in the per-sonal PEFT parameters with non-parametricknowledge from retrieval and profiles, adaptingLLMs to user behavior shifts. Experimentalresults demonstrate that OPPU significantlyoutperforms existing prompt-based methodsacross seven diverse tasks in the LaMP bench-mark. Further studies reveal OPPUs enhancedcapabilities in handling user behavior shifts,modeling users at different activity levels, main-taining robustness across various user historyformats, and displaying versatility with differ-ent PEFT methods.",
  "History": ": LLM ownership and behavior shift are twochallenges that developing personalized LLMs has toface. Ownership emphasizes that the model needs tobe owned by individual user to enhance customizationand privacy. Behavior shift adaption refers to the LLMsability to effectively generalize and adapt to emergingnew patterns in user behaviors. creasingly getting vital in areas like content recom-mendation (Qian et al., 2013; Wu et al., 2023; Baeket al., 2023), user simulation (Dejescu et al., 2023),personalized chatbots (Srivastava et al., 2020; Maet al., 2021), user profiling (Gu et al., 2020; Gaoet al., 2023), healthcare (Goldenberg et al., 2021),and education (Pratama et al., 2023).Large language models (LLMs) display emer-gent abilities not seen in smaller models (Wei et al.,2022; Lu et al., 2023), as they have billions of pa-rameters and are trained on vast corpora. However,existing LLMs predominantly follow the one-size-fits-all paradigm. They are generally trained onextensive, domain-agnostic datasets, which limitstheir effectiveness in meeting the specific needs andpreferences of individual users (Chen et al., 2023).Therefore, the challenge of integrating the stronggenerative capabilities of LLMs with the tailoredrequirements of individual users has emerged as asignificant area of research (Li et al., 2023).Existing works on personalizing LLMs have pre-dominantly concentrated on developing prompttemplates, which fall into three categories: vanilla,retrieval-augmented, and profile-augmented per-sonalized prompts.The vanilla personalized prompt approach leverages the in-context learningcapability of LLMs, utilizing the users entire orrandomly sampled history as contextual examples(Dai et al., 2023; Zhiyuli et al., 2023). Consideringthe growing length of user behavior history and thelimited LLM context length, some studies appliedretrieval methods to select the most relevant partof user behavior history to enhance LLM personal-ization (Mysore et al., 2023). Besides the retrieval,some techniques explicitly generate user prefer-ences and profiles in natural language to augmentLLMs input (Richardson et al., 2023).Despite much research progress has been madein LLM personalization, existing methods faceownership and behavior shift challenges (): Ownership: Existing methods are processed cen-tralized, where user history is encoded in a per-sonalized prompt and processed by centralizedLLMs. This paradigm limits the models cus-tomization and ability to provide deep, person-alized experiences tailored to individual users.Moreover, when using a centralized model, usersoften have to share personal data with the serviceprovider, which raises concerns about how userdata are stored, used, and protected. Behavior Pattern Generalization: As is re-vealed by Shi et al. (2023), LLMs can be easilydistracted by irrelevant context information thatretrieval can hardly avoid. In LLM personaliza-tion, where the retrieval corpus is confined to aspecific users behaviors, retrieval augmentationmight underperform, especially when the userspast behaviors do not closely mirror the patternsneeded for the query at hand. In light of these challenges, we propose OnePEFT Per User (OPPU), equipping each userwith a personalized, parameter-efficient fine-tuning(PEFT) module. Characterized by PEFTs plug-and-play functionality and the minimal weight ofupdated parameters (typically less than 1% of thebase LLM), OPPU facilitates LLM ownership andenhances generalization in scenarios of user be-havior shifts. By fine-tuning the PEFT modulewith the users personal behavior history, the per-sonalized PEFT parameters encapsulate behaviorpatterns and preferences. This process, when in-tegrated into base LLMs, allows users to obtaintheir private LLMs, ensuring LLM ownership andenhancing model customization. Furthermore, as isrevealed by Gupta et al. (2024), fine-tuning LLMs is more effective than retrieval augmentation whenthe retrieved instances are not highly relevant tothe query. The fine-tuned personal LLMs in OPPUare adept at capturing complex behavior patternsand thus capable of understanding new behaviorswith less reliance on highly relevant history data.Experimental results show that OPPU outperformsall baselines on seven public tasks in the Lan-guage Model Personalization (LaMP) benchmark(Salemi et al., 2023). Additional studies empha-size the importance of integrating non-parametricuser knowledge from retrieved history with para-metric knowledge from personal PEFT parameters.In scenarios of user behavior shifts, where historyis less relevant, OPPU significantly outperformsretrieval-based methods. Moreover, OPPU is re-silient to varying user history formats and demon-strates versatility across different PEFT methods,among other advantages.To summarize, the contribution of OPPU liesin its pioneering approach to PEFT-based LLMpersonalization. Each user (or user cohort) bene-fits from a personal PEFT module, which not onlyensures LLM ownership but also significantly im-proves the models ability to adapt to shifts in userbehavior. The superiority of OPPU is evidencedby state-of-the-art performance across seven tasksin the LaMP benchmark. By introducing this inno-vative parametric-based personalization technique,OPPU opens up new opportunities in democratiz-ing personalized LLMs.",
  "Research Problem Formulation": "For personalizing LLMs at time t, the output ru foruser u is conditioned on both input qu and the usersbehavior history Hu. Specifically, Hu = {hu},includes all user behaviors hu before time t. Userbehavior hu may consist of (xu, yu) pairs, aligningwith the task-specific query-answer format (qu, ru),or plain text sequences xu providing context forbehavior patterns. We aim to obtain personalizedparameters u for each user u.",
  "User Input": ": Overview of our proposed OPPU, where each user is equipped with a personal PEFT module and plug-inbase LLMs to get their individual LLM. Beyond parametric personalization via PEFT, OPPU is also compatiblewith the non-parametric user knowledge via retrieval and profile augmentation. for personalization tasks without involving targetuser preferences. Specifically, we provide threealternatives: base LLM (B) that only involvestask related data, retrieval-augmented base LLM(R) that augment input with top-k relevant userhistory, and profile-augmented base LLM (P) thatinvolves textual user profiles as input. Note thatintroducing RAG and PAG means users would ex-pose their historical data or profiles to a centralizedLLM, potentially affect the model ownership. Forusers prioritizing privacy and ownership, OPPUwithout retrieval avoids revealing user data to ser-vice providers. Conversely, those seeking optimalperformance and consent to reveal data to central-ized LLMs should opt for RAG or PAG. The fine-tuning objectives of three base models are:",
  "LP =CE[(P)(p(qu, Du, su)), ru],": "where CE denotes the cross entropy loss func-tion, t, r, and p denote prompt constructionfunction for base, retrieval-augmented, and profile-augmented LLM. The retrieved user history Du =R(qu, Hu, k) denotes the top-k user history fromretriever R. su = LLM(Hu) is a textual user pro-file generated by an instruction-tuned LLM, e.g.,Vicuna (Chiang et al., 2023), based on user history.To make this process more computationally ef-ficient, we adopt the low-rank adaptation (LoRA)(Hu et al., 2021) for base LLM task adaption thatonly updates about 0.5% external parameters com-pared to the total LLM parameter size. After train-ing, LoRA parameters are merged into the basemodel, equipping LLMs with task capabilities.",
  "One PEFT Per User (OPPU)": "Once the base model for task adaption is obtained,users can only access the base model parametersand their personal behavior history data, controllingprivacy risks. This section introduces personalizedLLMs for target users through parametric PEFTand integrates non-parametric knowledge such asretrieval and profile augmentation. For each user,we plug a personal trainable PEFT module (LoRAby default) (B)u , (R)u , (P)uto correspond-ing base LLM under three settings to obtain per-sonalized LLM (B)u , (R)u , and (P)u , while baseLLM parameters (B), (R), (P) are frozen.",
  "L(P)u=CE[(P)u (p(xu, D<t(xu)u), su), yu],": "where D<t(xu)u= R(t(xu), H<t(xu)u, k), H<t(xu)uis restricted to user us past behavior history thatoccurred before xu.User behavior history often does not align neatlywith the query format. For example, in personal-ized tweet paraphrasing tasks, where the input is atext sequence qu and the output is the paraphrased tweet ru, the history Hu only includes historicaltweets. In scenarios where user history does not di-rectly aligned with the specific task format, denotedas Hu = {xu}, we replace the user history outputyu in personal PEFT training objectives L(B)u , L(R)u ,L(P)uwith right-shifted history xu for unsupervisednext token prediction.By optimizing personal PEFT parameters withthe objectives mentioned above, OPPU comprehen-sively capture the user behavior patterns in PEFTparameters (B)u , (R)u , (P)u , creating per-sonalized LLMs owned by users. We envision theproposed OPPU as a versatile LLM personaliza-tion framework, where each user possesses theirown PEFT parameters that contain personal behav-ior history and preferences. By plugging their per-sonal PEFT parameters into the base LLMs, userscan get their personalized LLMs, while achievinga better understanding and generalization of userspreferences from the parametric dimension.",
  "Experimental Settings": "DatasetsWe use data from the Large LanguageModel Personalization (LaMP) benchmark (Salemiet al., 2023), which includes seven public languagemodel personalization tasks: four classificationtasks and three generation tasks.2 To promote LLMownership, we emphasize the need for users to con-tribute extensive historical data for personalizingtheir model. Therefore, we focus on the most activeusers, selecting 100 users with the longest historylogs from the time-based dataset version as thetest set, while using all other users for base LLMtraining. Dataset statistics are presented in . BaselinesWe compare our proposed OPPU withthe non-personalized baseline and the retrieval-augmented (RAG) and profile-augmented (PAG)LLM personalization methods. For all baselinesand OPPU, we choose one of the most widelyadopted open-source LLM Llama-2-7B (Touvronet al., 2023) as our base LLM and take BM25 (Trot-man et al., 2014) for all retrieval operations to en-sure efficient and fair comparison.3",
  "shows the performance on the test set forall seven public tasks in the LaMP benchmark, wehave observations as follows": "OPPU brings universal improvement.Mod-els equipped with OPPU outperform all base-line personalization methods across all seventasks. Notably, in personalized classification tasks,OPPU achieves an average relative improvementof 17.38% in MAE and 8.89% in RMSE for per-sonalized product rating prediction. Additionally,it shows an 11.87% improvement in accuracy and7.56% in F1-score for personalized movie tagging.For personalized text generation tasks, OPPU en-hances ROUGE-1 and ROUGE-L scores by 3.42%and 3.87%, respectively, in personalized scholarlytitle generation. Integrating non-parametric and parametricknowledge performs the best.CombiningOPPUs parametric knowledge stored in PEFTparameters and the non-parametric in retrieveditems and user profiles, results in notable perfor-mance gains. For instance, averaging across allseven tasks, combining retrieval in OPPU willbring 1.93% and 2.48% relative improvement com-pared with the non-retrieval and non-OPPU yetretrieval version model, respectively. Moreover,integrating OPPU with user profiles would alsobring 4.56% and 7.18% performance gain againstnon-profile and non-OPPU versions, respectively.Overall, combining non-parametric retrieval andprofile knowledge with parametric PEFT knowl-edge in OPPU delivers the best performance. Performance w.r.t. difference between task andhistory format.In tasks like personalized cita-tion identification, there is a notable discrepancybetween the user history format and the task itself.Here, the user history comprises the users publica-tion history, while the task involves binary classi-fication to identify the correct citation paper. Thisdisparity is also seen in the personalized tweet para-phrasing task. In these cases, OPPU significantlyenhances performance. Specifically, for personal-ized citation identification, OPPU increases accu- : Main experiment results on the LaMP benchmark. R-1 and R-L denote ROUGE-1 and ROUGE-L,respectively. k refers to the number of retrieved items, with k = 0 indicating no retrieval. indicates that highervalues are better, and implies lower values are preferable. For each task, the best score is in bold and the secondbest is underlined. indicates significant improvement against counterparts without OPPU.",
  "racy by 3.48% and F1-score by 3.52%, thanks topersonalized context knowledge provided throughpersonal PEFT": "The more retrieved items, the better perfor-mance.Our experimental results generally in-dicate that an increase in the number of retrieveditems correlates with improved performance. How-ever, we also observe that some data points dontfit this trend, and we hypothesize that this inconsis-tency may arise from the retrieved items introduc-ing noise and irrelevant behavior patterns, poten-tially complicating the models process of under-standing user preferences.",
  "Performance under User Behavior ShiftRe-cent studies have shown that retrieval-augmentedgeneration methods tend to underperform when theretrieved corpus does not contain highly relevant": ": Performance under user behavior shift, wherewe remove the user behavior history highly similar tothe query at hand. k denotes the number of retrievedhistory items, and k = 0 means non-retrieval. Armedwith irrelevant user history, the retrieval-only methodfalls short and performs close to the non-personalizedbaseline, while OPPU shows stronger generalizabilityin the user behavior shift scenario.",
  "full.527.474.571.521.539.483.579.533irrelevant.543.495.528.482.563.523": "documents (Shi et al., 2023; Gupta et al., 2024).This problem is common in personalization con-texts where the users behavior history does notclosely match their current queries. To simulatethis scenario, we use DeBERTa-v3 (He et al., 2022)to extract features from the users historical behav-iors and current query, computing cosine similarityto assess relevance. We then rank the historical be-haviors and select the top 100 items with the lowestrelevance scores as irrelevant user history. shows that limiting user history to lessrelevant items significantly reduces the perfor-mance of retrieval-based methods, often aligningwith non-personalized approaches.In contrast,OPPU demonstrates stronger robustness and gen- 1020304050> 50",
  ": Model performance on personalized movietagging and personalized tweet paraphrasing for userswith different numbers of behavior history": "eralization to less relevant history, even outper-forming models trained with all user history items.Additionally, the combination of parametric andnon-parametric knowledge (OPPU, k=1) enhancesrobustness in personalized text generation tasks,while models using only parametric knowledge(OPPU, k=0) perform better in personalized textclassification tasks. Modeling Users with Different Active LevelsIn our main experiment, we focus on highly ac-tive users. However, many users exhibit loweractivity levels, resulting in shorter behavior histo-ries. To examine the impact of user activity levelson model performance, we randomly selected 20users from each activity range. showsthat LLMs equipped with OPPU consistently out-perform baseline methods across various activitylevels. Key observations include: 1) The longer theuser history, the more pronounced the superiorityof retrieval + OPPU over baselines. 2) Includ-ing non-parametric user knowledge via retrievalimproves performance compared to methods with-out retrieval. 3) Integrating parametric knowledgein OPPU with non-parametric knowledge fromretrieval yields the strongest performance acrossdifferent user activity levels. Performance w.r.t. Retrieved History Items kIn this study, we alter the number of retrieved itemsof both retrieval-only baseline and retrieval+OPPUto gain a better understanding of the integrationof non-parametric and parametric user knowledge. illustrates that as we increase the num-ber of retrieved historical behavior items, both the # retrieved items k 0.5 0.6 0.7",
  ".425.489.430.492.429.505.445.519.442.522.457": "retrieval-only baselines and the retrieval+OPPUapproaches show improved performance. Interest-ingly, we observe that as the number of retrieveditems k becomes larger, the performance differencebetween the retrieval-only and retrieval+OPPU nar-rows. This trend could be attributed to the longerlogs of user behavior history in non-parametricprompts, which reduce the gap between the compre-hensive user behavior history encapsulated in per-sonalized PEFT parameters and the non-parametricuser knowledge included in the prompts. Robustness against Task FormatsOur mainresults demonstrate that OPPU significantly im-proves performance even when the user historycorpus does not strictly follow the task format. Wetested this robustness by ablating the history for-mat in personalized movie tagging (LaMP-2M) andpersonalized scholarly title generation (LaMP-5)tasks, covering both text classification and gener-ation categories. In both tasks, each user historyitem consists of input and output aligned with theuser query xu and output yu. We ablated historybehavior items from the input and output sides,comparing them with the retrieval baseline to testOPPUs robustness against mismatched formats.",
  "Scholarly Title Generation": "0.48 0.42 0.49 0.44 0.48 0.43 0.52 0.44 w/o PEFTPrompt Tuning (IA)3 LoRA ROUGE-1ROUGE-L 0.4 0.5 0.6 0.51 0.44 0.53 0.47 0.52 0.46 0.52 0.46 w/o PEFTPrompt Tuning (IA)3 LoRA : Performance of OPPU on personalized movietagging and personalized scholarly title generation taskswhen equipped with different PEFT methods. We findthat a larger proportion of trainable parameters generallyresults in better personalization performance. Shown in , OPPU achieves performanceclose to that with full history in the text generationtask, even with incomplete user behavior history.In news categorization, LLMs struggle with onlyparametric knowledge, but integrating retrieval aug-mentation, OPPU shows robust performance, out-performing models tuned on complete user his-tory data. Overall, results reveal that combiningnon-parametric and parametric knowledge makesOPPU robust to different user history formats. On PEFT Method ChoicesWe propose OPPUas a versatile PEFT-based LLM personalizationframework compatible with various PEFT meth-ods. This study evaluates OPPUs performanceacross different PEFT approaches, including LoRA,prompt tuning, and (IA)3, which plug in externallearnable parameters in the embedding space andscale the attention factor, respectively. As shownin , OPPU enhances performance with allthree PEFT types, demonstrating its effectivenessand versatility. Notably, LoRA typically delivers the highest performance, followed by (IA)3, andthen prompt tuning. This hierarchy aligns with theproportion of trainable parameters in each method:LoRA at 0.01%, (IA)3 at 0.06%, and prompt tun-ing at 0.001%. These results suggest that a greaternumber of trainable parameters in a personalizedPEFT method generally leads to improved person-alization performance. Case StudyTo illustrate the effectiveness ofOPPU, we conduct a case study on personalizedmovie tagging task for an individual user. shows that the non-personalized method, relyingsolely on query input, ignores user behavior historyand yields incorrect answers. The retrieval-basedmethod, though incorporating user history, failsto retrieve closely matched behaviors to the query,also resulting in errors. We argue that retrieval aug-mentation with a few user history examples cannotfully capture user preferences. In contrast, OPPUuses a personalized PEFT module to effectivelyunderstand the users behavior patterns across theentire user history. In this case, OPPU successfullyrecognizes the users frequent tagging of based ona book\" and provides the correct response. Similarities Between Personalized PEFTsTounderstand how user behavior patterns are reflectedin their private PEFT parameters, we analyze thecosine similarities between these parameters acrossdifferent users, as shown in . We selecttwo representative tasks from text classification andgeneration categories and compute the cosine simi-larities for 100 users PEFT parameters in the testset. The private PEFT similarities generally rangefrom 0.4 to 0.7, with the highest average similari-ties observed in the scholarly title generation task,likely due to its task-specific nature. Relative differ-ences among users offer additional insights: in per-",
  "Personalization of LLMs": "The thrust of existing LLM personalization re-search is centered on designing prompts that in-corporate historical user-generated content and be-havior. These approaches help LLMs understandusers preferences, tailoring responses to individualneeds (Tan and Jiang, 2023; Chen et al., 2023). Theendeavors towards personalized LLMs mainly fallinto three categories: vanilla, retrieval-augmented,and profile-augmented personalized prompts.In the vanilla personalized prompt category, re-searchers use in-context and few-shot learning toencode either complete or a sample of user be-havior history as contextual examples (Liu et al.,2023a; Wang et al., 2023). For instance, Dai et al.(2023) and Kang et al. (2023) encode the userspersonal rating history as few-shot demonstrationexamples. Moreover, some research works (Chris-takopoulou et al., 2023; Zhiyuli et al., 2023) alsodiscovered a long user history would bring bet-ter performance. To manage the growing user be-havior data and LLMs limited context windows,the retrieval-augmented personalized prompt ap-proach has emerged (Salemi et al., 2023; Li et al.,2023). For instance, Pearl (Mysore et al., 2023)proposes a generation-calibrated retriever to se-lect historic user-authored documents for promptaugmentation. Beyond simple retrieval, some re-searchers summarize user preferences and behaviorpatterns into natural language profiles for inputquery augmentation, termed profile-augmented per-sonalized prompts (Liu et al., 2023b; Sun et al.,2024). Richardson et al. (2023) use the instruction- tuned LLMs to generate an abstract summary ofuser history data, augmenting retrieval-based per-sonalization methods. There is also another lineof work focusing on personalized alignment meth-ods via parameter merging (Jang et al., 2023) andpersonalized reward model (Cheng et al., 2023).",
  "Parameter-Efficient Fine-tuning (PEFT)": "With the exponential growth in LLM parameters,fine-tuning all parameters is expensive (Liu et al.,2022b; Xu et al., 2023; Gupta et al., 2024; Jianet al., 2024). To address this, parameter-efficientfine-tuning (PEFT) methods update only a smallnumber of extra parameters while keeping pre-trained weights frozen (He et al., 2021; Fu et al.,2023; Liu et al., 2024; Dou et al., 2024; Zhanget al., 2024). For example, adapter tuning (Houlsbyet al., 2019) injects learnable parameters into eachfeedforward layer, updating only these during fine-tuning. Inspired by discrete textual prompts (Sanhet al., 2022; Wang et al., 2022), prefix tuning (Liand Liang, 2021) and prompt tuning (Lester et al.,2021) optimize prompts and prefixes for specifictasks. LoRA (Hu et al., 2021) adds low-rank matri-ces to approximate parameter updates, and (IA)3 (Liu et al., 2022a) scales activation in the attentionmechanism. These methods achieve performancecomparable to full fine-tuning by updating less than1% of the original parameters, are effective againstcatastrophic forgetting (Pfeiffer et al., 2021), andare robust to out-of-distribution samples (Li andLiang, 2021).Previous works focused on prompt design, lim-ited by model ownership and user behavior shifts.PEFTs small number of updated parameters andplug-and-play nature make it ideal for efficientLLM personalization and model ownership. OPPUintroduces personalization at the parametric levelvia a personal PEFT module, pioneers storing userhistory within personal PEFT parameters, equip-ping each user with a unique, easily integrablePEFT module for model ownership.",
  "Conclusion": "We introduced OPPU, equipping each user with apersonal PEFT module that facilitates model own-ership and generalization under behavior shifts.By tuning these parameters with a users history,OPPU captured personalized behavioral patterns.It integrated non-parametric user knowledge viaretrieval and user profiles, showing superior per-formance across all seven LaMP benchmark tasks.Additional experiments demonstrated OPPUs ver-satility, robustness, and effectiveness for users withvarying activity levels. Our framework paved theway for new opportunities in PEFT-based LLMpersonalization, enhancing LLM modularity foreffective and democratized personalization.",
  "Limitations": "We identify three key limitations in OPPU. Firstly,limited by the dataset, we mainly focus on onespecific task per user rather than examining userbehaviors across multiple tasks and domains. Forexample, in the movie tagging task, users are solelyengaged in that specific activity, without the inclu-sion of behaviors from other areas. Despite this,the OPPU framework is inherently adaptable toany text sequence generation task and is capableof conducting diverse user instructions across dif-ferent tasks and domains. The exploration of LLMpersonalization across a broader range of tasks anddomains remains an area for future investigation.Secondly, OPPU serves as a general frameworkthat incorporates the entirety of a users behaviorhistory into their private PEFT module. However,user interests are dynamic and may display incon-sistencies or conflicts over time. Future researchdirections include examining methodologies for se-lecting the most relevant or valuable items from ausers history and devising strategies to effectivelymanage any discrepancies or conflicts within thishistorical data.",
  "that personal data is handled respectfully and se-curely to prevent any unintended disclosures": "Data BiasPersonalizing LLMs heavily relies onthe personal data fed into the system. If this per-sonal data is biased or unrepresentative, the modelsoutputs could potentially perpetuate these biases,leading to unfair or prejudiced responses. It iscrucial to monitor and mitigate such biases in thepersonal data and the personalized model we ob-tain to ensure that personalized LLMs are fair andharmless in their responses. AccessibilityBy advancing the field of LLM per-sonalization, we aim to enrich user interactionswith AI systems. However, the complexity andresource-intensive nature of LLMs might pose ac-cessibility challenges. Smaller entities or individ-ual researchers with limited computational powerand budgetary constraints might find it difficult toengage with advanced personalized LLMs, poten-tially widening the gap in AI research and applica-tion. It is essential to develop strategies that makepersonalized LLM technologies more accessible toa broader range of users and researchers, ensuringequitable progress in this domain.",
  "Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.2023. Vicuna: An open-source chatbot impressinggpt-4 with 90%* chatgpt quality. See org (accessed 14 April 2023)": "KonstantinaChristakopoulou,AlbertoLalama,Cj Adams, Iris Qu, Yifat Amir, Samer Chucri, PierceVollucci, Fabio Soldo, Dina Bseiso, Sarah Scodel,et al. 2023. Large language models for user interestjourneys. arXiv preprint arXiv:2305.15498. Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu,Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang,and Jun Xu. 2023.Uncovering chatgpts capa-bilities in recommender systems.arXiv preprintarXiv:2305.02182. Cosmina Andreea Dejescu, Lucia V Bel, Iulia Melega,Stefana Maria Cristina Muresan, and Liviu Ioan Oana.2023. Approaches to laparoscopic training in veteri-nary medicine: A review of personalized simulators.Animals, 13(24):3781.",
  "Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong,Haofen Wang, and Jiawei Zhang. 2023.Chat-rec:Towards interactive and explainable llms-augmented recommender system.arXiv preprintarXiv:2303.14524": "Dmitri Goldenberg, Kostia Kofman, Javier Albert, SaraiMizrachi, Adam Horowitz, and Irene Teinemaa. 2021.Personalization in practice: Methods and applica-tions. In Proceedings of the 14th ACM internationalconference on web search and data mining, pages11231126. Yulong Gu, Zhuoye Ding, Shuaiqiang Wang, andDawei Yin. 2020. Hierarchical user profiling fore-commerce recommender systems. In Proceedingsof the 13th International Conference on Web Searchand Data Mining, pages 223231. Aman Gupta, Anup Shirgaonkar, Angels de Luis Bal-aguer, Bruno Silva, Daniel Holstein, Dawei Li, Jen-nifer Marsman, Leonardo O Nunes, Mahsa Rouzbah-man, Morris Sharp, et al. 2024. Rag vs fine-tuning:Pipelines, tradeoffs, and a case study on agriculture.arXiv preprint arXiv:2401.08406. Charles R Harris, K Jarrod Millman, Stfan J VanDer Walt, Ralf Gommers, Pauli Virtanen, David Cour-napeau, Eric Wieser, Julian Taylor, Sebastian Berg,Nathaniel J Smith, et al. 2020. Array programmingwith numpy. Nature, 585(7825):357362. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. 2021. Towards aunified view of parameter-efficient transfer learning.In International Conference on Learning Representa-tions. Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2022.Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding shar-ing. In The Eleventh International Conference onLearning Representations. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,Bruna Morrone, Quentin De Laroussilhe, AndreaGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.Parameter-efficient transfer learning for nlp. In In-ternational Conference on Machine Learning, pages27902799. PMLR. Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,et al. 2021. Lora: Low-rank adaptation of large lan-guage models. In International Conference on Learn-ing Representations. Joel Jang, Seungone Kim, Bill Yuchen Lin, YizhongWang, Jack Hessel, Luke Zettlemoyer, HannanehHajishirzi, Yejin Choi, and Prithviraj Ammanabrolu.2023. Personalized soups: Personalized large lan-guage model alignment via post-hoc parameter merg-ing. arXiv preprint arXiv:2310.11564. Yiren Jian, Tingkai Liu, Yunzhe Tao, Chunhui Zhang,Soroush Vosoughi, and Hongxia Yang. 2024. Ex-pedited training of visual conditioned language gen-eration via redundancy reduction. In Proceedingsof the 62nd Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers,Oral Presentation). Wang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Mah-eswaran Sathiamoorthy, Lichan Hong, Ed Chi, andDerek Zhiyuan Cheng. 2023. Do llms understanduser preferences? evaluating llms on user rating pre-diction. Preprint, arXiv:2305.06474. Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.The power of scale for parameter-efficient prompttuning. In Proceedings of the 2021 Conference onEmpirical Methods in Natural Language Processing,pages 30453059. Cheng Li, Mingyang Zhang, Qiaozhu Mei, YaqingWang, Spurthi Amba Hombaiah, Yi Liang, andMichael Bendersky. 2023. Teach llms to personalizean approach inspired by writing education. arXivpreprint arXiv:2308.07968.",
  "Sheng Lu, Irina Bigoulaeva, Rachneet Sachdeva,Harish Tayyar Madabushi, and Iryna Gurevych.2023.Are emergent abilities in large languagemodels just in-context learning?arXiv preprintarXiv:2309.01809": "Zhengyi Ma, Zhicheng Dou, Yutao Zhu, Hanxun Zhong,and Ji-Rong Wen. 2021. One chatbot per person:Creating personalized chatbots based on implicit userprofiles. In Proceedings of the 44th internationalACM SIGIR conference on research and developmentin information retrieval, pages 555564. Sheshera Mysore,Zhuoran Lu,Mengting Wan,Longqi Yang, Steve Menezes, Tina Baghaee, Em-manuel Barajas Gonzalez, Jennifer Neville, and TaraSafavi. 2023. Pearl: Personalizing large languagemodel writing assistants with generation-calibratedretrievers. arXiv preprint arXiv:2311.09180.",
  "high-performance deep learning library. Advances inneural information processing systems, 32": "Jonas Pfeiffer, Aishwarya Kamath, Andreas Rckl,Kyunghyun Cho,and Iryna Gurevych. 2021.Adapterfusion: Non-destructive task composition fortransfer learning. In Proceedings of the 16th Con-ference of the European Chapter of the Associationfor Computational Linguistics: Main Volume, pages487503. Muh Putra Pratama, Rigel Sampelolo, and Hans Lura.2023. Revolutionizing education: harnessing thepower of artificial intelligence for personalized learn-ing.Klasikal: Journal of Education, LanguageTeaching and Science, 5(2):350357.",
  "Alireza Salemi, Sheshera Mysore, Michael Bendersky,and Hamed Zamani. 2023. Lamp: When large lan-guage models meet personalization. arXiv preprintarXiv:2304.11406": "Victor Sanh, Albert Webson, Colin Raffel, Stephen HBach, Lintang Sutawika, Zaid Alyafeai, AntoineChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,et al. 2022. Multitask prompted training enableszero-shot task generalization. In ICLR 2022-TenthInternational Conference on Learning Representa-tions. Freda Shi, Xinyun Chen, Kanishka Misra, NathanScales, David Dohan, Ed H Chi, Nathanael Schrli,and Denny Zhou. 2023. Large language models canbe easily distracted by irrelevant context. In Inter-national Conference on Machine Learning, pages3121031227. PMLR.",
  "Danqing Wang, Kevin Yang, Hanlin Zhu, XiaomengYang, Andrew Cohen, Lei Li, and Yuandong Tian.2023. Learning personalized story evaluation. arXivpreprint arXiv:2310.03304": "Yizhong Wang, Swaroop Mishra, Pegah Alipoormo-labashi, Yeganeh Kordi, Amirreza Mirzaei, AtharvaNaik, Arjun Ashok, Arut Selvan Dhanasekaran, An-jana Arunkumar, David Stap, et al. 2022. Super-naturalinstructions: Generalization via declarativeinstructions on 1600+ nlp tasks. In Proceedings ofthe 2022 Conference on Empirical Methods in Natu-ral Language Processing, pages 50855109. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,Barret Zoph, Sebastian Borgeaud, Dani Yogatama,Maarten Bosma, Denny Zhou, Donald Metzler, et al.2022. Emergent abilities of large language models.Transactions on Machine Learning Research. Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz,et al. 2020. Transformers: State-of-the-art naturallanguage processing. In Proceedings of the 2020 con-ference on empirical methods in natural languageprocessing: system demonstrations, pages 3845.",
  "Chuhan Wu, Fangzhao Wu, Yongfeng Huang, andXing Xie. 2023. Personalized news recommenda-tion: Methods and challenges. ACM Transactions onInformation Systems, 41(1):150": "Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, XiaohuiTao, and Fu Lee Wang. 2023. Parameter-efficientfine-tuning methods for pretrained language models:A critical review and assessment. arXiv preprintarXiv:2312.12148. Chunhui Zhang, Yiren Jian, Zhongyu Ouyang, andSoroush Vosoughi. 2024. Working memory identifiesreasoning limits in language models. In Proceedingsof the 2024 Conference on Empirical Methods inNatural Language Processing.",
  "BEfficiency Analysis": "Personalization is a technique that aims at univer-sally benefiting everyone, where scalability andefficiency are crucial factors in large-scale deploy-ment. In this experiment, we study the trainingefficiency of our proposed OPPU. We specificallyexamine two critical factors: the number of userhistory items and the average token numbers perhistory item across classification and generationtasks. Given that the training of each users privatePEFT can occur simultaneously or in a distributedmanner, we choose not to consider the user countfactor in this scenario, concentrating instead on theefficiency of training for an individual user. Ini-tially, we set a consistent count of 100 whitespace-separated tokens for each history entry and varythe number of history items from 10 to 100. Wethen fix the history item count at 10 and adjust thetoken count from 10 to 100. The training time foreach configuration, necessary for users to developtheir personal PEFT modules. Presented in , the results suggest that training time increases linearly with the number of user history items. The-oretically, training time grows quadratically withthe increase in average tokens per history entry, yetour observations indicate a trend more akin to lin-ear growth. Its noteworthy that the longer trainingdurations for personalized movie tagging tasks, asopposed to personalized tweet paraphrasing, areattributed to different training epochs.",
  "The baseline details are presented as follows:": "Non-Personalized Baseline: We present twoapproaches under the non-personalized setting:non-retrieval and random history. Non-retrievalmethod refers to only feeding the users querywithout revealing the users behavior history tothe LLMs. Random history baseline means aug-menting the users query with random historybehavior from all user history corpus. Retrieval-Augmented Personalization (RAG):We follow the retrieval-augmented personaliza-tion method presented in LaMP (Salemi et al.,2023), where the users query is augmented withtop k retrieved items from the correspondingusers history corpus. We take k=1, 2, 4 in thiswork. Profile-Augmented Personalization (PAG):This method is taken from Richardson et al.(2023), in which the users input sequence wouldconcatenate the users profile summarizing theusers preference and behavior patterns. In ourexperiments, we generate user profiles using thevicuna-7B (Chiang et al., 2023) model. More-over, the profile-augmented method could becombined with the retrieval augmentation. Inthis case, we take the number of retrieval itemsk=1 following the setting of Richardson et al.(2023).",
  "We present the task details as follows to help read-ers gain a better understanding of the task format": "Personalized Citation Identification is a binarytext classification task. Specifically, given user uwrites a paper x, the task aims to make the modeldetermine which of the two candidate papers uwill cite in paper x based on the users historydata, which contains the publications of user u. Personalized News Categorization is a 15-waytext classification task to classify news articleswritten by a user u. Formally, given a news ar-ticle x written by user u, the language model isrequired to predict its category from the set ofcategories based on the users history data, which",
  "contains the users past article and correspondingcategory": "Personalized Movie Tagging is a 15-way textclassification task to make tag assignmentsaligned with the users history tagging prefer-ence. Specifically, given a movie description x,the model needs to predict one of the tags for themovie x based on the users historical movie-tagpairs. Personalized Product Rating is a 5-way textclassification task and can also be understood asa regression task. Given the user us historical re-view and rating pairs and the input review x, themodel needs to predict the rating correspondingto x selected from 1 to 5 in integer. Personalized News Headline Generation is atext generation task to test the models abilityto capture the stylistic patterns in personal data.Given a query x that requests to generate a newsheadline for an article, as well as the user profilethat contains the authors historical article-titlepairs, the model is required to generate a newsheadline specifically for the given user. Personalized Scholarly Title Generation is atext generation task to test personalized text gen-eration tasks in different domains. In this task,we require language models to generate titles foran input article x, given a user profile of historicalarticle-title pairs for an author. Personalized Tweet Paraphrasing is also a textgeneration task that tests the models capabili-ties in capturing the stylistic patterns of authors.Given a user input text x and the user profile ofhistorical tweets, the model is required to para-phrase x into y that follows the given users tweetpattern.",
  "I.4Personalized Product Rating": "Based on this users past reviews, what are the mostcommon scores they give for positive and nega-tive reviews? Answer in the following form: mostcommon positive score: <most common positivescore>, most common negative score: <most com-mon negative score>. User History: Answer:Lookat the following past movies this user has watchedand determine the most popular tag they labeled.Answer in the following form: most popular tag:<tag>. User History: {USER HISTORY} Answer:"
}