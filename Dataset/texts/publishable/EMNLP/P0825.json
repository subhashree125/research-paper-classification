{
  "Abstract": "Large language models are able to generatecode for visualisations in response to simpleuser requests. This is a useful application andan appealing one for NLP research becauseplots of data provide grounding for language.However, there are relatively few benchmarks,and those that exist may not be representa-tive of what users do in practice. This paperinvestigates whether benchmarks reflect real-world use through an empirical study compar-ing benchmark datasets with code from publicrepositories. Our findings reveal a substantialgap, with evaluations not testing the same dis-tribution of chart types, attributes, and actionsas real-world examples. One dataset is repre-sentative, but requires extensive modificationto become a practical end-to-end benchmark.This shows that new benchmarks are needed tosupport the development of systems that trulyaddress users visualisation needs. These ob-servations will guide future data creation, high-lighting which features hold genuine signifi-cance for users.",
  "Introduction": "Text-to-Vis is the task of receiving data and a re-quest for a visualisation expressed in human lan-guage and generating code that will produce thevisualisation. A system with this ability would en-able faster and more complex data analysis, butthere are relatively few benchmark datasets for thetask. Those that do exist either focus on generat-ing a single response (Luo et al., 2021; Srinivasanet al., 2021; Chen et al., 2021), or consider dia-logue, but with limited flexibility in code (Shaoand Nakashole, 2020; Song et al., 2024). Most ofthese datasets use generated data. The space ofcode variation was defined by researchers. Thisraises the question of whether these datasets arerepresentative of real-world use of data visualisa-tions. In this study, we gathered publicly availablecode from the Stack1 to analyse human prefer-ences when making visualisations using librariesacross four programming languages: Python, R,Javascript, and Vega. Since each library has dif-ferent names for the same visualisation types andproperties, we extracted key visualisation code anddeveloped a cross-language mapping for severalhundred functions and arguments.2 Using this aligned data, we analysed user be-haviours when making visualisations and identifiedsimilarities and differences between real-world andbenchmark datasets. Our analysis considered thechart types, functions called to define properties,and the arguments that modify how those functionsbehave. We observed that (1) existing benchmarkstend to focus on only one aspect of the Text-to-Vischallenge, either code synthesis, data presentation,or aesthetic attribute adjustment, and (2) only oneof the datasets is consistent with real-world use andthat dataset is limited by its lack of executability ofcode outputs.Success on existing benchmarks does not meansystems are useful for real-world use. We need newbenchmarks that cover all aspects of the problemand are consistent with patterns of use. Only thenwill we be able to measure progress on this valuableand challenging task.",
  "A 6TB collection of open source code from GitHub (Ko-cetkov et al., 2022)2For example, a bar plot is produced with bar() andbarh() in Python, but barplot() in R. Our data and code areat": "ward, datasets produced using this method oftencontain inherent problems. For instance, nvBench,the largest benchmark dataset for this task, wassynthesised from Spider (Yu et al., 2018), a text-to-SQL dataset containing several limitations (Suhret al., 2020), and was only partially reviewed bynovices and experts for quality assurance, result-ing in numerous issues (Li et al., 2024). Similarly,ChartDialogs contains limitations as the data forvisualisation were automatically generated. Theseshortcomings underscore the need for improvedmethodologies in dataset creation to ensure validityand usability.Recent research in AI has prioritised ecologi-cal validity to enhance benchmark dataset qualityacross various domains, aiming to align them withreal-world applications (De Vries et al., 2020; Qiet al., 2023; Lu et al., 2023). Ensuring that the dataused to train and test models accurately reflectsusers objectives in practical scenarios is crucial.However, prior research on Text-to-Vis has not con-sidered this aspect of dataset creation.",
  "Data Collection": "Instead of examining user preferences throughchart images or relying on experts to comprehendhow visualisations are made, our approach involvesthe analysis of publicly available programs specifi-cally written for creating visualisations, e.g., line,bar, and scatter charts. This means we consider awide range of samples from different programmersand their preferences when making visualisations.As a result, our analysis can provide a broad un-derstanding of the essential components that arewidely used.We used code from The Stack 3 to conductour investigation. We consider four diverse andwidely used visualisation libraries: Matplotlib4,Graphics5, ChartJS6, and Vega-Lite7, which arefor Python, R, Javascript, and JSON, respectively.After downloading, we selected files containingcode indicative of visualisation library usage (e.g.,import matplotlib.pyplot as plt). Finally,we used abstract syntax tree (AST) parsers andheuristics to accurately extract library-related vari-ables, function names, arguments, and explicit val-ues. The details are described in Appendix A, while",
  ": Description of benchmark datasets": "(upper) presents their statistics.We examined three publicly available bench-mark datasets: nvBench (Luo et al., 2021), Chart-Dialogs (Shao and Nakashole, 2020), and Plot-Coder (Chen et al., 2021). They vary in settingsand scales, as described in and 2. WhilenvBench and ChartDialogs are end-to-end Text-to-Vis benchmarks, with queries & data as inputand code & visualisations as output, PlotCoder ispurely a code synthesis dataset, with no data orvisualisations. Appendix B shows examples fromeach dataset.",
  "Cross-language Mapping Table": "To compare the data described in the previous sec-tion, we constructed a cross-language mapping ta-ble based on frequently used parameters. This in-volved selecting the top 500 frequently used pa-rameters, identifying categories and attributes, andchecking correctness based on the libraries doc-umentation and code execution. Ultimately, themapping table comprises 8 categories, 62 attributes,and around 850 parameters across the 4 visualisa-tion languages. shows an example forthe attribute x-axis title. Details can be found inAppendix C.",
  "Comparison of chart types": "(upper) depicts the distribution of fourcommon plot types across real-world datasets andnvBench 8. Each dataset shows distinct preferencesfor specific plot types. The distribution of nvBench,a benchmark based on the Vega-Lite grammar, issignificantly misaligned with that of Vega-Lite,where the bar chart dominates other types, account-ing for over 80%, while the remaining are around7%. (lower) depicts the distribution of sevenplot types across four Python-based datasets. Gen-erally, the distribution between Matplotlib and Plot-Coder shows notable similarity. This trend is be-cause both are derived from GitHub. In contrast,ChartDialogs contains a more uniform distributionof plot types. This is the result of its design, anddiffers from what we observe in the wild. Specifi-cally, ChartDialogs has fewer scatter plots and anoverabundance of pie charts, contours, and streamplots.These findings imply that nvBench and ChartDi-alogs are not testing the same distribution of plottypes as real-world data. As suggestions for futuredataset makers, it is crucial to tailor the distributionof chart types according to the specific needs anddomains of the intended users. At the same time,given the imbalanced distributions in real-worlddata, it is also valuable to conduct separate eval-uations focusing specifically on rarer plot types,acknowledging their distinct value.",
  ": Spearmans rank correlation coefficient interms of frequent attributes": "the normalised frequency for 62 attributes withineach dataset, as shown in located in Ap-pendix D. We used these frequencies to determinethe Spearmans rank correlation coefficient acrosseight datasets, as illustrated in .The real-world datasets have a significant cor-relation, with Spearmans values surpassing 0.7,except for ChartJS, which displays a moderatecorrelation with coefficients around 0.5. As forthe benchmarks, ChartDialogs and nvBench showa weak correlation with their direct counterparts,Matplotlib and Vega-Lite, respectively. This meansmany attributes that were frequently used by endusers have not been tested in these benchmarks.These include titles, axes-scale limits, tick labels,opacity, histogram bins, legend visibility, and mul-tiple plots handling, as visualised in in D.Conversely, PlotCoder demonstrates a strongalignment with real-world data, with Spearmansvalues ranging from 0.7 to 0.9. The correlationhighlights PlotCoders potential as a resource forcrafting end-to-end Text-to-Vis benchmarks. How-ever, it is an automatically extracted dataset. Itlacks data to plot in the input, visualisations inthe output, and information on which versions oflibraries it has as dependencies. This means thecode cannot be executed as is. Without executingit, there is not way to confirm whether the visualoutput aligns with user goals.",
  "Comparison of attributes when permitted": "Some attributes can only be activated (or permitted)if specific preconditions are met. For instance, the\"bar thickness\" attribute can only be set if the userplots data on a bar chart and adjusts the width pa-rameter. Consequently, these attributes may appearinfrequently in the dataset, but users often specifytheir preferred values. Therefore, analysing theseattributes is crucial for a deeper understanding ofend users preferences.In this analysis, we computed the frequency ofattributes for a given visualisation type or action(e.g. plt.bar()). This calculation is applied toeach attribute in the mapping table and visualisedas a heat map in located in Appendix D.We focus solely on examining this behaviour inPython-based datasets, including Matplotlib-nb,Matplotlib-py, PlotCoder, and ChartDialogs. Thisis because nvBench does not prioritise user inten-tion for modifying aesthetic attributes while othershave different characteristics.The Spearmans coefficient calculation amongthese datasets reinforces our findings in the previ-ous section. Matplotlib-nb, Matplotlib-py, and Plot-Coder show significant correlations, with Spear-mans scores above 0.8, whereas ChartDialogsscores below 0.1. While attributes such as axesscales, edge colour, marker size, pie chart char-acteristics, legend labels, and grid line attributesreceive considerable attention in ChartDialogs, endusers less frequently specify them and often relyon the librarys defaults.Dataset creators should consider attributes suchas histogram bins, pie precision digits, error-barvisibility, and annotation attributes, which are fre-quently customised by end users.",
  "Comparison of program complexity": "To compare complexity, we calculate the averagecount of distinct visualisation functions and param-eters within each code file and present the findingsin . In this section, we omit ChartDialogsbecause it is a slot-filling dataset, with a fixed num-ber of functions and parameters.Benchmarks differ significantly from their directcounterparts. They use far fewer functions and pa-rameters. In most real-world data, users employ 3to 7 functions and 10 to 14 parameters. The top 7functions used in Matplotlib-py include plotting thedata, saving figures, assigning titles, and adjustinglegends. The higher figures in the Vega-Lite datasetcan be explained by its nature as a visualisation lan-guage (not a library built on top of a programminglanguage).",
  "Conclusion": "In this paper, we analysed whether Text-to-Visbenchmarks accurately reflect real-world usageby presenting analyses of chart types, frequent at-tributes, and program complexity. Our results showthat only one of the standard three benchmarks isaligned with real-world use. That dataset has itsown critical limitation: it cannot be used as an end-to-end benchmark, going from a request and dataas input to a visualisation as output. As well as cri-tiquing current benchmarks, we provide guidancefor future benchmark development, suggesting theevaluation of relevant attributes and challengingcharts that better reflect end users preferences.Such a benchmark would guide the developmentof useful and impactful systems.",
  "This study offers analyses of datasets and acknowl-edges several limitations. Firstly, our examinationwas restricted to only four visualisation libraries,each corresponding to a different programming": "language. This narrow scope may not adequatelycapture the diversity of applications and use caseswithin the field. Although we attempted to analyseMatLab code files in The Stack dataset, they aremiscategorised in The Stack, processed with thewrong extension.9 Despite our efforts to clarifythis issue by reaching out to the project authors, wehave yet to receive a response. Secondly, our inves-tigation is based on public code, mainly represent-ing programmers with different visualisation levels,including novices, practitioners, and experts. If thetarget users are in a visualisation application likeTableau 10, our results may not be representative.Lastly, this study concludes with an analysis andassessment of existing benchmark datasets withoutproposing solutions. Nevertheless, we believe thatthe insights and recommendations provided in thiswork are valuable for any dataset maker and futurestudies.",
  "Ethics Statement": "The data used in this research can be found pub-licly in the repositories of the cited papers, GitHub,or HuggingFace. Those who want to use the pro-cessed data in our repository will need to followthe terms and conditions of The Stack dataset11. This material is partially supported by the Aus-tralian Research Council through a Discovery EarlyCareer Researcher Award and the CommonwealthScientific and Industrial Research Organisation(CSIRO). We extend our gratitude to the anony-mous reviewers for their constructive feedback andvaluable advice on our submissions. Xinyun Chen, Linyuan Gong, Alvin Cheung, and DawnSong. 2021. Plotcoder: Hierarchical decoding forsynthesizing visualization code in programmatic con-text. In Proceedings of the 59th Annual Meeting ofthe Association for Computational Linguistics andthe 11th International Joint Conference on Natu-ral Language Processing (Volume 1: Long Papers),pages 21692181.",
  "search on language user interfaces. arXiv preprintarXiv:2007.14435": "Denis Kocetkov, Raymond Li, LI Jia, Chenghao Mou,Yacine Jernite, Margaret Mitchell, Carlos Muoz Fer-randis, Sean Hughes, Thomas Wolf, Dzmitry Bah-danau, et al. 2022. The stack: 3 tb of permissively li-censed source code. Transactions on Machine Learn-ing Research. Guozheng Li, Xinyu Wang, Gerile Aodeng, ShunyuanZheng, Yu Zhang, Chuangxin Ou, Song Wang, andChi Harold Liu. 2024.Visualization generationwith large language models: An evaluation. arXivpreprint arXiv:2401.11255. Xing Han Lu, Siva Reddy, and Harm De Vries. 2023.The statcan dialogue dataset: Retrieving data tablesthrough conversations with genuine intents. In Pro-ceedings of the 17th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 27912821. Yuyu Luo, Nan Tang, Guoliang Li, Chengliang Chai,Wenbo Li, and Xuedi Qin. 2021. Synthesizing nat-ural language to visualization (nl2vis) benchmarksfrom nl2sql benchmarks. In Proceedings of the 2021International Conference on Management of Data,pages 12351247. Peng Qi, Nina Du, Christopher D Manning, and JingHuang. 2023. Pragmaticqa: A dataset for pragmaticquestion answering in conversations. In Findings ofthe Association for Computational Linguistics: ACL2023, pages 61756191. Yutong Shao and Ndapa Nakashole. 2020. ChartDi-alogs: Plotting from Natural Language Instructions.In Proceedings of the 58th Annual Meeting of the As-sociation for Computational Linguistics, pages 35593574, Online. Yuanfeng Song, Xuefang Zhao, and Raymond Chi-Wing Wong. 2024. Marrying dialogue systems withdata visualization: Interactive data visualization gen-eration from natural language conversations. In Pro-ceedings of the 30th ACM SIGKDD Conference onKnowledge Discovery and Data Mining, pages 27332744. Arjun Srinivasan, Nikhila Nyapathy, Bongshin Lee,Steven M Drucker, and John Stasko. 2021. Collect-ing and characterizing natural language utterancesfor specifying data visualizations. In Proceedingsof the 2021 CHI Conference on Human Factors inComputing Systems, pages 110. Alane Suhr, Ming-Wei Chang, Peter Shaw, and Ken-ton Lee. 2020. Exploring unexplored generalizationchallenges for cross-database semantic parsing. InProceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 83728388, Online. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-ing Yao, Shanelle Roman, Zilin Zhang, and DragomirRadev. 2018. Spider: A large-scale human-labeleddataset for complex and cross-domain semantic pars-ing and text-to-SQL task. In Proceedings of the 2018Conference on Empirical Methods in Natural Lan-guage Processing, pages 39113921, Brussels, Bel-gium.",
  "ACode Parsing": "After obtaining code files for Python and R, weused abstract syntax tree (AST) parsers and heuris-tics to accurately extract variables, function names,arguments, and explicit values. Subsequently, wetracked the assigned variables to correctly select thefunctions used in Matplotlib while a list of Graph-ics functions was used to filter for this library.To extract ChartJS specifications, we initiallyused an AST parser to extract all JSON data fromthe Javascript code files. Subsequently, a heuris-tic selection method was applied to filter JSONcontaining the three essential components of thislibrary, namely \"type,\" \"data,\" and \"options.\" Thisis because ChartJS relies on the JSON format asits foundation, serving as the input for executingfunctions in Javascript.Vega-Lite can appear in both JSON andJavascript files, as it is a JSON schema visualisationlanguage. Therefore, we used the above methodsfor extraction. In detail, after extracting JSON datafrom code files, we exclusively extracted snippetscontaining Vega-Lite schema 12, which is a manda-tory field of Vega-Lite specification.After extracting functions,arguments,as-signed values, and JSON specifications, tar-geting the visualisation libraries,we trans-formed them into a universal format to facil-itate more accessible analysis and further pro-cessing.For instance, a command in Pythonax.plot(x,color='green',marker='o'),which plots a line graph of x, with markero and colour green, can be parsed into aJSON as {\"func_name\": \"plot\", args: [\"x\"],kargs: {\"color\": \"green\", \"marker\": \"o\"}}.An example of translating JSON to universal for-mat can be seen in .Regarding nvBench and PlotCoder, they con-tain visualisation code in Vega-Lite and Python,so the process was the same as describedabove.When it comes to ChartDialogs, a",
  ": A sample from the PlotCoder dataset": "If a specific language lacked relevant parametersfor a given attribute in the top 500 (resulting in ablank cell), we persistently searched through theremaining list until a match was found. Cells whereno relevant parameter was identified led to the an-notation of not found. This identification and ver-ification process includes understanding plottingparameters, identifying them in API documents,asking ChatGPT 13 for explanations and relevantparameters and executing example codes. shows a small part of the table for con-text. The whole table can be found in our repositoryat",
  "n is the number of times that all argumentsare specified": "As for the heat map in , there are twocases influencing different levels. For attributes im-pacting the program level, such as title, x-axis title,and x-y tick labels, the percentage is derived fromhow frequently a program includes arguments fora specific attribute. Conversely, for local attributesaffecting the function level, like filled colour, opac-ity, and bar thickness, the percentage is calculatedbased on the frequency of functions containing ar-guments for the given attribute. The calculation isas follows:"
}