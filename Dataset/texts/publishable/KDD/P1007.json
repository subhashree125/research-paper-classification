{
  "ABSTRACT": "Smart homes, powered by the Internet of Things, offer great conve-nience but also pose security concerns due to abnormal behaviors,such as improper operations of users and potential attacks frommalicious attackers. Several behavior modeling methods have beenproposed to identify abnormal behaviors and mitigate potentialrisks. However, their performance often falls short because theydo not effectively learn less frequent behaviors, consider temporalcontext, or account for the impact of noise in human behaviors. Inthis paper, we propose SmartGuard, an autoencoder-based unsuper-vised user behavior anomaly detection framework. First, we design",
  "KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08": "a Loss-guided Dynamic Mask Strategy (LDMS) to encourage themodel to learn less frequent behaviors, which are often overlookedduring learning. Second, we propose a Three-level Time-aware Po-sition Embedding (TTPE) to incorporate temporal information intopositional embedding to detect temporal context anomaly. Third,we propose a Noise-aware Weighted Reconstruction Loss (NWRL)that assigns different weights for routine behaviors and noise behav-iors to mitigate the interference of noise behaviors during inference.Comprehensive experiments demonstrate that SmartGuard consis-tently outperforms state-of-the-art baselines and also offers highlyinterpretable results.",
  "KDD 24, August 2529, 2024, Barcelona, SpainJingyu Xiao, Zhiyao Xu and Qingsong Zou et al": "a new living environment, participants were required to inhabitthe experimental setting for a minimum of two weeks before theformal commencement of data collection. All users possessed com-prehensive knowledge of the IoT devices and applications in use.Subsequent to check-in, control of all devices was relinquished tothe users, who were duly informed in advance that their deviceusage would be subsequently reviewed and analyzed by our teamAnomaly Behavior Injection. We insert abnormal behaviorsin into normal behavior sequences to construct abnormalbehavior sequences. Then the abnormal behavior sequences. Thenthe abnormal behavior sequence and the normal behavior sequencetogether form the test dataset. The anomaly behavior sequencesexamples are shown in .",
  "INTRODUCTION": "The rapid growth of IoT solutions has led to an unprecedentedincrease in smart devices within homes, expected to reach approx-imately 5 billion by 2025 . However, the abnormal behaviorspose substantial security risks within smart homes. These abnor-mal behaviors usually originate from two primary sources. First,improper operations by users can cause abnormal behaviors, suchas inadvertently activating the air conditioners cooling mode dur-ing winter or forgetting to close a water valve. Second, maliciousattackers can exploit vulnerabilities within IoT devices and plat-forms, taking unauthorized control of these devices. For example,hackers can compromise IoT platforms, allowing them to disablesecurity cameras and manipulate home automation systems, creat-ing opportunities for burglary. These security concerns emphasizethe urgency of robust behavioral modeling methods and enhancedsecurity measures to safeguard smart home environments.Deep learning has been employed across various domains tomine correlations between behaviors for modeling user behav-ior sequences and address security issues .DeepMove leverages RNNs to model both long and short-termmobility patterns of users for human mobility prediction. To cap-ture the dynamics of users behaviors, SASRec proposes aself-attention based model to achieve sequential recommendation.More recent efforts primarily focus on transformer-basedmodels for their superior ability to handle sequential behavior data.However, we cannot borrow the above models to directly applythem in our scenarios, because of the following three challenges ofuser behavior modeling in smart homes.",
  ": Reconstruction losses for behaviors with differentoccurrence frequencies": "First, the occurrence frequencies of different user behaviors maybe imbalanced, leading to challenges in learning the semantics ofthese behaviors. This user behavior imbalance can be attributedto individuals living habits. For example, cook-related behaviors (e.g., using microwave and oven) of office workers may be infre-quent, because they dine at their workplace on weekdays and onlycook on weekends. On the other hand, some daily behaviors liketurning on lights and watching TV of the same users can be morefrequent. Behavior imbalance complicates the learning process formodels: some behaviors, which occur frequently in similar contexts,can be easily inferred, while others that rarely appear or manifestin diverse contexts can be more challenging to infer. We train anautoencoder model on AN dataset (shown in ), record the oc-currences and reconstruction loss of different behaviors. As shownin , with the number of occurrences of behavior decreases,the reconstruction loss tends to increase.Second, temporal context, e.g., the timing and duration of userbehaviors, plays a significant role in abnormal behavior detectionbut is overlooked by existing solutions. For example, turning onthe cooling mode of the air conditioner in winter is abnormal, butis normal in summer. Showering for 30-40 minutes is normal, butexceeding 2 hour suggests a user accident. Ignoring timing informa-tion hinders the identification of abnormal behavior patterns. Asshown in , sequence 1 represents a users normal laundry-related behaviors. Sequences 2 and 3 follow the same order assequence 1. However, in sequence 2, the water valve were opens at2 oclock in the night. In sequence 3, the duration between openingand closing the water valve is excessively long. Therefore thesetwo sequences should be identified as abnormal behaviors possiblyconducted by attackers intending to induce water leakage.",
  ": Example of three user behaviors with the samebehavior order. Sequence 2 and 3 are abnormal due to theirinappropriate timing and excessive duration": "Third, arbitrary intents and passive device actions can causenoise behaviors in user behavior sequences, which interfere modelsinference. shows noise behaviors in a behavior sequencerelated to a users behaviors after getting up. The user do someroutine behaviors like turn on the bed light, open the curtains,switch off the air conditioner, open the refrigerator, close therefrigerator and switch on the oven. However, there are alsosome sporadic actions which are not tightly related to the behaviorsequence, including 1) active behaviors, e.g., suddenly deciding toturn on the network audio to listen to music; 2) passive behaviorfrom devices, e.g., the self-refresh of the air purifier. These noise",
  ": An example of noise behaviors": "In this paper, we propose SmartGuard to solve above challenges.SmartGuard is an autoencoder-based architecture, which learns toreconstruct normal behavior sequences during training and identifythe behavior sequences with high reconstruction loss as anomaly.Firstly, we devise a Loss-guided Dynamic Mask Strategy (LDMS)to promote the models learning of infrequent hard-to-learn behav-iors. Secondly, we introduce a Three-level Time-aware PositionEmbedding (TTPE) to integrate temporal information into posi-tional embedding for detecting temporal context anomalies. Lastly,we propose a Noise-aware Weighted Reconstruction Loss (NWRL)to assign distinct weights to routine behaviors and noise behaviors,thereby mitigating the impact of noise behaviors. Our codes arereleased to the GitHub 1. Our contributions can be summarized asfollows:",
  "RELATED WORK2.1User Behavior Modeling in Smart Homes": "Some works propose to model user behavior (i.e., user device in-teraction) based on deep learning. uses event transition graphto model IoT context and detect anomalies. In , authors builddevice interaction graph to learn the device state transition rela-tionship caused by user actions. detects anomalies through cor-relational analysis of device actions and physical environment. infers user behavior through readings from various sensors installedin the users home. IoTBeholder utilizes attention-based LSTMto predict the user behavior from history sequences. SmartSense leverages query-based transformer to model contextual infor-mation of user behavior sequences. DeepUDI and SmartUDI use relational gated graph neural networks, capsule neuralnetworks and contrastive learning to model users routines, intents",
  "Attacks and Defenses in Smart Homes": "An increasing number of attack vectors have been identified insmart homes in recent years. In addition to cyber attacks, it is also aconcerning factor that IoT devices are often close association withthe users physical environment and they have the ability to alterphysical environment. In this context, the automation introducesmore serious security risks. Prior research has revealed that ad-versaries can leak personal information, and gain physical accessto the home . In , spoof attack is employed to exploitautomation rules and trigger unexpected device actions. apply delay-based attacks to disrupt cross-platform IoT informa-tion exchanges, resulting in unexpected interactions, rendering IoTdevices and smart homes in an insecure state. This series of attacksaim at causing smart home devices to exhibit expected actions,thereby posing significant security threats. Therefore, designing aneffective mechanism to detect such attacks is necessary. 6thSense utilizes Naive Bayes to detect malicious behavior associatedwith sensors in smart homes. Aegis utilizes a Markov Chain todetect malicious behaviors. ARGUS designed an Autoencoderbased on Gated Recurrent Units (GRU) to detect infiltration attacks.However, these methods ignore the behavior imbalance, temporalinformation and noise behaviors.",
  "METHODOLOGY4.1Solution Overview": "To achieve accurate user behavior sequence anomaly detection insmart homes, we propose SmartGuard, depicted in . Theworkflow of SmartGuard can be summarized as follows. Duringtraining, the Loss-guided Dynamic Mask Strategy (4.2) is initiallyemployed to mask hard-to-learn behaviors based on the loss vec-tor Lvec from the previous epoch. Subsequently, the Three-levelTime-aware Positional Encoder (4.3.1) is applied to capture order-level, moment-level, and duration-level temporal information ofthe behaviors, producing the positional embedding . This embed-ding is then added to the device control embedding to form thebehavior embedding h. Finally, h is fed into an -layer attention-based encoder and decoder to extract contextual information forreconstructing the source sequence. During the inference phase, theNoise-aware Weighted Reconstruction Loss Noise-aware WeightedReconstruction Loss (4.4) is utilized to assign different weights tovarious behaviors, determined by the loss vector from the trainingdataset, resulting in the final reconstruction loss . If the surpasses the threshold , SmartGuard triggers an alarm.",
  "Loss-guided Dynamic Mask Strategy": "Autoencoders , which take complete data instances as inputand target to reconstruct the entire input data, are widely used inanomaly detection. Different from traditional autoencoders, maskedautoencoders randomly mask a portion of input data, encoding thepartially-masked data and aiming to reconstruct the masked tokens.By introducing a more meaningful self-supervised task, masked au-toencoders have recently excelled in images learning. However,such reconstruction tasks without mask and with random mask aresub-optimal in our scenarios because they do not emphasize thelearning of hard-to-learn behaviors that occur rarely.We conduct experiments to verify the performance of autoen-coders trained with three mask options: 1) w/o mask: no maskstrategy is used, the objective function is to reconstruct the input; 2) random mask: masking behaviors at every epoch randomly toreconstruct the masked behaviors; 3) top- loss mask: maskingtop behaviors with higher reconstruction loss to reconstruct themasked behaviors. We set mask ratio as 20% for the latter two. shows the changing trends of the reconstruction loss andits variance of different behavior during training on SP dataset(described in ). First, as shown in (a), the modelwithout mask shows the fastest convergence trend, whereas theloss of the model with mask fluctuates. Model without mask cansimultaneously learn all behaviors, facilitating rapid convergence.In contrast, the mask strategy only encourages the model to focuson learning masked behaviors, which may hinder initial-stage con-vergence. Second, the model with top- loss mask strategy showslowest variance towards the end of training as shown in (b),because the top- loss mask strategy effectively encourages themodel to learn hard-to-learn behaviors (i.e., the behaviors with highreconstruction loss), thereby reducing the variance of behaviorreconstruction losses.In this paper, we design a Loss-guided Dynamic Mask Strategy.Intuitively, at the beginning of training, we encourage the modelto learn the relatively easy task to accelerate convergence, i.e.,behavior sequence reconstruction without mask. After training epochs without mask, we adopt the top- loss mask strategy toencourage the model to learn the masked behaviors withhigh reconstruction loss. We continuously track the modelsreconstruction loss of different behaviors by updating a loss vectorin each epoch to guide the mask strategy in the next epoch. Inepoch , the loss vector Lvec is calculated as:",
  "Autoencoder with Temporal Information": "4.3.1Three-level Time-aware Positional Encoder. The tem-poral information in user behavior sequence data primarily residesin the timing of control behaviors, which can be examined fromtwo perspectives: the absolute timing of each individual controlbehavior, and the relative timing gap between control actions onthe same device. On the one hand, the relative timing gap betweencontrol actions on the same device reflects the duration the deviceis in some specific state and the operation frequency of the user.On the one hand, user behaviors are usually time-regulated, andthe functionalities a device carried can determine the absolute tim-ing users operate on it. For example, users usually operate lights",
  ": Autoencoder training process on SP dataset underdifferent mask strategies": "in the morning and evening, and operate the microwave and theoven at meal time. Since certain operations frequently take placenearly simultaneously, we will also consider the order of behaviorsto provide a more comprehensive characterization of behaviorsthat occur successively. Therefore, we incorporate three types oftemporal information into our model. (1) Order-level temporalinformation: we use integer to denotes theorder-level information of the behavior, is the length of behaviorssequence . (2) Moment-level temporal information: we repre-sent the moment as hour of day and day of week based onbehaviors timestamp. (3) Duration-level temporal information:the duration for behavior is calculated as:",
  "(,2+1) = cos/100002/,(7)": "where denotes the -th dimension of the positional embedding, is the dimension of temporal embedding.To learn the representation for device control C, we firstencode device control into a low-dimensional latent space throughdevice control encoder, i.e., an embedding layer. Finally, we addpositional embedding to the device control embedding as followingto get the behavior embedding:",
  "h = + .(8)": "4.3.2Sequence Encoder. To learn the sequence embedding, weemploy transformer encoder consisting of multi-head attentionlayer, residual connections and position-wise feed-forward network(FNN). Given an input behavior representation h, the self-attentionlayer can effectively mine global semantic information of behaviorsequence context by learning query Q, key K and value V matricesof different variables, which are calculated as:",
  "Noise-aware Weighted Reconstruction Loss": "Although LDMS encourages the model to focus on learning be-haviors with high reconstruction losses, it remains challenging toreconstruct noise behaviors due to their inherent uncertainty. Thesignificant reconstruction loss associated with noise behaviors canovershadow other aspects during anomaly detection, potentiallyleading to the misclassification of normal sequences containingnoise behaviors as anomalies.To eliminate the interference of noise behaviors, we propose aNoise-aware Weighted Reconstruction Loss as the anomaly score.We can get the final loss vector after training:",
  "Datasets. We train SmartGuard on three real-world datasetsconsisting of only normal samples, two (FR/SP) from public datasets2": "and one anonymous dataset (AN) collected by ourselves. The datasetsdescription is shown in . All datasets are split into training,validation and testing sets with a ratio of 7:1:2. To evaluate theperformance of SmartGuard, we construct ten categories of abnor-mal behaviors as shown in and insert them among normalbehaviors for simulating real anomaly scenarios.",
  "Evaluation metrics. We use common metrics such as FalsePositive rate, False Negative Rate, Recall, and F1-Score to evaluatethe performance of SmartGuard": "5.1.4Complexity analysis. Suppose the embedding size is ,and the behavior sequence length is . The computational complex-ity of SmartGuard is mainly due to the self-attention layer and thefeed-forward network, which is (2 + 2). The dominant termis typically (2) from the self-attention layer. SmartGuard onlytakes 0.0145s, which shows that it can detect abnormal behaviorsin real time.",
  "Performance Comparison (RQ1)": "We use grid search to adjust the parameters of SmartGuard andreport the overall performance of SmartGuard and all baselinesin . Bold values indicate the optimal performance amongall schemes, and underlined values indicate the second best per-formance. First, SmartGuard outperforms all competitors in mostcases. This is because SmartGuardsimultaneously considers thetemporal information, behavior imbalance and noise behaviors. Second, SmartGuard significantly improves the performance onDM and DD type anomalies detection. We ascribe this superiority toour TTPEs effective mining of temporal information of behaviors.Third, the LOF, IF and 6thSense show the worst performance. Aegisand OCSVM outperforms LOF, IF and 6thSense, which benifitsfrom the Markov Chains modeling of behavior transitions andSVMs powerful kernel function. The Autoencoder outperform thetraditional models because of stronger sequence modeling capabil-ity. ARGUS outperforms Aueocoder because of stronger sequencemodeling capability of GRU. By exploiting transformer to minecontextual information, TransAE achieves better performance thanall other baselines, but is still inferior to our proposed scheme.",
  "Ablation Study (RQ2)": "SmartGuard mainly consists of three main components: Loss-guidedDynamic Mask Strategy (LDMS), Three-level Time-aware PositionEmbedding (TTPE) and Noise-aware Weighted ReconstructionLoss (NWRL). To investigate different components effectivenessin SmartGuard, we implement 5 variants of SmartGuard for abla-tion study (0-4). Y represents adding the corresponding compo-nents, X represents removing the corresponding components. 4 isSmartGuard with all three components. As shown in , eachcomponent of SmartGuard has a positive impact on results. Thecombination of all components brings the best results, which ismuch better than using any subset of the three components.",
  "Parameter Study (RQ3)": "5.4.1The mask ratio and the training step without mask. illustrates that SmartGuard achieves the optimal perfor-mance when = 0.4 and = 5. The parameter (Equation 3)determines the difficulty of the model learning task. A smaller fails to effectively encourage the model to learn hard-to-learnbehaviors, while a larger increases the learning burden on themodel, consequently diminishing performance. As for training stepswithout a mask, a smaller hinders the model from convergingeffectively at the beginning stage, whereas a larger impedes themodels ability to learn hard-to-learn behaviors towards the end,resulting in degraded performance. 5.4.2 of Noise-aware Weighted Reconstruction Loss. Theparameter (Equation 17) controls the weights assigned to poten-tial noise behaviors. A smaller results in a smaller weight for noisebehaviors, while a larger leads to a greater weight for noise behav-iors. As illustrated in (a), the False Positive Rate graduallydecreases as decreases, benefiting from the reduced loss weightassigned to noise behaviors. However, as depicted in (b),the False Negative Rate slightly increases as decreases. When = 0.1, SmartGuard achieves a balance, minimizing both the FalsePositive Rate and the False Negative Rate. 5.4.3The embedding size . We fine-tune the embedding sizefor time and device control, ranging from 8 to 512. As depictedin (a), an initial increase in the embedding dimension re-sults in a notable performance improvement, which is attributedto the larger dimensionality enabling behavior embedding to cap-ture more comprehensive information about the context, thereby",
  "Case Study (RQ4)": "To assess the interpretability of SmartGuard, we select a behaviorsequence from the test set of the AN dataset and visualize its atten-tion weights and reconstruction loss. Illustrated in , theuser initiated a sequence of actions: turning off the TV, stopping thesweeper, closing the curtains, switching off the bedlight, and lock-ing the smart lock before going to sleep. Subsequently, an attackertook control of IoT devices, turning off the camera, and openingthe window for potential theft. Examining (a), we observe",
  "Embedding Space Analysis (RQ5)": "We visualize the similarity between device embeddings and timeembeddings (i.e., hour embedding, day embedding and durationembeddings) to analyze whether the model effectively learns therelationship between behaviors. As shown in (a), openingthe curtains usually occurs between 6-9 and 9-12 oclock becauseusers usually get up during this period, while closing the curtainsgenerally occurs between 21-24 oclock because the user usually go to bed during this period. The dishwasher usually runs between12-15 and 18-21 oclock, which means that the user has lunch anddinner during this period, and then washes the dishes. As shownin (b), users generally watch TV and do laundry on Satur-days and Sundays. As shown in (c), users usually take abath for about 1-2 hours, bath time longer than this may indicateabnormality occurs.",
  "CONCLUSION": "In this paper, we introduce SmartGuard for unsupervised user be-havior anomaly detection. We first devise a Loss-guided DynamicMask Strategy (LDMS) to encourage the model to learn less frequentbehaviors that are often overlooked during the learning process.Additionally, we introduce Three-level Time-aware Position Embed-ding (TTPE) to integrate temporal information into positional em-bedding, allowing for the detection of temporal context anomalies.Furthermore, we propose a Noise-aware Weighted ReconstructionLoss (NWRL) to assign distinct weights for routine behaviors andnoise behaviors, thereby mitigating the impact of noise. Compre-hensive experiments conducted on three datasets encompassingten types of anomaly behaviors demonstrate that SmartGuard con-sistently outperforms state-of-the-art baselines while deliveringhighly interpretable results. We thank the anonymous reviewers for their constructive feedbackand comments. This work is supported by the Major Key Project ofPCL under grant No. PCL2023A06-4, the National Key Research andDevelopment Program of China under grant No. 2022YFB3105000,and the Shenzhen Key Lab of Software Defined Networking undergrant No. ZDSYS20140509172959989. The first author, JingyuXiao, in particular, wants to thank his parents Yingchao Xiao,Aiping Li and his girlfriend Liudi Shen for their kind support.He also thanks for all his friends who are the antidote duringhis tired period.",
  "Jiawang Bai, Kuofeng Gao, Shaobo Min, Shu-Tao Xia, Zhifeng Li, and Wei Liu.2024. BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP.In CVPR": "Z. Berkay Celik, Leonardo Babun, Amit Kumar Sikder, Hidayet Aksu, Gang Tan,Patrick D. McDaniel, and A. Selcuk Uluagac. 2018. Sensitive Information Trackingin Commodity IoT. In 27th USENIX Security Symposium, USENIX Security 2018,Baltimore, MD, USA, August 15-17, 2018, William Enck and Adrienne Porter Felt(Eds.). USENIX Association, 16871704. Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. 2019. Behaviorsequence transformer for e-commerce recommendation in alibaba. In Proceedingsof the 1st international workshop on deep learning practice for high-dimensionalsparse data. 14.",
  "Zhaomin Chen, Chai Kiat Yeo, Bu Sung Lee, and Chiew Tong Lau. 2018.Autoencoder-based network anomaly detection. In 2018 Wireless telecommu-nications symposium (WTS). IEEE, 15": "Zhangyu Cheng, Chengming Zou, and Jianwei Dong. 2019. Outlier detectionusing isolation forest and local outlier factor. In Proceedings of the conference onresearch in adaptive and convergent systems. 161168. Haotian Chi, Chenglong Fu, Qiang Zeng, and Xiaojiang Du. 2022. Delay WreaksHavoc on Your Smart Home: Delay-based Automation Interference Attacks. In43rd IEEE Symposium on Security and Privacy, SP 2022, San Francisco, CA, USA,May 22-26, 2022. IEEE, 285302. Gabriel de Souza Pereira Moreira, Sara Rabhi, Jeong Min Lee, Ronay Ak, andEven Oldridge. 2021. Transformers4rec: Bridging the gap between nlp andsequential/session-based recommendation. In Proceedings of the 15th ACM Con-ference on Recommender Systems (RecSys). 143153. Jie Feng, Yong Li, Chao Zhang, Funing Sun, Fanchao Meng, Ang Guo, andDepeng Jin. 2018. DeepMove: Predicting Human Mobility with AttentionalRecurrent Networks. In Proceedings of the 2018 World Wide Web Conference(Lyon, France) (WWW 18). International World Wide Web Conferences Steer-ing Committee, Republic and Canton of Geneva, CHE, 14591468.",
  "Earlence Fernandes, Jaeyeon Jung, and Atul Prakash. 2016. Security Analysisof Emerging Smart Home Applications. In Proceedings of IEEE Symposium onSecurity and Privacy, SP 2016, San Jose, CA, USA": "Chenglong Fu, Qiang Zeng, Haotian Chi, Xiaojiang Du, and Siva Likitha Valluru.2022. IoT Phantom-Delay Attacks: Demystifying and Exploiting IoT TimeoutBehaviors. In 52nd Annual IEEE/IFIP International Conference on DependableSystems and Networks, DSN 2022, Baltimore, MD, USA, June 27-30, 2022. IEEE,428440. Chenglong Fu, Qiang Zeng, and Xiaojiang Du. 2021. HAWatcher: Semantics-Aware Anomaly Detection for Appified Smart Homes. In 30th USENIX SecuritySymposium, USENIX Security 2021, August 11-13, 2021, Michael D. Bailey andRachel Greenstadt (Eds.). USENIX Association, 42234240.",
  "Kuofeng Gao, Yang Bai, Jindong Gu, Yong Yang, and Shu-Tao Xia. 2023. BackdoorDefense via Adaptively Splitting Poisoned Dataset. In CVPR": "Kuofeng Gao, Jindong Gu, Yang Bai, Shu-Tao Xia, Philip Torr, Wei Liu, andZhifeng Li. 2024. Energy-Latency Manipulation of Multi-modal Large LanguageModels via Verbose Samples. arXiv preprint arXiv:2404.16557 (2024). Tianbo Gu, Zheng Fang, Allaukik Abhishek, Hao Fu, Pengfei Hu, and PrasantMohapatra. 2020. IoTGaze: IoT Security Enforcement via Wireless ContextAnalysis. In 39th IEEE Conference on Computer Communications, INFOCOM 2020,Toronto, ON, Canada, July 6-9, 2020. IEEE, 884893. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollr, and Ross Girshick.2022. Masked autoencoders are scalable vision learners. In Proceedings of theIEEE/CVF conference on computer vision and pattern recognition. 1600016009. Hyunsik Jeon, Jongjin Kim, Hoyoung Yoon, Jaeri Lee, and U Kang. 2022. Accurateaction recommendation for smart home via two-level encoders and commonsenseknowledge. In Proceedings of the 31st ACM International Conference on Informationand Knowledge Management (CIKM). 832841. Yunhan Jack Jia, Qi Alfred Chen, Shiqi Wang, Amir Rahmati, Earlence Fer-nandes, Zhuoqing Morley Mao, and Atul Prakash. 2017.ContexloT: To-wards Providing Contextual Integrity to Appified IoT Platforms. In 24thAnnual Network and Distributed System Security Symposium, NDSS 2017,San Diego, California, USA, February 26 - March 1, 2017. The Internet So-ciety.",
  "Knud Lasse Lueth. 2018. State of the IoT 2018: Number of IoT devices now at 7B Market accelerating": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, GregoryChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019.Pytorch: An imperative style, high-performance deep learning library. Advancesin Neural Information Processing Systems (NIPS) 32 (2019). Phillip Rieger, Marco Chilese, Reham Mohamed, Markus Miettinen, HosseinFereidooni, and Ahmad-Reza Sadeghi. 2023. ARGUS: Context-Based Detection ofStealthy IoT Infiltration Attacks. In Proceedings of the 32nd USENIX Conference onSecurity Symposium (Anaheim, CA, USA) (SEC 23). USENIX Association, USA,Article 241, 18 pages.",
  "Amit Kumar Sikder, Hidayet Aksu, and A Selcuk Uluagac. 2017. {6thSense}: Acontext-aware sensor-based attack detector for smart devices. In 26th USENIXSecurity Symposium (USENIX Security 17). 397414": "Amit Kumar Sikder, Leonardo Babun, Hidayet Aksu, and A. Selcuk Uluagac.2019. Aegis: A Context-Aware Security Framework for Smart Home Systems. InProceedings of the 35th Annual Computer Security Applications Conference (SanJuan, Puerto Rico, USA) (ACSAC 19). Association for Computing Machinery,New York, NY, USA, 2841. Vijay Srinivasan, John A. Stankovic, and Kamin Whitehouse. 2008. Protectingyour daily in-home activity information from a wireless snooping attack. InUbiComp 2008: Ubiquitous Computing, 10th International Conference, UbiComp2008, Seoul, Korea, September 21-24, 2008, Proceedings (ACM International Confer-ence Proceeding Series, Vol. 344), Hee Yong Youn and We-Duke Cho (Eds.). ACM,202211. Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.2019. BERT4Rec: Sequential recommendation with bidirectional encoder rep-resentations from transformer. In Proceedings of the 28th ACM InternationalConference on Information and Knowledge Management (CIKM). 14411450. Shisong Tang, Qing Li, Xiaoteng Ma, Ci Gao, Dingmin Wang, Yong Jiang, QianMa, Aoyang Zhang, and Hechang Chen. 2022. Knowledge-based temporal fusionnetwork for interpretable online video popularity prediction. In Proceedings ofthe ACM Web Conference 2022. 28792887. Shisong Tang, Qing Li, Dingmin Wang, Ci Gao, Wentao Xiao, Dan Zhao, YongJiang, Qian Ma, and Aoyang Zhang. 2023. Counterfactual Video Recommendationfor Duration Debiasing. In Proceedings of the 29th ACM SIGKDD Conference onKnowledge Discovery and Data Mining. 48944903. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in Neural Information Processing Systems (NIPS) 30 (2017). Jincheng Wang, Zhuohua Li, Mingshen Sun, Bin Yuan, and John C. S. Lui. 2023.IoT Anomaly Detection Via Device Interaction Graph. In 53rd Annual IEEE/IFIPInternational Conference on Dependable Systems and Network, DSN 2023, Porto,Portugal, June 27-30, 2023. IEEE, 494507. Jingyu Xiao, Qingsong Zou, Qing Li, Dan Zhao, Kang Li, Wenxin Tang, RunjieZhou, and Yong Jiang. 2023. User Device Interaction Prediction via RelationalGated Graph Attention Network and Intent-aware Encoder. In Proceedings ofthe 2023 International Conference on Autonomous Agents and Multiagent Systems(AAMAS). 16341642. Jingyu Xiao, Qingsong Zou, Qing Li, Dan Zhao, Kang Li, Zixuan Weng, Ruoyu Li,and Yong Jiang. 2023. I Know Your Intent: Graph-enhanced Intent-aware UserDevice Interaction Prediction via Contrastive Learning. Proceedings of the ACMon Interactive, Mobile, Wearable and Ubiquitous Technologies (IMUWT/UbiComp)7, 3 (2023), 128.",
  "A.3Data collection": "Testbed and Participants. To create a practical and viable smarthome model, we implemented our experimental platform withinan apartment setting to gather user usage data of various devices,forming our smart home user behavior dataset (AN). Three vol-unteers were recruited to simulate the typical daily activities ofa standard family, assuming the roles of an adult male, an adultfemale, and a child. The experimental platform comprises a compre-hensive selection of 36 popular market-available devices, detailedin , with their deployment illustrated in .",
  ": Overview of testbed setup": "Normal Behavior Collection. We enlisted volunteers to re-side in apartments and encouraged them to utilize equipment inaccordance with their individual habits. Throughout the designatedperiod of occupancy, we refrained from actively or directly inter-vening in the users behavior. However, we implemented a systemwhere users consistently logged their activities. Following the con-clusion of the data collection phase, we reviewed the device usagelogs via the smart home app, amalgamating these logs with theusers behavior records to compile a comprehensive user behaviordataset. To mitigate potential biases arising from acclimating to",
  "A.4Detailed experimental settings": "All models (including baselines and SmartGuard) are implementedby PyTorch and run on a graphic card of GeForce RTX 3090Ti. All models are trained with Adam optimizer with learningrate 0.001. We train SmartGuard to minimize L in Equation (14).During training, we monitor reconstruction loss and stop trainingif there is no performance improvement on the validation set in 10 steps. For model hyperparameters of SmartGuard, we set thebatch size to 512 and the initial weights of TTPE are =0.1, = 0.4, = 0.4, and = 0.7. For mask ratioand step without step, we search in {0.2, 0.4, 0.6, 0.8} and {3, 4, 5, 6},respectively. We chose the number of encoder and decoder layersin {1, 2, 3, 4}, and the embedding size in {8, 16, 32, 64, 128, 256, 512}.",
  "A.5Mask strategy deep dive": "To verify the effectiveness of LDMS, we compared it with the threebaselines (w/o mask, random mask and top- loss mask) mentionedin the section 4.2. As illustrated in (a), LDMS consistentlyoutperforms all other mask strategies across four types of anom-alies. The results presented in (b) further demonstrate thatLDMS exhibits the smallest variance in reconstruction loss through-out the training process, which demonstrates that SmartGuardlearns both easy-to-learn behaviors and hard-to-learn behaviorsvery well. We also plotted the loss distribution diagram under dif-ferent mask strategies. As shown in , LDMS shows thesmallest reconstruction loss and variance, which demonstrates thatour mask strategy can better learn hard-to-learn behaviors. We canstill observe behaviors with high reconstruction loss as pointed bythe red dashed arrow after applying LDMS, which is likely to benoise behaviors, thus its necessary to assign small weights for thesenoise behaviors during anomaly detection for avoiding identifyingnormal sequences containing noise behaviors as abnormal."
}