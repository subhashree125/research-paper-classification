{
  "ABSTRACT": "Baidu runs the largest commercial web search engine in China,serving hundreds of millions of online users every day in responseto a great variety of queries. In order to build a high-efficiency spon-sored search engine, we used to adopt a three-layer funnel-shapedstructure to screen and sort hundreds of ads from billions of adcandidates subject to the requirement of low response latency andthe restraints of computing resources. Given a user query, the topmatching layer is responsible for providing semantically relevant adcandidates to the next layer, while the ranking layer at the bottomconcerns more about business indicators (e.g., CPM, ROI, etc.) ofthose ads. The clear separation between the matching and rankingobjectives results in a lower commercial return. The Mobius projecthas been established to address this serious issue. It is our firstattempt to train the matching layer to consider CPM as an addi-tional optimization objective besides the query-ad relevance, via Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from 19, August 48, 2019, Anchorage, AK, USA 2019 Association for Computing Machinery.ACM ISBN 978-1-4503-6201-6/19/08...$15.00 directly predicting CTR (click-through rate) from billions of query-ad pairs. Specifically, this paper will elaborate on how we adoptactive learning to overcome the insufficiency of click history at thematching layer when training our neural click networks offline,and how we use the SOTA ANN search technique for retrievingads more efficiently (Here ANN stands for approximate nearestneighbor search). We contribute the solutions to Mobius-V1 as thefirst version of our next generation query-ad matching system.",
  "Sponsored search; query-ad matching; active learning; click-throughrate (CTR) prediction; approximate nearest neighbor (ANN) search": "ACM Reference Format:1Miao Fan, 2Jiacheng Guo, 2Shuai Zhu, 2Shuo Miao, 1Mingming Sun, 1PingLi. 2019. MOBIUS: Towards the Next Generation of Query-Ad Matching inBaidus Sponsored Search. In The 25th ACM SIGKDD Conference on Knowl-edge Discovery and Data Mining (KDD 19), August 48, 2019, Anchorage, AK,USA. ACM, New York, NY, USA, 9 pages.",
  "INTRODUCTION": "Baidu Search (www.baidu.com), as the largest commercial searchengine in China, daily serves hundreds of millions of online users inresponse to a great variety of search queries. It is common knowl-edge that advertising has been the main revenue source for allmajor commercial search engine firms in the world. In this paper,",
  "KDD 19, August 48, 2019, Anchorage, AK, USAM. Fan, J. Guo, S. Zhu, S. Miao, M. Sun, and P. Li": "we focus on explaining some of the recent exciting developmentand invention in Baidus Search Ads system (conventionally knownas the Phoenix Nest inside Baidu). As shown by , it playsa vital role in retrieving advertisements (ads) which are relevantto user queries to attract clicks as advertisers are willing to paywhen their ads get clicked. The goal of Baidu sponsored searchsystem is to form and nourish a virtuous circle among online users,advertisers, and our sponsored search platform.",
  "The organicsearch results": ": A screen-shot of Baidu Search results on mobilephones given a search query tourism in Alaska (in Chinese).Our sponsored search engine is in charge of providing helpfulads on each page before the organic search results. Conventional sponsored search engines generallydisplay ads through a two-step process. The first step is to retrieverelevant ads given a query, and the next step is to rank these adsbased on predicted user engagement. As a high-efficiency sponsoredsearch engine for commercial use in Baidu, we used to adopt a three-layer funnel-shaped structure to screen and sort hundreds of adsfrom billions of ad candidates subject to the requirement of lowresponse latency and the restraints of computing resources. Asillustrated in , the top matching layer is responsible forproviding relevant ad candidates to the next layer given a user queryand the rich profile of the user. To cover more semantically relevant",
  "Hundreds of Ad Candidates": ": The three-layer funnel-shaped structure of the pre-vious sponsored search system in Baidu. Given a user query,it is highly efficient to retrieve hundreds of relevant and high-CPM ads from billions of ad candidates. ads, query expansion and natural language processing(NLP) techniques are mostly leveraged. The ranking layer at thebottom concerns more about business indicators , such as costper mile (CPM = CTR Bid), return on investment (ROI), etc., ofthe filtered ads provided by the upper layer.However, this separation/distinction between matching and rank-ing objectives lead to a lower commercial return for various reasons.Given a user query, we have to employ complex models and tospend a lot of computing resources on ranking hundreds or eventhousands of ad candidates. Perhaps most disappointingly, the rank-ing models report that many relevant ads are not offered by highCPM and will not be displayed. To address this issue, Baidu SearchAds has set up the Mobius project which aims towards the nextgeneration query-ad matching system in Baidus sponsored search.This project is expected to unify the diverse learning objectives in-cluding the query-ad relevance and many other business indicatorstogether, subject to lower response latency, restraints of computingresources and tiny adverse impact on user experience.In this paper, we introduce Mobius-V1 which is our first attemptfor teaching the matching layer to take CPM as an additional opti-mization objective besides the query-ad relevance. In other words,Mobius-V1 has the capability of accurately and rapidly predictingclick-through rate (CTR) for billions of user query & ad pairs. Toachieve this goal, we must resolve the following major problems: Insufficient click history: The original neural click modelemployed by the ranking layer was trained by high-frequencyads and user queries. It tends to estimate a query-ad pair ata much higher CTR for display once either a high-frequencyad or a high-frequency query appears, even though theymight have low relevance. High computational/storage cost: Mobius is expected toforecast multiple indicators (including relevance, CTR, ROI,etc.) of billions of user query & ad pairs. It naturally faces thechallenge of greater consumption on computing resources. To address the problems above, we first design a teacher-studentframework inspired by the idea of active learning to aug-ment the training data for our large-scale neural click model to pre-dict CTR for billions of user query & ad pairs. Specifically, an offline",
  "Towards the NextGeneration": ": The distinct objectives of matching and ranking layer lead to lower CPM which is one of the key business indicatorsof a sponsored search engine. Therefore, we are engaged in building a high-efficient query-ad matching system (i.e., Mobius) inBaidu sponsored search. Mobius is expected to unify the learning objectives of the query-ad relevance and many other businessindicators together, subject to lower response latency, limitation of computation resources and tiny adverse impact on userexperience. For now, we have deployed the first version of Mobius (Mobius-V1) which can more accurately predict CTR forbillions of user query & ad pairs. data generator is responsible for constructing synthetic query-adpairs given billions of user queries and ad candidates. These query-ad pairs are constantly judged by a teacher agent which is derivedfrom the original matching layer and is good at measuring the se-mantic relevance of a query-ad pair. It can help detect the bad cases(i.e., high CTR but low relevance) in the synthetic query-ad pairs.Our neural click model, as a student, is taught by the additional badcases to improve the ability of generalization on tail queries and ads.To save the computing resources and satisfy the requirement of lowresponse latency, we further adopt the most recent state-of-the-artapproximate nearest neighbor (ANN) search and Maximum InnerProduct Search (MIPS) techniques for indexing and retrieving alarge number of ads more efficiently.To tackle the aforementioned challenges, Mobius-V1, as the firstversion of our next generation query-ad matching system, is anintegration of the solutions above and has already been deployedin Baidus sponsored search engine.",
  "VISION OF BAIDUS SPONSORED SEARCH": "For a long period of time, the funnel-shaped structure is a classicalarchitecture of sponsored search engines . The majorcomponents include the query-ad matching and ad ranking. Thequery-ad matching is typically a lightweight module which mea-sures the semantic relevance between a user query and billions ofads. In contrast, the ad ranking module should concern much morebusiness indicators such as CPM, ROI, etc., and use complex neuralmodels to sort hundreds of ad candidates for display. This decou-pled structure is a wise option to save the expensive computingresources in the early days. Moreover, it can also facilitate bothscientific research and software engineering as the two modules canbe assigned to different research/development teams to maximizeindividual objectives.Baidus sponsored search used to adopt a three-layer funnel-shaped structure which is shown by . The optimizationobjective of the top matching layer (denoted by O) is to",
  "=1Relevance (,).(1)": "However, according to our long-term analysis on the perfor-mance of Baidus sponsored search engine, we find out that thedistinction/separation between matching and ranking objectivestends to lead to lower CPM which is one of the key business indica-tors for a commercial search engine. It is unsatisfactory when themodels in the ranking layer report that many relevant ads providedby the matching layer will not be displayed on search results asthey are estimated not to have higher CPM.With the rapid growth of computing resources, the Baidu Searchads team (Phoenix Nest) has recently established the Mobiusproject which aims towards the next generation query-ad matchingsystem in Baidus sponsored search. The blueprint of this project asillustrated in looks forward to unifying multiple learningobjectives including the query-ad relevance and many other busi-ness indicators into a single module in Baidus sponsored search,subject to lower response latency, limited computing resources andtiny adverse impact on user experience.This paper will report the first version of Mobius, i.e., Mobius-V1,which is our first attempt to teach the matching layer consideringCPM as an additional optimization objective besides the query-adrelevance. Here we formulate the objective of Mobius-V1 as follows,",
  "MOBIUS: NEXT GENERATION QUERY-ADMATCHING SYSTEM": "Mobius is Baidus internal code name of this project. Coinciden-tally, the well-known Mobius Loop is also the birds eye view ofBaidus Technology Park in Beijing, China; see . Mobius-V1 is our first attempt (which has been successfully deployed)to transfer our neural click model to the matching layer directlyfacing billions of user query and ads. As the scale of input datadramatically increases, we need to re-train our neural click modeloffline and update the techniques of indexing and retrieving ads.",
  "Active-Learned CTR Model": "For over 6 years, Baidus sponsored search engine has been using thedeep neural networks (DNN) for the CTR model (of gigantic size).Recently, Mobius-V1 has adopted an innovative new architecture.An intuitive and simple way of building Mobius-V1 is to reuse theoriginal CTR model in the ranking layer. It is a large-scale and sparsedeep neural network (DNN) which is in favor of memorization.However, it suffers from a severe bias on CTR prediction of eitherthe user queries or the ads in the tail. Consider, as shown in ,the two queries Tesla Model 3 and White Rose requested bythe same user as in the search log. For the funnel-shaped structureadopted in the past, the relevance between the query Tesla Model3 and the ad Mercedes-Benz is firstly guaranteed by the matchinglayer. Then our neural click model in the ranking layer tends topredict a higher CTR on the query-ad pair as Tesla Model 3 isa high-frequency query and leaves a rich click history on the adMercedes-Benz in our search log. However, in Mobius-V1 weattempt to use our neural click network to directly handle billionsof query-ad pairs lacking the guarantee of relevance. It is naturalthat many irrelevant query-ad pairs come out (e.g., the query WhiteRose and the ad Mercedes-Benz in ). Nevertheless, wehave found out that our neural click model still tends to predicthigher CTR for those irrelevant query-ad pairs.",
  "CTR (User_x, White Rose, Mercedes-Benz) = 0.73": ": An example of a bad case that the original CTRmodel could not handle well. As the neural click networkemployed by the ranking layer was originally trained byhigh-frequency ads and queries, it tends to estimate a query-ad pair at a higher CTR once a high-frequency ad (e.g.,Mercedes-Benz in this case) appears, even though WhiteRose and Mercedes-Benz have little relevance. According to our analysis on the query log in Baidus sponsoredsearch, the ads and the user queries suffer from the long-tail effectand the cold start problem. Therefore, we can not directly leveragethe original neural click model to accurately predict CTR for billionsof user queries and ads in the tail. The key to the problem is howwe teach our model learning to recognize the low relevance buthigh CTR query-ad pairs as the bad cases.",
  "end while": "To solve this problem, we propose to use the original relevancejudger in the matching layer as the teacher to make our neuralclick model aware of the low relevance query-ad pairs. Our neu-ral click model, as the student, acquires the additional knowledgeon relevance from the augmented bad cases in an active learningfashion. illustrates the fashion by a flow diagram and Algo-rithm 1 shows the training procedure of teaching our neural clickmodel with active learning in pseudo code. Generally speaking, theiterative procedure of active learning has two phases: data augmen-tation and CTR model learning. To be specific, we will elaboratethe modules in each phase step by step.The phase of data augmentation starts from loading a batch ofclick history (i.e., user query & ad pairs) from query logs into a dataaugmenter. Every time the data augmenter receives the query-adpairs, it splits them into two sets: a query set and an ad set. Thenwe apply a cross join operation () to the two sets for the sake ofconstructing more user query & ad pairs. Suppose that there are queries and ads in the batch of click history, and then the dataaugmenter can help generate synthetic query-ad pairs. Afterlisting all possible query-ad pairs, the relevance judger involvesin and takes charge of grading the relevance of these pairs. As wewant to discover the low relevance query-ad pairs, a threshold is",
  "CTR Prediction": "CTR prediction is another core task in sponsored search,as it directly influences some business indicators such as CPM. Itfocuses on predicting the probability that an ad would be clickedif shown as a response to a submitted query. Conventional ap-proaches on CTR prediction preferred handcrafted features of adimpressions obtained from historical click data by Bayesian or feature selection methods . Along with the recent emer-gence of Deep Learning , many approaches for CTRprediction utilize various deep neural nets to primarily alleviateissues of creating and maintaining handcrafted features by learningthem automatically from the raw queries and textual ads. BaiduSearch Ads (Phoenix Nest) has been successfully using ultra-high-dimensional and ultra-large-scale deep neural networks for trainingCTR models since 2013.",
  "Fast Ads Retrieval": "In Baidus sponsored search engine, we have been using the deepneural networks (i.e., user query DNN and ad DNN) illustratedby to acquire both the embeddings of queries and ads,respectively. Given a query embedding, Mobius must retrieve themost relevant and the highest CPM ads from billions of ad candi-dates as stated in Eq. (2). Of course, it is unpractical to calculate itexhaustively for each query although the brute-force search cantheoretically discover all the ads (i.e., 100% ad recall) we are lookingfor. The online services often have restricted latency constraintsand the ad retrieval must be done in a short period. Thus, we exploitapproximate nearest neighbor (ANN) search techniques to speedup the retrieval process, as shown by .",
  "Vector Compression": ": The fast ad retrieval framework. The two types ofvectors will be compressed first to save the memory space.After that, two strategies can be applied: (a) ANN search bycosine similarity and then re-rank by the business relatedweight (BRW); (b) Ranking by exploiting weight, which is aMaximum Inner Product Search (MIPS) problem. 3.2.1ANN Search. As shown in , the mapping functioncombining user vectors and ad vectors by cosine similarity and thenthe cosine values go through a softmax layer to produce the finalCTR. In this way, the cosine value and the CTR are monotonicallycorrelated. After the model is learned, it will be clear they arepositively or negatively correlated. If it is negatively correlated,we can easily transfer it to positively correlated by negating thead vector. In this way, we reduce the CTR ranking problem into acosine ranking problem, which is a typical ANN search setting.The goal of approximate nearest neighbor (ANN) search is toretrieve, for a given query object, the most similar set of objectsfrom a large corpus, by only scanning a small fraction of objects inthe corpus. This is a fundamental problem and has been activelystudied since the early days in computer science . Typically,popular algorithms for ANN have been based on the idea of space-partitioning, including tree-based methods , random hashingmethods , quantization based approaches , random partition tree methods , etc. For this particularproblem (which deals with dense and relatively short vectors), wehave found that random partition tree methods are fairly effective.There is a known implementation of random partition tree methodscalled ANNOY, among other variants . 3.2.2Maximum Inner Product Search (MIPS). In the above solution,the business-related weight information is considered after the uservector and ad vector matching. In practice, this weight is vital inads ranking. To take into account the weight information earlier inthe ranking, we formalize the fast ranking process by a weightedcosine problem as follows:",
  ",(3)": "where is the business related weight, is user-query embeddingand is the ad vector. Note that the weighted cosine poses an innerproduct searching problem, often referred as Maximum Inner Prod-uct Search (MIPS) . In this line of work, multiple frameworkscan be applied for fast inner product search . 3.2.3Vector Compression. Storing a high-dimensional floating-point feature vector for each of billions of ads takes a large amountof disk space and poses even more problems if these features needto be in memory for fast ranking. A general solution is compress-ing floating-point feature vectors into random binary (or integer)hash codes , or quantized codes . The compressionprocess may reduce the retrieval recall to an extent but it may bringsignificant storage benefits. For the current implementation, weadopted a quantization based method like K-Means to cluster ourindex vectors, rather than ranking all ad vectors in the index. Whena query comes, we first find the cluster that the query vector isassigned to and fetch the ads that belong to the same cluster fromthe index. The idea of product quantization (PQ) goes onemore step further to split vectors into several subvectors and tocluster each split separately. In our CTR model, as mentioned in.1, we split both query embeddings and ad embeddingsinto three subvectors. Then each vector can be assigned to a tripletof cluster centroids. For example, if we choose 103 centroids foreach group of subvectors, 109 possible cluster centroids can be ex-ploited which is adequate for a billion-scale multi-index forads. In Mobius-V1, we employ a variant algorithm called OptimizedProduct Quantization (OPQ) .",
  "EXPERIMENTS": "We conducted thorough experiments on Mobius-V1 before inte-grating it into Baidus sponsored search engine. Specifically, wefirst need to conduct the offline evaluation of our CTR predictionmodel and the new approach on ad indexing. We need to make surethat our CTR model with the updated method on retrieving adscan discover more relevant ads with higher CPM. Then we tried todeploy it online to process a proportion of the query flow in BaiduSearch. After Mobius-V1 had passed both the offline evaluation andthe online A/B test, we launched it on multiple platforms to monitorthe statistics of CPM, CTR and ACP (i.e., average click price).",
  "Offline Evaluation": "We loaded the search logs to collect the click/unclick history andbuilt a training set which contains 80 billion samples. We also usedthe search log to construct the test set which has 10 billion recordson ad click/unclick history. We compare the effectiveness of ouractively learned CTR model with two baseline approaches. Onemethod is the 2-class CTR model adopted by the original ranking",
  "MOBIUS: Next Generation of Query-Ad Matching in Baidus Sponsored SearchKDD 19, August 48, 2019, Anchorage, AK, USA": "Ricardo Baeza-Yates, Massimiliano Ciaramita, Peter Mika, and Hugo Zaragoza.2008. Towards Semantic Search. In International Conference on Application ofNatural Language to Information Systems. Springer, 411. Xiao Bai, Erik Ordentlich, Yuanyuan Zhang, Andy Feng, Adwait Ratnaparkhi,Reena Somvanshi, and Aldi Tjahjadi. 2018. Scalable Query N-Gram Embeddingfor Improving Matching and Relevance in Sponsored Search. In Proceedings ofthe 24th ACM SIGKDD International Conference on Knowledge Discovery and DataMining (KDD). London, UK, 5261. Andrei Broder, Peter Ciccolo, Evgeniy Gabrilovich, Vanja Josifovski, DonaldMetzler, Lance Riedel, and Jeffrey Yuan. 2009. Online Expansion of Rare Queriesfor Sponsored Search. In Proceedings of the 18th International conference on WorldWide Web (WWW). Madrid, Spain, 511520.",
  "Brute Force100%---Original Vector+ANN+Re-Rank7.3%120ms74ms100%Compressed Code+MIPS40.5%30ms16ms5%": "layer which was trained solely by the click history without usingany augmented data. The other approach is a 3-class CTR modeltrained by the randomly augmented data without being judged bythe relevance model (teacher). As shown by , our model canmaintain a comparable AUC with the original ranking model butsignificantly improves the relevance model score (from 0.312 to0.575) measured by our relevance model. In other words, the low-relevance but high-CPM query-ad pairs are successfully recognizedas the bad cases by our new CTR model in Mobius-V1.Moreover, we delivered the top 100,000 query-ad pairs with thehighest CTR predicted by each approach to the CrowdsourcingTeam in Baidu, so as to manually grade the query-ad relevanceranging from 0 to 4 (0: no relevance, 4: quite relevant) by humanexperts. The report of subjective opinions also demonstrates thatour CTR model in Mobius-V1 performs well on discovering relevantquery-ad pairs. In addition, we used the same set of to retrieve adsfrom two ad indexing system powered by random partition trees(ANN+Re-Rank) and OPQ (Compressed Code+MIPS), respectively. shows that OPQ increases the ad coverage rate by 33.2%.",
  "Online A/B Testing": "The online A/B testing was conducted between two different adretrieval strategies employed by Mobius-V1 from the perspectives ofaverage response time and memory usage. shows that OPQcan provide much lower latency than random partition tree methodsand reduce the average response time by 48ms/query. Furthermore,we examined the average response time of the top 3% high-CPMads which have greater business value but require more computingresources. It shows that OPQ cuts down the query latency by 75%(from 120ms to 30ms) and substantially saves memory consumption.",
  "System Launching": "After Mobius-V1 had successfully passed both the offline evaluationand the online A/B test, we decided to launch it on multiple plat-forms in and outside Baidu. These platforms include the Baidu Appon mobile phones, Baidu Search on PCs, and many other affiliatedwebsites/apps that our sponsored search engine serves. shows the statistics on CPM, CTR, and ACP according to our 7-day monitor on the entire online traffic. CPM is the primary metric toevaluate the performance of a sponsored search engine. Comparedwith the previous system, Mobius-V1 leads to a major improvementof CPM by 3.8% on the Baidu App and 3.5% on the Baidu Search,which are the main portals of our sponsored search engine.",
  "Query-Ad Matching": "Query-ad matching is an extensively studied task which aimsto retrieve advertisements that are not only the same with butalso semantically similar to the given query (e.g., the query U.S.tourist visa and the ads about travel agencies displayed in Fig-ure 2). As queries are commonly short texts, this issue has beenmostly addressed by the techniques of query expansion ,query rewriting and semantic matching . Besidesthat we can leverage different NLP tools to directly compute thesimilarity between queries and textual ads, the semantic relation-ship between queries and ads can also be captured via learningfrom ad impressions. DSSM is a well-known learning-to-matchparadigm which leverages a deep neural architecture to capturequery intent and to improve the quality of the learned semanticmatch given the click information.",
  "Q: Motivation why do we propose the Mobius project?": "A: We used to adopt a three-layer funnel-shaped structure toscreen and sort hundreds of ads for display from billions ofad candidates. However, the separation/distinction betweenmatching and ranking objectives leads to a lower commercialreturn. To address this issue, we set up Mobius-V1 whichis our first attempt to make the matching layer take busi-ness impact measures (such as CPM) into account instead ofsimply predicting CTR for billions of query-ad pairs.",
  "Q: Challenges what challenges have we encountered whilebuilding Mobius-V1?": "A: The first problem is the insufficient click history for train-ing the neural click model which is expected to have thegeneralization ability on the long-tail queries and ads. Asthe original neural click model employed by the rankinglayer was trained by high-frequency ads and queries, it tendsto estimate a query-ad pair at a higher CTR once either ahigh-frequency ad or a high-frequency query appears, eventhough they have no relevance at all. Another problem isthe low retrieval efficiency and high memory consumptiondue to the increasing number of queries and ad candidatesthat Mobius has to handle.",
  "Q: Solutions how do we design and implement Mobius toaddress those challenges?": "A: To overcome the issue of insufficiency of click history,we design a teacher-student framework inspired by ac-tive learning to augment the training data. Specifically, anoffline data generator is responsible for constructing syn-thetic query-ad pairs given billions of user queries and adcandidates. These query-ad pairs are constantly fed into theteacher agent which is derived from the original matchinglayer and is good at measuring the semantic relevance of a query-ad pair. The teacher agent can help detect the badcases (i.e., with higher CTR but lower relevance) as the aug-mented data from the generated query-ad pairs. Our neuralclick model in Mobius-V1, as a student, is taught by the addi-tional bad cases to improve the ability of generalization. Tosave the computing resources and satisfy the requirement oflow response latency, we tested a variety of space partition-ing algorithm for the approximate nearest neighbor (ANN)search and we have found that for our datasets, OPQ isable to achieve good performance for indexing and retrievingbillions of ads more efficiently.",
  "FUTURE WORK": "Since 2013, Baidu Search Ads (a.k.a. Phoenix Nest) has success-fully deployed ultra-large-scale deep neural networks for trainingCTR models. To move beyond the CTR model, Mobius has beenestablished as an innovative and forward-looking project. The ideaof unifying the objectives of optimizing the user experience andbusiness target also inspires other featured products such as Feeds.For future work, many potential directions can be explored. Forexample, we expect to be able to bring more business targets suchas ROI (return on investment), as additional learning objectivesinto the matching layer so that we can discover more business-friendly ads. Along with more optimization objectives for billionsof candidate ads and queries, the computational complexity willsignificantly increase. There ought to be a trade-off between theeffectiveness and efficiency of our sponsored search engine giventhe requirement of lower response latency and the restraints ofcomputing resources.The crucial step in Mobius project is the fast ads retrieval task viaapproximate near neighbor search (ANN). The current system hasused the cosine similarity to approximate the CTR, based on theirmonotonic correlation. If the final layer is more complicated, it willbe problematic to rank by cosine (or weighted cosine). Searchingby complicated measures has been studied, for example , whichcould be adopted by future versions of Mobius. Another promisingdirection is to adopt a GPU-based system for fast ANN, which hasbeen shown highly effective for generic ANN tasks . We are deeply grateful to the contributions of many colleaguesfrom Baidu. A few names are Lin Liu, Yue Wang, Anlong Qi, LianZhao, Shaopeng Chen, Hanju Guan, and Shulong Tan; but there arecertainly many more who have contributed to this large project. Vibhanshu Abhishek and Kartik Hosanagar. 2007. Keyword Generation for SearchEngine Advertising Using Semantic Similarity between Terms. In Proceedings ofthe 9th International Conference on Electronic Commerce (EC). Minneapolis, MN,8994.",
  "Jerome H. Friedman, J. Bentley, and R. Finkel. 1977. An Algorithm for FindingBest Matches in Logarithmic Expected Time. ACM Trans. Math. Software 3 (1977),209226": "Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. 2013. Optimized ProductQuantization for Approximate Nearest Neighbor Search. In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition (CVPR). 29462953. Aristides Gionis, Piotr Indyk, and Rajeev Motwani. 1999. Similarity Search inHigh Dimensions via Hashing. In Proceedings of 25th International Conference onVery Large Data Bases (VLDB). Edinburgh, Scotland, UK, 518529. Thore Graepel, Joaquin Quionero Candela, Thomas Borchert, and Ralf Her-brich. 2010. Web-Scale Bayesian Click-Through Rate Prediction for SponsoredSearch Advertising in Microsofts Bing Search Engine. In Proceedings of the 27thInternational Conference on Machine Learning (ICML). 1320. Mihajlo Grbovic, Nemanja Djuric, Vladan Radosavljevic, Fabrizio Silvestri, Ri-cardo Baeza-Yates, Andrew Feng, Erik Ordentlich, Lee Yang, and Gavin Owens.2016. Scalable Semantic Matching of Queries to Ads in Sponsored Search Adver-tising. In Proceedings of the 39th International ACM SIGIR Conference on Researchand Development in Information Retrieval (SIGIR). Pisa, Italy, 375384. Mihajlo Grbovic, Nemanja Djuric, Vladan Radosavljevic, Fabrizio Silvestri, andNarayan Bhamidipati. 2015. Context- and Content-aware Embeddings for QueryRewriting in Sponsored Search. In Proceedings of the 38th International ACMSIGIR Conference on Research and Development in Information Retrieval (SIGIR).Santiago, Chile, 383392. Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, An-toine Atallah, Ralf Herbrich, Stuart Bowers, et al. 2014. Practical Lessons fromPredicting Clicks on Ads at Facebook. In Proceedings of the Eighth InternationalWorkshop on Data Mining for Online Advertising (ADKDD). New York, NY, 19. Piotr Indyk and Rajeev Motwani. 1998. Approximate Nearest Neighbors: TowardsRemoving the Curse of Dimensionality. In Proceedings of the Thirtieth AnnualACM Symposium on the Theory of Computing (STOC). Dallas, TX, 604613. Michael Jahrer, A Toscher, Jeong-Yoon Lee, J Deng, Hang Zhang, and JacobSpoelstra. 2012. Ensemble of Collaborative Filtering and Feature EngineeredModels for Click Through Rate Prediction. In KDDCup Workshop.",
  "Ping Li, Anshumali Shrivastava, and Christian A. Konig. 2012. GPU-based Min-wise Hashing: GPU-based Minwise Hashing. In Proceedings of the 21st WorldWide Web Conference (WWW). Lyon, France, 565566": "Ping Li and Martin Slawski. 2017. Simple Strategies for Recovering Inner Productsfrom Coarsely Quantized Random Projections. In Advances in Neural InformationProcessing Systems (NIPS). Long Beach, CA, USA, 45704579. H. Brendan McMahan, Gary Holt, David Sculley, Michael Young, Dietmar Ebner,Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, SharatChikkerur, Dan Liu, Martin Wattenberg, Arnar Mar Hrafnkelsson, Tom Boulos,and Jeremy Kubica. 2013. Ad click prediction: a view from the trenches. InProceedings of the 19th ACM SIGKDD International Conference on KnowledgeDiscovery and Data Mining (KDD). Chicago, IL, 12221230.",
  "Burr Settles. 2012. Active Learning. Synthesis Lectures on Artificial Intelligenceand Machine Learning 6, 1 (2012), 1114": "Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Grgoire Mesnil. 2014.Learning Semantic Representations Using Convolutional Neural Networks forWeb Search. In Proceedings of the 23rd International Conference on World WideWeb (WWW). Seoul, Korea, 373374. Anshumali Shrivastava and Ping Li. 2014. Asymmetric LSH (ALSH) for SublinearTime Maximum Inner Product Search (MIPS). In Advances in Neural InformationProcessing Systems (NIPS). Montral, Qubec, Canada, 23212329. Anshumali Shrivastava and Ping Li. 2014. In Defense of MinHash Over SimHash.In Proceedings of the Seventeenth International Conference on Artificial Intelligenceand Statistics (AISTATS). Reykjavik, Iceland, 886894.",
  "Xiao Yan, Jinfeng Li, Xinyan Dai, Hongzhi Chen, and James Cheng. 2018. Norm-Ranging LSH for Maximum Inner Product Search. In Advances in Neural Infor-mation Processing Systems (NeurIPS). 29562965": "Hsiang-Fu Yu, Cho-Jui Hsieh, Qi Lei, and Inderjit S. Dhillon. 2017. A GreedyApproach for Budgeted Maximum Inner Product Search. In Advances in NeuralInformation Processing Systems (NIPS). Long Beach, CA, 54595468. Wei Vivian Zhang, Xiaofei He, Benjamin Rey, and Rosie Jones. 2007. QueryRewriting Using Active Learning for Sponsored Search. In Proceedings of the 30thInternational ACM SIGIR Conference on Research and Development in InformationRetrieval (SIGIR). Amsterdam, The Netherlands, 853854. Yuyu Zhang, Hanjun Dai, Chang Xu, Jun Feng, Taifeng Wang, Jiang Bian, BinWang, and Tie-Yan Liu. 2014. Sequential Click Prediction for Sponsored Searchwith Recurrent Neural Networks. In Proceedings of the Twenty-Eighth AAAIConference on Artificial Intelligence (AAAI). Qubec City, Qubec, Canada, 13691375."
}