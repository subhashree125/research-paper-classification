{
  "False Information Prompt": ". Answers produced by state-of-the-art LLMs on Whats Rambos first name?with no perturbation (col. 1), with false information injection (cols. 2 & 3), and withrandom information injection (col. 4). Green boxes indicate correct answers; red areincorrect. The transparency of the boxes indicates the uncertainty of the model: i.e., thelighter, the more uncertain. Notice how injecting the same false information multipletimes makes LLMs more uncertain (see GPT-3.5) and can even shift their originalcorrect answer to a wrong one (see Mistral and LLaMA). Abstract. Large Language Models (LLMs) have revolutionized numer-ous applications, making them an integral part of our digital ecosys-tem. However, their reliability becomes critical, especially when thesemodels are exposed to misinformation. This paper primarily analyzesthe susceptibility of state-of-the-art LLMs to factual inaccuracies whenthey encounter false information in a Q&A scenario, an issue that canlead to a phenomenon we refer to as knowledge drift, which significantlyundermines the trustworthiness of these models. We evaluate the fac-tuality and the uncertainty of the models responses relying on En-tropy, Perplexity, and Token Probability metrics. Our experiments re-veal that an LLMs uncertainty can increase up to 56.6% when thequestion is answered incorrectly due to the exposure to false informa-tion. At the same time, repeated exposure to the same false informa-",
  "A. Fastowski et al": "6. Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: pre-training of deep bidirec-tional transformers for language understanding. In: Burstein, J., Doran, C., Solorio,T. (eds.) Proceedings of the 2019 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies,NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long andShort Papers). pp. 41714186. Association for Computational Linguistics (2019).",
  "Introduction": "The rapid advancement in natural language processing (NLP) has seen signif-icant strides with the development of large-scale language models, such as theGPT family. These models have demonstrated remarkable capabilities in vari-ous tasks, including text generation, translation, and question-answering (QA).However, despite their impressive performance, critical challenges remain in un-derstanding and improving these models reliability and robustness, especiallyregarding their factual knowledge and uncertainty estimation.Understanding the reliability of language models is crucial, particularly inapplications where the consequences of incorrect or uncertain answers can besignificant. For instance, in fields like healthcare, law, and education, the abilityto trust the outputs of a language model is paramount. One key aspect of thisreliability is the models ability to handle false or misleading information. Byevaluating how language models respond to false information, we can gain in-sights into their internal knowledge structures and the robustness of their factualaccuracy. With our work, we are assessing what we refer to as the knowledge driftof these models, by which we refer to any changes in their internal knowledgeand beliefs. In our case, we are interested in models knowledge drift as a resultof manipulative user interactions.Specifically, we examine the impact of knowledge drift on model answer un-certainty by studying the effect of false information presented to it with theprompt. By leveraging the TriviaQA dataset , we aim to analyze how themodels performance varies with different types of misleading information. Suchinsights can help us identify potential vulnerabilities in its knowledge processing. Contributions. In this paper, we contribute to this body of research by evaluatingLLMs i.e., GPT-4o, GPT-3.5, LLaMA-2-13B, and Mistral-7B responses tofalse information in a QA task setting. By analyzing the models uncertaintyunder various metrics and answer accuracy under different conditions, we aimto shed light on the robustness of their knowledge obtained during pre-training.This paper provides three main contributions to this topic: 1. Impact of False Information on Uncertainty. We investigate how introduc-ing false information into question prompts affects LLMs performance anduncertainty estimation in a QA setting. Our analysis reveals that while false",
  "Understanding Knowledge Drift in LLMs through Misinformation3": "information initially increases the models uncertainty, repeated exposurecan lead to decreased uncertainty, indicating successful manipulation anddrift of the model away from its original, correct beliefs. 2. Effect of Random Information. We demonstrate that random, unrelated in-formation results in the highest levels of model uncertainty, suggesting thatthe model experiences greater confusion with irrelevant data than with tar-geted false information. This finding underscores the importance of contextrelevance in understanding the models responses. 3. Insights into Model Vulnerabilities and Robustness. Our study provides crit-ical insights into the vulnerabilities of LLMs to adversarial inputs. By high-lighting the limitations of current uncertainty estimation methods in adver-sarial attack detection, our work contributes to efforts aimed at enhancingthe robustness and trustworthiness of language models for practical applica-tions.",
  "Related Work": "Language model architectures and capabilities. Understanding LLM knowledgedrift necessitates grounding in the foundational works on language models. Pi-oneering efforts such as GPT , BERT , or T5 remain fundamentalto contemporary LLMs. This groundwork facilitated the recent advancementsin LLM capabilities, exemplified by Brown et al.s exploration of few-shotlearning and the development of powerful models like GPT-4 , or PaLM with its pathway architecture. Furthermore, the emergence of efficient and open-source models like LLaMA and its successors highlight the ongoing progressin LLM accessibility and development. Uncertainty in LLMs. While prior work has explored uncertainty quantificationin NLP tasks like calibration of classifiers and text regressors (), theseapproaches often rely on techniques directly transferable from other domains(e.g., Monte Carlo dropout, Deep Ensembles). However, as highlighted by ,generative tasks in NLP present unique challenges due to semantic equivalence.For instance, Jiang et al. demonstrate a weak correlation between answerconfidence (log-likelihood) and correctness in generative question answering.Recent efforts have tackled uncertainty or calibration in Natural LanguageGeneration (NLG) by prompting models to assess their outputs or fine-tuningmodels to predict uncertainty (). While these methods can be effective,they often require additional training data and supervision, leading to challengesin reproducibility, cost, and sensitivity to distribution shifts (e.g., hardware lim-itations preventing implementation as in ).The challenges associated with uncertainty estimation in NLG mirror thosein automatic NLG evaluation. For example, Ott et al. highlight performancelimitations in machine translation due to multiple valid translations for a singlesource sentence. Similarly, Sai et al. discuss the potential of paraphrasedetection for NLG evaluation, which may offer insights applicable to uncertaintyestimation tasks.",
  "Experiments": "In this work, we evaluate the factual knowledge of recent large language mod-els and their associated uncertainty levels in a Question Answering (QA) tasksetting. The objective is understanding how false information embedded in thequestion prompts influences the models performance and uncertainty metrics.We expect that the more false information is fed to these language models, themore certain they will become about it while giving up on accuracy since theybegin generating false information.",
  "Experimental Setup": "Dataset and Models. We have two requirements for choosing suitable LLMsfor our experiments: 1) performing reasonably well on closed-book question-answering without additional fine-tuning, and 2) providing access to the logprobabilities of the generated tokens. Hence, we experiment with GPT-4o, GPT-3.5, Mistral-7B, and LLaMA-2-13B1.To assess the first requirement, we test the models on 1000 samples from theTriviaQA dataset by prompting them with the given question. TriviaQA isa reading comprehension dataset consisting of question-answer-context triplets,where the contexts are text snippets containing the answer. In our setting, weignore the contexts and use only the question-and-answer pairs. We also ask the",
  "model to respond only with the exact answer to avoid verbosity in the modelsanswers and ease the comparison with the correct answers": "Performance evaluation. Our experiments are designed to assess two primaryfactors: i.e., 1) the correctness of the answers provided by the model, and 2) theuncertainty scores associated with the generated tokens.We begin by identifying questions from the TriviaQA dataset that the LLMcan answer correctly in a closed-book setting i.e., the model answers withoutadditional context or external information. This process allows us to focus onlyon the models correct knowledge since we will later try to manipulate it. Toassess the correctness of a response, we check if the true answer provided by thedataset is part of the model-generated answer. For example, the model mightproduce \"Chicago, Illinois\" as its answer, when the ground truth is \"Chicago\". Inthis example, our process would check if \"Chicago\" is part of \"Chicago, Illinois\",correctly marking the model answer as true (eventhough more verbose). shows the accuracy of each model when prompted once with the questions fromthe dataset.",
  "LLaMA-2-13B0.4281.3 1010": "We intentionally chose LLMs of varying levels of QA performance, as this dif-ference may lead to additional interesting insights about uncertainty develop-ments. For the subsequent experiments, only the samples answered correctly bythe model will be used for their respective evaluations. Uncertainty metrics. Given an input sequence x and parameters , an autore-gressive language model generates an output sequence y = [y1, ..., yT ] where Tis the length of the sequence. To quantify the models uncertainty, we rely onentropy (1) and perplexity (2), as previously introduced by . For calculatingthe entropy of each token, we take into account the top i = 10 probable tokensat each token position t. Lastly, as a more intuitively interpretable metric, we re-port the probability (3) of the generated tokens, averaged over all answer tokens.While the use of multiple metrics ensures the robustness in our measurements,they also capture slightly different dimensions: entropy focuses more on a token-level uncertainty, since we measure over multiple token options at each position,whereas perplexity and probability operate on more of a sentence level, simplyaveraging over all top-1-choice tokens in the generated sequence.",
  "texp(log p(yt | y<t, x))(3)": "Baseline and information injection. We establish a baseline by evaluating themodels answers to the identified questions and their corresponding uncertaintyscores. This baseline represents the models performance and certainty withoutany misleading information. We then introduce false information into the ques-tion prompts and measure its impact on the models answers and uncertaintyscores. Notice that we do not provide the model with the correct answer or pos-sible options for the posed question. Therefore, for visualization purposes, thefollowing prompts contain a straight line that separates the actual prompt fromthe ground truth. Additionally, the words before the colon are not included weuse them to give context to the reader. We rely on two types of prompts:",
  "Prompt V2:............Respond with the true, exact answer only": "While the first prompt simply asks the model to answer the question, the secondemphasizes choosing what the model believes to be the factually correct answer.This way, regardless of the injected false information in the overall prompt, we aim to reduce possible sycophantic behavior. Notice that the ... represent eithera FIP or RIP described above, followed by the question. In the above promptinstructions, exact aims to limit the answers verbosity, while true and exactadditionally emphasizes the truthfulness we wish to see from the LLM.",
  "Results and Discussion": "Evaluating Model Uncertainty and Knowledge Retention with Varying PromptIntegrity. Tables 2 and 3 show the changes in uncertainty scores according toPrompt V1 and V2, respectively. We report averages ( standard errors) over10 runs on the questions each model answered correctly. So, for example, if GPT-4o answered correctly 79% of the time, we test its uncertainty on this data subset.We are aware that the correct answers two models produce2 might be relatedto different questions, which, in turn, could result in unfair comparisons. Nev-ertheless, we argue that testing the models uncertainty on inherently incorrectanswers is insignificant since it does not provide any added value to studying theknowledge drift of LLMs. Therefore, we aim to verify how the correct knowledgeshifts when prompts are tainted with false information.It is interesting to note that we did not expect the models to produce incorrectanswers on the baseline (B) (see Tables 2 and 3) since the prompt has notchanged from the one used to identify the correct knowledge in . Still,when prompted multiple times, the models might engender wrong answers wenoticed a 1-2% drop of accuracy3 w.r.t. what was reported in . However, asexpected, all models have higher uncertainty scores on the incorrect answers forboth prompt types on B. All the reported metrics reflect this, i.e., higher entropy,higher perplexity, and lower token probability indicate higher uncertainty.These results are also consistent with FIP and RIP across the board, sug-gesting that false/random information has the same effect regarding correct vs.incorrect answers. Notice how, even when we inject false/random informationinto the prompts, the uncertainty levels on the correct answers remain similar(e.g., see GPT-3.5 B vs. FIP/RIP), suggesting that the correct models knowl-edge remains unscathed. Contrarily, we notice a drop in the latter if we lookat the uncertainty difference between B and FIP, e.g., in terms of entropy. We",
  "RIP 0.21.007 1.11.006 0.93.003 0.36.013 1.21.012 0.87.006": "argue that this happens because the incorrect answers likely reflect the incor-rect information embedded in the FIP, pushing the model to be overconfidentto distill fake information. Answers obtained with RIP generally exhibit highuncertainty scores since the given random information has nothing to do withthe question, confusing the model even more. This effect is visible especially inincorrect answers, though also present with correct answers. At the same time,it does not lead to major drops in accuracy for the same reason, i.e., the promptis not designed to target knowledge associated with the question.",
  "% FIP10 vs. B-30.4%-17.8%-41.1%-30.9%-73.5%-69.4%-80.9%-78.3%": "Influence of Repeated False Information on Model Confidence and Accuracy. Toassess the impact of false information reoccurrence on the uncertainty levels ofeach model, we perform an ablation study where we repeat the FIP k withk {1, 2, 5, 10}. This means, the prompt includes the false information snippetk times, and is then followed by the question itself. Figures 2 and 3 illustratethis effect for Prompt V1 and V2, respectively. Notice the general trend for allmodels on the uncertainty of the incorrect answers when k increases: all modelsbecome consistently less uncertain about their incorrect answers (drops in en-tropy and perplexity, increase in token probability). Meanwhile, the uncertaintyabout the correct answers hits a plateau. When prompted multiple times withfalse knowledge, we suspect that the models become convinced of the new, con-tradicting information presented to them, hence their uncertainty about givingincorrect responses drops. Similarly, we want to see the effect of FIP on the over-all accuracy of the models w.r.t. the baseline B. shows this phenomenonfor both prompt versions. As expected, the accuracy of correctly answering thegiven questions degrades abruptly when k increases. This is mostly emphasizedfor LLaMA-2-13B, reporting a degradation of 80.9% on FIP10. Interestingly,the accuracy degradation for Prompt V2 is lower than that of V1. We arguethat this happens due to specifically prompting the models to respond truthfullyhere. Uncertainty levels as indicators for adversarial attacks. In our setting, the FIPscan be framed as adversarial prompts, as they aim to shift the previous, correctbehaviour of the models. It is intuitive to assume rising uncertainty levels tobe indicative for such adversarial attacks, i.e. sudden higher confusion in themodel should indicate the model is currently being manipulated. While this canhold for single infusions of false information, uncertainty instead decreases themore false information is being presented, convincing the model. Hence, we argueuncertainty to not be a suitable tool for adversarial attack detection, calling fordifferent methods and rendering this potential approach inappropriate.",
  "Conclusion": "In this study, we explored the knowledge drift of GPT-4o, GPT-3.5, Mistral-7B, and LLaMA-2-13B through the impact of false information on their perfor-mance and uncertainty within a Question Answering setting using the TriviaQAdataset. Our findings reveal that presenting false information to the modelssuccessfully introduces knowledge drift in their responses, as well as generallyincreases the responses uncertainty. Notably however, repeated exposure to thesame false prompts can convince the model of the given false information, lead-ing it to be more certain of the incorrect answers. Additionally, we observed thatrandom and unrelated information results in the highest uncertainty, highlight-ing the models greater confusion with irrelevant and noisy data.Our findings provide valuable insights to the drift that is possible in LLMsinternal knowledge structures, and underscore the complexities in the knowl-edge processing of LLMs as well as its reflections in their uncertainty levels.With this, we aim to contribute to the broader effort to improve the robustnessand reliability of language models, particularly in the face of adversarial inputs.Understanding and enhancing their resilience remains imperative as languagemodels become increasingly integrated into critical applications.In the future, we will explore these dynamics across different datasets anddevelop advanced techniques to enhance LLMs robustness and trustworthinessfurther. An especially interesting extension of this study would be to infusethe false information into the model through continued training on the falsedata instead of presenting the false information during inference in the formof prompting, since this approach does effectively not change anything aboutthe models internal knowledge. Lastly, we aim to incorporate an adversarialprotection mechanism into state-of-the-art models to ensure their effective andsafe deployment in real-world scenarios. 1. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida,D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXivpreprint arXiv:2303.08774 (2023) 2. Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-lakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shotlearners. Advances in Neural Information Processing Systems 33, 18771901 (2020)",
  ". Glushkova, T., Zerva, C., Rei, R., Martins, A.F.: Uncertainty-aware machine trans-lation evaluation. arXiv preprint arXiv:2109.06352 (2021)": "8. Huang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H., Chen, Q., Peng, W.,Feng, X., Qin, B., et al.: A survey on hallucination in large language models: Prin-ciples, taxonomy, challenges, and open questions. arXiv preprint arXiv:2311.05232(2023) 9. Jiang, Z., Araki, J., Ding, H., Neubig, G.: How can we know When language modelsknow? on the calibration of language models for question answering. Trans. Assoc.Comput. Linguistics 9, 962977 (2021). 10. Jiang, Z., Araki, J., Ding, H., Neubig, G.: How can we know when language modelsknow? on the calibration of language models for question answering. Transactionsof the Association for Computational Linguistics 9, 962977 (2021)",
  ". Joshi, M., Choi, E., Weld, D.S., Zettlemoyer, L.: Triviaqa: A large scale dis-tantly supervised challenge dataset for reading comprehension. arXiv preprintarXiv:1705.03551 (2017)": "12. Kadavath, S., Conerly, T., Askell, A., Henighan, T., Drain, D., Perez, E., Schiefer,N., Hatfield-Dodds, Z., DasSarma, N., Tran-Johnson, E., et al.: Language models(mostly) know what they know. arXiv preprint arXiv:2207.05221 (2022) 13. Kuhn, L., Gal, Y., Farquhar, S.: Semantic uncertainty: Linguistic invariances foruncertainty estimation in natural language generation. In: The Eleventh Interna-tional Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May1-5, 2023. OpenReview.net (2023)",
  ". Sai, A.B., Mohankumar, A.K., Khapra, M.M.: A survey of evaluation metrics usedfor nlg systems. ACM Computing Surveys (CSUR) 55(2), 139 (2022)": "24. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,Rozire, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficientfoundation language models. arXiv preprint arXiv:2302.13971 (2023) 25. Wang, Y., Beck, D., Baldwin, T., Verspoor, K.: Uncertainty estimation and reduc-tion of pre-trained models for text regression. Transactions of the Association forComputational Linguistics 10, 680696 (2022) 26. Youssef, P., Koras, O.A., Li, M., Schltterer, J., Seifert, C.: Give me the facts! Asurvey on factual knowledge probing in pre-trained language models. In: Bouamor,H., Pino, J., Bali, K. (eds.) Findings of the Association for Computational Lin-guistics: EMNLP 2023, Singapore, December 6-10, 2023. pp. 1558815605. Associa-tion for Computational Linguistics (2023)."
}