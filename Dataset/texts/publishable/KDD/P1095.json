{
  "Abstract": "Customer service is often the most time-consuming aspect for e-commerce web-sites, with each contact typically taking 10-15 minutes. Effectively routing cus-tomers to appropriate agents without transfers is therefore crucial for e-commercesuccess. To this end, we have developed a machine learning framework that pre-dicts the complexity of customer contacts and routes them to appropriate agentsaccordingly. The framework consists of two parts. First, we train a teacher modelto score the complexity of a contact based on the post-contact transcripts. Then,we use the teacher model as a data annotator to provide labels to train a studentmodel that predicts the complexity based on pre-contact data only. Our exper-iments show that such a framework is successful and can significantly improvecustomer experience. We also propose a useful metric called complexity AUCthat evaluates the effectiveness of customer service at a statistical level.",
  "Introduction": "E-commerce businesses face a significant influx of customer service requests on a daily basis, mak-ing it one of the most time-consuming aspects of the industry. Processing each request typicallytakes 10-15 minutes. To illustrate, consider a company with 1 million users, where even a smallpercentage of customers requiring assistance, such as 0.1%, would result in over 10000 minutesspent on servicing customers daily. For companies with hundreds of millions of users, this numberbecomes overwhelmingly high and can become a bottleneck to expanding their business. Effectivelyguiding customers to the right service agents is crucial for achieving success in the industry, as em-phasized by Pi et al. (2023). However, the support requirements of customers can vary significantlydepending on the nature of their issues. Complex problems may demand the expertise of senioragents, while junior agents can effectively handle simpler issues such as returns or refunds. Misdi-recting a customer with a complex issue to a junior agent can result in poor customer experiences,involving transfers or repeated contacts. On the other hand, directing a customer with a simple issueto a senior agent is costly for the business and restricts the availability of senior agents to addresscomplex problems. Consequently, there is a growing demand for machine learning tools that canidentify the complexity of customer requests from the beginning and appropriately route them to thedesignated agent, thereby enhancing the overall customer experience.",
  "Hypothesis of Complexity": "Our approach to defining complexity involves initially training a machine learning classifier thatemploys the contact transcripts as inputs to predict the corresponding product or service labels (seeAppendix for an example transcript). We presume that this classifier possesses the fundamental do-main knowledge of the core business. If this is true, we conjecture that complexity can be describedusing the following quantities: Length: Contacts that are high in complexity generally necessitate multiple steps to diag-nose the underlying issues, resulting in longer transcripts than those with lower complexity.This can be mathematically represented by the number of sentences spoken by the agent(s)involved in the contact, as follows:",
  "i=1Si,agent": "Here, denotes the Kronecker delta function, Si represents the speaker of the i-th sentencein the contact transcript, and N denotes the total number of sentences in the transcript. Thevalue of L represents the total number of sentences spoken by the agent(s) in the contact,which can be used to measure the contacts complexity. Uncertainty: High-complexity contacts can be more challenging to comprehend, whichcan result in a greater level of uncertainty for the classifier regarding the customers issues.This can be mathematically represented by the entropy of the classifiers output distribution,as follows(Shannon, 1948):H =",
  "cpclog(pc)": "Here, p(c) denotes the probability of a particular class c being predicted by the classifier,and H represents the entropy of the classifiers output distribution. A higher entropy valueindicates greater uncertainty in the classifiers predictions, which may occur with high-complexity contacts. Skillfulness: Contacts that are high in complexity generally require a higher degree of skillto resolve, necessitating a highly proficient model that can effectively handle such contacts.This can be mathematically represented by the integral of the KL divergence(Kullbackand Leibler, 1951) between the outputs of using fewer trees and more trees in a gradientboosting trees model(Ke et al., 2017), as follows:",
  "iDKL(Pi||PM)": "Here, Pi denotes the output distribution using the first i trees, and M represents the totalnumber of trees in the model. It is essential to note that the KL boosting function (i) ex-hibits a slow decay to zero when more knowledge or skills are necessary, and a rapid decayto zero when less knowledge or skills are required. (a) demonstrates two examples ofhow the divergence boosting function behaves with more (red) and less (blue) skills.",
  "The Dataset": "Until now, there has been a dearth of publicly available datasets pertaining to customer servicetranscripts. To address this gap, we collaborated with customer service team to launch this researchproject, leveraging a dataset from an online chat system that allows customers to communicate withcustomer service representatives. We collected a total of 500,000 contact transcripts during 2022,",
  "(e)": ": ((a)-(c) We present the distribution of the sum of the three complexity measures usingvarious weights on length. (d) We convert the distribution from (c) into a uniform distribution usingquantile transformation. (e) The probability of finding high/medium/low complexity in differentscores is shown. Over 400 ground truth labels were generated via senior agents. The dash linesrepresent true numbers, while the solid lines are fitted curves using polynomial functions. Theblue line represents low complexity, the red line represents medium complexity, and the green linerepresents high complexity. each contact is associated with a unique label indicating the product or service discussed by thecustomer and agent. It is essential to note that the dataset only comprises customer text data, withall sensitive information, such as names and account details, anonymized to preserve confidentialitybefore sharing with researchers. Despite the dataset being pulled from a specific database, themethodology introduced in this article can be adapted to other scenarios as well.",
  "Complexity Score": "We employed a LightGBM(Ke et al., 2017) classifier with 60 boosting trees to predict the label of aproduct or service among 152 classes using TF-IDF(Rajaraman and Ullman, 2011) vectors generatedfrom 500K transcripts. The resulting L, H and S distributions are illustrated in (b)-(d), whichdemonstrate a long-tailed structure with high-complexity contacts located in the tail (red) regionand low-complexity contacts in the head (green) region. However, using three different complexitymeasures can be complicated for businesses. Therefore, we aimed to introduce a single variable thatrepresents complexity in a more straightforward manner. To achieve this, we converted all attributesto a normal distribution through quantile transformation(Gilchrist, 2000), ensuring that they havethe same scale and can be combined safely. We denoted the transformed variables as LN, HN, andSN, and we calculated a single variable Q using the following formula:",
  "Q = T UG (w LN + HN + SN )": "Here, we introduced a weight number w on LN for two reasons: first, short transcripts are unlikelyto be high-complexity contacts, and giving a higher weight to this attribute can increase the modelsprecision. Second, we found that an appropriate weight on LN can transform the distribution of thesum in the bracket from a skewed distribution to a Gaussian distribution that satisfies our expectation,where only a few contacts are extremely simple or extremely complex, as shown in (a)-(c). Wedetermined that using w = 2 resulted in a distribution closest to a Gaussian distribution and willuse this value unless otherwise stated. Additionally, we utilized another quantile transformation,denoted as T UG , to convert the Gaussian-like distribution to a uniform distribution within the bracket,enabling us to have a complexity score between 0 to 1 as shown in (d). The complexity scoreQ represents the percentile of a contacts complexity among all contacts. For example, a complexityscore of Q = 0.95 indicates that the contacts complexity is higher than that of 95% of all contacts.",
  "Validation": "We validated our complexity score using both objective and subjective metrics. For objective met-rics, we visually inspected 400 examples of high- and low-complexity contacts with complexityscores Q > 0.95 and Q < 0.05, respectively. Our survey showed that only 26% of high-complexitycontacts were resolved, while 62% required at least one transfer. In contrast, 85% of low-complexitycontacts were resolved, and only 11% required at least one transfer. This significant difference be-tween the two groups demonstrates the effectiveness of our complexity score. For subjective metrics, we collaborated with senior service agents to label the complexity of 400examples. We ensured at least two agents reviewed each contact and reached a consensus label oflow, normal, or high complexity. (e) presents the results of binning the complexity scoresinto 20 intervals and calculating the probability of finding a low, normal, or high complexity examplein each interval. The likelihood of finding a low complexity contact gradually decreases to 0.1 asthe complexity score increases, while the probability of finding a high complexity contact rapidlyincreases to 0.5 after a complexity score of 0.6. This confirms the consistency of our complexityscore with the agents understanding of contact complexity.",
  "Model Training": "We developed a teacher model that analyzes customer-agent conversation transcripts to calculatetheir complexity. However, as these transcripts are only available after each contact is finished, i.e.a post-contact feature, they cannot be used for intelligent routing. To solve this issue, we used thecomplexity scores generated by our teacher model as ground truth labels and trained a machinelearning model using pre-contact data only. Our proof of concept experiment aims to route highcomplexity contacts to senior agents, as these interactions have the greatest impact on customerexperience.",
  "To train the model, we followed these steps:": "First, we defined high complexity contacts as those with a complexity score Q of 0.8 orhigher and labeled them as y = 1. Contacts with a score below 0.8 were labeled as y = 0.We then trained a binary classifier based on these labels. We gathered more than 100 pre-contact features, such as customer profiles, purchase his-tory, contact history, and device usage, all of which were accessible at the beginning ofeach interaction. These features were employed to train a gradient boosting tree model(Keet al., 2017; Chen and Guestrin, 2016; Prokhorenkova et al., 2018). To handle the categori-cal features, we used the entity embedding method proposed by (Guo and Berkhahn, 2016)to convert them into continuous variables. Using this setup, we found that our model achieved a precision of 0.56 and recall of 0.28.In comparison, a model without entity embedding achieved a precision of 0.54 and recallof 0.05. Our results demonstrate that using entity embedding can significantly improve therecall rate and overall performance of the model.",
  "Experimental Results": "(d)-(f) depicts the dual transformation curve using different benchmark and target groups.Each subfigure contains two curves. The blue curve represents the case where the benchmark andtarget distributions are identical, resulting in an identity transform and a straight line as a reference. On the other hand, the green curve uses the true distribution of the target group, resulting in deviationfrom the blue curve. In (d), the background group is used as the benchmark and the controlgroup as the target. As shown in (b), the control group has the highest complexity contacts foraverage agents, hence the green curve is concave with an AUC of 0.69. We define the complexityeffectiveness as = 1 AUC/0.5, and the effectiveness of the control group is -39.6%, meaningthat it is approximately 40% less effective than the background. Similarly, in (e), the curves arevery close with AUC about 0.48, indicating that senior agents handling high complexity contacts areas effective as average agents handling normal complexity contacts. In (f), we compare thecontrol group as the benchmark and treatment as the target. The AUC is only 0.29, indicating thatsenior agents are 41.2% more effective than average agents in handling high complexity contacts. The analysis conducted can be expanded to encompass various product lines, as illustrated in in the appendix. The benchmark for this analysis is the background group, while the target groupcomprises contacts associated with different products or services such as Tablet, Smart Speaker,TV Stick and so on. To ensure the confidentiality of business information, the specific names ofthe products have been withheld. Through discussions with service agents, we have confirmed thatProduct-1 consistently receives the highest complexity scores. This is primarily attributed to the factthat most issues related to Product-1 require troubleshooting and cannot be resolved through simplereturn or refund processes.",
  "complexity score distributions for different groups, including (a) the background group, consistingof 10K randomly selected contacts (b) the control group, and (c) the treatment group": "The teacher model assumes a uniform distribution of overall complexity scores, so the complexityscore distribution for (a) is expected to be uniform. The distribution of complexity scores for (b)is negatively skewed, indicating that the student model correctly captures high complexity contacts.This suggests that contacts in the control group were lengthier, more uncertain, and required moreskills from regular agents than contacts in (a). In (c), the distribution of complexity scores tendsto be much lower than (b). Senior agents have more experience dealing with high complexity con-tacts, allowing them to handle such contacts more efficiently, with less ambiguity and fewer requiredskills. This reduces the complexity of the conversations, enabling the classifier to make the right de-cision with fewer trees and higher confidence. As a result, the complexity score in the treatmentgroup is much lower than that in the control group. In summary, our model can filter out high com-plexity contacts, and routing them to senior agents can significantly improve the overall customerexperience, as seen in the difference between (b) and (c).",
  "Routing Metrics": "Upon analyzing the advantages of directing high complexity contacts to senior agents, we conducteda comparison of essential metrics. Remarkably, the treatment group displayed a transfer rate approx-imately 53% lower than the control group. In terms of multi-transfers, the treatment group experi-enced a reduction of over 95% compared to the control group, effectively eliminating the majority ofmulti-transfers. Additionally, the treatment group showcased an average handle time that was 13%lower than the control group. These findings validate the effectiveness of our teacher-student model,showcasing its capacity to significantly enhance the customer experience by reducing transfers anddecreasing average handle times.",
  "; Fawcett, 2006; Kleiman and Page, 2019), for assessing the routing businesss effectiveness.To achieve this, we must introduce the notion of dual transformation": "Suppose we have two groups of contacts, the target group and the benchmark group, and we wantto compare whether the service agents handle the target group more effectively than the benchmarkgroup. The first step in defining Complexity AUC is to find a non-linear transformation that mapsthe complexity score distribution of the benchmark group to that of the target group. In other words,we need to find a transformation that makes the benchmark distribution identical to the target distri-bution. However, directly searching for such a transformation is difficult. One possible solution is to firstmap both distributions to a normal distribution. This is achieved by using operators T NB and T NTto transform QB and QT , respectively, to a normal distribution with mean = 0 and variance = 1,denoted as N( = 0, = 1):",
  "Here, T NB is an operator that transforms QB to N, and T NT is an operator that transforms QT to N": "In general, T N 1Talways exists provided that QT is differentiable, which can be achieved throughvarious methods such as Power transformation (Yeo and Johnson, 2000; Box and Cox, 1964) orQuantile transformation (Gilchrist, 2000). Therefore, we can always find the dual transformationT TB without difficulty. The meaning of T TB is also clear. It tells us how to shift the complexity scoresin the benchmark group to make its distribution identical to that of the target group.",
  "Area Under Curve": "Lets explore the mathematical properties of T TB . Since T TB is essentially a function that transformsa complexity score in the benchmark group to the corresponding score in the target group, xT =f(xB) T TB xB, where xB is a complexity score in the benchmark group. If we change xB linearlyfrom 0 to 1, we obtain a curve in a 2D plane where the x-axis represents xB and the y-axis representsf(xB). When the target distribution is identical to the benchmark distribution, the operator T BT is an identityoperator, resulting in a straight line connecting (0,0) and (1,1) with an area under the curve of 0.5,indicating that the complexity of the target group is statistically similar to that of the benchmarkgroup. However, a concave curve for f(xB) results in an area greater than 0.5, indicating morenegative skewness and greater complexity in the target group, while a convex curve leads to an arealess than 0.5, indicating more positive skewness and less complexity. When contact complexity islower than the background complexity, agents can handle the contact more efficiently, confidently,and with less ambiguity, reducing the likelihood of transfers or unresolved contacts. Therefore, thearea under the curve (Complexity AUC) evaluates how effectively agents manage specific contactgroups. The following guidelines may be used to interpret Complexity AUC:",
  "Conclusion": "We have developed an intelligent routing mechanism based on a teacher-student learning framework.The teacher model profiles the complexity of a contact based on the customer and agent conversationtranscript, which we then use as a data annotator to provide labels to train a student model thatpredicts complexity using pre-contact features such as customer profile, purchase history, contacthistory, and device usage statistics. Our experiments have shown that both the teacher and studentmodels are highly aligned with our expectations regarding the properties of complexity and cansignificantly improve customer experience. Furthermore, we proposed a novel machine learning metric called Complexity AUC, which evaluatesthe effectiveness of customer service in handling a particular group of contacts. The metric providesus with a new opportunity to identify bottlenecks in our intelligent routing system and evaluates itseffectiveness. By using Complexity AUC, we can gain insights into how the routing business worksand make data-driven decisions to further optimize the routing mechanism. Balyan, R., McCarthy, K. S., and McNamara, D. S. (2020). Applying natural language process-ing and hierarchical machine learning approaches to text difficulty classification. InternationalJournal of Artificial Intelligence in Education volume, 30:337370. Balyan, R., McCarthy, K. S., and McNamara, D. S. (2021). Comparing machine learning clas-sification approaches for predicting expository text difficulty.International Florida ArtificialIntelligence Research Society Conference."
}