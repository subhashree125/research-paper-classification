{
  "Abstract": "Spatial-temporal data, fundamental to many intelligent applica-tions, reveals dependencies indicating causal links between presentmeasurements at specific locations and historical data at the sameor other locations. Within this context, adaptive spatial-temporalgraph neural networks (ASTGNNs) have emerged as valuable toolsfor modelling these dependencies, especially through a data-drivenapproach rather than pre-defined spatial graphs. While this ap-proach offers higher accuracy, it presents increased computationaldemands. Addressing this challenge, this paper delves into the con-cept of localisation within ASTGNNs, introducing an innovativeperspective that spatial dependencies should be dynamically evolv-ing over time. We introduce DynAGS, a localised ASTGNN frame-work aimed at maximising efficiency and accuracy in distributeddeployment. This framework integrates dynamic localisation, time-evolving spatial graphs, and personalised localisation, all orches-trated around the Dynamic Graph Generator, a light-weighted cen-tral module leveraging cross attention. The central module canintegrate historical information in a node-independent manner toenhance the feature representation of nodes at the current moment.This improved feature representation is then used to generate adynamic sparse graph without the need for costly data exchanges,and it supports personalised localisation. Performance assessmentsacross two core ASTGNN architectures and nine real-world datasetsfrom various applications reveal that DynAGS outshines currentbenchmarks, underscoring that the dynamic modelling of spatial de-pendencies can drastically improve model expressibility, flexibility,and system efficiency, especially in distributed settings.",
  "Corresponding authors: Hong Rao and Xiaoxi He": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 25, August 37, 2025, Toronto, ON, Canada 2025 Copyright held by the owner/author(s).ACM ISBN 978-1-4503-XXXX-X/18/06",
  "Introduction": "Spatial-temporal data underpins many contemporary, intelligentweb applications. Often, these data exhibit spatial and temporaldependencies, indicating that the present measurement at a specificlocation (in either physical or abstract spaces) is causally dependenton the historical status at the same and other locations. Understand-ing these spatial and temporal dependencies is a crucial aspectof spatial-temporal data mining and is critical to spatial-temporalinference. Consequently, spatial-temporal graph neural networks(STGNNs) have emerged as a potent tool for modelling these de-pendencies, demonstrating significant success across various fields.A subclass of STGNN, known as adaptive spatial-temporal graphneural network (ASTGNN), has brought a new perspective to themodelling of these dependencies. Unlike traditional STGNNs thatuse pre-defined spatial graphs containing prior knowledge, AST-GNNs adopt a data-driven approach. They start with completegraphs and learn the spatial dependency based on the data. Thismethod allows for a more accurate and flexible representation ofthe spatial-temporal data. However, this approach also introduceshigher computational overhead due to the usage of complete graphs.A recent study has proposed a promising solution to thischallenge through the concept of localisation. Localisation involvespruning the spatial graph to lessen the overall data exchange be-tween nodes and the computational overhead. This reduction isfeasible due to the redundancy of information provided by spatialdependency compared to temporal dependency.",
  "KDD 25, August 37, 2025, Toronto, ON, CanadaWenying Duan, Shujun Guo, Zimu Zhou, Wei Huang, Hong Rao, and Xiaoxi He": "In this paper, we explore further the concept of localisationwithin ASTGNNs. We propose an innovative approach that tran-scends static spatial dependencies. We argue that the necessaryspatial dependencies, modelled by the sparsified spatial graph of anASTGNN, should not remain static across the entire time interval.Instead, these spatial dependencies, and thus the requireddata exchange between nodes, should be dynamically chang-ing over time. This concept is illustrated in the section with anorange background in : the topology and edge weights ofthe spatial graph are time-evolving.Moreover, the localisation of ASTGNN is particularly useful fordistributed deployments, where a reduction of data exchange be-tween nodes can significantly enhance overall system efficiency. Ex-isting dynamic spatial-temporal graph modeling techniquescan achieve time-varying sparse graphs, but we argue thiswill undermine the advantages of localised ASTGNN in dis-tributed deployment. This is because they require node-to-node data exchange at each interval to compute the spatialgraph, leading to significant communication overhead, espe-cially with large-scale and high-frequency spatial-temporaldata. To this end, we proposed DynAGS, a localised ASTGNN frame-work designed for optimal efficiency and accuracy in distributeddeployment. DynAGS has the following features: DynAGS allows for dynamic localisation, which can fur-ther decrease the amount of data exchange by a substantialmargin. Each node in our framework has the autonomy todecide whether and to which other nodes they need to senddata. Importantly, this decision is made using only locallyavailable historical data, making our framework perfectlysuited for distributed deployment. DynAGS introduces a time-evolving spatial graph, inwhich both the topology of the spatial graph, representedby a mask matrix, and the edge weights of the spatial graph,represented by the adjacency matrix, are dynamic. This fea-ture guarantees optimal expressibility and flexibility of themodel, and results in the minimal data exchange necessaryto sustain the desired performance. DynAGS supports personalised localisation, permittingeach node to select a personalised trade-off between datatraffic and inference accuracy. This flexibility allows eachnode to maximise its performance based on the availableresources, thereby optimising the efficiency and performanceof the entire heterogeneous system. To achieve these features, the DynAGS is designed around a cen-tral module known as the Dynamic Graph Generator (DGG). Centralto this module is a cross-attention mechanism. This mechanismadeptly amalgamates patch-level historical data with point-levelcurrent observational data to synthesize representations pertinentto the present time. Such representations are then utilised to gener-ate time-evolving spatial graphs. These representations adept in thecomprehensive integration of multi-scale temporal dependencies.This integration facilitates more nuanced modeling of temporaldependencies, spanning both long-term and short-term spectra,encapsulating elements such as periodicity, proximity, and trenddynamics. Consequently, this leads to an enhanced calibration ofincoming weights and connections for each respective node in the",
  "system. An overview of DynAGS is illustrated in . The maincontributions of our work are as follows:": "We introduce DynAGS, a novel ASTGNN framework de-signed for optimal efficiency and inference accuracy in dis-tributed deployment. This framework features dynamic lo-calisation, time-evolving spatial graphs, and personalisedlocalisation to reduce data exchange, enhance model flexibil-ity, and allow nodes to optimise performance based on theirown available resources. The performance of DynAGS is assessed using two back-bone ASTGNN architectures across nine real-world spatial-temporal datasets. The experimental findings indicate thatDynAGS significantly outperforms the state-of-the-art acrossvarious localisation degrees, ranging from 80% to 99.9%.Specifically, DynAGS, when operating at a localisation de-gree of 99.5%, produces results that are comparable to oreven superior to those of the current leading baseline at alocalisation degree of 80%. This results in a considerable de-crease in communication overhead, reducing it by no lessthan 30 times. The efficacy of DynAGS in diverse evaluation experimentssubstantiates our proposition that spatial dependency is in-trinsically time-dependent. This means that the necessaryspatial dependencies, and hence the required data exchangebetween nodes, should continually evolve over time ratherthan remain static. By embracing this dynamic approach inmodelling spatial dependencies within ASTGNNs, we en-hance the models expressibility and flexibility, and optimisesystem efficiency - a particularly significant improvement indistributed deployments.",
  "Related Work2.1Spatial-Temporal Graph Neural Networks": "STGNNs excel at uncovering hidden patterns in spatial-temporaldata , primarily by modeling spatial dependencies among nodesthrough adjacency matrices. These matrices are constructed usingeither pre-defined or self-learned methods.Pre-defined STGNNs use topological structures or spe-cific metrics (e.g., POI similarity) to build graphs , but theyrely heavily on domain knowledge and graph quality. Given theimplicit and complex nature of spatial-temporal relationships, self-learned methods for graph generation have risen in prominence.These methods provide innovative approaches to capture intricatespatial-temporal dependencies, offering a significant advantageover traditional pre-defined models.Self-learned STGNNs are divided into two primary categories:feature-based and random initialized methods. Feature-based ap-proaches, like PDFormer and DG , generate dynamic graphsfrom time-varying inputs, enhancing model accuracy. Random ini-tialized STGNNs, or ASTGNNs, perform adaptive graph genera-tion via randomly initialized learnable node embeddings. GraphWaveNet propose an AGCN layer to learn a normalized adap-tive adjacency matrix, and AGCRN designs a Node AdaptiveParameter Learning enhanced AGCN (NAPL-AGCN) to learn node-specific patterns. Due to its notable performance, NAPL-AGCNhas been integrated into various recent models . However,",
  ": An overview of the DynAGS framework with a pre-specified node numbers": "their static node embeddings limit the adaptability of the graphs. Re-cent advancements like DGCRN and DMSTGCN introducedynamic node embeddings, allowing for more flexible graph gener-ation in ASTGNNs. Despite these advancements, most self-learnedSTGNNs generate complete graphs with high computational andcommunication overheads. Methods such as GTS and ASTGAT mit-igate this by employing strategies like Top-K and Gumbel-softmaxfor partial adjacency matrix retention, enabling discrete and sparsegraph generation with dynamic topologies.Yet, these approaches still require computation of all node pairconnections to determine if there are edges between them, lead-ing to a quadratic increase in data exchange and computationaloverhead. Our work addresses these challenges in dynamic graphgeneration, proposing a novel approach in localised STGNNs. Byimplementing dynamic localisation, we enhance model expressive-ness and flexibility, while considerably reducing data exchangerequirements and computational overhead. This results in a moreefficient balance between model efficiency and accuracy.",
  "Graph Sparsification for GNNs": "With the increasing size of graphs, the computational cost of train-ing and inference for GNNs also rises. This escalating cost hasspurred interest in graph sparsification, which aims to create asmaller sub-graph while preserving the essential properties of theoriginal graph . SGCN is the pioneering work in graphsparsification for GNNs, which involves eliminating edges fromthe input graph and learning an additional DNN surrogate model.More recent works, such as UGS and GBET , examine graphsparsification from the perspective of the lottery ticket hypothesis.The aforementioned works only studied graph sparsification forstandard GNNs with non-temporal data and pre-defined graphs.AGS extended this concept to spatial-temporal GNNs withadaptive graph architectures, known as the localisation of AST-GNNs. However, AGS localises an ASTGNN by learning a fixedmask, overlooking the fact that spatial dependency varies over time.This approach led to sub-optimal generalisation on unseen data.Unlike AGS, DynAGS allows the dynamic localisation of ASTGNNs.",
  "NotationDefinition": "Gthe spatial graph used for modelling the spatial dependencyAthe adjacency matrix of Gthe number of nodesX,features of the -th node at timestep Athe normalised adaptive adjacency matrixAdynamic adaptive adjacency matrix at timestep Mdynamic mask to localise ASTGNN at timestep the look-back period length of the taskthe dimension of node embeddingthe dimension of node featurethe dimension of output feature of ASTGNNkernel and stride size of 1D average poolingweighting factor of 0-norm the conventions in related works , we denote thespatial-temporal data as a sequence of frames: {X1, X2, . . . , X, . . .},where a single frame X R is the-dimensional data collatedfrom different locations at time . For a chosen task time , weaim to learn a function mapping the historical observations intothe future observations in the next timesteps:",
  "X:,(2)": "where () is the activation function, A = I + A is the adjacencymatrix of the graph with added self-connections (with the originaladjacency matrix being A), and D is the diagonal degree matrixof A. R is a trainable parameter matrix. Z R isthe layer output, with indicating the number of output featuredimensions for each node. 3.2.2Adaptive Spatial-Temporal Graph Neural Network (ASTGNN).A crucial enhancement in modeling the spatial network is the adop-tion of Adaptive Graph Convolution Networks (AGCNs). Thesenetworks capture the dynamics within the graph, paving the wayfor the development of Adaptive Spatial-Temporal Graph NeuralNetworks (ASTGNNs) .In the subsequent discussion, we provide a brief overview oftwo representative ASTGNN models: (i) The Adaptive Graph Con-volutional Recurrent Network (AGCRN) ; (ii) STG-NCDE ,which is an extension of AGCRN with neural controlled differentialequations (NCDEs).",
  "Z = AX:,EW(3)": "In this context, E R , W R , and consequentlyEW R . A R represents the normalisedself-adaptive adjacency matrix . denotes the embeddingdimension. Each row of E embodies the embedding of a node.The embedding of the -th node, represented by the -th rowin E, is denoted as e R. During training, E is updatedto encapsulate the spatial dependencies among all nodes.Rather than learning the shared parameters in (2), NAPL-AGCN employs EW to learn parameters specific to each node.To capture both spatial and temporal dependencies, AGCRNcombines NAPL-AGCN and Gated Recurrent Units (GRU),replacing the MLP layers in NAPL-AGCN with GRU layers.",
  "STG-NCDE. STG-NCDE extends AGCRN by incorporatingtwo neural controlled differential equations (NCDEs): a tem-poral NCDE and a spatial NCDE": "3.2.3Localisation of ASTGNN. We denote an ASTGNN model asF (;, G), where encompasses all the learnable parameters, andG represents the spatial graph. The graph G is characterised byits adjacency matrix A, as derived from (3). The localisation ofF (;, G) is accomplished by pruning the spatial graph G. This canbe mathematically expressed as the Hadamard product of the adja-cency matrix A and a mask matrix M. Consequently, the prunedgraph has the adjacency matrix A M. During the pruningprocess, M is obtained by minimising the following objective:",
  "L = L(, A M) + M0(4)": "Here, L(, A M) is the training loss function calculated withthe pruned spatial graph, M0 is the 0-norm, and the weightingfactor regulates the degree of pruning. As M0 is not differen-tiable, Adaptive Graph Sparsification (AGS) , a recent localisa-tion framework for ASTGNNs, resolves this issue by optimisingthe differentiable approximation of the 0-regularisation of M0.However, in this paper, we contend that a static mask M anda static adjacency matrix A disregard the fact that spatial de-pendencies are dynamic over time, resulting in sub-optimal perfor-mance on unseen data. Consequently, we aim to achieve dynamiclocalisation by learning dynamic M R and A R ,which are adapted to the timestep .",
  "Method": "To enable dynamic localisation, it is necessary to generate the time-evolving mask matrix M and adaptive adjacency matrix Aasmentioned in Sec. 3.2. To achieve this, we have designed a frame-work, DynAGS, built around a core module known as the DynamicGraph Generator (DGG). An overview of the DynAGS framework isprovided in .",
  "Dynamic Graph Generator (DGG)": "As outlined in Sec. 3.1, in most spatial-temporal tasks, the look-backperiod is typically much shorter than the entire time intervalcovered by the available data. Previous studies have predominantlyutilized point-level data X from the look-back period for thegeneration of dynamic graphs, where { + 1, ,}. Thisapproach, however, lacks the integration of multi-scale informationwithin the temporal dimension and falls short in modeling long-term dependencies. Accordingly, for the -th node at a chosen tasktime , our DGG fuses the observations X in the look-back period with insights from residual historical data by a cross attention.For the -th node at a chosen task time , we define the residualhistorical data: X, = X1:() R .For the -th node at timestep { + 1, ,}, a crossattention accepts the node-specific residual historical data X, askey and value, and the observation at the moment X, R asthe query. The cross attention then output an vector h, R,which is subsequently utilised to determine the incoming weightsand connections of the corresponding node, as detailed in Sec. 4.2.Note that in practice, we put an upper-limit on the length ofX,based on datasets to ensure that the computational cost is",
  "acceptable. This may change the X, to X( +1):()": "4.1.1Down-sampling and Patching. The first challenge lies in ef-ficiently representing the residual historical data, given that thelength of this data, , is extremely large. Several studies on time-series processing propose reducing the input length via down-sampling or patching. In this case, we employ a combination of bothmethods to significantly decrease the length of the encoder input.For the -th node, given the residual historical data X, R ,we first apply 1D average pooling with a kernel and stride overX, on the time dimension to down-sample it into 1",
  ")V(7)": "In the end, the cross-attention outputs h, from the nodes arestacked together to form a matrix H = h1: This matrix H isused later to generate the time-evolving M and A, which isexplained in details in Sec. 4.2.In summary, the attention mechanism within DGG selectivelyconcentrates on sequences of historical data patches most pertinentto the current input. This process culminates in the final fused fea-ture representation, H, a thorough amalgamation of both currentobservation and historical data characteristics. The influence ofhistorical data on the models performance is elaborated in Sec. 6.1.",
  "(,) = min1, max0,(,) ( ) + (9)": "In this equation, is randomly sampled from a uniform distribution U(0, 1) and represents the temperature value. Following ,we set = 0.1 and = 1.1 in practice.For the-th node, the corresponding h can be mapped into m(,:), the -th row of M. This vector m(,:) determines whether the -thnode needs to send data to other nodes, and this decision is solelydependent on h. In other words, the -th node can independentlydecide whether to send data to other nodes without requiring anyadditional prior data exchange. The mask M, which determines thetopology of the spatial graph, evolves over time as it is computedfrom the cross attention output H at time .",
  "Here, E = HW is the time-evolving embedding, W R": "are learnable parameters. E signifies the time-evolving modifica-tion applied to the original node embedding E. Its worth noting thateven though the computation is expressed in matrix forms, eachrows computation in E, or e R, solely depends on h . Thisimplies that the computation of the node-specific time-evolvingembedding, e , is performed purely locally without the need forprevious data exchange. This process is akin to dynamic mask gen-eration.To avoid calculating the entire matrix multiplication E Et asin (3), which would necessitate all nodes to share their own time-evolving embedding e with each other at every timestep, therebycausing substantial communication overhead, we propose usingthe locally available time-evolving mask vector m(,:). This vectoralready determines for the -th node which neighbours it needs tocommunicate with at time . Consequently, each element (,)inthe time-evolving adjacency matrix A is calculated in a similarfashion to the graph attention network (GAT) :",
  ",(,) = 0(11)": "Here = { | (,) = 1} is a set of the -th nodes neighbours inthe sparse spatial graph. Even though the computation in (11) stillnecessitates the exchange of the time-evolving node embedding e,among neighboring nodes, this requirement has been significantlyreduced. Moreover, the node embedding e, R is quite smallin comparison to the volume of data exchanged between adjacentnodes. Consequently, the total communication overhead necessaryto support the dynamic spatial graph is minimal.Finally, we substitute A for A in (3) to obtain the dynamiclocalised ASTGNN:",
  "Here we discuss two additional enhancements made to our DynAGS,which improve its efficiency further": "4.3.1Efficient Dynamic Localisation. The generation of a dynamicmask increases the time complexity to O( 2) during inference.To address this, we significantly enhance the efficiency of DynAGSby initiating from a sparse spatial graph, pre-localised using AGS.Given a pre-defined graph sparsity , we first train a -localisedASTGNN F (;, A M) to achieve efficient inference. Here, represents the graph sparsity (100% means totally sparse), andM R is a mask matrix with a sparsity of . Based on the-localised ASTGNN, we only need to map entries {(,) |(,)",
  "Efficiency Analysis": "4.4.1Complexity Analysis. Dynamic localization notably decreasesthe complexity involved in generating dynamic graphs in ASTGNNs.The inference time complexity of unpruned NAPL-AGCN layersper timestep is O( 2 + 2 + 2). After sparsi-fication, the inference time complexity of NAPL-AGCN layers isO((1 ) 2 + Mt0 + 2). where denotesthe number of nodes, is the number of layers, is the size ofnode embedding, is the size of node feature, andMt0 is thenumber of remaining edges. For more comparative analysis on thecomplexity of AGS and DynAGS, please refer to Appendix A.2. 4.4.2Communication Analysis. This section presents a theoreticalanalysis of the communication costs for DynAGS and unprunedASTGNNs. For unpruned ASTGNNs, the communication overheadduring inference is:",
  "Dynamic Localisation of Spatial-Temporal Graph Neural NetworkKDD 25, August 37, 2025, Toronto, ON, Canada": "MAE 9999.5 99.9 Sparsity(%) PEMS03 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) MAE 9999.5 99.9 PEMS04 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) MAE 9999.5 99.9 PEMS07 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) 19.019.520.020.521.021.522.0 MAE 9999.5 99.9 GLA AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) RMSE 9999.5 99.9 Sparsity(%) AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) RMSE 9999.5 99.9 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) RMSE 9999.5 99.9 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) RMSE 9999.5 99.9 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) MAPE(%) 9999.5 99.9 Sparsity(%) AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) 13.013.514.014.515.015.516.016.517.0 MAPE(%) 9999.5 99.9 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) 8.59.09.510.010.511.0 MAPE(%) 9999.5 99.9 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) 12.012.513.013.514.014.515.015.5 MAPE(%) 9999.5 99.9 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) : Test accuracies of AGCRN, localised by DynAGS and AGS, evaluated on transportation datasets (PEMS03, PEMS04,PEMS07, and GLA). The black vertical dashed lines denote the accuracies of AGCRN when localised by AGS at a localisationdegree of 80%. MAE 9999.5 99.9 Sparsity(%) PEMS03 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) 20.521.021.522.022.523.023.524.024.5 MAE 9999.5 99.9 PEMS04 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) MAE 9999.5 99.9 PEMS07 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) 30.030.531.031.532.032.533.033.534.0 RMSE 9999.5 99.9 Sparsity(%) AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) RMSE 9999.5 99.9 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) RMSE 9999.5 99.9 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) MAPE(%) 9999.5 99.9 Sparsity(%) AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) 12.513.013.514.014.515.015.516.0 MAPE(%) 9999.5 99.9 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) 8.59.09.510.010.511.0 MAPE(%) 9999.5 99.9 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) : Test accuracies of STG-NCDE, localised by DynAGS and AGS, evaluated on transportation datasets. The black verticaldashed lines denote the accuracies of STG-NCDE when localised by AGS at a localisation degree of 80%. The absence of STG-NCDE on the GLA dataset indicates that the model incurs out-of-memory issue.",
  "(b) STG-NCDE": ": Test accuracies (MAE) of AGCRN (a) and STG-NCDE (b), localised by DynAGS and AGS, evaluated on transportationdatasets (PEMS03,PEMS04,PEMS07,and GLA).The black vertical dashed lines denote the accuracies of AGCRN and STG-NCDEwhen localised by AGS at a localisation degree of 80%. RMSE and MAPE results are suppressed for the sake of room and arereported in and . 21.522.022.523.023.524.024.525.025.5 MAE 99.5 99.9 Sparsity(%) TX AGCRN(Origin)AGCRN(AGS)AGCRN(Ours) MAE 99.5 99.9 CA AGCRN(Origin)AGCRN(AGS)AGCRN(Ours)",
  "new data, it was necessary to introduce distribution shifts into thetraining and testing data. This goal was achieved by using morechallenging settings rather than standard ones in these domains:": "The training/validation/testing sets are split by time. Conse-quently, less data is used for both training and testing. Thisapproach creates a larger time gap between the training setand testing set than the standard setting. This type of datasetsplitting introduces a distribution shift between training andtesting data, as the distributions of spatial-temporal datachange over time. We use longer input and output lengths for historical ob-servations (inputs) to predict future observations (outputs).The spatial-temporal dependencies of inputs and outputsmay differ over a large time span, as time progresses. Thisapproach can introduce a more significant temporal shiftbetween input and output. Detailed information for each datasets and their configurationsare provided in Appendix A.1.Baseline. AGS was selected as the sole baseline because, to thebest of our knowledge, AGS is currently the only method specifi-cally designed for the localisation of ASTGNNs. Moreover, it rep-resents a state-of-the-art method that was published recently. Webenchmarked the performance of DynAGS against AGS using tworepresentative ASTGNN backbone architectures: AGCRN and STG-NCDE. AGCRN stands out as the most representative ASTGNN forspatial-temporal forecasting, while STG-NCDE serves as a state-of-the-art extension of AGCRN.Implementation. The dataset-specific hyperparameters of AGCRNand STG-NCDE, followed the same setup as in the original papers.Other dataset-specific hyperparameters for reproducibility are pro-vided in Appendix A.3.",
  "Golem": "AGCRN(AGS)AGCRN(Ours) MAPE(%) @STG-NCDE STG-NCDE(AGS)STG-NCDE(Ours) MAPE(%) @STG-NCDE STG-NCDE(AGS)STG-NCDE(Ours) MAPE(%) @STG-NCDE STG-NCDE(AGS)STG-NCDE(Ours) : Test accuracies (MAPE) of AGCRN and STG-NCDE, localised by DynAGS and AGS, evaluated on blockchain datasets.The black and yellow vertical dashed lines denote the MAPE of AGCRN and STG-NCDE when localised by AGS at a localisationdegree of 80%, respectively. The MAPE of the original models are omitted, as the localised ones are always better.",
  "Test Accuracies": "Test accuracies (MAE) of AGCRN and STG-NCDE, localized byDynAGS and AGS across localisation degrees from 80% to 99.9%,are presented in , and for transporta-tion,biosurveillance and blockchain datasets, respectively. Fromthese results, the following observations can be made: DynAGS is adept at capturing dynamic spatial-temporal de-pendencies and delivers promising predictions for unseendata. Additionally, our DynAGS enhances the performanceof AGS noticeably. For the same localisation degree, ourssurpasses AGS by up to 13.5% less error across all datasets. DynAGS excels in the trade-off between data exchange andaccuracy. When compared to AGS at a localisation degreeof 80%, DynAGS at a localisation degree of 99.5% frequentlyattains comparable or even superior accuracies, implyingthat nearly 40 times less communication is needed to sustainthe same performance.",
  "Impact on Resource Efficiency": "As highlighted in Sec. 5.2, an ASTGNN localised via DynAGS to thedegree of 99.5% (average degree over timesteps) delivers resultscomparable or even better than that with AGS to a localisation de-gree of 80%. Thus, we evaluated the computational demand duringthe inference of 99.5%-localised AGCRN and STG-NCDE via Dy-nAGS and during the inference of 80%/99.5%-localised AGCRN andSTG-NCDE via AGS. We also simulated the distributed com-putation of DynAGS using Pythons multiprocessing libraryand compared the communication cost during inference of99.5%-localised AGCRN via DynAGS and during the infer-ence of 80%/99.5%-localised AGCRN via AGS, as shown in. The detailed description of simulation can be found inAppendix A.6. Our findings reveal that (i) both DynAGS and AGS can effec-tively reduce the computation required for inference, (ii) comparingDynAGS and AGS shows that DynAGS has a marginally highercomputing cost for AGCRN but is more efficient for STG-NCDEwhen performance is similar, and (iii) DynAGS cuts communicationcost by a factor of nearly 40 relative to AGS. Additionally, whenthe sparsity is the same, the computational overhead of DynAGS isstill slightly higher than that of AGS, but the performance of theformer is significantly better than the latter.In real-world distributed deployment scenarios, a significant por-tion of resources is allocated to communication between nodes,often overshadowing local computation costs . In this light,the marked reduction in communication cost by DynAGS becomesespecially significant. The minor rise in local computation by Dy-nAGS is less concerning, given that local computation is generallyless resource-intensive than inter-node communication.Thus, these outcomes affirm that the advantages of DynAGS, pre-dominantly the substantial communication cost savings, more thancompensate for the occasional minor upticks in computational de-mand. This positions DynAGS as a more resource-efficient solutionin real-world distributed deployment contexts.",
  "Ablation Studies6.1Impact of Residual Historical Data andDynamic Graph": "We evaluated the effects of residual historical data and dynamicgraph on AGCRN (on PEMS07, Bytom, and TX datasets) by com-paring DynAGS with its variants, DynAGS and DynAGS . Thevariant DynAGS is obtained by removing the cross attention,thereby eliminating the residual historical data. In contrast, theDynAGS variant uses only the node embedding E to generatea static mask, similar to AGS, resulting in a fixed spatial graphtopology over time.As observed from , DynAGS consistently outperformsDynAGS and DynAGS by a significant margin. These resultsconfirm that (i) the large-scale temporal features and global infor-mation provided by the residual historical data contribute positivelyto the performance of DynAGS, and (ii) dynamic graph modeling,which captures evolving spatial-temporal dependencies, signifi-cantly improves prediction performance.",
  "DynAGS vs. Other Non-Localised ASTGNNs": "Recent advancements in ASTGNNs have introduced improved ar-chitectures of AGCRN, including Z-GCNETs , STG-NCDE ,and TAMP-S2GCNets , tailored for various applications. Thismotivates us to evaluate how our localised AGCRNs compare tothese advanced variations.Results are shown in . Our re-sults demonstrate that the localised AGCRNs consistently achievesuperior inference performance, even surpassing state-of-the-art ar-chitectures, while significantly reducing computational complexity,thereby underscoring their efficacy and practicality.",
  "Personalised Localisation": "To further validate our personalised localisation approach, we con-ducted an additional ablation study. We divided all nodes into dis-tinct groups, with each group assigned a specific in-degree compres-sion ratio. Instead of merely testing these groups, we introduceda variation: while keeping a groups in-degree compression ratio unchanged, we adjusted the ratios of the other groups. The resultsfrom this exercise were clear. The performance of a particular groupremained unaffected by the in-degree sparsity alterations of othergroups.This experimental evidence underscores the fact that personalis-ing the localisation of ASTGNNs doesnt detract from performance,but rather presents a compelling and practical method. Furtherdetails of this experiment can be found in Appendix A.7.",
  "Conclusion": "This study introduced DynAGS, an innovative ASTGNN frameworkthat dynamically models spatial-temporal dependencies. Throughthe integration of dynamic localisation and a time-evolving spatialgraph, DynAGS has proven superior in performance across vari-ous real-world datasets, demonstrating significant reductions incommunication overhead. Our findings underscore the intrinsictime-dependent nature of spatial dependencies and advocate for ashift from static to dynamic representations in spatial-temporal dataanalysis. Looking ahead, DynAGS paves the way for future advance-ments in the realm of spatial-temporal data mining, emphasisingboth adaptability and efficiency.",
  "Liang Liwei abd Li Rongcong. 2019. Research on intelligent energy savingtechnology of 5G base station based on A. Mobile Communications 43, 12 (2019),3236": "Lei Bai, Lina Yao, Salil S. Kanhere, Xianzhi Wang, and Quan Z. Sheng. 2019.STG2Seq: Spatial-Temporal Graph to Sequence Model for Multi-step PassengerDemand Forecasting. In Proceedings of the Twenty-Eighth International JointConference on Artificial Intelligence, IJCAI 2019, August 10-16, 2019, Sarit Kraus(Ed.). ijcai.org, Macao, China, 19811987. Lei Bai, Lina Yao, Can Li, Xianzhi Wang, and Can Wang. 2020. Adaptive graphconvolutional recurrent network for traffic forecasting. In Advances in NeuralInformation Processing Systems. Virtual Event, 1780417815. Chao Chen, Karl Petty, Alexander Skabardonis, Pravin Varaiya, and Zhanfeng Jia.2001. Freeway Performance Measurement System: Mining Loop Detector Data.Transportation Research Record 1748, 1 (2001), 96102. Tianlong Chen, Yongduo Sui, Xuxi Chen, Aston Zhang, and Zhangyang Wang.2021. A Unified Lottery Ticket Hypothesis for Graph Neural Networks. InProceedings of the 38th International Conference on Machine Learning, ICML 2021,18-24 July 2021, Virtual Event (Proceedings of Machine Learning Research, Vol. 139),Marina Meila and Tong Zhang (Eds.). PMLR, 16951706. Yuzhou Chen, Ignacio Segovia-Dominguez, Baris Coskunuzer, and Yulia R. Gel.2022. TAMP-S2GCNets: Coupling Time-Aware Multipersistence KnowledgeRepresentation with Spatio-Supra Graph Convolutional Networks for Time-SeriesForecasting. In The Tenth International Conference on Learning Representations,ICLR 2022, Virtual Event, April 25-29, 2022. Yuzhou Chen, Ignacio Segovia-Dominguez, and Yulia R. Gel. 2021. Z-GCNETs:Time Zigzags at Graph Convolutional Networks for Time Series Forecasting. InProceedings of the 38th International Conference on Machine Learning, ICML 2021,18-24 July 2021, Virtual Event (Proceedings of Machine Learning Research, Vol. 139),Marina Meila and Tong Zhang (Eds.). PMLR, 16841694. Jeongwhan Choi, Hwangyong Choi, Jeehyun Hwang, and Noseong Park. 2022.Graph Neural Controlled Differential Equations for Traffic Forecasting. In Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Confer-ence on Innovative Applications of Artificial Intelligence, IAAI 2022, The TwelvethSymposium on Educational Advances in Artificial Intelligence, EAAI 2022 VirtualEvent, February 22 - March 1, 2022. AAAI Press, 63676374. Wenying Duan, Xiaoxi He, Lu Zhou, Lothar Thiele, and Hong Rao. 2023. Combat-ing Distribution Shift for Accurate Time Series Forecasting via Hypernetworks.In 2022 IEEE 28th International Conference on Parallel and Distributed Systems(ICPADS). 900907. Wenying Duan, Xiaoxi He, Zimu Zhou, Lothar Thiele, and Hong Rao. 2023.Localised Adaptive Spatial-Temporal Graph Neural Network. In Proceedings ofthe 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,KDD 2023, Long Beach, CA, USA, August 6-10, 2023. ACM, 448458. Xu Geng, Yaguang Li, Leye Wang, Lingyu Zhang, Qiang Yang, Jieping Ye, andYan Liu. 2019. Spatiotemporal multi-graph convolution network for ride-hailingdemand forecasting. In Proceedings of the AAAI conference on artificial intelligence,Vol. 33. 36563663. Shengnan Guo, Youfang Lin, Huaiyu Wan, Xiucheng Li, and Gao Cong. 2021.Learning dynamics and heterogeneity of spatial-temporal graph data for trafficforecasting. IEEE Transactions on Knowledge and Data Engineering 34, 11 (2021),54155428. Liangzhe Han, Bowen Du, Leilei Sun, Yanjie Fu, Yisheng Lv, and Hui Xiong. 2021.Dynamic and Multi-Faceted Spatio-Temporal Deep Learning for Traffic SpeedForecasting. In Proceedings of the 27th ACM SIGKDD Conference on KnowledgeDiscovery & Data Mining (Virtual Event, Singapore) (KDD 21). Association forComputing Machinery, New York, NY, USA, 547555. Rongzhou Huang, Chuyin Huang, Yubao Liu, Genan Dai, and Weiyang Kong.2020. LSGCN: Long Short-Term Traffic Prediction with Graph ConvolutionalNetworks. In Proceedings of the Twenty-Ninth International Joint Conference onArtificial Intelligence, IJCAI 2020, Christian Bessiere (Ed.). ijcai.org, 23552361. Jiawei Jiang, Chengkai Han, Wayne Xin Zhao, and Jingyuan Wang. 2023.PDFormer: Propagation Delay-Aware Dynamic Long-Range Transformer forTraffic Flow Prediction. In Thirty-Seventh AAAI Conference on Artificial Intel-ligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Arti-ficial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advancesin Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023,Brian Williams, Yiling Chen, and Jennifer Neville (Eds.). AAAI Press, 43654373. Renhe Jiang, Zhaonan Wang, Jiawei Yong, Puneet Jeph, Quanjun Chen, YasumasaKobayashi, Xuan Song, Shintaro Fukushima, and Toyotaro Suzumura. 2023.Spatio-Temporal Meta-Graph Learning for Traffic Forecasting. In Thirty-SeventhAAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference onInnovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium",
  "on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC,USA, February 7-14, 2023, Brian Williams, Yiling Chen, and Jennifer Neville (Eds.).AAAI Press, 80788086": "Fuxian Li, Jie Feng, Huan Yan, Guangyin Jin, Fan Yang, Funing Sun, Depeng Jin,and Yong Li. 2023. Dynamic graph convolutional recurrent network for trafficprediction: Benchmark and solution. ACM Transactions on Knowledge Discoveryfrom Data 17, 1 (2023), 121. Jian Li, Amol Deshpande, and Samir Khuller. 2009. Minimizing CommunicationCost in Distributed Multi-query Processing. In Proceedings of the 25th Interna-tional Conference on Data Engineering, ICDE 2009, March 29 2009 - April 2 2009,Shanghai, China, Yannis E. Ioannidis, Dik Lun Lee, and Raymond T. Ng (Eds.).IEEE Computer Society, 772783. Jiayu Li, Tianyun Zhang, Hao Tian, Shengmin Jin, Makan Fardad, and RezaZafarani. 2020. SGCN: A Graph Sparsifier Based on Graph Convolutional Net-works. In Advances in Knowledge Discovery and Data Mining - 24th Pacific-AsiaConference, PAKDD 2020, Singapore, May 11-14, 2020, Proceedings, Part I (LectureNotes in Computer Science, Vol. 12084), Hady W. Lauw, Raymond Chi-Wing Wong,Alexandros Ntoulas, Ee-Peng Lim, See-Kiong Ng, and Sinno Jialin Pan (Eds.).Springer, 275287. Yitao Li, Umar Islambekov, Cuneyt Gurcan Akcora, Ekaterina Smirnova, Yulia R.Gel, and Murat Kantarcioglu. 2020. Dissecting Ethereum Blockchain Analytics:What We Learn from Topology and Geometry of the Ethereum Graph?. In Pro-ceedings of the 2020 SIAM International Conference on Data Mining, SDM 2020,Cincinnati, Ohio, USA, May 7-9, 2020, Carlotta Demeniconi and Nitesh V. Chawla(Eds.). SIAM, 523531. Xu Liu, Yutong Xia, Yuxuan Liang, Junfeng Hu, Yiwei Wang, Lei Bai, ChaoHuang, Zhenguang Liu, Bryan Hooi, and Roger Zimmermann. 2023. LargeST: ABenchmark Dataset for Large-Scale Traffic Forecasting. In Advances in NeuralInformation Processing Systems. Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2023.A Time Series is Worth 64 Words: Long-term Forecasting with Transformers.In The Eleventh International Conference on Learning Representations, ICLR 2023,Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Hao Peng, Bowen Du, Mingsheng Liu, Mingzhe Liu, Shumei Ji, Senzhang Wang,Xu Zhang, and Lifang He. 2021. Dynamic graph convolutional network forlong-term traffic flow prediction with reinforcement learning. Inf. Sci. 578 (2021),401416. Youngjoo Seo, Michal Defferrard, Pierre Vandergheynst, and Xavier Bresson.2018. Structured Sequence Modeling with Graph Convolutional Recurrent Net-works. In Neural Information Processing - 25th International Conference, ICONIP2018, Siem Reap, Cambodia, December 13-16, 2018, Proceedings, Part I (Lecture Notesin Computer Science, Vol. 11301), Long Cheng, Andrew Chi-Sing Leung, and SeiichiOzawa (Eds.). Springer, 362373. Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, PietroLi, and Yoshua Bengio. 2018. Graph Attention Networks. In 6th InternationalConference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April30 - May 3, 2018, Conference Track Proceedings. OpenReview.net. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, andS Yu Philip. 2020. A comprehensive survey on graph neural networks. IEEETransactions on Neural Networks and Learning Systems 32, 1 (2020), 424. Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, and Chengqi Zhang. 2019.Graph Wavenet for Deep Spatial-Temporal Graph Modeling. In Proceedings ofthe 28th International Joint Conference on Artificial Intelligence (Macao, China)(IJCAI19). AAAI Press, 19071913. Sijie Yan, Yuanjun Xiong, and Dahua Lin. 2018. Spatial Temporal Graph Con-volutional Networks for Skeleton-Based Action Recognition. In Proceedings ofthe Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30thinnovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Sym-posium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans,Louisiana, USA, February 2-7, 2018, Sheila A. McIlraith and Kilian Q. Weinberger(Eds.). AAAI Press, 74447452. Huaxiu Yao, Fei Wu, Jintao Ke, Xianfeng Tang, Yitian Jia, Siyu Lu, Pinghua Gong,Jieping Ye, and Zhenhui Li. 2018. Deep multi-view spatial-temporal networkfor taxi demand prediction. In Proceedings of the AAAI conference on artificialintelligence, Vol. 32. Haoran You, Zhihan Lu, Zijian Zhou, Yonggan Fu, and Yingyan Lin. 2022.Early-Bird GCNs: Graph-Network Co-optimization towards More Efficient GCNTraining and Inference via Drawing Early-Bird Lottery Tickets. In Thirty-SixthAAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Confer-ence on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twel-veth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022Virtual Event, February 22 - March 1, 2022. AAAI Press, 89108918. Bing Yu, Haoteng Yin, and Zhanxing Zhu. 2018. Spatio-Temporal Graph Con-volutional Networks: A Deep Learning Framework for Traffic Forecasting. InProceedings of the Twenty-Seventh International Joint Conference on Artificial In-telligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, Jrme Lang (Ed.).",
  "summarise the specifications of the datasets used inour experiments. The detailed datasets and configurations are asfollows:": "Transportation: We analyze three widely-studied trafficforecasting datasets from the Caltrans Performance Mea-surement System (PEMS): PEMS03, PEMS04, and PEMS07. Additionally, we use the GLA (Greater Los Angeles)dataset from the recent LargeST benchmark , currentlyrecognized as the second largest spatio-temporal dataset. Inour experiments, PEMS03, PEMS04, PEMS07, and GLA arepartitioned in the ratio of 4.5:4:1.5 for training, validation,and testing, respectively. This partitioning scheme deviatesfrom the standard ratio of 6:2:2. For PEMS03, PEMS04, andPEMS07, traffic flows are aggregated in 5-minute intervals,and we perform a 24-sequence-to-24-sequence forecasting,differing from the standard 12-sequence-to-12-sequence ap-proach. For GLA, traffic flows are aggregated in 15-minuteintervals. Here, we adhere to the standard 12-sequence-to-12-sequence forecasting format for this dataset. Model accuracyis assessed using the Mean Absolute Error (MAE), Root MeanSquare Error (RMSE), and Mean Absolute Percentage Error(MAPE). Blockchain: We use three Ethereum price datasets: Bytom,Decentral, and Golem . These datasets are representedas graphs, where nodes and edges signify user addresses anddigital transactions, respectively. The interval between twoconsecutive timestamps is one day. In our settings, Bytom,Decentraland, and Golem token networks are divided with aratio of 6:3:1 for training, validation, and testing. This differs from the original setting, which uses a ratio of 8:2 for trainingand testing . We use 14 days of historical data to forecastthe subsequent 14 days. This differs from the original setting,which uses 7 days historical data to predict future 7 daysdata. Given that MAE and RMSE values for these datasets areexceedingly small and dont reflect performance accurately,only MAPE is used, following . Biosurveillance: We adopt the California (CA) and Texas(TX) COVID-19 biosurveillance datasets for forecastingthe number of hospitalised patients. The datas time intervalis one day. CA and TX datasets are split with a 6:3:1 ratiofor training, validation, and testing. 6 days of historical dataare used to predict the next 30 days. These differ from theoriginal settings , which split CA and TX datasets with an8:2 ratio for training and testing and use 3 days of historicaldatat to predict the next 15 days. Both MAE and MAPE areutilised as accuracy metrics.",
  ",(17)": "while the time complexity of AGS is Mt0 + 2.In contrast, the time complexity of unpruned ASTGNN is 2 + 2.It is evident that the additional time complexity introduced byDynAGS is 2 + O((1 ) 2 ), whichencompasses the cross attention terms and graph generation terms.Given that and , we have 2 + O((1) 2 ) 2 + 2. As mentioned in Sec. 5.2,compared to AGS at a localization degree of 80%, DynAGS at alocalization degree of 99.5% achieves comparable or even superioraccuracies. This demonstrates that DynAGS can effectively reducethe time complexity of ASTGNN, as the time complexity is reducedby 99.5% 2 , given that the computational overheadof graph convolution is significantly higher than the additionalcomplexity introduced by DynAGS. Furthermore, there is no dataexchange during the computation of cross attention and graphgeneration. Overall, DynAGS provides a highly efficient sparsedynamic graph learning method that requires very few resources.",
  "A.6Description of Distributed Simulation": "We used Pythons Multiprocessing module to simulate distributeddeployment, with each process representing a device. To facilitateand save memory, we created a shared memory ASTGNN model anddataset. It should be noted that in actual deployment, each device isequipped with an ASTGNN model and only has access to the datacorresponding to its own nodes before sending communicationrequests. However, the simulation can still accurately calculate thecommunication overhead.",
  "A.7Personalised Localisation ExperimentDetails": "We categorised all nodes into four distinct groups, labeled as I, II,III, and IV. In a personalised setup using DynAGS, these groupsachieved in-degree compression ratios of 30%, 50%, 80%, and 99%,respectively.To draw a meaningful comparison, for each group, we alsotrained a separate global-localised AGCRN using DynAGS, whereevery node in the network was subjected to the same in-degree com-pression ratio specific to that group. For instance, for Group III withan in-degree compression ratio of 80%, we trained a model where PEMS03PEMS04PEMS07PEMS08CATX MAE 17.4019.1021.57 16.10 25.83 8.55 17.3019.1421.83 16.33 26.56 8.62 30%-localised vs. personal-localised on Group 1 30%-localisedPersonal-localised PEMS03PEMS04PEMS07PEMS08CATX MAE 16.5022.0220.4919.30 214.07 25.43 16.3522.1320.3419.27 210.04 25.89 50%-localised vs. personal-localised on Group 2 50%-localisedPersonal-localised PEMS03PEMS04PEMS07PEMS08CATX MAE 17.7020.0621.4017.59 73.22 18.6717.5019.9321.3317.35 74.12 18.36 80%-localised vs. personal-localised on Group 3 50%-localisedPersonal-localised PEMS03PEMS04PEMS07PEMS08CATX MAE 17.24 12.69 23.3520.14 87.03 24.67 17.42 12.78 23.1919.86 84.44 23.90 99%-localised vs. personal-localised on Group 4 50%-localisedPersonal-localised",
  ": Results of personalised localisation": "all nodes, regardless of their group, had an in-degree compressionratio of 80%.The crux of our findings, depicted in , is that each groupsperformance in the personalised setting mirrored its performancein the corresponding global sparsity setup. For example, the GroupIII nodes in the personalised model performed as well as whenevery node in the system was globally set to an 80% in-degreecompression. This demonstrates that the individualised sparsityconfigurations of other groups did not hinder the performance ofany specific group.In essence, these results affirm that personalising the localisationof ASTGNNs doesnt compromise performance. Instead, it offersan effective and pragmatic approach."
}