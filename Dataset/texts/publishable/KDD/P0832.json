{
  "Abstract": "In cloud-centric recommender system, regular data exchanges be-tween user devices and cloud could potentially elevate bandwidthdemands and privacy risks. On-device recommendation emergesas a viable solution by performing reranking locally to alleviatethese concerns. Existing methods primarily focus on developinglocal adaptive parameters, while potentially neglecting the criti-cal role of tailor-made model architecture. Insights from broaderresearch domains suggest that varying data distributions mightfavor distinct architectures for better fitting. In addition, imposinga uniform model structure across heterogeneous devices may resultin risking inefficacy on less capable devices or sub-optimal perfor-mance on those with sufficient capabilities. In response to thesegaps, our paper introduces Forward-OFA, a novel approach for thedynamic construction of device-specific networks (both structureand parameters). Forward-OFA employs a structure controller toselectively determine whether each block needs to be assembledfor a given device. However, during the training of the structurecontroller, these assembled heterogeneous structures are jointlyoptimized, where the co-adaption among blocks might encountergradient conflicts. To mitigate this, Forward-OFA is designed toestablish a structure-guided mapping of real-time behaviors to theparameters of assembled networks. Structure-related parametersand parallel components within the mapper prevent each part fromreceiving heterogeneous gradients from others, thus bypassing thegradient conflicts for coupled optimization. Besides, direct mappingenables Forward-OFA to achieve adaptation through only oneforward pass, allowing for swift adaptation to changing interestsand eliminating the requirement for on-device backpropagation.",
  "Shengyu Zhang and Kun Kuang are corresponding authors": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 25, August 37, 2025, Toronto, ON, Canada 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-1245-6/25/08 Further sophisticated design protects user privacy and makes theconsumption of additional modules on device negligible. Experi-ments on real-world datasets demonstrate the effectiveness andefficiency of Forward-OFA.",
  "Recommender System, On-device Recommendation": "ACM Reference Format:Kairui Fu, Zheqi Lv, Shengyu Zhang, Fan Wu, and Kun Kuang. 2025. For-ward Once for All: Structural Parameterized Adaptation for Efficient Cloud-coordinated On-device Recommendation. In Proceedings of the 31st ACMSIGKDD Conference on Knowledge Discovery and Data Mining V.1 (KDD 25),August 37, 2025, Toronto, ON, Canada. ACM, New York, NY, USA, 12 pages.",
  "INTRODUCTION": "Recent advances in deep learning have significantly enhanced thecapabilities of recommender system, particularly in extracting userpreferences from intricate sequential data. Traditional on-cloud recommendation methods primarily focus on enhancing thescalability and generalizability of models deployed on cloud. Insuch systems, user requests are processed on cloud, and recommen-dation lists are subsequently delivered. This process necessitatestransmitting user data between remote devices and cloud, whichcan introduce substantial network overhead, especially in scenarioscharacterized by frequent user interactions. Moreover,these user requests often include sensitive information, such as re-cent item interactions(e.g. item id), or even user private profiles(e.g.ages, income, etc.). Uploading these sensitive data to cloud mayresult in a potential leakage of user privacy.Benefiting from the booming computational resources on mo-bile devices, recommender system is now deploying models di-rectly to mobile devices to better serve users. This par-adigm leverages the computational resources of devices to con-duct real-time reranking, eliminating the need for data up-loads to cloud servers for processing. Such an approach not only",
  "Aggregate": "global model Upload real-time sequential features Send compatible networks large network (food, computer, shoes) (digital product)small network Intuitive user complex interest : (a): Brief comparison between Forward-OFA and other methods used in on-device recommendation. (b): Each device hasits own specific behaviors which change frequently while cloud has access to all the historical data of all devices. Distributionshifts among them and within each device make models trained with data on cloud degrade on some devices. (c): Large networksare conducive to exploring complex user interests, while simple networks are suitable enough for intuitive users. (d): Mostusers own mobile devices that dont have a lot of computing resources and the computing resources available for the currentrecommendation task will change in real time due to the presence of other apps. mitigates network traffic burdens but also significantly enhancesuser privacy protections. Existing research in this area generallyfalls into two main categories: personalization-based and cost-aware mechanisms. The former focuses on tailoring device-specificparameters to enhance the modeling of long-tailedusers and items. Contrastingly, cost-aware methods prioritizeminimizing both the communication overhead involved in synchro-nizing local models from cloud and the computational cost of on-device inference due to the requirement for continuous adaptation.For instance, some studies explore optimal moments forupdating models from cloud to prevent unnecessary data transmis-sion. Other works make an effort to remove redundantparameters for efficient transmission and inference.While the aforementioned methods have made significant stridesin on-device recommendation, their focus has predominantly beenon network parameters, potentially overlooking the importance ofnetwork structures. Current research demonstrated that the unique-ness of network structures plays a vital role in data distributionmodeling. This phenomenon is illustrated in (c) ofrecommender system, where for some intuitive users(the gray one)smaller networks can already accurately model user interests. Incontrast, larger networks are more conducive to modeling com-plex user interests(the blue one), as these users prefer to exploreunknown interests. (b) reveals the extensive variability ofinterest between devices themselves and clouds. Consistently de-ploying identical networks does not ensure satisfactory services formost devices with changing interests. When it comes to resourceheterogeneity in (d), universally applying larger networkswill add a substantial burden for devices with limited resources anddisrupt the functionality of other applications due to continuousresource usage. On the contrary, applying smaller models for alldevices may not fully utilize the capabilities of more robust devices. Consequently, the adaptive construction of networks has becomecritical for user-oriented recommender systems. Given its demand-ing and challenging characteristics, this problem raises four crucialresearch challenges: (i): How to design the corresponding localstructures for each device accurately? (ii): How to efficientlybuild adaptive models for a given device to accommodate its localvarying interests? (iii): How to protect user privacy from exposureto cloud during this process? (iv): How to avoid increasing muchburden on devices due to the additional modules? Seq 1 Seq 2 Unused block Updated by Seq 1 Updated by Seq 2 Conflict block",
  ": When backpropagation, conflict gradients fromtwo sequences with different interests will prevent sharedblocks(red blocks) from being updated correctly": "In light of this, we propose Forward-OFA(Forward Once ForAll), an approach to building both compatible and compact net-works for a given device with just one forward pass and with-out necessitating further on-device backpropagation after train-ing on cloud. Current sequential recommenders, which consist ofstacked blocks, guide us to consider learning infer-ence paths containing one or more blocks as specific structures.In particular, we sampled heterogeneous paths from a discrete",
  "Structural Parameterized Adaptation for Efficient Cloud-coordinated On-device RecommendationKDD 25, August 37, 2025, Toronto, ON, Canada": "Jiangchao Yao, Shengyu Zhang, Yang Yao, Feng Wang, Jianxin Ma, Jianwei Zhang,Yunfei Chu, Luo Ji, Kunyang Jia, Tao Shen, et al. 2022. Edge-cloud polarization andcollaboration: A comprehensive survey for ai. IEEE Transactions on Knowledgeand Data Engineering 35, 7 (2022), 68666886. Fajie Yuan, Alexandros Karatzoglou, Ioannis Arapakis, Joemon M Jose, and Xi-angnan He. 2019. A simple convolutional generative network for next itemrecommendation. In Proceedings of the twelfth ACM international conference onweb search and data mining. 582590. Shengyu Zhang, Tan Jiang, Tan Wang, Kun Kuang, Zhou Zhao, Jianke Zhu, JinYu, Hongxia Yang, and Fei Wu. 2020. Devlbert: Learning deconfounded visio-linguistic representations. In Proceedings of the 28th ACM International Conferenceon Multimedia. 43734382. Shengyu Zhang, Dong Yao, Zhou Zhao, Tat-Seng Chua, and Fei Wu. 2021.Causerec: Counterfactual user sequence synthesis for sequential recommen-dation. In Proceedings of the 44th International ACM SIGIR Conference on Researchand Development in Information Retrieval. 367377.",
  "RELATED WORK2.1Sequential Recommendation": "The field of sequential recommendation has witnessed significantadvancements in recent years. Traditional sequential recommendersmainly rely on the Markov assumptions, where current inter-actions only depend on the most recent interaction or interactions, making it challenging to capture long-term dependencies. Afterthe emergence of neural networks, neural sequential recom-menders have started to utilize advanced techniques to enhancemodeling capabilities and account for long-term dependencies.For instance, GRU4Rec, a solution based on recurrent neural net-works, can effectively capture temporal dependencies in sequences.Given multiple history items for each user, determining the impor-tance of them to the target item becomes a critical issue. Attention-based mechanisms address it by assigning soft atten-tion weights to items at different time steps, allowing for moreflexible integration of information and more precise capture of userinterests. Additionally, some CNN-based recommendersutilize a convolutional network to process user sequential infor-mation by treating the users embeddings as a picture. However,all the above methods are essentially served on cloud, necessitat-ing frequent message transmission and raising privacy concerns.Compared with them, Forward-OFA and other on-device recom-mendation methods, are motivated to handle these problems byperforming recommendations locally owing to the increasing on-device resources. These model-agnostic frameworks reduce userresponse time and protect user privacy from exposure to cloud.",
  "On-device Recommendation": "Coinciding with the continuous upgrading of computing resourceson mobile devices, on-device recommendation is widely used toalleviate network congestion and privacy issues. The majority ofcurrent research aims to exploit the diversity of models on de-vices to enrich personalized user services. DCCL incor-porates meta-patch into recommenders to avoid the expensive up-date of finetuning the entire model, the gradient will be passed tocloud server for aggregation. DUET directly sends personalizedparameters to device to better adapt to the users local interest.PEEL, on the other hand, groups users on cloud and designs acompact and specific item embedding for each group, which notonly enhances the personalization but also reduces the space re-quired by embedding. Another method named DIET aims atdiscovering the most compatible parameters for device-specific dis-tribution within a random network to quickly adapt to local changes.Moreover, to keep pace with the evolving interests of users, recentstudies have introduced methods to detect changes on thedevice and request new models from cloud when nec-essary. Nevertheless, current methods mainly focus on parameteradaptation while potentially neglecting the potential of architec-tures, which is primarily emphasized in our paper.",
  "METHOD3.1Preliminary": "In recommender system, consider an item set = {1,2, ...,}and device(user) set = {1,2, ...,}, where and repre-sent the number of items and devices. Cloud has the capabilityto access the historical interactions of each device, denoted asD = {(,)}=1,...,,=1,... , where is the length of the his-torical interactions of device and = {1, ...1} denotes all",
  "Skip": ": Illustrations of all components in Forward-OFA. (a): At the beginning of each session when interest changes dramati-cally, cloud sends candidate item embeddings to device and those embeddings will be cached for its recommendation in thissession. (b): The structure controller consists of an extractor and a lightweight layer for searching the suitable path(distributionvector). The vector will be used in (c) to make structure-related parameters and alleviate the gradient conflict. (c): A mapper toassign personalized and structural parameters, aiming at removing gradient conflicts during training. (d): Each device does notnecessarily own the whole network, but only a sub-model to acquire efficiency.",
  "(1)": "Here, P and F here refer to the number of parameters and FLOPs forinference of . Besides, each device continue to generate real-timeinteraction data = {1 , ...,1}, which may not have appearedon cloud before. Ideally, should have good generalization abilityand show adequate adaptation on these emerging data.In on-device recommendation, we follow those widely-usedparadigms from recent works. At the start of eachsession, or when user interests change significantly, a series of candi-date item embeddings and the updated model are simultaneously sentto devices for local reranking. These embeddings are then cachedon devices in (b). As the recommended items for the userto click are chosen from these candidate items during this session,there is no requirement for any cloud-side involvement duringpredictions unless local interests change dramatically.",
  "Structure Controller for Specific Networks": "The use of stacked blocks has become prevalent in recent archi-tectures , significantly advancing the development of rec-ommender systems. However, those numerous blocks also pose achallenge for on-device recommendation, particularly for resource-sensitive devices where a larger network leads to longer computa-tion time. Additionally, complex structures can potentially resultin overfitting, especially on devices with limited data. Hence, itis crucial to precisely allocate appropriate blocks for each device toensure efficiency and reliability. A straight approach is to determine where to stop and discard the latter blocks. However, we find thatthose latter layers might also be crucial for stable prediction as theyare closer to the final classifier. In contrast, we tend to predict thepresence of each block individually to reserve those later but impor-tant blocks. Specifically, we seek a distribution variable 2 for each network, where is the number of blocks(residual blockor transformer block) in a given network. With this variable, foreach block with 1 , we assemble it if ,0 ,1 otherwisewe would skip it. Formally,",
  "0otherwise(3)": "Obviously the gradient of +1 cannot propagate back to vector due to the non-differential function in Equation 3. Thisinability to optimize through gradient backpropagation leads to themodel consistently choosing a single path for parameter optimiza-tion, thus failing to achieve the intended purpose of heterogeneousstructures. To overcome this, the Gumbel-Softmax straight-throughestimator is employed to address the non-differentiability of. For random variable , = ((,)) sampledfrom a Gumbel distribution, where , is sampled from a uniformdistribution (0, 1), it is used to reparameterize to obtain the",
  "=1=0 (,+, )/ ,(4)": "where serves as a temperature control for the smoothness of thesampling process. The closer is to 0, the more the reparameterizedvariables resemble the discrete distribution(i.e. one hot repre-sentation). As illustrated in (b), the binary vector can besampled during the forward pass, and the gradient of the discretevariable can be approximated by computing the gradient of thecontinuous softmax relaxation for backpropagation:",
  ", + , (,),(5)": "where denotes the stop gradient operation, namely, () = and () = 0.With Equation 5, the selected structures can evolve dynami-cally while trained with the parameters together. However, sucha method requires training an individual distribution vector foreach device due to their diverse interests, which is impracticalwithin recommender systems containing substantial and contin-uously expanding users. The expenses associated with retrainingthe distribution variable and network parameters for each devicecan be notably high. Additionally, the scarcity of local data for eachdevice adds to the difficulty of the training process. In response tothese difficulties, we propose the direct learning of a mapping fromuser interactions to its special distribution vector :",
  "= ((1 , ...,1).(7)": "In that case, the structure controller can be trained along withthe network parameters. This strategy can also be seen as a wayto share knowledge among devices with similar interests.In our framework, we place the structure controller on device asit only comprises a single GRU and another fully connected layer.Apart from this, these modules will be utilized to build adaptivesub-structures only when interest changes dramatically or at thebeginning of each session, which momentarily occupiesonly a few resources.",
  "Structural Parameters to Alleviate Conflicts": "Despite the promising approach we developed, there are still un-derlying challenges when training the structure controller and theoriginal parameters simultaneously. Illustrated in , differentinteractions tend to exhibit varying interests, which consequentlyrequires different network configurations. The optimization of het-erogeneous configurations thus introduces another critical problem,the co-adaptation of them with the shared parameters pre-vents each subnetwork achieves the optimal performance. In , the unshared blocks between the two sequences(e.g., the orange block owned by Seq 2 in the figure) result in the conflict on boththe former block(block 1) and the latter blocks(blocks 4, 5) dueto inconsistent gradient directions: i) The latter blocksreceive input from both block 1 and block 2, necessitating a consid-erable burden to fit both structures sharing the same parameters. ii)Regarding the former block, heterogeneous gradients from blocks2 and 4 are transmitted to it, leading to a more pronounced impact.These discrepancies ultimately result in suboptimal networks.In contrast to prior approaches utilizing few-shot learning , which still encounter potential conflict issues and introducesignificant overhead during training, we seek to bypass the sharedweights when updating the gradients by assigning structure-related parameters for the corresponding networks(conflicti). Similar to .2, given user real-time data , Forward-OFA extracts the latent interests in advance and leverages thepowerful hypernetwork to dynamically generate parametersfor each device. As the structures are also determined by devicedata and its more potential knowledge than the structure vari-ables {,0, ,1}=0,..., in our experiment it is adopted to constructstructure-related parameters to bypass the weight sharing problem.In technique, only a feature extractor and fully connectedlayers {}=1,..., are employed to establish a mapping from userreal-time data to the parameters of blocks.",
  "= {}=1,..., = { ((1 , ...,1))}=1,...,.(8)": "Under these circumstances, gradients would be directly propagatedback to {}=1,..., and {}=1,...,. Moreover, the layers withinthe mapper are executed in parallel, thereby there wont be het-erogeneous gradients from each other like in , overcomingconflict ii.Apart from this, as network structures can also have an impacton the compatible parameters, the output of the structure controllerin .2 is used as the initial state of : = {}=1,...,",
  "= { ((1 , ...,1, {,0, ,1}=0,...))}=1,...,.(9)": "With structural parameterization, our method constructs specificnetworks (in both structure and parameters) for each device basedon its local interests. Once a device desires to acquire new models, itautonomously builds network paths and sequential characteristics = (1 , ...,1, {,0, ,1}=0,...) itself, subsequently upload-ing them to cloud to acquire compatible and adaptive networks. Thewhole process is efficient as it only needs one forward inference oncloud without any backpropagation on device.",
  "Compact Constraint and Loss Function": "The structural parameterized networks primarily focus on rec-ommendation performance but may overlook the resource con-straints of mobile devices. For instance, two sub-structures mightachieve comparable performance for a specific user. As the methodabove does not consider efficiency, it could mistakenly select sub-structures with longer inference time. Towards this end, we intro-duce another compact constraint L with coefficient to encouragea lower probability of a block being executed:",
  "Datasets. We adopt Movielens-1M, MovieLens-10M, Amazon-Game, and Amazon-food, four real-world datasets in the exper-iment. The data preprocessing can be found in Appendix A.1": "4.1.2Evaluation metrics. Through our experiments, we employthe widely adopted metrics NDCG and Hit in our experi-ments to assess the accuracy of recommendations provided by eachmethod for devices. In terms of resource constraints, we primarilyfocus on the inference time on devices, calculated using FLOPs(floating point operations) of local networks during a single infer-ence. Additionally, users are required to update the latest modelsfrom the cloud as needed to meet their requirements. Therefore,the size of network transmission, which is reflected by the num-ber of bits in the recommenders(param), should also be taken intoconsideration. It is important to note that higher NDCG and Hitmean better performance while smaller FLOPs and paramsindicate less dependence on resources. Consistently, we utilizeNDCG@10 and Hit@10 throughout the experiment for uniformity. 4.1.3Baseline. To demonstrate the effectiveness of Forward-OFA,we adopt two widely used sequential recommender SASRec andNextItNet as base models, as they are made up of transformerand convolutional blocks, separately. We select 6 methods(Device-Rec, Finetune, DUET, Rare Gem, STTD and Gater) asbaselines. More descriptions can be found in the Appendix. 4.1.4Implementation Details. The SASRec and NextItNet we usedin the experiment are 6 and 12 blocks, respectively. We followconsistent training epochs for all models, except for Finetune andRare Gem, which require additional on-device tuning and sparsenetwork retraining. The learning rate is set to 0.001 for SASRecand 0.002 for NextItNet, respectively. Specifically for SASRec, thecoefficient for resource constraint is 0.005 for MovieLens-1M andAmazon-food, 0.001 for Amazon-game, and 0.01 for MovieLens-10M. For NextItNet, all datasets adopt 0.001 as the coefficient. InSASRec, the temperature is set to 5, 6, 5, and 7 for MovieLens-1M,MovieLens-10M, Amazon-game, and Amazon-food, respectively.In NextItNet, the temperature values are set to 12, 7, 6, and 6 forthe corresponding datasets. To ensure a fair comparison, we haveadjusted the hyperparameters of compression-based methods andcompared their performance with Forward-OFA under the same deviceconstraints.",
  "Overall Analysis(RQ1)": "To demonstrate the potential of Forward-OFA, we conduct experi-ments on four real-world benchmarks, comparing them to a rangeof baselines. The results are shown in . From these results,we can conclude that although deploying models on devices reducesthe frequency of transmission when requesting, DeviceRec demandsthe same number of floating-point operations for inference, whichis not practical for most devices with limited computing resources.Additionally, consistent networks for all devices may produce am-biguous recommendations for some users compared to those withadaptive networks, owing to the distinct distribution between themand cloud. It also requires enormous bandwidth to request the latestmodels from cloud in (a).STTD and Rare Gem aim to compress the model parameters toreduce the transmission cost during the cloud-coordinated modelupdating process. However, neither of them effectively mitigatesthe overhead caused by on-device inference. In fact, STTD evenincreases the computational cost due to the additional computa-tion required for the semi-tensor products matrix multiplication.Additionally, the random distribution of 0s in the sparse matrixobtained by Rare Gem still necessitates calculations once there arenon-zero elements in each layer. Gater speeds up local inferencebut fails to achieve a satisfactory compression ratio without en-suring comparable performance. Furthermore, these methods stillexhibit degradation on certain datasets, notably in Movielens-10Mof NextItNet, where Rare Gem reduced the NDCG and Hit by 16%.Finetune leverages limited interactions on devices to adjust theparameters. However, as depicted in , it does not yield pro-vide satisfying results compared to DeviceRec. We attribute thisphenomenon to the overfitting problem for devices with few in-teractions and changing interest between training and test data.Another personalized method DUET achieves relatively large im-provements in both NDCG and Hit. Personalized parameters foreach user can effectively capture the latent interests of each userin the behavior sequence. Nevertheless, as mentioned in , the consistent model structures in DUET may not be effectiveenough to serve some users. The same inference overhead as theoriginal model also limits its practical application.",
  "Improv.25.74%15.96%2.442.4829.36%22.60%3.483.5773.39%66.67%2.422.4530.24%30.99%4.825.02": "On the contrary, Forward-OFA constructs both local adaptiveparameters and structures for each device and adopts a structuralmapper to facilitate efficient adaptation. This approach not onlyachieves satisfying performance but also minimizes resource con-sumption, encompassing both transmission delay and inference.Notably, Forward-OFA outperforms other baselines on all four met-rics, underscoring the effectiveness of our framework. In particular,Forward-OFA improved by nearly 20% on the Movielens-1M andMovielens-10M datasets, and by 30% on Amazon-food of NextItNet.Its worth noting that the NextItNet utilized in our experimentshas more blocks than SASRec (6 vs 12), consequently resulting inrelatively smaller FLOPs on NextItNet compared to those on SASRec.",
  "In-depth Analysis(RQ2)": "4.3.1Block visualization. To gain a deeper understanding of theimpact of our framework, we conduct visualizations of block distri-butions and experiments to explore the influence of each componenton adaptive networks. In , we plot the distribution of localadaptive networks and usage of blocks in the test set, which canprovide insight into the functioning of our method. Based on thefigure, we have the following observations: 1) Different blocks ex-hibit significant variations in the number of times they are selected.Some blocks are infrequently chosen, while others are deemed morecrucial for most users, reflecting the diverse interests of users on thedevice. Most users tend to include the first and last blocks in theirlocal networks as the former mainly extract low-level knowledge ofuser interactions and the latter is responsible for the final sequentialrepresentation in recommender system. 2) Not all users require allblocks within their local network. On the contrary, only a smallfraction of users tend to retain the entire network, demonstratingthe significance of building adaptive networks for each user. 4.3.2Influence of adaptive networks. presents the perfor-mance of the ablation models and the base recommender DeviceRec.+controller denotes the augmentation of the base model with astructural controller to predict the network structure for devices.From the table, we can conclude that in this way this approachdoes not always lead to satisfactory performance, despite the com-pact model it selects as explained in . As introduced in.3, gradient conflicts result in incorrect updates within",
  "+mapper0.11930.20880.08160.14220.04610.0692": "the shared blocks. With fewer blocks in SASRec, more users opt toshare blocks, making it more pronounced in such scenarios. How-ever, we can still observe some improvements on Amazon-Gamewhere the conflict problem is relatively small and +controller con-sistently performs better than the base recommender. This findingdemonstrates heterogeneous structures are indeed beneficial fora precise recommendation. On the other hand, +mapper bypassesthe updates of shared blocks by associating parameters with userinterests and the network structure, thus significantly enhancingthe models effectiveness. 4.3.3Influence of sparsity constraint. We are interested in under-standing how the sparsity constraint L in .4 influenceour method, therefore we adjust the coefficient and plot the resultin . First, when we increase the coefficient , the average",
  "KDD 25, August 37, 2025, Toronto, ON, CanadaKairui Fu, Zheqi Lv, Shengyu Zhang, Fan Wu, & Kun Kuang": "FLOPs and params in the test set will continue to decline as higher forces the probability of unnecessary blocks being selected todecrease. Furthermore, even without the sparsity constraint, thelocal models still require fewer resources than the original recom-mender in , thus proving the effectiveness of our structurecontroller. Second, an appropriate constraint could help our choiceto be more deterministic and trust those more effective blocks,thereby enabling the learning of better subnetworks. Figures 5 (a)and (b) show that the sparsity constraint improves the recommen-dation, but it drops significantly when is so large that even usefulblocks are ignored. Therefore, choosing the balance between realperformance and resources becomes crucial for Forward-OFA. : The influence of the coefficient of the sparsityconstraint. The first and second columns are the experimen-tal results on Movielens and Amazon-game respectively. (a)and (b) report the results on NDCG and Hit while (c) and (d)represent the results on FLOPs and params. 4.3.4Influence of structure-based parameters. Here we take ananalysis to figure out how structure-related parameters benefitadaptive networks in our framework. Therefore we ablate somecomponents in our method and compare their performance on threedatasets. The ablated models are as follows: w.o. structural vector does not take the distribution vec-tor(Equation 7) as an auxiliary input for the adaptive pa-rameters. This allows the parameters to bypass conflictinggradient updates but ignore the structural information.",
  "first block uses the first blocks in the adaptive networks,where is the number of blocks": "last block consistently uses the last blocks in the adap-tive networks.The detailed performance is shown in , from which wehave the following observations: 1) Parameters containing the struc-tural information perform better than those without on all datasets,proving the necessity of the structure we generated as input. 2) Notusing model-specified blocks will yield poor results, whether usingthe first few or the last few blocks. We believe that not only the exact number of blocks, but the combination of blocks is crucial.Therefore, only by considering both the structural parameters andthe adaptive structure can the model that best suits the interests ofthe device be obtained.",
  "Forward-OFA0.12260.21410.08240.14290.04770.0708": "4.3.5Comparision between Forward-OFA and smaller baselines. Asdiscussed above, our framework can obtain better performancewith smaller models. Inspired by this, we begin to question whetherthis improvement is simply due to using small models rather thanadaptive networks. To further clarify the necessity of adaptive net-works, we replace all the baselines in with smaller models,each of which is 1 3 the size of the backbones in . We conductexperiments on three datasets and list the results in . Thefirst observation is that larger models may not always provide bet-ter performance. While most larger models achieve better results,on some datasets, the smaller SASRec performs better, which is con-sistent with what we introduced in . Another observationis that despite smaller models used by other baselines, our methodstill achieves the best performance, especially on Movielens-1Mand Movielens-10M where our approach has improved by approxi-mately 20% to 30%. It makes sense that users latent interests canonly be well modeled with adaptive models instead of uniformlylarge or small models.",
  "Complexity and Privacy Analysis(RQ3)": "As Forward-OFA takes user historical embeddings as input, upload-ing the sequence or embeddings to the cloud can lead to privacyissues by potentially leaking the devices privacy issues. In contrast,we address the privacy problem by placing the structure generator(a GRU and a fully connected layer) and the sequence extractorof the structural mapper(a GRU) on device. At the beginning ofeach session or when user interests dramatically change, de-vice would extract its own interest, select network paths itself andsend them to cloud, which can protect user privacy because cloudcan only get the sequence features extracted by devices. Othermodules will be saved on cloud to prevent much burden fordevices. Besides, as mentioned in .1 the candidate embed-dings of this session will be stored in the local cache, there is noneed to communicate with cloud in this session anymore.For the complexity, we show the parameters and FLOPs of eachcomponent in . Forward-OFA only adds a small fraction ofparameters(0.012 of SASRec and 0.005 of NextItNet), which istolerable for resource-sensitive devices as once deployed, these",
  "Improv.29.42%19.06%29.05%22.55%59.81%50.38%": "modules wont be updated from the cloud frequently likeparameters. Moreover, the extra modules on device take muchless time to do inference than the original model. The inference ofForward-OFA will only happen when interests on device change dra-matically or at the beginning of each session, occasional lightweightinference prevents it from occupying a large number of the devicesresources for a long time. As for modules on cloud, even if theyconsume more parameters, they are shared among all devices andcan be executed in parallel. Additionally, abundant computingresources on cloud make it efficient to hold these modules. : FLOPs and Param of each component on devicesand cloud. Backbone denotes the original sequential recom-mender, while Forward-device and Forward-cloud denote theextra component included by Forward-OFA, the former isstored on device while the latter is stored on cloud.",
  "Case Study(RQ4)": "To better understand the importance of adaptive networks for rec-ommendation, we present the case study results from the Movielens-10M dataset. Specifically, we sample four individual interactionsand generate the corresponding networks through Forward-OFAas shown in . The adaptive path(structure) is presentedin (b), where we can observe that different user interestscorrespond to different paths. Apart from this, some sequences(1,3, 4) only require part of the blocks to finish inference, while se-quence 2 needs all blocks. Without compatible networks, the finalrecommendation list will be a deviation from expected results. For a clearer understanding, we use the orange sequence(denotedas P) as the input of recommender and consider the following threenetworks: (1) the adaptive network generated from Forward-OFA.(2) the dense model trained with data on cloud. (3) incompati-ble networks generated using Forward-OFA and yellow interac-tions(denoted as Y). (a) demonstrates that the adaptivenetwork has the least parameters and provides the best result(withthe next-predicted item and relevant items). The trained networkgives moderate results, which contain relevant items but not theones that users are most interested in. Conversely, when utilizing Yto generate masks, which exhibit disparate preferences compared toP, the outcomes are unfavorable, yielding few relevant movie recom-mendations. These results further demonstrate that only networkslearned by Forward-OFA can best fit user requirements. Input User interaction Generate incompatible path",
  "CONCLUSION": "In this paper, we propose an efficient framework, Forward-OFA toconstruct local adaptive networks in only one forward pass. Thecompatible model provides better performance and lowers com-putational costs, as only a small fraction of blocks are necessaryand beneficial for each device. Extensive experiments and compre-hensive analysis of various real-world datasets and widely usedsequential recommenders demonstrate the feasibility and effective-ness of the Forward-OFA. Furthermore, this work can be viewed asan initiative to explore the possibility of searching for compatibleand lightweight networks for specific tasks.",
  "Acknowledgement": "This work was supported by the 2030 National Science and Technol-ogy Major Project (2022ZD0119100), the National Natural ScienceFoundation of China (62376243, 62402429, 62441605, 62037001), theStarry Night Science Fund at Shanghai Institute for Advanced Study(Zhejiang University), the Key Research and Development Programof Zhejiang Province(2024C03270), ZJU Kunpeng&Ascend Centerof Excellence. This work was also supported by Ant group. Theauthor gratefully acknowledges the support of Zhejiang UniversityEducation Foundation Qizhen Scholar Foundation.",
  "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016. Neuralmodule networks. In Proceedings of the IEEE conference on computer vision andpattern recognition. 3948": "Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and QuocLe. 2018. Understanding and simplifying one-shot architecture search. In Inter-national conference on machine learning. PMLR, 550559. Yukuo Cen, Jianwei Zhang, Xu Zou, Chang Zhou, Hongxia Yang, and Jie Tang.2020. Controllable multi-interest framework for recommendation. In Proceedingsof the 26th ACM SIGKDD International Conference on Knowledge Discovery & DataMining. 29422951. Jingfan Chen, Guanghui Zhu, Haojun Hou, Chunfeng Yuan, and Yihua Huang.2022. AutoGSR: Neural architecture search for graph-based session recommen-dation. In Proceedings of the 45th international ACM SIGIR conference on researchand development in information retrieval. 16941704. Tong Chen, Hongzhi Yin, Yujia Zheng, Zi Huang, Yang Wang, and Meng Wang.2021. Learning elastic embeddings for customizing on-device recommenders. InProceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & DataMining. 138147. Zhourong Chen, Yang Li, Samy Bengio, and Si Si. 2019. You look twice: Gaternetfor dynamic filter selection in cnns. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition. 91729180. Liu Chong, Xiaoyang Liu, Rongqin Zheng, Lixin Zhang, Xiaobo Liang, JuntaoLi, Lijun Wu, Min Zhang, and Leyu Lin. 2023. CT4Rec: Simple yet EffectiveConsistency Training for Sequential Recommendation. In Proceedings of the 29thACM SIGKDD Conference on Knowledge Discovery and Data Mining. 39013913. Chengquan Feng, Li Lyna Zhang, Yuanchi Liu, Jiahang Xu, Chengruidong Zhang,Zhiyuan Wang, Ting Cao, Mao Yang, and Haisheng Tan. 2024. {LitePred}:Transferable and Scalable Latency Prediction for {Hardware-Aware} NeuralArchitecture Search. In 21st USENIX Symposium on Networked Systems Designand Implementation (NSDI 24). 14631477. Kairui Fu, Qiaowei Miao, Shengyu Zhang, Kun Kuang, and Fei Wu. 2023. End-to-End Optimization of Quantization-Based Structure Learning and InterventionalNext-Item Recommendation. In CAAI International Conference on Artificial Intel-ligence. Springer, 415429. Kairui Fu, Shengyu Zhang, Zheqi Lv, Jingyuan Chen, and Jiwei Li. 2024. DIET:Customized Slimming for Incompatible Networks in Sequential Recommendation.In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery andData Mining. 816826. Yu Gong, Ziwen Jiang, Yufei Feng, Binbin Hu, Kaiqi Zhao, Qingwen Liu, andWenwu Ou. 2020. EdgeRec: recommender system on edge in Mobile Taobao. InProceedings of the 29th ACM International Conference on Information & KnowledgeManagement. 24772484.",
  "in social media. In Proceedings of the international AAAI conference on web andsocial media, Vol. 7. 311320": "Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, and JulianMcAuley. 2023. Text is all you need: Learning language representations forsequential recommendation. In Proceedings of the 29th ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining. 12581267. Shuchang Liu, Shuyuan Xu, Wenhui Yu, Zuohui Fu, Yongfeng Zhang, and AmelieMarian. 2021. FedCT: Federated collaborative transfer for recommendation.In Proceedings of the 44th international ACM SIGIR conference on research anddevelopment in information retrieval. 716725. Zheqi Lv, Shaoxuan He, Tianyu Zhan, Shengyu Zhang, Wenqiao Zhang, JingyuanChen, Zhou Zhao, and Fei Wu. 2024. Semantic Codebook Learning for DynamicRecommendation Models. In Proceedings of the 32nd ACM International Conferenceon Multimedia. 96119620.",
  "Zheqi Lv, Wenqiao Zhang, Zhengyu Chen, Shengyu Zhang, and Kun Kuang. 2024.Intelligent model update strategy for sequential recommendation. In Proceedingsof the ACM on Web Conference 2024. 31173128": "Zheqi Lv, Wenqiao Zhang, Shengyu Zhang, Kun Kuang, Feng Wang, YongweiWang, Zhengyu Chen, Tao Shen, Hongxia Yang, Beng Chin Ooi, et al. 2023. Duet:A tuning-free device-cloud collaborative parameters generation framework forefficient device model generalization. In Proceedings of the ACM Web Conference2023. 30773085. Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendationsusing distantly-labeled reviews and fine-grained aspects. In Proceedings of the2019 conference on empirical methods in natural language processing and the 9thinternational joint conference on natural language processing (EMNLP-IJCNLP).188197. Chaoyue Niu, Fan Wu, Shaojie Tang, Lifeng Hua, Rongfei Jia, Chengfei Lv, ZhihuaWu, and Guihai Chen. 2020. Billion-scale federated learning on mobile clients: Asubmodel design with tunable privacy. In Proceedings of the 26th Annual Interna-tional Conference on Mobile Computing and Networking. 114.",
  "Yoon-Joo Park and Alexander Tuzhilin. 2008. The long tail of recommendersystems and how to leverage it. In Proceedings of the 2008 ACM conference onRecommender systems. 1118": "Xufeng Qian, Yue Xu, Fuyu Lv, Shengyu Zhang, Ziwen Jiang, Qingwen Liu, XiaoyiZeng, Tat-Seng Chua, and Fei Wu. 2022. Intelligent request strategy design inrecommender system. In Proceedings of the 28th ACM SIGKDD Conference onKnowledge Discovery and Data Mining. 37723782. Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factor-izing personalized markov chains for next-basket recommendation. In Proceedingsof the 19th international conference on World wide web. 811820. Kartik Sreenivasan, Jy-yong Sohn, Liu Yang, Matthew Grinde, Alliot Nagle,Hongyi Wang, Eric Xing, Kangwook Lee, and Dimitris Papailiopoulos. 2022. Raregems: Finding lottery tickets at initialization. Advances in Neural InformationProcessing Systems 35 (2022), 1452914540. Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.2019. BERT4Rec: Sequential recommendation with bidirectional encoder rep-resentations from transformer. In Proceedings of the 28th ACM internationalconference on information and knowledge management. 14411450. Ximeng Sun, Rameswar Panda, Rogerio Feris, and Kate Saenko. 2020. Adashare:Learning what to share for efficient deep multi-task learning. Advances in NeuralInformation Processing Systems 33 (2020), 87288740. Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommenda-tion via convolutional sequence embedding. In Proceedings of the eleventh ACMinternational conference on web search and data mining. 565573. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processing systems 30 (2017). Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang, and Xing Xie. 2022. Fe-dAttack: Effective and covert poisoning attack on federated recommendation viahard sampling. In Proceedings of the 28th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining. 41644172. Yunjia Xi, Weiwen Liu, Yang Wang, Ruiming Tang, Weinan Zhang, Yue Zhu,Rui Zhang, and Yong Yu. 2023. On-device integrated re-ranking with heteroge-neous behavior modeling. In Proceedings of the 29th ACM SIGKDD Conference onKnowledge Discovery and Data Mining. 52255236. Xin Xia, Hongzhi Yin, Junliang Yu, Qinyong Wang, Guandong Xu, and QuocViet Hung Nguyen. 2022.On-device next-item recommendation with self-supervised knowledge distillation. In Proceedings of the 45th International ACMSIGIR Conference on Research and Development in Information Retrieval. 546555. Jiangchao Yao, Feng Wang, Xichen Ding, Shaohu Chen, Bo Han, Jingren Zhou,and Hongxia Yang. 2022. Device-cloud collaborative recommendation via metacontroller. In Proceedings of the 28th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining. 43534362. Jiangchao Yao, Feng Wang, Kunyang Jia, Bo Han, Jingren Zhou, and Hongxia Yang.2021. Device-cloud collaborative learning for recommendation. In Proceedingsof the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining.38653874.",
  "Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019. Deep learning based recom-mender system: A survey and new perspectives. ACM computing surveys (CSUR)52, 1 (2019), 138": "Tunhou Zhang, Dehua Cheng, Yuchen He, Zhengxing Chen, Xiaoliang Dai, LiangXiong, Feng Yan, Hai Li, Yiran Chen, and Wei Wen. 2023. NASRec: weight sharingneural architecture search for recommender systems. In Proceedings of the ACMWeb Conference 2023. 11991207. Yipeng Zhang, Xin Wang, Hong Chen, and Wenwu Zhu. 2023. Adaptive disen-tangled transformer for sequential recommendation. In Proceedings of the 29thACM SIGKDD Conference on Knowledge Discovery and Data Mining. 34343445.",
  "Yiyang Zhao, Linnan Wang, Yuandong Tian, Rodrigo Fonseca, and Tian Guo.2021. Few-shot neural architecture search. In International Conference on MachineLearning. PMLR, 1270712718": "Ruiqi Zheng, Liang Qu, Tong Chen, Kai Zheng, Yuhui Shi, and Hongzhi Yin. 2024.Personalized Elastic Embedding Learning for On-Device Recommendation. IEEETransactions on Knowledge and Data Engineering (2024). Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, YanghuiYan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-throughrate prediction. In Proceedings of the 24th ACM SIGKDD international conferenceon knowledge discovery & data mining. 10591068.",
  "AExperimental SetupA.1Datasets": "We evaluate our framework on four public benchmarks in recom-mender system Movielens-1M1, Movielens-10M2, Amazon-Food, and Amazon-Game3. The statistics of which are presented intable 6. Consistent with prior research, all user-item pairs with pos-itive ratings in the dataset are considered positive samples. Owingto variations in sparsity across the datasets, we exclude users anditems with fewer than 20 interactions to provide reliable results.To construct sequential scenarios of user interaction in practicalapplications, we order each users interactions by their interactiontime. The last interaction of each user will be used for test andothers will be used for training.",
  "Base Models. Given that current sequential recommenders pre-dominantly consist of Transformer and CNN, we choose SASRecand NextItNet as the base model, upon which we incorporatedifferent methods:": "SASRec integrates a self-attention mechanism intorecommender system, thereby enhancing the capture of ausers interests by considering the individual impact of eachitem in the historical sequence on the target item. NextItNet employs a convolutional neural network tointroduce local perception and parameter sharing. It standsas the pioneering work to utilize residual learning in recom-mender system, effectively modeling long-range and intri-cate dependencies within historical sequences.",
  "Target: Real Time Interaction , Device-specific Networks Next-clicked Item Input: Real Time Interaction , Device-specific NetworksOutput: Next-clicked Item": "to protect privacy. This process is both memory and calculation-efficient and does not occupy much resource on device. Then devicewill send its latent interest to cloud to finish the network assembly.In Algorithm 1, parameter mapping for each layer on cloud canbe executed in parallel. Compatible networks will be sent back tofinish later recommendations.",
  "CAssociation between Neural ArchitectureSearch and Forward-OFA": "Smilarity. Both NAS (Neural Architecture Search) and Forward-OFA aim to identify suitable networks for devices that adapt tospecific data distributions while minimizing resource consump-tion. They both address an often overlooked research problem: thefundamental significance of network structures to various datadistributions. Forward-OFA is also partially motivated by thosemethods to detect light subnetworks for each device.Difference. However, NAS-based methods have to searchappropriate structures in advance and retrain local data to getdevice-specific networks. The whole subnet-constructing processtakes a long time, making it impractical for billions of users. Forexample, the recent method LitePred matches and finetunes mod-els with device data, requiring substantial resources and leadingto longer responses. Besides, limited on-device interactions maycause overfitting or suboptimal performance. In contrast, Forward-OFA achieves adaptation through a single forward pass, directly mapping local interests to networks. We also conducted an addi-tional comparable experiment on LitePred, where the results withours are shown in . Owing to the accurate matching pro-cess, LightPred successfully outperforms DeviceRec and Finetune,demonstrating the necessity of discovering valuable subnetworks.However, as mentioned above, limited on-device interactions andvarious latent device interests restrict its potential to learn com-patible parameters, leading to a large performance drop comparedto Forward-OFA which directly builds networks from real-timeinteractions.",
  "DAdaptation Time to Changing Interests": "To build insights into the efficient adaptation of Forward-OFA, inthis section, we compare both the adaptation time once interestsshift and the communication time for the model update. In thissection, we will further validate the efficiency of Forward-OFA inquickly adapting to changing user interests. As shown in , wefound that on-cloud fine-tuning on an Nvidia RTX 3090 GPU(35.58TFLOPS) takes 1000 times longer than Forward-OFA. For mobiledevices like the iPhone 16(1789.4 GFLOPS), on-device fine-tuningcan exceed 94 seconds, which is about 10,000 times slower thanForward-OFA."
}