{
  "Faculty of Information Technology, Czech Technical University in Prague, Thkurova 9, Prague, 160 00, Czech Republic": "AbstractThe evaluation of recommendation systems is a complex task. The offline and online evaluation metricsfor recommender systems are ambiguous in their true objectives. The majority of recently publishedpapers benchmark their methods using ill-posed offline evaluation methodology that often fails topredict true online performance. Because of this, the impact that academic research has on the industryis reduced. The aim of our research is to investigate and compare the online performance of offlineevaluation metrics. We show that penalizing popular items and considering the time of transactionsduring the evaluation significantly improves our ability to choose the best recommendation model fora live recommender system. Our results, averaged over five large-size real-world live data procuredfrom recommenders, aim to help the academic community to understand better offline evaluation andoptimization criteria that are more relevant for real applications of recommender systems.",
  ". Introduction": "The evaluation of recommendation systems is a complex task. One of the primary reasons is thatseveral evaluation metrics represent distinct properties of a recommendation algorithm (RA).For instance, RMSE is a reliable marker of how a RA predicts a user-to-item taste , whereasRecall reflects how well the same algorithm retrieves a relevant list of items . Therefore, itis crucial to identify metrics that effectively evaluate and describe the target of a particularrecommender system (RS).A comprehensive search of the relevant literature revealed that the most prevalent method isoffline evaluation of static feedback, where data are collected from a real-world RS dataset .In offline evaluation, recommendation algorithms are trained on a training subset of data ,and then their recommendations are evaluated on test data using metrics such as Recall@ ,Precision@ , NDCG@ , MAP@ and HR@ .However, conventional offline evaluation of RS has significant disadvantages. For instance, 2nd Edition of EvalRS: a Rounded Evaluation of Recommender Systems, August 6 - August 10, 2023, Long Beach, CA,USA*Corresponding author. (P. Kasalick); (R. Alves); (P. Kordk) 0000-0001-6438-366X (P. Kasalick); 0000-0001-7458-5281 (R. Alves); 0000-0003-1433-0089 (P. Kordk)",
  "arXiv:2308.06885v1 [cs.IR] 14 Aug 2023": "the interaction observation follows a non-uniform distribution : certain items havemore interaction observations solely because the RS exposed them to more users. In addi-tion to observation bias, standard offline evaluation techniques are agnostic to the fact thatrecommendation algorithms perform in a live environment where it is only possible to useinteractions made in the past to predict future interactions: the commonly employed leave-one-out-cross-validation (LOOCV) method does not track the users behavior overtime.Several approaches have been proposed to address these drawbacks. Bias in the test datacaused by the missing-not-at-random (MNAR) problem can be partially suppressed by sam-pling or by using popularity-stratified recall , which gives a higher reward forrecommending less popular items. The unbiased evaluation method for simulating banditalgorithms introduced in was experimentally verified in and found to work only forTop-1 recommendations.Regarding time-aware validation processes, proposed leave-last-one-out-cross-validation(LLOOCV), where the validation set only considers the latest iteration of each user. Althoughthere is a clear past-future distinction for each user, in LLOOCV there is no such distinctionbetween users. offered a more comprehensive solution called -fold Sliding Window Eval-uation (SW-EVAL), which analyzes the recommendation algorithm based only on interactionsthat occurred after the conclusion of the training data.Unfortunately, despite the significant progress made by the RS community to debiasing offlineevaluation, the techniques often do not adequately reflect the nature of online recommendersystems. An optimal evaluation technique should review the entire system and consider theplatforms high-level objectives, such as the number of clicks on items, the number of itemspurchased, the number of adverts viewed, the customer lifetime value, etc. . Such metricsare unsuitable for the offline setting because a RSs environment is domain-dependent, dynamic,and constantly changing. Furthermore, the results are frequently unreproducible because theyare based on the monitoring of implicit interactions of a real user or asking them to use pop-upsurveys for explicit ratings of the recommended items .This paper fulfills the research objectives outlined for the EvalRS 2023 workshop byexperimentally exploring the relationship between offline and online metrics. We note that thiscomparison type is crucial for real-world recommenders commercial success. In real scenarios,it is possible to compare a large number of models by using offline metrics. However, to avoidcompromising the systems accuracy, only a limited number of models can be examined online.Our first step is to investigate which offline evaluation criteria enables to navigate towardsarchitectures and parametrizations with better online performance. More specifically, we areinterested in verifying whether a model with the highest Recall@ (when evaluated offline) alsoproduces the highest CTR (when evaluated online). We also verified the impact of includingpopularity-penalization and time aspect on the CTR-Recall@ relation. Finally, we introducea new offline evaluation metric more adaptive to live environments. Our metric, so-called@, simultaneously incorporates popularity penalization and time dependency ofinteractions.",
  ". Related work": "Due to the complexity of performing online evaluations, comparative studies between onlineand offline metrics are scarce. Added to that, existing solutions typically analyze a singledataset. For instance, in the authors compare offline and online metrics on the Swissnews website swissinfo.ch. They constructed a metric titled Success@ and used it to fit themodel. The value of Success@ in the (offline) validation set is then compared to the CTRin the online environment. As a result, they hypothesize that RAs that are dominant basedon offline metrics are no longer as effective in the online context since they favor popularitems. In contrast, the RA that recommends random items does significantly better in the onlineevaluation than in the offline evaluation because they encourage content exploration. As afollow-up, in they proposed a model predicting online performance based on five differentoffline metrics but did not find a universal formula for predicting online performance. A similarmismatch between offline and online evaluation is shown for individual domains. For example,in , the authors used the MovieLens dataset and evaluated the online performance of RAbased on feedback from 100 users invited to their experiment. The behavior of Docear usersreceiving recommendations of research articles was investigated in with the conclusionthat offline evaluations are probably not suitable to evaluate recommender systems in thisdomain. An experiment involving 4287 users of a travel agency was conducted in . However,according to the findings reported in , a general cross-domain comparison is missing. Astudy from Netflix from 2021 describes the same issue regarding deep-learning models.They identify a \"mismatch in offline and online settings\" as one of the unresolved practicalchallenges of current recommendation systems. The authors of the study give a hint that Netflixuses its own offline bias-suppressing metric for more corresponding offline evaluation. However,they failed to describe essential details, such as how to use contextual bandit techniques toremove various biases. The researchers in addressed the challenge of offline evaluation inindustry recommender systems by investigating behavioral principles and developing RecList,an open-source tool that employs NLP techniques to assess the effectiveness of a RA in commonrecommendation scenarios. All of the previously referenced research compare offline to onlinemetrics without taking popularity bias and time-dependence into account. Differently, weinvestigate how a correction for popularity bias and a validation set consisting purely ofinteractions that happened after the end of the training data affect the optimization ofonline metrics. We also analyzed multiple datasets.",
  ". Method": "In our experiment, we are examining whether (1) incorporating a time dimension into an offlineevaluation approach, and (2) reducing popularity bias by assigning less weight to errors offrequently interacted items, reduces the disparity between offline and online metrics. In theory,one expects that offline metrics (when employed to cross-validate models) result in improvedonline performance in live environments.Basic notation: Denote the set of interactions between users and items by = {1, . . . , }. Asingle interaction is denoted as ( Z) for {1, . . . , }, where = {1, . . . , }is a set of all RS users, = {1, . . . , } is a set of items which can be recommended, Z is theset of integers expressing the timestamp when the interaction was performed and is a totalnumber of interactions. Then we represent the set of interactions between the item and theuser as, = {(, , ) | (, , ) : = = }. Finally, define the set of relevant items for the user as . We assume that items thatare relevant for a given user can be extracted from explicit and/or implicit feedback. Thus, weconsidered the item relevant to the user if implicitly (e.g., view, click) interacted with .",
  "with1 = {2 | (2, 2) : 2 < 1}": "where is a list of interactions of user defined as = {(, ) | (, , ) : = }and the expression {2 | (2, 2) : 2 < 1} represents the set of items that were interactedby the user before timestamp 1.Including popularity-penalization from and incorporating it into Eq. (1), we get",
  "()": "and () denotes relative popularity of item . Note that for = 0, Eq. (3) and (1) areequivalent, and Eq. (4) with (2) as well.Remark: In , the authors explain the effect of the hyperparameter for offline evaluationbut no longer show whether penalizing popularity makes recall a more appropriate metric forthe online environment. proposed that reducing bias by penalization of popularity canlead to a better offline evaluation, but with no experimental verification.",
  ". Online evaluation": "Another approach to evaluating a RA is based on feedback from users interacting with a liverecommender system. More specifically, we work on a scenario where items are recommendedto a user, and the RS collects the users reactions to the recommended items. An example of areaction is when the user clicks on the recommended item.The most widely used online metric (given its universality) is the click-through rate (CTR).CTR can be seen as the ratio between the accepted recommendations and all offered recommen-dations . We consider that the user accepted the recommendation if he clicked on at leastone of the recommended items.Remark: A recommender system can observe implicit and explicit CTRs. An explicit CTRis calculated if the RS has clear evidence that a user clicked on a particular item as a resultof the recommendation. The implicit CTR can be calculated based on interactions if the RSdoes not have explicit knowledge that the item has been clicked through as a result of therecommendation.Formally, : {0, 1} defines a set of recommendations where each recommendationis represented by a timestamp , user and by a set of recommended items .When assuming that the set of interactions contains only \"clicked-type\" interactions, thenthe implicit CTR can be calculated as:",
  ". Methodology of the experiment": "The goal of our experiment is to find out how different versions of @ and@ as validation metric relates to CTR.Due to the limited resources to conduct online experiments, to perform our analysis, we firstselect a backbone algorithm: the item-NN algorithm with the similarity between itemsmeasured by the cosine similarities of latent space embeddings. The latent space embeddings arecreated using matrix factorization from implicit feedback with different data preprocessingand hyperparameters (such as latent space size and regularization) to ensure diverse performanceaccording to offline metrics. Second, we train our model and measure its performance usingdifferent versions of recall (i.e., @ and @ with as a hyperparameter).Third, our model is deployed to an RS with live interactions, and thus the CTR can be measured.Note that the RS is constantly receiving new interactions from users. Therefore, we re-trainedthe model to include them periodically.Once the individual steps of the experiment are fulfilled and implemented, the experiment isperformed on several datasets. The results of the experiment are measured CTRs along with thenumber of users that interacted with each model. The RS splits users for each model equally.Once the user was assigned to the model during the first recommendation, the same modelgenerated any further recommendations.Subsequently, the vector of CTRs can be taken for the dataset and its = 5 deployed models = (1 , 2 , . . . , 5 ). Similarly, one = (1 , 2 , . . . , 5 ) vector is measured for eachcombination of hyperparameters , , . The entire list of vectors of recalls is measuredaccording to their hyperparameters = {1,1,1, 2,2,2, . . . , , , },where is the total number of combinations of hyperparameters and , , are theindividual hyperparameter values.Inspired by practical application, we are interested in whether the best model according tooffline metrics for a given dataset is also the best according to online metrics. In other words,we find out what the chances are that if we choose the best model according to recall, it will bethe best model according to CTR. This can be measured using Recall@1. To distinguish between@ measuring the quality of item recommendations and Recall@1 measuringwhether the best model according to offline metrics is also the best model in online evaluation,we will explicitly denote the latter as Model Selection Recall (MSR). We define MSR as theratio of how many times the best model according to offline metrics has been selected by offlinemetrics, namely recalls with different hyperparameters (, ).",
  ". Datasets and collected data": "Commonly stable research datasets (such as MovieLens or Last.fm) cannot be used in ourexperiment since live users are required for online evaluation. Because of that, our work wasperformed by using real datasets with live customers. For each model and each dataset, iCTRwith a parameter = 10 (minutes) was measured over 18 days. The number of users and thenumber of their recommendations participating in iCTR measurement depend on the data setand other external circumstances and are shown in .The datasets have been selected to include different domains. In addition to e-commerceservices, a video streaming platform is represented. Also, dataset D is very different from othere-commerce customers as it is a B2B business with a few products and customers with anunconventional high number of interactions. Another parameter by which the datasets wereselected is the number of recommendations per day. The datasets with small traffic could notbe selected since the traffic needs to be divided between models and an estimate of CTR needsto be as accurate as possible.",
  ". Used hyperparameters and resulting MSR": "Thevaluesofhyperparametersforwhichrecallwasmeasuredwere{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 50} and {0.05|{0, 1, . . . , 20}}.The hyperparameter specifies whether is used Equation (3) or (4) for the cross-validationmethod. shows the relation between iCTR and recall for selected s and both values of hyperparameter. It can be seen that the inclusion of the time aspect using the LLOCVtechnique led to an increase in MSR, which means the best online model was more oftenselected using the offline metric. The same effect can be seen when using the penalization ofpopularity by the beta hyperparameter. However, it is not true that the more popularity ispenalized, the better the offline metric is.The best metric found with LLOOCV and = 0.30 has MSR= 34.29% and therefore, it isbetter in choosing the best online model than commonly used LOOCV technique with = 0(i.e., without popularity penalization) with MSR= 12.86%.",
  ". Conclusion": "We researched various solutions to better address the known issues of offline metrics with theaim of reducing the gap between offline and online metrics. Then we performed a large-scaleexperiment that examined the effect of penalizing popularity and including the time aspect ofrecommendations on the relation between recall and CTR. The experiment was conducted onfive commercial datasets covering multiple domains (such as e-commerce, video streaming).It was shown that, in general, maximizing recall does not always lead to maximizing CTR.Popularity-stratified recall proved to be a good approach for reducing the MNAR problemleading to better model selection for an online environment but measured using offline metrics.Offline evaluation with an emphasis on the time aspect of recommendations using leave-last-one-out cross-validation also showed that it can lead to an improvement in MSR. Using theproposed method, we were able to improve the selection of the best online model from theoriginal MSR= 12.86% to MSR= 34.29%.",
  ". Future work": "The relation between recall and CTR was investigated on five datasets and each contained fivedifferent models. To measure the relation more accurately, it would be advisable to have moremodels, but this greatly prolongs the experiment (since there is a limit traffic of new users) andincreases the already exhaustive engineering effort. Similarly, it would be interesting to includedatasets from other domains, such as news or bazaars, as these are very specific to the changingpopularity of the item. As a final improvement, the SW-EVAL method could be used for stricteradherence to the time aspect. Another extension advisable for the future is to examine howthe best possible offline metric relates to dataset attributes such as number of users, numberof items, sparsity of the interaction matrix, and domain of the dataset. This requires a largemeta-study based on experiments with dozens of real datasets for a long period of time.",
  "This work was supported by the Grant Agency of the Czech Technical University in Prague(SGS23/210/OHK3/3T/18). The experiment was carried out in cooperation with Recombee": "A. Ledent, R. Alves, M. Kloft, Orthogonal Inductive Matrix Completion, IEEE Transac-tions on Neural Networks and Learning Systems (2021) 112. doi:10.1109/TNNLS.2021.3106155, arXiv:2004.01653 [cs, stat]. T. Silveira, M. Zhang, X. Lin, Y. Liu, S. Ma, How good your recommender system is? Asurvey on evaluations in recommendation, International Journal of Machine Learning andCybernetics 10 (2019) 813831. doi:10.1007/s13042-017-0762-9. J. Ni, J. Li, J. McAuley, Justifying Recommendations using Distantly-Labeled Reviewsand Fine-Grained Aspects, in: Proceedings of the 2019 Conference on Empirical Methodsin Natural Language Processing and the 9th International Joint Conference on NaturalLanguage Processing (EMNLP-IJCNLP), Association for Computational Linguistics, HongKong, China, 2019, pp. 188197. URL: doi:10.18653/v1/D19-1018. D. Wannigamage, M. Barlow, E. Lakshika, K. Kasmarik, Steam Games Dataset : Player counthistory, Price history and data about games, volume 1, 2020. URL: doi:10.17632/ycy3sy3vj2.1, publisher: Mendeley Data.",
  "A. Gunawardana, G. Shani, S. Yogev, Evaluating Recommender Systems, Springer US,New York, NY, 2022, pp. 547601. URL:": "H. Wang, F. Zhang, M. Zhang, J. Leskovec, M. Zhao, W. Li, Z. Wang, Knowledge-awareGraph Neural Networks with Label Smoothness Regularization for Recommender Sys-tems, in: Proceedings of the 25th ACM SIGKDD International Conference on KnowledgeDiscovery & Data Mining, KDD 19, Association for Computing Machinery, New York,NY, USA, 2019, pp. 968977. doi:10.1145/3292500.3330836.",
  "N. Sachdeva, G. Manco, E. Ritacco, V. Pudi, Sequential Variational Autoencoders forCollaborative Filtering, Technical Report arXiv:1811.09975, arXiv, 2018. ArXiv:1811.09975[cs, stat] type: article": "H. Steck, Embarrassingly Shallow Autoencoders for Sparse Data, in: The World WideWeb Conference, WWW 19, Association for Computing Machinery, New York, NY, USA,2019, pp. 32513257. doi:10.1145/3308558.3313710. C. Yang, L. Bai, C. Zhang, Q. Yuan, J. Han, Bridging Collaborative Filtering and Semi-Supervised Learning: A Neural Approach for POI Recommendation, in: Proceedings of the23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,KDD 17, Association for Computing Machinery, New York, NY, USA, 2017, pp. 12451254.URL: doi:10.1145/3097983.3098094.",
  "WWW 18, International World Wide Web Conferences Steering Committee, Republic andCanton of Geneva, CHE, 2018, pp. 729739. doi:10.1145/3178876.3186154": "B. M. Marlin, R. S. Zemel, S. Roweis, M. Slaney, Collaborative filtering and the missing atrandom assumption, in: Proceedings of the Twenty-Third Conference on Uncertainty inArtificial Intelligence, UAI07, AUAI Press, Arlington, Virginia, USA, 2007, pp. 267275. B. M. Marlin, R. S. Zemel, Collaborative prediction and ranking with non-random missingdata, in: Proceedings of the third ACM conference on Recommender systems, RecSys 09,Association for Computing Machinery, New York, NY, USA, 2009, pp. 512. doi:10.1145/1639714.1639717. H. Steck, Item popularity and recommendation accuracy, in: Proceedings of the fifth ACMconference on Recommender systems, RecSys 11, Association for Computing Machinery,New York, NY, USA, 2011, pp. 125132. doi:10.1145/2043932.2043957. X. Li, J. She, Collaborative Variational Autoencoder for Recommender Systems, in:Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discoveryand Data Mining, ACM, Halifax NS Canada, 2017, pp. 305314. doi:10.1145/3097983.3098077.",
  "J. Vinagre, A. M. Jorge, J. Gama,Evaluation of recommender systems in streamingenvironments,Silicon Valley, United States, 2014. doi:10.13140/2.1.4381.5367,arXiv:1504.08175 [cs]": "D. Carraro, D. Bridge, A sampling approach to Debiasing the offline evaluation of recom-mender systems, Journal of Intelligent Information Systems 58 (2022) 311336. URL: doi:10.1007/s10844-021-00651-y. D. Carraro, D. Bridge, Debiased offline evaluation of recommender systems: a weighted-sampling approach, in: Proceedings of the 35th Annual ACM Symposium on Applied Com-puting, SAC 20, Association for Computing Machinery, New York, NY, USA, 2020, pp. 14351442. URL: doi:10.1145/3341105.3375759. L. Li, W. Chu, J. Langford, X. Wang, Unbiased offline evaluation of contextual-bandit-based news article recommendation algorithms, in: Proceedings of the Fourth ACMInternational Conference on Web Search and Data Mining, WSDM 11, Association forComputing Machinery, New York, NY, USA, 2011, p. 297306. URL: doi:10.1145/1935826.1935878. J. Beel, S. Langer, A comparison of offline evaluations, online evaluations, and user studiesin the context of research-paper recommender systems, in: S. Kapidakis, C. Mazurek,M. Werla (Eds.), Research and Advanced Technology for Digital Libraries, Springer Inter-national Publishing, Cham, 2015, pp. 153168. Q. Zhao, J. Chen, M. Chen, S. Jain, A. Beutel, F. Belletti, E. H. Chi, Categorical-attributes-based item classification for recommender systems, in: Proceedings of the 12th ACMConference on Recommender Systems, RecSys 18, Association for Computing Machinery,New York, NY, USA, 2018, pp. 320328. doi:10.1145/3240323.3240367.",
  "F. Bianchi, P. J. Chia, C. Greco, C. Pomo, G. Moreira, D. Eynard, F. Husain, J. Tagli-abue, Evalrs 2023. well-rounded recommender systems for real-world deployments, 2023.arXiv:2304.07145": "F. Garcin, B. Faltings, O. Donatsch, A. Alazzawi, C. Bruttin, A. Huber, Offline and onlineevaluation of news recommender systems at swissinfo.ch, in: Proceedings of the 8th ACMConference on Recommender systems - RecSys 14, ACM Press, Foster City, Silicon Valley,California, USA, 2014, pp. 169176. doi:10.1145/2645710.2645745. A. Maksai, F. Garcin, B. Faltings, Predicting online performance of news recommendersystems through richer evaluation metrics, in: Proceedings of the 9th ACM Conferenceon Recommender Systems, RecSys 15, Association for Computing Machinery, New York,NY, USA, 2015, p. 179186. URL: doi:10.1145/2792838.2800184. M. Rossetti, F. Stella, M. Zanker, Contrasting Offline and Online Results when EvaluatingRecommendation Algorithms, in: Proceedings of the 10th ACM Conference on Recom-mender Systems, RecSys 16, Association for Computing Machinery, New York, NY, USA,2016, pp. 3134. URL: doi:10.1145/2959100.2959176. J. Beel, M. Genzmehr, S. Langer, A. Nrnberger, B. Gipp, A comparative analysis of offlineand online evaluations and discussion of research paper recommender system evaluation,in: Proceedings of the International Workshop on Reproducibility and Replication inRecommender Systems Evaluation, RepSys 13, Association for Computing Machinery,New York, NY, USA, 2013, pp. 714. doi:10.1145/2532508.2532511. L. Peska, P. Vojtas, Off-line vs. On-line Evaluation of Recommender Systems in SmallE-commerce, Proceedings of the 31st ACM Conference on Hypertext and Social Media(2020) 291300. doi:10.1145/3372923.3404781, arXiv: 1809.03186. O. Jeunen, Revisiting offline evaluation for implicit-feedback recommender systems,in: Proceedings of the 13th ACM Conference on Recommender Systems, RecSys 19,Association for Computing Machinery, New York, NY, USA, 2019, pp. 596600. doi:10.1145/3298689.3347069. H. Steck, L. Baltrunas, E. Elahi, D. Liang, Y. Raimond, J. Basilico, Deep Learning forRecommender Systems: A Netflix Case Study, AI Magazine 42 (2021) 718. URL: doi:10.1609/aimag.v42i3.18140,number: 3. P. J. Chia, J. Tagliabue, F. Bianchi, C. He, B. Ko, Beyond ndcg: Behavioral testing ofrecommender systems with reclist, WWW 22 Companion, Association for ComputingMachinery, New York, NY, USA, 2022, p. 99104. URL: doi:10.1145/3487553.3524215. A. N. Nikolakopoulos, X. Ning, C. Desrosiers, G. Karypis, Trust Your Neighbors: AComprehensive Survey of Neighborhood-Based Methods for Recommender Systems, in:F. Ricci, L. Rokach, B. Shapira (Eds.), Recommender Systems Handbook, Springer US,"
}