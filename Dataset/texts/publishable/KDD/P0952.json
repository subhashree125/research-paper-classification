{
  "ABSTRACT": "Recent advances in product bundling have leveraged multimodal in-formation through sophisticated encoders, but remain constrainedby limited semantic understanding and a narrow scope of knowl-edge. Therefore, some attempts employ In-context Learning (ICL)to explore the potential of large language models (LLMs) for theirextensive knowledge and complex reasoning abilities. However,these efforts are inadequate in understanding mulitmodal data andexploiting LLMs knowledge for product bundling. To bridge thegap, we introduce Bundle-MLLM, a novel framework that fine-tunes LLMs through a hybrid item tokenization approach withina well-designed optimization strategy. Specifically, we integratetextual, media, and relational data into a unified tokenization, intro-ducing a soft separation token to distinguish between textual andnon-textual tokens. Additionally, a streamlined yet powerful multi-modal fusion module is employed to embed all non-textual featuresinto a single, informative token, significantly boosting efficiency.To tailor product bundling tasks for LLMs, we reformulate the taskas a multiple-choice question with candidate items as options. Wefurther propose a progressive optimization strategy that fine-tunesLLMs for disentangled objectives: 1) learning bundle patterns and2) enhancing multimodal semantic understanding specific to prod-uct bundling. Extensive experiments on four datasets across twodomains demonstrate that our approach outperforms a range ofstate-of-the-art (SOTA) methods.",
  "Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 25, August 37, 2025, Toronto, ON, Canada 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00",
  "INTRODUCTION": "Product bundling is a prevailing marketing strategy where multipleproducts are combined into an unified item set (i.e., bundle) andoffered together at a discounted price . Such strategy cansignificantly boost sales by encouraging customers to purchasemore items together, thereby increasing the average transactionvalue. It also enhances customer satisfaction by offering perceivedsavings and convenience, potentially leading to greater customerloyalty and repeat business. In practice, product bundling neces-sitates multi-faceted information and domain knowledge, whichis labor intensive and resources costly . To this end, automaticproduct bundling has attracted considerable interests, heralding anovel avenue for creating appealing bundles .",
  "KDD 25, August 37, 2025, Toronto, ON, CanadaXiaohao Liu, Jie Wu, Zhulin Tao, Yunshan Ma, Yinwei Wei, and Tat-seng Chua": "The development of automatic production bundling has clearlyevolved, shifting from mere co-occurrence-based methods to moresophisticated approaches that integrate multimodal information.Traditional methods predict the remaining items in a bundle basedon the co-occurrence relationships of items within existing bun-dles . Modern methods incorporate multimodal informationto enhance item representations, offering a stronger rationale forbundling at the content level. . However, these newer meth-ods are constrained by their limited semantic understanding andthe narrow scope of knowledge within a compact model. Semanticunderstanding helps construct the semantic context for items acrossmultiple modalities. For example, a cotton coat can be characterizedby various semantic attributes derived from different multimodalsources, such as yellow for its color and cotton fabric for its material.Furthermore, extensive knowledge takes into account the nuancedneeds of the input partial bundle, encapsulating complex bundlingstrategies. For instance, as shown in , a cotton coat pairedwith slim-fitting trousers might suggest a complementary need,associated with spring or autumn for seasonality, while two rocksongs might indicate a similar need. Despite their importance, theseaspects were largely overlooked until the advent of large languagemodels (LLMs), which are capable of understanding fine-grainedsemantics from tokens and are trained on diverse corpora.Recognizing the potential of adapting LLMs for product bundling,AICL employs in-context learning (ICL) to retrieve relevantbundle samples for effective prompting. However, such approachfalls short in two key areas: they fail to leverage multimodal features(i.e., media content and relational data) and cannot fully exploitLLMs specifically for bundling tasks. Multimodal features are cru-cial for enhancing item representation in product bundling. For in-stance, media content (i.e., acoustic or visual media) can compensatefor the limitations of textual descriptions by providing richer, morenuanced item representations. Similarly, relational data (i.e., user-item interactions and bundle-item affiliations) introduces globaland heterogeneous correlations among items, extending the limitedcontext that can be captured within prompts. Moreover, fine-tuningLLMs is essential to comprehend multimodal features and invokebundle-specific knowledge, therefore achieving accurate and rele-vant bundling outputs. However, recent multimodal LLMs (MLLMs),such as vision-text or audio-text models , rely heavilyon extensive data to align different modalities, primarily for cap-tioning tasks. This approach incurs significant resource costs andfalls short in addressing the diverse modalities (e.g., relational data)and the multiple items needed for effective product bundling. Thus,the challenge of fine-tuning bundle-specific LLMs to incorporatemultimodal information remains unresolved and demands furtherresearch.To this end, we propose a novel multimodal large language frame-work for product bundling, termed Bundle-MLLM. This frameworkintroduces hybrid item tokenization, enabling the model to processnot only descriptive text but also diverse item modalities. Specifi-cally, media features are extracted using foundation encoders likeBLIP2 for visual media and CLAP for acoustic media, whilerelational features are derived from a pre-trained collaborative filter-ing method (e.g., LightGCN ). Notably, all non-textual featuresare embedded into a single token per item through a streamlinedyet effective fusion module, designed to maximize the interplay among different modalities and boost efficiency. To tailor a bundle-specific MLLM, we fine-tune it with a few samples in a progres-sive optimization strategy to invoke its extensive knowledge toanswer product bundling. This strategy begins with updating theLoRA weights using text-only prompts, followed by training themultimodal fusion module and a projector to fit the task-alignedLLMs. By employing this strategy, we can maximize the synergybetween bundle pattern learning and multimodal semantic under-standing in Bundle-MLLM. Our design is both simple and efficient,demonstrating significant bundling efficacy that facilitates practicaldeployment and further advances domain development. Extensiveexperiments on four datasets across two domains demonstrate thatour method outperforms multiple leading methods, including thetraditional state-of-the-art (SOTA) method (i.e., CLHE ) andGPT-4 ). Furthermore, ablation studies and model analyses con-firm the effectiveness of key modules and underscore the criticalproperties of the proposed model, such as its anti-cold-start ca-pabilities. We summarize the key contributions of this work asfollows: We pioneer the adaptation of MLLMs for product bundling,addressing prevailing limitations such as inferior semantic un-derstanding and restricted scope of knowledge.",
  "PROBLEM FORMULATION": "Given a set of items I = {1,2, . . . , } and a set of bundlesB = {1,2, . . . ,}, where () denotes the number of items(bundles), i.e., = |I|, = |B|, we define a bundle as a set ofitems denoted as = {1,2, . . . ,}, where = || is the size of thebundle. Given the text token set T and media set M, each itemhas a textual input T (where is the token length of ),which can be its title, description, or metadata, and a media input M, which can be an image or audio of the item. In addition,for the items that have been online for a while, we have collectedsome item-level user feedback data, which is denoted as a user-iteminteraction matrix X = {, | U, I}, where U ={1,2, . . . ,} is the user set with a size of = |U|. Given the de-fined bundle-item affiliation matrix Y = {, | B, I},the product bundling aims to predict unseen bundles, denoted asB = {+1,+2, . . . ,+ }, where = | B|. We aim to obtaina model for an unseen bundle B such that, when given a fewseed items , i.e., , the partial bundle, the model can predictthe missing items \\ , thereby constructing the entire bundle.",
  "Tokenization": ": The overall framework of Bundle-MLLM, which incorporates multiple data formats, including textual, visual/acoustic,and relational features (i.e., user-item interactions and bundle-item affiliations). These heterogeneous features are extractedfrom various foundation encoders and embedded into a single multimodal token via a trainable fusion module, followed by aprojector to align with the LLM space. embedding. This approach also alleviates the computational burdenassociated with the autoregressive inference of LLMs. The bundleprompting helps organize the hierarchy and align tasks in a mannerthat LLMs prefer. Additionally, we employ progressive optimiza-tion to adapt LLMs from bundle pattern learning to multimodalsemantic understanding, maximizing the synergy of modules.",
  "Hybrid Item Tokenization": "To incorporate multiple data formats, including textual descriptions,media content, user-item interactions, and bundle-item affiliations,we introduce hybrid item tokenization.Textual Tokenization. Item textual features serve as a natural for-mat for LLM processing, involving titles or categorical tags that areinformative for direct semantic extraction. We leverage the inherenttokenizer and stored embedding table to transform textual inputinto latent embeddings. This process can be formally representedas:",
  "t; = Et(;), Et := Tokenizer LLM-Emb,(1)": "where [] is the position of the token within input . Here Et isdenoted as the composition of Tokenizer and LLM-Emb. The textualinput of item can be represented as a concatenation of an orderedseries of latent embeddings, formally, t = [t;1,t;2, . . . ,t; ]. Notethat all textual inputs, except the item description, such as taskinstructions, utilize the same procedure to obtain embedding thatis fed into the LLM.Heterogeneous Feature Extraction. The wealth of heteroge-neous multimodal information enhances the intrinsic semantics,enriching the sole textual effect. To integrate heterogeneous modal-ities (i.e., media content, user-item interactions and pre-definedbundle-item affiliations), we employ corresponding encoders to effectively extract informative representations. Note that we donot include textual information in this extraction framework sinceLLMs are inherently well-performing textual extractors. With theadvancement of foundation models, we utilize the pre-trained modelBLIP2 for visual content and CLAP for acoustic contentto yield the representations m R for item . Formally,",
  "].(5)": "where m, x, and y represent the textual indicators for each uni-modal feature (e.g., := \"media token: \") to organize them andguide the LLM in comprehending the subsequent latent token. How-ever, this approach requires a sophisticated design and might bemisled by the textual indicators. However, such approach requiresa sophisticated design, even being misled by the textual indica-tors. We propose to explicitly manifest the interplay of differentmodalities as follows:",
  "R0 = [m W,x W,y W].(7)": "For dimensional consistency, we use the transformation matrixW R and W, W R for the media content featureand the other two relational features. We then concatenate all threefeatures as the initial input to the first-layer self-attention, denoted as R0 R3 . Here, WQ, WK R are the trainable param-eters to project the input feature embeddings into the query andkey spaces; A is the attention matrix among the three modalitiesinprinR layer , followed by a softmax function for normalizedattention scores to obtain new representations at the next layer,shown as:R= softmax(A )R1.(8)",
  "Notably, we introduce a separate token <Sep> to distinguish thetextual token and the non-textual token. We are inspired by the": "soft-prompting technique to introduce a learnable token.Compared to the textual hard separator, this soft token is moreflexible and especially effective for datasets where the discrepancybetween textual and multimodal features is significant (e.g., Spotify).We also consider the direct cancellation of the separator as an alter-native for datasets where the textual description is well-categorizedinto different semantics, see the empirical evidence in .3.2.",
  "Input:": "Question: Given the partial bundle: 1. <item1> ; 2. <item2> ; ,which candidate item should be included into this bundle?Options:A. <itemA>; B. <itemB>; .Your answer should indicate your choice with a single letter (e.g., A, B, C, etc.). Choice: In the input seed bundle, numerical indicators are utilized to iden-tify items, while alphabetical indicators are assigned to candidateitems to serve as options within the multiple-choice framework.Adhering to a widely accepted protocol, we instruct the LLMs toprovide responses in the form of the corresponding option letter.In the embedding phase, these item placeholders are replaced bythe aforementioned hybrid tokens (i.e., ).",
  "Progressive Optimization": "To achieve effective product bundling capability, we employ a pro-gressive optimization strategy. For vanilla LLMs, a one-step opti-mization fails to balance bundling ability and multimodal semanticunderstanding, and even undermine each other from initializedsuboptimal states. To address this, we disentangle the completeoptimization objective into two phases: first, learning the bundlepatterns of LLMs to fit the task, and then adapting the semantic un-derstanding of multimodal features to fit the task-optimized LLMs.Bundle Pattern Learning. We adopt Parameter Efficient Fine-Tuning (PEFT) for efficiently optimizing the LLMs with a smaller setof parameters. This approach significantly reduces computationalrequirements while still achieving commendable performance. Specif-ically, we choose LoRA , a typical PEFT algorithm, which keepsthe LLM weights frozen and decomposes the updating weights intotrainable low-rank matrices. Relying on the above bundle prompt-ing, we can optimize the parameters of LoRA as follows:",
  "Fine-tuning Multimodal Large Language Models for Product BundlingKDD 25, August 37, 2025, Toronto, ON, Canada": "and efficacy of each design component. This work marks an initialstep in endowing LLMs with multimodal semantic understandingin product bundling. Moving forward, we will steer the future focusto 1) efficiently tackling much longer candidates and 2) endow-ing LLMs with personalized bundling ability. And we hope thatBundle-MLLM can inspire the researchers to explore a more unifiedframework, paving the way to the development of versatile productbundling. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, ShyamalAnadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774(2023).",
  "EXPERIMENTS": "To evaluate our proposed framework from different perspectives,we conduct experiments on four well-established datasets acrosstwo domains. SOTA conventional methods and advanced LLMsserve as our baselines for a comprehensive comparison. Further-more, ablation studies and model studies further substantiate ourclaim that the proposed framework is both reasonable and effective.To validate the superiority of our framework, we will address thefollowing research questions: RQ1: How does Bundle-MLLM perform compared to conven-tional product bundling models and LLM-based methods?",
  "Experimental Settings": "4.1.1Datasets. Following previous work on product bundling, weevaluate our model on four established datasets across differentdomains, each with distinct characteristics , as shown in .Specifically, POG is commonly used for fashion outfits, whileSpotify 1 is derived from music playlists. POG_dense is a variant ofPOG with denser user feedback, whereas Spotify_sparse features asmaller average bundle size than Spotify. Adhering to the typicalsetting , we randomly split all the bundles in each dataset intotraining, validation, and testing sets in an 8:1:1 ratio. The validationdatasets are employed to evaluate performance and find the optimalhyperparameters. 4.1.2Evaluation Protocols. For each bundle, we randomly select10 items to construct the candidate set, which includes one pos-itive item. The comparison with different numbers of candidateitems is detailed in .4.1. We use HitRate@1 as the mainmetric to evaluate whether the models can correctly predict thepositive item from the candidates. A higher HitRate@1 indicates agreater capability for accurate product bundling. To ensure a faircomparison, we implement all baselines with the same number ofcandidates and the same evaluation algorithm. Additionally, weutilize the ValidRatio to evaluate the validity of responses fromgenerative predictions . This metric addresses potential invalidresponses, such as nonsensical words or options, by quantifyingthe proportion of valid responses (i.e., correct options within thecandidate set) across all sequences. 4.1.3Baselines. To comprehensively evaluate our proposed method,we compare it with two groups of methods: conventional productbundling methods and LLM-based methods. CLHE serves as theSOTA baseline, while GPT-4 paired with ICL stands as the strongestLLM-based baseline. The following is a specific description of thesebaselines:",
  "Performance Comparison (RQ1)": "To address RQ1, we conduct a comparison with extensive baselinesfollowing the aforementioned evaluation protocol to demonstratethe superiority of Bundle-MLLM, as illustrated as . Ourobservations are as follows: As expected, Bundle-MLLM outperforms all baselines by a signif-icant margin across four datasets. Specifically, it achieves up toan 18.81% relative performance improvement w.r.t. HitRate@1,demonstrating its effective bundle capability with multimodalsemantic understanding of LLMs. Analyzing the results across four datasets, we find that the per-formance improvement on POG_dense is marginal comparedto the others. We attribute this to the sufficient data provided,such as significant dense user-item interactions and bundle-itemaffiliations, which facilitate bundle/item representation learn-ing for conventional methods (i.e., CLHE), thus narrowing theperformance gap with our proposed method.",
  "Bundle-MLLM (ours)0.41311.00000.66751.00000.82421.00000.79591.0000Rel.Imp.18.81%-4.84%-8.89%-10.13%-": "CLHE serves as the second strongest method by designing hi-erarchical learning on multimodal features and bundle repre-sentations, paired with contrastive learning. Its emphasis onintegrating multiple data formats underlines the rationale ofour work, which explores multimodal semantics with extensiveknowledge. GPT-4 achieves the best performance among the LLM-basedmethods, showcasing its complex reasoning capabilities. Equippedwith ICL, both GPT-4 and Llama2 gain certain improvementsbut still perform lower than conventional methods. Bundle-MLLM also achieves a high validity ratio of 100% acrossall datasets, illustrating the models instruction-following abil-ities when predicting the remaining items. Note that all theLLM-based methods might generate invalid responses. Notably,Llama2 serves as our LLM backbone and demonstrates signifi-cant improvements of Bundle-MLLM in the validity ratio. Re-markably, Bundle-MLLMs significant improvement in valid ra-tios can be attributed to its instruction-tuning on activatingbundle patterns for product bundling.",
  "Ablation Study (RQ2)": "To clarify RQ2, we conduct experiments to evaluate the impact of dif-ferent modalities and their interplays on Bundle-MLLM. Moreover,to enhance the significance of our proposed hybrid item tokeniza-tion, various token processing strategies and separators are tested.Additionally, we delve into the development of learning strategiesto justify the superiority of our proposed progressive optimization.",
  "(2) Bimodality: Textual description serves as the main component,paired with the other three types of modality features, denotedas Text + Media, Text + UI, and Text + BI": "(3) Multimodality: We directly use Bundle-MLLM for compari-son since its hybrid item tokenization involves all four modali-ties.The comparative results are illustrated in . We have thefollowing observations. Specifically, Bundle-MLLM achieves thebest performance by utilizing multiple data formats, demonstratingthe informative features explored by our proposed method for .Within the unimodality group, there is a distinct relationship ofText > BI > Media > UI for POG, Spotify, and Spotify_sparse,while BI outperforms others in POG_dense. We attribute this tothe unique statistics of dense bundle-item affiliations, similar toUI, which also shows moderated results with sufficient user-iteminteractions. Obviously, different bundle datasets have diverse em-phases on different modalities, informing an attentive method toeffectively integrate these data. The results of the interplay betweentextual description and other modalities (i.e., bimodality) highlightthe same conclusion that the direct combination of different modali-ties leads to inferior results. These observations not only clarify themotivation of our proposed nuanced multimodal process but alsodemonstrate the superiority and effectiveness of Bundle-MLLM.",
  "Textual Sep. 0.36310.62430.75280.7489No Sep.0.41310.66750.81830.7778Soft Sep.0.40770.64600.82420.7959": "We compare 1) Prompt, which uses a textual indicator for eachunimodal feature (see Equation 5), and 2) Fusion, which utilizes aself-attention mechanism to obtain a fused token (see Equation 9)w.r.t. HitRate@1. To distinguish the textual tokens and multimodaltokens, we include different separation strategies for <Sep>:(1) Textual Sep.: Use a textual indicator (e.g., <Sep> := \"contenttoken: \"). (2) No Sep.: Directly concatenate these two types of tokens.(3) Soft Sep.: Train an embedding dynamically adapted for differentcontexts. illustrates the detailed results. It is clear that the directprompt-based token organization is inferior to the nuanced fusionmethod, i.e., Fusion > Prompt, which contributes to finding theattentive features that adapt to varied bundle strategies. There arealso alternative methods to complete the hybrid tokens of both tex-tual description tokens and multimodal feature tokens. As expected,the simple textual separator is inferior to the other two approaches.However, we note that the benefit of Soft Sep. is lower than NoSep. in the POG and POG_dense datasets. This can be attributedto the well-categorized descriptions of items being consistent withthe semantic multimodal tokens, where any extra separation mightbreak such consistency. In contrast, for the discrepancy in Spotifyand Spotify_sparse, a dynamic soft separator outperforms.",
  "(4) S1S2: Utilizing the progressive strategy to optimize these twodisentangled objectives, adhering to our proposed progressiveoptimization in .3": "Since the single S2 does not optimize the product bundling capabil-ity of LLMs, showing low instruction-following performance, weexclude S2 from comparison in .Overall, S1S2 outperforms all other optimization strategies,which can be attributed to progressively optimizing disentangledobjectives, facilitating the successful adaptation of LLMs to mul-timodal product bundling. Notably, S1+S2 outperforms S1 whendenser relational data are provided in POG_dense and Spotify com-pared to their sparse versions, POG and Spotify_sparse, which showinconsistent optimization that leads to worse performance. Thisfinding further demonstrates that our optimization method notonly alleviates the optimizing inconsistency in sparse datasets butalso enhances performance improvements in dense datasets.",
  "Model Study (RQ3)": "For a deeper understanding of our proposed methods and to fur-ther address RQ3, we conduct experiments under various settings,particularly focusing on different candidate sizes and the cold-startscenario. These experiments explore the capabilities of our meth-ods in handling varied item lengths and invoking extensive knowl-edge for cold product bundling. Moreover, we analyze progressiveoptimization for improving effectiveness during training and ourproposed multimodal token process for boosting efficiency duringinference. 4.4.1The Impact of candidate size. We conduct experiments withdifferent numbers of candidate items for Bundle-MLLM, as shownin . As the number of candidate items increases, the tasksdifficulty rises and the prediction accuracy decreases. Nonetheless,Bundle-MLLM consistently outperforms the SOTA method (i.e.,CLHE) in all cases. This demonstrates the stability of our proposedmethod and highlights its potential to handle a large number ofcandidate items effectively.",
  ": The training loss curve across different optimizationstages": "4.4.2Cold-setting performance. Even with a few samples, Bundle-MLLM achieves superior performance, prompting an exploration ofthe cold-setting scenario through a comparison with CLHE. In thisanalysis, we re-split the POG dataset to ensure no overlap betweentraining and testing items. As illustrated in , compared toCLHE, the improvement is significant and increases with the num-ber of candidate items. Due to its reliance on sufficient supervisedlearning signals, CLHE shows a distinct gap in performance whenevaluated on a testing set that is significantly out-of-distributionfrom the training set, leading to inferior results. In contrast, Bundle-MLLM, with its extensive knowledge and activated bundle pattern,can still effectively tackle the product bundling task in a cold-settingscenario.",
  ": Inference time (bar) and average number of tokens(line) w.r.t. different item tokenization strategies": "stages on four datasets in . Our progressive optimizationstrategy first optimizes the LoRA using only textual information(i.e., S1), followed by optimizing the multimodal token process mod-ule (i.e., S2). This endows Bundle-MLLM with multimodal semanticunderstanding, further enhancing its product bundling capability.From this, we observe the gradual smoothing of the curves duringeach stage and a significant drop in loss when transitioning to thenext optimization stage. This transition breaks the standstill of theprevious status, further aligning Bundle-MLLM to product bundlingtasks. These observations firmly demonstrate the rationality andeffectiveness of our proposed optimization strategy. 4.4.4Inference efficiency. Benefiting from our proposed multi-modal token process, we improve the efficiency of autoregressiveinference in Bundle-MLLM. For a comprehensive comparison, wecalculate the inference time and the average number of tokens foreach bundling query using Text (only text information provided),Prompt, and Fusion (see .3.2), as shown in . Com-pared to Text, there is a slight increase in time and token count forFusion due to the incorporation of multiple data formats. However,when compared to Prompt, which provides the same information,Fusion significantly reduces both the time and token cost. Thisefficiency gain becomes more pronounced as the number of itemsin both the input partial bundle and candidate sets increases.",
  "CONCLUSION AND FUTURE WORK": "In this work, we pioneered a novel multimodal large language frame-work for product bundling, termed Bundle-MLLM, which integratesmultiple data formats (i.e., textual, visual/acoustic, and relationalinformation). Notably, Bundle-MLLM surpasses vanilla LLM adap-tations by activating untapped bundle patterns and compensatingfor sole textual effects. Bundle-MLLM employs advanced encodersto extract heterogeneous features and utilizes a simple yet effectivemultimodal process module to integrate them into a single token,which is then organized within a hybrid item tokenization scheme.By framing the product bundling task as a multiple-choice ques-tion, we implemented progressive optimization to achieve effectiveproduct bundling capability with comprehensive multimodal se-mantic understanding of LLMs. Empirical results demonstrated thatBundle-MLLM significantly outperforms all baselines in productbundling, underscoring its effectiveness and superior performance.Ablation studies and model analyses further validated the rationale",
  "Jianxin Chang, Chen Gao, Xiangnan He, Depeng Jin, and Yong Li. 2021. Bundlerecommendation and generation with graph neural networks. TKDE 35, 3 (2021),23262340": "Wen Chen, Pipei Huang, Jiaming Xu, Xin Guo, Cheng Guo, Fei Sun, Chao Li,Andreas Pfadler, Huan Zhao, and Binqiang Zhao. 2019. POG: personalized outfitgeneration for fashion recommendation at Alibaba iFashion. In KDD. 26622670. Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, ChangZhou, and Jingren Zhou. 2023. Qwen-Audio: Advancing Universal Audio Under-standing via Unified Large-Scale Audio-Language Models. CoRR abs/2311.07919(2023). arXiv:2311.07919 Qilin Deng, Kai Wang, Minghao Zhao, Runze Wu, Yu Ding, Zhene Zou, YueShang, Jianrong Tao, and Changjie Fan. 2021. Build your own bundle-a neuralcombinatorial optimization method. In ACM Multimedia. 26252633.",
  "Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang.2023. Clap learning audio concepts from natural language supervision. In ICASSP.IEEE, 15": "Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of trainingdeep feedforward neural networks. In Proceedings of the thirteenth internationalconference on artificial intelligence and statistics. 249256. Jiaxian Guo, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Boyang Li,Dacheng Tao, and Steven Hoi. 2023. From images to textual prompts: Zero-shot visual question answering with frozen large language models. In CVPR.1086710877.",
  "Yun He, Yin Zhang, Weiwen Liu, and James Caverlee. 2020. Consistency-awarerecommendation for user-generated item list continuation. In WSDM. 250258": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, JohannesWelbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van denDriessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, ErichElsen, Oriol Vinyals, Jack W. Rae, and Laurent Sifre. 2024. Training compute-optimal large language models. In NIPS (NIPS 22). 15 pages. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, SheanWang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of largelanguage models. arXiv preprint arXiv:2106.09685 (2021).",
  "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual Instruc-tion Tuning. In NeurIPS": "Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu,Zefeng Du, Shuming Shi, and Zhaopeng Tu. 2023. Macaw-llm: Multi-modallanguage modeling with image, audio, video, and text integration. arXiv preprintarXiv:2306.09093 (2023). Yunshan Ma, Yingzhi He, Xiang Wang, Yinwei Wei, Xiaoyu Du, Yuyangzi Fu, andTat-Seng Chua. 2024. MultiCBR: Multi-view Contrastive Learning for BundleRecommendation. ACM Transactions on Information Systems 42, 4 (2024), 123.",
  "Oren Sar Shalom, Noam Koenigstein, Ulrich Paquet, and Hastagiri P Vanchi-nathan. 2016. Beyond collaborative filtering: The list recommendation problem.In WWW. 6372": "Zhu Sun, Kaidong Feng, Jie Yang, Xinghua Qu, Hui Fang, Yew-Soon Ong, andWenyuan Liu. 2023. Dynamic In-Context Learning from Nearest Neighbors forBundle Generation. arXiv preprint arXiv:2312.16262 (2023). Zhu Sun, Jie Yang, Kaidong Feng, Hui Fang, Xinghua Qu, and Yew Soon Ong. 2022.Revisiting bundle recommendation: Datasets, tasks, challenges and opportunitiesfor intent-aware product bundling. In SIGIR. 29002911. Llama Team. 2024. The Llama 3 Herd of Models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-ale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXivpreprint arXiv:2307.09288 (2023).",
  "Zizhuo Zhang and Bang Wang. 2023. Prompt learning for news recommendation.In SIGIR. 227237": "Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen, Wenxuan Zhang, andMohamed Elhoseiny. 2023. ChatGPT Asks, BLIP-2 Answers: Automatic Ques-tioning Towards Enriched Visual Descriptions. arXiv preprint arXiv:2303.06594(2023). Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023.Minigpt-4: Enhancing vision-language understanding with advanced large lan-guage models. arXiv preprint arXiv:2304.10592 (2023).",
  "A.1Bundle Recommendation & Construction": "The development of product bundling has evolved into two maincategories: recommending pre-built bundles (i.e., bundlerecommendation) and generating bundles (i.e., bundle con-struction/product bundling) . Initially, bundle recommendationfocused on transferring benefits from item recommendation to bun-dle recommendation . Recent advancements include contrastive-based frameworks , deep attentive multi-task models,transformer-based models for bundle recommendation anddistillation-based method . Bundle construction aims to com-plete the given partial bundles, i.e., seed items, and retrieve theremaining items from the candidates . Concerning differentintents, it varies across different domains like fashion outfit ,music playlist , games package , and etc . To achievedynamical bundle construction, user behaviors are incorporate forpersonalization and offer informative bundle patterns hidden inuser-item interactions , or integrate visual informationto bundle complementary items . And most recent methodleverages both user feedback and mutlimodal features to aid the in-sufficiency bundle-item affiliations and cold-item issues . Therealso a single paper that utilizes LLMs with in-context learning viaitem titles and neighbor sessions . Different from these methods,we exhibit the semantics from multiple data formats to embody thebundle strategies, paired with activated LLMs, thus improving thebundle construction capability.",
  "Language models (LMs) have seen a significant transformation,particularly with the development of large language models (LLMs)": "that harness advanced neural network architectures like trans-formers and pre-training on expansive datasets . Modelslike GPT and BERT leverage these technologies to graspnuanced language patterns and generate responses with a deepcontextual awareness, far surpassing earlier models in both fluencyand versatility. Building on the success of LLMs and PEFT tofacilitate the training efficiency, the field has expanded into therealm of multimodal large language models (MLLMs) . SinceLLMs can only perceive text, bridging the gap between naturallanguage and other modalities falls into two categories: 1) utilizinga learnable interface to project information into the space that LLMcan understand , and 2) leveraging the textual proxy through expert models, such as image captioning model . Thesemodels integrate different formats of data, like vision , au-dio and graph into an unified framework. Inspiredby its complex reasoning on different modalities, MLLMs expand todiverse domains like biomedicine , robotics and recommen-dation system . In our work, we further step in leveragingmultimodal language model techniques to bundle construction,focusing on activating untapped bundle pattern with multimodalsemantics via progressive optimization.",
  "BIMPLEMENTATION DETAILSB.1Conventional Approaches": "All conventional product bundling methods are initialized using theXavier method and trained with the Adam optimizer . Weperform a grid search for the learning rate within {102, 103, 104}and for L2 regularization within {103, 104, 105, 106}. The em-bedding size is set to 64. For adapting conventional approaches forcomparison, we shorten the length of candidates as what Bundle-MLLM adopts, and use the computed score for ranking and outputthe predicted answer. Formally,",
  "B.2LLM-based Approaches": "For all implementations related to LLMs, we use a linear warm-up strategy, where the learning rate starts from 0 and increasesto 3 104 of the maximum learning rate. Each experiment istrained for a maximum of 10 epochs with a batch size of 16. Toevaluate the output of the LLM, we valid the answer to computethe ValidRatio metric. Then, all the answer will be transformed toa number to share a unified evaluation with conventional methods.Specifically, we apply answer = ( A), where is the validoutput of LLM methods, to obtain the corresponding numericalprediction. We do not include any MLLMs as baselines due to theirinability to comprehend multiple media content and relational data.Nonetheless, the ablation study in demonstrates that Bundle-MLLM outperforms Text+Media, an adaptation of MLLMs forproduct bundling that supports multiple media content as input.",
  "Candidate items": "Bundle pattern : Autumn/Winter Cozy Outfit.It provides a practical and stylish way to carry essentials while maintaining the overall aesthetic of the outfit. Its small size make it easy to carry, and its black color helps to create a cohesive look with the other items in the bundle. Additionally, its sleek and modern style complements the other items in the outfit, such as .",
  "B.3Bundle-MLLM": "We select Llama2-7B as the LLM backbone. Other LLMs is alsoavailable More recent released model like Llama3.1 will betuned in our future work. For a fair comparison, we select Llama2-7B as a baseline and view GPT4+ICL as the strongest LLM-basedbaseline. Notably, our method uses only 1024 samples for training,whereas all conventional baselines are trained with the entire giventraining dataset.",
  "CCASE STUDIES": "To provide an intuitive exhibition of Bundle-MLLM, we select aspecific product bundling question and inquire about the reasoning,as shown in . In this scenario, Bundle-MLLM makes the cor-rect prediction given the input partial bundle (i.e., pullover sweater, painter hat, and pencil pants). In contrast, CLHE incorrectly choosesa popular fashion item (i.e., pumpkin beret), disregarding the com-plementary requirements. To showcase the multimodal semanticunderstanding, we adapt Bundle-MLLM for chatting, asking for thereasoning in terms of bundling patterns and item semantics. Specif-ically, we use the trained adapters for multimodal feature trans-formation and instructed version of Llama2 for chatting. Bundle-MLLM predicts the bundle pattern as Autumn/Winter Cozy Outfit,which aligns with the given items and provides comprehensivereasons. These reasons include practical purposes (i.e., carryingessentials), aesthetic appeal (i.e., black color and cohesive look),and complementary requirements (i.e., complementing the otheritems). Overall, Bundle-MLLM achieves a significant breakthroughin understanding multimodal semantics from bundle patterns andenhances product bundling tasks."
}