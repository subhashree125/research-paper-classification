{
  "ABSTRACT": "With increasingly deployed deep neural networks in sensitive ap-plication domains, such as healthcare and security, its essentialto understand what kind of sensitive information can be inferredfrom these models. Most known model-targeted attacks assumeattackers have learned the application domain or training data dis-tribution to ensure successful attacks. Can removing the domaininformation from model APIs protect models from these attacks?This paper studies this critical problem. Unfortunately, even withminimal knowledge, i.e., accessing the model as an unnamed func-tion without leaking the meaning of input and output, the proposedadaptive domain inference attack (ADI) can still successfully esti-mate relevant subsets of training data. We show that the extractedrelevant data can significantly improve, for instance, the performanceof model-inversion attacks. Specifically, the ADI method utilizesa concept hierarchy extracted from a collection of available pub-lic and private datasets and a novel algorithm to adaptively tunethe likelihood of leaf concepts showing up in the unseen trainingdata. We also designed a straightforward hypothesis-testing-basedattack LDI. The ADI attack not only extracts partial training dataat the concept level but also converges fastest and requires the fewesttarget-model accesses among all candidate methods. Our code isavailable at ACM Reference Format:Yuechun Gu, Jiajie He, and Keke Chen. 2025. Adaptive Domain InferenceAttack with Concept Hierarchy. In Proceedings of ACM Conference (Con-ference17). ACM, New York, NY, USA, 15 pages.",
  "INTRODUCTION": "Large-scale deep learning models are increasingly deployed in ap-plication domains, playing pivotal roles in sectors where sensitiveor proprietary data is used in training the models . Thesemodels might be packaged in API services or embedded into applica-tions, becoming a concerning new attack vector. Recent studies haveshown several effective model-targeted privacy attacks, including Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from , July 2017, Washington, DC, USA 2025 Association for Computing Machinery.ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00 model inversion (or training data reconstruction) , propertyinference , and membership inference attacks .However, we have noticed all these model-targeted attacks de-pend on a certain level of knowledge about the application domain.(1) Model-inversion (MI) attacks depend on a learning procedure,e.g., a GAN method , to progressively adjust seed images froma domain or distribution similar to the training data domain towardsmost likely training examples. Without this domain knowledge, i.e.,with irrelevant auxiliary data, the attack performance can be sig-nificantly reduced . (2) Membership-inference attacks (MIA)estimate the possibility of a target sample belonging to the trainingdata of a model. Most MIA attacks1 assume attackers know the typeand distribution of training to train shadow models, which are alsoimportant for making sense the MIA result in practice. (3) Property-inference attacks try to uncover global properties of the training datathat the models creator did not intend to reveal. These attacks alsoneed shadow classifiers that are trained on tasks similar to that ofthe target classifier. Domain knowledge plays an important role intraining shadow classifiers.Thus, one may wonder whether it is possible to protect a modelfrom all these attacks by stripping off all the domain-related infor-mation in model inference. More specifically, can we achieve theprotection goal by packaging the model as a function call or a serviceAPI: = (), where an input leads to a prediction , with and encrypted in transmission? Such a service API mechanism is stillconvenient and deployable in a cloud-based application ecosystem.It leads to a question: whether this minimal-knowledge model de-ployment mechanism is sufficient to protect models. Recent studiesshow that even with such a minimal-knowledge setting, attackers canfind ways to re-establish knowledge about the application domain.Gu et al. first attempted to reconstruct the domain informationwith model accesses only. They use a generative adversarial network(GAN) approach (GDI) to infer which of a set of known candidatedatasets is most similar to (or likely from) the target models domain.However, the GDI method has several drawbacks. (1) Its dataset-oriented, i.e., assuming datasets similar to the target training dataexist in the candidate datasets. Our experiment shows if the candidatedataset is only partially similar to the target dataset, e.g., only oneor a few classes are similar, this method does not work well. (2)The GAN-based procedure requires excessive accesses to the targetmodel, which may raise an alarm to the model service owner.",
  "Conference17, July 2017, Washington, DC, USAYuechun Gu, Jiajie He, and Keke Chen": "(1) We demonstrate that our attacks provide more accurate domainestimations compared to GDI on high-resolution, ImageNet-relateddatasets and complex concept hierarchies. (2) Our attacks requiresignificantly fewer accesses to the target model to achieve superiorresults, which is crucial for attacks in privacy-sensitive environments.(3) We have thoroughly evaluated the impact of various parametersettings on LDIs performance.",
  "PRELIMINERIES2.1Definitions": "In our context, the machine learning model under attack denoted as (), processes input data (such as an image) and outputs a confi-dence vector, = (). This vector contains the probabilities thatthe input belongs to each potential class. The label, , correspondsto the class with the highest probability in this vector. The modelis trained using a dataset, , a subset drawn from an unknown\"Latent Domain,\" . Once the model is trained and deployed, usersinteract with it, typically via an API. They dont have any directaccess or knowledge about the original training data, . This lackof information about further complicated the attackers task.Data pool. A data pool may consist of multiple training datasetsfrom public or private domains, = {1, 2, . . . , }, where contains feature vector and label pairs.The attacker may prepare a data pool to perform the attack.Dataset similarity. Our proposed attack extracts data records, ,from the data pool that are closely related to the target training data . The effectiveness of the attack is measured by the similarityof to , i.e., Distance(, ). This similarity is crucial foreffective model-based attacks, such as model-inversion attacks . We use the Optimal Transport Dataset Distance (OTDD) toassess this similarity. OTDD measures dissimilarity between datasetsusing optimal transport distances, providing geometric insight andinterpretable correlations.",
  "ADAPTIVE DOMAIN INFERENCE ATTACK": "The proposed attack aims to directly identify the classes of recordsin a data pool similar to the training data of the target model. Theextracted samples can be used as auxiliary data in model-basedattacks. We first discuss the threat model, then briefly describe therecord-level LiRA domain inference attack (LDI), and finally presentthe motivation and detail of ADI.",
  "Threat Modeling": "Involved parties. The model owner may package the deep learningmodel as a web service and remove all semantic information fromthe input and output. The adversary can be any party who is curiousabout the model and the data used for training the model.Adversarial knowledge. We assume attackers have black-boxaccess to the model with limited knowledge about API input/outputinformation, such as input image size and the number of outputclasses. The model API accepts only encrypted inputs and outputs,preventing interception of domain information. This minimal knowl-edge assumption results in more powerful and practical attacks. Ourattack focuses on image classification models, with the API return-ing a class likelihood confidence vector. However, the attack can beextended to non-image tasks. For label-only outputs, attackers can",
  "Adaptive Domain Inference Attack with Concept HierarchyConference17, July 2017, Washington, DC, USA": "escalates the probability of the originating leaf cluster and its neigh-boring ones, while negative feedback reduces them.To clearly describe the probability adjustment and propagationprocedure, we give the following notations first. We denote the -thnode (from left to right) at the -th level ( 1.. from top to down)of the concept hierarchy as node . As such, each node can beuniquely identified. Let the probability associated with the node atiteration be ( ) . The number of siblings of the node can vary forsimplicity, we denote the siblings of the node as a set and thenumber of siblings as | |. We also assume a sampled record, , isdrawn from the leaf node at timestep . The following updatewill be recursively applied to the leaf node and its ancestors until theroot is reached.Target-node probability update. Let () denote a level-wiseadjustment. If the sample is positive, the adjustment is added tothe probability of node ; otherwise, the probability is decreasedby (). With the previously defined (,) function forsample , we define: (,) = (,)?1 : 1 and",
  "First Candidate LDI": "We wonder whether we can conduct a direct record-level inferencewithout the domain knowledge. Specifically, can we use a member-ship inference attack (without knowing the domain!) to collect thelikely in-domain records and then analyze them to infer the domaininformation? Most MIA attacks assume the known domain and de-pend on it to derive shadow models , except for a most recentdevelopment: the offline version of likelihood ratio attack (LiRA). Next, we explore the application of offline LiRA in domaininference and summarize its problems.Offline LiRA test. Carlini et al. present both an online andan offline LiRA test. The online LiRA test requires attackers totrain multiple shadow models on both in-domain and out-domaindatasets, resulting in extremely high computational costs. In contrast,the offline LiRA test does not use in-domain information and reliessolely on out-domain samples, as they observed in-domain sampleshave much higher likelihood ratios than out-domain samples. Thus,only a one-side hypothesis testing is applied on the larger likelihoodratio side to test whether the sample is out-domain. More details canbe found in the paper . This offline version significantly reducescomputational costs, but with the assumption that the out-domaindistribution roughly works for all target datasets. This method iswell-suited for our domain inference attack, as it does not assumeany target domain is known.Specifically, to test the membership of , we measure the proba-bility of observing confidence as high as the target models underthe null hypothesis that the target point (, ()) is a non-memberas follows:",
  "d hypothesis test represents the difference between the greatestand second greatest confidence scores of the predicted logits. Thefunction () = log1": "1applies logit scaling, and N () denotes aGaussian distribution. A one-sided hypothesis testing is conductedto conclude whether the confidence is high enough to reject the nullhypothesis. In practice, > 0.5 is used as the threshold to determinethe membership of in-domain samples .Offline LiRA domain Inference. We can apply the offline LiRAto a record-based domain inference attack as follows. We start with training shadow models on hypothetical in-domain shadowdatasets, each constructed by randomly picking samples from thedata pool by flipping a coin. For each shadow dataset, we have theremaining samples in the data pool as out-domain samples, whichare used to estimate the parameters and 2.Then, we test the membership of each instance in the data poolfor the target domain. The instance is used as the input to the tar-get model to generate the corresponding () and we apply theoffline LiRA to determine the membership. Once (likely) memberinstances are collected, the attacker can analyze them to derive thedomain information, e.g., based on the top-K most popular classesor clusters.We also tested two sampling methods to minimize the model ac-cess cost: random sampling and class-balanced sampling. In randomsampling, we uniformly randomly select a subset of data recordsfrom the data pool. In class-balanced sampling, we take the samenumber of samples from each class. Its specifically implementedby augmenting the less populated classes, i.e., via random rotationand flipping of existing samples in the class. The effect of differentsampling strategies is shown in .4.1. Detailed steps of LDIare provided in Algorithm 2 in Appendix A.Despite using this refined design, we observed that LDI performsunsatisfactorily due to several reasons. First, the offline LiRA methodassumes all out-domain instances output confidence values () fitin the normal distributions, which may not be precise, as discussedin the original paper . Second, if the target domain instances formonly a small portion of the data pool, either sampling method maywork ideally to extract the likely target domain instances. We showmore details in Appendix B.",
  "Concept Hierarchy for ADI": "Bearing the problems with LDI in mind, we introduce the adaptivedomain inference based on concept hierarchy (ADI). The concepthierarchy, depicted in , is pivotal to our approach, steeringthe attack towards probable classes. We will show that a concept hier-archy can better focus on the relevant concepts and concept branches,which is much more effective than a flat concept organization, usedby LDI. It also makes sense as an application dataset very likelyuses multiple relevant concepts, which can be better captured by thehierarchical structure.Constructed from the data pool as (), the concept hierarchyis expected to be large enough, so that it can intersect or even con-tain the concepts used by the target models potential domain. Thishierarchical structure spans levels with the root at level 1, andeach branch groups similar samples, where naming might not beimportant. For example, an skirt node under clothes may lead tosubclasses hoop skirt and mini skirt nodes, while skirt underbeef represents the images of a specific part of beef. Each nodecarries a local probability, starting at 1/ for siblings. Traversingfrom the root to a leaf and multiplying the probabilities on the path,we can get a leafs global probability. These leaf global probabilitiessum to 1, maintaining the algorithms integrity. As the attack pro-gresses, the algorithm adjusts local probabilities based on the targetmodels feedback, culminating in leaf nodes global probabilitiesthat reflect their presence in the target models training data.",
  "Details of ADI Attack": "Once the attacker has assembled a data pool and formulated a con-cept hierarchy, they use Algorithm 1 to adjust the node-associatedprobabilities. The core of the algorithm is an iterative process, asillustrated in . Each iteration involves drawing sample im-ages from leaf clusters, using a top-down random walk following thenodes local probabilities. The sampling process starts from the rootand randomly selects a child branch based on their local probabilitiesuntil a leaf is reached. Each sample is then fed into the target model () that produces a confidence vector, .The crux is how to use the confidence vector to tune the nodeprobabilities. The first step is to determine whether the node isrelevant enough to the domain, for which we design two methods.(1) LiRA-based method (LiRA-ADI): We use the offline LiRAtest directly to determine the likelihood of membership for the testsample, . If 0.5, we label the sample as a positive sample.However, the LiRA test requires a significant setup cost, i.e., trainingmultiple shadow models. Thus, we design the following alternativemethod. (2) Entropy-based method (Entropy-ADI): The second methodis built on the intuition that if an image is similar to a training dataclass, the confidence probability for that class, , significantlyexceeds others; if the model fails to recognize the image, classprobabilities tend to be similar. We capture this trait by using theconcept of normalized entropy, and employing a predefined entropythreshold to determine if the target model confidently recognizesthe sample. Since the range of possible entropy values is determinedby the number of classes, , i.e., [0, log2 ], the normalized entropyfunction is",
  "=12,(2)": "where = (1, . . . , ) is the confidence vector. The normalizationconverts all entropy values, regardless of the number of classes, tothe range , and allows us to establish a generalized algorithm,independent of the target models class count. We label samples withentropy as positive samples since a target-model-recognizedsample typically has a low-entropy confidence vector, and those withentropy > as negative samples. In experiments, we have observed = 0.83 seems to give the best result.To unify these two methods, we define the (,)function with mode indicating LiRA-ADI or Entropy-ADI, whichtests whether a sample is positive. Consequently, we reward theleaf node from which a positive sample was drawn by increasingthe leafs local probability. The node probability adjustment is thenpropagated to sibling and parent nodes, as detailed in the follow-ing section. Conversely, a negative example results in a decreasedprobability for its originating leaf cluster, and the adjustment isalso propagated to sibling and parent nodes. Through rounds ofthese adjustments, we hope that the leaf probabilities are stabilized,reflecting their relevance to the hidden domain of the target model.",
  "Recursively, the target node is moved up to the parent of node ,and the same target node and sibling probability update procedure isapplied until the root is reached": "3.4.2Adaptive adjustment and Probability Rebalancing.The bottom-up probability adjustment will progressively increasethe probabilities of the concepts (and their parents) relevant to thetarget domain. The updated node probabilities will immediatelyaffect the images sampled in the next iteration in Algorithm 1.The amount of adaptive adjustment is designed to attain optimalconvergence as detailed in .4.3. It also controls the intensityof probability update crossing levels. The setting of is a delicatebalance among convergence quality, convergence speed, and theoverall coverage of relevant concepts. If its too large, the optimalconvergence is difficult to reach, and the algorithm may be biasedtowards a few early visited concepts. If its too small, convergencemight be slow, increasing the attacks cost (i.e., the number of modelAPI accesses). The heuristic we use, which has proven successfulin experiments, ensures that () decays linearly towards the level: the higher the level (the smaller ), the less the adjustment ispropagated. Moreover, the more nodes the hierarchy has, denotedas ||, the less dramatic the adjustment should be to increase thecoverage of concepts. We have experimented with () = /|| withdifferent scaling factors (.4) and shown that () = /|| isappropriate.However, merely using the above heuristic is not enough. Weobserved that aggregation over rounds of adjustment may severely",
  "unbalance node probability distributions. To address this, we dis-count the probability ( )surpasses the sum of its siblings probabil-": "ities: ( ) . Specifically, we subtract the mean of the excessvalue and redistribute it proportionally to the other sibling nodes.This update maintains the sum of probabilities under each nodeequal to 1 while ensuring different nodes probabilities are adjustedproportionally.The rebalancing strategy is applied as follows if ( )> ( ) .",
  "Leveraging the methods as mentioned earlier, we give the adjustmentalgorithm, Adj_ Probs(, , , ). Due to the page limit, we showthe detailed algorithm in Appendix A": "3.4.3Convergence Condition. Once the relevant concepts getboosted probabilities, the late iterations will more likely fetch thesamples relevant to target domains. This positive feedback continuesuntil convergence. A critical question is when we should stop theiterations. With the rebalancing strategy, the relevant nodes proba-bility eventually converges to ( ) ( ) . However, its notsufficient to serve as a global convergence condition.We design a global strategy as follows. The intuition is that if themajority of the sampled batch fits the target model () well, i.e.,for most of them the normalized entropy ( ()) in Entropy-ADI, or the probability ( ()) 0.5 in LiRA-ADI, we considerthe concept probability adjustment converges. Thus, the convergefunction is a Boolean function defined as follows.",
  "() 0.5?1 : 0(6)": "In the entropy-based method, we have carefully evaluated thesetting of in experiments and found that = 0.83 works best withdifferent experimental datasets.Its important to note that the convergence will fail when theconcept hierarchy is not large enough to cover any class of the targetdomain. As observed in experiments, most convergences happenaround 50 iterations. We may use a relaxed upper bound, e.g., 100iterations, to determine whether convergence is not possible, i.e., theconcept hierarchy may not contain any class of the target domain.",
  "EXPERIMENTS": "ADI aim to address the limitations of the GAN-based GDI method: its difficult to identify candidate datasets at the dataset levelsimilar to the target training data, and excessive target model ac-cesses are also disadvantageous in private model application sce-narios, e.g., the attack is more likely to be detected. Our experi-ments demonstrate that ADI can effectively mitigate these limita-tions. Specifically, the experiments will achieve the following goals.",
  "Setup": "Concept hierarchies and datasets. We tested three methods forcreating concept hierarchies. (1) We used a dataset-based hierar-chy and synthesized domain datasets for parameter tuning. Thethree-layer hierarchy includes MNIST , EMNIST , LFW, CIFAR10, CIFAR100 , CINIC-10 , FASHION-MNIST, and CLOTHING . Each dataset is an intermediate node,with classes as leaf concepts. Images are scaled to 36x36x3. Formixed-domain training, we randomly selected 10, 20, and 30 classesfrom the 8 datasets to build synthesized training sets. Unselecteddata formed the adversaries data pool and hierarchy.(2) We also experimented with a larger hierarchy from ImageNet-1k and three ImageNet-related datasets: ImageNETTE , Image-Woof , and DeepFashion . The hierarchy includes 1000leaf nodes (ImageNet-1k classes) and up to 10 intermediate layers2.DeepFashion uses 10 classes from the \"Clothing\" branch of Ima-geNet. ImageNETTE includes 10 easily classified ImageNet classes,and ImageWoof includes dog breeds from ImageNet. These exam-ples demonstrate the functionality of a larger hierarchy. For moredetails, see Appendix E.Target models. We used ResNet-18 for the three synthesizeddatasets, training with an 8:2 split, repeated 10 times to observevariance. The training used SGD with a learning rate of 102, batchsize of 100, momentum of 0.9, and learning rate decay of 104.For the three high-resolution ImageNet-related datasets, we usedResNet-152, with the same split, repetitions, and hyperparametersas the ResNet-18 models. All models were trained using the FastForward Computer Vision (FFCV) pipeline for efficiency .Shadow models. Shadow models are crucial for the hypothesis-based membership inference attack, and thus vital to both LDI andADI. Since class numbers vary, we separate the data pool by targetdomain classes and train shadow models accordingly. For a -classtarget domain, we divide the attack pool into multiple -class sub-domains and randomly sample sub-domains to train shadowmodels per sub-domain. We set = 128 for synthesized datasetsand = 256 for ImageNet-related datasets, as suggested in . Fordetails on shadow models impacts, see Appendix C.",
  "Evaluation Metrics": "Dataset similarity. As mentioned in , we employ theOptimal Transport Dataset Distance (OTDD) to quantify dataset sim-ilarity between the original target dataset and the dataset extracted bythe attack. Lower OTDD values indicate greater similarity betweenthe estimated dataset and the original domain.Performance of model inversion attack. The GDI, LDI, andADI estimated data will be used as the auxiliary datasets to the",
  "The ImageNet-1K hierarchy is available at our code link": "model-inversion attack if the estimated data is similar to the orig-inal domain, the accuracy of the model-inversion attack will besignificantly boosted. We use the original model-inversion attackdesigned by Fredrikson et al. , the effectiveness of which isevaluated by the quality of its reconstructed training dataset. We usethe target model to recognize (i.e., classify) the reconstructed images the higher the classification accuracy, the better the reconstructeddata and thus the better the estimated data.",
  "Results on ImageNet-Related Datasets": "We show the methods attacking performance on the three ImageNet-related datasets. Due to page limitations, we include the results onsynthesized datasets in Appendix D. We use the parameters thatgenerate the best-estimated datasets with the smallest OTDD scoresfor the attacking methods. For LDI, we select 100% of instances frombalanced classes. For ADI, we use the optimal parameter settings for and (). We will discuss the parameter settings in later sections.Quality of estimated domain. To observe GDIs performance,we split ImageNet-1K into 100 datasets with 10 similar classeseach to build the landmark datasets, such as 10 dog breeds, 10types of clothing, etc. The results in a show that LiRA-ADIperforms the best with the smallest OTDD score, and Entropy-ADIalso outperforms LDI and GDI.Enhancing model inversion attacks. Model inversion attacksrequire knowledge of the target domain to implement or enhancethe attack. We are also interested in how the estimated domain canenhance model inversion attacks. We recover 1000 images for eachclass of the target model using model inversion attacks and testthe target models performance on these recovered datasets. In thebaseline scenario, we initialize the generated images randomly. Inother experiments, we randomly pick images from the estimateddomain as the initial images. b shows the enhancement ofmodel inversion attacks. Both ADI methods outperform other attackswith LDI ranked next.Model Accesses. One motivation for designing our attacks is toreduce the substantial number of accesses required by GDI. c illustrates the number of accesses needed for each attack to reachconvergence. We set the batch size for drawing instances and inter-acting with the target model is 1000. As shown in the results, ADIrequires significantly fewer accesses to implement compared to GDIand LDI. The large number of accesses needed by GDI is due tothe necessity of training a generative model for each dataset, whichinvolves multiple interactions with the target model. LDIs modelaccess is critically related to the result quality. We show in the nextsection that LDI needs to test almost all samples in the data pool toget the best-performing results, which can be expensive.",
  "In this section, we include more details for determining the optimalparameter settings for LDI and ADI methods": "4.4.1Effect of Sampling Methods for LDI. To understand theeffect of different sampling methods on the LDI results, we pro-gressively increase the number of tested samples. For simplicity, weaugmented small classes to make the class sizes even so that wecan use percentages to represent the sampling progress for both uni-form sampling and class-balanced sampling. The results are shown",
  "(d) Single datasets(random)": ": OTDD constantly decreases with the increasingsize of tested samples in LDI. Dataset Names: M-MNIST, E-EMNIST, L-LFW, C-CIFAR, CN-CINIC, FM-FashionMNIST,CL-CLOTHING, NETTE-ImageNETTE, Woof-ImageWoof,Fashion-DeepFashion. in . We conducted experiments on two types of target do-mains (and target models): a synthesized target domain consistingof randomly selected classes in the dataset pool, denoted by Mix-10,Mix-20, and Mix-30; uniform samples from each dataset, denotedby the corresponding dataset. The two sampling methods are used togenerate the test samples, aiming to reduce the cost of LDI, denotedby random and balanced. However, we found that the LDIsperformance is almost linearly related to the number of test samples.It does not reach the peak performance until all samples are tested.This pattern keeps across all datasets and sampling methods, which",
  "raises the concern that to achieve the best LDI performance, thenumber of model accesses will be significantly high. However, evenwith such a high cost, LDI still performs 20% worse than ADI": "4.4.2ADI Parameter Settings. The ADI algorithm containsseveral parameters to be experimentally explored and determined,including the threshold of the normalized entropy, in the entropy-based method, and the layer-wise adaptive probability adjustment in both entropy and LiRA-based method. We also want to observethe effect of probability rebalancing and assess the benefits of usingconcept hierarchies against a flat concept list. settings. The layer-wise probability adjustment plays a crucialrole in the speed, quality of convergence, and attack efficiency. Wefound that () = /|| for nodes at Level and the total numberof concepts || works reasonably well according to the heuristicmentioned in .4.1(a). We have also tested variantsof settings, i.e., () = 10",
  "|| , weobserved significant oscillations at higher OTDD levels, indicatingthe attack does not converge well (b). Conversely, reducing() to": "10|| resulted in a substantial slowdown in convergence,increasing the attack cost (c).Effect of rebalancing. Without probability rebalancing, ADI maylead to a biased concept distribution, i.e., focusing on increasingthe probabilities of a few branches of the concept hierarchy due totheir already high probabilities. Other methods, such as fine-tuningthe values, may help address this issue. However, we find therebalancing method works extremely well. Experiments show aremarkable improvement in ADI performance with rebalancing, asshown in . Without rebalancing, the ADI gives unstableresults. Due to the biased adjustment towards certain branches, theresults are often stuck at suboptimal levels.Effect of Concept Hierarchy. We wonder how the multi-layerhierarchical structure may affect the ADI algorithm, compared toa flat list of concepts. We construct the flat structure by removingall the internal nodes of the concept tree, i.e., the root node pointingto all leaves directly, so that the ADI algorithm still works withoutmodification. The algorithm still converges, but with much worsequality as shown in . Without the hierarchically organizedconcepts, the ADI estimated domains yield larger OTDD values,",
  ": OTDD values over epochs for ADI with flattened con-cept hierarchy": "settings in entropy-based ADI. The setting serves as thethreshold, indicating (1) the target model predicts the extractedsample with high confidence, i.e., a positive sample, and (2) theoverall quality of the batch of extracted samples, i.e., the convergencecondition. Its setting is critical for the entropy-based ADI attack toconverge quickly toward a high-quality result. We have conducted aset of experiments to investigate the optimal setting. showsvariable settings of and = 0.83 gives the best-extracted datasets",
  "RELATED WORK": "Machine learning models in fields like intrusion detection and health-care are increasingly exposed to cyber threats through API services. Significant threats include model inversion andinference attacks, which compromise training data privacy. Modelinversion attacks aim to recreate training examples using auxiliarydata, with recent GAN-based methods achieving high-quality results. Membership inference attacks identify whether specific datawas used in training , while property inference attacksreveal population-level attributes . Both strategies rely onunderstanding the target models training data distribution.The Fidel attack on federated learning exploits neuron data toinfer previous activations, posing a security risk that depends onhaving an auxiliary dataset resembling the victims training data.While the assumption of an attacker knowing the auxiliary data ordomain distribution may not hold for a breached model API serviceor federated learning, it is worth considering if a trained modelinherently contains domain information. A recent GAN-based model-domain inference (GDI) attack aims to address this . However,GDI does not work satisfactorily when only a few parts of candidatedatasets are relevant to the target dataset and requires excessivemodel accesses. Our proposed ADI attack directly addresses thesetwo problems with the GDI attack.",
  "CONCLUSION": "Deep neural network models can be exploited by model-targetedattacks, e.g., model inversion and membership inference attacks.However, attackers cannot apply these attacks effectively withoutknowing the models domain information. Thus, a possible protec-tion approach is to package a sensitive model as an unnamed functioncall/API service, hiding the models domain knowledge from attack-ers. However, we show that the Adaptive Domain Inference (ADI)attack can still identify the classes of samples relevant to the targetmodels domain by only accessing the unnamed model. We showthat the concept hierarchy extracted from a large data pool playsa significant role in ADIs iterative procedure. ADI surpasses thecompared domain inference attacks: GDI and LDI by a large marginin both the quality of extracted datasets and the lower number ofmodel accesses.",
  "David Alvarez-Melis and Nicolo Fusi. 2020. Geometric dataset distances viaoptimal transport. Advances in Neural Information Processing Systems 33 (2020),2142821439": "Bharat Bhushan, Ganapati Sahoo, and Amit Kumar Rai. 2017. Man-in-the-middleattack in wireless and computer networkingA review. In 2017 3rd Interna-tional Conference on Advances in Computing, Communication & Automation(ICACCA)(Fall). IEEE, 16. Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, andFlorian Tramer. 2022. Membership inference attacks from first principles. In 2022IEEE Symposium on Security and Privacy (SP). IEEE, 18971914.",
  "Li Deng. 2012. The mnist database of handwritten digit images for machinelearning research. IEEE Signal Processing Magazine 29, 6 (2012), 141142": "David Enthoven and Zaid Al-Ars. 2022. Fidel: Reconstructing private trainingsamples from weight updates in federated learning. In 2022 9th InternationalConference on Internet of Things: Systems, Management and Security (IOTSMS).IEEE, 18. Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model inversionattacks that exploit confidence information and basic countermeasures. In Pro-ceedings of the 22nd ACM SIGSAC conference on computer and communicationssecurity. 13221333. Karan Ganju, Qi Wang, Wei Yang, Carl A Gunter, and Nikita Borisov. 2018.Property inference attacks on fully connected neural networks using permutationinvariant representations. In Proceedings of the 2018 ACM SIGSAC conference oncomputer and communications security. 619633.",
  "Yuechun Gu and Keke Chen. 2023. GAN-based domain inference attack. InProceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 1421414222": "Brij B Gupta, Aakanksha Tewari, Ankit Kumar Jain, and Dharma P Agrawal. 2017.Fighting against phishing attacks: state of the art and future challenges. NeuralComputing and Applications 28 (2017), 36293654. Jamie Hayes, Luca Melis, George Danezis, and Emiliano De Cristofaro. [n.d.].LOGAN: Membership Inference Attacks Against Generative Models. Proceedingson Privacy Enhancing Technologies 2019, 1 ([n. d.]), 133152.",
  "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep resid-ual learning for image recognition. In Proceedings of the IEEE conference oncomputer vision and pattern recognition. 770778": "Jeremy Howard. [n.d.]. ImageNETTE. Hongsheng Hu, Zoran Salcic, Lichao Sun, Gillian Dobbie, Philip S Yu, and XuyunZhang. 2022. Membership inference attacks on machine learning: A survey. ACMComputing Surveys (CSUR) 54, 11s (2022), 137. Gary B Huang, Marwan Mattar, Tamara Berg, and Eric Learned-Miller. 2008.Labeled faces in the wild: A database forstudying face recognition in unconstrainedenvironments. In Workshop on faces inReal-LifeImages: detection, alignment,and recognition. Bo Hui, Yuchen Yang, Haolin Yuan, Philippe Burlina, Neil Zhenqiang Gong, andYinzhi Cao. 2021. Practical blind membership inference attack via differentialcomparisons. arXiv preprint arXiv:2101.01341 (2021).",
  "A Krizhevsky. 2009. Learning Multiple Layers of Features from Tiny Images.Masters thesis, University of Tront (2009)": "Guillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman,and Aleksander M adry. 2023. FFCV: Accelerating training by removing databottlenecks. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition. 1201112020. Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. 2016. Deepfash-ion: Powering robust clothes recognition and retrieval with rich annotations. InProceedings of the IEEE conference on computer vision and pattern recognition.10961104.",
  "Tomoya Matsumoto, Takayuki Miura, and Naoto Yanai. 2023. Membershipinference attacks against diffusion models. In 2023 IEEE Security and PrivacyWorkshops (SPW). IEEE, 7783": "Bhargav Pingle, Aakif Mairaj, and Ahmad Y Javaid. 2018. Real-world man-in-the-middle (MITM) attack implementation using open source tools for instructionaluse. In 2018 IEEE International Conference on Electro/Information Technology(EIT). IEEE, 01920197. Santiago Romero-Brufau, Daniel Whitford, Matthew G Johnson, Joel Hickman,Bruce W Morlan, Terry Therneau, James Naessens, and Jeanne M Huddleston.2021. Using machine learning to improve the accuracy of patient deterioration pre-dictions: Mayo Clinic Early Warning Score (MC-EWS). Journal of the AmericanMedical Informatics Association 28, 6 (2021), 12071215. Khader Shameer, Kipp W Johnson, Alexandre Yahi, Riccardo Miotto, LI Li, DoranRicks, Jebakumar Jebakaran, Patricia Kovatch, Partho P Sengupta, SenguptaGelijns, et al. 2017. Predictive modeling of hospital readmission rates usingelectronic medical record-wide machine learning: a case-study using Mount Sinaiheart failure cohort. In Pacific Symposium on Biocomputing 2017. World Scientific,276287.",
  "Wanrong Zhang, Shruti Tople, and Olga Ohrimenko. 2021. Leakage of datasetproperties in {Multi-Party} machine learning. In 30th USENIX security sympo-sium (USENIX Security 21). 26872704": "Yuheng Zhang, Ruoxi Jia, Hengzhi Pei, Wenxiao Wang, Bo Li, and Dawn Song.2020. The secret revealer: Generative model-inversion attacks against deep neuralnetworks. In Proceedings of the IEEE/CVF conference on computer vision andpattern recognition. 253261. Yuheng Zhang, Ruoxi Jia, Hengzhi Pei, Wenxiao Wang, Bo Li, and Dawn Song.2020. The secret revealer: Generative model-inversion attacks against deep neuralnetworks. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition. 253261.",
  "BIAS IN LDI": "Assuming the false negative rate (FNR) of membership inferenceattacks in LDI is FNR and the true positive rate (TPR) is TPR, andthe sizes of the target domain, non-target domains in the data pool,and the estimated domain by the LDI are target, non-target, andestimated, respectively, the size of the estimated domain is given by:",
  "TPR target + FNR non-target": "If non-target target, which is likely in the real world, theattack success rate will be significantly decreased by the FNR ofmembership inference attacks. To validate this phenomenon, we setup a simple experiment where the target model is trained on halfof one of CIFAR-10, MNIST, and EMNIST. The data pool consistsof the other half of the target domain and the other two datasets.Since CIFAR-10 and MNIST have 50k images and EMNIST has640k images, this setup will simulate the bias of the data pool. Forcomparison, we also repeat the experiment on the three datasetsbut with a reduced EMNIST, i.e., randomly picking 10 classes ofEMNIST. We use both LDI and LiRA-based-ADI to estimate thetarget domain.In the case where the model is not trained on EMNIST andnon-target = MNIST + EMNIST target = CIFAR, the successrate will be significantly reduced by the EMNIST dataset.",
  ": Success rate of target domains shows that the bias ofthe data pool significantly impacts the dataset": "Experimental results support our theoretical analysis. As shownin a, if we do not reduce the size of EMNIST, the successrates of LDI for CIFAR-10 and MNIST are significantly decreased,while the success rate for EMNIST is much higher. In contrast,ADI demonstrates resistance to the bias in the data pool. bpresents the results of both attacks with the size of EMNIST reducedto be similar to that of CIFAR-10 and MNIST. The success ratesof HDI across the three datasets become comparable, as opposedto the results in a. This confirms that LDI suffers from",
  "IMPACT OF SHADOW MODELS": "Both LDI and ADI are designed based on hypothesis-based member-ship inference attacks. As introduced in .1, these member-ship inference attacks require adversaries to train multiple shadowmodels on the target domain. Since adversaries are assumed to beunaware of the target domain in the domain inference attack game,we train the shadow models on the entire data pool. One might won-der about the performance of ADI and LDI if the shadow modelsare trained on sub-domains, such as only CIFAR datasets or MNIST,etc. As reported in , membership inference attacks still performwell even when the shadow model is not trained on the target do-mains. Therefore, we hypothesize that ADI and LDI will also showmeaningful results if the shadow models are trained on sub-domains.To validate this hypothesis, we trained the shadow models on onlyCIFAR-10, MNIST, and Fashion-MNIST. The target domain is amixed domain composed of random 10 classes from the synthesizeddatasets. shows that training shadow models using theentire data pool is a proper choice. Training the shadow modelon a specific sub-domain decreases the attack performance andincreases the performance uncertainty. This uncertainty is caused bythe randomness of the target domains. For example, if the Mix-10target domain mostly consists of CIFAR-like classes, the attack willperform poorly if the shadow models are trained on MNIST-likedomains.",
  "STRAIGHTFORWARD RESULTSON IMAGENET CONCEPT HIERARCHY": "We show the straightforward attacking results of LiRA-based-ADIon the model trained on the DeepFashion. Specifically, we conductthe attack for 10 times and check the branches with highest probabil-ity and treat this branch as the estimated domain of the model. a shows the results with the concept hierarchy, illustrating that theattacker can confidently deduce the target domain as clothing-relateditems. Conversely, b indicates the results without concepthierarchy. It shows less accurate estimation, evidenced by the pres-ence of non-clothing items such as restaurant and mask, whichintroduce ambiguity.",
  ": The quality of identified concepts": "To enhance our comprehension of the concept hierarchy, wepresent visual representations in Figures 13 and 14, illustratingthe concept hierarchy within ImageNet-1K. displays thecomprehensive hierarchy for all 1000 classes of ImageNet-1K, while focuses on the hierarchy within a specific \"covering\" node.Additionally, depicts the concept hierarchy for mixeddatasets, which were utilized in our experiments. For simplicity, theleaf nodes in these hierarchies are denoted by numbers. Attackerscan deduce the concepts of these nodes through an abstraction of theimage content present in the leaf nodes."
}