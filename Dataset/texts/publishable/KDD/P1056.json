{
  "ABSTRACT": "Evaluating anomaly detection algorithms in time series data is crit-ical as inaccuracies can lead to flawed decision-making in variousdomains where real-time analytics and data-driven strategies areessential. Traditional performance metrics assume iid data and failto capture the complex temporal dynamics and specific characteris-tics of time series anomalies, such as early and delayed detections.We introduce Proximity-Aware Time series anomaly Evaluation(PATE), a novel evaluation metric that incorporates the temporalrelationship between prediction and anomaly intervals. PATE usesproximity-based weighting considering buffer zones around anom-aly intervals, enabling a more detailed and informed assessmentof a detection. Using these weights, PATE computes a weightedversion of the area under the Precision and Recall curve. Our exper-iments with synthetic and real-world datasets show the superiorityof PATE in providing more sensible and accurate evaluations thanother evaluation metrics. We also tested several state-of-the-artanomaly detectors across various benchmark datasets using thePATE evaluation scheme. The results show that a common metriclike Point-Adjusted F1 Score fails to characterize the detection per-formances well, and that PATE is able to provide a more fair modelcomparison. By introducing PATE, we redefine the understandingof model efficacy that steers future studies toward developing moreeffective and accurate detection models.Public source code:",
  "INTRODUCTION": "Anomaly detection in time series (TS) data, the process of identi-fying unusual patterns that deviate from the expected norm, hasbecome increasingly important across various domains . Therapid advancement of data-driven decision-making and real-timeanalytics has opened opportunities for developing more accurateanomaly detection methods. Such developments often lead to mod-els competing to claim the status of State-of-the-Art (SOTA).Achieving this status is not just a matter of academic prestige;it often directs the focus of future research, influences industryadoption, and guides the development of practical applications.However, choosing an appropriate evaluation metric is critical toavoid incorrect conclusions about a models performance. Relyingon evaluation metrics that do not accurately reflect the true effec-tiveness of the models can lead to flawed decisions in real-worldapplications. This is particularly consequential in critical domains,",
  "p32": ": Illustration of anomaly detection in time series data.13 represent the actual anomalies as ground truth. Pre-dictions are denoted by . The durations of both events areindicated by the length of the boxes. Overlapping areas be-tween and demonstrate where the model has correctlyidentified anomalies. such as medical diagnostics or financial fraud detection, where re-lying on a poorly evaluated model can have serious repercussions.Standard evaluation metrics such as Precision and Recall areeffective for point-based anomaly detection as they assess the accu-racy of detecting isolated iid events. In this context, each data pointis evaluated independently, allowing for straightforward calcula-tion of these metrics. However, in TS data, events and anomaliestypically occur in time intervals. This complexity causes several sit-uations: 1) Early Detection, when potential anomalies are identifiedbefore they fully manifest, based on subtle changes in the data pat-tern over time. shows an example of early detection whereprediction 11 detects the anomaly event 1 earlier than its actualoccurrence. Although 11 does not align exactly with 1, such earlydetection is valuable for early response actions and should be ap-propriately appreciated in evaluation metrics. 2) Delayed Detection,occurs when an anomaly event is not detected immediately but isidentified at a later time, even after its actual occurrence. In , the anomaly event 1 is detected with a delay by prediction event12. Although 12 does not align precisely with 1, this type of de-layed detection should be accounted for in the evaluation process,as it reflects the models ability to eventually identify anomalies,even after some delay.Another situation, 3) Onset Response Time, refers to how closethe detection of an anomaly is to the start of the event. Timely de-tection is valuable, especially in scenarios where immediate actionis required. In , anomaly event 2 is detected by 21 and22. However, 21 aligns more closely with the beginning of theanomaly event 2, indicating a faster response than 22. Evaluationmetrics should reward those that occur promptly after the onsetof an anomaly. Finally 4) Coverage level of Predictions, refers to therange that a prediction covers an actual anomaly. The effectivenessof a prediction can be measured by how much of the anomaly itsuccessfully captures. In , predictions 31 and 32 both de-tect anomaly event 3, but 31 covers 3 more than 32. This moreextensive coverage by 31 makes it a more effective prediction for3. Accordingly, evaluation metrics need to consider the coveragerange of the predictions over the duration of the anomalies.",
  "Ramin Ghorbani, Marcel J.T. Reinders, and David M.J. Tax": "Benchmark Dataset Experiments: We validated PATE onall standard benchmark datasets used in this study. As shown in, PATEs computation times are comparable to those ofthe AUC-PR metric and significantly faster than the VUS metric,remaining under one second for smaller datasets and under twoseconds for larger ones. Note that further speed enhancementscould be achieved on HPC systems or with parallel processing.",
  "Precision/Recall (F1 Score)------R-based-----TS-Aware/ETS-Aware----Affiliation----PA-F1-----AUC-ROC/PR-----VUS-ROC/PR---PATE": "Our approach integrates buffer zones around the anomaly eventsand utilizes a special proximity-based weighting mechanism, en-abling a detailed assessment of both early/delayed detections andaddressing the onset response time challenge. PATE avoids thesubjectivity of threshold-dependent metrics by integrating overthe range of thresholds, offering a fair and unbiased evaluation,especially in research settings where expert knowledge might notbe available for setting the exact desirable parameters based onthe application. illustrates a comparison between existingmetrics and PATE, highlighting the comprehensive adaptabilityreconsideration of PATE in evaluating the TS anomaly detection.",
  "PROPOSED EVALUATION METRIC - PATE": "A time series is denoted as a sequence of observations = { }=1,where represents the length of the time series, and each is theobserved data point at time .An actual anomaly event (labeled as positive in the ground truthlabels) is a subsegment within the time series, denoted as =(,) for points and with 1 . The set of allanomaly events in the time series is represented as = {}=1,where is the number of anomaly events present in the time series.In practice, the detection models output continuous anomalyscores, denoted as = { }=1, representing the likelihood of eachobservation to be anomalous. These scores are then convertedinto binary predictions by applying a threshold , where scoresequal to or exceeding the threshold are classified as anomalies.We define a prediction event as a subsegment identified by thesebinary predictions to be anomalous, denoted as () = (, ) forpoints and with 1 . The set of all predictionevents is represented as = ()=1, where is the number ofprediction events identified by the model.The effectiveness of the anomaly detection model is determinedby how well these () events align with the events. PATEdistinguishes several categories of matches between ground truthand predictions based on their temporal relationships and assignsproximity-specific weights to each point in each category. Theseweights are then used to compute a weighted version of Precisionand Recall scores. The final measure of PATE is a weighted AUC-PR,which is derived from these weighted Precision and Recall scores.Further details on these computations are provided in the followingsections.",
  "n1i2n2i3n3i4n4": ": Illustration of the Categorization and Weighting Mechanism in the PATE Method. Prediction events (1 7) arerepresented by orange boxes, while anomaly events (1 4) are depicted by blue boxes. TP weights are illustrated with a blueline, FP weights with a red line, and FN weights with a purple line. Note that the solid segments of the lines, incontrast to the dotted segments, indicate the activated weights for the example scenario depicted in the figure.",
  "Categorizing the Events": "illustrates the different categories of anomaly and pre-diction events in relation to each other. In assessing each (),we consider its overlap, proximity, or distance (temporal relation)from each . This approach allows for the clear differentiationof the diverse scenarios: complete and partial detection of anom-alies, early or delayed detection, and instances where anomaliesare either partially or entirely missed. Specifically, we categorizethe anomaly and prediction events as follows:",
  "True-Detection: Sub-segments of the prediction event ()that overlap with an anomaly event , indicating anomalies thatare accurately identified and not missed. Examples are segments1, 5, and 62 in": "Post-Buffer Detection: Sub-segments of the prediction event () that fall into a buffer zone immediately following an anomalyevent (See segments 2 and 63 in ). This categoryhighlights the capacity of the model for delayed detection. Thepost-buffer zone size, denoted by , can be adjusted by expertsbased on specific application needs. When is unknown for a spe-cific application, we can consider a range of values for ratherthan a fixed one = {0, 1, . . . ,max}. This approach allows fora comprehensive assessment of the models performance acrossdifferent scenarios, as each buffer size can provide a different per-spective on the performance of the model. Details on how thesebuffer sizes contribute to the overall PATE score will be discussedin the following sections.",
  "Pre-Buffer Detection: Sub-segments of the prediction event () that fall into a zone that precedes the start of an anomalyevent . This category highlights the capacity of the model for": "early detection, signaling potential anomalies ahead of time. Similarto the post-buffer zone, the size of the pre-buffer zone, denoted by, varies within the set = {0, 1, . . . ,max} with the same approachfor the assessment. The assignment of points to this category is con-ditional on not overlapping with the Post-Buffer zone of a precedinganomaly 1, ensuring that the model early warning is distinctfrom a delayed detection of the previous event. In other words, thePost-Buffer category has priority, and therefore, if < 1 +then the Pre-Buffer zone starts at 1 + + 1 instead of .Furthermore, Pre-Buffer detection is dependent on the successfuldetection of the subsequent anomaly event . In situations whereno part of the subsequent event is detected by a True-Detection,this Pre-Buffer detection is considered a false alarm rather thana meaningful early detection. Consequently, this early prediction () is reclassified as False Positive (the Outside category, which isdiscussed below). Further details are given in Appendix C. In , 4 and 61 are the examples of pre-buffer detection category,whereas 7 is not considered in this category. Outside: Sub-segments of the prediction event () locatedoutside the ranges of anomaly event and its buffer zones. Theseare instances where the model incorrectly flags normal behavior asanomalous (False Positive), like segments 3 and 7 in .",
  "Weighting Process": "After each individual time point is assigned to its category, wedefine weights for each of these points to determine their con-tribution to the True Positive (TP), False Positive (FP), and FalseNegative (FN) metrics of the detector. It is important to note thattime points at which no anomaly is present and no prediction ismade, True Negatives (TN), do not actively contribute to the perfor-mance metrics and are, therefore, implicitly assigned a weight ofzero, reflecting their non-contribution. The bottom half of visually represents the variations in weights across all differentcategories. True-Detection Weights: Each point from the True-Detectioncategory, lying within the range of an anomaly event , isconsidered correctly identified. Thus, such points are assigned themaximum weight of 1 as True Positives:",
  "TP( ) = 1for ( )(1)": "Post-Buffer Detection Weights: Each point from the post-buffer category, in the range of (,+], is evaluated in relation tothe anomaly event . These points, while not being true positivesin the traditional sense, receive a weight based on their proximityto the , which captures the diminishing influence of an anomalyover time as the distance from the anomaly event increases.",
  "= |( + ) |for Post-Buffer ( )(2)": "Here, the numerator calculates the distance of from each pointwithin the anomaly event, and the denominator normalizes thisagainst the total potential spread within the buffer zone. With thismethod, we account for the proximity to the entire anomaly, not justits endpoint. Thus, we address the delayed detection by recognizingthat any point within the actual anomaly range might influencepredictions in the buffer zone, not just the most immediate or finalpoints of the anomaly. This also implies that the lengths of theanomalies influence the weights. For smaller anomalies, pointsin the Post-Buffer zone are closer to the anomaly onset, and willtherefore be assigned with higher true positive weights. Furtherdetails, regarding the impact of anomaly length on the weights, aregiven in Appendix B.In the Post-Buffer zone, as the distance from increases, thelikelihood of a detection being a False Positive rises. Thus, theweights assigned to false positives in this zone are calculated asthe complement of the TPs weights, acknowledging the reducedsignificance of detections further from the actual anomaly. visually shows the variations in TP and FP weights across thePost-Buffer categories (2 and 6(3)).",
  "= | ( ) |for Pre-Buffer ( )(5)": "Here, the numerator represents the distance of from everypoint in , capturing how early occurs relative to the anomaly.The denominator provides normalization against the total potentialspread within the pre-buffer zone. This mechanism recognizes thatany point within the anomaly event might have an influence onthe zone.Similar to the Post-Buffer zone, the likelihood of a point being aFalse Positive increases as the distance from the increases. Thus,the weights assigned to FPs are calculated as the complement ofthe TPs weights, reflecting the reduced relevance of prematuredetections. shows the variations in weights of the Pre-Buffer categories (4 and 6(1)).",
  "FP() = 1 TP()for Pre-Buffer ()(6)": "Total Missed Anomalies Weights: When the entire range of is undetected, each within its interval receives a maximum FalseNegative weight of 1. This assignment underscores the completefailure of the model in detecting the anomaly event. The variationsin FN weight across 4 as a total missed event are shown in .",
  "FN() = 1for Total-Missed (7)": "Partial Missed Anomaly Weights: When is only partiallydetected, the undetected points within , are evaluated basedon their proximity to the start of the anomaly event. The closerthe points are to the anomaly onset the higher the FN weight,emphasizing the onset response time in detection. Here for Partial Missed , we have:",
  "PATE: Proximity-Aware Time series anomaly Evaluation": "a versatile and applicable measure across a broader spectrum ofanomaly detection approaches and contexts.In conclusion, PATE represents a significant advancement in theevaluation of time series anomaly detection methods which hasthe potential to guide future research, influence industry adoption,and enhance the development of practical applications in criticaldomains such as healthcare and finance.",
  "PATE Final Score": "The PATE final metric is designed to comprehensively evaluateanomaly detection by considering a full range of combinations ofpre-buffer () and post-buffer () sizes. For each combination of and , we apply a range of thresholds () to convert the con-tinuous anomaly scores () into binary predictions, capturing themodels performance across different sensitivity levels. Based onthese binary predictions, we identify the prediction events andthen categorize all prediction and anomaly events. Based on thiscategorization, we assign appropriate weights to each observation.We calculate weighted Precision and Recall across all thresholdsin the considered range for each specific combination of and .Using these calculations, we construct the Precision-Recall curvefor each combination and compute the area under the curve (AUC-PR). Note that the weights TP(), FP(), and FN() are assignedbased on the categorization of each time point. For time points thatdo not fall into any specific category, the weights are consideredto be 0. Thus, the summation in the formulas for Precision andRecall effectively includes only those time points that have beencategorized.",
  "EXPERIMENTS AND RESULTS3.1Synthetic Data Experiments": "To highlight the merits of PATE, we first compare PATE with al-ternative evaluation metrics on a synthetic time series with a bi-nary anomaly detector. The alternative measures can be threshold-dependent or independent. Threshold-independent metrics are in-herently evaluated across a range of possible thresholds. For thisexample, we consider thresholds = {0, 1} to distinguish between normal and anomalous predictions. For threshold-dependent met-rics, we define the optimal threshold as = 1, identifying pointspredicted as 1 (anomalous) for evaluation. shows anomaly 1 with its pre and post-buffer zones.Below, ten different detection scenarios are shown, 1, . . . ,10. Re-sults in demonstrate that PATE effectively distinguishes thescenarios based on temporal proximity, duration, coverage level,and response timing. For instance, although 1 is temporally closeto the anomaly event, it fails to detect any part of it. In the contextof time series, where past data is crucial for prediction, the inabilityto detect any part of the anomaly after it starts suggests that the",
  "Real-World Data Experiments": "To validate the practicality and effectiveness of PATE in real-worldapplications, we extracted some examples from the publicly avail-able and widely used datasets, UCR-KDD21 and MIT-BIH Ar-rhythmia (MBA) ECG . The goal is to evaluate how well PATE,alongside other evaluation metrics, distinguishes between variousdetection models. To ensure a fair comparison, we compare PATEwith threshold-independent evaluation metrics, guaranteeing anunbiased comparison of metrics performances.We analyzed the anomaly scores generated by 1) a Perfect Model,which serves as the benchmark by perfectly identifying anomalies;2) established models like MultiVariate Normal distribution (MVN), Autoencoder (AE), and Local Outlier Factor (LOF); 3) abaseline Random Score that assigns scores uniformly at randomfrom a distribution. This selection covers a spectrum fromtheoretically ideal to practically random, offering a comprehensiveview of the metrics potential evaluation range. Detailed implemen-tation of the models is available in our public code repository. showcases two real-world examples: (a) Weather Tem-perature data from UCR-KDD21 and (b) ECG data. The top rowof each example shows the time series data with actual anomalieshighlighted in red. The next rows illustrate the the output of thePerfect Model, and Models 1 and 2 (represented by MVN, LOF, orAE), demonstrating their respective detection scores. The final rowdisplays a random score for baseline comparison. quan-titatively compares various metrics. PATE consistently rates thePerfect Model highest and the Random Score lowest, showing itscapability to recognize optimal detection and effectively penalize",
  "VUS-ROC": "AnomalyTrans 0.06 0.91 0.03 0.96 0.49 0.50 0.13 0.94 0.02 0.97 0.49 0.52 0.19 0.94 0.02 0.97 0.53 0.54 0.33 0.98 0.02 0.99 0.51 0.52DCDetector0.07 0.87 0.01 0.94 0.50 0.51 0.14 0.97 0.02 0.98 0.50 0.58 0.12 0.96 0.02 0.99 0.49 0.50 0.32 0.98 0.02 0.99 0.50 0.52USAD0.16 0.94 0.13 0.91 0.63 0.72 0.17 0.91 0.06 0.92 0.53 0.58 0.73 0.85 0.25 0.83 0.82 0.61 0.45 0.89 0.07 0.91 0.60 0.61LSTM0.25 0.80 0.14 0.87 0.76 0.81 0.19 0.82 0.08 0.87 0.57 0.64 0.71 0.82 0.03 0.85 0.82 0.60 0.55 0.93 0.15 0.94 0.73 0.73",
  ": Segments of anomaly scores of SOTA models for SWaT and SMD dataset. The highlighted regions in red indicate the trueanomaly periods (labeled by an expert)": "poor performance. In contrast, VUS-ROC/PR and AUC-ROC metricsseem less capable of such differentiation with the baselines.Moreover, PATE accurately takes into account the time seriescontext and delayed detection effect, offering a more realistic andconservative assessment compared to VUS-ROC and AUC-ROCmetrics, which appear to overestimate the performance of Models1 and 2. This overestimation is evident in the Weather Temperaturedata, where Model 2 is inaccurately scored high by VUS-ROC andAUC-ROC despite its poor detection. Additionally, AUC-PR is alsonot sensitive in evaluation. For instance, in the Weather Tempera-ture data, Model 1s delayed yet successful detection is incorrectlyevaluated with a very low score, similar to the detection of Model 2.Similarly, in the ECG data, PATEs evaluation reflects the inconsis-tent anomaly detection pattern of Model 2 (AE) compared to Model1 (MVN). However, AUC-ROC/PR and VUS-ROC do not effectivelyconsider this difference. Overall, PATEs assessments across bothexamples underscore its effectiveness in real-world applications.",
  "Impact Analysis: SOTA Models": "We re-evaluated several recent SOTA anomaly detection methodsto not only assess their true performance but also to examine thestability of their ranking across various benchmark datasets whenevaluated with different metrics, including PATE. Our compara-tive analysis includes models such as DCdetector , Anomaly-Trans , and USAD , all of which have been recognized fortheir high performance in recent studies, alongside a Transformerand LSTM model, as simpler reconstruction-based anomaly detectorbaselines. These models are tested across the benchmark datasets of SMD , MSL , SWaT , and PSM , used in previous works.Implementation details are available in our public code repository.In the literature on SOTA models, the PA-F1 is the most fre-quently used and widely accepted metric. Additionally, in somecases, the standard F1 Score and Point-Adjusted variant of AUC-ROC (PA-AUC-ROC) are also employed. For a comprehensive com-parison, we included these metrics in our comparative analysis.Results, shown in , highlight a significant discrepancy be-tween PATE scores and those obtained from other metrics likePA-F1, Standard F1 Score, and PA-AUC-ROC. Notably, models thatperformed exceptionally well under PA-F1 and PA-AUC-ROC, suchas AnomalyTrans and DCdetector, exhibit markedly lower scoreswhen evaluated with PATE. For instance, for the SMD dataset,AnomalyTrans achieves a PA-F1 score of 0.91, showcasing high per-formance, yet its PATE score is only 0.06, indicating a substantialreduction in performance. To visually illustrate the differences indetection quality, shows a portion of the anomaly scoresfor the SWaT and SMD. The figures show that AnomalyTrans andDCdetector models struggle with consistent detection. In particular,for the SWaT, the peaky detections by these models hardly alignwith the expert-labeled anomaly intervals, and the high values re-ported for PA-F1 and PA-AUC-ROC do not reflect this detectionpattern. This suggests that these metrics may overestimate modeleffectiveness.Next, shows that the Standard F1 Score, AUC-ROC, andVUS-ROC, do not exhibit such overestimations. However, they lacksensitivity to the finer aspects of detection as discussed in section2.1. For instance, on the SWaT dataset, the Standard F1 Score is",
  "ABLATION ANALYSIS: BUFFER SIZES": "The adaptability of PATE to accommodate different buffer sizesis one of its key strengths. This flexibility allows for an expert-driven and context-specific approach to model evaluation, ensuringthat the unique characteristics of each dataset are appropriatelyconsidered. illustrates the mean performance of DCdetec-tor, AnomalyTrans, USAD, LSTM, and Transformer across all fourbenchmark datasets using PATE. Results show that PATE consis-tently ranks models such as Transformer and LSTM the highestacross different buffer sizes. This consistency in model rankings,irrespective of buffer size, highlights PATEs robustness as an eval-uation metric, and showcases PATEs reliability for diverse applica-tions, ensuring a consistent and dependable assessment for anomalydetection models. Pre- and Post- Buffer Sizes (e and d) 0.2 0.3 0.4 0.5 0.6 Mean PATE Performance AnomalyTransDCdetectorUSADLSTMTransformer",
  "DISCUSSION AND CONCLUSION": "We proposed PATE, a novel approach to evaluate anomaly detec-tion models in time series data. PATE addresses the limitations ofexisting evaluation metrics by categorizing the anomaly and predic-tion events and assigning proximity-based weighting, consideringdifferent buffer zones around the anomaly event. PATE computesthe area under the Precision-Recall curve, where the Precision andRecall are computed from weighted versions of True Positive, FalsePositive, and False Negative performances.Our experiments with both synthetic and real-world data demon-strate that PATE effectively differentiates between models based ontheir actual performance, considering early and delayed detection,onset response time, coverage level of the anomaly event, and con-sistency in detection. The re-evaluation of SOTA anomaly detectionmethods using PATE reveals notable differences in performance as-sessments compared to other metrics. For instance, point-adjustedmetrics often overestimate the performance of models. However,in practice, metrics such as ROC-AUC and VUS-ROC offer morereasonable estimates for SOTA models, though they might over-look subtle detection errors and sometimes lack discriminabilitybetween models. This analysis not only questions the true perfor-mance of current SOTA models but also indicates a shift in theirrankings, challenging the prevailing understanding of the superi-ority of these models. PATEs ability to provide a more matching,context-sensitive, and transparent assessment highlights its po-tential as a more appropriate metric that can set a new standardfor evaluating advancements in anomaly detection. Additionally,PATEs adaptability to various buffer sizes without compromisingconsistency and fairness in model evaluation further highlights itsrobustness and applicability across diverse applications.To address the specific scenarios where either an expert haspredetermined the threshold or models inherently output binarylabels, we have developed PATE-F1 as an essential extension ofthe original PATE framework. The methodology and experimentalinsights on PATE-F1 are detailed in Appendix D. PATE-F1 effectivelydistinguishes between different scenarios based on temporal prox-imity, duration, coverage level, and response timing, setting it apartfrom other metrics that face limitations in capturing these aspectsin evaluation. Additionally, our findings indicate that the originalPATE framework, through strategic threshold application, naturallyextends to effectively evaluate binary outputs. However, employ-ing PATE-F1 in such scenarios offers a more direct and simplifiedapproach. This adaptation ensures PATEs methodology remains",
  "Charu C Aggarwal and Charu C Aggarwal. 2017. An introduction to outlieranalysis. Springer": "Julien Audibert, Pietro Michiardi, Frdric Guyard, Sbastien Marti, and Maria AZuluaga. 2020. Usad: Unsupervised anomaly detection on multivariate time series.In Proceedings of the 26th ACM SIGKDD international conference on knowledgediscovery & data mining. 33953404. Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and Jrg Sander. 2000.LOF: identifying density-based local outliers. In Proceedings of the 2000 ACMSIGMOD international conference on Management of data. 93104.",
  "Sounak Chakraborty. 2011. An Intermediate Course in Probability. Varun Chandola, Arindam Banerjee, and Vipin Kumar. 2009. Anomaly detection:A survey. ACM computing surveys (CSUR) 41, 3 (2009), 158": "Ramin Ghorbani, Marcel JT Reinders, and David MJ Tax. 2024. Personalizedanomaly detection in PPG data using representation learning and biometricidentification. Biomedical Signal Processing and Control 94 (2024), 106216. Alexis Huet, Jose Manuel Navarro, and Dario Rossi. 2022. Local evaluationof time series anomaly detection algorithms. In Proceedings of the 28th ACMSIGKDD Conference on Knowledge Discovery and Data Mining. 635645. Kyle Hundman, Valentino Constantinou, Christopher Laporte, Ian Colwell, andTom Soderstrom. 2018. Detecting spacecraft anomalies using lstms and nonpara-metric dynamic thresholding. In Proceedings of the 24th ACM SIGKDD interna-tional conference on knowledge discovery & data mining. 387395. Won-Seok Hwang, Jeong-Han Yun, Jonguk Kim, and Hyoung Chun Kim. 2019.Time-series aware precision and recall for anomaly detection: considering varietyof detection result and addressing ambiguous labeling. In Proceedings of the 28thACM International Conference on Information and Knowledge Management. 22412244. Won-Seok Hwang, Jeong-Han Yun, Jonguk Kim, and Byung Gil Min. 2022. Doyou know existing accuracy metrics overrate time-series anomaly detections?. InProceedings of the 37th ACM/SIGAPP Symposium on Applied Computing. 403412. Siwon Kim, Kukjin Choi, Hyun-Soo Choi, Byunghan Lee, and Sungroh Yoon. 2022.Towards a rigorous evaluation of time-series anomaly detection. In Proceedingsof the AAAI Conference on Artificial Intelligence, Vol. 36. 71947201.",
  "George B Moody and Roger G Mark. 2001. The impact of the MIT-BIH arrhythmiadatabase. IEEE engineering in medicine and biology magazine 20, 3 (2001), 4550": "John Paparrizos, Paul Boniol, Themis Palpanas, Ruey S Tsay, Aaron Elmore, andMichael J Franklin. 2022. Volume under the surface: a new accuracy evaluationmeasure for time-series anomaly detection. Proceedings of the VLDB Endowment15, 11 (2022), 27742787. Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. 2019. Robustanomaly detection for multivariate time series through stochastic recurrentneural network. In Proceedings of the 25th ACM SIGKDD international conferenceon knowledge discovery & data mining. 28282837.",
  "AREPRODUCIBILITY STATEMENT": "To ensure the reproducibility of our work, the source code, alongwith comprehensive documentation, is publicly available at: repository includes detailed instructions for using PATE,including how to set the buffer size, and complete descriptions of allmodels implemented for our experiments, covering configurationsettings, training procedures, and experimental details to ensureaccurate replication. Researchers seeking additional informationare encouraged to contact the corresponding author.",
  "BEFFECT OF ANOMALY LENGTH ON BUFFERWEIGHTS": "To explore the effect of anomaly length on the assignment ofweights within the PATE framework, we consider three distinctanomaly events with varying durations:1,2, and3, with1 beingthe longest and 3 the shortest. Each was followed by a post-bufferzone of fixed size . depicts the potential True Positive(TP) weights along the timeline, capturing the period before theanomaly, within its range, and throughout the post-buffer zone.The analysis of this figure indicates that TP weights for detectionsin the post-buffer zone are higher for 3, the shortest anomaly,and progressively lower for 1 and 2, the longer anomalies. Thisobservation underscores the direct correlation between the dura-tion of an anomaly and the corresponding TP weights assigned topost-buffer detections. Higher TP weights for detections followingshorter anomalies signify the critical nature of these detections,as they are in closer proximity to the anomaly onset. The PATEweighting mechanism accommodates this by adjusting the weightsbased on the distance from detections to the entire anomaly. Thisphenomenon also extends to the pre-buffer zone, where early de-tections are similarly influenced by the length of the forthcominganomaly.",
  "CCLARIFICATION ON EARLY AND DELAYEDDETECTIONS": "To understand the distinct approaches PATE takes toward EarlyDetection (in the pre-buffer zone) and Delayed Detection (in thepost-buffer zone), it is essential to consider the foundational goalof this evaluation metric.For an anomaly detector, the ability to learn from past dataand accurately predict future anomalies is essential. An early pre-diction that fails to correspond with an actual, subsequent anom-aly suggests a fundamental modeling failure of the datas under-lying structurelike sounding an alarm for an event that neverhappens. Ideally, if a model detects early signs of an impendinganomaly, it should also identify the anomaly when it occurs. Theearly signssmall changes or patterns of deteriorationlead to alarger and more evident departure from the norm. If the model hascorrectly identified these early signs, it should also recognize theanomaly itself, given the now more noticeable deviation. When theearly detection is successfully followed by a true detection of theanomaly, the early detection is not considered just a lucky guess. Itsupports the models predictive power and consistency.In contrast, the context for delayed detection significantly differsas it showcases the capability of the model to identify anomaliespost hoc. The model is apparently able to detect some deviationin the input, albeit a bit late. Such late detections still allow forthe identification of the anomaly. Failing to have True Positivedetections in the anomaly event is therefore not considered fatalfor the Delayed Detection. shows the detection responses by three different modelsto an anomalous event, shown by the shaded area in red. Model1 (top panel) reveals an early detection followed by True Positivedetections, indicated by peaks aligning with the anomaly window.This pattern exemplifies an acceptable detection where the modelpreemptively and accurately identifies an anomaly. Model 2 (middlepanel), however, demonstrates early detection without subsequentTPs during the actual anomaly, missing the critical deviation. Thisoutcome might suggest a misinterpretation of the anomaly patternby Model 2, potentially leading to a false alarm scenario. Conversely,Model 3 (bottom panel) shows a peak that arises post the onset ofthe anomaly, exemplifying a delayed detection. This detection isvalued as it demonstrates the capacity of the model for retrospectiveanalysis, acknowledging and learning from the anomaly event afterits occurrence.",
  "DPATE-F1 - ADJUSTED FOR BINARY SCORES": "Methodology: To enhance the applicability of PATE in scenarioswhere models use predetermined thresholds or where expert knowl-edge informs threshold determination, we propose an adapted ver-sion, PATE-F1. This adaptation leverages the core principles of PATEby assigning proximity-specific weights to categorized points andcalculating weighted Precision and Recall. Unlike the original PATE,which evaluates a range of thresholds (), PATE-F1 is tailored forbinary scenarios, without the variation of thresholds but ratherdifferent combinations of buffer zones ( and ). For each combina-tion, weighted Precision and Recall are calculated using equations9 and 10 as detailed in .3. Subsequently, the F1 score foreach combination is determined as follows:",
  "Here, || and || represent the number of distinct pre-buffer ()and post-buffer () sizes, respectively": "Experimental Results: We extend our analysis to PATE-F1 bycomparing the evaluations against threshold-dependent metrics,tailored for binary score predictions. shows 10 differentdetection scenarios shown by prediction events 1, . . . , 10. shows that similar to the original PATE, PATE-F1 effectively differ-entiates between scenarios based on temporal proximity, duration,coverage level, and response timing. This alignment with PATEsevaluation logic underlines the adaptability of our methodology tobinary score scenarios without compromising the depth of analysisprovided by the range of thresholds in the original framework.",
  "ECOMPLEXITY TIME ANALYSIS": "We evaluated the computational efficiency of the PATE algorithmagainst established metrics like AUC-PR and VUS-PR through ex-periments on synthetic and real benchmark datasets. These ex-periments were conducted on a standard MacBook with a 2 GHzQuad-Core Intel Core i5 processor, Intel Iris Plus Graphics 1536 MB,and 16 GB RAM, reflecting the performance on commonly availablehardware. Although PATE supports parallel execution to poten-tially decrease computation time, especially on High-PerformanceComputing (HPC) systems, we used a serial computation approachfor consistent comparisons with other metrics. Synthetic Data Experiments: We generated synthetic timeseries data ranging from 1,000 to 100,000 points with anomaly ratiosof 2%, 5%, and 10% to reflect various common scenarios. As shownin , PATEs computation time increases linearly with datalength and varies slightly with different anomaly ratios. Despite this,computation times remained under one second across all conditions,highlighting PATEs efficiency without parallel processing."
}