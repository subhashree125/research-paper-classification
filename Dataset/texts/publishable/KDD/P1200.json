{
  "ABSTRACT": "Multimodal entity linking (MEL) task, which aims at resolving am-biguous mentions to a multimodal knowledge graph, has attractedwide attention in recent years. Though large efforts have been madeto explore the complementary effect among multiple modalities,however, they may fail to fully absorb the comprehensive expres-sion of abbreviated textual context and implicit visual indication.Even worse, the inevitable noisy data may cause inconsistency ofdifferent modalities during the learning process, which severelydegenerates the performance. To address the above issues, in thispaper, we propose a novel Multi-GraIned Multimodal InteraCtionNetwork (MIMIC) framework for solving the MEL task. Specifi-cally, the unified inputs of mentions and entities are first encodedby textual/visual encoders separately, to extract global descriptivefeatures and local detailed features. Then, to derive the similaritymatching score for each mention-entity pair, we device three inter-action units to comprehensively explore the intra-modal interactionand inter-modal fusion among features of entities and mentions. Inparticular, three modules, namely the Text-based Global-Local inter-action Unit (TGLU), Vision-based DuaL interaction Unit (VDLU) andCross-Modal Fusion-based interaction Unit (CMFU) are designedto capture and integrate the fine-grained representation lying inabbreviated text and implicit visual cues. Afterwards, we introducea unit-consistency objective function via contrastive learning toavoid inconsistency and model degradation. Experimental results",
  "Corresponding Author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 23, August 610, 2023, Long Beach, CA, USA 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00",
  "INTRODUCTION": "Entity linking (EL), also known as entity disambiguation, plays afundamental but imperative role to connect a wide and diverse va-riety of web content to referent entities of a knowledge graph (KG),which supports numerous downstream applications such as searchengines , question answering , dialog systems and so on. Over the past years, large efforts have been dedicatedto text-based entity linking. However, in the surge of multimodalinformation, images along with text have become the most widely-seen medium to publishing and understanding web information,which also brings challenges to the comprehension of complexmultimodal content. Thereby, multimodal entity linking (MEL), re-solving the visual and textual mentions into their correspondingentities of a multimodal knowledge graph (MMKG), is desperatelydesired. For instance, as shown in , the short sentencecontains an affiliated image to complement the textual context of",
  ": Examples of multimodal entity linking. Left: twomultimodal mentions. Right: multimodal knowledge graph": "mention. In this case, it is challenging for text-based EL methods todetermine which entity is related to the entity Leonardo in .Differently, visual information, e.g., the character portraits, bringsvaluable content and alleviates ambiguity of textual modality. Thus,it is intuitive to integrate visual information with textual contextswhen linking the multimodal mentions to heterogeneous MMKGentities.Along this line, prior arts attempted to solve the MEL task viaexploring complementary effects of different modalities by leverag-ing concatenation operation , additive attention , and cross-attention mechanism on public benchmark datasets such asTwitterMEL , WikiMEL . Although these studies for MELhave shown promising progress compared with text-based EL meth-ods, MEL is still not a trivial task due to the following reasons:(1) Short and abbreviated textual context. The sentence of men-tion contexts contains limited information due to text lengthor the known topic, which is commonly seen in social mediaplatforms. Therefore, it is necessary to capture the fine-grainedclues lay in the textual context. (2) Implicit visual indication. Due to the semantic gap betweenlow-level visual information and high-level semantic cues, itmight be difficult to capture the implicit indications that corre-spond to the category or description of entities. For example,the portrait could imply occupation and gender of one person,which may not be extracted via simple detection or matchingtools. In this case, it is necessary to design one specific moduleto capture the implicit multimodal cues from explicit visualfeatures. (3) Modality Consistency. Recent studies have revealed thatjoint learning of multiple modalities may cause contradictionor degeneration when optimization due to the inevitable noisydata, or excessive influence of a specific modality. Therefore, itis necessary to model consistency and enhance the cooperativeeffect among modalities. To deal with these issues, in this paper, we propose a novel Multi-GraIned Multimodal InteraCtion network (MIMIC) for MEL task,which consists of two layers, namely an input and feature encod-ing layer, as well as a multi-grained multimodal interaction layer.Specifically, in the input and feature encoding layer, we design a uni-fied input format for both multimodal mention and MMKG entities.Then, the encoder extracts both local and global features of textual and visual inputs for obtaining global descriptive semantics, whilereserving fine-grained details in words or image patches. Also, inthe multi-grained multimodal interaction layer, we devise three par-allel interaction units to fully explore multimodal schemata. First,to capture the clues that lie in the abbreviated text, we propose aText-based Global-Local interaction Unit (TGLU), which not onlyconsiders lexical coherence from a global view but also mines fine-grained semantics by utilizing attention mechanism. Afterwards, toaddress the challenge of visual indication, we design a Vision-basedDuaL interaction Unit (VDLU) and a Cross-Modal Fusion-basedinteraction Unit (CMFU), for explicit and implicit indications, re-spectively. In detail, the tailored VDLU introduces a dual-gatedmechanism to amplify the explicit visual evidence within featuresas well as enhance robustness against noisy images from the Inter-net. Meanwhile, different from utilizing concatenation or attention,the CMFU module first projects extracted global textual featuresand local visual features into a vector space, and then fuse themwith a gated operation, which could effectively mine the implicit se-mantic relevance of multiple modalities to complement each other.Moreover, to attain the consistency of different modalities and units,we introduce a unit-consistent loss function based on contrastivetraining to improve intra-modal and inter-modal learning for mul-tiple interaction units. To the best of our knowledge, technicalcontributions of this paper can be summarized as follows: We propose a multi-grained multimodal interaction network forsolving multimodal entity linking task, which could universallyextract features for both multimodal mentions and entities. Andthe proposed network could be easily extended by adding newinteraction units. We devise three interaction units to sufficiently explore and ex-tract diverse multimodal interactions and patterns for entitylinking. Moreover, we introduce the unit-consistent loss func-tion to enhance the intra-modal and inter-modal representationlearning. We perform extensive experiments on three public multimodalentity linking datasets. Experimental results illustrate that ourmethods outperform various competitive baselines. The ablationstudy also validates the effectiveness of each designed module.",
  "Text-based Entity Linking": "This line of research links mentions to a known knowledge graphvia utilizing textual information of context and entities. Accordingto the granularity of different methods, we roughly divide the ex-isting studies into two groups: local-level methods and global-levelmethods. The former approaches primarily perform entity linkingby mapping mention along with its surrounding words or sentencefor similarity calculation. Early research leveraged word2vec andconvolutional neural networks (CNN) to capture the correlationbetween mention context and entity information .Thereafter, Eshel et al. integrated entity embedding into the re-current neural network (RNN) with attention mechanism in order to",
  "Multi-Grained Multimodal Interaction Network for Entity LinkingKDD 23, August 610, 2023, Long Beach, CA, USA": ": Performance comparison on three MEL datasets. We run each method three times with different random seeds andreport the mean value of every metric. The best score is highlighted in bold and the second best score is underlined. The symbol\"\" denotes the p-value of the t-test compared with the second best score is lower than 0.005 and \"\" means the p-value is lowerthan 0.01 but higher than 0.005.",
  "Multimodal Entity Linking": "Since social media and news posts are in the form of texts andimages, combining both textual and visual information for entitylinking is crucial and practical. As one of the pioneering research,Moon et al. introduced images to assist entity linking due tothe polysemous and incomplete mentions from social media posts.Beyond that, Adjali et al. utilized unigram and bigram embed-dings as textual features and pretrained Inception to extractvisual features. After the extraction, a concatenation operation wasapplied to fuse the features and the model was optimized with thetriple loss. They also constructed a MEL dataset of social mediaposts from Twitter. Wang et al. further explored inter-modalcorrelations via a text and vision cross-attention, where a gatedhierarchical structure is incorporated. To remove the negative effectcaused by noisy and irrelevant images, Zhang et al. consideredthe correlation between the category information of images andthe semantic information of text mentions, in which the imageswere filtered by a predefined threshold. Gan et al. constructed adataset that contains long movie reviews with various related enti-ties and images. A recent research incorporated scene graphs of images to obtain object-level encoding towards detailed semanticsof visual cues.Although these research studies have shown that visual infor-mation is beneficial to the performance of entity linking to someextent, the utilization of visual information in conjunction withtextual context remains largely underdeveloped.",
  "Problem Formulation": "First, we define related mathematical notations as follows. Typically,a multimodal knowledge base is constructed by a set of entities ={E}=1, and each entity is denoted as E = (e, e, e, e ), wherethe elements of E represent entity name, entity images, entitydescription, and entity attributes, respectively. Since our researchconcentrates on local-level entity linking, the textual inputs are inthe format of sentences instead of documents. Here, a mention andits context are denoted as M = (m, m, m ), where m, mand m indicate the words of mention, the sentence in which themention is located, and the corresponding image, respectively. Therelated entity of the mention M in the knowledge base is E.Along this line, given a mention M, the task of multimodal entitylinking targets to retrieve the ground truth entity E from the entityset of knowledge base. This task can be obtained by maximizingthe log-likelihood over the training set while optimizing themodel parameters , i.e.,",
  "Input and Encoding Layer": "In this layer, we design a unified input format, which allows men-tions and entities to share the same visual/textual encoder. Weintroduce the input format and encoding process in the followingsubsections. 3.2.1Visual Feature Encoding. To capture the expressive fea-tures of images, we employ the pre-trained Vision Transformer(ViT) as the visual encoder backbone. Given the image e ofan entity E, we first rescale each image into pixelsand reshape it into = 2 flattened 2D patches, where is the number of channels, is the image resolution and represents the patch size. After that, the patches go through theprojection layer and multi-layer transformer of the standard ViT.We add a fully connected layer to convert the dimension of outputhidden status into . Thus the hidden status of entity image aredenoted as VE = [v0[CLS]; v1E ; . . . ; vE R(+1). We take thecorresponding hidden state of the special token [CLS] as globalfeature vE R and the whole hidden states as local features",
  "IE = [CLS]e [SEP]e [SEP],(2)": "where is a set of entity attributes collected from the knowledgebase including entity type, occupation, gender, and so on. Differentattributes are separated by a period. Then we feed the tokenizedsequence IE into BERT and the hidden states are denoted as TEi =[t0[CLS]; t1E ; . . . ; tE R(+1) , where is the dimension oftextual output features, and is the length. We also regard thehidden state of [CLS] as global textual feature tE and the entirehidden states TE as local textual features TE .As for the mention M, we use the concatenation of the words ofmention and the sentence where the mention is located to composethe input sequence. This can be illustrated as,",
  "Multi-Grained Multimodal InteractionLayer": "To derive similarity matching scores for each mention-entity pair,we devise three interaction units by fully exploring the intra-modaland inter-modal clues in different granularities. As illustrated in, the interaction layer consists of three parallel units: (1) Text-based Global-Local interaction Unit (TGLU) is dedicatedto capturing lexical information among abbreviated text in bothwhole and partial views; (2) Vision-based DuaL interaction Unit(VDLU) concentrates on revealing the explicit visual correlation be-tween mention images and entity images; (3) Cross-Modal Fusion-based interaction Unit (CMFU) focuses on capturing fine-grainedimplicit semantics to supplement the interaction of different modali-ties. Each unit takes features from an entity and a mention as inputsand then calculates a score as:",
  "= (M, E) = ( + + )3,(7)": "where , , and are the scores calculated by TGLU, VDLU,and CMFU respectively. The final score is defined as the averageof the three scores. In the following subsections, we elaborate onthem in detail one by one. 3.3.1Text-based Global-Local interaction Unit. Text is the ba-sic but imperative information for entity linking. Previous methodsutilized the hidden status of [CLS] as global features while los-ing the local features, or integrated Conv1D to measure characterlevel similarity whereas ignoring the global coherence. To measureglobal consistency, we use the dot product of two normalized globalfeatures as the global-to-global score, mathematically formulatedas,2= tE tM.(8)Based on the designed unified textual input, Equation 8 directlymeasures the global correlation of text input of mention and en-tity. Then we make further efforts to discover fine-grained cluesamong local features. Specifically, we utilize the attention mech-anism to capture the context of different local features, and therepresentation is calculated as follows:",
  "= DUAL2 (vM, vE , VE),(11)": "where DUAL2(vA, vB, VB) represents the dual-gated mechanismby considering the feature interaction from to . Without losinggenerality, here we use and to represent entity () or mention() for illustrating DUAL2(, , ) function. We first utilize meanpooling and layer norm over VB to get the pooled vector andcombine it with vA as follows:",
  "According to the above formulas Equation 12 - Equation 14 on thecalculation of DUAL2, similarly, we can obtain 2and 2,which lead us to the final score": "3.3.3Cross-Modal Fusion-based interaction Unit. As men-tioned before, the images contain implicit indications which can beinferred from multiple modalities. To highlight the subtle cues orsignals among different features, the designed CMFU considers thecross-modal alignment and fusion via a gated function based onthe extracted local and global features. In order to obtain the unit-related features for the subsequent operations as well as compactthe dimension of features, we convert textual and visual featuresvia two fully connected layers as follows,",
  ", = FC2 (VE), FC2 (VM),(15)": "in which FC1 is defined by 1 R and 1 R , FC2is defined by 2 R and 2 R . After projection, weintroduce a function FUSE(,) for the fine-grained fusion oftextual and visual features, where represents entity () or mention(). Without losing generality, we take the fusion of entity side asan example. First, the element-wise dot product scores of textualand visual features are applied to guide the aggregation of imagepatch information,",
  "exp ((M, E)),(20)": "where E is the negative entity from the knowledge base and weuse in-batch negative sampling in our implementation. However,the function (M, E) calculates the average scores of three units.This may result in one of the units taking the dominant position,causing the whole model to excessively rely on its score. In addi-tion, inconsistencies in scoring may also occur as different unitsconsider different perspectives. To this end, we propose to designindependent loss functions for each unit as follows,",
  "Experimental Setup": "4.1.1Datasets. In the experiments, we selected three public MELdatasets WikiMEL, RichpediaMEL and WikiDiverse to verify the effectiveness of our proposed method.WikiMEL is collected from Wikipedia entities pages andcontains more than 22k multimodal sentences. RichpediaMEL is obtained form a MMKG Richpedia . The authors of Richpe-diaMEL first extracted entities form Richpedia and then obtainmultimodal information form Wikidata . The main entity typesof WikiMEL and RichpedaiMEl are person. WikiDiverse isconstructed from Wikinews and covers various topics includingsports, technology, economy and so on. We used Wikidata as ourknowledge base (KB) and removed the mention that we could notfind the corresponding entity in Wikidata. Linking a mention to alarge-scale MMKG or multimodal knowledge base is extremely time-consuming, especially when taking images into consideration. Tofairly conduct experiments, we followed the previous studies ,and used a subset KB of Wikidata for each dataset. We used theoriginal split of the three datasets. For both WikiMEL and Richpe-diaMEL, 70%, 10% and 20% of the data are divided into training set,validation set and test set respectively. As for WikiDiverses, theproportions are 80%, 10% and 10%. Appendix A.1 provides detailedstatistical information about the datasets. 4.1.2Baselines. We compared our method with various compet-itive baselines including text-based methods, MEL methods andVision-and-Language Pre-training (VLP) models. Specifically, thetext-based methods include BLINK , BERT , RoBERTa .MEL methods contain DZMNED , JMEL , VELML ,GHMFC . Moreover, the VLP models include CLIP , ViLT ,ALBEF , METER , and these models are usually pre-trainedwith large-scale image-text corpus with image-text matching lossand mask language modeling loss. Detailed descriptions of baselinesare provided in Appendix A.2. 4.1.3Evaluation Metrics. When evaluating, we calculated the sim-ilarity between a mention and all entities of KB to measure theiraligning probability. The similarity scores are sorted in descendingorder to calculate H@k, MRR and MR. We provide the calculationmethods for each metric in Appendix A.3.H@k indicates the hit rate of the ground truth entity when onlyconsidering the top-k ranked entities. MRR represents the meanreciprocal rank of the ground truth entity. MR is the mean rankof the ground truth entity among all entities. Hence, both H@kand MRR are the higher the better, but a lower MR indicates betterperformance. 4.1.4Implementation Details. Our model weights are initializedwith pre-trained CLIP-Vit-Base-Patch322, where ViT-B/32 Trans-former architecture is employed as an image encoder and the patchsize P is 32. All images are rescaled into 224 224 resolution andwe used zero padding to handle the mentions and entities withoutimages. The maximal length of text input is set to 40 and the di-mension of textual output features, i.e., is set to 512. As for theparameters in the interaction layer, , and are set to 96 for allthree datasets. We used the deep learning framework PyTorch",
  "MIMIC87.9895.0796.3791.8211.0281.0291.7794.3886.9555.1163.5181.0486.4373.44227.08": "to implement our method and trained it on a device equipped withan Intel(R) Xeon(R) Gold 6248R CPU and a GeForce RTX 3090 GPU.We trained our MIMIC using AdamW optimizer with a batchsize of 128 to accommodate maximal GPU memory and betas areset to (0.9, 0.999). The number of epochs and learning rate are well-tuned to 20 and 1 105 respectively. All methods are evaluatedon the validation set and the checkpoint with the highest MRRis selected to evaluate on the test set. As for the baselines, we re-implemented DZMNED, JMEL, VELML according to the originalliterature due to they did not release the code. We ran the officialimplementations of the other baselines with their default settings.",
  "Experimental Results": "4.2.1Overall Comparison (RQ1). We compared our proposed MIMICwith baselines on three benchmark datasets. As shown in , average scores of the performance on the test set across threerandom runs are reported. Overall, our proposed MIMIC achievesthe best metrics on three datasets, with 3.59%, 6.19%, 1.75% absoluteimprovement of MRR on WikiMEL, RichpediaMEL and WikiDi-verse respectively. This demonstrates the superiority of MIMICfor solving the MEL task. According to the experimental results of, we further have the following observations and analysis.First, compared with MEL and VLP methods, the text-based ap-proaches show promising performance. It suggests that textualinformation is still the basic but crucial modality for MEL becausethe text provides a measurement from the surface. It is noticed thatBLINK slightly underperforms BERT on WikiMEL and Richpedi-aMEL but outperforms BERT on WikiDiverse. Although BLINKutilizes two encoders to extract global representations for mentionsand entities separately, similar to BERT, it ignores the local featuresin the short and abbreviated text which impairs their performanceMoreover, compared with the state-of-the-art MEL methods, thetext-based approaches still have a gap in performance because theyonly rely on textual inputs but ignore visual information, whichbrings difficulty to identify vague mentions within the limited text. Second, different MEL methods have their respective pros andcons. Benefiting from the hierarchical fine-grained co-attentionmechanism, GHMFC achieves the best result on three datasetsamong all MEL baselines. In particular, compared with all otherbaselines, GHMFC achieves 72.92% and 80.76% for H@1 and MRR re-spectively on RichpediaMEL, which is only inferior to our proposedMIMIC. It indicates that effectively incorporating visual featuresinto multimodal interaction contributes to improving the perfor-mance of MEL. Different MEL methods show a large gap. As shownin , JMEL underperforms DZMEND, VELML and GHMFC onthree datasets, which may result from the strategy of multimodalfusion. JMEL utilizes simple concatenation and a fully connectedlayer to fuse textual and visual features. In contrast, both DZMENDand VELML use additional attention mechanism to fuse differentfeatures. It suggests that shallow modality interaction and naivemultimodal fusion bring no improvement even degeneration onthe performance of MEL.Third, VLP methods also demonstrate competitive evaluationresults compared with MEL baselines. CLIP achieves the second bestmetrics except for MR on both WikiMEL and WikiDiverse, whichbenefits from pre-training with the large-scale image-text corpus.ALBEF and METER also display similar results with CLIP. We arguethat these methods could be further exploited by considering fine-grained interaction and delicate designed fusion.Finally, the experimental results demonstrate the effectivenessand superiority of our proposed MIMIC. Compared with the sec-ond best metric, MIMIC gains 4.75%, 8.1% and 2.3% absolute im-provement of Hit@1 on WikiMEL, RichpediaMEL and WikiDiverserespectively. We also performed significant tests to further validatethe statistical evidence between MIMIC and other baselines. Specif-ically, the p-values of MRR on three datasets are 0.002, 0.0001 and0.009 respectively. All p-values are under 0.01 and show a significantadvantage in statistics.",
  ": Performance comparison of low resource settings on RichpediaMEL and WikiDiverse. Details are zoomed in for bettervisualization": "Therefore, it is necessary to investigate the performance of the mod-els in low-resource scenarios. We conducted experiments using 10%and 20% of the training data while keeping the validation and testsets unchanged. Experimental results are shown in . Inoverview, most of the MEL methods manifest a significant dropin performance. Except for ViLT, other VLP methods benefit fromlarge-scale multimodal pre-training and show a slight decrease inperformance, which means that well-trained weights guaranteea reasonable performance in a low resource setting. With the in-crease in training data, nearly all methods e.g., DZMNED, JMEL andVELML, show an obvious improvement, which means sufficienttraining data is necessary to improve the performance. Notably,GHMFC outperforms our proposed MIMIC with 10% training dataon RichpediaMEL but underperforms MIMIC with 10% trainingdata on WikiDiverse while showing a clear gap. It suggests thatGHMFC does not generalize well on different datasets. When theproportion comes to 20%, our proposed MIMIC surpasses GHMFCin every metric on RichpediaMEL and shows an obvious margin.From 10% to 20%, the absolute improvement of H@1, H@3 and MRRof MIMIC are 11.2%, 6.6% and 8.11%, respectively. This phenomenonreveals that detailed inter-modal and intra-modal interaction unitsof MIMIC have better adaptability with the increase in trainingdata. As for WikiDiverse, CLIP slightly underperforms MIMIC onH@1 and MRR in the 10% setting. With the increase in trainingproportion, the gap between MIMIC and CLIP gradually becomeslarger, which validates MIMIC has better capability and potentialin the low resources scenario. 4.2.3Ablation Study (RQ3). To delve into the effect of three pro-posed interaction units and unit-consistent loss function, we de-signed two groups of experiments for the ablation study. In the firstgroup, we remove , and separately from loss function, i.e.,Equation 22. We denote these variants as w/o , w/o and w/o respectively. In the second group, we further compare MIMICwith the following variants: (1) w/o TGLU + : removing the text-based global-local matching unit and its loss function; (2) w/o VDLU+ : removing the vision-based dual matching units along withits loss function; (3) w/o CMFU + : removing the cross-modalfusion-based matching unit and its loss function. illustratesthe experimental results.Overall, removing any interaction unit or loss function from thefull model results in an evident decline in almost every metric tovarying degrees, which proves the effectiveness of the designed in-teraction units and unit-consistent loss function. The performanceof w/o and w/o drops marginally on WikiMEL. It is noticedthat w/o outperforms the full model diminutively on H@10and H@20. However, the model w/o shows an obvious declinein H@1 and MRR. One possible reason is that improves overallperformance but has a side effect on some hard samples depend-ing on the dataset. On RichpediaMEL, a significant performancedrop of w/o can be observed. H@1 degrades from 81.02% to72.82% and MRR drops from 86.95% to 81.61%. This demonstratesthe unit-consistent loss function improves intra-modal and inter-modal learning because it helps that the ground truth entity couldbe retrieved from any single interaction unit. The unit-consistentloss function also alleviates the modality inconsistency caused bynoisy data. Moreover, excluding any interaction units leads to adecrease in performance as well. Specifically, the variant w/o VDLU+ shows the worst H@1 and MRR on WikiMEL. In terms ofRichpediaMEL, the model w/o TGLU + has the worst MRR,which suggests that the two datasets have different salient modali-ties and schemata. Hence it is necessary to explore the interactionand fusion in multimodal and multi-grained ways. The combinationof our proposed interaction matching units gives an effective boostto most metrics, proving the efficacy of our design.",
  ": Parameter sensitivity analysis on WikiMEL and RichpediaMEL regarding different values": "4.2.4Parameter Sensitivity Analysis (RQ4). In this section, we in-vestigated the sensitivity of parameters on two datasets, WikiMELand RichpediaMEL. The experimental results are shown in .First, we analyzed the effect of various dimensions of TGLU, VDLUand CMFU, namely , and . We can see that the performanceraise up gradually with the increase in dimension and then dropsslowly. It suggests that three interaction units need a proper di-mension to encode semantics features, but a large dimension maycause redundancy, leading to a decrease in performance. Second,we explored the impact of the learning rate. The result shows thatperformance benefits from a small and suitable learning rate be-cause we initialized MIMIC with pre-trained model weights. Asthe learning rate gets larger, the performance starts to degeneratebecause of converging to a suboptimal solution. We also analyzedthe effect of batch size. Based on the results, a larger batch sizegenerally improves the performance of MIMIC. The reason is thatMIMIC utilizes in-batch contrastive learning. Hence a large batchsize means more negative samples in a single batch, which couldenhance the representation learning process.",
  "In this paper, we proposed a novel Multi-Grained Multimodal In-teraction network (MIMIC) for solving multimodal entity linking": "task, which comprehensively explores intra-modal and inter-modalpatterns to extract explicit and implicit clues. Concretely, we firstdesigned a unified input format to encode both entities and men-tions into the same vector space, which reduces the feature gapbetween entities and mentions. Then, we devised three interactionunits, namely Text-based Global-Local interaction Unit, Vision-based DuaL interaction Unit and Cross-Modal Fusion-based in-teraction Unit, to explore the explicit and implicit semantics rele-vance within extracted multimodal features. Afterwards, we alsointroduced a unit-consistent loss function to improve multimodallearning and enhance the consistency of our model against noisydata. Extensive experiments on three public datasets have validatedthe effectiveness of our MIMIC framework compared with severalstate-of-the-art baseline methods. This work was supported by the grants from National NaturalScience Foundation of China (No.U22B2059, 62222213, 62276245,62072423), the Anhui Provincial Natural Science Foundation (GrantNo. 2008085J31), and the USTC Research Funds of the Double First-Class Initiative (No.YD2150002009).",
  "Tao Cheng and Kevin Chen-Chuan Chang. Entity search engine: Towards ag-ile best-effort information integration over the web. In CIDR, pages 108113.www.cidrdb.org, 2007": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT:pre-training of deep bidirectional transformers for language understanding. InNAACL-HLT (1), pages 41714186. Association for Computational Linguistics,2019. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, GeorgHeigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth16x16 words: Transformers for image recognition at scale. In ICLR. OpenRe-view.net, 2021. Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang,Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, Zicheng Liu, andMichael Zeng. An empirical study of training end-to-end vision-and-languagetransformers. In CVPR, pages 1814518155. IEEE, 2022.",
  "Phong Le and Ivan Titov. Improving entity linking by modeling latent relationsbetween mentions. In ACL (1), pages 15951604. Association for ComputationalLinguistics, 2018": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, AbdelrahmanMohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: denoisingsequence-to-sequence pre-training for natural language generation, translation,and comprehension. In ACL, pages 78717880. Association for ComputationalLinguistics, 2020. Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Gotmare, Shafiq R. Joty, CaimingXiong, and Steven Chu-Hong Hoi. Align before fuse: Vision and languagerepresentation learning with momentum distillation. In NeurIPS, pages 96949705, 2021.",
  ". ACM, 2018": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, OmerLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustlyoptimized BERT pretraining approach. CoRR, abs/1907.11692, 2019. Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois,and Sameer Singh. Entity-based knowledge conflicts in question answering. InEMNLP (1), pages 70527063. Association for Computational Linguistics, 2021.",
  "Seungwhan Moon, Leonardo Neves, and Vitor Carvalho. Multimodal namedentity disambiguation for noisy social media posts. In ACL (1), pages 20002008.Association for Computational Linguistics, 2018": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, GregoryChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, AlbanDesmaison, Andreas Kpf, Edward Z. Yang, Zachary DeVito, Martin Raison,Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, andSoumith Chintala. Pytorch: An imperative style, high-performance deep learninglibrary. In NeurIPS, pages 80248035, 2019. Matthew E. Peters, Mark Neumann, Robert L. Logan IV, Roy Schwartz, VidurJoshi, Sameer Singh, and Noah A. Smith. Knowledge enhanced contextual wordrepresentations. In EMNLP/IJCNLP (1), pages 4354. Association for Computa-tional Linguistics, 2019. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models fromnatural language supervision. In ICML, volume 139 of Proceedings of MachineLearning Research, pages 87488763. PMLR, 2021. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbig-niew Wojna. Rethinking the inception architecture for computer vision. In CVPR,pages 28182826. IEEE Computer Society, 2016.",
  "Junshuang Wu, Richong Zhang, Yongyi Mao, Hongyu Guo, Masoumeh Soflaei,and Jinpeng Huai. Dynamic graph convolutional networks for entity linking. InWWW, pages 11491159. ACM / IW3C2, 2020": "Ledell Wu, Fabio Petroni, Martin Josifoski, Sebastian Riedel, and Luke Zettlemoyer.Scalable zero-shot entity linking with dense entity retrieval. In EMNLP (1), pages63976407. Association for Computational Linguistics, 2020. Wenhan Xiong, Mo Yu, Shiyu Chang, Xiaoxiao Guo, and William Yang Wang.Improving question answering over incomplete kbs with knowledge-aware reader.In ACL (1), pages 42584264. Association for Computational Linguistics, 2019.",
  "GHMFC": ": Case study for MEL. Each row is a case, which contains mention, ground truth entity, and top three retrieved entitiesof three methods, i.e., MIMIC (ours), GHMFC , CLIP . The italic and underlined words in mention are mention words.Each retrieved entity is described with three parts, Wikidata QID, entity name, a short description, and three parts are separatedby \"|\". A blank square means that the corresponding entity has no image. The symbol \" \" marks the correct entity.",
  "DZMNED is the first method for MEL, which utilizes addi-tional attention mechanism to fuse visual features, word-leveltextual features and char-level features": "JMEL extracts both unigram and bigram embeddings as tex-tual features. Different features are fused by concatenation anda fully connected layer. We replace the textual encoder with apre-trained BERT for a fair comparison. VELML utilizes VGG-16 network to obtain object-level vi-sual features. We use pre-trained BERT to replace the originalGRU textual encoder. The two modalities are fused with addi-tional attention mechanism. GHMFC proposes hierarchical cross-attention to capturethe underlying fine-grained correlation among textual and visualfeatures and uses contrastive learning for optimization.The third group of baselines includes Vision-and-Language Pre-training models. CLIP employs two Transformer-based encoders to attainvisual and textual representation, which pre-trains on massivenoisy web data with contrastive loss.",
  "ViLT proposes to use shallow textual and visual embeddings,and concentrates on deep modality interaction via a stack ofTransformer layers": "ALBEF first aligns visual and textual features with image-text contrastive loss and then fuses them with a multimodalTransformer encoder. Momentum distillation is further appliedto improve learning from noisy data. METER utilizes the co-attention schema to exploit the se-mantic relation of different modalities, where each layer consistsof a self-attention module, cross-attention module and a feed-forward network.",
  "A.4Case Study": "For a more illustrative demonstration of the proposed MIMIC, weprovided two cases and compared MIMIC with two strong competi-tors, i.e., GHMFC and CLIP , which is shown in .In the first case, although three methods predict the correct entityin the top three retrieved entities, MIMIC distinguishes better be-tween space shuttle and space capsule by capturing the detailedinformation within the mention image. In the second case, twocompetitors retrieve rock band Bush in the first place. MIMIC notonly considers textual clues Bush from the surface but also takesthe visual scene of politics from the images into account, whichhelps to identify the correct entity."
}