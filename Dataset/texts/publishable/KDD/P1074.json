{
  "ABSTRACT": "The core challenge of de novo protein design lies in creating proteinswith specific functions or properties, guided by certain conditions.Current models explore to generate protein using structural andevolutionary guidance, which only provide indirect conditions con-cerning functions and properties. However, textual annotations ofproteins, especially the annotations for protein domains, whichdirectly describe the proteins high-level functionalities, properties,and their correlation with target amino acid sequences, remainunexplored in the context of protein design tasks. In this paper,we propose Protein-Annotation Alignment Generation (PAAG), amulti-modality protein design framework that integrates the textualannotations extracted from protein database for controllable gener-ation in sequence space. Specifically, within a multi-level alignmentmodule, PAAG can explicitly generate proteins containing specificdomains conditioned on the corresponding domain annotations,and can even design novel proteins with flexible combinations ofdifferent kinds of annotations. Our experimental results underscorethe superiority of the aligned protein representations from PAAGover 7 prediction tasks. Furthermore, PAAG demonstrates a sig-nificant increase in generation success rate (24.7% vs 4.7% in zincfinger, and 54.3% vs 22.0% in the immunoglobulin domain) in com-parison to the existing model. We anticipate that PAAG will broaden",
  "Work was done when Chaohao Yuan worked as an intern at Tencent AI Lab.Project Lead.Corresponding Author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from acronym XX, June 0305, 2018, Woodstock, NY 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-XXXX-X/18/06",
  "Annotation-guided protein design, multi-modality alignment": "ACM Reference Format:Chaohao Yuan, Songyou Li, Geyan Ye, Yikun Zhang, Long-Kai Huang,Wenbing Huang, Wei Liu, Jianhua Yao, and Yu Rong. 2018. Annotation-guided Protein Design with Multi-Level Domain Alignment. In Proceedingsof Make sure to enter the correct conference title from your rights confirmationemai (Conference acronym XX). ACM, New York, NY, USA, 12 pages.",
  "INTRODUCTION": "Protein design is a crucial task for its immense potential ondrug discovery , enzyme engineering , immunongineer-ing and so on. The generation of proteins with specific proper-ties, behaviors, or functions, such as optimizing the binding affinityto given molecules or incorporating a particular ion-bindingsite , is known as de novo protein design. This process presentsa significant challenge due to the vast space of protein sequencesand the complexity of protein functions. Recently, machine learn-ing models have shown profound potential for protein design. Theexisting studies mostly rely on the structural or evolutionaryinformation as the guidance to design proteins. However, inmany cases, these conditions can only offer indirect guidance to-wards the desired protein design targets to their inherent ambiguity.For example, the same protein sequence segment can be either actas receptors to regulate synaptic function or helpers to locatetarget proteins to specific subcellular locations .In addition to the structural and evolutionary information, thecurrent protein dataset, such as Swiss-Prot and UniProtKB ,",
  "Conference acronym XX, June 0305, 2018, Woodstock, NYTrovato and Tobin, et al": ": (a) The example of property annotations (in bold)and domain annotations (in colors). (b). The illustration ofannotation-guided protein design with PAAG. Given the in-put of textual description within immunoglobulin domainannotation, PAAG can generate the proteins containing im-munoglobulin domain. contains rich textual annotations derived from wet laboratory ex-periments and literature. illustrates an example of thetextual annotations on the zinc-finger protein. Generally, these an-notations can be categorized into property annotation and domainannotation. Property annotation represents a piece of text that de-picts the global property of proteins, such as protein names, numberof amino acids, subcellular localization and thermostability .Conversely, domain annotation pertains to the knowledge derivedfrom the local domain of proteins, which is a subregion ofamino acid sequence that is self-stabilizing and represents certainstructural and functional aspects of the protein. These annotationsprovide both coarse and fine-grained information regarding pro-teins functions, properties, and interactions, thereby encompassingknowledge with the potential to guide the generation and designof novel proteins.For instance, as one of crucial functional domains of DNA/RNAbinding proteins , the zinc-finger domain naturally has manyvariants, such as C2H2 type, CCHC type and Zinc ribbon type.These variants exhibit significant differences in both structural andevolutionary features among them which is hard covered by struc-tural and evolutionary conditions. On the contrary, the zinc-fingerannotation from the protein database inherently provides a moreeffective means of describing the high-level knowledge span acrossboth sequences and structures. Hence, we aim to investigate thefollowing question: Is it possible to leverage such textual annotationsto guide the delicate controllable protein design?Recently, several primary attempts have been made to leveragesuch textual annotations to guide the protein generation. Exam-ples include training an individual protein caption model to guidethe diffusion generation process , and incorporating the anoverall text description through a global language-to-protein align-ment model . However, current models cannot flexibly combinethe different conditions and lack of the capability for fine-grainedcontrol, such as specifying the generation of particular domains.To fill these gaps, in this paper, we introduce a novel framework,Protein-Annotation Alignment Generation(PAAG), that enablesannotation-guided protein design by aligning protein sequences with their textual annotations. Specifically, we first consider bothproperty and domain annotations in proteins and design a multi-level alignment module to align the representations of sequencesand annotations extracted from the existing encoders in both globaland local level. For the generation task, PAAG utilizes an autoregres-sive decoder to generate protein sequences guided by the alignedrepresentation of textual annotations. Additionally, PAAG employsan end-to-end training pipeline that joint training of alignmentand generation tasks without freezing the parameters in sequenceand text encoders. This joint training enhances the understandingof the complex and flexible annotation condition, resulting in im-proved guided generation. demonstrates an example ofannotation-guided generation. In experiments, we first investigatethe quality of protein representations from PAAG by seven predic-tive downstream tasks. PAAG surpasses state-of-the-art baselinewith an average relative improvement of 1.5%. Subsequently, threeprotein generation tasks are conducted to assess the capabilities ofPAAG. In the case of unconditional generation, PAAG produces se-quences exhibiting the highest degree of novelty while maintainingthe distribution of natural proteins. For the two conditional proteindesign tasks, PAAG demonstrates a nearly threefold increase ingeneration success rate (24.7% vs 4.7% in zinc finger, and 54.3%vs 22.0% in the immunoglobulin domain)1 in comparison to theexisting model. Our contributions are summarized as follows: We propose the first annotation-guided protein design paradigm,integrating local and global annotation information. Our pro-posed framework PAAG is the first approach that can generateproteins containing specific domains, guided by their correspond-ing annotations with high success rate. PAAG features a multi-level alignment module for handling anno-tation and protein data alignment at various granularities. Jointtraining of alignment and generation tasks allows the model toproduce improved protein representations, consequently boost-ing performance in predictive and generative tasks. Comprehensive experiments on 7 predictive and 3 generativetasks showcase PAAGs superiority compared to the existingmethods. Notably, PAAG is not only capable of generating pro-teins that include a single annotation, but it also successfullygenerates proteins adhering to the flexible conditions of multipleannotation combinations.",
  "PRELIMINARIES2.1Protein and Its Textual Annotations": "The primary structure of a protein can be represented as an aminoacid sequence = (1,2, ,), where is the -th amino acidschosen from 20 different amino acids which are represented as20 characters. Given a protein , the annotation-sequence pair setD = {(,:)}=1 is constructed by extracting the correspon-dence between textual annotations and protein domains & prop-erties from protein database, such as UniProtKB , where isthe textual annotations of the -th domain : of the protein and is the number of annotation-domain pairs of the protein . Forthe property annotation, the corresponding domain is the entiresequence, i.e, 1:| |. This annotation-domain pair set D provides",
  "Encoders for Proteins and Annotations": "We adopt a protein encoder (PE) to generate both the protein and itsdomain representations based on the amino acid sequence as =PE(), where PE is the protein encoder and is the embeddingof . For textual annotations, we generate their representationsusing the pre-trained language model LM as A = LM((A)),where A { |(,:) D} is a subset of the annotations inthe annotation-domain pair set for a protein , () is a templatefunction which converts a set of annotations to a textual description,the details of () can be found in Appendix A.2, and A is theembedding of the annotation set. Note that the annotation subsetcan cover all annotations in D or just one. If A covers onlyone annotation 0, we can skip the template function and directlygenerate the embedding of 0 using LM.We employ transformer-based protein encoder and text encoderas our base model and initialize them with pre-trained models.Specifically, we employ SciBERT, which is pre-trained on com-puter science and biological datasets, to initialize the text encoderLM. The pre-trained protein encoder, ProtBERT, is used toinitialize the protein encoder PE.",
  "Multi-level Protein and AnnotationAlignment": "We will first employ template function to translate these annota-tions as textual descriptions and then utilize a language model toextract the representation of the annotation set A = LM((A)).Then the protein sequence is generated using a decoder basedon this representation as = D(A).To better integrate the information between proteins and anno-tations, we aim to align the multi-level representations of proteinsand annotations. Specifically, we conduct local alignment and globalalignment by performing contrastive learning at domain level andprotein level, respectively.We measure the alignment score using the cosine similaritybetween the embeddings of protein and the annotation set, whichis defined as",
  "(A,) =ProjA (A), Proj ()ProjA (A)Proj () ,(1)": "where Proj() = + projects the input into a latent spacewith dimension . R|| and R are trainable parame-ters. During alignment, we encourage the matched (positive) pairsto have representations with higher similarity than unmatched(negative) pairs. We next will explain in detail how to construct thepositive and negative pairs in our multi-level framework. 3.1.1Local Alignment. Given a protein, and its annotation-domainpairs set D, in domain-level, we aim to align the representationof the domain : and its corresponding annotation and use allannotation-domain pairs in D as positive pair. To construct thenegative pairs, we randomly sample the sub-regions outside thedomain :, i.e. 1:1 +1:, as the negative samples : of thedomain.Annotation-Domain Contrastive(ADC) Loss: In designingthis loss, since our objective is on identifying functional regionswithin proteins, we consider amino acid sequences from the sameprotein as negative samples. Specifically, we adopt InfoNCE lossas the ADC loss to align the representations of local annotation and functional domain : as",
  "=1 exp((,: )/),(2)": "where is the number of functional domains for the protein , is the number of negative samples, and is a learnable temperatureparameter.The local alignment enables PAAG to explicitly learn the relationbetween the functional domain and its annotation. Therefore, wecan use the model to controllably generate the specific functionaldomain given annotations. 3.1.2Global Alignment. To enable global alignment, we utilize tem-plate function () to construct protein-level textual descriptionand form the positive protein-description pairs as {((A),)}.Since multiple annotations and the entire protein will be morecomplex, to enlarge the number of negative samples for globalalignment, we follow the setting of MOCO to construct mo-mentum encoders as:",
  "+ (1 ) .(3)": "where the encoder can be either protein encoder PE or text en-coder LM, and is the momentum hyperparameter. We followimplementation details in and to construct the momen-tum encoders for both encoders. The momentum encoders extractconsistent features to increase the number of negative samples, andthe dynamic dictionaries will store these features.Annotation-Protein Contrastive (APC) Loss is designed toalign the representations of global properties A with protein .Specifically, for each protein sequence and annotation set, we calcu-late the softmax-normalized sequence-to-annotation and annotation-to-sequence similarity as:",
  "=1 exp((,)/),(5)": "where is a learnable temperature parameter, and indicatetheir representations will be extracted by respective momentumencoders.Denote a2s() and s2a() as the ground-truth one-hot simi-larity, where negative pairs have a probability of 0 and the positivepair has a probability of 1. The annotation-protein contrastive lossis defined as the cross-entropy H between and :",
  "L": ": The overall framework of PAAG. The same parameters share the same color. PAAG contains three modules. (1)Protein & Annotation Encoding module encode the input protein sequence & domains and corresponding annotations to theembeddings. (2) Multi-level alignment module projects the protein and annotation embeddings into and employs Annotation-Protein Contrasive (APC) loss, Annotation-Domain Contrasive (ADC) loss and Annotation-Protein Matching (APM) loss toalign them in a same latent space. (3) Conditional Protein Decoding accepts the annotation embedding as input and generatethe protein sequence.",
  "+H( a2s(), a2s()).(6)": "Through APC loss, PAAG aligns the representations of twomodalities, improving the quality of aligned representation, whichis crucial for downstream tasks such as classification and regression.Annotation-Protein Matching (APM) Loss. Inspired by ,we introduce a multi-modal encoder ME, which integrates therepresentation of annotations and protein as ,A = ME(,(A)),to identify whether the given pairs of protein description(A) andprotein are matched or not. Specially, the multimodal encoder MEis transformer-based and shares the parameters of self-attentionand feed-forward layers with PE and has additional cross-attentionlayers between self-attention layers and feed-forward layers tointegrate the text information. The cross-attention layers share thesame cross-attention parameters as the decoder.Annotation-Protein Matching (APM) Loss aims to facilitate thelearning of multimodal representations. Additionally, we use thehard negative strategy, where we first select the most similar neg-ative pairs and then use these most challenging negative pairsto optimize the model through in the training of the model withthe APM loss, enabling the encoder to learn informative represen-tations. To construct the APM loss, we extract the multi-modalrepresentation as ,A = ME(,(A)) and use a classifier to clas-sify its probability of being positive or negative, which is denotedby APM(A,). And we compute the cross-entropy between theground-truth label APM(,) indicating the protein-descriptionpair being positive or negative to obtain the APM loss as",
  "Conditional Protein Decoding": "Protein Decoder. Our ultimate goal is generating functional pro-teins conditioned on a set of annotations A = {}=1. We adoptan auto-regressive protein decoder D that can receive the conditionfrom the language model. Specifically, the decoder will first pro-cess the protein sequence through causal attention layers to enableauto-regressive generation, then integrate the information fromannotations via cross-attention layers followed by feed-forwardlayers. Note that the causal attention layers are initialized using thesame weights as the self-attention layers in the protein encoder andthe feed-forward layers share the same parameters as the proteinencoder. Here, the parameter-sharing mechanism enables highertraining efficiency . The cross-attention layers are randomlyinitialized and trained from scratch.Protein Modeling (PM) Loss. To guide the learning of themodel, we estimate the PM loss as",
  "Training Objectives": "In the training of PAAG, we optimize four objective functions.These functions are designed to align the representations betweenannotations and protein sequences, integrate the two modalities,and reconstruct the protein sequence. In contrast to ProteinDT ,which splits the training process into three separate stages, PAAGjointly optimizes these four objective functions in an end-to-endmanner. The overall pretraining objective of PAAG is :",
  "where M () is metric function for the annotation": "In this paper, we mainly focus on two type of metrics, functional-domain metric and global-property metric. Functional-domain metric: for the annotation describing acertain functional domain, M () will invoke a profile hiddenMarkov models from Pfam to search for the optimal matchwithin protein regarding to the domain described by annota-tion and assign an e-value to this match. Given an e-valuethreshold , we have:M, () = 1 ( < ),(11)",
  "Construction of ProtAnnotation Dataset": "To enable multi-level alignment, we build the ProtAnnotation datasetwith annotation-sequence pair set for protein design task. Specifi-cally, we select the proteins with Domain entry in UniProtKB to build ProtAnnotation dataset.However, since the biases within our training dataset will sig-nificantly impact the performance of PAAG, we limit our selectionto protein-annotation pairs from a refined subset of UniProtKB,namely Swiss-Prot. Each entry in Swiss-Prot has been manuallyreviewed and supplemented with detailed information with proteinfunction, structure, and interactions. Ultimately, the ProtAnnota-tion consists of 129,727 proteins. Moreover, in our alignment, weincorporate temperature to control the similarity scores ofdistribution between positive and negative pairs. An appropriatetemperature can soften these scores, making the model less sensi-tive to noisy samples. This adaptability allows the learning processto focus more on meaningful correlations and reduce the impact ofirrelevant variations, ultimately enhancing the PAAGs robustness. The domain annotations are extracted by these Domain entriesincluding the domain description and start & end index of thisdomain. Additionally, we select four properties as the propertyannotations, that is, protein_name, organism_name, lengthand SIMLARITY. The textual description is assembled by thetemplate function , which can accept any subset of annotationas input. More details, including the data examples are deferred inAppendix A.2.",
  "Quality of Aligned Representation": "We first conduct multiple experiments on predictive tasks, to evalu-ate the quality of protein representation produced by PAAG.Settings: To make a fair comparison with ProtST, we use thesame proteins in dataset ProtDescribe to first pretrain PAAG,followed by full-model fine-tuning on various downstream tasks.For the full-model fine-tuning, we add a task head for each task andfine-tune the model for 100 epochs. We use the validation set toselect the model and report the results on random seed 0, adheringto the same settings as in ProtST . More details are deferred inAppendix A.6.Benchmark Tasks: We adopt 7 downstream tasks within twotask types as the benchmark task. Protein Localization Prediction aims to forecast the subcellu-lar locations of proteins. Derived from DeepLoc , we focus ontwo similar tasks. The subcellular localization prediction (Abbr. as,Sub) encompasses 10 location categories and binary localizationprediction (Abbr. as, Bin) that includes 2 location categories, sol-uble and membrane-bound. The splits of data follow the originalsplit in DeepLoc. Fitness Landscape Prediction is primarily focused on theprediction of the effects of residue mutations on the fitness ofproteins. We evaluate our models on -lactamase (Abbr., -lac)landscape from PEER , the AAV and Thermostability (Abbr.,Thermo) landscapes from FLIP , and the Fluorescence (Abbr.,Flu) and Stability (Abbr., Sta) landscapes from TAPE . Thesplits of data follow the splitting setting of ProtST Baselines: We adopt two types of baseline. The first type isthe models trained from scratch, including CNN , ResNet ,LSTM , Transformer . The second type is the pre-trainedmodels with full model tuning, including OntoProtein , Prot-Bert and ESM2 . For the ProtST and PAAG, we train twovariants with different initialization weights from ProtBert andESM2 to verify the enhancement of text knowledge on protein rep-resentations from different protein encoders. We utilize distinct sub-scripts to denote the initialized parameters, such as PAAGProtBertand PAAGESM2.Results: reports the results of all models on seven base-lines. As illustrated in , PAAG achieves superior performancein comparison to the baselines on all 7 tasks. These results high-light the robust generalization capabilities of PAAG for downstreamtasks. Specifically, PAAG outperforms the vanilla pretrained models,ProtBert and ESM2, in all cases, indicating that PAAG can furtherenhance the quality of protein representation by incorporating theknowledge from textual annotations. According to the results in, multi-modal training has a positive influence on the perfor-mance of downstream tasks in both ProtST and PAAG. Additionally,",
  "Unconditional Protein Generation": "To verify the learning effect of the decoder, we compare the abil-ity of different models in unconditional generation task. A gooddecoder is able to generate protein sequences that conform to thedistribution of the training set while simultaneously exhibitingadequate novelty.Setting: In unconditional generation task, we only specify thelength of generated proteins and conditions. We sample the samelength from natural proteins, to ensue a fair comparisons acrossdifferent models.Baselines: We compare PAAG with 3 representative proteindesign models, i.e., ProGen , Chroma and ProteinDT .Furthermore, we introduce two naive baselines: RandomUniformand RandomEmpirical. RandomUniform generates protein sequenceby randomly selecting amino acids based on a uniform distribution,while RandomEmpirical adheres to the empirical amino acid distri-bution in the training dataset. Additionally, we report the results ofthe sequence set sampled from natural proteins, denoted as Natural,to serve as a reference. The details of baselines are in Appendix A.5.Evaluation metrics: To evaluate the quality of generated pro-tein sequences, we employ three metrics: Distinct-n, Diversity andNovelty. Suppose S is the protein sequence set. Distinct-n is a classical metric in natural language process-ing that measures textual diversity of generated text by countingdistinct n-grams. We use normalized Distinct-n to assess the frac-tion of repetitive sequence motifs in sequences from S, whichexhibits the biological importance . A higher Distinct-n sug-gests fewer repetitive amino acid segments. We set = 2 here. Diversity measures the dissimilarity of sequences in S. We em-ploy Mmseq2 to compute the dissimilarity between eachpair of sequences, and utilize the mean of these dissimilarities asDiversity. A higher Diversity signifies a greater diversity in S.",
  "PAAG exhibits the closest Distince-n score compared with naturalproteins, which proves that proteins generated by PAAG possesssimilar amino acid distribution as natural proteins": "Since RandomEmpirical generates protein sequence following theempirical amino acid distribution in the natural proteins, it isreasonable to obtain the closest diversity to natural proteins.Compared with other learning-based model, PAAG still maintainsthe closest diversity to natural proteins proving PAAG has thesimilar distribution at protein level. ProGen and Chroma have considerably lower Distince-n scorein comparison to PAAG and natural proteins, implying an abun-dance of repetitions within generated protein sequence. WhileProteinDT has higher Distince-n score, it also fails to capture theintrinsic amino acid distrbution in natrual proteins.",
  "Protein Design with Domain Annotations": "In this section, we evaluate the performance of PAAG in generatingproteins under the given domain annotations.Settings: We utilize two biologically significant domains, zinc-finger domain and immunoglobulin domain , as the targetdomain annotation to generate the proteins respectively. For eachcase, we generate = 300 protein sequences given the length of theproteins and the annotation set containing the domain annotationsof zinc-finger or immunoglobulin domain. For all models, theprotein sequences are generated with the same length sampled fromnatural proteins that have corresponding domain in UniprotKB. Tofurther evaluate the generalization ability of PAAG, we furthertest our model on generating proteins with EGF-like domain. Thedetails can be found in Appendix A.7.1.Based on (11), we further define a metric: success rate SR =, ()",
  "Ramdom": "(d) Immunoglobulin Domain : Figure (a) and (b) show the SR on zinc-finger domain and immunoglobulin domain over all models. Figure (c) and(d) show their distributions of e-value. White bar indicates the mean e-value of each set. PAAG consistently exhibits betterperformance on all metrics compared with other models. Fine-tuning also introduces additional improvement for PAAG.",
  ": Visualization of the generated results on zinc-finger and immunoglobulin domain. The corresponding prompt andgeneration qualify (e-value) is listed below": "Model training: We train PAAG on ProtAnnotation for total100 epochs, employing a learning rate of 3e-5 and incorporating awarm-up phase with a batch size of 32. Additionally, we extract asubset of ProtAnnotation which only contains proteins with zinc-finger and immunoglobulin domain. This subset is subsequentlyutilized to fine-tune the model over 5 epochs, adopting a learningrate of 1e-5 and maintaining the batch size at 32. We denote thisfine-tuned version as PAAGft, and the PAAG without fine-tuningis denoted as PAAG. The generalization ability of PAAGft is alsoevaluated in A.4. More details are deferred to Appendix A.7.2.Baselines: We adopt ProtenDT , Chroma and Pro-Gen as the baselines due to their public availability of modelweights and their capacity to accept text and keywords as con-ditional inputs for protein generation. ProGen utilizes keywordsas its condition tags. Here, we take the keywords correspond tozinc-finger (KW-0863) and immunoglobulin domain (KW-0393)in Uniport to generate functional proteins. Chroma adopts ProCapto understand the textual conditions to guide its diffusion process.We give the same text prompts to Chroma to enable its controllablegeneration. We also include two trivial baselines RandomUniformand RandomEmpirical as the blank references.Results: In (a) and (b), we plot the curves depictingthe variation in success rate SR for different models in two pro-tein design tasks as the quality threshold varies from 0.001 to100. Additionally, in (c) and (d), we employ violin plotsto illustrate the distribution of e-value for generated sequencesmatched in Pfam. Due to Pfams limitations, e-values exceeding 100are adjusted to the maximum e-value score among all method re-sults for the current task. We also show the distribution of e-values",
  "ProGen and ProteinDT outperform other baseline methods. Thismay be due to they memorizing proteins with zinc and lg domainin the training data and output them when using keywords ortextual prompts": "Chromas performance is not good, resembles the unconditionedresults. This may be because Chromas training text primarilyfocuses on structural descriptions, making it less sensitive to thedomain annotations.Visualizations: We provide the visualization of generated pro-teins, folded by Omegafold , in . The figure 4 highlightsgenerated domains in red and provides textual descriptions ande-value for each sequence. We observe the generated sequencesaccurately produce target domains as specified in the annotationset. Interestingly, in scenarios such as two zinc finger cases, whenthe prompt specifies the presence of multiple zinc finger domains,PAAG generates multiple functional domains in response. However,PAAG fails to capture the precise numbers of these domains, whichcan be a direction for future improvement.Prompt: We further investigate the relationship between thenumber of domains in prompts and the proteins generated by PAAG.By varying only the domain count in prompts, we generate 900",
  ": The relation of number specified in prompt withgenerated domains by PAAG": "proteins with Small (1-3), Median (4-6), and Large (7-9) numberof domains. shows that increasing domain numbers inprompts leads to a corresponding rise in generated domains acrossdifferent e-values, indicating that PAAG can discern and gener-ate the specified number of domains from text prompts throughmulti-level alignment. Given these observations, another interest-ing question is whether multiple domains in prompt will increasesuccess rates. To this end, we re-evaluate PAAG using prompts spec-ified only one domain (PAAGsingle domain). The results are in in Appendix. We can find while multiple domains improve successrates, PAAGsingle domain also outperforms the baselines, confirmingthe importance of multi-level alignment.",
  "Protein Design with Property Annotations": "In this section, we explore the potential of PAAG to generate pro-teins with certain properties guided by property annotations.Settings: We employ the subcellular location of proteins as anexample property. An additional dataset, termed ProtLocation, isgenerated by extracting subcellular location labels, encompassingBin (2-class) task as delineated in .2, from Deeploc .These labels are then incorporated into annotation-sequence pairsderived from Uniprot. ProtLocation includes 10100 proteins fortraining, while a separate set of 2434 test proteins is reserved forconstructing the annotation set for generation. More details can befound in Appendix A.6.3.Evaluation protocol: For the generated sequences, we employthe official server provided by Deeploc as the pre-defined oracleto construct the Global-property metric M () for predicting binarylocation label. A generation is deemed successful if the predictedlabel aligns with the input annotation label.Results: As shown in , PAAG achieves overall 74.78% suc-cess rate, indicting PAAG captures the difference between solubleand membrane-bound, and can generate properties given corre-sponding property annotations. We next will move to more chal-lenging setting, generating proteins with both domain and propertyannotations.",
  "Case study: Joint Generation with Domainand Property Annotations": "In this part, we explore the potential of PAAG in generating proteinsguided by the flexible combination of different annotations. Specifi-cally, we generate the zinc-finger proteins in membrane-bound andsoluble by the model in .5 following the same protocol in.4. Despite this challenging setting, the joint success rateSR1 of generating proteins with zinc-finger and corrected propertyis SR1 = 10.17%, However, given the current success rates of othermodels in generating zinc-finger proteins have approached zero, thetask of producing such proteins with specific domain and propertyannotations remains unattainable by existing models. Zinc-fingerdomains are naturally occurring soluble proteins involved in DNAediting. Our generation of zinc-finger proteins anchored to themembrane underscores the efficacy of PAAG in producing novel,non-existent proteins. We showcase four examples of the generatedresults in . As shown in , PAAG can successfullygenerate proteins guided by both domain and property annotations,demonstrating its potential in complex protein design tasks.",
  "Ablation Study": "To ascertain the contribution of each component towards the gener-ation of the functional proteins, the ablation study reports the SR1of zinc-finger and immunoglobulin domains in the absence of eachalignment loss, as presented in . We observe that L is thekey to the high SuccessRate of PAAG. The SuccessRate decreases to0% without L, underscoring the importance of incorporatingdomain range into the learning framework. Furthermore, Land L also enhance the SuccessRate of generating high-qualityimmunoglobulin domains by 4.33% and 3.33%, respectively. More-over, L and L are more important to zinc-finger domain,by improving the SR1 by 12% and 9.67%.",
  "RELATED WORK": "Moltimodal Representation Learning. By harnessing the po-tential of extensive image-text pair data, the Contrastive Language-Image Pretraining (CLIP) model, as proposed by Radford et al. ,employs contrastive learning to align the representations betweenimage and text modalities.Following by CLIP, many image-textpertaining model are proposed, such as BILP , BLIP-2 ,InstructBLIP and ClipCap . Beyond the image-text pre-training, several studies introduce the more modalities, such asvideos , audios and even molecules into a unifiedrepresentation. Specifically, for multimodal learning on protein se-quences, OntoProtein first learns protein representations bycombining them with textual descriptions in a knowledge graph.ProtST constructs a large-scale dataset containing alignedpairs of protein sequences and property descriptions, and pretrain",
  ": Visualization of jointly generation with domain and property annotations. PAAG is capable of integrating domainand property annotations": "a protein-biotext model to improve performance on downstreampredictive task and enables zero-shot retrieval.Protein Generation Model. With huge success of languagemodels , several studies treat protein sequencesconsisting chains of amino acids as a type of languages and pre-train models on millions of protein sequences. Upon the pretrainedmodel, they generate the protein sequences in an autoregressivemanner. In addition to the autoregressive model, Evodiff ex-tracts the evolutionary information from protein sequences andproposes an evolution-guided diffusion model to generate proteinsequences. Given the significance of structural information for pro-tein function, a group of methods , known as inversefolding, utilize the structure as a conditional input, allowing for thegeneration of amino acid sequences. Some studies integrateboth sequential and structural information and propose a co-designmodel that accepts sequences and structures as conditions. Recently,ProteinDT first proposes a text-sequence alignment framework,enabling its capabilities for text-guided protein generation and edit-ing. Chroma trains a protein caption model ProCap and utilizeit as a classifier guidance to generate proteins via a diffusion model.Although ProteinDT and ProtST similarly align the pro-tein and textual representations, the multi-level alignment frame-work enables PAAG to capture the both local and global propertiesof the protein. Furthermore, the momentum encoders and an ad-ditional matching loss assist PAAG to more effectively align theprotein-level annotation with the proteins.",
  "CONCLUSION": "This paper presents PAAG, a multi-modality framework that firstincorporates the rich annotation information derived from proteindatabase, achieving the superior performance in various applica-tions, such as representation learning and annotation-guided pro-tein design. Crucially, we demonstrate that it is possible to use theflexible combinations of various kinds of textual annotations toguide the protein design process. We hope that PAAG will expandthe possibilities of protein design and establish a robust foundationfor future advancements in protein-related applications. Futureresearch directions include functional protein editing, co-design ofprotein sequence-structure within alignment frameworks, and ex-ploring the potential of larger protein dataset with lower annotationquality. The authors would like to thank Siying Xu (China Academy of Art)for her help in visualizing the framework and results. This work wasjointly supported by the following projects: the National NaturalScience Foundation of China (No. 62376276); Beijing Nova Program(No. 20230484278); Beijing Outstanding Young Scientist Program(No. BJJWZYJH012019100020098); the Fundamental Research Fundsfor the Central Universities, and the Research Funds of RenminUniversity of China (23XNKJ19). Sarah Alamdari, Nitya Thakkar, Rianne van den Berg, Alex Xijie Lu, Nicolo Fusi,Ava Pardis Amini, and Kevin K Yang. 2023. Protein generation with evolutionarydiffusion: sequence is all you need. bioRxiv (2023), 202309.",
  "SarahAlamdari,NityaThakkar,RiannevandenBerg,AlexX.Lu, Nicolo Fusi, Ava P. Amini, and Kevin K. Yang. 2023.Pro-teingenerationwithevolutionarydiffusion:sequenceisallyouneed.bioRxiv(2023)": "Jos Juan Almagro Armenteros, Casper Kaae Snderby, Sren Kaae Snderby,Henrik Nielsen, and Ole Winther. 2017. DeepLoc: prediction of protein subcellularlocalization using deep learning. Bioinformatics 33, 21 (2017), 33873395. Miguel A Andrade, Chris P Ponting, Toby J Gibson, and Peer Bork. 2000.Homology-based method for identification of protein repeats using statisticalsignificance estimates. Journal of molecular biology 298, 3 (2000), 521537.",
  "The UniProt Consortium. 2022.UniProt: the Universal Protein Knowl-edgebase in 2023.Nucleic Acids Research 51, D1 (11 2022), D523D531. arXiv:": "Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, WeishengWang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. InstructBLIP: TowardsGeneral-purpose Vision-Language Models with Instruction Tuning. In Thirty-seventh Conference on Neural Information Processing Systems. Christian Dallago, Jody Mou, Kadina E Johnston, Bruce J Wittmann, NicholasBhattacharya, Samuel Goldman, Ali Madani, and Kevin K Yang. 2021. FLIP:Benchmark tasks in fitness landscape inference for proteins. bioRxiv (2021),202111.",
  "Noelia Ferruz, Steffen Schmidt, and Birte Hcker. 2022. ProtGPT2 is a deepunsupervised language model for protein design. Nature communications 13, 1(2022), 4348": "Yaron Geffen, Yanay Ofran, and Ron Unger. 2022. DistilProtBert: a distilled proteinlanguage model used to distinguish between real proteins and their randomlyshuffled counterparts. Bioinformatics 38, Supplement_2 (2022), ii95ii98. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Mo-mentum contrast for unsupervised visual representation learning. In Proceedingsof the IEEE/CVF conference on computer vision and pattern recognition. 97299738. KN Houk, Andrew G Leach, Susanna P Kim, and Xiyun Zhang. 2003. Bindingaffinities of hostguest, proteinligand, and proteintransition-state complexes.Angewandte Chemie International Edition 42, 40 (2003), 48724897. Chloe Hsu, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, AdamLerer, and Alexander Rives. 2022. Learning inverse folding from millions ofpredicted structures. In International Conference on Machine Learning. PMLR,89468970.",
  "A Klug and D Rhodes. 1987. Zinc fingers: a novel protein fold for nucleic acidrecognition. In Cold Spring Harbor symposia on quantitative biology, Vol. 52. ColdSpring Harbor Laboratory Press, 473482": "Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016. ADiversity-Promoting Objective Function for Neural Conversation Models. InProceedings of the 2016 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies, Kevin Knight, AniNenkova, and Owen Rambow (Eds.). Association for Computational Linguistics,San Diego, California, 110119. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrappinglanguage-image pre-training with frozen image encoders and large languagemodels. arXiv preprint arXiv:2301.12597 (2023).",
  "Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. BLIP: BootstrappingLanguage-Image Pre-training for Unified Vision-Language Understanding andGeneration. In ICML": "Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong,and Steven Chu Hong Hoi. 2021. Align before fuse: Vision and language repre-sentation learning with momentum distillation. Advances in neural informationprocessing systems 34 (2021), 96949705. Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, NikitaSmetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et al. 2023. Evolutionary-scale prediction of atomic-level protein structure with a language model. Science379, 6637 (2023), 11231130. Shengchao Liu, Yanjing Li, Zhuoxinran Li, Anthony Gitter, Yutao Zhu, JiaruiLu, Zhao Xu, Weili Nie, Arvind Ramanathan, Chaowei Xiao, Jian Tang, HongyuGuo, and Anima Anandkumar. 2023. A Text-guided Protein Design Framework.arXiv:2302.04611 [cs.LG] Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, LingLiu, Jian Tang, Chaowei Xiao, and Anima Anandkumar. 2022. Multi-modalmolecule structure-text model for text-based retrieval and editing. arXiv preprintarXiv:2212.10789 (2022). Shengchao Liu, Yutao Zhu, Jiarui Lu, Zhao Xu, Weili Nie, Anthony Gitter, ChaoweiXiao, Jian Tang, Hongyu Guo, and Anima Anandkumar. 2023. A text-guidedprotein design framework. arXiv preprint arXiv:2302.04611 (2023). Ali Madani, Ben Krause, Eric R Greene, Subu Subramanian, Benjamin P Mohr,James M Holton, Jose Luis Olmos, Caiming Xiong, Zachary Z Sun, Richard Socher,et al. 2023. Large language models generate functional protein sequences acrossdiverse families. Nature Biotechnology 41, 8 (2023), 10991106.",
  "Liam R Marshall, Oleksii Zozulia, Zsofia Lengyel-Zhand, and Ivan V Korendovych.2019. Minimalist de novo design of protein catalysts. ACS catalysis 9, 10 (2019),92659275": "Jaina Mistry, Sara Chuguransky, Lowri Williams, Matloob Qureshi, Gustavo ASalazar, Erik LL Sonnhammer, Silvio CE Tosatto, Lisanna Paladin, Shriya Raj,Lorna J Richardson, et al. 2021. Pfam: The protein families database in 2021.Nucleic acids research 49, D1 (2021), D412D419. T.K. Mohandas, X.-N. Chen, L.B. Rowe, E.H. Birkenmeier, A.S. Fanning, J.M.Anderson, and J.R. Korenberg. 1995. Localization of the Tight Junction ProteinGene TJP1 to Human Chromosome 15q13, Distal to the Prader-Willi/AngelmanRegion, and to Mouse Chromosome 7. Genomics 30, 3 (1995), 594597.",
  "Ron Mokady, Amir Hertz, and Amit H Bermano. 2021. Clipcap: Clip prefix forimage captioning. arXiv preprint arXiv:2111.09734 (2021)": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,Gretchen Krueger, and Ilya Sutskever. 2021.Learning Transferable VisualModels From Natural Language Supervision. In Proceedings of the 38th Inter-national Conference on Machine Learning (Proceedings of Machine LearningResearch, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, 87488763. Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Peter Chen, JohnCanny, Pieter Abbeel, and Yun Song. 2019. Evaluating protein transfer learningwith TAPE. Advances in neural information processing systems 32 (2019).",
  "Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal.2023.Any-to-Any Generation via Composable Diffusion.arXiv preprintarXiv:2305.11846 (2023)": "Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim,Helen E Eisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles,et al. 2023. De novo design of protein structure and function with RFdiffusion.Nature 620, 7976 (2023), 10891100. Fandi Wu, Yu Zhao, Jiaxiang Wu, Biaobin Jiang, Bing He, Longkai Huang,Chenchen Qin, Fan Yang, Ningqiao Huang, Yang Xiao, et al. 2024. Fast andaccurate modeling and design of antibody-antigen complex using tFold. bioRxiv(2024), 202402. Ruidong Wu, Fan Ding, Rui Wang, Rui Shen, Xiwen Zhang, Shitong Luo, Chen-peng Su, Zuofan Wu, Qi Xie, Bonnie Berger, et al. 2022. High-resolution de novostructure prediction from primary sequence. BioRxiv (2022), 202207. Zhirong Wu, Yuanjun Xiong, X Yu Stella, and Dahua Lin. 2018. UnsupervisedFeature Learning via Non-Parametric Instance Discrimination. In Proceedings ofthe IEEE Conference on Computer Vision and Pattern Recognition. Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan,Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. 2021. Video-CLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding. CoRRabs/2109.14084 (2021). arXiv:2109.14084",
  "Processing Systems 35 (2022), 3515635173": "Ningyu Zhang, Zhen Bi, Xiaozhuan Liang, Siyuan Cheng, Haosen Hong, ShuminDeng, Jiazhang Lian, Qiang Zhang, and Huajun Chen. 2022. OntoProtein: ProteinPretraining With Gene Ontology Embedding. arXiv:2201.11147 [q-bio.BM] Yikun Zhang, Geyan Ye, Chaohao Yuan, Bo Han, Long-Kai Huang, Jianhua Yao,Wei Liu, and Yu Rong. 2024. Atomas: Hierarchical Alignment on Molecule-Text forUnified Molecule Understanding and Generation. arXiv preprint arXiv:2404.16880(2024). Kangfei Zhao, Yu Rong, Biaobin Jiang, Jianheng Tang, Hengtong Zhang, Jeffrey XuYu, and Peilin Zhao. 2023. Geometric Graph Learning for Protein Mutation EffectPrediction. In Proceedings of the 32nd ACM International Conference on Informationand Knowledge Management. 34123422.",
  "AEXPERIMENTAL SETTINGSA.1General settings": "Backbone Models of PAAG: We use ProtBert-BFD to initial-ize our protein encoder and SciBert for the text encoder. Dueto limited training data, we opt for a lighter decoder initialized byDistilProtBert . Exploring a decoder with more parameters is afuture direction.Training Configurations: We train PAAG on ProtAnnotation,using the AdamW optimizer (with a learning rate of 3e-5 and zeroweight decay) for 100 epochs. Our generation experiments areconducted on 16 NVIDIA Tesla A100-SX4-40GB GPUs.",
  "The explanation of four property annotations:": "protein_name: The protein name is a naming method used todescribe the function, characteristics, or origin of a protein. Thesenames usually contain information about the proteins structure,function, substrate specificity, and biological process. organism_name: Organism names are used to identify andclassify different species of living organisms, including bacte-ria, fungi, plants, and animals. These names usually consist ofthe genus and species of the organism, and sometimes includeadditional information such as strain or cultivar. length: The number of amino acids in the protein sequence. SIMILARITY: In the context of biology and protein classification,similarity refers to the shared characteristics or features amongdifferent proteins. This can include similar structures, functions,or evolutionary origins. Proteins with high similarity are oftengrouped into the same family or subfamily.",
  "A.4The evaluation metric for theunconditional generation": "In the computation of the Distinct-n metric, we assign a value of to 2. For the Novelty metric, the default parameters of Mmseq2are employed with an exception for the e-value threshold, which isdesignated as 100. In the case of the Diversity metric, the defaultparameters of Mmseq2 are again utilized, but with alterations to thee-value threshold and sensitivity, set to 1000000 and 15 respectively.During the application of Mmseq2 for the calculation of Noveltyand Diversity metrics, the dissimilarity for unmatched sequencesis assumed to be 1.0.",
  "A.5More details of baselines": "ProGen represents each protein property as keyword tags,which are prepended to amino acid sequences during training to en-able auto-regressive reconstruction. This allows ProGen to generateprotein sequences from keyword tags. For controllable generation,ProGen translates desired properties into keyword tags to guidesequence generation.Chroma is a protein design model focusing on generatingprotein backbones. It uses classifier guidance and ProCap, a protein-to-text model, for text-guided designs. With a sequence samplingmodel, Chroma can generate both structure and sequence based onspecified conditions.ProteinDT is a text-guided protein design model that alignsprotein and text representations, then uses aligned text representa-tions to design protein sequences via autoregressive and diffusionmodels. For fair comparison, we use autoregressive ProteinDT asour baseline. In unconditional generation, due to the lack of a spe-cific template in ProteinDT, we randomly sample prompts from itsdataset to evaluate the quality of generated proteins. In conditionalexperiments, ProteinDT receives the same prompts as PAAG.In unconditional generation, PAAG accepts only the proteinsequence length and the order of protein generated, using thetemplate: protein number k, contains l amino acids. For generatingproteins with functional domains, we randomly sample organismnames, lengths, and similarities from natural proteins with thecorresponding domains. Generative hyper-parameters are specified",
  "A.6Training configurations": "A.6.1Predictive Task. In downstream prediction tasks, PAAG usesembeddings in the aligned space by employing both the proteinencoder and the projector head for more informative aligned fea-tures. Hyperparameters for predictive tasks using PAAG-ProtBertand PAAG-ESM-2 are detailed in and . A.6.2Protein Design with Domain Annotations. While trainingPAAG with ProtAnnotation, we incorporate momentum contrast with a queue size of 16,384 and momentum of 0.995. The learnabletemperature in contrastive learning is set to 0.07, and our latentspace is aligned to 256 dimensions with a linear layer. Following, we initialize learnable alpha at 0.4 for soft labeling. Whenfine-tuning on datasets with only zinc-finger and immunoglobulindomains, we maintain these hyperparameters but reduce the queuesize to 4,096 due to less training data. Additionally, weight decayfor AdamW is set at 0.05.In our generative task, we use nucleus sampling in the decoder,sampling amino acids based on probability rather than alwaysselecting the highest probability ones. The Top-p hyperparameteris set to 0.9, meaning the decoder considers the top 90% probabilitymass. We also set the decoders repetition penalty to 1.2. A.6.3Protein Design with Property Annotations. Due to limitedtraining data, we fine-tune the models from .4 for 100epochs. We retain the ProtAnnotation training hyperparametersbut lower the learning rate to 1e-5 without warm-up.For binary localization properties, soluble and membrane-bound,we set template function () as is soluble or is membrane-bound.",
  "ProGen1%1%1%1%1%1%ProteinDT0%0%0%0%0%0%PAAG28.67%28.33%26.00%22.33%17.33%11.67%": "A.7.2More evaluation on the proteins generated by fine-tunedPAAG. To ensure fine-tuned PAAG isnt just memorizing the train-ing set, we evaluate distinct-2, diversity, and novelty for uncondi-tionally generated sequences. shows that, even after fine-tuning, PAAG produces proteins with higher novelty than Chromaand ProGen, indicating it doesnt overfit, thanks to label smooth-ing. Additionally, fine-tuned PAAGs diversity is closest to naturalproteins, suggesting it captures similar amino acid distributions."
}