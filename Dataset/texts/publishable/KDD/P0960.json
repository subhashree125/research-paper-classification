{
  "ABSTRACT": "Powerful foundation models, including large language models (LLMs),with Transformer architectures have ushered in a new era of Gener-ative AI across various industries. Industry and research communityhave witnessed a large number of new applications, based on thosefoundation models. Such applications include question and answer,customer services, image and video generation, and code comple-tions, among others. However, as the number of model parametersreaches to hundreds of billions, their deployment incurs prohibi-tive inference costs and high latency in real-world scenarios. Asa result, the demand for cost-effective and fast inference using AIaccelerators is ever more higher. To this end, our tutorial offers acomprehensive discussion on complementary inference optimiza-tion techniques using AI accelerators. Beginning with an overviewof basic Transformer architectures and deep learning system frame-works, we deep dive into system optimization techniques for fastand memory-efficient attention computations and discuss how theycan be implemented efficiently on AI accelerators. Next, we describearchitectural elements that are key for fast transformer inference.Finally, we examine various model compression and fast decodingstrategies in the same context.",
  "Inference optimization, LLMs, Transformer, and foundation models": "ACM Reference Format:Youngsuk Park, Kailash Budhathoki, Liangfu Chen, Jonas Kbler, Ji-aji Huang, Matthus Kleindessner, Jun Huan, Volkan Cevher, Yida Wang,and George Karypis. 2024. Inference Optimization of Foundation Modelson AI Accelerators. In Proceedings of the 30th ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining (KDD 24), August 2529, 2024,Barcelona, Spain. ACM, New York, NY, USA, 11 pages.",
  "OVERVIEW": "The substantial size of modern large language models (LLMs),such as Llama-2/3 70B , Claude 3 Opus 137B , and Groq-1314B , presents significant challenges in both training and in-ference phases. Training LLMs, in particular, demands considerableresources and has been the subject of extensive research. In con-trast, inference consumes fewer computational resources but occursmuch more frequently once the model has been trained. This phaseis crucial as it encompasses various applications where the valueof LLMs is realized, including text translation, sentiment detection,code generation, text summarization, and question answering.Customers naturally demand faster and more cost-effective infer-ence. To meet the user demands, it is essential to reduce latencythetime required to complete a generationand to increase through-put, which is the number of requests processed per unit of time. Thelatency and throughput of LLMs depend on multiple factors, such asthe hardware utilized, the capability of software frameworks to op-timally leverage the available hardware, and the model architectureitself. Therefore, efforts to improve speed and costs benefit fromoptimizations across all these dimensions. To this end, this sectionprovides an overview of the characteristics of LLM inference, alongwith the corresponding systems and hardware requirements.",
  "KDD 24, August 2529, 2024, Barcelona, SpainYoungsuk Park & Kailash Budhathoki et al": ": Canonical knowledge distillation process by Hintonet al. , where small student model distills a large teachermodel via minimizing a distillation loss. This loss on a trans-fer dataset is then backpropagated to the student model. is a range of possibilities in selecting the transfer set on which totrain the smaller distilled model . For example, symbolic distilla-tion approaches synthesize data from the teacher model tothis end. Distillation also comes with a trade-off between size andquality, which determines the improvement in throughput/latency.",
  ") thus relating the tokens": "to each other (the mask enforces that tokens can only attendto their predecessors). The FFN is applied on each token inde-pendently. Both attention and FFN add their outputs onto theembedding, which is passed through the skip connections. As the model parameters of LLM increases, the decoding phaseof LLM inference is inherently memory-bound due to its low arith-metic intensity, meaning that loading and moving the model weightsinto the on chip memory takes significantly more time than theactual computations. This challenge becomes particularly acute",
  "Computational and Memory Requirements": "Modern computer chips employ specialized tensor units to effi-ciently perform tensor computations, such as matrix multiplication,which are fundamental in large foundation model workloads. Ex-amples of these units include Nvidia TensorCore , AMD Ma-trixCore , and the systolic arrays found in Google TPU and AWS Trainium . These tensor units are designed to processhigh-performance tensor computations such as matrix multiplica-tion to meet the extensive demands of LLM workloads, especiallyduring the training phase.Inference tasks, however, present a distinct challenge, as pow-erful tensor units alone are insufficient for optimal performance.To address memory-bound during decoding process, modern chipsincorporate high-bandwidth memory, typically in the form of StaticRandom Access Memory (SRAM). SRAM offers low latency andhigh throughput, suitable for the substantial memory requirementsof inference workloads. However, the high cost of SRAM limits itscapacity, requiring careful data manipulation to optimize its usage. High performance kernels. Inference-purposed kernels, such asDeepSpeed-Inference , FasterTransformer , and transformers-neuronx , adhere to these guidelines to efficiently process theworkloads. They can be designed by experienced performance-tuning experts or generated by machine learning compilers. Ineither case, a deep understanding of both chip architecture and in-ference workloads is essential for efficiently mapping and schedul-ing computations onto the hardware. By leveraging this knowledge,these kernels can fully optimize the utilization of high-bandwidthmemory and tensor units, ultimately enhancing the efficiency ofinference workloads on modern computer chips. Hardware Accelerators. While the majority of the LLM workloadsare now done on GPUs following the SIMT (single instruction, mul-tiple threads) paradigm, LLM inference actually can also be acceler-ated with systolic array and High Bandwidth Memory (HBM) basedsystems (e.g. Google TPUs , AWS Trainium/Inferentia and Intel Gaudi ) with lower power consumption and lowercost accordingly. Systolic array based systems can accelerate matrixmultiplication with instruction-level parallelism . To acceleratememory access speed of a large amount of data, HBM is used asa replacement of Double Data Rate (DDR) and careful memoryplanning is required as the capacity of HBM is limited compared tothe model size . There are also systems that utilize FPGAs for compute acceleration, and systems that utilize inter-node con-nectivity for large-scale transformer inference. Techniques to Mitigate Memory Bound. In addition, to mitigatethe memory-bound issues in LLM inference, practitioners employvarious techniques that can be broadly categorized into two mainapproaches. First, semantic-preserving methods aim to reduce mem-ory usage while maintaining the original prediction via system opti-mization (). Examples includes KV caches , FlashAtten-tion , and FlashDecoding . Conversely, architectural/algorithmic",
  "Distributed Solution Frameworks": "The memory-bound nature of LLM inference and the limited capac-ity of HBM on individual accelerators present significant challengesin meeting the growing demands of LLM workloads. LLMs withhundreds of billions of parameters typically do not fit on a singlenode for inference, let alone a single accelerator. Consequently,a distributed solution becomes necessary. However, implement-ing such a solution for LLM inference introduces challenges likeefficient model partitioning, communication, and load balancing.Addressing these challenges is crucial for enabling scalable pro-cessing of large-scale LLM inference workloads. Typically, we canemploy a combination of multiple parallel strategies to achievestate-of-the-art performance for LLM inference, each with its ownadvantages and disadvantages.Tensor parallelism is designed to distribute large chunks of ten-sor computation workloads across multiple accelerators and aggre-gate the final results via collective communication. This approachcan help reduce end-to-end latency when collective communica-tion is efficient (e.g., NVIDIA NVLink , AWS Neuron CollectiveCommunication ). However, if the tensor computation work-load is small, the extra overhead in collective communication candiminish overall performance. Since inter-node communication istypically higher than intra-node communication, tensor parallelismis most effectively utilized within a single node.Pipeline parallelism is employed to distribute model layersacross accelerators. As both model weights and KV cache for eachlayer can be distributed to different accelerators, and only the in-puts/outputs of the layers need to be transferred across devices,pipeline parallelism is relatively independent of the collective com-munication bandwidth. This strategy allows for the distribution ofmodels that are too large for a single node. To increase hardwareutilization, overlapping different pipeline stages is typically nec-essary. Pipeline parallelism is preferable over tensor parallelismwhen the entire model does not fit on a single node for inference.Sequence parallelism is a critical technique for supportinglong context. The core concept of sequence parallelism involvesdistributing sequences along the sequence dimension, enabling theparallel decoding of small batches of long sequences. This tech-nique is implemented by solutions such as FlashDecoding andPagedAttention V2 .Expert parallelism (EP) facilitates the distribution of Mixtureof Expert (MoE) models across multiple accelerators. The MoEmodel architecture is designed to skip inactive expert computation,while still maintaining the capability to achieve high accuracy com-pared to dense models. Since expert weights are typically large,distributing and dynamically loading these weights can be costly.To reduce collective communication and avoid the dynamic loadingof expert weights, EP keeps each expert within a small group ofaccelerators . As the input/output data is considerably smaller",
  "SYSTEM OPTIMIZATION": "We explores semantic-preserving optimizations for LLM inferencefrom a systems perspective. By strategically organizing computa-tions, significant improvements in inference speed and memoryefficiency can be achieved without compromising the semanticintegrity of the model. In this seciton, we discuss on reducingredundant computations through the use of key-value caches (Sec-tion 2.1), optimizing attention implementation to minimize memoryaccess (.2), enhancing throughput via handling batchesof requests (.3), and reducing unused memory fragmenta-tion via distributing sequences (.4). These optimizationswere mainly developed based on GPUs, but the main concepts arelargely applicable to other AI accelerators with some specific im-plementation tweak. The following subsections delve into each ofthese approaches in detail, examining their theoretical foundations,practical implementations and challenges therein.",
  "Fast Attention Computation via Caching": "Generating tokens in an autoregressive fashion is a widely adoptedapproach like GPT and Llama , yet it can pose computa-tional challenges. During the auto-regressive generation, decodingstep to generate every next token requires to fetch previous tokens.This requires to compute their hidden representation of keys andvalues in attention mechanism, which could be repetitive duringthe sequence of token generation. KV- cache stores and reusesthese past key-value pairs, eliminating the need of recalculationfor every new token. This technique significantly improves theefficiency of inference, by reducing the quadratic complexity ofattention computation w.r.t. a sequence length to be linear.However, the memory footprint of KV cache growing linearlyw.r.t. the sequence length can be substantial, as it requires addi-tional memory to store the cached keys and values. To addressthis, several techniques has been introduced to reduce the memoryspace required for the KV cache. Low-bit precision data types havebeen utilized in KVQuant , which brings million-scale contextlength support on a single A100-80G GPU. StreamingLLM introduced the concept of attention sink, which preserves decent ac-curacy by leveraging initial tokens without exhausting the long con-text window size. Generalized block-sparse attention patterns, e.g.BigBird ), allow the training of long context support, withoutdegrading accuracy at inference stage. Heavy-Hitter Oracle is a cache eviction policy which retains Heavy Hitters tokens, i.e.,tokens contributing most of the value in attention scores, based onlocal statistics at each decoding step. However, all of these can leadto a potential degradation of accuracy.The aforementioned KV cache strategies can be implementeddifferently depending on hardware. To be specific, the KV cachememory space size can be formulated as 2 bytes, where isbatch size, is sequence length, is number of KV heads, is sizeof the attention head, is the number of layers, is size of each dataelement in number of bytes. The size of is determined at runtimefor batch inference. and are fixed by the model configuration.This leaves the optimization space for reducing KV cache memory",
  "Efficient Attention Computation": ": Flash Attention by Dao et al. . The outer loopiterates over K and V blocks and loads them to fast SRAM. Ineach block, inner loops iterates over Q blocks, loading themto SRAM, and writing the attention output back to HBM. Modern LLMs have extended the support of context length fromthe order of thousands to millions within a few years from less than1k (e.g., GPT-2 ) to 200k+ (e.g., Claude 3 ). The main challengeof expanding the context window lies in the extensive computa-tional requirements and memory consumption for the attentioncomputation. As the model considers more tokens simultaneously,the compute/time complexity and memory demands of calculationsincrease significantly, scaling quadratically with the size of thecontext window. FlashAttention was introduced to addressthese challenges, which reformulates the attention computation as asequence of matrix multiplications and applies block-sparse decom-position. By processing attention in smaller blocks, FlashAttentionreduces the memory footprint of attention computation, avoidingthe need to materialize the entire attention matrix in memory atonce. The key advantage of FlashAttention is its ability to minimizedata movement between different memory hierarchies. By care-fully selecting the block size based on the memory hierarchy andcapacity of the device, FlashAttention ensures that the data can beefficiently processed without requiring multiple transfers betweenmemory levels. For example, on GPUs, the block size is typicallysmall to fit within the L2 cache, minimizing expensive memory accesses. In contrast, devices like AWS Trainium or Google TPU,which have a large scratchpad memory in the tens of megabytes(MBs), can leverage larger block sizes to maximize computationalefficiency by processing more data in parallel.For large context, Blockwise Parallel Transformer (BPT) further minimize memory consumption on feedforward networkby computing them in a block-wise manner. Enhancing BPT, RingAttention utilizes blockwise computation for self-attentionand feedforward processes to distribute extended sequences acrossmultiple devices by dividing the input text into smaller, more man-ageable blocks. These blocks are processed on separate devicesorganized in a ring-like configuration, enabling parallel processing.When it comes to inference compared with training, relativelysmaller batch size can lead to different bottleneck. Flash-Decoding, based on FlashAttention, introduces a new parallelization di-mension: the keys/values sequence length. It stores minimal extradata in global memory while fully utilizing the accelerator, evenwith small batch sizes, provided the context length is sufficientlylarge. For the smaller chunks of split keys/values, it computes theattention of the query with each chunk in parallel using FlashAt-tention, and reduce across all chunks to calculate the final output.",
  "Continuous Batching": "LLM inference is inherently memory-bound if only one sequence isprocessed. To increase the throughput for a large number of inputprompts, the most straightforward approach was to allocate a fixedtime window for decoding a fixed number of sequences. This iscommonly known as static batching, which has been implementedin FasterTransformer and many others . The advantageof static batching comes from the minimized latency for decod-ing with small batch sizes. As batch size gets bigger to achievehigher throughput, a mechanism in improving effective utilizationof batched decoding is needed.Static batching results in resource waste as some sequences reachthe end earlier than the others in the same batch. Orca proposedthe idea of a dynamic sequence eviction strategy. The strategy essen-tially removes the sequences that generated EOS token, and insertsnew prompts into the decoding batch. The approach is commonlyreferred to as continuous batching. In addition to the proposedmechanism in handling continuous batching, Ocra also introducedthe idea of flattening multiple input prompts and concatenate theminto the prefill kernel, in order to reduce padding and kernel launchoverhead. The block diagonal causal attention mask is commonlyused to achieve a throughput gain with FlashAttention.",
  "PagedAttention and its Derived Applications": "Since the length of output tokens is unpredictable, the most straight-forward approach was to maintain the maximal sequence lengthfor each decoding request. As most part of the reserved memorywont be actually used, this would introduce a large amount ofinternal memory fragmentation. As illustrated in the , in-ternal memory fragmentation refers to the memory space that isallocated but not effectively utilized for sequence decoding. Exter-nal memory fragmentation indicates the device memory space thatis free but not allocated for usage. To reduce both internal and ex-ternal memory fragmentation, PagedAttention introduced the",
  "Inference Optimization of Foundation Models on AI AcceleratorsKDD 24, August 2529, 2024, Barcelona, Spain": "Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, FedericoLebrn, and Sumit Sanghai. 2023. Gqa: Training generalized multi-query trans-former models from multi-head checkpoints. arXiv preprint arXiv:2305.13245(2023). Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cap-pelli, Ruxandra Cojocaru, Mrouane Debbah, tienne Goffinet, Daniel Hesslow,Julien Launay, Quentin Malartic, et al. 2023. The falcon series of open languagemodels. arXiv preprint arXiv:2311.16867 (2023).",
  ": Overview of grouped-query method by Ainslie et al": "In multi-head attention, distinct queries brings linear increase onthe number of heads for keys and values, requiring larger memorybound and prohibiting potential latency improvement. However,MQA involves employing multiple query heads alongside a singlekey/value head, thereby accelerating decoder inference. GQA anadvancement over MQA, strikes a balance by utilizing an interme-diate number of key-value heads (more than one but fewer than thequery heads). The GQA model efficiently partitions the query intoheads segments akin to the original multi-head attention mecha-nism, while dividing the key and value into handful of groups. Forexample, Llama-3 70B uses 64 query heads which are groupedonto 8 key-value heads. This arrangement allows a handful of queryheads to share the same key-value heads to interact. By leverag-ing repeated key-value pairs, the GQA approach enhances overallmodel performance while preserving quality.When it comes to the MQA/GQA inference strategy in a dis-tributed setting, there are a number of approaches. If possible, thecommon practice is to evenly distribute KV heads across multipleaccelerators. This assumes that the number of KV heads are divisi-ble by the number of accelerators. For handling the case where thereare more accelerators than number of KV heads, Pope et al. has",
  "Mixture of Experts for Transformer": "Mixture of Experts (MoE) architecture from is de-signed to activate part of expert computation by skipping inactiveones, while maintaining the capability in achieving high accuracy.This allows for pretrained models to utilize significantly less compu-tational resources and thus increase the models size or the datasetit handles within the same computational budget compared toa dense model both in training and inference. This MoE compo-nent becames a popular design choice in favor of fast inferenceamong Transformer class . Among many variance ofMoE , it typically tries to comprise twoprimary components: First, sparse MoE layers replace conventionaldense feed-forward network (FFN) layers. These MoE layers arecomprised of a set number of \"experts\" (e.g., 8 in Mistral ), whereeach expert functions as an individual neural network. While theseexperts are typically FFNs in practice, they can also encompass moreintricate networks or even form a hierarchical MoE structure .Second, a gate network or router determines the allocation of to-kens to specific experts. Notably, tokens can be directed to multipleexperts. This decision is governed by the routing mechanism asa critical design choice for efficient inference and training. Therouter, comprising learned parameters, is pretrained concurrentlywith the remainder of the network and plays a pivotal role in tokenallocation within MoEs.Routers for sparse MoEs can be categorized into two main vari-ants: Token Choice, which assigns experts to individual tokens, andExpert Choice, which assigns tokens to individual experts. Token Choice can be optimal for latency constrained applications, sincethe number of activated experts is small. Expert Choice is used forthroughput optimizations, especially when total number of expertsare small and the tokens can be balance among all experts. In suchapplications, Expert parallelism (EP) keeps each expert within asmall group of accelerators, leading fast inference by alleviatingcollective communications and dynamic loading.",
  "Other Architectures": "Sliding Window Transformer (SWT) is a variant of the self-attention mechanism designed to handle long sequences more ef-ficiently by dividing the input sequence into smaller, overlappingchunks or \"windows.\" For each token, the attention score is com-puted only over a window of length sequence rather than theentire (previous) sequence. This attention mechanism sequentiallyslides across the input sequence to compute all localized attentionscores. As the layers of the SWT get deeper, the localized attentionmechanism extends the receptive fields w.r.t. input tokens, preserv-ing a comprehensive understanding of the entire sequence, similarto a CNN. Each SWT requires only linear complexity (), miti-gating the quadratic complexity (2) in standard self-attention.Mixture-of-Depth allows some tokens to take paths acrosslayers dynamically, skipping certain layers based on specific criteria,e.g., CALM with exit criteria during forward pass, insteadof all tokens passing through every layer of Transformer. Thisapproach enables the model to allocate computational resourcesmore efficiently, focusing more layers on complex parts of the inputwhile using fewer layers for simpler parts. The mixture of depth canhelp reduce computational costs and improve forward/backwardspeed without significantly compromising model performance.",
  "MODEL COMPRESSION": "Model compression techniques compress a model or input,thereby reducing the memory footprint and latency of LLMs. Thesemethods come with challenges as they typically introduce trade-offs between inference improvement and accuracy. Quantizationof model weights (.1) has essentially has become a stan-dard nowadays. Pruning parts of models has posed more challengesbut also seen much progress targeted specifically to LLMs (Sec-tion 4.2). Lastly, entirely compressed models can be trained throughdistillation from a large teacher model (.3).",
  "Quantization": "Quantization is a model-compression technique that representsweights or activations of the network with low-precision data types(e.g., 8-bit integer) instead of high-precision data types (e.g., FP32),therewith reducing the storage when loading the model/activationsin hardware (see ). Reduced data precision poses trade-offbetween latency-throughput-accuracy. It also requires support fromthe target hardware to realize maximum speedup . Quantiza-tion is applied either during training or after training.Post-training quantization (PTQ) methods quantize weights oractivations of a pre-trained model (e.g., LLM.int8() , ZeroQuant-V2 , SmoothQuant , GPTQ , Quip# , OmniQuant ,AQLM , PV-Tuning , Outlier Suppression+ , QLoRA ).",
  ": INT8 quantization represents weights oractivations in FP32 data types into 8-bit integers": "Additional fine-tuning is often needed to recover the downstreamaccuracy drop .Quantization aware training (QAT) emulates inference-time quan-tization, creating a model that can be quantized later post-training(e.g., 1-bit LLM , 1.58-bit LLM , LLM-QAT , QLLM ).They stipulate full pre-training of the base model. A promising re-cent work reported accuracy of a model with only ternary{1, 0, 1} weights on par with a full-precision model.The upper bound for both latency and throughput improvementfrom weight-only quantization is the ratio of source precision datatype to the target precision data type. For example, the upper boundfor latency/throughput improvement with INT8 quantization downfrom 32-bit floating point format (FP32) is 4. As INT8 parametersrequire 4 fewer bits than FP32, we can increase the batch sizeas well as perform more computations on the same data size inone go. But memory saving does not directly translate to improvedthroughput/latency due to several factors like memory bandwidth,hardware limitations, and quantization/de-quantization overhead.",
  "Pruning": "Pruning is a compression technique to remove redundant param-eters from the model. The goal is to maintain prediction qualityof the model while shrinking its size, and thereby increasing itsefficiency. Pruning requires strategies to identify which parts toremove and, potentially, how to adapt the remaining parts in orderto compensate for quality degradation.Structured Pruning removes whole components of the networksuch as neurons, attention heads, and layers .For example, SliceGPT effectively decreases the embedding di-mension of the model, whereas LLM-Pruner scores coupledstructures in the decoder-layer and removes the least importantones. Sheared Llama and LoRAPrune similarly removeentire structures. When pruning larger structures like channels,blocks, or embedding dimensions, the speedups can easily be real-ized end-to-end (e.g., [78, ], [9, ]).Unstructured Pruning removes individual weights of the network.Clearly, weights that are 0 can be ignored without any loss in ac-curacy, but also very small weights can be set to zero. Pruningweights that are not small enough will finally lead to degradationof the model, which sets the limit for the speedup. Given a desiredsparsity ratio and a matrix , the simplest strategy is to prunethe weights with the smallest magnitude, which corresponds tominimizing the Frobenius norm between the dense matrix andits sparse approximation , i.e., 2 . This approach, re-ferred to as magnitude pruning, quickly leads to drastic accuracydegradation . Wanda and RIA improve over simple magnitude pruning by reweighing the matrix weights withthe norm of the corresponding input activation. Another popularTransformer pruning method is SparseGPT, which jointly opti-mizes the pruning mask as well as the remaining weights in orderto minimize ( ) 2 , where represents a sample of inputsto the linear layer. Since finding the optimal pruning mask is acombinatorial problem, SparseGPT employs heuristics to make itcomputationally feasible. While most methods apply the sparsityuniformly across layers, owl , BESA , and ISC derivea criteria to prune layers to different levels.Unstructured sparsity is mainly of academic interest since, sofar, it does not lead to speedup on hardware accelerators (Flash-LLM recently provided some steps in this direction). However,most methods can also be applied to achieve N:M structured sparsity,where only out of consecutive elements are allowed to be non-zero. Some hardware accelerators support these patterns and allowfor memory savings and speedups .While pruning can in principle be done during pretraining , most recent work focuses on the post-training setting.Nonetheless, in order to recover from the accuracy loss due topruning many works consider applying a short training strategyafter pruning. This is either done via a standard pretraining loss or with variants of distillation losses .To increase efficiency, some works do not update all remainingparameters, but employ parameter efficient techniques like LoRA. Generally, such strategies help recovering the accuracy loss,but are also prone to overfitting to the specific dataset used and can compromise the generality of the model.",
  "Distillation": "Knowledge distillation (KD) () is a model compres-sion technique in which we train a small model (called student)to match closely the performance of a larger model or an ensem-ble of models (called teacher). To this end, KD connects a studentmodel with the teacher model by a distillation loss, which penal-izes differences in the outputs of the two models at certain layers(see ). The standard KD approachalso called last-layer-only approachtrains the student to match the performance ofthe teacher at the last layer (e.g., ). Another approachalsocalled layer-wise approachtrains the student to match the hiddenrepresentation of the teacher at each layer (e.g., ). Layer-wise-distillation approaches report improved results on downstreamtasks compared to last-layer-distillation approaches , but theystipulate the same number of layers in the student as the teacher. Ingeneral, KD approaches are flexible with regard to the exact struc-ture of the student model, which allows optimizing the student forvarious target hardwares. Another advantage is that the distillationprocess runs entirely after training the large teacher model.Distillation does not affect the training of a teacher model, butdistillation effort by itself can be a major training effort for thefollowing reasons. First, the number of steps can be similar topre-training a small model. Second, the distillation loss usually isa combination of the pure student/teacher loss together with anoriginal loss, for which typically the original pre-training data isrecommended . To compute the distillation loss, we also needto make a forward pass of a teacher model to get logits. But there",
  "FAST DECODING": "As discussed, vanilla auto-regressive decoding is memory bound.Speculative decoding (SD) exploits the fact that multipledraft tokens can be verified in a single forward pass of the targetmodel. The draft tokens are then accepted based on a rejectionsampling scheme or deterministic approaches . Pro-cessing the draft tokens requires additional computations in thetarget model, but the main bottleneck remains the loading of theweights. Hence, the verification of the additional draft tokens comesat negligible additional latency. But once draft tokens are accepted,multiple tokens are decoded with a single call to the target model,resulting in an overall latency reduction. Noticeably also, opposedto the compression techniques in , the output distributionprovably remains the same [18, Theorem 1].Beyond the verification also the draft token generation adds tothe latency. We classify SD methods broadly into two categories,based on whether or not they use a separate model for drafting.Seminal work uses a smaller model from the target modelsfamily as draft model, e.g., T5-small as the draft model for T5-XXL,whereas Chen et al. train a separate draft model from scratch.Choosing an appropriate draft model for a target model can betricky. In light of this, some SD methods take advantage of the targetmodel itself. For example, self-speculative decoding draftstokens using the target model but skips some of its intermediatelayers. Medusa trains multiple feed-forward heads on top of thelast Transformer layer. The -th head is responsible for predicting( + )-th token into the future. EAGLE improves the headsby introducing auto-regression on features at the last Transformerlayer. PaSS appends special look-ahead\" tokens to the promptas input, and generates draft tokens in parallel using the targetmodel itself. Lookahead Decoding applies Jacobi method that drafts multiple tokens in parallel. In some applications(e.g., Question Answering), one can draft tokens by matching theirprefix in a document , or a database .",
  "There are two orthogonal paths to further speed up speculativedecoding. One is to draft multiple sequences for verification. Theother is to improve the acceptance rate. We elaborate on them next": "Multiple Drafted Sequences. In the vanilla case of a single draftedsequence, all drafted tokens after the first rejection position arewasted. In contrast, drafting multiple sequences increases the chanceof having a longer accepted sub-sequence. The multiple sequencesare often organized in a tree structure to share some prefixes. Cor-respondingly, the verification is made more efficient by introducingtree attention, with a specialized attention mask that reflects thetoken dependencies in the tree. This approach is first proposedin SpecInfer , adopted in several aforementioned papers (e.g.,Medusa , EAGLE , Lookahead Decoding ), and furtherdeveloped by Chen et al. . Depending on the model architec-tures, speedups reported are often in 2 3. Aligning Draft to Target Model. In , the rejection rate is shownto be equal to the Total Variation divergence (TV-div) between targetand draft models token probabilities. This neat theoretical resulthas motivated Distillspec to knowledge distill from the tar-get to draft model. With the better aligned draft model, 10 45%further speedups are reported. Regarding the objective functionfor distillation, it could be either conventional KullbackLeiblerdivergence (KL-div) or the more relevant TV-div. Note that KL-divcan be considered as a surrogate for TV-div due to Pinskers inequal-ity. Interestingly, does not observe an obvious advantage ofTV-div against KL-div.",
  "CONCLUSION": "This paper provides a comprehensive overview of efficient infer-ence methods for LLMs, covering system optimization, structuredTransformer architectures, model compression, and algorithmicallyfaster decoding, especially in the context of AI accelerator. Thesetechniques aim to facilitate effective computation, often consideringinput-output (IO) communication during attention score calcula-tion, reducing extensive and repetitive self-attention mechanisms,minimizing memory idleness and compressing models themselves.Inference optimization is not only crucial for Transformer-basedLLMs, but also other foundation models like Stable Diffusion orthe Transformer alternative of State Space Models (SSMs) .Several of the techniques presented in this paper have been suc-cessfully applied to these models too; e.g., in Stable Diffusion withFlashAttention , quantization , sparsity ,and distillation , or in SSMs with Mixture of Experts .Nevertheless, many of the challenges remain largely unresolved,particularly when dealing with extremely long context lengths andsequences, necessitating tailored efforts depending on the types ofdevices used. We are confident that researchers and developers willcontinue to strive towards narrowing these gaps, thereby enhancingthe accessibility of Generative AI systems. Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra,Bhargav S Gulavani, Alexey Tumanov, and Ramachandran Ramjee. 2024. TamingThroughput-Latency Tradeoff in LLM Inference with Sarathi-Serve. arXivpreprint arXiv:2403.02310 (2024).",
  "AWS-Neuron. 2024. Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150 (2020)": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shot learners. Advances in neuralinformation processing systems 33 (2020), 18771901. Nafea Bshara. 2024. AWS Trainium: The Journey for Designing and OptimizationFull Stack ML Hardware. In Proceedings of the 29th ACM International Conferenceon Architectural Support for Programming Languages and Operating Systems,Volume 3. 44.",
  "T. Cai, Y. Li, Z. Geng, H. Peng, J. D. Lee, D. Chen, and T. Dao. 2024. Medusa:Simple LLM Inference Acceleration Framework with Multiple Decoding Heads.arXiv preprint arXiv:2401.10774 (2024)": "Harrison Chase. 2022. LangChain. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Lau-rent Sifre, and John Jumper. 2023. Accelerating Large Language Model Decodingwith Speculative Sampling. arXiv:2302.01318 Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau1,Laurent Sifre1, and John Jumper. 2023. Accelerating large language modeldecoding with speculative sampling. arXiv preprint 2302.01318 (2023). Y. Chen, R. Sarokin, J. Lee, J. Tang, C. Chang, A. Kulik, and M. Grundmann.2023. Speed Is All You Need: On-Device Acceleration of Large Diffusion Modelsvia GPU-Aware Optimizations. In IEEE/CVF Conference on Computer Vision andPattern Recognition Workshops (CVPRW).",
  "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the Knowledgein a Neural Network. arXiv:1503.02531 [stat.ML]": "Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W Mahoney,Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. 2024. KVQuant: Towards10 Million Context Length LLM Inference with KV Cache Quantization. arXivpreprint arXiv:2401.18079 (2024). Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, SheanWang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of largelanguage models. arXiv preprint arXiv:2106.09685 (2021). HuggingFace. 2023. Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, An-drew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2017. Quantizationand Training of Neural Networks for Efficient Integer-Arithmetic-Only Infer-ence. arXiv:1712.05877 [cs.LG] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, De-vendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,Guillaume Lample, Lucile Saulnier, Llio Renard Lavaud, Marie-Anne Lachaux,Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothe Lacroix,and William El Sayed. 2023. Mistral 7B. arXiv:2310.06825 [cs.CL] Norm Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai,Nishant Patil, Suvinay Subramanian, Andy Swing, Brian Towles, et al. 2023.Tpu v4: An optically reconfigurable supercomputer for machine learning withhardware support for embeddings. In Proceedings of the 50th Annual InternationalSymposium on Computer Architecture. 114.",
  "Norman P Jouppi, Cliff Young, Nishant Patil, and David Patterson. 2018. Adomain-specific architecture for deep neural networks. Commun. ACM 61, 9(2018), 5059": "Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal,Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. 2017.In-datacenter performance analysis of a tensor processing unit. In Proceedingsof the 44th annual international symposium on computer architecture. 112. Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, KeshavSanthanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T Joshi,Hanna Moazam, et al. 2023. Dspy: Compiling declarative language model callsinto self-improving pipelines. arXiv preprint arXiv:2310.03714 (2023).",
  "S. Kim, K. Mangalam, S. Moon, J. Malik, M. Mahoney, A. Gholami, and K. Keutzer.2024. peculative decoding with big little decoder. Advances in Neural InformationProcessing Systems (2024)": "Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz,Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. 2022.Sparse upcycling: Training mixture-of-experts from dense checkpoints. arXivpreprint arXiv:2212.05055 (2022). Eldar Kurti, Elias Frantar, and Dan Alistarh. 2023.ZipLM: Inference-Aware Structured Pruning of Language Models. In Advances in Neu-ral Information Processing Systems, A. Oh, T. Neumann, A. Globerson,K. Saenko, M. Hardt, and S. Levine (Eds.), Vol. 36. Curran Associates, Inc.,6559765617. Se Jung Kwon, Jeonghoon Kim, Jeongin Bae, Kang Min Yoo, Jin-Hwa Kim,Baeseong Park, Byeongwook Kim, Jung-Woo Ha, Nako Sung, and Dongsoo Lee.2022. AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation ofLarge-Scale Pre-Trained Language Models. arXiv:2210.03858 [cs.LG]",
  "Yaniv Leviathan, Mantan Kalman, and Yossi Matias. 2023. Fast inference fromtransformers via speculative decoding. In International Conference on MachineLearning. PMLR, 1927419286": "Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer.2021. Base layers: Simplifying training of large, sparse models. In InternationalConference on Machine Learning. PMLR, 62656274. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, AbdelrahmanMohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. BART:Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,Translation, and Comprehension. arXiv:1910.13461 [cs.CL] Bingbing Li, Santosh Pandey, Haowen Fang, Yanjun Lyv, Ji Li, Jieyang Chen,Mimi Xie, Lipeng Wan, Hang Liu, and Caiwen Ding. 2020. Ftrans: energy-efficient acceleration of transformers using fpga. In Proceedings of the ACM/IEEEInternational Symposium on Low Power Electronics and Design. 175180.",
  "Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik P. Kingma, Stefano Ermon,Jonathan Ho, and Tim Salimans. 2023. On Distillation of Guided DiffusionModels. arXiv:2210.03142 [cs.CV]": "X. Miao, G. Oliaro, Z. Zhang, X. Cheng, Z. Wang, R. Y. Wong, Z. Chen, D.Arfeen, R. Abhyankar, and Z. Jia. 2023. Specinfer: Accelerating generative llmserving with speculative inference and token tree verification. arXiv preprintarXiv:2305.09781 (2023). Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey,Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John Ka-malu, et al. 2022. Fp8 formats for deep learning. arXiv preprint arXiv:2209.05433(2022).",
  "Jeff Pool, Abhishek Sawarkar, and Jay Rodge. 2021. Accelerating Inferencewith Sparsity Using the NVIDIA Ampere Architecture and NVIDIA Ten-sorRT": "Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Brad-bury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. 2023. Ef-ficiently scaling transformer inference. Proceedings of Machine Learning andSystems 5 (2023). Princeton-NLP. 2023. Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza YazdaniAminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. 2022. Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale. In International conference on machine learning. PMLR,1833218346. David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter ConwayHumphreys, and Adam Santoro. 2024. Mixture-of-Depths: Dynamically allocat-ing compute in transformer-based language models. arXiv:2404.02258 [cs.LG] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjrnOmmer. 2022. High-Resolution Image Synthesis With Latent Diffusion Mod-els. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR). 1068410695.",
  "Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019. Patient Knowledge Distil-lation for BERT Model Compression. arXiv:1908.09355 [cs.CL]": "Chaofan Tao, Lu Hou, Haoli Bai, Jiansheng Wei, Xin Jiang, Qun Liu, Ping Luo,and Ngai Wong. 2023. Structured Pruning for Efficient Generative Pre-trainedLanguage Models. In Findings of the Association for Computational Linguistics:ACL 2023. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, GuillemCucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, SagharHosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux,Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, XavierMartinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, AndrewPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, RuanSilva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang,Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, IliyanZarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, AurelienRodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2:Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL]",
  "Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christo-pher De Sa. 2024. QuIP#: Even Better LLM Quantization with Hadamard Inco-herence and Lattice Codebooks. arXiv:2402.04396 [cs.LG]": "Mart van Baalen, Andrey Kuzmin, Suparna S Nair, Yuwei Ren, Eric Mahurin,Chirag Patel, Sundar Subramanian, Sanghyuk Lee, Markus Nagel, Joseph Soriaga,and Tijmen Blankevoort. 2023. FP8 versus INT8 for efficient deep learninginference. arXiv:2303.17951 [cs.LG] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all youneed. Advances in neural information processing systems 30 (2017). vLLM Contributors. 2024. Changyuan Wang, Ziwei Wang, Xiuwei Xu, Yansong Tang, Jie Zhou, and JiwenLu. 2023. Towards accurate data-free quantization for diffusion models. arXivpreprint arXiv:2305.18723 (2023). Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, LingxiaoMa, Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei. 2023. BitNet: Scaling 1-bitTransformers for Large Language Models. arXiv:2310.11453 [cs.CL] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong,Jinyang Guo, and Xianglong Liu. 2023. Outlier Suppression+: Accurate quanti-zation of large language models by equivalent and optimal shifting and scaling.arXiv:2304.09145 [cs.CL] Peter West, Chandra Bhagavatula, Jack Hessel, Jena D. Hwang, Liwei Jiang,Ronan Le Bras, Ximing Lu, Sean Welleck, and Yejin Choi. 2022. Symbolic Knowl-edge Distillation: from General Language Models to Commonsense Models.arXiv:2110.07178 [cs.CL] xai. 2024. Open Release of Grok-1. Haojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang, Zhongzhu Zhou, XiafeiQiu, Yong Li, Wei Lin, and Shuaiwen Leon Song. 2023. Flash-llm: Enabling cost-effective and highly-efficient large generative model inference with unstructuredsparsity. arXiv preprint arXiv:2309.10285 (2023). Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. 2024. ShearedLLaMA: Accelerating Language Model Pre-training via Structured Pruning.In The Twelfth International Conference on Learning Representations. Mengzhou Xia, Zexuan Zhong, and Danqi Chen. 2022. Structured PruningLearns Compact and Accurate Models. In Proceedings of the 60th Annual Meetingof the Association for Computational Linguistics (Volume 1: Long Papers), ACL2022, Dublin, Ireland, May 22-27, 2022, Smaranda Muresan, Preslav Nakov, andAline Villavicencio (Eds.). Association for Computational Linguistics, 15131528.",
  "Zhao Yang. 2023. Support FP8-E5M2 KV Cache by zhaoyang-star Pull Request#2279 vllm-project/vllm": "Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and Yuxiong He. 2023.ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Compre-hensive Study to Low Rank Compensation. arXiv:2303.08302 [cs.LG] Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia,Mykola Pechenizkiy, Yi Liang, Zhangyang Wang, and Shiwei Liu. 2023. Outlierweighed layerwise sparsity (owl): A missing secret sauce for pruning llms tohigh sparsity. arXiv preprint arXiv:2310.05175 (2023). Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, ChrisAlberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,et al. 2020. Big bird: Transformers for longer sequences. Advances in neuralinformation processing systems 33 (2020), 1728317297. J. Zhang, J. Wang, H. Li, L. Shou, K. Chen, G. Chen, and S. Mehrotra. 2023.Draft & verify: Lossless large language model acceleration via self-speculativedecoding. arXiv preprint arXiv:2309.08168 (2023). Mingyang Zhang, Hao Chen, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu,and Bohan Zhuang. 2023. LoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning. arXiv preprint arXiv:2305.18403 (2023). Qingru Zhang, Simiao Zuo, Chen Liang, Alexander Bukharin, Pengcheng He,Weizhu Chen, and Tuo Zhao. 2022. PLATON: Pruning Large TransformerModels with Upper Confidence Bound of Weight Importance. arXiv preprintarXiv:2206.12562 (2022). Yingtao Zhang, Haoli Bai, Haokun Lin, Jialin Zhao, Lu Hou, and Carlo VittorioCannistraci. 2024. Plug-and-Play: An Efficient Post-training Pruning Methodfor Large Language Models. In The Twelfth International Conference on LearningRepresentations.",
  "Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou.2021. Moefication: Transformer feed-forward layers are mixtures of experts.arXiv preprint arXiv:2110.01786 (2021)": "Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, RuisiCai, Zhao Song, Yuandong Tian, Christopher R, Clark Barrett, ZhangyangWang, and Beidi Chen. 2023. H2O: Heavy-Hitter Oracle for Efficient GenerativeInference of Large Language Models. arXiv:2306.14048 Hongbin Zheng, Sejong Oh, Huiqing Wang, Preston Briggs, Jiading Gai, Ani-mesh Jain, Yizhi Liu, Rich Heaton, Randy Huang, and Yida Wang. 2020. Opti-mizing memory-access patterns for deep learning accelerators. arXiv preprintarXiv:2002.12798 (2020). Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun,Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez,et al. 2023. Efficiently programming large language models using sglang. arXivpreprint arXiv:2312.07104 (2023). Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu,Xin Jin, and Hao Zhang. 2024. DistServe: Disaggregating Prefill and Decod-ing for Goodput-optimized Large Language Model Serving. arXiv preprintarXiv:2401.09670 (2024). Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao,Andrew M Dai, Quoc V Le, James Laudon, et al. 2022. Mixture-of-experts withexpert choice routing. Advances in Neural Information Processing Systems 35(2022), 71037114. Y. Zhou, K. Lyu, A. S. Rawat, A. K. Menon, A. Rostamizadeh, S. Kumar, J. F.Kagy, and R. Agarwal. 2023. Distillspec: Improving speculative decoding viaknowledge distillation. arXiv preprint arXiv:2310.08461 (2023)."
}