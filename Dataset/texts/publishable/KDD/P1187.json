{
  "ABSTRACT": "Brain extraction, registration and segmentation are indispensablepreprocessing steps in neuroimaging studies. The aim is to extractthe brain from raw imaging scans (i.e., extraction step), align itwith a target brain image (i.e., registration step) and label theanatomical brain regions (i.e., segmentation step). Conventionalstudies typically focus on developing separate methods for theextraction, registration and segmentation tasks in a supervisedsetting. The performance of these methods is largely contingenton the quantity of training samples and the extent of visual in-spections carried out by experts for error correction. Nevertheless,collecting voxel-level labels and performing manual quality con-trol on high-dimensional neuroimages (e.g., 3D MRI) are expensiveand time-consuming in many medical studies. In this paper, westudy the problem of one-shot joint extraction, registration andsegmentation in neuroimaging data, which exploits only one la-beled template image (a.k.a. atlas) and a few unlabeled raw imagesfor training. We propose a unified end-to-end framework, calledJERS, to jointly optimize the extraction, registration and segmen-tation tasks, allowing feedback among them. Specifically, we usea group of extraction, registration and segmentation modules tolearn the extraction mask, transformation and segmentation mask,where modules are interconnected and mutually reinforced by self-supervision. Empirical results on real-world datasets demonstratethat our proposed method performs exceptionally in the extraction,registration and segmentation tasks.",
  "brain extraction, skull stripping, registration, alignment, segmenta-tion, joint learning, one-shot": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 23, August 610, 2023, Long Beach, CA, USA 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00 : The problem of one-shot joint extraction, registration andsegmentation in neuroimaging data. Given a set of unlabeled rawimages of the patients heads, a standard template image of the brainand the segmentation labels of the template image, the goal is to traina model to perform brain extraction, registration and segmentationtasks simultaneously on raw images of new patients heads. ACM Reference Format:Yao Su, Zhentian Qian, Lei Ma, Lifang He, and Xiangnan Kong. 2023. One-shot Joint Extraction, Registration and Segmentation of Neuroimaging Data.In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discoveryand Data Mining (KDD 23), August 610, 2023, Long Beach, CA, USA. ACM,New York, NY, USA, 12 pages.",
  "INTRODUCTION": "Background. Brain extraction (a.k.a. skull stripping), registrationand segmentation serve as preliminary yet indispensable steps inmany neuroimaging studies, such as anatomical and functional anal-ysis , brain networks discovering ,multi-modality fusion , diagnostic assistance , andbrain region studies . The brain extraction targets the re-moval of non-cerebral tissues (e.g., skull, dura, and scalp) from apatients head scan; the registration step aims to align the extractedbrain with a standard brain template; the segmentation step in-tends to label the anatomical brain regions in the raw imaging scan.These three tasks serve as crucial preprocessing steps in many neu-roimaging studies. For example, in brain functional and anatomicalanalysis, upon extracting and aligning the brain, the interferenceof non-cerebral tissues, imaging modalities, and viewpoints can beeliminated, thereby enabling accurate quantification of shifts in theshape, size, and signal; and labeled anatomical brain regions (e.g.,",
  "KDD 23, August 610, 2023, Long Beach, CA, USAYao Su, Zhentian Qian, Lei Ma, Lifang He, & Xiangnan Kong": "Zeynettin Akkus, Alfiia Galimzianova, Assaf Hoogi, Daniel L Rubin, and Bradley JErickson. 2017. Deep learning for brain MRI segmentation: state of the art andfuture directions. Journal of digital imaging (2017). Brian B Avants, Charles L Epstein, Murray Grossman, and James C Gee. 2008.Symmetric diffeomorphic image registration with cross-correlation: evaluatingautomated labeling of elderly and neurodegenerative brain. MIA (2008).",
  ": Related works in one-shot brain extraction, registrationand segmentation": "non-brain tissue from the raw image. Failing to do this may lead toerroneous transformations, rendering the registration invalid. Lack of labels for segmentation: Collecting the voxel-level seg-mentation labels is also difficult. Although we provide a templatewith its segmentation labeled (in template image space), the seg-mentation (in raw image space) of the raw image is not available. Dependencies among extraction, registration and segmentation:Conventional research typically treats extraction, registration, andsegmentation tasks separately. However, these tasks exhibit a highdegree of interdependency. The accuracy of registration and seg-mentation tasks heavily relies on the extraction task. The registra-tion process assists the extraction task in capturing cerebral/non-cerebral information from raw and template images, and providingsegmentation labels for guiding the segmentation task. The seg-mentation task can inversely force the extraction and registrationtasks to provide precise results. Thus, a holistic solution is requiredto effectively manage the interdependencies among these tasks.Proposed Method. To address the aforementioned challenges,we propose JERS, a unified end-to-end framework for joint brainextraction, registration, and segmentation. showcases acomparison between our method and state-of-the-art approaches.Specifically, JERS contains a group of extraction, registration andsegmentation modules, where the extraction module gradually elim-inates the non-brain tissue from the raw image, producing an ex-tracted brain image; the registration module incrementally alignsthe extracted image with the template and warps the templates",
  "One-shot Joint Extraction, Registration and Segmentation of Neuroimaging DataKDD 23, August 610, 2023, Long Beach, CA, USA": "the image obtained at the final stage. With each stage , it formsan extraction mask M , which is utilized to eliminate non-cerebraltissues in the preceding extracted brain image E1. Specifically, weemploy the 3D U-Net as the base network to learn (). Whenconducting inference, the output M is binarized by a Heaviside stepfunction. To facilitate effective gradient backpropagation duringthe training phase, we employ a Sigmoid function with a largeslope parameter to approximate the Heaviside step function. ()employs a shared-weight scheme, which means () is repeatedlyapplied across all stages with the same set of parameters. Theprocess can be formally expressed as:",
  "Notations and Definitions": "Definition 1 (Source, target and target segmentation mask).Suppose we are given a training dataset D ={S}=1, (T, B)thatconsists of source images S R , and a pair of targetimage T R and its corresponding segmentation maskB {0, 1} . Here, the source image S is the raw MRIscan of a patients head, the target T is a standard template of thebrain, and the segmentation mask B is the one-hot encoding ofthe target T segmentation. , , and denote the width, heightand depth dimensions of the 3D images, denotes the number ofanatomical labels (e.g., the number of labelled brain regions). Toensure simplicity, we make the assumption that the source andtarget images are resized to the same dimension, denoted as . Next, we omit the subscript of S for ease of notation.Definition 2 (Brain extraction mask). Brain extraction maskM {0, 1} is a binary tensor of identical dimensions to thesource image S. It represents cerebral tissues in S with a value of 1and non-cerebral tissues with 0. The extracted image E = S M isobtained by applying the M on S via a element-wise product .Definition 3 (Affine transformation and warped image). Inorder to maintain generality, we consider the transformation in theregistration task to be affine-based. Extending this work to encom-pass other types of registration, such as nonlinear/deformable reg-istration, is straightforward. The affine transformation parametersa R12 is a vector used to parameterized an 3D affine transfor-mation matrix A R44. The warped image W = T (E, a) resultsfrom applying the affine transformation on the extracted imageE, where T (, ) denotes the affine transformation operator. At thevoxel level, the relationship between W and E can be expressed as:",
  "Problem Formulation": "The goal of joint brain extraction, registration, and segmentationis to collectively learn the extraction function : R R , the registration function : R R R12, and the segmentation function : R R ,as shown in . Specifically, the extraction function () uti-lizes the source image S as input to predicts a brain extractionmask M = (S). The registration function (, ) takes the ex-tracted brain image E = M S and the target image T to predictthe affine transformation parameter a = (E, T), then obtainingthe warped image W = T (E, a). The warped segmentation maskV = T (B, a1) is generated by warping the target segmentationmask B using inversed affine transformation a1. Finally, the seg-mentation function () takes the source image S as input topredict a source brain segmentation mask R = (S). The optimalparameter , and can be found by solving the followingoptimization problem:",
  "LW, T+ LR, V, (3)": "where the image pair (S, T, B) is sampled from the training datasetD. L(, ) is image dissimilarity criteria, e.g., mean square errorand L(, ) is segmentation dissimilarity criteria, e.g., cross en-tropy error. These two criteria guide a joint optimization of extrac-tion, registration and segmentation functions, allowing feedbackamong them.To the best of our knowledge, this work is the first endeavorin finding an optimal solution for the one-shot joint brain imageextraction, registration, and segmentation problem. Our approacheliminates the necessity of labeling the brain extraction masks,transformation, and segmentation masks of the source image. Weonly require one pair of a target image and its corresponding seg-mentation mask to guide the training, as opposed to other fullysupervised methods .",
  "Extraction Module": "For the extraction module, we have adopted the multi-stage designparadigm, a strategy that has proven effective in previous works . This design allows for a progressive refinement inthe removal of non-cerebral tissues, culminating in an image withonly cerebral tissues at the final stage. Following the multi-stagedesign of , the extraction module incorporates extractionstages, and each stage contains two key components as follows.",
  "Registration Module": "Similar to the extraction module introduced in .1, weexecute a multi-stage paradigm to tackle the reg-istration task. Following the multi-stage settings of , the moduleis composed of cascading stages, with each stage holding thekey components as follows. 3.2.1Registration Network: . The purpose of the registrationnetwork (, ) is to slowly adapt the extracted brain image tobetter match the target image. Each stage generates a currentaffine transformation Ai based solely on the prior warped imageW1 and the target image T. Similar to the extraction network in .1.1, a 3D CNN-based encoder is used to learn (, ) anda shared weight design is employed across stages using identicalparameters. As a formal expression,",
  "where Ai represents the output affine transformation of the -thstage for = and W0 = E": "3.2.2Composition Layer: COMP. For each stage , upon deter-mining the current affine transformation Ai through (, ), wecombine all preceding transformations: Ac = Ai A1c, where denoting the matrix multiplication operation. Thus, the combinedtransformation Ac can always be performed on the extracted imageE to preserve the sharpness of the warped images . For = 1,the initial affine transformation A0c is designated as an identitymatrix, indicating no displacement. 3.2.3Spatial Transformation Layer: STL. A crucial process in imageregistration involves reconstructing the warped image W from theextracted brain image E using the affine transformation operator.Utilizing the combined transformation Ac , we introduce a spatialtransformation layer (STL) that resamples voxels from the extractedimage to produce the warped image through W = T (E, Ac ).",
  "Inverse Warping and Segmentation Module": "This section introduces the two key designs to solve the segmen-tation task for the input source image S: 1) Inverse Warping takesthe target segmentation mask B and the composed affine trans-formation Ac as inputs, and generates a segmentation mask V inthe source image space; 2) Segmentation Network takes the sourceimage S as input, and predicts the brain segmentation mask R for S. 3.3.1Inverse Warping. In the training dataset, only the target im-age T is labeled with its segmentation mask B. In order to generatethe segmentation label for guiding the segmentation network, wewarp the target image segmentation make B into the source imagespace by applying the inverse of Ac :",
  "=1R(M), (9)": "where ,, are the parameters for the extraction, registration,and segmentation networks, respectively. L(, ) is the imagesimilarity loss, L(, ) is the segmentation loss, R is the extractionregularization term, and , are positive trade-off weights.The image similarity loss L(, ) is a loss function measuringthe similarity between the final warped image W and the targetimage T:",
  "LW, T = LT E, Ac, T.(10)": "We employ the prevailing negative local cross-correlation loss forL(, ), which is resistant to variations in voxel intensity typi-cally encountered across different scans and datasets . Theimage similarity loss can guide the learning of both the extractionand registration modules.The segmentation loss L(, ) is a loss function measuringthe similarity between the predicted segmentation mask R and the",
  "R(M) = =1=1=1 M2.(12)": "This regularization term quantifies the edge strength of the pre-dicted extraction mask M, i.e., the likelihood of a voxel being anedge voxel. Through the minimization of this term, we can suppressthe occurrence of edges, subsequently producing a smooth extrac-tion mask. In this scenario, we apply the 2-norm of the first-orderderivative of M as the regularization term.By leveraging the differentiability in each component of thisdesign, our model can be jointly and progressively optimized in anend-to-end fashion. This training approach empowers us to uneartha joint optimal solution for the aggregate tasks of brain extraction,registration, and segmentation. Thus our work stood in markedcontrast to other works that resort to alternative trainingof individual modules and can only reach a sub-optimal solution.",
  "EXPERIMENTS4.1Datasets": "The effectiveness of our proposed method is evaluated across threepublic real-world 3D brain MRI datasets: 1) LPBA40 includes 40raw T1-weighted 3D MRI scans, coupled with brain masks and 56anatomical structures as segmentation ground truths; 2) CC359 contains 359 raw T1-weighted 3D brain MRI scans and the brainmasks. It also includes labeled white matter as the segmentationground truth; 3) IBSR consists of 18 scans with manual segmen-tation labels. But due to its small size, it serves only for the evalua-tion of the model trained on CC359. Brain masks and anatomicallabels facilitate the accuracy evaluation for extraction and segmenta-tion, respectively. Additional details can be found in Appendix A.2.",
  "Compared Methods": "We outline the comparison of our JERS with several representativemethodologies in brain extraction, registration and segmentation,as illustrated in . Notably, there are no existing solutions thatcan seamlessly learn brain extraction, registration, and segmenta-tion in an end-to-end framework. Hence, we designed three-stagepipeline for comparison, combining various extraction, registration,and segmentation methods. Baselines detailed in Appendix A.4. Brain Extraction Tool (BET) : This technique within the FSLpackage uses deformable models for skull stripping. 3dSkullStrip (3dSS) : This is a BET variant method within theAFNI package. It uses a spherical surface to perform skull stripping. Brain Surface Extractor (BSE) : This method uses morphologicaloperations and edge detection for brain extraction, which leveragesanisotropic diffusion filtering and a Marr Hildreth edge detector toidentify the brain boundary.",
  "JERS (ours)": "FMRIBs Linear Image Registration Tool (FLIRT) : This is anautomatic affine brain image registration tool in the FSL package. Advanced Normalization Tools (ANTs) : This is a state-of-the-artmedical image registration toolbox. We set the registration typeand optimization as affine and cross-correlation metrics. VoxelMorph (VM) : This is an unsupervised image registrationmethod that uses a neural network to predict the transformation. Cascaded Registration Networks (CRN) : This is an unsuper-vised multi-stage image registration method, which incrementallyaligns the source image to the target image. Directly Warping (DW) : It is an operation used to generatethe segmentation mask by registration. The segmentation mask ofthe target image can be directly warped to the source image spaceby the completed registration. DeepAtlas : This is a joint learning network for image regis-tration and segmentation tasks. ERNet : This is an unsupervised learning method for jointextraction and registration.",
  "Experimental Results": "Our JERS is compared with baseline methods on extraction, reg-istration and registration accuracy. We measure the performanceof each task with its corresponding metrics, and also record thetime taken for each baseline method. Based on the experimentalresults, we find that JERS not only consistently outperforms otheralternatives in terms of extraction, registration, and segmentation,but also exhibits superior time efficiency and robustness. 4.3.1Evaluation Metrics. To evaluate the extraction and segmen-tation accuracy, we measure the volume overlap between the pre-dicted and ground-truth masks. To evaluate the registration perfor-mance, we calculate the mutual information between the warpedimage and the target image. Details can be found in Appendix A.1. 4.3.2Experiment Setting. We divide the datasets into training, val-idation and test sets. The training set is utilized for parameterlearning of the model, while the validation set is employed to eval-uate the performance of hyperparameter settings (e.g., the weightof the segmentation loss term). The test set is used only once toreport the final evaluation results for each model. It should be notedthat the IBSR dataset is exclusively used for testing purposes. Wedescribe the detail of the data processing, JERS settings and baselinesettings in Appendix A.2, A.3 and A.4. The source code is availableat",
  "ExtRegSegExtRegSegExtRegSegExtRegSegDiceext MI Diceseg Diceext MI Diceseg Diceext MI Diceseg": "BET FLIRT DW 0.935 0.0280.627 0.0100.613 0.0250.811 0.0870.481 0.0240.748 0.0620.911 0.0380.521 0.0220.800 0.0103dSS FLIRT DW 0.902 0.0320.627 0.0100.601 0.0170.849 0.0370.500 0.0140.791 0.0340.869 0.0390.508 0.0230.788 0.021BSE FLIRT DW 0.938 0.0220.668 0.0100.620 0.0080.846 0.1120.518 0.0350.804 0.0190.873 0.0640.521 0.0260.799 0.015 BET ANTs DW 0.935 0.0280.630 0.0130.625 0.0100.811 0.0870.476 0.0270.744 0.0620.911 0.0380.524 0.0220.792 0.0183dSS ANTs DW 0.902 0.0320.632 0.0110.524 0.1330.849 0.0370.498 0.0160.758 0.0410.869 0.0390.508 0.0260.762 0.030BSE ANTs DW 0.938 0.0220.671 0.0080.614 0.0230.846 0.1120.524 0.0350.807 0.0120.873 0.0640.522 0.0270.787 0.016 BET VM DW 0.935 0.0280.627 0.0160.607 0.0190.811 0.0870.465 0.0340.804 0.0150.911 0.0380.525 0.0230.795 0.0133dSS VM DW 0.902 0.0320.623 0.0090.590 0.0090.849 0.0370.496 0.0180.814 0.0070.869 0.0390.509 0.0270.789 0.017BSE VM DW 0.938 0.0220.670 0.0030.616 0.0100.846 0.1120.515 0.0410.805 0.0160.873 0.0640.524 0.0280.795 0.013 BET CRN DW 0.935 0.0280.633 0.0170.618 0.0220.811 0.0870.467 0.0340.806 0.0150.911 0.0380.527 0.0230.800 0.0113dSS CRN DW 0.902 0.0320.630 0.0120.610 0.0130.849 0.0370.498 0.0170.817 0.0070.869 0.0390.513 0.0280.794 0.014BSE CRN DW 0.938 0.0220.674 0.0060.626 0.0100.846 0.1120.518 0.0400.809 0.0170.873 0.0640.527 0.0280.796 0.013 BET DeepAtlas 0.935 0.0280.627 0.0160.645 0.0090.811 0.0870.467 0.0330.814 0.0170.911 0.0380.524 0.0240.811 0.0113dSS DeepAtlas 0.902 0.0320.625 0.0130.640 0.0100.849 0.0370.498 0.0180.828 0.0070.869 0.0390.509 0.0270.808 0.013BSE DeepAtlas 0.938 0.0220.667 0.0060.648 0.0090.846 0.1120.518 0.0410.817 0.0170.873 0.0640.525 0.0290.807 0.014",
  "JERS (ours)0.944 0.0080.679 0.0050.651 0.0110.937 0.0050.574 0.0060.840 0.0100.917 0.0190.550 0.0250.832 0.010": "4.3.3Extraction, Registration and Segmentation Results. presents the results for the compared methods as well as the pro-posed JERS in extraction, registration, and segmentation tasks.Through a comprehensive evaluation across three datasets, JERSoutperforms existing methods in all metrics.For the extraction task, we observed that the joint-based ex-traction methods (JERS and ERNet) outperform other single-stageextraction methods, especially on the CC359 dataset. Specifically,we observed a gain in extraction dice score up to 10.4% compared tothe best single-stage extraction method 3dSkullStrip. Furthermore,joint-based extraction methods prove to be more robust comparedto other alternatives, given their steady performance and achieve-ment of the lowest standard deviation across all datasets.When observing registration performance, joint-based registra-tion methods (JERS and ERNet) also outperform all other methodsacross all datasets. Significantly, our findings suggest that the reg-istration performance of most methods is constrained by the resultof its corresponding extraction method. This underscores the factthat the precision of extraction considerably influences the subse-quent registration tasks quality. Joint-based registration methodsleverage this characteristic to yield enhanced results through col-laborative learning.For the segmentation task, once again, we find that joint-basedsegmentation methods (JERS and DeepAtlas) are superior to DW.This demonstrates that joint learning with the registration task canhelp the segmentation task to boost its performance.Overall, joint-based methods (JERS, ERNet and DeepAtlas) out-perform other pipeline-based methods in their respective joint tasks.However, the partially joint methods perform poorly on their stand-alone task. ERNet performs well on extraction and registration butinferior on the segmentation task. Similarly, although DeepAtlasachieves good results on the segmentation task, the stand-alone ex-traction method limits its extraction and registration performance.Benefiting from fully end-to-end joint learning, only JERS can per-form well in all tasks. 4.3.4Qualitative Analysis. In , we visually compare theperformance of our JERS and other approaches on the LPBA testset. Upon observation, it is evident that JERS achieves more accu-rate brain extraction compared to BET, 3dSkullStrip, and BSE. Thebrain extraction mask predicted by JERS overlaps closely with theground truth extraction mask, while the masks predicted by otherextraction methods include noticeable non-brain tissues. Regard-ing registration results, JERS also outperforms the other methods.The final registered image of JERS exhibits a higher resemblanceto the target image compared to the alternatives. Importantly, theinaccurate extraction results with non-brain tissue further impactthe subsequent registration results and ultimately affect the overallperformance. This supports our assertion that failed extraction canpropagate errors to the subsequent registration task, resulting inirreparable consequences. In terms of the segmentation task, JERSproduces results that closely overlap to the ground truth segmenta-tion mask. Overall, we can see that better extraction leads to betterregistration, and better registration yields better segmentation. 4.3.5Ablation Study. To demonstrate the effectiveness of our JERS,we compared five variants of JERS in . We first freeze theextraction module, the registration module, and the segmentationmodule of JERS respectively. JERS w/o Ext consistently produces theextraction mask with all values of 1 (i.e., no pattern be removed fromthe source image). JERS w/o Reg only outputs the identity affinematrix representing no displacement applied to the image. JERS w/oSeg directly warps the segmentation mask to the source image space(i.e., no segmentation network exists). The results show that theextraction module, the registration module, and the segmentationmodule are essential to JERS, and removing any of them degradesthe performance of all tasks. We then evaluate the effects of multi-stage and extraction mask smoothing design. The results show thatthey significantly boost the performance of all tasks. Since all tasksare learned in a collective manner, the performance boost of onemodule is shared by all other modules.",
  "TemplateMask": ": Visual comparisons for brain extraction, registration and segmentation tasks. We render a 3D visualization of the image and displaythe middle slice in three different planes: sagittal, axial and coronal. The left side contains the source and target (template) images and theircorresponding ground truth labels. We show the extraction, registration and segmentation results of each method and its correspondingpredictive labels used for performance evaluations. For the extraction task, a predicted extraction mask (marked by green color) shouldcoincide as much as possible with the ground truth extraction mask of the source image. Likewise, in the segmentation task, a predictedsegmentation mask (marked by different color regions) should well-overlap with the ground-truth segmentation mask of the source image. Forthe registration task, the higher the similarity of the registered brain to the template brain, the better.4.3.6Running Efficiency. We measure the efficiency of JERS bycomparing its running time with other baselines. The measurementis made on the same device with an Intel Xeon E5-2667 v4 CPUand an NVIDIA Tesla A100 GPU. As indicated in , all joint-based methods are faster than existing three-stage pipeline-basedmethods. This is because they can efficiently perform their corre-sponding extraction, registration and segmentation tasks end-to-end on the same device. GPU implementations for BET, 3dSkullStrip,BSE, FLIRT, and ANTs are not available . 4.3.7Influence of Parameters. We study two crucial hyperparame-ters of our JERS: the number of stages for extraction and registrationmodules and the value of segmentation loss weight . In our multi-stage design, the number of stages in the network represents thedepth of the model and the number of iterations for the extractionand registration tasks. Essentially, increasing the number of stagesallows for more refinements in the extraction and registration pro-cesses. As illustrated in (a, b, c), we adjust the number ofstages of extraction and registration to study their influence. Theoutcomes show that increasing the stages boosts the performanceof all tasks, validating the notion that a multi-stage frameworkresults in superior overall performance in a joint learning system. As mentioned in .4, we introduce a segmentation lossterm to learn a better segmentation network. To show the effective-ness of the loss term, we vary different values of the loss weight as shown in (d). As the weight of the loss term graduallyincreases, the segmentation dice score grows as well. This indicatesthat our JERS benefits from the segmentation loss term.",
  "RELATED WORK": "Neuroimage extraction. In the past decade, numerous methodshave emerged, highlighting the significance of the brain extrac-tion problem. Smith et al. introduced a deformable model thatfits the brain surface using a locally adaptive set model. 3dSkull-Strip is a modified version of that employs points outsidethe brain surface to guide mesh evolution. Shattuck et al. uti-lized anisotropic diffusion filtering and a 2D Marr Hildreth edgedetector for brain boundary identification. However, these meth-ods heavily rely on parameter tuning and manual quality control,which are time-consuming and labor-intensive. Recently, brainextraction has benefited from the introduction of deep learningapproaches, which exhibit exceptional performance and speed.",
  "JERS (ours)0.0455": "Kleesiek et al. proposed a voxel-wise 3D CNN for skull strip-ping, while Hwang et al. demonstrated the effectiveness of3D-UNet in achieving competitive results. However, these learning-based approaches often necessitate a substantial amount of properlylabeled data for effective training, which is a challenge consideringthat neuroimage datasets are typically small and costly to annotate.Neuroimage registration. Conventional techniques for imageregistration typically aim to maximize image similarityby iteratively optimizing transformation parameters. Commonlyused intensity-based similarity measures include normalized cross-correlation (NCC) and mutual information (MI), among others.However, this iterative optimization approach often suffers fromhigh computational costs and being stuck in local optima, resultingin inefficient and unreliable registration outcomes. Recently, nu-merous deep learning-based methods have been proposed, offeringimproved computational efficiency and registration performance.For instance, Sokooti et al. introduced a multi-scale 3D CNNcalled RegNet, which learns the displacement vector field (DVF)for 3D chest CT registration. Although these methods demonstratecompetitive results, they require supervision. To overcome this lim-itation, unsupervised registration methods have garneredsignificant attention and shown promising outcomes.Neuroimage segmentation. CNN-based approaches have demon-strated superior performance in terms of speed and accuracy forsupervised neuroimage segmentation. Based on the dimensionalityof network operation, current work mainly falls into two cate-gories: 2D CNN-based network and 3D CNN-based network. 2DCNN-based network processes the volumetric neuroimage dataslice by slice, and assembles the final segmentation mask by puttingsegmentation results on all the slices together. The most represen-tative networks include UNet and Attention-UNet . Sincethe 2D CNN-based network treats each slice separately, the spa-tial information encoded between slices is not utilized, negativelyaffecting the segmentation performance. This limitation promptsthe use of the 3D CNN-based network for 3D segmentation. Themost representative network among all is the 3D UNet . Despitethe success CNN-based approaches had in neuroimage segmen-tation, those approaches still possess a data-hungry nature andrequire a large number of labeled neuroimages for training, whichis expensive to acquire.",
  ": Effect of varying the number of stages of the JERS and thesegmentation loss weight": "Joint neuroimage registration and segmentation The neuroim-age registration and segmentation tasks are deeply co-related andmutually facilitating, thus should not be treated separately. Specifi-cally, the anatomy structure produced by neuroimage segmentationcan provide auxiliary information for neuroimage segmentation.This idea is practiced in and demonstrated to be effective. Com-plementary, the registration task can also aid the segmentationtask, typically studied in the scope of atlas-based segmentationand data augmentation. In atlas-based segmentation, the atlas labelis transferred to the unlabeled image space using the geometrictransformation estimated by a registration module . Takingthe idea a step further, for data augmentation, new image-labelpairs are generated by means of sampling the geometric and styletransformation . The above methods still focus on a singletask. To address this limitation, exploring work has been conductedto combine registration and segmentation together, by optimiz-ing them either jointly or alternatively , and is able toobtain better results.",
  "CONCLUSION": "This paper introduces a novel unified framework, called JERS, forone-shot joint extraction, registration and segmentation. In contrastto prior research, our proposed method seamlessly integrates thethree tasks into a single system, enabling joint optimization. Specifi-cally, JERS contains a group of collective extraction, registration andsegmentation modules. These three modules help each other boostextraction, registration, and segmentation performance with onlyone labeled template available. Furthermore, our method facilitatesincremental progress for each task, thereby enhancing the overallperformance to a greater extent. The experimental results substan-tiate that JERS not only surpasses state-of-the-art approaches interms of extraction, registration, and segmentation accuracy butalso exhibits high robustness and time-efficiency.",
  "Wanquan Feng, Juyong Zhang, Hongrui Cai, Haofei Xu, Junhui Hou, and HujunBao. 2021. Recurrent multi-view alignment network for unsupervised surfaceregistration. In CVPR": "Yuting He, Tiantian Li, Guanyu Yang, Youyong Kong, Yang Chen, Huazhong Shu,Jean-Louis Coatrieux, Jean-Louis Dillenseger, and Shuo Li. 2020. Deep comple-mentary joint model for complex scene registration and few-shot segmentationon medical images. In ECCV. Yipeng Hu, Marc Modat, Eli Gibson, Wenqi Li, Nooshin Ghavami, Ester Bon-mati, Guotai Wang, Steven Bandula, Caroline M Moore, Mark Emberton, et al.2018. Weakly-supervised convolutional neural networks for multimodal imageregistration. MIA (2018). Shuai Huang, Jing Li, Jieping Ye, Adam Fleisher, Kewei Chen, Teresa Wu, andEric Reiman. 2011. Brain effective connectivity modeling for Alzheimers diseaseby sparse Gaussian Bayesian network. In SIGKDD.",
  "Mark Jenkinson and Stephen Smith. 2001. A global optimisation method forrobust affine registration of brain images. MIA (2001)": "Konstantinos Kamnitsas, Christian Ledig, Virginia FJ Newcombe, Joanna P Simp-son, Andrew D Kane, David K Menon, Daniel Rueckert, and Ben Glocker. 2017.Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesionsegmentation. MIA (2017). Jens Kleesiek, Gregor Urban, Alexander Hubert, Daniel Schwarz, Klaus Maier-Hein, Martin Bendszus, and Armin Biller. 2016. Deep MRI brain extraction: A 3Dconvolutional neural network for skull stripping. NeuroImage (2016).",
  "Frederik Maes, Dirk Vandermeulen, and Paul Suetens. 2003. Medical imageregistration using mutual information. Proc. IEEE (2003)": "Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee, Mattias Heinrich, Kazu-nari Misawa, Kensaku Mori, Steven McDonagh, Nils Y Hammerla, BernhardKainz, et al. 2018. Attention u-net: Learning where to look for the pancreas.arXiv preprint arXiv:1804.03999 (2018). Evangelos E Papalexakis, Alona Fyshe, Nicholas D Sidiropoulos, Partha PratimTalukdar, Tom M Mitchell, and Christos Faloutsos. 2014. Good-enough brainmodel: Challenges, algorithms and discoveries in multi-subject experiments. InSIGKDD.",
  "A.1Evaluation Metrics": "Our defined problem aims to solve the brain extraction, registrationand segmentation tasks simultaneously: 1) identify the brain region(i.e., whole cerebral tissue) within the source image; 2) align theextracted cerebral tissues to the target image; 3) identify the anatom-ical segmentation within the source image. Thus, we evaluate theaccuracy of extraction, registration and segmentation to show theperformance of our proposed method and compared methods. A.1.1Extraction Performance. The brain MRI datasets incorporatethe ground truth of the brain mask, representing the labeling ofbrain tissue in the source image. To assess the accuracy of extraction,we evaluate the volume overlap of brain masks using the Dice score,which can be expressed as:",
  "W() T()(14)": "where W() and T() are the marginal probability distributionsof image W and T, respectively. WT(,) is the joint probabilitydistribution. The mutual information measures the mutual depen-dence between W and T. If the warped image W and the targetimage T are geometrically aligned, we expect the mutual informa-tion to be maximal. A.1.3Segmentation Performance. We evaluate the segmentationaccuracy by measuring the volume overlap of anatomical segmen-tation, which are the location labels of different tissues in the brainMRI image. If the segmentation task performs well, the predictedsegmentation mask should overlap with the ground truth. Similarto the extraction evaluation, we use Dice score to evaluate the over-lap of the segmentation masks. A Dice score of 1 signifies that thecorresponding structures overlap with the ground truth, whereasa score of 0 denotes the complete absence of overlap. If the imageincludes multiple labeled anatomical structures, the final score iscalculated as the average of the Dice scores for each structure.",
  "A.2Details of Data Preprocessing": "The proposed method and baselines are evaluated on three differentpublic brain MRI datasets, LPBA40, CC-359 and IBSR. LONI Probabilistic Brain Atlas (LPBA40) : The dataset com-prises 40 raw T1-weighted 3D brain MRI scans, each from a differentpatient. It includes corresponding brain masks and segmentation ground truth for 56 anatomical structures. The brain mask is usedfor evaluating the extraction accuracy, while the anatomical seg-mentations are used for evaluating the segmentation accuracy. Sim-ilar to , our focus is on atlas-based registration, where thefirst scan serves as the target image and the remaining scans arealigned to it. Out of the 39 remaining scans, 30 are used for training,5 for validation, and 4 for testing. All scans are cropped and resizedto 96 96 96 dimensions. Calgary-Campinas-359 (CC-359) : The dataset consists of 359raw T1-weighted 3D brain MRI scans from 359 different patients.Additionally, it includes corresponding brain masks and labeledwhite matter as ground truth. The brain masks are used to evaluatethe accuracy of extraction, while the white matter masks are usedfor segmentation evaluation. Similar to the LPBA40 dataset, ourfocus is on atlas-based registration. For CC359, we divide the datasetinto training, validation, and test sets, consisting of 298, 30, and 30scans, respectively. All scans are cropped and resized to 969696. Internet Brain Segmentation Repository (IBSR) : The datasetconsists of 18 raw T1-weighted 3D brain MRI scans from 18 differentpatients, accompanied by corresponding segmentation results. Thesegmentation results are merged to create the brain mask. Giventhe limited sample size, this dataset is exclusively used for testingthe model trained on CC359. Consequently, all 18 scans are alignedwith the first scan of CC359. After cropping, all scans are resized to96 96 96 dimensions.",
  "A.3Details Settings of JERS": "Training settings of JERS. Our experiments are conducted onRed Hat Enterprise Linux 7.3, utilizing an Intel Xeon E5-2667 v4CPU and an NVIDIA Tesla A100 GPU. The code is implemented inPython 3.7.6, and the neural networks are built using PyTorch 1.7.1.The implementation also makes use of Numpy 1.21.6, SimpleITK2.0.2, and Nibabel 3.1.1. To overcome GPU memory limitations, weemploy batch gradient descent, with each training batch consist-ing of one image pair. The models are optimized using the Adamoptimizer, with a learning rate of 1 106. We also apply imageaugmentation techniques, including random translation, rotation,and scaling, to the source images during training. For more details,please refer to .",
  "LPBA40 5 50.98 1.02CC359 3 30.99 1.01": "Parameters settings of JERS. The extraction and registrationstages are set to 5 in this work. The segmentation loss parameter and extraction mask smooth parameter in Eq. (9) are 0.1 and1, respectively. The extraction network contains 10 convolutionallayers with 16, 32, 32, 64, 64, 64, 32, 32, 32 and 16 filters. The regis-tration network adopt 3D CNNs and fully-connected layers to mapthe input to the dimension of 1 12. It contains 6 convolutionallayers with 16, 32, 64, 128, 256 and 512 filters. The segmentationnetwork contains 10 convolutional layers with 128, 256, 256, 512,512, 512, 256, 256, 256 and 128 filters.",
  "A.4Settings of Baselines": "The settings of baselines are followed by for a fair comparison.Brain Extraction Tool (BET) : This skull stripping methodis a component of the FSL (FMRIB Software Library) package. Itemploys a deformable approach to accurately fit the brain surfaceby utilizing locally adaptive set models. The command we use forBET is bet <input> <output> -f 0.5 -g 0 -m, where f and g arefractional intensity threshold and gradient in fractional intensitythreshold, respectively. We set them to default values.3dSkullStrip : This modified version of BET (Brain Extrac-tion Tool) is integrated into the AFNI (Analysis of Functional Neu-roImages) package. It performs skull stripping by employing theexpansion paradigm of the spherical surface. The command weuse for 3dSkullStrip is 3dSkullStrip -input <input> -prefix<output> -mask_vol -fac 1000. fac is set to the default value.Brain Surface Extractor (BSE) : This method extracts thebrain region by utilizing morphological operations and edge de-tection techniques. It incorporates anisotropic diffusion filteringto enhance image quality and a Marr Hildreth edge detector toaccurately identify the boundaries of the brain. The command weuse for BSE is bse -i <input> -o <output> mask <mask> -ptrim auto timer . Hyperparameters are set to default values.FMRIBs Linear Image Registration Tool (FLIRT) : This isa fully automated affine brain image registration tool included inthe FSL (FMRIB Software Library) package. It performs the regis-tration process without requiring manual intervention, allowingfor the alignment of brain images based on affine transformations.The command we use for FLIRT is flirt -in <source> -ref<target> -out <output> -omat <output parameter> -bins256 -cost corratio -searchrx -90 90 -searchry -90 90-searchrz -90 90 -dof 12 -interp trilinear.Advanced Normalization Tools (ANTs) : It is a cutting-edgemedical image registration toolkit widely used in the field. In our approach, we employ the affine transformation model and cross-correlation metric provided by ANTs for the registration process.VoxelMorph (VM) : This unsupervised image registration methodutilizes a neural network to predict the transformation betweenimages. In order to ensure a fair comparison, we re-implementedthe method using an affine transformation. The network architec-ture consists of 6 convolutional layers with filter sizes of 16, 32,64, 128, 256, and 512. The deformation regularization ratio is setto 10, ensuring smooth and controlled transformations during theregistration process.Directly Warping (DW) : This operation refers to generatinga segmentation mask through the process of registration. Oncethe registration is completed, the segmentation mask of the targetimage can be directly warped and transformed into the sourceimage space.Cascaded Registration Networks (CRN) : This is an unsu-pervised multi-stage registration method that involves iterativelytransforming the source image to align with a target image. Sameto JERS, the number of stages is set to 5. Within each stage, we con-figure the network architecture with 6 convolutional layers usingfilter sizes of 16, 32, 64, 128, 256, and 512.DeepAtlas : This is an unsupervised learning method for jointregistration and segmentation. For a fair comparison, we configure6 convolutional layers with 16, 32, 64, 128, 256 and 512 filters forthe registration module, and the segmentation network contains10 convolutional layers with 128, 256, 256, 512, 512, 512, 256, 256,256 and 128 filters.ERNet : This is an unsupervised learning method for jointextraction and registration. For a fair comparison, the number ofstages is set to 5, and we configure 6 convolutional layers with16, 32, 64, 128, 256 and 512 filters, and the registration networkcontains 6 convolutional layers with 16, 32, 64, 128, 256 and 512filters."
}