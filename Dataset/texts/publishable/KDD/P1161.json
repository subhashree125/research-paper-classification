{
  "ABSTRACT": "Online advertisements are important elements in e-commerce sites,social media platforms, and search engines. With the increasingpopularity of mobile browsing, many online ads are displayed withvisual information in the form of a cover image in addition to textdescriptions to grab the attention of users. Various recent studieshave focused on predicting the click rates of online advertisementsaware of visual features or composing optimal advertisement ele-ments to enhance visibility. In this paper, we propose AdvertisementStyle Editing and Attractiveness Enhancement (AdSEE), which ex-plores whether semantic editing to ads images can affect or alterthe popularity of online advertisements. We introduce StyleGAN-based facial semantic editing and inversion to ads images and traina click rate predictor attributing GAN-based face latent representa-tions in addition to traditional visual and textual features to clickrates. Through a large collected dataset named QQ-AD, containing20,527 online ads, we perform extensive offline tests to study howdifferent semantic directions and their edit coefficients may impactclick rates. We further design a Genetic Advertisement Editor toefficiently search for the optimal edit directions and intensity givenan input ad cover image to enhance its projected click rates. OnlineA/B tests performed over a period of 5 days have verified the in-creased click-through rates of AdSEE-edited samples as comparedto a control group of original ads, verifying the relation betweenimage styles and ad popularity. We open source the code for AdSEEresearch at",
  "Information systems Display advertising; Computationaladvertising; Computing methodologies Image represen-tations": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 23, August 610, 2023, Long Beach, CA, USA. 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00",
  "Advertisement Image Editing; StyleGAN; Click-through Rate Pre-diction; Genetic Algorithms": "ACM Reference Format:Liyao Jiang, Chenglin Li, Haolan Chen, Xiaodong Gao, Xinwang Zhong,Yang Qiu, Shani Ye, and Di Niu. 2023. AdSEE: Investigating the Impact ofImage Style Editing on Advertisement Attractiveness. In Proceedings of the29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining(KDD 23), August 610, 2023, Long Beach, CA, USA. ACM, New York, NY,USA, 13 pages.",
  "INTRODUCTION": "Online or digital advertisements are crucial elements in e-commercesites, social media platforms, and search engines. With the increas-ing popularity of mobile browsing, many online ads are displayedon cellphones with visual information frequently in the form of acover image in addition to text description, since visual informationis not only more direct but can also grab peoples attention com-pared to text-only ads. In fact, previous studies have shownthat appealing cover images lead to a higher Click-Through Rate(CTR) in online ads.Therefore, a number of recent studies on online ads have fo-cused on extracting visual features for visual-aware CTR predic-tion . Furthermore, while many online ad images containhuman faces, previous studies have verified that incor-porating human faces in online ad correlates to more attentiontowards the ads, as well as that eye gaze directions have an im-pact on user response. Another related research direction focuseson Advertisement Creatives selection , which searches from alarge pool of creative elements and templates to compose a good addesign. Thanks to recent advancements in generative adversarialnetworks (GANs), e.g., SytleGAN , image editing has beenmade possible, especially with respect to facial semantics. However,existing studies have not investigated the impact of style editing inrecommender systems.In this paper, we propose the Advertisement Style Editing andAttractiveness Enhancement (AdSEE) system, which aims to doa reality check to answer a long-standing question in AI ethicswhether editing the facial style in an online ad can enhance its",
  "KDD 23, August 610, 2023, Long Beach, CA, USA.Liyao Jiang et al": "positive and have a mean of 0.0012 which is a 3.9% increase relativeto the 0.0308 mean of these test images. This demonstratesthat our method can enhance image attractiveness when applied toother image recommendation scenarios like e-commerce besidesour own advertisement QQ-AD dataset. This shows AdSEE can beused to extract knowledge and can enhance image attractiveness byfacial image style editing. Moreover, the results show the existenceof a correlation between image style editing and click rates in .Semantic Editing Directions. To figure out the most impor-tant semantic editing directions that improve the attractiveness of acover image the most, we sample 1000 images from the QQ-AD testset and run AdSEE 10 times. In each run, we allow editing in onlyone out of the top ten directions discovered by SeFa . In (b), we observe that editing on directions 4, 7, 1 results in thelargest increase in . That is, these directions have the largestimpact on the attractiveness of an ad among the other editing direc-tions. We further analyze the details on semantic editing directionsin of the Appendix Section B.1.",
  "RELATED WORK": "Click-Through Rate Prediction. A CTR predictor aims to predictthe probability that a user clicks an ad given certain contexts whichplay an important role in improving user experience for many on-line services, e.g., e-commerce sites, social media platforms, andsearch engines. Recent studies extract visual features from thecover image of ad for better CTR predicting . Chenet al. apply deep neural network (DNN) on ad image for CTRprediction. Liu et al. propose the CSCNN model to encode adimage and its category information, and predict the personalizedCTR with user embeddings. Li et al. utilize multimodal featuresincluding categorical features, image embeddings, and text embed-dings to predict the CTR of E-commerce products. The sparsity anddimensionality of features vary drastically among different modal-ities. Therefore, it is crucial to effectively model the interactionsamong the features from different modalities . AutoInt is shownto achieve great performance improvement on the prediction taskson multiple real-world datasets. Thus, in this paper, we build a clickrate predictor to estimate the averaged click rate of an ad amongadvertising audience based on the best-performing AutoInt model compared with many state-of-the-art models in AppendixSection A.Creatives Selection. Another research direction of display ad-vertising focuses on creatives selection. Previous studies in this lineof research use the bandit algorithm for the news recommenda-tions , page optimization , and real online advertising .Chen et al. propose an automated creative optimization frame-work to search for the optimal creatives from a pool. In this work,instead of choosing from various creatives, we enhance an existingad through direct facial feature editing.Face Image Generation and Editing. Generative AdversarialNetworks (GANs) have achieved impressive results on a rangeof image generation tasks. Style transfer is the task of renderingthe content of one image in the style of another. StyleGAN pro-poses a style-based generator using the AdaIN operation andcan generate higher quality photo-realistic images compare to otheralternatives . Based on the StyleGAN2 model, Tov et al.propose the e4e encoder to map real face images to the latentembedding space of the StyleGAN2-FFHQ model. Shen andZhou propose a closed-form factorization method to find the latentdirections of face image editing without supervision. Following thestyle transfer direction, Durall et al. propose FacialGAN to transfer the style of a reference face image to the target faceimage. However, FacialGAN requires a standard face image as refer-ence, which can not be satisfied when we have arbitrary faces in cover images. Instead, our work utilizes the SeFa image editingmethod to find the face editing directions without any supervisionor reference images which is automated and efficient. To adjust the to their best appearances that may lead to higher click rates, wefind the optimal face editing intensity through the guidance of thepredicted click rate. We adopt StyleGAN2 as our backbone imagegeneration model because StyleGAN2 offers state-of-the-art gen-eration quality and is applicable to many domains including faces,cars, animals, etc. Many works have chosen to extend StyleGAN2including thus allowing many possible applicationsincluding image editing with SeFa .",
  "METHOD": "In this section, we describe the detailed model adopted in the AdSEEframework.We consider advertisement (ad) data with category information,cover image, and query text. Each advertisement is displayed to theuser within the app feed as a card which includes a cover imageand a query text as the advertisement title. Specifically, for a givenadvertisement = (, ,), C where represents thecategory, e.g., Sports, Game, that belongs to, C denotes theset of all the considered categories, and , represent the coverimage and query text of , respectively. An impression refers tothe event when an ad is shown/exposed to a target user by theonline advertising system. Therefore, to assess the attractivenessof , we calculate its averaged click rate as",
  "System Overview": "provides an overview of our proposed AdSEE framework.First, we build a Click Rate Predictor (CRP) which takes an ad asinput and predicts its averaged click rate defined in (1). Trainedwith a regression task, the CRP estimates the click rate of any given which can be used to guide the ad editing module. Second, webuild the Genetic Advertisement Editor (GADE) module to enhancethe overall attractiveness indicated by of through editingits cover image . The GADE module utilizes genetic algorithmto explore human facial feature editing directions in the form theface latent codes. It aims to find the best editing direction andediting intensities which may lead to the highest attractivenessenhancement reflected by the increase in predicted Click Rate withguidance from the CRP.",
  "Click Rate Predictor": "As shown in , we extract sparse and dense features from theraw input ad data, i.e., (, ,) and use the AutoInt modelstructure to predict the average click rate for .Sparse Features. The category information of an ad, e.g., Gameis encoded as a one-hot vector, e.g. . The length of theencoded vector depends on the size of the category set, i.e., |C|.The content of the cover image is also crucial to its overall at-tractiveness. Therefore, apart from the ad category, we furtherextract sparse features from the cover image of an ad. Specifically,we adopt the SOLO instance segmentation model to iden-tify the segmentation masks of all instances which belong to theCOCO class, e.g., person, cat, etc. Formally, for an advertise-ment = (, ,), we haveInstance = SOLO()",
  "Class = Unique(Instance),(2)": "where Instance is the list of all detected instances by the SOLOmodel from the cover image , and Unique(Instance) identifies allthe unique COCO classes, Class, from the instance list. The SOLOmodel supports the detection of 80 classes of COCO object labels. Therefore, we convert the detected Class to a multi-hot encodedvector of size 80, e.g., [0, 1, 0, ..., 0, 1], where each 1 indicates thepresence of a certain COCO class in the cover image.For instances that fall in the Person COCO class, we extracttheir corresponding person images according to their segmenta-tion masks. Specifically, for a person instance, we apply GaussianBlur to the unmasked area (non-person area) to blur the back-ground out and isolate individual person to obtain person image,i.e., , for the j-th person in the cover image . Then, we feedall the person images, i.e., = {, }, = 1, , , where isthe total number of persons in cover image , into the Dlib face alignment model to align the facial landmarks and crop to facewhich yields face images = {, }, = 1, , , where represents the number of detected faces from the person images. That is, = Dlib().(3) Note that, we remove that do not contain a face image becausewe cannot perform facial feature editing if there is no face in a coverimage. In addition, we remove ads with more than = 5 faces fromthe dataset to avoid extracting low-resolution and unrecognizableface images from a cover image. Thereafter, we encode the facecount, i.e., where 1 5, into a one-hot sparse vectorwith the length of 5, for example, a face count vector indicates 2 faces are detected from a cover image.Dense Features. We further extract dense features from thecover image and query text of an ad for the click rate prediction.First, we adopt the e4e model to encode each face image,,, into a real-valued dense vector representation ,. Formally,for the j-th face image of cover image , we have",
  "= .(5)": "We apply the best-performing max-pooling operation (among max-pooling, average-pooling, and concatenation operations) on alongits first dimension to obtain the latent face code with the shapeof . Then, is flattened and used as a dense feature for theclick rate prediction. With the latent representation , the CRPencodes the attractiveness of ads from their facial features whichenables using it to guide the GADE module for face style editingand cover image enhancement of ad.Second, apart from facial features, the attractiveness of an adalso relies on the overall content and quality of the cover image.Therefore, we encode the whole cover image into a latent imagerepresentation to boost the click rate prediction. Specifically, we usetwo different image embedding methods to get more comprehensiveand effective embeddings of the cover images. 1) We adopt the imageembedding model which is pre-trained with the multi-label imageclassification task on the open image dataset . With more than9.7 million images and around 20 thousand labels, the embeddingprovided by the multi-label classifier carries fine-grained image",
  ": The system architecture of the proposed framework": "content information. 2) Another method of cover image embeddingis provided by Sogou2. Sogou provides the service of searchingpictures through text, in which both text and pictures are encodedinto latent vectors for picture and text matching. Therefore, theembedding of the cover image provided by Sogou contains semanticinformation which is useful in judging the attractiveness of thecover image. For a given ad, we concatenate the image embeddingsfrom the above two models to obtain the final embedding of thecover image.Finally, we use a pre-trained Bert-Chinese model to extracttext embedding from the query text,, associated with. The Bertmodel takes the query text as input and outputs the embeddings ofthe words in the text. Then, we apply the max-pooling operationon the word embeddings to get the embedding of the query text.Click Rate Prediction Let ,1, ,,6 denotes the 6 extractedfeatures, including sparse features, ad category, multi-hot classlabel, one-hot face count, and dense features, latent face representa-tion, cover image embedding, and text embedding, for. Then, Weapply the AutoInt model to the extracted features, ,1, ,,6,to predict the averaged click rate. We selected the best-performingAutoInt model in our evaluation of many SOTA models in AppendixSection A.For a given advertisement , to allow the interaction betweensparse and dense features, the Embedding layer of the AutoIntmodel maps all 6 extracted features into a fix-length and low-dimensional space through embedding matrices, i.e.,",
  "= FC( ,1 ,2 ,6 ,1,2 ,6),(8)": "where ,1,2 ,6 denotes the vector after concatenating thevectors ,1, , ,6, and represent point-wise addition. The out-put, , of the fully-connected layer, FC(), denotes the predictedaverage click rate of .The loss function of the Click Rate Predictor (CRP) is defined asthe mean square error (MSE) between the predicted click rate andthe target click rate:",
  "AdSEE: Investigating the Impact of Image Style Editing on Advertisement AttractivenessKDD 23, August 610, 2023, Long Beach, CA, USA": "Editing module and the Genetic Algorithm Optimization (GAO)module in detail below.Semantic Face Editing. Following Tov et al. , we adopt theclosed-form semantic factorization method SeFa to identifya set of edit directions from the latent space of the pre-trainedStyleGAN2-FFHQ face image generator (). SeFa utilizeseigen-decomposition on the matrix , where is the weightmatrix of (), to find a set of edit directions, i.e., = {}=1where corresponds to the eigenvector associated with the -thlargest eigenvalue of the matrix . Each edit direction 512 corresponds to some face semantic concept, e.g. smile, eye-openness, age.With the identified edit directions , we apply the () opera-tion to the face set to edit the facial image styles and enhancethe attractiveness of a given cover image . Formally, we have",
  ", = (,) = (,) = (, + ,), = 1, , ,(10)": "where we alter the face image , by linearly moving its originalface latent codes , along the identified direction ,. Then,we use () to generate edited face images , from edited facestyle vectors ,. In addition, , R is the editing intensitycoefficients given by a genetic algorithm for face image ,, and, denotes the linear combination of the edit directions .Cover Image Editing. Then, we use the OpenCV and Dlib libraries to swap the edited face images back into to obtainthe edited cover image . The face swap operation () can beformulated as = (, , ),(11) where the edited face images = {, }=1 are defined in (10).We measure the attractiveness enhancement of the edited faces over the original faces using the difference in the predictedclick rates of and , i.e.,",
  "= (12)": "where is the predicted average click rate of the edited coverimage , and is the predicted average click rate of the originalcover image defined in (8). Therefore, the enhancement of theattractiveness depends on the editing intensity coefficients ={, }=1 and the identified editing directions .Genetic Algorithm. To maximize the attractiveness enhance-ment defined in (12), we adopt the genetic algorithm to searchfor the optimal editing intensity coefficients for all the detectedfaces in cover image . We selected the genetic algorithm tooptimize the editing intensities because of its efficiency and effec-tiveness in a large search space. Alternatively, a gradient-basedoptimization approach will require backward passes through manycomponents including the large generator model which is prohib-itively expensive. Then, we generate the best-edited cover imageaccording to to (11).We summarize the searching procedure of the genetic algorithmin Algorithm 1. We set the editing intensity coefficients for as the genotype in the genetic algorithm. Then, the fitnessmeasurement () for genotype is set to be the predicted clickrate defined in (8). That is,",
  "Fitness Function:Fitness measurement () defined in (13).Initialization: Generate the initial population 1 byrandomly generating of genotypes.Generation Loop:for do": "Fitness Evaluation: evaluate the fitness for each genotypein with ().Parent Selection: use rank selection method to select parents from for mating.Crossover: apply the uniform crossover operation amongthe parents to create off-springs.Mutation: apply the random mutation operation to percent of off-spring genotypes.Survivor Selection: keep all parents, andkeep at most fitgenotypes from the off-springs. All the kept genotypesare treated as the next population +1.",
  "Output: The best genotype": "where the Predictor() is the CRP, and , defined in (11), is theedited cover image of ad . Thus, guided with (), the geneticalgorithm is supposed to search for the best genotype, i.e., editingintensity coefficients.At the initialization step, we create an initial population de-noted as 1 by randomly generating PopulationSize number ofgenotypes, each with the same shape as . Then, we repeat thegeneration loop times. Each iteration consists offive steps including Fitness Evaluation, Parent Selection, Crossover,Mutation, and Survivor Selection.After generations, we return the best genotype with the highest fitness value, which also results in the best im-provement of the predicted click rate defined in (12). Finally,we use the best genotype to generate the best cover image according to (11).",
  "EVALUATION": "In this section, we conduct extensive experiments to evaluate theeffectiveness of the proposed click rate predictor and the AdSEEframework. We also perform offline and online analyses based onthe introduced QQ-AD dataset to offer insights on the connectionbetween style editing and possible click rate enhancement. Further-more, we also evaluate AdSEE on the public CreativeRanking dataset. Due to space constraints, we qualitatively evaluate AdSEEedited images by putting examples in the Appendix Section B.2.",
  "Datasets": "QQ-AD Dataset. To evaluate our proposed approach, we collectedreal advertisement data from the QQ Browser mobile app. Note thatthe common recommender model datasets such as Avazu andCriteo do not apply to our work because they do not containany image. Each record consists of its category information,cover image, and query text. In addition, we also collected the num-ber of impressions, i.e., the number of times an ad is shown to anaudience, and the number of clicks, i.e, the number of times thatan ad was clicked by an audience. Shown in , we collecteda total number of 158,829 ads from December 19, 2021, to January18, 2022. As our goal is to enhance the attractiveness of ad imagesthrough facial feature editing, we remove ads that do not containa face in their cover image. In addition, we also remove ads withmore than m=5 faces in its cover image from the collected datasetto avoid extracting low-resolution and unrecognizable face imagesfrom the cover image of an ad. Finally, we have 20,527 ads witha valid number of faces in the collected QQ-AD dataset. That is,around 12.92% of the collected ads from the QQ Browser mobileenvironment contain 1-5 faces that can be enhanced with our Ad-SEE framework. The number of impressions and clicks for AdSEEapplicable images in the QQ-AD dataset accounts for 19.12% and19.48% of the total number of impressions and clicks, respectively.This suggests that an ad image with 1 to 5 faces is common in theQQ Browser mobile environment, and editing the facial featurescan potentially have a significant impact on the overall user clicks,impressions, and click rates. We randomly split the ads in QQ-ADdataset into three parts for training (64%), validation (16%), andtesting (20%).CreativeRanking Dataset. We further evaluate AdSEE onthe relevant public dataset CreativeRanking3 published by Wanget al. . We process the CreativeRanking dataset to be similar toour image enhancement task. Each row in CreativeRanking datasetcontains an e-commerce image, a product name, a number of clicks,a number of shows, and a show date. We aggregate the total clicksand the total shows for the same product and the same image overdifferent dates resulting in each row corresponding to an image-product pair and the corresponding total show, total click, andaverage click rate. Similar to the features used in , we usethe same one-hot face count (from 0 to 5) and multi-hot class labelas the sparse feature, and we use face latent code and image embed-ding as the dense feature. Differently, we replace the categorysparse feature with the product name index as a sparse feature andwe do not use any text embedding feature since there is no textdata in CreativateRanking. In this dataset, there can be differentimages that are for the same product so each row in our dataset",
  "Dataset available at": "is a product-image pair. We remove any product-image pair withless than 100 total impressions, with more than 1000 total impres-sions, or with 0 total clicks. This yields 267,362 product-imagespairs which we split into three parts for training (60%), validation(20%), and testing (20%). We use the train set which contains bothimages with face and images with no face to train the CRP model.However, the GADE model should be applied to images containingfaces, so we further filter any images with no faces or more than5 faces resulting in a total of 23,713 valid images with a desirablenumber of faces in the entire dataset.",
  "Evaluation on QQ-AD and CreativeRanking": "In the offline evaluation, we first evaluate the proposed CRP modelon the click rate prediction task and compare it against a widerange of baseline methods on both of the QQ-AD dataset and theCreativeRanking dataset. Then, we want to analyze whether styleediting using the GADE module is linked to attractiveness and popularity improvements. Thus, we edit the ads with the GADEmodule and evaluate the improvement of the attractiveness, mea-sured by , of the edited ads through the CRP model on both ofthe QQ-AD dataset and the CreativeRanking dataset. Finally, weperform case studies to analyze the more attractive face editingdirections on the QQ-AD dataset.Evaluation of the CRP model In this experiment, we com-pare the proposed CRP method with the following state-of-the-artbaseline methods that use different features for average click rateprediction on the QQ-AD dataset. CRP-NIMA: This is the baselinemethod of where the NIMA score mean and standard-deviationare used as dense features. CRP-OpenImage: Use the image em-beddings obtained from the multi-label image classification modelpre-trained on Open Image dataset as dense features. CRP-Sogou: Use the image embeddings obtained from the Sogou modelfor searching pictures through text as dense features. CRP-e4e:Use the max-pooled face latent codes obtained from the pre-trainede4e FFHQ encoder model as dense features.The implementation details including hyperparameters, pre-trained models, and environment are introduced in Appendix Sec-tion C. Note that, we train the proposed model and all the otherbaseline methods with the same MSE loss for a fair comparison.Moreover, the sparse features, i.e., ad category, multi-hot class label,and one-hot face count, are used in all methods. Furthermore, weadopt the mean absolute error (MAE), mean absolute percentageerror (MAPE), normalized discounted cumulative gain (NDCG),Spearmans , and Kendalls to evaluate the performance of dif-ferent models for the average click rate prediction task. summarizes the performances of the proposed CRP modeland all the baseline methods on the QQ-AD dataset. We can clearlysee that our proposed CRP significantly outperforms all the otherbaselines on all the evaluation metrics. The superiority of the pro-posed method over other baselines can be attributed to the adop-tion of multi-modal dense features, i.e., face latent code, imageembedding, and text embedding. Note that, the CRP-NIMA baselinemethod from , which uses image quality NIMA score as a densefeature, is the worst model in terms of NDCG@10 and NDCG@50when compared against the rest of the methods where image em-bedding and face latent code is adopted as the dense feature. This,",
  ": Analysis of predicted average click rate difference in the offline evaluations": "again, demonstrates the importance of our extracted dense featuresfor the accurate click rate prediction of an ad and the correlationbetween image style and ad popularity. summarizes the performances of the proposed CRP modeland all the baseline methods on the CreativeRanking dataset.We train our CRP predictor on the preprocessed CreativeRankingdataset with MSE loss. We call the proposed combined set of featuresC5, which includes sparse features one-hot face count, one-hotproduct name, and multi-hot class label. In addition, C5 includesdense features face latent code, and image embedding. First, wecompare the performance of different feature combinations on theCreativeRanking dataset and we found our proposed combined setof features (C5) outperforms all other feature combinations on 5out of 6 metrics. All instances use the AutoInt model and share thesame training settings for a fair comparison. Results on both QQ-ADand CreativeRanking demonstrate the benefits of the multi-modalfeatures we proposed to use. Specifically, using a combination offace latent vectors, image embeddings, and text embeddings canachieve better performance than the baseline features. Evaluation of the GADE model. In this experiment, we wantto answer the question: does editing facial styles of an using ourAdSEE model improve the attractiveness of an ad? An edited adand its corresponding original ad will form an evaluation pair.In (a), we can observe that the values of the arepositive for all the evaluation pairs in the test set of QQ-AD dataset.This shows that facial style editing do improve the attractivenessof an through using our AdSEE framework. Furthermore, the has a mean of 0.049 and is right skewed which means thatmost of the samples have a relatively small positive increase in thepredicted click rate, i.e., , after being edited by the AdSEE model.Whereas, a few ads have a large increase in the predicted click rate.This is reasonable because most of the cover images of the ads arealready well-designed and have decent attractiveness.In addition, we use the GADE module together with the CRPpredictor to optimize a random sample of 500 images from theCreativeRanking dataset test set (keeping images with 1 to 5faces). We summarize the predicted average click rate difference in (c). We observe the for all 500 test images are all",
  "Online A/B Tests": "We further report the results of an online A/B test, by comparing250 ad images edited and altered by the AdSEE model as well asthe 250 original ad images tested over the QQ Browser mobileapp users in a 5-day period. All of the 250 original images andthe corresponding 250 edited images contain faces. These fallinto 19 categories (genres), including photography, sports, fashion,show, game, TV, movie, education, science, culture, food, life, comic,inspiration, other, pics, folk arts, novel, and career. The online A/Btests were performed over a period of 5 days from Feb 5th, 2022 toFeb 9th, 2022, where we collected the number of impressions andclicks, and click rates to compare AdSEE with the control groupof unaltered images. Recall that an impression refers to the eventwhen an ad is shown/exposed to a user by the online advertisingsystem and that click rate equals to the number of clicks dividedby the number of impressions.The result shows that images edited by facial style editing withAdSEE received a significant increase in attractiveness compared tothe original images in every metric. When performing online splittests, the AdSEE images and the original images were uploadedat the same time and presented on QQ Browser. After 5 days, wecollected the number of impressions and clicks, and conducted thefollowing comparisons. shows that AdSEE images werepresented a larger number of times and showed a higher clickrate hence attracting more clicks on ads each day separately. Byconducting Paired Sample T-Test, we validated that the experimentgroup was significantly better than the control group. A largernumber of impressions indicate that AdSEE-enhanced images arerecommended more times by the recommender model, which meansthe independent production recommendation model that is nottrained on AdSEE-edited images \"believes\" AdSEE-edited imagesare more attractive to users and may lead to increase in click rates.In addition, a higher click rate indicates better attractiveness tousers. shows the difference in performance in each of the 19categories, we largely improved the popularity and attractiveness ofimages in the photograph category and sports category in terms ofevery metric. shows the cumulative frequency distribution",
  "LIMITATIONS AND DISCUSSION": "This work represents one of the first efforts to explore the poten-tial impact of art and image synthesis on recommender systems.Specifically, we aim to investigate if there is a linkage/correlationbetween popularity and image styles through a data-science ap-proach, which we believe is a valuable question to ask for the AIcommunity as well as AI ethics community. We verified the exis-tence of this linkage with both offline experiments and online A/Btesting. However, we do not aim to commercialize AdSEE as a trafficbooster at the moment. In addition, any exploitation of the researchresults for commercial use is subject to further consideration ofethical requirements and regulations.A similar case also applies to recent advancements in content gen-eration like image generation models StyleGAN3 , DALLE2 ,and Imagic , which are widely popular in AI research becausethey can automatically generate state-of-the-art synthetic imagesthat may match the quality of real images created by cameras andhuman artists. Meanwhile, we recognize that the nature of syn-thetic image generation tasks inherently brings risks to areas suchas information objectivity, misleading information, copyright, dataprivacy, data fairness, etc. Therefore, we believe it is crucial thatany research in the image generation area should be performedwith broader societal and ethical impacts in mind.We hold copyright protection, data privacy, information objectiv-ity, user consent, and right-to-correct as our core ethical values. Thepotential ethical issues are related to the specific application contextand we adopt a series of ethical protection measures throughoutthe design, development, and evaluation of AdSEE. During the col-lection of our QQ-AD dataset, we check the copyright licenses foreach image and only select images with appropriate licenses thatallow commercial use and free modification. We do not publish ourQQ-AD dataset to ensure that copyright licenses are not violatedand data privacy is protected. As a normal process, the platformadvertisement censoring team censors every image on the platformfor legal compliance and ethical control, which also includes alloriginal and edited images used in the online experiments. Theusers participating in the online experiments are beta testers andinternal employees who provided consent to opt into the beta test-ing program. The users have the option to provide feedback or optout of the program at any time.",
  "CONCLUSION": "We present the AdSEE system which aims at finding out whetheronline advertisement visibility and attractiveness can be affectedby semantic edits to human facial features in the ad cover images.Specifically, we design a CRP module to predict the click rate ofan ad based on the face latent embeddings offered by a StyleGAN-based encoder in addition to traditional image and textual embed-dings. We further design a GADE module to efficiently search foroptimal editing directions and coefficients using a genetic algorithm.Based on analyzing the introduced QQ-AD dataset, we identify se-mantic edit directions that are key to popularity enhancement. From",
  "Junxuan Chen, Baigui Sun, Hao Li, Hongtao Lu, and Xian-Sheng Hua. 2016. Deepctr prediction in display advertising. In Proceedings of the 24th ACM internationalconference on Multimedia. 811820": "Jin Chen, Ju Xu, Gangwei Jiang, Tiezheng Ge, Zhiqiang Zhang, Defu Lian, andKai Zheng. 2021. Automated Creative Optimization for E-Commerce Advertising.In Proceedings of the Web Conference 2021. 23042313. Haibin Cheng, Roelof van Zwol, Javad Azimi, Eren Manavoglu, Ruofei Zhang,Yang Zhou, and Vidhya Navalpakkam. 2012. Multimedia features for click pre-diction of new ads in display advertising. In Proceedings of the 18th ACM SIGKDDinternational conference on Knowledge discovery and data mining. 777785. Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al.2016. Wide & deep learning for recommender systems. In Proceedings of the 1stworkshop on deep learning for recommender systems. 710. Weiyu Cheng, Yanyan Shen, and Linpeng Huang. 2020. Adaptive factorizationnetwork: Learning adaptive-order feature interactions. In Proceedings of the AAAIConference on Artificial Intelligence, Vol. 34. 36093616.",
  "Estevo S Gedraite and Murielle Hadad. 2011. Investigation on the effect of aGaussian Blur in image filtering and segmentation. In Proceedings ELMAR-2011.IEEE, 393396": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarialnets. Advances in neural information processing systems 27 (2014). Chunhui Gu, Chen Sun, David A Ross, Carl Vondrick, Caroline Pantofaru, YeqingLi, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Suk-thankar, et al. 2018. Ava: A video dataset of spatio-temporally localized atomicvisual actions. In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition. 60476056. Gianluigi Guido, Marco Pichierri, Giovanni Pino, and Rajan Nataraajan. 2019.Effects of face images and face pareidolia on consumers responses to print ad-vertising: an empirical investigation. Journal of Advertising Research 59, 2 (2019),219231.",
  "Ruining He and Julian McAuley. 2016. VBPR: visual bayesian personalized rank-ing from implicit feedback. In Proceedings of the AAAI Conference on ArtificialIntelligence, Vol. 30": "Xiangnan He and Tat-Seng Chua. 2017. Neural factorization machines for sparsepredictive analytics. In Proceedings of the 40th International ACM SIGIR conferenceon Research and Development in Information Retrieval. 355364. Daniel N Hill, Houssam Nassif, Yi Liu, Anand Iyer, and SVN Vishwanathan. 2017.An efficient bandit algorithm for realtime multivariate optimization. In Proceed-ings of the 23rd ACM SIGKDD International Conference on Knowledge Discoveryand Data Mining. 18131821. Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: combining fea-ture importance and bilinear feature interaction for click-through rate prediction.In Proceedings of the 13th ACM Conference on Recommender Systems. 169177.",
  "Davis E. King. 2009. Dlib-ml: A Machine Learning Toolkit. Journal of MachineLearning Research 10 (2009), 17551758": "Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, JordiPont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov,Tom Duerig, and Vittorio Ferrari. 2020. The Open Images Dataset V4: Unifiedimage classification, object detection, and visual relationship detection at scale.IJCV (2020). Lihong Li, Wei Chu, John Langford, and Robert E Schapire. 2010. A contextual-bandit approach to personalized news article recommendation. In Proceedings ofthe 19th international conference on World wide web. 661670. Xiang Li, Chao Wang, Jiwei Tan, Xiaoyi Zeng, Dan Ou, Dan Ou, and Bo Zheng.2020. Adversarial multimodal representation learning for click-through rateprediction. In Proceedings of The Web Conference 2020. 827836. Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, andGuangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature in-teractions for recommender systems. In Proceedings of the 24th ACM SIGKDDinternational conference on knowledge discovery & data mining. 17541763. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, DevaRamanan, Piotr Dollr, and C Lawrence Zitnick. 2014. Microsoft coco: Commonobjects in context. In European conference on computer vision. Springer, 740755. Hu Liu, Jing Lu, Hao Yang, Xiwei Zhao, Sulong Xu, Hao Peng, Zehua Zhang,Wenjie Niu, Xiaokun Zhu, Yongjun Bao, et al. 2020. Category-Specific CNN forVisual-aware CTR Prediction at JD. com. In Proceedings of the 26th ACM SIGKDDInternational Conference on Knowledge Discovery & Data Mining. 26862696. Qiang Liu, Shu Wu, and Liang Wang. 2017. Deepstyle: Learning user preferencesfor visual recommendation. In Proceedings of the 40th international acm sigirconference on research and development in information retrieval. 841844.",
  "Qiang Liu, Feng Yu, Shu Wu, and Liang Wang. 2015. A convolutional clickprediction model. In Proceedings of the 24th ACM international on conference oninformation and knowledge management. 17431746": "Wantong Lu, Yantao Yu, Yongzhe Chang, Zhen Wang, Chenhui Li, and Bo Yuan.2021. A dual input-aware factorization machine for CTR prediction. In Proceed-ings of the twenty-ninth international conference on international joint conferenceson artificial intelligence. 31393145. Sepideh Nasiri, Negar Sammaknejad, and Mohamad Ali Sabetghadam. 2020. Theeffect of human face and gaze direction in advertising. International Journal ofBusiness Forecasting and Marketing Intelligence 6, 3 (2020), 221237. Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang.2016. Product-based neural networks for user response prediction. In 2016 IEEE16th international conference on data mining (ICDM). IEEE, 11491154.",
  "Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and Daniel Cohen-Or. 2021.Designing an encoder for stylegan image manipulation. ACM Transactions onGraphics (TOG) 40, 4 (2021), 114": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processing systems 30 (2017). Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong,and Ed Chi. 2021. Dcn v2: Improved deep & cross network and practical lessonsfor web-scale learning to rank systems. In Proceedings of the web conference 2021.17851797. Shiyao Wang, Qi Liu, Tiezheng Ge, Defu Lian, and Zhiqiang Zhang. 2021. A Hy-brid Bandit Model with Visual Priors for Creative Ranking in Display Advertising.In Proceedings of the 30th international conference on World wide web. Xinfei Wang. 2020. A Survey of Online Advertising Click-Through Rate Predic-tion Models. In 2020 IEEE International Conference on Information Technology,BigData and Artificial Intelligence (ICIBA), Vol. 1. 516521.",
  "Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. 2020.SOLOv2: Dynamic and Fast Instance Segmentation. Proc. Advances in NeuralInformation Processing Systems (NeurIPS) (2020)": "Yue Wang, Dawei Yin, Luo Jie, Pengyuan Wang, Makoto Yamada, Yi Chang, andQiaozhu Mei. 2016. Beyond ranking: Optimizing whole-page presentation. InProceedings of the Ninth ACM International Conference on Web Search and DataMining. 103112. Song Weiping, Shi Chence, Xiao Zhiping, Duan Zhijian, Xu Yewen, Zhang Ming,and Tang Jian. 2018. AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks. arXiv preprint arXiv:1810.11921 (2018).",
  "Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. 2021. GAN inversion: A survey. arXiv preprint arXiv:2101.05278(2021)": "Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua.2017. Attentional factorization machines: Learning the weight of feature interac-tions via attention networks. arXiv preprint arXiv:1708.04617 (2017). Xiao Yang, Tao Deng, Weihan Tan, Xutian Tao, Junwei Zhang, Shouke Qin, andZongyao Ding. 2019. Learning compositional, visual and relational representa-tions for CTR prediction in sponsored search. In Proceedings of the 28th ACMInternational Conference on Information and Knowledge Management. 28512859.",
  "ABASE RECOMMENDER MODELS": "To select the best-performing base recommender model for our task,we compare the performances among many SOTA models using ourproposed set of features and on our dataset. For these experiments,we use the same set of features and the same experiment settingsfor each base recommender model for a fair comparison.We briefly introduce the most important base recommender mod-els in our comparison due to the vast amount of models compared.Rendle propose the Factorization Machine (FM) model to learnthe first- and second-order interactions of features. To model the in-teractions of both the sparse and dense features, Wide & Deep uses DNN to extract dense features and adopts a Logistic Regres-sion (LR) model to learn the interactions between the dense andthe sparse features. However, the Wide & Deep model requiresmanual feature engineering for the sparse features, which needsdomain expertise. To alleviate this downside, Guo et al. proposethe DeepFM model to learn the first-order and high-order in-teractions automatically with an FM module and a DNN module,respectively. Recently, the AutoInt model was proposed whichutilizes state-of-the-art deep learning techniques including atten-tion mechanism , and residual connections to learn boththe first-order and high-order interactions automatically. summarizes the performances of the different base rec-ommender models on the QQ-AD dataset. Each model shares thesame set of features described in .2 to ensure a fair com-parison, i.e. ad category, multi-hot class label, one-hot face count,latent face representation, cover image embedding, and text em-bedding. We can clearly see that AutoInt model significantlyoutperforms all the other baselines on all the evaluation metrics.The high performance of the AutoInt base recommender model islikely due to its adoption of a powerful multi-head self-attentiveneural network with residual connections to model both the low-order and high-order feature interactions. These experiments alsodemonstrate that our method and features used for CR predictionare scalable to many models of various sizes and different designs.Furthermore, we repeat this base recommend model comparisonon the CreativeRanking dataset and found the AutoInt model alsoprovides the most robust performance among the compared models.",
  "BQUALITATIVE STUDYB.1Semantic Editing Directions": "In (b) from , we observe that editing on directions4, 7, 1 results in the largest increase in . That is, these direc-tions have the largest impact on the attractiveness of an ad amongthe other editing directions. We further analyze the semantics of theediting directions in . We visualize each editing directionby generating images from a range of editing intensity coefficients.Each row in the figure corresponds to one editing direction, andeach column corresponds to a particular editing intensity value inthe range of 5, 2.5, 0, 2.5, 5 from left to right. We can see that fordirection 4, which corresponds to the vertical orientation of theface, the best average editing coefficient value found by the AdSEEmodel is -2.77 which means a face slightly facing downward isfound to be more attractive. Similarly, for direction 7, which corre-sponds to the gender of the face, the best average editing coefficientvalue found by the AdSEE model is 2.26 which means a face with",
  "ModelMAE Spearmansrho Kendallstau PearsonsR": "DeepFM 0.02630.50060.35260.5970CCPM 0.02690.47180.33010.5651PNN 0.02640.48650.34140.5849WDL 0.02640.49730.35020.5949MLR 0.02650.48180.33800.5938NFM 0.02640.49940.35180.5982AFM 0.02700.46610.32600.5630DCNMix 0.02670.48630.34140.5893xDeepFM 0.02640.50780.35700.5918AutoInt 0.02620.51220.36090.6113ONN 0.02640.48930.34390.5883FiBiNET 0.02620.49640.34990.6059IFM 0.02690.48180.33690.5673DIFM 0.02680.48880.34180.5692AFN 0.02680.48150.33630.5626 more feminine features is more attractive. With editing direction 1,which corresponds to the smilingness of the face, the best averageediting coefficient value found by AdSEE is -2.63 which shows thata person with a smiling face is more attractive.",
  "B.2AdSEE Edited Images": "In this section, we qualitatively evaluate AdSEE by showing exam-ples of AdSEE-enhanced images. shows some examples ofthe enhanced ads by the AdSEE model. The two examples are fromtwo different ad categories, i.e., others and sports. Nevertheless, theAdSEE model consistently chooses to enhance the attractivenessof the face by making it smile. In addition, the eyes in Examples 1and 2 are edited to look downwards. These observations match ouranalysis of the average editing coefficient, where smilingness andvertical face orientation are attractive editing directions.",
  "Environment. We open source the implementation of AdSEE4": "so our method can be easily studied, reproduced, and extended.For all the experiments, we implement our model with PyTorch1.7.0 in Python 3.7.16 environment and train on a Tesla P40GPU with a memory size of 24 GB. We also try our system on anRTX 2080Ti GPU with 11GB of memory, which can still handleour entire AdSEE system efficiently when tuning down the batchsize hyperparameters. We provide the virtual environment anddependency setup script in our code repository for reproducibility.Pre-trained Models. Besides the important e4e encodermodel, the StyleGAN2-FFHQ generator, and the SOLO instancesegmentation model described in , we enumer-ate all the pre-trained models in our system. We adopt the pre-trained Bert-Chinese model to extract the 768-dimensionaltext embedding of the ad query texts as a dense feature. Within the",
  ": Examples of s enhanced by AdSEE where we showthe category, text, and cover image. Left: Original coverimage, Right: Enhanced cover image": "Face Segmentation Module, we utilize the Dlib face alignmentmodel to extract aligned human faces. As for the Image Embed-ding Model, we use the Tencent internal Sougou Image EmbeddingModel and multi-label classification model described in for experiments on QQ-AD dataset. In addition, we adopt the pub-licly available ResNet-18 model as the image embedding model for experiments on the public CreativeRanking dataset. Forthe CRP-NIMA baseline model, we use the NIMA image quality as-sessment model pre-trained on the AVA dataset to obtainthe ad cover image quality score mean and standard-deviation forall cover images in the QQ-AD dataset.Hyper-parameters and Implementation Details. For all ofthe base recommender models including AutoInt, we adopt theimplementations from the DeepCTR library and use the de-fault hyperparameters of each model. For both of the QQ-AD andCreativeRanking , we use the train set to train the model anduse the validation set to tune the hyper-parameters, select featuresand determine early stop, and evaluate the performance on the testset. For the CRP model training on both datasets, we find a learningrate of 1 4 performs well and we use a batch size of 256. For theCRP model trained on the QQ-AD dataset and CreativeRankingdataset, we train them for 37 epochs and 18 epochs respectively.For the GADE module, we use the following settings for exper-iments on the QQ-AD dataset. In Algorithm 1, we set the Popu-lationSize to 75 and set the NumGenerations to 20. In the ParentSelection step, we select 10 genotypes as parents by performingthe rank selection method. In the Crossover step, the parents in themating pool will create 65 off-springs using the uniform crossoveroperation. Then, the 10 parents are combined with the 65 offspringto form a new population of 75 genotypes. Next, we randomly select20% of the 75 genotypes to mutate in the Mutation step. For eachgenotype selected for mutation, we randomly change one of itsgenes by perturbing its value by a value in the range of [0.1, 0.1].The search space for each gene value is limited to a value betweenthe range of with a step of 0.1. The gene values are randomlyinitialized to a value in the range of . In each genotype, wehave 20 genes that correspond to the top 20 editing directions foundby SeFa.On the CreativeRanking dataset, we use a slightly differentset of settings for the GADE module that is suitable for this dataset.We use a PopulationSize of 30 and set the NumGenerations to 5.In the Parent Selection step, we select 10 genotypes as parents byperforming the rank selection method. In the Crossover step, theparents in the mating pool will create 20 off-springs using theuniform crossover operation. Then, the 10 parents are combinedwith the 20 offspring to form a new population of 30 genotypes.Next, we randomly select 20% of the 30 genotypes to mutate inthe Mutation step. For each genotype selected for mutation, werandomly change one of its genes by perturbing its value by avalue in the range of [0.01, 0.01]. The search space for each genevalue is limited to a value between the range of [1.5, 1.5] with astep of 0.01. The gene values are randomly initialized to a value inthe range of [0.1, 0.1]. In each genotype, we have 20 genes thatcorrespond to the top 20 editing directions found by SeFa.For the operation used to convert face latent codes to a fixedlength, we compare four operations including max-pooling, average-pooling, aggregation and concatenation with padding to a fixedlength. We found max-pooling to be the best performer on bothdatasets and use the max-pooling operation throughout our experi-ments. In the implementation, we apply standardization to the clickrate label and we predict the standardized click rate. Even moredetailed implementation-related settings and their values can befound in the code."
}