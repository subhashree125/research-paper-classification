{
  "ABSTRACT": "Harnessing Large Language Models (LLMs) for recommendationis rapidly emerging, which relies on two fundamental steps tobridge the recommendation item space and the language space:1) item indexing utilizes identifiers to represent items in thelanguage space, and 2) generation grounding associates LLMsgenerated token sequences to in-corpus items. However, previousmethods exhibit inherent limitations in the two steps. Existing ID-based identifiers (e.g., numeric IDs) and description-based identifiers(e.g., titles) either lose semantics or lack adequate distinctiveness.Moreover, prior generation grounding methods might generateinvalid identifiers, thus misaligning with in-corpus items.To address these issues, we propose a novel Transition paradigmfor LLM-based Recommender (named TransRec) to bridge itemsand language. Specifically, TransRec presents multi-facet identifiers,which simultaneously incorporate ID, title, and attribute for itemindexing to pursue both distinctiveness and semantics. Additionally,we introduce a specialized data structure for TransRec to ensuregenerating valid identifiers only and utilize substring indexing toencourage LLMs to generate from any position of identifiers. Lastly,TransRec presents an aggregated grounding module to leveragegenerated multi-facet identifiers to rank in-corpus items efficiently.We instantiate TransRec on two backbone models, BART-large andLLaMA-7B. Extensive results on three real-world datasets underdiverse settings validate the superiority of TransRec. Corresponding author. This research is supported by A*STAR, CISCO Systems (USA)Pte. Ltd and National University of Singapore under its Cisco-NUS Accelerated DigitalEconomy Corporate Laboratory (Award I21001E0002), and the National Natural ScienceFoundation of China (62272437). Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "LLM-based Recommendation, Item Indexing, Generation Ground-ing, Multi-facet Identifier": "ACM Reference Format:Xinyu Lin, Wenjie Wang, Yongqi Li, Fuli Feng, See-Kiong Ng, and Tat-SengChua. 2024. Bridging Items and Language: A Transition Paradigm for LargeLanguage Model-Based Recommendation. In Proceedings of the 30th ACMSIGKDD Conference on Knowledge Discovery and Data Mining (KDD 24),August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages.",
  "INTRODUCTION": "Large Language Models (LLMs) have achieved remarkable successacross diverse domains due to their emergent com-petencies, including possessing rich knowledge , instructionfollowing , and in-context learning . Recently, there hasbeen a notable surge in exploring the benefits of adapting LLMs torecommendations. In particular, LLMs have showcased the potentialin discerning nuanced item semantics , understandingmultiple user interests , and generalizing to cold-startitem recommendations . In light of these, the prospectof harnessing LLMs as recommender systems, i.e., LLM-basedrecommenders, emerges as a particularly promising avenue forfurther exploration.Given that recommender systems and LLMs work in the itemspace and the language space, respectively, the key to buildingLLM-based recommenders lies in bridging the item space and thelanguage space (refer to ). Here, the item space includesall the existing items on the recommender platform. To bridgethe two spaces, it involves two essential steps: item indexing andgeneration grounding. The item indexing step assigns each itemwith a unique identifier (e.g., item title or numeric ID) in naturallanguage, and subsequently the users historical interactions areconverted into a sequence of identifiers. To yield recommendations,given the identifier sequence of the users historical interactions,the generation grounding step utilizes LLMs to generate a tokensequence in the language space and inversely ground the token",
  ": Illustration of the two pivotal steps for LLM-basedrecommenders: item indexing and generation grounding": "sequence to the items in the item space. However, existing workhas intrinsic limitations in both steps.Item indexing. Previous studies can be categorized into ID-based identifiers and description-based identifiers . ID-based identifiers utilize numeric IDs (e.g., 15308) to representitems, effectively capturing the uniqueness of items .Nevertheless, IDs lack explicit semantics1 and hinder the knowl-edge generalization of LLMs. Worse still, LLMs require sufficientinteractions to fine-tune each ID identifier, decreasing the gener-alization ability to large-scale and cold-start recommendations. Description-based identifiers adopt semantic descriptions (e.g.,titles and attributes) to index items . However, item de-scriptions lack adequate distinctiveness due to the existence ofcommon words (e.g., Will and Be in movie titles). Moreover,item descriptions might not consistently align with user-iteminteractions: two items with similar descriptions may not havesimilar interactions. This divergence is a possible reason for thewide usage of IDs in existing feature-based recommendations. Generation grounding. In previous work, LLMs autoregres-sively generate a sequence of tokens via beam search, and thenground the token sequence to the identifiers of items via exactmatching . Nevertheless, unconstrained generation over thewhole vocabulary of LLMs may yield invalid item identifiers,leading to out-of-corpus recommendations . Thus, additionalmatching strategies (e.g., L2 distance matching ) are necessary toground the out-of-corpus identifiers to existing identifiers, whichhowever is computationally expensive (cf. .3.2).Drawing upon the above insights, we establish some keyobjectives for the two steps. For item indexing, we posit thatitem identifiers should at least satisfy two criteria: 1) distinctivenessthat ensures the items are distinguishable from each other; and 2)semantics that guarantees the full utilization of the rich knowledgein LLMs, enhancing the generalization abilities of LLM-based rec-ommenders. For generation grounding, we consider constrainedgeneration to ensure generating valid identifiers without additionalmatching. Nonetheless, a crucial challenge is that constrainedgeneration heavily relies on the generation quality of the initialtokens during beam search. Because constrained generation strictlystarts from the first token of valid identifiers , LLMs cannotgenerate the ideal item if the generated first token is incorrect. Assuch, we consider position-free constrained generation to allow LLMsto generate from any position in the valid identifiers.To improve item indexing and generation grounding as thebridge between the item and language spaces, we propose a noveltransition paradigm for LLM-based recommenders (shorted asTransRec). Specifically, 1) as depicted in (a), TransRec",
  "Generated Identifiers": ": Overview of TransRec. Item indexing assigns eachitem a multi-facet identifier. For generation grounding,TransRec generates a set of identifiers in each facet and thengrounds them to in-corpus items for ranking. indexes items with multi-facet identifiers, which simultaneouslyconsiders item IDs, titles, and attributes (e.g., category) as theidentifiers in separate facets to pursue both distinctiveness andsemantics. And 2) for generation grounding, to achieve position-freeconstrained generation, we introduce FM-index, a data structurethat supports LLMs to generate any segment of valid identifiers.Besides, to further enhance the position-free generation abilityof LLMs, we propose also using substrings of identifiers to indexitems (e.g., Lipstick-Red in Everyday Elegance Lipstick-Red) forinstruction tuning. Lastly, LLMs will generate valid identifiers ineach facet as in (b), and we present an aggregated groundingmodule to leverage generated identifiers to rank in-corpus items.To validate the effectiveness of TransRec, we conduct extensiveexperiments on three real-world datasets under diverse settings,including full training and few-shot training with warm- and col-start testings. Empirical results on two backbone models BART-large and LLaMA-7B reveal the superiority of TransRecover traditional models and LLM-based models. The code and dataare at summary, this work offers several significant contributions:",
  "PRELIMINARY": "This section first introduces the process of establishing LLM-based recommenders, including instruction tuning and generationgrounding. And then, we highlight the two key steps to bridgebetween the item space and the language space, and reveal thevulnerabilities of existing work. Task formulation. Let U and I denote the sets of users anditems, respectively. We represent the historical interaction sequenceof a user by S = [1,2, . . . ,] in a chronological order, where U, I, and = |S|. Given a users historical interactions,the sequential recommendation task2 is to predict this users nextliked item +1 I.",
  ": Illustration of instruction tuning of LLMs. (a)depicts the conversion from recommendation data to instruc-tion data; (b) presents the optimization of LLMs based on theinstruction data": "To leverage LLMs for recommendation, existing work mainlyresorts to instruction tuning, to narrow the intrinsic discrepancybetween the LLMs pre-training data and the recommendationdata. After instruction tuning, LLMs are instructed to generaterecommended items based on the users historical interactions.In the following, we introduce the preliminary knowledge ofinstruction tuning and generation grounding.",
  "Instruction Tuning": "Instruction tuning involves three phases: item indexing, datareconstruction, and LLM optimization. Item indexing. For each item I in recommendation data, itemindexing assigns an identifier I in natural language, where Iis the identifier corpus. To achieve item indexing, exiting work canbe categorized into ID-based identifiers (e.g., numeric IDs ),and description-based identifiers (e.g., titles and attributes ). Data reconstruction. Based on the item identifiers, each usersinteraction sequence S = [1,2, . . . ,] could be converted to asequence of identifiers S = [1, 2, . . . , ]. Thereafter, instructiondata D = {(,)} are constructed for instruction tuning,where and denote the instruction input and output, respectively.Specifically, as shown in (a), the instruction input containsthe task description illustrating the recommendation task; and theusers historical interactions [1, 2, . . . , 1] in natural language.The instruction output is usually set to the identifier of the next-interacted item, i.e., = . LLM optimization. Given the instruction data D, thelearnable parameters ( ) of an LLM can be optimized byminimizing the negative log-likelihood of instruction output conditioned on input :",
  "Generation Grounding": "After the instruction tuning, we can effectively leverage LLMs togenerate recommendations via the generation grounding step, i.e.,generating token sequences by LLMs and grounding them to thein-corpus items. Generation. Given an instruction input, which contains the taskdescription and the users historical interactions [1, 2, . . . , ], LLM-based recommender autoregressively generates a token sequence step by step via beam search. Formally, when beam size = 1, ateach time step , we have",
  "= arg maxV (| <,),(2)": "where V is the token vocabulary of the LLM. The LLM-basedrecommender keeps generating until it meets stopping criteria (e.g., is the stop token EOS). Grounding. The generated token sequence in the languagespace is then grounded to a set of existing identifiers as recommen-dations:{| I} Ground( ), where Ground() is the grounding approach, such as exact match-ing , and distance-based matching .To sum up, bridging the item space and the language space forbuilding LLM-based recommenders involves two fundamental steps:item indexing and generation grounding. Upon the two steps,LLMs can follow the standard operations of data reconstruction andinstruction tuning in the language space. However, previous worksuffers from intrinsic limitations in the two steps. For item indexing,existing ID-based identifiers and description-based identifiers eitherlose semantics or lack adequate distinctiveness, leading to theunderutilization of rich knowledge in LLMs or losing salientfeatures crucial to the recommendation. Notably, little effort hasbeen made to study or compare the ID- and description-basedidentifiers specifically for LLM-based recommendation (relatedwork is discussed in ), where the potential enhancementfrom the incorporation of both IDs and semantics specificallyfor LLM-based recommendation deserves more exploration. Forgeneration grounding, Eq. (2) allows for generating any tokenfrom the LLMs vocabulary at each step, potentially leading toout-of-corpus identifiers. Additional matching approaches canmitigate the out-of-corpus issue, which however is time-consuming(cf. .3.2).",
  "Multi-facet Item Indexing": "To alleviate the intrinsic limitations of existing item indexing meth-ods, we postulate two criteria for item identifiers: 1) distinctivenessto ensure the items are distinguishable from each other; and 2)semantics to make full utilization of rich knowledge in LLMs,enhancing the generalization abilities. 3.1.1Multi-facet Identifier. Well meeting the above two criteria,we propose multi-facet identifiers for the item indexing step. Inparticular, we simultaneously incorporate three facets to representan item from different aspects: Numeric ID guarantees the distinctiveness among items. Weassign each item a unique random numeric ID, denoted by (e.g.,3471). By tuning over user-item interactions described by unique",
  "Given the following purchase history of a user, what is the next possible item to be purchased by the user? 15826; 8792; 513; 7382; 9014; || ID || +": "Given the following purchase history of a user, what is the next possible item to be purchased by the user? Wilson NBA Basketballs; Advancourt Sneakers; ; Logitech K270 Wireless Keyboard; || title || + Given the following attributes of purchase history of a user, what is the next possible attribute of item to be purchased by the user? Sports; Shoe; Headphone & Earphones; ; Electronics; || attribute || +",
  ": Illustration of the reconstructed data based on themulti-facet identifiers. The bold texts in black refer to theusers historical interactions": "IDs, LLMs are incentivized to align the numeric IDs with the user-item interactions, which can capture crucial Collaborative Filtering(CF) knowledge. In essence, items with similar interactions areendowed with similar ID representations. Item title ensures rich semantics that can capitalize the wealth ofworld knowledge in LLMs. An item title, denoted by , e.g., RougeCoco Hydrating Creme Lipstick Chanel #432, typically contains aconcise and descriptive name, conveying some general informationabout the item. Item attribute serves as a complementary facet to injectsemantics, particularly in cases where item titles may be lessinformative or unavailable. For an item that has multiple attributessuch as Makeup, Eyes, Multicolor, Gluten Free, we denote eachattribute as and the complete attributes as = [1,2, . . . ,].In summary, for each item in the recommendation data, wecan obtain the multi-facet identifier = {,,}. Based on themulti-facet identifiers, we then construct the instruction data inlanguage space for the instruction tuning of LLMs. 3.1.2Data Reconstruction. As shown in , we converteach users interaction sequence S = [1,2, . . . ,] into instruc-tion data in three facets, separately. For each facet, we constructinstruction input and output based on the users interactionsequence. We fix the templates of task descriptions3, and mainlyfocus on the reconstruction of the users historical interactions andthe instruction output in the following.To form the users historical interactions for the ID facet, weconvert the first 1 items in S to their numeric IDs, andthen separate each item with a semicolon. A sequence of ID-facet identifiers is denoted as 1; 2; . . . ; 14. Likewise, wecan obtain the users historical interactions in the title and attributefacets, referred to as 1; 2; . . . ; 1, and 1; 2; . . . ; 1,respectively. As for the instruction output, for the ID facet, we usethe numeric ID of the last item in the users interaction sequence, i.e.,. For the title facet, we utilize substrings with arbitrary length {1, . . . , | |}, sampled from the title . This is to encourageLLMs to generate from any positions that are possibly relevantto the users interests. Here, for each users interaction sequence,we sample substrings of the last items title and construct instruction input-output pairs. Lastly, for the attribute facet, eachattribute is independently used as one instruction output,resulting in || instruction input-output pairs. We denote the sets",
  ": Demonstration of the generation grounding step inTransRec. Red, blue, and green denote the facets of ID, title,and attribute, respectively": "of the instruction data from ID, title, and attribute facets as D,D, and D, respectively. Moreover, to explicitly distinguishdifferent facet data, we add a facet prefix after the instruction inputas shown in .Based on the reconstructed instruction data D = D D D , the LLM is optimized via Eq. (1). Note that we onlyemploy a single LLM in TransRec for the instruction tuning.",
  "After instruction tuning, the next step of TransRec is generationgrounding (see ), which aims to deliver in-corpus itemrecommendations based on the users historical interactions": "3.2.1Position-free Constrained Generation. Out-of-corpusidentifiers and over-reliance on the quality of initially generatedtokens are two critical problems in the generation process. To tacklethe issues, we consider LLMs to conduct position-free constrainedgeneration. Remarkably, we introduce FM-index , a specializeddata structure, that simultaneously supports position-free andconstrained generation. FM-index. FM-index is a special prefix tree that supports searchfrom any position . This capability enables FM-index to 1) find allvalid successor tokens of a given token; and 2) allow the generationto start from any token of the valid identifiers5. Specifically, takingthe item in (a) as an example, we flatten the multi-facetidentifier as <IDS> 1023 <IDE> Urban Decay Eyeshadow PaletteNaked Heat <AS> Makeup <AE> <AS> Eyes <AE>, where <IDS>,<IDE>, <AS>, <AE> are the special tokens that indicate the startand the end of each ID and fine-grained attribute, respectively. Theflattened identifier will then be stored in the Wavelet Tree .Given a start token (e.g., BOS) or a token sequence, the FM-indexcan find a list of all possible successor tokens in( log( )), where is the vocabulary size of the LLMs (refer to Appendix A.1 fordetailed explanations). Identifier generation. Given the users historical interactions inthe format of instruction input as in , TransRec generatesvalid identifiers in each facet via constrained beam search basedon the FM-index. Notably, the special tokens are utilized to indicatewhich facet is being generated. By constraining the starting tokenof generation, e.g., <IDS>, and the ending token of generation,e.g., <IDE>, TransRec generates a set of valid ID identifiers and",
  "Bridging Items and Language: A Transition Paradigm for Large Language Model-Based RecommendationKDD 24, August 2529, 2024, Barcelona, Spain": "Keqin Bao, Jizhi Zhang, Wenjie Wang, Yang Zhang, Zhengyi Yang, YanchengLuo, Fuli Feng, Xiangnaan He, and Qi Tian. 2023. A bi-step grounding paradigmfor large language models in recommendation systems. arXiv:2308.08434. Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He.2023. Tallrec: An effective and efficient tuning framework to align large languagemodel with recommendation. In RecSys. ACM.",
  "( ) (1 ( |)) },(3)": "where is the generated identifier, ( |) is the token scoregiven by the LLMs, and ( ) is the unconditional probability thatmeasures the frequency of tokens. For intuitive understanding,Eq. (3) can mitigate the issue by upweighting the tokens that aremore distinctive, i.e., less frequent. We obtain the unconditionalprobability ( ) by",
  "( ( )),(5)": "where is the multi-facet identifiers of the item, and is a hyper-parameter to control the strength of intra-facet grounding scores. Inter-facet aggregation. To aggregate the grounding scoresfrom three facets, we should consider the disparate influence ofeach facet in different scenarios. For instance, when seeking books,the titles may become crucial, while for restaurants, the category(e.g., Mexican cuisine) possibly takes precedence. As such, TransRecbalances the strength of each facet for the final ranking. Formally,the final grounding score for user and item is obtained by:",
  "grounding scores, we can obtain a ranking list of in-corpus itemsas in and return top-ranked items as recommendations": "Instantiation. TransRec is a model-agnostic method that can beapplied to any backbone LLMs and diverse tuning techniques. Toinvestigate the feasibility of TransRec on LLMs with different sizesand architectures, we instantiate TransRec on two LLMs, i.e., BART-large and LLaMA-7B . BART-large is an encoder-decodermodel with 406M parameters, and LLaMA-7B is a decoder-onlymodel with 7B parameters.",
  "Experimental Settings": "4.1.1Datasets. We conduct experiments on three popular bench-mark datasets: 1) Beauty is the collection of user interactionswith beauty products from Amazon review datasets7. 2) Toysis also one representative recommendation dataset drawn fromAmazon review datasets, where each toy product has substantialmeta information. 3) Yelp8 is a popular restaurant dataset with richuser interactions on extensive dining places. For the three datasets,we assign each item a random unique numeric ID. We then usethe assigned IDs, item titles, and item categories for the ID, title,and attribute facets for TransRec, respectively. Following , weadopt the leave-one-out strategy9 to split the datasets into training,validation, and testing sets. In addition, we consider two trainingsettings: 1) full training uses all users interactions in the trainingset to train the models; and 2) few-shot training randomly selects users interactions to train the models, where = 1024 or 2048.The statistics of the datasets are summarized in in Appendix. 4.1.2Baselines. We compare TransRec with both traditional rec-ommenders (MF, LightGCN, SASRec, ACVAE, and DCRec) and LLM-based recommenders (P5, SID, SemID+IID, CID+IID, and TIGER).1) MF is one of the most representative collaborative filteringmodels, which decomposes the user-item interactions into the userand the item matrices. 2) LightGCN is a graph-based modelwhich linearly propagates the user and item representations fromthe neighborhood. 3) SASRec is a representative sequentialrecommender model that adopts self-attention mechanism to learnthe item dependency from users interactions. 4) ACVAE incorporates contrastive learning and adversarial training intothe VAE-based sequential recommender model. 5) DCRec employs contrastive learning and disentangles user interest andconformity to mitigate the popularity bias. 6) P5 is a unified",
  "KDD 24, August 2529, 2024, Barcelona, SpainXinyu Lin et al": "framework, which uses additional data (e.g., users reviews) for pre-training of LLMs on various tasks. We train P5 on sequential tasksfor a fair comparison and assign random numeric IDs to items toprevent potential data leakage issue (cf. Appendix A.6). 7) SID leverages collaborative information by sequential indexing. Theitems interacted consecutively by a user are assigned consecutivenumeric IDs. 8) SemID+IID designs the numeric ID basedon the items meta information such as attributes, where itemswith similar semantics have similar numeric IDs. 9) CID+IID considers the co-occurrence matrix of items to design the numericID, where items that co-occur in user-item interactions will havesimilar numeric IDs. 10) TIGER generates item identifierthrough a trainable codebook, which utilizes the item title anditem descriptions to create new item tokens entailed by semantics.Evaluation. We adopt the widely used metrics Recall@ andNDCG@ to evaluate the models , where is 5 or 10. 4.1.3Implementation Details. We employ BART and LLaMAas backbone LLMs for TransRec, and we denote the two variantsas TransRec-B and TransRec-L, respectively. For TransRec-B,we follow to sample subsequences of users interactionsfor training, which is widely used in sequential recommendermodels . As for LLaMA, the training on subsequences isinvolved in the training objectives of decoder-only architecture ,and only uses the entire user sequence for training. Besides, foreach users interaction sequence, we iteratively discard the firstitem in the sequence until the length of instruction input does notexceed the maximum input length of LLMs (1024 for BART and512 for LLaMA). TransRec is trained with Adam (TransRec-B)and AdamW (TransRec-L) on four NVIDIA RTX A5000 GPUs.We fully tune the model parameters of TransRec-B and performthe parameter-efficient fine-tuning technique LoRA to tuneTransRec-L. For a fair comparison, we set the beam size to 20 forTransRec and all LLM-based baselines. Detailed hyper-parametersettings for baselines and TransRec are presented in Appendix A.2.",
  "The results of the baselines and TransRec with BART as thebackbone model under the full training setting are presented in, from which we have the following observations:": "Among traditional recommenders, sequential methods (SASRec,ACVAE, DCRec) surpass non-sequential methods (MF and Light-GCN) on both Beauty and Toys. The better performance stemsfrom the sequential modeling of the users interaction sequence,which captures dynamic shifts in user interests and intricate itemdependencies. Moreover, ACVAE usually outperforms other tra-ditional recommenders. This is because adversarial training andcontrastive learning encourage high-quality user representationand enhance the discriminability between items. CID+IID consistently yields better performance than SemID+IID,which is consistent with the findings in . This is reasonablesince CID+IID leverages the co-occurrence of items to constructhierarchical numeric IDs, i.e., items with similar interactions havesimilar IDs. As such, the IDs are integrated with collaborativeinformation, which strengthens the key advantage of ID-basedidentifiers. In contrast, SemID+IID simply constructs IDs based on items meta information, i.e., items with similar semanticshave similar identifiers. However, this can lead to misalignmentbetween item identifiers and user behavior, thus degrading theperformance (cf. ). TIGER usually achieves comparable performance to most of thetraditional recommenders and surpasses SemID+IID on bothBeauty and Toys. The better performance is attributed to 1) theadditional utilization of description for capturing semantics; and2) the learnable codebook to learn nuanced semantics comparedto the manually defined semantic IDs (SemID+IID). Besides, P5yields unsatisfactory performance on the three datasets. Webelieve that the inconsistency with the observations in isdue to the sequential indexing strategy, which potentially leadsto data leakage . TransRec consistently yields the best performance across thethree datasets, validating the superiority of our proposed tran-sition paradigm. Notably, TransRec outperforms LLM-basedrecommenders by a large margin without requiring informationon cold-start items to construct item identifiers. This furtherdemonstrates the strong generalization ability of TransRec (seemore analysis on the generalization of TransRec in Appendix A.7).The superiority of TransRec is attributed to 1) the utilization ofmulti-facet identifiers, which simultaneously satisfies semanticsand distinctiveness to leverage the rich knowledge in LLMs andcapture salient item features; and 2) the constrained and position-free generation that guarantees in-corpus item generation andmitigates the over-reliance on initial tokens.",
  "In-depth Analysis": "4.3.1Few-shot Training (RQ2). To study how TransRec per-forms under limited data, we conduct few-shot training withrandomly selected users interactions, where is set as 1024or 2048. To evaluate the models, we involve users in few-shottraining (i.e., warm-start users) and another randomly selected users that have not been seen in few-shot training (i.e., cold-startusers) for testing. In addition, we split the testing set into warmand cold sets, where interactions between warm-start users andwarm-start items belong to the warm set, otherwise the cold set.We compare both TransRec-B and TransRec-L with competitivebaselines. The results on warm and cold sets over Beauty10 arepresented in . It is observed that 1) traditional methodsyield competitive performance on warm-start recommendation.This is because these ID-based methods are effective in capturingcollaborative information from user-item interactions. ACVAEyields better performance on cold sets, as it discards user em-bedding, enabling effective generalization for cold-start users. 2)Medium-sized LLMs (CID+IID and TransRec-B) struggle with few-shot training. This indicates that a considerable amount of datais necessary to adapt medium-sized LLMs to perform well onrecommendation tasks, which is also consistent with previouswork . The better performance of CID+IID compared to TransRec-B possibly arises from the utilization of all item informationto assign identifiers, as opposed to our strict reliance solely onwarm items. 3) Notably, TransRec-L outperforms all the baselines,particularly surpassing by a large margin on the cold set. This",
  "highlights the remarkable generalization ability of recently emergedLLMs with vast knowledge base, enabling more effective adaptationto the recommendation task with limited data": "4.3.2Ablation Study (RQ3). To analyze the effect of each facetin multi-facet identifiers, we remove the ID, title, and attributeseparately, referred to as w/o ID, w/o title, and w/o attribute,respectively. We also test TransRec with a single facet, i.e., ID, title,and attribute, respectively. In addition, we disable the FM-indexand conduct unconstrained generation, denoted as w/o FM-index.Results on Beauty are presented in . From the figure, we canfind that: 1) removing either ID, title, or attribute facet will decreasethe performance, indicating the effectiveness of each facet inrepresenting an item for LLM-based recommendation. 2) Discardingthe ID or title facet typically results in more significant performancereductions compared to removing the attribute facet. This isreasonable since removing IDs falls short of meeting distinctivenesscriteria for identifiers, hindering LLMs from capturing salientfeatures of items. Meanwhile, titles tend to possess more intricatesemantics than attributes, exerting a more significant effect onenhancing recommendations. The crucial role of title facet isalso indicated by the performance of TransRec with each singlefacet. 4) It is not surprising that removing FM-index fails to give",
  ": Ablation study of each facet (i.e., ID, title, andattribute) of multi-facet identifier and the FM-index": "appropriate recommendations (inferior performance of w/o FM-index), because it may generate our-of-corpus identifiers. Thisimplies the necessity of position-free constrained generation.Effect of grounding strategies. We also compare the ag-gregated grounding module of TransRec with three potentialgrounding strategies. Following , we utilize LLMs to extractthe representations of generated identifiers and the identifiers ofin-corpus items, respectively. We then calculate the dot product,the negative L2 distance, and the cosine similarity between thegenerated identifiers and each in-corpus item as the groundingscore for each in-corpus item, respectively, as three strategies. Fromthe results in , we can observe that 1) potential groundingstrategies fail to yield satisfying results. This is because thesestrategies utilize the representations extracted from LLMs, thusrelying heavily on the semantics similarity. As such, they mayinaccurately ground the generated tokens that lack meaningfulsemantics to the valid identifiers. 2) TransRec is more time-efficient compared to other grounding strategies. Because the threestrategies introduce extra LLMs forward process for extracting therepresentations, causing high computation burdens. 4.3.3Hyper-parameter Analysis. We further study the sensitiv-ity of hyper-parameters to facilitate future applications of TransRec.The performance of TransRec with different values of and arepresented in . It is observed that 1) as varies from 1 to 3,the performance gradually improves. The possible reason is that thescales of original probabilities of LLMs are limited ( = 1), leading",
  ": Effect of and in TransRec": "to insufficient discrepancies between identifiers and consequentlyreducing the informativeness of the ranking. 2) However, it iscrucial not to indiscriminately increase , as this may lean towardsrecommendations of representative items. 3) We should carefullychoose to balance the strength between facets since a small weakens the consideration of salient features while a large might undermine other facets, thus hurting recommendations.Analysis of and are presented in Appendix A.8.",
  "RELATED WORK5.1LLMs for Recommendation": "Recently, leveraging LLMs for recommendation has received muchattention due to LLMs rich world knowledge, strong reasoning, andgeneralization abilities . Existing work onLLMs for recommendation can be mainly divided into two groups.1) LLM-enhanced recommenders , which considerLLMs as powerful feature extractors for enhancing the featurerepresentation of users and items. Another line of research lies in 2)LLM-based recommenders , which directly leverage theLLMs as recommender systems. In the wake of ChatGPTs releaseand its remarkable prowess in reasoning, early studies delve intoLLMs zero-shot/few-shot recommendation capabilities throughin-context learning . However, due to the intrinsic gapbetween the LLMs pre-training and recommendation tasks ,the recommendation abilities are somehow limited by merely usingin-context learning . Therefore, to narrow the gap and elicit thestrong capabilities of LLMs, recent studies utilize instruction tuningfor LLMs to improve the performance . In this work, wehighlight the two fundamental steps of LLM-based recommendersto bridge the item and language and spaces, showing existingproblems and the key objectives in the two steps.",
  "Item Indexing and Generation Grounding": "To bridge the item space and the language space, the two vital stepsare item indexing and generation grounding. Previous indexingmethods can be categorized into two groups. 1) ID-based identifiersrepresent each item by a unique numeric ID, to learn salient features from user-item interactions . 2) Description-basedidentifiers employ item descriptions to represent an item .Nevertheless, ID-based identifiers lack semantics, while identifier-based identifiers lack adequate distinctiveness. To alleviate theissues, propose to design IDs based on descriptions, whichhowever is essentially a description-based identifier and yields lesssatisfying recommendations (cf. .2). Although and compare the two types of identifiers independently in a discrimina-tive manner, TransRec employs the generative approach to leverageboth semantics and distinctiveness for effective item indexing. Aconcurrent study also explores multiview identifiers in passageranking. However, it solely considers the semantics to strengthenthe correlation between query and passages. In contrast, our studytakes the initial endeavor to investigate multi-facet identifiers inLLM-based recommendation, highlighting the distinctiveness as acriterion for identifier design to capture the crucial CF knowledge.The generation grounding step involves the generation of tokensequences and the grounding to the in-corpus items. However, thisstep has received little scrutiny in previous studies. They mainlyconduct autoregressive generation over the whole vocabularyand ground the generated tokens by exact matching, which maylead to out-of-corpus recommendations. and mitigate theout-of-corpus issue by utilizing the representations obtained byLLMs to match between generated tokens and in-corpus items,e.g., L2 distance. However, these approaches inevitably suffer fromhigh computational burdens as they require extra computationsfor representation extraction. Different from previous work, wepropose an effective generation grounding step, which utilizes aspecialized data structure to achieve guaranteed in-corpus andenhanced recommendations.",
  "CONCLUSION AND FUTURE WORK": "In this work, we highlighted the two fundamental steps for LLM-based recommenders: item indexing and generation grounding. Tomake full utilization of LLMs and strengthen the generalizationability of LLM-based recommenders, we posited that item identifiersshould pursue distinctiveness and semantics. In addition, we consid-ered position-free constrained generation for LLMs to yield accuraterecommendations. To pursue these objectives, we proposed anovel transition paradigm, namely TransRec, to seamlessly bridgethe language space and the item space. We utilized multi-facetidentifiers to represent an item from ID, title, and attribute facetssimultaneously. Besides, we employed the FM-index to guaranteehigh-quality generated identifiers. Furthermore, we introduced anaggregated grounding module to ground the generated identifiersto the items. Empirical results on three real-world datasets underdiverse settings validated the superiority of TransRec in improvingrecommendation accuracy and generalization ability.This work highlights the key objectives of indexing approachesand generation grounding strategies, leaving many promising direc-tions for future exploration. Particularly, 1) although incorporatingID, title, and attribute is effective, it is worthwhile to automaticallyconstruct multi-facet identifiers to reduce the noises in naturaldescriptions; and 2) it is meaningful to devise better strategies forgrounding modules, to effectively combine the ranking scores fromdifferent facets, such as using neural models in a learnable manner.",
  "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and YusukeIwasawa. 2022. Large language models are zero-shot reasoners. NeurIPS 35 (2022),2219922213": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, AbdelrahmanMohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoisingsequence-to-sequence pre-training for natural language generation, translation,and comprehension. arXiv:1910.13461. Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, and JulianMcAuley. 2023. Text Is All You Need: Learning Language Representations forSequential Recommendation. In KDD. ACM, 12581267. Ruyu Li, Wenhao Deng, Yu Cheng, Zheng Yuan, Jiaqi Zhang, and Fajie Yuan.2023. Exploring the Upper Limits of Text-Based Collaborative Filtering UsingLarge Language Models: Discoveries and Insights. arXiv:2305.11700.",
  "Zhengyi Yang, Xiangnan He, Jizhi Zhang, Jiancan Wu, Xin Xin, Jiawei Chen,and Xiang Wang. 2023.A Generic Learning Framework for SequentialRecommendation with Distribution Shifts. In SIGIR. ACM": "Fajie Yuan, Guoxiao Zhang, Alexandros Karatzoglou, Joemon Jose, Beibei Kong,and Yudong Li. 2021. One person, one model, one world: Learning continual userrepresentation without forgetting. In SIGIR. 696705. Zheng Yuan, Fajie Yuan, Yu Song, Youhua Li, Junchen Fu, Fei Yang, YunzhuPan, and Yongxin Ni. 2023. Where to go next for recommender systems? id-vs.modality-based recommender models revisited. In SIGIR. 26392649. Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-RongWen. 2023. Recommendation as instruction following: A large language modelempowered recommendation approach. arXiv:2305.07001.",
  ": Example of transformed matrix based on BWT": "As shown in the example, the first column is the repeated tokensorted lexicographically, and the last column is called the stringsBWT. Based on the matrix, it can then access every token throughself-indexing and can find all valid token successors. Notably, onlythe first and last columns are explicitly stored in the FM-index,contributing to its space efficiency. Then, the storage of thesecolumns is managed by the Wavelet Tree, a hierarchical structurethat employs wavelet transformations for compact encoding andrapid querying of the FM-index. This integrated approach enhancesthe overall performance and scalability of the data structure. Moredetailed explanations can also be found in .",
  "A.2Hyper-parameter Settings": "The best hyper-parameters of models are selected by Recall onthe validation set. For the traditional recommender models, wesearch the embedding size, learning rate, and weight decay from{16, 32, 64, 128, 256}, {14, 13, 12}, and {14, 13, 12, 11},respectively. The search scopes for some model-specific hyper-parameters are as follows. For LightGCN, we tune the num-ber of GCN layers and the dropout ratio in {1, 2, 3, 4, 5} and{0.1, 0.3, 0.5}, respectively. We search the training sequence lengthin {10, 20, 50, 100} for both SASRec and ACVAE. For SASRec, weselect the number of attention blocks in {1, 2, 3} and the number ofattention heads in {1, 2, 4, 8}. For ACVAE, the weight of contrastiveloss term is searched in {0.1, 0.3, 0.5, 0.7}. For LLM-based recom-menders , we follow the searching scopes stated in theirpapers. For TransRec, we set the sampling number of substrings as 5. The scaling factor for the intra-facet grounding scores and thebias terms for the intra-facet aggregation , , and are searched in ranges of {1, 2, 3, 4}, {0, 5, 8, 10},{2, 0, 2, 5}, and{2, 5, 7, 10}, respectively.",
  "A.3Performance of TransRec on T5-small": "To achieve fair comparisons, we employ a consistent backbone LLMand prompt template across all LLM-based methods. Specifically,we employ T5-small to instantiate SID, SemID+IID, CID+IID, andTransRec. Besides, to avoid the bonus from multiple prompttemplates, we implement all methods with the same single prompt.From the results in , we can find that TransRec outperformsbaselines by a large margin, which demonstrates the effectivenessof multi-facet identifier and generation grounding.",
  "A.5Constrained Generation with Trie": "We employ Trie, a data structure that supports generating validtokens strictly from the first token of the item identifier duringconstrained generation. From the results in , we can findthat generating from any position outperforms generating fromthe first token, which is due to potential misalignment between thefirst token and the user preference. This validates the effectivenessof position-free generation supported by FM-index.",
  "A.6Potential Data Leakage Issue of P5": "As discussed in , the original sequential indexing methodutilized in P5 suffers from the potential data leakage issue.Specifically, P5 assigns consecutive numeric IDs to the interacteditems in the user sequence, where the items in training andtesting sets have a high probability of sharing the same tokenafter tokenization. For instance, P5 may represent a user sequenceas [7391, 7392, . . . , 7398, 7399], where 7399 is in the testing set.Based on the SentencePiece tokenizer , these numeric IDs will be tokenized into 73 and 91, 73 and 92, and so forth. Assuch, the item identifiers in the training and the testing sets willshare the same token 73. This can lead to strong correlationsbetween the historical interactions and next-interacted items, thussignificantly benefiting the prediction accuracy of the testing item.However, such benefits are from the consecutive numbers, whichare unattainable during the indexing process in real-world scenarios.To solve this issue, we adopt the datasets in P5 for experimentsbut rearrange the numeric ID for items with random numeric IDinstead of consecutive IDs for both our method and P5.",
  "A.7User Group Evaluation": "To analyze how TransRec improves the performance and thegeneralization ability of LLM-based recommenders, we test theperformance of TransRec over different sparsity of users andcompare it with the competitive baseline CID+IID. Specifically,we divide the users into three groups according to the sequencelength of historical interactions. We select users with interactionslarger or equal to 8 into group 1, denoted as G1; and then splitthe rest of the users with interactions larger or equal to 4 intogroup 2, otherwise group 3, denoted as G2, and G3, respectively.As such, from G1 to G3, the user sparsity increases. The resultsof the three groups on Beauty are presented in . We canfind that: 1) From G1 to G3, the performance of both CID+IIDand TransRec decreases. This makes sense because it can bedifficult to capture the user preference shifts from only a smallnumber of interactions. Nevertheless, 2) TransRec consistentlyoutperforms CID+IID under different levels of user sparsity. Inparticular, TransRec improves the performance of sparse usersremarkably by a large margin (significant improvements on G3),indicating the strong generalization ability of TransRec.",
  "A.8Hyper-parameter Analysis": "Effect of . We vary the bias for the title facet i.e., ,and present the results in . It is observed that TransRecachieves the best performance when = 0, indicating thatbias for the title facet may not necessarily need careful adjustment.One possible reason is that titles usually contain common words,resulting in a mild gap between the pre-training data and the titles.In contrast, IDs that are less common in pre-training data probablylead to a larger gap between the pre-training data and the identifiers,thereby requiring a larger bias to improve the strength of the IDfacet. This is also evidenced by (b), where TransRec achievesthe best performance when = 5.Effect of . The results of different bias values forthe attribute facet are reported in (b). From theresults, we can find that 1) gradually increasing the bias for theattribute facet does not affect the performance too much, indicatingthat TransRec might be less sensitive to the attribute strength. Thisis reasonable since attribute entails some coarse-grained semantics,such as category, and color, which can be shared by extensive items.2) Similar to the ID facet, strengthening either the attribute or titlefacet too much can hurt the performance. The reason is that lettingthe title or attribute facet dominate the grounding can decreaseitem discriminability."
}