{
  "Abstract": "Graph Neural Networks (GNNs) have shown remarkable perfor-mance in various tasks. However, recent works reveal that GNNs arevulnerable to backdoor attacks. Generally, backdoor attack poisonsthe graph by attaching backdoor triggers and the target class labelto a set of nodes in the training graph. A GNN trained on the poi-soned graph will then be misled to predict test nodes attached withtrigger to the target class. Despite their effectiveness, our empiricalanalysis shows that triggers generated by existing methods tend tobe out-of-distribution (OOD), which significantly differ from theclean data. Hence, these injected triggers can be easily detected andpruned with widely used outlier detection methods in real-worldapplications. Therefore, in this paper, we study a novel problemof unnoticeable graph backdoor attacks with in-distribution (ID)triggers. To generate ID triggers, we introduce an OOD detectorin conjunction with an adversarial learning strategy to generatethe attributes of the triggers within distribution. To ensure a highattack success rate with ID triggers, we introduce novel modulesdesigned to enhance trigger memorization by the victim modeltrained on poisoned graph. Extensive experiments on real-worlddatasets demonstrate the effectiveness of the proposed method ingenerating in distribution triggers that can bypass various defensestrategies while maintaining a high attack success rate. Our code isavailable at:",
  "Backdoor Attack; Graph Neural Networks": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08 ACM Reference Format:Zhiwei Zhang, Minhua Lin, Enyan Dai, and Suhang Wang. 2024. Rethink-ing Graph Backdoor Attacks: A Distribution-Preserving Perspective . InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discoveryand Data Mining (KDD 24), August 2529, 2024, Barcelona, Spain. ACM, NewYork, NY, USA, 12 pages.",
  "Introduction": "Graph-structured data is pervasive in real world, such as social net-works , molecular structures , and knowledge graphs .With the growing interest in learning from graphs, Graph NeuralNetworks (GNNs), which have shown great ability in node repre-sentation learning on graphs, have become increasingly prominent.Generally, GNNs adopt the message-passing mechanism, whichupdate a nodes representation by recursive propagation and ag-gregation of information from a nodes neighbors. The learnednode representations preserve both node attributes and local graphstructure information, which can benefit a range of downstreamtasks, such as node classification , graph classification, and link prediction .Though GNNs have achieved remarkable performance acrossvarious applications, recent studies have shown that theyare vulnerable to backdoor attacks. Generally, backdoor attacksgenerate and attach backdoor triggers to a selected group of nodes,known as target nodes, and assign target nodes a target class. Trig-gers are typically a node or a subgraph and can either be predefinedor generated by a trigger generator. When a GNN model is trainedon a dataset poisoned with these triggers, it learns to associate thepresence of the trigger with the target class. Consequently, dur-ing inference, the backdoored model will misclassify test nodesattached with the trigger to the target class, while maintain highprediction accuracy on clean nodes, i.e., nodes without triggersattached. Backdoor attacks on graphs pose a significant threat tothe adoption of GNNs in real-world, especially on high-stake sce-narios such as banking systems and cybersecurity. For example, anadversary could inject backdoor triggers to the training data forfraud detection in transaction networks, and bypass the detectionof a model trained on such poisioned graph by disguising illegalbehaviors with backdoor triggers.Hence, graph backdoor attack is attracting increasing attentionand several initial efforts have been taken . For example,SBA conducts pioneering research on graph backdoor attacks.",
  "KDD 24, August 2529, 2024, Barcelona, SpainZhiwei Zhang, Minhua Lin, Enyan Dai, and Suhang Wang": "denotes nodes from target class, our goal is to generate triggersthat guide the surrogate model to learn a high cosine similaritybetween embeddings ( ) and (), while ensuring the similar-ity between ( ) and () is lower, where V\\V belongsto non-target class. Combining the aforementioned two goals, wepropose the following loss function for the trigger generator :",
  "Related Works2.1Graph Neural Networks": "With the increasing need for learning on graph structured data,Graph Neural Networks (GNNs), which have shown great power inmodeling graphs, are developing rapidly in recent years .Most GNN variants operate under the message-passing framework,which integrates pattern extraction and interaction modeling acrosseach layer . Essentially, GNNs handle messages derivedfrom node representations, propagating these messages throughvarious message-passing mechanisms to enhance node represen-tations. These refined representations are subsequently applied todownstream tasks With the evolution of GNN technology, numer-ous advancements have been made to augment their performanceand application scope. Innovations in self-supervised learning tech-niques for GNNs aim to lessen the dependency on annotated data. Additionally, significant strides have been madein enhancing the fairness , robustness and interpretabilityof GNN frameworks . Furthermore, specialized GNN archi-tectures have been developed to address the unique challengespresented by heterophilic graphs , broadening the potential usecases of GNNs in complex networked systems.",
  "Backdoor Attacks on Graph": "Backdoor attacks have been widely studied in image domain . Initial work directly poison training samples . Othershave explored the invisibility of triggers . Besides, the hid-den backdoor could also be embedded through transfer learning, modifying model parameters , and adding extra maliciousmodules . Recent studies have begun to delve into backdoorattacks on GNNs, focusing on a strategy distinct from the moreprevalent poisoning and evasion attacks. Backdoor attacks involveinjecting malicious triggers in the training data, which cause themodel to make incorrect predictions when these triggers are pre-sented in test samples. This form of attack subtly manipulates thetraining phase of a model, ensuring it performs as expected underregular conditions but fails in the presence of trigger-embedded in-puts. Among the pioneering efforts, SBA introduced a methodfor injecting universal triggers into training samples through asubgraph-based approach. However, the attack success rate is poor.",
  "Rethinking Graph Backdoor Attacks: A Distribution-Preserving PerspectiveKDD 24, August 2529, 2024, Barcelona, Spain": "GTA furthered this by developing a technique for generatingadaptive triggers, customizing perturbations for individual samplesto enhance attack effectiveness. In UGBA , an algorithm for se-lecting poisoned nodes is introduced to optimize the utilization ofthe attack budget. Additionally, an adaptive trigger generator isemployed to create triggers that demonstrate a high cosine sim-ilarity to the target node. While GTA and UGBA achieve a highattack success rate, the generated triggers tend to be outliers. Thisis because it is more efficient for a victim model to associate outliertriggers with the target class, leading the unconstrained triggergenerator to exploit this shortcut for a higher attack success rate.The aforementioned graph backdoor methods either have lowattack success rate or outlier issues that makes them ineffective inpresence of outlier detection. A detailed review of existing outlierdetection on graph is given in Appendix D. Our proposed methodis inherently different from these methods as (i) we aim to generateunnoticeable in-distribution triggers capable of bypassing the com-monly used outlier detection methods in real-world applications.(ii) we focus on guaranteeing a high attack success rate with thesein-distribution triggers.",
  "Notations": "We denote an attributed graph as G = (V, E, X), where V ={1, . . . , } represents the set of nodes, E V V is the setof edges, and X = {x1, . . . , x } denotes the set of node attributes,with x being the attribute of node . The adjacency matrix of thegraph G is denoted as A R , where = 1 if nodes and are connected; otherwise, = 0. In this paper, we concentrate onbackdoor attack for semi-supervised node classification task withinthe inductive setting. Specifically, the training graph G includesa small subset of labeled nodes V V with labels as Y =1, . . . ,. The remaining nodes of G are unlabeled, denoted asV . We denote V = V V as the node set for the traininggraph. The test nodes, denoted as V , are not part of the traininggraph G, i.e., V V = . We aim to add backdoor triggerswithin budget to the training graph such that a GNN model trainedon the backdoored graph will be fooled to give targeted label fortest nodes attached with triggers.",
  "Preliminaries of Graph Backdoor Attacks": "Next, we elaborate on the attackers objectives, knowledge, and ca-pabilities, followed by the details of the inductive setting employedfor evaluating the attack.Attackers Goal: The attacker aims to add backdoor triggers, i.e.,nodes or subgraphs, to a small set of target nodes in the traininggraph and label them as a target class, such that a GNN modeltrained on the poisoned graph will memorize the backdoor triggerand be misguided to classify target nodes attached with triggersas the target class. Meanwhile, the attacked GNN model shouldbehave normally for clean nodes without triggers attached.Attackers Knowledge and Capability: In the context of mostpoisoning attacks , attackers have access to the training data of",
  "None84.930.4 | 84.732.0 | 84.686.6 | 84.992.3 | 84.9OD84.829.6 | 84.931.7 | 84.60.0 | 85.00.0 | 84.7": "the target model. However, they lack information about the specificsof the target GNN models, including their architecture. Attackershave the capability to attach triggers and labels to nodes withina predefined budget prior to the training of the target models inorder to poison the graphs. In the inference phase, attackers retainthe ability to attach triggers to the target test nodes.Evaluation Setting: Given V V as a set of poisoned node,we attach the generated trigger = (X , A ) to the node Vand assign V with target class to form the backdoored dataset.The victim model is then trained on this dataset. During inference,triggers generated by trigger generator are attached to test nodes V to evaluate the attack performance.",
  "Outlier Issues of Graph Backdoor Attacks": "An implicit requirement for backdoor attacks is that the generatedtriggers should be indistinguishable from clean inputs. This condi-tion is commonly satisfied in the image domain by constrainingbackdoor triggers to the input, such as using small patch patternsor imperceptible perturbations. However, in the context of back-door attacks on graphs, where new samples are generated, withoutspecific design to constrain in-distribution trigger generation, thetrigger generator may take a shortcut and produce outlier triggerswhich can be easily memorized by the victim model. Though suchtriggers have high attack success rate, they are outliers and canbe easily removed by simple outlier detection algorithms, makingthem ineffective in practice.To show that the triggers generated by existing graph backdoorattack methods are outliers, we conduct analysis on Pubmed dataset. We first adopt existing backdoor attack algorithms to addbackdoors to the graph under the semi-supervised setting. Wethen apply Principal Component Analysis (PCA) to reduce thedimensionality of node attributes for both clean nodes and triggersnodes, and visualize them in a 2-dimensional space as shown in , where the blue and the red dots denote the clean node and thegenerated triggers, respectively. From the figure, it is obvious thatthe generated triggers of GTA and UGBA are very differentand far from the clean data distribution, showing that the triggersgenerated by many existing algorithms are outliers.To show that such triggers are ineffective in practice, i.e., canbe easily detected, we employ outlier detection (OD) to defendagainst existing backdoor attacks. Specifically, we adopt DOMI-NANT , a popular unsupervised outlier detection method basedon autoencoder for graph-structured data, which utilize the recon-struction error on both graph structural and node attribute foroutlier detection. The intuition is that the autoencoder will be bet-ter at reconstructing instances that are similar to the majority ofthe data it was trained on (presumably normal data) and worse atreconstructing outliers . Given a backdoored dataset, we trainDOMINANT on it and then discard those samples with high re-construction loss. Experiment results on Pubmed with |V |",
  ": PCA visualization of features of clean and generated triggers by different attacks. Red dots are overlapped in (a) asSBA generate the same trigger for all target nodes": "set as 40 are presented in . The architectures of the targetmodel is GCN and the size of triggers is limited to containthree nodes. We filter out the top 3% of samples with the highestreconstruction losses. More results on other datasets can be foundin . The accuracy of the backdoored GNN on clean test set isalso reported in to show how the defense strategy affect theprediction performance. Accuracy on a clean graph without anyattacks is reported as reference. All the results are averaged scoresof 5 runs. The details of evaluation protocol is in Sec 5.1.3. Fromthe table, we can observe: (i) Both GTA and UGBA exhibit a highattack success rate without a defense method. However, employinga straightforward outlier detection strategy effectively eliminatesall of their triggers, degrading the attack success rate to 0; (ii) ForSBA, which generates triggers based on the mean and standarddeviation of the clean input, it achieves a low attack success ratedespite its triggers not being classified as outliers. Evidently, ex-isting backdoor attacks methods on graph suffer from either a lowattack success rate or outlier issues. Thus, it is important to designa framework capable of generating ID triggers that can achieve ahigh attack success rate and bypass outlier detection.",
  "Problem Definition": "Our preliminary analysis shows that existing backdoor attacks ei-ther have a low attack success rate or encounter outlier issues. Toaddress these problems, we propose to develop a novel and effectivedistribution preserving graph backdoor attack that can generatein-distribution triggers capable of bypassing commonly employedoutlier detection techniques, while maintaining a high attack suc-cess rate. As we aim to bypass outlier detection techniques, wedefine the distribution preserving as follows.In-Distribution Constraint on Triggers. Let G = (V T, E E, XX) be the backdoored graph, where T represents the set ofgenerated triggers, E denotes the edge set containing edges withinthe triggers T and edges attaching these triggers to nodes V, and X represents the node attributes of the generatedtriggers. Let be an outlier detection model trained on G. Then,our in-distribution constraint on trigger is defined as:",
  "where () is the anomaly score of and is a threshold whichcan be tuned based on datasets.Following , the clean prediction for a node can be denotedas () = (G), where G is the -hop subgraph centered at": ". For a node attached with the trigger , the predicted labelis denoted as ( ), where = G,and () being theoperation of trigger attachment. With the above descriptions andnotations, the effective distribution preserving graph backdoorattack is formally defined as: Problem 1 (Distribution Preserving Graph Backdoor At-tack). Given a clean attributed graph G = (V, E, X) with a setof nodes V provided with labels Y, we aim to learn an adaptivetrigger generator () . This generator will produce triggersthat bypass outlier detection while ensuring that a GNN , trainedon the poisoned graph will classify the test node attached with thetrigger to the target class . This objective is achieved by solving:",
  "(2)": "where () is the cross entropy loss, is the target class label and denotes the parameters of the adaptative trigger generator . In theconstraints, the node size of trigger | | is limited by , and the sizeof poisoned nodes is limited by . The architecture of the target GNN is unknown. Hence, a surrogate GNN classifier with parameters is used.",
  "Methodology": "In this section, we present the details of the proposed framework,which aims to optimize Eq. (2) to conduct effective distributionpreserving graph backdoor attacks. Two challenges remain to beaddressed: (i) how to generate ID triggers that have the capabilityto bypass outlier detection defense methods; (ii) how to learn thetrigger generator to obtain triggers that meet ID constraint whilemaintaining a high attack success rate. To address these challenges,a novel framework DPGBA is proposed, which is illustrated in . DPGBA is composed of an OOD detector , a trigger generator and a surrogate node classifier . Specifically, to address the firstchallenge, an adversarial training strategy involving an OOD detec-tor and a trigger generator is introduced. The OOD detectoris trained to differentiate between clean data from a graph G andtriggers generated by . In turn, the trigger generator enhances itscapability to create triggers that closely mimic the clean data. To",
  "In-Distribution Triggers Generation": "In this subsection, we detail the design of the trigger generator and the adversarial learning strategy proposed to ensure thegenerator produces triggers that are in-distribution.To make the trigger generator more effective and flexible in gen-erating triggers, instead of using predefined triggers, following ,we use an MLP as the adaptive trigger generator , which generatetriggers based on the target nodes node attributes. Specifically,given a node , generates the node features and graph structureof a trigger to be attached to as:",
  "where x is the node attributes of , and W , W are the learnableparameters for feature and structure generation. We generate thesynthetic features X R and adjacency matrix A R": "for trigger nodes corresponding to node . In accordance with thediscrete nature of real-world graphs, we binarize A for the forwardcomputation to align with the binary structure of the graph, whilethe continuous adjacency matrix is utilized during the gradientcomputation in backpropagation following .To make sure that can generate distribution preserving trig-gers, we adopt the adversarial learning strategy. Specifically, weintroduce an OOD detector which aims to differentiate if aninput is from the original graph { V, = 1} or from generatedtriggers { T, = 0}. Following the Generative AdversarialNetwork (GAN) framework in , refines its ability to discernbetween clean inputs and generated triggers by minimizing thebinary classification loss. Concurrently, the generator craft trig-gers to deceive by maximizing the binary classification loss. Thismin-max game equips the generator with the ability to producetriggers that are indistinguishable from in-distribution data. Themin-max process is mathematically described as:",
  "Tlog(1 ()),(4)": "where V V is a selected set of representative nodes. The reasonwhy we select V instead of using V is because benchmark datasetsinherently contain outlier samples at the edges of the feature distri-bution . These outliers, when taken as inputs by the detector , simplify the task for the trigger generator to deceive by pro-ducing triggers similar to these outliers, thereby undermining theobjective of generating in-distribution triggers. Thus, the carefulchoice of representative samples for training the OOD detector iscritical. To obtain V, we pretrain an auto-encoder on the originalgraph G and select samples whose reconstruction losses are closeto the mean loss, e.g., within one standard deviation. The detailsare described in Appendix B. The idea behind using an autoencoderfor this purpose is that it learns to identify and capture the mostcrucial and frequently occurring features and patterns within thedata. These chosen samples can be regarded as typical based onthe criteria of reconstruction loss.",
  "Enhancing ID Trigger Effectiveness": "Though using an OOD detector and adversarial strategy restricts to generating in-distribution triggers, our empirical results indicatethat the attack success rate is not comparable to that achieved byGTA and UGBA . This is because the generated triggersby GTA and UGBA are outliers that deviate a lot from the origi-nal nodes, which makes it easier for the victim model to create ashortcut to associate the backdoor trigger with the target class. Incontrast, when using ID triggers, it becomes challenging for thevictim model trained on our poisoned dataset to discern a specifictrigger pattern, resulting in non-activation when we attach thetrigger to the target node. To ensure the attack performance withID triggers, we design novel modules to enhance trigger memoriza-tion by the victim model and improve attack adaptability againstunseen, new targets. Next, we give the details of each module. 4.2.1Enhancing Memorization of Triggers. A key to successfulgraph backdoor attacks is the ability of having the victim modelto correlate attached triggers with the target class. Thanks to themessage-passing mechanism, trigger attributes can directly influ-ence the attributes of the target nodes. Considering the diverseattributes across target nodes, we propose to encourage the genera-tor to produce triggers that, once attached to different target nodes,can prompt the victim model to learn similar embeddings for thepoisoned target nodes. Specifically, for each pair of nodes Vand , V, when attaching triggers to them, our objective isto ensure that these triggers can guide the surrogate classifier to learn a high cosine similarity between ( ) and ( ), where = G,denotes attached with backdoor trigger and ( ) represents the learned embedding of poisoned target node by the surrogate classifier, i.e., the last layer embedding of beforefeeding to Softmax function. This approach ensures that the triggerattributes significantly impact the target node attributes, makingthem the dominant features within the embeddings of poisonedtarget nodes learned by the victim model. This dominance ensuresthat the victim model memorizes these triggers more effectively,resulting in a higher likelihood of a successful attack.Moreover, once triggers have exerted a strong influence on thetarget node embeddings, enhancing the feature-level similarity be-tween the poisoned target nodes and the nodes of the target classcan further mislead the victim model into misclassifying the poi-soned target nodes as belonging to the specific target class. Specif-ically, for a pair of nodes V and V, where V V",
  "( ( ), ()) + V\\V ( ( ), ()) ),": "(5)where measures the cosine similarity of the embeddings. By min-imizing L, the victim model trained on the poisoned dataset canbetter correlate the presence of triggers with the target class, ulti-mately leading to a higher attack success rate. 4.2.2Enhancing Attack Effectiveness against Unseen Targets. Tofully harness the attack budget, we propose implementing a strategythat involves assigning varying weights to accessible nodes. Theidea is to enhance the adaptability and effectiveness of the triggergenerator against new and unseen targets by prioritizing nodesthat have proven to be particularly challenging to attack. The coreof our challenge lies in measuring the difficulty level of attackingeach node. To measure this, we employ the predicted probabilitydistribution provided by the surrogate model for poisoned nodes.Specifically, for a given node V, = ( ) gives theprobability that poisoned node is classified to the target class bysurrogate model. A large indicates a successful attack, suggestingthat the trigger generator has effectively learned to attack this target,and therefore, we assign it a smaller weight. Conversely, a targetwith a small is considered more challenging and is assigned alarger weight, directing the trigger generator to focus more on thistarget. Then, we integrate this strategy into the outer loss in Eq. (2)and obtain:",
  "V ( ( ) ,) ,(7)": "where represents the parameters of the surrogate model , isthe label of labeled node V and is the target class label.Then, with L in Eq. (6) aimed at misleading the surrogate model to predict various nodes from V to be once attached withgenerated triggers, L in Eq. (4) constraining the in-distributionproperty of generated triggers, and L in Eq. (5) enhancing the",
  ".. = arg minL (,)(8)": "where and are scalars to control the contributions of L andL, , and represent the parameters for trigger generator ,surrogate model and OOD detector , respectively. We adoptbi-level optimization to optimize Eq. (8). Next, we give details ofeach optimization process.Lower level Optimization In lower-level optimization, the surro-gate model will be trained on the backdoored dataset. We update for inner iterations with fixed to approximate as:",
  "+1= + L (,),(10)": "where denotes model parameters after iterations, is thelearning rate for training the surrogate model.Upper level optimization In the upper level optimization, theupdated surrogate model parameters and OOD detector param-eters are used to approximate and , respectively. We thenapply first-order approximation to compute gradients of by: +1= L ( ,) + L ( ,) + L ( ,) ,(11)where and indicate gradient propagation stopping, denotesmodel parameters after iterations. The training algorithm ofDPGBA is given in Algorithm 1. Time complexity analysis can befound in Appendix J.",
  "Cora2,7085,4291,4437Pubmed19,71744,3385003Flickr89,250899,7565007OGB-arxiv169,3431,166,24312840": "graph that links image captions sharing the same properties. OGB-arixv is a large-scale citation network. The statistics of the datasetsare summarized in . More details of the dataset can be foundin Appendix C. 5.1.2Compared Methods. We compare DPGBA with representa-tive and state-of-the-art graph backdoor attack methods, includingUGBA , GTA , SBA-Samp and its variant SBA-Gen. Moredetails of these compared methods can be found in Appendix H.For a fair comparison, hyperparameters of all the attack methodsare tuned based on the performance of the validation set. 5.1.3Evaluation Protocol. Following the evaluation protocol inUGBA , we conduct experiments on the inductive node clas-sification task. In this setup, attackers do not have access to testnode during trigger generator training. We randomly exclude 20%of nodes from the original dataset, denoted as V , using half astargets for assessing attack effectiveness and the other half as cleantest nodes for evaluating the accuracy of models under attack onnormal samples. The training graph G consists of the remaining 80%of nodes, with the labeled node set and validation set each contain-ing 10% of nodes. We measure backdoor attack performance usingthe average success rate (ASR) on target nodes and clean accuracyon clean test nodes. A two-layer GCN acts as the surrogate modelfor all attack strategies. To evaluate the transferability of backdoorattacks, we target GNNs with different architecturesGCN, Graph-Sage, and GAT. We conduct experiments on each GNN architecturefive times and report the average ASR and clean accuracy from thetotal of 15 experiments. The attack budget on size of poisonednodes V is set as 10, 40, 160, and 565 for Cora, Pubmed, Flickr, andOGB-arxiv, respectively. The number of nodes in the trigger sizeis limited to 3 for all experiments. Our DPGBA deploys a 2-layerGCN as the surrogate model. A 2-layer MLP is used as the triggergenerator. More details of the hyperparameter setting can be foundin Appendix I.For the defense strategy OD, in line with the in-distributionconstraint outlined in Sec. 3.4, we use DOMINANT as andtrain it on the poisoned graph G with triggers attached to nodesin V. The threshold , as specified in Eq. (1), is set such that datapoints with a reconstruction loss greater than comprise 3% of thedataset. The remaining 97% of the data points have a reconstructionloss at or below . Before training the surrogate model on thepoisoned graph G, we prune those nodes with reconstruction lossabove . Once is trained, and are fixed for the testing phase toperform inference on test nodes in V and the associated generatedtriggers. Nodes with a reconstruction loss above are pruned.",
  ": Impacts of sizes of poisoned nodes on Flicker": "OD defense strategy as outlined in Sec. 5.1.3. We report the averageresults in backdooring three different GNN architectures in Tab. 3.Detailed results for each architecture are provided in Tab. 4 6 inAppendix E. From the table, we make the following observations: When no backdoor defense strategy is applied, DPGBA shows acomparable or slightly better ASR than leading baselines suchas GTA and UGBA, while SBA-Samp and its variant, SBA-Gen,consistently achieve lower ASRs. This indicates the effectivenessof our modules in enhancing the influence of triggers on the tar-get nodes. Regarding clean accuracy, our framework consistentlydemonstrates comparable results with all the baselines. When applying a simple outlier detection defense, triggers gener-ated by GTA and UGBA are removed, but DPGBA still achievesover 90% ASR. This demonstrates that our DPGBA effectivelygenerates imperceptible ID triggers that can successfully bypasscommonly used outlier detection methods in real applications. Though we employ GCN as the surrogate model during training,the generated triggers consistently achieve high ASR across threedifferent GNN architectures, as shown in Tab. 4 6. This indicatesthe transferability of the trigger generator within our framework.",
  "Impact of the Size of Poisoned Nodes": "To answer RQ2, we conduct experiemnts to explore the attackperformance of DPGBA given different budgests in the size ofpoisoned nodes. Specifically, we vary the size of poisoned samplesas {40, 80, 120, 160, 200}. The other settings are the same as theevaluation protocol in Sec. 5.1.3. Hyperparameters are selectedwith the same process as described in Appendix I. showsthe results on Flicker dataset. We have similar observations onother datasets. We only report the attack success rate as we didnot observe any significant change in clean accuracy for all thebaselines and our DPGBA. From , we can observe that: The attack success rate of UGBA and DPGBA consistentlyrises as the number of poisoned samples increases in (a), whichaligns with our expectation. Our method maintains a comparableASR when no defense is applied, highlighting the effectivenessof our attack performance enhancement module. When OD defense is applied on the backdoor attacks in (b), ourDPGBA still achieve promising performances. In contrast, allthe baseline methods achieve an almost 0% ASR, as anticipated.That is because our method can generate trigger nodes within-distribution property.",
  "In-distribution Property Analysis": "In this subsection, to further demonstrate the in-distribution prop-erty of triggers generated by our framework, we first conduct back-door attacks on Flicker and OGB-arxiv datasets, then apply theoutlier detection method on the poisoned graph, and finally showthe reconstruction loss for both clean data and generated triggers.The histograms of the reconstruction loss are plotted in . Fromthe figure, we observe that the reconstruction loss of the generatedtriggers closely aligns with the mean of the distribution of recon-struction losses for clean inputs. This alignment can be attributedto the selection of representative samples V for the OOD detec-tor and adversarial learning to make the trigger in-distribution.Additional experiments in Appendix F demonstrate the efficacy ofour generated triggers in bypassing various advanced graph outlierdetection methods.",
  "Ablation Studies": "To answer RQ3, we conduct ablation studies to explore the effectsof the ID constraint and the enhancing triggers attack performancemodule. To demonstrate the effectiveness of the ID constraint mod-ule, we set = 0 and obtain a variant named as DPGBA\\D. To showthe benefits brought by our enhancing attack performance module,we train a variant DPGBA\\E which set the as 0. We also implementa variant of our model by removing both ID constraint and enhanc-ing attack performance module, which is named as DPGBA\\DE.The average results and standard deviations on Pubmed and OGB-arxiv are shown in . All the settings of evaluation follow thedescription in Sec. 5.1.3. And the hyperparameters of the variantsare also tuned based on the validation set for fair comparison. From, we observe that: (i) When no defense method is applied,DPGBA demonstrates a comparable attack performance, despite ASR (%) PubmedOGB-arxiv DPGBA\\DEDPGBA\\E DPGBA\\DDPGBA ASR (%) PubmedOGB-arxiv DPGBA\\DEDPGBA\\E DPGBA\\DDPGBA",
  ": Ablation studies on Pubmed and OGB-arxiv": "DPGBA\\DE and DPGBA\\D taking shortcuts to generate outlier trig-gers. However, when the OD defense method is employed, DPGBAstill exhibits a high ASR, while triggers generated by DPGBA\\DEand DPGBA\\D are almost all eliminated. This observation indi-cates the effectiveness of the proposed ID constraint module ingenerating ID triggers; and (ii) Compared to DPGBA\\E, DPGBAachieves superior attack performance under both defense settings,which shows the effectiveness of our enhancing attack performancemodule.",
  "Hyper-parameter Sensitivity Analysis": "In this subsection, we further investigate how the hyperparameter and affect the performance of DPGBA, where and control theweight of ID constraint and enhancing attack performance module,respectively. To explore the effects of and , we vary the valuesof and as {0.01, 0.1, 1, 10, 100} for Flicker dataset. We reportthe attack success rate (ASR) of attacking in both no defense andOD defense settings in . The test model is fixed as GCN. Weobserve that (i) In the absence of defense strategies, increasing improves attack effectiveness, while higher values lead to reducedattack performance. (ii) With outlier detection method deployed, topreserve the in-distribution characteristic of generated triggers andensure a high attack success rate, it is recommended to set 1and adjust accordingly as increases, ensuring remains closelyaligned with . This observation eases hyperparameter tuning.",
  ": Hyperparameter Sensitivity Analysis": "a novel problem of conducting effective distribution-preservinggraph backdoor attacks. Specifically, an Out-Of-Distribution (OOD)detector, in conjunction with an adversarial learning strategy, isimplemented to constrain the in-distribution property of generatedtriggers. Additionally, a novel module is proposed to guide thevictim model trained on the poisoned dataset to better correlate thepresence of triggers with the target class. Extensive experimentson large-scale datasets demonstrate that our proposed method caneffectively bypass commonly used outlier detection methods inreal-world applications while achieving a high attack success ratein backdooring various target GNN models. This material is based upon work supported by, or in part by, ArmyResearch Office (ARO) under grant number W911NF-21-1- 0198,Department of Homeland Security (DHS) CINA under grant numberE205949D, and Cisco Faculty Research Award.",
  "Minhua Lin, Teng Xiao, Enyan Dai, Xiang Zhang, and Suhang Wang. 2023. Certi-fiably Robust Graph Contrastive Learning. In Thirty-seventh Conference on NeuralInformation Processing Systems": "Kay Liu, Yingtong Dou, Yue Zhao, Xueying Ding, Xiyang Hu, Ruitong Zhang,Kaize Ding, Canyu Chen, Hao Peng, Kai Shu, Lichao Sun, Jundong Li, George HChen, Zhihao Jia, and Philip S Yu. 2022. BOND: Benchmarking UnsupervisedOutlier Node Detection on Static Attributed Graphs. In Advances in Neural Infor-mation Processing Systems, Vol. 35. 2702127035.",
  "Zhiwei Liu, Liangwei Yang, Ziwei Fan, Hao Peng, and Philip S. Yu. 2022. FederatedSocial Recommendation with Graph Neural Network. TIST (Aug. 2022), 124": "Qian Ma, Hongliang Chi, Hengrui Zhang, Kay Liu, Zhiwei Zhang, Lu Cheng,Suhang Wang, Philip S Yu, and Yao Ma. 2024. Overcoming Pitfalls in GraphContrastive Learning Evaluation: Toward Comprehensive Benchmarks. arXivpreprint arXiv:2402.15680 (2024). Xiaoxiao Ma, Jia Wu, Shan Xue, Jian Yang, Chuan Zhou, Quan Z Sheng, Hui Xiong,and Leman Akoglu. 2021. A comprehensive survey on graph anomaly detectionwith deep learning. IEEE Transactions on Knowledge and Data Engineering 35, 12(2021), 1201212038.",
  "Zhilin Yang, William Cohen, and Ruslan Salakhudinov. 2016. Revisiting semi-supervised learning with graph embeddings. In ICML. PMLR, 4048": "Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, andYang Shen. 2020. Graph Contrastive Learning with Augmentations. In Advancesin Neural Information Processing Systems, Vol. 33. 58125823. Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and ViktorPrasanna. 2020. GraphSAINT: Graph Sampling Based Inductive Learning Method.In International Conference on Learning Representations.",
  "Muhan Zhang and Yixin Chen. 2018. Link prediction based on graph neuralnetworks (NIPS18)": "Zaixi Zhang, Jinyuan Jia, Binghui Wang, and Neil Zhenqiang Gong. 2021. Back-door attacks to graph neural networks. In Proceedings of the 26th ACM Symposiumon Access Control Models and Technologies. 1526. Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, and Cheekong Lee. 2022.ProtGNN: Towards Self-Explaining Graph Neural Networks. In Thirty-SixthAAAI Conference on Artificial Intelligence, AAAI 2022. AAAI Press, 91279135.",
  "ATRAINING ALGORITHM": "The DPGBA algorithm is detailed in Algorithm 1. Initially, we iden-tify the poisoned nodes V and label them with the target class (lines 3-4). From lines 5-13, the trigger generator is trained toboth attack the surrogate model and deceive the OOD detector, utilizing a bi-level optimization approach. Specifically, in thelower level, we update the surrogate model (lines 6-8) and the OODdetector (lines 9-11) through gradient descent on and , respec-tively, guided by Eq. (9) for and Eq. (10) for . In the upper level,the generator is updated (line 12) by applying gradient descenton , as outlined in Eq. (11). After that, from line 14 to 17, we usethe well-trained to generate a trigger for each poisoned node V and attach with to obtain the poisoned graph G.",
  "BREPRESENTATIVE NODES SELECTION": "For selecting representative nodes V from a clean graph G, we usethe outlier detection method outlined in DOMINANT . Thisapproach is first applied to G to determine the mean and standarddeviation of the reconstruction losses. Representative nodes arethen selected based on their reconstruction loss , ensuring that < + . The parameter is set to 1 for the Cora dataset andadjusted to 0.01 for the Pubmed, Flicker, and OGB-arxiv datasets.",
  "CDATASETS DETAILS": "Cora and PubMed : They are citation networks where nodesdenote papers, and edges depict citation relationships. In Coraand CiteSeer, each node is described using a binary word vector,indicating the presence or absence of a corresponding word froma predefined dictionary. In contrast, PubMed employs a TF/IDFweighted word vector for each node. For all three datasets, nodesare categorized based on their respective research areas. Flicker : In this graph, each node symbolizes an individualimage uploaded to Flickr. An edge is established between thenodes of two images if they share certain attributes, such asgeographic location, gallery, or user comments. The node fea-tures are represented by a 500-dimensional bag-of-word modelprovided by NUS-wide. Regarding labels, we examined the 81tags assigned to each image and manually consolidated theminto 7 distinct classes, with each image falling into one of thesecategories. OGB-arxiv : It is a citation network encompassing all Com-puter Science arXiv papers cataloged in the Microsoft AcademicGraph. Each node is characterized by a 128-dimensional featurevector, which is derived by averaging the skipgram word embed-dings present in its title and abstract. Additionally, the nodes arecategorized based on their respective research areas.",
  "DADDITIONAL RELATED WORKS": "Graph outlier detection is a critical task in machine learning, involv-ing the identification of anomalous nodes within a graph. Unliketraditional outlier detection on tabular or time-series data, graphoutlier detection presents unique challenges due to the rich informa-tion inherent in graph structures and the computational complexityof training with complex machine learning models. The emergenceof deep learning techniques has revolutionized outlier detection,",
  "OGB-arxivNone65.639.77.2 | 65.30.343.010.4 | 65.60.494.32.5 | 64.80.197.60.1 | 65.10.295.41.3 | 65.20.2OD65.338.36.1 | 65.50.542.111.3 | 65.30.300.0 | 65.10.500.0 | 65.50.492.10.9 | 65.40.3": "shifting from traditional methods to neural network approaches. One popular neural network architecture for this task is theautoencoder (AE) , which learns to reconstruct the originaldata and identifies outliers based on reconstruction errors. Thisunsupervised learning approach makes AEs effective for detectingoutliers without the need for labeled data. Furthermore, graph neu-ral networks (GNNs) have demonstrated superior performance incapturing complex patterns within graph data, considering bothnode attributes and graph structure. GNNs encode representationsfor each node, enabling effective outlier detection. Notably, GNNscan be combined with AEs , leveraging thestrengths of both approaches for more robust outlier detection ingraph data.",
  "EEXPERIMENTS ON ATTACKTRANSFERABILITY": "To demonstrate the transferability of our trigger generator in attack-ing various GNN architectures, we employ GCN as the surrogatemodel and evaluate the ASR and clean accuracy when attackingGCN, GraphSage and GAT , respectively. The results arepresented in Tab. 4 6. From the tables, we observe that our DPGBAconsistently achieves a high attack success rate while maintain-ing the clean accuracy across different target models and various",
  "FAGAINST VARIOUS OUTLIER DETECTIONMETHODS": "To further demonstrate the in-distribution property of the triggersgenerated by our DPGBA, we adopt various state-of-the-art graphoutlier detection methods, including DOMINANT , DONE and its variant AdONE, AnomalyDAE , GAAN and CONAD, as defense mechanisms and conduct backdoor attacks on fourdatasets. The other settings are the same as the evaluation protocolin Sec. 5.1.3. The results of ASR are reported in Tab. 7. From thetable, we observe that DPGBA consistently exhibits its capabilityto evade various graph outlier detection methods and maintain ahigh attack success rate. This consistency underscores the practicalapplication value of DPGBA in real-world scenarios.",
  "None97.792.398.895.6DOMINANT94.491.296.093.2DONE94.390.997.394.4AdONE95.792.098.093.4AnomalyDAE96.491.797.495.1GAAN96.891.898.694.9CONAD96.691.398.594.7": "edges connecting nodes with low cosine similarity. All experimen-tal configurations adhere to the evaluation protocol outlined in.1.3. Following , we incorporate the unnoticeable lossproposed in to ensure that generated triggers exhibit high co-sine similarity to target nodes. We set the pruning threshold toexclude approximately 10% of dissimilar edges. presentsthe results of ASR and clean accuracy. From the table, we observethat DPGBA consistently exhibits comparable ASR and slightlyhigher clean accuracy compared to UGBA across four datasets.Notably, generated triggers in DPGBA maintain in-distributionproperty, whereas UGBA fails to evade detection by outlier detec-tion methods. These findings indicate the superior performanceand robustness of DPGBA in diverse settings.",
  "HDETAILS OF COMPARED METHODS": "The details of compared methods are described following SBA-Samp : This method introduces a static subgraph asa trigger into the training graph for each poisoned node. Thesubgraphs connections are formed based on the Erdos-Renyi(ER) model, while its node features are randomly selected fromthose in the training graph. SBA-Gene: An adaptation of SBA-Samp, SBA-Gen differentiatesitself by employing synthetically generated features for the trig-ger nodes. These features are drawn from a Gaussian distribution,the parameters of whichmean and varianceare derived fromthe attributes of actual nodes. GTA : GTA utilizes a trigger generator that crafts subgraphsas triggers tailored to individual samples. The optimization ofthe trigger generator focuses exclusively on the backdoor attackloss, disregarding any constraints related to trigger detectability.",
  "IIMPLEMENTATION DETAILS": "A 2-layer GCN is utilized as the surrogate model, another 2-layerGCN is used for , while a 2-layer MLP serves as the in-distributiontrigger generator. We set all hidden dimensions to 256. The numberof inner iteration steps, N and K, are consistently set to 1 and 20across all experiments. The hyperparameters and are selectedbased on the grid search on the validation set. For the OD defense,the pruning threshold is set to exclude roughly 3% of the sampleswith the highest reconstruction loss.",
  "JTIME COMPLEXITY ANALYSIS": "During the bi-level optimization phase, the computation cost ofeach outter iteration consist of updating of surrogate GCN modeland OOD detector in inner iterations and training adaptive triggergenerator. Let denote the embedding dimension. The cost forupdating the surrogate model is approximately (|V|), where is the average degree of nodes and is the number of inner itera-tions for the surrogate model, which is generally small. The cost forupdating the OOD detector is approximately ((|V | + |T |)),where is the number of inner iterations for the OOD detector.For trigger generator, the cost for optimizing L is (|V |),for optimizing L is ((|V | + |T |)), and for optimizing Lis (|V| + |V |2 + |V ||V ||V|), where |V| and |V | aregenerally small compared to |V|. In our empirical experimentsconducted on large-scale datasets, such as Flickr and OGB-arxiv,which comprise 899,756 and 169,343 nodes respectively, we stream-lined the training process by selecting a subset of V and setting|V | = 4096 for each epoch. Despite this simplification, DPGBAstill achieve a high attack success rate, as evidenced in Tab. 4 6.In , we report the overall training time and correspondingASR of our DPGBA compared to GTA and UGBA on the OGB-arxivdataset. All models were trained on a Nvidia A6000 GPU with 48GBof memory. The results indicate that DPGBA requires only approx-imately 20 seconds more training time compared to the baselineson the OGB-arxiv dataset. Given that our DPGBA achieves an ASRof over 90%, while the baseline methods achieve nearly 0% withOD defense adopted, this additional time is justified. This demon-strates that DPGBA effectively generates triggers that the victimmodel quickly memorizes, highlighting its potential for conductingscalable targeted attacks."
}