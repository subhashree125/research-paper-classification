{
  "ABSTRACT": "The graph-based recommendation has achieved great success in re-cent years. However, most existing graph-based recommendationsfocus on capturing user preference based on positive edges/feedback,while ignoring negative edges/feedback (e.g., dislike, low rating)that widely exist in real-world recommender systems. How to utilizenegative feedback in graph-based recommendations still remainsunderexplored. In this study, we first conducted a comprehensiveexperimental analysis and found that (1) existing graph neural net-works are not well-suited for modeling negative feedback, whichacts as a high-frequency signal in a user-item graph. (2) The graph-based recommendation suffers from the representation degenera-tion problem. Based on the two observations, we propose a novelmodel that models positive and negative feedback from a frequencyfilter perspective called Dual-frequency Graph Neural Network forSign-aware Recommendation (DFGNN). Specifically, in DFGNN,the designed dual-frequency graph filter (DGF) captures both low-frequency and high-frequency signals that contain positive andnegative feedback. Furthermore, the proposed signed graph regu-larization is applied to maintain the user/item embedding uniformin the embedding space to alleviate the representation degenerationproblem. Additionally, we conduct extensive experiments on real-world datasets and demonstrate the effectiveness of the proposedmodel. Codes of our model will be released upon acceptance. Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from acronym XX, June 0305, 2018, Woodstock, NY 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-XXXX-X/18/06",
  "Sign-aware recommendation, Signed graph neural network, Nega-tive feedback, Recommendation": "ACM Reference Format:Yiqing Wu, Ruobing Xie, Zhao Zhang, Xu Zhang, Fuzhen Zhuang, LeyuLin, Zhanhui Kang, and Yongjun Xu. 2018. DFGNN: Dual-frequency GraphNeural Network for Sign-aware Feedback. In Proceedings of Make sure toenter the correct conference title from your rights confirmation emai (Confer-ence acronym XX). ACM, New York, NY, USA, 11 pages.",
  "INTRODUCTION": "Personalized recommender systems aim to provide appropriateitems to the users on platforms. In such systems, the interactionsbetween users and items can naturally be regarded as a user-itembipartite graph. Inspired by the success of Graph Neural Networks(GNNs), the graph-based recommendation model has been widelystudied and achieved remarkable progress. However, most existinggraph-based recommendation models are merely built on positiveedges/feedback. Actually, there is always various negative feedbackin real-world recommender systems, such as a low rating, dislike,and short watching time. The negative feedback generally repre-sents what people do not like or even hate. Recommendations ofsuch items to users may significantly harm the user experience.Besides, only focusing on what users like while ignoring what usersdislike may lead to homogenization in the recommended items .Although negative feedback is crucial and is believed to benefitthe recommendation , how to model the negative feedback ingraph-based recommendation still remains underexplored. A trivialsolution is to introduce negative feedback edges to the graph and",
  "Conference acronym XX, June 0305, 2018, Woodstock, NYTrovato et al": "model them in a manner similar to positive feedback. However,existing GNNs heavily rely on the homophily assumption, wheretwo nodes connected by an edge tend to be more similar. While,for negative feedback, two nodes connected by an edge may implydissimilarity. A similar field is the signed graph neural network. It isdesigned for processing signed graphs, which contain two oppositeedge types (e.g., accept/reject, fraud/trustworthiness). The signal inthe signed graph generally is objective, and most of them are builtupon balance theory, which means the friend of my friend is myfriend, and the enemy of my enemy is my friend. However, the userfeedback in recommendations is subjective and personalized, andthe balance theory does not always hold in recommendations .Currently, limited works have studied the negative feedback ingraph-based recommendations. While they still adopt GNNs thatrely on the homophily assumption.Based on the above analysis, we start to ponder \"How to modelpositive and negative feedback in graph-based recommendation?\" Tothis end, we deeply contemplate the characteristics of positive andnegative feedback. (1) Positive and negative feedback carry differ-ent meanings. Generally, if an item fits users interests, the peoplewill produce positive feedback. In recommendation, this implies acertain degree of similarity between items and users. In contrast,negative feedback implies dissimilarity. While this makes the nega-tive feedback violate the homophily assumption of GNN. (2) Thereasons for negative feedback are heterogeneous. Compared to posi-tive feedback, the reasons for users produce negative feedback arediverse and complex. It could stem from a variety of reasons suchas the user hating the categories of the item, the items not meetingthe users expectations, or even due to the user hating a specificword in the title. (3)The negative feedback is generally sparse. Therecommender system aims to recommend what people will like,the negative feedback actually is an anomaly, making it sparse. Forexample, in the Amazon dataset, less than 10% of reviews have lowratings.Looking deeper into the structure information of graph-basedrecommendation, we can discover that: a user node is connectedto many nodes that are similar to it by positive feedback, result-ing in a smooth similarity terrain. However, there are also a fewstrange peaks that stand out, appearing out of place, that is thenegative feedback. This inspires us to think about positive and neg-ative feedback essentially from the perspective of signal processing.In which the smooth and unchanged signals are regarded as low-frequency signals. While steep, rapid changes are considered ashigh-frequency signals. This is highly analogous to the characteris-tics of positive and negative feedback. To evaluate our assumption,we utilize graph spectral theory to experimentally analyze the pos-itive and negative feedback in the frequency-domain (detailed inSec. 3.1) and find that the positive feedback indeed implies low-frequency signals in the graph while the negative feedback implieshigh-frequency signals in the graph. Furthermore, the GNNs tendto over-smooth low-frequency signals. Through experiment, wealso find that the learned embedding suffers from a representationdegeneration problem, which means the embedding shrinks to asmall piece of embedding space thus resulting in a loss of expressivepower. This issue has a more significant impact on signed graphs, asit leads to an inability to distinguish between negative and positivefeedback. Considering the characteristics of negative and positive feed-back, we propose a novel method to model them in graph-basedrecommendations, called Frequency-aware signed graph neuralnetwork (DFGNN). Specifically, considering the low-frequencysignal and high-frequency signal contained in positive and neg-ative feedback, respectively, we propose a dual-frequency graphfilter(DGF). In DGF we utilize the designed low-passing graph fil-ter to model the positive feedback while adopt the high-passingfilter to model the negative feedback. To alleviate the representa-tion degeneration problem caused by GNN, we propose a signedgraph regularization (SGR) loss. SGR considers both the similarand dissimilar information in the signed graph and regularizes thelearned embedding to be uniformly distributed in the space. Theexperiments on real-world datasets demonstrate that our modelachieves significant improvements over competitive baselines.Overall, the contribution of this paper is summarized as follows: We first conduct a comprehensive analysis of positive andnegative feedback in the graph-based recommendation fromthe frequency perspective. Furthermore, we design a dual-frequency filter to both model low-frequency signals andhigh-frequency signals contained in positive and negativefeedback.",
  "PRELIMINARIES2.1Signed Graph in Recommendation": "In real-world online recommender systems, users generally havevarious interactions with items and generate feedback to the sys-tem, including both positive feedback (e.g. like, buy) and negativefeedback (e.g. dislike, low rating). Based on the positive and neg-ative feedback, we define the signed graph in recommendation.Specifically, we denote the signed graph as = {,, {E+, E}},where = {1,2, ...,| |} and = {1, 2, ..., | | are the user setand item set in the system, and {E+, E} are positive and negativeedges in the graph, respectively. If a user has positive feedback foran item, there will be a positive edge = (, , +) E+. Conversely,a negative edge = (, , +) E will be created if there is negativefeedback.",
  ": Different frequency signals in the time domain andfrequency domain": "low frequencies correspond to smooth signals, while high fre-quencies correspond to non-smooth signals. The amplituderepresents the strength of the signal at the correspondingfrequency. To better understand this, we demonstrate this phenom-enon in . We can see that () (i.e., (a)) is smoother, thusits corresponding low frequency is higher(i.e., (d)). In contrast,0.2(10) oscillates rapidly (i.e., (b)), and correspondingly,the amplitude of high frequencies is higher (i.e. (e)). (c)looks complex in the time domain, but in the frequency domain((f)), it looks simpleits just the superposition of two sinewaves, () and 0.2(10).To extend the Fourier transform to the graph/non-euclideandomain, Laplacian matrices are introduced to the graph processingfield. Classical Laplacian matrice is defined as = , where is the adjacent matrix and is the degree matrix of a graph. is areal symmetric matrix, thus it can be written as = , where = {1,2, ..., } is the eigenvector and = {1, 2, ..., } isthe eigenvalue of . Similar to the Fourier transform in the timedomain, the Fourier transform in the graph/non-euclidean domainis defined as:",
  "Analysis of Positive and Negative Feedbackfrom Frequency-domain": "Generally, the user behaviors in a system can be modeled to a signalgraph, where users and items are nodes, edges represent interac-tions on the graph, and user and node features serve as signalson the graph. As introduced in Sec.2.3, with the help of the graphFourier transform, we can analyze signals from a frequency perspec-tive. To explore the characteristics of positive/negative feedbackon the graph, we conduct experimental analysis in the frequencydomain. Specifically, we first employ the classical Matrix Factoriza-tion and learn one-dimensional embeddings for users and itemsas the signals of the graph. Then we construct two graphs that onlycontain positive edges and negative edges respectively, denoted as+ and . Last, we apply graph Fourier transform to the + and. We conduct this experiment on ml-100k dataset and show thedistribution of frequency in . By comparing Fig 2 (a) and Fig 2(b), we can observe that the signal of + is prominent in low fre-quencies, while is prominent in high-frequency signals. Recallthat, the low frequencies represent the smooth signals, and highfrequencies represent unsmooth signals. In the temporal domain,smoothness can be defined as the magnitude of the signal changebetween timestep 1 and . Similarly, in the graph, smooth-ness can be measured by the difference between the nodesconnected by an edge. The smaller the difference betweennodes connected by an edge, the smoother the graph. Therefore theresults in Fig 2 align with intuition. In recommendation, we gen-erally assume that users have higher similarity to items they like,and correspondingly, the + is dominated by low-frequency sig-nal. On the contrary, users are dissimilar to items they dislike, andcorrespondingly, G exhibits more high-frequency signals. Mostexisting GNNs actually act as a low-passing filter, whichmeans the GNN extracts low-frequency signals while discardinghigh-frequency signals. Unfortunately, most existing Graph-basedrecommendation models adopt the low-passing GNN to encodegraphs. Thus, they may not be suitable for capturing negative feed-back signals.",
  "The Representation Degeneration Problemin Graph-based Recommendation": "Recently, graph neural networks have achieved great success inrecommendation systems . However, GNNs are knownas the over-smoothing effect , which means the node featuretends to be similar during the GNN training, especially in deepgraph neural networks. The over-smoothing effect sometimes willcause the representation degeneration problem. In this issue, theembeddings shrink to a small piece of space, thus losing the expres-sive power. To explore whether the graph-based recommendationwill suffer from this problem, we train a graph model (i.e. GCN)and a non-graph model (i.e. NCF) on Amazon ArtsCrafts reviewdataset, respectively. We project the learned embedding matrix into2D by SVD and visualize it. The results are shown in . Fromthis figure, we can find that the embedding learned by NCF is moreuniform than that learned by GCN. Most node embeddings learned",
  "(b) positive feedback": ": The distribution of f() calulated based on nor-malzied Laplacian matrix. (a) It is the distribution of onlynegative edges graph. (b) It is the distribution of only positiveedges graph. We evenly split the frequency into ten bucketsand calculate the normalized f(). by GCN fall into a narrow zone. Furthermore, we analyze the dis-tribution of singular values of embedding matrices and show theresults in . We can observe a rapid decline in the singular val-ues of GCN, indicating that the learned embeddings are low-rankand have less expressive power.",
  "METHOD": "Based on the above experimental observations, in this paper, wepropose a novel graph model called DFGNN to solve the issuesin the sign-aware graph recommendation. Our DFGNN containstwo components: (1) dual frequency graph filter and (2) graphregularization. The dual-frequency graph filter aims to both capturethe high-frequency signal and the low-frequency in the negativefeedback and positive feedback respectively. Furthermore, the graphregularization module is adopted to alleviate the representationdegeneration problem. In the following, we will make a detailedintroduction to our DFGNN.",
  "Dual Frequency Graph Filter": "The negative feedback is crucial and widely present in recommen-dation systems, which show what people dislike or even hate whatkind of items. However, how to deal with negative feedback isnot a trivial problem. As the analysis in Sec 3.1, different from the positive feedback, the negative feedback usually acts as the high-frequency signal in the graph. Hence we propose a dual-frequencygraph filter (DGF), which uses a low-passing graph filter (LGF) tocapture the low-frequency signal in positive feedback and adoptsa high-passing graph filter (HGF) to capture the high-frequencysignal in negative feedback.",
  "=(()()) = () ,(3)": "where () is the filter kernel function with respect to the eigenval-ues of the Laplacian matrix. Generally, for a low-pass filter, ()is high at low but low at high . For a high-pass filter, it is theopposite. (() is high at high and low at low . 4.1.2Low-passing Graph Filter. After defining the graph filter, wecan design the low-passing filter. Luckily, most existing GNNs act aslow-passing graph filters. In this work, we adopt the classical GCNs as our low-passing graph filter. The GCNs can be formulatedas:+1 = ( ), 0 = ,(4)",
  "12 is a designed matrix, where = + are theaugmented adjacent matrix with adding self-loops and = + are the degree matrix of": "0.000.250.500.751.001.251.50 Eigenvalue( ) 0.4 0.2 0.0 0.2 0.4 0.6 0.8 1.0 value of g( ) GCNs's graph filter kernal (1- ) with layer stack k = 1k = 2k = 3k = 4k = 5 0.000.250.500.751.001.251.50 Eigenvalue( ) 0.5 0.0 0.5 1.0 1.5 2.0 Our high-passing filter( ) with layer stack k = 1k = 2k = 3k = 4k = 5",
  ": The graph filter kernel function of LGF and HGFwith layer stack. The left is the kernel (1-) and the right is": "4.1.3High-passing Graph Filter. The Eq. (3) guides us on how todesign a high-passing graph filter. However, it still is not a trivialtask, as the eigenvectors are difficult to compute. Hence, wedesign a high-passing filter based on the existing low-passing filter(i.e., GCNs). To achieve it, We first hand on why GCNs are low-passing filter. The normalized Laplacian matrix is definedas = 1",
  "DFGNN: Dual-frequency Graph Neural Network for Sign-aware FeedbackConference acronym XX, June 0305, 2018, Woodstock, NY": "(3) Compared with all baselines, our DFGNN significantly out-performs them on all datasets, even with improvements of over 50%observed in some datasets. This can be attributed to our special de-sign for handling negative feedback. Traditional GNNs are designedfor homogeneous graphs and rely on the homophily assumption,which assumes that nodes connected by edges are similar. While inthe sign-aware recommendation, users and items nodes connectedby negative feedback intuitively indicate dissimilarity. Our analysisof negative signals from a frequency view also evaluates it. Basedon this, we design dual-frequency graph filters and signed graphregularization, which can break the homophily assumption, andthus better capture the signal from negative feedback.",
  "Graph Encoder": "In this section, we introduce how to encode the graph informationwith our DGF. Generally, the user feedback in recommendationcontains various noises. Looking back to , we can see thatfor positive feedback there still exists high-frequency information,which can be regarded as noise in the graph. The low-passing filterusually is utilized to denoise. Hence, we first use LGF on the positivefeedback. It has two functions: (1) capturing what users like by thepositive feedback. (2) weaking the noise in the graph. It can beformulated as:",
  "Signed Graph Regularization": "As discussed before, the graph neural network in recommenda-tion may suffer from representation degeneration problems, whichmeans the graph embedding shrinks to a piece of embedding spaceand loses representative power. Intuitively, if we could make theembedding uniformly distributed in the space, this issue could bealleviated. Uniform means that the average distance of each node toall nodes should be as large as possible, so the uniform constraintloss is defined as:",
  "(,)/,(9)": "where and are the learned embedding of user and item, the() is the distance function, is temperature. In this work, weadopt cosine similarity to measure the distance between two nodes.However, complete uniform distribution still will lead to a lossof representational capacity issue. Because the core of recommen-dation systems lies in matching items that are similar to the usersinterests. This implies that representations of some embeddingshould be similar. Furthermore, in sign-aware recommendation, things become even more complex. Negative feedback implies thatsome users and items are dissimilar, and their representations inthe space should be as far apart as possible. Therefore, to simulta-neously consider uniformity and similarity/dis-similarity betweenuser and items, we designed alignment loss, we formulated it as:",
  "= + .(11)": "In this work, we apply our SGR on the user and item embeddinglayer, while do not apply it to the encoded node representation.Because we empirically found that seting results in worse perfor-mance. It may be because the encoded representation in the toplayer is highly task-dependent, and forcing the representation tobe uniformly distributed may not be effective.",
  "EXPERIMENT": "In this section, we aim to answer the following questions: (1)RQ1:How does DFGNN perform compared with other state-of-the-art (SOTA) baselines on recommendation task? (Sec. 5.3) (2) RQ2:How does DFGSN perform compared with other state-of-the-art(SOTA) baselines on feedback type recognition task? (Sec. 5.4) (3)RQ3: The proposed two modules in DFGNN indeed work? (Sec. 5.5)(4) RQ4: How does our signed graph regulation loss help in thetraining? (Sec. 5.6)",
  "Experimental Setting": "Datasets In this paper, to evaluate the performance of the pro-posed DFGNN, we conduct comprehensive experiments on classicalML1M, different categories of Amazon Review datasets, andYelp dataset. They are all real-world rating datasets. each instancewas assigned a rating from 1 to 5. we regard the ratings that arebelow 3 as negative feedback and the ratings that are higher than 3",
  "%": "as positive feedback. Due to the space limit, we detailly introduceand analyze them in the AppendixA.1.Evaluation Protocols. We evaluate the proposed model on twotasks called the recommendation ranking task and feedback typerecognition task. For the recommendation ranking task, we adoptwidely accepted ranking metrics: top-K hit rate (HIT@k),top-K (NDCG@k), and MRR, with = {1050}. Feedback typerecognition task can be regarded as a binary classification task. Con-sidering the imbalance between positive and negative samples(asshown in Table.3), here we use classical AUC and F1-Macro as eval-uation metrics. It is necessary to note that we do not adopt accuracy,precision, recall, etc., as metrics, since they are not applicable toimbalanced datasets. For each task, we report the mean results withfive random runs.Implement Details. We carefully tune all the baselines and com-pare them fairly. Due to the space limit, we show the details inAppendix. A.1.1.",
  "Baselines": "To validate the performance of DFGNN, we compare our methodwith classical and state-of-the-art unsigned/signed graph neuralnetworks: (1) GCN, (2) GAT , (3) SGCN (4)SBGNN ,(5) SBGCL (6) SIGRec. For unsigned GNNs, we only retainthe positive feedback edge. We implement our method by PyG1, which is a popular graph learning framework. For baseline GCN,GAT, and SGCN we directly use the implementation of PyG. Forbaseline SBGNN and SBGCL, we directly use the authors releasedcodes. For SIGRec as there is no available open-sourced code, weimplement it by ourselves. They are detailly introduced in Appen-dix B",
  "Performance on Recommendation": "We show the results of our DFGNN in on the recommenda-tion task, which aims to recommend items to users from a set ofcandidates. From the table, we can see that:(1) Compared with the unsigned graph neural network (i.e., GCNand GAT), the signed graph neural network generally outperformsthem across all datasets. It demonstrates the importance of negativefeedback. The signed graph neural network both considers thepositive feedback and negative feedback information in the graph,thus achieving a better performance.(2) We note that the SGCN generally performs worse than SBGCL.SBGCL is specifically designed for bipartite graphs. It extends thebalance theory tp the butterfly structure in bipartite graphs. Itmatches the graph structure characteristics in recommendation,which are a user-item bipartite graphs. In contrast, SGCN is basedon the balance theory with a triangle structure, which may lead toits suboptimal performance.",
  ": Ablation study of feedback type recognition taskon Arts and GFood dataset": "This section evaluates our DFGNN on the feedback type recog-nition task. Given a user and item pair that the user has interactedwith, this task aims to predict whether the user will give posi-tive/negative feedback. This task focuses more on identifying whatusers dislike, as recommending items that users find displeasing cansignificantly harm the user experience. We report the experimentresults in . From this table, we have:(1) We note that compared to the unsigned graph models, thesigned graph neural networks perform almost entirely better thanthem. This may be attributed to the positive/negative feedback pre-diction is a harder task. In fact, users only interact with an itemwhen they have a certain level of interest in it. Negative feedbackmay be provided later after that. Compared to selecting items thatusers like from random negative items (task in recommendation),",
  ": Ablation study of recommendation task on Artsand GFood dataset": "this task is more difficult. In this scenario, a lack of precise mod-eling of negative feedback signals may limit the improvement inprediction accuracy.(2) Compared to both signed graph neural networks and un-signed graph neural networks, our DFGNN still outperforms all ofthem except F1-Macro in the Yelp dataset. As analyzed before, posi-tive/negative recognition is not trivial. Though edges constructedbased on co-negative relationships (e.g., 1, 2 are both disliked by1) in the SBGNN and SBGCL intuitively can utilize low-passingfilters to capture negative feedback information. They still struggleto capture high-passing signals between nodes with opposing rela-tionships (e.g., 1 dislikes 1). With the help of our dual-frequencygraph filters, our model can capture the high-frequency signal inthe negative feedback. Furthermore, the signed graph regulariza-tion further alleviates the representation degeneration problem andhelps the graph filters work better.",
  "Ablation Study": "In this section, we conduct ablation experiments to evaluate theeffectiveness of the proposed module in our DFGNN. We trans-form our model into three versions (1) Basic is the basic version ofDFGNN, which only contains the positive edge and low-passingGNN (2) Basic + LGF is the version that applies low-passing graphfilter on negative edges. (3) Basic + DGF, which adopts our DGFto encode the signed graph (4) DFGNN is the full version of ourmodel, which both adopts our SGR and DGF to the signed graph.We conduct the ablation experiment on the recommendation rank-ing and feedback type recognition task. And show the results in and . From the figures, we can find that the proposedDGF and SGR can indeed improve the performance of two tasks.",
  "Uniform Analysis": "In this paper, based on the observation of the representation degen-erative problem, we propose SGR to alleviate it. To evaluate ourSGR and further understand how our SGR improves performance:(1) we visualize the learned node embedding, and (2) we give outthe normalized singular values of the embedding encoded by GNNsand the average distance between nodes. Specifically, we projectthe learned embedding to 2D by SVD for visualization. We use theaverage L2 distance of normalized embedding to measure uniform.The larger the value of distance, the more uniform the learnedembedding is. We run the DFGNN and GCN on the Arts dataset.The results are shown in and . By observing the we can find that the DFGNN learned embedding is uniform to thatof GCN. Secondly, the embedding learned by feedback type predic-tion task is more narrow. This implies the feedback type predictiontask is a more difficult task. Furthermore, by observing , Wecan see singular values of the embeddings learned by our DFGNNdecrease more slowly. The distances of the learned embeddingsare also larger. This indicates the designed SGR indeed can helplearned embedding to be more uniform. Thus, it can improve theperformance of our model.",
  "RELATED WORK": "GNN and Graph-based Recommendation Graph Neural net-works(GNN) are studied to model graph data and have achievedgreat success. GCNs firstly extend convolution oper-ations to graph data. Further, GraphSage, GAT, GIN et.alabstract GNN to a message-passing schema and propose variousinformation aggregation strategies. Recommender system aims torecommend appropriate items to users by utilizing user-item in-teraction information. Recently, recommendation methods basedon deep learning have achieved great success.Borrowing the promising performance, GNNs are introduced toexplore high-order interaction information contained in recom-mendations. Pinsage firstly applies graph neural network tothe recommendation. NGCF extends collaborative filtering tograph. LightGCN removes the non-linear layer between GCNsublayer to alleviate the over-parameter issues. despite the successof graph-based recommendations, they are merely used to modelpositive feedback. How to model negative feedback in graph-basedrecommendation remains underexplored. In this work we focus onmodeling negative feedback in graph-based recommendation.Signed graph neural network In the signed graph, there aretwo opposite edges (e.g. reject/accept, trustworthy/trustworthy).Signed graph neural networks are proposed to deal with such graphstructures. SGCN first introduces the GCNto the signed graph by utilizing the balance theory. SBGNNextends the balance theory to the bipartite graph. SBGCL modi-fied the graph contrastive learning to the customized signed graph.However, most of them do not focus on the recommendation sce-nario and adopt classical graph neural networks as encoders, whichheavily rely on homophily assumption. In this paper, we find thisassumption may not be invalid for recommendation.",
  "CONCLUSIONS": "In this paper, we first essentially analyze the negative and positivefeedback from the graph signal frequency perspective. Based onour experimental observations, we propose a novel Dual-frequencyGraph Neural Network (DFGNN), which models positive/negativefeedback with a designed dual-frequency filter and alleviates repre-sentation degeneration problem with a signed-graph regularizationloss. Extensive experiments validate the power of our FDGNN.",
  "Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representationlearning on large graphs. Advances in neural information processing systems 30(2017)": "Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, YongDong Zhang, and MengWang. 2020. LightGCN: Simplifying and Powering Graph Convolution Networkfor Recommendation. In Proceedings of the 43rd International ACM SIGIR Confer-ence on Research and Development in Information Retrieval (Virtual Event, China)(SIGIR 20). Association for Computing Machinery, New York, NY, USA, 639648.",
  "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-SengChua. 2017. Neural collaborative filtering. In Proceedings of the 26th internationalconference on world wide web. 173182": "Junjie Huang, Huawei Shen, Qi Cao, Shuchang Tao, and Xueqi Cheng. 2021.Signed bipartite graph neural networks. In Proceedings of the 30th ACM Interna-tional Conference on Information & Knowledge Management. 740749. Junjie Huang, Huawei Shen, Liang Hou, and Xueqi Cheng. 2019. Signed graphattention networks. In Artificial Neural Networks and Machine LearningICANN2019: Workshop and Special Sessions: 28th International Conference on Artificial Neu-ral Networks, Munich, Germany, September 1719, 2019, Proceedings 28. Springer,566577. Junjie Huang, Ruobing Xie, Qi Cao, Huawei Shen, Shaoliang Zhang, Feng Xia,and Xueqi Cheng. 2023. Negative can be positive: Signed graph neural networksfor recommendation. Information Processing & Management 60, 4 (2023), 103403. Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and LarryHeck. 2013. Learning deep structured semantic models for web search usingclickthrough data (CIKM 13). Association for Computing Machinery, New York,NY, USA, 23332338.",
  "Yongjun Xu, Fei Wang, Zhulin An, Qi Wang, and Zhao Zhang. 2023. Artificialintelligence for sciencebridging data to wisdom. The Innovation 4, 6 (2023)": "Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton,and Jure Leskovec. 2018. Graph convolutional neural networks for web-scalerecommender systems. In Proceedings of the 24th ACM SIGKDD internationalconference on knowledge discovery & data mining. 974983. Shuhan Yuan, Xintao Wu, and Yang Xiang. 2017. SNE: signed network embedding.In Advances in Knowledge Discovery and Data Mining: 21st Pacific-Asia Conference,PAKDD 2017, Jeju, South Korea, May 23-26, 2017, Proceedings, Part II 21. Springer,183195.",
  "recommendation for deep collaborative filtering. In IJCAI International JointConference on Artificial Intelligence": "Zeyu Zhang, Jiamou Liu, Kaiqi Zhao, Song Yang, Xianda Zheng, and Yifei Wang.2023. Contrastive learning for signed bipartite graphs. In Proceedings of the 46thInternational ACM SIGIR Conference on Research and Development in InformationRetrieval. 16291638. Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang,Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-rec: Self-supervised learning forsequential recommendation with mutual information maximization. In CIKM.",
  "AEXPERIMENTAL SETTINGA.1Dataset": "In this paper, to evaluate the performance of the proposed DFGNN,we conduct comprehensive experiments on classical ML1M, dif-ferent categories of Amazon Review datasets, and Yelp dataset.Amazon Review datasets are collected from the Amazon shoppingwebsite. Each instance consists of a users rating for an item, rangingfrom 1 to 5. we select three categories dataset, they are ArtsCraftsand Sewing,Grocery and Gourmet Food, and Kindle Store. For alldatasets, we regard the ratings that are below 3 as negative feed-back, and the ratings that are higher than 3 as positive feedback.Following previous works, we discard the users and itemswith less than five interactions. For each dataset we randomly select70% instances as training data, 10% instances as validation data, and20% instances as testing data. The detailed statistical informationof those datasets is shown in Tab. 3",
  "Arts56123228474135545.85%Kindle1397299882120204965.68%GFood127399413179903608.74%Yelp20745392422313996319.94%ML1M6040366873901222.16%": "A.1.1Implement Details. . For all the models the embedding sizeof items and users is set to 64 and the batch size is set to 512.The GNN layer K is set to 2 in our method. For all methods. Weoptimize all models with the Adam Optimizer and carefully searchfor the hyper-parameters of all baselines. To avoid overfitting, weadopt the early stop strategy with a patience of 20 epochs. Forthe recommendation ranking task, we adopt the random negativesampling strategy with a 1:1 sampling rate. We conduct a gridsearch for hyper-parameters. We search for models learning ratesamong {12, 33, 13, 34, 14, 35}."
}