{
  "Abstract": "Deep neural network based Outlier Detection (DOD) has seen arecent surge of attention thanks to the many advances in deeplearning. In this paper, we consider a critical-yet-understudied chal-lenge with unsupervised DOD, that is, effective hyperparameter(HP) tuning/model selection. While several prior work report thesensitivity of OD models to HP settings, the issue is ever so crit-ical for the modern DOD models that exhibit a long list of HPs.We introduce HyPer for tuning DOD models, tackling two fun-damental challenges: (1) validation without supervision (due tolack of labeled outliers), and (2) efficient search of the HP/modelspace (due to exponential growth in the number of HPs). A keyidea is to design and train a novel hypernetwork (HN) that mapsHPs onto optimal weights of the main DOD model. In turn, HyPercapitalizes on a single HN that can dynamically generate weightsfor many DOD models (corresponding to varying HPs), which of-fers significant speed-up. In addition, it employs meta-learning onhistorical OD tasks with labels to train a proxy validation function,likewise trained with our proposed HN efficiently. Extensive experi-ments on different OD tasks show that HyPer achieves competitiveperformance against 8 baselines with significant efficiency gains.",
  "3jIkyk=</latexit>fval": ": HyPer framework illustrated. (top) Offline meta-training of val (depicted in) on historical datasets for proxyvalidation (4.1); (bottom) Online model selection on a newdataset (4.2). We accelerate both meta-training and modelselection using hypernetworks (HN) (depicted in; 3.1). (e.g. depth, width), regularization (e.g. dropout rate, weight decay),and optimization HPs (e.g. learning rate). As expected, their per-formance is highly sensitive to the HP settings . This makeseffective HP or model selection critical, yet computationally costlyas the model space grows exponentially large in the number of HPs.Hyperparameter optimization (HPO) can be written as a bilevelproblem, where the optimal parameters W (i.e. NN weights) onthe training set depend on the hyperparameters .",
  "= arg min Lval(; W)..W = arg minW Ltrn(W; )(1)": "where Lval and Ltrn denote the validation and training losses,respectively. There is a body of literature on HPO for supervisedsettings , as well as for OD that use labeled outliers forvalidation . While supervised model selection leveragesLval, unsupervised OD posits a unique challenge: it does not exhibitlabeled hold-out data to evaluate Lval. It is unreliable to employthe same loss Ltrn as Lval as models with minimum training lossdo not necessarily associate with accurate detection .Prior work. Earlier work have proposed intrinsic measures forunsupervised model evaluation, based on input data and output (out-lier scores) characteristics , using internal consensus amongvarious models , as well as properties of the learned weights",
  "KDD 24, August 2529, 2024, Barcelona, SpainXueying Ding, Yue Zhao, and Leman Akoglu": "differ (smaller kernels would result in larger output size); therefore,we need to guarantee the spatial size by similarity padding zerosaround the input of that convolutional layer. The padding is similarto how we construct the architecture masking A and similar to thepadding approach discussed in .Diverse Weight Generation. While HN is a universal functionapproximator in theory, it may not generalize well to offer goodapproximations for many unseen architectures, especially giventhat the number of s during training is limited. When there isonly little variation between two inputs, the HN provides moresimilar weights, since the weights are generated from the same HNwhere implicit weight sharing occurs. Num of Epochs",
  "Problem and Preliminaries": "The sensitivity of outlier detectors to the choice of their hyper-parameters (HPs) is well documented . Deep-NN basedOD models are no exception, if not even more vulnerable to HPconfiguration , as they exhibit a long list of HPs; architectural,regularization and optimization HPs. In fact, it would not be anoverstatement to point to unsupervised outlier model selection asthe primary obstacle to unlocking the ground-breaking potentialof deep-NNs for OD. This is exactly the problem we consider. Problem 1 (Unsupervised Deep Outlier Model Selection(UDOMS)). Given a new input dataset (i.e., detection task) Dtest =(Xtest, ) without any labels, and a deep-NN based OD model ;Output model parameters corresponding to a selected hyperparame-ter/model configuration (where is the model space) to employon Xtest to maximize s detection performance. Key Challenges: Our work addresses two key challenges thatarise when tuning deep neural network models for outlier detection:(Ch1) Validation without supervision & (Ch2) Large HP/model space.First, unsupervised OD does not exhibit any labels and thereforemodel selection via validating detection performance on labeledhold-out data is not possible. While model parameters can be es-timated end-to-end through unsupervised training losses, such asreconstruction error or one-class losses, one cannot reliably use thesame loss as the validation loss; in fact, low error could easily asso-ciate with poor detection since most DOD models use point-wiseerrors as their outlier scores.Second, model tuning for the modern OD techniques based ondeep-NNs with many HPs is a much larger scale ball-game thanthat for their shallow counterparts with only 1-2 HPs. This is bothdue to their () large number of HPs and also () longer trainingtime they typically demand. In other words, the model space thatis exponential in the number of HPs and the costly training ofindividual models necessitate efficient strategies for search.",
  "Hypernetwork: Train One, Get Many": "To tackle the challenge of model-building efficiency, we propose aversion of hypernetworks (HN) that can efficiently train DOD mod-els with different hyperparameter configurations. A hypernetwork(HN) is a network generating weights (i.e. parameters) for anothernetwork (in our case, the DOD model) . Our input to HN is , which is one HP configuration and breaks down into twocomponents as = , corresponding to regularizationHPs (e.g. dropout, weight decay) and architectural HPs (number oflayers and width of each layer). Parameterized by , the HN mapsa specific hyperparameter configuration to DOD model weightsW () := (;), which parameterize the DOD model forhyperparameter configuration .We propose changes to the HN , such that (1) the outputW can adjust to different architectural shapes, (2) HN can outputsufficiently diverse weights in response to varying inputs, and (3)HN training is more efficient than training individual DOD models.Architecture Masking. To allow the HN output to adapt tovarious architectures, we let the output Ws size be equal to size ofthe largest architecture in model space . Then for each , webuild a corresponding architecture masking and feed the masked-version of W to the DOD model. In other words, the output Whandles all smaller architectures by properly padding zeros.Taking DOD models built upon MLPs as an example (see ),we make HN output W R( ), where and denotethe maximum depth and maximum layer width from . Assume contains the abstraction of a smaller architecture; e.g., layers with corresponding width values {1,2 . . . ,} all lessthan or equal to . The architectural HP N is defined as:",
  "A[,0:,:] = 1, if = 0A[,0:,0:[]] = 1, otherwise(2)": "where is the last non-zero entry in [0 : ]. (e.g., for = and = 4, the last nonzero entry is where = 3.) Then, th layer weights are multiplied by maskingas A[,:,:] W,, where non-zero entries are of shrunk dimen-sions. If A[,:,:] contains only zeros, layer weights become all zeros,representing a \"No operation\" and is ignored in the DOD model.We find that this masking works well with linear autoencoderswith a \"hourglass\" structure, in which case the maximum width is the input dimension. For convolutional networks, even thoughwe can tune depths and channels, we can also include kernel sizesand dilation rate by properly padding W with zeros . Wemake HN output W R( ), where , , represent maximum number of layers, channels, and kernel sizespecified in , respectively.Assume contains the abstraction of a smaller architectures,e.g. layers with corresponding channel values {1, 2, ..., }",
  "A 2 {0, 1}(455)": ": Illustration of the proposed HN. (Top) HN generatesweights for a 4-layer AE, with layer widths equal to .Weights W is fed into the DOD model, while hidden layersdimensions are shrunk by the masking A. (Bottom) HN gen-erates weights for a 2-layer AE, with layer widths equal to. is padded as , and the architecture mask-ing at the second and third layer are set to all zeros. WhenW is fed into the DOD model, zero masking enables the \"NoOperation\" (No-op), in effect shrinking the DOD model from4 layers to 2 layers.all less than or equal to , and {1, 2, ..., } are less than orequal to . Then, the N2 is given as:",
  "]= 1, otherwise": "(4)Again, [2()] is the last entry corresponding to the non-zero input channel in . Similar to the linear operation, atlayer , if is all zero, then the resulting A[,:,:,:,:] wouldcontain only zeros and represent a \"No-op\" in the DOD model.Otherwise, assume we want obtain a smaller kernel size, atlayer , the corresponding A[,:,:,:,:] pads zeros around the size center. The masked weights A[,:,:,:,:] W, are equivalentto obtaining smaller-size kernel weights. Notice that, when kernelsizes are different, the output of the layers operation will also",
  ": Loss of individual modelsduring scheduled training. Lightercolors depict loss curves of deeperarchitectures, which enter train-ing early. Over epochs loss is mini-mized for all models collectively": "We employ two ideastoward enabling theHN to generate moreexpressive weights inresponse to changesin . First is toinject more variationwithin its input spacewhere, instead of di-rectly feeding in ,we input the posi-tional encoding of eachelement in . Po-sitional encoding transforms each scalarelement into a vector embedding, which encodes more granularinformation, especially when contains zeros representing ashallower sub-architecture. Second idea is to employ a scheduledtraining strategy of the HN as it produces weights for both shal-low and deep architectures. During HN training, we train with associated with deeper architectures first, and later, for shallowerarchitectures are trained jointly with deeper architectures. Ourscheduled training alleviates the problem of imbalanced weightsharing, where weights associated with shallower layers are up-dated more frequently as those are used by more number of ar-chitectures. illustrates how the training losses change forindividual architectures during the HNs scheduled training.Batchwise Training. Like other NNs, HN allows for severalinputs { }=1 synchronously and outputs { W ()}=1. To speedup training, we batch the input at each forward step with a set ofdifferent architectures and regularization HP configurations. As-suming { }=1 = {}=1 are the set of sampledHP configurations from the model space, given training points X,the HN loss for one pass is calculated as:",
  "=1LtrnW (), x(5)": "where the training loss L is the same loss as that of the DODmodel of interest; e.g. reconstruction loss for autoencoder-basedOD models, one-class losses , or regularized reconstruction loss, etc. We feed the HN-generated weights (instead of learningthe actual weights) as well as the training data X to the DOD model, which then outputs the outlier scores O := (X; W ()).The outlier scores are used to compute the training loss L, aswell as in training our proxy validation function as described next. (See ) During training, the gradients can further propagatethrough the generated weights to update the HN parameters .In summary, our HN mimics fast DOD model building acrossdifferent HP configurations. This offers two advantages: () trainingmany different HPs jointly in meta-training and () fast DOD modelparameter generation during online model search. Notably, our HNcan tune a wider range of HPs including model architecture, andas shown in 5.3, provides superior results to only tuning .",
  "Validation without Supervision viaMeta-learning": "Given the lack of ground truth labels on a new dataset, modelselection via supervision is not feasible. Instead, we consider trans-ferring supervision from historical datasets through meta-learning,enabling model performance evaluation on the new dataset. Meta-learning uses a collection of historical tasks Dtrain = {D1, . . . , D }that contain ground-truth labels, i.e. {D = (X, y)}=1. For manyOD settings, such historical datasets are obtainable. For example, fora new disease diagnosis, many previous medical records and medi-cal images are available for normal vs. abnormal health conditions,which can be utilized as historical datasets.Given a DOD algorithm for UDOMS, we let denote theDOD model with a specific HP setting from the set {1, . . . , } . HyPer uses Dtrain to compute two quantities. First, we obtainhistorical outlier scores of each with HP setting on eachD = (X, y) Dtrain. Let O, := (X, W()) denote theoutput outlier scores of trained with a specific HP configura-tion for the data points X in dataset D, where W() ares estimated (i.e. HN-generated) weights for . Second, wecan calculate the historical performance matrix P R , whereP, := perf(O,, y) denotes s detection performance (e.g. AU-ROC) on dataset D, evaluated based on y.With the historical outlier scores and the performance matrixin hand, we train a proxy validator called val, which provides uswith performance estimation when we encounter a new datasetand no label is given. As shown in (right), the high-level ideaof val is to learn a mapping from data and model characteristics(e.g., distribution of outlier scores) to the corresponding OD per-formance across historical datasets and models (with differentHP configurations). Since it is costly to train all these OD modelsindividually on all datasets from scratch, we utilize our previouslyproposed HN only once per dataset across different HP configura-tions, which generates the weights and outlier scores for all models.With the hypernetwork and meta-learning from historical datasetintroduced, we present the full framework of HyPer in 4 and thetraining details of val in 4.1.",
  "HyPer Framework for UDOMS": "HyPer consists of two phases (see ): (4.1) offline meta-trainingover the historical datasets, and (4.2) online model selection for agiven test dataset. In the offline phase, we train the proxy validatorval, which allows us to predict model performance on the testdataset without relying on labels. During online model selection,we alternate between training our HN to efficiently generate modelweights for varying HPs around a local neighborhood, and refining",
  "Offline Training on Historical Datasets": "In HyPer, we train a proxy validator val across historical datasets,so that we can predict the performance of an OD model on the testdataset. val maps HP configuration (), data embedding, and modelembedding onto the corresponding model performance across his-torical datasets. The goal is to predict detection performance solelybased on the characteristics of the input data and the trained model,along with the HP values. We create the data embedding and modelembedding as described below.Data Embeddings. Existing work captures the data char-acteristics of an OD dataset via extracting meta-features, such asthe number of samples and features, to describe a dataset. Althoughsimple and intuitive, meta-features primarily focus on general datacharacteristics with a heuristic extraction process, and are insuffi-cient in model selection . In this work, we design a principledapproach to capture dataset characteristics. First, the datasets mayhave different feature and sample sizes, which makes it challengingto learn dataset embeddings. To address this, we employ featurehashing , (), to project each dataset to a-dimensional unifiedfeature space. To ensure sufficient expressiveness, the projectiondimension should not be too small ( = 256 in our experiments).Subsequently, we train a cross-dataset feature extractor (), a fullyconnected neural network, trained with historical datasets labels,to learn the mapping from hashed samples to the correspondingoutlier labels. i.e. : (X) y for the -th dataset. Trainingover datasets, the latent representations by () are expected tocapture the outlying characteristics of datasets. Finally, we use max-pooling to aggregate sample-wise representations into dataset-wiseembeddings, denoted by pool{( (X))}.Model Embeddings. In addition to data embeddings, we needmodel embeddings that change along with the varying hyperpa-rameter settings to train an effective proxy validator. Here we usethe historical outlier scores and historical performance matrix, aspresented in 3.2, to learn a neural network () that generates themapping from the outlier scores onto detection performance, i.e. : O, P,. To handle size variability of outlier scores (due tosample size differences across datasets) as well as to remain agnos-tic to permutations of outlier scores within a dataset, we employthe DeepSet architecture for (), and use the pooling layersoutput as the model embedding, denoted by pool{(O,)}.Effective and Efficient val Training. By incorporating theaforementioned components, we propose the proxy validator val,which tries to learn the following mapping:",
  "P,": "{1, . . . , }, {1, . . . ,}(6)We train val with lightGBM (one may use any regressor),across historical datasets and models with varying HP config-urations. The loss function is the squared error between predictionP and P: L = =1=1 P, P, 2.By considering both data and model embeddings in Eq. (6), valpredicts performance more effectively compared to existing worksthat solely rely on HP values and dataset meta-features . Notice that obtaining the model embeddings in Eq. (6) that relyon the outlier scores O, requires training the DOD model for eachdataset D and each HP configuration , which can be computa-tionally expensive. To speed up this process for meta-training, weuse HN-generated weights rather than training these individualDOD models from scratch, to obtain O, := (X; W() ()), where W() () denotes model s weights for HP configuration, as generated by our HN trained on D.At test time, the proxy validator val provides performance evalu-ation by taking in the new datasets embedding and a DOD modelsembedding with a HP configuration, without requiring the groundtruth labels. In this way, we are able to leverage the benefits ofmeta-learning from historical datasets, and estimate the perfor-mance of a specific DOD model on a new dataset, when the newdataset contains no labels.",
  "Online Model Selection": "Model selection via proxy validator. With our meta-trained valat hand, given a test dataset Xtest, a simple model selection strat-egy would be to train many DOD models with different randomlysampled HPs on Xtest to obtain outlier scores, and then select theone with the highest predicted performance by val.However, training OD models from scratch for each HP can becomputationally expensive. To speed this up, we also train a HN onXtest and subsequently obtain the outlier scores Otest from the DODmodel with HN-generated weights for randomly sampled HPs. Weselect the best HP configuration according to Eq. (7).",
  "argmaxval(Xtest, , Otest,)(7)": "Moreover, we propose to iteratively train our HN over locallyselected HPs, since training a global HN to generate weightsacross the entire and over unseen is a challenging task especiallyfor large model spaces , which can impact the quality of thegenerated weights and subsequently affect the overall performanceof the selected model. Therefore, we propose two stages of alternateupdating. One stage trains the HN according to a neighborhoodof sampled HP configurations around the current best HP, whilethe other stage applies the generated weights from HN to obtainoutlier scores, and subsequently find the new best HP configurationthrough the performance proxy validator.Training local HN iteratively and adaptively. We designHyPer to jointly optimize the HPs and the (local) HN parameters in an alternating fashion; as shown in (bottom). Algorithm1 provides the step-by-step outline of the process. Over iterations,it alternates between two stages:",
  "(S1) HN training that updates HN parameters to approximatethe best-response in a local neighborhood around the currenthyperparameters curr via Lhn (Lines 48), and": "(S2) HP optimization that updates curr in a gradient-free fash-ion by estimating detection performance through val of alarge set of candidate s sampled from the same neighbor-hood, using the corresponding approximate best-response,i.e. the HN-generated weights, W () (Line 9).To dynamically control the sampling range around curr, weuse a factorized Gaussian with standard deviation to generate",
  ": return argmaxS val(Xtest, , Otest,) Eq. (10)": "local HP perturbations (|). We initialize to be a scale factorvector, each value is within R+, and dynamically change the valueof , which becomes curr to control the radius of sampling neigh-borhood. curr is used in (S1) for sampling local HPs and is thenupdated in (S2) at each iteration (Line 10).Updating curr and curr. HyPer iteratively explores promis-ing HPs and the corresponding sampling range. To update currand the sampling factor curr, we maximize:E ( | ) [val(Xtest, + , W ( + ))]",
  "=1 val(Xtest, + , W ( + )) + H((|)) (9)": "In each iteration of the HP update, we first fix curr and findthe configuration in S with the highest value of Eq. (9), where wesample local configurations around each S, i.e., + |currfor 1, . . . , . After curr is updated, we fix it and update thesampling factor curr also by Eq. (9), using samples based on each : HyPer and baselines for time (in mins) and perfor-mance comparison with categorization by whether it selectsmodels (2nd column), uses meta-learning (3rd column), andrequires model building at the test time (4th column). Over-all, HyPer (with patience = 3) achieves the best detectionperformances (also see and 5). Compared to the SOTAELECT, HyPer has markedly shorter offline and online time.",
  "Ours1,32014170.2954": "from a pre-specified range for each HP. To ensure encounteringa good HP configuration, we set to be a large number, e.g. 500.We provide details and pre-specified range in Appx. A.1.Selecting the Best Model/HP . We employ val to choosethe best HP among all the locally sampled HPs S during thelast iteration of HN training. Note that HyPer directly uses theHN-generated weights for fast computation, without the need totrain any model from scratch for evaluation by val. With the gener-ated weights W () , the DOD model produces the corresponding",
  "argmaxSval(Xtest, , Otest,) .(10)": "Initialization and Convergence. We initialize curr and currwith the globally best values across historical datasets. We considerHyPer as converged if the highest predicted performance by valdoes not improve in consecutive iterations. A larger , referredas patience, requires more iterations to converge yet likely yieldsbetter results. Note that can be decided by cross-validation onhistorical datasets during meta-training. We present an empiricalanalysis of initialization and patience in the experiments.",
  "Experiments5.1Experiment Settings": "Benchmark Data. We show HyPers effectiveness and efficiencywith fully connected AutoEncoder (AE) for DOD on tabular data,using a testbed consisting of 34 benchmark datasets from two differ-ent public OD repositories; ODDS and DAMI (Pima datasetis removed). In addition, we run HyPer with convolutional AE onMNIST and FashinMNIST datasets. We treat one class as the normalclass, while downweighting the ratio of another class at 10% as theoutlier class. We train and validate HyPer with inliers and outliersfrom classes (30 tasks/datasets in total) and evaluate 8 taskson , to avoid data leakage in (meta)training/testing data.Baselines. For tabular dataset, we include 8 baselines for com-parison ranging from simple to state-of-the-art (SOTA); provides a conceptual comparison of the baselines. They are orga-nized as (i) no model selection: (1) Default uses the default HPsused in a popular OD library PyOD , (2) Random picks anHP/model randomly (we report expected performance); (ii): model",
  "Fast Unsupervised Deep Outlier Model Selection with HypernetworksKDD 24, August 2529, 2024, Barcelona, Spain": "patience (p) 0.730 0.735 0.740 0.745 avg. best fval patience (p) 0.2750 0.2850 0.2950 0.3050 0.3150 avg. ROC Rank : Analysis of the effect of patience : (left) avg. valvalue change when increasing from 1 to 4; (right) avg. ROCRank (lower is better) with increasing . Larger leads tomore exploration and tends to offer better performance. potentially better performance. However, this also prolongs theconvergence time. In our experiments, we set = 3 to balance per-formance and runtime. The specific value of can be determinedthrough cross-validation over the historical datasets.",
  "Experiment Results": "Tabular. shows that HyPer outperforms all baselineswith regard to average ROC Rank on the 35 testbed.In addi-tion, provides the full performance distribution across alldatasets and shows that HyPer is statistically better than all base-lines, including SOTA meta-learning based ELECT and MetaOD.Among the zero-shot baselines, Default and Random perform sig-nificantly poorly while the meta-learning based GB leads to compa-rably higher performance. Same as previous study , the internalconsensus-based MC can be no better than Random.HN-powered efficiency enables HyPer to search more broadly. and Appx. show that HyPer offers significant speedup over the SOTA method ELECT, with an average offline trainingspeed-up of 5.77 and a model selection speed-up of 4.21. UnlikeELECT, which requires building OD models from scratch duringboth offline and online phases, HyPer leverages the HN-generatedweights to avoid costly model training for each candidate HP.Meanwhile, HyPer can also afford a broader range of HP con-figurations thanks to the lower model building cost by HN. This 0.00.20.40.60.81.0 ROC Rank (lower the better)",
  "Ours": "ELECT * MetaOD* * ISAC* * GB* * Random* * * MC* * * AS* * Default* * Ours (reg&width)* * * Ours (reg&depth)* * * Ours (reg only)* * * : Distribution of ROC Rank across datasets. HyPerachieves the best performance. Bottom three bars depictHyPers variants that do not fully tune architecture HPs (forablation). Paired test results are depicted as significant w/",
  "at 0.1, at 0.01, at 0.001. See -values in Appx. Table A1": "capability contributes to the effectiveness of HyPer, which brings7% avg. ROC Rank over ELECT.Meta-learning methods achieve the best performance atdifferent budgets. and Appx. show that the bestperformers at different time budgets are global best (GB), MetaOD,and HyPer, which are all on the Pareto frontier. In contrast, simpleno-model-selection approaches, i.e., Default and Random, are typi-cally the lowest performing methods. Specifically, HyPer achievesa significant 2 avg. ROC Rank improvement over the default HPin PyOD ), a widely used open-source OD library. Althoughmeta-learning entails additional (offline) training time, it can beamortized across multiple future tasks in the long run.Image. shows that HyPer is outperforming both Ran-dom selection and Default LeNet AutoEncoder model, as it is ableto effectively learn from historical data to tune HPs. In addition, weevaluate HyPer s performance across datasets by online trainingwith 5 FashionMNIST tasks, with the same search space of HPsand same offline-training solely on MNIST anomaly detection tasks. shows that HyPer outperforms Random on average,but notnecessarily on all datasets. It may be due to the fact that MNIST(used for meta-learning) and FashionMNIST (test tasks) are from dif-ferent distributions and share less similarities for effective transferthrough meta-learning.",
  "Inlier/Outlier ClassRandomHyPer": "Inlier: T-shirt Outlier: Trouser0.60040.6132Inlier: Trouser Outlier: Pullover0.89020.9878Inlier: Dress Outlier: Coat0.87150.8456Inlier: Sandal Outlier: Shirt0.90240.8752Inlier: Sneaker Outlier: Bag0.94570.9033 1 2 3 4 5 6 7 8 910 # iterations depth 1 2 3 4 5 6 7 8 910 # iterations shrinkage rate 1 2 3 4 5 6 7 8 910 # iterations 0.00.10.20.30.40.50.60.70.80.9 dropout 1 2 3 4 5 6 7 8 910 # iterations 0.000.010.020.030.040.050.060.070.080.090.10 weight decay 1 2 3 4 5 6 7 8 910 # iterations depth 1 2 3 4 5 6 7 8 910 # iterations shrinkage rate 1 2 3 4 5 6 7 8 910 # iterations 0.00.10.20.30.40.50.60.70.80.9 dropout 1 2 3 4 5 6 7 8 910 # iterations 0.000.010.020.030.040.050.060.070.080.090.10 weight decay : Trace of HP changes over iterations on spamspace:(top) tuning regularization HPs only; (bottom) tuning bothregularization and architectural HPs (ours). When arch. isfixed, reg. HPs incur more magnitude changes and reachlarger values to adjust model complexity. HyPer tunes com-plexity more flexibly by also accommodating arch. HPs.",
  "Ablation Studies": "Benefit of Tuning Architectural HPs via HN. HyPer tacklesthe challenging task of accommodating architectural HPs besidesregularization HPs. Through ablations, we study the benefit ofour novel HN design, as presented in 3.1, which can generateDOD model weights in response to changes in architecture. Bottomthree bars of show the performances of three HyPer variantsacross datasets. The proposed HyPer (with median ROC Rank =0.1349) outperforms all these variants significantly (with <0.001),namely, only tuning regularization and width (median ROC Rank= 0.2857), only tuning regularization and depth (median ROC Rank= 0.3095), and only tuning regularization (median ROC Rank =0.3650). By extending its search for both neural network depthand width, HyPer explores a larger model space that helps findbetter-performing model configurations.HP Schedules over Iterations. In , we more closely an-alyze how HPs change over iterations on spamspace, comparingbetween (top) only tuning reg. HPs while fixing model depth andwidth (i.e., shrinkage rate) and (bottom) using HyPer to tune allHPs including both reg. and architectural HPs. Bottom figures showthat depth remains fixed at 4, shrinkage rate increases from 1 to2.25 (i.e., width gets reduced), dropout to 0.2, and weight decayto 0.05overall model capacity is reduced relative to initialization.In contrast, top figures show that, when model depth and widthare fixed, regularization HPs compensate more to adjust the model capacity, with a larger dropout rate at 0.4 and larger weight decay at0.08, achieving ROC rank 0.3227 in contrast to HyPers 0.0555. Thiscomparison showcases the merit of HyPer which adjusts modelcomplexity more flexibly by accommodating a larger model space. 1 2 3 4 5 6 7 8 9 101112131415 # random initializations 0.01 0.02 0.03 0.04 0.05 0.06 ROC Rank random init.ours (meta init.) 1 2 3 4 5 6 7 8 9101112131415 # random initializations 0.025 0.050 0.075 0.100 0.125 0.150 0.175 ROC Rank random init.ours (meta init.) 1 2 3 4 5 6 7 8 9 101112131415 # random initializations 0.00 0.02 0.04 0.06 0.08 0.10 0.12 ROC Rank random init.ours (meta init.) 1 2 3 4 5 6 7 8 9 101112131415 # random initializations 0.4 0.5 0.6 0.7 0.8 0.9 ROC Rank random init.ours (meta init.) : Comparison of ROC Rank (lower is better) of HyPerwith meta-initialization (in blue) with increasing numbers ofrandomly initialized HNs, on ODDS_wine (upper left), WDBC (up-per right), HeartDisease (lower left) and Ionosphere (lowerright). It needs 9 randomly initialized HNs to achieve thesame performance as HyPer on ODDS_wine . In general,HyPerfinds a good model with much less running time. Effect of Meta-initialization. In , we demonstrate theeffectiveness of meta-initialization by comparing it with random ini-tialization on four datasets. In addition to utilizing meta-initialization,one could run HyPer multiple times with randomly initialized HPsand select the best model based on val. To simulate this scenario,we vary the number of random initializations (x-axis) and record allthe val values along with the corresponding ROC Rank. For eachdataset, we select the best model based on val across all trials. Weincrease the number of random trials from 1 to 15, where the high-est val value among the 15 random initialized trials is chosen asthe best model. Meta-initialization is indeed a strong starting pointfor HyPers HP tuning. For example, on the ODDS_wine dataset, itrequires 9 randomly initialized HNs to attain the same performanceas our approach with meta-initialization, showing a 9-fold increasein the time required for online selection. In other cases, training 15randomly initialized HNs fails to achieve the same performance asmeta-initialization, further validating its advantages.Effect of Patience. The convergence criterion for HyPer isbased on the highest predicted performance by val remaining un-changed for consecutive iterations (patience). As illustrated in, increasing the value of allows for more exploration and",
  "Limitation": "To analyze HyPer s performance with respect to train/test datasets,we compare HyPer s predictions to the test datasets similarity tothe training datasets. We first adapt the code from MetaODs featureextractor1 and extract features that represent the underlying datadistribution, including mean, standard deviation, kurtosis, sparsity,skewness, and etc. Since each dataset is now represented as a vec-tor of meta-features, we are able to measure the pairwise cosinesimilarity between datasets. For each dataset, we then calculate theaverage cosine similarity to the training datasets. shows the Top-5 datasets where HyPer has the mostAUROC performance differences to the Top-1 baseline (listed inTable A3), and the datasets average cosine similarity to the trainingdatasets. We observe that HyPer s performance can be subparwhen the test dataset has a small cosine similarity to the trainingdatasets, since all the 5 datasets have smaller cosine similarity thanthe average pairwise dataset similarity. We can further concludethat one working assumption of HyPer is that test dataset has asimilar data distribution to at least a few of the training datasets.: Average Cosine Similarity to Training Dataset vs.HyPers AUROC Difference. HyPer performs worse when testdataset has small consine similarity to training data.",
  "Related Work": "Supervised Model Selection. Supervised model selection lever-ages hold-out data with labels. Randomized , bandit-based ,and Bayesian optimization (BO) techniques are various leadingapproaches. Self-tuning networks (STN) utilizes validation data to alternatively update the HPs in the HP space along with the cor-responding model weights. Under the context of OD, recent workinclude AutoOD that focuses on neural architecture search,as well as PyODDS and TODS for model selection, allof which rely on hold-out labeled data. Clearly, these supervisedapproaches do not apply to UDOMS.Unsupervised Model Selection. To choose OD models in anunsupervised fashion, one approach is to design unsupervised in-ternal evaluation metrics that solely depend on inputfeatures, outlier scores, and/or the learned model parameters. How-ever, a recent large-scale study showed that most internal metricshave limited performance in unsupervised OD model selection .More recent solutions leverage meta-learning that selects the modelfor a new dataset by using the information on similar historicaldatasetsSOTA methods include MetaOD and ELECT .Their key bottleneck is efficiently training the candidate modelswith different HPs, which is addressed in our work.Hypernetworks. Hypernetworks (HN) have been primarilyused for parameter-efficient training of large models with diversearchitectures as well as generating weights for diverselearning tasks . HN generates weights (i.e. parameters) foranother larger network (called the main network) . As such,one can think of the HN as a model compression tool for training,one that requires fewer learnable parameters. Going back in history,hypernetworks can be seen as the birth-child of the fast-weightsconcept by Schmidhuber , where one network produces context-dependent weight changes for another network. The context, inour as well as several other work , is the hyperparameters(HPs). That is, we train a HN model that takes (encoding of) the HPsof the (main) DOD model as input, and produces HP-dependentweight changes for the DOD model that we aim to tune. Training asingle HN that can generate weights for the (main) DOD model forvarying HPs can effectively bypass the cost of fully-training thosecandidate models from scratch.",
  "Conclusion": "We introduced HyPer, a new framework for unsupervised deepoutlier model selection. HyPer tackles two fundamental challengesthat arise in this setting: validation in the absence of supervision andefficient search of the large model space. To that end, it employsmeta-learning to train a proxy validation function on historicaldatasets to effectively predict model performance on a new taskwithout labels. To speed up search, it utilizes a novel hypernetworkdesign that generates weights for the detection model with varyingHPs including model architecture, achieving significant efficiencygains over individually training the candidate models. Extensiveexperiments on a large testbed with 35 benchmark datasets showedthat HyPer significantly outperforms 8 simple to SOTA baselines.We expect that our work will help practitioners use existing deepOD models more effectively as well as foster further work on unsu-pervised model selection in the era of deep learning.",
  "Andrew Brock, Theodore Lim, James M. Ritchie, and Nick Weston. 2018. SMASH:One-Shot Model Architecture Search through HyperNetworks.. In ICLR (Poster).OpenReview.net": "Guilherme Oliveira Campos, Arthur Zimek, Jrg Sander, Ricardo J. G. B. Campello,Barbora Micenkov, Erich Schubert, Ira Assent, and Michael E. Houle. 2016. Onthe evaluation of unsupervised outlier detection: measures, datasets, and anempirical study. Data Min. Knowl. Discov. 30, 4 (2016), 891927. Guilherme Oliveira Campos, Arthur Zimek, Jrg Sander, Ricardo J. G. B. Campello,Barbora Micenkov, Erich Schubert, Ira Assent, and Michael E. Houle. 2016. Onthe evaluation of unsupervised outlier detection. DAMI 30, 4 (2016), 891927. Xueying Ding, Lingxiao Zhao, and Leman Akoglu. 2022. Hyperparameter Sen-sitivity in Deep Outlier Detection: Analysis and a Scalable Hyper-EnsembleSolution. In Advances in Neural Information Processing Systems, Alice H. Oh,Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (Eds.). Sunny Duan, Loic Matthey, Andre Saraiva, Nick Watters, Christopher Burgess,Alexander Lerchner, and Irina Higgins. 2020. Unsupervised Model Selection forVariational Disentangled Representation Learning.. In ICLR. OpenReview.net.#DuanMSWBLH20",
  "Serdar Kadioglu, Yuri Malitsky, Meinolf Sellmann, and Kevin Tierney. 2010. ISAC- Instance-Specific Algorithm Configuration.. In ECAI, Vol. 215. 751756": "Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma,Qiwei Ye, and Tie-Yan Liu. 2017. Lightgbm: A highly efficient gradient boostingdecision tree. Advances in neural information processing systems 30 (2017). Boris Knyazev, Michal Drozdzal, Graham W Taylor, and Adriana Romero Soriano.2021. Parameter prediction for unseen deep architectures. Advances in NeuralInformation Processing Systems 34 (2021), 2943329448. Kwei-Herng Lai, Daochen Zha, Guanchu Wang, Junjie Xu, Yue Zhao, DeveshKumar, Yile Chen, Purav Zumkhawaka, Minyang Wan, Diego Martinez, andXia Hu. 2021. TODS: An Automated Time Series Outlier Detection System. InThirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-ThirdConference on Innovative Applications of Artificial Intelligence, IAAI 2021, TheEleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021,Virtual Event, February 2-9, 2021. AAAI Press, 1606016062. Lisha Li, Kevin G. Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and AmeetTalwalkar. 2017. Hyperband: A Novel Bandit-Based Approach to HyperparameterOptimization. J. Mach. Learn. Res. 18 (2017), 185:1185:52.#LiJDRT17 Yuening Li, Zhengzhang Chen, Daochen Zha, Kaixiong Zhou, Haifeng Jin,Haifeng Chen, and Xia Hu. 2021. AutoOD: Neural Architecture Search forOutlier Detection. In 37th IEEE International Conference on Data Engineer-ing, ICDE 2021, Chania, Greece, April 19-22, 2021. IEEE, 21172122. Yuening Li, Daochen Zha, Praveen Kumar Venugopal, Na Zou, and Xia Hu. 2020.PyODDS: An End-to-end Outlier Detection System with Automated MachineLearning. In Companion of The 2020 Web Conference 2020, Taipei, Taiwan, April20-24, 2020, Amal El Fallah Seghrouchni, Gita Sukthankar, Tie-Yan Liu, andMaarten van Steen (Eds.). ACM / IW3C2, 153157. Zinan Lin, Kiran Thekumparampil, Giulia Fanti, and Sewoong Oh. 2020. InfoGAN-CR and ModelCentrality: Self-supervised model training and selection for disen-tangling GANs. In International Conference on Machine Learning. PMLR, 61276139. Martin Q Ma, Yue Zhao, Xiaorong Zhang, and Leman Akoglu. 2023. The Needfor Unsupervised Outlier Model Selection: A Review and Evaluation of InternalEvaluation Strategies. ACM SIGKDD Explorations Newsletter 25, 1 (2023). Xiaoxiao Ma, Jia Wu, Shan Xue, Jian Yang, Chuan Zhou, Quan Z Sheng, Hui Xiong,and Leman Akoglu. 2021. A comprehensive survey on graph anomaly detectionwith deep learning. IEEE Transactions on Knowledge and Data Engineering (2021). Matthew MacKay, Paul Vicol, Jonathan Lorraine, David Duvenaud, and Roger B.Grosse. 2019. Self-Tuning Networks: Bilevel Optimization of Hyperparametersusing Structured Best-Response Functions.. In ICLR (Poster). OpenReview.net. Henrique O. Marques, Ricardo J. G. B. Campello, Jrg Sander, and Arthur Zimek.2020. Internal Evaluation of Unsupervised Outlier Detection. ACM Trans. Knowl.Discov. Data 14, 4 (2020), 47:147:42. #MarquesCSZ20",
  "Marcin Przewilikowski, Przemysaw Przybysz, Jacek Tabor, M Ziba, and Prze-mysaw Spurek. 2022. HyperMAML: Few-Shot Adaptation of Deep Models withHypernetworks. arXiv preprint arXiv:2205.15745 (2022)": "Shebuti Rayana. 2016. ODDS Library. Lukas Ruff, Jacob R Kauffmann, Robert A Vandermeulen, Grgoire Montavon,Wojciech Samek, Marius Kloft, Thomas G Dietterich, and Klaus-Robert Mller.2021. A unifying review of deep and shallow anomaly detection. Proc. IEEE 109,5 (2021), 756795. Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib AhmedSiddiqui, Alexander Binder, Emmanuel Mller, and Marius Kloft. 2018. DeepOne-Class Classification. In Proceedings of the 35th International Conference onMachine Learning (Proceedings of Machine Learning Research, Vol. 80), JenniferDy and Andreas Krause (Eds.). PMLR, 43934402. Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib AhmedSiddiqui, Alexander Binder, Emmanuel Mller, and Marius Kloft. 2018. DeepOne-Class Classification. In Proceedings of the 35th International Conference onMachine Learning (Proceedings of Machine Learning Research, Vol. 80), JenniferDy and Andreas Krause (Eds.). PMLR, 43934402.",
  "Johannes von Oswald, Christian Henning, Benjamin F. Grewe, and Joo Sacra-mento. 2020. Continual learning with hypernetworks. In International Conferenceon Learning Representations": "Xiaoxing Wang, Chao Xue, Junchi Yan, Xiaokang Yang, Yonggang Hu, and KeweiSun. 2021. Mergenas: Merge operations into one for differentiable architecturesearch. In Proceedings of the Twenty-Ninth International Conference on Interna-tional Joint Conferences on Artificial Intelligence. 30653072. Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and JoshAttenberg. 2009. Feature hashing for large scale multitask learning. In Proceedingsof the 26th annual international conference on machine learning. 11131120.",
  "AppendixAAdditional Experiment Settings and ResultsA.1Algorithm Settings and Baselines": "Setting of HN: The HN utilized in the experiments consists of twohidden layers, each containing 200 neurons. It is configured witha learning rate of 1e-4, a dropout rate of 0.2, and a batch size of512. We find this setting give enough capacity to generate variousweights for linearAEs. Because of the meta-learning setting, thehyperparameters of HN can be tested with validation data and testresults, on historical data.Meta-training for val. Table A2 includes the HP search spacefor training in fully-connected AE. In the table, compression raterefers to how many of the widths to shrink between two adja-cent layers. For example, if the first layer has width of 6, compres-sion_rate equals 2 would gvie the next layer width equal to 3. Wealso notice that some datasets may have smaller numbers of fea-tures. Thus, with the corresponding compression rate, we also havediscretized the width to the nearest integer number. Thus, for somedatasets, the HP search space will be smaller than 240. In addition,for HPs in Convolutional AE, HyPer conducts a search among thefollowing HPs: Number of encoders is within , kernel size isin , channels is , lr is , weight decay is, and dropout is [0, 0.1, 0.2].HN (Re-)Training during the Online Phase: In order to facil-itate effective local re-training, we set a training epoch of = 100for each iteration, indicating the sampling of 100 local HPs for HNretraining. In Eq. (9), we designate the number of sampled HPs andthe sampling factor as 500, i.e., = = 500.Specification of : It is noted that for some values of , thesampled may not be a valid HP configuration. For example, itis not possible to have floating number as number of layers, or itis not practical when dropout is larger than 0.5. We discard theimpossible .Convergence: To achieve favorable performance within a rea-sonable timeframe, we set the patience value as = 3.Baselines: We have incorporated 8 baselines, encompassing aspectrum from simple to state-of-the-art (SOTA) approaches. offers a comprehensive conceptual comparison of these baselines.(i) no model selection:(1) Default employs the default HPs utilized in the widely-usedOD library PyOD . This serves as the default option forpractitioners when no additional information is available. (2) Random randomly selects an HP/model (the reported per-formance represents the expected value obtained by averag-ing across all DOD models).(ii) model selection without meta-learning:(3) MC utilizes the consensus among a group of DODmodels to assess the performance of a model. A model isconsidered superior if its outputs are closer to the consensusof the group. MC necessitates the construction of a modelgroup during the testing phase. For more details, please referto a recent survey .(iii) model selection by meta-learning requires first building acorpus of historical datasets on a group of defined DOD models andthen selecting the best from the model set at the test time. Although",
  "(7) MetaOD employs matrix factorization to capture bothdataset similarity and model similarity, representing one ofthe state-of-the-art methods for unsupervised OD modelselection": "(8) ELECT iteratively identifies the best model for the testdataset based on performance similarity to the historicaldataset. Unlike the above meta-learning approaches, ELECTrequires model building during the testing phase to computeperformance-based similarity.Baseline Model Set. We use the same HP search spaces forbaseline models as well as the HN-trained models. Table A2 providesthe detailed HP search space for fully connected AE. For Conv AE,the HP search space is: Number of encoders is within , kernelsize is in , channels is , lr is , weightdecay is , and dropout is [0, 0.1, 0.2].",
  "A.2Additional Results": "In addition to the distribution plot in , we provide the -valuesof Wilcoxon signed rank test between HyPer and baselines in A1.See 5 for the experiment analysis. Full results are in Table A3.Table A1: Pairwise statistical tests between HyPer and base-lines by Wilcoxon signed rank test. HyPer are statisticallybetter than baselines at different significance levels.",
  "IT/ACr87YeXbenPdFa8HJZw7hF5yPb/TKjP8=</latexit>16": ": Illustration of the proposed HN. HN generates weights for a 3-layer convolutional networks , with channels equal to, and kernels equal to . The HN weights W is of size 3 16 16 6 6, and similarily we construct the same-sizearchitecture masking A. At the first layer, we need to pad A for 1 zero, among the third and fourth dimension (we pad startingfrom the left and from the top). This will enable us to extend W to a convolutional operation of kernel size 5, from fixedkernel size 6. To match the padding operation, we also pad the input X along the first and second dimension, with 1. The restlayers follow similairly.",
  "DatasetDefaultRandomMCGBISACASMetaODELECTOurs": "DAMI_Annthyroid0.7124 (1)0.5972 (6)0.6123 (3)0.5929 (7)0.6018 (5)0.5873 (9)0.6050 (4)0.6148 (2)0.5888 (8)DAMI_Cardiotocography0.7159 (6)0.7202 (5)0.7024 (7)0.7740 (3)0.7571 (4)0.6940 (8)0.6458 (9)0.7818 (2)0.7866 (1)DAMI_Glass0.7442 (1)0.7055 (6)0.6304 (9)0.7244 (3)0.6699 (8)0.7230 (4)0.7225 (5)0.7431 (2)0.6917 (7)DAMI_HeartDisease0.3045 (9)0.4276 (6)0.4214 (7)0.5250 (5)0.5382 (2)0.4214 (7)0.5312 (4)0.5348 (3)0.5926 (1)DAMI_PageBlocks0.8722 (8)0.9107 (5)0.9219 (2)0.9162 (4)0.9255 (1)0.9002 (6)0.6247 (9)0.8791 (7)0.9215 (3)DAMI_PenDigits0.3837 (9)0.5248 (6)0.5422 (5)0.5491 (4)0.5069 (8)0.6953 (1)0.6278 (3)0.5084 (7)0.6792 (2)DAMI_Shuttle0.6453 (8)0.9462 (2)0.9400 (5)0.9342 (7)0.9436 (3)0.9530 (1)0.5525 (9)0.9405 (4)0.9391 (6)DAMI_SpamBase0.5208 (7)0.5232 (5)0.4907 (9)0.5210 (6)0.5263 (4)0.5552 (1)0.5307 (3)0.5135 (8)0.5525 (2)DAMI_Stamps0.8687 (6)0.8687 (6)0.8926 (4)0.8981 (3)0.9079 (1)0.8618 (8)0.7112 (9)0.8897 (5)0.9003 (2)DAMI_Waveform0.6810 (7)0.6772 (8)0.6560 (9)0.6941 (2)0.6924 (4)0.6890 (6)0.6900 (5)0.7019 (1)0.6929 (3)DAMI_WBC0.7493 (9)0.9769 (6)0.9770 (5)0.9682 (8)0.9742 (7)0.9779 (3)0.9809 (2)0.9779 (3)0.9826 (1)DAMI_WDBC0.8092 (7)0.8366 (4)0.8146 (9)0.8597 (3)0.8683 (2)0.8092 (7)0.8361 (5)0.8213 (6)0.9039 (1)DAMI_Wilt0.5080 (1)0.4524 (8)0.4832 (2)0.4653 (7)0.4700 (4)0.4700 (4)0.4714 (3)0.4700 (4)0.3709 (9)DAMI_WPBC0.4090 (8)0.4464 (5)0.3972 (9)0.4679 (3)0.4548 (4)0.4285 (7)0.4456 (6)0.4726 (2)0.4824 (1)ODDS_annthyroid0.7353 (1)0.6981 (7)0.6963 (8)0.6982 (6)0.7067 (2)0.7067 (2)0.6903 (9)0.7058 (4)0.7014 (5)ODDS_arrhythmia0.7769 (9)0.7786 (7)0.7810 (4)0.7767 (9)0.7831 (1)0.7798 (6)0.7824 (3)0.7807 (5)0.7827 (2)ODDS_breastw0.5437 (9)0.6187 (7)0.8939 (3)0.9071 (1)0.8032 (5)0.7986 (6)0.5913 (8)0.8649 (4)0.9045 (2)ODDS_glass0.6195 (1)0.5849 (3)0.5453 (8)0.5897 (2)0.5962 (4)0.5962 (4)0.5654 (7)0.5957 (5)0.5993 (6)ODDS_ionosphere0.8708 (4)0.8497 (8)0.8711 (3)0.8252 (9)0.8422 (7)0.8350 (8)0.8727 (2)0.8686 (5)0.8509 (6)ODDS_letter0.5555 (9)0.5758 (8)0.5918 (7)0.6068 (6)0.6244 (5)0.6155 (6)0.6446 (1)0.6211 (4)0.6102 (8)ODDS_lympho0.9096 (9)0.9959 (3)0.9988 (2)0.9842 (7)0.9929 (5)0.9953 (4)0.9971 (4)1.0000 (1)0.9925 (6)ODDS_mammography0.5287 (9)0.7612 (3)0.7233 (6)0.8362 (2)0.7189 (7)0.7116 (8)0.8640 (1)0.7673 (4)0.8542 (5)ODDS_mnist0.8518 (7)0.8915 (4)0.8662 (6)0.8959 (3)0.9011 (2)0.8580 (5)0.9070 (1)0.9032 (2)0.8994 (4)ODDS_musk0.9940 (9)1.0000 (1)1.0000 (1)1.0000 (1)1.0000 (1)1.0000 (1)1.0000 (1)1.0000 (1)1.0000 (1)ODDS_optdigits0.5104 (5)0.4950 (9)0.5092 (6)0.4806 (9)0.5115 (4)0.5171 (3)0.4973 (8)0.5338 (2)0.5584 (1)ODDS_pendigits0.9263 (8)0.9295 (6)0.9265 (7)0.9305 (5)0.9208 (9)0.9386 (2)0.9360 (3)0.9346 (4)0.9435 (1)ODDS_satellite0.7681 (1)0.7284 (9)0.7445 (4)0.7352 (8)0.7433 (5)0.7324 (7)0.7486 (3)0.7571 (2)0.7432 (6)ODDS_satimage-20.9707 (7)0.9826 (3)0.9865 (1)0.9744 (6)0.9838 (2)0.9798 (5)0.9871 (1)0.9786 (8)0.9853 (4)ODDS_speech0.4761 (4)0.4756 (5)0.4692 (7)0.4726 (6)0.4832 (1)0.4692 (7)0.4706 (8)0.4774 (3)0.4707 (2)ODDS_thyroid0.9835 (1)0.9661 (2)0.9652 (3)0.9535 (5)0.9635 (4)0.9652 (3)0.9740 (2)0.9689 (6)0.9667 (7)ODDS_vertebral0.6019 (1)0.5378 (2)0.5629 (3)0.5253 (4)0.4602 (6)0.5629 (3)0.4657 (5)0.5629 (3)0.4757 (7)ODDS_vowels0.4897 (8)0.5903 (7)0.5965 (6)0.6309 (5)0.6414 (4)0.6686 (1)0.6216 (3)0.5247 (9)0.6686 (1)ODDS_wbc0.4146 (9)0.8401 (5)0.7640 (7)0.8808 (4)0.8745 (6)0.8469 (8)0.8770 (3)0.8469 (8)0.9289 (1)ODDS_wine0.7864 (2)0.5430 (7)0.4084 (8)0.7539 (3)0.5387 (6)0.4084 (8)0.6296 (4)0.6218 (5)0.8287 (1)"
}