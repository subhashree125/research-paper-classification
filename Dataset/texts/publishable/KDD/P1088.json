{
  "ABSTRACT": "Pretrained Large Language Models (LLMs) have gained significantattention for addressing open-domain Question Answering (QA).While they exhibit high accuracy in answering questions related tocommon knowledge, LLMs encounter difficulties in learning aboutuncommon long-tail knowledge (tail entities). Since manually con-structing QA datasets demands substantial human resources, thetypes of existing QA datasets are limited, leaving us with a scarcityof datasets to study the performance of LLMs on tail entities. In thispaper, we propose an automatic approach to generate specializedQA datasets for tail entities and present the associated research chal-lenges. We conduct extensive experiments by employing pretrainedLLMs on our newly generated long-tail QA datasets, comparingtheir performance with and without external resources includingWikipedia and Wikidata knowledge graphs.",
  "These authors contributed equally. Corresponding author: <>Accepted to Second Workshop on Knowledge Augmented Methods for Natural Lan-guage Processing, in conjunction with KDD 2023": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from -KDD23, August 07, 2023, Long Beach, CA 2023 Association for Computing Machinery.ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00 and domain-specific information. These models have achieved re-markable success in QA tasks, eliminating the need for externaldocuments during inference by implicitly storing knowledge intheir parameters .However, the impressive achievements of LLMs in QA tasks areprimarily observed with regard to common concepts that frequentlyappear on the internet (referred to as \"head entities\"), which arethus more likely to be learned effectively by LLMs during pre-training time. Conversely, when it comes to dealing with long-tailknowledge, which encompasses rarely occurring entities (referredto as \"tail entities\"), LLMs struggle to provide accurate answersand often exhibit hallucination issues . Due to the predominantfocus of most QA datasets on head entities , researchinvestigating the performance of LLMs on long-tail knowledgehas been limited. Recently, Kandpal et al. conducted a studyto bridge this gap by constructing dedicated QA datasets for tailentities. Their approach involved leveraging the entity frequency inWikipedia to define tail entities and quantitatively demonstratingthe limitations of LLMs in handling such entities.Wikipedia documents and Wikidata knowledge graphs are the primary external resources from which QA models acquireknowledge. Consequently, the distribution of tail entities is largelydetermined by the knowledge distributions within Wikipedia andWikidata. In this study, we propose a novel approach to definingtail entities based on their degree information in Wikidata, as op-posed to relying on Wikipedia. By doing so, we generate QAdatasets with distinct distributions from previous works , thusfostering diversity within tail-knowledge QA datasets. Within thecontext of Wikidata, the degrees of entities reflect their level of en-gagement with general knowledge. Hence, we leverage this degreeinformation to define tail entities.The construction of QA datasets typically requires significanthuman resources, hindering the creation of diverse datasets fromvarious domains that are essential for testing the robustness ofcurrent QA models. In this study, our main emphasis lies on the au-tomatic generation of long-tail QA datasets. However, we encounterseveral challenges in this process, such as filtering noisy questions,question granularity, difficulty control, and prompt engineering.These challenges necessitate further research to identify fundamen-tal solutions. We present these challenges through insightful casestudies, aiming to stimulate additional research in this area andfoster the development of QA models.Lastly, we assess the performance of pretrained LLMs, specifi-cally GPT3, on our tail entity datasets. Our findings reveal distinctpatterns compared to prior work , which defines tail entitiesbased on Wikipedia rather than Wikidata. This underscores theimportance of utilizing diverse QA sets to accurately gauge therobustness of QA models. Moreover, we investigate strategies to",
  "KnowledgeNLP-KDD23, August 07, 2023, Long Beach, CAKumar, Kim, and Ravi, et al": "enhance the performance of pretrained LLMs by incorporating ex-ternal resources, such as external documents or knowledge graphs,during inference time on our automatically-generated long-tail QAdatasets. We link these experimental results and the challengesencountered during the automatic QA dataset generation process.In summary, our contributions encompass: Introduction of novel tail knowledge QA datasets derived fromthe Wikidata knowledge graph.",
  "RELATED WORK": "Fact Learning by LLMs: LLMs have shown state-of-the-art performance across various NLP tasks. LLMs have been shownto memorize facts successfully by learning high-frequency patternsin the training data . Kandpal et al. show that an LLMsability to answer a question is affected by how many times it hasseen relevant documents related to the question in its pre-trainingdata. They show that LLMs struggle to reason accurately over rarerentities in the pre-training data (ROOTS , C4 , Wikipedia, OpenWebText ). In this work, instead of using the pre-training corpus, we define tail entities using Wikidata knowledgegraphs and construct a long-tail knowledge dataset that can be usedto study the open-domain QA performance of LLMs.Open-domain Question Answering: ODQA is widely used tomeasure the fact-learning performance of LLMs. However, mostof them are composed of common knowledge (i.e., head-entityquestions), which prevents deep investigations into LLMs ability tolearn facts about uncommon concepts. For instance, TriviaQA isgenerated from trivia websites, where questions are generally aboutpopular entities or facts. Similarly, NaturalQA is constructedmanually using queries issued to the Google search engine. Onereason that it is hard to find tail-entity datasets is, most of the time,QA datasets are hand-crafted, requiring a large number of humanresources; thus, the types of QA datasets are limited to certain types(e.g., head entities). Here, we focus on how to generate tail-entitydatasets while minimizing human resources and analyze why theautomatic QA dataset construction is nontrivial.",
  "In this section, we describe open research challenges we faced whileconstructing QA datasets automatically from knowledge graphs": "3.2.1Degree bounds for tail entities. There are no strictly-formulateddefinitions for tail entities that are widely accepted. Degree boundsthat instantly bring in differences in model performance are alsohard to be decided in advance. As a result, degree bounds for tailentities should be selected arbitrarily. In our experiments, we clas-sify entities with node degrees between 15 and 100 as coarse-tailentities and entities with node degrees below 3 as fine-tail entitiesand compare the LLM performance on them. shows thedegree distribution of entities in Wikidata.",
  "Filtering out noisy triplets": "Ambiguous entities: Multiple entities can have the same sur-face forms. For instance, Jesus can refer to either 1999 Biblicaltelefilm directed by Roger Young, 1979 film by Peter Sykes, or cen-tral figure of Christianity in Wikidata. While these entities can bedistinguished by their unique IDs in knowledge graphs, questionsgenerated from such entities are ambiguous to answer (e.g., Whowas cast in Jesus?). This introduces complications in evaluating thecorrectness of a models answers. Similarly, answer entities canhave several correct surface forms. World War II can be written asWW2 , WWII. In our experiment, we use correct answers along withtheir aliases from Wikidata for evaluation. However, aliases pro-vided by Wikidata do not cover all possible surface forms, leadingto high false negatives. Ambiguous properties: In Wikidata, a large number of prop-erties cannot be used to generate sensible questions. For instance,subclass of, instance of, or part of would generate questions thatare too vague to answer even for humans. Another example is fam-ily name, which will generate questions that already contain theanswer in them (e.g., What is the family name of Barack Obama?).Wikidata also has properties that merely link entities to images orURLs, such as logo image, official website, and official blog URL. Ques-tions generated with these properties are not helpful in evaluatingQA model performance, so they need to be filtered out.As there is no straightforward metric to quantify the appropri-ateness of each property for question generation, property filteringis difficult to be automated. Property filtering requires human judg-ment, which can be problematic because it can be subjective aswell as difficult to scale. In our experiment, we filter out proper-ties by manually going through all the properties that are initiallyextracted. 3.2.3Difficulty control. Questions generated from different proper-ties can have different levels of difficulty. For example, the propertydriving side only has two possible choices, right and left, for theobject (answer) entity. In contrast, the property child has approxi-mately 800 possible choices for the object entity in Wikidata. Thedifficulty of selecting the correct answer for these two propertiescan therefore be very different.Our goal is to generate long-tail QA sets based on the degreeof subject (question) entities in knowledge graphs. In other words,",
  ": Density of properties per the number of possible s2 objectentities before (Top) and after (Bottom) the difficulty controlling": "we want the difficulty of questions solely affected by the degree ofquestion entities, not by properties. In , we match coarse-taildataset and fine-tail dataset to contain the same number of tripletsfor each property, normalizing the difficulty of QA sets in terms ofproperties. 3.2.4LLM prompt for question generation. While the answer entityof a triplet is not part of the generated question, we find that thequality of generated questions improves when the complete tripletis provided in the prompt, instead of the first two elements (i.e.,subject entity and property). For instance, given a triplet [david peelyates, conflict, world war ii], we get \"What conflict was David PeelYates involved in?\" from GPT3 when using just the subject entityand property in prompt. On the contrary, when we use all subject,property, and object entities, the generated question becomes \"Whatconflict did David Peel Yates serve in?\". By including the answerentity world war ii in the prompt, GPT3 understands conflict isabout a war, not about people, and generates a question with amore proper verb serve in. This shows that prompt engineering forgenerative LLMs is crucial for the quality of generated QA datasets. 3.2.5Granularity of questions. Given a question, there could beseveral correct answers with different granularity. Unless the ques-tion specifies the granularity of the answer (e.g., which country orwhich city), QA datasets and models could easily pick different gran-ularity of answers. For instance, when asked Where was Lovelyzformed?, a model could answer South Korea while the QA datasethas Seoul (the capital of South Korea) as the correct answer andmarks the predicted answer wrong. To specify the granularity ofthe answer in the question, generative LLMs should already knowabout the question/answer entity, which becomes problematic if",
  "EVALUATION WITH LLMS AND EXTERNALRESOURCES4.1Experiment Setup": "GPT3: We use the text-davinci-003 version of GPT3 for all our ex-periments (e.g., question generation, answering questions). Themodel is accessible via the OpenAI API2. Specifically, we make useof the Completions API to prompt the model.Wikipedia: We use Wikipedia articles to retrieve relevant para-graphs on-the-fly to augment the GPT3 prompt with additionalcontext. To find the relevant paragraphs using Dense Passage Re-triever (DPR) , we use the same Wikipedia dump as in the originalpaper .Wikidata: Wikidata knowledge graph consists of 103, 305, 143 en-tities and 11, 007 properties. We access Wikidata using the Slingtool in a triplet format (subject, property, object).Tail-entity datasets: We sample triplets from Wikidata to createCoarse-tail and Fine-tail datasets. Each dataset has 27, 691 tripletsand 422 unique properties after the difficulty control (details in Sec-tion 3.2.3). One question&answer pair consists of a GPT3-generatedquestion, an answer (i.e., object entity in the original triplet), andassociated aliases for the answer.",
  "Answer the given question:where was obama born? => hawaiiwhat color is the sky? => bluewhere was lovelyz formed? =>": "shows GPT3 performance on existing QA datasets (Trivi-aQA , WebQA , and NaturalQA ) and our newly-generatedCoarse- and Fine-tail QA datasets. GPT3 shows consistently lowerperformance on our tail datasets than the existing QA datasets,while performing better on Coarse-tail set than Fine-tail set. Thisresults coincide with , showing again that LLMs struggle to learnlong-tail knowledge.We perform manual error analysis on our tail QA dataset. Werandomly sample 100 questions that got wrong from Fine-tail QAset and categorize their error cases into 6 cases. As shown in ,45% of errors are from GPT3s completely wrong predictions. 19%of errors are due to different granularity of answers, and 12% oferrors are due to questions that are incorrectly generated by LLMs.As we describe in .2, this result shows the limitations ofauto-generated QA datasets and underscores the imperative forfurther research in this domain.",
  "LLM prompting with Dense PassageRetrieval": "One common way to augment LLMs for long-tail knowledge isretrieving relevant passages from external documents and refer-ring them during inference time . In this section, we checkwhether GPT3 can see the same improvement in our datasets. Weuse Dense Passage Retriever (DPR) that has trained on Natu-ral Questions to retrieve the top 100 relevant passages fromWikipedia.We first evaluate how successfully DPR retrieves a passage thatcontains the correct answer. In , we observe that DPR per-forms consistently worse on our tail datasets than the existingdatasets. This shows that DPR, which has pretrained on Wikipediawith head-entity QA datasets, also struggles to retrieve long-tailknowledge.Next, we use the top-ranked passages retrieved by DPR to aug-ment GPT3. We pass the top-1 retrieved passages to GPT3 as addi-tional context along with the question as follows: Question: Where is Nelson's Pillar located?Document: Nelson's Pillar was a large granitecolumn capped by a statue of Horatio Nelson,built in the centre of what was then SackvilleStreet in Dublin, Ireland.Answer: Dublin, Ireland In , we observe a decrease in GPT3 accuracy compared toits original accuracy when prompted with DPR retrieved passages.The accuracy of 26.5% for Coarse-tail and 22.1% for Fine-tail QAsets plummet to 14.3% and 18.2%, respectively. This decline in accu-racy can be attributed to the fact that DPRs retrieval often leadsto irrelevant passages on long-tail knowledge, as shown in .Consequently, the presence of these additional contexts confusesGPT3 and adversely affects its performance. These findings high-light the crucial relationship between the performance of LLMsand the retrieval models, indicating that the performance of LLMsis inherently limited by the effectiveness of the retrieval models.Therefore, it is essential for retrieval models to also consider andaddress the challenges associated with long-tail knowledge.",
  "LLM prompting with DPR and knowledgegraphs": "Knowledge graphs (KG) have been widely used to augment LLMs . In this section, we examine how external knowledge graphscan cooperate with another external resource, Wikipedia to im-prove LLM performance for tail entities. We use Wikidata as ourexternal knowledge graph after removing all triplets used for theQA generation. To avoid additional finetuning, we implement azero-shot LLM+DPR+KG baseline: we first sample triplets relevantto the question from the knowledge graph then use the sampledtriplets to rerank the DPR-retrieved passages; then we pass thetop-1 retrieved passages to GPT3 as additional context along withthe question. To sample relevant triplets from knowledge graphs,we first find a path from the subject entity to the object entity andconcatenate the surface forms of all entities on the path. We then",
  "Original26.5%22.1%w/ DPR14.3%18.2%": "compute its textual similarity with the passages retrieved by DPRusing SBERT . We use this similarity score to re-rank the DPRresults and observe the changes in the Top- retrieval accuracy.As shown in , DPR retrieval accuracy is improved by upto 6% with the help of knowledge graphs. The improvement inDPR retrieval accuracy leads to the improvement of GPT3s QAperformance. shows GPT3 also has improved from 22.1%(no external resources) to 30.95% (with DPR and knowledge graphs)on our Fine-tail QA datasets. This highlights that joint learning oftwo external resources could be the key to solving the long-tailknowledge problems. : Top- DPR retrieval accuracy and GPT3 performance onFine-tail QA before/after the retrieved passage re-ranking usingknowledge graphs. The third and final columns (re-rank) show howTop- DPR retrieval accuracy and GPT3 performance have changedafter the re-ranking.",
  "CONCLUSION": "Our work highlights the limitations of pre-trained LLMs in han-dling long-tail knowledge in open-domain Question Answering. Toinvestigate this limitation, we first propose to generate QA datasetsspecialized for tail entities automatically using degree informationfrom the Wikidata knowledge graph. Our automatic QA generationapproach aims to overcome the resource-intensive nature of manualdataset construction, allowing for the creation of diverse long-tailQA datasets. In the process of automatic QA dataset generation,we identify and discuss several open research challenges, such asdegree bounds, question granularity, difficulty control, and promptengineering, which require further investigation for fundamentalsolutions. We evaluate the performance of GPT3 on our generatedlong-tail QA datasets. Additionally, we explore the utilization of ex-ternal resources, such as external documents or knowledge graphs,to improve the performance of LLMs on long-tail knowledge. Wehope this work paves the way for further research in the automaticQA dataset generation and the long-tail knowledge problem inopen-domain QA tasks. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Seman-tic parsing on freebase from question-answer pairs. In Proceedings of the 2013conference on empirical methods in natural language processing. 15331544. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shot learners. Advances in neuralinformation processing systems 33 (2020), 18771901. Matthew Dunn, Levent Sagun, Mike Higgins, V Ugur Guney, Volkan Cirik, andKyunghyun Cho. 2017. Searchqa: A new q&a dataset augmented with contextfrom a search engine. arXiv preprint arXiv:1704.05179 (2017). Aaron Gokaslan and Vanya Cohen. 2019. Openwebtext corpus. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination innatural language generation. Comput. Surveys 55, 12 (2023), 138.",
  "Nora Kassner, Benno Krojer, and Hinrich Schtze. 2020. Are pretrained languagemodels symbolic reasoners over knowledge? arXiv preprint arXiv:2006.10413(2020)": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, AnkurParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, KentonLee, et al. 2019. Natural questions: a benchmark for question answering research.Transactions of the Association for Computational Linguistics 7 (2019), 453466. Hugo Laurenon, Lucile Saulnier, Thomas Wang, Christopher Akiki, AlbertVillanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, EduardoGonzlez Ponferrada, Huu Nguyen, et al. 2022. The bigscience roots corpus: A1.6 tb composite multilingual dataset. Advances in Neural Information ProcessingSystems 35 (2022), 3180931826.",
  "Denny Vrandei and Markus Krtzsch. 2014. Wikidata: a free collaborativeknowledgebase. Commun. ACM 57, 10 (2014), 7885": "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, ShuohuiChen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt:Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068(2022). Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang,Christopher D Manning, and Jure Leskovec. 2022. Greaselm: Graph reasoning en-hanced language models for question answering. arXiv preprint arXiv:2201.08860(2022)."
}