{
  "ABSTRACT": "Federated learning enables learning from decentralized data sourceswithout compromising privacy, which makes it a crucial technique.However, it is vulnerable to model poisoning attacks, where mali-cious clients interfere with the training process. Previous defensemechanisms have focused on the server-side by using careful modelaggregation, but this may not be effective when the data is not iden-tically distributed or when attackers can access the information ofbenign clients. In this paper, we propose a new defense mechanismthat focuses on the client-side, called FedDefender, to help benignclients train robust local models and avoid the adverse impact ofmalicious model updates from attackers, even when a server-sidedefense cannot identify or remove adversaries. Our method consistsof two main components: (1) attack-tolerant local meta update and(2) attack-tolerant global knowledge distillation. These componentsare used to find noise-resilient model parameters while accuratelyextracting knowledge from a potentially corrupted global model.Our client-side defense strategy has a flexible structure and canwork in conjunction with any existing server-side strategies. Eval-uations of real-world scenarios across multiple datasets show thatthe proposed method enhances the robustness of federated learningagainst model poisoning attacks.",
  "Equal contribution to this work": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 23, August 610, 2023, Long Beach, CA, USA. 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00 ACM Reference Format:Sungwon Park, Sungwon Han, Fangzhao Wu, Sundong Kim, Bin Zhu, XingXie, and Meeyoung Cha. 2023. FedDefender: Client-Side Attack-TolerantFederated Learning. In Proceedings of the 29th ACM SIGKDD Conference onKnowledge Discovery and Data Mining (KDD 23), August 610, 2023, LongBeach, CA, USA. ACM, New York, NY, USA, 12 pages.",
  "INTRODUCTION": "Federated learning has become a popular model training method toguarantee the minimum level of data privacy . In each traininground of federated learning, clients optimize their local models andsend updates to the central server to aggregate them to produce aglobal model of the entire data distribution. Thus federated learn-ing enables clients to jointly train a global model without directlysharing their private training data, making it a privacy-friendly so-lution . Federated learning has been rapidly adopted by variousapplications that require data privacy .Despite its advantages, federated learning is vulnerable to attacksdue to its decentralized nature. Any client can easily participatein the training process, introducing the possibility of malicious-ness . For example, federated learning systems assume that allparticipants are benign and that their data can help improve the per-formance of resulting models. This leaves room for model poisoningattacks, wherein malicious users deceive the system by pretendingto be benign and sending poisoned updates to the central server.This type of attack can disrupt parameter optimization andadversely impact the models performance , undermining theintegrity of the federated learning system.Existing studies focus on using robust aggregation on the serverside as a defense against model poisoning attacks. A central servercan be trained to preserve updates from likely benign clients whilediscarding updates from likely corrupted clients or adversaries.Statistics like the trimmed mean or median can be used for outlier-resistant aggregation instead of simply averaging all updates. Fu etal., extended this method by introducing a concept of confidencecomputed from residuals of repeated median estimator . Otherdetection algorithms like Norm Bound, Multi-Krum, and FoolsGoldassign lower weights to outlier updates and reduce their adverseeffect . However, these methods are limited in their abilityto detect adversaries in the real world, where local datasets areno longer independent and identically distributed (i.e., non-IID).Non-IID makes benign local updates diverse and indistinguishablefrom corrupted ones . Furthermore, extant defenses have been",
  "KDD 23, August 610, 2023, Long Beach, CA, USA.Sungwon Park et al": "weights are excluded from global knowledge distillation, in learn-ing calibrated global knowledge. Motivated by the notion of self-knowledge distillation , we extract knowledge from thelocal model using an auxiliary classifier and transfer it backto the original local model . By doing so, we can distill the cal-ibrated global knowledge learned from the refined label (Eq. 12)to the latter part of the model, effectively regularizing the entiremodel. Moreover, the output from the auxiliary head can be consid-ered as a different view (i.e., augmentation) of the same instance.Maximizing the agreement between these two views can furtherenhance the model training. The self-knowledge distillation lossbetween the auxiliary classifier and the original model isdefined as follows.",
  "Designing an attack-tolerant local meta update that helps dis-cover noise-tolerant parameters for local models by utilizing asynthetically corrupted training set": "Introducing an attack-tolerant global knowledge distillation tech-nique that efficiently aligns the local models knowledge to theglobal data distribution while reducing the adverse effects offalse information in the possibly-corrupted global model. Showcasing that FedDefender can easily be applied in combina-tion with any server-side defense strategies to enhance accuracyby 17-20% under poisoning attacks across various datasets.The code and implementation details are available at the followingURL:",
  "RELATED WORK2.1Poisoning Attacks in Federated Learning": "In recent years, there is a growing concern about security issues inmachine learning, leading to the emergence of various model poi-soning attacks . Federated learning is particularly susceptibleto these attacks because malicious users can easily access interme-diate processes of the training and send poisoning updates to thecentral server . Attackers can attack the training withoutaccess to the entire data distribution, as the data is decentralizedand cannot be shared among clients .Model poisoning attacks can be divided into two categories: tar-geted attacks and untargeted attacks . In targeted attacks,malicious clients inject a backdoor into the central server so thatany instance with the backdoor trigger will be classified as tar-geted without degrading the overall performance of the model.In untargeted attacks, on the other hand, attackers aim to degrademodel performance indiscriminately across all classes. A simple andwidely used untargeted attack is the label-flipping attack , inwhich malicious clients corrupt local models by randomly flippinglabels of training samples from original classes to other classes.Recently, some studies have used benign clients partial informa-tion to improve the stealthiness of untargeted attack performance.For example, Baruch et al. infer the standard deviation and the in-tensity factor based on benign clients updates and inject perturbedmodel updates accordingly . Fang et al. inject malicious updatesby adding constant opposite direction noise estimated from benignclients updates mean . Shejwalkar et al. propose a model poi-soning attack by calculating a dynamic malicious update oppositeto the direction of benign model updates .This paper focuses on defending against untargeted attacks infederated learning using the client-side defense.",
  "Server-Side Defenses Against PoisoningAttacks in Federated Learning": "In federated learning, dimension-wise averaging is a commonlyused and effective method for aggregating local updates .However, this naive averaging method is vulnerable to model poi-soning attacks, which may succeed even with just a single maliciousmodel update . To address this threat, two server-side defensestrategies have been proposed: coordinate-wise aggregation anddetection-wise aggregation. Both aim to filter out malicious up-dates from adversary clients during the aggregation and ensure theintegrity of the federated learning process.Coordinate-wise Aggregation. Coordinate-wise aggregation in-troduces outlier-resistant operations instead of averaging. For ex-ample, Trimmed Mean eliminates the largest and smallest values in",
  "FedDefender: Client-Side Attack-Tolerant Federated LearningKDD 23, August 610, 2023, Long Beach, CA, USA": ": Illustration of federated learning under poisoning attacks. Attackers attempt to send malicious updates to the centralserver to corrupt the aggregated model. Unlike existing works that primarily focus on robust aggregation on the server side(stage 4), FedDefender focuses on training robust local models by benign clients (stage 2) to protect against malicious updatesfrom adversaries. each dimension, before computing the mean . Median is anotherdimension-wise aggregation algorithm that computes the medianof the updates in place of averaging . However, both approachesare ineffective when the data distribution is non-IID as they mayoverlook underrepresented updates. To tackle this limitation, Resid-ualBase proposes a residual-based aggregation method , whichcalculates residuals of each parameter in the local model using arepeated median estimator. These residuals are then used to deter-mine parameter confidence and detect false updates.Detection-wise Aggregation. To mitigate the adverse effect ofmalicious clients updates, detection-wise aggregation adjusts learn-ing rates based on the abnormality score of each local update. NormBound disregards clients with the norm of local updates being abovea certain threshold by exploiting the observation that maliciousclients often produce updates with a larger variance and normthan benign clients . Krum introduces a Byzantine-resilientalgorithm assuming that malicious updates would be placed farfrom benign updates in the Euclidean space . More specifically,it selects a single local client update that is the most similar to its 2 neighboring updates to produce the global model, where is the expected number of malicious local clients. Krum can beextended to Multi-Krum by iteratively running the algorithm toselect multiple local updates, which shows better robustness thanthe original Krum. FoolsGold identifies grouped actions of targetedattacks based on the similarity score among local updates . Un-like benign clients, malicious clients in a targeted attack scenarioshare the common loss objective and tend to have a more similarupdate pattern than benign clients. In this context, FoolsGold ad-justs the learning rates of local models in proportion to the degreeof diversity in each local update.While the client-side defense has been relatively under-investigated,FL-WBC proposed a client-side defense strategy specifically de-signed to mitigate backdoor attacks . In contrast, our proposedmethod, FedDefender, stands as a pioneering client-side defensestrategy against untargeted attacks in federated learning. By com-plementing existing server-side defense strategies, it significantly",
  "Step.2 Attack-Tolerant Global Knowledge Distillation (Sec. 4.2)": ": The overall design of FedDefender framework. FedDefender comprises two steps: (1) attack-tolerant local meta-update,which finds local model parameters that are less prone to overfitting to noise, and (2) attack-tolerant global knowledgedistillation, which aims to convey correct global knowledge from a potentially contaminated global model, regularizing thelocal model to mitigate data bias. where |D| = =1 |D |. This process is repeated for each commu-nication round.Threat model. We hypothesize that all benign clients are honestand follow the proposed protocol, whereas malicious clients donot. Adversaries aim to disrupt the convergence and deterioratethe performance of the global model (i.e., an untargeted attack) bypoisoning the local models of malicious clients. We also assumethat the central server is trustworthy and performs the requiredtask faithfully. Regarding the attackers capability, we assume thatadversaries can bypass the verification process and infiltrate thefederated system as participating clients (e.g., during optimization).We consider two scenarios for the threat model: (1) attackers haveno access to benign clients information, and (2) attackers havepartial access to benign client information, such as the standarddeviation or average updates from benign clients.",
  "Federated Learning When Facing PoisoningAttacks": "Poisoning attacks rely on sending corrupted local updates to thecentral server, ultimately contaminating the trained global model.Several malicious objectives can be used for such attacks. For in-stance, attackers may adopt the same loss objective as legitimateclients for optimization, but use deliberately corrupted training setD to convey false information (i.e., L (, D)). Additionally, at-tackers can also introduce an adversarial objective L that disruptsthe overall convergence (i.e., L (, D)).We denote the subset of malicious clients as S and the subset ofbenign clients as S from the entire set of clients S (i.e., S = S S). To defend against poisoning attacks, the optimal objectivefor training the global model weight can be defined as:",
  "||,(Eq. 4)": "where 1 is an indicator function. |D | is removed in order to mit-igate the attack scenario where attackers try to manipulate the sizeof the local training data.In contrast to existing defenses that focus on robust aggregationstrategies, FedDefender aims to directly solve Eq. 3 by redesigning thelocal model train process (stage 2) and modifying the local updatesof legitimate clients . The detail is described in the next section.",
  "FedDefender comprises the following two steps,": "Step 1. Attack-Tolerant Local Meta Update (.1): Wetrain the benign local model in a robust manner via meta-learning. The goal is to discover the models parameters thatproduce accurate predictions even after it has been perturbed bynoisy information. To achieve this, we first generate a noisy syn-thetic label, then apply one gradient update to perturb the localnetwork parameters. Afterward, the gradient for the perturbednetwork to predict the correct outputs is computed and utilizedto optimize the original local model. Step 2. Attack-Tolerant Global Knowledge Distillation (Sec-tion 4.2): If the global aggregation defense cannot block updatesfrom malicious clients, the global model is no longer trustwor-thy. Given the meta-updated local model from the previous step,we apply global knowledge distillation to an auxiliary classifierusing intermediate feature maps to neutralize the adverse impacton the contaminated global model . Self-knowledge distilla-tion is applied between the auxiliary classifier and the originalclassifier to further incorporate the global knowledge into thedeeper layers of the local model. present the overall pipeline of our proposed defense. Ourtechnique can mitigate model poisoning attacks in federated learn-ing. We will now provide a detailed description of each componentof FedDefender next.",
  "Attack-Tolerant Local Meta Update": "If the local model parameters are only optimized with a tradi-tional supervised loss term (e.g., cross-entropy), it is susceptibleto overfitting and being influenced by noise generated by mali-cious clients. Our key idea is to learn noise-tolerant parameters in a way that \"vaccinates\" the local model against modelpoisoning with synthetic noise, drawing inspiration from recentmeta-learning works . The proposed local meta-updatereplicates the training context with a model poisoning attack andmakes the network less sensitive to noisy perturbations.Local model poisoning with synthetic noise. Let us denote amini-batch from local dataset D as X = {(x, y)}=1, where xis an input instance and y is the corresponding one-hot label. Wewant to generate synthetic batches X = {(x, y)}=1 with labelnoise to simulate the poisoning attack and perturb the local model.Excessive perturbation can overly deform the models decisionboundary, leading to degraded performance. To mitigate this riskof severe deformation, we create more realistic noisy labels thatresemble the distribution of y by transferring labels from similarsamples. For each (x, y) X, we calculate its feature represen-tation x from the models backbone network. Then, a randominstance from top- nearest neighbors in the representation spaceis randomly selected to replace the original label:",
  "X = {(x, y)|(x, y) X and y = Sampley(N (x,))},(Eq. 5)": "where N (x,) indicates the set of -nearest neighbors of x fromthe representation space made by . Sampley() is the randomselector function to extract the synthetic label. Since the nearestneighbors N (x,) are likely to share the same label as x, this cangenerate noise within an acceptable range (i.e., 5-20% error rate).Given the synthetic batch samples X, we perturb the local modelparameter using one gradient descent step:",
  "L,(Eq. 7)": "where (,) is the conventional cross entropy between and ,and is a learning rate.Local model correction with meta update. To discover themodel parameter that is less susceptible to noisy training by mali-cious clients, we propose a meta update to guide the model training.Given the perturbed model , we optimize a classification lossL (Eq. 8) for one gradient descent step to encourage the per-turbed local model to give correct predictions from X. Notethat the optimization process is applied to the parameters of theoriginal local model (Eq. 9), although the loss calculation is basedon the perturbed models parameters (Eq. 8). This prevents thelocal model from being contaminated by synthetic noise during thetraining.",
  "Attack-Tolerant Global KnowledgeDistillation": "Given the meta-updated network , our next step is to enhancethe performance of the refined local model through global knowl-edge distillation. In the case of non-IID settings, learning relyingon local data leads to representation bias, as local data distributiondiffers from the global distribution. Regularization techniques, in-cluding global knowledge distillation, can be employed to controlthe local updates, thus preventing the local drift fallacy .In the scenario of a model poisoning attack, however, the cred-ibility of the global model can be compromised if the globaldefense fails to block malicious updates from hostile clients. Thiscan lead to suboptimal results when the is perturbed. We pro-pose attack-tolerant global knowledge distillation to mitigate theadverse effects of a potentially corrupted global model .Refined knowledge distillation with an auxiliary network.We start with the fact that a deeper layer in deep neural networksis much easier to overfit to noise (i.e., memorization) due to theinherent nature of gradient descent-based optimization .In this context, FedDefender transfers global knowledge to a shallowintermediate part of the local model to reduce the adverse effect offalse information. We attach an auxiliary classifier on top of inter-mediate layers to produce a new model parameter , and performknowledge distillation on it (i.e., and share the shallow sectionof the entire network). Given an input data point x from mini-batchX, we regard the output probability from the global model as atarget to train the local model parameter with auxiliary classifier. The global knowledge distillation loss is defined as follows:",
  "x,yX ( (x,), (x)),(Eq. 11)": "where Sharpen() is the function that adjusts the confidence withsharpening temperature .Since the global models prediction is still not reliable in theattack scenario, we further introduce a simple strategy to refinetarget probabilities for robust knowledge distillation. Given a datapoint (x, y) from X, we calculate the scale coefficient , whichrepresents the cosine similarity between original label y and theglobal model prediction (x). FedDefender then generates a refinedlabel y by applying a linear sum to calibrate the global informationas follows:",
  "y = (1 ) y + (x,).(Eq. 12)": "If the global models prediction is well aligned with the ground-truth labels, the scale coefficient becomes large and the final labelweighs more on the global models knowledge. Meanwhile, if theprediction is far different from the truth, we neglect the globalmodels information. We construct a modified batch X with therefined labels y, and transfer the global knowledge to with thisbatch (Eq. 13).",
  "Experimental Setup": "Data settings. We use four image classification benchmark datasetsin our experiments. The first two are CIFAR-10 and CIFAR-100, eachcontaining 60,000 images of 32x32 pixels . CIFAR-10 comprises10 classes, such as airplanes, cats, and dogs, while CIFAR-100 com-prises 100 classes. The third dataset is TinyImageNet, which com-prises 100,000 images of 64x64 pixels and 200 classes. The fourth isthe Federated Extended MNIST (FEMNIST) dataset, which contains805,263 images of handwriting digits/characters of the size of 28x28pixels .The Dirichlet distribution is used to simulate the non-IID charac-teristics of CIFAR-10, CIFAR-100, and TinyImageNet datasets on fed-erated learning. We denote the Dirichlet distribution by (, ),where is the total number of local clients and represents theconcentration parameter controlling the degree of non-IIDness ofdecentralized local data distributions. The probability , is sam-pled from (, ) and assigns the proportion of class in -thclients dataset. With this non-IID distribution strategy with a low value, local clients will have a disparate class distribution fromone another. The default values for and are set to 20 and 0.5, : Performance improvement with FedDefender on clas-sification accuracy in Scenario-1 over four datasets. Ourmodel brings non-trivial improvement for all server-sidebaseline algorithms for both last and best accuracy.",
  "ResidualBase73.61 75.10 44.80 45.13 35.0538.6019.44 23.86+ FedDefender79.28 80.83 50.62 50.98 36.2239.2422.41 24.27": "respectively. In the case of FEMNIST, on the other hand, the writ-ers of digits/characters in the data are randomly distributed to clients. is set to 20 for FEMNIST as well.Implementation details. The number of communication roundsis set to 100, with 1 epoch per round for all federated learning ex-periments. Half of the clients (i.e., 10 = /2 clients) are randomlyselected in each round for communication to make the federatedsetting more realistic. ResNet18 is used as the default backbonenetwork following the literature in federated learning . Thelearning rate (), weight decay, and momentum for the SGD opti-mizer are set to 0.01, 1e-5, and 0.9, respectively. The batch size isset to 64. For knowledge distillation objectives, the temperature is set to 2. Random crop, color jitter, and random horizontal flip areused for data augmentations in local model training.Model poisoning attack scenario. We divide a total of clientsinto benign and malicious clients with a ratio of 8:2 (i.e., attacker ra-tio = 20%) as the default setting. For example, given to be 20, we as-sign four clients to play the adversarial role from a pool of 20 clients.Because we randomly select 10 out of 20 clients in every commu-nication round, the number of malicious clients in each round mayvary. We consider two representative scenarios of an untargetedpoisoning attack on the federated learning framework as follows:Scenario-1) No access to benign clients information: Thisscenario assumes that malicious clients cannot obtain any infor-mation about benign clients. We consider the label flipping attackas the model poisoning attack in this case, as it does not requireprior knowledge of the training data or the benign clients updateinformation. For instance, in CIFAR-10, a class dog image mayhave a random label of ship or horse. This false model updateis sent to the central server after training the local network withrandomly flipped noisy labels.Scenario-2) Partial access to benign client information: Thisscenario represents a more advanced attack in which maliciousclients can access local updates from benign clients. Updates from",
  "Defense Performance Evaluation": "Baselines. We have implemented six server-side defense strategiesas baselines for untargeted attacks: (1) No Defense refers to thetypical Federated Averaging (FedAvg) algorithm without any ro-bust aggregation strategy. (2) Median is an aggregation algorithmthat computes the median of each dimension of the updates ratherthan the average. (3) Trimmed Mean is an aggregation algorithmthat computes the mean of local updates by removing the largestand smallest values. (4) Norm Bound is an algorithm that removes updates if the norm of the local update is above a certain threshold.(5) Multi-Krum is an algorithm that iteratively selects local up-dates using the Krum method, which selects a single honest clientby calculating the Euclidean distance between the clients updateand the updates of its neighbors. (6) ResidualBase is an algorithmthat computes parameter confidence using the residuals of eachparameter from the repeated mean.Evaluation. All models are evaluated using the same attacks andexperimental settings (e.g., client pool, ratio of attacker numbers,attack strategy, number of local epochs, optimizer, and communi-cation rounds) to ensure a fair comparison. We report the top-1classification accuracy on the test set, including the last and bestaccuracy. The reported accuracy (labeled Last and Best) is calcu-lated by averaging the accuracy of the last and best five rounds,respectively.Result. compares the performance of defense algorithmsagainst the label flipping attack described in Scenario 1. We cansee that FedDefender consistently improves the performance whenapplied to existing server-side defense algorithms. Despite un-filtered malicious updates from failures in server-side defense orthe absence of a defense strategy (i.e., No Defense), incorporatingFedDefender can lower the risk of performance degradation.",
  ": Performance comparison of ablations across com-munication rounds on CIFAR-10. Removing or altering anycomponent results in a performance drop": "reports the results against advanced attacks in Scenario2, where malicious clients have access to information from benignclients. FedDefender consistently outperforms the baselines acrossall cases. Server-side defenses alone can lead to removing updatesof benign users during training, which can cause the performance todrop. For example, existing server-side defense strategies performpoorly in LIE cases, even compared to No Defense. However, addingFedDefender can alleviate this issue and improve performance, high-lighting the effectiveness of our approach.",
  "Sever-Side Only: A model that uses only the server-sidedefense method without any attack-tolerant local updates": "confirms that the full model provides the best perfor-mance among the ablations. Due to space limitations, we reportresults when our method is used along with Multi-Krum. Abla-tions using Step 1 only shows a more robust performance thanusing the server-side defense strategy alone. Altering Step 2 to aconventional knowledge distillation, on the other hand, degradesperformance compared with entirely removing Step 2. This is likelydue to corrupted information in the global model, , and demon-strates the need for a carefully designed attack-tolerant knowledgedistillation.Comparison with alternative global regularization approaches.The proposed global knowledge distillation (.2) may bereplaced with existing global regularization techniques that dontconsider malicious attacks. We have tested the three alternativesthat replace the global knowledge distillation with existing methodsFedProx , Scaffold , or Moon on top of the local metaupdate module. These existing methods also regularize the local",
  "Full Components78.1779.9681.8782.78Local Meta Update + Scaffold74.8178.5280.3280.57Local Meta Update + FedProx74.6778.0180.8382.01Local Meta Update + Moon73.1375.1278.0578.99": "model and address the local drift fallacy caused by the disagreementbetween local and global data distribution. compares our method with these alternatives over twodifferent global aggregation strategies: simple averaging (i.e., NoDefense) and Multi-Krum. FedDefender outperforms in both thelast and best accuracy across both defense cases. When the globalmodel is heavily perturbed (i.e., No Defense), the existing globalknowledge distillation method experiences a significant drop inaccuracy compared with when the global model is less perturbed(i.e., Multi-Krum). We also find that our proposed global knowledgedistillation leads to smaller accuracy fluctuation across trainingrounds compared with other methods (see in the Appendix).These results demonstrate the critical role that our attack-tolerantglobal knowledge distillation plays.Qualitative analyses. We investigate how well FedDefender discov-ers the attack-tolerant parameters for benign local models. We areinspired by the experiment to visualize the loss landscape in and follow it as follows. We add random direction perturbationsto the model parameter (for example, for two random directions Xand Y in a) and examine how the accuracy changes. Results ina indicate that our local meta update (step 1) produces a higherand smoother accuracy surface compared with when this step ismissing. Resistance to random parameter perturbations suggeststhat, when utilizing FedDefender, each benign client can find asolution with flat minima in the loss curve within the parameterspace.Next, we examine how well the correct global knowledge isconveyed to the local model by FedDefender under poisoning attacks.Based on the experiment, we may exactly compute a hypotheticalglobal model that is immune to attacks (i.e., a clean global model)by removing all adversarial clients in the global aggregation phase.Mathematically, the corrupted and clean global models in round are defined as follows:",
  "(c) Similarity of local & clean global (Eq. 20)": ": Qualitative analyses on FedDefender. (a) The accuracy surface of the global model with or without a local meta updateafter adding small perturbations to the model parameter. X and Y refer to two random directions for the perturbation. (b-c) Thebox plots the similarity between the local and global models (either corrupted or clean) with/without our proposed knowledgedistillation. : Performance comparison over different experimen-tal settings with varying hyper-parameters. The resultsdemonstrate that FedDefender consistently enhances the per-formance when it is applied on top of the existing aggrega-tion strategy.",
  "(c) Effect of the level of non-IIDness": "Figures 4b and 4c show the relationship between local and globalmodels for each class in CIFAR-10. The results show that localmodels in FedDefender have a substantially higher similarity tothe clean global model (0.64) compared with the corrupted globalmodel (0.53). In contrast, a vanilla knowledge distillation does not guarantee the same ability to filter out contaminated knowledgefrom the global model (clean: 0.55 vs corrupted: 0.61). This suggeststhat our method can effectively distill more correct and calibratedknowledge from the corrupted global model.Robustness test. To test robustness, we consider different experi-mental settings and vary the key hyperparameters. These include(a) the number of participating clients , (b) the percentage of at-tackers that infiltrate the system , and (c) the level of non-IIDnessin the distributed local dataset, which is controlled by the beta pa-rameter in Dirichlet distribution. A lower beta parameter leads tohigher non-IIDness. shows the results for using Multi-Krumas a baseline aggregation strategy on CIFAR-10.The complexity of data training increases as we increase thenumber of clients, the percentage of attackers, and the level of non-IIDness. Irrespective of these changes, FedDefender consistentlydemonstrates significant performance improvements.",
  "CONCLUSION": "This paper proposed FedDefender, a client-side approach to improveexisting server-side defense strategies against model poisoning at-tacks. We proposed two attack-tolerant training components forbenign clients: (1) attack-tolerant local meta update and (2) attack-tolerant global knowledge distillation. With these components, ourmethod helps mitigate model poisoning attacks and produces moretrustworthy results, even when server-side defenses fail to filter outmalicious updates. As a result, our method has achieved a mean-ingful robustness improvement against various model poisoningattacks when used in conjunction with existing server-side defensestrategies. We hope that our technique can serve as a foundationfor further research in client-side robust federated learning.",
  "Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer.2017. Machine learning with adversaries: Byzantine tolerant gradient descent. InAdvances in NeurIPS, Vol. 30": "Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, AlexIngerman, Vladimir Ivanov, Chloe Kiddon, Jakub Konen`y, Stefano Mazzocchi,Brendan McMahan, et al. 2019. Towards federated learning at scale: Systemdesign. In Proceedings of MLSys, Vol. 1. 374388. Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konen`y,H Brendan McMahan, Virginia Smith, and Ameet Talwalkar. 2018. Leaf: Abenchmark for federated settings. arXiv preprint arXiv:1812.01097 (2018).",
  "Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. 2017. Badnets: Identifyingvulnerabilities in the machine learning model supply chain. arXiv preprintarXiv:1708.06733 (2017)": "Sungwon Han, Sungwon Park, Sungkyu Park, Sundong Kim, and Meeyoung Cha.2020. Mitigating embedding and class assignment mismatch in unsupervisedimage classification. In Proceedings of ECCV. Springer, 768784. Sungwon Han, Sungwon Park, Fangzhao Wu, Sundong Kim, Chuhan Wu, XingXie, and Meeyoung Cha. 2022. FedX: Unsupervised Federated Learning withCross Knowledge Distillation. In Proceedings of ECCV. 691707.",
  "Qinbin Li, Bingsheng He, and Dawn Song. 2021. Model-contrastive federatedlearning. In Proceedings of CVPR. 1071310722": "Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar,and Virginia Smith. 2020. Federated optimization in heterogeneous networks.Proceedings of Machine Learning and Systems 2 (2020), 429450. Bing Luo, Xiang Li, Shiqiang Wang, Jianwei Huang, and Leandros Tassiulas.2021. Cost-effective federated learning in mobile edge networks. IEEE Journal onSelected Areas in Communications 39, 12 (2021), 36063621.",
  "Lingjuan Lyu, Han Yu, and Qiang Yang. 2020. Threats to federated learning: Asurvey. arXiv preprint arXiv:2003.02133 (2020)": "Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, andBlaise Aguera y Arcas. 2017. Communication-efficient learning of deep net-works from decentralized data. In Proceedings of AISTATS. 12731282. Sungwon Park, Sungwon Han, Sundong Kim, Danu Kim, Sungkyu Park, Se-unghoon Hong, and Meeyoung Cha. 2021. Improving unsupervised image clus-tering with robust learning. In Proceedings of CVPR. 1227812287.",
  "Sachin Ravi and Hugo Larochelle. 2017. Optimization as a model for few-shotlearning. In Proceedings of ICLR": "Nicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletari, Holger R Roth, ShadiAlbarqouni, Spyridon Bakas, Mathieu N Galtier, Bennett A Landman, Klaus Maier-Hein, et al. 2020. The future of digital health with federated learning. NPJ digitalmedicine 3, 1 (2020), 17. Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer,Tudor Dumitras, and Tom Goldstein. 2018. Poison frogs! targeted clean-labelpoisoning attacks on neural networks. In Advances in NeurIPS, Vol. 31.",
  "Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. 2018.Byzantine-robust distributed learning: Towards optimal statistical rates. In Pro-ceedings of ICML. 56505659": "Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, andKaisheng Ma. 2019. Be your own teacher: Improve the performance of convolu-tional neural networks via self distillation. In Proceedings of ICCV. 37133722. Weiming Zhuang, Yonggang Wen, Xuesen Zhang, Xin Gan, Daiying Yin,Dongzhan Zhou, Shuai Zhang, and Shuai Yi. 2020. Performance optimizationof federated person re-identification via benchmark analysis. In Proceedings ofACM MM. 955963.",
  "AAPPENDIXA.1Training Details": "The overall training process is presented in Algorithm 1, and the Fed-Defender is open-sourced at default backbone network for our experiments is ResNet18 ,following prior work in federated learning . For the attack-tolerant local meta update (Step 1), we use a first-order approxima-tion to speed up computation. The number of nearest neighbors,k, used to perturb the label is set to 10. In attack-tolerant globalknowledge distillation (Step 2), we attach an auxiliary classifier tothe output of the second residual block layers feature map.We followed the details from original works for implementingserver-side defense baselines. When deciding hyperparameters ofMulti-Krum and Norm Bounding algorithms, we assumed that thecentral server already knows the upper bound of attacker numbers.The two hyperparameters in ResidualBase algorithm, confidenceinterval and clipping threshold, are set to 2.0 and 0.05 respectively.In our experiment, Scenario-2 assumes a central-server aggrega-tion agnostic attack, where malicious users have access to benignclients update information. We evaluate using three attack algo-rithms: LIE, STAT-OPT, and DYN-OPT. For LIE, we set the intensityfactor to 0.3. For DYN-OPT, we set the initial intensity factor andthreshold to 10 and 1 5, respectively. The optimal intensity factorvalue is then determined by repeatedly finding the optimal valuebetween the smallest and largest values, until the intensity factorchange falls under the threshold.",
  "A.2Further Results on Defense Performance": "Comparison with other possible defense strategy. We haveconducted additional comparison experiments with more recentbaselines, including under a label flipping attack scenario(Scenario 1). FL-WBC is a client-side defense, while the others areserver-side defenses. Based on the experimental results in ,we can confirm that our proposed model still offers a performanceimprovement for the local model when integrated with other server-side defenses. It is important to note that FL-WBC was originallydesigned exclusively for backdoor attacks, resulting in smaller per-formance gains compared with our method in untargeted attackscenarios.Detection recall of Multi-Krum. shows the detectionrecall kernel density plot of the Multi-Krum algorithm, plotted",
  "Bucket70.12 72.14 48.45 48.46 37.5538.8719.88 22.70+ FedDefender 77.57 79.37 53.32 54.44 39.0640.0721.82 24.63": "against the level of non-IID data distribution. As non-IID data dis-tribution becomes more extreme (i.e., lower values of ), updatesfrom benign clients become increasingly diverse. This makes itharder for server-side defenses to identify malicious client updates.In this light, we propose a client-side defense strategy, FedDefender,to achieve additional robustness and account for inherent modelpoisoning attacks during training.Comparison with other global regularization approachesacross communication rounds. Our proposed architecture intro-duces an attack-tolerant global knowledge distillation techniqueto mitigate the negative effects of a malicious global model duringtraining. As an alternative to this method, one may also use existingapproaches, such as FedProx , Scaffold , or Moon , inconjunction with our local meta update module. comparesthe classification accuracy of our method and alternative global reg-ularization methods across communication rounds, without the useof any server-side defense. The models with other global regular-ization methods exhibit significant fluctuations in accuracy when alarge number of attackers participate in each round. In contrast, ourmethods accuracy remains stable and consistent throughout therounds, even when the number of participating attackers is high.Model comparison across communication rounds. The addi-tion of FedDefender results in significant improvement in robustnessagainst model poisoning attacks, compared to relying on server-sidedefense only. plots the accuracy changes throughout train-ing rounds, including six baselines: No Defense, Median, TrimmedMean, Norm Bound, Multi-Krum, and ResidualBase.",
  "Algorithm 1 Attack tolerant local update algorithm of FedDefender": "Input: -th Local Dataset D, global model , -th local model , -th local auxiliary classifier model temperature , learning rate Output: One epoch updated -th local model /* In k-th local client one epoch update process */ // -th local model synchronization by downloading the current global model .detach()// Global model gradient detach for knowledge distillationfor mini batch X D do /* Step 1. Attack-Tolerant Local Meta Update */X = {(x, y) | (x, y) X and y = Sampley(N (x, ))}// Perturbed mini batchX using synthetic label L =1| X|x,y X ( y, (x))// Local model poisoning with synthetic noises LL =1|X|x,yX (y, (x))// Local model correction with meta update L/* Step 2. Attack-Tolerant Global Knowledge Distillation */X = {(x, y) | (x, y) X and y = (1 ) y + (x,)}// Refined mini batchX using global model L =1| X|x,y X (y, (x))// Calibrate global knowledge distillation"
}