{
  "ABSTRACT": "Multivariate time series classification (MTSC) has attracted signifi-cant research attention due to its diverse real-world applications.Recently, exploiting transformers for MTSC has achieved state-of-the-art performance. However, existing methods focus on genericfeatures, providing a comprehensive understanding of data, but theyignore class-specific features crucial for learning the representativecharacteristics of each class. This leads to poor performance in thecase of imbalanced datasets or datasets with similar overall pat-terns but differing in minor class-specific details. In this paper, wepropose a novel Shapelet Transformer (ShapeFormer), which com-prises class-specific and generic transformer modules to captureboth of these features. In the class-specific module, we introducethe discovery method to extract the discriminative subsequencesof each class (i.e. shapelets) from the training set. We then pro-pose a Shapelet Filter to learn the difference features between theseshapelets and the input time series. We found that the differencefeature for each shapelet contains important class-specific features,as it shows a significant distinction between its class and others. Inthe generic module, convolution filters are used to extract genericfeatures that contain information to distinguish among all classes.For each module, we employ the transformer encoder to capturethe correlation between their features. As a result, the combina-tion of two transformer modules allows our model to exploit thepower of both types of features, thereby enhancing the classifi-cation performance. Our experiments on 30 UEA MTSC datasetsdemonstrate that ShapeFormer has achieved the highest accuracyranking compared to state-of-the-art methods. The code is availableat",
  "Information systems Data mining; Computing method-ologies Machine learning": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 2024, Aug 25 - 29, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-XXXX-X/18/06",
  "INTRODUCTION": "A multivariate time series (MTS) is a collection of data points whereeach point is composed of multiple variables that have been ob-served or measured over time. This data structure is prevalent invarious fields, such as economics , weather prediction , ed-ucation , and healthcare . Time series classification standsout as a fundamental and crucial aspect within the domain of timeseries analysis . However, there are still many challenges in theresearch on MTS classification (MTSC) , especially in capturingthe correlations among variables.Over the past few decades, various approaches have been intro-duced to enhance the performance of MTSC .Among these, shapelets, which are class-specific time series subse-quences, have demonstrated their effectiveness in .This success comes from the fact that each shapelet contains class-specific information representative of its class. It is evident that thedistance between the shapelet and the time series of its class is farsmaller than the time series of other classes (see ). Hence,there has been an increased focus on harnessing the capabilities ofshapelets in the field of MTSC.In 2017, Vaswani et al. introduced the breakthrough Trans-former architecture, initially designed for Natural Language Pro-cessing but later demonstrating success in Computer Vision tasks. Following these successes, Transformer-based models havebeen effectively applied to MTSC. GTN employs a two-towermulti-headed attention approach to extract distinctive informationfrom input series, SVP-T captures short- and long-term depen-dencies among subseries using clustering and employing them asinputs for the Transformer, and ConvTran integrates absoluteand relative position encoding for improved position embedding inthe Transformer model.Obviously, Transformers utilised in MTSC have demonstratedstate-of-the-art (SOTA) performances . Existing methods",
  "Class": "distance=0.1 distance=2.7 distance=1.9 : The illustration depicts the shapelet in the Atrial Fibrilla-tion dataset. The best-fit subsequence is the subsequence with thesortest distance to the shapelet in the time series. It is clear that theshapelet can discriminate between classes by utilising their distanceto the best-fit subsequences. (a) Generic Feature (b) Class-specific Feature",
  ": The separating hyperplane using (a) the generic featurehas a higher overall accuracy, while the hyperplane using (b) theclass-specific feature is better in classifying a single class": "only discover the generic features from timestamps orcommon subsequences in time series as inputs for the Trans-former model to capture the correlation among them. These fea-tures merely contain generic characteristics of time series, offeringa broad understanding of the data. Nevertheless, they overlook theessential class-specific features necessary to allow the model tocapture the representative characteristics of each class. As a result,the model exhibits poor performance in two cases: 1) the datasethas instances that are very similar in overall patterns, differing onlyin minor class-specific patterns, effective classification cannot beachieved using solely generic features; 2) the imbalanced dataset,where generic features only focus on classifying the majority classesand ignore those of minority. As can be seen in , the hy-perplane created using the generic feature (a) attemptsto classify the majority classes (orange triangles and blue circles)and ignores the minority (green squares), while the class-specificfeature (b) tries to separate each class from the others.To address the aforementioned problem, we propose a novelmethod called Shapelet Transformer (ShapeFormer), which com-prises class-specific and generic transformer modules to captureboth of these features. In the class-specific module, we initially in-troduce Offline Shapelet Discovery, inspired by , to MTS. Basedon this, we extract a small number of high-quality shapelets fromthe training set. Subsequently, we propose a Shapelet Filter thatleverages the precomputed shapelets to discover the best-fit subse-quences in the input time series. Following this, the Shapelet Filter learns the difference between the embedding of these shapelets andtheir most fitting subsequences derived from the input time series.As shown in , the distance of shapelets to the time series inthe same class is far smaller than the time series of other classes.Similar to the distance, our difference feature also highlights thesubstantial distinctions among classes. Additionally, rather thanusing the original shapelets extracted from the dataset, we proposeconsidering these shapelets as the initialisation and then dynami-cally optimising shapelets during training to effectively representthe distinguishing information. In the generic module, we utiliseconvolution filters for the extraction of features over all classes. Foreach module, we employ the transformer encoder to capture thedependencies between their features. Through the integration ofthese two modules, our ShapeFormer excels in capturing not onlyclass-specific features but also generic characteristics from timeseries data. This dual capability contributes to an enhancement inthe overall performance of classification tasks.Our contributions can be summarised as follows:",
  "We propose the Offline Shapelet Discovery for MTS to effectivelyand efficiently extract shapelets from training set": "We propose the Shapelet Filter, which learns the difference be-tween shapelets and input time series, which contain importantclass-specific features. The shapelets are also dynamically opti-mised during training to effectively represent the class distin-guishing information. We conduct experiments on all 30 UEA MTS datasets and demon-strate that ShapeFormer has achieved the highest accuracy rank-ing compared to SOTA methods.To the best of our knowledge, our ShapeFormer is a pioneeringtransformer-based approach that leverages the power of shapeletsfor MTSC.",
  "RELATIVE WORKS2.1Multivariate Time Series Classification": "We categorise the MTSC methods into two main categories: non-deep learning, and deep learning.Non-deep learning methods. They primarily utilise distancemeasures , such as Euclidean Distance , Dynamic TimeWarping, and its diverse variants , to calculate the similar-ity between time series. Otherwise, they leverage special features,such as bag of patterns , Symbolic Aggregate approXimation, bag of SFA symbols , and convolution kernel features for classification. gives a comprehensive survey of theconventional methods mentioned.Deep learning methods. Various neural network methods wereproposed for MTSC . Specifically, the LSTM-FCN modelfeatures an LSTM layer and stacked CNN layers which directlyextract features from time series. These features are subsequentlyfed into a softmax layer to produce class probabilities. However,it has a limitation in capturing long dependencies among differ-ent variables. To address this, Hao et al . proposed to use oftwo cross-attention modules to enhance their CNN-based model.TapNet constructs an attentional prototype network that in-corporates LSTM, and CNN to learn multi-dimensional interaction",
  "Transformer-Based Time Series Classifiers": "In 2017, Vaswani et al. introduced the Transformer architecture,achieving a breakthrough in Natural Language Processing anddemonstrating notable success in Computer Vision tasks . Re-cently, it has proven effective in time series classification tasks.Specifically, GTN utilises a two-tower multi-headed attentionapproach for extracting distinctive information from the input se-ries. The integration of the output from the two towers is achievedthrough gating, implemented by a learnable matrix. ConvTran was proposed to enhance the position embedding by leveragingboth absolute and relative position encoding. SVP-T uses clus-tering to identify time series subsequences and employs them asinputs for the Transformer, enabling the capture of long- and short-term dependencies among subseries. Recently, the application ofpretrained transformer-based self-supervised learning models likeBERT has achieved significant success not only in the fieldof NLP but also in other areas . Inspired by thesesuccesses, many models attempt to adopt a similar structure fortime series classification . It is noteworthy that most pre-vious transformer-based methods effectively exploit the genericinformation of time series.",
  "Shapelet Discovery for Time Series": "Shapelets refer to short subsequences within time series that con-tain class-specific information by exhibiting a small distance to thetime series of the target class and a larger distance to other classes(see ). Additionally, each shapelet can encompass crucialsubsequences located at different positions and variables within atime series. This coverage enables them to effectively represent thetime series. In the last decade, the effectiveness of shapelets for timeseries has been proven by many related studies .The original shapelet discovery method extracts all possiblesubsequences in the training set and considers the subsequencesas shapelets when they have the highest information gain ratio. Itrequires excessive computing time and is hard to apply to MTSC.Other methods use random shapelets that lack position and vari-able information , or employ the common subsequences asshapelets, which unfortunately have limited discriminative features. Recently, proposed the hyperfast Offline Shapelet Dis-covery (OSD), which utilises important points to extract a smallnumber of high-quality shapelets from the original time series data.It has been demonstrated to be a SOTA method for univariate timeseries classification.",
  ": The general architecture of ShapeFormer": "= 1, . . . , , where 1 signifies a value for variable at times-tamp within X. Consider a training dataset D = {(, )}=1,where is the number of time series instances and the pair (, )represents a training sample and its corresponding label, respec-tively. The objective of MTSC is to train a classifier () to predicta class label for a multivariate time series with an unknown label.Time Series Subsequence. Given a time series of length , atime series subsequence [ : ] = , ..., is a consecutivesubsequence of time series , where is a start index and is anend index.Perceptual Subsequence Distance (PSD). Given a time series of length , and a subsequence = 1, ..., of length , with ,the PSD of and is determined as:",
  "SHAPELET TRANSFORMER MODEL": "We propose ShapeFormer, a transformer-based method that lever-ages the strength of both class-specific and generic features in timeseries. In contrast to existing transformer-based MTSC methods, our approach first extracts shapelets from the trainingdatasets (.1). Subsequently, these extracted shapelets areused to discover discriminative features in time series through theuse of a class-specific transformer module (.2). Addition-ally, we introduce the use of convolution layers with a generic trans-former module to extract generic features in time series (.3). Finally, the overall architecture of ShapeFormer is summarisedin .4 and .",
  "Shapelet Discovery": "This section introduces the Offline Shapelet Discovery (OSD) method,inspired by , to multivariate time series. In contrast with othermethods, our OSD employs Perceptually Important Points (PIPs), condensing time series data by choosing points that closelyresemble the original, to efficiently select high-quality shapelets.The selection process is based on the reconstruction distance, withthe highest index continuously chosen. We define the reconstruc-tion distance as the perpendicular distance between a target point",
  ": The process of Offline Shapelet Discovery": "and a line reconstructed by the two nearest selected importantpoints. The process of our OSD is illustrated in and the pseudo-code is presented in Algorithm 1. Given the datasetD = {(, )}=1, our method contains two main phases, includingshapelet extraction and shapelet selection.In the first phase, our OSD initially extracts shapelet candidatesby identifying PIPs. Specifically, the first and last indices are addedto the PIPs set. Subsequently, the index with the highest reconstruc-tion distance is continuously added to the PIPs set. Each time anew PIP is added, we extract new shapelet candidates with threeconsecutive PIPs points. This means that, with each new PIP, amaximum of three shapelet candidates can be added to the set. Inthis paper, we set the number of PIPs as = 0.2 , where represents the time series length. Our method aims to select amaximum of 3 candidates, therefore, we only extract an av-erage of 5900 candidates for each dataset. This count is significantlysmaller than the 45 million candidates typically extracted throughclassic shapelet discovery methods , thereby significantlyspeeding up the process. We then store four types of informationfor each shapelet, including the value vector of shapelets, its startindex, end index, and variables.In the second phase, our method selects an equal number ofshapelets for each class. Given the shapelet candidate of class ,we first compute its PSD with all instances in the training datasets(Eq. 1). After that, their distance will be used to find optimal in-formation gain. This implies that the optimal information gain isthe highest ratio achievable by the shapelet . Finally, the top candidates with the highest information gain are chosen as theshapelets and stored in the shapelet pool .",
  "return S": "Shapelet Filter. Given a shapelet pool S (as discussed in .1), an input time series and its label , we first select the best-fitsubsequence for each shapelet in S (refer to a). Specifically,with each shapelet S, its length , start index , end index and variables , we calculate the distance CID of them with allsubsequences in time series . After that, the subsequence withthe shortest distance will be selected as an important subsequence of .",
  "= [index : index + ] .(3)": "To reduce computing time and effectively utilise the position in-formation of the shapelet, we propose limiting the search for thebest-fit subsequence to a neighbouring area within the hyperpa-rameter window size on both the left and right sides of the actualposition of the shapelet. This means that one shapelet only calcu-lates the distance with maximum 2 + 1 subsequences in .",
  "= P () P () ,(6)": "where P is the linear projector of R with is length of shapeletand is the embedding size of difference features.Similar to the distance between shapelet and time series, our dif-ference feature also highlights the substantial distinctions amongclasses. Furthermore, by directly incorporating the shapelets incomputing the difference features (Eq. 6), the shapelets are nowconsidered as the learnable parameters of the Shapelet Filter com-ponent. Therefore, rather than using fixed shapelets, we can usethem as the initial parameters of the Shapelet Filter, which will beoptimised during training.Position Embedding. The difference features are then inte-grated with position embeddings to capture their order. To betterindicate the position information of shapelets, the embeddings ofthree types of positions are considered, including the start index,end index, and variables. Specifically, we propose to use a one-hotvector representation for these indices and then employ a linearprojector to learn their embedding.",
  "= + PE() + PE() + PE() .(8)": "We also observed that the performance is enhanced when we onlyuse the position of shapelets instead of the position of best-fitsubsequences. This improvement can be attributed to the fact thatthe fixed position is easier to learn than the unstable position ofbest-fit subsequences. Transformer Encoder. The class-specific difference features, alongwith their corresponding position embeddings, are then input intoa transformer encoder to learn their correlation. Specifically, weemploy the multi-head attention mechanism (MHA) for thispurpose. Given an input series, = 1, . . . , and the projections, , R represent query, key, and value matrices,respectively. These matrices, , , , undergo reshaping intoR (/) to signify the attention heads and are subse-quently concatenated into standard dimensions after computation.Each attention head within this set is capable of capturing distinctrelationships of the features. Finally, these matrices are used tocompute an output Zspe = spe1, ...,spewhere spe R :",
  ") ,(10)": "Thanks to the class-represented characteristics of these features,the attention score for features within the same class is boostedcompared to features in different classes. This enhancement helpsthe model better distinguish between different classes. Additionally,owing to the nature of shapelets, the difference features possessthe ability to identify significant subsequences across different tem-poral locations and variables within the time series. This capabilityenables the module to effectively capture temporal and variabledependencies in time series data.Class Token. Existing transformer-based methods apply averagingpooling to Zspe, to obtain the final token for classification .However, our class-specific transformer module utilises differencefeatures that capture the distinctive characteristics of each shapelet.Applying average pooling may diminish these properties, poten-tially limiting performance. To address this, we propose using onlythe first difference feature of the highest information gain shapeletspe1as the class token spefor final classification. The reason forthis is that when averaging all tokens, there is a loss of informa-tion regarding distinct features . Moreover, the first token spe1,which carries the highest information gain, harbors the most crucialfeatures for effectively classifying time series.",
  "Generic Transformer": "Besides leveraging the power of class-specific features, in this sec-tion, we introduce the generic transformer module, utilising convo-lution filters to extract generic features in the time series. Specifi-cally, we employ two CNN components , each comprisingConv1D, BatchNorm, and GELU, to effectively discover generic fea-tures. The first block is designed to capture the temporal patternsin time series by using the Conv1D filter R1 . On the otherhand, the second block uses the Conv1D filter R 1 to capturethe correlation between variables in time series. In this context, represents the number of variables, and is the kernel size of theconvolution filter, which is fixed at 8 in all experiments. From that,",
  "Overall Architecture of ShapeFormer": "To enhance clarity, we present the overall architecture of Shape-Former in . Our method initiates by extracting shapeletsfrom the training datasets. Subsequently, for a given input timeseries , it is processed through dual transformer modules, com-prising the class-specific shapelet transformer and the generic con-volution transformer. The outputs from these two modules are thenconcatenated and fed into the final classification head.",
  "EXPERIMENTS5.1Experimental Setting": "Dataset. We assess our approach using the UEA archive, a well-known benchmark made up of 30 distinct datasets for MTSC .It covers various domains, including Human Activity Recognition,Motion classification, ECG classification, EEG/MEG classification,Audio Spectra classification, and more. The sample sizes of datasetsin the UEA archive range from 27 to 50,000, the time series lengthsspanning 8 to 17,984, and dimensions varying from 2 to 1,345.Metrics. We use classification accuracy to evaluate model perfor-mance and compare methods based on their average ranks andwin/draw/loss counts on all datasets. Finally, we evaluate the sta-tistical significance of performance differences using the p-value ofFriedman and Wilcoxon signed-rank test .Implementation Details. Our model was trained using the RAdamoptimiser with an initial learning rate set as 0.01, a momentum of0.9, and a weight decay of 5e-4. The training process involved abatch size of 16 for a total of 200 epochs. We configured the num-ber of attention heads to be 16 and followed the protocol outlinedin . This protocol involves splitting the training set into80% for training and 20% for validation, allowing us to fine-tune hyperparameters. Once the hyperparameters were finalised, we con-ducted model training on the entire training set and subsequentlyevaluated its performance on the designated official test set.Environment. All the experiments are conducted on a machinewith one Intel(R) Xeon(R) Silver 4214 CPU @ 2.20GHz and oneNVIDIA Tesla V100 SXM2.",
  "Baselines": "We have selected 12 baseline methods for the comparative exper-iments, comprising two distance-based methods: EDI, ;a pattern-based algorithm: WEASEL+MUSE ; a feature-basedalgorithm: MiniRocket ; an ensemble method: LCEM ; threedeep learning models: MLSTM-FCNs , Tapnet , Shapenet; an attention-based model: WHEN ; and three transformer-based models: TST , ConvTran , SVP-T . They all attainedthe SOTA performance described in the most recent research. Thedetails of 12 baseline methods are shown in Appendix A.",
  "Performance Evaluation": "illustrates the experimental results of our method with 12other competitors on the UEA multivariate time series classificationarchive . The accuracy of 12 baseline methods are taken from, except the results of WHEN, and ConvTran are taken fromtheir original papers . The best result on each dataset isindicated in bold, and the summarised information is provided inthe last six lines of the table.The results show that among all methods, ShapeFormer achievesthe best performance in both the highest average rank (2.5) andthe largest number of top-1 (the best in 15 out of 30 datasets). Thisindicates that ShapeFormer can be taken as a SOTA for MTSC. Therank index signifies that, even on some datasets where our modeldoes not exhibit the highest performance, its results remain highlycompetitive. Specifically, the average rank of our method is slightlyhigher compared to that of the runner-up, WHEN, a difference of0.617. Meanwhile, the gap in average rank between ShaperFormerand three Transformer-based methods (TST, ConvTran, SVP-T) islarge, with 5.617, 3.15, and 2.783 respectively. The p-value is 0.05,which confirms there ranks have statistically significant differences.Specifically, the p-values for ShapeFormer in comparison to allmethods are below 0.05, which indicates the results are statisti-cally significant except for WHEN. However, regarding the numberof top-1, our ShapeFormer attained SOTA results in 15 datasetscompared to WHEN, only 4 datasets.",
  "Ablation Study and Model Design": "Effectiveness of Using Shapelets. In , we compare theperformance when using random subsequences, common subse-quences as mentioned in , and shapelets in our methods. Theresults demonstrate that the shapelets outperform the other twomethods in terms of accuracy across all five datasets. This highlightsthe benefit of highly discriminative shapelet features in increasingthe performance of the transformer-based model, thereby indicatingthe contribution of our work.Component Evaluation. We begin by evaluating the impact oftwo key modules in our ShapeFormer: the Class-specific Trans-former (.2) and the Generic Transformer (.3),",
  ": Accuracies of using shapelets and two other types of subse-quences": "in comparison with the baseline method, SVP-T on the first10 datasets of UEA archive . In this process, individual compo-nents are incrementally incorporated to assess their impact on theultimate accuracy. As depicted in , applying the generictransformer alone exhibits a lower accuracy compared to the base-line. In contrast, utilising only the class-specific module results insignificantly improved performance over the baselines, emphasisingthe effectiveness of class-specific features in the transformer-basedtime series model. Furthermore, the combination of class-specific genericbaselineclass-specificclass-specific+generic",
  ": Average ranks for 3 variations of ShapeFormer and thebaseline (SVP-T - the current SOTA transformer-based method)": "and generic components shows a positive impact on the enhance-ment of classification accuracy. This combination harnesses thepower of both features, significantly boosting overall performance.Choosing between the Position of Shapelets and Best-fit Sub-sequences. Our ShapeFormer leverages shapelets to find the best-fit subsequences and employ the difference features calculatedby them as the inputs for the Transformer encoder. Then there isa question \"Should we choose the positions of the shapelets or thebest-fit subsequences for position embedding?\". illustratesa comparison between the accuracies achieved by employing theposition of the best-fit subsequences and shapelets, as indicatedin Eq. 8, for position embedding in the Transformer encoder. Theoutcomes indicate that our approach exhibits superior performancewhen utilising the position of shapelets across all five datasets un-der consideration. This enhancement can be ascribed to the fact",
  ": Average accuracy ranks of different class token designs": "that learning from the fixed position of shapelets is easier comparedto the unstable position of the best-fit subsequences.Comparison with Various Methods for Calculating Differ-ence Features. The critical difference diagram in displaysthe performance of using different methods for calculating differ-ence features in Eq. 6, including Manhattan Distance, EuclideanDistance, and the subtraction between P () and P (). The re-sults demonstrate that: 1) All calculation methods for differencefeatures yield better results compared to the SVP-T baseline; 2)Using subtraction exhibits the highest performance. Although thesubtraction is simple, its superiority lies in effectively capturingrelative changes by considering both the magnitude and directionof changes between embedding vectors P () and P ().Different Class Token Designs. The output of the class-specifictransformer consists of a series of tokens spe1, . . . ,spe. The ques-tion at hand is, \"Are there any effective ways to design class tokensbefore feeding them to the classification head?\". In , we anal-yse the impact of different class token designs on the performanceof ShapeFormer. The results indicate that: 1) Our ShapeFormer out-performs the baseline with all types of class token designs, demon-strating the advantage of our method; 2) Utilising the first tokenspe1as the final class token spefor ShapeFormer yields the bestperformance. This is due to the fact that learning or averaging alltokens results in a loss of information on difference features .Furthermore, the first token, containing the highest informationgain, possesses the most discriminative features for classifying timeseries.",
  ": Effectiveness of (a) class-specific and generic scale factorsand (b) different dropout ratios": "shapelets. Regarding the window size, we propose to tune it exclu-sively during the shapelet discovery phase. For each dataset, wewill select a window size from the set , aim-ing to provide the top 100 shapelets with the highest informationgain. This tuning technique can significantly reduce training timesince it only operates during the shapelet discovery phase. As forthe number of shapelets, considering the diverse characteristics ofdifferent datasets, we choose this number from .The details of our tuned parameters are shown in Appendix B.Number of PIPs. As shown in the following , the modelaccuracy increases as we increase the number of PIPs (npips) from0.05T to 0.2T. Afterward, accuracy remains stable even with furtherincreases in npips. Therefore, we set npips at 0.2 for all of ourexperiments.The Scale Factors spe and gen. In a, we compare theimpact of different scale factors of the class-specific and genericembedding sizes on the classification accuracy of ShapeFormer.The results show that: 1) The pair of spe = 128 and gen = 32 hasachieved the highest accuracy; 2) In general, a larger class-specificembedding size has achieved better performance, indicating thebenefit of using shapelets in a transformer-based time series model.Dropout Ratios. In b, we analyse the impact of differentdropout ratios of ShapeFormer. It is evident that our methods workwell and achieve high performance with small dropout ratios, withthe ratio of 0.4 yielding the highest performance.",
  "Improving Performance in Specific Datasetsby Optimizing Scale Factor": "In MTSC, it is crucial to develop models that generalise well across amajority of datasets rather than models tailored to specific datasets.For example, in terms of InsectWingbeat dataset, we observed thatsetting gen (embedding size of generic feature) to 256 leads tosignificantly better performance (0.704) compared to gen at 32(0.314) (our chosen parameter). However, this improvement comesat the cost of decreased performance on other datasets (from 0.864to 0.831). Therefore, we recommend tuning this hyperparameter toachieve better performance on specific datasets if needed.",
  "A Case Study of LSST (Imbalanced Datasets)": "To illustrate the effectiveness of combining both class-specific andgeneric features transformer modules to classify imbalanced data,we conducted experiments on the LSST dataset. The LSST datasetcomprises 16 classes, and we randomly selected 4 classes to berepresented by the colors blue, orange, green, and red, with 35, 270,382, and 63 instances, respectively. It is clear that the sizes of blueand red classes are significantly smaller compared to the sizes ofthe green and orange classes. a shows that the generictransformer prioritises majority classes (green and orange), butneglects minority ones (blue and red). However, in b, thecombination of class-specific and generic transformers effectivelydistinguishes all four classes.",
  "A Case Study of BasicMotions": "To interpret ShapeFormer results, we use the BasicMotions datasetfrom the UEA archive , focusing on human activity recognitionwith 4 classes (playing badminton, standing, walking, and running).Each class is associated with 6 variables, and 10 shapelets are set foranalysis. Randomly selecting a walking instance from the trainingset. a showcases the top three shapelets for this class andthree from others, highlighting ShapeFormers ability to identifycrucial subsequences across diverse locations and variables in thetime series. Moreover, shapelets within the same walking classtend to share greater similarity with best-fit subsequences thanthose from other classes.In b, the attention heat map for all 40 shapelets across 4classes reveals that shapelets within the same class generally attainhigher attention scores. For instance, 20 and 23 belonging to thewalking class show a small difference feature (Eq. 6), resulting inhigher attention scores. This enhanced attention allows the modelto focus more on the correlation between shapelets within the sameclasses, thereby improving overall performance.",
  "CONCLUSION": "In this paper, we propose a novel Shapelet Transformer (Shape-Former) for multivariate time series classification. It consists ofdual transformer modules aimed at identifying class-specific andgeneric features within time series data. In particular, the firstmodule discovers class-specific features by utilising discriminativesubsequences (shapelets) extracted from the entire dataset. Mean-while, the second transformer module employs convolution filters",
  "Shapelets": "(a) Shapelets and Their Best-fit Subsequences(b) Attention Heat Map ID: 20 ID: 23 ID: 25 ID: 12ID: 37 ID: 1 : (a) The green box depicts the top three shapelets, andthe orange box displays three random shapelets from other classes,extracted in one random input time series of the walking class in theBasicMotions dataset. (b) The attention heat map for all shapelets. to extract generic features across all classes. The experimental re-sults show that by combining both modules, our ShapeFormer hasachieved the highest rank in terms of classification accuracy whencompared to the SOTA methods. In future work, we intend to utilisethe power of shapelets in many different time series analysis taskssuch as forecasting or anomaly detection. Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large,Aaron Bostrom, Paul Southam, and Eamonn Keogh. 2018. The UEA multivariatetime series classification archive, 2018. arXiv preprint arXiv:1811.00075 (2018). Gustavo EAPA Batista, Xiaoyue Wang, and Eamonn J Keogh. 2011. A complexity-invariant distance measure for time series. In Proceedings of the 2011 SIAM inter-national conference on data mining. SIAM, 699710.",
  "Donald J Berndt and James Clifford. 1994. Using dynamic time warping tofind patterns in time series. In Proceedings of the 3rd international conference onknowledge discovery and data mining. 359370": "Fu Lai Korris Chung, Tak-Chung Fu, Wing Pong Robert Luk, and Vincent To YeeNg. 2001. Flexible time series pattern matching based on perceptually importantpoints. In Workshop on Learning from Temporal and Spatial Data in InternationalJoint Conference on Artificial Intelligence. Angus Dempster, Daniel F Schmidt, and Geoffrey I Webb. 2021. Minirocket:A very fast (almost) deterministic transform for time series classification. InProceedings of the 27th ACM SIGKDD conference on knowledge discovery & datamining. 248257.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:Pre-training of deep bidirectional transformers for language understanding. arXivpreprint arXiv:1810.04805 (2018)": "Thuc-Doan Do, Tuan Minh Tran, Xuan-May Thi Le, and Thuy-Van T Duong. 2017.Detecting special lecturers using information theory-based outlier detectionmethod. In Proceedings of the International Conference on Compute and DataAnalysis. 240244. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, GeorgHeigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformersfor image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).",
  "Kevin Fauvel, lisa Fromont, Vronique Masson, Philippe Faverdin, and Alexan-dre Termier. 2020. Local cascade ensemble for multivariate data classification.arXiv preprint arXiv:2005.03645 (2020)": "Navid Mohammadi Foumani, Chang Wei Tan, Geoffrey I Webb, and Mahsa Salehi.2023. Improving Position Encoding of Transformers for Multivariate Time SeriesClassification. arXiv preprint arXiv:2305.16642 (2023). Seyed Navid Mohammadi Foumani, Chang Wei Tan, and Mahsa Salehi. 2021.Disjoint-cnn for multivariate time series classification. In 2021 InternationalConference on Data Mining Workshops (ICDMW). IEEE, 760769. Ge Gao, Qitong Gao, Xi Yang, Miroslav Pajic, and Min Chi. 2022. A reinforcementlearning-informed pattern mining framework for multivariate time series classi-fication. In In the Proceeding of 31th International Joint Conference on ArtificialIntelligence (IJCAI-22). Josif Grabocka, Nicolas Schilling, Martin Wistuba, and Lars Schmidt-Thieme.2014. Learning time-series shapelets. In Proceedings of the 20th ACM SIGKDDinternational conference on Knowledge discovery and data mining. 392401. Josif Grabocka, Martin Wistuba, and Lars Schmidt-Thieme. 2016. Fast classi-fication of univariate and multivariate time series through shapelet discovery.Knowledge and information systems 49 (2016), 429454.",
  "Eamonn Keogh and Chotirat Ann Ratanamahatana. 2005. Exact indexing ofdynamic time warping. Knowledge and information systems 7 (2005), 358386": "Sang-Wook Kim, Dae-Hyun Park, and Heon-Gil Lee. 2004. Efficient processingof subsequence matching with the Euclidean metric in time-series databases.Information processing letters 90, 5 (2004), 253260. Xuan-May Le, Minh-Tuan Tran, and Van-Nam Huynh. 2022. Learning Percep-tual Position-Aware Shapelets for Time Series Classification. In Joint EuropeanConference on Machine Learning and Knowledge Discovery in Databases. Springer,5369.",
  "Jessica Lin, Rohan Khade, and Yuan Li. 2012. Rotation-invariant similarity in timeseries using bag-of-patterns representation. Journal of Intelligent InformationSystems 39 (2012), 287315": "Jason Lines, Luke M Davis, Jon Hills, and Anthony Bagnall. 2012. A shapelettransform for time series classification. In Proceedings of the 18th ACM SIGKDDinternational conference on Knowledge discovery and data mining. 289297. Minghao Liu, Shengqi Ren, Siyuan Ma, Jiahui Jiao, Yizhou Chen, Zhiguang Wang,and Wei Song. 2021. Gated transformer networks for multivariate time seriesclassification. arXiv preprint arXiv:2103.14438 (2021). Amy McGovern, Derek H Rosendahl, Rodger A Brown, and Kelvin K Droegemeier.2011. Identifying predictive multi-dimensional time series motifs: an applicationto severe weather prediction. Data Mining and Knowledge Discovery 22 (2011),232258.",
  "Andrew J Patton. 2012. A review of copula models for economic time series.Journal of Multivariate Analysis 110 (2012), 418": "Alejandro Pasos Ruiz, Michael Flynn, James Large, Matthew Middlehurst, andAnthony Bagnall. 2021. The great multivariate time series classification bakeoff: a review and experimental evaluation of recent algorithmic advances. DataMining and Knowledge Discovery 35, 2 (2021), 401449. Alejandro Pasos Ruiz, Michael Flynn, James Large, Matthew Middlehurst, andAnthony Bagnall. 2021. The great multivariate time series classification bakeoff: a review and experimental evaluation of recent algorithmic advances. DataMining and Knowledge Discovery 35, 2 (2021), 401449.",
  "Patrick Schfer and Ulf Leser. 2017. Multivariate time series classification withWEASEL+ MUSE. arXiv preprint arXiv:1711.11343 (2017)": "ABA Stevner, Diego Vidaurre, Joana Cabral, K Rapuano, SrenFnsVind Nielsen,Enzo Tagliazucchi, Helmut Laufs, Peter Vuust, Gustavo Deco, Mark W Woolrich,et al. 2019. Discovery of key whole-brain transitions and dynamics during humanwakefulness and non-REM sleep. Nature communications 10, 1 (2019), 1035. Chang Wei Tan, Angus Dempster, Christoph Bergmeir, and Geoffrey I Webb.2022. MultiRocket: multiple pooling operators and transformations for fast andeffective time series classification. Data Mining and Knowledge Discovery 36, 5(2022), 16231646.",
  "Minh-Tuan Tran, Trung Le, Xuan-May Le, Mehrtash Harandi, and Dinh Phung.2024. Text-Enhanced Data-free Approach for Federated Class-Incremental Learn-ing. arXiv preprint arXiv:2403.14101 (2024)": "Minh-Tuan Tran, Trung Le, Xuan-May Le, Mehrtash Harandi, Quan Hung Tran,and Dinh Phung. 2023. NAYER: Noisy Layer Data Generation for Efficientand Effective Data-free Knowledge Distillation. arXiv preprint arXiv:2310.00258(2023). Tuan Minh Tran, Xuan-May Thi Le, Hien T Nguyen, and Van-Nam Huynh. 2019.A novel non-parametric method for time series classification based on k-NearestNeighbors and Dynamic Time Warping Barycenter Averaging. EngineeringApplications of Artificial Intelligence 78 (2019), 173185. Tuan Minh Tran, Xuan-May Thi Le, Vo Thanh Vinh, Hien T Nguyen, and Tuan MNguyen. 2017. A weighted local mean-based k-nearest neighbors classifier fortime series. In Proceedings of the 9th International Conference on Machine Learningand Computing. 157161. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processing systems 30 (2017). Jingyuan Wang, Chen Yang, Xiaohan Jiang, and Junjie Wu. 2023. WHEN: AWavelet-DTW Hybrid Attention Network for Heterogeneous Time Series Analy-sis. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discoveryand Data Mining. 23612373. Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and MingshengLong. 2022. Timesnet: Temporal 2d-variation modeling for general time seriesanalysis. In The eleventh international conference on learning representations. Carl Yang and Jiawei Han. 2023. Revisiting citation prediction with cluster-aware text-enhanced heterogeneous graph neural networks. In 2023 IEEE 39thInternational Conference on Data Engineering (ICDE). IEEE, 682695.",
  "Lexiang Ye and Eamonn Keogh. 2009. Time series shapelets: a new primitive fordata mining. In Proceedings of the 15th ACM SIGKDD international conference onKnowledge discovery and data mining. 947956": "Wenhui Yu, Xiao Lin, Junfeng Ge, Wenwu Ou, and Zheng Qin. 2020. Semi-supervised collaborative filtering by text-enhanced domain adaptation. In Pro-ceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery& Data Mining. 21362144. Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang,Yunhai Tong, and Bixiong Xu. 2022. Ts2vec: Towards universal representation oftime series. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36.89808987. George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty,and Carsten Eickhoff. 2021. A transformer-based framework for multivariate timeseries representation learning. In Proceedings of the 27th ACM SIGKDD conferenceon knowledge discovery & data mining. 21142124. Xuchao Zhang, Yifeng Gao, Jessica Lin, and Chang-Tien Lu. 2020. Tapnet: Multi-variate time series classification with attentional prototypical network. In Pro-ceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 68456852.",
  "ABASELINES": "12 baseline methods are utilised in the comparative experiments,comprising two distance-based methods, a pattern-based algorithm,a feature-based algorithm, an ensemble method, three deep learningmodels, an attention-based model, and three transformer-basedmodels. They all attained the SOTA performance described in themost recent research. EDI and : The two benchmark classifiers based on Eu-clidean Distance and dimension-dependent dynamic time warp-ing. WEASEL+MUSE : A classifier based on a bag-of-pattern ap-proach demonstrated SOTA performance when compared withsimilar competitors for MTSC. We choose this algorithm as therepresentative baseline among pattern-based methods.",
  "DatasetWindow sizeNumber of shapelets (per class)": "ArticularyWordRecognition5010AtrialFibrillation1003BasicMotions10010CharacterTrajectories503Cricket20030DuckDuckGeese10100ERing50100EigenWorms1010Epilepsy2030EthanolConcentration200100FaceDetection1010FingerMovements2030HandMovementDirection200100Handwriting2030Heartbeat200100InsectWingbeat1030JapaneseVowels101LSST2010Libras1030MotorImagery10030NATOPS201PEMS-SF5010PenDigits410PhonemeSpectra2030RacketSports1010SelfRegulationSCP1100100SelfRegulationSCP2100100SpokenArabicDigits10100StandWalkJump10100UWaveGestureLibrary1010 Tapnet : A classifier constructs an attentional prototype net-work. Tapnet incorporates LSTM, and CNN to learn multi dimen-sional interaction features. We opt for it as another representativeof the deep learning method. Shapenet : Shapenet aims to learn representations of differentshapelet candidates in a unified space and selects final shapeletsby training a dilated causal CNN module followed by standardclassification. This model can capture dependencies among vari-ables. We choose it as a representative of the shapelet-basedmethod. WHEN : An attention-based method that learns heterogene-ity by utilising a hybrid attention network, incorporating bothDTW attention and wavelet attention. It achieved SOTA perfor-mance for MTSC on the UEA datasets. TST : A transformer-based framework for MTS representa-tion learning. TST is considered as baseline method that takes thevalues at each timestamp as the input for the Transformer model.It gains great performance for many sequential tasks, such asregression, and classification."
}