{
  "ABSTRACT": "Hierarchical text classification (HTC) aims to assign one or morelabels in the hierarchy for each text. Many methods represent thisstructure as a global hierarchy, leading to redundant graph struc-tures. To address this, incorporating a text-specific local hierarchyis essential. However, existing approaches often model this localhierarchy as a sequence, focusing on explicit parent-child rela-tionships while ignoring implicit correlations among sibling/peerrelationships. In this paper, we first integrate local hierarchies intoa manual depth-level prompt to capture parent-child relationships.We then apply Mixup to this hierarchical prompt tuning schemeto improve the latent correlation within sibling/peer relationships.Notably, we propose a novel Mixup ratio guided by local hierar-chy correlation to effectively capture intrinsic correlations. ThisLocal Hierarchy Mixup (LH-Mix) model demonstrates remarkableperformance across three widely-used datasets.",
  "Corresbonding author:": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from acronym XX, August 0307, 2025, Woodstock, NY 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-XXXX-X/18/06",
  "Statistics": ": (a) A toy example of global hierarchy in HTC.(b) The local hierarchy of CS/Machine Learning andMath/Statistics, which are extracted from (a). (c) Transfor-mation from explicit parent-child relations (a) to spatial in-clusion relations in latent space. Mixup enables the captureof varying degrees of implicit sibling/peer label correlationthrough different Mixup ratios . labels containing lower-depth ones. Each text is then assigned toone or more labels within this hierarchy.The key challenge of HTC lies in effectively modeling the large-scale, imbalanced, and structured label hierarchy . Some exist-ing works consider the hierarchy as a directed acyclicgraph, utilizing structure encoders to obtain label representationsthat incorporate hierarchical information. Nevertheless, this strat-egy allows each text to share the entire static global hierarchy,introducing redundancy in the graph . This redundancy be-comes particularly noticeable as the hierarchy size increases, withnumerous labels becoming irrelevant to a specific given target text. In contrast, alternative approaches proposed in invokea local hierarchy, defined as a text-relevant sub-hierarchy extractedfrom the global hierarchy. (a-b) illustrate examples of thesetwo types of hierarchies. This local hierarchy can be represented asan inclusion relationship in the latent space, as depicted in (c). For example, Software and Machine Learning should occupythe same subspace within CS, while Geometry and Statisticsshould exist in a subspace within the category of Math. On top",
  "Conference acronym XX, August 0307, 2025, Woodstock, NYTrovato and Tobin, et al": "of that, by treating a local hierarchy as a sequence, language mod-els are able to capture parent-child relationships inherent in thehierarchy. Furthermore, motivated by the effectiveness of promptsin leveraging sequential information, in this work, we formulatethe local hierarchy as a sequence using a hierarchical template toencode and align its structure.It is worth mentioning that labels may hold inherent relevancethat extends beyond the constraints imposed by parent-child re-lationships in the hierarchy. (c) highlights a correlationbetween different local hierarchies. Specifically, while CS/MachineLearning and Math/Statistics occupy separate subspaces in thehierarchy, they may exhibit close proximity in the latent space,indicating a nuanced relationship that goes beyond the immediatehierarchical constraints. In fact, due to the widespread correlationamong labels in HTC, including both sibling relationships and peerrelationships , capturing the correlation between local hier-archies becomes crucial. Existing methods, however, lack explicitmechanisms to tackle this specific challenge. An avenue of promiselies in Mixup , a technique utilized for augmenting latent corre-lations between input pairs through the generation of intermediatesamples . Therefore, alongside hierarchical prompt tuning, weincorporate Mixup to reveal and leverage the implicit correlationbetween local hierarchies.More specifically, we first apply prompt tuning to HTC, wherethe local hierarchy is treated as a sequence. Virtual tokens areintroduced to represent each depth in the hierarchy, thereby disas-sembling the parent-child relationship and aligning the hierarchyby depth. Within this hierarchical prompt tuning framework, weincorporate Mixup to capture implicit correlations among siblingand peer relationships. Notably, unlike Vanilla Mixup, which in-terpolates between the input and corresponding target using thesame Mixup ratio sampled from a Beta Distribution, recent studies adopt Mixup ratios based on instance correlations. Thisapproach adjusts the mixture degree based on instance similar-ity, assigning higher mixture ratios to more similar instances togenerate more informative samples. Conversely, less similar exam-ples invoke a lower degree of mixture to prevent the generation ofout-of-distribution data . As shown in (c), CS/MachineLearning should have a higher degree of mixing (e.g., with a Mixupratio 0.6) with Math/Statistic, and a lower degree of mixing (e.g.,with a Mixup ratio 0.9) to Math/Geometry. Motivated by thisobservation, we propose a novel approach for Mixup ratio controlin our framework, dubbed local hierarchy Mixup (LH-Mix). In par-ticular, LH-Mix assesses the correlation of a local hierarchy pairusing the representation derived from the hierarchical prompt. Byincorporating a heuristic function to determine the correlation andMixup ratio, LH-Mix is able to learn hierarchical label correlationsmore effectively than vanilla Mixup.Below we summarize the main contributions of our work: We introduce LH-Mix, which integrates Mixup based on adepth-level hierarchical prompt, effectively modeling the lo-cal hierarchy within sequences. To the best of our knowledge,LH-Mix is the first application of Mixup in HTC.",
  "RELATED WORK2.1Hierarchical Text Classification (HTC)": "HTC refers to a specific type of multi-label text classification prob-lem, where an instance lies in one or more paths from a taxonomichierarchy in a top-down manner. The hierarchical structure is typi-cally represented as a tree or a directed acyclic graph, where theroot represents the highest-level label, and the leaf nodes corre-spond to the most specific labels. The key challenge in HTC liesin how to effectively utilize the large-scale, imbalanced, and struc-tured label hierarchy. To perform HTC, various techniques can beemployed. Previous works can be categorized into two types :global approach and local approach. The global approach tacklesthe HTC problem as a flat multi-label classification problem, em-ploying global hierarchy as input and building a single classifier forall labels. There are various strategies that incorporate the globalhierarchy, such as capsule network , reinforcement learning, meta-learning , contrastive learning , hyperbolic rep-resentation , or structure encoder . On the contrary, thelocal approach typically involves constructing individual classifiersfor each label , each parent node , or each level of the labelhierarchy .The current SOTA HTC model, HBGL , proposes hierarchy-guided BERT with both global and local hierarchies to utilize theprior knowledge of the pre-trained language model. The suboptimalmodel HPT transforms the global prediction to a local one byhierarchical prompt tuning. These successful applications of localhierarchy have served as inspiration for further investigation oflocal hierarchy in this paper.",
  "Mixup": "Mixup is a data augmentation method proposed by , whichaims to enhance the generalization capabilities of neural networksby generating in-between samples through linearly interpolatingpairs of input text and their corresponding labels. Vanilla Mixup in-terpolates input pairs by a mixing ratio sampling from a basic BetaDistribution. However, the mixing ratio obtained through a blindsampling process may not be optimal. To enhance performance,several studies have attempted to control the mixing ratio for pairs.AdaMixUp incorporates the automatic mixing policies fromthe data through the utilization of an additional network and anobjective function to prevent manifold intrusion. CAMixup adjusts the mixing ratio through the relation between the predictconfidence and accuracy. Remix assigns labels in a manner thatfavors the minority class by providing disproportionately higherweights to the minority class. Nonlinear Mixup incorporatesa nonlinear interpolation policy for both the input and label pairs,wherein the mixing policy for the labels is adaptively learned, lever-aging the information from the mixed input. Basically, Mixup couldenhance model performance by an adaptive mixing ratio that cap-tures deeper relationships within the latent space between samples.",
  "PRELIMINARIES3.1HTC Setting": "Given a training dataset {(,)}=1, where denotes the inputtext, is the corresponding label of , and denotes the sizeof training dataset. Let V be the label set. In the HTC setting,V is further divided into different subsets, denoted as V ={V1, ..., V, ..., V}, where is the total depth of the hierarchyand V is the label set of the -th depth. Importantly, V can beorganized into a tree structure, representing the global hierarchyof the entire dataset. In addition, each input contains multiplelabels from V, and these labels can form a separate subtree, knownas the local hierarchy. The goal of HTC is to assign labels from thelocal hierarchy in V to each text in the test dataset.",
  "Hierarchical Prompt for HTC": "Typically, a prompt contains a task-specific template with the textserving as input, exploiting the knowledge ingrained within thepre-trained language model . A vanilla prompt framework inHTC, as outlined in , involves the creation of soft tokenswithin the template, each specifically associated with a depth inthe hierarchy. For instance, the input can be formulated as:[CLS] [Dth1] [MASK] ... [Dth] [MASK] ... [Dth] [MASK] [SEP] [SEP]In particular, each token [Dth] serves to prompt the prediction ofthe associated label V within its -th layer hierarchy. This utilizesthe hidden output of the [MASK] token immediately following it.More precisely, let h[MASK] be the hidden output of [MASK] in the-th layer, the classification procedure is then defined as:",
  "p = V (h[MASK]),(1)": "where p R|V | is the prediction score vector, and V is a map-ping from hidden output to the prediction scores in the -th layer.Note that V contains a classifier trained through the MaskedLanguage Model task, along with a label words verbalizer that ad-heres to the label words of V. Notably, by using this hierarchicalprompt for HTC, the classification process is now partitioned acrosseach level of the hierarchy, rather than predicting all labels in asingle classifier. This approach encodes the local hierarchy as asequence, allowing for alignment of the hierarchy at each depthlevel for every input. Consequently, this hierarchical prompt designgreatly facilitates subsequent Mixup procedures. We will elaborateon this later.In the HTC task, many existing models treat the task as a multiple-label classification and invoke the conventional Binary Cross En-tropy (BCE) loss. Nevertheless, it has been pointed out that BCE lossignores the correlation between labels. A more effective alternative,as suggested in , is the Zero-bounded Multi-label Cross Entropy(ZMLCE). Particularly, the ZMLCE loss considers that all scores for",
  "p),": "(2)where V and V are the positive label set and negative labelset in the -th layer, respectively, and denotes the -th element ofa vector (e.g., p refers to the -th element of the prediction scorevector). In the inference phase, a prediction score greater than 0 isconsidered as positive; otherwise, it is considered as negative.",
  "LH-MIX FRAMEWORK": "Mixup, as introduced in , uses linear interpolation betweeninputs and their respective labels to generate in-between samples,thereby enriching the inherent structures within the latent space. In this work, we integrate Mixup into the hierarchical promptframework to better capture the correlations within the hierarchi-cally structured labels of HTC. In particular, we propose a noveladaptive Mixup ratio strategy, guided by the local hierarchy cor-relation, to further refine the vanilla Mixup scheme. We term thisenhanced approach LH-Mix. The framework of LH-Mix is shownin .",
  "Local Hierarchy Correlation": "The design of this hierarchical prompt scheme is motivated by thefact that the local hierarchy can be effectively represented as a se-quence. Specifically, an important characteristic of hierarchy is itsstrict hierarchical relationship, where the prediction of lower-levellabels relies on the prediction of higher-level labels. To achievethis characteristic, we introduce a depth token [Dth] to representthe depth level of each label. The local hierarchy can be reformu-lated to a sequence of tokens and this will be further translatedto a soft token as hierarchical prompts via a pre-trained languagemodel. We believe this soft prompt integrates both the level andthe label information, allowing the representation of hierarchies assequences. More specially, for a specific input , its local hierarchycan similarly be expressed as such a sequence by substituting the[MASK] token with the corresponding gold label. For example, thetwo local hierarchies CS/Machine Learning and Math/Statisticsin can be respectively represented as:[CLS][Dth1]CS[Dth2]Machine Learning[SEP][CLS][Dth1]Math[Dth2]Statistics[SEP]Consequently, we transform the local hierarchy into a sentence.In addition, when a local hierarchy contains multiple paths, weconcatenate the labels at the same level together and place themafter the corresponding [Dth] token.Furthermore, notice that by feeding a sentence into a pre-trainedlanguage model and extracting the hidden output of [CLS] token,one can obtain a sentence embedding that can be used to calcu-late sentence similarity . Similarly, in our context, the [CLS]output corresponding to the local hierarchy sequence serves asthe representation of the local hierarchy. This representation canbe effectively utilized to compute the local hierarchy correlation.Formally, consider two inputs and , let their local hierarchyrepresentations be denoted by h[CLS] and h[CLS], respectively.",
  "Local Hierarchy Correlation Guided MixupRatio": "Mixup captures sample correlation by generating in-between sam-ples via a Mixup ratio. The magnitude of the Mixup ratio can becontrolled to generate varying degrees of hard examples, therebydetermining the extent to which Mixup affects the data . demonstrate that examples with different levels of simi-larity should be mixed with varying intensities. Therefore, giventhe varying correlation among different local hierarchy pairs, it ismore appropriate in this context to apply different Mixup ratiosto each local hierarchy pair, rather than drawing the Mixup ratiofrom a fixed distribution (e.g., Beta distribution). While numerousstudies have explored the relationship between correlation and theMixup ratio, there is no well-established theoretical framework thatprecisely characterizes the numerical relationship between them.In fact, rigorously justifying the effectiveness of Mixup still remainsan open problem. Hence, to reflect the relationship between theMixup ratio and similarity, we design a heuristic function basedon intuition. The underlying intuition here is that as the similarity between two local hierarchies increases, we expect Mixup to bettercapture the latent correlation between them. Specifically, we expectMixup to have more impact (i.e., with a Mixup ratio approaching0.5) on highly correlated local hierarchies, whereas its impact is di-minished (i.e., with a Mixup ratio approaching 1) on less correlatedlocal hierarchies. Bear this in mind, to formulate the relationshipbetween local hierarchy similarity and Mixup ratio , we haveheuristically designed the following function:",
  "= ( 0.5) + ,(4)": "where > 0 controls the rate of change of with respect to ,and (0.5, 1] controls the upper bound of . This function hasthe advantage of covering various common linear or nonlinearrelationships between and by simply adjusting the values of and . To gain better understanding about Eq. 4, we visualize theeffects of and in . In particular, when = 1, there is alinear relationship between and . When < 1, decreases at aslower rate as increases. Conversely, when > 1, decreases ata faster rate as increases. Moreover, determines the maximumvalue of , signifying the minimum impact of Mixup.",
  "Local Hierarchy Mixup": "We now apply Mixup with the local hierarchy correlation guidedratio to both input and output simultaneously at each depth of thehierarchy.Regarding input Mixup, following the approach of previousMixup variants in text classification , we interpolate the hiddenoutput corresponding to the [MASK] token for an input pair at eachdepth of the hierarchy as follows:",
  "p = V (h[MASK]).(6)": "For output Mixup, the straightforward approach involves mix-ing the labels, as formulated in the vanilla Mixup technique .Alternatively, several studies have demonstrated that mixing thelosses is also a valid method. In particular, it has been establishedthat gradients for label mixing and loss mixing are equivalent in thecontext of cross-entropy loss . In the scenario of the ZMLCE loss,characterized by the division of positive and negative texts with0 as an anchor, all positive labels and negative labels are treatedas distinct combinations. While this design enables ZMLCE to fo-cus on correlations between labels, it neglects the consideration ofrelative magnitudes among positive labels or negative labels. Thisoversight can obscure the interpretation of label mixing, makingits meaning less explicit. Therefore, we opt to mix the loss terms inour LH-Mix, defined as:L (,p, V,, V,, V,, V,)",
  "(8)": "Clearly, Eq. 8 delivers a linear combination for the gradients ofthe mixed prediction scores corresponding to and . In addition,the vanilla Mixup, specifically in the case of cross-entropy, alsogenerates a linear combination of the gradients from the mixed prediction score. In this sense, the loss mixing for ZMLCE alignsmore closely with the vanilla Mixup. This also validates our decisionto adopt Eq. 7 for optimization, favoring loss mixing over labelmixing.",
  "EXPERIMENT5.1Datasets and Evaluation Metrics": "We evaluate our model on three widely used datasets: WebOfScience(WOS) , NYTimes (NYT) and RCV1-V2 . The statisticinformation is shown in . For evaluation metrics, we adoptMacro-F1 and Micro-F1 to measure the results following the previ-ous work . Micro-F1 considers the overall precisionand recall of all instances, while Macro-F1 represents the averageF1-score across labels.",
  "Implement Details": "Following the previous works, we exploit the pre-trained modelbert-base-uncased from Hugging Face Transformers 1 to eval-uate our model. For hierarchical prompting, the newly added tokens [Dth] are randomly initialized, and the label words ver-balizer embeddings are initialized by the average of the label namerepresentation. All parameters are fine-tuned by an Adam opti-mizer with the learning rate as 3e-5. For LH-Mix related parameters,as shows, we choose from [0.1, 0.3, 0.6, 1, 2, 5, 10] and from [0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]. This configuration is able toencompass the majority of common relationships between and .To accelerate the convergence of the model, we adopt a two-steptraining strategy. Initially, we train the model for 5 epochs withoututilizing Mixup, and then for the remaining epochs, we incorporateMixup during the training process. We employ the early stoppingstrategy if there is no improvement of Macro-F1 after 5 epochs. Allexperiments are conducted on a Tesla V100-SXM2-32GB GPU, witheach epoch taking approximately 1800s for training on mediumsize dataset NYT. For more details about training, we will releasecode our the Github 2.",
  ": Ablation study of different variants": "we compare our model with 4 strong baselines: TextRNN , Hi-AGM , HTCInfoMax , and HiMatch . In the large languagemodel, we generally report instruction-tuned results by ChatGPT and supervised fine-tuned results of LLaMA-2-7B . In thepre-trained language model, except for substituting the encoderas BERT of the 4 mentioned hierarchy-aware baselines, we alsoconsider 5 newly proposed models including HGCLR , HPT, HBGL , HJCL , and HiTIN . Among these baselinemodels, HPT and HBGL overall achieve state-of-the-art perfor-mance. For more details on these comparable models, please referto Appendix A.1.",
  "The main results of Micro-F1 and Macro-F1 of three datasets areshown in . As the Table shows, LH-Mix achieves the bestperformance in five out of the total six metrics, indicating the": "effectiveness of LH-Mix. To further evaluate the reproducibilityand significance of LH-Mix, we conduct experiments on statisticalperformance in Appendix A.3.With a detailed investigation of these results, we first observethat WOS shows a smaller improvement compared to NYT andRCV1-V2. We believe this is due to the smaller number of the depthof WOS, which results in easier classification. We then observe thatLH-Mix shows a larger improvement in Macro-F1. We believe thisis because Macro-F1 measures label-level F1, while Micro-F1 mea-sures more strict instance-level F1. LH-Mix primarily improves thehierarchical label correlation, thus leading to a greater improvementfor Macro-F1.Furthermore, comparing the HTC results of the instruction-tuned large language model reported in , which is based onChatGPT gpt-turbo-3.5, the results indicate that large modelsstill face significant challenges in encoding complex hierarchicalstructures. Comparing the results with the supervised fine-tuned",
  "Ablation Study": "To evaluate the importance of each variant in LH-Mix, we conducta series of ablation experiments, including the following varia-tions: BERT, which serves as the baseline model without any ad-ditional variant; Prompt, which introduces hierarchical templatesfor prompts tuning; +Mixup, which apply the vanilla Mixup withMixup ratio from conventional Beta Distribution based on BERTor Prompt; +LH-Mix, which further utilize hierarchy correlationguided Mixup ratio for +Mixup. The results are shown in .The results that Prompt is superior to the basic BERT model,demonstrating the effectiveness of hierarchical templates. Addition-ally, the performance of the +Mixup is better than its correspondingbasic BERT or Prompt model, indicating the utility of using Mixupfor encoding hierarchical label correlation. Furthermore, +LH-Mixoutperforms +Mixup, providing evidence for the capability of incor-porating hierarchical label correlation in Mixup ratio controlling.Additionally, we notice that numerous models treat the hierarchyas a graph and employ GNN-based models to generate structurallabel embedding for classification. Thus, we compare Prompt witha series of GNN-based graph models to evaluate the effect of thehierarchical prompt scheme in Appendix A.4.",
  ": Performance on different and , when fixing = 1and = 1 respectively": "are shown in . To save space, the results of WOS are shownin Appendix A.2. Results in the Figure indicate that +Mixup and+LH-Mix continue to perform better than Prompt with the decreas-ing of training samples. This also proves the effectiveness of Mixupmethods in hierarchy label correlation capturing on even sparserdatasets. Additionally, +LH-Mix is better than +Mixup generally,and as the dataset decreases extremely to 10%, the performancegap between +LH-Mix and +Mixup is enlarged. This observationdemonstrates the efficiency of the adaptive Mixup ratio guided bylocal hierarchy correlation.",
  "Parameter Analysis": "In LH-Mix, the parameters and are crucial for controlling therelationship between hierarchy correlation and Mixup ratio. Toinvestigate the impact of and , we fix = 1 and = 1, andanalyze the results as the other parameter varied. The results areshown in . Due to the relatively small changes of Micro-F1(as explained in .4), the trends may not accurately reflectthe final results. Therefore, we temporarily focus on the Macro-F1for consideration. 5.7.1Effect of . When = 1 is fixed, for NYT, the Macro-F1initially increases and then decreases with increasing . For RCV1-V2, the Macro-F1 gradually decreases. We speculate that this islikely due to differences in the statistical characteristics of thedatasets. This demonstrates that the relationship between and does indeed impact the effectiveness of LH-Mix. 5.7.2Effect of . Reviewing , as increases, there is ahigher likelihood for to be larger, indicating that LH-Mix is morelikely to have a minor effect. From , when = 1 is fixed,the results for both NYT and RCV1-V2 gradually decrease withincreasing . This indicates that a higher degree of LH-Mix leadsto better performance, confirming the effectiveness of the LH-Mix.",
  "(a) Related labels of Hockey(b) Visualization of label correlation": ": (a) Example of hierarchically related labels on Hockey. (b) The visualization of label correlations related to Hockey(the blue dot) before and after training by LH-Mix. We select the most similar 30 labels to Hockey for display and mark themwith orange scatters. Two shades of gray spheres construct the top 3 and 15 similar label spaces, respectively. Red dots indicatelabels that are not initially ranked within the top 3 but are included after training, while green dots represent labels that arenot initially ranked within the top 15 but are included after training. : Performance of comparable models on differentdepths of the hierarchy and different frequencies of labels.The texts in the Figure indicate the superior performance of+Mixup and +LH-Mix compared to Prompt.",
  "Variation of Label Correlation by LH-Mix": "To further analyze whether LH-Mix learns more accurate hierar-chical label correlation, we conduct a case study on label similarity.Specifically, we select the label Hockey as the target and computethe top 30 similar labels before and after learning with LH-Mix. Thevisualization of label correlation by t-SNE is shown in , marked by Initialize and LH-Mix.From the Figure, we first observe that the most related labelNHL has a higher similarity rank after learning (from top 15to top 3). Additionally, the rank of labels Baseball, Basketball,and NFL, which are also closely related to NHL, have improved(from top 30 to top 15). Noted, for conciseness and clarity, we onlyemphasize the labels with significant differences before and aftertraining. However, for labels with minor variations, such as NBA, MLB, and Football, we do not provide any special markings.This does not imply that these labels are irrelevant; rather, it isbecause these labels have consistently been learned effectively.These observations indicate better performance in hierarchicallabel correlation after learning with LH-Mix.",
  "Improvements of LH-Mix on Hierarchy": "To evaluate the performance of LH-Mix in different hierarchicalstructures, we analyze the performance of the Prompt, vanillaMixup (+Mixup), and LH-Mix (+LH-Mix) models on the NYT dataset,which contains the most complex hierarchical structure. Specifi-cally, the complexity of the hierarchical structure is reflected intwo aspects: the depth of the hierarchy and the distribution of labelfrequency. It is generally believed that the deeper the label hier-archy and the lower the label frequency, the more challenging itis to learn label representations, resulting in lower accuracy. Thecomparison of these models is shown in .From the Figure, we find that as the label hierarchy deepenedand the label frequency decreased, the improvement of +Mixupand +LH-Mix compared to the baseline Prompt is larger and larger.This demonstrates the effectiveness of Mixup-related methods inenhancing the learning performance of labels in complex structures.Furthermore, we observe that the improvement achieved by +LH-Mix is superior to that of +Mixup, indicating that our proposed LH-Mix method can further capture the hierarchical label correlation.",
  "CONCLUSION": "HTC is an important scenario within multi-label text classificationand has numerous applications. In this paper, we first propose ahierarchical template to model and align the local hierarchy in theprompt tuning framework. By employing this hierarchical prompttuning, we formulate the parent-child relationships explicitly andeffectively. Based on this, we employ Mixup to capture the implicitsibling/peer relationships under the latent label space. Especiallywe induce a local hierarchy correlation guided Mixup strategy toregulate the Mixup ratio for improved hierarchical label correla-tions, named LH-Mix. Extensive experiments on three widely-usedHTC datasets confirm the effectiveness of our model.",
  "Raphael Baena, Lucas Drumetz, and Vincent Gripon. 2022. Preventing manifoldintrusion with locality: Local mixup. arXiv preprint arXiv:2201.04368 (2022)": "Siddhartha Banerjee, Cem Akkaya, Francisco Perez-Sorrosal, and Kostas Tsiout-siouliklis. 2019. Hierarchical Transfer Learning for Multi-label Text Classification..In Proceedings of the 57th Annual Meeting of the Association for ComputationalLinguistics. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shot learners. Advances in neuralinformation processing systems 33 (2020), 18771901.",
  "Oscar Chang, Dung N Tran, and Kazuhito Koishida. 2021. Single-Channel SpeechEnhancement Using Learnable Loss Mixup.. In Interspeech. 26962700": "Boli Chen, Xin Huang, Lin Xiao, Zixin Cai, and Liping Jing. 2020. Hyperbolicinteraction model for hierarchical multi-label classification. In Proceedings of theAAAI conference on artificial intelligence. 74967503. Haibin Chen, Qianli Ma, Zhenxi Lin, and Jiangyue Yan. 2021. Hierarchy-awareLabel Semantics Matching Network for Hierarchical Text Classification. In Pro-ceedings of the 59th Annual Meeting of the Association for Computational Linguis-tics and the 11th International Joint Conference on Natural Language Processing.43704379. Jiaao Chen, Zichao Yang, and Diyi Yang. 2020. MixText: Linguistically-InformedInterpolation of Hidden Space for Semi-Supervised Text Classification. In Proceed-ings of the 58th Annual Meeting of the Association for Computational Linguistics.21472157. Hsin-Ping Chou, Shih-Chieh Chang, Jia-Yu Pan, Wei Wei, and Da-Cheng Juan.2020. Remix: rebalanced mixup. In Computer VisionECCV 2020 Workshops:Glasgow, UK, August 2328, 2020, Proceedings, Part VI 16. Springer, 95110. Zhongfen Deng, Hao Peng, Dongxiao He, Jianxin Li, and Philip Yu. 2021. HTCIn-foMax: A Global Model for Hierarchical Text Classification via InformationMaximization. In Proceedings of the 2021 Conference of the North American Chap-ter of the Association for Computational Linguistics: Human Language Technologies.32593265.",
  "Hongyu Guo, Yongyi Mao, and Richong Zhang. 2019. Mixup as locally linearout-of-manifold regularization. In Proceedings of the AAAI conference on artificialintelligence, Vol. 33. 37143722": "Jie He, Vctor Gutirrez-Basulto, Jeff Z Pan, et al. 2023. Instances and Labels:Hierarchy-aware Joint Supervised Contrastive Learning for Hierarchical Multi-Label Text Classification. arXiv preprint arXiv:2310.05128 (2023). SangHun Im, GiBaeg Kim, Heung-Seon Oh, Seongung Jo, and Dong Hwan Kim.2023. Hierarchical text classification as sub-hierarchy sequence generation. InProceedings of the AAAI Conference on Artificial Intelligence. 1293312941. Ting Jiang, Deqing Wang, Leilei Sun, Zhongzhi Chen, Fuzhen Zhuang, andQinghong Yang. 2022. Exploiting Global and Local Hierarchies for HierarchicalText Classification. In Proceedings of the 2022 Conference on Empirical Methods inNatural Language Processing. 40304039.",
  "Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-mization. arXiv preprint arXiv:1412.6980 (2014)": "Kamran Kowsari, Donald E Brown, Mojtaba Heidarysafa, Kiana Jafari Meimandi,Matthew S Gerber, and Laura E Barnes. 2017. Hdltex: Hierarchical deep learningfor text classification. In 2017 16th IEEE international conference on machinelearning and applications (ICMLA). IEEE, 364371. DavidD. Lewis, Yiming Yang, TonyG. Rose, and Fan Li. 2004. RCV1: A NewBenchmark Collection for Text Categorization Research. Journal of MachineLearning Research,Journal of Machine Learning Research (Dec 2004).",
  "Evan Sandhaus. 2008. The New York Times Annotated Corpus. Linguistic DataConsortium (Oct 2008)": "Ramit Sawhney, Megh Thakkar, Shrey Pandit, Ritesh Soun, Di Jin, Diyi Yang,and Lucie Flek. 2022. DMIX: Adaptive Distance-aware Interpolative Mixup.In Proceedings of the 60th Annual Meeting of the Association for ComputationalLinguistics (Volume 2: Short Papers). 606612. Kazuya Shimura, Jiyi Li, and Fumiyo Fukumoto. 2018. HFT-CNN: LearningHierarchical Category Structure for Multi-label Short Text Categorization.. InProceedings of the 2018 Conference on Empirical Methods in Natural LanguageProcessing.",
  "Carlos N Silla and Alex A Freitas. 2011. A survey of hierarchical classificationacross different application domains. Data mining and knowledge discovery 22(2011), 3172": "Junru Song, Feifei Wang, and Yang Yang. 2023. Peer-Label Assisted HierarchicalText Classification. In Proceedings of the 61st Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Papers). 37473758. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-AnneLachaux, Timothe Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, et al. 2023. Llama: Open and efficient foundation language models. arXivpreprint arXiv:2302.13971 (2023).",
  "Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.Journal of machine learning research 9, 11 (2008)": "Zihan Wang, Peiyi Wang, Lianzhe Huang, Xin Sun, and Houfeng Wang. 2022.Incorporating Hierarchy into Text Encoder: a Contrastive Learning Approach forHierarchical Text Classification. In Proceedings of the 60th Annual Meeting of theAssociation for Computational Linguistics (Volume 1: Long Papers). 71097119. Zihan Wang, Peiyi Wang, Tianyu Liu, Binghuai Lin, Yunbo Cao, Zhifang Sui, andHoufeng Wang. 2022. HPT: Hierarchy-aware prompt tuning for hierarchical textclassification. arXiv preprint arXiv:2204.13413 (2022). Jonatas Wehrmann, RodrigoC. Barros, and Ricardo Cerri. 2018. HierarchicalMulti-Label Classification Networks. International Conference on Machine Learn-ing,International Conference on Machine Learning (Jul 2018). Yeming Wen, Ghassen Jerfel, Rafael Muller, Michael W Dusenberry, Jasper Snoek,Balaji Lakshminarayanan, and Dustin Tran. 2020. Combining ensembles anddata augmentation can harm your calibration. arXiv preprint arXiv:2010.09875(2020). Jiawei Wu, Wenhan Xiong, and WilliamYang Wang. 2019. Learning to Learnand Predict: A Meta-Learning Approach for Multi-Label Classification. CornellUniversity - arXiv,Cornell University - arXiv (Sep 2019). Yuan Wu, Diana Inkpen, and Ahmed El-Roby. 2020. Dual mixup regularizedlearning for adversarial domain adaptation. In Computer VisionECCV 2020: 16thEuropean Conference, Proceedings, Part XXIX 16. 540555. Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, andYoungjoon Yoo. 2019. CutMix: Regularization Strategy to Train Strong Classifierswith Localizable Features. In International Conference on Computer Vision (ICCV).",
  "Hongyi Zhang, Moustapha Cisse, YannN. Dauphin, and David Lopez-Paz. 2017.mixup: Beyond Empirical Risk Minimization. arXiv,Learning (Oct 2017)": "Shaofeng Zhang, Meng Liu, Junchi Yan, Hengrui Zhang, Lingxiao Huang, Xi-aokang Yang, and Pinyan Lu. 2022. M-mix: Generating hard negatives via multi-sample mixing for contrastive learning. In Proceedings of the 28th ACM SIGKDDConference on Knowledge Discovery and Data Mining. 24612470. Yanzhao Zhang, Richong Zhang, Samuel Mensah, Xudong Liu, and Yongyi Mao.2022. Unsupervised sentence representation via contrastive learning with mixingnegatives. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36.1173011738. Jie Zhou, Chunping Ma, Dingkun Long, Guangwei Xu, Ning Ding, Haoyu Zhang,Pengjun Xie, and Gongshen Liu. 2020. Hierarchy-Aware Global Model for Hi-erarchical Text Classification. In Proceedings of the 58th Annual Meeting of theAssociation for Computational Linguistics. 11061117.",
  "AAPPENDIXA.1Details of Comparable Models": "TextRNN utilizes TextRNN to encode the input and treat HTC asa global text classification problem.HiAGM utilizes a directed graph hierarchy and hierarchy-aware structure encoders to model label dependencies. It proposesan end-to-end hierarchy-aware global model with two variants:HiAGM-LA, which learns hierarchy-aware label embeddings andperforms inductive fusion of label-aware text features, and HiAGM-TP, which directly feeds text features into hierarchy encoders.HTCInfoMax proposes an information maximization approachwith two modules: text-label mutual information maximizationand label prior matching. The first module captures interactionsbetween text and labels to filter irrelevant information, while thesecond enhances the structure encoders ability to represent alllabels, tackling label imbalance in HTC.HiMatch introduces a hierarchy-aware label semantics match-ing network to formulate the text-label semantics relationship. Itprojects text and label semantics into a joint embedding space andutilizes a joint embedding loss and a matching learning loss tomodel the matching relationship.HGCLR introduces hierarchy-guided contrastive learning,embedding the label hierarchy into the text encoder by constructingpositive samples based on the hierarchy for hierarchy-aware textrepresentation.HPT introduces hierarchy-aware prompt tuning for HTC usingmulti-label MLM. It employs dynamic virtual templates and softprompts with label words to integrate label hierarchy knowledge,alongside a zero-bounded multi-label cross-entropy loss to alignHTC and MLM objectives.HBGL introduces Hierarchy-guided BERT with global andlocal hierarchies, which leverages large-scale parameters and priorlanguage knowledge to model hierarchies. It directly models seman-tic and hierarchical information with BERT, avoiding the intentionalfusion of separate modules.HJCL introduces hierarchy-aware joint supervised contrastivelearning, which combines supervised contrastive learning withHTC. It utilizes instance-wise and label-wise contrastive learningtechniques and carefully constructs batches to achieve the con-trastive learning objective.HiTIN proposes hierarchy-aware tree isomorphism networkto enhance text representations using only label hierarchys syntac-tic information. It converts the label hierarchy into a coding treeguided by structural entropy and incorporates hierarchy-awareinformation into text representations through a structure encoder.ChatGPT represents utilizing instruction-tuned language models,such as ChatGPT. Results are directly reported from . Theymanually design templates and use gpt-turbo-3.5 for predictions.Noted, does not report the results for WOS and NYT, which isprobably because the flattened hierarchical labels in the templateoccupy most of the tokens. Texts and labels in WOS and NYT arerelatively long, making them hard to implement.LLaMA represents utilizing fine-tuned language models, such asLLaMA. We conduct classification training using the hidden layeroutput of the last token of the LLaMA-2-7B. Additionally, we useLoRA to efficiently fine-tune the LLM.",
  "A.3Statistical Performance of LH-Mix": "A.3.1Performance on Reproducibility. To ensure reproducibilityand robustness, we repeat the experiments five times using differ-ent random seeds for LH-Mix and comparable models. For reasonsof performance and code accessibility, we specially select the twoSOTA models HPT and HBGL for comparison. The average andstandard deviation results are reported in . From the Table,we find that all models exhibit relatively stable performance. More-over, LH-Mix consistently achieves optimal performance than othermodels overall. A.3.2T-test with SOTA Models. We conduct T-tests on LH-Mixcompared to the two SOTA models, HPT and HBGL. The resultingP-values are presented in . As per the conventional practice,a P-value less than 0.05 indicates statistical significance. From ouranalysis, we observe that LH-Mix exhibits a statistically significantimprovement over HPT in 5 out of the 6 metrics, and over HBGL in 4out of 6 metrics. Combining these findings with the results discussedin .4 of our paper, we conclude that the improvementachieved by LH-Mix is statistically significant.",
  "A.4Comparison with GNN-based Models": "We notice that numerous studies have suggested that graph en-coders can obtain better hierarchical label embeddings. However,graph encoders primarily focus on modeling the global hierarchy.As outlined in our introduction, we believe that capturing the lo-cal hierarchy can also provide sufficient information. We also em-pirically evaluate some representative graph encoders within ourframework. The results are shown in the . We find that thegain from the graph encoder is limited. Considering the extra timecost, we believe the hierarchical prompt is effective in capturinghierarchical information."
}