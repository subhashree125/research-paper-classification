{
  "ABSTRACT": "POI tagging aims to annotate a point of interest (POI) with someinformative tags, which facilitates many services related to POIs,including search, recommendation, and so on. Most of the existingsolutions neglect the significance of POI images and seldom fusethe textual and visual features of POIs, resulting in suboptimal tag-ging performance. In this paper, we propose a novel Multi-ModalModel for POI Tagging, namely M3PT, which achieves enhancedPOI tagging through fusing the target POIs textual and visualfeatures, and the precise matching between the multi-modal rep-resentations. Specifically, we first devise a domain-adaptive imageencoder (DIE) to obtain the image embeddings aligned to theirgold tags semantics. Then, in M3PTs text-image fusion module(TIF), the textual and visual representations are fully fused intothe POIs content embeddings for the subsequent matching. In ad-dition, we adopt a contrastive learning strategy to further bridgethe gap between the representations of different modalities. Toevaluate the tagging models performance, we have constructedtwo high-quality POI tagging datasets from the real-world busi-ness scenario of Ali Fliggy. Upon the datasets, we conducted theextensive experiments to demonstrate our models advantage overthe baselines of uni-modality and multi-modality, and verify theeffectiveness of important components in M3PT, including DIE, TIFand the contrastive learning strategy.",
  "The two corresponding authors have the same contribution": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 23, August 610, 2023, Long Beach, CA, USA 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00",
  "point of interest, POI tagging, multi-modality": "ACM Reference Format:Jingsong Yang, Guanzhou Han, Deqing Yang, Jingping Liu, Yanghua Xiao,Xiang Xu, Baohua Wu, and Shenghua Ni. 2023. M3PT: A Multi-Modal Modelfor POI Tagging. In Proceedings of the 29th ACM SIGKDD Conference onKnowledge Discovery and Data Mining (KDD 23), August 610, 2023, LongBeach, CA, USA. ACM, New York, NY, USA, 11 pages.",
  "INTRODUCTION": "A point of interest (POI) is a specific location that someones may feelhelpful or interesting, including a park, restaurant, shop, museum,and so on. In the last decades, various services related to POIs havebecome very popular on Web. POI tagging, i.e., annotating POIswith some informative tags (labels), which not only help users betterunderstand POIs characteristics, but also are useful to discover therelatedness or similarities between different POIs. As a result, POItagging can facilitate many downstream applications, such as POIsearch and recommendation .Many previous solutions achieve POI tagging based on textualdata but only have limited performance. These solutions generallyextract some features from the textual data relevant to a given POI,to infer the probability that each tag can be used to annotate thePOI. The textual data mainly includes users check-in logs ,POI taxonomy and descriptions . However, consideringonly textual data for POI tagging may suffer from the problemsof false positive (FP) and false negative (FN). FP refers to the factthat it might annotate the POI with the tags that are semanticallyrelated to the textual data but incorrect in fact. FN refers to the fact",
  ": A toy example to demonstrate the effects of incor-porating images into POI tagging": "that it might overlook the tags that are semantically dissimilar tothe textual data but correct in fact.We have found that there is sufficient visual data (such as im-ages) related to POIs on many real-world platforms, which is infact an essential supplement of the textual data, to solve the afore-mentioned problems. For example, displays not only thetextual data of a specific POI (Momi Cafe) including its name, de-scription and user comments, but also some images posted in thecomments. As shown in the figure, the FP and FN problems arealleviated or solved if the images are considered besides the tex-tual data. Specifically, the POI was wrongly labeled with /teatasting due to the comment ! / Come toMomi Xafe for drinking milk tea and reading book at night!. Whilethe images do not display the scene of tea tasting, hence helpingthe model correct such FP problem. Besides, the incorrect tag /teahouse, and /cat coffee could be filtered out since theirsemantics are not related to the images. For FN, the correct tag /Web celebrity photograph and /Web celebritycheck-in would be inferred as they have similar semantics to theimages, although they are less semantically similar to the texts. Thesignificance of POI images on discovering correct tags inspires usto leverage the textual and visual data simultaneously to achieveeffective POI tagging.A straightforward approach of leveraging images is to transfer anexisting multi-modal model (e.g., CLIP and BLIP ). However,this approachs effect is limited for the following reasons, especiallygiven that the tagging task we focus on in this paper is specific toa real-world tour scenario of Ali Fliggy1. 1) There is a gap betweenthe textual and visual representations in general, which wouldlikely lead to the mismatching between the POI and the gold tags.2) Leveraging the existing image pre-trained models (such as ViT) might result in unsatisfactory representations of POI images,since our task is a domain-specific tagging task.To address these problems, in this paper we propose a novelMulti-Modal Model for POI Tagging, namely M3PT, which is builtbased on a matching framework. Our model achieves enhancedtagging performance through the full fusion of the given POIstextual and visual features, and the precise matching between thePOIs multi-modal representation and the candidate tags represen-tation. Specifically, in M3PTs feature encoding module, we devisea Domain-adaptive Image Encoder, namely DIE, to obtain the em-beddings of POI images aligned to their gold tags semantics. Then,we design a Text-Image Fusion module, namely TIF, to fuse the",
  "It is a famous Chinese tour platform and its URL is": "textual and visual features of the POI. In TIF, we construct a clus-tering layer followed by an attention layer to distill the significantfeatures for generating the POIs content embedding. Furthermore,we adopt a contrastive learning strategy to refine the embeddingsof different modalities, and thus the cross-modality gap is bridged.In addition, the matching-based framework enables our M3PT toconveniently achieve the precise matching for a new tag, which isbeyond the premise in traditional multi-label classification, i.e., theclassification towards the close set of predefined labels.Our contributions in this paper are summarized as follows.1. To the best of our knowledge, this is the first work to exploit amulti-modal model incorporating the textual and visual semanticsto achieve the POI tagging on a real-world business platform. Tothis end, we propose a novel POI tagging model M3PT based on amulti-modal matching framework, to achieve the precise matchingbetween POIs and tags.2. We specially devise a domain-adaptive image encoder DIE inour model to obtain the optimal embedding for each input image,which is aligned to the semantics of the images gold tags. Theimage embeddings generated by DIE are better adaptive to therequirements of the real-world scenario, resulting in enhancedtagging performance.3. We have constructed two high-quality POI tagging datasetsfrom the real-world tour scenario of Ali Fliggy, upon which weevaluate the modelss tagging performance. Our extensive exper-iment results not only demonstrate our M3PTs advantage overthe previous models of uni-modality and multi-modality, but alsojustify the effectiveness of the important components in M3PT.",
  "RELATED WORK2.1POI Tagging Model": "Existing POI tagging solutions can be divided into three groups.The first group uses users check-in data of POIs as inputs, andextracts discriminative features to predict location labels. Krummet al. presented a set of manually designed features thatare extracted from the public place logs and individual visits. Someother methods leverage the features of user check-in activitiesand other behavior data to train the generative probabilistic modelsto infer POI tags. The authors in discussed morecomprehensive check-in data including POI unique identifiers, userunique identifiers, the number, time and duration of check-ins, thelatitude/longitude of user positions, as well as user demographicinformation. Label annotation for POIs was first studied in ,which introduces a collective classification method for feature ex-traction.The authors in studied how to select the most relevantfeatures for POI tagging. Yang et al. proposed an updatablesketching technique to learn compact sketches from user activitystreams and then used a KNN classifier to infer POI tags. Wang et al. proposed a graph embedding method to learn POI embeddingsfrom POI temporal bipartite graphs, which are then used by anSVM classifier of POIs labels.The second group includes the methods proposed in ,which also use more fine-grained information of POIs besides usercheck-in data. For example, includes general tags that may berelated to categories and other information, e.g., \"godzilla\". isthe first to use POI name and address tokens, that are obtained by",
  "'\"#": ": Our proposed M3PT consists of three modules. In the feature encoding module with a domain-adaptive image encoder(DIE), the textual and visual data of the given POI, and the candidate tag are encoded into the feature embeddings, respectively.Then, in the text-image fusion module (TIF), the text and image feature embeddings are fused into the POIs content embedding.At last, the final probability is computed in the multi-modal matching module based on the matching between the contentembedding and tag embedding. the pre-training on a domain and language-specific corpus. Lagos etal. found that POIs have several distinctive properties, includ-ing notably multiscript names, geospatial identities, and temporallydefined context. Thus they proposed an approach to complete POIsemantic tags in a crowdsourced database automatically.Similar to our work, the third group leverages image tagging, ofwhich many solutions use ViT as the backbone to accomplishmultiple image classification. Although many algorithms have beenproposed for automatic image annotation , image tag re-finement is treated as an independent problem, and has become anexciting issue .",
  "Vision-language Pre-trained Model": "Vision-language pre-training (VLP) aims to improve the perfor-mance of downstream vision and language tasks by pre-trainingmodels on large-scale image-text pairs. The pre-training tasks ofVLP models mainly include image-text contrastive learning andlanguage modeling (LM) based tasks. VirTex , ICMLM , andConVIRT have demonstrated the potential of Transformer-based LM, masked LM, and contrastive LM on learning image repre-sentations from the text. CLIP, BLIP , FILIP , ALIGNm and UNIMO mainly make use of cross-modal contrastivelearning which aligns the textual and visual information into aunified semantic space. VisualBERT , UNITER , and M6 employ LM-like objectives, including both masked LM (e.g., MaskedLanguage/Region Modeling) and auto-regressive LM (e.g., image captioning, text-grounded image generation). In addition, somemethods (such as BriVL ) rely on a pre-trained object detectionmodel, such as Faster-RCNN , to extract regional image fea-tures offline. It requires extra labeled bounding-box data and makesthe approach less scalable. Recent efforts such as SOHO andSimVLM try to alleviate this burden via visual dictionary orPrefixLM .",
  "OVERVIEW3.1Problem Formalization": "Given a POI , the task objective of our model is to filter out someappropriate tags from all candidate tags, to characterize . Expressly,in our scenario of Ali Fliggy, some textual contexts and images re-lated to are also provided to the model to achieve the taggingtask better. The textual contexts include the name, category, de-scription, and user comments of . The images include s mainpicture displayed on the platform and the pictures posted in usercomments. Formally, we denote all textual contexts of as ={1,2, . . . , }, and all related images as = {1, 2, . . . , }. Themodel should compute the following matching score (probability)between and a candidate tag ,",
  "As shown in , our proposed M3PT achieving POI taggingpipeline can be divided into the following three phases": "1. Feature Encoding. The candidate tag , the textual contexts, andimages of the given POI are encoded into the feature embeddingsin this first phase. The relevant operations are conducted in thefeature encoding module of our model, which consists of a textencoder and an image encoder. Specifically, we take BERTbase as the text encoder to encode into a tag embedding and encode into a set of text feature embeddings of simultaneously. Forthe image feature encoding of , we adopt a new image encoderDIE, which is specific to our tagging task in Fliggys domain, as wementioned in . We will introduce the details of the featureencoding module, including DIE, in .1. 2. Text-image Fusion. The operations in the second phase areconducted in the TIF module of our model. The TIF mainly consistsof two layers. The first layer is a clustering layer, through whichs text embeddings and image embeddings obtained from the firstphase are aggregated into a single textual representation and asingle visual representation of , respectively. Then, these tworepresentations are concatenated and fed into the attention layer ofTIF to generate s content embedding for the POI-tag matching inthe subsequent phase. As we emphasized in , the clusteringand attention operations in TIF are proposed to distill the significantfeatures to represent s content, whose details will be introducedin .2. 3. Multi-modal Matching. To conduct the operations in the thirdphase, we build a multi-modal matching module fed with s con-tent embedding and s tag embedding generated in the previousphases to output the final score ,. The main body of this moduleis also a BERTbase encoder, whose details will be introduced in.3.",
  "METHODOLOGY4.1Multi-modal Feature Encoding": "In the feature encoding module of our model, the multi-modalfeatures of (including textual and visual features) and s textualfeatures are first encoded into embeddings, respectively.Specifically, s textual inputs {1,2, . . . , } are fed into a textencoder constructed based on BERT to generate a set of s textembeddings. The text encoder is initialized with the first 6 layersof BERT. Formally, for an input text (1 ), we get itsfeature embedding as",
  "t = () R.(2)": "4.1.1Domain-adaptive Image Encoder. Given that our taggingtask is specific to the real-world scenario of Fliggy, we proposea new image encoder DIE to encode the input images of , i.e.,{1, 2, . . . , }, into a set of image feature embeddings, instead of using an existing image pre-trained model . To adapt tothe particular goal of our POI tagging task, the embedding of animage in our model should be learned concerning the tags with thesame semantics as it, i.e., its gold tags. It indicates that the learnedembedding of an image should help the model recognize its goldtags. To this end, unlike the traditional image encoders such as ViT, which only receive images as input, our DIE takes an image andthe text having the same semantics to constitute an input sample.Since DIE is also a pre-trained encoder, we propose a pretrainingmask task. Mask Learning. Specifically, given an image , we first compose asentence to indicate s semantics, denoted as , which includes agold tag of , denoted as . Then, we convert into by replacingone token in with a special token [MASK]. For example, if animage and its annotated tag =cup are obtained, we have",
  "= This is a cup, = This is a [MASK]": "Accordingly, we take (, ) as the input of the mask learningtask. As displayed in , and are fed into a ViT encoderand a BERT in DIE, to generate s embedding v R ands embedding x R, respectively. Then, based on these twoembeddings, DIE computes the following probability distributionindicating which token is on the position of [MASK] as,",
  "y, = (v, x),(3)": "where () represents all calculations in the prediction layer ofDIE. Specifically, y, is a vector of dimensions and is the sizeof token vocabulary. Each entry of y, is the probability that thecorresponding token is on the position of [MASK]. To compute thisprobability distribution more precisely, v is first refined in termsof x through the cross-attention operations as . Then y, iscomputed based on the refined v through a fully-connected layerand softmax operation. At last, a token with the largest probabilityin y, is predicted as on the position of [MASK].To pretrain DIE, we formulate the mask learnings loss as",
  "Texp vt/ ,(5)": "where is any one tag in s paired tag set T including its goldtag , and t R is s embedding obtained from the BERTencoder. In addition, is the temperature used to maintain thebalance between the alignment and consistency of the contrastedsamples . To train the model better in contrastive learning, wetend to select the incorrect tags but semantically similar (havingclose embeddings) to the image as hard negative samples. Accord-ingly, a small makes the model focus on discriminating the hardnegative samples, resulting in better performance.",
  "(7)where , and , are both the real label indicating the matchingbetween and": "Image-tag Matching Learning. Furthermore, to better achievethe matching between images and tags, we append an image-tagmatching loss for DIEs pretraining. We use , to denote thematching probability between and. As the operations in Equation3, v is first refined in terms of t through cross-attention and thenused to compute ,. Then, the matching loss is",
  "Text-image Fusion": "The operations in this phase are conducted in the TIF module,where the textual and visual embeddings of are fused into acondensed representation, namely content embedding, to achievethe subsequent matching conveniently. 4.2.1Clustering Layer. The first step in TIF is to aggregate themultiple embeddings from either modality into a single embedding.To this end, we first build a clustering layer in TIF to perform thefollowing operations.For an image s feature embedding v = [1, 2, . . . , ] obtainedby Equation 10, each (1 ) can be regarded as a frame-level descriptor of . We used a clustering algorithm the same asNeXtVLAD to cluster the frame-level descriptors of into groups. Suppose the -th clusters centroid is (1 ), werefine in terms of as",
  "= =1 ,, 1 , 1 ,(13)": "where ,is the -th refined frame-level descriptor of the -thimage embedding obtained by Equation 11. Accordingly, we have aggregated descriptors in total, which are then reduced intoa single visual representation of dimensions, denoted as v R ,through a fully-connected layer.To aggregate the text embeddings generated in the previousphase into a single textual representation, denoted as x R , weadopt the same operations introduced above for generating thesingle visual representation. 4.2.2Attention Layer. Next, we input the concatenation of x andv into the attention layer in TIF. This layer reassigns the featureweights in the input through the attention and reshape operationsand outputs an embedding of dimensions. Thus, the more sig-nificant features are highlighted in the output, which is just scontent embedding, denoted as c R. Overall, c condenses thesignificant multi-modal features of .",
  "Mutil-modal Matching": "The objective of the multi-model matching module in the thirdphase is to achieve the precise matching between and , based ons content embedding c and s tag embedding t.In fact, the architecture of this module is similar to the predic-tion layer in DIE since both of their objectives are to achieve thematching between two objects. Specifically, the candidate tags em-bedding t is refined in terms of s content embedding c through thecross-attention operations and then used to compute , througha fully-connected layer and softmax operations. As last, M3PT pre-dicts that is a correct tag of if , > .",
  "(, )N (,, ,),(14)": "where N is the training set consisting of POI-tag pairs.To improve the alignment of s content embedding and s tagembedding, we also append a contrastive learning loss the sameas Equation 57 for our model training, except that v is replacedwith c when computing (,) and (, ). Thus, this contrastivelearning loss is",
  "EXPERIMENTS5.1Dataset Construction": "We have constructed the following two datasets for our evaluationexperiments, of which the detailed statistics are listed in .MPTD1: The first dataset, named MPTD1 (Multi-modal POI Tag-ging Dataset), was constructed directly from the real-world tourscenario of Fliggy, including more than 60,000 POIs, 354 uniquetags, and more than 190,000 POI-tag pairs. For each POI in thisdataset, its related textual contexts (texts in short) include its fullname, introduction, categories, and user comments collected fromFliggys website. Each POIs related images include the main pictureshown on the top of its introduction page, as well as the picturesposted in its user comments. We first collected the original tags foreach POI in some ways, including basic rules, manual selections,semantic-based algorithms, etc. When sufficient original tags hadbeen collected, the experts verified and refined them based on theiractual effects on Fliggys tour platform. At last, the reserved tagswere identified as the gold (gound-truth) tags for these POIs. AllPOIs were divided into the training set, validation set, and test setaccording to 8:1:1. In each set, besides the POIs, their gold tags,texts, and images were also included.MPTD2: Although the texts and images of each POI in MPTD1 aresufficient, the model training is time-consuming if we feed all ofthem into the model. In addition, most of them are not semanticallyrelated to the POIs tags. Thus, to compare all models capabilities ofleveraging a POIs textual and visual features that are semanticallyrelated to its gold tags more efficiently, we further constructed amore concise dataset MPTD2. We randomly selected about one-tenth of the POIs in MPTD1, together with their tags, texts, andimages. For each selected POI, we recruited some volunteers tocheck its texts and images and only retain those with similar se-mantics to its tags. For example, only the comments that directlymention the tags or are highly semantically related to the tags wereretained. Similarly, only the images verified as semantically relatedto the POIs tags were retained. As a result, the texts and imagesretained in MPTD2 can be directly leveraged as pieces of evidenceto judge the POIs matching to its tags. The ratio of the training set,validation set, and test set in MPTD2 is also 8:1:1.",
  "Experimental Setup": "5.2.1Baselines. We compared our M3PT with the following mod-els in our experiments, which can be categorized into two groupsof uni-modal models and one group of multi-modal models.The baselines in the first group only leverage the textual featuresof POIs to achieve POI tagging. In these models, a given POIs textsare fed into their encoders to generate the textual embedding ofthe POI, which is used as the POIs content embedding. This groupincludes BERT , ALBERT and ERNIE . The first twoare both classic pre-trained language models. ERNIE incorporates knowledge maps into the pretraining task to improve its repre-sentation capability, which has been successfully employed in theprevious multi-label classification task.The baselines in the second group are also uni-modal models,which only leverage the POIs images to achieve POI tagging. Specif-ically, only the images of a given POI are input into the modelsto generate the visual feature embedding used as its content em-bedding. This group includes the ResNet-based models includingResNet-101 , ResNet101(ASL) and TResNet . All ofthem have been widely used to encode images in the computervision (CV) community. In addition, we also considered ViT andViT-Q2L . Inspired by the success of Transformer in natu-ral language processing (NLP), these two models were built basedon the architecture of self-attention and cross-attention, showingtheir advantages over the image encoders based on convolutionalneural networks (CNNs).As M3PT, the baselines in the third group are both multi-modalmodels, including M3TR and TAILOR . M3TR employsthe vision Transformer and builds a cross-modal attention moduleand a label-guided augmentation module to achieve multi-modalmulti-label classification better. TAILOR first encodes the uni-modalfeatures of texts and images and then fuses them into the cross-modal features as the encoder inputs to model the relationshipbetween the modality and each label. 5.2.2Evaluation Metrics. Our POI tagging task is just a multi-labelclassification if we regard the set of all tags as the label set of the POIclassification. So we used the following metrics to evaluate all com-pared models performance in our experiments. We first consideredthe label-based classification metric M-P (Macro Precision), M-R(Macro Recall) and M-F1 (Macro F1) . When computing theirscores, we first took a tag along with all POIs annotated by it as onesample, and reported the average score of all samples. Second, wealso considered the object-based classification metrics, includingP-e (Precision-exam), R-e (Recall-exam), and F1-e (F1-exam) .When computing three metrics scores, we took a POI along withall of its tags as one sample, and reported the average score of allsamples. In addition, we also considered HLS (Hamming Loss) ,which is used to measure the misclassification samples on a singlelabel (tag). Thus, a more miniature HLS score indicates the modelsbetter performance. 5.2.3Hyperparameter Settings. We adopted AdamW as theoptimizer Moreover, we set the initial learning rate to 1e-4 forpre-training. With steps increasing, we decreased the learning ratelinearly to 1e-5. The settings of some crucial hyperparameters in",
  "Main Results": "The following performance scores of all compared models are re-ported as the average results of three runnings to alleviate the biasof single running. lists the overall tagging performance ofall models on MPTD1 and MPTD2 datasets, where the best andsecond-best scores in each group are in bold and underlined, respec-tively. In particular, to verify our M3PTs capability of leveraginguni-modal features, we further proposed two ablated variants ofM3PT. Wherein M3PT(text) only leverages the textual features, i.e.,it only inputs the text embeddings into the cluster layer of TIFand directly uses the single textual representation x as the POIscontent embedding c. Similarly, M3PT(image) only inputs imageembeddings into TIF to generate the content embedding. M3PTsperformance improvement ratio relative to the best baseline (un-derlined) is also listed in each group. The results demonstrate thatM3PT performs the best on most metrics. Moreover, we also havethe following observations and analyses.1. Compared with the first group of textual modality and thethird group of multi-modality, M3PT is not the best on some metrics.Through the investigation of the two datasets, we found that theimages of a POI are semantically related to only about one-fifthof the POIs tags on average. It implies that M3PT can not exertits advantage sufficiently when only leveraging these images todiscover the correct tags.2. Compared with MPTD1, M3PTs performance improvementover the baselines on MPTD2 is more apparent. It is because thePOIs texts and images in MPTD2 are more related to the tags, helping M3PT discover the matching across different modalities.Consequently, our model can achieve more precise POI taggingon MPTD2. It implies that our model is better at exploiting thehigh-quality dataset.",
  "Detailed Analysis": "5.4.1Ablation Studies. We have conducted some ablation stud-ies to justify the effects of the essential modules and strategy wedesigned in M3PT, including incorporating POI images, TIF, andthe contrastive learning of POI-tag (PTC). lists the resultsof different ablated variants of M3PT on MPTD2 and MPTD1, re-spectively, where the variant without image is just M3PT(text) in. In addition, in the variant without TIF, the single textualrepresentation x is just the sum of text embeddings. Similarly,the single visual representation v is just the sum of image em-beddings. Then, these two representations are concatenated andreshaped into s content embedding c.Besides the metric scores, the performance drop ratios of allvariants relative to M3PT are also listed in . The resultsshow that incorporating either one component of PTC, TIF, andimage is helpful to M3PTs performance improvement. Comparedwith TIF and image, PTC is more helpful for M3PT to achieve precisePOI tagging since the performance drop of the variant without PTCis the most obvious. 5.4.2Impact of Domain-adaptive Image Encoder. As mentionedbefore, the DIE specially designed in M3PT is an image encodermore adaptive to the requirements of the real-world scenario ofFliggy, resulting in M3PTs enhanced performance. To verify it, wefurther propose some variants of M3PT by replacing the DIE withsome previous image encoders, including ViT , CLIP andBLIP .Due to space limitation, we only report the compared modelsscores of mAP, M-F1, and F1-e in since they are the most rep-resentative metric of the ranking metrics, label-based classification",
  ": The precision of top-3/5 tags for the POIs on onMPTD2 predicted by the M3PTs with different image en-coders": "metrics, and object-based classification metrics. The results showthat, with DIE, our M3PT performs better on both classification andranking the correct tags. In addition, to concretely exhibit DIEsadvantage in ranking the gold (correct) tags, in , we displaythe precision of top-3/5 tags predicted for the POIs on MPTD2. Ob-viously, all models precision of top-5 is higher than that of top-3.Nevertheless, DIEs precision gap between top-3 and top-5 is muchnarrower than the other image encoders. Furthermore, in the tableof , we list the top-3 tags predicted by the models for twoPOIs from MPTD2, where the incorrect tags are marked red. It alsojustifies that the M3PT with DIE can predict a bigger probabilityfor the correct tags than the baselines.",
  ": Top-3 tags predicted by the models for two POIs onMPTD2 (better viewed in color)": "5.4.3Performance on Ranking Correct Tags. We also evaluatedM3PTs performance on the three ranking metrics. However, due tospace limitation, we only compared it with the baseline performingthe best in each group according to . The comparison resultsare listed in , showing that our model still has an advantagein ranking the correct tags on higher positions. 5.4.4Hyperparameter Tuning. For the important hyperparametersin M3PT, we have studied their impacts on the performance of POItagging. Due to space limitation, we only display the results of 1and .We first focus on the impact of the temperature in (,) inEquation 15, i.e., 1. As we mentioned before, the temperature inthe scoring function of contrastive learning is used to maintainthe balance between the alignment and consistency of contrastedsamples. The results depicted in show that M3PT performs",
  ": The tuning results of prediction threshold on thetwo datasets": "better when1 = 0.08 0.12. It indicates that a suitable temperatureshould not be too big. Otherwise, it would make the model lessfocused on discriminating the hard negative samples and thus failto obtain satisfactory performance.We also display the tuning results of the prediction threshold ()in . It shows that = 0.5 is the best setting, which just equalsthe default threshold in generic binary classifications.",
  "Case Study": "We further display the tagging results of M3PT and the best baselinein each group for a specific POI /LongquanMountain Urban Forest Park from MPTD2. The top-5 tags predictedby the compared models are listed in , where the incorrecttags are marked red, and the green tags were assessed as correct butnot in this POIs gold tag set. Moreover, the rest black tags are gold",
  "Top-5 predicted tags": "POI name: Longquan Mountain Urban Forest ParkComment 1: Saw a pair of huge fire balloons when close to observation deck.Comment 2: Good scenery, a good place for hiking.Comment 3: Traveled Wufeng ancient town when in Longquan Mountain. : The top-5 tags predicted by the models for a POIfrom MPTD2. The incorrect tags are marked red, the greentags were assessed as correct but not in the POIs gold tag set.And the rest tags are its gold tags (better viewed in color). Itshows that M3PT predicts more gold tags. tags. At the same time, we display some texts and images of thisPOI on the left, which were fed into the models for tag prediction.Obviously, the incorrect tag /fire balloon and /ancienttown were predicted by ERNIR and TAILOR, due to the misleadingof Comments 1 and 3. While the incorrect tag /dabble and/tea garden were predicted by ViT and TAILOR, due to themisleading of some images. In fact, these incorrect tags only havethe similar semantics to uni-modal (either textual or visual) data.They can be filtered out through the alignment (matching) of cross-modality. Compared with the baselines, our M3PT achieves thefull fusion and precise matching between the textual and visualfeatures, and are less disturbed by the noise of each modal data.Consequently, all of the 5 tags predicted by M3PT are correct.",
  "CONCLUSION": "Towards the POI tagging in the real-world tour scenario of AliFliggy, in this paper, we propose a novel multi-modal model, namelyM3PT, which incorporates the textual and visual features of POIssimultaneously to achieve the tagging task. In M3PT, we speciallydevise a domain-adaptive image encoder (DIE) to generate theimage embeddings to better adapt to the requirements of the real-world scenario. In addition, we build the text-image fusion module(TIF) in our model, to achieve the full fusion and precise matchingbetween textual and visual features. We further adopt a contrastivelearning strategy to bridge the gap among the corss-modal rep-resentations in the model. Our extensive experiments with twodatasets that were constructed from the Fliggy platform, not onlydemonstrate M3PTs advantage, but also justify the rationalitiesand effectiveness of its important components. This work was supported by Alibaba Group through Alibaba In-novative Research Program, Chinese NSF Major Research Plan(No.92270121), Shanghai Science and Technology Innovation Ac-tion Plan (No.21511100401), and Shanghai Sailing Program (No.23YF1409400).",
  "Kobus Barnard, Pinar Duygulu, David A. Forsyth, Nando de Freitas, David M.Blei, and Michael I. Jordan. 2003. Matching Words and Pictures. J. Mach. Learn.Res. 3 (2003), 11071135": "Emanuel Ben-Baruch, Tal Ridnik, Nadav Zamir, Asaf Noy, Itamar Friedman,Matan Protter, and Lihi Zelnik-Manor. 2020. Asymmetric loss for multi-labelclassification. arXiv preprint arXiv:2009.14119 (2020). Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan,Yu Cheng, and Jingjing Liu. 2020. UNITER: UNiversal Image-TExt RepresentationLearning. In Computer Vision - ECCV 2020 - 16th European Conference, Glasgow,UK, August 23-28, 2020, Proceedings, Part XXX (Lecture Notes in Computer Science,Vol. 12375), Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm(Eds.). Springer, 104120.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:Pre-training of deep bidirectional transformers for language understanding. arXivpreprint arXiv:1810.04805 (2018)": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, GeorgHeigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image isWorth 16x16 Words: Transformers for Image Recognition at Scale. In 9th Interna-tional Conference on Learning Representations, ICLR 2021, Virtual Event, Austria,May 3-7, 2021. OpenReview.net. Zheyun Feng, Songhe Feng, Rong Jin, and Anil K. Jain. 2014. Image Tag Comple-tion by Noisy Matrix Recovery. In Computer Vision - ECCV 2014 - 13th EuropeanConference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VII (LectureNotes in Computer Science, Vol. 8695), David J. Fleet, Toms Pajdla, Bernt Schiele,and Tinne Tuytelaars (Eds.). Springer, 424438. Liming Gao, Dongliang Liao, Gongfu Li, Jin Xu, and Hankz Hankui Zhuo. 2022.Semantic IR fused Heterogeneous Graph Model in Tag-based Video Search. InCompanion of The Web Conference 2022, Virtual Event / Lyon, France, April 25 -29, 2022, Frdrique Laforest, Raphal Troncy, Elena Simperl, Deepak Agarwal,Aristides Gionis, Ivan Herman, and Lionel Mdini (Eds.). ACM, 9498. Shantanu Godbole and Sunita Sarawagi. 2004. Discriminative Methods for Multi-labeled Classification. In Advances in Knowledge Discovery and Data Mining, 8thPacific-Asia Conference, PAKDD 2004, Sydney, Australia, May 26-28, 2004, Proceed-ings (Lecture Notes in Computer Science, Vol. 3056), Honghua Dai, RamakrishnanSrikant, and Chengqi Zhang (Eds.). Springer, 2230.",
  "Kingshy Goh, Edward Y. Chang, and Beitao Li. 2005. Using One-Class and Two-Class SVMs for Multiclass Image Annotation. IEEE Trans. Knowl. Data Eng. 17,10 (2005), 13331346": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep ResidualLearning for Image Recognition. In 2016 IEEE Conference on Computer Visionand Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016. IEEEComputer Society, 770778. Tieke He, Hongzhi Yin, Zhenyu Chen, Xiaofang Zhou, Shazia W. Sadiq, andBin Luo. 2016. A Spatial-Temporal Topic Model for the Semantic Annotation ofPOIs in LBSNs. ACM Trans. Intell. Syst. Technol. 8, 1 (2016), 12:112:24. Vinod Hegde, Josiane Xavier Parreira, and Manfred Hauswirth. 2013. SemanticTagging of Places Based on User Interest Profiles from Online Social Networks. InAdvances in Information Retrieval - 35th European Conference on IR Research, ECIR2013, Moscow, Russia, March 24-27, 2013. Proceedings (Lecture Notes in ComputerScience, Vol. 7814), Pavel Serdyukov, Pavel Braslavski, Sergei O. Kuznetsov, JaapKamps, Stefan M. Rger, Eugene Agichtein, Ilya Segalovich, and Emine Yilmaz(Eds.). Springer, 218229. Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, andJianlong Fu. 2021. Seeing Out of the Box: End-to-End Pre-Training for Vision-Language Representation Learning. In IEEE Conference on Computer Vision andPattern Recognition, CVPR 2021, virtual, June 19-25, 2021. Computer Vision Foun-dation / IEEE, 1297612985. Yuqi Huo, Manli Zhang, Guangzhen Liu, Haoyu Lu, Yizhao Gao, Guoxing Yang,Jingyuan Wen, Heng Zhang, Baogui Xu, Weihao Zheng, Zongzheng Xi, YueqianYang, Anwen Hu, Jinming Zhao, Ruichen Li, Yida Zhao, Liang Zhang, YuqingSong, Xin Hong, Wanqing Cui, Dan Yang Hou, Yingyan Li, Junyi Li, Peiyu Liu,Zheng Gong, Chuhao Jin, Yuchong Sun, Shizhe Chen, Zhiwu Lu, Zhicheng Dou,Qin Jin, Yanyan Lan, Wayne Xin Zhao, Ruihua Song, and Ji-Rong Wen. 2021.WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training.CoRR abs/2103.06561 (2021). arXiv:2103.06561 Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham,Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021.Scaling UpVisual and Vision-Language Representation Learning With Noisy Text Super-vision. In Proceedings of the 38th International Conference on Machine Learn-ing, ICML 2021, 18-24 July 2021, Virtual Event (Proceedings of Machine LearningResearch, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, 49044916. Yvonne Kammerer, Rowan Nairn, Peter Pirolli, and Ed Huai-hsin Chi. 2009.Signpost from the masses: learning effects in an exploratory social tag searchbrowser. In Proceedings of the 27th International Conference on Human Factors inComputing Systems, CHI 2009, Boston, MA, USA, April 4-9, 2009, Dan R. Olsen Jr.,Richard B. Arthur, Ken Hinckley, Meredith Ringel Morris, Scott E. Hudson, andSaul Greenberg (Eds.). ACM, 625634. John Krumm and Dany Rouhana. 2013. Placer: semantic place labels from diarydata. In The 2013 ACM International Joint Conference on Pervasive and UbiquitousComputing, UbiComp 13, Zurich, Switzerland, September 8-12, 2013, FriedemannMattern, Silvia Santini, John F. Canny, Marc Langheinrich, and Jun Rekimoto(Eds.). ACM, 163172. John Krumm, Dany Rouhana, and Ming-Wei Chang. 2015. Placer++: Semanticplace labels beyond the visit. In 2015 IEEE International Conference on PervasiveComputing and Communications, PerCom 2015, St. Louis, MO, USA, 23-27 March,2015. IEEE Computer Society, 1119. Nikolaos Lagos, Salah Ait-Mokhtar, and Ioan Calapodescu. 2020. Point-Of-InterestSemantic Tag Completion in a Global Crowdsourced Search-and-DiscoveryDatabase. In ECAI 2020 - 24th European Conference on Artificial Intelligence,29 August-8 September 2020, Santiago de Compostela, Spain, August 29 - Sep-tember 8, 2020 - Including 10th Conference on Prestigious Applications of Artifi-cial Intelligence (PAIS 2020) (Frontiers in Artificial Intelligence and Applications,Vol. 325), Giuseppe De Giacomo, Alejandro Catal, Bistra Dilkina, Michela Milano,Senn Barro, Alberto Bugarn, and Jrme Lang (Eds.). IOS Press, 29933000. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, PiyushSharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learningof language representations. arXiv preprint arXiv:1909.11942 (2019). Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. 2022.BLIP:Bootstrapping Language-Image Pre-training for Unified Vision-Language Un-derstanding and Generation. In International Conference on Machine Learning,ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA (Proceedings of MachineLearning Research, Vol. 162), Kamalika Chaudhuri, Stefanie Jegelka, Le Song,Csaba Szepesvri, Gang Niu, and Sivan Sabato (Eds.). PMLR, 1288812900.",
  "Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang.2019. VisualBERT: A Simple and Performant Baseline for Vision and Language.CoRR abs/1908.03557 (2019). arXiv:1908.03557": "Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu,and Haifeng Wang. 2021. UNIMO: Towards Unified-Modal Understanding andGeneration via Cross-Modal Contrastive Learning. In Proceedings of the 59thAnnual Meeting of the Association for Computational Linguistics and the 11thInternational Joint Conference on Natural Language Processing, ACL/IJCNLP 2021,(Volume 1: Long Papers), Virtual Event, August 1-6, 2021, Chengqing Zong, Fei Xia,Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics,25922607.",
  "Xue Li, Bin Shen, Bao-Di Liu, and Yu-Jin Zhang. 2016. A Locality SensitiveLow-Rank Model for Image Tag Completion. IEEE Trans. Multim. 18, 3 (2016),474483": "Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang, PengWang, Ang Wang, Le Jiang, Xianyan Jia, Jie Zhang, Jianwei Zhang, Xu Zou,Zhikang Li, Xiaodong Deng, Jie Liu, Jinbao Xue, Huiling Zhou, Jianxin Ma, JinYu, Yong Li, Wei Lin, Jingren Zhou, Jie Tang, and Hongxia Yang. 2021. M6: AChinese Multimodal Pretrainer. CoRR abs/2103.00823 (2021). arXiv:2103.00823 Rongcheng Lin, Jing Xiao, and Jianping Fan. 2018. Nextvlad: An efficient neuralnetwork to aggregate frame-level features for large-scale video classification. InProceedings of the European Conference on Computer Vision (ECCV) Workshops.00. Zijia Lin, Guiguang Ding, Mingqing Hu, Jianmin Wang, and Xiaojun Ye. 2013.Image Tag Completion via Image-Specific and Tag-Specific Linear Sparse Re-constructions. In 2013 IEEE Conference on Computer Vision and Pattern Recog-nition, Portland, OR, USA, June 23-28, 2013. IEEE Computer Society, 16181625.",
  "M3PT: A Multi-Modal Model for POI TaggingKDD 23, August 610, 2023, Long Beach, CA, USA": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual ModelsFrom Natural Language Supervision. In Proceedings of the 38th InternationalConference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event (Pro-ceedings of Machine Learning Research, Vol. 139), Marina Meila and Tong Zhang(Eds.). PMLR, 87488763. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring theLimits of Transfer Learning with a Unified Text-to-Text Transformer. J. Mach.Learn. Res. 21 (2020), 140:1140:67. Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. 2015. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.In Advances in Neural Information Processing Systems 28: Annual Conference onNeural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec,Canada, Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama,and Roman Garnett (Eds.). 9199. Tal Ridnik, Hussam Lawen, Asaf Noy, Emanuel Ben Baruch, Gilad Sharir, andItamar Friedman. 2021. TResNet: High Performance GPU-Dedicated Architec-ture. In IEEE Winter Conference on Applications of Computer Vision, WACV 2021,Waikoloa, HI, USA, January 3-8, 2021. IEEE, 13991408.",
  "Mert Blent Sariyildiz, Julien Perez, and Diane Larlus. 2020.Learning Vi-sual Representations with Caption Annotations. CoRR abs/2008.01392 (2020).arXiv:2008.01392": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processing systems 30 (2017). Feng Wang and Huaping Liu. 2021. Understanding the Behaviour of ContrastiveLoss. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021,virtual, June 19-25, 2021. Computer Vision Foundation / IEEE, 24952504. Yan Wang, Zongxu Qin, Jun Pang, Yang Zhang, and Jin Xin. 2017. SemanticAnnotation for Places in LBSN through Graph Embedding. In Proceedings ofthe 2017 ACM on Conference on Information and Knowledge Management, CIKM2017, Singapore, November 06 - 10, 2017, Ee-Peng Lim, Marianne Winslett, MarkSanderson, Ada Wai-Chee Fu, Jimeng Sun, J. Shane Culpepper, Eric Lo, Joyce C.Ho, Debora Donato, Rakesh Agrawal, Yu Zheng, Carlos Castillo, Aixin Sun,Vincent S. Tseng, and Chenliang Li (Eds.). ACM, 23432346. Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and YuanCao. 2022. SimVLM: Simple Visual Language Model Pretraining with WeakSupervision. In The Tenth International Conference on Learning Representations,ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.",
  "Yiming Yang. 1999. An Evaluation of Statistical Approaches to Text Categoriza-tion. Inf. Retr. 1, 1-2 (1999), 6990": "Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiao-dan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. 2022. FILIP: Fine-grainedInteractive Language-Image Pre-Training. In The Tenth International Conferenceon Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenRe-view.net. Mao Ye, Dong Shou, Wang-Chien Lee, Peifeng Yin, and Krzysztof Janowicz.2011. On the semantic annotation of places in location-based social networks.In Proceedings of the 17th ACM SIGKDD International Conference on KnowledgeDiscovery and Data Mining, San Diego, CA, USA, August 21-24, 2011, Chid Apt,Joydeep Ghosh, and Padhraic Smyth (Eds.). ACM, 520528.",
  "Yi Zhang, Mingyuan Chen, Jundong Shen, and Chongjun Wang. 2022. Tailorversatile multi-modal learning for multi-label emotion recognition. arXiv preprintarXiv:2201.05834 (2022)": "Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D. Manning, and Curtis P.Langlotz. 2020. Contrastive Learning of Medical Visual Representations fromPaired Images and Text. CoRR abs/2010.00747 (2020). arXiv:2010.00747 Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu.2019. ERNIE: Enhanced Language Representation with Informative Entities. InProceedings of the 57th Conference of the Association for Computational Linguistics,ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, Anna Ko-rhonen, David R. Traum, and Llus Mrquez (Eds.). Association for ComputationalLinguistics, 14411451.",
  "Jiawei Zhao, Yifan Zhao, and Jia Li. 2021. M3tr: Multi-modal multi-label recogni-tion with transformer. In Proceedings of the 29th ACM International Conference onMultimedia. 469477": "Jingbo Zhou, Shan Gou, Renjun Hu, Dongxiang Zhang, Jin Xu, Airong Jiang, YingLi, and Hui Xiong. 2019. A Collaborative Learning Framework to Tag Refinementfor Points of Interest. In Proceedings of the 25th ACM SIGKDD InternationalConference on Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK,USA, August 4-8, 2019, Ankur Teredesai, Vipin Kumar, Ying Li, Rmer Rosales,Evimaria Terzi, and George Karypis (Eds.). ACM, 17521761."
}