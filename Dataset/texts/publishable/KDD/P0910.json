{
  "ABSTRACT": "Large Language Models (LLMs) have revolutionised the capabilityof AI models in comprehending and generating natural languagetext. They are increasingly being used to empower and deployagents in real-world scenarios, which make decisions and takeactions based on their understanding of the context. Therefore re-searchers, policy makers and enterprises alike are working towardsensuring that the decisions made by these agents align with humanvalues and user expectations. That being said, human values anddecisions are not always straightforward to measure and are subjectto different cognitive biases. There is a vast section of literature inBehavioural Science which studies biases in human judgements.In this work we report an ongoing investigation on alignment ofLLMs with human judgements affected by order bias. Specifically,we focus on a famous human study which showed evidence oforder effects in similarity judgements, and replicate it with vari-ous popular LLMs. We report the different settings where LLMsexhibit human-like order effect bias and discuss the implications ofthese findings to inform the design and development of LLM basedapplications.",
  "LLMs, Behavioural Science, Context Effects": "ACM Reference Format:Sagar Uprety, Amit Kumar Jaiswal, Haiming Liu, and Dawei Song. 2024.Investigating Context Effects in Similarity Judgements in Large LanguageModels. In . ACM, New York, NY, USA, 6 pages. Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from , July 2017, Washington, DC, USA 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-x-xxxx-xxxx-x/YY/MM",
  "INTRODUCTION": "Large Language models (LLMs) like GPT-4 and LLaMa fam-ily have been at the forefront of ushering a new wave of re-search, funding, investments and public opinion in Artificial Intelli-gence (AI). This is due by their remarkably improved capabilities inreasoning , cross-domain generalisation and the emer-gence of a new type of learning - in-context learning . Someresearchers have gone as far as suggesting that LLMs are the pre-cursor to a generalist form of AI , whereas there is another schoolof thought which says that the autoregressive training and nextword prediction architecture of LLMs are not enough to imitatehuman cognitive intelligence . Nevertheless, it is accepted thatLLMs are useful tools to augment our workflows and assist in differ-ent tasks like coding assistant1, copywriting, marketing and sales2,systematic reviews3, search engines4, etc. There is ongoing researchin developing specialised and generalist LLM agents which can useother tools like browsers and APIs5 to perform tasks like bookingholidays, reserving tables, online shopping, etc. Increasingly, manyof our daily workflows, including our professional work will beeither automated or augmented with the help of LLM-based agents.These agents will interact with and impact the society in general.Thus, it is imperative that the decisions made by these agentsthrough LLMs align with not only the intentions of the users ofthese agent applications, but also societal values and conventions.There is a parallel line of research in this direction under the um-brella of AI Alignment .There are several facets of AI alignment, depending upon the ap-plication domain of the AI model. However, the underlying themeof current alignment research is the focus on avoiding negative orincorrect outcomes from LLMs and also the focus on values .There is a large class of human judgements where the outcomeis neither negative nor incorrect, rather there is no single correctoutcome. This is because the outcome depends on the context ofjudgement. In this work, we focus on the alignment of context-sensitive judgements produced by LLMs with those produced byhumans. There are several implications of aligning human-LLM",
  "Conference17, July 2017, Washington, DC, USAUprety et al": "judgements and decisions. Firstly, it can help test theories in psy-chology and cognitive science on large scale human-like data .Secondly, it will help improve performance of LLM agents in real-world scenarios by making them robust to change in contexts anduncertainty. We focus on a certain kind of judgements - similarityjudgement, which underpins a wide range of applications such assearch, question answering and recommendation. The ability tojudge similarity of stimuli, concepts, entities, etc. is an importantaspect decision-making in humans . Our research question is :(RQ): How well do LLMs align with humans in context-sensitive similarity judgements?We replicate a study by Tversky and Gati , which was one ofseveral which investigated violation of symmetry in human similar-ity judgements. Similarity of concepts and entities is mathematicallyrepresented using distance metrics in some coordinate space. Oneof the chief properties of such spaces is that metric distance orvector projections are symmetric, i.e. the similarity of two entitiesA and B is same as similarity between B and A. A large array ofwork from Amol Tversky has investigated and found thatsimilarity judgements lose their symmetric property when madeunder context-sensitive scenarios. The concepts and entities consid-ered in these studies have ranged from country-pairs to geometricfigures. In the present work, we use the study conducted to judgesimilarity of pairs of countries where the context is created by theorder of the country in the pair. For e.g. one group of participantsasked to rate the similarity of China and North Korea and anothergroup is asked to rate the similarity of North Korea and China. Ourhypothesis is that LLMs will also exhibit human-like assymmetricjudgements because of the change of context exhibited by changingthe order of countries in a pair. We also conduct a novel variant ofthe experiment by prompting the LLMs with both orders of coun-tries in the same prompt. This is akin to asking a human participantto rate the similarity of countries in both orders. Most humansshould give the exact same score in such a case, as the concept ofsimilarity is intuitively understood to be symmetric. Similarly, wehypothesise that LLMs should also render the exact same score forthe two different orders of the country pairs.We study eight different LLMs - both closed-source and open-source, small and large in their number of parameters. We utilisedifferent temperature settings and different variants of prompts interms of the text. Like human judgements, LLMs are also foundto produce different ratings for similarity based on the order ofcountries in the pair, however not all such instances are statisticallysignificant. We report on the model-temperature pairs which showstatistically significant order effects, discuss the implications ofthese results and discuss the future steps in this line of research.",
  "RELATED WORK": "Research at the intersection of cognitive science and LLMs is basedon chiefly two perspectives. Although both perspectives seek toknow how close are LLMs to humans, one line of work is interestedin utilising LLMs to study cognitive models and theories on a largescale by replicating human behaviour with synthetic data generatedfrom LLMs . The other research direction concerns with theapplication of LLMs in automation and augmentation, wherein itis essential that decisions and judgements by LLM based agentsalign with human preferences and judgements . In either case, while comparing humans and LLMs in terms of responses,judgements and decisions, it is important to know whether the un-derlying mechanisms of representation of concepts and reasoningare similar or not. In general, it is seen that even in cases whereLLM judgements are similar to humans, the underlying processesof reasoning and representation are different . For e.g., re-veals that while LLMs are effective at understanding and explainingvariability in human decisions under risk, their risk assessmentsare purely data-driven, lacking the emotional and psychologicalfactors influencing human decisions.A large array of recent work investigates presence of differenthuman-like cognitive biases in LLMs . In , theauthors repeatedly prompt LLMs with the same query and studythe variability in the responses. It is found that at temperature =1.0, the variability displays similar patterns to human judgementsvariability. In and , LLMs are tested against popular taskslike Wason selection and Conjunction fallacy and found to containhuman-like bias in some of the tasks. Another important study found presence of common cognitive biases like anchoring, framing,group atttribution in 4 LLMs - GPT-3.5-Turbo, GPT-4, Llama2 7B,and Llama2 13B. However, they were able to devise strategies tomitigate these biases to a good extent.Human judgements typically optimise over a lot of differentfactors thus leading to a lot of trade-offs (Bounded rationality). Onesuch trade-off between truthfulness and helpfulness to user goalsof LLM responses is studied in . This balance is crucial because,like humans, LLMs must sometimes prioritise helpfulness over stricthonesty to be effective communicators. For instance, an LLM mightsimplify or approximate numerical values to aid understanding,akin to how humans might round numbers in conversation tomake information more comprehensible. It was found that LLMresponses also display human-like patterns, which can be furthertuned towards either factor using different techniques like RLHFor Chain-of-Though Prompting (CoT).",
  "TVERSKY AND GATIS SIMILARITYEFFECTS STUDY": "In this study , participants were shown pairs of 21 countriesand asked to rate the similarity between them on a scale of 0 to20. The participants were divided into two groups, and each grouprated similarity of 21 countries. The only difference between thegroups was the order of countries in the pair. It was found thatthere were statistically significant differences in similarity scoresfor the country pairs in the two groups, thus providing evidencethat human given similarity judgements may not always follow thesymmetry assumption.Tversky and Gati consider similarity judgements to be an ex-tension of similarity statements (a is like b), which are in general- directional. When two entities are compared, one is usually thesubject and the other is the referent. The referent is always the moreprominent of the two entities (i.e. has with more salient features). Inthe example given earlier in the paper, China would be the referentand North Korea the subject. Tversky and Gati hypothesise that if(,) and (,) are the similarity of a pair of subject and referentin the two orders, then (,) > (,). Similarity of two entitieswould be more if the first entity is the less prominent one of thepair. This is validated in their experiment for the 21 country pairs",
  "SSDgpt-40.5-1.9170.035Symmetric": "when they find that a one-sided paired t-test leads to statisticallysignificant improvement in similarity scores when a less prominentcountry (in terms of size, or economic and political influence) isplaced in the first position in a pair.One possible explanation of difference in similarity scores be-cause of different order of presentation is that different orders leadto different factors used by participants to evaluate the similarityof the two countries. E.g., in case of North Korea and China, whenpresented in this order, participants consider specific factors likepolitical system, culture, proximity as some of the factors whichare common to both countries. However, when people come acrossChina as the first country in the pair, the factors of size, economy,etc. come into consideration. Across these factors North Koreadoesnt seem much similar to China and thus people give a lowerscore. Thus, a more prominent country, when considered first, hasmany more features, thereby leading to a small fraction overlappingwith those of a smaller country. While this explanation sounds intu-itive, it violates the symmetric property of distance-based similaritymeasures.",
  "EXPERIMENTS4.1Models": "We conduct investigations into order effect in similarity using 8LLMs - Mistral 7B6, Llama2 7B, Llama2 13B, LLama2 70B, Llama38B, Llama3 70B , GPT-3.5 and GPT-4 . For the open-sourcemodels we use their non-quantised 16 bit versions, with defaultvalues of top_p (0.9) and top_k (50) parameters, while the temper-ature parameter is varied. The different temperature parametersused are 0.001, 0.5, 1.0, 1.5. Note that we did not use temperature= 0, as the Llama models require a non-zero temperature value,unlike OpenAI models. We thus keep the lowest temperature valueto as low as possible, and avoid using temperature = 0 just for theOpenAI models.The prompts for each of the LLMs consist of a system message -which also instructs the model to generate response in a structuredjson format, and the specific questions which are part of the study.4.2Experiment 1 - Single pair prompts In the original study the participants were asked to \"assess thedegree to which country A similar to country B\". Therefore, theequivalent prompt to this question is \"On a scale of 0 to 20, where0 means no similarity and 20 means complete similarity, assess thedegree to which {country_1} is similar to {country_2}?\". Moreover,each prompt is preceded by a system message and we assert the LLM to strictly return a number between 0 and 20 in a json format.However, apart from this prompt, we also try two other variants bychanging the language of the prompt. It is well-known that LLMoutputs, even at zero temperature, are sensitive to choice of wordsin a prompt . The expected output is a json of shape: {score: }.The three variants of single pair prompts with appropriate labelsare:",
  "Experiment 2 - Dual pair prompts": "In this experimental setting we prompt the LLMs with both theorders of the two countries present in the same prompt, thus elimi-nating the context created by changing orders. Although there willalways be a particular order in which the countries are presented,if the other order immediately follows the first order, the contexteffect is negated.Each prompt consist of two statements for each order. The ex-pected output is a json with two keys - score_1 and score_2. Weagain use the three different wording styles in both statements. Thethree prompts with their labels are: Dual Similar Degree (DSD) -Question 1: On a scale of 0 to 20, where 0 means no simi-larity and 20 means complete similarity, assess the degree towhich {country_1} is similar to {country_2}?Question 2: On a scale of 0 to 20, where 0 means no simi-larity and 20 means complete similarity, assess the degree towhich {country_2} is similar to {country_1}? Dual Similar To (DST) -Question 1: On a scale of 0 to 20, where 0 means no similar-ity and 20 means complete similarity, is {country_1} similarto {country_2}?Question 2: On a scale of 0 to 20, where 0 means no similar-ity and 20 means complete similarity, is {country_2} similarto {country_1}? Dual Similar And (DSA) -Question 1: On a scale of 0 to 20, where 0 means no sim-ilarity and 20 means complete similarity, how similar are{country_1} and {country_2}?Question 2: On a scale of 0 to 20, where 0 means no sim-ilarity and 20 means complete similarity, how similar are{country_2} and {country_1}?",
  ": Distribution of similarity differences for all countries and all models for single prompt settings": "shows the distribution of difference in scores for all country pairs forall models, temperature settings and single prompt styles. We alsoplot the human similarity score differences from Tversky and Gatisstudy. It is evident that all models show asymmetry in similarityjudgements, a departure from the mathematical definition. However,as will be detailed below, not all settings have statistically significantdifferences. For the dual prompt setting, the hypothesis is that allsimilarity judgements should be symmetrical, as when deliberatelyshown both orders, humans should give the same rating to thecountry pairs. The similarity differences for dual prompt settingsare plotted in figure 3.Similar to , an order effect is calculated when the meandifference between the similarity scores is statistically significant( < 0.05) based on a one-sided paired t-test. The direction beingthe increase in similarity score when the less prominent country isplaced at first in the pair. According to our second alignment criteria,the scores across the dual prompt styles should be symmetric for allmodels. Of the 96 different settings, we find only 9 settings whereboth the criteria are met. lists all these settings. We list thet-statistic and p-value for each model-temperature-single promptstyle setting. We also check the effects in the corresponding dualprompt version for the same model-temperature setting. Eitherthere are no similarity score differences for all 21 countries in thetwo orders for that setting, or the differences are not statisticallysignificant. These could be due to other reasons such as promptsensitivity .We can see that only 3 models have perfectly symmetric similar-ity scores for all the dual prompt styles and temperature settings- Llama3 8B, Llama3 70B and GPT-4. There are three instances ofmodels scoring perfectly symmetrically in the dual prompt settings- GPT-4 for the original studys SSD prompt style and Llama3_8Bfor the SST prompt style for two different temperature settings.We can thus say that GPT-4 is aligned with human judgements forthis similarity task (for temperature = 0.5) as the SSD prompt isclosest to the original study setting. However, the order effect in theoriginal study is not hypothesised to be due to the wording of thequestion itself, but rather due to the order of the countries in the pair. It is safe to assume that if the questions in the original studywere of style SST, the order effects would have still been present inthe similarity judgements. Therefore, we can say that Llama3_8Bmodel also aligns with human judgements for this task. Moreover,it is aligned for two temperature values versus one value for GPT-4.However, largely the LLMs do not show significant asymmetry insimilarity judgements. Only some models with certain temperaturesettings do so. This means that LLMs can be made to align theirbehaviour with human behaviour by tuning these settings. Theremight be certain applications where this alignment is beneficialand certain where it is not. For example, consider an e-commerceLLM-based chat agent asked by whether Phone A is similar toPhone B or not. If another user asks the agent the same question inanother order (perhaps they are more inclined towards the otherphone), the agent should ideally come up with the same response.Should the LLM exhibit order effects in similarity judgements, itwould report inconsistent results to different users solely basedon the order of items in query. On the other hand, there might becertain applications e.g. in dating, mental health support whereLLMs might be better-off aligned to human order bias.",
  "CONCLUSION & FUTURE WORK": "In this paper, we explored the how aligned are LLM judgementswith human judgements via similarity ratings to pairs of countriesin different orders. Our investigation involves studying the effectof context-driven asymmetry in human judgements utilising Tver-skys framework of similarity judgements . We find that outof eight LLMs studied across different temperature settings andprompting styles, only Llama3 8B and GPT-4 models report statisti-cally significant order effects in line with human judgements. Evenwith these models, changing the temperature setting can lead to dis-appearance of the effect. In the future, we aim to elicit and comparethe reasoning generated by these models by different promptingapproaches like Chain of Thought prompting . We can thencompare the crtieria used by LLMs to arrive at their similarity scoreand whether and how this criteria is different for different order ofcountries in the pair.",
  "Sudeep Bhatia and Russell Richie. 2022. Transformer networks of human con-ceptual knowledge. Psychological Review (2022)": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Pra-fulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, RewonChild, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christo-pher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,and Dario Amodei. 2020. Language models are few-shot learners. In Proceedings ofthe 34th International Conference on Neural Information Processing Systems (<conf-loc>, <city>Vancouver</city>, <state>BC</state>, <country>Canada</country>,</conf-loc>) (NIPS 20). Curran Associates Inc., Red Hook, NY, USA, Article 159,25 pages. Sbastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, EricHorvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023.Sparks of artificial general intelligence: Early experiments with gpt-4. arXivpreprint arXiv:2303.12712 (2023).",
  "Amos Tversky and Itamar Gati. 1978. Studies of Similarity. In Cognition and Cate-gorization, Eleanor Rosch and Barbara Lloyd (Eds.). Lawrence Elbaum Associates,11978": "Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, SebastianBorgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al.2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682(2022). Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoningin large language models. Advances in neural information processing systems 35(2022), 2482424837."
}