{
  "Abstract": "Large language models (LLMs) can be prone to hallucinations generating unreliable outputs thatare unfaithful to their inputs, external facts or internally inconsistent. In this work, we address severalchallenges for post-hoc hallucination detection in production settings. Our pipeline for hallucinationdetection entails: first, producing a confidence score representing the likelihood that a generated answeris a hallucination; second, calibrating the score conditional on attributes of the inputs and candidateresponse; finally, performing detection by thresholding the calibrated score. We benchmark a variety ofstate-of-the-art scoring methods on different datasets, encompassing question answering, fact checking,and summarization tasks. We employ diverse LLMs to ensure a comprehensive assessment of perfor-mance. We show that calibrating individual scoring methods is critical for ensuring risk-aware down-stream decision making. Based on findings that no individual score performs best in all situations, wepropose a multi-scoring framework, which combines different scores and achieves top performance acrossall datasets. We further introduce cost-effective multi-scoring, which can match or even outperform moreexpensive detection methods, while significantly reducing computational overhead.",
  "Introduction": "Despite their impressive capabilities, large language models (LLMs) can be prone to generating hallucinations undesirable outputs that are incorrect, unfaithful, or inconsistent with respect to the inputs (or the outputitself) . These unreliable behaviors pose significant risks for adopting LLMs in real-world applications.Challenges in detecting hallucinations lie, among other things, in hallucinations taking different forms, beingcontext-dependent and sometimes being in conflict with other desirable properties of generated text .Hallucinations may be harmless in some contexts, but can be undesired or potentially dangerous in otherapplications (e.g., erroneous medical advice). Detecting and quantifying hallucination risk is thus a criticalcapability to enable safe applications of LLMs and improve generated outputs.Prior work has proposed various approaches for detecting and mitigating hallucinations in LLM-generatedoutputs, including verifying faithfulness to inputs , assessing internal coherence , consulting externalknowledge sources , and quantifying model uncertainty . However, deploying these methodsin production settings is far from trivial due to several challenges: First, there is limited comparativeevaluation illuminating how different detection methods perform. Second, existing approaches for detectinghallucinations differ greatly in their computational demands, and guidelines are lacking on cost-effectivenesstrade-offs to inform method selection for real-world applications with constraints.Third, hallucinationdetection in the real world often requires careful consideration of risks and false positive/negative trade-offs,requiring methods to provide well-calibrated probability scores. Fourth, many applications of LLMs take",
  ": Schematic overview of our proposed hallucination detection approach": "the form of calls to black-box APIs, which sometimes requires methods to employ workarounds to assess themodels confidence in its generated output.In this work, we provide a framework for detecting hallucinations in the outputs of any LLM in amodel-agnostic manner. Our approach relies on quantifying the probability that a generated answer is ahallucination. After computing initial scores, we employ state-of-the-art calibration techniques to providecalibrated probabilities of the generation containing a hallucination, which can subsequently be used fordecision making or other downstream tasks.We evaluate a variety of scores proposed in the literaturefor hallucination detection on several metrics, across multiple datasets encompassing question answering,fact checking, and summarization tasks. We employ a range of different LLMs to ensure a comprehensiveassessment of performance.Critically, as we show that no single score performs best across all datasets, we introduce multi-scoring,a simple way of combining multiple calibrated scores, which achieves superior performance compared toany individual score alone.Furthermore, we propose cost-effective multi-scoring, which finds the subsetof best-performing scores for any fixed cost budget, and combines them in a multi-scoring fashion. Ourempirical demonstrations reveal that cost-effective multi-scoring not only matches but often surpasses theperformance of individual scores that incur significantly higher costs. Consequently, our proposed methodachieves superior hallucination detection outcomes while maintaining a substantially lower cost footprint.We summarize our contributions as follows: 1.We benchmark a variety of hallucination detectionmethods across the literature on several metrics, over different datasets and LLMs. 2. We introduce multi-scoring, a novel approach that aggregates multiple complementary scores and outperforms individual scores.3.We further propose cost-effective multi-scoring, which optimally balances detection performance andcomputational constraints.",
  "Formalizing Hallucination Detection": "We study the problem of quantifying the probability that a generated output from a language model containshallucinations. More formally, let x represent an input token sequence to a language model G, and let zrepresent a generated output text sequence from the model. We define a binary random variable y {0, 1}that indicates whether z is a permissible output (y = 1) or contains a hallucination (y = 0).Our goal is to develop a scoring function to model the probability that a given output text contains ahallucination conditioned on the input.1 This is critical, as in real-world scenarios, we need to set risk-aware",
  "Note that while we are interested in whether the entire output contains any hallucination, one could easily adapt themethods at different granularities, such as the sentence or phrase level": "thresholds, balancing false positive/negative rates to accommodate the production requirement.Havingaccess to scores allows us to flexibly set the threshold. Conceptually, the key reason to model this probabilis-tically is that there is inherent uncertainty in determining whether a given text contains a hallucination ornot. Even human raters may disagree on the assessment, based on their own knowledge and definition. Somekey contributors to this epistemic uncertainty are: First, that no system has complete world knowledge toperfectly assess factual correctness. Second, that there is ambiguity in whether something is a hallucination.Finally, any automatic scoring model may make occasional errors, so probabilistic scores reflect confidence.Therefore, while conditional on x and z the true hallucination label y is fixed, our estimate of y remainsuncertain. The probabilistic score thus reflects this epistemic uncertainty the degree of belief that zcontains a hallucination given the available knowledge: p(y = 0 | x, z; ), where denotes parameters ofthe scoring function. We refer to this conditional probability function as the hallucination score, denoteds(x, z). The hallucination score can then be applied to downstream tasks, including making risk-awarebinary decisions.We discuss concrete instances of hallucination scores in the following subsection. Generally, the form ofs can vary between hallucination detection approaches. In particular, s may depend on multiple candidatetexts generated for the same input, as implemented by a number of hallucination detection methods proposedin the literature.We denote K candidate generated outputs as Z = [z1, ..., zK].Then s(x, Z) couldquantify inconsistencies within texts in Z using different metrics , as discussed below.",
  "Scoring Methods": "Many hallucination detection methods proposed in the literature make use of LLMs to judge the output ofan LLM (either the same or a different one). For clarity, we thus distinguish between generator and detectorLLMs. The generator is defined as the model used to generate the original response. The detector LLM isthe model used to score a generated text for the presence of hallucinations. In general, the generator anddetector LLMs may coincide. However, it may be more desirable to use different LLMs in some scenarios,e.g., when computational cost is a greater concern, where a smaller LLM may be used to judge outputs ofa more expensive LLM, or when using hallucination scoring methods that require white-/grey-box access,while the generator is black-box.Today, many interactions with LLMs take the form of API-calls.Typically, only the output tokensare returned with no access to the logits of the predicted tokens, treating the LLM effectively as a blackbox. Sometimes, inference parameters may be accessible, allowing for setting different temperature (amongothers) values, thereby providing some (grey-box) access to the model. Generally, hallucination detectionmethods vary in their required model access, ranging from black-box APIs to full white-box access to themodel weights. That is, while some methods only require token-level outputs, other methods may needaccess to the logits of the generated tokens, or some control over inference parameters like temperature.In our experiments, we evaluate a comprehensive set of hallucination scoring methods. Generally, we donot make any assumptions about the generator LLM being white-, grey- or black-box. We divide methodsinto single-generation methods, which require only one generated output, and multi-generation methodswhich are based on multiple alternative generations.",
  "ScoreLogitAccessNLImodel# LLMcalls# NLIcalls# Generations": "P(True)YesNo100P(True) VerbalizedNoNo101P(InputContradict)YesNo100P(SelfContradict)YesNo100P(FactContradict)YesNo100Inverse PerplexityYesNo100NLI (DeBERTa)NoYes010SelfCheckGPT-NLINoYesKMKKHallucinationRailYesNoK0KSimilarityDegreeNoYesKKK : Summarizing properties of different scoring methods including model access and inference timecosts. K denotes the number of multiple generations, M denotes the number of sentences in the response.The column of # Generations denotes the number of LLM calls required besides the LLM call that generatesthe original response.In this work we set M = 1 as we consider the responses as one sentence in ourexperiment. generator as a plug-in estimator implies that the confidence estimate is detached from the generator. If ourgenerator provides access to the models logits we can directly use them to score hallucinations. Alternatively,we can use another LLM to generate the logits of output tokens.",
  "The question is: {x}The answer is: {z}The answer is:": "We compute P(True) by first taking the softmax over the first generated output tokens logits, and thennormalizing across the True and False tokens.2 That is, let z(1) = (z(1)1 , . . . , z(1)|V |) be the logit vectorover the vocabulary V for the first generated token when asking the LLM to evaluate whether the answer iscorrect. We then take the softmax over these logits, p(w | z(1)) =exp(z(1)w )",
  "P(InputContradict): Provide a \\True\\ or \"False\\ response": "2Note that alternatively, we can prompt the LLM to bind the True/False tokens to symbols like response options A orB . However, this did not change results in our experiments. Further, if the detector LLM provides only black-box access,one can sample the corresponding tokens and use the empirical distribution to approximate, but thereby incurring a highercomputational cost.",
  "Text: {z}The text is factually sound:": "NLI Text ClassificationNatural language inference (NLI) models provide an alternative to assess thecorrectness of the model output. As hallucination detection requires checking for contradictions, we computethe score as 1 Scontradict, where Scontradict refers to the softmax probability of the output conflicting withthe question. Specifically, we use a DeBERTa model fine-tuned on an MNLI task as the underlying NLImodel, which we refer to as NLI (DeBERTa). Verbalized ProbabilitiesInstead of analyzing a models logits, scores are elicited by asking an LLM toprovide a confidence verbatim, i.e., by generating tokens that indicate the numerical confidence withthe following prompt:",
  "Multi-generation": "Multi-generation methods are based on quantifying the consistency across multiple generated outputs fromthe generator LLM. This follows the notion of white-/grey-box uncertainty quantification via logits that if theLLM is confident in its response, multiple generated responses will probably be alike and contain compatiblefacts. Conversely, for fabricated information, sampled responses are more likely to differ and contradict oneanother. Crucially, this rests on the assumption that the models confidence is calibrated, as we discuss andevaluate below. SelfCheckGPTThis method exploits a DeBERTa NLI model to assess whether the answer A0 is consistentwith K alternative generated answers A1, ..., AK . Each sentence of A0 is compared against the full setof answers A1, ..., AK. A consistency score per sentence is obtained via an NLI model, then the final scoreis obtained by averaging over the sentence-wise scores. Similarity DegreeThis method is based on computing pairwise similarities between the multiple re-sponses using NLI models, and then quantifying uncertainty based on the distribution of similarities .Here, we compute the pairwise similarities between responses via the contradict class probability of an NLImodel and construct a degree matrix, where each diagonal element corresponds to the total (sum) similarityof the corresponding response to all other responses. We use the degree of the candidate response A0 as theconfidence estimate, following favourable results in prior work , but note that other metrics have alsobeen proposed in the original paper. NeMO Guardrails: Hallucination RailUnlike SelfCheckGPT or Similarity Degree, the score here iscomputed via one LLM call, where we check for agreement between the concatenated K additional gen-erations and the candidate output and not averaged over sentences or computed on pairs of responses.Specifically, we use the softmax probability normalized over yes/no tokens using the following prompt:",
  "Calibration": "The calibration step is performed via the following multicalibration approach, using the Fortuna library :To obtain groups, we compute embeddings of the input text x and the generated response z, such thatour embedding is e := [embed(x), embed(z)]. We obtain embeddings from Universal AnglE Embedding ,which is the SOTA in the MTEB benchmark at the time of writing.We subsequently reduce thedimension of x via UMAP and perform soft-clustering via Gaussian Mixture Models, as a simple off-the-shelf algorithm (fitting the number of cluster components via BIC ). The calibration error is measured foreach group g G separately. Calibration then involves iteratively patching the group with the largest erroruntil the calibration error drops below a threshold for all groups.4 This provably converges to a calibratedmodel with theoretical guarantees . We fit the calibrator to a random calibration fold of 80% of the dataand report only held-out test results. For all binary predictions, we set the threshold to the 50th percentileto not impose preferences over false true/negative rates, but note that in practice any threshold could beapplied.",
  "E[y | p(y = 0|x, z) = p] = p(3)": "A naive approach for obtaining scores would be to compute the models probabilities marginally, ignoringthe context/question x and generated response z and directly estimating p(y = 1). However, this does notallow for conditioning on individual inputs to obtain calibrated probabilities p(y = 1 | x, z) for specific x,and z, which is, however, impossible to guarantee . Common calibration methods include temperaturescaling (Platt scaling) of logit outputs , isotonic regression or histogram binning , whichoperate marginally, and can thereby not account for different confidence levels for different inputs (e.g., withan LLM being more confident on certain domains than others).An alternative is to partition the inputs into G groups and compute calibration separately for each groupg G. However, this assumes the groups are disjoint and does not handle inputs belonging to multiplegroups, which is often necessary . More advanced calibration methods, such as multicalibration, whichwe use in this work, allow defining G potentially overlapping groups.Prior work has shown theeffectiveness of modern calibration techniques in scoring LLM confidence, albeit in a white-box setting .We describe our calibration approach in the experimental section.",
  "Multi-Scoring: Combining Scores": "Different scoring methods capture different aspects of hallucinations, e.g., incorrect, non-factual, non-grounded, irrelevant, inconsistent with other answers, etc. As a result, some scores may work better onsome data or for some specific models, and worse on others. Therefore, we design a multi-scoring methodto combine the complementary information from individual scores into a single, strong predictor.Denote each available score by sn, for n = 1, . . . , N. To obtain an aggregated score, we run a logistic re-gression using as features the concatenations of the logit of each score, i.e. [logit(sn(x, z)), . . . , logit(sN(x, z))],and the labels of the calibration dataset as target variables. 3",
  "Cost-Effective Multi-Scoring": "Scoring methods based on multiple generations incur the cost of additional generations as well as the costof checking their consistency. Similarly, multi-scoring can be a viable choice when there are only few LLMgenerations to check for hallucinations, but incurs considerable computational cost, especially when usingmulti-generation methods. To avoid prohibitive computational costs in practice, we propose cost-effectivemulti-scoring, where we set a fixed computational budget and compute the best performing combination ofscores that stay within the specified budget.Given an input text x and generated text z, we have N scoring functions s1(x, z), ..., sN(x, z) withassociated costs c1, ..., cN (e.g., number of generations required). We are given a total computational budgetB. Our goal is to find the optimal subset of scores S {1, ..., N} that maximizes detection performancewhile staying within budget B:",
  "iSci B(4)": "where L measures loss on a validation set. This is a constrained optimization problem over subsets ofscoring functions. When B = mini ci, it reduces to selecting the single best score at the lowest cost. WhenB = i ci, it recovers full multi-scoring. In between, the optimal subset S provides the best trade-offbetween performance and cost. We note that in general, this problem is computationally challenging, giventhe exponential runtime complexity. However, given that there are only generally a small set of potentialscores (N 10) and the logistic regression is very fast to fit, computing all combinations takes only 1.8seconds on a single Intel Xeon processor (3.1 GHz), iterating over all candidate solutions sequentially. Whenthis approach is not feasible, some exemplary alternatives include classic greedy forward-selection methodsor regularising the model via an L1 penalty while scaling the regularisation term to accommodate scorecost. Overall, cost-effective multi-scoring allows for flexibly balancing multiple scores under computationalconstraints. Quantifying CostWhile the cost ci of scoring function si is presented abstractly above, accuratelyquantifying computational cost can be challenging in practice. The actual runtime per method is a reasonableproxy, however the runtime depends on model architecture, hardware acceleration, batching, etc.Oneapproach is to directly benchmark each scoring functions average runtime empirically on the target hardware.However, this overlooks nuances like caching effects and ignores runtime variability due to implementationaldifferences. To simplify our analysis, and as LLM calls are generally much more expensive than callingsmaller NLI models based on parameter size, we leverage the number of LLM calls required per method asa proxy. See for an overview. If more precision is desired, we suggest empirically running differentscores and computing their computational cost in the actual application, as the cost will depend on theprecise setup and a range of factors. 3Note that we also conducted experiments with alternative models, such as XGBoost and Random Forest , butresults did not show significant improvements, so we opted for logistic regression for simplicity.",
  "Datasets": "TriviaQATriviaQA is a commonly used factual open-response QA dataset. Originally set up as areading comprehension task, it is today often used without context as a closed-book (free-recall) task . Weuse the validation fold, containing 17944 question-answer pairs. We use mistralai/Mistral-7B-Instruct-v0.2 to generate candidate answers. To decide whether a given response should be labelled as positive ornegative, we check whether the correct answer is contained in the generated answer after removing formatting,following the original evaluation script . FEVERThe closed-response Fact Extraction and VERification dataset provides a comprehensivebenchmark of factual hallucination detection. We take the test fold, containing 14027 labelled examples ofsource documents, claims and whether they are supported, refuted or contain not enough info. For thepurpose of hallucination detection, we look at all claims that are either supported or refuted. HaluEvalWe include the hallucination detection dataset HaluEval and use the summarization task,which contains 10000 labeled examples of source documents, summaries and labels for whether the summarycontains hallucinations.",
  "Individual Scoring Methods": "presents the results of different hallucination detection methods on all datasets. Multi-generationmethods (SelfCheckGPT-NLI, HallucinationRail and SimilarityDegree) are employed only for TriviaQA,since the latter provides the true answer to each question, which can be compared to the alternative generatedanswers to assess their agreement. In contrast, for HaluEval, BIG-Bench and FEVER, the true answer is notprovided. Instead, we compare the given candidate answer from the dataset against the provided binary labelof correctness. If we exclude Multi-Scoring methods, multi-generation methods such as SelfCheckGPT-NLIand SimilarityDegree (with 10 generations) achieve the best performance on TriviaQA, P(True) is the beston HaluEval and BIG-Bench, and P(InputContradict) performs best on FEVER. Thus, we find that thereis no single best scoring method across all datasets. This is likely because different scoring methods capturedifferent aspects of hallucination, supporting the notion that hallucination is a multi-faceted concept. Multi-generation methods measure hallucination based on the consistency of the generator LLMs responses andon the ability of the comparison method to identify whether multiple alternative responses are in agreement.We find that SelfCheckGPT-NLI performs best in our experiments in comparison to SimilarityScore andHallucinationRail.While all of these methods follow similar approaches, there are subtle differences inhow they assess the consistency between different responses. More generally, as we discuss below, multi-generation methods are appropriate only if one can assume that there is exactly one correct response, butcan fail otherwise.Other methods are based on different notions of hallucination, such as NLI (DeBERTa) measuring theentailment of the response given the input. Interestingly, variants of P(True) can be appropriate for detectingdifferent kinds of hallucinations. P(True) explicitly asks the evaluator LLM to check whether the responseis correct. In some contexts, the evaluator LLM may have the ability to directly evaluate this. However, inother situations, as we see with the FEVER dataset, other methods such as P(InputContradict) can be moreappropriate, if we are trying to directly target a specific form of hallucination, which may not be subsumedunder a general correct or not? prompt. Other applied scenarios may target even more different (ormore specific) kinds of hallucinations, though the variants we include in this work are designed to cover thespace in a reasonable manner. Similarly, while the datasets included in these experiments were selected tocover a wide range of different kinds of hallucinations, real-world applications may show even new kinds ofhallucinations, for which no public datasets are available. Overall, these findings highlight the need for ourmulti-scoring method which can absorb the strength of each individual method and can be easily applied toa concrete hallucination detection setting while only requiring a relatively small amount of labeled data.Overall, neither the inverse perplexity score nor the NLI (DeBERTa) scores emerge as the best scores forany of the datasets we consider. While they may add information that can be exploited in (cost-effective)multi-scoring, as reported below, individually they perform worse than some of the other scores.",
  "Multi-Scoring": "To evaluate the proposed multi-scoring method, we conduct experiments on all datasets combining the scoresvia logistic regression. As presented in , the multi-scoring ensemble achieves an F1 score of 0.9106 onTriviaQA, outperforming the best individual F1 score of 0.8614 (achieved by SelfCheckGPT-NLI). Similarly,Brier and Accuracy also show the highest performance for multi-scoring. For HaluEval, we also find thatmulti-scoring outperforms the best individual score (P(True)) in all metrics. BIG-Bench shows more nuance,as multi-scoring outperforms the best individual score (P(True)) on Brier and Accuracy, but not on F1.This reflects the fact that the metrics capture different properties, and practical considerations may requirea decision as to which metric should be prioritized in a given application. For FEVER, multi-score againoutperforms the best individual score (P(InputContradict)) in all metrics.Thus we see that combining scores performs better than the top performing individual scores, showingthat combinations of different scoring signals complement each other and can boost performance.Thisdemonstrates that combining complementary signals enables more robust hallucination detection, whilebalancing the strength of each scoring method. Crucially, while some settings may benefit from combinationsof different signals (such as when trying to detect different kinds of hallucinations in a given generatedoutput), in other settings the data-driven selection of an informative signal may be sufficient (such as whentrying to detect a more narrowly defined notion of hallucination). These scenarios are covered by our useof multi-scoring, which allows for arriving at optimally combined hallucination scores (or binary decisions) : Hallucination detection results on all datasets of calibrated scoring methods. Methods that are notapplied to a given dataset are marked as . All scores were computed with Mistral-7B-Instruct-v0.2.TriviaQA responses were generated with Mistral-7B-Instruct-v0.2. Excluding multi-scoring, the bestperforming scores are underlined. Including multi-scoring, the best performing scores are displayed in bold-face.",
  "Multi-Score0.11050.91060.85930.19110.76680.70750.20450.59660.65900.05440.93710.9351": "Cost-Effective (C = 1)0.18190.82630.74900.19800.75950.69350.20660.64700.64170.06340.93280.9309Cost-Effective (C = 2)0.17720.83080.75200.19110.76680.70750.20450.59660.65900.06360.00050.9328Cost-Effective (C = 3)0.17270.83240.75370.19110.76680.70750.20450.59660.65900.05440.93710.9351Cost-Effective (C = 4)0.17180.82770.74810.19110.76680.70750.20450.59660.65900.05440.93710.9351Cost-Effective (C = 5)0.14850.86190.80110.19110.76680.70750.20450.59660.65900.05440.93710.9351 for a given application. Meanwhile, as computational cost can be a concern with scaling LLM applications,it may not be desirable to always use a full ensemble of scores, and we would rather compute the mostperformant score at a fixed computational cost.",
  "Cost-effective Multi-Scoring": "Shown in , cost-effective multi-scoring methods are also amongst the top performers, but at consid-erably lower cost compared to multi-score. We see that at cost C = 1, measured as the number of LLMcalls (but see our discussion about measuring cost above), we recover the best individual scores, which variesacross each dataset. As we increase the budget, the cost-effective multi-scores converge to the performance ofthe multi-score itself. Notice that in several instances, cost-effective multi-score with C = 2 already performsas good as multi-score itself. For TriviaQA, we see that performance increases on all metrics as we increasethe computational budget, with cost-effective multi-score at C = 5 outperforming SelfCheckGPT-NLI as themost performant individual scores at half the computational cost in all metrics but Brier. HaluEval showsno improvement beyond a budget of C = 2, which is likely due to the kind of hallucination to detect beingmore narrowly defined and well-capture by a small number of signals, thereby not benefiting from additionalscores. For BIG-Bench, as discussed for multi-score, we recover the best individual score at C = 1, andmatch multi-score at higher budgets. FEVER results indicate that, again, we recover the best individualscore at budget C = 1, and can already recover the full multi-score at a budget of C = 3.We now take a closer look at cost-effect multi-scoring when including multi-generation scoring methods.The overall cost budget is varied over the entire range. For each budget, we solve the constrained optimizationin Eq. 4 to find the optimal subset S of scores. We compare the hallucination detection F1 score achieved bythe cost-effective ensemble versus individual scoring functions and the full ensemble with all scores. shows the results for TriviaQA for responses generated via Mixtral-8x7B-v0.1. With a minimal budgetof B = 1, cost-effective selection recovers the best single method, as expected. As the budget increases, itselectively adds more expensive functions, gradually improving F1, though the gains are marginal at higherbudgets per unit cost. At the maximum budget, it recovers the unconstrained full ensemble performance. Inbetween, cost-effective multi-scoring incorporates both less and more and expensive methods to maximizedetection within the computational constraints.These results demonstrate that the proposed cost-effective multi-scoring approach can intelligently bal-ance the trade-off between computational expense and hallucination detection effectiveness. It outperforms 123456789 10 11 12 13 14 15 16 17 18 19 Computational Cost (#LMM Calls) 0.88 0.88 0.89 0.89 0.90 0.90 0.91 F1 Score Cost-Effective Multi-ScoreP(True)SelfCheckGPT-5SelfCheckGPT-10",
  ": Hallucination detection F1 versus computational budget B for cost-effective multi-scoring": "individual scoring functions and makes selective use of more costly scores to maximize detection performanceunder a fixed computational budget.A key consideration in practice would be whether to include potentially more costly multi-generationmethods, such as SelfCheckGPT-NLI. In particular as implied by the results presented in , we maybe interested in reducing the number of required multiple generations while matching performance at lowercomputational costs. To this end, we compare SelfCheckGPT (as the single best performing method forTriviaQA) alone with SelfCheckGPT combined with P(True) at different numbers of additional responsesgenerated via Mixtral-8x7B-v0.1. Here, we count the evaluation of P(True) as one additional LLM call,just as generating one additional output from the generator LLM. As presented in , at one additionalgeneration, SelfCheckGPT (in the degenerate case of only one generated output) combined with P(True) thusrecovers P(True). As the number of additional generations increases, the combination quickly outperformsSelfCheck alone. In particular, we observe that cost-effective multi-scoring with 3 LLM calls is already asgood as SelfCheckGPT with 9 LLM calls in our experiment. This highlights how we can save costs bycombining multi-generation methods with other scores while requiring fewer additional generated responsesfrom the generator LLM than if we wanted to achieve the same performance with multi-generation methodsalone. Number of Additional LMM Calls 0.86 0.87 0.88 0.89 0.90 F1 Score SelfCheckGPTSelfCheckGPT & P(True)P(True)",
  "Exploration of Relationships between Scores": "As discussed above, different scoring methods can target different kinds of hallucations. To explore thisempirically, we here explore their relationships via Spearman rank correlations. As presented in ,calibrated scores across all different individual scores are positively correlated. Meanwhile, the magnitudeof their correlations is smaller than would perhaps be expected if one were to consider hallucinations asa uniform phenomenon. At the same time, we can see certain clusters of more strongly inter-correlatedscores emerge. For example, multi-generation methods including SelfCheckGPT, HallucinationRail and Sim-ilarityDegree, which check the consistency across multiple generations, show comparably high correlations.P(True) and P(True) Verbalised and P(InputContradict) emerge as a similarly correlated cluster of strongercorrelations. Overall, this supports the idea that different scores can capture distinct information, and theneed to empirically select appropriate (combinations of) scores for a given application, as we propose withcost-effective multi-scoring.",
  "Experiments across Different LLMs": "Most of the scoring methods considered in this work rely on detector LLMs to compute scores for hallu-cination detection. In of Appendix A, thus present additional data for different LLMs, namelyMixtral-8x7B-Instruct-v0.1, falcon-7b-instruct and OpenLLaMA-13b. Overall, we see that hallucina-tion detection performance is correlated with performance on general LLM benchmarks5. Thus, more overallcapable LLMs are likely to perform better at hallucination detection than less capable ones. However, con-crete applications may need to take the inference cost of LLMs into account, where one may not always beable to use the most expensive model. We note that cost-effective multi-scoring could here also make use ofscores computed via LLMs with different computational costs to find cost-effective combinations.",
  ": Example of multi-generation failure-case in NLP systems, illustrating conflicting responses": "Our results indicate that methods based on the uncertainty among multiple generations can providestrong signals when such multiple generations are available. However, such methods can also suffer fromproblems in practice. Methods based on multiple generations make use of the uncertainty of the generator,that is the distribution over generations z given the input x, for model G, i.e., pG(z | x) and the consistencybetween actual sampled generations z. We are interested in classifying whether the candidate output containsa hallucination or not, that is estimating and making use of p(y | x, z). The generating models uncertaintypG(z | x) can be a useful proxy for p(y | x, z), but only under particular conditions.On a technical level, sampling multiple answers requires access to the generators temperature parameter,as temperatures of zero collapse the generations to a single response, as is mentioned, e.g., in . There-fore, these methods are technically grey-box models, as they require some access to the models inferenceparameters.More conceptually, the generators uncertainty (or confidence ) over pG(z | x) reflects uncertaintyover generated tokens. All recently proposed multi-scoring methods we are aware of are based on the ideaof scoring the consistency across multiple generations using model-based metrics. However, self-consistencyacross multiple generations is neither a necessary nor sufficient criterion for a hallucination to be present.Sufficiency is not given as a model may consistently provide an incorrect response. Self-consistency acrossmultiple generations is also not necessary, as many tasks allow for multiple hallucination-free answers thatare contradictory to each other.As a relatively harmless example, in tasking a model to generate cooking recipes for lunch, the modelmay generate, among other things, a recipe for a salad and a recipe for a soup. Clearly, the steps in preparingthese dishes contain contradictory information, while otherwise being free of hallucinations in themselves.Thus, if there is more than one correct response, the self-consistency assessment in multi-generation methodsmay falsely score an output as likely to be hallucinated. As there are numerous situations where there existmultiple correct responses, assuming that there is only one could lead to worse LLM responses, also via aloss of diversity.Practically, methods based on multiple generations can be costly due to added computational overhead,as we have seen above. Finally, these methods are based on the assumption that LLMs are calibrated. Assuch, multi-generation methods do not allow for detecting cases where the generator LLM is confident yetwrong.",
  "Conclusion": "In this work, we compared a comprehensive set of scoring methods to provide calibrated probability scoresfor the presence of hallucinations in generated LLM outputs.Overall, we have observed that no singlehallucination detection score performs best across all datasets. Our experiments showed that combinationsof scores, as suggested with multi-scoring, can effectively combine complementary signals to yield higher hallucination detection performance than any individual score. Further, we demonstrate that cost-effectivemulti-scoring can find the highest performing scores at a given computational budget.More generally, our findings support the notion that hallucinations can be rather multi-faceted thanpresent a uniform phenomenon. Thus, detecting hallucinations may require different methods. In concretesettings, one may be interested in even more fine-grained detection of particular types of hallucinations, suchas in specific domains like code generation .The approach outlined in this work requires only a small amount of labeled data to calibrate hallucinationscores and combine scores via (cost-effective) multi-scoring. However, it is important to acknowledge thelimitations of the current work. Future research could explore the effectiveness of this approach on morediverse datasets, investigate alternative scoring methods, or extend the methodology to multi-modal models.While improvements in LLM performance can be expected to also lower their rate of occurrence, hallucina-tions are unlikely to go away completely . Thus, even as models improve further, detecting hallucinationsis likely to remain relevant for applying LLMs in a reliable and trustworthy manner in future.In summary, our work presents a promising approach for detecting hallucinations in LLM outputs bycombining multiple scoring methods in a cost-effective way, offering a pathway towards more reliable andtrustworthy language models that can be applied in real-world settings with more confidence. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Be-ichen Zhang, Junjie Zhang, Zican Dong, et al.A survey of large language models. arXiv preprintarXiv:2303.18223, 2023. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, AndreaMadotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM ComputingSurveys, 55(12):138, 2023. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao,Yu Zhang, Yulong Chen, et al. Sirens song in the ai ocean: A survey on hallucination in large languagemodels. arXiv preprint arXiv:2309.01219, 2023.",
  "Gianluca Detommaso, Martin Bertran, Riccardo Fogliato, and Aaron Roth. Multicalibration for confi-dence scoring in llms. arXiv preprint arXiv:2404.04689, 2024": "Fred Jelinek, Robert L Mercer, Lalit R Bahl, and James K Baker. Perplexitya measure of the difficultyof speech recognition tasks. The Journal of the Acoustical Society of America, 62(S1):S63S63, 1977. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, NicholasSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) knowwhat they know. arXiv preprint arXiv:2207.05221, 2022.",
  "Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantlysupervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017": "Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprintarXiv:2109.01652, 2021. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b.arXiv preprint arXiv:2310.06825, 2023.",
  "James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a large-scaledataset for fact extraction and verification. arXiv preprint arXiv:1803.05355, 2018": "Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. Halueval: A large-scalehallucination evaluation benchmark for large language models. In Proceedings of the 2023 Conferenceon Empirical Methods in Natural Language Processing, pages 64496464, 2023. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,Adam R Brown, Adam Santoro, Aditya Gupta, Adri`a Garriga-Alonso, et al. Beyond the imitation game:Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615,2022.",
  "Glenn W Brier. Verification of forecasts expressed in terms of probability. Monthly weather review,78(1):13, 1950": "Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford,Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral ofexperts. arXiv preprint arXiv:2401.04088, 2024. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru,Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, BadreddineNoune, Baptiste Pannier, and Guilherme Penedo.Falcon-40B: an open large language model withstate-of-the-art performance. 2023.",
  "Xinyang Geng and Hao Liu. Openllama: An open reproduction of llama, May 2023": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, et al. Huggingfaces transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. Gianluca Detommaso, Alberto Gasparin, Michele Donini, Matthias Seeger, Andrew Gordon Wilson,and Cedric Archambeau. Fortuna: A library for uncertainty quantification in deep learning. arXivpreprint arXiv:2302.04019, 2023."
}