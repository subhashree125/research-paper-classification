{
  "ABSTRACT": "Industrial recommender systems face the challenge of operating innon-stationary environments, where data distribution shifts arisefrom evolving user behaviors over time. To tackle this challenge,a common approach is to periodically re-train or incrementallyupdate deployed deep models with newly observed data, resultingin a continual learning process. However, the conventional learn-ing paradigm of neural networks relies on iterative gradient-basedupdates with a small learning rate, making it slow for large recom-mendation models to adapt. In this paper, we introduce ReLoop2, aself-correcting learning loop that facilitates fast model adaptationin online recommender systems through responsive error compen-sation. Inspired by the slow-fast complementary learning systemobserved in human brains, we propose an error memory mod-ule that directly stores error samples from incoming data streams.These stored samples are subsequently leveraged to compensatefor model prediction errors during testing, particularly under dis-tribution shifts. The error memory module is designed with fastaccess capabilities and undergoes continual refreshing with newlyobserved data samples during the model serving phase to supportfast model adaptation. We evaluate the effectiveness of ReLoop2on three open benchmark datasets as well as a real-world produc-tion dataset. The results demonstrate the potential of ReLoop2 inenhancing the responsiveness and adaptiveness of recommendersystems operating in non-stationary environments.",
  "Both authors contributed equally. Jieming Zhu is the corresponding author.Work done during internship at Huawei Noahs Ark Lab": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 23, August 610, 2023, Long Beach, CA, USA 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00",
  "Recommender systems, continual learning, distribution shift, modeladaptation, retrieval augmentation": "ACM Reference Format:Jieming Zhu, Guohao Cai, Junjie Huang, Zhenhua Dong, Ruiming Tang,and Weinan Zhang. 2023. ReLoop2: Building Self-Adaptive RecommendationModels via Responsive Error Compensation Loop. In Proceedings of the 29thACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD23), August 610, 2023, Long Beach, CA, USA. ACM, New York, NY, USA,11 pages.",
  "INTRODUCTION": "Nowadays, personalized recommendation has emerged as a promi-nent channel across a range of online applications, including e-commerce, news feeds, and music apps. It enables the delivery oftailored items to users based on their individual interests. Theprovision of high-quality recommendations not only enhancesuser engagement but also fuels revenue growth for platforms. Toachieve accurate recommendations, deep learning-based modelshave gained widespread adoption in industry owing to their flexibil-ity and ability to capture intricate user-item relationships. However,industrial recommender systems often operates in non-stationaryenvironments, where data distribution shifts occur as a resultof evolving user behaviors over time. This can lead to the dete-rioration of well-trained recommendation models during onlineserving, and thus poses a challenge for models to quickly adaptunder distribution shifts.To address this challenge, previous research efforts have beenmade from two aspects: behavior sequence modeling and incre-mental model training. The first line of research aims to capturedynamic patterns at the feature level by modeling sequential userbehavior sequences. Notable progress has been made in this area,particularly in click-through rate (CTR) prediction tasks , withmodels incorporating attention, GRU, and transformer architec-tures, such as DIN , DIEN , and BST . These modelsformulate CTR prediction as a few-shot learning task , wheregiven k historical behaviors from a user (i.e., k-shot samples), thegoal is to predict whether the user will click on the next item. Few-shot learning enables rapid adaptation to new users with only afew observed samples . However, these studies do not explicitlyaddress the test-time distribution shift problem. In parallel, another",
  "KDD 23, August 610, 2023, Long Beach, CA, USAJieming Zhu et al": "and random down-sampling, storing these raw data samples stillimposes a considerable burden on memory. Therefore, for onlinerecommendation tasks with limited computing resources, ANNsearch may not be the optimal choice.Ideally, we aim to design a lightweight and fast-access memorythat avoids directly storing massive data points in RAM, elimi-nates the need for iterative and non-streaming processes like k-means, and avoids constructing complex index structures such asgraphs, which are either time-consuming or difficult to parallelize.To achieve these objectives, we propose an alternative design of theerror memory by employing the LSH-based data sketching algo-rithm on the streaming data . LSH, as introduced in .3,enables efficient bucketing of each data point in constant time usingfixed hash functions. Data sketching supports the construction of acompact sketch that summarizes the streaming data. The sketch-ing algorithm compresses a set of high-dimensional vectors into asmall array of integer counters, which is sufficient for estimatingthe similarity of similar samples in Equation 16.Formally, we define the memory as a sketch consisting of repeated arrays, denoted as for . Each array is indexed by independent hash functions () = {() | },where () is a singed random projection described inEquation 4. Consequently, an input can be hashed into an index in2 buckets: () . For example, setting = 16 can re-sult in approximately 65,536 buckets. While the sketch is originallydesigned for kernel density estimation with integer counters , inour design, we store a tuple of summation values at bucket in thearray , denoted as [] = (_[],_[],_ []).To ensure more stable estimations, the same process is repeated times using different sets of hash functions { () | }. In summary, the memory can be viewed as a concate-nated array of size 2 3. More specifically, we formulate thememory writing and reading processes as follows:Memory Writing. For each observed sample from the datastream, we obtain (,,_). Instead of directly storing the rawsamples in the memory following Equation 15, we apply each set ofhash functions to map the key vector to the correspondingbucket and update the sketch array [] as follows.",
  "CTR Prediction": "Typically, input samples consist of two main types of features:categorical features, and numerical features. In our approach, weutilize embedding techniques to map these features into a lower-dimensional embedding space. Specifically, for numerical features,we first discretize them into categorical bucket features. Then, samewith categorical features, we apply one-hot/multi-hot encodingand embedding table lookups to obtain embedding vectors. Let = {1,2, ...,} denotes a data instance with feature fields.Then we could get its feature embeddings as = {1,2, ...,},which serve as the input to a deep neural network.Given the set of feature embeddings, various CTR predictionmodels have been proposed to model feature interactions (e.g.,DeepFM , DCN , DCN-V2 ) and sequential user inter-ests (e.g., DIN , DIEN , and BST ). At last, the CTR model",
  "Fast Model Adaptation": "In this section, we analyze the motivation behind the need for fastmodel adaptation to address the problem of distribution shift. In, we present our observations regarding the dynamic datadistribution from various perspectives, including data variance,feature dynamics, overall CTR, and category-specific CTR overtime. To illustrate this, we split the test dataset of MicroVideo(detailed in ) into ten chronological time slots, simulatingan online advertising scenario. Specifically, (a) depicts thedata variance (from embedding ) for each time slot, revealingthe spread of data instances relative to their average. A highervalue indicates a greater deviation from the average. (b)demonstrates the changes in the number of users and items overtime. Both (a) and (b) highlight the covariate shift in feature .",
  "Time Slot": "0.04 0.06 0.08 0.10 0.12 CTR of Categories (d) CTR of Category 1CTR of Category 2 70K 80K 90K 100K 110K 120K Number of Items Number of UsersNumber of Items : Observations of data distribution shifts on the Mi-croVideo dataset. (a) Variance of data samples over each timeslot. (b) Number of users and items involved over each timeslot. (c) The averaged CTR over each time slot. (d) The aver-aged CTR of two unique categories over each time slot. Furthermore, (c) showcases the dynamic nature of CTRover time, while (d) exhibits the dynamic CTR based on differentcategories. These figures reveal the label shift in and the conceptdrift between and, respectively. Collectively, these visualizationsdemonstrate a significant level of data change occurring over time.As a result, there is a pressing demand for fast model adaptation toswiftly adjust to the dynamic patterns present in the data.",
  "Locality Sensitive Hashing": "This section provides a brief review of the classical Locality Sen-sitive Hashing (LSH) algorithm , which is a widely adoptedsublinear-time algorithm for approximate nearest neighbor search.In LSH, a hash function () Z a mapping that assigns an input to an integer in the range . LSH encompasses a familyof such hash functions with a key property: similar points have ahigh probability of being mapped to the same hash value . Moreformally, a LSH family is defined as follows .",
  "if (,) 0 then (,) 1 if (,) 0 then (,) 2": "Typically, 1 > 2 and < 1 is required for approximate nearestneighbor search. We use the notation (,) to denote the collisionprobability () = () between and , where their hashvalues are equal. One sufficient condition for being a LSH family isthat the collision probability (,) is a monotonically increasingfunction of similarity between the two data points, i.e.,",
  "(,) = (,)(3)": "where () is required to be a monotonically increasing function.In other words, similar data points are more likely to collide witheach other under LSH mapping.Among the widely known LSH families, SimHash is a popu-lar LSH that applies the technique of Signed Random Projections(SRP) for the cosine similarity measure. Given a vector, SRP utilizes a random vector with each component generatedfrom i.i.d. normal, i.e., (0, 1), and only stores the sign of theprojection. Hence, SimHash is given by",
  "where (,) is monotonic to the cosine similarity": "22 .It is important to note that each hash function () generates asingle bit using SRP, resulting in two possible hash values {0, 1}. Byindependently sampling hash functions with different vectors,we can generate new hash values in the range by com-bining the outcomes of the independent SRP bits. The collisionprobability is equal to (,), the power of of Equation 5.",
  "Overview": "Deep learning-based recommendation models, such as CTR pre-diction models discussed in .1, are typically optimizedusing back-propagation algorithms within the empirical risk mini-mization (ERM) framework. These models assume a stationary datadistribution (i.e., the training and testing data are drawn from thesame distribution) and require a small learning rate to gradually in-corporate information into model weights. However, in real-worldonline recommendation scenarios, the rapid emergence of newusers and items, along with potential changes in user behaviorover time, result in the distribution shift challenge. Consequently,a well-trained model may gradually degrade after deployment. Toaddress this challenge, we propose the ReLoop2 framework for fastmodel adaptation, as depicted in .Our framework employs a slow-fast learning paradigm, wherethe base model undergoes slow gradient updates, while an episodicmemory module, free from back-propagation, is introduced to fa-cilitate rapid acquisition of new knowledge. The base model is astandard parametric neural network that learns common knowl-edge through gradual gradient updates. In contrast, the memorymodule is a non-parametric component that stores recently ob-served samples and enables fast learning and adaptation from thesesamples. This slow-fast learning paradigm aligns with the theoryof complementary learning systems (CLS) in human brains .Specifically, we refer to the memory module as the error memory,which stores the recent error samples produced by the base modelon the incoming data stream. These error samples provide insightsinto cases where the model makes incorrect predictions, particu-larly in the presence of distribution shifts. By directly capturingand remembering these error samples, we can estimate errors in a non-parametric manner and subsequently correct the model out-put through error compensation. This establishes a continual fastadaptation process for the model within the evolving dynamics ofthe non-stationary data distribution. New error samples observedduring model deployment are continuously written back to theerror memory, enabling the tracking of changing dynamics in theonline data.",
  "Error Compensation Loop": "(b) depicts our error compensation loop for fast modeladaptation, comprising three key components: the base model ,the fast-access error memory , and the error estimation moduleE. Our learning framework is generic and compatible with variousbase models used for CTR prediction. We formulate the base modelas follows: = ()(6) where represents the feature embeddings of a data instance. denotes the predicted click probability from thebase model. We provide a brief overview of feature embeddingand CTR modeling approaches in .1. It is worth notingthat the model function can be implemented using any exist-ing CTR prediction model, such as DeepFM , DCN , andDIN . The base model approximates the ground truth label by minimizing the error between and within an empiricalrisk minimization framework:",
  "min 2 ,where = (7)": "Ideally, under the assumption of independent and identically dis-tributed (i.i.d.) data, the error should be a small random variableclose to zero after model training. However, the base model de-grades when confronted with distribution shifts, resulting in anenlarged error during model serving.To address this issue, we propose a proactive approach to es-timate the model prediction error and correct the model outputaccordingly. However, directly obtaining from Equation 7 is not",
  "= + (9)": "where denotes the final output with model adaptation. Thecompensation weight adjusts the proportion of error compensa-tion. It is important to note that the value of may exceed therange after error compensation. In such cases, we clamp thevalue within the range.In the following sections, we will describe our designs for theerror estimation module E and the error memory module . 3.2.1Error Estimation Module. Given the error memory that re-tains recently observed data samples, our goal is to estimate theprediction error based on similar samples to a new input . For-mally, we aim to retrieve a set of top-k similar samples from thememory, as described below:",
  "K =(, , _) | (10)": "where = (,) denotes the similarity between the hiddenvectors of the query sample and memory instance . Additionally, and _ represent the ground-truth label and the predictionvalue of the base model, respectively. The derivation of similarsamples K is provided in .2.2.Inspired by the attention mechanism employed in content-addressingmemory networks , we can estimate the attention-weightedground truth and prediction value as follows.",
  "= + (1 ) (13)": "where is a weight that balances the two error measures. Notably,when = 0, the error is estimated from the model predictions onsimilar samples. When = 1, the error is computed from the groundtruth labels of similar samples. In the latter case, we substitute into Equation 9 and can obtain the corrected model prediction witherror compensation as follows:",
  "This can be seen as a weighted ensemble of the base models output and the estimation from k-nearest neighbors (KNN). Forsimplicity, we use = 1 in our experiments": "3.2.2Fast-Access Error Memory. In this section, we focus on thekey component of our framework, the error memory . In onlinerecommendation systems, data is acquired sequentially over time,and the model generates click predictions based on the receivedsamples. The true labels are received when users interact with therecommended items. During this process, we can easily obtain thehidden representation from a specific hidden layer of the basemodel (same with ), the base model output _, and the truelabel . To enable fast adaptation to distribution shifts, we utilizethe memory to store these recently observed samples. Ideally, ourmemory consists of a set of key-value pairs formulated as follows:",
  "However, designing the memory poses two main challenges inreal-world recommender systems due to the large volume of clickdata:": "Fast Access. For real-time online CTR prediction, modelinference must meet stringent latency requirements. There-fore, it is crucial to enable fast access to the error memory.However, retaining a large number of recently observed datasamples for adaptation makes the memory size too large toutilize traditional attention-based content addressing mech-anisms in memory networks , which needs to read allmemory slots for each query. Memory Size. The memory module requires a substantialmemory size to store an adequate number of data samples.In our production system, the number of samples can easilyreach millions within a 10-minute timeframe. Storing sucha large number of data samples consumes significant com-puting resources (e.g., RAM) for model serving. Therefore,minimizing memory resource consumption for the errormemory is highly desirable. To address these challenges, we explore two potential solutions:approximate nearest neighbor (ANN) search and LSH-based sketch-ing. ANN search techniques are widely used in industry to effi-ciently retrieve top-k nearest vectors in sub-linear time. Thesetechniques have been successful in various retrieval-augmentedmachine learning tasks (e.g., language modeling , machine trans-lation ). Additionally, they are supported by mature tools andlibraries, including Faiss , ScaNN and Milvus . How-ever, constructing popular ANN indices like HNSW and IVFPQin Faiss involves time-consuming steps (e.g., k-means) and re-quires substantial memory (gigabytes of RAM). To reduce memoryconsumption, we propose filtering the data samples based on themodels errors. Specifically, we only store samples with relativelylarge errors (greater than a threshold ) since they indicate signifi-cant degradation in the base model. Despite applying error filtering",
  "(17)": "Note that the values in [] are initially set to zero and can bereset regularly or when the base model has been updated to refreshthe memory. The updates on all memory arrays can be performedin parallel.Memory Reading. Given a query sample vector , we canapply the same set of hash functions to map the query to bucket. We then obtain the summation values from each sketch array [] and compute the readout values via averaging them over allbuckets as follows:",
  "(18)": "After parallel reading from sketch arrays, we obtain the readoutresults of similar samples K =(,,_) | ,which can then be used in Equation 10 for error estimation.Compared to traditional memory that stores raw samples, ourLSH-based sketch memory offers several advantages. It enablesfast construction time (O(1) writing time per sample), has a lowmemory requirement (constant memory size of 2 3), andeliminates the need for query-time distance computations (O(1)reading time per query). It is worth noting that our sketch memoryis not only practical to implement but also enjoys strong theoreticalguarantees .In this way, the error memory module helps estimate the poten-tial error of the base model based on observed similar data samples,contributing to an error compensation loop that continuously andadaptively corrects the model output. This results in a slow-fastjoint learning framework for fast model adaptation.",
  "Datasets. We conduct experiments on three open benchmark datasets,and a large-scale production dataset": "AmazonElectronics is a subset of the Amazon dataset , awidely used benchmark dataset for recommendation. We followthe DIN work to preprocess the dataset. Specifically, the Ama-zonElectronics contains 1,689,188 samples, 192,403 users, 63,001goods and 801 categories. Features include goods_id, category_id,and their corresponding user-reviewed sequences: goods_id_list,category_id_list. MicroVideo is an open dataset for short video recommendation,which has been released by . We follow the same preprocessingsteps. It contains 12,737,617 interactions that 10,986 users havemade on 1,704,880 micro-videos. The labels include click or non-click, while the features include user_id, item_id, category, andthe extracted image embedding vectors of cover images of micro-videos. KuaiVideo is another open dataset for short video recommenda-tion. We follow the work to obtain the preprocessed dataset.Specifically, we randomly selected 10,000 users and their 3,239,534interacted micro-videos. It contains several interaction data be-tween users and videos, such as user_id, photo_id, duration_time,click, like, and so on. In addition, 2048-dimensional video embed-dings are provided as content features.",
  "gAUC AUC gAUCAUCgAUC AUC gAUCAUC": "xDeepFM87.9088.1388.7388.9668.8973.6269.5374.11DCNv287.9088.1288.8188.9968.5973.4469.7174.25AOANet87.9188.1288.7588.9468.5873.4669.1173.86DIN88.3588.6089.1089.3468.8373.6069.2273.86DIEN88.5688.8889.3389.6268.6773.2169.1573.60 features, such as doc_id, category_id, short-term interest topic_id,and anonymous data masking user_id. We use the latest 2-hoursamples as testing data, and split it into 12 consective parts inchronological order.Base models. We compare our model with the following main-stream base models for CTR prediction. Shallow models: FM , FmFM . Feature interaction models: DeepFM , xDeepFM , DCN ,AutoInt+ , DCNv2 , AOANet . Behavior sequence models: DIN , DIEN , BST .Metrics. We adopt the most popular ranking metrics, AUC and gAUC (i.e., user-grouped AUC), to evaluate the modelperformance. In addition, we report the relative improvements(RelImp) over the classic factorization machine (FM) model.We note that the preprocessed datasets and evaluation settingsfor all the baseline models we studied are available on the BARSbenchmark website:",
  "We evaluate the ReLoop2 module on existing models, includingmany state-of-the-art (SOTA) methods. The performances are shown": "in . Through the analysis of experiment results, we get someconclusions as follows: deep-learning-based methods get higheraccuracy than the low-rank-based methods, thus revealing thepowerful feature interaction ability of neural networks. In addi-tion, xDeepFM obtains the second-best results on the MicroVideodataset, indicating that a well-designed structure could fully use theadvantages of the factorization machine component. Whats more,DIEN method obtains the second-best results on AmazonElectron-ics and KuaiVideo datasets, which benefits from the evolution ofuser interests and exploitation of the sequential features. In ad-dition, we can see that our BASE+ReLoop2 outperforms all theother baseline methods since the error memory module is appliedto the baseline method to augment the base encoder, and the er-ror compensation helps to adapt to data distribution shift rapidly.Specifically, we choose DIEN as the base model for AmazonElec-tronics and KuaiVideo, and DCNv2 for MicroVideo because of theirrelatively better performance. It is worth noting that our ReLoop2framework is model agnostic to all the existing models, which isshown in . After applying ReLoop2 to the five state-of-the-art models respectively, we can obtain the new SOTA.Evaluation on production dataset. The comparison of ourmodel with the baseline on the product dataset is shown in .The baseline is an incremental learning method, which serves asthe base encoder, so the results of the first part of the test set areexactly the same. From the second part, we utilize the previousparts as fast access error memory to learn the error compensationrapidly, and the performance demonstrates the efficiency of ourReLoop2 apporach.",
  "is time slot, as we split the test dataset of MicroVideo into ten timeslots in chronological order evenly to simulate online advertisingtask. Four methods are compared in": "DCNv2 is the baseline model in this experiment. DCNv2+IncCTR applies the incremental training method, In-cCTR , on top of DCNv2. Specifically, after model trainingon the training data and model evaluation on the first part of thetest data, we continually train the model using the first part ofthe test data and then evaluate it on the second part. The processgoes on like this on ten test parts. Note that we only pass the testdata once for IncCTR as suggested in the paper. DCNv2+ReLoop2 applies fast model adaptation (FMA) to DCNv2. DCNv2+IncCTR+ReLoop2 applies both incremental trainingand fast model adaptation (FMA) to DCNv2. Note that our ReLoop2framework is orthogonal to the incremental training techniquesince ReLoop2 do not need extra training. In , ReLoop2 outperforms IncCTR most of the time on bothgAUC and AUC, except for the last two time slots, where AUC ofIncCTR exceeds that of ReLoop2. It is understandable since, withthe passage of time, new data distribution changes dramaticallyfrom the initial data distribution, and as a result, the accuracy of theoriginal base models prediction for new data decreases. As ReLoop2relies on base model prediction and error memory with no trainingprocess, it is likely that the AUC of IncCTR exceeds that of ReLoop2when the time slot increases. From another point of view, additionaltraining, like incremental learning, is necessary since it can makethe model have better control over new data by updating the modelparameters. By combining IncCTR and ReLoop2, DCNv2 achievesthe best performance in , demonstrating the efficiency ofour fast model adaptation module.",
  "Ablation Studies": "4.4.1Effect of for Memory Reading. We investigate the effectof K in . When K is small, error compensation is mainlydetermined by a small number of neighbors, which can not standfor the average error in the error memory, leading to a higher butnot the best gAUC. When K is too large, the final output will beinfluenced by those neighbor samples that are not so similar to itself,resulting in a slight decrease in gAUC, but it is generally stable.",
  ": Effect of compensation weight on AmazonElec-tronics and MicroVideo": "The best results are obtained when we choose an appropriate K.Through our experiment, we find that K=180 and K=70 achieve thebest performance of DCNv2 on AmazonElectronics and MicroVideo,respectively. 4.4.2Effect of Compensation Weight . We investigate the effectof compensation weights in . When is small, the finaloutput is mainly determined by the base model output, thus a bithigher but relatively close to the baseline. Specifically, when = 0,the final output is the same as that of the base model, serving as thebaseline. When is too large, error compensation contributes moreto the final output. gAUC of the final output decreases because ofthe lower percentage of base model, whose accuracy is supported bya large amount of training data. We empirically find = 0.9 and =0.4 achieve the best performance of DCNv2 on AmazonElectronicsand MicroVideo, respectively.",
  "RELATED WORK": "CTR Prediction. CTR prediction plays a key role in online ad-vertising, recommender system, and information retrieval. Even asmall improvement in CTR prediction can have a significant impact,benefiting both users and platforms. As a result, extensive researchefforts have been dedicated to this field, both in academia and indus-try. In general, the goal of CTR prediction is to generate probabilityscores that represent user preferences for item candidates in spe-cific contexts. Recently, a plethora of CTR prediction approacheshave been proposed, ranging from traditional logistic regression(LR) models , factorization machines (FM) models , tovarious deep neural network (DNN) models. Many of these modelsfocus on designing feature interaction operators to capture complexrelationships among features, such as product operators ,bilinear interaction and aggregation , factorized interactionlayers , convolutional operators , and attention opera-tors . Additionally, user behavior sequences play a crucial rolein modeling user interests. Different models have been employedfor behavior sequence modeling, including attention-based mod-els , memory network-based models , and retrieval-based models . Notably, the BARS benchmark pro-vides a comprehensive review and benchmarking results of existingCTR prediction models. However, all of these models focus on mod-eling sequential patterns during training and assume fixed parame-ters during testing, making them incapable of handling distributionshifts.Incremental Learning. Incremental learning is a general frame-work that aims to continuously update model parameters to acquirenew knowledge from new data while preserving the models abil-ity to generalize on old data . In the context of recommendersystems, incremental training has been widely adopted to copewith the data distribution shift and minimize the generalization gapbetween training and testing. Typically, model parameters from theprevious version are reused as an initialization for the next roundof training . To alleviate the catastrophic forgetting problem,Wang et al. proposed the IncCTR method, which uses knowl-edge distillation to strike a balance between retaining the previouspattern and learning from the new data distribution. In our earlierwork, we introduced the ReLoop framework, which establishes aself-correcting learning loop during the model training phase. How-ever, ReLoop2 focuses on test-time adaptation instead. Integratingboth techniques is an interesting direction and we leave it for futureresearch. Other studies apply meta-learning techniquesto incremental training of recommendation models, aiming to facil-itate knowledge transfer from old parameters to new parameters.A recent study proposes an adaptive incremental learning al-gorithm for mixture-of-experts (MoE) models to adapt to conceptdrift. Instead, our work is orthogonal to incremental training andfocuses on enabling fast model adaptation through error compen-sation using a non-parametric memory approach. Furthermore, incontrast to the majority of continual learning studies , ReLoop2employs a refreshed error memory for model adaptation, deviatingfrom the conventional practice of utilizing a memory buffer forexperience replay to prevent catastrophic forgetting.Retrieval Augmentation. Our work also draws some inspira-tion from recent research on retrieval augmented machine learning techniques . Retrieval augmentation focuses on retrieving sim-ilar key-value pairs from the external memory to enhance modelgeneralizability, particularly for rare events or long-tail classes .This approach has been successfully applied in various domains,including neural machine translation , visual recogni-tion , question answering , retrieval-augmented pre-training and text-to-image generation . However, unlikethese studies that retrieve data for model training, our work lever-ages refreshed online data for retrieval-augmented model adapta-tion. Additionally, we present a time- and memory-efficient designfor top-k retrieval in large-scale online recommendation scenarios.",
  "CONCLUSION": "In this paper, we make a pioneering effort towards fast adaptationof CTR prediction models for online recommendation. To addressthe challenge of distribution shifts in streaming data, we intro-duce a slow-fast learning paradigm inspired by the complementarylearning systems observed in human brains. In line with this par-adigm, we propose ReLoop2, a self-correcting learning loop thatfacilitates fast model adaptation in online recommender systemsthrough responsive error compensation. Central to ReLoop2 is anon-parametric error memory module that is designed to be time-and space-efficient and undergoes continual refreshing with newlyobserved data samples during model serving. Through comprehen-sive experiments conducted on open benchmark datasets and ourproduction dataset, we demonstrate the effectiveness of ReLoop2in enhancing model adaptiveness under distribution shifts.",
  "Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. 2019. BehaviorSequence Transformer for E-commerce Recommendation in Alibaba. CoRRabs/1905.06874": "Xusong Chen, Dong Liu, Zheng-Jun Zha, Wengang Zhou, Zhiwei Xiong, andYan Li. 2018. Temporal Hierarchical Attention at Category- and Item-Level forMicro-Video Click-Through Prediction. In 2018 ACM Multimedia Conference onMultimedia Conference (MM). 11461153. Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, et al. 2016. Wide& Deep Learning for Recommender Systems. In Proceedings of the 1st Workshopon Deep Learning for Recommender Systems (DLRS@RecSys). 710.",
  "Michel X. Goemans and David P. Williamson. 1995. Improved ApproximationAlgorithms for Maximum Cut and Satisfiability Problems Using SemidefiniteProgramming. J. ACM 42, 6 (1995), 11151145": "Renchu Guan, Haoyu Pang, Fausto Giunchiglia, Ximing Li, Xuefeng Yang, andXiaoyue Feng. 2022. Deployable and Continuable Meta-learning-Based Recom-mender System with Fast User-Incremental Updates. In The 45th InternationalACM SIGIR Conference on Research and Development in Information Retrieval(SIGIR). 14231433. Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.DeepFM: A Factorization-Machine based Neural Network for CTR Prediction. InInternational Joint Conference on Artificial Intelligence (IJCAI). 17251731. Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, andSanjiv Kumar. 2020. Accelerating Large-Scale Inference with Anisotropic VectorQuantization. In Proceedings of the 37th International Conference on MachineLearning (ICML), Vol. 119. 38873896. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.2020. Retrieval Augmented Language Model Pre-Training. In Proceedings of the37th International Conference on Machine Learning (ICML) (Proceedings of MachineLearning Research, Vol. 119). 39293938.",
  "Jiangpeng He, Runyu Mao, Zeman Shao, and Fengqing Zhu. 2020. IncrementalLearning in Online Scenario. In 2020 IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR). 1392313932": "Ruining He and Julian J. McAuley. 2016. Ups and Downs: Modeling the Visual Evo-lution of Fashion Trends with One-Class Collaborative Filtering. In Proceedingsof the 25th International Conference on World Wide Web (WWW). 507517. Xiangnan He and Tat-Seng Chua. 2017. Neural Factorization Machines for SparsePredictive Analytics. In Proceedings of the 40th ACM SIGIR Conference on Researchand Development in Information Retrieval (SIGIR). 355364. Monika Rauch Henzinger. 2006. Finding near-duplicate web pages: a large-scaleevaluation of algorithms. In Proceedings of International ACM SIGIR Conferenceon Research and Development in Information Retrieval (SIGIR). 284291. Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun,Cordelia Schmid, David A. Ross, and Alireza Fathi. 2022. REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowl-edge Memory. CoRR abs/2212.05221 (2022). Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: combining fea-ture importance and bilinear feature interaction for click-through rate prediction.In Proceedings of ACM Conference on Recommender Systems, (RecSys). 169177. Piotr Indyk and Rajeev Motwani. 1998. Approximate Nearest Neighbors: TowardsRemoving the Curse of Dimensionality. In Proceedings of the Thirtieth AnnualACM Symposium on the Theory of Computing (STOC). 604613.",
  "Lukasz Kaiser, Ofir Nachum, Aurko Roy, and Samy Bengio. 2017. Learning toRemember Rare Events. In The 5th International Conference on Learning Repre-sentations (ICLR)": "Petros Katsileros, Nikiforos Mandilaras, Dimitrios Mallis, Vassilis Pitsikalis,Stavros Theodorakis, and Gil Chamiel. 2022. An Incremental Learning frameworkfor Large-scale CTR Prediction. In Sixteenth ACM Conference on RecommenderSystems (RecSys). 490493. Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and MikeLewis. 2021. Nearest Neighbor Machine Translation. In Proceedings of the 9thInternational Conference on Learning Representations (ICLR).",
  "Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and MikeLewis. 2021. Nearest Neighbor Machine Translation. In 9th International Confer-ence on Learning Representations (ICLR)": "Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and MikeLewis. 2020. Generalization through Memorization: Nearest Neighbor LanguageModels. In Proceedings of the 8th International Conference on Learning Representa-tions (ICLR). Dharshan Kumaran, Demis Hassabis, and James L McClelland. 2016. WhatLearning Systems Do Intelligent Agents Need? Complementary Learning SystemsTheory Updated. Trends in Cognitive Sciences 20(7) (2016), 512534. Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, VladimirKarpukhin, Naman Goyal, Heinrich Kttler, Mike Lewis, Wen-tau Yih, Tim Rock-tschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented Gen-eration for Knowledge-Intensive NLP Tasks. In Annual Conference on NeuralInformation Processing Systems (NeurIPS).",
  "System. In Proceedings of the 27th ACM International Conference on Multimedia,(MM). ACM, 14641472": "Zekun Li, Zeyu Cui, Shu Wu, Xiaoyu Zhang, and Liang Wang. 2019. Fi-GNN:Modeling Feature Interactions via Graph Neural Networks for CTR Prediction.In Proceedings of the 28th ACM International Conference on Information andKnowledge Management, (CIKM). ACM, 539548. Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, et al. 2018. xDeepFM: CombiningExplicit and Implicit Feature Interactions for Recommender Systems. In Proceed-ings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &Data Mining, (KDD). 17541763.",
  "Jian Liang, Ran He, and Tieniu Tan. 2023. A Comprehensive Survey on Test-TimeAdaptation under Distribution Shifts. CoRR abs/2303.15361 (2023)": "Guoliang Lin, Hanlu Chu, and Hanjiang Lai. 2022. Towards Better Plasticity-Stability Trade-off in Incremental Learning: A Simple Linear Connector. InIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 8998. Bin Liu, Ruiming Tang, Yingzhi Chen, Jinkai Yu, Huifeng Guo, and Yuzhou Zhang.2019. Feature Generation by Convolutional Neural Network for Click-ThroughRate Prediction. In The World Wide Web Conference, (WWW). 11191129.",
  "Congcong Liu, Yuejiang Li, Xiwei Zhao, Changping Peng, Zhangang Lin, andJingping Shao. 2022. Concept Drift Adaptation for CTR Prediction in OnlineAdvertising Systems. CoRR abs/2204.05101 (2022)": "Qiang Liu, Feng Yu, Shu Wu, and Liang Wang. 2015. A Convolutional ClickPrediction Model. In Proceedings of the 24th ACM International Conference onInformation and Knowledge Management, (CIKM). ACM, 17431746. Alexander Long, Wei Yin, Thalaiyasingam Ajanthan, Vu Nguyen, Pulak Purkait,Ravi Garg, Alan Blair, Chunhua Shen, and Anton van den Hengel. 2022. Re-trieval Augmented Classification for Long-Tail Visual Recognition. In IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR). 69496959. Yu A Malkov and Dmitry A Yashunin. 2018. Efficient and robust approximatenearest neighbor search using hierarchical navigable small world graphs. IEEEtransactions on pattern analysis and machine intelligence 42, 4 (2018), 824836. Kelong Mao, Jieming Zhu, Liangcai Su, Guohao Cai, Yuru Li, and Zhenhua Dong.2023. FinalMLP: An Enhanced Two-Stream MLP Model for CTR Prediction. InProceedings of the AAAI Conference on Artificial Intelligence (AAAI). James L McClelland, Bruce L McNaughton, and Randall C OReilly. 1995. Whythere are complementary learning systems in the hippocampus and neocortex:insights from the successes and failures of connectionist models of learning andmemory. Psychological review 102(3):419 (1995). H. Brendan McMahan, Gary Holt, David Sculley, Michael Young, Dietmar Ebner,Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, SharatChikkerur, Dan Liu, Martin Wattenberg, Arnar Mar Hrafnkelsson, Tom Boulos,and Jeremy Kubica. 2013. Ad click prediction: a view from the trenches. InProceedings of the 19th ACM SIGKDD International Conference on KnowledgeDiscovery and Data Mining (KDD). 12221230. Yuxian Meng, Xiaoya Li, Xiayu Zheng, Fei Wu, Xiaofei Sun, Tianwei Zhang, andJiwei Li. 2022. Fast Nearest Neighbor Machine Translation. In Findings of theAssociation for Computational Linguistics (ACL). 555565. Danni Peng, Sinno Jialin Pan, Jie Zhang, and Anxiang Zeng. 2021. Learningan Adaptive Meta Model-Generator for Incrementally Updating RecommenderSystems. In Fifteenth ACM Conference on Recommender Systems (RecSys). 411421. Qi Pi, Weijie Bian, Guorui Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Practiceon Long Sequential User Behavior Modeling for Click-Through Rate Prediction.In Proceedings of the 25th ACM SIGKDD International Conference on KnowledgeDiscovery & Data Mining (KDD). ACM, 26712679. Qi Pi, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, XiaoqiangZhu, and Kun Gai. 2020. Search-based User Interest Modeling with LifelongSequential Behavior Data for Click-Through Rate Prediction. In Proceedings ofthe ACM International Conference on Information and Knowledge Management(CIKM). 26852692. Jiarui Qin, Weinan Zhang, Xin Wu, Jiarui Jin, Yuchen Fang, and Yong Yu. 2020.User Behavior Retrieval for Click-Through Rate Prediction. In Proceedings ofthe 43rd International ACM SIGIR conference on research and development inInformation Retrieval (SIGIR). 23472356. Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang.2016. Product-Based Neural Networks for User Response Prediction. In Proceed-ings of the IEEE 16th International Conference on Data Mining (ICDM). 11491154. Kan Ren, Jiarui Qin, Yuchen Fang, Weinan Zhang, Lei Zheng, Weijie Bian, GuoruiZhou, Jian Xu, Yong Yu, Xiaoqiang Zhu, and Kun Gai. 2019. Lifelong SequentialModeling with Personalized Memorization for User Response Prediction. InProceedings of the 42nd International ACM SIGIR Conference on Research andDevelopment in Information Retrieval (SIGIR). ACM, 565574.",
  "Yang Sun, Junwei Pan, Alex Zhang, and Aaron Flores. 2021. FM2: Field-matrixedFactorization Machines for Recommender Systems. In Proceedings of the WebConference (WWW). 28282837": "Jianguo Wang, Xiaomeng Yi, Rentong Guo, Hai Jin, Peng Xu, Shengjun Li, Xi-angyu Wang, Xiangzhou Guo, Chengming Li, Xiaohai Xu, Kun Yu, Yuxing Yuan,Yinghao Zou, Jiquan Long, Yudong Cai, Zhenxiang Li, Zhifeng Zhang, YihuaMo, Jun Gu, Ruiyi Jiang, Yi Wei, and Charles Xie. 2021. Milvus: A Purpose-BuiltVector Data Management System. In International Conference on Management ofData (SIGMOD). 26142627.",
  "Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. 2023. A Comprehen-sive Survey of Continual Learning: Theory, Method and Application. CoRRabs/2302.00487 (2023)": "Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & Cross Networkfor Ad Click Predictions. In Proceedings of the 11th International Workshop onData Mining for Online Advertising (ADKDD). 12:112:7. Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong,and Ed Chi. 2021. DCN V2: Improved Deep & Cross Network and PracticalLessons for Web-scale Learning to Rank Systems. In Proceedings of the WebConference (WWW). 17851797.",
  "Jason Weston, Sumit Chopra, and Antoine Bordes. 2015. Memory Networks. In3rd International Conference on Learning Representations (ICLR)": "Hamed Zamani, Fernando Diaz, Mostafa Dehghani, Donald Metzler, and MichaelBendersky. 2022. Retrieval-Enhanced Machine Learning. In The 45th InternationalACM SIGIR Conference on Research and Development in Information Retrieval(SIGIR). 28752886. Yang Zhang, Fuli Feng, Chenxu Wang, Xiangnan He, Meng Wang, Yan Li, andYongdong Zhang. 2020. How to Retrain Recommender System?: A SequentialMeta-Learning Method. In Proceedings of the 43rd International ACM SIGIR Con-ference on Research and Development in Information Retrieval (SIGIR). 14791488.",
  "Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu,and Kun Gai. 2018. Deep Interest Evolution Network for Click-Through RatePrediction. CoRR abs/1809.03672 (2018)": "Guorui Zhou, Xiaoqiang Zhu, Chengru Song, Ying Fan, Han Zhu, Xiao Ma,Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Networkfor Click-Through Rate Prediction. In Proceedings of the SIGKDD InternationalConference on Knowledge Discovery & Data Mining (KDD). 10591068. Jieming Zhu, Jinyang Liu, Shuai Yang, Qi Zhang, and Xiuqiang He. 2021. OpenBenchmarking for Click-Through Rate Prediction. In The 30th ACM InternationalConference on Information and Knowledge Management (CIKM). 27592769. Jieming Zhu, Kelong Mao, Quanyu Dai, Liangcai Su, Rong Ma, Jinyang Liu,Guohao Cai, Zhicheng Dou, Xi Xiao, and Rui Zhang. 2022. BARS: Towards OpenBenchmarking for Recommender Systems. In Proceedings of the 45th InternationalACM SIGIR Conference on Research and Development in Information Retrieval(SIGIR). 29122923. Jieming Zhu, Guohao Cai Qinglin Jia, Quanyu Dai, Jingjie Li, Zhenhua Dong,Ruiming Tang, and Rui Zhang. 2023. FINAL: Factorized Interaction Layer forCTR Prediction. In Proceedings of the 46th International ACM SIGIR Conference onResearch and Development in Information Retrieval (SIGIR)."
}