{
  "ABSTRACT": "Time series analysis stands as a focal point within the data min-ing community, serving as a cornerstone for extracting valuableinsights crucial to a myriad of real-world applications. Recent ad-vances in Foundation Models (FMs) have fundamentally reshapedthe paradigm of model design for time series analysis, boostingvarious downstream tasks in practice. These innovative approachesoften leverage pre-trained or fine-tuned FMs to harness generalizedknowledge tailored for time series analysis. This survey aims tofurnish a comprehensive and up-to-date overview of FMs for timeseries analysis. While prior surveys have predominantly focused oneither application or pipeline aspects of FMs in time series analysis,they have often lacked an in-depth understanding of the underlyingmechanisms that elucidate why and how FMs benefit time seriesanalysis. To address this gap, our survey adopts a methodology-centric classification, delineating various pivotal elements of time-series FMs, including model architectures, pre-training techniques,adaptation methods, and data modalities. Overall, this survey servesto consolidate the latest advancements in FMs pertinent to timeseries analysis, accentuating their theoretical underpinnings, recentstrides in development, and avenues for future exploration.",
  "Q. Wen is the corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08...$15.00",
  ": Roadmaps of representative TSFMs": "ACM Reference Format:Yuxuan Liang, Haomin Wen, Yuqi Nie, Yushan Jiang, Ming Jin, DongjinSong, Shirui Pan, and Qingsong Wen. 2024. Foundation Models for TimeSeries Analysis: A Tutorial and Survey. In Proceedings of the 30th ACMSIGKDD Conference on Knowledge Discovery and Data Mining (KDD 24),August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.",
  "INTRODUCTION": "Time series data are characterized by their sequential order andtemporal dependencies, encapsulating valuable information aboutthe dynamics of diverse systems and processes . Varioustime series data (e.g., stock price, traffic flow, electricity) presentunique challenges and opportunities for computational analysis,each requiring tailored approaches to effectively capture their in-herent properties. The analysis and understanding of time seriesdata is an important piece of data mining, facilitating crucial in-sights and decisions in many domains , including finance, healthcare , cloud computing , environments, energy , and urban computing .In recent years, the advancements of deep learning, especiallythe transformer-based models , have revolutionized the field oftime series analysis . Following the pioneering work , therehas been a surge in research interest exploring the application of",
  "(Ours)Methodology": "transformers for time series analysis . The motiva-tion behind deep learning and transformers lies in their ability toautomatically learn comprehensive representations from raw data,thus capturing complex nonlinear relationships and temporal de-pendencies without the need for manual feature engineering. Suchcapability leads to significant performance improvements comparedwith traditional statistical methods across numerous time series ap-plications. Foundation models (FMs), such as large language models(LLMs) in natural language processing (NLP) and advancedmodels in computer vision (CV) , have emerged as powerfulparadigms capable of achieving state-of-the-art performances intheir respective fields. The success of these FMs can be attributedto their ability to leverage vast amounts of data to cultivate general-purpose representations, subsequently fine-tuning them, or evendeploying them directly in a zero-shot manner to excel across adiverse spectrum of downstream tasks. This approach not onlyeconomizes on the need for task-specific model development butalso encapsulates a broad understanding of the world, endowingthese models with exceptional versatility and efficiency .Inspired by the remarkable achievements of FMs in broad do-mains like CV and NLP, the concept of Time Series FoundationModels (TSFMs) has garnered attention as a promising directionfor time series analysis. TSFMs aim to harness the power of thefoundation model paradigm to develop generalized models profi-cient in understanding and forecasting time series data spanningdiverse domains. By capitalizing on large-scale time series datasets,TSFMs hold the promise of attaining superior performance on aspectrum of time series tasks, offering a unified framework thatcan accelerate research and application developments in this field.Despite the promising prospects and rapid development of TSFMs,a systematic analysis of TSFMs from a methodological standpointhas been notably absent in prior literature. Existing studies, as de-picted in , have concentrated on either the data perspective or the pipeline perspective of TSFMs. To bridge this gap,this survey aims to provide a comprehensive methodological anal-ysis of foundation models for learning a variety of time series. Thisexamination will center on scrutinizing their model architectures,pre-training techniques, adaptation methods, and data modalities.Through this endeavor, we seek to illuminate an overall picture ofcore elements in TSFMs, thereby enhancing comprehension regard-ing the rationale behind their efficacy and the mechanisms drivingtheir substantial potential in time series analysis.In contrast to previous surveys, this manuscript incorporatesthe most extensive array of time series data types (see ), spatial time series, as well as other types such as the trajectoryand event. We further summarize the developmental roadmap ofcurrent TSFMs in , in order to foster further innovationsand understanding in the dynamic and ever-evolving landscape ofTSFMs. In short, our major contributions lie in three aspects: Comprehensive and up-to-date survey. We offer a compre-hensive and up-to-date survey on foundation models for a widespectrum of time series, encompassing standard time series, spa-tial time series, and other types (i.e., trajectories and events). Novel methodology-centric taxonomy. We introduce a noveltaxonomy that offers a thorough analysis from a methodologicalstandpoint on TSFMs with the first shot, enabling a full under-standing of the mechanism on why and how FMs can achieveadmirable performance in time series data.",
  "BACKGROUND": "Foundation Models. Foundation models (FMs), also known aslarge pre-trained models, are a class of deep models that are pre-trained on vast amounts of data, thus equipped with a wide range ofgeneral knowledge and patterns. To this end, these models serve asa versatile starting point for various tasks across different domains.Specifically, FMs can be fine-tuned or adapted to specific tasks withrelatively small amounts of task-specific data, showcasing remark-able flexibility and efficiency. In CV, FMs such as text-promptedmodel CLIP and visual-prompted model SAM have pro-pelled advancements in image recognition, object detection, andmore. In NLP, FMs such as BERT and GPT-3 have revolu-tionized text understanding and generation tasks. Inspired by thegreat success of FMs in the above domains, this survey delves intothe utilization of these models in the realm of time series analysis.Concretely, we investigate TSFMs from a methodology perspec-tive: the components of foundation models encompass the datamodality, architecture, pre-training, and adaptation technicals: 1)data modality refers to the type of data used for model training,from single modality such as time series, text, images, and audioto multimodality; 2) architecture refers to which deep neural net-work is adopted as the backbone of FM, with Transformers being a popular choice for their ability to handle sequential dataeffectively; 3) Pre-training involves how to train the model onlarge, diverse datasets to gain a broad understanding of the data,using supervised or self-supervised learning; 4) Adaptation, such asfine-tuning or few-shot learning, is employed to accommodate thepre-trained FMs to specific tasks. This comprehensive frameworkof FMs, spanning from data modality to adaptation, facilitates theunderstanding of using them in time series analysis.Categories of Time Series. A time series is commonly describedas an ordered sequence of data points. illustrates varioustypes of time series discussed in this survey, including standardtime series, spatial time series, trajectories, and events. Note thattrajectories and events can be regarded as time series since eachdata point is associated with a specific timestamp (and location),allowing for analysis using time series techniques such as anomalydetection. These time series are formulated as follows.",
  ": Illustration of various types of time series": "Definition 1 (Standard Time Series). The standard time series isdefined as a sequence of data points ordered by time. It can bedenoted by X = {x1, x2, , x } R , where x R is thedata point at time step , and is the dimension of each data points.When = 1, X is referred to as a univariate time series, while > 1, X is a multivariate time series.Definition 2 (Spatial Time Series). It refers to a sequence of datapoints with both temporal and spatial dimensions, which can be rep-resented by X = {X1, X2, , X } R , where X R denotes the signals generated by sensors with each equippedwith features. Besides, the sensors are usually associated withspatial correlations, according to which the spatial time series can befurther divided into two subtypes: i) spatio-temporal graph, whenthe spatial correlation of those sensors is described by a graph with adjacent matrix A R ; ii) spatio-temporal raster, whensensors are distributed uniformly as a grid in geographical space.Definition 3 (Trajectory). A trajectory is a sequence of times-tamped locations that describe the movements of an object in thegeographical space. It can be formulated as T = {(1,2, , } R 2, where means the objects location at time , representedby the two-dimensional coordinates, i.e., latitude and longitude.Definition 4 (Event Sequence). An event sequence is a tempo-rally ordered set of events that describe the progression of actionsor occurrences within a specific context. It can be formalized asE = {(1,1), (2,2), . . . , (,)}, where is an event describedby a predicate-argument structure that captures the nature of theoccurrence, and denotes the timestamp when occurs.",
  "TAXONOMY": "The proposed taxonomy is illustrated in , and the relatedworks can be found in . The proposed taxonomy unfoldsa structured and comprehensive classification to enhance the un-derstanding of foundation models on time series analysis. It isorganized into four hierarchical levels, starting with the data cate-gory, followed by the model architecture, pre-training techniques,and finally, the application domain. Unlike previous taxonomies,ours distinguishes itself by delving deeper into the foundation mod-els from the methodology perspective, with a keen focus on theirarchitectural designs, pre-training, and adaptation techniques. Thismethod-centric view is pivotal for researchers, providing valuableinsights into the mechanisms of why and how foundation modelsshow great potential for time series analysis.Diving into the details of data categories, we classify the timeseries data into three distinct types: standard time series, spatialtime series, and others, which encompass trajectory and event data.Standard time series data, characterized by their sequential orderand temporal dependencies, form the backbone of traditional time series analysis. Spatial time series data, on the other hand, introducean additional layer of complexity by incorporating geographicalor spatial information, making them crucial for applications inurban computing and environmental monitoring. Lastly, the otherscategory, including trajectory and event data, represents diversedatasets where time plays a critical role, such as the movement ofobjects over time or the occurrence of specific events, offering abroadened perspective on time series analysis.From the methodology perspective: i) regarding model architec-ture, the proposed taxonomy highlights three primary categories:Transformer-based, non-Transformer-based, and diffusion-basedmodels. Transformer-based models leverage self-attention mecha-nisms to capture long-range dependencies within time series, of-fering significant advantages in handling sequential data. Non-transformer-based models, with their diverse architectures, caterto a wide range of time series tasks by efficiently processing tem-poral patterns. Diffusion-based models, a novel addition, employstochastic processes to model the data generation process, present-ing innovative solutions for time series analysis. ii) In terms ofpre-training techniques, the proposed taxonomy divides them intofully-supervised and self-supervised methods, the latter of whichincludes contrastive, generative, and hybrid approaches. This clas-sification shows how different FMs are trained with or withoutlabels. iii) Adaptation strategies, such as zero-shot learning, promptengineering, tokenization, and fine-tuning, further exemplify theversatility of FMs in customizing to specific time series applications.",
  "Standard Time Series": "Standard time series possess diverse properties, including varyingsampling rates and temporal patterns, which pose significant chal-lenges in developing relevant FMs. These models aim to identifyuniversal patterns within extensive time series data from variedsources, either to enhance specific tasks or for broad analysis.Most of the existing attempts are in the category of task-orientedstandard time series foundation models. They leverage single ormultiple data modalities to craft robust models targeting particulartime series tasks, typically forecasting or classification. For modelsinvolved only in a single (i.e., time series) modality, they may eitherbe developed from scratch or on existing pre-trained models fromother domains like large language or vision models .In the first group, Lag-Llama and TimeGPT-1 representpioneering efforts as forecasting foundation models. Both modelsundergo pre-training on a vast collection of time series data span-ning multiple domains. Lag-Llama employs a decoder-only trans-former architecture, utilizing lags as covariates, whereas TimeGPT-1 features an encoder-decoder structure with several transformerlayers, facilitating efficient zero-shot forecasting. Another note-worthy contribution is TTMs , a recent endeavor in creating adomain-agnostic forecasting model built upon TSMixer , whichitself is pre-trained on diverse time series datasets from various",
  ": A comprehensive taxonomy of TSFMs, categorized according to data and methodologies": "domains. Echoing Lag-Llamas approach, TimesFM emergesas a decoder-only model exhibiting strong zero-shot forecastingcapabilities. Concurrently, Moirai introduces an approach withits masked encoder-based universal forecasting transformer, cou-pled with a new pre-training dataset (LOTSA), containing 27 bil-lion observations from nine distinct domains. Additionally, theexploration extends to diffusion models like TimeGrad andTransFusion , which primarily focus on optimizing a variationalbound on data likelihood, transforming white noise into meaningfulsamples of the target distribution. Pre-training from scratch can be expensive, which has spurredthe development of alternative approaches that leverage pre-trainedmodels from other domains, such as large language, vision, andacoustic models. For instance, LLM4TS and TEMPO suc-cessfully perform time series forecasting across various datasets byfine-tuning GPT-2 backbones, predicated on the notion thatLLMs can be adapted to process non-linguistic datasets by activat-ing their inherent capabilities. Similarly, Voice2Series engagesin the synchronization of time series and acoustic data to harnessthe classification prowess of an acoustic model for time series data.",
  "Foundation Models for Time Series Analysis: A Tutorial and SurveyKDD 24, August 2529, 2024, Barcelona, Spain": "5.1.2Non-Transformer-based Models. Excluding the widespreadadoption of Transformers, a diverse array of traditional pre-trainingmethods leveraged models such as Multi-Layer Perceptrons (MLPs), Recurrent Neural Networks (RNNs) , and ConvolutionalNeural Networks (CNNs) as the backbone for pre-training.These models, each with their unique strengths, are notable fortheir effectiveness in both conventional and spatial time series data.MLPs and CNNs are both acclaimed for their capabilities inmodeling spatial and temporal data effectively. CNN-based archi-tectures, in particular, have garnered significant attention in self-supervised learning for general time series representation, witha notable emphasis on the usage of ResNet and dilatedconvolution layers as foundational backbones. Those ap-proaches predominantly employ 1D convolutional operations. Incontrast, TimesNet introduces a novel perspective by convert-ing 1D time series data into 2D tensors, facilitating the adaptiveidentification of multi-periodicity and the extraction of complextemporal variations through the use of a parameter-efficient incep-tion block. MLP-based models, on the other hand, are lauded fortheir lightweight design, offering benefits in terms of reduced com-putational time and cost. TSMixer and TTMs , as instances,both claiming superior efficiency in memory usage and processingspeed while still delivering competitive performance.RNNs have been acknowledged for their proficiency in temporaldata modeling . Recently, there has been a resurgence ofinterest in RNN architectures, which poses a compelling challengeto the prevailing Transformer-based models. This trend is drivenby the quest for models that are not only more resource-efficientbut also adept at handling longer sequences through their inherentlinear complexity. A notable embodiment is the RWKV-TS ,which leverages the RWKV , an RNN-type foundation modelarchitecture, demonstrating promising potential for general timeseries analysis. This emerging trend presents a valuable opportunityfor time series research and applications. 5.1.3Diffusion-based Models. Diffusion-based foundation mod-els have gained prominence in CV and video due totheir proficiency in learning complex data distributions, yet theirexploration in time series analysis remains nascent. These modelsfunction by gradually introducing and then reversing noise to data,effectively learning the generative process of original data throughthe reverse diffusion process. This unique mechanism equips diffu-sion models with great potential to serve as versatile foundationmodels capable of tackling prediction, imputation, and anomalydetection in time series.In standard time series and other temporal data, diffusion modelspredict future states by capturing temporal dynamics, generatingsmooth transitions from current to potential future states .Applied to spatial time series, they extend this capability to modelspatial correlations alongside temporal ones, providing insightsinto the interplay between space and time, particularly beneficialin fields like traffic forecasting .",
  "Spatial Time Series": "In complex real-world systems, time series data often display intri-cate spatial dependencies alongside temporal dynamics, manifest-ing in forms such as spatio-temporal graphs and rasters. Similar tothe discussion in Sec. 4.1, research on spatial time series typicallyencompasses areas such as forecasting and classification. Unlikefoundation models for standard time series, most existing researchon spatial time series is still in its early stages, often characterizedby being domain-specific, single-modality, and task-oriented. Inthe following, we categorize related works into two specific datamodalities and discuss them in different subsections. 4.2.1Spatio-Temporal Graph. Most foundation models for spatio-temporal graphs are task-oriented and only focused on graph data.In the transportation sector, TFM utilizes graph structuresand algorithms to analyze the behavior and interactions within transportation systems, showing promising results in urban trafficforecasting. ST-LLM combines spatio-temporal informationwith a partially frozen LLM to improve traffic predictions, whileDiffSTG applies denoising diffusion models to spatio-temporalgraphs for probabilistic traffic forecasting. Efforts towards domain-agnostic models include STEP , which links spatio-temporalGNNs with a pre-trained transformer for enhanced forecasting bylearning from extensive historical data. Similarly, STGCL andSPGCL explore the integration of contrastive learning intospatio-temporal graph forecasting, indicating its potential benefits.Research on general-purpose models for spatio-temporal graphs islimited. A notable example, USTD , introduces a unified modelfor both forecasting and kriging tasks, employing an uncertainty-aware diffusion approach to address diverse challenges effectively. 4.2.2Spatio-Temporal Raster. Spatio-temporal raster refers to adata modality that captures and organizes spatial information overvarious time points in a grid-like format. This modality is primar-ily utilized in climate foundation models. For instance, FourCast-Net is a global, data-driven weather forecasting model deliver-ing accurate short to medium-range predictions worldwide. Similarmodels, such as FengWu and W-MAE , follow suit. No-tably, Pangu-Weather , which is trained on 39 years of global data,achieves superior deterministic forecasting outcomes across all eval-uated variables compared to leading numerical weather predictionsystems. On a different note, ClimaX aims at general-purposeclimate foundation models, pre-trained with diverse datasets cover-ing various variables, spatio-temporal scopes, and physical contexts.It is designed for fine-tuning across a wide array of climate andweather-related tasks, such as forecasting, projection, and down-scaling, even for atmospheric variables and spatio-temporal scalesnot encountered during its pre-training phase. However, there isa scarce number of domain-agnostic models for spatio-temporalraster data. DYffusion , for example, capitalizes on the tempo-ral dynamics inherent in raster data, integrating these dynamicsdirectly with the models diffusion steps to create a stochastic, time-conditioned interpolator and forecasting network.",
  "Others": "In addition to standard and spatial time series, various other typesof data incorporate the temporal dimension, including trajecto-ries, events, and clinical records. A majority of studies in this cat-egory focus on trajectory data. For Transformer-based models,AuxMobLCast fine-tunes pre-trained LLMs through mobilityprompting and auxiliary POI Category classification to forecast hu-man mobility patterns, effectively bridging the gap between naturallanguage processing and temporal sequence prediction. LLM-Mob encodes human mobility data into structured prompts that in-struct LLMs to consider both long-term and short-term behavioralpatterns, along with time-specific context, to generate accurate andexplainable predictions of future locations. For non-Transformer-based models, Trembr leverages auto-encoding techniques toextract road network and temporal information embedded in trajec-tories effectively. While START introduces a hybrid approachto trajectory embedding learning by combining masked languagemodel and SimCLR to enhance its learning capability.More recently, GTM separates trajectory features into three",
  ": Architectures of TSFMs": "domains, which can be masked and generated independently tomeet specific input and output requirements of a given task. Then,GTM is pre-trained by reconstructing densely sampled trajectoriesin an auto-regressive manner given re-sampled sparse counter-parts. For the diffusion-based model, DiffTraj reconstructsand synthesizes geographic trajectories from white noise througha conditioned reverse trajectory denoising process.",
  "Architecture": "As shown in , we first delve into the architecture of TSFMs,including Transformer-based models, non-Transformer-based nodelsand diffusion-based models, focusing on the underlying mechanismsthat shape their capabilities, as well as how they could be appliedon various time series. 5.1.1Transformer-based Models. The architecture of FMs has seena significant convergence towards the Transformer , a modelarchitecture first introduced for NLP tasks. The core innovation ofthe Transformer lies in its utilization of the attention mechanism,which allows the model to dynamically focus on different parts ofthe input data. The attention function can be succinctly describedas Attention(, , ) = Softmax( / ) , where , , and represent the queries, keys, and values matrices respectively,each with dimensions , and serves as a scaling factorto moderate the dot products magnitude. It is evident from theformula that the attention mechanism has the capability to learnglobal, long-range dependencies in data. This distinguishes it fromprevious architectures, which were often limited by their localreceptive fields or dependency windows. Besides, the Transformersdesign is inherently friendly to parallelization, which allows forsignificant scalability, enabling the processing of large datasetsand the construction of models with billions of parameters. Suchscalability and efficiency in capturing intricate data patterns haveled to the widespread adoption of the Transformer architecturebeyond its initial application in natural language processing (NLP) to fields including computer vision (CV), speech, video, timeseries () and beyond.The choice of foundation model framework remains debatedin the realm of time series analysis, contrasting the trend towardsdecoder-only models in natural language processing. Notable works in this area includes encoder-only , encoder-decoder, and decoder-only models. Ansari et al. analyze the applicability of the encoder-decoder framework todecoder-only models. Liu et al. discuss that while the encoder-only model is favored in time series forecasting for its effectivenesson small datasets, the decoder-only architecture, with its stronggeneralization and capacity, could be preferred for large-scale timeseries models. The diversity in the architectural choices underscoresthe potential and necessity for further exploration within this field.In terms of standard time series analysis, the Transformer archi-tecture leverages its sequence modeling capabilities to capture tem-poral dynamics. This includes either repurposing pretrained LLMsfor time series to leverage their preexisting sequence modelingstrengths , or directly using Transformer as a base for TSFMs,training from scratch to achieve models best suited for the specificsof time series data . Besides, various techniques have been inno-vated to augment the functionality of Transformer models in timeseries analysis comprehensively. A common practice in TSFMs seg-ments time series into patches, which can effectively encapsulatelocal dynamics within input tokens .Another critical design is the normalization layer, where reversibleinstance normalization techniques, standardizing data throughinstance-specific mean and variance then reverting it at the outputlayer, have found extensive application across the above models.Moreover, specialized approaches such as multi-resolution analy-sis, exemplified by Moirai through the employment of vary-ing patch sizes, and decomposition strategies, as implemented byTEMPO via the separation of complex interactions into thetrend, seasonal, and residual components, have been shown toenhance model efficacy substantially.For spatial time series, the attention mechanism is utilized tomodel both the spatial and temporal dependency. For instance, ST-LLM employs a novel partially frozen attention strategy fortraffic prediction, leveraging spatial-temporal embeddings to cap-ture the intricate dynamics of traffic data across space and time.Conversely, other studies opt for independent modeling of spa-tial and temporal relationships. TFM is a case in point, whichemploys attention mechanisms within a dynamic graph encoderfor spatial modeling, integrating time encoding for temporal as-pects, embodying principles of transformers in addressing trafficsystems spatial-temporal dependencies. Besides simultaneouslymodeling spatial and temporal relationships, there exists an alterna-tive approach that augments the Transformer model with additionalspatial models or external spatial information to enhance its capa-bilities in the temporal modeling of time series. An example of thisis STEP , which uses unsupervised pre-trained TransFormerblocks to model temporal relationship from long-term history timeseries, while applying a graph structure learner and spatio-temporalGNNs based on the representation of TransFormer blocks. Further-more, the application of Transformer models extends to the domainof spatial-temporal prompt learning, as evidenced by initiativessuch as MetePFL and FedWing .In addition to conventional time series data, the Transformerarchitecture has demonstrated efficacy across a diverse array oftemporal datasets, such as trajectory and healthcare records, as sum-marized in . This expansion highlights the Transformersversatile capacity for temporal data analysis.",
  "In this part, we review TSFMs from the pipeline perspective, in-cluding diverse model acquisition and adaptation mechanisms": "5.2.1Pre-training. Pre-training is an initial and crucial step forbuilding TSFMs, since the knowledge learned in this phase enablesthe models to generalize across different contexts and quickly adaptto various downstream tasks with minimal adaptations. On theother hand, the diverse nature of pre-training data (e.g., standardtime series, spatial time series, and trajectories), as well as the waythe data is used, lead to a wide spectrum of pre-training mechanismswhen building and deploying foundation model. In this survey, wepropose a new perspective mostly based on learning objectives inthe pre-training phase, to categorize existing methods for TSFMs.These mechanisms include fully-supervised, self-supervised (gener-ative, contrastive, hybrid of generative and contrastive), and others.Fully-supervised pre-training refers to the strategy where thefoundation model is initially trained on one or multiple large time se-ries datasets with labels to capture the complex temporal dynamicsand learn generalizable representations. TTMs proposes a uni-versal time series foundation model supervised framework that isable to handle the heterogeneity of multiple time series datasets andeffectively build the forecasting capability during pre-training, viathe design of multi-resolution enhancements (e.g., adaptive patch-ing, data augmentation via downsampling, etc.) Fully-supervisedpre-training for TSFMs is particularly suited for scenarios wherethere is sufficient labeled historical data. Moreover, this pre-trainingtechnique is more frequently used in some domain-specific applica-tions such as transportation and climate , where themodel can be directly tailored for downstream forecasting taskswith the ease of minimal adaptations.We categorize the generative pre-training strategy as a generalmodeling of time series representations, including reconstructionand probabilistic modeling of time series inputs. In reconstruction-based pre-training, an effective learning objective is to recover theoriginal input space via masked autoencoding strategies . Inthe probabilistic modeling methods, the latent representation spaceformed from temporal or spatial-temporal encoders is optimized to-ward an estimated density via maximizing log-likelihood, based onwhich the forecasts can be sampled . Moreover, it is also ben-eficial to leverage contrastive learning to enhance the robustness ofpre-training time series foundation models. The key is to constructand utilize the self-supervision signals by generating informativepositive pairs as well as filtering out unsuitable negative pairs whenperforming augmentation . In addition to the aforementionedtwo self-supervised strategies, the efficacy of the hybrid varianthas also been validated, where the pre-trained model on fewer timeseries data outperforms the supervised counterpart .In general, self-supervised pre-training enables foundation mod-els to exploit the vast amounts of unlabeled time series data, pro-viding generic temporal knowledge that can be further fine-tunedfor specific downstream tasks. Compared with fully-supervisedpre-training, it provides a more generic and realistic solution forthe acquisition of a time series foundation model.Note that the above pre-training methods typically build themodel from scratch and obtain the universal knowledge from datawith the same modality (i.e., time series). Nevertheless, recent ad-vancements in time series research have heightened the usage ofLLMs ,VLMs , and AMs that are pre-trained from other datamodalities (text sequence, image-text sequence, acoustic signals).",
  ": Illustration of different adaptation techniques": "Direct usage (also called zero-shot), means no further fine-tuningon the target datasets, suggesting the sufficient capability of apre-trained model for downstream tasks. It can also indicate thehomogeneity between the pre-trained dataset and target dataset,especially for some real-world applications where a foundationmodel is built to fulfill domain-specific tasks .Fine-tuning is a common strategy to adapt foundation models totarget tasks. Based on the way the foundation model is used on thetarget dataset, there are three mainstream works: fine-tuning thewhole model or specific components (e.g., training posi-tional embeddings and layer normalization, while keeping feedfor-ward and attention layers frozen when fine-tuning LLMs) ,to directly infer results, or integrate foundation models as part ofthe whole model .Prompt engineering is more specialized in LLM-based TSFMs.The prompt can be handcrafted with task-specific textual input anddirectly used to query the output for downstream prediction or intermediate embedding as feature enhancement . Be-sides, the prompt can also be parameterized vectors and end-to-endlearnable when optimizing the model on target datasets . Incomparison to static prompts, the use of trainable prompts enhancesthe ability of LLMs to comprehend and match the context of giventime series inputs. For example, TEMPO constructs a trainableprompt pool with distinct key-value pairs, and retrieves the mostrepresentative prompt candidates with the highest similarity scores.Time series tokenization aims to effectively represent the timeseries as embeddings, which is also more frequently adopted intransformer-based architectures . Common tokeniza-tion techniques include reversible instance normalization thatmitigates distribution shift, patching with channel independencestrategy that effectively and efficiently extracts the time series con-text , as well as the joint usage of time series decompositionto explicitly represent explainable components for the ease ofsubsequent temporal modeling.In addition to the main branches of adapting TSFMs, it is alsoworth noting that some fine-tuning strategies take real-world con-straints into account. For example, the fine-tuning is performed ina privacy-preserved manner .",
  "Modality": "During the pre-training/adaptation of TSFMs, prior methods in-volve single or multiple data modalities, where standard time series,trajectory, raster, and text can be treated as different forms withunique domain perspectives. In this part, we review the data modal-ities that are used in existing TSFMs across different domains. 5.3.1Single-modality. A majority of current TSFMs are constructedand tailored on the basis of single-modal data. Compared withmulti-modal methods, the single-modal time series modeling strat-egy gains the advantages of inherent simplicity and bypasses thechallenges of handling modality gaps, yet frequently demonstratesexcellent empirical results across a wide range of real-world appli-cations, such as traffic and climate forecasting . 5.3.2Multi-modality. However, the single-modal methods maynot encapsulate the full picture for several challenging downstreamtasks in finance and healthcare domains . To copewith this issue, there have been initiatives towards developing multi-modal, task-oriented FMs, where additional information providesuseful information to enhance the model capability. In Chen et al.,an external ChatGPT is queried to construct an evolving graphstructure representing companies, based on the analysis of newsheadlines at specific time steps. As such, the inferred graph andstock prices are fed into the time series model (that uses GNN andLSTM for information propagation) to generate stock price move-ment predictions. Another example in healthcare also demonstratedthe effectiveness of multi-modal medical context modeling, whichaligns the embedding of ECG (Electrocardiogram) and correspond-ing medical text reports under a self-supervised contrastive learningframework and performs ECG classification. In general multi-modaltime series analysis, similar cross-modality alignment strategies(e.g., contrastive learning , reprogramming , token-wiseprompting ) are adopted, where the multi-modal inputs are of-ten the textual description of datasets and pre-training word embed-ding from LLMs. As a notable example, Time-LLM introducesa reprogramming framework that aligns the language knowledgefrom pre-trained word embedding and time series information vialinear projection and multi-head attention, where the handcrafteddataset descriptions are also used to quired text token embeddingsas prompts, which further enhances the embedding space and in-forms the LLM to comprehend the task contexts. As such, utilizingmulti-modal data facilitates the repurpose of the existing LLM intotime series forecasters without additional computational costs.",
  "CONCLUSION": "The rapid development of FMs has revolutionized the research fieldsin different domains. In this survey, we provide a comprehensiveand updated review of FMs specifically designed for time series anal-ysis. A novel taxonomy is proposed from a methodology-centricperspective by classifying FMs based on key components includingmodel architecture, pre-training technique, adaptation technique,and data modality. Our survey facilitates understanding the under-lying mechanism of applying the FMs to time series. Furthermore,we believe that the consolidation of the latest advancements, aswell as the potential future direction (see Appendix), can inspiremore innovative works within the field of time series analysis.",
  "This work is mainly supported by the Guangzhou-HKUST(GZ)Joint Funding Program (No. 2024A03J0620). It is also funded byGuangzhou Municipal Science and Technology Project 2023A03J0011": "Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, PedroMercado, Huibin Shen, Oleksandr Shchur, Syama Syndar Rangapuram, Sebas-tian Pineda Arango, Shubham Kapoor, Jasper Zschiegner, Danielle C. Maddix,Michael W. Mahoney, Kari Torkkola, Andrew Gordon Wilson, Michael Bohlke-Schneider, and Yuyang Wang. 2024. Chronos: Learning the Language of TimeSeries. arXiv preprint arXiv:2403.07815 (2024). Muhammad Awais, Muzammal Naseer, Salman Khan, Rao Muhammad Anwer,Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, and Fahad Shahbaz Khan.2023. Foundational Models Defining a New Era in Vision: A Survey and Outlook.arXiv preprint arXiv:2307.13721 (2023).",
  "Marin Bilo, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka, and StephanGnnemann. 2022. Modeling temporal data as continuous functions with processdiffusion. arXiv preprint arXiv:2211.02590 (2022)": "Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora,Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, EmmaBrunskill, et al. 2021. On the opportunities and risks of foundation models.arXiv preprint arXiv:2108.07258 (2021). Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, DavidSchnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, andAditya Ramesh. 2024. Video generation models as world simulators. (2024). Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shot learners. Advances in neuralinformation processing systems 33 (2020), 18771901.",
  "Ching Chang, Wen-Chih Peng, and Tien-Fu Chen. 2023. LLM4TS: Two-StageFine-Tuning for Time-Series Forecasting with Pre-Trained LLMs. arXiv preprintarXiv:2308.08469 (2023)": "Yanchuan Chang, Jianzhong Qi, Yuxuan Liang, and Egemen Tanin. 2023. Con-trastive Trajectory Similarity Learning with Dual-Feature Attention. In 2023IEEE 39th International Conference on Data Engineering (ICDE). IEEE, 29332945. Kang Chen, Tao Han, Junchao Gong, Lei Bai, Fenghua Ling, Jing-Jia Luo, XiChen, Leiming Ma, Tianning Zhang, Rui Su, et al. 2023. FengWu: Pushing theSkillful Global Medium-range Weather Forecast beyond 10 Days Lead. arXivpreprint arXiv:2304.02948 (2023). Shengchao Chen, Guodong Long, Tao Shen, and Jing Jiang. 2023. PromptFederated Learning for Weather Forecasting: Toward Foundation Models onMeteorological Data. In International Joint Conference on Artificial Intelligence.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding.In NAACL-HLT. 41714186": "Jiaxiang Dong, Haixu Wu, Yuxuan Wang, Yunzhong Qiu, Li Zhang, JianminWang, and Mingsheng Long. 2024. TimeSiam: A Pre-Training Framework forSiamese Time-Series Modeling. arXiv preprint arXiv:2402.02475 (2024). Jiaxiang Dong, Haixu Wu, Haoran Zhang, Li Zhang, Jianmin Wang, and Ming-sheng Long. 2023. SimMTM: A Simple Pre-Training Framework for MaskedTime-Series Modeling. Advances in Neural Information Processing Systems (2023). Yuntao Du, Jindong Wang, Wenjie Feng, Sinno Pan, Tao Qin, Renjun Xu, andChongjun Wang. 2021. Adarnn: Adaptive learning and forecasting of timeseries. In Proceedings of the 30th ACM international conference on information &knowledge management. 402411.",
  "Wenying Duan, Liu Jiang, Ning Wang, and Hong Rao. 2019. Pre-Trained Bidirec-tional Temporal Representation for Crowd Flows Prediction in Regular Region.IEEE Access 7 (2019), 143855143865": "Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, and JayantKalagnanam. 2023. TSMixer: Lightweight MLP-Mixer Model for MultivariateTime Series Forecasting. arXiv preprint arXiv:2306.09364 (2023). Vijay Ekambaram, Arindam Jati, Nam H Nguyen, Pankaj Dayama, ChandraReddy, Wesley M Gifford, and Jayant Kalagnanam. 2024. TTMs: Fast Multi-level Tiny Time Mixers for Improved Zero-shot and Few-shot Forecasting ofMultivariate Time Series. arXiv preprint arXiv:2401.03955 (2024). Cheng Feng, Long Huang, and Denis Krompass. 2024. Only the Curve ShapeMatters: Training Foundation Models for Zero-Shot Multivariate Time SeriesForecasting through Next Curve Shape Prediction. arXiv:2402.07570 [cs.LG]",
  "Junfeng Hu, Xu Liu, Zhencheng Fan, Yuxuan Liang, and Roger Zimmermann.2023. Towards Unifying Diffusion Models for Probabilistic Spatio-TemporalGraph Learning. arXiv:2310.17360 [cs.LG]": "Hongbin Huang, Minghua Chen, and Xiao Qiao. 2024. Generative Learning forFinancial Time Series with Irregular and Scale-Invariant Patterns. In The TwelfthInternational Conference on Learning Representations. Jiawei Jiang, Dayan Pan, Houxing Ren, Xiaohan Jiang, Chao Li, and JingyuanWang. 2022. Self-supervised Trajectory Representation Learning with TemporalRegularities and Travel Semantics. CoRR abs/2211.09510 (2022). Lavender Yao Jiang, Xujin Chris Liu, Nima Pour Nejatian, Mustafa Nasir-Moin,Duo Wang, Anas Abidin, Kevin Eaton, Howard Antony Riina, Ilya Laufer,Paawan Punjabi, et al. 2023. Health system-scale language models are all-purpose prediction engines. Nature (2023), 16. Yushan Jiang, Zijie Pan, Xikun Zhang, Sahil Garg, Anderson Schneider, YuriyNevmyvaka, and Dongjin Song. 2024. Empowering Time Series Analysis withLarge Language Models: A Survey. arXiv preprint arXiv:2402.03182 (2024). Ming Jin, Huan Yee Koh, Qingsong Wen, Daniele Zambon, Cesare Alippi, Ge-offrey I Webb, Irwin King, and Shirui Pan. 2023. A survey on graph neuralnetworks for time series: Forecasting, classification, imputation, and anomaly",
  "detection. arXiv preprint arXiv:2307.03759 (2023)": "Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi,Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, et al. 2023. Time-LLM:Time series forecasting by reprogramming large language models. arXiv preprintarXiv:2310.01728 (2023). Ming Jin, Qingsong Wen, Yuxuan Liang, Chaoli Zhang, Siqiao Xue, Xue Wang,James Zhang, Yi Wang, Haifeng Chen, Xiaoli Li, et al. 2023. Large models fortime series and spatio-temporal data: A survey and outlook. arXiv preprintarXiv:2310.10196 (2023). Ming Jin, Yifan Zhang, Wei Chen, Kexin Zhang, Yuxuan Liang, Bin Yang, Jin-dong Wang, Shirui Pan, and Qingsong Wen. 2024. Position: What Can LargeLanguage Models Tell Us about Time Series Analysis. In International Conferenceon Machine Learning (ICML24). Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, andJaegul Choo. 2021. Reversible instance normalization for accurate time-seriesforecasting against distribution shift. In International Conference on LearningRepresentations. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, LauraGustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.2023. Segment anything. In Proceedings of the IEEE/CVF International Conferenceon Computer Vision. 40154026.",
  "Jun Li, Che Liu, Sibo Cheng, Rossella Arcucci, and Shenda Hong. 2023. FrozenLanguage Model Helps ECG Zero-Shot Learning. In Medical Imaging with DeepLearning": "Rongfan Li, Ting Zhong, Xinke Jiang, Goce Trajcevski, Jin Wu, and Fan Zhou.2022. Mining spatio-temporal relations via self-paced graph contrastive learning.In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery andData Mining. 936944. Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang,and Xifeng Yan. 2019. Enhancing the locality and breaking the memory bottle-neck of transformer on time series forecasting. Advances in neural informationprocessing systems 32 (2019). Yan Li, Xinjiang Lu, Yaqing Wang, and Dejing Dou. 2022. Generative time seriesforecasting with diffusion, denoise, and disentanglement. Advances in NeuralInformation Processing Systems 35 (2022), 2300923022. Yan Lin, Jilin Hu, Shengnan Guo, Bin Yang, Christian S. Jensen, Youfang Lin, andHuaiyu Wan. 2024. GTM: General Trajectory Modeling with Auto-regressiveGeneration of Feature Domains. arXiv:2402.07232 [cs.LG]",
  "Mingzhe Liu, Han Huang, Hao Feng, Leilei Sun, Bowen Du, and Yanjie Fu. 2023.PriSTI: A Conditional Diffusion Framework for Spatiotemporal Imputation.arXiv preprint arXiv:2302.09746 (2023)": "Xu Liu, Junfeng Hu, Yuan Li, Shizhe Diao, Yuxuan Liang, Bryan Hooi, andRoger Zimmermann. 2024. UniTime: A Language-Empowered Unified Modelfor Cross-Domain Time Series Forecasting. arXiv:2310.09751 [cs.LG] Xu Liu, Yuxuan Liang, Chao Huang, Yu Zheng, Bryan Hooi, and Roger Zimmer-mann. 2022. When do contrastive learning signals help spatio-temporal graphforecasting?. In Proceedings of the 30th International Conference on Advances inGeographic Information Systems. 112. Xin Liu, Daniel McDuff, Geza Kovacs, Isaac Galatzer-Levy, Jacob Sunshine,Jiening Zhan, Ming-Zher Poh, Shun Liao, Paolo Di Achille, and Shwetak Patel.2023. Large Language Models are Few-Shot Health Learners. arXiv preprintarXiv:2305.15525 (2023).",
  "William Peebles and Saining Xie. 2023. Scalable diffusion models with trans-formers. In Proceedings of the IEEE/CVF International Conference on ComputerVision. 41954205": "Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho,Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV,et al. 2023. Rwkv: Reinventing rnns for the transformer era. arXiv preprintarXiv:2305.13048 (2023). Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sand-hini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.2021. Learning transferable visual models from natural language supervision.In International conference on machine learning. PMLR, 87488763.",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, IlyaSutskever, et al. 2019. Language models are unsupervised multitask learners.OpenAI blog 1, 8 (2019), 9": "Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Arian Khorasani, GeorgeAdamopoulos, Rishika Bhagwatkar, Marin Bilo, Hena Ghonia, Nadhir VincentHassen, Anderson Schneider, et al. 2023. Lag-llama: Towards foundation modelsfor time series forecasting. arXiv preprint arXiv:2310.08278 (2023). Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf. 2021. Au-toregressive denoising diffusion models for multivariate probabilistic time seriesforecasting. In International Conference on Machine Learning. PMLR, 88578868.",
  "Yilong Ren, Yue Chen, Shuai Liu, Boyue Wang, Haiyang Yu, and Zhiyong Cui.2024. TPLLM: A Traffic Prediction Framework Based on Pretrained LargeLanguage Models. arXiv preprint arXiv:2403.02221 (2024)": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjrnOmmer. 2022. High-resolution image synthesis with latent diffusion models. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition.1068410695. Zezhi Shao, Zhao Zhang, Fei Wang, and Yongjun Xu. 2022. Pre-training en-hanced spatial-temporal graph neural network for multivariate time seriesforecasting. In Proceedings of the 28th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining. 15671577.",
  "Lifeng Shen and James Kwok. 2023. Non-autoregressive Conditional DiffusionModels for Time Series Prediction. arXiv preprint arXiv:2306.05043 (2023)": "Xiaoming Shi, Siqiao Xue, Kangrui Wang, Fan Zhou, James Y Zhang, Jun Zhou,Chenhao Tan, and Hongyuan Mei. 2023. Language Models Can Improve EventPrediction by Few-Shot Abductive Reasoning. In Advances in Neural InformationProcessing Systems. Md Fahim Sikder, Resmi Ramachandranpillai, and Fredrik Heintz. 2023. Trans-fusion: generating long, high fidelity time series using diffusion models withtransformers. arXiv preprint arXiv:2307.12667 (2023).",
  "Chenxi Sun, Yaliang Li, Hongyan Li, and Shenda Hong. 2023. TEST: TextPrototype Aligned Embedding to Activate LLMs Ability for Time Series. arXivpreprint arXiv:2308.08241 (2023)": "Peiwang Tang and Xianchao Zhang. 2022. MTSMAE: Masked Autoencoders forMultivariate Time-Series Forecasting. In 2022 IEEE 34th International Conferenceon Tools with Artificial Intelligence (ICTAI). IEEE, 982989. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all youneed. In Advances in Neural Information Processing Systems 30. 59986008.",
  "Xuhong Wang, Ding Wang, Liang Chen, and Yilun Lin. 2023. Building Trans-portation Foundation Model via Generative Graph Transformer. arXiv preprintarXiv:2305.14826 (2023)": "Xu Wang, Hongbo Zhang, Pengkun Wang, Yudong Zhang, Binwu Wang,Zhengyang Zhou, and Yang Wang. 2023. An observed value consistent diffusionmodel for imputing missing values in multivariate time series. In Proceedingsof the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.24092418. Yuxuan Wang, Haixu Wu, Jiaxiang Dong, Yong Liu, Yunzhong Qiu, HaoranZhang, Jianmin Wang, and Mingsheng Long. 2024. TimeXer: EmpoweringTransformers for Time Series Forecasting with Exogenous Variables. arXivpreprint arXiv:2402.19072 (2024). Zepu Wang, Yuqi Nie, Peng Sun, Nam H Nguyen, John Mulvey, and H VincentPoor. 2023. St-mlp: A cascaded spatio-temporal linear framework with channel-independence strategy for traffic forecasting. arXiv preprint arXiv:2308.07496(2023).",
  "Zhixian Wang, Qingsong Wen, Chaoli Zhang, Liang Sun, and Yi Wang. 2023.DiffLoad: uncertainty quantification in load forecasting with diffusion model.arXiv preprint arXiv:2306.01001 (2023)": "Haomin Wen, Youfang Lin, Yutong Xia, Huaiyu Wan, Qingsong Wen, RogerZimmermann, and Yuxuan Liang. 2023. DiffSTG: Probabilistic spatio-temporalgraph forecasting with denoising diffusion models. In the 31st ACM InternationalConference on Advances in Geographic Information Systems. 112. Qingsong Wen, Linxiao Yang, Tian Zhou, and Liang Sun. 2022. Robust timeseries analysis and applications: An industrial perspective. In 28th ACM SIGKDDConference on Knowledge Discovery and Data Mining. 48364837. Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan,and Liang Sun. 2023. Transformers in time series: A survey. In InternationalJoint Conference on Artificial Intelligence(IJCAI). 67786786.",
  "Christopher Wimmer and Navid Rekabsaz. 2023. Leveraging vision-languagemodels for granular market change prediction. arXiv preprint arXiv:2301.10166(2023)": "Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese,and Doyen Sahoo. 2024. Unified training of universal time series forecastingtransformers. arXiv preprint arXiv:2402.02592 (2024). Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and MingshengLong. 2022. Timesnet: Temporal 2d-variation modeling for general time seriesanalysis. In The eleventh international conference on learning representations. Yutong Xia, Yuxuan Liang, Haomin Wen, Xu Liu, Kun Wang, Zhengyang Zhou,and Roger Zimmermann. 2024. Deciphering spatio-temporal graph forecasting:A causal lens and treatment. Advances in Neural Information Processing Systems36 (2024). Qianqian Xie, Weiguang Han, Yanzhao Lai, Min Peng, and Jimin Huang. 2023.The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModalStock Movement Prediction Challenges. arXiv preprint arXiv:2304.05351 (2023).",
  "Hao Xue and Flora D Salim. 2022. PromptCast: A New Prompt-based LearningParadigm for Time Series Forecasting. arXiv preprint arXiv:2210.08964 (2022)": "Hao Xue, Bhanu Prakash Voutharoja, and Flora D Salim. 2022. Leveraginglanguage foundation models for human mobility forecasting. In the 30th Inter-national Conference on Advances in Geographic Information Systems. 19. Tijin Yan, Hongwei Zhang, Tong Zhou, Yufeng Zhan, and Yuanqing Xia. 2021.ScoreGrad: Multivariate probabilistic time series forecasting with continuousenergy-based generative models. arXiv preprint arXiv:2106.10121 (2021).",
  "Chao-Han Huck Yang, Yun-Yun Tsai, and Pin-Yu Chen. 2021. Voice2series:Reprogramming acoustic models for time series classification. In Internationalconference on machine learning. PMLR, 1180811819": "Xi Yang, Aokun Chen, Nima PourNejatian, Hoo Chang Shin, Kaleb E Smith,Christopher Parisien, Colin Compas, Cheryl Martin, Anthony B Costa, Mona GFlores, et al. 2022. A large language model for electronic health records. NPJDigital Medicine 5, 1 (2022), 194. Chin-Chia Michael Yeh, Xin Dai, Huiyuan Chen, Yan Zheng, Yujie Fan, AudreyDer, Vivian Lai, Zhongfang Zhuang, Junpeng Wang, Liang Wang, et al. [n. d.].Toward a foundation model for time series data. In Proceedings of the 32nd ACMInternational Conference on Information and Knowledge Management.",
  "Xiyuan Zhang, Ranak Roy Chowdhury, Rajesh K. Gupta, and Jingbo Shang. 2024.Large Language Models for Time Series: A Survey. arXiv:2402.01801 [cs.LG]": "Xiang Zhang, Ziyuan Zhao, Theodoros Tsiligkaridis, and Marinka Zitnik. [n. d.].Self-supervised contrastive pre-training for time series via time-frequency con-sistency. Advances in Neural Information Processing Systems 35 ([n. d.]). Yingying Zhang, Zhengxiong Guan, Huajie Qian, Leili Xu, Hengbo Liu, Qing-song Wen, Liang Sun, Junwei Jiang, Lunting Fan, and Min Ke. 2021. CloudRCA: Aroot cause analysis framework for cloud computing platforms. In Proceedings ofthe 30th ACM International Conference on Information & Knowledge Management.",
  "Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. 2023. One FitsAll: Power General Time Series Analysis by Pretrained LM. Advances in NeuralInformation Processing Systems (2023)": "Zhengyang Zhou, Qihe Huang, Kuo Yang, Kun Wang, Xu Wang, Yudong Zhang,Yuxuan Liang, and Yang Wang. 2023. Maintaining the Status Quo: CapturingInvariant Relations for OOD Spatiotemporal Learning. In Proceedings of the 29thACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 23).36033614. Yuanshao Zhu, Yongchao Ye, Shiyao Zhang, Xiangyu Zhao, and James Yu. 2024.Difftraj: Generating gps trajectory with diffusion probabilistic model. Advancesin Neural Information Processing Systems 36 (2024). Zhaoyang Zhu, Weiqi Chen, Rui Xia, Tian Zhou, Peisong Niu, Bingqing Peng,Wenwei Wang, Hengbo Liu, Ziqing Ma, Xinyue Gu, et al. 2023. Energy fore-casting with robust, flexible, and explainable machine learning algorithms. AIMagazine 44, 4 (2023), 377393.",
  "AAPPENDIX": "In this section, we discuss the future research directions and oppor-tunities of TSFMs from the methodology perspective.Incooporating Multi-modalities. As illustrated in this sur-vey, a majority of current foundation models for time series aredeveloped based on a single modality. However, many real-worlddynamic systems are coupled with various modalities (time series,text, even image data). It would be a promising direction to leveragevarious modalities along with the time series in TSFM to learn morecomprehensive and generalized knowledge, therefore significantlyboosting the performance of different downstream tasks.Exploring more Efficient Architectures. Currently, the Trans-former serves as the dominant architecture for building the foun-dation model. Though promising, Transformer-based foundationmodels have quadratic scaling with respect to the sequence lengthdue to their self-attention mechanism. This makes them compu-tationally expensive and memory-intensive for processing longsequences. Therefore, it is an interesting avenue for future studyto explore more efficient FM backbone architectures, such as state-space models Mamba .Developing more Effective Pipelines. Time series data hasunique properties such as temporal distribution shift (i.e., the data distribution will evolve over time) and causality (i.e.,casual relationship can exist between different points in the timeseries) . Therefore, it would be another existing as well as chal-lenging future direction to develop TSFMs that can well addressthe temporal distribution shift or have a powerful Interpretabilityfor downstream tasks.Protecting Privacy. Protecting privacy is an essential concernwhen training foundation models on diverse sources and modali-ties of data, which raises potential risks of exposing sensitive in-formation. As such, one future direction is the development ofrobust privacy-preserving techniques for training the TSFM frommulti-source datasets, as well as keeping the utility of the trainedFMs. This may include the advancement of federated learning ap-proaches, where models can be trained across multiple decentral-ized devices or servers without exchanging raw data."
}