{
  "Abstract": "As one of the most successful generative models, diffusion modelshave demonstrated remarkable efficacy in synthesizing high-qualityimages. These models learn the underlying high-dimensional datadistribution in an unsupervised manner. Despite their success, dif-fusion models are highly data-driven and prone to inheriting theimbalances and biases present in real-world data. Some studieshave attempted to address these issues by designing text promptsfor known biases or using bias labels to construct unbiased data.While these methods have shown improved results, real-world sce-narios often contain various unknown biases, and obtaining biaslabels is particularly challenging. In this paper, we emphasize thenecessity of mitigating bias in pre-trained diffusion models withoutrelying on auxiliary bias annotations. To tackle this problem, wepropose a framework, InvDiff, which aims to learn invariant se-mantic information for diffusion guidance. Specifically, we proposeidentifying underlying biases in the training data and designing anovel debiasing training objective. Then, we employ a lightweighttrainable module that automatically preserves invariant semanticinformation and uses it to guide the diffusion models samplingprocess toward unbiased outcomes simultaneously. Notably, weonly need to learn a small number of parameters in the lightweightlearnable module without altering the pre-trained diffusion model.Furthermore, we provide a theoretical guarantee that the implemen-tation of InvDiff is equivalent to reducing the error upper boundof generalization. Extensive experimental results on three publicly",
  "Equal contribution.Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 25, August 0307, 2025, Toronto, ON, Canada 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-XXXX-X/18/06",
  "INTRODUCTION": "Diffusion models have emerged as the most success-ful generative models to date. They have demonstrated remark-able success in synthesizing high-quality images and have alsoshown potential in a variety of domains, ranging from computervision to temporal data modeling and data min-ing . The scale of images generated by these models, especiallytext-to-image diffusion models, is staggering. For instance, overten million users utilize Stable Diffusion and DALL-E 3 to generate visually realistic images from textual descriptions .Diffusion models learn the underlying high-dimensional data dis-tribution in an unsupervised manner. Despite their success, thesemodels are highly data-driven and prone to inheriting the imbal-ances and biases present in real-world training data . Asdiffusion models become increasingly prevalent, mitigating the in-fluence of bias becomes more critical, yet this issue has receivedlittle attention within the generative model community.Real-world datasets inevitably exhibit biases and undesirablestereotypes, which impact the behavior of diffusion models. Toillustrate this, we provide an intuitive experiment demonstratingthe impact of biased datasets on the state-of-the-art text-to-imagediffusion model, Stable Diffusion. As shown in , we fine-tune Stable Diffusion on the biased Waterbirds dataset (wherelandbirds are usually in terrestrial backgrounds and waterbirdsare usually in aquatic backgrounds). In (a), we count theproportions of the two types of birds in the two backgrounds in",
  ": Proportion and samples of water/land bird in ter-restrial/aquatic backgrounds": "the original dataset. In (b), we report the proportions ofthe fine-tuned Stable Diffusion models generated results, usingthe types of birds as prompts. We find that the Stable Diffusionmodels generated results unconsciously perpetuate the same biaspresent in the dataset. The bias in diffusion models also raises fair-ness concerns. For example, researchers have found that in imagesgenerated by Stable Diffusion, women are underrepresented in high-paying occupations and overrepresented in low-paying ones .Recently, researchers have made progress in developing diffusiondebiasing methods. Previous efforts can be divided into two cate-gories: (1) Prompt-based methods for known biases . Thesemethods suggest adding ethical interventions to prompts, like allindividuals can be a lawyer irrespective of their gender, to alleviatespecific known biases such as gender and race. (2) Unbiased data-based methods. Given some unbiased data as a prerequisite, thesemethods train density ratio or a discriminator to encouragebiased diffusion models convergence to an unbiased distribution.Despite their success, we argue that these bias annotations such asbiases type and labels are usually unattainable. In fact, real-worlddata is often complex and contains unknown biases, making thesemethods less effective when dealing with arbitrary and unknownbiases. In such scenarios, obtaining unbiased data becomes evenmore challenging.In this work, we investigate the challenging yet practical researchproblem of mitigating unknown biases in text-to-image diffusionmodels without relying on auxiliary bias annotations. Bias in modelsoften occurs when they learn the spurious correlations (i.e., short-cut) present in the data . For instance, if a diffusion modelerroneously learns spurious correlations between gender and occu-pation in training data, it will exhibit bias when generating imagesfrom occupation-related text descriptions. Following this line, anintuitive idea is to encourage the diffusion models sampling pro-cess to focus on the semantic information from the text descriptionwhile neglecting the corresponding spurious correlations. However, due to the entanglement of semantic information and biases, achiev-ing this goal remains challenging. To this end, we draw inspirationfrom invariant learning , which can achieve guaranteed per-formance under distribution shifts and received great attention inrecent years. Invariant learning keeps invariant semantic informa-tion across different training environments, where environmentsare variables that should not affect the prediction.In this paper, we propose a novel bias mitigation frameworkInvDiff for text-to-image diffusion models without relying on aux-iliary bias annotation. The main idea of InvDiff is to encourage thediffusion model to focus on the invariant semantic information fromthe text description. To preserve the invariant semantic information,we first design a novel debiasing objective. Then we propose a max-min training game for both potential bias annotation inference andbias mitigation. Specifically, we first infer potential bias annotationsby maximizing the objective. Given the annotations, we finetunethe diffusion model to unbias by minimizing the proposed objective.Notably, we only need to learn a small number of parameters inthe lightweight learnable module without altering the pre-traineddiffusion model. Furthermore, we provide a theoretical guaranteethat the implementation of InvDiff is equivalent to reducing theerror upper bound of generalization. The main contributions of thiswork are as follows: We investigate the challenging yet practical new research prob-lem of mitigating unknown biases in text-to-image diffusion mod-els without relying on auxiliary bias annotations. We proposea novel bias mitigation framework InvDiff, which encouragesthe diffusion models sampling process to focus on the invariantsemantic information.",
  "RELATED WORK2.1Bias in Diffusion Models": "Over the past few years, diffusion models have shown a great abilityto generate images with high visual quality. Despite their success,de-biasing is still a fundamental challenge that diffusion models face.Diffusion are known to produce biased and stereotypical imagesfrom neutral prompts . For instance, researchers found thatStable Diffusion (SD) predominantly produces male images whenprompted with various occupations, and the generated skin tone isconcentrated on the center few tones. Diffusion models are highlydata-driven and prone to inherit bias in real-world data .Whats worse, diffusion models not only perpetuate biases foundin the training data but also may amplify it .",
  "InvDiff: Invariant Guidance for Bias Mitigation inDiffusion ModelsKDD 25, August 0307, 2025, Toronto, ON, Canada": "model. We tested the impact of the on debiasing using the Fair-face dataset. As shown in , results are presented for boththe handcrafted grouper and the soft grouper. It can be observedthat under both grouper settings, as delta increases, bias showsa decreasing trend, and the standard deviation range graduallynarrows, indicating that a larger delta indeed facilitates debiasing.Additionally, we found that for the handcrafted grouper, a notice-able decrease in bias occurs when delta increases to 0.6, while forthe soft grouper, a noticeable decrease is observed when delta in-creases to 0.9. This suggests that the optimal delta setting may differfor different types of groupers. More experimental results can befound in a. Analysis of multi-prompt debiasing on the Waterbirdsdataset. We know that for generative models, the acceptable set ofprompts is infinite, and the model needs to debias across different prompts. We tested 30 prompts on the Waterbird dataset, repre-senting 30 different types of birds. shows histograms andkernel density estimation plots of the bias distribution frequencyunder different settings. The x-axis represents the values of thebias metric, and the y-axis shows the number of prompts with biasvalues falling within the corresponding interval. It can be seenthat for the biased model, the bias metric is concentrated in therange of 0.8-1.0, whereas for our model, the prompts falling withinthe 0.8-1.0 bias range have significantly decreased. Our model caneffectively mitigate this bias.",
  "PRELIMINARY3.1Diffusion Models": "Denoising Diffusion Probabilistic Models. Diffusion mod-els are latent variable models, which aim to model distribution (0) = (0: ) 1: that approximates the data distributionP(0). Here 1, . . . , are latents of the same dimensionality asthe data 0 P (0). The denoising diffusion models consist of twoprocesses, the forward process and the reverse process, respectively.In the forward process, Gaussian noise is gradually added to thedata 0 according to a variance schedule { }1: , finally obtainingrandom noise . The process can be formulated as a Markov chain:",
  "(1)": "The noisy distribution at any intermediate timestep is ( | 0) =N ; 0, (1 ) I, and = =1 (1 ). Namely, = 0 + 1 , where N (0, I) is a Gaussian noise.In the reverse process, a generative model learns to estimatethe analytical true posterior in order to gradually recover 0 froma Gaussian noise input N (0, I). The process can be definedas a Markov chain:",
  "(4)After training the and given a Gaussian noise input, we caniteratively sample from the reverse process to reconstruct 0": "Text-to-Image Diffusion Models. In cases where text descrip-tion (i.e., prompts) are available, diffusion models can modelconditional distributions of the form ( |). Some studies, includ-ing Stable Diffusion , implement conditional diffusion with aconditional denoising autoencoder (,,) and paves the wayto controlling the synthesis process through input text prompt .In the sampling, the label-guided model estimates the noise with alinear interpolation = (1 + ) (,,) (,) to recover1, which is often referred as classifier-free guidance .",
  "Invariant Learning": "The Environment Invariance Constraint (EIC). Invariantlearning (IL) is an emerging technique for improving discrimi-native models robustness by blocking spurious correlations in data.IL is based on the assumption that the causal mechanism remainsinvariant across various environments (a.k.a., domains), while thespurious correlation varies. For example, the correlation betweenthe background green grass and the label landbird is unstable acrossthe images of data collected from different locations. In this light,IL pushes models to capture the causal mechanism by penalizingthe variance of model performance across environments.Formally, consider the task of learning a predictor : X Y,which maps input X to output Y. Suppose the predictor can be decomposed into = , where : X H denotes afeature encoder which maps the input into a representation spaceH, : H Y is a classifier. Suppose the training data D arecollected under multiple environments E, i.e., D = {}E. = , =1 contains data sampled from the probability distributionP (X Y). The target of IL is to encourage the encoder to extractinvariant features associated with causal mechanisms by satisfyingthe following constraint:",
  "R (, ) + Var R (, ) ,(6)": "where the R (, ) represents the training loss of on the envi-ronment , and the second term is the constraint over the varianceacross environments. The training process of Eq.(6) enforces theoptimal classifier on top of the representation space to be thesame across all environments, therefore encouraging the encoder extract invariant and stable features automatically.Note that invariant learning was originally developed for dis-criminative models. This approach requires learning an encoder to extract features, and then based on these features performing aclassification task to ensure balanced performance across differentenvironments. However, this method cannot be directly appliedto diffusion models. As in diffusion, is generated rather thanprovided, making it infeasible to extract invariant features of .Additionally, there is no classifier mapping feature embedding to for generative tasks. In this paper, inspired by invariant learning,we designed a debiasing objective applicable to diffusion models.",
  "KDD 25, August 0307, 2025, Toronto, ON, CanadaTrovato et al": "Felix Friedrich, Manuel Brack, Lukas Struppek, Dominik Hintersdorf, PatrickSchramowski, Sasha Luccioni, and Kristian Kersting. 2023. Fair diffusion: Instruct-ing text-to-image generation models on fairness. arXiv preprint arXiv:2302.10893(2023). Jean-Christophe Gagnon-Audet, Kartik Ahuja, Mohammad Javad Darvishi Bayazi,Pooneh Mousavi, Guillaume Dumas, and Irina Rish. 2023. WOODS: Benchmarksfor Out-of-Distribution Generalization in Time Series. Transactions on MachineLearning Research (2023). Fea-tured Certification.",
  "METHODOLOGY": "In this section, we present our InvDiff framework, which finetunesbiased diffusion models to unbiased without bias annotation. Westart by formulating the debiasing target in .1. .2explains the motivation behind invariant guidance. Subsequently,in .3, we introduce the proposed debiasing objective forboth potential bias annotation inference and bias mitigation. Weprovide a theoretical analysis in .4.",
  "Formalization of Unbiased Diffusion Model": "Given training dataset D = {,}=1, data X and text prompts Y, the training process of current text-to-image diffusion mod-els is to try its best to approximate the conditional distributionP( |) in training data. However, real-world data often containspurious correlations, which are correlations between meaninglessfeatures and text prompts. Diffusion models are prone to learningthe easy-to-fit spurious correlations, resulting in generating biaseddata. For example, if most of the presidents in the training set aremen, the diffusion model may learn about the spurious correlationsbetween the job and the gender. When generating images taking\"president\" as a text prompt, the model is highly likely to generatea male president. We can denote as the invariant semanticinformation of an instance which defines its text prompt (e.g.,the semantic information of a person in political scenes with formalattire), where as spurious correlation toward (e.g., gendercharacteristics). and denote the corresponding randomvariables. In this work, our goal is to mitigate bias in diffusion mod-els by eliminating the influence of arbitrary and unknown spuriouscorrelations. Namely, we aim to obtain a debiasing diffusion modelwhose conditional generation results only depend on P(|).",
  "Invariant Guidance": "Generally, one can have a pre-trained biased text-to-image diffu-sion model on dataset D, e.g., DDPM model with parameters , (1 | ,) = N (1; (,,) , I), which are trainedto fit the biased conditional distribution P( |) on D. We de-note the parameters of the ideal text-to-image diffusion modelwhose conditional generation results only depend on P(|) as (1 | ,) = N1; (,,) , Iwith parameters .The spurious correlation between and leads to the generatedresults of pre-trained diffusion models containing (e.g., whentaking \"president\" as a condition, the model always generate a malepresident), while the ideal diffusion model can generate samples only depend on (e.g., generate president images with variousgenders). Due to the existence of spurious correlations, there is agap between the posterior mean predicted by the actual conditionaldiffusion model ( (,,)) and the ideal one ( (,,)).From the perspective of the posterior mean gap, we can drawinspiration from the classifier-guided sampling method .The classifier-guided sampling methods show that one can traina classifier ( | ) and use its gradient log ( | ) asthe mean shift item to guide pre-trained unconditional diffusionmodel to sample towards specified class . By introducing the priorknowledge from , the classifier-guided sampling methods fill theposterior mean gap between the unconditional diffusion processand the ideal conditional diffusion process. Similar to the classifier-guided sampling method that utilizes condition as prior knowl-edge to fill the gap, we aim to introduce invariant features asprior knowledge for biased pre-trained conditional diffusion model.The mean shift item from invariant features can help the reverseprocess fill the gap and focus on reconstructing invariant featuresrather than spurious correlations. InvDiff follows this principle toobtain an unbiased diffusion model based on a pre-trained biaseddiffusion model.Given a pre-trained biased text-to-image diffusion model ondataset D, (1 | ,) = N (1; (,,) , I), our tar-get is to utilize the knowledge from invariant semantic informationto guide the diffusion process. Similar to the classifier-guided sam-pling methods that use the gradient log ( | ) as the meanshift item to guide pre-trained unconditional diffusion model tosample towards specified condition , we use the gradient infor-mation from , i.e., log| , making the diffusionprocess more focuses on invariant semantic information. Therefore,the target diffusion process can be formulated as:",
  "N1; (,,) + log| , I.(7)": "If is available, we can obtain the log| and di-rectly utilize the Eq.(7) to guide the sampling process. Nevertheless,obtaining the invariant semantic information is not a trivial task.Specifically, (i) is generally not directly provided by the dataset. (ii) The extraction of does not follow a unified rule. Forinstance, in the bird generation task, the foreground serves as theinvariant information, whereas in the grassland generation task,the background is the invariant information. So we cant directlyextract fixed foreground or background et al., as semantic infor-mation. Therefore, there is still an urgent need to design a moregeneral data-driven approach to preserve invariant representations.In the next subsection, we will elaborate on our solution.",
  "Samples with Small Loss": ": An overview of InvDiff. We first design a novel debiasing objective L for diffusion models. Then we propose amax-min game with the debiasing objective. We first infer potential bias annotations by maximizing the objective. Given theannotations, we finetune the biased model to unbiased by minimizing the proposed objective. biased model to unbiased by minimizing the proposed objective.The overview of InvDiff is shown in . Debiasing Objective for Diffusion Models. Inspired by in-variant learning as discussed in .2, which using theEnvironment Invariance Constraint (EIC) (Eq. 5) to encourage the en-coder to extract invariant features. We propose incorporating thisapproach into diffusion models to automatically learn the invariantrepresentation . In general, the training process of InvDiff gen-erally comprises two phases: (i) Potential Bias Annotation Inferenceand (ii) Invariant Learning Regularization. In phase (i), training datais grouped into environments by maximizing violations of the EICprinciple. These groups intend to encode variations of spuriousinformation while preserving the causal mechanism. The phase (ii)employs EIC as a regularization term to learn invariant representa-tions based on the grouping results from the previous phase. Wewill elaborate on them in detail. Potential Bias Annotation Inference. In this subsection, weintroduce a novel differentiable bias annotation inference methodfor diffusion models. We maximize violation of the EIC principleto divide the training data into several groups (i.e., environments),here the groups are expected to hold the invariant mechanism andreflect spurious correlations. Specifically, we use a learnable matrixW R to indicate which group the sample belongs to. Here is the number of training samples and is a hyperparameterrepresenting the number of groups. W represent the probabilityof sample belongs to group , i.e., W = 1 and W 0. Weoptimize W with the following objectives:",
  "parameter of dispersion degree. By maximizing the variance of lossacross different environments, we obtain the group indicator matrix": "W. We can use the sample grouping result vector W R as biasannotation. Each value in the vector represents the probability ofthe sample belonging to the corresponding group . Invariant Learning Regularization. After grouping trainingdata into environments, in the invariant learning regularizationphase we employ EIC as a regularization term. We learn invariantrepresentations based on the grouping results and the minimiza-tion of EIC. Note that as discussed in .2, invariant learningcannot be directly applied to diffusion models. As are gener-ated rather than given, making it impossible to extract invariantfeatures from . Furthermore, there is no classifier to map featureembeddings to label in generative tasks. To address these challengesspecific to generative tasks, we shift our focus to extracting featuresfrom , replacing the encoder to capture invariant informationeffectively. Specifically, we employ an encoder () for learninginvariant representations. Then, we incorporate () and the EICregularization term into Eq.(7), and rewrite it as:",
  "W ,, + , (),2)": "(10)Since log| is intractable, we can employ a gradi-ent estimator (, (),) to simulate it. and is the hyper-parameters. The pretrained diffusion model and the group indicatematrix W is fixed. The EIC regularization term encourages the en-coder to extract invariant semantic information automatically.We found that a lightweight module with only a small num-ber of parameters can yield effective results without altering thepre-training diffusion model. serves as a mean shift item toonly guide diffusion models sampling process more focuses oninvariant information. Compared to fine-tuning the entire model, itis easier to find optimal solutions for debiasing while maintaininggeneration quality.",
  ",(12)": "where = and each is the error of an ideal joint hypothesisfor Q and P, P () is the error for a hypothesis on distribution P.From Proposition 1, the upper bound of the models error inthe unseen target domain Q can be expressed as Eq. (12). A lowervalue of Q() indicates better generalization performance of themodel. Then, we analyze each term of Eq. (12). For term , canbe ignored in practice because it is small in reality. For term , P () represents the error in the training domain. EmpiricalRisk Minimization (ERM) is an appropriate method for controllingthis term. InvDiff optimizes it by minimizing the first term in Eq.(10).For term , 1 2 minSO HH(S, Q) is the smallest H-divergencebetween S and Q. Given that Q is unknown, the only way to re-duce this term is to expand the range of O, thereby increasingthe likelihood of finding an S that is closer to Q. According to Eq.(11), maximizing the distribution gap between P and P achievesthis. In InvDiff, we infer group labels by maximizing L, whichincreases the distributional disparity between groups. For term , 1",
  "Experimental Settings": "5.1.1Datasets. We conduct experiments on three publicly avail-able benchmark datasets. Note that we access the bias attributeannotation only for the data construction and evaluations. Thetraining sets are biased, while the test sets are not.(1) Waterbirds: The Waterbirds image dataset containstwo major categories of birds: waterbird and landbird, and eachcategory has several specific species of birds. Images are spuriouslyassociated with the background \"water\" or \"land\". There are 4,795training samples while only 56 samples are waterbirds on land and184 samples are landbirds on water. The remaining training datainclude 3,498 samples from landbirds on land, and 1,057 samplesfrom waterbirds on water. For the prompt settings, because thereare significant morphological differences between different birdspecies, even if both species are waterbirds or landbirds, there arestill substantial differences between them. To be more realistic, weset the prompts during training and testing to specific bird species.(2) CelebA: CelebA defines an image classification taskwhere the input is a face image of celebrities and we use the clas-sification label as its corresponding gender. We follow the datapreprocess procedure from . The label is spuriously correlatedwith hair color blond or black. In CelebA, the minority groupsare (blond, male) and (black, female). For more extensive testing,we constructed different group ratios for the following four groups:(blond, male), (blond, female), (black, male), (black, female). Thesample ratio is 1:2:2:1. The (blond, male) group has 1,387 samples,which is the total number of blond male samples in the dataset.(3) FairFace: FairFace is a dataset balanced in terms of gen-der and race, using binary gender and including eight races. Con-sidering the accuracy of the classifier during evaluation, we con-solidate them into four broader classes, following previous work:WMELH = {White, Middle Eastern, Latino Hispanic}, Asian = {EastAsian, Southeast Asian}, Indian, and Black. Our data consist ofeight groups: (Female, White), (Female, Asian), (Female, Indian),(Female, Black), (Male, White), (Male, Asian), (Male, Indian), and(Male, Black). For unbiased data, the ratio is 1:1:1:1:1:1:1:1, and is3:2:1:1:1:1:2:3 for biased data. The minimum group size is 1,500samples. For the biased dataset, there are a total of 21,000 samples,while the unbiased dataset contains 12,000 samples.",
  "Bias Metric , which assesses the extent of bias in the resultsof the generative model. For every prompt P, we compute thebias(P) =1": "(1)/2, :< | freq() freq()|, where freq()is class s frequency in the generated images. We train the environ-ment classifier for the Waterbirds Dataset, hair color classifier forthe CelebA Dataset, and the race classifier for the FairFace dataset.The number of class is 2/2/4, and the number of images for eachprompt is 32/128/123 for Waterbirds/CelebA/FairFace. (2) Gener-ation Quality Metric. We use CLIP-T , the CLIP similaritybetween the generated image and the prompt, to evaluate the gener-ation quality. We choose the CLIP-ViT-Base-16 for evaluation.(3) Hybrid Metrics: We use FID and Recall to measure thedifference between the generated results and the original unbiasedtest data distribution. We used the VGG16 for evaluation. 5.1.3Comparison Methods. The competitive baselines can becategorized into four groups. (1) The Stable Diffusion modelwhich is trained on the biased training dataset. (2) SOTA diffusiondebiasing methods: As far as we know, InvDiff is the first study tomitigate unknown biases in diffusion models without relying onauxiliary bias annotations. Therefore, we choose two SOTA diffu-sion debiasing methods relying on auxiliary bias annotations forcomparison. TIW is a time-dependent importance reweightingmethod designed to mitigate diffusion models biases. TIW requiresan unbiased dataset as annotations. Fair-Diffusion aims toreduce known biases associated with human faces, such as genderand race. It needs a bias classifier as an annotation and cant be usedin the Waterbirds dataset. (3) The third group contains four ablationcounterparts of InvDiff, InvDiff -Full-Hard, InvDiff -Part-Hard, InvDiff -Full-Soft, and InvDiff -Part-Soft. The \"-Full\" suffix de-notes finetuning all the diffusion models parameters. The \"-Part\"suffix denotes only training a small network as Eq.(10). The \"-Hard\" suffix represents the model use bias annotation in the datasetas the group result. The \"-Soft\" suffix denotes we obtain the groupresult using Eq.(8) (4) We use the image generated by InvDiff as adata augmentation method to verify the debiasing effectiveness. Weselect vanilla ERM and SOTA debiasing classification meth-ods without bias annotation for comparison, including Mixup,LfF, Resample , Reweight , EIIL . (See AppendixA for the detailed training configurations).",
  "Image Generation Results (RQ1)": "We first present the comparison between InvDiff and the baselinemethods across three datasets, as shown in . For the Bias andCLIP-T metrics, the value before the brackets represents the mean,while the value in brackets indicates the variance. The experimentalresults demonstrate that our method consistently achieves the low-est bias across all three datasets. With bias annotation, our modelsdebiasing performance (InvDiff -Full-Hard and InvDiff -Part-Hard)significantly outperforms the state-of-the-art comparison methodthat also utilizes bias annotation. Even without bias annotation, ourmodels (InvDiff -Full-Soft and InvDiff -Part-Soft) still produce supe-rior results in most cases. Additionally, the FID, Recall, and CLIP-Tvalues are comparable to those of the Stable Diffusion model, indi-cating that InvDiff can maintain the quality of generated imageswhile effectively reducing bias. shows the image randomlysampled from our unbiased model.Nevertheless, we observe that when sensitive attribute anno-tations are unavailable, the debiasing effect of InvDiff -Part-Softon the CelebA dataset seems less pronounced (Bias 0.80 0.70).The reason is that compared to other datasets (FairFace with onlyhuman faces, Waterbird with birds and two kinds of backgrounds),CelebA contains more complex features including hair, eyeglasses,hats, mustaches, etc. Therefore, there may be many latent biasesin CelebA, while the bias constructed in the CelebA dataset is onlybetween gender and hair color. Without bias annotations, the soft-version model likely has to balance not only the biases betweenhair color and gender but also other complex unknown biases. Tovalidate this, we further conduct experiments on CelebA to explorewhether our soft-version model can mitigate other unknown biases.Specifically, we first trained 20 binary classifiers for 20 face-related",
  ": The Impact of Parameters on Model Performance and Training Time": "attributes using 200,000 images collected from the CelebFaces At-tributes Dataset 1. We then assessed whether the images generatedby our models on CelebA can mitigate these potential biases inStable Diffusion. The bias metric results in validate oursuppose, confirming the significance of its performance withoutbias annotations. TimeGradOurs 0.015 0.020 0.025 0.030 0.035 0.040 0.045 CRPS",
  "Hyperparameter Sensitivity Analysis (RQ2)": "Impact of Parameter Quantity of . In our settings, we fix thepretrained conditional diffusion model and train the lightweightlearnable module . We investigate the impact of parameter quan-tity of and present the results for hyperparameter = 0.2 and0.3 in Figure. 5a. It shows that even when the parameters of are only 15M, it can still achieve a certain degree of debiasing com-pared to the original biased model (Bias 0.89 0.71). However,when the parameter size of is small, it may not ensure stableimage generation quality. When the parameter size of is around200M (Param2), image quality begins to decline. Notably, whenthe parameter size of ranges from 860M to 550M, the modelmaintains relatively stable output quality and debiasing capability. Analysis of groups (environments) number . representsthe number of groups we assume can potentially distinguish be-tween various bias situations. In general, we can empirically choose as the product of the number of categories in sensitive attributes: = =1 , where is the total number of sensitive attributes considered (e.g., gender, hair color). is the number of categoriesin the -th sensitive attribute. We found that our method remainsrelatively stable across all three datasets with respect to . Weperformed a grid search between , and found that the perfor-mance was poor when set to 2, while both 4 and 8 yielded effectiveresults. Ultimately, in our experiments, we set E=4, 4, 8 for Water-bird, CelebA, and FairFace, respectively. Debiasing Results at Different Levels of Bias. In real-worlddata, the degree of bias is complex and uneven. Using the CelebAdataset as an example with two features, hair color and gender, thedata might be biased in the blonde hair category while being unbi-ased in the black hair category, with varying degrees of bias. Themodel should be effective against different types of bias withoutmaking an unbiased model more biased. illustrates ourmodels debiasing capability for different types of bias. In a,the biased groups are blonde hair, black hair, blonde males, blondefemales, black-haired males, and black-haired females, with pro-portions of 1:2:2:1, 1:4:4:1, and 1:8:8:1, respectively. In b,the proportions for the biased blonde hair and unbiased black hairgroups are 1:2:2:2, 1:4:4:4, and 1:8:8:8. It can be seen that our modeleffectively addresses varying degrees of bias in the data withoutintroducing bias in previously unbiased models (1:1:1:1) and canalso correct imbalanced biases. Training Time Analysis. When training our model, we traina module to debias a biased model, where the parameter sizeof module can be much smaller than that of the biased model.We tested the training time for different parameter sizes of , asshown in b. In these experiments, we used a single A100GPU, FP16 mixed precision, 10,000 steps, and a batch size of 64.When the parameter size of the trainable module is reduced, thetraining time can be effectively decreased.(See Appendix B for more hyperparameter analysis).",
  "InvDiff for More Tasks (RQ3)": "InvDiff for Data Augmentation. We further use InvDiff asa data augmentation method, and compare its performance withSOTA debiasing methods that dont rely on bias annotation. Theexperimental results are shown in . For InvDiff, we generate4,795 samples and add them to the training set. InvDiff demon-strates good performance, further illustrating the effectiveness ofour proposed method. InvDiff for Time Series Forecasting. We investigate whetherInvDiff can mitigate bias in conditional diffusion models beyondtext-to-image tasks. We conduct additional experiments on time",
  "Conclusion": "In this paper, we addressed the problem of mitigating unknownbiases in diffusion models without relying on bias annotations orunbiased datasets. We proposed InvDiff, a general debiasing frame-work over pre-trained conditional diffusion models by incorporat-ing invariant semantic information as guidance. Specifically, weemployed a lightweight trainable model that utilizes the invari-ant semantic information to guide the diffusion models sampling",
  "InvDiff0.9020.8580.9400.842": "process toward unbiasing. Simultaneously, we design a novel dif-fusion training loss that automatically learns invariant semanticinformation. In InvDiff, we only need to learn a small number ofparameters in the lightweight learnable module without changingthe pre-trained diffusion model. Experimental results on variousdatasets and settings validate InvDiff s notable benefits. We extend our sincere gratitude to Bowen Deng for his significantcontributions to the time series forecasting experiments in Sec-tion 5.4, conducted during the camera-ready phase. This work wassupported in part by grants from the National Natural Science Foun-dation of China (Grant No. 62402159, U23B2031, 72188101), the Na-tional Key Research and Development Program of China (Grant No.2021ZD0111802), and the Fundamental Research Funds for the Cen-tral Universities (Grant No. JZ2023HGQA0471, JZ2024HGTA0187).",
  "Bloomberg. 2023. The Bias in Generative AI: 2023 Report. Accessed: 2024-08-06": "Yimeng Chen, Ruibin Xiong, Zhi-Ming Ma, and Yanyan Lan. 2022. When DoesGroup Invariant Learning Survive Spurious Correlations?. In Advances in NeuralInformation Processing Systems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave,and Kyunghyun Cho (Eds.). Marc Cheong, Ehsan Abedin, Marinus Ferreira, Ritsaart Reimann, Shalom Chal-son, Pamela Robinson, Joanne Byrne, Leah Ruppanner, Mark Alfano, and ColinKlein. 2024. Investigating Gender and Racial Biases in DALL-E Mini Images. 1, 2,Article 13 (jun 2024), 20 pages. Jaemin Cho, Abhay Zala, and Mohit Bansal. 2023. Dall-eval: Probing the reasoningskills and social biases of text-to-image generation models. In Proceedings of theIEEE/CVF International Conference on Computer Vision. 30433054.",
  "Eunji Kim, Siwon Kim, Chaehun Shin, and Sungroh Yoon. 2023. De-stereotypingtext-to-image models through prompt tuning. ICML Workshop on Challenges inDeployable Generative AI (2023)": "Yeongmin Kim, Byeonghu Na, Minsang Park, JoonHo Jang, Dongjun Kim, WanmoKang, and Il chul Moon. 2024. Training Unbiased Diffusion Models From BiasedDataset. In The Twelfth International Conference on Learning Representations. David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, JonathanBinas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. 2021.Out-of-distribution generalization via risk extrapolation (rex). In International Conferenceon Machine Learning. PMLR, 58155826. Tuomas Kynknniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and TimoAila. 2019. Improved precision and recall metric for assessing generative models.Advances in neural information processing systems 32 (2019). Xihui Liu, Dong Huk Park, Samaneh Azadi, Gong Zhang, Arman Chopikyan,Yuxiao Hu, Humphrey Shi, Anna Rohrbach, and Trevor Darrell. 2023. Morecontrol for free! image synthesis with semantic diffusion guidance. In Proceedingsof the IEEE/CVF Winter Conference on Applications of Computer Vision. 289299.",
  "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. 2015. Deep learningface attributes in the wild. In Proceedings of the IEEE international conference oncomputer vision. 37303738": "Wang Lu, Jindong Wang, Xinwei Sun, Yiqiang Chen, Xiangyang Ji, Qiang Yang,and Xing Xie. 2024. Diversify: A General Framework for Time Series Out-of-Distribution Detection and Generalization. IEEE Transactions on Pattern Analysisand Machine Intelligence 46, 6 (2024), 45344550. Ranjita Naik and Besmira Nushi. 2023. Social Biases through the Text-to-ImageGeneration Lens. In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics,and Society (AIES 23). Association for Computing Machinery, New York, NY,USA, 786808. Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. 2020.Learning from failure: De-biasing classifier from biased classifier. Advances inNeural Information Processing Systems 33 (2020), 2067320684. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,Gretchen Krueger, and Ilya Sutskever. 2021.Learning Transferable VisualModels From Natural Language Supervision. In Proceedings of the 38th Inter-national Conference on Machine Learning (Proceedings of Machine LearningResearch, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, 87488763. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, AlecRadford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation.In International conference on machine learning. Pmlr, 88218831. Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf. 2021. Au-toregressive denoising diffusion models for multivariate probabilistic time seriesforecasting. In International Conference on Machine Learning. PMLR, 88578868. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjrnOmmer. 2022. High-resolution image synthesis with latent diffusion models. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition.1068410695.",
  "Shiori Sagawa*, Pang Wei Koh*, Tatsunori B. Hashimoto, and Percy Liang. 2020.Distributionally Robust Neural Networks. In International Conference on LearningRepresentations": "Patrick Schramowski, Manuel Brack, Bjrn Deiseroth, and Kristian Kersting.2023. Safe latent diffusion: Mitigating inappropriate degeneration in diffusionmodels. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. 2252222531. Preethi Seshadri, Sameer Singh, and Yanai Elazar. 2024. The Bias AmplificationParadox in Text-to-Image Generation. Annual Conference of the North AmericanChapter of the Association for Computational Linguistics (2024).",
  "Vladimir N Vapnik. 1999. An overview of statistical learning theory. IEEEtransactions on neural networks 10, 5 (1999), 988999": "Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu,Yiqiang Chen, Wenjun Zeng, and Philip S. Yu. 2023. Generalizing to UnseenDomains: A Survey on Domain Generalization. IEEE Transactions on Knowledgeand Data Engineering 35, 8 (2023), 80528072. Shuliang Wang, Xinyu Pan, Sijie Ruan, Haoyu Han, Ziyu Wang, Hanning Yuan,Jiabao Zhu, and Qi Li. 2024. DiffCrime: A Multimodal Conditional DiffusionModel for Crime Risk Map Inference. In Proceedings of the 30th ACM SIGKDDConference on Knowledge Discovery and Data Mining. 32123221. Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao,Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. 2023. Diffusion models: Acomprehensive survey of methods and applications. Comput. Surveys 56, 4 (2023),139.",
  "Tao Yang, Yuwang Wang, Yan Lu, and Nanning Zheng. 2023. DisDiff: Unsu-pervised Disentanglement of Diffusion Probabilistic Models. In Thirty-seventhConference on Neural Information Processing Systems": "Yuzhe Yang, Haoran Zhang, Dina Katabi, and Marzyeh Ghassemi. 2023. Change ishard: a closer look at subpopulation shift. In Proceedings of the 40th InternationalConference on Machine Learning (ICML23). JMLR.org, Article 1652, 39 pages. Huaxiu Yao, Yu Wang, Sai Li, Linjun Zhang, Weixin Liang, James Zou, and ChelseaFinn. 2022. Improving out-of-distribution robustness via selective augmentation.In International Conference on Machine Learning. PMLR, 2540725437.",
  "AAppendix: Experimental DetailsA.1Training Configuration": "A.1.1Model Architecture. Our method learns a parameter-efficientgradient estimator on a pre-trained biased model. For the pre-trained biased model, we select \"CompVis/stable-diffusion-v1-4\" and fine-tune it on the biased dataset to facilitate validation. Forthe network architecture of , we choose a UNet. The down blocktypes are \"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\",\"CrossAttnDownBlock2D\", and \"DownBlock2D\". The mid blocktype is \"UNetMidBlock2DCrossAttn\". The up block types are \"Up-Block2D\", \"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\", \"CrossAt-tnUpBlock2D\". For more information, please refer to . Thestructure of the pre-trained unbiased model is consistent with theParam0 structure in .",
  "Param0860M(320, 640, 1280, 1280)Param1551M(320, 640, 960, 960)Param2220M(160, 320, 640, 640)Param356M(64, 160, 320, 320)Param415M(32, 64, 160, 160)": "A.1.2Searched Parameters. We completed the experiment on asingle card GPU 80G-A100. The hyperparameter search ranges fortraining the model are as follows. We tune the batch size for all themodels in {8, 32, 64}. Learning rate is chosen from {1e-4, 1e-5}. Thescheduler of learning rate is chosen from {constant, linear, cosine,cosine with restarts, constant with warm up}. The warm up step istuned from {0, 500, 1000}. Parameter quantity of is chosen from{860M, 551M, 220M, 56M, 15M}. For the hyperparameter and ,the range is {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.8, 0.9, 1, 2, 10} and {0.2, 0.4,0.6, 0.8, 1, 2, 5, 10, 20, 50, 100}, respectively.",
  "A.2Prompt Details": "A.2.1Waterbirds. We do not list training occupations here due totheir large quantity. The test occupations are [a baird sparrow,a bay breasted warbler, a black capped vireo, ablue grosbeak, a boat tailed grackle, a bronzedcowbird, a california gull, an american redstart,a baltimore oriole, a belted kingfisher, a blacktern, a blue headed vireo, a bobolink , a brownpelican , a canada warbler , an anna hummingbird,abankswallow,ablackandwhitewarbler,ablack throated blue warbler, a blue jay, a brandtcormorant, a brown thrasher, an acadian flycatcher,a barn swallow, a black billed cuckoo, a blackthroated sparrow, a blue winged warbler, a brewerblackbird, a cactus wren, an american goldfinch,a bird]",
  "BExperimentsB.1Hyperparameter Sensitivity Analysis": "Analysis of the dispersion degree of the grouping. Whenthe dispersion degree is set larger, data with the same true labelare more likely to be dispersed into different groups. To explorethe mechanism by which the soft grouper functions, we testedour soft grouper under six settings on the CelebA dataset. In Fig-ure 9, we show the distribution of data with different true featuresacross different soft groups. It can be observed that when is small ( = 0), data with the same prompt are more likely to begrouped together, such as males and females with blond hair beingconcentrated in one group (for the four-group setting, this is 2;for the eight-group setting, this is 7). This makes it difficult forthe model to distinguish biased features effectively. As increases,the distribution differences of samples with the same true featuresacross soft groups become larger, which helps the model to achievebetter debiasing. Impact of . is a hyperparameter that controls the degreeof debiasing. The range of is generally set from 0 to . Thelarger the , the higher the degree of debiasing. We tested theeffect of values on model performance using the CelebA dataset,training on a biased dataset and testing on an unbiased dataset, see. Within the range of = 0.2 to 20, we observed that as increases, the mean of the bias shows a slow increase, but thestandard deviation decreases significantly. This indicates that thedebiasing effect is more stable and more robust to different prompts.The overall trend of FID decreases, and the overall trends of recalland CLIP-T increase, indicating that image quality is maintainedor even improved. However, if becomes too large, it can affectthe quality of image generation. When increases to 50, all metricscollapse significantly, indicating that the model cannot generatethe target images effectively, let alone debias them. Impact of . The is a hyperparameter of the trainable model , controlling the extent of the debias modules influence on theoriginal biased model. The parameter is set within the range of (0-1),with higher values indicating a greater influence on the original"
}