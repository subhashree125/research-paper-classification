{
  "ABSTRACT": "Large language models (LLMs) have demonstrated exceptional per-formance across a wide variety of domains. Nonetheless, generalistLLMs continue to fall short in reasoning tasks necessitating special-ized knowledge. Prior investigations into specialized LLMs focusedon domain-specific training, which entails substantial efforts indomain data acquisition and model parameter fine-tuning. To ad-dress these challenges, this paper proposes the Way-to-Specialist(WTS) framework, which synergizes retrieval-augmented gener-ation with knowledge graphs (KGs) to enhance the specializedcapability of LLMs in the absence of specialized training. In distinc-tion to existing paradigms that merely utilize external knowledgefrom general KGs or static domain KGs to prompt LLM for en-hanced domain-specific reasoning, WTS proposes an innovative\"LLMKG\" paradigm, which achieves bidirectional enhancementbetween specialized LLM and domain knowledge graph (DKG). Theproposed paradigm encompasses two closely coupled components:the DKG-Augmented LLM and the LLM-Assisted DKG Evolution.The former retrieves question-relevant domain knowledge fromDKG and uses it to prompt LLM to enhance the reasoning capabil-ity for domain-specific tasks; the latter leverages LLM to generatenew domain knowledge from processed tasks and use it to evolveDKG. WTS closes the loop between DKG-Augmented LLM and LLM-Assisted DKG Evolution, enabling continuous improvement in thedomain specialization as it progressively answers and learns fromdomain-specific questions. We validate the performance of WTSon 6 datasets spanning 5 domains. The experimental results show",
  "Specialized large language models, domain knowledge graph, retrieval-augmented generation": "ACM Reference Format:Yutong Zhang, Lixing Chen, Shenghong Li, Nan Cao, Yang Shi, Jiaxin Ding,Zhe Qu, Pan Zhou, and Yang Bai. 2025. Way to Specialist: Closing LoopBetween Specialized LLM and Evolving Domain Knowledge Graph. In Wood-stock 18: ACM Symposium on Neural Gaze Detection, June 0305, 2018, Wood-stock, NY. ACM, New York, NY, USA, 17 pages.",
  "INTRODUCTION": "Large language models (LLMs), e.g., GPT-4 , Gemini , andLlama have demonstrated their exceptional performance acrossa wide range of general domains . Yet, there isa prevalent recognition that LLMs often exhibit limitations whenconfronted with reasoning tasks that necessitate specialized knowl-edge . Most explorations of LLMs to date for specific domains,e.g., medical and law fields, have leveraged domain-specialized training or parameter fine-tuning techniques in pursuitof performance enhancement. These processes necessitate the ac-quisition of high-quality instruction data and sophisticated designof training pipelines, both of which entail substantial efforts .In light of these challenges, we focus on steering foundation mod-els via Retrieval-Augmented Generation (RAG) to excel inspecialty areas. RAG falls under the category of prompt engineering",
  "Conference acronym XX, June 0305, 2018, Woodstock, NYYutong Zhang, Lixing Chen, Shenghong Li, Nan Cao, Yang Shi, Jiaxin Ding, Zhe Qu, Pan Zhou, and Yang Bai": "evaluation of methods, no extra context information except theretrieved results is offered as the support information during theexperiments. The detailed descriptions of their usage in this paperare as follows:ChatDoctor5kChatDoctor5k is a generated conversa-tion between patients and physicians from ChatGPT grounded ona disease database. Each input describes the patients symptomspossibly with past medical history or etiological factor attached.And the desired output needs to give professional suggestions cov-ering the diagnosis, symptoms, recommended treatments, medicaltests, and so on. We sampled 1000 dialogues as the test set to testthe long text generation capability of WTS.PubMedQAPubMedQA contains tests requiring to an-swer biomedical research questions with yes, no or maybe giventhe context provided by PubMed abstracts. To fully display the per-formance of knowledge augmentation, we adopt no given contextand reasoning-free setting and use the labeled 1000 Q&A pairs inthe experiment.MedMCQAMedMCQA is a large-scale, Multiple-ChoiceQuestion Answering (MCQA) dataset designed to address real-world medical entrance exam questions. We sample 1000 single-choice questions and 1000 multiple-choice questions from the devsubset of the dataset with random 2 to 4 choices as the test set.SciQSciQ is a multiple-choice format science exam datasetabout physics, chemistry and biology. Each question has 4 answeroptions. We use the \"test\" subset of SciQ containing 1000 questionsand provide no additional paragraph with supporting evidence toevaluate our methods.ScienceQAScienceQA is a large-scale science question-answering dataset containing various knowledge domains. In ourexperiment, we sample the 500 natural science questions, 500 lan-guage science questions, and 225 social science questions in thetest subset of ScienceQA with no image and no textual context.Simple QuestionsSimple Questions is a large-scale Q&Adataset with 100k Q&A pairs constructed based on Freebase knowl-edge base. Each pair in Simple Questions has a corresponding factfrom Freebase and can be rephrased as the form of (subject, rela-tionship, ? ). We sample 1000 pairs as the test set.",
  "KG\" paradigmimproves \"LLM": "KG\" framework by incorporating sophisticated in-teractions between KGs and large LLMs, which partially addressesthe challenges associated with multi-hop reasoning. These works utilize general KGs, e.g., Wikidata , Freebase , toenhance their performance for common-sense questions. Never-theless, the knowledge encapsulated in general KGs is coarse andoften lacks specialized information, potentially leading to knowl-edge mismatches when addressing domain-specific questions. Dueto this limitation, recent advancements have beeninspired to incorporate Domain Knowledge Graphs (DKGs) to KG-augmented LLMs to enhance domain-specific reasoning, particu-larly in the medical field . However, these studies rest onthe precondition that a comprehensive DKG has been establishedalready, which is currently true only for a few specialized domains,e.g., medical and food knowledge . Most domainscontinue to face challenges with the absence and incompletenessof DKGs. The utilization of low-quality DKGs can adversely affectthe performance of domain-specific reasoning. Furthermore, theseSOTA solutions utilize static DKGs for knowledge retrieval andprompt generation, which may run the risk of being outdated andmisaligned with the evolving knowledge demand during implemen-tation . Additionally, DKGs may also require personalization orcustomization to address unique needs, which is also overlookedin previous works. These challenges underscore the necessity ofemploying evolving DKGs that continuously update and integrateappropriate domain knowledge.To fill in the above deficiencies, this work introduces the Way-to-Specialist (WTS) framework, which designs an innovative strategyfor developing specialized LLMs through the mutual enhancementbetween LLM and DKG. In contrast to the paradigms LLM",
  "KGand LLM": "KG that operate in unidirectional enhancement pro-cesses using KG to augment input prompts of LLMs, WTS designsa novel LLMKG paradigm which not only leverages DKG toprompt LLM for the enhancement of domain-specific reasoningcapability but also utilizes LLM to evolve DKG for the enhancementof domain knowledge comprehensiveness (illustrated in ).Such bidirectional enhancement between LLM and DKG forms in-teractive loops as WTS progressively addresses and learns from domain-specific questions. The novelties and contributions of ourwork are summarized as follows:1) WTS introduces an innovative LLMKG paradigm whichenables the bidirectional enhancement of LLM and DKG for con-structing specialized LLMs. This paradigm comprises two integratedcomponents: the DKG-Augmented LLM and the LLM-Assisted DKGEvolution. The DKG-Augmented LLM retrieves question-relevantdomain knowledge from the DKG and uses it to prompt LLM foranswer generation. Subsequently, the LLM-Assisted DKG Evolutionleverages LLM to generate new knowledge from the processed ques-tion and use it to evolve the DKG. In particular, WTS can initiatewith an empty or incomplete DKG and progressively complete it toenhance the reasoning capability for domain-specific tasks.2) WTS designs an interactive knowledge retrieval mechanismfor DKG-Augmented LLM. WTS maintains DKGs in the form ofvector databases. In each retrieval iteration, WTS assesses the simi-larity of questions and knowledge in DKG by measuring the dis-tance between their feature embeddings generated by the languageembedding model of the vector database. Further, WTS utilizesLLM to evaluate the semantic relevance of knowledge triples to thequestion and prune less relevant ones. The retrieved knowledgeis used to prompt the question reasoning, and the LLM generatesthe answer along with a recommendation to determine whetherthe retrieval proceeds to the next iteration to further enrich theexternal knowledge.3) WTS proposes an LLM-based knowledge generation schemeand a redundancy-aware DKG update mechanism for LLM-AssistedDKG Evolution. LLM-based knowledge generation leverages LLMto efficiently generate knowledge triples from the unstructured in-formation in questions, answers, and knowledge retrieved by DKG-Augmented LLM. The redundancy-aware DKG update mechanismevaluates the redundancy between newly generated knowledge andexisting knowledge in DKG, then excludes redundant knowledgeduring evolution to guarantee the knowledge efficiency of DKG,which facilitates the knowledge retrieval in DKG-Augmented LLM.4) We conduct experiments of WTS on 6 datasets spanning 5domains, including 4 specialized domains (medical, natural science,social science, and linguistics) and 1 general domain. The resultsshow that WTS achieves overall the best performance in all 4 spe-cialized domains, achieving a maximum performance improvementof 11.3% compared to the current SOTA.",
  "RELATED WORKS": "Large Language Models & Knowledge Graphs: The integrationof KG and LLM can be categorized into two branches: KG for LLMand LLM for KG. The works in the branch of KG for LLM mainlyuse Retrieval-Augmented Generation (RAG) to incorporatenon-parametric information from KGs to pre-trained LLM for addressing knowledge-intensive text generationtask. KAPING pioneered KG-augmented LLM by retrievingrelevant knowledge triples from KGs and prepending them to theinput question as a prompt. KAPING only utilizes a single-layerKG, making it challenging to address questions that require multi-hop knowledge analysis on KGs. To address this issue, the works conduct multi-hop knowledge retrieval over KGs toimprove the reasoning capacity of LLMs. KG-augmented LLMs also",
  ": Comparison of LLMKG against SOTA paradigms in KG-augmented LLM": "have spurred research into specialized LLMs in medical field , politics , scholar and law . These works utilizeexisting RAG frameworks, e.g. mind map , reason chain ,and aligned embeddings , to generate input prompts. The worksin the branch of LLM for KG leverages LLMs to support knowledgeengineering tasks , including KG construction and KGcompletion . KG construction involves collecting andintegrating information from various sources . Recent workshave adopted both schema-based methods and schema-free methods with LLM to construct KGs based on structuredtexts. KG completion involves inferring missing knowledge triples.These works are typically categorized into rule-based methods , embedding-based methods , and text-based methods . LLM is mostly implemented with text-based methods due toits exceptional capability in semantic understanding. For example,KICGPT and KERMIT utilized LLMs as entity-sortingassistants and data-enhancement tools and validated the advantageof using LLMs in KG completion. However, these methods werelimited to completing the KG based on existing entities and couldnot introduce new entities and triples into the KG.Specialized LLM: Despite the remarkable performance of LLMs,they still struggle with problems that require specialized domain ex-pertise . Recent efforts have sought to address this issue throughfine-tuning, which involves partial modification of model parame-ters, or through RAG-based prompting, which incorporates special-ized domain knowledge, e.g., in the medical andfinance field. The works have investigated fine-tuning techniques to LLMs and achieved considerable performanceimprovements for domain-specific questions. However, fine-tuningtechniques require substantial computational resources and timeand may lead to catastrophic forgetting where the LLM loses its general knowledge while adapting to the specialized domain.Compared to fine-tuning techniques, RAG dynamically integratesup-to-date external domain knowledge into LLM reasoning, pro-viding enhanced efficiency and simplified implementation. Theseworks, e.g., MVPKG in and CMCKG , all utilize manuallyconstructed KGs. However, most specialized domains still lack avail-able KGs, which hinders the implementation of KG-augmented LLM.Furthermore, KGs may become outdated, potentially diminishingthe effectiveness of RAG methods.",
  "METHODOLOGY OF WAY-TO-SPECIALIST3.1Preliminaries": "As WTS rests on a synergistic integration of domain knowledgegraph (DKG) and retrieval-augmented LLM, it is essential to providea brief introduction to these two techniques prior to delineatingthe design of WTS.Domain Knowledge Graph: A DKG is characterized as a col-lection of knowledge triples G = {(s,,o)|s E, R,o E},where (s,,o) is a knowledge triple, E is the entity set, R is therelation set, s, , and o correspond to the subject entity, relationand object entity, respectively. WTS maintains DKG in the formof vector databases, where entities and relationships withinthe DKG are represented as high-dimensional feature embedding.This approach enables efficient storage, retrieval, and analysis ofthe DKG through vector-based operations, drastically improvingthe efficiency of DKG-based prompting (will be demonstrated insubsequent sections).Retrieval-augmented LLM: Retrieval-augmented LLM in-volves integrating an external knowledge base with LLM to providerelevant information during the generation process. The key idea is",
  "Architecture of WTS": "As illustrated in , WTS encompasses two closely inter-connected components: DKG-Augmented LLM and LLM-AssistedDKG Evolution. DKG-Augmented LLM prompts LLM with domainknowledge in DKG to improve the reasoning capability of LLM fordomain-specific questions. LLM-Assisted DKG Evolution employsLLM to generate knowledge triples from questions and answers,and use them to evolve DKG. 3.2.1DKG-Augmented LLM. The DKG-Augmented LLM con-sists of four modules: entity extraction module, retrieval module,pruning module, and reasoning module.Entity Extraction Module: Upon receiving a question , WTSutilizes the entity extraction module to identify the topic entitiesof the question. While traditional entity extraction methods, e.g.,Hidden Markov Models (HMMs) and Conditional RandomFields (CRFs) , exist for implementation, they often strugglewith handling domain terms. Inspired by the superior performanceof LLMs in entity recognition , we utilize LLM for en-tity extraction. Specifically, the entity extraction module promptsLLM (the input prompt is given in Appendix D.1, Prompts 1) toextract primary entities, denoted by E = {,1,,2, ...,, },from question . This process can be mathematically written as E = LLMEnt(), where LLMEnt is the prompted LLM. The extractedentities will be fed to the subsequent retrieval module. Note thatthe number of extracted entities is restricted to to mitigate theretrieval overhead for non-informative entities.Retrieval Module: Given extracted entities, the retrieval mod-ule retrieves within the DKG to obtain knowledge triples relatedto the extracted entities in E. For ease of description, we for nowconsider that the retrieval module operates with a non-empty DKG.Subsequent sections will provide a detailed exposition of the con-struction and evolution of DKG. The retrieval process executes in aniterative manner, with the retrieval depth progressively increasingover successive iterations.We delineate 1st-depth retrieval to facilitate a comprehensive un-derstanding of the retrieval process. The 1st-depth retrieval acquiresknowledge triples associated with entities in E. It is important tonote that DKG can be of considerable scale with dense relationsbetween entities. This complexity presents stressing challenges toretrieval efficiency. Our retrieval module employs a dual approach,using Exact Match for coarse-grained retrieval and Similarity Re-trieval for fine-grained retrieval, to mitigate the retrieval overheadover large DKGs. We let E (1)(E (1) E) be the input to the 1st-depth retrieval. For each entity E (1), the retrieval modulefirst executes Exact Match as coarse filtering to include knowledgetriples in DKG that have as their subject entity or object entity.Mathematically, the output of Exact Match at 1-st depth can be writ-ten as T (1),EM= { = (,,) G | = or = , E (1)}.Subsequently, Similarity Retrieval is implemented to refine knowl-edge triples in T (1),EM. Similarity Retrieval uses the pre-trained",
  "Way to Specialist: Closing Loop Between Specialized LLM and Evolving Domain Knowledge GraphConference acronym XX, June 0305, 2018, Woodstock, NY": "Detailed information about datasets and metrics is shown in Ap-pendix B.1 and B.2.WTS is compared with 4 SOTA baselines using prompting-basedmethods, including standard prompting (I/O prompt) on ChatGPT-3.5-turbo and GPT-4o, Think-on-Graph (ToG) with constantFreebase knowledge graph , and Chain-of Thought (CoT) with 5-shot prompting. 4.1.2Hyperparameters. The experiments run WTS with two LLMs,ChatGPT-3.5-turbo and GPT-4o, using OpenAI API. The tempera-ture is set to 0.2 and the maximum token length of output is set to2048. For all datasets, WTS is initialized with an empty vector data-base as its DKG. With ChromaDB3 serving as the vector database,we employ the pre-trained sentence transformer, all-mpnet-base-v24, as the embedding model. The embedding similarity is measuredby the cosine distance and the maximum permissible similarity gapis set to = 0.55. To facilitate result evaluation, WTS utilizesprompts to restrict its output format to JSON. This restriction isalso applied to GPT-3.5-turbo and GPT-4o baselines for fairnessconsideration. For the ToG and CoT baselines, minimal instructionsare added to their original prompts to ensure consistency in theevaluation process. Further details are provided in Appendix B.3.",
  "| (,) (,), T, T ()\\T. By applying the": "pruning module, the retrieved triples at -depth becomes T ().Correspondingly, the input entity set to -depth retrieval shouldbe modified as E (+1)= {| T (), =1 E () }.The above procedures implement width pruning, which con-strains the number of knowledge triples at each retrieval depthlevel. Actually, WTS also executes depth pruning, which entailsearly exit of retrieval before reaching the maximum depth . The depth pruning is conducted during the reasoning process. Therefore,it will be detailed in the subsequent reasoning module.Reasoning Module: The reasoning process performs progres-sively as the retrieval depth increases. Suppose that we have com-pleted -depth of retrieval and pruning, the retrieved knowledgetriples are denoted as T ()= =1 T (). The reasoning modulearranges the knowledge triples to form a prompt using the for-mat in Appendix D.1, Prompt 3, and then feeds the prompt andquestion to LLM to generate the answer ()to question , i.e.,",
  "()= LLMRea(, T ())": "The format of the ()is stipulated with the system prompt for ease of performance evaluation. If the evaluation is positive,the retrieval stops, otherwise, the retrieval enters the ( + 1)-thiteration. This process is repeated until the maximum search depth is reached. Take-away: DKG-Augmented LLM is not merely an isolated se-quence of retrieving information from the DKG and reasoningwith the LLM. Rather, it involves tight-coupling interactionsbetween DKG and LLM through the process, e.g., LLM is em-ployed for semantic relevance evaluation in retrieval widthpruning and early exit judgment in retrieval depth pruning.This allows LLM to retrieve relevant knowledge more effi-ciently over DKG, thereby enhancing its reasoning capacityfor domain-specific questions. 3.2.2LLM-Assisted DKG Evolution. LLM-assisted DKG endeav-ors to enhance the completeness of DKG by progressively incorpo-rating domain knowledge acquired during question answering. Theobjective of DKG completion is twofold. Firstly, it aims to incorpo-rate new knowledge (entities and relations) previously absent fromboth DKG and inherent knowledge of LLM. This is pertinent in sce-narios where no triples are retrieved for a question or the evaluationof the answer ()remains negative at the final depth . Secondly,DKG completion seeks to enhance the characterization of knowl-edge dependencies by activating knowledge connections previouslyneglected. This is pertinent to scenarios where a positive answeris obtained after multiple iterations of knowledge retrieval, whichindicates relevant knowledge in the current DKG is not adequatelyconnected. In this case, additional relations should be incorporatedinto DKG to ensure closer association between related entities.Domain Knowledge Generation: The fundamental step ofDKG evolution is generating knowledge triples based on questionsand answers. However, the extraction of knowledge graphs fromunstructured data presents significant challenges, primarily due tothe inherent variability and complexity of natural language .Given the irregular and unstructured nature of question-answerpairs, our work leverages LLMs to design a schema-free solutionfor KG triple generation. Firstly, WTS prompts LLM (the inputprompt is given in Appendix D.1, Prompt 4) to extract domainknowledge from question-answer pairs, using entities in the previ-ously retrieved knowledge triples, i.e., T (), as references. ThenLLMs generate knowledge triples based on the extracted domainknowledge. Given question and the gold answer , the module",
  "WTS Formation Pipeline": "In previous sections, we have presented DKG-Augmented LLM andLLM-Assisted DKG Evolution independently. Next, to show howthese two components work in tandem to complement each otherscapabilities.Let us start from the initialization stage. The initial DKG can beeither an empty dataset or an established DKG composed of knowl-edge triples. This enables WTS to support various implementationscenarios, including those where the domain lacks a pre-existingKG or where the available DKG is incomplete.As shown in , the formation pipeline of WTS involvescyclical interactions between DKG-Augmented LLM and LLM-AssistedDKG Evolution throughout the process of question answering. Thetimeline of WTS formation is discretized by the arrival of questions.The DKG possessed by WTS at the reception of question is de-noted by G, which is used in DKG-Augmented LLM to prompt theLLM for answer generation. Subsequently, WTS calls LLM-AssistedDKG Evolution to generate knowledge triples T + based on question and gold answer , and retrieved knowledge triples T (). Thegenerated triples T + are then incorporated to update DKG fromG to G+1. The updated DKG G+1 will be used to answer thesubsequent question + 1.From a practical perspective, the formation pipeline of WTS canbe split into two phases, Apprenticeship and Mastership, contin-gent upon the availability of gold answers. During Apprenticeship,WTS functions akin to an apprentice, receiving gold answers froma mentor after addressing questions. This scenario is applicablewhen comprehensive Q&A datasets are established in a particular",
  ": Illustration of WTS formation pipeline": "domain. Furthermore, this scenario may also occur when an expertutilizes WTS as an assistant and constantly provides gold answersto questions. The period of Mastership typically follows the Ap-prenticeship stage, once a DKG has been sufficiently established forprompting domain-specific tasks. During Mastership, WTS oper-ates autonomously without the guidance of gold answers. Instead,WTS relies on the user feedback (e.g., Good Response and BadResponse buttons in ChatGPT) to determine whether to extractknowledge triples from generated answers. If positive feedback isreceived, WTS considers the generated answer as a gold answerto extract T + as aforementioned in LLM-Assisted DKG Evolution.Conversely, if negative feedback (or no feedback) is received, WTSrelies solely on the question T + for DKG evolution. Take-away: The formation of WTS is in essence a processof closing the loop between DKG-Augmented LLM and LLM-Assisted DKG Evolution as WTS progressively answers andlearns from domain-specific questions. The connector is anevolving DKG that is utilized by DKG-Augmented LLM toprompt the question reasoning and updated by LLM-AssistedDKG Evolution based on the retrieved knowledge and answersto enhance domain knowledge comprehensiveness for subse-quent questions.",
  "EXPERIMENTS4.1Experiment Setup": "4.1.1Datasets and Baselines. We evaluate each method on 6 Q&Adatasets, covering 5 domains, including medical (ChatDoctor5k ,PubMedQA , MedMCQA ), natural science, social science,linguistics (SciQ , ScienceQA ), and one general domainQ&A datasets (Simple Questions ). The MedMCQA dataset in-cludes both single-choice and multiple-choice questions, while inthe ScienceQA dataset, SOC, NAT, and LAN refer to the socialscience, natural science, and linguistics domains, respectively.For the dataset ChatDoctor5k, we evaluate the text generationtask with BERTScore , which computes a similarity score ofthe LLM-generated answer and the real answer. To calculate theBERTScore, we use the bert-base-uncased model as the embed-ding model. For the other dataset, we use Accuracy as the metric.",
  "Results and Evaluations": "4.2.1Comparison to Baselines. compares the performance of WTS and baselines. Theresults indicate that WTS (with GPT-4o) outperforms the baselinesin 5 of 6 evaluated datasets. The exception occurs on the SimpleQAwhich is a general domain Q&A dataset, and in this case, ToGachieves the best performance as it includes a well-establishedgeneral KG, i.e., Freebase KG, for knowledge retrieval.WTS demonstrates a substantial performance improvement overthe standard I/O prompt methods (i.e., GPT-3.5 and GPT-4o) on all 5domain datasets. The highest performance improvement is achievedon the PubMedQA, providing 103.8% and 126.9% accuracy gain forGPT-3.5 and GPT-4o, respectively. However, it is noteworthy thatin the ChatDoctor5k dataset, GPT-4o underperforms GPT-3.5. Thisanomaly is attributed to the conservation of GPT-4o, wherein itfrequently qualifies its responses as non-professional advice andrecommends patients to consult a medical professional.WTS exhibits notable advantages in comparison with CoT acrossall evaluated datasets. The most significant performance improve-ments are 15.6% and 12.1% on the single-choice and multiple-choiceMedMCQA datasets, respectively. These enhancements are attribut-able to the incorporation of external domain knowledge. In contrast,CoT relies solely on the intrinsic knowledge of the LLM, whichlacks domain-specialized information.For commonsense reasoning (i.e., SimpleQA), ToG outperformsother methods as it utilizes commonsense knowledge retrievedfrom Freebase. However, when applied to specialized domains, theperformance of ToG becomes inferior because the knowledge inFreebase is coarse and insufficient for domain-specialized questions.To examine the complexity of the knowledge retrieval process,we calculate the average retrieval time and the execution time ofDKG-Augmented LLM (with GPT-3.5) for a single question. The result is depicted in , which reveals that the executiontime is two orders of magnitude greater than the retrieval time,indicating that the retrieval time has a negligible impact on theoverall efficiency of WTS. ChatDoctor5k PubMedQAMedMCQA(Single) MedMCQA(Multi)",
  ": DKG Size of WTSwith GPT-3.5 and GPT-4o": "We further investigate the impact of base LLM models on theretrieval process of WTS. illustrates the retrieval depth forquestions when WTS is implemented with GPT-3.5 and GPT-4o.The results indicate that a more powerful base model can betterleverage its internal knowledge, thereby avoiding deep retrievalof domain knowledge in DKG. Additionally, the enhanced textunderstanding and summarization abilities of stronger base mod-els facilitate superior knowledge extraction, leading to improvedperformance.",
  "MethodMetricPreRecBertScoreAccAccAccAccAccAccAccAcc": "GPT-3.50.7750.7920.7830.1570.5120.5070.9090.8420.8840.7240.220WTS(EM)0.7750.8090.7910.2810.5760.5480.9150.8740.9330.7860.256WTS(EM-ESR)0.7780.8080.7920.2930.6040.5630.9170.8760.9600.8080.264WTS(EM-QSR)0.7810.8100.7950.3200.6220.5570.9420.8900.9640.8100.264 (1) Exact Match (EM). This method retrieves knowledge triplesthat have the target entity as their subject or object entity.(2) Exact Match with Entity Similarity Retrieval (EM-ESR).This method first uses the exact match method for knowledge tripleretrieval. The retrieved triple will then be refined by evaluating thesimilarity between the embedding of extracted topic entities andknowledge triples retrieved by exact match.(3) Exact Match with Question Similarity Retrieval (EM-QSR). This method follows a similar procedure as EM-ESR. Thekey difference is that EM-QSR calculates the similarity between theembedding of questions and knowledge triples. presents the performance of WTS with different retrievalmechanisms configured in DKG-Augmented LLM. We see that EM-QSR exhibits the highest performances on most datasets as it fullyutilizes the entities and other semantic information in question toidentify most relevant knowledge triples. EM-ESR ranks secondbecause it captures less semantic information compared to EM-QSR.The inferior performance of EM indicates that relying solely onexact matching and neglecting the semantic information of thequestion fails to obtain high-quality knowledge.How does DKG evolution affect the WTS performance?Next, we show how DKG evolution benefits the reasoning perfor-mance of specialized LLM. shows the accuracy improve-ment of WTS over GPT-3.5 as WTS proceeds. It is evident that asthe number of processed Q&A samples increases, the size of DKGincreases accordingly. It can be observed that the performance im-provement experiences fluctuations during the formation processof WTS. This is attributed to the uncertain knowledge overlapbetween previously answered questions and questions yet to beanswered. Despite these fluctuations, the overall trend indicates anenhancement in generation quality. illustrates the sizes ofDKGs constructed by WTS with different base models and retrievalmechanisms. Disregarding the minor randomness introduced by the temperature parameter of LLMs, we see that employing ad-vanced models and retrieval mechanisms results in the constructionof compact DKGs. This is because an advanced base model pos-sesses more comprehensive inherent knowledge, and an advancedretrieval model can acquire relevant knowledge triples from DKGwith greater efficiency.",
  "Chatdoctor5k0.7920.7950.7940.794PubMedQA0.2110.2560.3200.275MedMCQA(Single)0.3580.4570.6220.550MedMCQA(Multi)0.3600.4930.5570.527": "How does the retrieval depth affect the WTS performance?To investigate the impact retrieval Depth , we conducted exper-iments of WTS with various depths. For ChatDoctor5k and Pub-MedQA, the maximum retrieval depth ranges from 1 to 4, while forMedMCQA, it is set from 2 to 5. illustrates the performanceon ChatDoctor5k, PubMedQA, and MedMCQA datasets with vary-ing retrieval depths. As shown in the results, the performance ofWTS improves with increased search depth, suggesting that fur-ther enhancement could be achieved with deeper exploration inDKG. However, this improvement decreases as the retrieval depthbecomes excessive. This is primarily because deeper retrieval intro-duces more irrelevant information, which disrupts the generationof accurate answers and increases computational overhead. Theexecution time of the different maximal retrieval depths on thesedatasets is shown in Appendix C .",
  "(h) ScienceQA-LAN": ": Accuracy improvements and KG size in medicaldomain.framework, combining specialized LLMs with evolving domainknowledge graphs to address knowledge-intensive tasks in spe-cific domains. Through its two tightly integrated componentstheDKG-Augmented LLM and the LLM-Assisted DKG EvolutionWTSdemonstrates superior performance over existing prompting-basedmethods in domain-specific tasks, all while avoiding additionaltraining costs. Our experimental results demonstrate that WTSsurpasses the previous state-of-the-art in four specialized domains,achieving a maximum performance improvement of 11.3%. In futurework, we will focus on developing more efficient and high-qualityapproaches for dynamic knowledge graph completion. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, ShyamalAnadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774(2023).",
  "Dhananjay Ashok and Zachary C Lipton. 2023. Promptner: Prompting for namedentity recognition. arXiv preprint arXiv:2305.15444 (2023)": "Jinheon Baek, Alham Fikri Aji, and Amir Saffari. 2023. Knowledge-AugmentedLanguage Model Prompting for Zero-Shot Knowledge Graph Question Answer-ing. In The 61st Annual Meeting Of The Association For Computational Linguistics. Teodoro Baldazzi, Luigi Bellomarini, Stefano Ceri, Andrea Colombo, AndreaGentili, and Emanuel Sallinger. 2023. Fine-tuning large enterprise languagemodels via ontological reasoning. In International Joint Conference on Rules andReasoning. Springer, 8694. Zhen Bi, Jing Chen, Yinuo Jiang, Feiyu Xiong, Wei Guo, Huajun Chen, and NingyuZhang. 2024. Codekgc: Code language model for generative knowledge graphconstruction. ACM Transactions on Asian and Low-Resource Language InformationProcessing 23, 3 (2024), 116. Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor.2008. Freebase: a collaboratively created graph database for structuring humanknowledge. In Proceedings of the 2008 ACM SIGMOD international conference onManagement of data. 12471250.",
  "Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. 2015. Large-scale simple question answering with memory networks.arXiv preprintarXiv:1506.02075 (2015)": "Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Ok-sana Yakhnenko. 2013. Translating embeddings for modeling multi-relationaldata. Advances in neural information processing systems 26 (2013). Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter,Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, BenjaminChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, IlyaSutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.In Advances in Neural Information Processing Systems, H. Larochelle, M. Ran-zato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates,Inc., 18771901.",
  "Yihan Cao, Yanbin Kang, and Lichao Sun. 2023.Instruction mining: High-quality instruction data selection for large language models. arXiv preprintarXiv:2307.06290 (2023)": "Salvatore Carta, Alessandro Giuliani, Leonardo Piano, Alessandro SebastianPodda, Livio Pompianu, and Sandro Gabriele Tiddia. 2023. Iterative zero-shot llmprompting for knowledge graph construction. arXiv preprint arXiv:2307.01128(2023). Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, GauravMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Se-bastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways.Journal of Machine Learning Research 24, 240 (2023), 1113.",
  "Zhouyu Jiang, Ling Zhong, Mengshu Sun, Jun Xu, Rui Sun, Hui Cai, Shuhan Luo,and Zhiqiang Zhang. 2024. Efficient Knowledge Infusion via KG-LLM Alignment.arXiv preprint arXiv:2406.03746 (2024)": "Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu.2019. PubMedQA: A Dataset for Biomedical Research Question Answering. InProceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-cessing and the 9th International Joint Conference on Natural Language Processing(EMNLP-IJCNLP). 25672577. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,Naman Goyal, Heinrich Kttler, Mike Lewis, Wen-tau Yih, Tim Rocktschel,et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks.Advances in Neural Information Processing Systems 33 (2020), 94599474. Haotian Li, Lingzhi Wang, Yuliang Wei, Richard Yi Da Xu, and Bailing Wang.2023. KERMIT: Knowledge Graph Completion of Enhanced Relation Modelingwith Inverse Transformation. arXiv preprint arXiv:2309.14770 (2023).",
  "(llama) using medical domain knowledge. Cureus 15, 6 (2023)": "Yong Lin, Lu Tan, Hangyu Lin, Zeming Zheng, Renjie Pi, Jipeng Zhang, ShizheDiao, Haoxiang Wang, Han Zhao, Yuan Yao, et al. 2023. Speciality vs generality:An empirical study on catastrophic forgetting in fine-tuning foundation models.arXiv preprint arXiv:2309.06256 (2023). Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu, Derong Xu, Feng Tian, andYefeng Zheng. 2024. When MOE Meets LLMs: Parameter Efficient Fine-tuningfor Multi-task Medical Applications. In Proceedings of the 47th International ACMSIGIR Conference on Research and Development in Information Retrieval. 11041114. Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and PingWang. 2020. K-bert: Enabling language representation with knowledge graph. InProceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 29012908. Zhengliang Liu, Aoxiao Zhong, Yiwei Li, Longtao Yang, Chao Ju, Zihao Wu,Chong Ma, Peng Shu, Cheng Chen, Sekeun Kim, et al. 2023. Tailoring largelanguage models to radiology: A preliminary approach to llm adaptation for ahighly specialized domain. In International Workshop on Machine Learning inMedical Imaging. Springer, 464473.",
  "Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. 2023. Reasoningon graphs: Faithful and interpretable large language model reasoning. arXivpreprint arXiv:2310.01061 (2023)": "Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, andHannaneh Hajishirzi. 2022. When not to trust language models: Investigat-ing effectiveness of parametric and non-parametric memories. arXiv preprintarXiv:2212.10511 (2022). Nicholas Matsumoto, Jay Moran, Hyunjun Choi, Miguel E Hernandez, MythreyeVenkatesan, Paul Wang, and Jason H Moore. 2024. KRAGEN: a knowledge Graph-Enhanced RAG framework for biomedical problem solving using large languagemodels. Bioinformatics (2024), btae353. Lars-Peter Meyer, Claus Stadler, Johannes Frey, Norman Radtke, Kurt Junghanns,Roy Meissner, Gordian Dziwis, Kirill Bulert, and Michael Martin. 2023. Llm-assisted knowledge graph engineering: Experiments with chatgpt. In Workingconference on Artificial Intelligence Development for a Resilient and SustainableTomorrow. Springer Fachmedien Wiesbaden Wiesbaden, 103115. Grgoire Mialon, Roberto Dess, Maria Lomeli, Christoforos Nalmpantis, RamPasunuru, Roberta Raileanu, Baptiste Rozire, Timo Schick, Jane Dwivedi-Yu, AsliCelikyilmaz, et al. 2023. Augmented language models: a survey. arXiv preprintarXiv:2302.07842 (2023). Nandana Mihindukulasooriya, Sanju Tiwari, Carlos F Enguix, and Kusum Lata.2023. Text2kgbench: A benchmark for ontology-driven knowledge graph gener-ation from text. In International Semantic Web Conference. Springer, 247265.",
  "OpenAI. [n. d.]. Chat Completions Guide. Accessed: 2024-08-06": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, PamelaMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022.Training language models to follow instructions with human feedback. Advancesin neural information processing systems 35 (2022), 2773027744. Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022.MedMCQA: A Large-scale Multi-Subject Multi-Choice Dataset for Medical do-main Question Answering. In Proceedings of the Conference on Health, Inference,and Learning (Proceedings of Machine Learning Research, Vol. 174), Gerardo Flores,George H Chen, Tom Pollard, Joyce C Ho, and Tristan Naumann (Eds.). PMLR,248260. Jeff Z Pan, Simon Razniewski, Jan-Christoph Kalo, Sneha Singhania, JiaoyanChen, Stefan Dietze, Hajira Jabeen, Janna Omeliyanenko, Wen Zhang, Matteo Lis-sandrini, et al. 2023. Large language models and knowledge graphs: Opportunitiesand challenges. arXiv preprint arXiv:2308.06374 (2023).",
  "Nita Patil, Ajay Patil, and BV Pawar. 2020. Named entity recognition usingconditional random fields. Procedia Computer Science 167 (2020), 11811188": "Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, QiuyuanHuang, Lars Liden, Zhou Yu, Weizhu Chen, et al. 2023. Check your facts and tryagain: Improving large language models with external knowledge and automatedfeedback. arXiv preprint arXiv:2302.12813 (2023). Andr Gomes Regino, Rodrigo Oliveira Caus, Victor Hochgreb, and Julio Cesardos Reis. 2022. From Natural Language Texts to RDF Triples: A Novel Approachto Generating e-Commerce Knowledge Graphs. In International Joint Conferenceon Knowledge Discovery, Knowledge Engineering, and Knowledge Management.Springer, 149174. Ali Sadeghian, Mohammadreza Armandpour, Patrick Ding, and Daisy Zhe Wang.2019. Drum: End-to-end differentiable rule mining on knowledge graphs. Ad-vances in Neural Information Processing Systems 32 (2019). Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, and Pushpak Bhat-tacharyya. 2022. Scienceqa: A novel resource for question answering on scholarlyarticles. International Journal on Digital Libraries 23, 3 (2022), 289301.",
  "Prateek Sancheti, Kamalakar Karlapalem, and Kavita Vemuri. 2024. LLM DrivenWeb Profile Extraction for Identical Names. In Companion Proceedings of the ACMon Web Conference 2024. 16161625": "Apoorv Saxena, Aditay Tripathi, and Partha Talukdar. 2020. Improving multi-hopquestion answering over knowledge graphs using knowledge base embeddings.In Proceedings of the 58th annual meeting of the association for computationallinguistics. 44984507. Viktor Schlegel and Andr Freitas. 2019. DBee: A database for creating andmanaging knowledge graphs and embeddings. In Proceedings of the ThirteenthWorkshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13). 177185.",
  "Tilahun Abedissa Taffa and Ricardo Usbeck. 2023. Leveraging llms in scholarlyknowledge graph question answering. arXiv preprint arXiv:2311.09841 (2023)": "Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-BaptisteAlayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth,et al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprintarXiv:2312.11805 (2023). Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, LauraGutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. 2023. Large language modelsin medicine. Nature medicine 29, 8 (2023), 19301940. Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kul-shreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022.Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239(2022). Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-AnneLachaux, Timothe Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, et al. 2023. Llama: Open and efficient foundation language models. arXivpreprint arXiv:2302.13971 (2023). Tho Trouillon, Johannes Welbl, Sebastian Riedel, ric Gaussier, and GuillaumeBouchard. 2016. Complex embeddings for simple link prediction. In Internationalconference on machine learning. PMLR, 20712080.",
  "Denny Vrandei and Markus Krtzsch. 2014. Wikidata: a free collaborativeknowledgebase. Commun. ACM 57, 10 (2014), 7885": "Bo Wang, Tao Shen, Guodong Long, Tianyi Zhou, Ying Wang, and Yi Chang.2021. Structure-augmented text representation learning for efficient knowledgegraph completion. In Proceedings of the Web Conference 2021. 17371748. Liang Wang, Wei Zhao, Zhuoyu Wei, and Jingming Liu. 2022. SimKGC: SimpleContrastive Knowledge Graph Completion with Pre-trained Language Models.In Proceedings of the 60th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers). 42814294. Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang, Fei Wu, Tianwei Zhang,Jiwei Li, and Guoyin Wang. 2023. Gpt-ner: Named entity recognition via largelanguage models. arXiv preprint arXiv:2304.10428 (2023). Xiaoyan Wang, Pavan Kapanipathi, Ryan Musa, Mo Yu, Kartik Talamadupula,Ibrahim Abdelaziz, Maria Chang, Achille Fokoue, Bassem Makni, Nicholas Mattei,et al. 2019. Improving natural language inference using external knowledge inthe science questions domain. In Proceedings of the AAAI conference on artificialintelligence, Vol. 33. 72087215. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoningin large language models. Advances in neural information processing systems 35(2022), 2482424837.",
  "Yilin Wen, Zifeng Wang, and Jimeng Sun. 2023. Mindmap: Knowledge graphprompting sparks graph of thoughts in large language models. arXiv preprintarXiv:2308.09729 (2023)": "Yike Wu, Nan Hu, Guilin Qi, Sheng Bi, Jie Ren, Anhuan Xie, and Wei Song.2023. Retrieve-rewrite-answer: A KG-to-text enhanced LLMS framework forknowledge graph question answering. arXiv preprint arXiv:2309.11206 (2023). Fei Xia, Bin Li, Yixuan Weng, Shizhu He, Kang Liu, Bin Sun, Shutao Li, and JunZhao. 2022. MedConQA: medical conversational question answering systembased on knowledge graphs. In Proceedings of the 2022 Conference on EmpiricalMethods in Natural Language Processing: System Demonstrations. 148158. Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, MichihiroYasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I Wang, et al.2022. Unifiedskg: Unifying and multi-tasking structured knowledge groundingwith text-to-text language models. arXiv preprint arXiv:2201.05966 (2022). Yao Xu, Shizhu He, Jiabei Chen, Zihao Wang, Yangqiu Song, Hanghang Tong,Kang Liu, and Jun Zhao. 2024. Generate-on-Graph: Treat LLM as both Agentand KG in Incomplete Knowledge Graph Question Answering. arXiv preprintarXiv:2404.14741 (2024).",
  "Liang Yao, Chengsheng Mao, and Yuan Luo. 2019. KG-BERT: BERT for knowledgegraph completion. arXiv preprint arXiv:1909.03193 (2019)": "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, andKarthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving withlarge language models. Advances in Neural Information Processing Systems 36(2024). Miao Zhang, Rufeng Dai, Ming Dong, and Tingting He. 2022. Drlk: dynamichierarchical reasoning with language model and knowledge graph for questionanswering. In Proceedings of the 2022 Conference on Empirical Methods in NaturalLanguage Processing. 51235133.",
  "B.2Metrics": "For the dataset ChatDoctor5k, we evaluate the text generation taskwith BERTScore , which computes a similarity score for eachtoken in the candidate sentence with each token in the referencesentence. The complete score matches each token in x to a tokenin x to compute recall, and each token in x to a token in x to com-pute precision. We use greedy matching to maximize the matchingsimilarity score, where each token is matched to the most similartoken in the other sentence. We combine precision and recall tocompute an F1 measure. For a reference x and candidate x, therecall, precision, and F1 scores are:",
  "B.3Evaluation": "To facilitate result evaluation, WTS employs prompts to constrainits output format to JSON. To prevent irrelevant generations frominterfering with answer evaluation, the LLMs are instructed toprovide the label of their answers in multiple-choice questions.Consequently, the evaluation is based solely on whether the labelcorresponds to the correct answer. This constraint is also applied tothe GPT-3.5-turbo and GPT-4o baselines for the sake of fairness. Forthe ToG and CoT baselines, to avoid imposing format restrictionsthat may affect their performance, we use a specific signal by adding( before the answer and ) after it. This method ensures the preciseextraction of answers and upholds consistency throughout theevaluation process.",
  "CSUPPLEMENTARY RESULTS": "shows the average execution time of WTS for a question,where represents the maximal retrieval depth. It is apparentthat WTS with GPT-4o API incurs a higher time cost compared toGPT-3.5. shows the performance of the Mastership phase on theChatDoctor5k dataset. During the Apprenticeship phase, we sam-pled 800 data to conduct medical Q&A and consistently evolve theDKG. In the Mastership phase, we tested the performance of WTSwith another 200 non-overlapping samples. The results show thatWTS achieves state-of-the-art performance compared to the fourbaseline models.The execution time (sec) of the different maximal retrieval depthson three medical domain datasets, ChatDoctor5k, PubMedQA, andMedMCQA, is shown in . For ChatDoctor5k and PubMedQA,the maximum retrieval depth ranges from 1 to 4, while for MedM-CQA, it is set from 2 to 5. The results indicate that as the maximumretrieval depth increases, execution time correspondingly rises.We analyzed the answers across six datasets to investigate theevidence for WTS in generating answers, as illustrated in .The results indicate that in the general domain and certain sciencedomains, e.g., social science, which includes a considerable amountof commonsense questions, a significant proportion of the answersrely exclusively on the intrinsic knowledge embedded within LLMsparameters. However, in specialized domains such as the medicaldomain and linguistics, accurate answer generation requires specificdomain knowledge. Consequently, a substantial proportion of thesequeries rely on a combination of knowledge from both retrievedtriples and the LLMs inherent knowledge for answer generation. shows the size of the knowledge graph constructed in theprocess of WTS. The results show that advanced models with moreinherent knowledge lead to the construction of more concise DKGs.Furthermore, the advanced retrieval mechanisms also enhance theconciseness of DKGs by optimizing the WTSs capability to utilizeknowledge efficiently.",
  "D.1WTS (Way-to-Specialist)": "In WTS, there are four kinds of interaction with the LLMs. Thefollowing shows the prompts of WTS including Question EntityExtract Prompt, Triple Score and Prune Prompt, LLM Reason withTriples Prompt, and LLM Generate KG Triple Prompt.As shown in Prompt 1, the \"Question Entity Extraction\" promptof WTS is designed to extract key entities from questions. The prompt instructs the LLM to function as a medical assistant, per-forming Named Entity Recognition to identify and output up to fivemeaningful entities. This prompt ensures that the most relevantinformation is captured concisely.",
  "Prompt 1. Question Entity Extraction": "system prompt:You are a [medical] assistant to carry out [Medical] Name EntityRecognition from a question and output JSON with only one keyentity, like {entities: [entity1, entity2,...]} user prompt:Get at most 5 meaningful entities in the question: { Question }. As shown in Prompt 2, the \"Score and Prune\" prompt of WTSis designed to evaluate and rank the retrieved KG triples based ontheir relevance to a given question. The prompt instructs the LLMto assign a relevance score to each triple, and output a structuredJSON object listing the most relevant triples in descending order oftheir scores.",
  "Prompt 2. Triple Score and Prune": "system prompt:You are a [medical] expert to score the triples based on the rele-vance of the question and output JSON with only one key triplelike { triples: [{ triple: { head: xxx, relation: xxx, tail: xxx }, score:xxx },...] }. The score is between 0 and 1, and the order is fromhigh to low. user prompt:Based on the question: { Question }, score the triples: { Triples }. The \"LLM Reason with Triples\" prompt of WTS is designed toinstruct LLM in generating answers to questions by reasoning withboth its inherent knowledge and provided KG triples. The systemprompt restricts the LLM to output a JSON object that includes theconfidence level of LLM, the answer to the question, and the sup-porting information used to generate the answer, ensuring clarityin the response and precision in further evaluation.",
  "Prompt 3. LLM Reason with Triples": "system prompt:You are a [medical] expert to answer patients questions basedon your own knowledge and the given information and giveconfidence [Yes/No]. The output should be a JSON with threekeys confidence, answer, and support_info like {confidence: xxx,answer: xxx, support_info: xxx} user prompt:Based on your own knowledge and the <knowledge triple>,choose one of the <Option> to answer the question. knowledgetriple: { Triples }, Q: { Question }, Option: { Option }, A: ? The \"Generate KG Triple\" prompt in the WTS framework in-structs the LLM to extract and construct KG triples from the pro-vided question, answer, and entities. The prompt instructs the LLMto produce KG triples formatted with a head, relation, and tail,which creates a structured representation of knowledge.",
  "Prompt 4. LLM Generate KG Triple": "system prompt:You are a [medical] expert to extract general [medical] knowledgefrom question and answer to create [medical] knowledge graphtriples. The output should be a JSON with only one key: tripleslike this: { triples: [<triple1>, <triple2>, ...] }, and each triple shouldbe like: { head: xxx, relation: xxx, tail: xxx }. user prompt:Based on the question: { Question }, answer: { Answer } and entity:{ Entity }, extract and output triples.",
  "Prompt 5. Directly Generate Answer": "system prompt:You are an assistant to answer questions to output JSON. user prompt:Here is a question, choose one of the <Option> to answer thequestion. Your answer must be [0/1/2/3] which is the order ofyour choice and your output has only one key answer.Q: { Question }, Option:{ Option }, A: ?Example:Q: Which time zone is sesto ed uniti located in?Option: [Central European Time Zone, Greenwich Mean Time,Coordinated Universal Time, Central Standard Time]A: { answer: 0 }",
  "Prompt 6. Directly Generate Answer": "system prompt:You are an assistant to answer questions. user prompt:Select the [0/1/2/3] which is the order of your choice in Option aspart of your answer.Example:Q: Sammy wanted to go to where the people were. Where mighthe go?Option: [race track, populated areas, desert, apartment]A: The answer must be a place with a lot of people. Race tracks,deserts, apartments, and roadblocks dont have a lot of people,but populated areas do. So the answer is (1).Q: Google Maps and other highway and street GPS services havereplaced what?Option: [united states, mexico, countryside, atlas]A: The answer must be something that is used to do what GoogleMaps and GPS services do, which is to give directions. Of the",
  "DatasetWTS(GPT-3.5 w/CM)WTS(GPT-3.5 w/EM-ESR)WTS(GPT-3.5 w/EM-QSR)WYS(GPT-4o w/EM-QSR)": "ChatDoctor5k3,7123,5543,1783,136PubMedQA1,2271,6351,4801,178MedMCQA(Single)1,0241,0331,124533MedMCQA(Multi)8071,3611,204655SimpleQA1,0951,1341,097828sciq18118419152ScienceQA-NAT15617114621ScienceQA-SOC4329251ScienceQA-LAN385329317159 above choices, only atlases are used to give directions. So theanswer is (3).Q: Before getting a divorce, what did the wife feel who was doingall the work?Option: [harder, anguish, bitterness, tears]A: The answer should be the feeling of someone getting divorcedwho was doing all the work. Of the above choices, the closestfeeling is bitterness. So the answer is (2).Q: What home entertainment equipment requires cable?Option: [television, radio shack, substation, cabinet]A: The answer must require cable. Of the above choices, onlytelevision requires cable. So the answer is (0).Q: What kitchen appliance uses water to function?Option: [microwave, toaster, dishwasher, blender ]A: The answer must use water to function. Of the above choices,only the dishwasher uses water to function. So the answer is (2).Q: { Question }, Option: { Option }, A: ?",
  "Prompt 7. Directly Generate Answer": "system prompt:You are an AI assistant that helps people find information. user prompt:Select the [0/1/2/3] which is the order of your choice in Option aspart of your answer.Example:Q: Sammy wanted to go to where the people were. Where mighthe go?Option: [race track, populated areas, desert, apartment]A: The answer must be a place with a lot of people. Race tracks,deserts, apartments, and roadblocks dont have a lot of people,but populated areas do. So the answer is (1). Q: Google Maps and other highway and street GPS services havereplaced what?Option: [united states, mexico, countryside, atlas]A: The answer must be something that is used to do what GoogleMaps and GPS services do, which is to give directions. Of theabove choices, only atlases are used to give directions. So theanswer is (3).Q: Before getting a divorce, what did the wife feel who was doingall the work?Option: [harder, anguish, bitterness, tears]A: The answer should be the feeling of someone getting divorcedwho was doing all the work. Of the above choices, the closestfeeling is bitterness. So the answer is (2).Q: What home entertainment equipment requires cable?Option: [television, radio shack, substation, cabinet]A: The answer must require cable. Of the above choices, onlytelevision requires cable. So the answer is (0).Q: What kitchen appliance uses water to function?Option: [microwave, toaster, dishwasher, blender]A: The answer must use water to function. Of the above choices,only the dishwasher uses water to function. So the answer is (2).Q:{ Question }, Option: { Option }, A: ?",
  "Prompt 10. Extract Entity": "system prompt:You are an AI assistant that helps people find information. user prompt:Get the meaningful entities in the question. The answer shouldbe like this:Q: Doctor, Ive been experiencing a condition called cryp-torchidism. My testicles have not descended properly into thescrotum. What medical tests do I need to take?A:entities: [cryptorchidism, testicles] in the form of a list.Q: { Question }",
  "Prompt 11. Score Entity Candidates": "system prompt:You are an AI assistant that helps people find information. user prompt:Please score the entities contribution to the question on a scalefrom 0 to 1 (the sum of the scores of all entities is 1).Example:Q: The movie featured Miley Cyrus and was produced by TobinArmbrust?Relation: film.producer.filmEntities: The Resident; So Undercover; Let Me In; Begin Again;The Quiet Ones; A Walk Among the TombstonesScore: 0.0, 1.0, 0.0, 0.0, 0.0, 0.0 The movie that matches the givencriteria is \"So Undercover\" with Miley Cyrus and produced byTobin Armbrust. Therefore, the score for \"So Undercover\" wouldbe 1, and the scores for all other entities would be 0.Q: { Question } Relation: { Relation } Entities: { Entities }",
  "Prompt 12. Reason Answer": "system prompt:You are an AI assistant that helps people find information. user prompt:Given a question and the associated retrieved knowledge graphtriplets (entity, relation, entity), you are asked to answer the ques-tion with these triplets and your knowledge. You must select the[0/1/2/3] which is the order of your choice in Option as part ofyour answer.Q: Find the person who said Taste cannot be controlled by law,what did this person die from?Option: [illness, earthquake, murder, accident]Knowledge Triplets: Taste cannot be controlled by law., me-dia_common.quotation.author, Thomas JeffersonA: Based on the given knowledge triplets, its not sufficient toanswer the entire question. The triplets only provide informa-tion about the person who said \"Taste cannot be controlled bylaw,\" which is Thomas Jefferson. To answer the second part ofthe question, its necessary to have additional knowledge aboutwhere Thomas Jeffersons dead. In my opinion, the answer to thequestion is (0).Q: The artist nominated for The Long Winter lived where? Option: [De Smet, Denmark, Italy, Tokyo]Knowledge Triplets: The Long Winter, book.written_work.author,LauraIngallsWilderLauraIngallsWilder,peo-ple.person.places_lived,Unknown-EntityUnknown-Entity,people.place_lived.location, De SmetA: Based on the given knowledge triplets, the author of The LongWinter, Laura Ingalls Wilder, lived in De Smet. Therefore, theanswer to the question is (0).Q: Who is the coach of the team owned by Steve Bisciotti? Option:[Donald J. Trump, Vincent Thomas Lombardi, Joe Paterno, JohnHarbaugh]Knowledge Triplets: Steve Bisciotti, sports.professional_sports_team.owner_s,BaltimoreRavensSteveBis-ciotti,sports.sports_team_owner.teams_owned,BaltimoreRavensSteveBisciotti,organiza-tion.organization_founder.organizations_founded,AllegisGroupA: Based on the given knowledge triplets, the coach of the teamowned by Steve Bisciotti is not explicitly mentioned. However, itcan be inferred that the team owned by Steve Bisciotti is the Bal-timore Ravens, a professional sports team. Therefore, additionalknowledge about the current coach of the Baltimore Ravens canbe used to answer the question. In my opinion, the answer to thequestion is (3).Q: Rift Valley Province is located in a nation that uses which formof currency?Option: [Afghan afghani, Japanese yen, Kenyan shilling, Euro]Knowledge Triplets: Rift Valley Province, location. admin-istrative_division.country, Kenya Rift Valley Province, loca-tion.location.geolocation, UnName_Entity Rift Valley Province,location.mailing_address.state_province_region, UnName_EntityKenya, location.country.currency_used, Kenyan shillingA: Based on the given knowledge triplets, Rift Valley Province islocated in Kenya, which uses the Kenyan shilling as its currency.Therefore, the answer to the question is (2).Q: The country with the National Anthem of Bolivia borderswhich nations?Option: [Uruguay, Brazil, Venezuela, Colombia]Knowledge Triplets: National Anthem of Bolivia, govern-ment. national_anthem_of_a_country.anthem, UnName_EntityNational Anthem of Bolivia, music.composition.composer,Leopoldo Benedetto Vincenti National Anthem of Bolivia, mu-sic.composition.lyricist, Jos Ignacio de Sanjins UnName_Entity,government.national_anthem_of_a_country.country, Bolivia Bo-livia, location.country.national_anthem, UnName_EntityA: Based on the given knowledge triplets, we can infer that theNational Anthem of Bolivia is the anthem of Bolivia. Therefore,the country with the National Anthem of Bolivia is Bolivia itself.However, the given knowledge triplets do not provide informationabout which nations border Bolivia. To answer this question, weneed additional knowledge about the geography of Bolivia and itsneighboring countries. In my opinion, the answer to the questionis (1).Q: { Question }",
  "Prompt 13. Information Evaluate": "system prompt:You are an AI assistant that helps people find information. user prompt:Given a question and the associated retrieved knowledge graphtriplets (entity, relation, entity), you are asked to answer whetherits sufficient for you to answer the question with these tripletsand your knowledge (Yes or No). Besides, you must select the[0/1/2/3] which is the order of your choice in Option as part ofyour answer.Q: Find the person who said Taste cannot be controlled by law,what did this person die from?Option: [illness, earthquake, murder, accident]Knowledge Triplets: Taste cannot be controlled by law., me-dia_common.quotation.author, Thomas JeffersonA: No. Based on the given knowledge triplets, its not sufficientto answer the entire question. The triplets only provide informa-tion about the person who said \"Taste cannot be controlled bylaw,\" which is Thomas Jefferson. To answer the second part ofthe question, its necessary to have additional knowledge aboutwhere Thomas Jeffersons dead. In my opinion, the answer to thequestion is (0).Q: The artist nominated for The Long Winter lived where?Option: [De Smet, Denmark, Italy, Tokyo]Knowledge Triplets: The Long Winter, book.written_work.author,LauraIngallsWilderLauraIngallsWilder,peo-ple.person.places_lived,Unknown-EntityUnknown-Entity,people.place_lived.location, De SmetA: Yes. Based on the given knowledge triplets, the author of TheLong Winter, Laura Ingalls Wilder, lived in De Smet. Therefore,the answer to the question is (0).Q: Who is the coach of the team owned by Steve Bisciotti?Option: [Donald J. Trump, Vincent Thomas Lombardi, Joe Paterno,John Harbaugh]KnowledgeTriplets:SteveBisciotti,sports.professional_sports_team.owner_s,BaltimoreRavensSteveBisciotti,sports.sports_team_owner.teams_owned,BaltimoreRavensSteveBisciotti,organiza-tion.organization_founder.organizations_founded,AllegisGroupA: No. Based on the given knowledge triplets, the coach of theteam owned by Steve Bisciotti is not explicitly mentioned. How-ever, it can be inferred that the team owned by Steve Bisciottiis the Baltimore Ravens, a professional sports team. Therefore,additional knowledge about the current coach of the BaltimoreRavens can be used to answer the question. In my opinion, theanswer to the question is (3).Q: Rift Valley Province is located in a nation that uses which formof currency?Option: [Afghan afghani, Japanese yen, Kenyan shilling, Euro]KnowledgeTriplets:RiftValleyProvince,loca-tion.administrative_division.country,KenyaRiftValleyProvince, location.location.geolocation, UnName_Entity RiftValley Province, location.mailing_address.state_province_region, UnName_Entity Kenya, location. country.currency_used, KenyanshillingA: Yes. Based on the given knowledge triplets, Rift Valley Provinceis located in Kenya, which uses the Kenyan shilling as its currency.Therefore, the answer to the question is (2).Q: The country with the National Anthem of Bolivia borderswhich nations?Option: [Uruguay, Brazil, Venezuela, Colombia]Knowledge Triplets: National Anthem of Bolivia, govern-ment. national_anthem_of_a_country.anthem, UnName_EntityNational Anthem of Bolivia, music.composition.composer,Leopoldo Benedetto Vincenti National Anthem of Bolivia, mu-sic.composition.lyricist, Jos Ignacio de Sanjins UnName_Entity,government.national_anthem_of_a_country.country, Bolivia Bo-livia, location.country.national_anthem, UnName_EntityA: No. Based on the given knowledge triplets, we can infer that theNational Anthem of Bolivia is the anthem of Bolivia. Therefore,the country with the National Anthem of Bolivia is Bolivia itself.However, the given knowledge triplets do not provide informationabout which nations border Bolivia. To answer this question, weneed additional knowledge about the geography of Bolivia and itsneighboring countries. In my opinion, the answer to the questionis (1).Q:{ Question }, Option: { Option }, Knowledge Triplets: { KnowledgeTriplets }, A: ?"
}