{
  "Abstract": "Logs are ubiquitous digital footprints, playing an indispensable rolein system diagnostics, security analysis, and performance optimiza-tion. The extraction of actionable insights from logs is criticallydependent on the log parsing process, which converts raw logs intostructured formats for downstream analysis. Yet, the complexities ofcontemporary systems and the dynamic nature of logs pose signifi-cant challenges to existing automatic parsing techniques. The emer-gence of Large Language Models (LLM) offers new horizons. Withtheir expansive knowledge and contextual prowess, LLMs havebeen transformative across diverse applications. Building on this,we introduce LogParser-LLM, a novel log parser integrated withLLM capabilities. This union seamlessly blends semantic insightswith statistical nuances, obviating the need for hyper-parametertuning and labeled training data, while ensuring rapid adaptabilitythrough online parsing. Further deepening our exploration, weaddress the intricate challenge of parsing granularity, proposing anew metric and integrating human interactions to allow users tocalibrate granularity to their specific needs. Our methods efficacyis empirically demonstrated through evaluations on the Loghub-2kand the large-scale LogPub benchmark. In evaluations on the Log-Pub benchmark, involving an average of 3.6 million logs per datasetacross 14 datasets, our LogParser-LLM requires only 272.5 LLM Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from , July 2017, Washington, DC, USA 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-x-xxxx-xxxx-x/YY/MM invocations on average, achieving a 90.6% F1 score for groupingaccuracy and an 81.1% for parsing accuracy. These results demon-strate the methods high efficiency and accuracy, outperformingcurrent state-of-the-art log parsers, including pattern-based, neuralnetwork-based, and existing LLM-enhanced approaches.",
  "Introduction": "Logs are pervasive records in the digital realm, vital for system di-agnostics, security analysis, and performance optimization. As wenavigate the complexities of contemporary digital environments,our systems, applications, and networks consistently generate vastamounts of logs. These abundant logs serve as an invaluable re-source for understanding system behaviors, tracking activities, anduncovering hidden patterns. Their importance cannot be overstated,especially given the sophisticated nature of present-day systemsand the crucial need for maintaining robust and efficient operations.This rich information source of cloud computing aids in tasks suchas anomaly detection , failure prediction , and",
  "Conference17, July 2017, Washington, DC, USAAoxiao Zhong et al": ": A demonstration of granularity variations in logparsing. Colors denote groups of templates. Applicability isrepresented on the vertical axis, while Specificity is repre-sented on the horizontal axis. 3.1.2Applicability. Applicability in log parsing gauges a templatesadaptability across varied log entries, primarily based on the struc-ture of its placeholders. The more structurally generic they are, thebroader their reach, translating to higher applicability.High Applicability (Low Granularity): Templates here havea wide-reaching, generic structure, suitable for numerous logs.Low Applicability (High Granularity): These are designedfor specific log subsets, with unique structural placeholders.Both specificity and applicability play crucial roles in determin-ing the outcome of log templates, subsequently affecting metricsthat measure grouping and parsing accuracy of log parsing. To-gether, they delineate the granularity of log parsing. The idealgranularity often finds a midpoint between these two dimensionsand is shaped by user preferences and the nuances of individualuse cases. Notably, even a minor discrepancy in granularity canresult in substantially different groupings, a difference that canbe exaggerated when using inappropriate metrics. This highlightsthe pressing need for a well-conceived metric. It is essential to rec-ognize the inherently subjective nature of granularity. As such, itis inappropriate to strictly label a particular granularity as domi-nant or to view benchmark dataset labels as definitive standards. illustrates the parsing outcomes at varying granularities,using three representative log messages from the Windows datasetin loghub-2k. Applicability is shown through session names, whilespecificity is shown through client names. High applicability andlow specificity result in a generic structure that matches more logentries, indicating lower granularity. The benchmark uses granu-larity I) as its labeled ground truth.",
  ": An example of log parsing": "from individual log messages. Concurrently, the prefix tree providesefficient log clustering grounded in syntax. On the one hand, theenhanced accuracy of the LLM template extractor ensures thatthe prefix tree is meticulously constructed and updated. On theother hand, the prefix tree aids in trimming the computationaloverhead of LLM by eliminating repetitive LLM calls. We havealso integrated an automatic merging mechanism to rectify anytemplate imperfections stemming from LLMs. These elements cometogether harmoniously to form a synergized parsing framework.Moreover, we exploit the in-context learning (ICL) capabilities ofLLMs and implement named entity recognition (NER) promptingto further boost the accuracy of our LLM template extractor.During the evaluation of our method, we encountered an intrigu-ing observation. While our method generally produces satisfactoryresults, it doesnt always align with the annotated labels from thebenchmark. This discrepancy can be attributed to what we termas the Granularity of Log Parsing. Both the annotated labels andthe models outputs are logical in their own right. However, thesedifferences in granularity can significantly impact existing metrics,as log entries parsed at differing granular levels are deemed incor-rect. To more accurately quantify and understand this granularitydiscrepancy between parsing results, we introduce the metrics ofGranularity Distance. We further integrate human interactions inour method to allow users to calibrate the granularity based ontheir specific needs.We comprehensively evaluated our approach on the loghub-2k and the extensive logPub datasets provided by theLogPAI team. Remarkably, LogParser-LLM surpasses previous state-of-the-art parsers, achieving a 48.3% and 32.0% increase in the F1score for grouping and template accuracy, all without the need fordomain-specific human effort. Notably, after calibrating granularitywith ICL using a mere 32-shot labeled data for each domain, theperformance enhancement reaches up to 56.8% and 69.7%. Giventhat each of the 14 datasets averaged 3.6 million logs, LLMs wereonly queried an average of 272.5 times, minimizing overhead andshowcasing the feasibility of our methods real-world application.The key contributions of this paper are summarized as follows:(1) Introduction of LogParser-LLM , a novel method leveragingLLMs for log parsing that merges syntactic and semantic insights,featuring an LLM template extractor and prefix tree to reduce LLMcalls while processing millions of log lines efficiently.",
  "LogParser-LLM: Advancing Efficient Log Parsing with Large Language ModelsConference17, July 2017, Washington, DC, USA": "Additionally, granularity distance satisfies the properties of non-negativity and identity of indiscernibles, akin to the traditionalmetrics in distance measurement. This ensures a consistent andlogical comparison of log parsing granularity between differentparsing results.It is straightforward to compute GD when logs are accuratelytokenized, and each token is categorized as either a parameter ora template. However, such precise labeling and tokenization areoften absent. To circumvent this, an approximate version of GGDcan be derived by merely tallying the merge and split operationsrequired to transition from one grouping to another.",
  "Related Work and Motivation": "Log parsing, extensively explored in research , identifiesstatic templates and dynamic parameters within log entries. Asshown in , the template \"Successfully connected to <*>for <*>\" includes dynamic elements like \"/10.190.173.170:50010\"and blk_1073742826_2022\" as parameters. We categorize log pars-ing techniques into syntax-based, semantic-based, interactive, andLLM-based methods. We assess their pros and cons and identifyopportunities for innovation, particularly in leveraging Large Lan-guage Models to improve log parsing capabilities.",
  "Syntax-based Log Parsers": "Syntax-based parsers detect templates by identifying repeating pat-terns as static and others as parameters. Frequency-based parserslike SLCT , LFA , LogCluster , and Logram , buildon token recurrence. Similarity-based parsers, including LKE ,LogSig , LogMine , SHISO , and LenMa cluster logsby similarity. Heuristics-based parsers such as AEL , IPLoM ,Drain , Spell , Brain , and MoLFI , apply specificstrategies including the longest common subsequence-based ap-proach, iterative partitioning, prefix trees, and evolutionary algo-rithms for template extraction. These methods are fast and cost-efficient but may miss semantic details and require domain-specifictuning.",
  "Semantic-based Log Parsers": "Semantic parsers have evolved with neural networks like bidirec-tional LSTM, as seen in Semparser and Uniparser , andpre-trained language models such as LogPPT . VALB fur-ther enhances the models semantic understanding by classifyingspecific parameter categories. These models require labeled datafor training and classify tokens into templates or parameters. Theyoffer semantic understanding and can generalize across log types,but also demand resource-intensive training and periodic updates,presenting significant operational challenges.",
  "LLMs-based Log Parsing": "Large Language Models (LLMs) have emerged as transformativetools in numerous domains, demonstrating their prowess and ver-satility. Their pre-training on vast datasets, which include diversecontent such as code and log data, makes them particularly adeptfor specialized tasks like log parsing. Studies like have begun to tap into this potential, primarily focusing on promptengineering to improve template extraction efficiency. While theseadvancements highlight the promise of LLMs in log parsing, theypredominantly utilize a line-by-line parsing approach. Thismethod, although innovative, leads to high computational demandsdue to LLMs extensive parameter spaces, making these approachesimpractical for real-world applications due to the significantcomputational overhead.The benefits of LLMs extend beyond their raw computationalability, offering deep semantic understanding and the capacityto generalize across different log formats, adapting seamlesslyto new data types. This adaptability is crucial, as it reduces the needfor extensive preprocessing, hyper-parameter tuning, and manuallabeling, streamlining the deployment process.Despite these advantages, the practical deployment of LLMs inlog parsing is hindered by their high operational costs. Effec-tive utilization requires careful prompt tuning, a process thatcan be as resource-intensive as the computational demands of themodels themselves. This challenge underscores the need for moreefficient approaches that can leverage the strengths of LLMs with-out incurring prohibitive costs, ensuring their viability for broader,real-world application.",
  "Granularity of Log Parsing": "In this section, we delve into the granularity of log parsing. Startingwith .1, we characterize its two primary facets: Specificityand Applicability, elucidating them through an illustrative exam-ple. In .2, we first highlight the shortcomings of existingmetrics, emphasizing their inability to capture granularity nuances.Concluding the section, we introduce the granularity distance, anovel metric adept at gauging granularity discrepancies on twodistinct levels, effectively addressing the gaps in prior metrics.",
  "The granularity of log parsing is pivotal for how the parsing re-sult looks like. We primarily characterized the granularity by twodimensions: specificity and applicability": "3.1.1Specificity. Specificity in log parsing indicates the depth ofdetail within a template. It is primarily driven by the informationand content of templates. The more detailed they are, the higherthe specificity.High Specificity (High Granularity): Such templates havefewer, more detailed variable parts, aligning with a narrower set oflogs due to their intricacy.Low Specificity (Low Granularity): These are more general,with numerous variable components, catering to a broader logrange.The desired level of specificity often varies based on the loganalysis context and user needs.",
  "Measuring Granularity Discrepancy": "Existing evaluation metrics, while versatile, emphasize either theaccuracy of grouping logs or the fidelity in extracting templatesand parameters. Both dimensions are indispensable, especially con-sidering their implications for downstream tasks like log anomaly detection. However, these metrics often overlook the subtle gran-ularity differences inherent in log parsing. Existing benchmarkdatasets are anchored to the annotators subjective inter-pretations, suggesting that multiple valid granular interpretationscan exist for a single log. Such diversity challenges the conven-tional wisdom of treating annotated labels as an unequivocal goldstandard. Instead of a myopic focus on exact matches, a more encom-passing metric that can quantify and understand this granularitydiscrepancy is imperative. 3.2.1Existing metrics. We examine four prevalent metrics in thissection. The widely recognized message-level metrics, GroupingAccuracy (GA) and Parsing Accuracy (PA) , focus on thevolume of messages associated with each template, often prioritiz-ing templates with a larger number of log messages. To addressthis bias, template-level metrics like F1-score of Group Accuracy(FGA) and F1-score of Template Accuracy (FTA) have beenintroduced, ensuring an equitable evaluation of each template. Thedetailed definitions can be found in Appendix B.GA and PA primarily evaluate based on the volume of log mes-sages, making them susceptible to biases from imbalanced templates.In real-world scenarios, less frequent templates, such as error mes-sages, might be of paramount importance. Their misinterpretationcould be detrimental, yet this might not be reflected effectively usingthese metrics. Template-level metrics ensures a holistic evaluationof log parsers, giving equal importance to each template. However,while these metrics minimize biases from frequent templates, theystill present challenges. If a token is interpreted differently basedon granularity nuances, whether designated as a static part or aparameter, it might result in considerable variances in templatecounts. Additionally, such metrics dont provide a clear insight intogranularity differences. 3.2.2Granularity Distance (GD). In light of the discussions aboveand the sensitivity of existing metrics to subtle granularity discrep-ancies, we introduce the Granularity Distance metric. Inspired bythe traditional edit distance, this metric calculates the minimumoperations necessary to transform one parsing result into another.It serves as a quantitative reflection of the least human interven-tion needed to attain the desired granularity. This metric can bedissected into two main components:Grouping Granularity Distance(GGD): This aspect empha-sizes the grouping of log messages. The aim is to match the expectedgrouping of log messages without mandating identical templateswithin those groups.Parsing Granularity Distance(PGD): This is a more rigorousmetric requiring an exact match for each log template. Disparitiesin the parsed templates increment the distance.For the operations contributing to this distance:Operations on GGD: 1) Merge: Combine groups by changingone static section to variable. 2) Split: Separate groups by switchingone variable to static section.Operations on PGD: 1) Static to Variable: Convert a static sec-tion of the template to a variable. 2) Variable to Static: Revert avariable within the template to a static section.Similar to the edit distance, granularity distance possesses sym-metrical properties, meaning the distance from one log templateto another is the same as the distance from the second to the first.",
  "Methodology": "In this section, we introduce LogParser-LLM tailored to tacklethe challenges previously highlighted. Our approach is built uponfour key pillars: 1) Enhanced Template Extraction: Leveragingthe prowess of LLMs, we aim to boost the accuracy of templateextraction. 2) Efficient LLM Use: We design an algorithm that har-nesses the advanced capabilities of LLMs while optimizing resourceconsumption. 3) Reduced Human Effort with Broad Applica-bility: Our method minimizes human intervention, especially inlabel annotation and hyper-parameter tuning, yet remains versatileacross various domains and log formats. 4) Interactive FeedbackIntegration: Our method is integrated with human feedback forparsing granularity calibration. The following sections delve deeperinto these principles, elucidating the techniques and decisions un-derpinning our approach.",
  "Preprocessing": "Our method hinges on minimal preprocessing, using only a basicregular expression to extract log content. While many approachesdemand greater domain knowledge, often employing regular ex-pressions to substitute common variables like IP addresses andblock IDs , we retain the original message, ensuring the LLMgrasps the logs full context. Unlike other strategies that use distinctseparators for log tokenization , we consistently tokenizeusing spaces. Hence, unless otherwise specified, tokens in follow-ing sections are space-separated, capitalizing on the LLMs nativetokenizer. This streamlined preprocessing minimizes the need forspecialized expertise, yet upholds strong log parsing efficacy.",
  "Base Algorithm with Prefix Parse Tree": "Central to our methodology is a base algorithm employing a prefixparse tree, inspired by the efficiency demonstrated in Drain .This section elaborates on the data structures integral to the al-gorithm, detailing their design and their roles in addressing theaforementioned principles. Specifically, well elucidate how incom-ing logs are matched with existing clusters during tree traversal,how and when LLMs are invoked for template extraction, and thedynamics of updating the tree with new templates obtained fromthe LLM extractor. 4.2.1Data Structures. Three primary data structures form thebackbone of our approach: a set of log clusters, a template pool,and a prefix parse tree. offers a visual representation ofthis organizational structure. The subsequent discussion delineatestheir respective functionalities: Log Cluster: A log cluster is a collection of logs with the sametemplate. It keeps track of the individual log IDs and stores a logembedding, created by an LLM encoder, for future use. Each clusteris characterized by its log template, extracted via LLM, and possiblymultiple syntax templates aiding the prefix tree in its traversal andtemplate matching processes. While syntax templates corresponddirectly with the tokens of the raw logs, identifying static andvariable parts, the log templates from the LLM may represent severaltokens with a single placeholder. These syntax templates are storedin a dictionary, utilizing token counts as keys and correspondingtemplate lists as values.Template Pool: The template pool establishes a linkage, map-ping log templates to their respective log clusters.Prefix Parse Tree: In this tree structure, every nodebar therootsymbolizes a token. The wildcard token \"<*>\" serves as auniversal matcher for any token. Crucially, not just the leaf nodes,but virtually all nodes (excluding the root) can possess pointersto log clusters matching the token sequence extending from theroot. A unique feature to note is that a single log cluster might beaccessible from multiple nodes, courtesy of the potential existenceof various syntax template variants for a given log cluster. 4.2.2cluster matching with tree search. Upon receiving a new log,our first step is tokenization. Tokens are then processed sequentially,with each token checked against nodes in the prefix tree. Aftermatching the initial token, we proceed to the subsequent token,considering only the children of the previously matched node. Thisprogression continues either until all tokens are matched or whenno further matching tokens exist. Throughout this traversal, logclusters referenced by the encountered nodes are shortlisted aspotential candidates for a thorough match evaluation.At this juncture, we have pinpointed a subset of log clustersconsistent with the rules encoded in the tree path. Our task now isto determine the genuine match from these candidates. Contraryto existing methodologies such as those in , which deploysimilarity metrics and predefined, dataset-specific thresholds, ourapproach crystallizes outcomes into three distinct categories: i)Strict match, ii) Loose match, and iii) No match. For each prospec-tive cluster, an initial check compares the token count between theincoming log and the clusters syntax templates. Discrepant tokencounts immediately exclude the possibility of a match. Followingthis, a loose match is attempted, aligning tokens from the syntaxtemplate and the log. Here, any token within the syntax templatecontaining the \"<*>\" wildcard can align with any log token. Toillustrate, a token such as \"prefetching...<*>\" can loosely align withany log entry with a singular token. After achieving a loose match,regular expressions ensure a rigorous alignment with elementsoutside the \"<*>\" in the syntax template. A complete token align-ment signifies a strict match. It is worth noting that the matchingprocess stops upon achieving a strict match. In scenarios where astrict match is identified, the log is straightforwardly added to thematched cluster. Conversely, in the absence of a strict match, theLLM template extractor is invoked for template extraction, followedby the necessary updates to the data structures.Our methods precision, rooted in the capabilities of LLMs, elim-inates the need for meticulous hyperparameter tuning across log",
  "(c)": ": An example demonstrating the data structures in our method: (a) A prefix parse tree with nodes linking to log clusters,(b) a Template Pool mapping log templates to log clusters, and (c) Log Clusters containing collections of logs with the same logtemplate. sources. Leveraging LLMs proficiency, which typically yields se-mantically accurate templates for individual logs, facilitates thisstringent matching paradigm. Not only does it simplify the tuningprocess, but it also optimizes the number of calls to the LLM. Ideally,if we operate under the assumption that LLMs generate templatesmirroring the ground truth, the volume of LLM calls is effectivelycapped at the total number of distinct syntax templates. This countis in the ballpark of the total number of log templates, offering ascalable approach. 4.2.3Parse tree update. The comprehensive update rule is elu-cidated in Algorithm 1. As outlined in lines 8-9, for scenarios ofeither a loose match or no match, the LLM is invoked to derive a logtemplate. If this extracted template already resides in the templatepool, it suggests that the current log pertains to an existing clusterbut with an alternative syntax template variant. In such cases, theassociated cluster can be swiftly identified via the template poolmapping. It then becomes essential to integrate the novel syntaxtemplate into the cluster and adjust the tree to accommodate nodesthat align with this new syntax template.Conversely, if the template isnt found in the template pool yet aloose match has been identified, the LLM is once more consulted. Itstask here is to determine if the loosely matched cluster can integratethis new log. A positive outcome leads to the generation of a mergedtemplate. Subsequently, both the syntax and log templates of thecluster undergo an update, with the merged template being addedto the template pool.If a log, after undergoing the entire aforementioned process, stillhas not been allocated to an existing cluster, it is indicative of aunique log template. Such instances mandate the creation of a newcluster, with corresponding updates made to the tree.",
  "Enhancing LLM Template Extraction": "While the base algorithm already paves the way for efficient andprecise log cluster matching, there remains room to refine the ac-curacy of the LLM template extractor. To this end, we introducevariable-aware prompting, amalgamating it with in-context learn-ing. This fusion not only amplifies the LLMs task comprehensionbut also augments its overall performance.",
  "Alternatively, supervised fine-tuning of an LLM using labeled datapresents another viable strategy": "4.3.1Variable-Aware Prompting. Past research has highlightedthe benefits of identifying and classifying specific variables withinlogs. By categorizing these variables, not only is the accuracy oftemplate extraction enhanced, but it also proves advantageous forsubsequent tasks. Drawing inspiration from this research and theconcept of chain-of-thought prompting , we restructure ourprompts. These prompts now serve dual purposes: they identifyvariables and categorize them into one of the ten classificationsas outlined in . This refined approach prompts the model tounderstand and determine which components should be classifiedas variables and the reasoning behind such categorization. 4.3.2In-Context Learning with K-Shot Demonstrations. In-contextlearning (ICL) has become a favored approach when using LLMsfor downstream tasks without the need for finetuning . Typ-ically, ICL-based prompts contain three elements: Instruction: Atask-specific description. Demonstrations: A set of examples, es-sentially pairs of queries coupled with their ground truth answers.Query: The direct question to which the LLM provides a response.Each time the LLM is called upon for template extraction, we drawa sample of = 3 examples from our existing pool of log templatepairs. Incorporating the principles of Variable-Aware Prompting,we include ten examples, each representing a distinct type of log pa-rameter, as seed examples. Subsequent template extraction resultsexpand this pool. To obtain these samples, we calculate cosine sim-ilarity between LLM embedding of query log and all embeddingspresent in the example pool. The top-k samples are then chosen ask-shot demonstrations within the prompt.",
  "Optimal Granularity via Human-in-Loop": "Integrating human expertise into the automated log parsing pro-cess is key to achieving the right granularity. Human input can beseamlessly incorporated at various stages of the parsing pipeline toenhance accuracy and maintain consistency:1) Pre-Processing Intervention: Experts annotate a sample oflogs before parsing begins. These annotations serve dual purposes:they can be used as seed examples for In-Context Learning (ICL) orto fine-tune LLMs, ensuring the models output aligns more closelywith specific parsing needs.2) Real-Time Calibration: During the parsing process, humanjudgment can be applied to guide decisions on template merging,ensuring the parsing maintains the desired level of granularitythroughout.3) Post-Processing Refinement: After parsing, the systemidentifies potential merges or splits based on semantic similarityor template variability. Experts review these suggestions, makingadjustments to achieve the optimal granularity.In , we demonstrate how LogParser-LLM-C incorporatespre-processing intervention, enhancing the base LogParser-LLMscapabilities. For real-time calibration, human expertise can be usedto refine the merging process in line with desired granularity levels,as outlined in line 15 of Algorithm 1. Post-processing refinementcan integrate methods like those suggested in for effective final",
  "Experiments": "We assess the effectiveness of our method using two datasets:loghub-2k and logPub . First, we detail the experimentalsettings. Subsequently, we outline the evaluation metrics employed,highlighting a novel metric we introduce to gauge the granularitydistance of parsing outcomes. In examining results from the loghub-2k dataset, our primary objective is to elucidate the contribution ofeach design component of our method. With the logPub benchmark,our intent is to demonstrate both the effectiveness and efficiencyof our approach when handling large-scale datasets in practice.",
  "Experimental Settings": "5.1.1Datasets. Loghub-2k is a widely recognized benchmark inthe field of log parsing. It encompasses logs from 16 diverse systems,including distributed systems, supercomputers, operating systems,mobile platforms, server applications, and individual software pack-ages. For every system source, 2,000 log messages are meticulouslyannotated. Complementing this, LogPub is a more recent, expansiveiteration of Loghub-2k. It features 14 systems, with each averaginga substantial 3.6 million log lines, and showcases a pronouncedincrease in the number of log templates. This dataset offers a real-istic, large-scale environment, paving the way for comprehensiveevaluations of log parsing methodologies. 5.1.2Implementation Details. Our experimental setup involves aserver powered by Ubuntu 20.04.3 LTS with 512GB of RAM. Weuse both ChatGPT (version gpt-3.5-turbo-0301) and GPT-4 (versiongpt-4-0613) for template extraction. For embedding the logs, thetext-embedding-ada-002 method is adopted. All interactions withthese models are facilitated through the official OpenAI API. Toguarantee consistency in our findings and support reproducibility,we maintain the temperature parameter at 0 to minimize variability.For fine-tuning our LLM, the Llama-2-13b model serves asthe foundation. Comprehensive details regarding this fine-tuningprocess can be found in Appendix B.2. For in-context learning, weuniformly sample 32 log-template pairs from the first 10% of eachdataset based on token length as candidate logs. The same samplesare employed for fine-tuning.",
  "GAPA FGA FTA GGD PGDGAPA FGA FTA GGD PGD GAPAFGA FTA GGD PGDGAPA FGA FTA GGD PGDGAPAFGA FTA GGD PGD": "Proxifier69.2 68.8 20.6 17.641450.9 63.4 28.6 45.751098.9 100.0 87.0 95.71151.0 63.4 40.0 53.35998.9 100.0 87.0 95.711Linux68.6 11.1 77.8 25.93043228.5 16.4 45.1 23.2 108274 20.5 16.8 71.2 42.82910427.0 16.3 80.1 46.6188153.449.491.1 74.01068Apache100.0 72.7 100.0 51.702194.8 94.2 68.7 26.9113178.6 94.8 60.5 36.8623100.0 85.7 100.0 65.508100.0 99.5 100.0 82.805Zookeeper99.4 84.3 90.4 61.423098.8 98.8 66.1 51.0143196.7 84.5 91.8 80.941098.8 81.9 86.2 72.421999.596.892.9 85.7113Hadoop92.1 54.1 78.5 38.41821069.1 88.9 62.8 47.638119 48.3 66.6 52.6 43.4468193.8 67.6 87.3 55.01410894.590.6 88.9 81.01141HealthApp86.2 31.2 1.00.41113846.1 81.7 74.5 46.2166099.8 99.7 94.7 82.24899.8 58.2 95.6 81.845100.0 98.296.5 89.037OpenStack75.22.90.70.2 6618 23** 100.0 51.6 96.9 28.91753.4 40.6 87.4 73.844100.0 49.6 100.0 79.2011100.0 100.0 100.0 97.901HPC79.3 72.1 30.9 15.21017877.7 94.1 66.0 35.1105878.2 99.7 78.0 76.8123186.4 94.2 76.0 72.6618086.499.8 76.8 74.6629Mac76.1 35.7 22.96.9102 1347 73.7 68.8 69.9 28.373624 54.4 39.0 49.3 27.4 17748989.7 30.3 84.7 36.24244491.576.4 86.4 60.633297OpenSSH70.7 58.6 87.2 48.733327.5 28.9 0.90.5152627.7 65.48.1 10.5172678.0 69.0 96.1 88.31978.0 100.0 96.1 98.712Spark88.8 39.4 86.1 41.21823985.4 79.5 2.01.262186 47.6 95.2 37.4 29.97522197.6 80.2 85.2 46.31614897.699.7 88.2 68.111101Thunderbird 83.1 21.6 23.77.1137 2043 57.9 65.4 68.2 29.0 194976 56.4 40.1 21.6 11.7 282 10127357.1 80.0 56.0 10466267.564.3 83.1 59.388615BGL91.9 40.7 62.4 19.34843491.8 94.9 62.4 21.943209 24.5 93.8 25.3 26.16916493.8 81.0 78.9 50.03415488.997.684.0 71.62485HDFS99.9 62.1 93.5 60.926100.0 94.8 96.8 58.11172.1 94.3 39.1 31.21859100.0 94.8 74.7 57.8526100.0 100.0 96.8 96.811",
  "LogParser-LLM091.869.967.866.8492.174.772.674.93294.890.584.985.320095.998.096.896.9": "Comparison with Existing LLM-based Parsers Existing LLM-based parsers, which process logs line-by-line, are impractical forevaluation on the expansive LogPub dataset due to the immensenumber of LLM calls required. We therefore use the Loghub-2kdataset for comparison, but advise caution in interpreting theseresults because of the datasets limited scope and the possibilitythat a few well-chosen labeled samples might cover the majorityof templates. In s results, our method either matches orexceeds the performance of existing approaches with an equivalentnumber of labeled logs, highlighting our methods effective use ofLLMs for log parsing despite the constraints.Accuracy Evaluation We compare our model to three state-of-the-art methods: two syntax-based methods, Drain andBrain , and one semantic-based method, LogPPT . As in-dicated in , the previous state-of-the-art methods achieved higher GA and PA values because they were meticulously tunedwith hyperparameters on each dataset to optimize these metrics.However, these values alone do not necessarily indicate superiorperformance. When evaluating with template-level metrics such asFGA and FTA, as well as our proposed GGD, our model outperformsthem without the need for any domain-specific configuration.Ablation The results of our ablation study for different compo-nents, including in-context learning (ICL), variable-aware prompt(VA), and automatic template merge (Merge), are presented in Ta-ble 4. The numbers clearly demonstrate that each proposed compo-nent positively impacts the methods performance, as evidencedby the reduction in GGD. Notably, the most significant perfor-mance boost comes from the transition from GPT-3.5 to GPT-4.Furthermore, the enhancements from other components are evenmore pronounced with GPT-4, underscoring the potency of morepowerful LLMs. Using GPT-4 on its own, even without ICL, yieldsimpressive results, showcasing its capacity to adhere to specific in-structions and complete tasks in a zero-shot scenario. However, it isimportant to note that integrating these components also increasesthe associated costs when invoking the LLM.",
  "Evaluation on LogPub": "Accuracy and Generalizability Results from the expansive log-Pub dataset are shown in . We use LogParser-LLM-C todenote calibrated variants of our method. It is clear that our model,LogParser-LLM , even without granularity calibration, significantlysurpasses all baseline methods in GA, FGA, and PTA, marking im-provements of 7.8%, 48.3%, and 32.0% compared to the best baselineresults. However, PA performance lags, mostly due to granularitynuances complicating the LLMs ability to generate templates thatperfectly match annotated labels. A standout point is the consistentperformance of our method across the 14 datasets, achieved withoutdomain-specific tweaks, maintaining uniform settings throughout.Upon introducing domain-specific granularity calibration with ICLin LogParser-LLM-C, there is a noticeable boost, especially in tem-plate parsing metrics such as PA and FTA. This highlights thereduced discrepancy in the applicability of log parsing achievedthrough ICL.Granularity Discrepancy Evaluation Both Grouping Granu-larity Distance (GGD) and Parsing Granularity Distance (PGD) arecalculated and shown in . PGD is computed using spaces asdelimiters for tokenization, representing a lower bound since pre-cise tokenization isnt feasible for such large datasets. This approxi-mation remains valuable for consistent cross-method comparisons.Unlike message-level GA and PA metrics, which depend on logvolume, the proposed metrics avoid template imbalance and providea clearer performance indicator. For example, in the Linux dataset,Drains GA is 68.6 compared to our 53.4. However, Drains GGDis 30 versus our 10, indicating significantly more effort needed toalign Drains results with the ground truth.Compared to template-level metrics, GGD and PGD show thatsmaller GD correlates with higher FGA and FTA. However, FGAand FTA can overly penalize repetitive differences. For example,if a ground truth template \"instance: <*>\" has many instance IDsnot correctly identified as variables, it increases the number ofidentified templates, skewing precision calculations. GGD and PGDcount such differences only once, offering a fairer measurement.For instance, GA and PA for Uniparser on OpenSSH are 0.9 and0.5, respectivelyvalues that indicate a significant gap compared toother methods and are not informative. Conversely, GGD and PGDfor Uniparser on OpenSSH are 15 and 26, respectively, providingan informative and intuitive comparison. This robustness is alsoobserved in HealthApp, OpenStack, and Thunderbird datasets.Evaluation with Different LLMs By design, our framework isversatile enough to be compatible with any language model that canprocess individual log messages and accordingly generate log tem-plates. This evaluations primary objective is to assess the impactof different LLMs on the efficacy and efficiency of our approach.Our results, as presented in , demonstrate that usingGPT-4 as the LLM template extractor paired with ICL yields op-timal performance. However, this comes at the cost of increased **The reason Drain has a GGD of 6618 but a PGD of 23 is that its preprocessing convertsall numbers to the variable \"<*>\". Complicated instance IDs such as \"3edec1e4-9678-4a3a\" are preprocessed to\"<*>edec<*>e<*>-<*>-<*>a<*>a\". This results in a significantnumber of redundant log clusters, leading to a high GGD. However, when calculatingthe PGD, this is considered a single variable token which is correctly parsed and thusdoes not contribute to the PGD. computational time due to GPT-4s extensive parameter count. No-tably, both granularity calibration and ICL enhance performanceand concurrently decrease the number of required LLM calls. Thisis congruent with our frameworks foundational assumption thatLLMs can generate nearly perfect log templates. For the fine-tunedLLM, we commenced with the widely-recognized open-source LLM,Llama-2-13b. Despite its performance not being optimal, it remainscompetitive, closely paralleling the results of prior state-of-the-artsemantic-based models like Uniparser and LogPPT. This subparperformance may be attributed to our not having meticulouslycurated the fine-tuning dataset and its limited size. While a morethoughtfully curated, expansive training dataset and hyperparame-ter tuning could enhance its performance, this conflicts with ourintent: to construct a robust log parser that necessitates minimalhuman intervention and domain-specific knowledge.For runtime efficiency, we delineate runtime into two facets:the time required for LLM calls and the time for our log parsersother operations. This distinction is crucial, given that OpenAIservice calls depend on service availability and are subject to ratelimitations, which complicates consistent performance evaluation.To estimate cumulative processing time, we multiply the averageresponse time (in optimal scenarios) by the total number of LLMcalls. Our base algorithms average runtimes stand at 461.67s and433.22s. If we disregard potential rate caps, the mean response timesare 0.52s for GPT-3.5 and 4.18s for GPT-4. With an average of 272.5LLM calls, this equates to overall runtimes of 612.31s for GPT-3.5and 1572.27s for GPT-4. For comparison, Drain, one of the fastest ex-isting methods, averages 483.2s to process about 3.6 million logs. Incontrast, a conventional line-by-line LLM parsing approach wouldnecessitate 3.6 million LLM calls, leading to an impractical runtimeof approximately 22 days for GPT-3.5, not accounting for other oper-ational overheads. This analysis not only emphasizes our methodscompetitive efficiency but also its practicality, overcoming the in-herent impracticality of existing LLM-based parsers by significantlyreducing the number of necessary LLM calls.",
  "Conclusion": "In this study, we introduce LogParser-LLM, a novel approach to logparsing that seamlessly integrates the strengths of Large LanguageModels (LLMs). Centralizing around a prefix tree and an LLM-basedtemplate extractor, LogParser-LLM not only streamlines the extrac-tion of semantically rich log templates but also ensures efficiencythrough strategic LLM call reductions. While demonstrating com-pelling results, we also uncovered nuances in parsing granularity,prompting the creation of the Granularity Distance metric. Ourrigorous tests on benchmark datasets reveal that LogParser-LLMsignificantly outshines existing parsers in accuracy and efficiency,demonstrating its potential as a valuable tool for both researchersand practitioners in the field of log analysis.",
  "Min Du and Feifei Li. 2016. Spell: Streaming parsing of system event logs. In 2016IEEE 16th International Conference on Data Mining (ICDM). IEEE, 859864": "Min Du, Feifei Li, Guineng Zheng, and Vivek Srikumar. 2017. Deeplog: Anomalydetection and diagnosis from system logs through deep learning. In Proceedingsof the 2017 ACM SIGSAC conference on computer and communications security.12851298. Qiang Fu, Jian-Guang Lou, Yi Wang, and Jiang Li. 2009. Execution anomalydetection in distributed systems through unstructured log analysis. In 2009 ninthIEEE international conference on data mining. IEEE, 149158. Qiang Fu, Jian-Guang Lou, Yi Wang, and Jiang Li. 2009. Execution anomalydetection in distributed systems through unstructured log analysis. In 2009 ninthIEEE international conference on data mining. IEEE, 149158. Ying Fu, Meng Yan, Jian Xu, Jianguo Li, Zhongxin Liu, Xiaohong Zhang, and DanYang. 2022. Investigating and improving log parsing in practice. In Proceedingsof the 30th ACM Joint European Software Engineering Conference and Symposiumon the Foundations of Software Engineering. 15661577.",
  "Justine Gangneux. 2019. Rethinking social media for qualitative research: Theuse of Facebook Activity Logs and Search History in interview settings. TheSociological Review 67, 6 (2019), 12491264": "Nentawe Gurumdimma, Arshad Jhumka, Maria Liakata, Edward Chuah, andJames Browne. 2015. Towards detecting patterns in failure logs of large-scaledistributed systems. In 2015 IEEE International Parallel and Distributed ProcessingSymposium Workshop. IEEE, 10521061. Hossein Hamooni, Biplob Debnath, Jianwu Xu, Hui Zhang, Guofei Jiang, andAbdullah Mueen. 2016. Logmine: Fast pattern recognition for log analytics.In Proceedings of the 25th ACM International on Conference on Information andKnowledge Management. 15731582.",
  "Pinjia He, Jieming Zhu, Zibin Zheng, and Michael R Lyu. 2017. Drain: An onlinelog parsing approach with fixed depth tree. In 2017 IEEE international conferenceon web services (ICWS). IEEE, 3340": "Shilin He, Qingwei Lin, Jian-Guang Lou, Hongyu Zhang, Michael R Lyu, andDongmei Zhang. 2018. Identifying impactful service system problems via loganalysis. In Proceedings of the 2018 26th ACM joint meeting on European softwareengineering conference and symposium on the foundations of software engineering.6070. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, SheanWang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of largelanguage models. arXiv preprint arXiv:2106.09685 (2021).",
  "Yintong Huo, Yuxin Su, Cheryl Lee, and Michael R Lyu. 2023. SemParser: ASemantic Parser for Log Analytics. In 2023 IEEE/ACM 45th International Conferenceon Software Engineering (ICSE). IEEE, 881893": "Zhihan Jiang, Jinyang Liu, Junjie Huang, Yichen Li, Yintong Huo, Jiazhen Gu,Zhuangbin Chen, Jieming Zhu, and Michael R Lyu. 2023. A Large-scale Bench-mark for Log Parsing. arXiv preprint arXiv:2308.10828 (2023). Zhen Ming Jiang, Ahmed E Hassan, Parminder Flora, and Gilbert Hamann. 2008.Abstracting execution logs to execution events for enterprise applications (shortpaper). In 2008 The Eighth International Conference on Quality Software. IEEE,181186. Zanis Ali Khan, Donghwan Shin, Domenico Bianculli, and Lionel Briand. 2022.Guidelines for assessing the accuracy of log message template identification tech-niques. In Proceedings of the 44th International Conference on Software Engineering.10951106.",
  "Van-Hoang Le and Hongyu Zhang. 2023. Log Parsing with Prompt-based Few-shot Learning. arXiv preprint arXiv:2302.07435 (2023)": "Zhenhao Li, Chuan Luo, Tse-Hsun Chen, Weiyi Shang, Shilin He, Qingwei Lin,and Dongmei Zhang. 2023. Did We Miss Something Important? Studying and Ex-ploring Variable-Aware Log Abstraction. arXiv preprint arXiv:2304.11391 (2023). Qingwei Lin, Hongyu Zhang, Jian-Guang Lou, Yu Zhang, and Xuewei Chen. 2016.Log clustering based problem identification for online service systems. In Pro-ceedings of the 38th International Conference on Software Engineering Companion.102111. Yilun Liu, Shimin Tao, Weibin Meng, Jingyu Wang, Wenbing Ma, Yanqing Zhao,Yuhang Chen, Hao Yang, Yanfei Jiang, and Xun Chen. 2023. LogPrompt: PromptEngineering Towards Zero-Shot and Interpretable Log Analysis. arXiv preprintarXiv:2308.07610 (2023). Yudong Liu, Xu Zhang, Shilin He, Hongyu Zhang, Liqun Li, Yu Kang, Yong Xu,Minghua Ma, Qingwei Lin, Yingnong Dang, et al. 2022. Uniparser: A unified logparser for heterogeneous log data. In Proceedings of the ACM Web Conference2022. 18931901. Chuan Luo, Pu Zhao, Bo Qiao, Youjiang Wu, Hongyu Zhang, Wei Wu, WeihaiLu, Yingnong Dang, Saravanakumar Rajmohan, Qingwei Lin, et al. 2021. NTAM:Neighborhood-temporal attention model for disk failure prediction in cloudplatforms. In Proceedings of the Web Conference 2021. 11811191. Adetokunbo AO Makanju, A Nur Zincir-Heywood, and Evangelos E Milios. 2009.Clustering event logs using iterative partitioning. In Proceedings of the 15thACM SIGKDD international conference on Knowledge discovery and data mining.12551264. Salma Messaoudi, Annibale Panichella, Domenico Bianculli, Lionel Briand, andRaimondas Sasnauskas. 2018. A search-based approach for accurate identifica-tion of log message formats. In Proceedings of the 26th Conference on ProgramComprehension. 167177.",
  "Priyanka Mudgal and Rita Wouhaybi. 2023. An Assessment of ChatGPT on LogData. arXiv preprint arXiv:2309.07938 (2023)": "Meiyappan Nagappan and Mladen A Vouk. 2010. Abstracting log lines to logevent types for mining software system logs. In 2010 7th IEEE Working Conferenceon Mining Software Repositories (MSR 2010). IEEE, 114117. Antonio Pecchia, Marcello Cinque, Gabriella Carrozza, and Domenico Cotroneo.2015. Industry practices and event logging: Assessment of a critical softwaredevelopment process. In 2015 IEEE/ACM 37th IEEE International Conference onSoftware Engineering, Vol. 2. IEEE, 169178. Xiang Rao, Huaimin Wang, Dianxi Shi, Zhenbang Chen, Hua Cai, Qi Zhou, andTingtao Sun. 2011. Identifying faults in large-scale distributed systems by filteringnoisy error logs. In 2011 IEEE/IFIP 41st International Conference on DependableSystems and Networks Workshops (DSN-W). IEEE, 140145. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deep-speed: System optimizations enable training deep learning models with over100 billion parameters. In Proceedings of the 26th ACM SIGKDD InternationalConference on Knowledge Discovery & Data Mining. 35053506. Daan Schipper, Maurcio Aniche, and Arie van Deursen. 2019. Tracing backlog data to its log statement: from research to practice. In 2019 IEEE/ACM 16thInternational Conference on Mining Software Repositories (MSR). IEEE, 545549.",
  "Keiichi Shima. 2016. Length matters: Clustering system log messages usinglength of words. arXiv preprint arXiv:1611.03213 (2016)": "Liang Tang, Tao Li, and Chang-Shing Perng. 2011. LogSig: Generating sys-tem events from raw textual logs. In Proceedings of the 20th ACM internationalconference on Information and knowledge management. 785794. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-ale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXivpreprint arXiv:2307.09288 (2023). Risto Vaarandi. 2003. A data clustering algorithm for mining patterns from eventlogs. In Proceedings of the 3rd IEEE Workshop on IP Operations & Management(IPOM 2003)(IEEE Cat. No. 03EX764). Ieee, 119126. Risto Vaarandi and Mauno Pihelgas. 2015. Logcluster-a data clustering andpattern mining algorithm for event logs. In 2015 11th International conference onnetwork and service management (CNSM). IEEE, 17.",
  "Liming Wang, Hong Xie, Ye Li, Jian Tan, and John Lui. 2023. Interactive LogParsing via Light-weight User Feedbacks. arXiv preprint arXiv:2301.12225 (2023)": "Xuheng Wang, Xu Zhang, Liqun Li, Shilin He, Hongyu Zhang, Yudong Liu,Lingling Zheng, Yu Kang, Qingwei Lin, Yingnong Dang, et al. 2022. SPINE: ascalable log parser with feedback guidance. In Proceedings of the 30th ACM JointEuropean Software Engineering Conference and Symposium on the Foundations ofSoftware Engineering. 11981208. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoningin large language models. Advances in Neural Information Processing Systems 35(2022), 2482424837.",
  "Siyu Yu, Pinjia He, Ningjiang Chen, and Yifan Wu. 2023. Brain: Log Parsing withBidirectional Parallel Tree. IEEE Transactions on Services Computing (2023)": "Shenglin Zhang, Ying Liu, Weibin Meng, Zhiling Luo, Jiahao Bu, Sen Yang, PeixianLiang, Dan Pei, Jun Xu, Yuzhi Zhang, et al. 2018. Prefix: Switch failure predictionin datacenter networks. Proceedings of the ACM on Measurement and Analysis ofComputing Systems 2, 1 (2018), 129. Xu Zhang, Yong Xu, Qingwei Lin, Bo Qiao, Hongyu Zhang, Yingnong Dang,Chunyu Xie, Xinsheng Yang, Qian Cheng, Ze Li, et al. 2019. Robust log-basedanomaly detection on unstable log data. In Proceedings of the 2019 27th ACMJoint Meeting on European Software Engineering Conference and Symposium onthe Foundations of Software Engineering. 807817. Xu Zhang, Yong Xu, Si Qin, Shilin He, Bo Qiao, Ze Li, Hongyu Zhang, Xukun Li,Yingnong Dang, Qingwei Lin, et al. 2021. Onion: identifying incident-indicatinglogs for cloud systems. In Proceedings of the 29th ACM Joint Meeting on EuropeanSoftware Engineering Conference and Symposium on the Foundations of SoftwareEngineering. 12531263. Jieming Zhu, Shilin He, Jinyang Liu, Pinjia He, Qi Xie, Zibin Zheng, and Michael RLyu. 2019. Tools and benchmarks for automated log parsing. In 2019 IEEE/ACM41st International Conference on Software Engineering: Software Engineering inPractice (ICSE-SEIP). Los Alamitos, CA, USA (2019), 121130.",
  "AAdditional DiscussionA.1Challenges of Log Parsing in Practice": "The challenges associated with log parsing encompass several keyaspects.Huge Volume. Modern systems generate vast amounts of logdata, which are difficult to manage, store, and analyze. For instance,services like Amazon, Alibaba, and Facebook generate billionsof visits per day, each creating multiple log entries . Logparsing, along with tasks like anomaly detection and root causeanalysis, is crucial for minimizing system downtime and financialloss . The requirement for real-time, streaming log parsingmakes handling such vast volumes challenging.Constantly Evolving. Systems and technologies continuouslyevolve, leading to changes in log entry types, formats, structures,and content. New features and components introduce novel logformats, necessitating updates to log templates for accurate parsing.Without timely template updates, parsing algorithms may fail toextract relevant information, leading to inaccuracies and incom-plete analysis. Proactively updating log templates ensures effectiveparsing and adaptation to dynamic log generation.Diverse Sources. Logs from different systems often have diverseformats, posing a challenge for log parsing algorithms. Each sys-tems unique log format can vary significantly in structure, syntax,and content. Effective log parsing algorithms must generalize tohandle various formats without relying on system-specific rules orassumptions.",
  "A.2Insights and Opportunities of Log Parsingwith LLMs": "The relentless growth in log volumes, the ever-evolving nature oflogs, and the vast diversity in log sources have presented dauntingchallenges in the realm of log parsing. Syntax-based parsers, whileefficient, often grapple with the dynamic nuances introduced bylog evolution and diverse sources. LLMs, with their deep semanticunderstanding and adaptability, are poised as a promising solutionbut need prompt tuning and optimization to handle vast volumes.Moreover, the vital role of log data in modern systems under-scores the need for log parsing tools that embody certain foun-dational principles. In practice, a log parser must be Accurate, ensuring accurate interpretation of every piece of information. Effi-ciency is paramount to handling the voluminous log data churnedout by popular platforms and sprawling systems. The parsersEvolvability will be its asset, granting it the flexibility to keep pacewith system updates and new feature integrations. To confront themultifarious log formats from diverse sources, it is imperative thata parser is Generalizable, ensuring it doesnt rely too heavily onsystem-specific constructs.Furthermore, for real-time responsiveness, the parser needs tooperate in an Online manner. This demands the tools agility toadapt and recalibrate as new log entries stream in. Addressing thechallenge of different granularities in log parsing is also of utmostimportance. Ensuring the capability to Calibrate Granularityprovides flexibility in parsing logs, given that a single log can beinterpreted in multiple, yet reasonable, ways based on granularity.Given the unique strengths and challenges of each approach, acompelling motivation emerges: to amalgamate the adaptability anddepth of LLMs with the efficiency intrinsic to syntax-based parsers.This convergence promises a robust and versatile log parsing solu-tion, aptly suited to address both present and future challenges inlog management.",
  "BExisting metrics": "Grouping Accuracy (GA) GA measures the ratio of correctlygrouped log messages. A message is considered correctly groupedif and only if its template group is exactly aligned with groundtruth grouping.Parsing Accuracy (PA) PA assesses the ability to extract tem-plates accurately, critical for tasks like anomaly detection. it is thefraction of messages parsed correctly, meaning all template andvariable tokens are identified accurately.F1 score of Grouping Accuracy (FGA) FGA is a template-levelmetric that evaluates the fraction of correctly grouped templates.Using the true number of templates (), parsed templates (),and correctly parsed templates (), we calculate the Precision( =",
  ") and Recall ( =": ") of Grouping Accuracy. FGA istheir harmonic mean.F1 score of Template Accuracy (FTA) FTA is the harmonicmean of Recall of Template Accuracy (RTA) and Precisionof Template Accuracy (PTA). Like FGA, FTA evaluates correcttemplate identification at the template level. A template is correctif log messages with the same parsed template share the sameground-truth template and the parsed template matches the ground-truth template exactly. Using to denote the number of templates",
  "CAdditional Implementation Details": "Fine-tuning Settings The llmama-2-13b model was finetunedon a server equipped with 8 Tesla A100 80GB GPUs using theHugging Face Transformers package. The model was finetunedfor 50 epochs with 32 samples for each dataset. During inference,we utilized DeepSpeed with 8-bit quantization to expedite theinference process on a single Tesla A100 80GB GPU. Additionally,the model was fine-tuned using LoRA with rank set to 64. For",
  "optimization, we employed the AdamW optimizer with an initial": "learning rate of 2e-4, which was linearly scheduled down to 0. Thebatch size was 16.Prompts We demonstrate the final prompt used for the ICL-based method in and the fine-tuning-based method in. The prompts for automatic template merge check andverification are shown in and , respectively. \"\"\"Asalogparser,yourtaskistoanalyzelogsandidentifydynamicvariables.Thesevariablesare distinct from static parts, which are hardcodedsections in the logging code. The categories of dynamicvariables are concluded as: Object ID (OID): Includes variables like session IDsand user IDs.Location Indicator (LOI): Path information, URIs, andIP addresses.Object Name (OBN): Domain names, task names, job names.Type Indicator (TID): Category for type indicators.Switch Indicator (SID): Category for switch indicators(only numerical ones).Time/Duration of an Action (TDA): Timespan or durationof actions.Computing Resources (CRS): Memory, disk space, numberof bytes.Object Amount (OBA): Number of errors, nodes, etc.Status Code (STC): Error codes (only numerical ones).Other Parameters (OTP): All other types of variables. To parse the logs, substitute dynamic variables withtheir respective category tokens, denoted by <XXX>.Everythingoutsidethe<XXX>shouldremainexactlyunchanged! Do not fix any typo! If a variable comprisesseveral smaller, fine-grained variables, dont dissectit. Instead, replace the entire compound variable witha single <XXX> token. Do not substitute all content inthe log as a variable; only genuine dynamic variablesshould be replaced."
}