{
  "ABSTRACT": "Budget allocation of marketplace levers, such as incentives for dri-vers to complete certain trips or promotions for riders to take moretrips have long been both a technical and business challenge atUber. It is crucial to understand the impact of lever budget changeson the market and to estimate their cost efficiency given the needto achieve predefined budgets, where the eventual goal is to findthe optimal allocations under those constraints that maximize someobjective of value to the business. In this paper, we introduce anend-to-end machine learning and optimization procedure to auto-mate budget decision-making for cities where Uber operates. Thisprocedure relies on a suite of applications, including feature store,model training and serving, optimizers and backtesting to measurethe prediction and causal accuracy. We propose a state-of-the-artdeep learning (DL) estimator based on S-Learner that leveragesmassive amount of user experimental and temporal-spatial obser-vational data. We also built a novel tensor B-Spline regressionmodel to enforce efficiency shape control while retaining the so-phistication of the DL models response surface, and solved thehigh-dimensional optimization problem with Alternating Direc-tion Method of Multipliers (ADMM) and primal-dual interior pointconvex optimization. This procedure has demonstrated substantialimprovement in Ubers ability to allocate resources efficiently.",
  "Authors listed alphabetically.Formerly at Uber Technologies, Inc": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 2024 Workshop, August 2526, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00",
  "Causal machine learning, deep neural network, convex optimiza-tion, shape-constrained, measurement": "ACM Reference Format:Bobby Chen, Siyu Chen, Jason Dowlatabadi, Yu Xuan Hong, VinayakIyer, Uday Mantripragada, Rishabh Narang, Apoorv Pandey,Zijun Qin, Abrar Sheikh, Hongtao Sun, Jiaqi Sun, MatthewWalker, Kaichen Wei, Chen Xu, Jingnan Yang, Allen T. Zhang,and Guoqing Zhang. 2024. Practical Marketplace Optimization at UberUsing Causally-Informed Machine Learning. In Proceedings of 2nd Workshopon Causal Inference and Machine Learning in Practice (KDD 2024 Workshop).ACM, New York, NY, USA, 10 pages.",
  "INTRODUCTION": "Each year, Uber manages a multi-billion dollar marketplace. In thethird quarter of 2023 alone , Ubers mobility business recorded$17.9 billion in Gross Bookings (GBs). To attract riders and driversto use the products and to influence the behavior of the market-place, Uber invests in various marketplace levers, allocating budgetsacross different regions and lever types to optimize business objec-tives. Historically, budget allocation was done manually, with teamsreviewing data and adjusting numbers in spreadsheets based onpersonal experience and past allocations. This method did not effec-tively utilize historical data on rider and driver behavior under dif-ferent conditions, lacked a clear objective, and was labor-intensive,making it difficult to measure its effectiveness quantitatively. Wepresent an automated system that leverages causal deep learningmodels to predict how each marketplace lever affects driver andrider behavior and a set of optimization algorithms to decide opti-mal budget allocation across levers and regions. The causal deeplearning model leverages historical data to predict supply, demand,and a custom marketplace objective in the upcoming week foreach region and a given set of incentives. The system then uses asmoothing model to generate high level functions between incen-tive budgets and the outcome for the optimization stage. Finally,the system finds the optimal budget allocation for each region andincentive lever subject to business operations constraints, using anoptimization system to solve a non-convex problem. In addition,we also devised a framework and system to measure the businessimpact of the budget allocation when compared to a baseline allo-cation. This system is deployed in production on a weekly basis",
  "BACKGROUND AND RELATED WORK": "Uber, being a publicly traded company, gives guidance to the publicabout expectations for performance on certain business metricsfor each quarter. Two important metrics are Gross Bookings (GB),which is a measure of how much business was conducted on theUber platform, and Net Income (NI), which is the generally acceptedaccounting practice definition of profit . The internal teams atUber must make prediction on how different products can affectmarket and therefore affect the key financial metrics. This financialtask can be formulated as follows:",
  "(1)": "where weeks are calendar weeks, cities are approximately inde-pendent geographical entities, and levers are different marketplaceprograms that Uber can allocate money to in order to affect market-place outcomes. The function is some objective of importanceto the business. The variables ,, represent budgets for specificweeks, cities, and levers, which can be positive or negative; we usethe convention that positive budgets correspond to Uber spending.The variable is a total budget constraint on the system. The vari-ables ,, and ,, represent floors and ceilings, respectively, onhow much money can be spent in a lever in a specific city-week,which can again be positive or negative.The objective of Equation 1 is to maximize business through-put subject to a fixed budget constraint. The challenge is that theeffects of budgeting a certain lever may not be linear and there maybe substantial interaction effects between levers. In the followingdiscussion, we confine ourselves to discussing only Ubers Ridesbusiness and further confine the problem to a single week and thebudgets are full-week treatments.",
  "where b is what we believe the feature vector under our controlwill be valued and X is what we predict any feature of interest will": "be valued. Importantly, (b) ?= (b) and (X) ?= (X),which is to say, neither control variables nor other confoundingfeatures can be guaranteed to be observed. The control variableshave continuous values, in this case budgets.Let us consider how data can be collected in this context. Weare concerned with marketplace outcomes, which are outcomesaggregated over some finer granularity. In our case, we are lookingat outcomes aggregated per city per week. The finer granularity,in the case of Uber, could be riders, drivers, trips, etc. In a market-place, it is generally assumed that there can be interference effectsbetween treatment groups during experiments. Therefore, ex-perimental results obtained by conducting a randomized controlledtrial on a higher granularity treatment entity may not extrapolateto applying a treatment to the entire population of treated entities.Since the standard assumptions of collecting unconfounded, in-dependent, and identically distributed data do not apply, we can notrely on standard techniques or bounds to guarantee performanceof any estimator. Instead, we must develop empirical techniquesagainst which to test any estimator using both predictive accuracy,referring to how well absolute outcomes of the estimator agreewith test data, as well as causal accuracy, referring to how wellderivatives of the estimator agree with test data.",
  "Related Work": "Time series forecasting continues to be an area of significant in-terest in the literature. Autoregressive statistical approaches relatedto ARIMA remain performant in certain domains, includingstock prediction and pandemic spread prediction. In light ofthe success of transformers in the natural-language processingspace, many proposals have been made to apply transformers totime series prediction problems in order to model both multi-variatedependencies and overall complexity in the time series . Recent results showing the effectiveness of simple linear modelson standard datasets has led to additional recent work on MLP-only approaches . In contrast to the literature mentioned above,which focuses primarily on high predictive accuracy, as measuredgenerally by mean-squared error (MSE) and mean absolute error(MAE), we are concerned also with causal accuracy of the model.Furthermore, identification of a generally superior model archi-tecture has so far defied researchers in the time series forecastingspace, so performant models in new domains remain of interest.Causal ML approaches have explored how to infer heteroge-neous treatment effects using multiple different strategies and showing that, under certain conditions, certainerror terms can be substantially reduced. This body ofwork nearly entirely relies on two assumptions that we cannot rely",
  "Practical Marketplace Optimization at Uber Using Causally-Informed Machine LearningKDD 2024 Workshop, August 2526, 2024, Barcelona, Spain": "on in practical marketplace applications. Relying on the notationthat are per-unit features, R is the observed treatmenteffect and 0, 1 is the binary treatment assignment, the first as-sumption is that the treatment assignment is unconfounded, i.e.{ (0), (1)} | . This is nearly always false in any avail-able observational marketplace data. To give an obvious example:larger cities have larger budgets. The second assumption is that anydata of interest is independent and identically distributed. Neitherof these conditions hold under realistic marketplace conditions. Anexample of the first is that the level of treatment in one week mayimpact how users return in the following week, resulting in differ-ent returns. An example of the second is that there could be a majorsporting event one week that results in very different user behaviorcompared to any other week. While some of the techniques areinformative, the theoretical findings have no applicability to thepractical case.",
  "User Causal Effect Estimator": "To better leverage A/B experiments data which contains the infor-mation of user behaviors between treatment and control groups,We adopted the S-metalearner framework with a ResNetmodel to estimate individual treatment effects (ITE).We have explored other metalearners such as the stacked R-metalearner, and the decision towards S-metalearner took vari-ous considerations into account including feasibility of availablecomputing resources, the implementation and maintenance effortsrequired from a practical standpoint, and experiment results in Ap-pendix B show S-metalearner provided better incremental efficiencywith diminishing returns, aligning well with business intuition.As illustrated in , the model consists of two stages:Embedding stage Sparse embeddings. Our input features consist of some cat-egorical sparse features. Since DNNs are usually good athandling dense numerical features, we map the sparse fea-tures into trainable dense vectors using embeddings. Dense feature extractor. We also extract an embedding rep-resentation for dense numerical features in order to bettercapture higher order interactions between the dense fea-tures. The dense feature embeddings are generated throughmultilayer perceptrons (MLP). Residual network stage. Sparse and dense feature embeddingsare concatenated and fed into a series of ResNet blocks and eventu-ally generate predictions. ResNet blocks are chosen instead of MLPin order to better handle vanishing gradients and reduce the risk ofoverfitting when feature sizes are large.During analysis of deployed budget recommendations, we alsodiagnosed and resolved an endogeneity issue that is naturallyintroduced from clustering data across cities and weeks duringmodel training. It led to possible cross week extrapolations as the training data from different weeks could be endogenous and moreseverely, the treatment-control effects are masked out by cross-week data. This issue was resolved by adding additional positionalencoding for the week in the training and serving data.",
  "Smoothing Layer": "The DL model in .1 produces a mapping from budgets tomarketplace outcomes, which can be fed into an optimizer. How-ever, most optimization routines require numerous passes over thesurface, necessitating many evaluations of the DL model, whichis computationally costly, potentially prohibitively so. Addition-ally, the mapping produced by DL models does not necessarilysatisfy business intuition (e.g., non-negativity, monotonicity, con-vexity). Enforcing such business intuition using flexible functionalforms is generally intractable. For instance, verifying polynomialnon-negativity and convexity is NP-hard.Our goal is to generate a differential surface for derivative-basedoptimization algorithms but also to ensure low-cost evaluationsand compliance with business intuitions.We solve this bottleneck by capturing the DL surface with alower-dimensional representation. We begin by leveraging theAdaptive Sparse Grids (ASG) algorithm to trans-late the DL surface to a computationally cheap grid-based represen-tation. Typically, grid-based representations suffer from a curse ofdimensionality as the number of grid nodes increases exponentiallywith dimensionality (in this case, the number of marketplace leverswe are optimizing). ASG solves this issue by limiting grid nodes to KDD 2024 Workshop, August 2526, 2024, Barcelona, SpainBobby Chen, Siyu Chen1, Jason Dowlatabadi1, Yu Xuan Hong1, Vinayak Iyer1, Uday Mantripragada1, Rishabh Narang1, Apoorv Pandey1, Zijun Qin1, Abrar Sheikh1, HongtaoSun12, Jiaqi Sun1, Matthew Walker1, Kaichen Wei1, Chen Xu1, Jingnan Yang1, Allen T. Zhang1, and Guoqing Zhang1 locations that result in the highest contributions to accuracy and itserves as the foundation of the smoothing layer.We then select B-spline functions as our smoothing model due totheir flexibility and analytical derivative forms . Unlike mono-tonic regression, B-splines offer the advantage of fitting deriva-tives, thereby avoiding optimization issues and aligning better withbusiness intuition by providing continuous lever efficiencies ratherthan step-wise constants, B splines with quadratic basis functions al-low us to enforce monotonicity and convexity, enhancing tractabil-ity , while Cox regression, often used in survival analysis,yields monotonically decreasing curves and is non-convex. Addi-tionally, B-spline functions can adopt shape controls such as spotand universal derivative bounding, in a multidimensional treatmentscenario, without parametric assumptions, thus reducing the riskof model misspecification. Furthermore, we enhance the modelcausal accuracy by integrating network effects-corrected budgetefficiency experimental data through an additional penalty term inthe objective function.",
  "Until Recall hierarchical surplus measures residual error ateach node, stop when all children nodes below maximumacceptable residual error | |;": "3.2.1Adaptive Sparse Grid. Adaptive sparse grids(ASG), constructedby preserving only a subset of the nodes on a dense grid, have beenshown to maintain a comparable level of error while alleviatingthis curse of dimensionality . When the function represen-tation is derived from linearly interpolating between nodes, errorsin the representation stem from not having sufficient resolutionin areas where the function is highly nonlinear. ASG iterativelyrefine the grid and add nodes where errors remain high. The adap-tation process is governed by a score called the hierarchical surplus,which essentially measures how much the underlying objectivefunction at a node deviates from a linear interpolation via the sur-rounding nodes. High values of the surplus imply uncaptured localcurvature, and suggest additional nodes nearby could be useful. Asan example, below shows how the adapted sparse gridplaces greater density around the nonlinear portion of the function (,) = (|0.5 4 4| + 0.1) 1. 3.2.2B-Spline with Business Penalty. We formulate each cityscost surface smoothing as a constrained least square problemin Equation (4). The fitting objective function consists of two parts.The first part is to minimize the sum of squared difference betweenB-spline smoothing model inferred () and DL model inferred () over all budget points = { | = 1, . . . , } in searchspace grid ; while the second part is to minimize the sum ofsquared difference between B-spline inferred budget efficiency andA/B experiment measured budget efficiency, which is essentially the",
  ", {1, . . . , }(7)": "where (; ) is the th B-Spline basis function of degree over the knots for the th incentive lever. The number of basisfunctions for th lever is denoted as . There are =1 B-splinebasis function coefficient 1... to fit. Ignore the lever index hereafter for simplicity. Knots consist of + 1 internal knotsplus boundary knots on each side. The complete knots vector isequal to {0, ,+ }. Each basis function is defined by a recursiverelationship. The 0th degree B-spline has the form",
  "Optimizer": "We use an optimization system to solve a global problem as de-scribed in Equation 1, which seeks to solve an objective maximiza-tion problem under constraints. While, as discussed in Sec. 3.2, weare able to guarantee monotonicity and concavity in individualdimensions, that does not extend to the entire surface.In addition to the predictions from the estimator, we alsointroduce a penalty term in the form of Hellinger distance that forces the system to prefer allocations similar to a referenceallocation:",
  "|0,, |||b0|| |2,(12)": "where b is the budget vector for the week to be optimized, b0 isthe reference budget vector, and are scaling terms per cityand lever, respectively, and the cube root term reduces the penaltyterm size as the budget difference increases, with the logic that thereference allocation should not be preferred if the total differencebetween the allocations is large.In order to solve the non-linear, non-convex problem, which weknow remains differentiable, we adopt an Alternating DirectionMethod of Multipliers (ADMM) approach. Specifically, weseparated the single-city cross-lever problem and the cross-cityproblem, and solved the problems iteratively.Here is the reformation of the problem:",
  "Update z step, this step brings all the city together and onlyconsiders cross city optimization": "Update y step, this step updates the center of the update xstep.Because our problem is close to convex, and the constraints ap-plied to our smoothing model, by tuning , we are able to ensurethat the overall problem has a positive, semidefinite Hessian. Theimplementation of the algorithm is detailed in Appendix D.",
  "Business Value Evaluation": "In contrast to many other machine learning applications, whichare mainly evaluated on predictive accuracy, our business use caserequires us to predict marketplace outcomes for the coming weeksand generate budget allocations that maximize those outcomes.Therefore, we need a customized evaluation framework to measure the quality of these budget allocations. One common approach,running large-scale experiments in the marketplace, is prohibitivelycostly and impractical for continuous model evaluation. Instead,we rely on existing A/B experiments that adjust budgets at the userlevel to estimate our business impact.The Business Value Evaluation (BVE) framework combines ex-isting A/B experimental data with simulated allocations to produceestimates that can be directly interpreted as weekly business impact.In particular, this can be calculated as:",
  ",,,(15)": "Where indexes incentive levers and c denotes cities, and IOBis the implied return on additional budget from the existing A/Bexperimental data. To compute the integral, we take two approaches.(1) A linear approximation of the integral which implies simplycomputing the difference in budget between the optimal and actualallocations, then multiplying these differences by the return on thisincremental budget. (2) A log-linear relationship between budgetsand IOB and estimates this relationship from historical data. Thisensures that we capture the curvature in the relationship betweenbudget and IOB.The key input of BVE is the incremental return on budget (IOB)which is estimated from user level A/B experiments which eithermake treatment more generous or target additional units. Giventhese continuously running experiments, we can measure the lift ofan additional dollar budget on our with the following regressionas:",
  "= + + +": "Where refers to a treated unit and are a vector of pre-XP co-variates. Adding in controls into this regression framework allowsus to soak up idiosyncratic noise and increase precision . Thecoefficient gives us the incremental lift on per user (iO). More-over, for each of these XPs, the incremental budget per user (iB) isdeterministic and given by the XP design. This implies that we cancompute .The main challenge in computing IOB based on unit level A/Bsat Uber is the presence of network effects or spillover effects. Whatmakes this a unique problem without standard off the shelf solutionsis that the Uber marketplace has multiple rationing mechanismsduring periods of relative undersupply; namely prices and ETAs(expected time to arrival). This implies that the Stable Unit Treat-ment Value Assumption (SUTVA) is often violated, where outcomesfor one unit depends on the treatment assignments of others. As asimple example, allocating budget towards incentives which boostdemand when there is acute undersupply can lead to user levelA/B effects grossly overestimating the true marketplace effect. Thekey mechanism behind this is that treated users on the demandside of the market might cannibalize supply which would havebeen otherwise available to the control group users. To accountfor this issue, we developed an economic model which predicts theincremental marketplace changes in endogenous quantities suchas our to changes in exogenous quantities like supply and de-mand. This model is characterized by a set of key elasticities whichgovern how both users and aggregate marketplace outcomes re-spond to changes in Ubers rationing mechanisms; prices and ETAs. KDD 2024 Workshop, August 2526, 2024, Barcelona, SpainBobby Chen, Siyu Chen1, Jason Dowlatabadi1, Yu Xuan Hong1, Vinayak Iyer1, Uday Mantripragada1, Rishabh Narang1, Apoorv Pandey1, Zijun Qin1, Abrar Sheikh1, HongtaoSun12, Jiaqi Sun1, Matthew Walker1, Kaichen Wei1, Chen Xu1, Jingnan Yang1, Allen T. Zhang1, and Guoqing Zhang1 The paper closest in spirit to our approach is which developsa marketplace model to understand how pricing can prevent themarket from reaching undesirable equilibria. Our approach is alsosimilar to recent papers in the literature which developmarketplace models in ride-sharing markets to study optimal pric-ing and matching policies. With key elasticities estimated frompreviously run XPs, we can predict the change in our due tochanges in model inputs such sessions and supply hours. We thenuse a first-order Taylor approximation to translate A/B outcomesinto marketplace outcomes in the following way:",
  "EVALUATION AND RESULTS": "We conducted comprehensive backtesting using Uber marketplacehistorical data to evaluate the efficacy and robustness of our pro-posed methodology. This approach allows us to measure incremen-tal out-of-sample performance and the business impact of counter-factual budget allocations from our system, all while fully account-ing for complexities in the underlying data generating processWe begin with predictive accuracy to ensure our model capturesmarketplace dynamics effectively. We compute wMAPE (weightedMean Absolute Percentage Error) and wBIAS (weighted BIAS) atthe city-week level, appropriately weighting cities of varying sizesand matches the granularity of the budget allocation decision. shows how these metrics vary across methodologies and regions.Across both regions, the Causal DL model substantially outperformsthe stacked Estimator model in terms of wMAPE and wBIAS.We complement these predictive accuracy metrics with businessimpact metrics from the aforementioned Business Value Evaluationframework in .4. Internally, we compute business impactfrom Equation 15 in terms of when comparing candidate modelsfor production usage. While we are unable to share this directly,we can share the following closely related metric for each model:",
  ",B,,": "Marginal efficiency captures the value of an additional dollar ofbudget allocated proportionally to all cities and levers by weightingefficiency estimates accordingly. High marginal efficiency impliesbudgets are being deployed in an efficient manner. reportsthe percentage change in this metric against a baseline model forour backtests which show that the Causal DL model with modelenhancements outperforms the baseline model in both regions.Finally, shows an example of how budget allocationsdiffer across select cities (y-axis) and weeks (x-axis) between twodifferent models. Colors and annotations indicate the percentagechange in budgets across the models. Importantly, note how thebudget differences are mostly small. This allows us to comfortablyuse a linearized version of Equation 15 to measure business impact.",
  "CONCLUSION": "We present an end-to-end causal machine learning and optimizationsystem designed to enhance Ubers marketplace budget allocationprocess. This system automates weekly budgeting decisions tomaximize marketplace objectives within predefined constraints,improving overall operational efficiency. Our approach addresses ahigh-dimensional optimization problem involving various market-place levers and their respective budgets. To solve this, we employa deep learning estimator based on an S-Learner approach, leverag-ing extensive experimental and temporal-spatial observational datato estimate causal effects. We also incorporate a tensor B-Splineregression model that captures the detailed response surface of DLmodels while ensuring practical efficiency. Additionally, our systemutilizes ADMM and primal-dual interior point convex optimizationtechniques, implemented on Ray, to handle large-scale nonlinear,non-convex problems efficiently. The deployment of this systemnot only simplifies decision-making but ensures these decisions aredata-driven and aligned with Ubers strategic objectives.",
  "of the national academy of sciences, 116(10):41564165, 2019": "Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang,and Xifeng Yan. Enhancing the locality and breaking the memory bottleneck oftransformer on time series forecasting. Advances in neural information processingsystems, 32, 2019. Bryan Lim, Sercan Ark, Nicolas Loeff, and Tomas Pfister. Temporal fusiontransformers for interpretable multi-horizon time series forecasting. InternationalJournal of Forecasting, 37(4):17481764, 2021. Ziqi Liu, Dong Wang, Qianyu Yu, Zhiqiang Zhang, Yue Shen, Jian Ma, WenliangZhong, Jinjie Gu, Jun Zhou, Shuang Yang, and Yuan Qi. Graph representationlearning for merchant incentive optimization in mobile payment marketing.In Proceedings of the 28th ACM International Conference on Information andKnowledge Management, CIKM 19, page 25772584, New York, NY, USA, 2019.Association for Computing Machinery. Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, RichardLiaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I Jordan,et al. Ray: A distributed framework for emerging {AI} applications. In 13thUSENIX symposium on operating systems design and implementation (OSDI 18),pages 561577, 2018.",
  "Yu Wang, Wotao Yin, and Jinshan Zeng. Global convergence of admm in non-convex nonsmooth optimization. Journal of Scientific Computing, 78:2963, 2019": "Li Yu, Zhengwei Wu, Tianchi Cai, Ziqi Liu, Zhiqiang Zhang, Lihong Gu, XiaodongZeng, and Jinjie Gu. Joint incentive optimization of customer and merchant inmobile payment marketing. Proceedings of the AAAI Conference on ArtificialIntelligence, 35(17):1500015007, May 2021. Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effectivefor time series forecasting? In Proceedings of the AAAI conference on artificialintelligence, volume 37, pages 1112111128, 2023.",
  "Christoph Zenger. Sparse grids. In Proceedings of the Research Workshop of theIsrael Science Foundation on Multiscale Phenomenon, Modelling and Computation,page 86, 1991": "Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,and Wancai Zhang. Informer: Beyond efficient transformer for long sequencetime-series forecasting. In Proceedings of the AAAI conference on artificial intelli-gence, volume 35, pages 1110611115, 2021. Tian Zhou, Ziqing Ma, Qingsong Wen, Liang Sun, Tao Yao, Wotao Yin, Rong Jin,et al. Film: Frequency improved legendre memory model for long-term time seriesforecasting. Advances in Neural Information Processing Systems, 35:1267712690,2022. Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin.Fedformer: Frequency enhanced decomposed transformer for long-term seriesforecasting. In International Conference on Machine Learning, pages 2726827286.PMLR, 2022.",
  "BMETALEARNER COMPARISON": "In , we evaluate the predicted incremental supply efficiencyof the S-learner with percentage error (BIAS) at the city-week gran-ularity, which shows closer alignment with experimental groundtruth compared to the R-learner. The untuned S-learner model ex-hibits a median BIAS similar to the current baseline performance.Unlike the baseline, the S-learner does not consistently underes-timate lever efficiency, though certain weeks display significantprediction errors.",
  "DOPTIMIZER IMPLEMENTATION": "In order to solve the per-city problems in the b-update step, weuse cvxopt , which is a primal-dual interior-point convex solverthat uses Nesterov-Todd scaling . Since the modified problemis both city-independent and convex, this inner solver works wellfor us.A major challenge to solving the problem in a single thread,instead of in parallel, is that the constraint complexity is high, andcollinear constraints can affect the Cholesky decomposition in theNesterov-Todd scaling step. The single city constraints are mucheasier to manage and therefore never cause this problem in thecvxopt step. Regardless, managing constraints, including onlinevalidations when receiving constraints from client teams remains akey practical effort of the system. KDD 2024 Workshop, August 2526, 2024, Barcelona, SpainBobby Chen, Siyu Chen1, Jason Dowlatabadi1, Yu Xuan Hong1, Vinayak Iyer1, Uday Mantripragada1, Rishabh Narang1, Apoorv Pandey1, Zijun Qin1, Abrar Sheikh1, HongtaoSun12, Jiaqi Sun1, Matthew Walker1, Kaichen Wei1, Chen Xu1, Jingnan Yang1, Allen T. Zhang1, and Guoqing Zhang1 Recall that our constraints that cross multiple cities are equalityconstraints. Therefore,() in Equation 13 is essentially an indicatorfunction that is zero-valued when the constraint is met and takes alarge value when the constraint is not met.The update z step can therefore be translated to a simple qua-dratic optimization problem. We can then use the Cauchy-Schwartzinequality to simplify further, eventually resulting in an algebraicexpression, that for the constraint in Equation 1, results in:",
  "framework, we achieved approximately a 50x speedup periteration in that step": "Adaptive . In order to improve stability of the system, weadaptively increase when non-convex sub-problems aredetected. As mentioned above, this modifies the eigenvaluesof the Hessian to ensure convexness. Early Stopping & Optimal criteria. The ADMM algorithmcan quickly get to a point where it is close to the global op-tima, but it takes a long time to provide a solution withmulti-digit accuracy. Our implementation requires a compro-mise between runtime and accuracy. We define a solution isoptimal if: Constraints are obeyed within 0.1% accuracy All sub problems (single city) are converged The change in objective value is small from the previousiteration"
}