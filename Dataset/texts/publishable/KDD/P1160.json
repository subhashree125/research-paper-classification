{
  "ABSTRACT": "We introduce OpportunityFinder, a code-less framework for per-forming a variety of causal inference studies with panel data for non-expert users. In its current state, OpportunityFinder only requiresusers to provide raw observational data and a configuration file.A pipeline is then triggered that inspects/processes data, choosesthe suitable algorithm(s) to execute the causal study. It returns thecausal impact of the treatment on the configured outcome, togetherwith sensitivity and robustness results. Causal inference is widelystudied and used to estimate the downstream impact of individualsinteractions with products and features. It is common that thesecausal studies are performed by scientists and/or economists peri-odically. Business stakeholders are often bottle-necked on scientistor economist bandwidth to conduct causal studies. We offer Oppor-tunityFinder as a solution for commonly performed causal studieswith four key features: (1) easy to use for both Business Analystsand Scientists, (2) abstraction of multiple algorithms under a singleI/O interface, (3) support for causal impact analysis under binarytreatment with panel data and (4) dynamic selection of algorithmbased on scale of data.",
  "Both authors contributed equally to this research": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from acronym XX, June 0305, 2018, Woodstock, NY 2018 Association for Computing Machinery.ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00",
  "INTRODUCTION": "Automated machine learning (AutoML) frameworks for predictivemachine learning (ML) have advanced significantly over the pastdecade with the introductions of AutoGluon , Auto-sklearn, H2O . AutoMLs biggest advantage is abstracting awaythe implementation of underlying algorithms and hyper-parametertuning, and making it easy for scientists and engineers to experi-ment with a large number of models and identify the one that worksbest. The demand of AutoML has risen from the fact that no singleML algorithm works best in all scenarios. This has been even morechallenging in the causal inference literature. Different methodsrely on different set of assumptions for the identification ofcausal treatment effects1: CIA (conditional independence assump-tion or unconfoundedness), propensity overlap, SUTVA (stable unittreatment value assignment), exchangeability (same outcome dis-tribution would be observed if exposed and unexposed individualswere exchanged) etc.Causal inference framework DoWhy supports explicit mod-eling and testing of causal assumptions, but it is still a low level API.AutoCausality, which is built on the top of EconML andDoWhy, supports automated hyperparameter tuning, but it onlyfocuses on the estimation part and assumes that the causal graphprovided by the user accurately explains data-generating process.Both AutoCausality and DoWhy do not support panel data2, whichis a mainstream at real-world problems. Most real-world causalstudies have panel data of different aggregated granularities, e.g.,yearly to daily levels, at different scales, e.g., few individuals tolarge populations of million entities. To the best of our knowledge,there is no AutoML-like causal inference framework that supportspanel data and abstracts away the know-how of causal studies fromthe users.In this study, we introduce OpportunityFinder (OPF), our firststep in democratizing causal inference techniques. As of our firstcontribution, Project OPF implements an auto causal inferenceframework that supports panel and cross-sectional data and offersa wide range of causal inference algorithms. The decision to choosethe algorithm is automated and abstracted away from the user. Oursecond contribution is the automated transformation from inputpanel data into list of cohort datasets when needed. Cohort-basedresults are then aggregated for a final result. In the third contri-bution, OPF provides data visualization to illustrate causal impact.Combining numerical and graphical reports help non-expert users 1A causal effect can be defined as the difference between hypothetical outcomes thatresult from two or more alternative treatments, with only one outcome of a treatmentbeing observed each time2Panel data contains observations collected across multiple individuals at a regularfrequency, and ordered chronologically.",
  "Conference acronym XX, June 0305, 2018, Woodstock, NYHuy Nguyen, Prince Grover, and Devashish Khatwani": "of using various base learners. The key assumption is thatthe base learners are correctly specified. They are not specif-ically designed for panel data and require a relatively largesample size. The limitation for auto causal framework is inthe choice of base learner. Difference in Differences (DiD): DiD compares the aver-age change in outcome over time that occurs in the treat-ment group to the average change over time that happens inthe control group. Its designed to handle unobserved, time-invariant confounders. Its a simple and intuitive method forpanel data, widely used in economic studies. Problem withDiD is that is relies on strong parallel trends assumption thatis often violated in the real world setting. We do not use DiDor variant Synthetic DiD in our implementations. Propensity Score Matching (PSM): The propensity scoreis the conditional probability of receiving treatment givenpre-treatment characteristics. This approach has been tra-ditionally popular because of its simplicity, interpretabilityand ability to handle large covariates. But DML is based onsimilar principle and overcomes the limitations that PSM hasand is more robust. Complementing DML, SC methods donot require unconfoundedness assumption that PSM does.Therefore, we do not use PSM in the OPF.",
  "LITERATURE REVIEW": "Traditional econometric techniques such as propensity score match-ing, instrumental variable estimation, and difference-in-differences(DiD), offer rigorous methods for estimating average treatmenteffects under specific assumptions, but often struggle to accountfor high-dimensional covariates and complex interactions . TheSynthetic Control Method (SCM) extends these approaches by con-structing a synthetic control unit as a weighted combination ofpotential control units, providing a more flexible comparison forthe treated unit . The Generalized Synthetic Control (GSC) fur-ther expands SCM by incorporating interactive fixed effects models,thus accommodating multiple treated units and variable treatmentperiods .Recently, machine learning techniques have been widely inte-grated into causal inference due to notable works by various teams,e.g., DoubleML , EconML , CausalML . Double MachineLearning (DML) provides a flexible approach, leveraging machinelearning for nuisance parameter estimation while maintaining ro-bustness against mis-specification . Beyond average treatmenteffect, machine learning enables approaches to estimate individualtreatment effects, e.g., heterogeneous treatment effect estimator inEconML and uplift modeling in CausalML. Deep learning methods,such as those based on Neural Networks (NN), have shown promisein estimating individual treatment effects due to their ability tomodel complex, high-dimensional data, thus uncovering nuancedcausal relationships .",
  "FRAMEWORK DESIGN": "The key contributions of our design are (1) integration of severalcausal modeling models, (2) branching based on type of observa-tional data (cross sectional vs. panel) and number of treatment units,and (3) execution in the users own AWS environment where theyhave access to CloudWatch logs for debugging and can visualize theprogress. Current OpportunityFinder deployment allows code-lessUI without having to move data outside the AWS account as demon-strated in . While this design is tied to the MLOps set-up ofour organization, OpportunityFinder source code is independentfrom deployment platforms. shows the design of OpportunityFinder. Once a usertriggers a job, CloudFormation kicks off a set of AWS services in-cluding SageMaker, Lambda and Glue jobs. Data Validation modulechecks treatment and control data for basic requirements. Sage-Maker Pipeline then starts with performing follow-up components.Data Processing transforms panel data into cohorts (where needed),handles missing data, extracts lag/lead features, performs optionaldata scaling and normalization. Causal Estimation decides mostsuitable causal model given data, and executes the model. ResultValidation performs validation tests for sanity and sensitivity, and returns the estimated treatment effect in a standardized format intothe users S3 bucket.The data processing can vary for different underlying models.For example, Generalized Synthetic Control (GSC) works welleven if there is one treated unit, but it requires panel data withat-least 7 pre-treatment periods. Double Machine Learning (DML) is a better solution for large-scale data but requires breakingdown treatments into the cohorts of different weeks, months orquarters, depending on the number of treated individuals in eachcohort.On completion of causal estimation, a series of sensitivity andplacebo tests are applied to assess the robustness of the findings toviolations of the underlying assumptions. These validations include(but not limited to), direction of causal relationship, sensitivityof causal estimate to small variations in observations data (e.g.,down-sampling, random co-variate) and variations in model hyper-parameters (e.g., number of pre-treatment periods used for findingsynthetic controls). The results of these validation tests are writtento the S3 bucket for user reference.",
  "Data Requirements": "OpportunityFinder requires user to provide two datasets and aconfiguration file (examples shown in ). The first data, i.e.,treatment data, should contain IDs of the treated units 3 and datewhen the treatment happened. The second data, also known as,baseline observational or control data, contains the observationalinformation about all IDs that received treatment as well as the onesthat did not receive treatment during the same period. Control datashould contain time-based, e.g., daily, weekly or monthly, outcomevariables (i.e., target) of interest such as ad spend, click count overthe historical period. At the same level of time granularity, user isrecommended to add a superset of possible variables (i.e., features)that are related to the outcome and the treatment. Among thosesuperset of variables, the model will search for the ones that can helpin removing the confounding and mediating effects, an essentialfor accurate causal estimates.Configuration file has optional and mandatory requirements.Optional requirements like list of features to scale, choice of algo-rithm, choice of hyper-parameters allow user flexibility, but are notnecessary and can be automatically handled by the framework. Themandatory requirements include columns that specify time, unit id,outcome variable and pre/post-treatment evaluation window, e.g.,4 weeks, 6 months. Based on user-provided configuration and datavalidation, input panel data might be segmented into cohorts andfeature engineering would be performed, before passing to causalanalysis algorithms.",
  "output and within lower and upper bounds of at-least 2 more esti-mators (voting mechanism)": "3.2.2Cohort Data. One of key functions provided by Opportuni-tyFinder is the transformation of panel data into cohort, i.e., sec-tional data, which allows techniques like double machine learning towork. Each cohort corresponds to a set of treated units that receivedtreatment in a closed period. First, treatment data is processed toextract list of treatment times and number of treated units at eachtime. A cohort is a set of one, two or more consecutive treatmenttimes and constrained by three parameters: Minimum/maximumnumber of treatment times, and minimum number of treated units.",
  "Deep Neural Networks: we implement four state of the artsDNN algorithms for estimation of treatment effect, BCAUSS, DRAGON , TARNET , and GANITE": "3.2.4Validation Tests. DML models and their treatment effect es-timation are validated through refutation tests by DoWhy package:add random common cause, add unobserved common causes, datasubsets validation, and placebo treatment. For a robust causal modeland valid treatment effect, first three tests should return treatmenteffect similar to original model while fourth test must have effectclose to zero. GSC model is validated with a suite of sensitivitytests that check for changes in the estimated causal effect withsmall changes in data like random down-sampling, different pre-treatment window for learning synthetic control weights and areduced covariate list. The expectation is that the causal effectshould not change the direction of estimation with small changesin the setting. Example test results are shown in Appendix B. Infuture, we will equip validation tests for DNN models. 3.2.5Data Visualization. A challenge that prevents the adoptionof causal inference studies is a lack of ground-truth data whichmakes estimation error impossible to assess. OpportunityFinderaddresses this by providing visualizations that naively explain thetreatment effect to some extent. For example, it returns a plot thatshows the trend of outcome for treated and control units over time.The visualizations are a part of Logging and Monitoring module.While such plots do not confirm calculated treatment effect bycausal models, they help non-expert users to comprehend causalinference results. Example visualizations are shown in Appendix C.",
  "Limitations and Risks": "As of today, OpportunityFinder (OPF) does not implement causalgraph generation algorithms. This also means that the tool hasless flexibility for someone who wants to control covariates andexperiment with different algorithms. We plan to integrate causaldiscovery module in near future. OPF applies our best heuristics after exploring input data toselect the right algorithm. Due to the lack of ground truth datain causal inference, our framework can make mistakes withoutknowing that the estimated effect it returns is wrong. We select amodel based on standard error and ensemble by voting to mitigatethis limitation upto some extent. The accuracy of estimate stilldepends on the observational data given by the user.For real-world problems, OPF does not necessarily use estimationmodels that gave best score on benchmark data. Our experimentsshow that simpler estimators work more reasonably than DNNmodels on our use-case data.",
  "IHDP (public benchmark)": "The Infant Health and Development Program (IHDP) is a ran-domized controlled study designed to evaluate the effect of special-ist visit on cognitive test scores of premature infants. This datasetis cross-sectional data, with binary treatment (specialist visits), con-tinuous outcome (cognitive scores) and has known ground truthATE. As shown in , our implementation of DML modelsachieved competitive performance. Results on BCAUSS, TARNETand DRAGON are based on our implementation and slightly differfrom reported numbers . The difference is because DNN methodsare executed within OPF pipeline and data are not prepared as thesame as previous studies.",
  "Smoking (public benchmark)": "The goal of smoking data is to analyze the causal effect of Propo-sition 99 on cigarette sales. This data has small size with just onetreated unit, thus causal estimations based on machine learning(DML, NN) do not apply. OpportunityFinder chooses to run GSCand does not create any cohorts. shows comparison of re-sults using OPF on this dataset with previous research works.We observe that the range of ATE estimate lies between -11.1 to-27.1, and the results from OPF are within the range of previousstudies.4 When we use cigarette retail price as a covariate, the ATEreduces to -14.0, which is closer to SDID. It is discussed in SDID paper that their results (-15.6) are more credible among other",
  "Synthetic Data 1: Cross Sectional (synthetic)": "In addition to public datasetes, we validate OpportunityFinder out-puts on two synthetic datasets with known ATE.5 The first syntheticdataset is a linear cross sectional dataset that we generated usingDoWhy package. We created the dataset with 2 instrument,5 common causes, 5000 samples and binary treatment with sometreatment noise. Because this data is cross sectional, OPF rejectsGSC, but branches off to the second stage of decision path, whereit evaluates multiple models, including DML , and neural netbased estimators including BCAUSS , TARNET , DRAGON and GANITE . As we see in , all models (exceptGANITE) give ATE close to the true ATE. OPF finally selects themodel with least standard error if the mean is within the range of2+ other models, and ends up selecting LinearReg+LinearDML.",
  "Synthetic Data 2: Large Panel (synthetic)": "In the second dataset, we add non-linear confounding effect andcorrelated variables on a panel data, to test the efficacy of differentsupported models to be able to remove the bias. This data contains52,000 rows, 3 confounders with non linear effect on treatmentand outcome, 1000 units, 263 treated units and 52 time periods.The properties of this dataset enables OpportunityFinder to run allimplemented algorithms and select based on standard errors. Asshown in , all models except GANITE, perform well thatestimated ATE are close to ground-truth. Due to lowest variance inestimations from GSC together with mean estimation lying between2+ other estimators, OPF chooses GSC results for the end user.",
  "Discussion on Model Choice": "While our approach for model selection is evolving, the results onsynthetic and public datasets show that our current 2 stage decisionpath works well. The two stage decision path allows automatedrejection of estimators if they are not built for the use case athand. For example, GSC is supposed to be used for panel databut becomes computationally inefficient with >500,000 data sizes.OpportunityFinder does not run GSC for such large data sizes. Asthe research evolves, especially with neural networks for causalinference, we plan to incorporate the new models as well as updatethe models selection criteria. For example, we will explore providingresults from ensemble of models and finding the expected causalpath using causal discovery algorithms.",
  "APPLICATIONS ON REAL WORLD DATA": "OpportunityFinder has already been used in multiple use cases.In this section, we present two most important applications ofOPF within our organization. Most commonly used down streamimpact metric in real applications is uplift, which is defined asthe percentage increase/decrease in the outcome attributed to thetreatment over a defined period. It is calculated as ATE or ATTdivided by average over control units.",
  "Opportunity for Partners": "Advertising partners are the agencies and tool providers that haveexpertise in interacting with Ads products and help sellers/vendorsin setting ad campaigns. Our team helps in identifying the ac-tions that would enable partners to create maximum value forsellers/vendors. Such actions are considered opportunity for part-ners. Their impacts are measured on a wide list of business outcomemetrics such as revenue and adspend. Traditionally, it used to take1-3 weeks of an Economist time to update the studies on ad-hoc re-quests. Since January 2023, we have been using OpportunityFinderto refresh the studies. Each refresh completes in a day with minimalhuman involvement.OPF chooses GSC due to: number of total events < 500,000, thenumber of treated units per monthly cohort < 50 and control units< 5,000. In , we show two such opportunities: adoption of X,and adoption of Y6 by the partner for at-least one of their customers.Their lifts on three business metrics. Comparing to results of priorstudies, we can see delta between past and current downstreamimpact, which is caused due to behavioral and market changesover time. We further compare GSC results with DML and DNNmodels. While DML models return lift scores about 5% to 10% higherthan GSC, DNN estimated lifts are from 20% to hundred percenthigher which are beyond acceptable range. ML-based models mayover-estimate when input data is small.",
  "Opportunity for Advertisers": "This study estimates the effect of advertising partners on sell-ers/vendors outcomes related to Ads business, e.g., ad spend. Thisstudy has been traditionally taking multi-weeks of scientists effortfor each refresh. In 2023, the study was expanded in both numberof outcome variables and numbers of groups of partners and adver-tisers. Each combination of outcome and entity group is a separatecausal study. OpportunityFinder helped accelerate the study so thatall experiments were completed within a month.With a large number of advertisers, input data is redirected toDML and DNN causal models. Input panel data is then transformedinto cohorts, before feature engineering and model training. Based",
  "Metric 468%45%62%15%17%14%12%14%Metric 558%48%64%16%14%13%16%12%": "on ATE and standard error results on validation datasets, OPFchooses Rand.Forest+LinearDML as final model. Our results werereviewed by domain experts and in range of results from priorstudies. In , we report lift metric returned by all possiblemodels on a dataset. Three DNN models over-estimate treatmenteffect, and only GANITE yields numbers close to DML models.",
  "CONCLUSIONS AND FUTURE WORK": "This paper presents OpportunityFinder (OPF), a codeless frame-work for causal inference studies, with a focus on panel data withbinary treatment. Our experiments on multiple public, syntheticand internal datasets show that OPF can handle a diverse set of sce-narios and our decision criteria for algorithm selection works wellfor given use-cases. We also see that in most of the cases, simpleralgorithms like DML and GSC work well. We are able to use OPFon datasets ranging from small panel data to a large data with morethan one million observations. We are actively taking feature requests from current OPF users.With causal discovery component, we will explore how hypothesisformulation before estimation can improve the estimation capa-bility, especially with large set of observational data that a nonexpert user tends to provide. We also aim to provide a master list ofvariables that can be collected for causal inference studies withinour organization, and let OPF auto-shortlist covariates using datadriven approaches for removing bias. We plan to extend OPF byincorporating more estimators like meta learners, implement indi-vidual and heterogenous treatment effects, and support categoricaland continuous treatments. With more and more causal inferencealgorithms being integrated into OPF, we will implement additionalmodel selection, e.g., prediction/regression accuracy of base learn-ers. Moreover, we will experiment model ensembles to providethe final output. Last but not least, we have refactored Opportuni-tyFinder source code to make it a stand-alone library independentof AWS ecosystem.",
  "Dmitry Arkhangelsky, Susan Athey, David A. Hirshberg, Guido W. Im-bens, and Stefan Wager. 2021.Synthetic Difference in Differences.arXiv:1812.09970 [stat.ME]": "Philipp Bach, Victor Chernozhukov, Malte S. Kurz, and Martin Spindler. 2022.DoubleML An Object-Oriented Implementation of Double Machine Learningin Python. Journal of Machine Learning Research 23, 53 (2022), 16. Keith Battocchi, Eleanor Dillon, Maggie Hei, Greg Lewis, Paul Oka, MirunaOprescu, and Vasilis Syrgkanis. 2019.EconML: A Python Package for ML-Based Heterogeneous Treatment Effects Estimation. Version 0.x.",
  "The following models are considered for the auto causal framework:": "Synthetic Control (SC) and Generalized Synthetic Con-trol (GSC): SC allows for comparative case studies using aweighted combination of control units to create a syntheticcontrol unit. GSC extends this by considering interactivefixed effects models. The key assumption is that the out-come of treated units is a linear function of the outcomesof the control units in the absence of treatment. GSC allowsrelationship to vary over time, unlike traditional SC meth-ods. Both methods are well suited for panel data with smallsample sizes but require domain knowledge for selection ofcontrol units. The limitation for implementing GSC in theauto causal framework is computational inefficiency withlarge observational data or with more number of covariates. Double Machine Learning (DML): DML leverages ma-chine learning to estimate treatment effects in a semi-parametricmanner, allowing for complex relationships. The key require-ment for DML to work well is the availability of high-qualityand diverse covariate data. DML can handle large datasetsand does not specifically require panel data. It allows fordifferent ML models to be used in the two stages, providingversatility. Causal Forests: Causal Forests extend random forests toestimate heterogeneous treatment effects, offering flexibilityand the ability to capture complex relationships. The keyassumption is the unconfoundedness or ignorability assump-tion. It is not inherently designed for panel data and requiresa relatively large sample size. The limitation for auto causalframework is that it does not handle panel data well and wedid not find it to work well in our experiments. Neural Network based approaches: Several approachesutilize neural networks for causal inference, each with itsunique proposition: BCAUSS, Dragonnet and TARNet modeltreatment assignments and potential outcomes in a multi-task learning setup, allowing finding of least dissimilar treatedand untreated observations. GANITE leverages the powerof generative adversarial networks (GANs) to estimate indi-vidual treatment effects. They require relatively large andhigh-quality datasets, otherwise can over/under estimate thetreatment effects. These methods can handle large datasetsbut are not specifically designed for panel data.",
  "BVALIDATION TESTS FOR TREATMENTEFFECT RESULTS": "To demonstrate the validation test for treatment effect results, wereport refutation test outputs for LinearReg+LinearDML model andits ATE on Synthetic#1 data in . Sensitivity test for GSCmodel are reported in . All refutation tests passed, placeboATE are close to zero while other test ATE close to original model.This confirms the model is robust against changes in settings andestimated ATE is consistent.",
  "CVISUALIZATION OF OUTCOME VARIABLESIN DIFFERENT DATA": "We display data plots generated by OpportunityFinder when run-ning different datasets. plots average outcome of treated(orange line) and control (blue line) units for a cohort. Dash-redvertical bars indicate start and end date of cohort. These plots aregenerated by our data processing module. For Smoking and Texasdata, Figures 4, 5 are generated by our GSC model that black lineshows time-series of outcome values of treated unit, and dash-blueline show that of synthesized control.",
  "DADDITIONAL RESULTSD.1Texas data": "shows impact on black and white male incarceration fromprison expansion in Texas since 1993. The numbers represent aver-age percentage lift on the respective observational metric in Texasvs. other states due to the expansion. The covariates used to removebias include poverty rates, white male incarceration, percentageof population between 15 and 19, income, unemployment rate andAIDS mortality. We see that the Texas had an average of 2x (100%)more black male incarceration compared to the other states after",
  "D.2NSW and Castle data": "In this section, we show results on two additional datasets, NSW and Castle . Caslte is a panel data with year level informa-tion for 10 years, covering 50 states out of which 21 adopted thecastle doctrine law. Castle law designates a persons abode or anylegally occupied place (for example, a vehicle or home) as a place inwhich that person has protections and immunity permitting, in cer-tain circumstances, to use force (up to and including deadly force)to defend oneself against an intruder, free from legal prosecution forthe consequences of the force used. The study done by aimedat finding the effect of castle doctrine law on increase in homicide.This data has 550 rows and 170 features (potential covariates), andbased on researchers outcome, expected lift in homicide of 8% (weare accessing if OPF models reproduce the study). We see that insuch small data sizes with large number of potential covariates,linear models do the best, and boosted trees can be very off.",
  "%38%": "NSW is a famous experimental data that is complemented withadditional synthetic data where researchers added selection bias inthe control population. This is used in multiple research works toreplicate the results of randomized trials. It contains 50,000 rows and180 features, and has 100 simulated variations. It is not a panel dataand we tested it with DML variations. We observe underestimation"
}