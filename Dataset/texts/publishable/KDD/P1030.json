{
  "ABSTRACT": "In recent years, graph neural networks (GNNs) have emerged as apotent tool for learning on graph-structured data and won fruit-ful successes in varied fields. The majority of GNNs follow themessage-passing paradigm, where representations of each node arelearned by recursively aggregating features of its neighbors. How-ever, this mechanism brings severe over-smoothing and efficiencyissues over high-degree graphs (HDGs), wherein most nodes havedozens (or even hundreds) of neighbors, such as social networks,transaction graphs, power grids, etc. Additionally, such graphs usu-ally encompass rich and complex structure semantics, which arehard to capture merely by feature aggregations in GNNs.Motivated by the above limitations, we propose TADA, an effi-cient and effective front-mounted data augmentation framework forGNNs on HDGs. Under the hood, TADA includes two key modules:(i) feature expansion with structure embeddings, and (ii) topology-and attribute-aware graph sparsification. The former obtains aug-mented node features and enhanced model capacity by encodingthe graph structure into high-quality structure embeddings withour highly-efficient sketching method. Further, by exploiting task-relevant features extracted from graph structures and attributes, thesecond module enables the accurate identification and reductionof numerous redundant/noisy edges from the input graph, therebyalleviating over-smoothing and facilitating faster feature aggrega-tions over HDGs. Empirically, TADA considerably improves thepredictive performance of mainstream GNN models on 8 real ho-mophilic/heterophilic HDGs in terms of node classification, whileachieving efficient training and inference processes.",
  "Computing methodologies Neural networks; Supervisedlearning by classification; Mathematics of computing Ap-proximation algorithms": "Permission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s).ACM ISBN 978-1-4503-XXXX-X/18/06.",
  "graph neural networks, data augmentation, sketching, sparsification": "ACM Reference Format:Yurui Lai, Xiaoyang Lin, Renchi Yang, and Hongtao Wang. 2024. EfficientTopology-aware Data Augmentation for High-Degree Graph Neural Net-works: Technical Report. In Proceedings of the 30th ACM SIGKDD Con-ference on Knowledge Discovery and Data Mining (KDD 24), August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 16 pages.",
  "INTRODUCTION": "Graph neural networks (GNNs) are powerful deep learning archi-tectures for relational data (a.k.a. graphs or networks), which haveexhibited superb performance in extensive domains spanning acrossrecommender systems , bioinformatics , transportation, finance , and many other .The remarkable success of GNN models is primarily attributed tothe recursive message passing (MP) (a.k.a. feature aggregation orfeature propagation) scheme , where the features of a node areiteratively updated by aggregating the features from its neighbors.In real world, graph-structured data often encompasses a wealthof node-node connections (i.e., edges), where most nodes are adja-cent to dozens or hundreds of neighbors on average, which are re-ferred to as high-degree graphs (hereafter HDGs). Practical examplesinclude social networks/medias (e.g., Facebook, TikTok, LinkedIn),transaction graphs (e.g., PayPal and AliPay), co-authorship net-works, airline networks, and power grids. Over such graphs, theMP mechanism undergoes two limitations: (i) homogeneous noderepresentations after merely a few rounds of feature aggregations(i.e., over-smoothing ), and (ii) considerably higher computationoverhead. Apart from this, the majority of GNNs mainly focus ondesigning new feature aggregation rules or model architectures,where the rich structural features of nodes in HDGs are largelyoverlooked and under-exploited.To prevent overfitting and over-smoothing in GNNs, a series ofstudies draw inspiration from Dropout and propose to ran-domly remove or mask edges , nodes , subgraphs from the input graph G during model training. Although such ran-dom operations can be done efficiently, they yield information lossand sub-optimal results due to removing graph elements whileoverlooking their importance to G in the context of tasks. Recently,some researchers applied graph sparsification techniques",
  "KDD 24, August 2529, 2024, Barcelona, SpainYurui Lai et al": "from 2. Large datasets Reddit2, Ogbn-Proteins and Amazon2Mare downloaded from DSpar 3.For all the tested GNN models, we set the number of layers to2, and hidden dimension is in {128, 256}. The weight decay is inthe range from 5 105 to 0, the learning rate is in the interval, and the dropout rate is in {0.1, 0.2, 0.3, 0.4, 0.5}.We set the total number of training epochs in the range {800, 1000,1500, 1800}. For TADA, we set the number of pretraining epochs( in Module I of TADA) to 128. We set the random walk steps to 2, and the weight of RWR-Sketch to 1. reports the settings of parameters used in TADA whenworking in tandem with GNN models: GCN, GAT, SGC, APPNP,and GCNII on 8 experimented datasets.",
  "RELATED WORKS": "Data Augmentation for GNNs. Data augmentation for GNNs(GDA) aims at increasing the generalization ability of GNN mod-els through structure modification or feature generation, whichhas been extensively studied in the literature . Ex-isting GDA works can be generally categorized into two types: (i)rule-based methods and (ii) learning-based methods. More specif-ically, rule-based GDA techniques rely on heuristics (pre-definedrules) to modify or manipulate the graph data. Similar in spirit toDropout , DropEdge and its variants randomly remove or mask edges, nodes, features, subgraphs, or mes-sages so as to alleviate the over-fitting and over-smoothing issues.However, this methodology causes information loss and, hence,sub-optimal quality since the removal operations treat all graphelements equally. In lieu of removing data, Ying et al. proposeto add virtual nodes that connect to all nodes and createnew data samples by either interpolating training samples orhidden states and labels . Besides, recent studies explored extracting additional node features from graph structures.For instance, Song et al. augment node attributes with nodeembeddings from DeepWalk and Velingker et al. expandnode features with random walk measures (e.g., effective resis-tance, hitting and commute times). These approaches enjoy bettereffectiveness at the expense of high computation costs, which areprohibitive on large HDGs.Along another line, learning-based approaches leverage deeplearning for generations of task-specific augmented samples. Moti-vated by the assumption that graph data is noisy and incomplete,graph structure learning (GSL) methods learn bettergraph structures by treating graph structures as learnable parame-ters. As an unsupervised learning method, graph contrastive learning(GCL) techniques have emerged as a promising avenueto address the challenges posed by noisy and incomplete graphdata, enhancing the robustness and generalization of graph neu-ral networks (GNNs) on high-dimensional graphs (HDGs). UnlikeGSL and GCL, extend adversarial training to graphdomains and augments input graphs with adversarial patterns byperturbing node features or graph structures during model training.Rationalization methods seek to learn subgraphs that arecausally related with the graph labels as a form of augmented graphdata, which are effective in solving out-of-distribution and databias issues. Recently, researchers utilized reinforce-ment learning agents to automatically learn optimal augmentationstrategies for different subgraphs or graphs. These learning-basedapproaches are all immensely expensive, and none of them tacklethe issues of GNNs on HDGs as remarked in .",
  "Efficient Topology-aware Data Augmentation for High-Degree Graph Neural NetworksKDD 24, August 2529, 2024, Barcelona, Spain": "there exists a large body of literature on this topic, most of which canbe summarized into three categories as per their adopted method-ology: (i) random walk-based methods, (ii) matrix factorization-based methods, and (iii) deep learning-based models. In particular,random walk-based methods learn node embeddingsby optimizing the skip-gram model or its variants with ran-dom walk samples from the graph. Matrix factorization-based ap-proaches construct node embeddings throughfactorizing node-to-node affinity matrices, whereas capitalize on diverse deep neural network models for node rep-resentation learning on non-attributed graphs. Recent evidencesuggests that using such network embeddings , or resistiveembeddings and spectral embeddings as complementarynode features can bolster the performance of GNNs, but result inconsiderable additional computational costs. Graph Sparsification. Graph sparsification is a technique aimedat approximating a given graph G with a sparse graph containinga subset of nodes and/or edges from G . Classic sparsificationalgorithms for graphs include cut sparsification and spectralsparsification . Cut sparsification reduces edges whilepreserving the value of the graph cut, while spectral sparsifiersensure the sparse graphs can retain the spectral properties of theoriginal ones. Recent studies employ these techniques asheuristics to sparsify the input graphs before feeding them intoGNN models for acceleration of GNN training . In spite of theirimproved empirical efficiency, these works fail to incorporate nodeattributes as well as task information for sparsification. To removetask-irrelevant edges accurately, Zheng et al. and Li et al. cast graph sparsification as optimization problems and apply deepneural networks and the alternating direction method of multipliers,respectively, both of which are cumbersome for large HDGs.",
  "PRELIMINARIES3.1Notations": "Throughout this paper, sets are denoted by calligraphic letters, e.g.,V. Matrices (resp. vectors) are written in bold uppercase (resp.lowercase) letters, e.g., M (resp. x). We use M and M:, to representthe th row and column of M, respectively.Let G = (V, E) be a graph (a.k.a. network), where V is a set of nodes and E is a set of edges. For each edge , E connectingnodes and , we say and are neighbors to each other.We use N () to denote the set of neighbors of node , wherethe degree of (i.e., |N ()|) is symbolized by (). Nodes in G are endowed with an attribute matrix X R, where stands for the dimension of node attribute vectors. The diagonaldegree matrix of G is denoted as D = diag((1), ,()). Theadjacency matrix and normalized adjacency matrix are denoted asA and A = D 1",
  "H(0) = (Xorig) R(1)": "where () stands for a nonlinear activate function, trans() corre-sponds to a layer-wise feature transformation operation (usuallyan MLP including non-linear activation ReLU and layer-specificlearnable weight matrix), and aggr(G, ) represents the operationof aggregating -th layer features H() from the neighborhood alonggraph G, e.g., aggr(G, H( )) = AH(1) in GCN and aggr(G, H( )) =(1)AH(1) + H(0) in APPNP. Note that H(0) = (Xorig) R is the initial node features resulted from a non-linear trans-formation from the original node attribute matrix X using an MLPparameterized by learnable weight orig. As demystified in a num-ber of studies , after removing non-linearity, thenode representations H( ) learned at the -layer in most MP-GNNscan be rewritten as linear approximation formulas:",
  "Although GNNs achieve superb performance by the virtue of thefeature aggregation mechanism, they incur severe inherent draw-backs, which are exacerbated over HDGs, as analysed below": "Inadequate Structure Features. Intuitively, structure featuresplay more important roles for HDGs as they usually encompassrich and complex topology semantics. However, the extant GNNsprimarily capitalize on the graph structure for feature aggregation,failing to extract the abundant topology semantics underlying G.To validate this observation, we conduct a preliminary empiri-cal study with 4 representative GNN models on 2 benchmarkingHDGs in terms of node classification. manifeststhat by concatenating the input attribute matrix X (i.e., X A) withthe adjacency matrix A as node features, each GNN model cansee performance gains (up to 3.17%). In Appendix B.1, we furthertheoretically show that expanding features with A can alleviatethe feature correlation in standard GNNs and additionally in-corporate high-order proximity information between nodes as intraditional network embedding techniques .",
  "()2X": "when poly(A,) = A and P, respectively, both of which are essen-tially irrelevant to node . In other words, the eventual represen-tations of all nodes are overly smoothed with high homogeneity,rendering nodes in different classes indistinguishable and resultingin degraded model performance . Costly Feature Aggregation. Aside from the over-smoothing,the sheer amount of feature aggregation operations of GNNs overHDGs, especially on sizable ones, engender vast computation cost.Recall that each round of feature aggregation in GNNs consumes() time. Compared to normal scale-free graphs with averagenode degrees / = (log()) or smaller, the average node de-grees in HDGs can be up to hundreds, which are approximately(log2()). This implies an ( log2() ) asymptotic cost in totalfor each round of feature aggregation. . . GNNs",
  ": Overview of TADA": "A workaround to mitigate the over-smoothing and computationissues caused by the feature aggregation on HDGs is to sparsifyG by identifying and eradicating unnecessary or redundant edges.However, the accurate and efficient identification of such edgesfor improving GNN models is non-trivial in the presence of nodeattributes and labels and remains under-explored.In sum, we need to address two technical challenges:",
  "Synoptic Overview of TADA": "As illustrated in , TADA acts as a front-mounted stage forMP-GNNs, which compromises two main ingredients: (i) featureexpansion with structure embeddings (Module I), and (ii) topology-and attribute-aware graph sparsification (Module II). The goal ofthe former component is to generate high-quality structure embed-dings Htopo capturing the rich topology semantics underlying G forfeature expansion, while the latter aims to sparsify the structure ofthe input graph G so as to eliminate redundant or noisy topologicalconnections in G with consideration of graph topology and nodeattributes. Module I: Feature Expansion. To be more specific, in Module I,TADA first applies a hybrid sketching technique (Count-Sketch +RWR-Sketch) to the adjacency matrix A of G and transforms thesketched matrix A R ( , typically = 128) into thestructure embeddings Htopo R of all nodes via an MLP:",
  "H(0) = (1 ) Hattr + Htopo R(6)": "The hyper-parameter controls the importance of node topologyin the resulting node representations.Notice that H(0) and related learnable weights are pre-trainedby the task (i.e., node classification) with a single-layer MLP asclassifier (using epochs). In doing so, we can extract task-specificfeatures in H(0) to facilitate the design of Module II, and all theseintermediates can be reused for subsequent GNN training. Module II: Graph Sparsification. Since H(0) captures the task-aware structure and attribute features of nodes in G, Module II canharness it to calculate the centrality values of all edges that assesstheir importance to G in the context of node classification. Given thesparsification raio , the edges with lowest centrality values aretherefore removed from G, whereas those important ones will bekept and reweighted by the similarities of their respective endpointsin H(0). Based thereon, TADA creates a sparsified adjacency matrixdenoted as A as a substitute of A. The behind intuition is thatadjacent nodes with low connectivity and attribute homogeneityare more likely to fall under disparate classes, and hence, theirdirect connection (i.e., edges) can be removed without side effects.Finally, the augmented initial node representations H(0) and thesparsified adjacency matrix A are input into the MP-GNN modelsGNN(, ) for learning final node representations:",
  "Efficient Feature Expansion with StructureEmbeddings": "Recall that in Module I, the linchpin to the feature expansion (i.e.,building structure embeddings Htopo) is A R, a sketch ofthe adjacency matrix A. Notice that even for HDGs, A is highlysparse ( 2) and the distribution of node degrees (i.e., thenumbers of non-zero entries in rows/columns) is heavily skewed,rendering existing sketching tools for dense matrices unsuitable. Inwhat follows, we delineate our hybrid sketching approach speciallycatered for adjacency matrix A. Count-Sketch Method. To deal with the sparsity of A, our first-cut solution is the count-sketch (or called sparse embedding) technique, which achieves (nnz(A)) = () time for computingthe sketched adjacency matrix A R:",
  "which completes the proof": "Recall that the ideal structure embeddings Htopo is obtainedwhen A is replaced by the original adjacency matrix A in Eq. (4),i.e., Htopo = (Atopo). Assume that W = topo is the learnedweights in this case. If we input A = AR to Eq. (4) and assume thenewly learned weight matrix is topo = RW, the resulted structureembeddings Htopo will be similar to the ideal one Htopo accordingto Theorem 4.1, establishing a theoretical assurance for derivinghigh-quality structure embeddings Htopo from A.By Theorem 4.1, we can further derive the following propertiesof A in preserving the structure in G:",
  "Due to the space limit, we defer the proofs to Appendix A.2": "Limitation of Count-Sketch. Despite the theoretical merits ofapproximation guarantees and high efficiency offered by the count-sketch-based approach, it is data-oblivious (i.e., the sketching matrixis randomly generated) and is likely to produce poor results, espe-cially in dealing with highly skewed data (e.g., adjacency matrices).To explain, we first interpret as a randomized clustering member-ship indicator matrix, where (), = 1 indicates assigning eachnode to ()-th (() {1, ,}) cluster uniformly at random.Each diagonal entry in is either 1 or 1, which signifies that thecluster assignment in is true or false. As such, each entry R,represents",
  "(8)": "Accordingly, A, quantifies the strength of connections from tothe -th cluster via its neighbors. Since is randomly generated,distant (resp. close) nodes might fall into the same (resp. different)clusters, resulting in a distorted distribution in A. Optimization via RWR-Sketch. As a remedy, we propose RWR-Sketch to create a structure-aware sketching matrix S R.TADA will combine S with count sketch matrix R to obtain the finalsketched adjacency matrix A:",
  "A = A (R + S),(9)": "where is a hyper-parameter controlling the contribution of theRWR-Sketch in the result. Unlike R, the construction of S is framedas clustering nodes in G into disjoint clusters as per their topo-logical connections to each other in G. Here, we adopt the promi-nent random walk with restart (RWR) model to summarizethe multi-hop connectivity between nodes. To be specific, we con-struct S as follows:",
  "N( )(,)(12)": "Denote by G = (V, E) this edge-reweighted graph. The subse-quent task is hence to sparsify G.In the literature, a canonical methodology to create thesparsified graph G is to sample edges with probability proportionalto their effective resistance (ER) values and add them withadjusted weights to G. Theoretically, G is an unbiased estimationof the original graph G in terms of the graph Laplacian and",
  "x R, (0, 1](1 )xLx xLx (1 + )xLx": "with a probability of at least 1 . First, this approach fails toaccount for node attributes. Second, the computation of the ER ofall edges in G is rather costly. Even the approximate algorithms struggle to cope with medium-sized graphs. Besides, theedge sampling strategy relies on a large as it will repeatedly pickthe same edges.",
  "EXPERIMENTS5.1Experimental Setup": "Datasets. lists the statistics of 8 benchmark HDGs (/ 18) tested in our experiments, which are of diverse types and variedsizes. |Y| symbolizes the distinct number of class labels of nodesin G. The homophily ratio (HR) of G is defined as the fraction ofhomophilic edges linking same-class nodes . We refer to agraph with 0.5 as homophilic and as heterophilic if <0.5. Particularly, datasets Photo , WikiCS , Reddit2 ,and Amazon2M are homophilic graphs, whereas Squirrel ,Penn94 , Ogbn-Proteins , and Pokec are heterophilicgraphs. Amazon2M and Pokec are two large HDGs with millionsof nodes and tens of millions of edges. More details of the datasetsand train/validation/test splits can be found in Appendix D.1. Baselines and Configurations. We adopt five popular MP-GNNarchitectures, GCN , GAT , SGC , APPNP , andGCNII as the baselines and backbones to validate TADA in semi-supervised node classification tasks (.2). To demonstratethe superiority of TADA, we additionally compare TADA againstother GDA techniques in .3 its variants with other featureexpansion and graph sparsification strategies in .4. Theimplementation details and hyper-parameter settings can be foundin Appendix D.2. All experiments are conducted on a Linux machinewith an NVIDIA Ampere A100 GPU (80GB RAM), AMD EPYC 7513CPU (2.6 GHz), and 1TB RAM. Source codes can be accessed at",
  ": Maximum GPU Memory Usage": "OOM represents that the model fails to report results due to theout-of-memory issue. It can be observed that TADA consistentlyimproves the baselines in accuracy on both homophilic and het-erophilic graphs in almost all cases. Notably, on the Squirrel dataset,the five backbones are outperformed by their TADA counterpartswith significant margins of 17.29%-20.14% in testing accuracy. Thereason is that Squirrel is endowed with uninformative nodal at-tributes, and by contrast, its structural features are more conducivefor node classification. By expanding original node features withhigh-quality structure embeddings (Module I in TADA), TADA isable to overcome such problems and advance the robustness andeffectiveness of GNNs. In addition, on Reddit2 and Ogbn-Proteinswith average degrees (/) over hundreds, TADA also yields pro-nounced improvements in accuracy, i.e., 2.28% and 2.94% for GCN,as well as 1.96% and 2.23% for GCNII, respectively. This demon-strates the effectiveness of our graph sparsification method (ModuleII in TADA) in reducing noisy edges and mitigating over-smoothingissues particularly in graphs (Reddit2 and Ogbn-Proteins) consistingof a huge number of edges (analysed in .3). On the restHDGs, almost all GNN backbones see accuracy gains with TADA.Two exceptions occur on heterophilic HDG Pokec, where GCN andSGC get high standard deviations (1.36% and 5.56%) in accuracywhile GCN+TADA and SGC+TADA attenuate average accuraciesbut increase their performance stability. Efficiency. To assess the effectiveness of TADA in the reduction ofGNNs feature aggregation overhead on HDGs, Figures 3, 2, and 4plot the inference times and training times per epoch (in millisec-onds), as well as the maximum memory footprints (in GBs) needed",
  "*Best is bolded and runner-up underlined": "by four GNN backbones (GCN, SGC, APPNP, and GCNII) and theirTADA counterparts on a heterophilic HDG Ogbn-Proteins and ahomophilic HDG Reddit2. We exclude GAT as it incurs OOM errorson these two datasets, as shown in . From , we notethat on Ogbn-Proteins, TADA is able to speed up the inferencesof GCN, APPNP, and GCNII to 121.7, 198.2, and 86 faster, re-spectively, whereas on Reddit2 TADA achieves comparable runtimeperformance to the vanilla GNN models. This reveals that Reddit2and Ogbn-Proteins contains substantial noisy or redundant edgesthat can be removed without diluting the results of GNNs if TADAis included. Apart from the inference, TADA can also slightly ex-pedite the training in the presence of Module I and Module II (see), indicating the high efficiency of our techniques developedin TADA. In addition to the superiority in computational time, itcan be observed from that TADA leads to at least a 24% and16% reduction in memory consumption compared to the vanillaGNN models.In a nutshell, TADA successfully addresses the technical chal-lenges of GNNs on HDGs as remarked in .3. Besides, werefer interested readers to Appendix D.3 for the empirical studiesof TADA on low-degree graphs.",
  "Comparison with GDA Baselines": "This set of experiments evaluates the effectiveness TADA in improv-ing GNNs performance against other popular GDA techniques:DropEdge and GraphMix . presents the test accu-racy results, training and inference times per epoch (in milliseconds)achieved by two GNN backbones GCN and GCNII and their aug-mented versions on Ogbn-Proteins and Reddit2. We can make thefollowing observations. First, TADA +GCN and TADA +GCNII dom-inate all their competitors on the two datasets, respectively, in terms of classification accuracy as well as training and inference efficiency.On Ogbn-Proteins, we can see that the classification performance ofGCN+DropEdge, GCNII+DropEdge, and GCNII+GraphMix is eveninferior to the baselines, while taking longer training and inferencetimes., which is consistent with our analysis of the limitations ofexisting GDA methods on HDGs in Sections 1 and 2.",
  "Ablation Study": "presents the ablation study of TADA with GCN as thebackbone model on Reddit2 and Ogbn-Proteins. More specifically,we conduct the ablation study in three dimensions. Firstly, westart with the vanilla GCN and incrementally apply componentsCount-Sketch, RWR-Sketch (Module I), and our graph sparsificationtechnique (Module II) to the GCN. Notice that Module II is builton the output of Module I, and, thus, can only be applied after it.From , we can observe that each component in TADA yieldsnotable performance gains in node classification on the basis of theprior one, which exhibits the non-triviality of the modules to theeffectiveness of TADA.On the other hand, to demonstrate the superiority of our hy-brid sketching approach introduced in .2, we substituteCount-Sketch and RWR-Sketch in Module I with random projec-tion , -SVD , DeepWalk , node2vec , and LINE ,respectively, while fixing Module II. That is, we employ the randomprojections of adjacency matrix A, the top- singular vectors (as in), or the node embeddings output by DeepWalk, node2vec, andLINE as A for the generation of structure embeddings. As reportedin , all these five approaches obtain inferior classificationresults compared to TADA with Count-Sketch + RWR-Sketch onReddit2 and Ogbn-Proteins.Finally, we empirically study the effectiveness of our topology-and attribute-aware sparsification method in .3 (Mod-ule II) by replacing it with random sparsification(RS), -NeighborSpar , SCAN and the DSpar . Random sparsificationremoves edges randomly, and -Neighbor Spar samples atmost edges for each neighbor. SCAN removes the edges withthe lowest modified Jaccard similarity, while Dspar identifies thesubset of dropped edges based on their estimated ER values in theoriginal unweighted graph. shows that all these four vari-ants are outperformed by TADA by a large margin. On Reddit2 andOgbn-Proteins, TADA takes a lead of 0.89% in classification accuracycompared to its best variant with -Neighbor Spar.",
  "Hyper-parameter Analysis": "This section empirically studies the sensitivity of TADA to hyper-parameters including the weight for structure embeddings (Eq. (6)),structure embedding dimension (.2), RWR-Sketch weight (Eq. (9)), and sparsification ratio (.3), on two datasetsOgbn-Proteins and Reddit2.(a) depict the node classification accuracy results ofGCN+TADA when varying in {4, 16, 64, 128, 256}. We can makeanalogous observations on Reddit2 and Ogbn-Proteins. That is, theperformance of GCN+TADA first improves as is increased from 4to 128 (more structure features are captured) and then undergoes adecline when = 256, as a consequence of over-fitting.In (b), we plot the node classification accuracy valuesattained by GCN+TADA when is varied from 0 to 1.0. Note thatwhen = 0 (resp. = 1.0), the initial node features H(0) definedin Eq. (6) will not embody structure features Htopo (resp. nodeattributes Hattr). It can be observed that GCN+TADA obtains im-proved classification results on Reddit2 when varying from 0 to0.9, whereas its performance on Ogbn-Proteins constantly down-grades as enlarges. The degradation is caused by its heterophilic",
  ": The final node representations of Photo obtainedby GCN and GCN+TADA. Nodes are colored by their labels": "property and using its topological features for graph sprasification(.3) will accidentally remove critical connections.From (c), we can see that the best performance is achievedwhen = 0.1 and = 0.3 on Reddit2 and Ogbn-Proteins, respectively,which validates the superiority of our hybrid sketching approachin .2 over Count-Sketch or RWR-Sketch solely.As displayed in (d), on Reddit2, we can observe thatGCN+TADA experiences an uptick in classification accuracy whenexcluding 10%-70% edges from G using Module II in TADA, followedby a sharp downturn when > 70%. On Ogbn-Proteins, the bestresult is attained when = 0.9, i.e., 90% edges are removed fromG. The results showcase that Module II can accurately identify upto 70%-90% edges from G that are noisy or redundant and obtainperformance enhancements.",
  "Visualization of TADA": "visualizes (using t-SNE ) the node representations ofthe Photo dataset at the final layers of GCN and GCN+TADA. Nodeswith the same ground-truth labels will be in the same colors. In(b), we can easily identify 8 classes of nodes as nodes withdisparate colors (i.e., labels) are all far apart from each other. Incomparison, in (a), three groups of nodes with differentcolors are adjacent to each other with partial overlapping and somenodes even are positioned in other groups and distant from theirtrue classes. These observations demonstrate that TADA can en-hance the quality of nodes representations learned by GCN, andthus, yield the higher classification accuracy, as reported in .",
  "CONCLUSION": "In this paper, we present TADA, an efficient and effective data aug-mentation approach specially catered for GNNs on HDGs. TADAachieves high result utility through two main technical contri-butions: feature expansion with structure embeddings via hybridsketching, and topology- and attribute-aware graph sparsification.Considerable experiments on 8 homophilic and heterophilic HDGshave verified that TADA is able to consistently promote the per-formance of popular MP-GNNs, e.g., GCN, GAT, SGC, APPNP, andGCNII, with matching or even upgraded training and inferenceefficiency.",
  "Songtao Liu, Rex Ying, Hanze Dong, Lanqing Li, Tingyang Xu, Yu Rong, PeilinZhao, Junzhou Huang, and Dinghao Wu. 2022. Local augmentation for graphneural networks. In ICML. 1405414072": "Xin Liu, Mingyu Yan, Lei Deng, Guoqi Li, Xiaochun Ye, Dongrui Fan, ShiruiPan, and Yuan Xie. 2022. Survey on graph neural network acceleration: Analgorithmic perspective. arXiv preprint arXiv:2202.04822 (2022). Zirui Liu, Kaixiong Zhou, Zhimeng Jiang, Li Li, Rui Chen, Soo-Hyun Choi, andXia Hu. 2023. DSpar: An Embarrassingly Simple Strategy for Efficient GNNtraining and inference via Degree-based Sparsification. TMLR (2023).",
  "Daniel A Spielman and Shang-Hua Teng. 2004. Nearly-linear time algorithmsfor graph partitioning, graph sparsification, and solving linear systems. In STOC.8190": "Rakshith S Srinivasa, Cao Xiao, Lucas Glass, Justin Romberg, and Jimeng Sun.2020. Fast graph attention networks using effective resistance based graphsparsification. arXiv preprint arXiv:2006.08796 (2020). Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and RuslanSalakhutdinov. 2014. Dropout: a simple way to prevent neural networks fromoverfitting. The journal of machine learning research 15, 1 (2014), 19291958. Jonathan M Stokes, Kevin Yang, Kyle Swanson, Wengong Jin, Andres Cubillos-Ruiz, Nina M Donghia, Craig R MacNair, Shawn French, Lindsey A Carfrae,Zohar Bloom-Ackermann, et al. 2020. A deep learning approach to antibioticdiscovery. Cell 180, 4 (2020), 688702.",
  "Mengying Sun, Jing Xing, Huijun Wang, Bin Chen, and Jiayu Zhou. 2021. MoCL:Data-driven Molecular Fingerprint via Knowledge-aware Contrastive Learningfrom Molecular Graph. SIGKDD (2021)": "Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei.2015. Line: Large-scale information network embedding. In Proceedings of the24th international conference on world wide web. 10671077. Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Mehdi Az-abou, Eva L Dyer, Remi Munos, Petar Velikovi, and Michal Valko. 2021. Large-Scale Representation Learning on Graphs via Bootstrapping. In ICLR.",
  "BMODULE I: FEATURE EXPANSIONB.1Theoretical Analysis of Expanding NodeFeatures with Adjacency Matrices": "Let be the feature space in standard GNNs. As per Eq. (2), canbe formulated by the multiplication of graph structure matrices(e.g., normalized adjacency matrix A and transition matrix P) andnode attribute matrix, e.g., ( ) = ( A) X, where () denotesa polynomials -order term . As pinpointed in ,as Z increases, the feature subspace (+) will be linearlycorrelated with ( ), i.e., there exist a weight matrix W such that( )W (+) 2 0. Recall that in standard GNNs, all featuresubspaces usually share common parameter weights. For example,given two linearly correlated feature subspaces, () and (), theoutput of a GNN C R (e.g., node-class predictions and is thenumber of classes) is expressed by C = (() + ()) ,where is a transformation weight matrix. However, provedthat C can be solely represented by either () or () ,indicating that standard GNN models have redundancy and limitedexpressiveness of the feature space.By concatenating the original feature subspaces ( ) with adja-cency matrix A, Theorem B.1 shows that the GNN models can bemore accurate in the recovery of the ground-truth Cexact R",
  "(( ) A) Cexact < ( ) Cexact": "Proof. Given any node feature matrix R , the goal oflinear regression problem = Cexact R is to find a weightmatrix R such that approximates Cexact with minimalerror. Let UV be the exact full singular value decomposition(SVD) of . Since the inverse of UV is V1 U and UU =VV = I, we have",
  "UU I Cexact= ( ) Cexact .(18)": "When = ( ), we have = and is a thin matrix. Asshown in the proof of Theorem 4.2 in , UU will be ratherdense (far from the identity matrix), and hence, rendering Cexactinaccurate. Also, by Eq. (18), the approximation error is ( ) Cexact , which is large since is large.By contrast, when = ( ) A, = + and we have",
  "return A A;": "Complexity Analysis of Count-Sketch. Recall that A is a sparsematrix containing non-zero entries and R = where is a diagonal matrix and each column in R solely has asingle non-zero entry. Therefore, the sparse matrix multiplicationAR in Eq. (7) (Line 1 in Algorithm 1) consumes () time. Complexity Analysis of RWR-Sketch. According to Lines 215in Algorithm 1, the sorting at Line 2 takes ( + |C| log) timewhen using the max-heap. Notice that each of iterations at Lines6-7 conducts sparse matrix multiplication, where P contains non-zero entries and is of size |C|. Hence, Lines 4-8 consume( |C|) time. Line 9 needs a full sorting of set C, which incursan (|C| log(|C|)) cost. As for Lines 12-14, for each node, thecost of Lines 13-14 is (|C|), resulting in a total cost of ( |C|).The normalization of the matrix S requires () time aseach column has only one non-zero element. After constructing Swherein each column has only one non-zero entry, the sketchingoperation A S is a sparse matrix multiplication, which can bedone in () time. Therefore, RWR-Sketch takes ( + |C|)time in total.The overall computational cost of Algorithm 1 is bounded by( |C|), which can be reduced to () since || can be regardedas a constant.",
  "Complexity Analysis. Since initial node representations H(0)": "are a matrix, the edge reweighting at Line 2 in Algorithm 2takes () time in total. Both the computation of degrees ofnodes at Lines 4-5 and the calculation of edge centrality valuesat Lines 6-7 require () time. The partial sorting for extractingthe edges with smallest centrality values at Line 8 consumes( + log) time. Lines 9-10 needs () time for processingall the edges. Therefore, the total computational complexityis bounded by ( + log).",
  "DADDITIONAL EXPERIMENTSD.1Dataset details": "presents the full statistics as well as the details of thetrain/validation/test splits of the 8 datasets in our experiments.For a graph with node class labels Y, we define its homophilyratio (HR) as the fraction of homophilic edges linking same-classnodes : = |(, )|(, ) E = |/|E|.Photo is a segment of the Amazon co-purchase graph, wherenodes represent goods and edges indicate that two goods are fre-quently purchased together. The node features are extracted fromthe product reviews and node class labels correspond to productcategories.WikiCS is collected from Wikipedia, where nodes are wikipages and edges represent hyperlinks between pages. The nodeattributes are word embeddings constructed from the articles andnode class labels correspond to 10 branches of computer science.Reddit2 is constructed based on Reddit posts. The edge be-tween two nodes (i.e., posts) indicates that the same user comments",
  "=256,=0.5, =0.1,lr=0.001,weight-decay=1e-5,dropout=0.1": "on both of them. Node class labels are the communities wherethe nodes are from, and the node attributes are off-the-shelf 300-dimensional GloVe CommonCrawl word vectors of the post.Amazon2M is an Amazon product co-purchasing networkwhere nodes and edges represent the products and co-purchasingrelationships between products, respectively. The node attributesare the bag-of-words of the description of products and node classesrepresent product categories.Squirrel is a network consisting of Wikipedia pages on squirreltopics, respectively. Nodes are Wikipedia articles and edges are hy-perlinks between to pages. Node attributes of Squirrel are a groupof selected noun from the article. Nodes are divided into differentclasses based on their traffic.Penn94 is a subgraph extracted from Facebook in which nodesrepresent students and edges are their friendships. The nodal at-tributes include major, second major/minor, dorm/house, year, andhigh school. The node class labels are students genders.Ogbn-Proteins is a protein association network. Nodes repre-sent proteins, and edges are associations between proteins. Edgesare multi-dimensional features, where each dimension is the ap-proximate confidence of different association types in the range of. Each node can carry out multiple functions,and each function represents a label. A multi-label binary classification task on thisgraph is to predict the functions of each node (protein).Pokec is extracted from a Slovak online social network, whosenodes correspond to users and edges represent directed friendships.Node attributes are constructed from users profiles, such as ge-ographical region and age. The users genders are taken as nodeclass labels.Cora is a citation network where nodes represent papers andnode attributes are bag-of-words representations of the paper.arXiv-year is also a citation network. Nodes stand for papers,and edges represent the citation relationships between papers. Foreach node (i.e., paper), its attributes are the Word2vec representa-tions of its title and abstract. Node class labels correspond to thepublished years of papers.",
  "D.3Performance of TADA on Low-degreeGraphs (LDGs)": "shows the node classification results of five GNN mod-els and their TADA-augmented counterparts on two low-degreegraphs Cora and arXiv-Year. Particularly, on Cora with averagenode degree 2.0, we can observe that TADA slightly degrade theclassification performance of most GNN backbones except SGC.In contrast, on graph arXiv-Year with higher average degree (6.9),TADA can promote the classification accuracy of four GNN models(GCN, SGC, APPNP, and GCNII) with remarkable gains and leadto performance degradation for GAT. The observations indicatethat TADA is more suitable for GNNs over HDGs as it will causeinformation loss and curtail the classification performance of GNNson graphs with scarce connections."
}