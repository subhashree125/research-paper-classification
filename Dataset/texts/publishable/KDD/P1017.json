{
  "Abstract": "In this paper, we study cascading failures in power grids through thelens of information diffusion models. Similar to the spread of rumorsor influence in an online social network, it has been observed thatfailures (outages) in a power grid can spread contagiously, driven byviral spread mechanisms. We employ a stochastic diffusion modelthat is Markovian (memoryless) and local (the activation of onenode, i.e., transmission line, can only be caused by its neighbors).Our model integrates viral diffusion principles with physics-basedconcepts, by correlating the diffusion weights (contagion probabili-ties between transmission lines) with the hyperparametric Informa-tion Cascades (IC) model. We show that this diffusion model can belearned from traces of cascading failures, enabling accurate model-ing and prediction of failure propagation. This approach facilitatesactionable information through well-understood and efficient graphanalysis methods and graph diffusion simulations. Furthermore, byleveraging the hyperparametric model, we can predict diffusionand mitigate the risks of cascading failures even in unseen gridconfigurations, whereas existing methods falter due to a lack oftraining data. Extensive experiments based on a benchmark powergrid and simulations therein show that our approach effectivelycaptures the failure diffusion phenomena and guides decisions tostrengthen the grid, reducing the risk of large-scale cascading fail-ures. Additionally, we characterize our models sample complexity,improving upon the existing bound.",
  "Introduction": "Interest in information diffusion and influence, initially motivatedby social networks and viral marketing , has expanded signif-icantly in recent years. Researchers have recognized the potentialapplicability of these methods beyond the social media sphere, asdiffusion phenomena and algorithms for understanding viral spread,leveraging its reach or mitigating its risks, are now being applied inpublic health and epidemiology , transportation and logistics , internet virus propagation and cyber-security , complexbiological systems , or ad-hoc communication networks .Besides these diverse fields, one critical domain stands out, per-taining to cascading failures in power transmission networks, whereviral spread phenomena can exert a significant impact, potentiallydisrupting our society. Just as rumors or influence may spread inan online social network, it has been observed and confirmed byresearch that failures (outages) in a power grid are contagious andcan be described by viral spread mechanisms . Indeed, dueto recent events like the 2021 Texas blackouts , the issue of cas-cading grid failures has made headlines and gained a lot of attention. In short, a cascading failure in a power grid represents theevent of successive interdependent failures of components in thesystem. Usually initiated by one or a few source outages, due to en-dogenous or exogenous disturbances, they propagate in a relativelyshort lapse of time, potentially leading to large blackouts .The analysis and prediction of such events is challenging be-cause cascading failures are in general non-contiguous / non-localwith respect to the physical topology of the power grid. This meansthat a failure of a specific transmission line may cause the failureof another one that is geographically distant and not directly cou-pled , e.g., may be even located hundreds of kilometers away, asdocumented in the Western US blackout of July 2, 96 . This moti-vated researchers to study analytical frameworks that are based ongraphs whose topology is not necessarily close to that the physicalgrid . In particular, data-driven approaches were proposed,leading to the development of graph models of the observed in-teractions among components of the power grid .See for a recent survey on cascading failure analysis in powergrids, based on interaction graphs, which model power grids withthe nodes representing electrical components of interest such asbuses or transmission lines and the edges representing interactionsobserved in known cascades of failures. However, constructing interaction / diffusion graphs that accu-rately capture cascading patterns hinges on two crucial factors:sufficient data quantity and high data quality. Only then can net-work analysis effectively reveal these patterns and ultimately guidedecisions for mitigating cascading blackouts (e.g., by upgradingselected transmission lines). Yet real-world historical data of cas-cading failures is hard to obtain, and the alternative to it, comingfrom grid simulators (e.g., quasi-steady-state or dynamic powernetwork models ) may introduce artificial biases and errors.Furthermore, a significant limitation of these methods is the inabil-ity to adapt to unseen grid configurations. They rely on past data,",
  "Bin Xiang, Bogdan Cautis, Xiaokui Xiao, Olga Mula, Dusit Niyato, and Laks V.S. Lakshmanan": "Margaret J Eppstein and Paul DH Hines. 2012. A random chemistry algorithmfor identifying collections of multiple contingencies that initiate cascading failure.IEEE Transactions on Power Systems 27, 3 (2012), 16981705. Christophe Fraser, Steven Riley, Neil M Ferguson, Peter C Gandy, Thomas HRyan, Donald J Read, and et al. 2007. Tracking Ebola Epidemics with Big Data:Algorithms and Insights. PLoS Med 4, 9 (2007), e311.",
  "We revisit the hyperparametric IC model of , adapting it tothe spread of failures in a diffusion graph having as nodes thetransmission lines of the power grid": "We present a learning algorithm for this model, based on knowncascades of failure traces. Initializing with a complete graphrepresenting all possible connections (accounting for non-localpropagation), the algorithm learns diffusion probabilities overedges, effectively sparsifying the network to reflect the mostinfluential interactions. Extensive experiments, based on a benchmark power grid andsimulations therein, show that our approach can accuratelymodel and predict how failures may propagate, and that its pre-dictions can guide the decisions to consolidate the grid and thusreduce the risk of large cascading failures. Furthermore, by leveraging the hyperparametric model, we showthat we can make diffusion predictions and mitigate the risks ofcascading failures even in unseen grid configurations, for whichexisting methods cannot apply due to lack of training data.",
  "The state of the art for analyzing and learning cascading failures inpower grids can be mainly categorized into deterministic and sto-chastic ones. We note that while deep learning has shown promise": "in predicting cascading failures (e.g., ), these approaches lackinterpretability and rigorous theoretical performance guarantees.Deterministic methods are mainly based on the OPA (ORNL-PSerc-Alaska) model , which simulates the power systemsresponse after contingencies of transmission line failures. Theycan provide detailed processes, tracing the cascading failures inthe system, but they usually incur performance issues due to thecomputation of optimal power flow.Stochastic approaches adopt different types of models based onMarkov chains or statistical learning. The main idea of these ap-proaches is to estimate an influence (or contagion) probability ma-trix. Specifically, traditional Markov chain-based approaches view a cascading process as a sequence of transitions amongstates, where each state encapsulates the status of a group of nodes.The effectiveness of such an approach is impacted by the design ofthe state space and by the combinatorial characteristics of states.Specifically, by design, the individual node-to-node transition prob-abilities are not directly estimated in such approaches. To addressthis issue, branching processes (a variant of Markov chains) havebeen used in , where each failure in one stage is assumedto generate some random failures in the subsequent stage, followingan offspring distribution such as Poisson. Finally, the more recentstatistical learning approaches apply non-parametric regres-sion models to fit the cascading processes, based on given historicaltraces of cascading failures.As mentioned previously, a major limitation of the aforemen-tioned approaches is their inability to adapt to unseen grid config-urations. More precisely, such models can only be applied to thesame fixed power system configuration that produced the cascadinglogs. As shown in our experiments, any small change or perturba-tion in the power system may drastically impact their effectiveness.This poses a significant challenge, since some decisions may betime-critical and cannot be delayed until new cascading traces areproduced (e.g., by grid simulations) to retrain upon.Information diffusion and influence study was pioneered by theseminal works of . Diffusion models such as IndependentCascades (IC), Linear Threshold (LT), and generalizations thereofwere introduced in . The problem of Influence Maximization(IM) selecting a set of seed nodes that maximize the expectedspread in a diffusion network under a certain diffusion model wasalso introduced and studied extensively. See the recent survey for a detailed review of the IM literature.While classic IM methods assume that the underlying diffusionnetwork, including the influence probabilities associated with eachconnection, is known, this assumption rarely holds in practice. Arich body of work has been devoted to learning the underlyingdiffusion network when historical cascades of propagation tracesare available . Often, the drawback of such methods is their high sample-comple-xity, as discussed in , which proposes an alternative approach,with lower sample-complexity, based on a hyperparametric ICmodel. They assume nodes / edges have features, which inducecorrelations between the diffusion probabilities of different edges.This allows to minimize training data requirements for robust pre-dictions, as also shown in .Our study starts with the thesis that methods of informationdiffusion analysis from the rich IM literature may be applicable to",
  "Predicting Cascading Failures with a Hyperparametric Diffusion Model": "When S+, |V| > 1, rank( ) > rank( T). Then, such that T( T) = 0 and T( ) 0. Then TQ 0,Q is not negative semi-definite, thus ( | , ,) is not concave.For a special sample, i.e., when |V| = 1, S+, we study thedefiniteness of Q:",
  "Diffusion Model3.1Classic Diffusion Models": "We start with the premise that CFs bear resemblance to the well-studied processes of information diffusion in generic settings, par-ticularly using the classic Independent Cascade (IC) model ,which we briefly review next. IC Model: In the classic IC model, we have a graph G(V, E) anda probability associated with every edge (, ) E. Diffusionproceeds in discrete time steps. At time = 0, only the seed nodes,the initiators of a cascade, are active. Once activated, a node remainsactive. Every node that became active at time > 0 has one chanceto activate each of its inactive neighbors , with probability .The propagation terminates when a fixpoint is reached and no morenodes can become active. The classic IM problem aims at finding seed nodes that lead to the maximum number of activated nodes inexpectation, also known as (expected) spread. Hyperparametric IC Model: An instance of IC model is charac-terized by the edge probabilities. Since these parameters may not beknown or learned exactly, there has been an investigation of IM overa set of model instances corresponding to the uncertainty in ourknowledge of edge probabilities . A particularly elegantapproach among those is the so-called hyperparametric IC model. It postulates a vector of features associated with each nodewhich are relevant to the node exerting and experiencing influence.Examples of such features include age, gender, profession, degree,pagerank, etc. The influence between a pair of nodes is a functionof the node features and a global low dimensional hyperparameter. More precisely, the hyperparametric model : restricts the IC model by imposing correlations among edge prob-abilities. Each edge (, ) E is associated with a -dimensionalvector , where which encodes the features",
  "Adaptation to Power Grids": "Diffusion graph: Earlier attempts to model power grids as graphs,with nodes representing generators or buses and edges represent-ing transmission lines were found to be ineffective in predictingfailures as these can propagate in a manner that transcends the gridtopology . As such, we do not discuss these approaches further.Network models where nodes represent transmission lines andedges are based on observed cascades of failures have been foundto be more successful in modeling and predicting failure cascades. However, unlike in applications such as social networks, thediffusion graph underlying cascading failures in power grids is notexplicitly available and must be learned from available cascades.In these applications, a set of cascades K is available, where foreach K, the cascade consists of a sequence of sets of nodes (i.e.,transmission lines) (V0, V1, ..., VT ), where V is the set of nodesthat failed at time [0, T]. It is assumed that any node that failedat time could influence any node that failed at the next time step + 1. With no further information available on failure propagation,we allow the possibility that the diffusion graph contains an edge(, ), for all V and V+1. Given a set of such cascades K,we learn the most likely diffusion graph that explains all observedfailure cascades using a learning algorithm, which tries to sparsifythe above graph. Choice of Model: As discussed in , most prior researchon cascading failures (CFs) uses statistical or probabilistic frame-works, such as branching processes or Markov chains, to analyzeinteractions. However, these methods often work only for specificcases, rendering them less reliable for broader applications. Weposit that hyperparametric modeling can offer a more effectivesolution, as evidenced by its potential in studies of information dif-fusion in social networks . Nevertheless, CFs in power gridspose unique challenges: non-local cascades, complex networks, andintricate physical properties. To address these issues, we leveragethe physical and topological features of power grids to unravel theintricate dynamics of cascading failures. We propose an adaptationof the hyperparametric model that quantifies influence probabilitiesusing these features, coupled with data from observed CF events. Influence probability: Various functions can be used to modelthe relationship between features, the hyperparameter, and thelikelihood of influence. We adopt here the logistic function, whichis frequently used in the existing literature. Let denote the in-fluence probability from line to , the vector of features of theendpoints (transmission lines) and , and the hyperparametervector. Then, the influence probability function is",
  "S+ = {(, ) | V,, V,+1}, K, T.(2)": "That is, in cascade , each V,+1 at time + 1 is activated dueto the set of nodes V, which became active at time . Note thatany node V, could have activated V,+1, we have noinformation on exactly which one, and we use this pairwise notationto adhere to the standard notation where every node is atomicand does not contain sets. For convenience, we sometimes abusenotation and denote the positive samples as S+ = {(V,, ) | V,+1}, K, T.This setup also implicitly includes a set S of negative samples,representing node pairs that do not influence each other within agiven time step:",
  "S = {(, ) | V,, +1=0 V, }, K, T.(3)": "Again, we sometimes abuse notation for convenience and writethe negative sample as S = {(V,, ) | +1=0 V, }, K, T. Let = (V, ) S+, be a positive sample, whereV is the set of nodes that may influence in sample . Based onEq. (1), the likelihood of a positive sample in one diffusion step, forany S+, is:",
  "For any negative sample = (V, ) S, , the likelihood is1 ( | , )": "Estimator: Given this CF model, the traditional Maximum Like-lihood Estimation (MLE) method can be applied to estimate thehyperparameter. The idea is to maximize the likelihood betweenthe predictions of the hyperparametric model and the ground truthof event data.Based on Eq. (4), the likelihood of one diffusion step can beformulated as S+ ( | , ) S (1 ( | , )). Let =IS+ () denote an indicator function which equals 1 if S+, 0otherwise. Then, the log-likelihood, known as the cross-entropy is:",
  "Learnability": "In this section, we study the Probably Approximately Correct (PAC)learnability of our model, drawing on the theory of samplecomplexity analysis for the MLE approach. The log-likelihoodfunction family w.r.t. the cascading failure model, which is initiallypart of an infinite hypothesis space, is transformed into a finitehypothesis space using covering theory and Lipschitz continuityanalysis for the diffusion probability function. This transformationallows us to examine the complexity more effectively. We examinethe conditional concavity of the empirical log-likelihood function,which paves the way for applying Rademacher complexity to assessthe models sample complexity. Rademacher complexity is crucialas it evaluates the expressiveness of a function class by its abilityto fit a hypothesis set to a random distribution, which is closelylinked to sample complexity .Building on this foundation, we derive the sample complexityfor our model. Detailed proofs of these theoretical findings can befound in the appendix.",
  "Definition 4.1 (Agnostic PAC learnability )": "A hypothesis class H is PAC learnable if, for any , (0, 1), andfor any distribution D over the product space of examples X andlabels Y, there exists a polynomial function H and a learningalgorithm A, such that when A is run on H(,) i.i.d. samplesfrom D, it produces a hypothesis that, with probability 1 ,achieves a loss LD () that is within of the minimum possibleloss over all hypotheses in H. Following , we assume that the influence probability is restricted to the interval , where (0, 0.5) is a constantthat controls the precision of our estimates. W.l.o.g., we also assumethat each node in the network has at least one significant feature.Together, these assumptions allow us to bound the magnitude ofthe influence weights, which enables us to define the range of thehypothesis space H.",
  ")-Lipschitz w.r.t. -norm 1, where = dim()": "To examine the sample complexity of our model, we first investi-gate whether the log-likelihood function is concave. The idea is thatif it is concave, then we can derive the sample complexity basedon optimization theory . If not, then a canonical approach isto utilize the Rademacher complexity framework. The followinglemma settles the question.",
  "Lemma 4.3. The log-likelihood for one sample, i.e., ( | , ,),is concave in if the sample is either negative with = 0, or positivewith = 1 and |V| = 1, S+. Otherwise, it is not concave": "It follows from Lemma 4.3 that, in general, the expected log-likelihood LS( | ) over S is non-concave. More results on thegeneral and conditional concavity analysis can be found in theappendix.Given this, we resort to Rademacher bound theory to characterizethe sample complexity of our model . Let F be the family oflog-likelihood functions ( | , ,) on hypothesis H, definedas:F def= { : (,) ( | , ,) | H}.",
  "Overall Solution": "establishes that the expected log-likelihood is in generalnon-concave. The standard solution strategies for this type of prob-lem include stochastic gradient descent (SGD) or a quasi-Newtonapproach such as L-BFGS-B . In our work, we adopt the latter,as it offers faster convergence and can be more easily parallelized.From a computational standpoint, the availability of an analyticgradient enables us to accurately compute values for either theentire dataset, or for large batches thereof in parallel. This com-putation is facilitated by a GPU-accelerated workflow, designedto leverage the structural properties of the gradient and of thedataset. Specifically, observe that the gradient of the log-likelihoodfunctions mainly consists of terms such as ( | , ) and ().With fixed in each iteration, the terms [ T]V2 can bepre-computed. The subsequent computation of the terms ( | , )and () involves simple operations such as summation or multi-plication with V. Consequently, the gradient ( | , ,)for individuals, as well as LS( | ) for entire sample set, canbe computed in parallel.Moreover, as discussed in Sections 3 and 4, the interactionswithin our model form a complete graph. For each cascade K,a given sample ((V, ), 1) S implies the existence of samples{((V, ), 0) | V\\V {}}, for all s.t. ((V, ), 1) S.Therefore, |S| |K||V|. When accounting for the |V| contin-gencies cascading (see , 1), the size of is O(|V|+1).Despite the relative sparsity of samples, they can be efficiently en-coded and aggregated using the sample key (V, ) into differentmatrices for optimized storage and computation.Algorithm 1 summarizes the overall workflow for learning aHyperparametric diffusion model for predicting Cascading Failures(denoted HCF).",
  "Experiments": "Our approach was implemented in Python using GPUs, with theOpenCL standard1. The experiments were mainly performed ona laptop Intel(R) Core(TM) i7-8550U CPU @ 1.80GHz with 16 GBmemory (denoted L) and a server with AMD EPYC 7713 processorand 128 GB memory (denoted S). Datasets. Acquiring real-world cascading failure datasets in thepower systems community is challenging, due to privacy concernsand confidentiality. Available data, such as the one used in ,often lacks crucial physical settings like power supply / demand,while the topology can only be partially recovered from the cas-cading traces. This limited information hinders the application ofparametric models that depend on it for reasoning.Therefore, for our analysis, we leverage the IEEE300 standardpower grid (i.e, a 300Bus having 516 transmission lines), whichoffers comprehensive details like power supply / demand, lineimpedance, capacities, and topology. This information facilitatesthe extraction of the relevant physical features used in our learn-ing process. Specifically, we extract 22 basic features based on thepre-cascading balanced power flow and grid topology.To generate CF events, we leverage the widely used DCSep-Sim simulator. We simulate power flow and cascading pro-cesses by strategically cutting lines, triggering power imbalances,and rerouting flow. If overloaded lines exceed their capacity, outagesoccur, cascading through the system until equilibrium is reached.We generate 100k Monte Carlo traces per power grid instance, byhaving each transmission line fail with a probability of 1/516.A similar experiment on a dataset obtained from the larger Polishgrid is described in the appendix. Validation and testing approach. Due to the stochastic nature ofour model, the traditional cross-validation and testing process cannot be directly applied. Instead, we propose a different validationand testing process, relying on the IEEE300 standard power gridand the DCSepSim simulations. Given an initial power grid con-figuration, we apply perturbations on the power demand settingsor on the physical topology, to obtain a set of different power gridinstances. The model will be trained on only one of the power gridinstances, and tested on the others. In some of the experiments, theinitial power grid instance is close to saturation with respect to thepower supply / demand and thus more inclined to exhibit cas-cading failures in order to observe the performance of the testedmethods under extreme conditions. Here we define 2 subscript no-tations, in the context of a each figure: for a model learned fromthe instance and tested on that same instance, and 1 for a modellearned on instance 1 in that figure and tested on instance . Thetop 5% most critical transmission lines w.r.t. cascading failure sizewere considered for the evaluation. Evaluation metrics. Our evaluation focuses on two error typesrelated to predicting cascading failures. The first metric, Distribu-tion Error (DE), compares the expected number of failures per lineacross the actual DCSepSim simulations and the model-generatedcascades. For each line in the cascading failure dataset, we count thefailures. Then, using the trained model and Monte Carlo diffusions,",
  "Source code is available at": "we generate new cascades and repeat the count on those simulatedlines. Finally, we compare the distributions of failure counts usingeither mean absolute error or relative error.The second metric, Probability Error (PE), is based on the prob-ability matrix, and can only be applied for our hyperparametricmodel. We stress that in the cascading failures data the interactionsbetween lines are only partially observed, i.e., given a cascadingsample (V, ), there is no information for which node Vtriggered the activation of node. This means that simple frequency-based estimation would lead to biased results due to over-counting,hence ground truth influence probabilities are unavailable. To miti-gate this issue, we define 3 types of mean absolute / relative errors(using the same subscript notation): 1 for the prediction error, 11 for the probabilities changes from instance to instance1, and 1 11 for the probabilities changes when applying themodel learned on instance 1 in the figure to instance . Baseline methods. Recall that existing non-parametric approaches,like , struggle with even slight changes in a power grid(e.g., power demand or topology). Trained on data from a specificgrid instance, they lack generalizability and require retraining fornew instances. Due to this reliance on specific grid data, our methodoffers an advantage under certain conditions. If the topology re-mains fixed while other factors like power demand change, we candirectly compare our approach with the existing ones, by evalu-ating their pre-trained models on unseen instances. However, ifthe topology itself undergoes changes, adapting these models forcomparison on new instances becomes unfeasible.In what follows, for a fixed topology incurring power demandchanges, we compare the performance of our approach with thestate-of-the-art Branching Process (in short, BP) method of .A similar experiment involving another state-of-the-art method is described in the appendix.",
  "Distribution of Cascades": "We first compare the probability distribution and occurrence rate ofCFs in the original dataset (standard IEEE300 with power demandfactor 1).(a) shows the probability distribution of cascade sizes (num-ber of failures) for (i) the cascades original CF data, (ii) the onessimulated by our hyperparametric diffusion model, (iii) the onessimulated by the BP approach. Note that the original data exhibitsan abnormal distribution, beginning with many small size cascades,reaching a trough around size 10, and following with several smallpeaks around size 20. Regarding the simulated data, both modelsalign well with the original data for small cascades, while for themoderate and larger cascade, our model shows better prediction.(b) compares the failure occurrence rate of lines (excludinginitial failures) in the original cascade dataset (empirical) and in ourhyperparametric diffusion model. Our prediction aligns well withthe dataset in the range of occurrence rate larger than 103, anddiverges moderately for the lower range, where are situated thesmall cascading failures, which are more difficult to be predicted.",
  ": Probability distribution of cascades and failure oc-currence rate (excluding initial failures) for IEEE300": "standard IEEE300 power grid, and the CFs generated by the DCSep-Sim simulator, we increase the user power demands of a specificregion in the grid to different levels moderate, heavy, or severe and regenerate corresponding CFs. Note that the increasing fac-tors remain in a range that ensures the resulting power system isstable, i.e., works normally with a balanced power flow. Second, inSec. 6.2.2, from the CFs of the original grid, we pick the 10 mostfrequent lines therein, and remove each of them, one at a time, fromthe original grid, to generate 10 new grids having a slightly differentphysical topology; again, all these have a balanced power flow. Therationale is that these lines can be seen as highly critical. We use asimilar subscript notation as before, in the context of each figure: for a model learned from the instance and tested on that sameinstance, and for a model learned on instance in that figure andtested on instance . 6.2.1Power demand changes. In , the power demands ofthe original power grid instance in the region < 20% quantile arescaled from 1 to 10 to produce 10 different grid instances. Whenthe increasing factor is higher than 10, the power grid will notwork normally. Therefore, the settings we selected provide a coarse-grained full-scope examination of performance by power demand.The plots show the mean relative DEs and PEs of all lines.Figs. 2(a) and 2(e) show respectively the mean relative DEs andPEs for the full-scope demand change (, with basis instance1.0). First, in (a), when the demand factor < 5.0, our testingresults show large relative errors; however, the corresponding ab-solute errors in this region for all approaches are quite small (seeAppendix A) and the relative error in this case is less indicative ofperformance as the values are relatively small. For the retrainingresults, the performance is quite constant. Beyond the < 5.0 region,the testing results are very similar, but the errors become quitelarge. This is to be expected, since the power grid has experiencedlarge power demand changes. Indeed, it is challenging for the modelthat is trained on the initial instance (1.00) to capture a very dis-tant instance such as 10.0. As for the retraining results, when thedemand factor is in the region > 5.0, our model achieves lowerrelative errors (around 0.3) than BP, with a gap around 0.35. In(e) mean relative PEs we can notice that 1, which isan indicator of our model generalization capability, follows a similartrend as with the DE values. Recall 11 indicates the changesin predicted probabilities when retraining on a different instance.We can notice that the cascading behavior of the grid instancesindeed experience significant changes when the demand factor is in the region < 5.0. The model learned on the initial instance becomesless applicable as the power demand changes.Regarding the moderate demand changes (in the range ),zoomed upon in (b) and 2(f), the basis instance for training ismoved to 4.0. The results show a different pattern compared withthe previous full-scope case. Now, prediction on other instancesbecomes more accurate, as the power changes are less drastic. Inthe region > 4.2, all approaches have an error around 0.6, and showa stable performance. For the retraining () results, our model hasrelative errors around 0.2 0.3, and performs better than BP, bya margin of around 0.1 (so around 50%). For the generalizationtesting results (1), our model shows similar performance as BP(but outperforms it, by a margin of around 18%, in terms of meanabsolute error, see Appendix A). For the PEs, in (f), the patternis also rather different from the full-scope case. The results arealigned with the corresponding failure distribution errors. Oneconclusion we can draw here is that in practice it may be preferableto retrain in different regions in the spectrum of power demandchanges, in order to have more robust / generalizable predictions.In the region of heavy power demand changes (range ),zoomed upon in Figs. 2(c) and 2(g), the general trend is again quitedifferent from the moderate case, showing a wave or step pattern.This is most likely indicative of the complex behaviors of CFs asthe demand is changing. Regarding the DE metric, after retrainingat 6.0, all models have a longer applicable range from 6.0 to 6.6. Incomparison, our model outperforms BP by a 0.2 margin in average,when testing generalizability (1), and a 0.4 margin when retrain-ing (). Beyond point 6.6, there is again a large increase of errors.Nevertheless, both our testing and retraining errors are lower thanthe ones of BP. The PE values shown in (g) follow a similarstep pattern as the DE ones in (c).Finally, moving to the severe demand changes (range ),shown in Figs. 2(d) and 2(h), in contrast to the heavy case, theerror trends for all approaches become smoother. For (d), theDEs present a Z-shape pattern. Our retraining () results are stablearound 0.28, while the testing ones (1) increase smoothly up toaround 0.60. Both the absolute and relative PEs of the BP model stayaround 60%. Our model outperforms BP with a margin of around0.35 when retraining, resp. around 0.23 when testing directly. ThePEs in (h) remain lower than 0.4 and lower than those observedin the previous three cases. We believe that in this severe case, asthe demands increase towards the limit, the CF patterns convergeas well, leading to more stable performance.One important conclusion from these three cases moderate,heavy, or severe power demand changes is that the distributionof CFs changes significantly; this leads to settings where the basicassumption of Poisson offspring distribution (on which BP relies)no longer holds, causing BPs decrease in performance. In contrast,since our hyperparametric diffusion model learns to predict thediffusion probabilities, and by extension the CFs, based on thephysical and topological features, it can capture the underlyingphysics dynamics better than BPs non-parametric approach. 6.2.2Topology changes. When changing the topology of the powergrid, by removing one transmission line, the state-of-the-art ap-proaches cannot be applied without retraining (no cross train-testexperiment is possible). Therefore, only their retraining results ()",
  ": Training the model with CF data from one grid, testing on the others (IEEE300 dataset)": "can be obtained. In , we start from the original power grid(marked as Org.), each time removing one line (shown in x-axis)and generate 10 different power grids. In the figures, the valuemarked in red refers to the training instance. In terms of retrain-ing results, our model slightly outperforms BP, with error around0.2-0.3. For testing results (), our model has a general stable perfor-mance around 0.3 on an average. There are a few peaks appearingfor instances 56, 140, and 274: when the model is trained basedon instance Org., 85, 138, or 379, the testing results on instance56 show a larger relative error, approximately in the range 0.6-0.8.When the model is trained on instance 379, the test results show alarge error on instances 140 and 274.",
  "Mitigating CFs": "In this section, based on the diffusion probability matrix we learned,we consider an initial attempt to mitigate the risks of CFs, by in-creasing the capacity of certain nodes in our diffusion graph (i.e., transmission lines in the original power grid). In order to select thelines to be consolidated, we apply the traditional IM algorithm CELF to retrieve the top 10 seeds, corresponding in some sense to the10 most critical lines, their failure leading to the largest expectedcascade. We doubled the capacity of these lines, and re-simulatethe CFs in order to observe to which extent the CFs were mitigated.Figures 4(a) and 4(b) illustrate respectively the physical gridand the diffusion graph, along with the 10 selected transmissionlines. They can be viewed as dual graphs, where the edges in(a) (marked in gray) correspond to nodes (marked in red) inthe diffusion graph. Recall that, in the power grid, nodes representload or generator buses. We can observe that the grid is sparse,while the diffusion graph is rather dense (conceptually complete);in our illustration, we filtered out the edges with 0.01.We compare the distribution of cascade sizes, before and afterdoubling the capacity of the selected 10 seed nodes, in (c).There is a clear effect of reduction in CFs for all size ranges (i.e.,",
  "(c) Cascading failures reduction": ": Physical graph of power grid, and the diffusion graph learned from the hyperparametric model, filtered with theprobability value 0.01. Cascade failures before and after increasing the capacities of ten most critical branches (IEEE300dataset). , , , , as the maximum cascade for thispower grid is of size 60). For each of these ranges, CFs are respec-tively decreased by around 18%, 36%, 9%, and 75%. We interpret thisresult as a promising one, and we intend to explore further optionsfor reducing the potential spread in the resulting diffusion graph.",
  "Running Time": "Running time is mainly influenced by two factors: the specific gridinstance and the size of the diffusion samples used for training. shows both the training and testing time for 3 instanceswith power demand changes on the IEEE300 grid. It is importantto note that training for any method in this context is unlikely tobe real-time. HCFs training is much longer compared with BP,when running on L. However, once our model is learned, it can bedirectly applied when the grid changes; the other approaches needto retrain the model on a new dataset, which requires a long time tocollect (or, in some cases, a new dataset might not even be available).The ability to function effectively in new grid configurations iscrucial, as decisions about grid provisioning are time-sensitive.This highlights once more the importance of model adaptation tounseen grid configurations.",
  "Conclusion": "We present a new approach for predicting cascading failures inpower grids by integrating physical and topological features fromthe grid into a hyperparametric model. Our methodology spans theextraction of critical features, construction of a Maximum Likeli-hood Estimation (MLE) optimization framework, and application ofthe L-BFGS-B algorithm to learn the hyperparameters influencing the diffusion of failures. The resulting diffusion probability matrixis used to run Monte Carlo simulations, providing a robust pre-dictive framework for cascading failures. In the future, we plan toexplore how real-time data from fluctuating grid conditions can beintegrated to dynamically adjust the predictive model. This research is part of the programme DesCartes and is supportedby the National Research Foundation, Prime Ministers Office, Sin-gapore under its Campus for Research Excellence and TechnologicalEnterprise (CREATE) programme. Lakshmanans research was sup-ported in part by a grant from NSERC (Canada).",
  "Ian Dobson. 2012. Estimating the propagation and extent of cascading lineoutages from utility data with a branching process. IEEE Transactions on PowerSystems 27, 4 (2012), 21462155": "Ian Dobson, Benjamin A Carreras, Vickie E Lynch, and David E Newman. 2001.An initial model for complex dynamics in electric power system blackouts. InProceedings of the 34th Annual Hawaii International Conference on System Sciences.IEEE. Ian Dobson, Benjamin A Carreras, Vickie E Lynch, and David E Newman. 2007.Complex systems analysis of series of blackouts: Cascading failure, critical points,and self-organization. Chaos: An Interdisciplinary Journal of Nonlinear Science17, 2 (2007). Pedro M. Domingos and Matthew Richardson. 2001. Mining the network valueof customers. In Proceedings of the seventh ACM SIGKDD international conferenceon Knowledge discovery and data mining, San Francisco, CA, USA, August 26-29,2001. 5766. Nan Du, Yingyu Liang, Maria-Florina Balcan, and Le Song. 2014. InfluenceFunction Learning in Information Diffusion Networks. In Proceedings of the31th International Conference on Machine Learning, ICML 2014, Beijing, China,21-26 June 2014 (JMLR Workshop and Conference Proceedings, Vol. 32). JMLR.org,20162024.",
  "Paul Hines, Karthikeyan Balasubramaniam, and Eduardo Cotilla Sanchez. 2009.Cascading failures in power grids. IEEE Potentials 28, 5 (2009), 2430": "Paul Hines, Eduardo Cotilla-Sanchez, and Seth Blumsack. 2010. Do topologicalmodels provide good information about electricity infrastructure vulnerability?Chaos: An Interdisciplinary Journal of Nonlinear Science 20, 3 (2010), 033122. Paul DH Hines, Ian Dobson, and Pooya Rezaei. 2016. Cascading power outagespropagate locally in an influence graph that is not the actual grid topology. IEEETransactions on Power Systems 32, 2 (2016), 958967.",
  "R. Kinney, P. Crucitti, R. Albert, and V. Latora. 2005. Modeling cascading failuresin the North American power grid. The European Physical Journal B 46, 1 (July2005), 101107": "Jure Leskovec, Andreas Krause, Carlos Guestrin, Christos Faloutsos, Jeanne M.VanBriesen, and Natalie S. Glance. 2007. Cost-effective outbreak detection innetworks. In Proceedings of the 13th ACM SIGKDD International Conference onKnowledge Discovery and Data Mining, San Jose, California, USA, August 12-15,2007. 420429. Bo Li, Liejun Duan, Ligeng Fu, Shaohui Sun, Zhiqiang Li, and Ruoyu Wang. 2017.Optimizing Traffic Signal Control Using Information Diffusion in Urban TrafficNetworks. IEEE Transactions on Intelligent Transportation Systems 18, 8 (2017),22242236. Yandi Li, Haobo Gao, Yunxuan Gao, Jianxiong Guo, and Weili Wu. 2023. A Surveyon Influence Maximization: From an ML-Based Combinatorial Optimization.ACM Trans. Knowl. Discov. Data 17, 9, Article 133 (jul 2023), 50 pages.",
  "Mahshid Rahnamay-Naeini and Majeed M Hayat. 2016. Cascading failures ininterdependent infrastructures: An interdependent Markov-chain approach. IEEETransactions on Smart Grid 7, 4 (2016), 19972006": "Michael Roth, Torsten Knuppel, Kevin Star, and Ignacio Perez-Arriaga. 2022.Investigating the February 2021 Texas Blackouts: Understanding the CascadingFailures that Led to Widespread Outages. Proc. IEEE 110, 6 (2022), 905924. Kazumi Saito, Ryohei Nakano, and Masahiro Kimura. 2008. Prediction of Informa-tion Diffusion Probabilities for Independent Cascade Model. In Knowledge-BasedIntelligent Information and Engineering Systems. Springer Berlin Heidelberg,Berlin, Heidelberg, 6775.",
  "Xinyu Wu, Dan Wu, and Eytan Modiano. 2021. Predicting failure cascades inlarge scale power systems via the influence model framework. IEEE Transactionson Power Systems 36, 5 (2021), 47784790": "Wenwu Yu, Pietro DeLellis, Guanrong Chen, Mario di Bernardo, and JrgenKurths. 2012. Distributed Adaptive Control of Synchronization in ComplexNetworks. IEEE Trans. Automat. Control 57, 8 (2012), 21532158. Kai Zhou, Ian Dobson, Paul DH Hines, and Zhaoyu Wang. 2018. Can an influencegraph driven by outage data determine transmission line upgrades that miti-gate cascading blackouts?. In 2018 IEEE International Conference on ProbabilisticMethods Applied to Power Systems (PMAPS). IEEE, 16. Kai Zhou, Ian Dobson, Zhaoyu Wang, Alexander Roitershtein, and Arka P Ghosh.2020. A Markovian influence graph formed from utility line outage data tomitigate large cascades. IEEE Transactions on Power Systems 35, 4 (2020), 32243235.",
  "AAdditional Experimental ResultsA.1Other Experiments on the IEEE300 Dataset": "shows the mean absolute error for line failure distributionand the diffusion probability matrix on the IEEE300 grid, comple-menting the mean relative error results shown in , once againevaluating the performance of the tested methods when the powerdemand changes. The variation trend of the error is consistent withthe one in , and in all cases, HCF exhibits a better generaliza-tion performance than the BP method.",
  "A.2Experiments on the Polish Grid": "In the domain of power systems, the power grid network IEEE300(300 nodes, 516 edges) is a relatively large physical network corre-sponding to a metropolitan scale. To further evaluate the scalabilityof the compared methods, we expanded our experiments to testwith a larger power system, the Polish power grid (2383 nodes, 2896edges), obtained from the open-source MATPOWER framework2.To obtain the dataset of cascading failures, the grid simulator isrunning up to 70h for generating 500K cascades per instance. Herewe generated 10 instances with the demand factor ranging from 1.0to 1.5. Note that when the increasing demand factor is above 1.5 thePolish power grid is over-saturated and cannot be balanced. Oncemore, following the evaluation design of , only the most criticaltransmission lines w.r.t. the cascading failures size were consideredfor the evaluation (top 0.8% lines).From , we can conclude that HCF has good predictiveperformance on this larger dataset, and achieves better resultscompared with BP. We would like to emphasize here that the errors",
  "A.3Experiments with SOTA method MK": "Despite the common limitation of existing algorithms in dealingwith unseen configurations, for a more comprehensive evalua-tion, we expand the experiments to include an additional method,MK (). It uses non-parametric regression to model thecascading processes. The performance of MK oscillates a lot, andfor most instances, it exhibits on average poorer performance com-pared with the other methods, whenever the cascading failuresdistribution experiences significant changes as the demand factorchanges. When the demand factor is within (see (d)),cascading failures are large but the change in cascading failuresdistribution is small, thus MK performs better. Finally, training MKtakes longer (24h) and has a large memory footprint (60GB) perinstance, as MK entails solving a large-size constrained nonlinearoptimization problem, and the testing time takes around 18 minper instance3.",
  ": Mean relative error for line failure distribution and diffusion probability matrix on IEEE300 grid (compared with MK)": "From Eq. (4), we have ( | , ) , where =|V| 1 is the number of nodes activated at step in cascade .Then, based on Eq. (5), ( | , ,) [ log , log(1 )] isbounded, and based on Eq. (17), the gradient ( | , ,) on is derived as:",
  "V (1 )(T).(34)": "Then, is positive semi-definite by definition. Now we studythe concavity of ( | , ,) in terms of negative and positive sam-ples.For a negative sample, = 0 and 2( | , ,) = is negative semi-definite based on the definition, implying that( | , ,) is concave.For a positive sample, = 1 and 2( | , ,) = 1",
  "CAdditional Formal ResultsC.1General Non-concavity": "Lemma 4.3 analyzes the concavity of one-sample log-likelihood.Then, the expected log-likelihood LS( | ) depends on the datasetS. Here we construct two examples showing that LS( | ) canbe concave or non-concave on different datasets. i) S containsonly negative or one-effective positive samples directly followingLemma 4.3, then LS( | ) over S is concave. ii) S contains onlyone specific type of positive samples, written as {((V\\, ), 1), V}, where |V| > 1. In this case, there is no implication of othernegative samples. Then, based on Lemma 4.3, LS( | ) over S isnon-concave.",
  "V2,(T).(62)": "Let C = {, V2, } represent the set of all interactions.Then, C C+, S+, I () = 0, i.e., is not covered bythe positive samples, and = S I () 0.Then, if 0, C+, RS will be negative semi-definiteby definition, and 2 LS( | ) will be negative semi-definite. Theproof is completed. Lemma C.1 provide a condition for the expected log-likelihoodbeing concave. Then, given a dataset, its always able to checkbeforehand the concavity and decide which algorithm to use. Notethat Lemma C.1 evaluates the concavity by each time looking at theentire set or subset of dataset that has been selected for computing.Now we start to review Lemma C.1 on a general dataset.Let A () denote the number of samples in A that cover. Then, we have S() = S I () in Lemma C.1, and",
  ".(66)": "Without any details on a general dataset, given a positive sample S+ with |V| > 1, the empirical estimation of is largerthan for some , i.e., due to the combination effectfrom V and the over-counting effect of empirical estimation fromS+ ()S () . In this case, RHS 0, hence we cannot determine the signof , thus the concavity of LS( | ). However, we stress thatRHS is a quite loose estimated bound for , taking the minimumvalue .For a general dataset, maximizing LS( | ) will make the sam-ple probability , S+ approach to 1, thus, also approachto 1, and S+ () S() 0. Therefore, if |S| |S+|,which could be frequently seen in cascading failures scenario, withcertain probability, LS( | ) tend to be concave.",
  "C.3Discussion on General Dataset": "Here we analyze the property of a general dataset S. The cascadinginfluence graph is a complete graph in this context, then everyevent will involve interactions from some nodes to the rest in V(see ), i.e., given a sample (,) S where = (V, ),it implies that S also contains other positive or negative samples{(,) | = (V, ), V\\ V}.Then, for a general dataset S with |S| |V|, with very highprobability, S can cover all the interactions among |V|. This cov-ering problem can be described as: given V, each node has equalprobability being taken; if every round randomly picking nodesfrom V then returning them, what is the probability for |S| roundspicking to ensure every node being taken at least one time? Thecovering probability is then = (1 (1 1"
}