{
  "Venkataramana RunkanaTCS ResearchPune,": "ABSTRACTPre-trained large language models (PLLMs) like OpenAI ChatGPTand Google Gemini face challenges such as inaccurate factual re-call, hallucinations, biases, and future data leakage for temporalKnowledge Graph (tKG) forecasting. To address these issues, we in-troduce sLA-tKGF (small-scale language assistant for tKG forecast-ing), which utilizes Retrieval-Augmented Generation (RAG) aided,custom-trained small-scale language models through a tabula rasaapproach from scratch for effective tKG forecasting. Our frameworkconstructs knowledge-infused prompts with relevant historical datafrom tKGs, web search results, and PLLMs-generated textual de-scriptions to understand historical entity relationships prior to thetarget time. It leverages these external knowledge-infused promptsfor deeper understanding and reasoning of context-specific seman-tic and temporal information to zero-shot prompt small-scale lan-guage models for more accurate predictions of future events withintKGs. It reduces hallucinations and mitigates distributional shiftchallenges through comprehending changing trends over time. As aresult, it enables more accurate and contextually grounded forecastsof future events while minimizing computational demands. Rig-orous empirical studies demonstrate our frameworks robustness,scalability, and state-of-the-art (SOTA) performance on benchmarkdatasets with interpretable and trustworthy tKG forecasting.KEYWORDS Temporal Knowledge Graphs, Retrieval-Augmented Generation1INTRODUCTIONKnowledge Graphs (KGs) and their dynamic extension, TemporalKnowledge Graphs (tKGs), play a pivotal role in AI applications likerecommendation engines and web searches by structuring data intograph-formatted databases. KGs utilize triples (,,)where isthe subject, is the object, and describes their relationto encodefacts, while tKGs add a time element () to represent time-validityof the fact, allowing them to capture how facts evolve over time.tKG reasoning, essential for deriving new insights, encompassesinterpolationto fill in historical data gaps andextrapolation, for future event prediction . Predicting fu-ture events in tKGs is difficult, it requires handling unseen timeperiods and entirely new entities, demanding advanced methods tonavigate the ever-changing nature of relationships. Closed-sourcepretrained large language models(PLLMs) like OpenAI ChatGPTand Google Gemini show potential for predicting future eventsdue to their extensive pre-training knowledge and reasoning abil-ities. However, these large-scale models face challenges like factrecall issues stemming from complex architectures, resulting inunreliable predictions (hallucinations), potential bias from trainingdata, and inadvertent leveraging of future data during pretraining,causing data leakage for tKG forecasting. Detecting data leakage is challenging when opaque training datasets are used for pre-training proprietary LLMs. Ensuring predictions rely solely onlegitimate predictive abilities without the undue influence fromfuture data remains a critical challenge for trustworthy tKG fore-casting. To address the limitations of existing methods, we presentsLA-tKGF, a small-scale language assistant for tKG forecastingbased on Retrieval-Augmented Generation (RAG) to groundpredictions in historical context with source attribution and trace-ability. The framework employs a multi-layered stacked vanillatransformer architecture as its backbone language model andcustom trained from scratch to avoid biases and data leakage in-herent in pre-trained LLMs. The framework incorporates three keycomponents: (i) retrieval of relevant historical knowledge fromthe tKGs. By retrieving historical facts based on context and se-mantic similarity to the query, we can infer causality and gaininsights into temporal dynamics. Additionally, we employ text-embedding model and semantic similarity for query matching tofilter out anachronistic/irrelevant information. (ii) utilizing PLLMsto analyze entity relationships prior to the target time to generatetextual descriptions of historical entity relationships based on theirinternal knowledge acquired from vast pre-training text corpora.(iii) web scraping for up-to-date contextual information relevantto the query. We utilize advanced pre-processing techniques likesentence tokenization, temporal tagging, and date conversion forexcluding future facts beyond the target time and retain appro-priate scraped data. By incorporating information from diversesources within a carefully crafted knowledge-augmented prompt,the framework generates factually accurate predictions groundedin historical context and ensures explainability. It minimizes therisk of bias, hallucinations, and avoids data leakage by pruningout-of-bound information. This Tabula Rasa approach of trainingthe framework from scratch ensures a foundation of accountabilityand trustworthiness in tKG forecasting by ensuring predictionsare truly based on past knowledge and patterns. providesan overview of the proposed approach. In summary, our proposedretrieval-augmented small-scale language model significantly im-proves tKG forecasting. It achieves this by dynamically accessingand leveraging external, continually evolving diverse data sources.This enhances the frameworks utility in real-world applications,enabling it to generate historically accurate and well-explained fore-casts. Experiments on real-world benchmark datasets demonstratethe framework effectiveness.2PROBLEM FORMULATION An tKG captures the dynamic evolution of entity relationshipsover time, unlike static KGs, which are fixed. Let V, R, T, and Fsymbolize sets of entities, relations, timestamps, and facts, respec-tively. A tKG snapshot G := V, R, F at discrete timestamp",
  "30th, ACM KDD August 25 - 29, 2024, 2024, Barcelona, SpainGeethan Sannidhi, Sagar Srinivas Sakhinana, and Venkataramana Runkana": "offers a static view of graph relationships at that specific time. Eachsnapshot represents facts F as quadruples = (,,,) at time, indicating a timestamped relationship between entities and via relation . A tKG comprises a sequence of these snapshots,G = G1, . . . , GT , showcasing the graphs evolution over time. tKGforecasting predicts missing information in future snapshots basedon past events. Given a target quadruple (,,,), the goalis to predict the missing object entity in the query (,, ?,)using historical facts O = {(,,,) | < }, which representobserved events before target time . This process utilizes histori-cal facts C(,,1) and C(,1), representing past relationshipsinvolving and , or with any relation, up to 1, respectively.To forecast, all entities in V are considered potential candidates,and the most likely candidate is selected. Reciprocal relations areconsidered to account for bidirectional relationships, including both(,,,) and its reciprocal (, 1,,), where 1 is the inverse of. This approach ensures accurate predictions by considering thedynamics of relationships from both directions.",
  "PROPOSED METHOD": "sLA-tKGF framework utilizes RAG to enhance the capabilities ofthe small-scale language model to predict future events in tKGs. Weconstruct knowledge-infused prompts using historical tKG data,web information, and PLLM-generated past entity relationship de-scriptions for zero-shot prompting the language model for reliableand trustworthy forecasting, outperforming conventional methods.3.1Web-Search-Driven RetrievalIn developing a RAG framework that leverages web-based informa-tion, a critical challenge is addressed: excluding future facts beforeevaluating the relevance of retrieved web passages to a query. Thisapproach focuses on preprocessing web passages to identify andexclude sentences containing future facts relative to a specifiedtemporal threshold. This is achieved by using natural language pro-cessing techniques for sentence tokenization, temporal tagging, andresolving temporal expressions to absolute dates. By establishing arelevance period based on the query context, the framework filtersout sentences with temporal references beyond this period, ensur-ing that only contextually and temporally relevant informationis retained. Preparing the search data involves collecting varioustypes of web content, parsing and chunking text, and embeddingtext chunks using text-embedding model. Each natural languagequestion (verbalized query) is embedded similarly, then compared to the stored retrieved chunks for semantic similarity, and the mostrelevant chunks are integrated to generate an accurate response.By filtering out future or irrelevant information, our approach en-hances the quality and reliability of real-time web informationretrieval and generation.3.2Retrieving Relevant Historical tKG DataWe leverage the following multi-step methods to incorporate rele-vant historical information from dynamic tKGs. (a) Query Verbal-ization: We convert structured queries (,, ?,) about futureevents into natural language questions using PLLMs like GPT-4.This allows the small-scale language model to interpret symbolicknowledge for forecasting tasks. For example, the query (BarackObama, visit, ?, 2015-01-25) can be verbalized as Which country didBarack Obama visit on January 25, 2015?. (b) Knowledge Retrieval &Verbalization: Historical events relevant to the verbalized query upto the target time from evolving tKGs are incorporated to improveforecasting accuracy. It improves forecasting by grounding queriesin historical context by identifying recurring patterns, trends, andcausal relationships of the evolving entity relationships over timein graph-structured data. Motivated by the concept of window sizein time series forecasting, which indicates the number of past obser-vations used to predict future values, we adopt a similar approachin tKG forecasting. For a query at target time with subject and relation , we retrieve prior tKG snapshots G(,1)as background knowledge. A knowledge-infused prompt is con-structed from historical facts related to . Assuming a set of retrieved facts related to the query, we describe this as follows:",
  "=1": "This set is constructed by combining facts involving withinthe time window to 1, covering both facts directly relatedto and (i.e., C(,,1)), and those involving in any re-lation(i.e., C(,,,1)), but only prior to . This approachenhances forecasting accuracy by providing a comprehensive viewof s historical occurrences. (c) Irrelevant Knowledge Rejection:We reject irrelevant facts using sentence embedding techniques,retaining only facts pertinent to the verbalized query based on se-mantic similarity. We then inject relevant historical knowledge intoprompts to anchor predictions within historical tKG data, enhancingaccuracy. For the query (Barack Obama, visit, ?, 2015-01-25) verbal-ized as What country did Barack Obama visit on January 25, 2015?,using relevant historical facts like Barack Obama visited Franceon June 5, 2014, Barack Obama visited Australia on November 15,2014, etc., construct a knowledge-augmented prompt for the small-scale language models zero-shot prediction on tKG forecasting.3.3PLLMs-Generated Historical SummaryWe query off-the-shelf PLLMs to generate a summary descriptionof the historical entity relationships prior to the target time, basedon the parametric knowledge acquired during pretraining.3.4Overall FrameworkWe construct knowledge-augmented prompts for grounded and in-terpretable forecasts by uniquely combining the relevant historicalknowledge from tKGs, contemporary data through web search, andhistorical entity relationship descriptions generated by off-the-shelfPLLMs, setting a foundation for trustworthy tKG forecasting. In",
  "Retrieval-Augmented Generation Meets Data-Driven Tabula Rasa Approach for Temporal Knowledge Graph Forecasting30th, ACM KDD August 25 - 29, 2024, 2024, Barcelona, Spain": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, ShyamalAnadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774(2023). Ivana Balazevic, Carl Allen, and Timothy Hospedales. 2019. TuckER: TensorFactorization for Knowledge Graph Completion. In Proceedings of the 2019 Con-ference on Empirical Methods in Natural Language Processing and the 9th In-ternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP).Association for Computational Linguistics, Hong Kong, China, 51855194. Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, LaurenceGolding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. 2022.Gpt-neox-20b: An open-source autoregressive language model. arXiv preprintarXiv:2204.06745 (2022).",
  "Evaluation Protocol": "We evaluate our framework using standard metrics for the tKGforecasting task, specifically the Mean Reciprocal Rank (MRR) andthe Hits@K metric. For each quadruple (,,,) in the test set,we generate a query quadruple: (,, ?,) and the small-scalelanguage model will rank all entities V in its predictions. The rankof the actual missing entity V for each query quadruple isdenoted by (). The MRR evaluation metric is defined as follows:",
  "()": "MRR measures the average ranking of the actual missing entitiesin the predictions, with a higher MRR indicating better performance.The Hits@K metric (where is in {1, 3, 10}) evaluates the accuracyof predicting missing entities at different positions in the predictedrankings. It measures the proportion of ground-truth missing enti-ties ranked within the top- positions, evaluating the frameworksability to predict missing entities accurately. For the tKG forecast-ing task, we consider time-aware filtering as the evaluation set-ting to prevent valid predictions from being considered errors. Forexample, consider a test query like (Barack Obama, visit, ?, 2015)with the true answer being India. Other valid predictions, suchas (Barack Obama, visit, Germany, 2015), might exist but would beremoved by the time-aware filter. It allows us to determine the rankof India without interference from alternative valid predictions.",
  "Implementation Details": "By employing a Tabula Rasa approachstarting from a cleanslatethe framework addresses the critical challenges of data leak-age in predicting future events in tKGs. Our orchestrated frame-work workflow utilizes LlamaIndex for the development ofPLLMs-powered applications. We utilize DuckDuckGos search en-gine for searching the web. We access PLLMs using a text-basedhigh-level API through Language Model as a Service (LaMaaS, ).The framework is trained on tKG forecasting, aiming to minimizecross-entropy loss and predict missing entities in future events.Framework hyperparameters include a batch size () set at 48, thenumber of epochs () at 30, and the hidden/embedding dimension() at 128. The Adam optimizer () is employed with an initiallearning rate of 13. A learning rate decay scheduler halves thelearning rate if the validation loss doesnt improve over 5 epochs,and early stopping is implemented to prevent overfitting. Fourrepresentative off-the-shelf PLLMs used are GPT-4, GPT-3.5-turbo,GPT-3.0-text-davinci-003, and Google Bard. Framework hyperpa-rameters remain consistent across PLLMs and benchmark datasets,demonstrating versatility. The context window () is set to 25 forknowledge injection of historical events from evolving tKG into theprompt, and the quadruple retrieval hop is set to one. No limit isimposed on the number of facts retrieved for knowledge injection.Utilizing 8 V100 GPUs, each with 8 GB of GPU memory, ensuresefficient training. Due to high computational costs, experiments arerun three times, and averaged results from independent experimen-tal runs are reported in Tables 2, 3, and 4 for robust comparisons.Refer appendix for hyperparameter tuning results.",
  "Dataset#Training#Validation#Test#Entities#RelationTime granularityobs": "ICEWS14 323, 895341, 40912, 49826024 hours365ICEWS18 373, 01845, 99549, 54523, 03325624 hours304ICEWS05-15 369, 10446, 18846, 03710, 48825124 hours4, 017WIKI 539, 28667, 53863, 11012, 554241 year232YAGO 161, 54019, 52320, 02610, 623101 year189ACLED-CD221,788216222243624 hoursN/A : The dataset statistics include the number of quadruples in the training, validation, and test sets, denoted as #Training,#Validation, and #Test, respectively. obs indicates the total snapshots of the temporal knowledge graph (tKG), with eachsnapshot capturing its state at a distinct time point. 4.6ResultsOur study compared the proposed framework with various super-vised learning methods for tKG forecasting. demonstratesthat sLA-tKGF W/GPT-4, outperformed baseline algorithms. Wereport baseline results from prior research for fair and con-sistent comparison. Tables 3 and 4 provide further comparisonsusing several popular baselines, with results reported from an ear-lier study. Our experimental results confirm the effectivenessof sLA-tKGF framework in constructing knowledge-augmentedprompts, involving: (1) retrieving historical knowledge from tKGs,(2) incorporating web search results for current information, and (3)using pre-trained large language models for summarizing historicalentity-relationships, to query the small-scale language model andgenerate accurate and interpretable forecasts.",
  "Ablation Study": "Given the complexity and multifaceted nature of the proposedsLA-tKGF framework for tKG forecasting, we propose several ab-lation experiments to evaluate the individual contributions of thecomponents and their effectiveness within the framework. Each ofthese ablation studies can provide insights into the significance ofthe individual components and their collective impact on the overallefficacy of the sLA-tKGF framework. To design ablation studies forthe sLA-tKGF framework, we systematically disable each compo-nent individually to evaluate its impact on the overall performance.This approach helps in understanding the contribution of individualcomponents to the frameworks accuracy, reliability, and robust-ness. The ablation study aims to evaluate the contributions of threekey components in the framework for forecasting future events: (a)historical knowledge retrieval (HKR) from tKGs, (b) web search forcontextual information (WSCI), and (c) descriptive text generation(DTG) using PLLMs. The HKR component, which retrieves relevanthistorical knowledge from dynamic tKGs, is assessed by contrastingits performance when incorporated versus omitted. Similarly, theimpact of the WSCI component, providing up-to-date context priorto the target time through web search, is evaluated by disabling itand observing the changes in forecasting accuracy. The utility ofthe DTG component, which utilizes PLLMs to analyze historicalentity relationships to generate descriptive texts, is quantified byincluding or excluding it and observing the impact on forecastingaccuracy. The ablated variants are as follows:",
  "Deep Dive into Long-term tKG forecasting": "Existing tKG forecasting research often overlooks insights fromedge dissolution and formation. Our work considers the creationand dissolution of relationships (edges) between entities in tKGs ascritical evolutionary factors for accurate tKG forecasting by captur-ing the essence of how relationships evolve and change over time.Nevertheless, the high dimensionality and dynamic complexity oftKGs pose challenges for both short and long-term event forecast-ing. Our approach uses historical context within an evolving tKGto improve both short-term and long-term forecasting accuracy.Short-term forecasts predict near-future changes, capturing theimmediate evolution of events, while long-term forecasts antici-pate broader trends that unfold over an extended period. StandardtKG forecasting relies on static KG snapshots, denoted as G(, ),from a historical window up until time , to predict events at time + (e.g., one day). In real-world scenarios, the absence of graphinformation motivates the evaluation of forecasting techniques formaking predictions about the distant future ( ). In the con-text of long-term tKG forecasting, the objective is to predict missingentities at a future time +, beginning with an initial forecast pe-riod from to +. This initial forecast targets the short-termfuture, represented by . The forecasting process then extends intothe final forecast period, from + to +, to focus on long-termpredictions, where signifies the extended future. Experimentalresults on the ICEWS05-15 dataset, using sLA-tKGF W/GPT-4, fordifferent values are shown in . As increases (1 dayto 8 days), performance declines in both single-step and multi-stepapproaches, indicating increased difficulty in predicting furtherinto the future. This decline in performance is attributed from theassumption that dynamics at + apply at + . With larger, this assumption falters due to data distributional shifts and theperformance drops because of the lack of recent relevant historicalfacts in the augmented prompt for accurate predictions at + .",
  "Inductive Link Prediction in tKGs": "In real-world applications, new entities emerge over time, necessi-tating a tKG forecasting framework that generalizes well to unseendata. An efficient framework must demonstrate robust generaliza-tion abilities to effectively handle new, unencountered entities. Toevaluate this capability, we introduce the inductive link predic-tion task, which showcases the frameworks ability to predict linksinvolving unseen entities. This task involves selecting test",
  "W/Google Bard77.8182.4592.0886.5790.6192.53": ": The table presents tKG forecasting results on two benchmark datasets on multi-step tKG forecasting task. The evaluationmetrics are MRR (%) and Hits@1/3/10 (%). The best model for each dataset is highlighted in bold. quadruples where either the subject or object entity, or both, werenot seen during training. For example, consider the ICEWS05-15 testset with the quadruple (India, announce, new economic policy, 2015-09-01)",
  "H@1H@3H@10H@1H@3H@10H@1H@3H@10H@1H@3H@10H@1H@3H@10": "sLA-tKGF W/GPT-4 (uses both)0.9080.9380.9730.8460.8910.9680.4990.6330.8020.5450.6880.8280.4810.6480.775sLA-tKGF W/GPT-4 (Single Entity)0.7540.7860.8250.7090.7580.8230.4270.5430.6840.4640.5850.7020.4140.5510.659sLA-tKGF W/GPT-4 (Entity Pair)0.8100.8360.8750.7560.7970.8670.4400.5590.7180.4870.6160.7430.4330.5810.702 : The table presents experimental results from a study on the impact of retrieved fact types on tKG forecastingperformance. Two types of historical facts, Single-Subject Entity and Subject Entity-Relation Pair, were evaluated for theirimpact on forecasting accuracy. Results indicated that the performance of the sLA-tKGF W/GPT-4 framework varies based onthe dataset and the type of historical fact used. In essence, the research highlights the importance of including both historicalfact types in the augmented prompts for optimal performance in the forecasting task across various datasets.",
  ": The table presents the results for inductive futurelink prediction task on ICEWS05-15 dataset in terms of MeanReciprocal Rank(MRR) metric": "We conducted future link prediction experiments on sets of in-ductive link prediction quadruples, and the results are shown in. We evaluated our framework against robust baselines, us-ing the ICEWS05-15 dataset. Despite not being explicitly designedfor knowledge graph tasks like inductive link prediction, our pro-posed framework (sLA-tKGF W/GPT-4) surprisingly outperformsall baselines across various metrics, as demonstrated in . Thissuccess stems from our frameworks ability to leverage both the web search results and historical entity-relationships based descrip-tions generated by PLLMs using implicit pre-training knowledge.In essence, when applied to the test dataset, the framework uti-lizes patterns and information from its training data, augmented byknowledge-rich prompts at inference time, to successfully predictrelationships between unseen entities within an evolving tKG.",
  "CONCLUSION": "Despite their strengths, pre-trained large language models strugglewith future predictions for tKG forecasting due to limitations includ-ing hallucinations, inaccurate fact recall, and future data leakage.Our framework tackles these challenges by utilizing a retrieval-augmented small-scale language model, trained from a clean-slateapproach with knowledge-augmented prompts to achieve accuratetKG forecasts. We construct these prompts that include historicaldata from tKGs, web search results, and text descriptions generatedby off-the-shelf PLLMs, enabling contextually grounded forecastswith improved factual accuracy, reducing hallucinations and over-coming distributional shifts. This scalable, robust approach achievesstate-of-the-art (SOTA) performance on benchmark tKG datasets.It paves the way for trustworthy tKG forecasting by offering trace-ability, explainability, and interpretability.",
  "Elizabeth Boschee, Jennifer Lautenschlager, Sean OBrien, Steve Shellman, JamesStarz, and Michael Ward. 2015. ICEWS Coded Event Data": "Shib Sankar Dasgupta, Swayambhu Nath Ray, and Partha Talukdar. 2018. HyTE:Hyperplane-based temporally aware knowledge graph embedding. In Proceedingsof the 2018 Conference on Empirical Methods in Natural Language Processing. 20012011. Alberto Garca-Durn, Sebastijan Dumani, and Mathias Niepert. 2018. LearningSequence Encoders for Temporal Knowledge Graph Completion. In Proceedingsof the 2018 Conference on Empirical Methods in Natural Language Processing.Association for Computational Linguistics, Brussels, Belgium, 48164821.",
  "Julia Gastinger, Timo Sztyler, Lokesh Sharma, and Anett Schuelke. 2022. On theEvaluation of Methods for Temporal Knowledge Graph Forecasting. In NeurIPS2022 Temporal Graph Learning Workshop": "Rishab Goel, Seyed Mehran Kazemi, Marcus Brubaker, and Pascal Poupart. 2020.Diachronic Embedding for Temporal Knowledge Graph Completion. In Thirty-Fourth AAAI Conference on Artificial Intelligence. 39883995. Rishab Goel, Seyed Mehran Kazemi, Marcus Brubaker, and Pascal Poupart. 2020.Diachronic embedding for temporal knowledge graph completion. In Proceedingsof the AAAI Conference on Artificial Intelligence, Vol. 34. 39883995.",
  "Zhen Han, Peng Chen, Yunpu Ma, and Volker Tresp. 2021. Explainable Sub-graph Reasoning for Forecasting on Temporal Knowledge Graphs. In Interna-tional Conference on Learning Representations": "Zhen Han, Zifeng Ding, Yunpu Ma, Yujia Gu, and Volker Tresp. 2021. LearningNeural Ordinary Equations for Forecasting Future Links on Temporal KnowledgeGraphs. In Proceedings of the 2021 Conference on Empirical Methods in NaturalLanguage Processing. Association for Computational Linguistics, Online andPunta Cana, Dominican Republic, 83528364. Tingsong Jiang, Tianyu Liu, Tao Ge, Lei Sha, Baobao Chang, Sujian Li, and ZhifangSui. 2016. Towards Time-Aware Knowledge Graph Completion. In COLING 2016,26th International Conference on Computational Linguistics. 17151724. Woojeong Jin, He Jiang, Meng Qu, Tong Chen, Changlin Zhang, Pedro Szekely,and Xiang Ren. 2019. Recurrent event network: Global structure inference overtemporal knowledge graph. arXiv preprint arXiv:1904.05530 (2019). Woojeong Jin, Meng Qu, Xisen Jin, and Xiang Ren. 2020. Recurrent EventNetwork: Autoregressive Structure Inferenceover Temporal Knowledge Graphs.In Proceedings of the 2020 Conference on Empirical Methods in Natural LanguageProcessing (EMNLP). Association for Computational Linguistics, Online, 66696683.",
  "Julien Leblay and Melisachew Wudage Chekol. 2018. Deriving validity timein knowledge graph. In Companion Proceedings of the The Web Conference 2018.17711776": "Dong-Ho Lee, Kian Ahrabian, Woojeong Jin, Fred Morstatter, and Jay Pujara. 2023.Temporal Knowledge Graph Forecasting Without Knowledge Using In-ContextLearning. arXiv preprint arXiv:2305.10613 (2023). Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,Naman Goyal, Heinrich Kttler, Mike Lewis, Wen-tau Yih, Tim Rocktschel,et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks.Advances in Neural Information Processing Systems 33 (2020), 94599474. Zixuan Li, Xiaolong Jin, Wei Li, Saiping Guan, Jiafeng Guo, Huawei Shen,Yuanzhuo Wang, and Xueqi Cheng. 2021. Temporal knowledge graph reasoningbased on evolutional representation learning. In Proceedings of the 44th Inter-national ACM SIGIR Conference on Research and Development in InformationRetrieval. 408417. Jerry Liu. 2022. LlamaIndex. Yushan Liu, Yunpu Ma, Marcel Hildebrandt, Mitchell Joblin, and Volker Tresp.2022. Tlogic: Temporal logical rules for explainable link forecasting on temporalknowledge graphs. In Proceedings of the AAAI Conference on Artificial Intelligence,Vol. 36. 41204127.",
  "Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2014. Em-bedding entities and relations for learning and inference in knowledge bases.arXiv preprint arXiv:1412.6575 (2014)": "Cunchao Zhu, Muhao Chen, Changjun Fan, Guangquan Cheng, and Yan Zhan.2020. Learning from History: Modeling Temporal Knowledge Graphs withSequential Copy-Generation Networks. arXiv preprint arXiv:2012.08492 (2020). Cunchao Zhu, Muhao Chen, Changjun Fan, Guangquan Cheng, and Yan Zhang.2021. Learning from history: Modeling temporal knowledge graphs with sequen-tial copy-generation networks. In Proceedings of the AAAI Conference on ArtificialIntelligence, Vol. 35. 47324740.",
  "ADDITIONAL EXPERIMENTS6.1The Impact of Temporal and SequentialInformation on the Zero-Shot tKGForecasting Task": "The proposed framework, sLA-tKGF, introduces a novel two-prongedapproach to predict future events in tKGs by combining a clean-slatetrained, small-scale language model with the Retrieval-AugmentedGeneration (RAG) technique. It emphasizes accuracy and bias-freepredictions by leveraging historical tKG data, current web infor-mation, and contextually relevant historical entity relationshipdescriptions generated by PLLMs. We examined the performance ofthe sLA-tKGF framework by understanding temporal informationin retrieved historical events from tKGs, comparing its performancein the tKG forecasting task with and without explicit timestampsin the knowledge-augmented prompt. We also investigated the im-pact of shuffling historical facts without time information on theperformance of the sLA-tKGF framework. The results, shown in Ta-ble 8, reveal that the sLA-tKGF frameworks performance worsenswithout temporal information. This decline is exacerbated by therandom arrangement of events, as demonstrated in . Theseoutcomes underscore the frameworks dependence on chronologi-cal sequencing for accurate predictions, emphasizing the criticalrole of temporal information and sequence in the accuracy of thesLA-tKGF W/GPT-4 framework for tKG forecasting. The consistentsignificance of temporal and sequence information across datasets(YAGO, WIKI, ICEWS14, ICEWS18, ACLED-CD22) reinforces thereliability and applicability of these findings.",
  "Effect of Types of Knowledge-InfusedAugmented Prompts on tKG Forecasting": "In this section, we evaluate the impact of augmenting natural lan-guage questions(verbalized queries) with external knowledge fromhistorical events sampled from tKGs on the sLA-tKGF W/GPT-4frameworks performance in tKG forecasting tasks. We explorevarious knowledge retrieval strategies for constructing knowledge-augmented prompts, focusing on optimizing the frameworks per-formance in tKG forecasting. shows how different strate-gies affect the sLA-tKGF W/GPT-4 frameworks performance acrossdatasets (YAGO, WIKI, ICEWS14, ICEWS18, ACLED-CD22) forSingle-Step and Multi-Step forecasting. The results demonstratethat selectively incorporating relevant prior knowledge signifi-cantly enhances tKG forecasting performance, surpassing baselinesthat either omit additional knowledge or use it indiscriminately.",
  "Balancing Accuracy and Efficiency:Optimizing Historical Context inAugmented Prompts for tKG Forecasting": "Our framework utilizes historical events or facts from time ( )to to predict missing entities in the query quadruple at , where represents the historical context window size, a configurablehyperparameter. We experimented with various lengths to evaluatethe impact of historical context on the forecasting performance ofthe sLA-tKGF W/GPT-4 framework for tKGs. According to , leveraging a greater amount of past data to generate knowledge-augmented prompts for the small-scale language model enhancesthe accuracy of missing entity predictions, as evidenced by im-proved mean reciprocal rank (MRR) scores, albeit at the expense ofhigher computational demands. The small-scale language modelwithin the sLA-tKGF W/GPT-4 framework is constrained by themaximum input token sequence length. Although extending thehistorical window marginally increases MRR scores, it leads to thecreation of longer and more complex prompts that are impracticalfor broad-scale application. To strike a balance between accuracyand computational efficiency, we selected a historical context win-dow of 25 for our experiments. We varied the historical facts inthe prompts to identify the optimal setup for generating accurateforecasts, with the goal of minimizing wall-clock time. Wall-clocktime for sLA-tKGF W/GPT-4 queriesthe time elapsed from querysubmission to responseis influenced by the size of the small-scalelanguage model, the complexity of the query, and the available com-putational resources. While typically, small-scale language modelsreturn responses within seconds, more complex or extensive queriesmay require longer processing times.",
  "Study of tKG forecasting task withnon-uniform time intervals": "Many state-of-the-art techniques struggle with tKGs featuring ir-regular time intervals, unlike the sLA-tKGF framework, which ef-fectively addresses this issue by leveraging knowledge-augmentedprompting for small-scale language models in temporal KG fore-casting. The framework excels in handling real-world complexitiesand data sparsity, capturing complex dynamics and causal rela-tionships more accurately, thus offering a versatile and reliablesolution for temporal KG forecasting. Experimental evaluations onthe ICEWS05-15_continuous dataset, a subset created by samplingfrom the original ICEWS05-15 dataset to simulate non-periodic ob-servations in continuous time with 1-4 units interval, support ourclaim. The sLA-tKGF framework, trained on this benchmark and as-sessed using the Mean Reciprocal Rank (MRR) metric, demonstratesstrong performance, especially with the sLA-tKGF W/GPT-4 con-figuration, on temporal KGs with irregular intervals. The datasetstatistics for ICEWS05-15_continuous are presented in .We trained the sLA-tKGF framework with various off-the-shelf Pre-trained Large Language Models (PLLMs) on this new benchmark",
  "Impact of Retrieved Fact Types": "In this work, we introduce a zero-shot learning method to predictmissing entities in query quadruples. Our approach includes: (a)Constructing a historical context for a query quadruple (, , ?,)using historical facts from previous static KG snapshots G:1to form a knowledge-infused augmented prompt. (b) Converting retrieved facts and the query into verbalized sentences, employingsentence embedding for knowledge distillation, and using semanticsimilarity to filter facts, thereby constructing augmented prompts.(c) Estimating the missing entity conditioned on the augmentedprompt, following ( | (,, ?,), G(,1)). We eval-uate the impact of single-subject entity and subject entity-relationpair historical facts on forecasting accuracy. The former involvesonly the subject entity (), while the latter includes both the sub-ject () and the relation (). We examine these effects across",
  "ICEWS05-15_continuous148, 67317, 1951718810488251NA1543": ": The table displays the statistics of the new dataset. #Training, #Validation, #Test represent the number of quadruplesin the training set, validation set, and test set, respectively. obs represents the total number of snapshots in the new benchmarktKG forecasting dataset, where each snapshot captures the state of the tKG at a specific point in time.",
  ": The table shows the tKG forecasting results onIrregular temporal KGs": "various benchmark KG datasets, analyzing the influence of differ-ent retrieved facts on the frameworks performance. We find thatsingle-subject entity queries benefit from a broader range of his-torical facts, improving performance, while subject entity-relationpair queries yield a more targeted set of facts, potentially enhanc-ing outcomes. Our findings, as indicated in , demonstratethat the performance of the sLA-tKGF W/GPT-4 framework variesdepending on the dataset. The WIKI and ICEWS18 benchmarksexhibit improvements with entity-focused facts, whereas ICEWS14performs better with pair-focused facts. Our study explores theimpact of either single-subject entity or subject-relation pair facts, revealing that different datasets benefit from specific types of facts.This leads to more context-aware enhancements in the sLA-tKGFW/GPT-4 framework.6.6Impact of Directionality in HistoricalModeling on tKG forecasting Performance In the tKG forecasting task, unidirectional refers to when thesubject entity () or subject-relation pair (,) from historicalfacts matches their position in the query quadruple (,, ?,).Bidirectional denotes cases where they can appear in any posi-tion. For bidirectional modeling, historical facts are transformed byswapping entities and inverting the relation, e.g., (,,,) becomes(, 1,,). We use PLLMs such as GPT-4 to obtain reciprocal rela-tions, enhancing tKG forecasting by incorporating diverse historicalcontexts. For instance, (Barack Obama, visit, India, 2015-01-25) be-comes its reciprocal (India, visited by, Barack Obama, 2015-01-25).Our study evaluates the directionalitys impact on the tKG fore-casting, finding that bidirectional modeling slightly improves per-formance, notably on ICEWS datasets. The experimental resultshighlight the value of appropriate relation modeling for sLA-tKGFW/GPT-4 in understanding context, offering modest performanceboosts in tKG forecasting. shows the experimental results.6.7Impact of PLLMs size The sLA-tKGF framework leverages a blend of historical tKG data,current web-scraped information, and contextually relevant de-scriptions of past entity relationships generated by pre-trained lan-guage models (PLLMs) to construct knowledge-augmented prompts.These prompts query the small-scale language model to estimate",
  "sLA-tKGF W/GPT-4 ()0.9080.9380.9730.8460.8910.9680.4990.6330.8020.5450.6880.8280.4810.6480.775sLA-tKGF W/GPT-4 ()0.9300.9650.9990.8750.9150.9820.5020.6500.8240.5640.7070.8520.4960.6650.797": ": The table presents the experimental results on the study of the impact of directionality in historical modeling on tKGforecasting performance. In the context of tKG forecasting, unidirectional and bidirectional indicate whether entities inhistorical facts align with the positions of the query quadruple. Unidirectional maintains alignment, while bidirectionalallows entities to shift after transforming facts with inverse relations. The aim is to determine if incorporating both originalhistorical facts and their inverse relations in the bidirectional modeling context can enhance the performance of tKGforecasting by offering a more varied historical context. The adoption of bidirectional relation modeling for tKG forecastingshows marginal improvements, with particularly notable gains evident in the ICEWS benchmark datasets. forecasts. Designed to enhance both reliability and accountabil-ity, the framework offers a significant advancement over tradi-tional forecasting methods. In this study, we examine the powerlaw relationship between PLLM model size and performance ontKG forecasting using our sLA-tKGF framework. We explore theeffect of PLLM model size on sLA-tKGF framework performancein zero-shot tKG forecasting through experiments with variousPLLMs. lists the language models used, including GPT-2 , GPT-J , and GPT-NeoX , all employing the GPT-2BPE tokenizer with similar vocabulary sizes. Our findings, de-tailed in , confirm that larger models yield better results,supporting the scaling laws in zero-shot learning. These modelsdemonstrate enhanced linguistic comprehension, more complexarchitectures, and enhanced generalization capabilities.",
  ": Overview of different GPT-based models by familyand parameter count": "Our results highlight the advantages of using larger PLLMs re-sulting in sLA-tKGF framework with complex pattern recognitionin tKG forecasting. With PLLMs like GPT-2, GPT-3, and GPT-4 show-ing remarkable abilities in natural language tasks, GPT-4 standsout for its scale and advanced learning capabilities. Yet, achievingsuccess in tKG forecasting may require domain-specific expertiseto develop effective zero-shot prompts.",
  "Hyperparameter Tuning": "Our sLA-tKGF-GPT-4 framework improves tKG forecasting accu-racy by integrating PLLMs with small-scale language models. Hy-perparameter tuning is challenging due to its complex dimensional-ity, computational demands, and dataset-specific requirements. Weopt for random search over grid search or Bayesian optimizationfor efficient hyperparameter exploration, seeking the optimal con-figuration on benchmark datasets. The small-scale language modelsare trained on knowledge-augmented prompting for tKG forecast-ing, aiming to predict missing entities by minimizing cross-entropyloss. Hyperparameter optimization for sLA-tKGF-GPT-4 focuseson batch size ( 16, 48), epochs ( 10, 30), and embedding di-mension ( 64, 128). shows tuning results on benchmark",
  "Hyperparameter Tuning of PLLMs": "Top-P and temperature serve as hyperparameters in pre-trainedlarge language models (PLLMs), such as GPT-4 and Google Gemini.These hyperparameters influence the predictability and variety ofmodel outputs. The temperature parameter primarily affects outputpredictability. Lower temperature values result in more determin-istic responses, while higher values enable greater variability. A temperature value of 1.0 maintains the original output probabil-ities. On the other hand, Top-P (nucleus) sampling dynamicallychooses tokens based on their cumulative probability. This methodstrikes a balance between text generation diversity and coherenceby adjusting the threshold. By fine-tuning these parameters, it ispossible to strike an optimal balance between randomness and de-terminism in generated outputs. Regarding the sLA-tKGF-GPT-4variant, hyperparameter optimization studies indicate that Top-Pranges over , and temperature falls within the range of ."
}