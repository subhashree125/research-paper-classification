{
  "ABSTRACT": "Recent studies give more attention to the anomaly detection (AD)methods that can leverage a handful of labeled anomalies alongwith abundant unlabeled data. These existing anomaly-informedAD methods rely on manually predefined score target(s), e.g., priorconstant or margin hyperparameter(s), to realize discriminationin anomaly scores between normal and abnormal data. However,such methods would be vulnerable to the existence of anomalycontamination in the unlabeled data, and also lack adaptation todifferent data scenarios.In this paper, we propose to optimize the anomaly scoring func-tion from the view of score distribution, thus better retaining the di-versity and more fine-grained information of input data, especiallywhen the unlabeled data contains anomaly noises in more practicalAD scenarios. We design a novel loss function called Overlap lossthat minimizes the overlap area between the score distributionsof normal and abnormal samples, which no longer depends onprior anomaly score targets and thus acquires adaptability to vari-ous datasets. Overlap loss consists of Score Distribution Estimatorand Overlap Area Calculation, which are introduced to overcomechallenges when estimating arbitrary score distributions, and toensure the boundness of training loss. As a general loss component,Overlap loss can be effectively integrated into multiple networkarchitectures for constructing AD models. Extensive experimentalresults indicate that Overlap loss based AD models significantlyoutperform their state-of-the-art counterparts, and achieve betterperformance on different types of anomalies.",
  "Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 23, August 610, 2023, Long Beach, CA, USA 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00",
  "INTRODUCTION": "Anomaly detection (AD) is the task of identifying unusual instancesthat deviate significantly from the majority of data, which has beenapplied in wide-ranging domains, such as social media analysis, rare disease detection , intrusion detection ,and financial fraud detection . Previous research efforts focus on unsupervised AD which does not require anylabeled training data, but unsupervised methods lack any guidanceof true anomalies . Therefore recent studies propose to learnvaluable distinguishing features from a few labeled anomalies thatmay be identified by domain experts in practice, which is termedas the \"anomaly-informed\" AD methods .Current anomaly-informed methods, including semi- and weakly-supervised AD algorithms, mainly devise specific forms of lossfunctions to leverage such limited label information. These involverepresentation learning based minus loss in Unlearning , in-verse loss in DeepSAD , and hinge loss in REPEN , as wesummarized in a~1c. In these methods, an anomaly scoreis generated based on the learned feature transformation of inputdata, such as the reconstruction error or the embedding distance.However, optimization in the representation space would lead todata-inefficient learning and suboptimal anomaly scoring .Therefore several works fulfill an end-to-end learningfashion of anomaly score to obtain better performance, designingloss functions to map input instances to their corresponding anom-aly score target(s), or to predefine a margin hyperparameter torealize the difference in anomaly scores between unlabeled samplesand labeled anomalies, as shown in d~1e. Nevertheless, sucha predefined score target or margin would constrain the modelsadaptability to different datasets, and further tuning these hyperpa-rameters for realizing adaptation is often difficult, considering thescarcity of labeled data in practical AD scenarios.In this paper, we aim to acquire an anomaly scoring functioncapable of realizing adaptive anomaly score discrimination for di-verse data scenarios, thus addressing the above issues in previousanomaly-informed loss functions. Notably, we devise the Overlaploss that minimizes score distribution overlap between normal sam-ples and anomalies, depending on the model itself to decide thesuitable distribution of output anomaly scores. This kind of adapt-ability eliminates the dependency on predefined anomaly scoretargets. However, a non-trivial challenge is to estimate arbitrarydistributions of anomaly scores, which are caused by the scarcity oflabeled anomalies and anomaly noises in the unlabeled data. In theOverlap loss, we design a simple and effective method to estimate",
  "(f) Overlap loss (proposed)": ": Loss function comparison. Unlike previous AD loss functions that suffer unbounded training loss or rely on specificanomaly score target(s) to guide model training, Overlap loss ensures a bounded training loss without requiring predefinedtarget(s) by first estimating score distributions of normal and abnormal data, and further minimizing their overlap area. the overlap area of arbitrary score distributions, while ensuring acorrect order in the output anomaly scores and the boundness oftraining loss to better achieve stability in model training.The main contributions of this paper can be summarized asfollows: (1) We propose the Overlap loss for the AD community,which can achieve adaptive score distribution discrimination ininput data, realizing sufficient global insight of anomaly scores in anend-to-end gradient update fashion. (2) We verify the effectivenessof the proposed Overlap loss on several network architecturescovering both AD and classification tasks. Extensive results on 25datasets suggest that the proposed Overlap loss could be servedas a basis for further development in AD tasks. We open-sourcethe proposed method, related codes, and all testing datasets forAD communities at (3) Wedecouple the loss functions from several popular AD algorithmsand analyze them in a unified framework, including embeddingvariation and network parameter changes. Moreover, we investigatethe detection performance of different loss functions on varioustypes of anomalies, therefore further exploring the pros and consof these methods.2RELATED WORK A desirable anomaly detection approach should produce not onlya binary output (normal or abnormal) but also assign a degree ofbeing an anomaly (anomaly score) to each observation . Priorliterature can be divided into two categories, i.e., AD algorithmswithout or with supervision. The former assumes that no labeleddata is available during the model training stage and is proposedwith different assumptions of data distribution , whereas thelatter leverages a limited number of labeled samples which may beverified by some domain experts or automatic detecting systems.AD Algorithms without Supervision. Typical anomaly de-tection methods are constructed for learning anomaly patternsin an unsupervised manner. These include shallow unsupervisedmodels like CBLOF and ECOD , or ensemble method Iso-lation Forest . More recently, deep learning (DL) techniqueslike DeepSVDD and GAN-based MO-GAAL have beenproposed for improving the performance of unsupervised AD tasks.AD Algorithms with Supervision. Unsupervised methodscan not achieve satisfactory performance in practical applicationswithout the guidance of labeled data. Therefore several studieshave also investigated utilizing partially labeled data to improvedetection performance, which can be summarized into the followingthree categories: (i) AD methods that are trained only on labeled normal samples,and detect anomalies that deviate from the normal representationlearned in the training process .(ii) AD methods that additionally leverage a limited number oflabeled anomalies. A common problem of the above methods usingonly normal samples is that many of the anomalies they identifyare data noises or uninteresting data instances due to the lack ofprior knowledge about the abnormal behaviors . This resultsin the development of semi- and weakly-supervised AD methods,which not only learn from numerous unlabeled data but also utilizelimited information of labeled anomalies.Among them, Unlearning uses the minus loss form to pro-vide an opposite direction of gradient update between the recon-struction error of the normal data and anomalies. DeepSAD employs the inverse loss to penalize the inverse of the embeddingdistance such that the representation of anomalies must be mappedfurther away from the initial center of the hypersphere. REPEN introduces a ranking model-based framework, which applies thehinge loss to encourage a distance separation of low-dimensionalrepresentation between normal samples and anomalies.Several works are proposed to realize end-to-end learning ofanomaly score, as they indicate that the above representation learn-ing based AD methods would lead to data-inefficient learning andsuboptimal detection performance . Specified with the devia-tion loss, DevNet leverages a prior probability and a marginhyperparameter to enforce significant deviations in anomaly scoresbetween normal and abnormal data. FEAWAD incorporates theDAGMM network architecture with the deviation loss, for thebetter use of the information among hidden representation, recon-struction residual vector and reconstruction error transformed bythe auto-encoder . PReNet formulates the scoring functionas a pairwise relation learning task, where it defines three constanttargets to enforce large margins among the anomaly scores of threetypes of instance pairs.(iii) Fully-supervised methods are not specific for AD tasks ingeneral . Previous studies often use existing binary clas-sifiers for this purpose such as Random Forest and MLP. One knownrisk of supervised methods is that ground truth labels maybe notnecessarily accurate enough (i.e., there often exist some unlabeledanomaly noises in normal samples) to capture all types of anomalies,therefore these supervised methods may fail to detect unknowntypes of anomalies .",
  "Anomaly Detection with Score Distribution DiscriminationKDD 23, August 610, 2023, Long Beach, CA, USA": "Table A4: Average AUC-ROC performance over 25 real-world datasets. Each experiment is repeated 5 times. stands for theratio of labeled anomalies to all true anomalies in the training set. Perf. shows the relative improvement of Overlap lossbased models over their corresponding counterparts. , and denote statistical significance at 1%, 5% and 10% of Wilcoxonsigned rank test, respectively. The best results are in bold.",
  "=1, where": "R represents the input feature and is the label of identi-fied anomalies. Usually, we have , since only limited priorknowledge of anomalies is available. Such data assumption is morepractical for AD problems, and has been studied in recent works. Given such a dataset, our goal is to train a model,to effectively assign higher anomaly score for the abnormal data.",
  "Overview of the Proposed Overlap Loss": "Overlap loss first employs a Score Distribution Estimator for es-timating the unknown probability density function (PDF) of theoutput anomaly scores in neural networks and then conducts Over-lap Area Calculation between the anomaly score distributions of theunlabeled samples and labeled anomalies. Finally, Overlap loss min-imizes the calculated overlap area of score distributions to providethe gradient for backpropagation in neural networks. The proposedOverlap loss fulfills the following properties: (i) the boundness oftraining loss for better convergence in the model training. (ii) elim-inating explicit training target of anomaly score (e.g., constant ormargin hyperparameter(s)) to enhance the model adaptability todifferent datasets. (iii) optimizing the entire anomaly score distri-bution, instead of pointwise optimization between the estimatedanomaly scores and their corresponding targets.",
  "Overlap Loss for Score DistributionDiscrimination": "In the following subsections, we illustrate two main parts of pro-posed Overlap loss: Score Distribution Estimator and Overlap AreaCalculation, along with their corresponding basic ideas and chal-lenges, as complements to our final solutions. 3.3.1Score Distribution Estimator. Instead of pointwise optimiza-tion of the output anomaly scores, here we consider optimizingthe anomaly score from a distribution view. Let R be thehidden representation space, an end-to-end anomaly scoring net-work (; ) : R can be defined as a combination of a featurerepresentation learner (; ) : and an anomaly scoringfunction (; ) : R, in which = {, }. If we denotethe anomaly score of normal data as (; ) = and that ofabnormal data as (; ) = , a density estimator () is thenapplied to estimate the PDFs of both and in a training batch.A straightforward idea is to employ a prior distribution, e.g., theGaussian distribution, as the score distribution estimator. Gaussiandistribution inherits several good properties. For instance, the in-tersection point used for estimating the score distribution overlapin a can be calculated by the following formula :",
  "( )2 + 2 2 2 log": "2 2(1)where and are their corresponding mean and variance ofscore distributions, respectively. The main challenge of this basicidea is that the number of labeled anomalies is usually too smallto satisfy the Gaussian distribution assumption according to thecentral limit theorem , while enforcing the anomaly scores tofollow this Gaussian prior would limit the representational abilityof neural networks and further distort the anomaly scoring space,resulting in suboptimal performance.To address the above challenges, we employ a score distributionestimator that is capable of estimating arbitrary distribution ofoutput anomaly scores. In this paper, we use the non-parametricKernel Density Estimation (KDE) method for estimating the arbi-trary anomaly score distribution that may be caused by the scarcityof labeled data or the anomaly contamination in the unlabeled data.Actually, other differentiable density estimators can also be appliedinto our proposed Overlap loss.If we denote the output anomaly score as (; ) = , the em-pirical cumulative distribution function (ECDF) can be defined as () = 1 =1 1 , where 1 is the indicator function and isthe number of partitions. () is an unbiased estimator ofthe cumulative distribution function (CDF) (), and can be furtherused for estimating the PDF by the following equation:",
  "(c) Proposed score distribution overlap": ": Anomaly score distribution overlaps. (a) The prior assumption of Gaussian distributions limits the representationalability of neural networks. (b) Overlap of arbitrary score distributions leads to the disorder in anomaly scores. (c) The proposedOverlap loss minimizes the overlap area of arbitrary score distributions while ensuring correct order in anomaly scores.",
  ", for estimating the unknown PDFs of anomalyscores, where the PDFs are further utilized to calculate the overlaparea of score distributions, as described in the following subsection": "3.3.2Overlap Area Calculation. Once we obtain the estimatedscore distributions (i.e., PDFs), the score distribution overlap canbe calculated as the overlap area of PDFs between normal samplesand the abnormal ones.An optional method is to directly use the integral to approximatethe score distribution overlap, as illustrated in Eq. 4. The overlaparea between the PDFs of and is formulated as the integral ofthe one with the smaller probability density:",
  "min(,)min (), ()(4)": "The main challenge of the above basic idea is that such a methoddoes not necessarily guarantee a correct gradient update directionfor anomaly scores, as illustrated in b. The neural networkscould minimize the overlap area of the score distributions, whilemistakenly assigning lower anomaly scores for the anomalies (e.g.,the left side in the score distribution of the anomalies in b)instead of the normal ones. This problem can be remedied throughthe multi-task learning form by combining Eq.4 with a rankingloss term , as shown in Eq.5. However, although it ensures theorder in anomaly scores, i.e., the anomaly scores of abnormal datashould be further ranked higher than that of normal data, sucha method may suffer from the difficult optimization problem inmulti-task learning, sometimes leading to worse performance anddata inefficiency compared to learning tasks individually .",
  "min(,)min (), () + max (0, )": "(5)Our proposed Overlap loss aims to calculate the overlap areaof arbitrary score distributions while ensuring the correct orderin anomaly scores. We manage to acquire the intersection point of these arbitrary score distributions (see c), and thescore distribution overlap between and in a training batchcan be further formulated as Eq.6, where () and () are theestimated CDF of normal and abnormal data, respectively. (, ) = ( > ) + ( < ) = 1 () + ()(6) As shown in c, the Overlap loss formulated in Eq.6 guar-antees the order in output anomaly scores. A small overlap areawith correct score order means a close to zero loss of (, ).If the anomaly scores of abnormal data are smaller than that ofnormal data, (, ) would penalize this disorder and be closeto 2, since both ( > ) and ( < ) are close to 1, respec-tively. Moreover, (, ) is naturally bounded to due to theproperty of PDF.However, for two arbitrary score distributions, we can not di-rectly calculate the intersection point by the formula suitable forGaussian distribution in Eq.1. Instead, we acquire the intersectionpoint as the corresponding x value of the non-zero element of in Eq.7 for the arbitrary score distribution scenario, where is generated by the arithmetic sequence = min (, ) + ( 1) max(,)min(,)",
  "= sgn (+1) (+1) sgn () ()(7)": "shows toy examples of calculating the intersectionpoint(s) . For most cases where there is only one intersectionpoint between () and (), is regarded as the x value of thesign change point of PDF differences, as shown in a and 3d.Even if the two score distributions are far apart (see b), wecould still extend the x range of their PDFs and acquire , as shownin e. It is worth noting that for the case in b, Over-lap loss would reach its upper bound with an overlap area of 2 forpenalizing the disorder in estimated anomaly scores, as illustratedin Eq.6. For the case where there exist multiple intersection points(shown in c and 3f), we randomly choose one of them as .We show in the Appendix that the detection performance of thisstrategy is very close to that of ensembling different intersectionpoints while improving the efficiency of model training.After that, the integral of CDF () can be approximated viathe trapezoidal rule in Eq.8, where is adjusted based onthe intersection point as = [ min (, )] /.",
  "Network Architecture": "Overlap loss is instantiated into an end-to-end neural network thatconsists of a feature representation layer (; ) and a scoringlayer (; ). The BatchNorm layer is applied after the scoringlayer to normalize the output anomaly scores. After that, scoredistributions of both normal data and anomalies are estimated bythe KDE estimators, where their score distribution overlap is furthercalculated via the proposed Overlap loss, as shown in .We point out that the proposed Overlap loss can be effectively in-tegrated into multiple popular network architectures, including thewidely-used MLP and AutoEncoder in AD tasks, and some cutting-edge architectures like ResNet and Transformer in the classificationtasks. Algorithm 1 provides detailed steps of instantiated modelsbased on our proposed Overlap loss.",
  "EXPERIMENTS4.1Experiment Setting": "Datasets. We apply 25 publicly available real-world datasets formodel evaluation. These datasets include several domains suchas disease diagnosis, speech recognition, and image identification.Detailed dataset description is illustrated in Appendix .1. For eachdataset, 70% data is split as the training set and the remaining 30%as the testing set, where the same proportion of anomalies is keptby the stratified sampling. We discuss the model performance in.2.1 w.r.t. different ratios of labeled anomalies to all trueanomalies = /( + ) in the training set, where labeledanomalies are sampled from the entire anomaly data and the restof instances remain unlabeled.Baselines. We compare the proposed method with the follow-ing baselines1, and summarize them according to their networkarchitectures and levels of supervision, as is shown in .",
  "FTTransformer . A Transformer architecture implementswith Feature Tokenizer. FTTransformer has been proven to bebetter than other DL solutions on tabular tasks": "Metrics. We evaluate the above models by two metrics: the AUC-ROC (Area Under Receiver Operating Characteristic Curve) and theAUC-PR (Area Under Precision-Recall Curve) values. We mainlyreport the AUC-PR results due to the space limit, and demonstratethe AUC-ROC results in the Appendix. We find that the results ofthese two metrics are generally consistent. Besides, we apply the 1Unlearning is not included here since it is originally proposed for the time-seriestask, while we explore the Minus loss of Unlearning in .3. We do not includethe results of DAGMM for comparison as it may not converge on some datasets.We exclude the semi-supervised Dual-MGAN method for comparison since it istoo computationally expensive.",
  "Tokenizer": ": AD model instantiated by the proposed Overlap loss, which consists of a representation layer (; ) and a scoringlayer (; ) with batch normalization. The output anomaly scores are used for estimating the score distributions (PDFs) ofnormal samples () and that of anomalies () via the KDE estimators. Finally, the calculated overlap area of anomaly scoredistributions is minimized. pairwise Wilcoxon signed rank test to examine the significanceof proposed methods against its competitors.Training details For the proposed Overlap loss based AD mod-els, we use the SGD optimizer with 0.001 learning rate and 0.7momentum. The weight decay is set to 0.01. The bandwidth in theKDE method is set to 1. The in Eq.7~9 is set to 1000 by default.We train the Overlap loss based MLP and AutoEncoder models(namely MLP-Overlap and AE-Overlap) for 20 epochs, where batchsize of 256 is used. For ResNet and FTTransformer architectures, wetrain the Overlap loss based models (namely ResNet-Overlap andFTTransformer-Overlap) 100 training epochs just as their originalpaper. We provide the training details of compared baselines inAppendix .2, which are mainly according to their original papers.All the experiments are run on a Tesla V100 GPU accelerator.",
  "Experimental Results": "4.2.1Model Performance. shows the average model perfor-mance over 25 real-world datasets, and we report the full results inthe Appendix of supplementary materials. Above all, we verify theeffectiveness of the proposed Overlap loss on various network ar-chitectures, including MLP, AutoEncoder, ResNet, and Transformer.The Overlap loss based AD models generally outperform corre-sponding baselines w.r.t. the ratios of labeled anomalies = 5%, = 10% and = 20%.Specifically, experimental results show that the MLP-Overlapachieves a relative improvement Perf. of AUC-PR over its coun-terpart DevNet 2.89% and PReNet 1.82% w.r.t. = 5%. These re-sults indicate that compared to the current state-of-the-art weakly-supervised AD methods, Overlap loss is still more effective whenonly a handful of labeled anomalies (say 5% labeled anomalies) are available in the training process, considering that such limitedlabel information would bring challenges for estimating anomalyscore distribution. Besides, we show that end-to-end AD meth-ods, including DevNet, PReNet, and our MLP-Overlap, statisticallyoutperform those unsupervised (e.g. Iforest) or semi-supervisedrepresentational learning (e.g., DeepSAD) AD methods, since end-to-end anomaly score learning can leverage the data much moreefficiently than the two-step AD approaches .For = 10%, the relative improvement of Overlap loss basedmodel is more significant, as more labeled anomalies are beneficialfor the Overlap loss to estimate more accurate anomaly score distri-butions, and thus to better measure the score distribution overlap.The MLP-Overlap Perf. of AUC-PR over DevNet is 7.75% and5.67% over PReNet w.r.t. = 10%. Furthermore, we prove the supe-riority of the proposed Overlap loss on other network architecturessuch as AutoEncoder and ResNet. In terms of AUC-PR, the Perf. ofAE-Overlap over fully- and weakly-supervised FEAWAD is 28.04%and 9.29%, respectively, and Perf. of ResNet-Overlap over ResNetis 56.30%, where all the relative improvements are significant at 1%significance level. Although FTTransformer has been proven to bea strong solution for tabular-based tasks , we still observe Perf. of FTTransformer-Overlap over FTTransformer 5.50%~6.61%on AUC-PR. 4.2.2Runtime Analysis. We show the model training time in Fig-ure 5. This result shows that ECOD is the fastest algorithm asit treats each feature independently. Both MLP-Overlap and AE-Overlap are faster than their counterparts, since our methods needfewer training epochs while achieving better detection performance.For ResNet and FTTransformer (FTT) architectures, our methodsare comparable to or relatively slower than the counterparts. This",
  "FTTransformer-Overlap (ours)Weak0.6270.277/0.6860.282/0.7300.285/": "is mainly due to the fact that Overlap loss requires more trainingepochs for more complex network architectures (especially for FT-Transformer) than those simple architectures like MLP. Thereforewe apply the same training strategy (100 epochs with early stop-ping) for ResNet and FTTransformer, as well as our Overlap lossbased versions ResNet-Overlap and FTTransformer-Overlap. Theextra training time is mainly caused by the calculation of Overlaploss, compared to the supervised binary cross entropy loss. IForest ECOD DeepSVDD GANomaly DeepSAD",
  ": Boxplot of model training time": "4.2.3Ablation Study. In , we report the AUC-PR results ofseveral basic methods mentioned in . We instantiate thebasic method of Overlap-Gaussian by replacing the scoring layer (; ) with the VAE structure, where the anomaly scores ofnormal and abnormal data are sampled from their correspondingGaussian distribution via the reparameterization trick . Thecalculated intersection point of Eq.1 can be used for estimatingscore distribution overlap via Eq.6.First, we observe that Overlap-Gaussian has the worst perfor-mance. This is because the scarceness of labeled anomalies makesits score distribution often present a certain arbitrariness, whereasthe Gaussian assumption is detrimental to the representation ofscoring function. Second, the disorder in anomaly scores leads toperformance degradation in the Overlap-Arbitrary. Ranking lossterm can be served as an effective way to guarantee the order in anomaly scores, as the Overlap-Combined method significantlyimproves the AUC-PR performance. Third, the proposed Overlaploss outperforms all basic methods in most cases, since it can ef-fectively estimate arbitrary score distributions of output anomalyscores while avoiding the score disorder problem that occurs in theOverlap-Arbitrary method. Compared to the Overlap-Combinedmethod, Overlap-Proposed achieves better performance, probablybecause it realizes a unified loss function form, rather than a com-bination of two different loss parts. Besides, we observe that themulti-task loss form in the Overlap-Combined method fails in morecomplex network backbones like FTTransformer.",
  "Further Exploration into AD Loss Functions": "While most of the existing research focuses on proposing and eval-uating specific models or architectural designs of AD methods ,we manage to go a step further and directly compare different lossfunctions in the same network architecture. We introduce the de-coupling methods in the Neural Architecture Search (NAS) problem, where we mainly concern the design space of loss func-tions instead of other perspectives like architecture settings .Such an analytical method could eliminate the effects of modelconfigurations such as dropout and activation layers, while fullyfocusing on the role of loss functions (i.e., training objectives) inthe anomaly detection tasks.We decouple the loss functions in several popular AD modelsmentioned in , including the Minus loss in Unlearning ,Inverse loss in DeepSAD , Hinge loss in REPEN , Deviationloss in DevNet and FEAWAD , Ordinal loss in PReNet, and our proposed Overlap loss, as shown in . For theconsistency of comparison, we replace the original reconstructionerror in Minus loss and the Euclidean distance of embedding inInverse loss with the absolute anomaly score. A network with one-hidden-layer of 20 neurons is applied to ensure the comparabilityof different loss functions. The ReLU activation layer is employed",
  "KDD 23, August 610, 2023, Long Beach, CA, USAMinqi Jiang, Songqiao Han, & Hailiang Huang": ": AUC-PR results of ablation studies. Overlap-Gaussian refers to the basic method mentioned in .3.1. Overlap-Arbitrary refers to the basic method of Eq.4. Overlap-Ranking isolates the ranking loss in Eq.5. Overlap-Combined correspondsto the combined loss form of both Overlap-Arbitrary and Overlap-Ranking as illustrated in Eq.5. Overlap-Proposed refers tothe final solution in this paper.",
  "MinusL = | | + max (0, | |)InverseL = | | + 1/| |HingeL = max (0, + )DeviationL = | | + max (0, )OrdinalL =, , +, , +, ,OverlapEq.9": "in this network. We train the network for 200 epochs of 256 batchsize and use the SGD optimizer with 0.01 learning rate and 0.7momentum. The weight decay is set to 0.01. The hyperparameter in the Minus loss is set to 5, and the hyperparameter in the Hinge and Deviation loss is set to 5. The anomaly scoresin Deviation loss are normalized as Z-Score. Consistent with theoriginal paper, we set ,, , and , in the Ordinal loss to 0,4, and 8, respectively. We first investigate different loss functionson 25 real-world datasets and then report their performances indetecting various types of anomalies. 4.3.1Exploration of AD Loss Functions on Real-world Datasets. Inthis subsection, we analyze different loss functions on real-worlddatasets with respect to the following two perspectives: (i) Embed-ding transformation. The transformed embedding of input features can be seen as a visualization of representation layer variationfor realizing the training objective. (ii) Network Parameter Changes.This is often discussed in the continual learning problem, wheredrastic changes in network parameters may suffer from the problemof catastrophic forgetting . Similarly, we investigatethe network parameter changes of different loss function based ADmodels when achieving their corresponding training objectives.Embedding Transformation during Model Training. Wetake the vowels dataset as an example to demonstrate the embed-ding transformation in the feature representation layer during thetraining process, as shown in . Similar experimental resultscan be observed in other real-world datasets, and we provide theseresults in the Appendix of supplementary materials. indi-cates that the Deviation and Ordinal loss tend to seriously distortthe embedding of original input data after a few training epochs.This is due to the fact that these two loss functions explicitly guidenetworks to map the anomaly score of each instance or pair to one or more fixed score constants or a score margin, thus hindering thediversity of learned feature representation. Besides, the unlabelednormal samples are contaminated by the unlabeled anomalies, anddefining an identical training target for these two types of datawould limit the representational ability of the learned models.Our proposed Overlap loss generates a relatively mild trans-formation of the input features. As mentioned before, Overlaploss based AD models can achieve superior detection performance,therefore we speculate that good detection performance does notalways require an excessive transformation in the representationspace, where the model can merely transform the embeddings thathave the most impact on the score distribution discrimination andremain more fine-grained information of input data.Network Parameter Changes. We show the results of net-work parameter changes on the 25 real-world datasets in ,where the sum of norms of parameter differences in each layerare calculated between the initialized model and its updated ver-sion as 022. The result indicates that compared to theother loss functions, the AD model based on our proposed Overlaploss inherits smaller network parameter changes. This result corre-sponds to the good properties of Overlap loss, where (i) Overlaploss is naturally bounded, avoiding drastic updating of anomalyscores (and network parameters). (ii) Overlap loss does not requirethe prior target of anomaly score, therefore reducing unnecessaryscoring function updates in the training stage and being capableof adapting to different datasets with minimum adjustment of thescore distribution. MinusInverseHingeDeviation OrdinalOverlap 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Parameter Changes (log)",
  ": Network parame-ter changes in the trainingstage": "4.3.2Exploration of AD Loss Functions on Different Types of Anom-alies. While extensive AD methods have been proven to be effectiveon real-world datasets, previous studies often neglect to discussthe pros and cons of AD methods regarding specific types of anom-alies . In fact, public datasets often consist of a mixture ofdifferent types of anomalies. We follow to create realisticsynthetic datasets based on the above 25 datasets by injecting fourtypes (namely local, global, clustered, and dependency) of anomalies",
  "Minus0.2550.8220.9920.369Inverse0.2350.6470.9000.198Hinge0.2710.8530.9960.413Deviation0.2460.8510.9870.303Ordinal0.2470.8490.9910.327Overlap0.4390.9290.9980.571": "shows the AUC-PR results of loss function comparisonon different types of anomalies. These results are consistent withthe findings in , where the other loss functions devised for semi-or weakly-supervised AD algorithms perform relatively poorly onlocal and dependency anomalies. Unlike clustered anomalies, thepartially labeled anomalies of local and dependency anomalies cannot well capture all characteristics of specific types of anomalies,and learning such decision boundaries for separating normal andabnormal data is often challenging (see Figure A2g~A2k in theAppendix). Therefore, the incomplete label information may biasthe learning process of these loss functions, which explains theirrelatively inferior performances compared to the Overlap loss.In contrast, Overlap loss performs significantly better on local,global, and dependency anomalies, and achieves satisfactory re-sults on clustered anomalies. For example, the average AUC-PRof Overlap loss on the local anomalies is 0.439, compared to thesecond-best Hinge loss of 0.271. Similar result can be observed forthe dependency anomalies, where the AUC-PR of Overlap loss is0.571, compared to the second-best Hinge loss of 0.413.Such results verify that Overlap loss can effectively leverage theprior knowledge of both partial labels and anomaly types. Thatis to say, Overlap loss based AD models (e.g., ResNet-Overlap) can achieve superior performance when only a limited numberof labeled anomalies are available during the training stage. Fur-thermore, if one could get access to the valuable prior knowledgeof anomaly types , Overlap loss can be served as an effectivesolution to learn the pattern of this specific type (e.g., dependency)of anomalies.5CONCLUSION In this paper, we propose a novel loss function called Overlap lossfor AD tasks. Overlap loss liberates the AD models from the pre-defined anomaly score targets, e.g., predefined constant or marginhyperparameter(s), thus adapting well to various datasets. By di-rectly optimizing distribution overlap to realize score distributiondiscrimination, Overlap loss can retain more fine-grained informa-tion of input data, and also avoids dramatic changes in networkparameters which may lead to overfitting or catastrophic forgettingproblem. Extensive experimental results verify that the proposedOverlap loss can be effectively instantiated to different network ar-chitectures, including MLP, AutoEncoder, ResNet, and Transformer.Moreover, Overlap loss significantly outperforms other popular ADloss functions on various types of anomalies.For the future, we plan to improve the optimization process ofOverlap loss by leveraging more complex score distribution esti-mators, such as the Gaussian Mixture Model (GMM) . Besides,we will extend our work to more general scenarios in weakly-supervised AD tasks , such as the inaccurate supervision and inexact supervision problems.6ACKNOWLEDGMENTS We thank anonymous reviewers for their insightful comments anddiscussions. We appreciate the paper suggestions from all SUFE AILab members. Hailiang Huang is supported by the National NaturalScience Foundation of China under Grant No. 72271151. We thankthe financial support provided by FlagInfo-SHUFE Joint Laboratory.",
  "Samet Akcay, Amir Atapour-Abarghouei, and Toby P Breckon. 2018. Ganomaly:Semi-supervised anomaly detection via adversarial training. In Asian conferenceon computer vision. Springer, 622637": "Samet Akay, Amir Atapour-Abarghouei, and Toby P Breckon. 2019.Skip-ganomaly: Skip connected and adversarially trained encoder-decoder anomalydetection. In 2019 International Joint Conference on Neural Networks (IJCNN).IEEE, 18. Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, andTinne Tuytelaars. 2018. Memory aware synapses: Learning what (not) to forget.In Proceedings of the European Conference on Computer Vision (ECCV). 139154. Simon Duque Anton, Suneetha Kanoor, Daniel Fraunholz, and Hans Dieter Schot-ten. 2018. Evaluation of machine learning-based anomaly detection algorithmson an industrial modbus/tcp data set. In Proceedings of the 13th internationalconference on availability, reliability and security. 19.",
  "Forecasting against Distribution Shift. In International Conference on LearningRepresentations": "Diederik P. Kingma and Max Welling. 2014. Auto-Encoding Variational Bayes. In2nd International Conference on Learning Representations, ICLR 2014, Banff, AB,Canada, April 14-16, 2014, Conference Track Proceedings, Yoshua Bengio and YannLeCun (Eds.). James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, GuillaumeDesjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, AgnieszkaGrabska-Barwinska, et al. 2017. Overcoming catastrophic forgetting in neuralnetworks. Proceedings of the national academy of sciences 114, 13 (2017), 35213526.",
  "Meng-Chieh Lee, Shubhranshu Shekhar, Christos Faloutsos, T Noah Hutson, andLeon Iasemidis. 2021. Gen 2 Out: Detecting and Ranking Generalized Anomalies.In Big Data. IEEE, 801811": "Yuening Li, Zhengzhang Chen, Daochen Zha, Kaixiong Zhou, Haifeng Jin,Haifeng Chen, and Xia Hu. 2021. Autood: Neural architecture search for outlierdetection. In 2021 IEEE 37th International Conference on Data Engineering (ICDE).IEEE, 21172122. Zhe Li, Chunhua Sun, Chunli Liu, Xiayu Chen, Meng Wang, and Yezheng Liu.2022. Dual-MGAN: An Efficient Approach for Semi-supervised Outlier Detectionwith Few Identified Anomalies. ACM Transactions on Knowledge Discovery fromData (TKDD) (2022). Zheng Li, Yue Zhao, Xiyang Hu, Nicola Botta, Cezar Ionescu, and George Chen.2022. Ecod: Unsupervised outlier detection using empirical cumulative distribu-tion functions. IEEE Transactions on Knowledge and Data Engineering (2022).",
  "Emilio Parisotto, Lei Jimmy Ba, and Ruslan Salakhutdinov. 2016. Actor-Mimic:Deep Multitask and Transfer Reinforcement Learning. In ICLR (Poster)": "Tahereh Pourhabibi, Kok-Leong Ong, Booi H Kam, and Yee Ling Boo. 2020. Frauddetection: A systematic literature review of graph-based anomaly detectionapproaches. Decision Support Systems 133 (2020), 113303. Theodoros Rekatsinas, Saurav Ghosh, Sumiko R Mekaru, Elaine O Nsoesie, John SBrownstein, Lise Getoor, and Naren Ramakrishnan. 2015. Sourceseer: Forecastingrare disease outbreaks using multiple data sources. In Proceedings of the 2015SIAM International Conference on Data Mining. SIAM, 379387.",
  "Oren Rippel, Manohar Paluri, Piotr Dollar, and Lubomir Bourdev. 2015. Metriclearning with adaptive density discrimination. arXiv preprint arXiv:1511.05939(2015)": "Lukas Ruff, Jacob R Kauffmann, Robert A Vandermeulen, Grgoire Montavon,Wojciech Samek, Marius Kloft, Thomas G Dietterich, and Klaus-Robert Mller.2021. A unifying review of deep and shallow anomaly detection. Proc. IEEE 109,5 (2021), 756795. Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib AhmedSiddiqui, Alexander Binder, Emmanuel Mller, and Marius Kloft. 2018. Deep one-class classification. In International conference on machine learning. 43934402. Lukas Ruff, Robert A. Vandermeulen, Nico Grnitz, Alexander Binder, EmmanuelMller, Klaus-Robert Mller, and Marius Kloft. 2020. Deep Semi-SupervisedAnomaly Detection. In 8th International Conference on Learning Representations,ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Jonas Soenen, Elia Van Wolputte, Lorenzo Perini, Vincent Vercruyssen, WannesMeert, Jesse Davis, and Hendrik Blockeel. 2021. The Effect of HyperparameterTuning on the Comparative Evaluation of Unsupervised Anomaly DetectionMethods. In Proceedings of the KDD21 Workshop on Outlier Detection and De-scription. Outlier Detection and Description Organising Committee, 19.",
  "Additional Training Details": "For unsupervised baselines, Iforest, ECOD, and DeepSVDD arebuilt using the PyOD library. Labeled anomalies are combinedwith unlabeled data for constructing the validation set, in orderto tune the hyperparameters of these unsupervised methods viathe grid search method, since tuning their hyperparameters on asmall validation set often yields better performance than using thedefault settings . Table A2 shows the hyperparameter grids,where ECOD is not considered since it is a parameter-free method.",
  "We replace the convolutional layer in the original GANomalywith the dense layer for evaluating it on the tabular data, where thehidden size of the encoder-decoder-encoder structure of GANomaly": "is set to half of the input dimension. We realize the PReNet in Py-Torch as we do not find the open-source codes, and set the hy-perparameters in PReNet according to its original paper. Othermodels are built based on their corresponding source codes. If notspecified, we train these baseline models according to their defaulthyperparameters mentioned in the original papers.",
  "Experimental Results of SelectingIntersection Points": "We demonstrate the experimental results of different intersectionpoint selection strategies in Table A3. The detection performanceof a randomly selected intersection point is very close to that ofensembling all the calculated intersection points w.r.t. differentratios of labeled anomalies = 5%, = 10% and = 20%. This isdue to the fact that the calculation of the overlap area repeats manytimes (epochsbatchsize), which is essentially similar to the averageresults of the ensemble strategy. Moreover, random sampling of theintersection points can improve computational efficiency since theoverlap area only needs to be estimated once in a training batch. Table A3: AUC-ROC and AUC-PR results of different inter-section point selection strategies. MLP-Overlap correspondsto the default strategy that randomly chooses one of the in-tersection points for estimating the overlap area via Eq.9.MLP-Overlap-E refers to the ensemble strategy by takingthe average of the overlap areas calculated based on eachintersection point.",
  "Detailed Experimental Results": "We show the AUC-ROC results of model performance in Table A4and ablation study in Table A5, corresponding to .2.1 and4.2.3 in the main paper, respectively. These experimental resultsare basically consistent with the main paper. Besides, we showthe detailed results of model comparison on 25 real-world datasetsw.r.t. = 5%, = 10% and = 20% in Table A6~A11. The bestperforming method(s) is marked in bold.",
  "TransformerFTTransformerSup0.8270.159+3.00%0.8590.146+1.80%0.8890.129+1.23%FTTransformer-Overlap (ours)Weak0.8510.138/0.8740.130/0.9000.127/": "Table A5: AUC-ROC results of ablation studies. Overlap-Gaussian refers to the basic method mentioned in .3.1.Overlap-Arbitrary refers to the basic method of Eq.4. Overlap-Ranking isolates the ranking loss in Eq.5. Overlap-Combinedcorresponds to the combined loss form of both Overlap-Arbitrary and Overlap-Ranking as illustrated in Eq.5. Overlap-Proposedrefers to the final solution in this paper.",
  "Additional Results of AD Loss FunctionExploration": "In addition to the main paper that demonstrates the embeddingvariations on the vowels dataset in .3.1, here we provideanother example of the skin dataset, as shown in Figure A1. Com-pared to the other loss functions, our proposed Overlap loss betterretains the ringlike shape in the embedding of input feature whileachieving satisfactory detection performance.We follow to generate the following four types of syn-thetic anomalies, which are further used to evaluate different ADloss functions. The AUC-ROC results of loss function comparisonon different types of anomalies also indicate that our proposedOverlap loss significantly outperforms other counterparts, as isshown in Table A12. Local anomalies refer to the anomalies deviant from their localneighborhoods . GMM procedure is used to generatesynthetic normal samples, and then scale the covariance matrix = by a scaling parameter = 5 to generate local anomalies.",
  "Unif min, max, where the boundaries are de-fined as the min and max of an input feature, e.g., -th feature, and = 1.1 controls the outlyingness of anomalies": "Dependency anomalies refer to the samples that do not fol-low the dependency structure that normal data follows , i.e.,the input features of dependency anomalies are assumed to beindependent of each other. Vine Copula method is appliedto model the dependency structure of original data, where theprobability density function of generated anomalies is set to com-plete independence by removing the modeled dependency (see). KDE method estimates the probability density function offeatures and generates normal samples. Clustered anomalies, also known as group anomalies ,exhibit similar characteristics . We scale the mean featurevector of normal samples by = 5, i.e., = , where controlsthe distance between anomaly clusters and the normals, and usethe scaled GMM to generate anomalies.",
  "Minus0.6290.9360.9960.738Inverse0.5470.8230.9370.570Hinge0.6070.9380.9970.761Deviation0.5880.9590.9900.652Ordinal0.6040.9540.9940.687Overlap0.7420.9810.9980.847": "We further investigate two case studies by generating visualizedtwo-dimensional synthetic samples of the above local and clusteredanomalies, as shown in Figure A2. The anomaly ratios of these twodatasets are set to 5%. The results indicate that all the compared lossfunctions can correctly detect anomalies for the two-dimensionalclustered anomalies (with 1.000 AUC-ROC and AUC-PR). This resultcan be expected since few labeled clustered anomalies can alreadyrepresent similar behaviors of the entire clustered anomalies. Forthe local anomalies, however, we observe most of the compared lossfunctions perform poorly. In contrast, Overlap loss achieves betterdetection performance, and successfully learns a suitable decisionboundary (see Figure A2l), where the learned decision boundaryfits well with the local anomalies that are often overlapped orsurrounded by the normal samples."
}