{
  "ABSTRACT": "Generative retrieval has recently emerged as a promising approachto sequential recommendation, framing candidate item retrieval asan autoregressive sequence generation problem. However, existinggenerative methods typically focus solely on either behavioral orsemantic aspects of item information, neglecting their complemen-tary nature and thus resulting in limited effectiveness. To addressthis limitation, we introduce EAGER, a novel generative recommen-dation framework that seamlessly integrates both behavioral andsemantic information. Specifically, we identify three key challengesin combining these two types of information: a unified generativearchitecture capable of handling two feature types, ensuring suffi-cient and independent learning for each type, and fostering subtleinteractions that enhance collaborative information utilization. Toachieve these goals, we propose (1) a two-stream generation archi-tecture leveraging a shared encoder and two separate decoders todecode behavior tokens and semantic tokens with a confidence-based ranking strategy; (2) a global contrastive task with summarytokens to achieve discriminative decoding for each type of informa-tion; and (3) a semantic-guided transfer task designed to implicitlypromote cross-interactions through reconstruction and estimationobjectives. We validate the effectiveness of EAGER on four publicbenchmarks, demonstrating its superior performance compared",
  "Equal contribution.Corresponding authors": "KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.This is the authors version of the work. It is posted here for your personal use. Notfor redistribution. The definitive Version of Record was published in Proceedings ofthe 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 24),August 2529, 2024, Barcelona, Spain,",
  "Generative Recommendation, Autoregressive Generation, SemanticTokenization, Behavior-Semantic Collaboration": "ACM Reference Format:Ye Wang, Jiahao Xun, Minjie Hong, Jieming Zhu, Tao Jin, Wang Lin, HaoyuanLi, Linjun Li, Yan Xia, Zhou Zhao, and Zhenhua Dong. 2024. EAGER: Two-Stream Generative Recommender with Behavior-Semantic Collaboration.In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discoveryand Data Mining (KDD 24), August 2529, 2024, Barcelona, Spain. ACM, NewYork, NY, USA, 10 pages.",
  "INTRODUCTION": "Recommender systems are widely adopted solutions for manag-ing information overload, designed to identify items of interestto users from a large item corpus. Modern recommender systemstypically integrate representation learning and search index con-struction to refine the matching process. Initially, users and itemsare encoded into latent representations within a shared latent spaceusing models like two-tower architectures and sequentialrecommendation models . Subsequently, to efficiently re-trieve top-k items for users, approximate nearest neighbor (ANN)search indexes are constructed using tools such as Faiss andSCANN . Despite notable progress, the separate phases of repre-sentation learning and index construction often operate indepen-dently, presenting challenges for achieving end-to-end optimization",
  ": A framework of generative recommendation": "and consequently limiting the overall effectiveness of recommendersystems .To address this limitation, many research efforts have been made.One prominent direction involves constructing tree-based matchingindexes , which optimize both a matching model and a treestructure index for items. However, these methods often face chal-lenges such as low inference efficiency due to the tree structure andlimited utilization of item semantic information . Recently,generative retrieval has emerged as a promising new para-digm for information retrieval, which has been recently applied forgenerative recommendation . Unlike traditional representation-based user-item matching approaches, this paradigm employs anend-to-end generative model that predicts candidate item identifiersdirectly in an autoregressive manner. Specifically, Specifically, thesemethods begin by tokenizing each item into a set of discrete se-mantic codes1 = {1,2, ...}, and then utilize an encoder-decodermodel (e.g. Transformer ) to serve as an end-to-end index forretrieval. In this setup, the encoder encodes the interaction history{1,2, ...,1} between users and item, while the decoder predictsthe code sequence of the next item . The overall framework isillustrated in .However, existing generative recommendation approaches sufferfrom a significant drawback in how they utilize item information,often focusing narrowly on either behavioral or semantic aspects.Behavioral information is derived from user-item interaction his-tories, while semantic information encompasses textual or visualdescriptions of items. For instance, RecForest utilizes a pre-trained DIN model to extract behavior-based item embeddingsfor constructing semantic codes, while TIGER utilizes the Sentence-T5 model to derive semantic-based item embeddings fromtextual descriptions. However, these approaches often focus exclu-sively on one aspect, overlooking the complementary relationshipbetween behavior and semantics. On one hand, advances in pre-trained modality encoders such as BERT and ViT facilitatethe integration of multimodal features, enhancing prior knowledgeand finding wide applications in multimodal recommendation mod-els . On the other hand, behavioral data captures user-specificpreferences through interaction sequences, making it particularlyeffective in recommendation contexts. Conversely, semantic infor-mation offers broader, unbiased insights into item characteristics,fostering better generalization across different domains.In this paper, we propose EAGER, a novel two-strEAm GEnerati-ve Recommender with behavior-semantic collaboration. We ana-lyze the challenges of modeling behavior and semantics within a",
  "Note that we use \"code\" and \"token\" interchangeably": "unified generative framework and address them from the followingthree aspects:Firstly, a unified generative architecture for handling twodistinct types of information is crucial. Given the inherent differ-ences in feature spaces between behavior and semantics, directlyintegrating them through feature fusion at the encoder side poseschallenges, as demonstrated in previous two-tower models .Therefore, our approach constructs separate codes for behaviorand semantics, employing a two-stream generation architecturewhere each serves as a distinct supervision signal at the decoderside. This architecture includes a shared encoder for encoding userinteraction history and two separate decoders for predicting behav-ior and semantic codes respectively, thereby avoiding prematurefeature interaction. During inference, we enhance the merging ofresults from both streams by utilizing the prediction entropy ofeach stream as a confidence measure for item ranking, ensuringeffective predictions.Secondly, ensuring sufficient and independent learning iscrucial to fully leverage the potential value of each type of informa-tion. Previous works have typically employed autoregressiveapproaches to learn each token one by one, focusing on discrete andlocal information rather than capturing global insights. In EAGER,we introduce a global contrastive task with a summary token. Thismodule draws inspiration from two main sources: (1) traditionaldual-tower models use contrastive learning to acquire discrimina-tive item features. Similarly, we aim for our decoder model to graspglobal discriminatory capabilities alongside its autoregressive gen-eration capability, thereby enhancing the extraction of item featureswithin a contrastive paradigm; (2) Transformer models utilizespecial tokens to encapsulate global information, prompting us toappend a summary token at the end of the token sequence. Thistoken summarizes the accumulated knowledge in a unidirectionalmanner, serving as the focal point for distillation.Thirdly, while separate decoding and prediction reranking haveshown effectiveness, integrating subtle interaction can enhancesharing of both knowledge flows. As mentioned earlier, directfeature-level interactions often yield sub-optimal outcomes .Therefore, we introduce a carefully crafted semantic-guided transfertask to promote implicit knowledge exchange. Specifically, we pro-pose that semantic information can guide behavioral aspects, andwe design an auxiliary transformer with dual objectives: reconstruc-tion and recognition. The reconstruction objective involves predict-ing masked behavior tokens using global semantic features, whilethe recognition objective aims to differentiate whether a behaviortoken aligns with a specified global semantic feature. Through theseobjectives, this module indirectly optimizes interaction betweenbehavioral and semantic features using the transformer model.In summary, our main contributions are as follows:",
  "RELATED WORK": "Sequential Recommendation. Using deep sequential models tocapture user-item patterns in recommender systems has developedinto a rich literature. GRU4REC was the first to use GRU-basedRNNs for sequential recommendations. SASRec adopts self-attention which is similar to decoder-only transformer models.Inspired by the success of masked language modeling in languagetasks, BERT4Rec utilizes transformer models with maskingstrategies for sequential recommendation tasks. 3-Rec notonly relies on masking but also pre-trains embeddings throughfour self-supervised tasks to enhance the quality of item and userembeddings. The above mentioned methods mainly depend on anapproximate nearest neighbor (ANN) search index (e.g., Faiss )to retrieve the next item. Besides, tree-based methods have shown promising performance in recommender systems. Forexample, RecForest constructs a forest by creating multiple treesand integrates a transformer-based structure for routing operations.Recently, TIGER introduced the idea of semantic id, whereeach item is represented as a set of tokens derived from its side in-formation, and then predicts the next item tokens in a seq2seq way.In this work, we aim to further explore a two-stream generationarchitecture to act as an end-to-end index for top-k item retrieval. Generative Retrieval. Generative retrieval has been recentlyproposed as a new retrieval paradigm, which consists of two mainphases: discrete semantic tokenization and autoregres-sive sequence generation . In the domain of documentretrieval, researchers have explored the use of pre-trained languagemodels to generate diverse types of document identifiers. Notably,DSI and NCI leverage the T5 model to produce hi-erarchical document IDs. Conversely, SEAL (with BART backbone) and ULTRON (using T5) utilize titles or substringsas identifiers. Another approach, AutoTSG , adopts term-setsfor identification purposes. Generative document retrieval has alsoextended to various domains. For instance, IRGen employs aViT-based model for image search, while TIGER utilizes T5for recommender systems. However, these studies often face chal-lenges in large-scale item retrieval within recommender systemsdue to the resource-intensive nature of pre-trained language mod-els. In contrast, our paper delves into integrating both behavior andsemantics for generative retrieval in such systems.",
  "METHOD3.1Problem Formulation": "Given the entire set of items X and interacted items history X ={x1, x2, , x1} X of a user, the sequence recommendationsystem returns a list of item candidates for the next item x.In generative recommendation, the identifier of each item x isrepresented as a serialized code Y = [y1, y2, , y] Y, where is the length of the code. The goal of the generative model islearning a mapping : X Y, which takes a users interacted itemsequence as input and generates item codes (candidate identifiers). For training, the model first feeds the users behavior X into theencoder, then leverages the auto-regressive decoder to generatethe item code Y step by step. The probability of interaction can becalculated by:",
  "Overall Pipeline": "We present our overall EAGER framework in . EAGER con-sists of (1) a two-stream generation architecture to unify item rec-ommendation for both behavior and semantic information, (2) aglobal contrastive task with a summary token to capture globalknowledge for better auto-regressive generation quality, and (3) asemantic-guided transfer task to achieve the cross-information andcross-decoder interaction.First, in our two-stream generation architecture, we model userinteractions history and obtain interaction features via the encoder.Then we extract both behavior and semantic features to constructtwo codes, and leverage two decoders to separately predict them inan auto-regressive way. Meanwhile, we optimize a summary tokenin our global contrastive task and leverage it to improve cross-decoder interaction in our semantic-guided transfer task. Aftertraining, we adopt a confidence-based ranking strategy to mergethe results from two different predictions.",
  "Two-stream Generation Architecture": "To handle two different information, i.e. behavior and semantic, weleverage the powerful modeling capabilities of transformer modelsand design a two-stream generation architecture. This frameworkconsists of a shared encoder for modeling user interaction, twoseparate codes and decoders for two-stream generation.Shared Encoder. The sequence modeling of user interaction his-tory X = {x1, x2, } is based on the stacked multi-head self-attention layers and feed-forward layers, as proposed in Trans-former. It is worth nothing that we only adopt a shared encoderinstead of two encoders, which is enough to generate rich repre-sentation for the subsequent separate decoding. We denote theencoded historical interaction features as H = Encoder(X).Dual Codes. We first extract the behavior and semantic item em-beddings E and E using two pre-trained models, where the behav-ior encoder is a two-tower model (e.g. DIN ) that only uses IDsequence for recommendation and the semantic encoder is a gen-eral modality representation model (e.g. Sentence-T5). With the twoextracted embeddings, we separately apply the widely-used hierar-chical k-means algorithm to each one, where each cluster is evenlydivided into K child clusters until each child cluster merely containsone single item. As shown in , we can obtain two codes Y and Y, corresponding to behavior and semantic, respectively.Dual Decoders. To accommodate two different codes, we employtwo separate decoders to decode and generate the prediction foreach of them, allowing each decoder to specialize in one single code.Compared to one shared decoder that generates two identifiersin an auto-regressive way, such design mitigates the supervisiondifference and offers higher efficiency with parallel generation. For",
  "Global Contrastive Task": "To endow each generative decoder with a sufficient discriminativecapability, we design a global contrastive task with a summarytoken to distill the global knowledge.Summary Token. For the input Y of each decoder, we consider theleft-to-right order of auto-regressive generation and insert a learn-able token y[EOS] at the end of the sequence to construct modifiedinputs Y = {ySOS, y1, y2, , y, y[EOS]}. This design encouragesthe preceding tokens in the codes to learn more comprehensiveand discriminative knowledge, enabling the final token to make asummary. During updates, the gradient on the summary token canbe backpropagated to the preceding tokens. Contrastive Distillation. To make the summary token captureglobal information, we adopt contrastive learning paradigm to dis-till the item embedding E and E from the pre-trained encoder.Here we adopt the positive-only contrastive metric instead ofcommonly used Info-NCE to achieve this objective. The full lossLcon is given by summing two losses Lcon and Lcon, where eachis given by:",
  "Semantic-guided Transfer Task": "Through the aforementioned components, our model can effectivelyutilize two types of information for prediction. However, we donot stop at this point. Instead of completely independent decoding,we further propose a semantic-guided transfer task to utilize thesemantic knowledge to guide the behavior learning.To enable the knowledge flow between two sides while avoidingdirect interaction, we build an independent bidirectional Trans-former decoder as an auxiliary module. We first add a token y[cls] atthe beginning of behavior codes to obtain the sequence {y[cls], y1, y2, , y } as the input to the decoder. Then the embedding EOS ofthe semantic summary token y[EOS] is input to the cross atten-tion, allowing each behavior token in the decoder to attend overglobal feature of the semantic. We denote the output features as{r[cls], r1, r2, , r }. To conduct our transfer training, we designtwo following objectives: reconstruction and recognition.",
  "Beauty22,36312,101198,3600.00073Sports and Outdoors35,59818,357296,1750.00045Toys and Games19,41211,924167,5260.00073Yelp30,43120,033316,3540.00051": "Reconstruction. We reconstruct the masked behavior codes viathe semantic global feature, aiming to enable the each behaviortoken benefit from the semantic. For reconstruction training, werandomly mask m% of tokens in the behavior code to obtain themasked code {y[cls], y1, y[mask], , y }, where y[mask] means themasked token. Then, we obtain the corresponding output features{r[cls], r1, r[mask], , r } and apply the contrastive loss to developthe reconstruction by:",
  "(4)": "where y is the feature of ground truth of the -th masked token,y is the feature of sampled tokens.Recognition. Besides, we also build a binary classifier to judgewhether the behavior codes is relevant or irrelevant to the seman-tic global feature. For recognition training, we construct negativesamples by randomly replacing the m% of tokens in the behaviorcode with the sampled irrelevant token, e.g. . We add a linear layer on the corresponding output of thetoken [CLS] and utilize a linear layer with sigmoid activation tocalculate the score +/ for positive/negative samples. The binarycross-entropy loss is utilized for recognition, given by:",
  "LEAGER = Lgen + 1Lcon + 2(Lrecon + Lrecog)(6)": "where 1 and 2 are loss coefficients.Inference with Confidence-based Ranking. Since we have tworesults derived from the behavior and the semantic streams, wefirst obtain top- predictions via beam search from each stream.With the 2* prediction codes, we calculate the log probabilitiesover the codes as the confidence score of each prediction, which issimilar to the perplexity used in the language model and the lowervalue indicates more confidence. Finally, we rank these predictionsby their confidence scores and obtain the top- predictions, whichcorresponds to items.",
  "Experimental Setting": "Dataset. We conduct experiments on four open-source datasetscommonly used in the sequential recommendation task. For alldatasets, we group the interaction records by users and sort themby the interaction timestamps ascendingly. Following , weonly keep the 5-core dataset, which filters unpopular items andinactive users with fewer than five interaction records. Statistics ofthese datasets are shown in . Amazon: Amazon Product Reviews dataset , containing userreviews and item metadata from May 1996 to July 2018. Here weuse three categories (Beauty, Sports and Outdoors, and Toys andGames) for evaluations. Yelp 20192: Yelp Challenge releases the review data for smallbusinesses (e.g., restaurants). Following the previous setting ,we only use the transaction records from January 1st, 2019 toDecember 31st, 2019. We view these businesses as items. Evaluation Metrics. We employ two broadly used criteria for thematching phase, i.e., Recall and Normalized Discounted Cumula-tive Gain (NDCG). We report metrics computed on the top 5/10/20recommended candidates. Following the standard evaluation proto-col , we use the leave-one-out strategy for evaluation. For eachitem sequence, the last item is used for testing, the item before thelast is used for validation, and the rest is used for training. Duringtraining, we limit the number of items in a users history to 20. Implementation Details. For two-stream generation architec-ture, we set the number of encoder layers to 1, and the number ofdecoder layers to 4. Following previous works , we adoptpre-trained DIN as our behavior encoder and Sentence-T5 as oursemantic encoder, and set the hidden size to 128 as reported in .The cluster number in hierarchical k-means is set to 256. Forglobal contrastive task, we adopt Smooth 1 distance to serve asthe distilling loss. For semantic-guided transfer task, we randomlymask 50% behavior codes for reconstruction, and randomly replace50% behavior codes with the sampled code to construct negativepairs for recognition. To train our model, we adopt Adam optimizerwith the learning rate 0.001, and employ the warmup strategy forstable training. EAGER is not sensitive to the hyper-parameters forthe GCT and STT tasks because these tasks converge quickly. Sothe loss coefficients 1, 2 are both set to 1.",
  "TIGER : TIGER uses pretrained T5 to learn semantic ID foreach item and autoregressively decodes the identifiers of thetarget candidates with semantic ID": "Results. Tab. 2 reports the overall performance of four datasets.The results for all baselines without the superscript are takenfrom the publicly accessible results . For missing statistics,we reimplement the baseline and report our experimental results.From the results, we have the following observations: EAGER almost achieves better results than base modelsamong different datasets. Especially, EAGER performs consid-erably better on the Beauty benchmark compared to the second-best baseline with up to 31.49% improvement in Recall@5 and32.26% improvement in NDCG@5 compared to TIGER. Similarly,on the larger Yelp dataset, EAGER is 20.45% and 3.51% better inRecall@5 and NDCG@5, respectively. We attribute the improve-ments to the fact that EAGER succeeds in integrating behaviorand semantics information under a two-stream unified generativearchitecture with dual identifiers. EAGER beats the previous generative models on most datasets.EAGER differs from existing models through its two-stream de-coder architecture and multi-task training, which facilitate adeeper understanding of behavior-semantic relationships andcapture crucial global information of inter-codes. These superiorimprovements validate the effectiveness of our designs and thenecessity of incorporating both behaviors and semantic informa-tion. Generative models outperform other traditional baselinesin most cases across four datasets. The limitation could po-tentially arise from the utilization of a simplistic inner productmatching approach, which may restrict their ability to effectivelymodel intricate user-item interactions. Furthermore, in practicalscenarios, the construction of ANN indexes primarily focuseson achieving rapid matching, leading to additional performancedegradation due to misaligned optimization objectives. However,the challenge can be overcome by generative methods that lever-age beam search strategies to directly predict item codes, therebyboosting the models resilience and robustness.",
  "demonstrate the effectiveness and robustness of the proposedthree modules as well as the benefits of the two-stream generativeparadigm": "Removing GCT leads to more performance drops than removingSTT, suggesting that global information distillation of the inter-code is slightly more important than the knowledge flow betweenthe intra-code. The observation also highlights the crucial role ofboth tasks in enabling the model to acquire more powerful dualitem identifiers. Removing TSG leads to the most performance decline, whichindicates that the base model can significantly enhance its per-formance by integrating behavior-semantic information of items.The results again verify the superiority of the dual decoder ar-chitecture with confidence-based ranking.",
  "KDD 24, August 2529, 2024, Barcelona, SpainYe Wang et al": "Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factor-izing personalized markov chains for next-basket recommendation. In Proceedingsof the 19th international conference on World wide web. 811820. Hongyu Shan, Qishen Zhang, Zhongyi Liu, Guannan Zhang, and ChenliangLi. 2023. Beyond Two-Tower: Attribute Guided Representation Learning forCandidate Retrieval. In Proceedings of the ACM Web Conference 2023. 31733181. Zihua Si, Zhongxiang Sun, Jiale Chen, Guozhang Chen, Xiaoxue Zang, Kai Zheng,Yang Song, Xiao Zhang, and Jun Xu. 2023. Generative Retrieval with Seman-tic Tree-Structured Item Identifiers via Contrastive Learning. arXiv preprintarXiv:2309.13375 (2023). Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.2019. BERT4Rec: Sequential recommendation with bidirectional encoder rep-resentations from transformer. In Proceedings of the 28th ACM internationalconference on information and knowledge management. 14411450. Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommenda-tion via convolutional sequence embedding. In Proceedings of the eleventh ACMinternational conference on web search and data mining. 565573. Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta,Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, et al. 2022. Transformer memory as adifferentiable search index. Advances in Neural Information Processing Systems35 (2022), 2183121843. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processing systems 30 (2017). Jinpeng Wang, Jieming Zhu, and Xiuqiang He. 2021. Cross-Batch NegativeSampling for Training Two-Tower Recommenders. In The 44th International ACMSIGIR Conference on Research and Development in Information Retrieval (SIGIR).16321636. Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Qi Chen,Yuqing Xia, Chengmin Chi, Guoshuai Zhao, Zheng Liu, et al. 2022. A neuralcorpus indexer for document retrieval. Advances in Neural Information ProcessingSystems 35 (2022), 2560025614. Ji Yang, Xinyang Yi, Derek Zhiyuan Cheng, Lichan Hong, Yang Li, Simon Xi-aoming Wang, Taibai Xu, and Ed H. Chi. 2020. Mixed Negative Sampling forLearning Two-tower Neural Networks in Recommendations. In Companion of",
  "Hyper-Parameter Analysis (RQ3)": "Layer Number. In our practice, we found that the number ofencoder layers has a negligible effect on performance, whereas thenumber of decoder layers has a more significant influence. It sug-gests that the decoder plays a more important role in our EAGER.Therefore, we focus on the decoder layer here. To study the impactof the number of decoder layers on model performance, we ana-lyze the changes in Recall@10 and NDCG@10 across two datasetsby varying layer scales. As shown in , there is a continuousimprovement in the models performance in both datasets by in-creasing the number of layers. This can be attributed to the fact thatlarger parameters can enhance the models expressive capability.However, the deeper the model, the slower the inference speed. Cluster Number. We investigated the impact of employing vary-ing branch numbers on model performance. The increase inbranch number leads to a corresponding decrease in the length of the item identifier according to the total number of items. Theexperiment is conducted on two datasets, Beauty and Toys. aTheresults are illustrated in . We observe that as increases from64 to 512, the model performance of the base and ours both mono-tonically increase on the Toys dataset. However, an interestingtrend emerged on the Beauty dataset, where we observed a declinein performance as increased from 256 to 512. The performance",
  "CONCLUSION AND FUTURE WORK": "In this paper, we introduce a novel framework, EAGER, designedto integrate behavioral and semantic information for unified gener-ative recommendation. EAGER comprises three key components:(1) a two-stream generation architecture that combines behavioraland semantic information to enhance item recommendation, (2)a global contrastive task with a summary token to capture globalknowledge for improved auto-regressive generation quality, and (3)a semantic-guided transfer task that facilitates interactions acrosstwo decoders and their features. Extensive comparisons with state-of-the-art methods and detailed analyses demonstrate the effective-ness and robustness of EAGER. In future work, we plan to furtherenhance generative recommendation models by incorporating largelanguage models and multimodal AI techniques .",
  "We thank MindSpore3 for the partial support of this work, whichis a new deep learning computing framework": "Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Scott Yih, SebastianRiedel, and Fabio Petroni. 2022. Autoregressive search engines: Generatingsubstrings as document identifiers. Advances in Neural Information ProcessingSystems 35 (2022), 3166831683. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. Asimple framework for contrastive learning of visual representations. In Interna-tional conference on machine learning. PMLR, 15971607.",
  "Balzs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.2015. Session-based recommendations with recurrent neural networks. arXivpreprint arXiv:1511.06939 (2015)": "Dietmar Jannach and Malte Ludewig. 2017. When recurrent neural networksmeet the neighborhood for session-based recommendation. In Proceedings of theeleventh ACM conference on recommender systems. 306310. Bowen Jin, Hansi Zeng, Guoyin Wang, Xiusi Chen, Tianxin Wei, Ruirui Li,Zhengyang Wang, Zheng Li, Yang Li, Hanqing Lu, Suhang Wang, Jiawei Han,and Xianfeng Tang. 2023.Language Models As Semantic Indexers.CoRRabs/2310.07815 (2023).",
  "Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, Peitian Zhang, Yutao Zhu, andZhicheng Dou. 2024. From Matching to Generation: A Survey on GenerativeInformation Retrieval. CoRR abs/2404.14851 (2024)": "Qijiong Liu, Hengchang Hu, Jiahao Wu, Jieming Zhu, Min-Yen Kan, and Xiao-Ming Wu. 2024. Discrete Semantic Tokenization for Deep CTR Prediction. InCompanion Proceedings of the ACM on Web Conference (WWW). 919922. Qijiong Liu, Jieming Zhu, Yanting Yang, Quanyu Dai, Zhaocheng Du, Xiao-MingWu, Zhou Zhao, Rui Zhang, and Zhenhua Dong. 2024. Multimodal Pretraining,Adaptation, and Generation for Recommendation: A Survey. CoRR abs/2404.00621(2024). Chen Ma, Peng Kang, and Xue Liu. 2019. Hierarchical gating networks forsequential recommendation. In Proceedings of the 25th ACM SIGKDD internationalconference on knowledge discovery & data mining. 825833. Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel.2015. Image-based recommendations on styles and substitutes. In Proceedingsof the 38th international ACM SIGIR conference on research and development ininformation retrieval. 4352. Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, DanielCer, and Yinfei Yang. 2022. Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models. In Findings of the Association for ComputationalLinguistics: ACL 2022. 18641874. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits oftransfer learning with a unified text-to-text transformer. The Journal of MachineLearning Research 21, 1 (2020), 54855551. Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan H Keshavan, TrungVu, Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Q Tran, Jonah Samost, et al. 2023.Recommender Systems with Generative Retrieval. arXiv preprint arXiv:2305.05065(2023).",
  "Peitian Zhang, Zheng Liu, Yujia Zhou, Zhicheng Dou, and Zhao Cao. 2023. Term-Sets Can Be Strong Document Identifiers For Auto-Regressive Search Engines.arXiv preprint arXiv:2305.13859 (2023)": "Tingting Zhang, Pengpeng Zhao, Yanchi Liu, Victor S Sheng, Jiajie Xu, DeqingWang, Guanfeng Liu, Xiaofang Zhou, et al. 2019. Feature-level Deeper Self-Attention Network for Sequential Recommendation.. In IJCAI. 43204326. Yidan Zhang, Ting Zhang, Dong Chen, Yujing Wang, Qi Chen, Xing Xie, HaoSun, Weiwei Deng, Qi Zhang, Fan Yang, et al. 2023. IRGen: Generative Modelingfor Image Retrieval. arXiv preprint arXiv:2303.10126 (2023). Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, YanghuiYan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-throughrate prediction. In Proceedings of the 24th ACM SIGKDD international conferenceon knowledge discovery & data mining. 10591068. Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang,Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-rec: Self-supervised learning for se-quential recommendation with mutual information maximization. In Proceedingsof the 29th ACM international conference on information & knowledge management.18931902.",
  "Yujia Zhou, Jing Yao, Zhicheng Dou, Ledell Wu, Peitian Zhang, and Ji-Rong Wen.2022. Ultron: An ultimate retriever on corpus with a model-based indexer. arXivpreprint arXiv:2208.09257 (2022)": "Han Zhu, Daqing Chang, Ziru Xu, Pengye Zhang, Xiang Li, Jie He, Han Li, JianXu, and Kun Gai. 2019. Joint optimization of tree-based index and deep modelfor recommender systems. Advances in Neural Information Processing Systems 32(2019). Han Zhu, Xiang Li, Pengye Zhang, Guozheng Li, Jie He, Han Li, and Kun Gai.2018. Learning tree-based deep model for recommender systems. In Proceedingsof the 24th ACM SIGKDD International Conference on Knowledge Discovery & DataMining. 10791088."
}