{
  "ABSTRACT": "Text relevance or text matching of query and product is an essentialtechnique for the e-commerce search system to ensure that thedisplayed products can match the intent of the query. Many studiesfocus on improving the performance of the relevance model insearch system. Recently, pre-trained language models like BERThave achieved promising performance on the text relevance task.While these models perform well on the offline test dataset, thereare still obstacles to deploy the pre-trained language model to theonline system as their high latency. The two-tower model is ex-tensively employed in industrial scenarios, owing to its ability toharmonize performance with computational efficiency. Regrettably,such models present an opaque black box nature, which preventsdevelopers from making special optimizations. In this paper, weraise deep Bag-of-Words (DeepBoW) model, an efficient and in-terpretable relevance architecture for Chinese e-commerce. Ourapproach proposes to encode the query and the product into thesparse BoW representation, which is a set of word-weight pairs.The weight means the important or the relevant score between thecorresponding word and the raw text. The relevance score is mea-sured by the accumulation of the matched word between the sparseBoW representation of the query and the product. Compared topopular dense distributed representation that usually suffers fromthe drawback of black-box, the most advantage of the proposedrepresentation model is highly explainable and interventionable,which is a superior advantage to the deployment and operationof online search engines. Moreover, the online efficiency of theproposed model is even better than the most efficient inner productform of dense representation. The proposed model is experimentedon three different datasets for learning the sparse BoW representa-tions, including the human-annotation set, the search-log set andthe click-through set. Then the models are evaluated by experienced",
  "Jiwei Tan is the corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08 human annotators. Both the auto metrics and the online evalua-tions show our DeepBoW model achieves competitive performancewhile the online inference is much more efficient than the othermodels. Our DeepBoW model has already deployed to the biggestChinese e-commerce search engine Taobao and served the entiresearch traffic for over 6 months.",
  "E-Commerce, Text Matching, Relevance": "ACM Reference Format:Zhe Lin, Jiwei Tan, Dan Ou, Xi Chen, Shaowei Yao, and Bo Zheng. 2024. DeepBag-of-Words Model: An Efficient and Interpretable Relevance Architecturefor Chinese E-Commerce. In Proceedings of the 30th ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining (KDD 24), August 2529, 2024,Barcelona, Spain. ACM, New York, NY, USA, 11 pages.",
  "INTRODUCTION": "The popularization of mobile internet has significantly elevated theprominence of online commerce in daily life. Hundreds of millionsof customers purchase products they want on large e-commerceportals, such as Taobao and Amazon. The search engine emerges asthe essential technology in assisting users to discover products thatare in accord with their preferences. Different from general searchengines like Google, commercial e-commerce search engines areusually designed to improve the users engagement and conversion,possibly at the cost of relevance in some cases . The exhibitionof products in search results that are inconsistent with the queryintent has the potential to diminish the customer experience andhamper customers long-term trust and engagement. Consequently,measuring relevance between the text of search query and productsto filter the irrelevant products is an indispensable process in thee-commerce search engine.",
  "KDD 24, August 2529, 2024, Barcelona, SpainZhe Lin, et al": "Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddingsusing Siamese BERT-Networks. In Proceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the 9th International Joint Confer-ence on Natural Language Processing (EMNLP-IJCNLP), Kentaro Inui, Jing Jiang,Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics,Hong Kong, China, 39823992. Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu,and Mike Gatford. 1994. Okapi at TREC-3. In Proceedings of The Third TextREtrieval Conference, TREC 1994, Gaithersburg, Maryland, USA, November 2-4,1994 (NIST Special Publication, Vol. 500-225), Donna K. Harman (Ed.). NationalInstitute of Standards and Technology (NIST), 109126. Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Grgoire Mesnil. 2014.A Latent Semantic Model with Convolutional-Pooling Structure for Informa-tion Retrieval. In Proceedings of the 23rd ACM International Conference on Con-ference on Information and Knowledge Management (Shanghai, China) (CIKM14). Association for Computing Machinery, New York, NY, USA, 101110. Richard Socher, Danqi Chen, Christopher D. Manning, and Andrew Y. Ng. 2013.Reasoning with neural tensor networks for knowledge base completion. In Pro-ceedings of the 26th International Conference on Neural Information ProcessingSystems - Volume 1 (Lake Tahoe, Nevada) (NIPS13). Curran Associates Inc., RedHook, NY, USA, 926934. Nandan Thakur, Nils Reimers, Andreas Rckl, Abhishek Srivastava, and IrynaGurevych. 2021. BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation ofInformation Retrieval Models. In Thirty-fifth Conference on Neural InformationProcessing Systems Datasets and Benchmarks Track (Round 2). Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is Allyou Need. In Advances in Neural Information Processing Systems, I. Guyon, U. VonLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.),Vol. 30. Curran Associates, Inc. Shengxian Wan, Yanyan Lan, Jun Xu, Jiafeng Guo, Liang Pang, and Xueqi Cheng.2016. Match-SRNN: modeling the recursive matching structure with spatialRNN. In Proceedings of the Twenty-Fifth International Joint Conference on ArtificialIntelligence (New York, New York, USA) (IJCAI16). AAAI Press, 29222928. Wei Wang, Bin Bi, Ming Yan, Chen Wu, Jiangnan Xia, Zuyi Bao, Liwei Peng, andLuo Si. 2020. StructBERT: Incorporating Language Structures into Pre-trainingfor Deep Language Understanding. In International Conference on Learning Rep-resentations.",
  "Deep Bag-of-Words Model: An Efficient and Interpretable Relevance Architecture for Chinese E-CommerceKDD 24, August 2529, 2024, Barcelona, Spain": "We use PyTorch to implement our model and train the modelwith Adam optimizer. The hyper-parameters of Adam optimizerare 1 = 0.9, 2 = 0.999, = 108 and the learning rate is set to0.0001. Query-document pairs are batched together by approximatesequence length. Each training batch contains a set of sentencepairs with about 50000 tokens. The hyper-parameters and the bestmodel are chosen from the experimental results on the validationset. We train our model on 2 Tesla V100 GPU and it usually takesabout 3 days for the model to converge. The convergence is reachedwhen the ROC-AUC does not improve on the validation set.",
  "RELATED WORK2.1Text Matching": "The text matching task takes textual sequences as input and pre-dicts a numerical value or a category indicating their relationship.Text matching is a long-stand problem and a hot research topicas its important in information retrieval and search system. Thee-commerce relevance learning can be regarded as a text-matchingtask. Early work mostly performs keyword-based matching thatrelies on manually defined features, such as TF-IDF similarity andBM25 . These methods cannot effectively utilize raw text fea-tures and usually fail to evaluate semantic relevance.Recently with the development of deep learning, neural-basedtext-matching models have been employed to solve the semanticmatching task and have achieved promising performance. The archi-tecture of the neural-based text-matching model can be roughly di-vided into the interaction-based model and the representation-based(two-tower) model. The interaction-based model usually puts all candidate text together as the input. The modelcan employ the full textual feature to calculate the matching fea-ture as the low layer, and then aggregate the partial evidence ofrelevance to make the final decision. So interaction-based modelcan leverage sophisticated techniques in the aggregation procedureand achieve better performance. More recent studies are built uponthe pre-trained language model like BERT . With an extremelylarge corpus for pre-training, these methods can achieve new state-of-the-art performance on various benchmarks. The architectureof these models is the pre-trained bidirectional Transformer ,which can also be regarded as an interaction-based model. Thetypical paradigm of the BERT-based relevance model is to feed textpair into BERT and then build a non-linear classifier upon BERTs[CLS] output token to predict the relevance score .Although having excellent performance in the text-matchingtask, interaction-based models are still hard to be deployed to prac-tical online service as they are mostly time-consuming, and thefeatures of queries and documents cannot be pre-computed offline. Two-tower models are widely used in many online search systems.The two-tower model consists of two identical neural networks,each taking one of the two inputs. DSSM is a two-towermodel that employs two separate deep full-connected networks toencode the candidate texts. Meanwhile, more sophisticated archi-tectures can be adopted to enhance the ability of learning semanticrepresentations. LSTM-DSSM and LSTM-RNN use RNNsto explicitly model word dependencies in the sentences. Typicallydot-product, cosine, or parameterized non-linear layers are usedto measure the similarity between representations of all candidatetexts. Since individually encoding both the queries and documents,the embeddings of them can be pre-computed offline. Therefore,representation-based methods are online efficient and are widelyused in industrial search engines. However, the encoding proce-dure of two inputs is independent with each other, making the finalclassifier hard to predict their relationship.",
  "Search Relevance Matching": "Relevance in search engine is a special sub-task of the text-matchingwhich computes the relevance score between the query and theproduct (as the document). Different from the typical text-matchingtask which all input texts are semantically similar and homogeneous(i.e. having comparable lengths), the length of query may be muchshorter than the length of document. Query only needs to matchthe partial semantics in the document.A large number of models are proposed for conducting matchingin search. Neural Tensor Network (NTN) is originally pro-posed to explicitly model multiple interactions of relational data.It achieves powerful representation ability that can represent mul-tiple similarity functions, including cosine similarity, dot product,and bilinear product, etc. Qiao et al. apply the BERT model toad-hoc retrieval and passage retrieval. Reimers and Gurevych propose Sentence-BERT for reducing the computational overheadfor text matching. Bai et al. conduct a pilot study to map thefrequency-based and BoW representation of a document to a sparseterm importance distribution for text retrieval.E-commerce search is a special scenario of the Web search sys-tem. Both tasks model the semantic matching between query andcandidate and require high efficiency and low latency in the onlinesearch system. Differently, in Web search the query and documentare usually very different in length, making most methods not fea-sible for the e-commerce relevance task. Currently, there is not acommonly-used public benchmark for the Chinese e-commercerelevance task, so previous works usually evaluate their modelson the online service and the real-world dataset constructed fromthe online platforms. Guo et al. introduce a typical frameworkfor e-commerce relevance learning. A Siamese network is adoptedto learn pair-wise relevance of two products to a query. They in-vestigate training the model with user clicks and batch negatives,followed by finetuning with human supervision to calibrate thescore by pair-wise learning. Xiao et al. propose a co-trainingframework to address the data sparseness problem by investigatingthe instinctive connection between query rewriting and semanticmatching. Yao et al. propose to learn a two-tower relevancemodel from click-through data in e-commerce by designing a point-wise loss function. Zhang et al. also find the weakness of",
  "METHODOLOGY3.1Overview": "The proposed DeepBoW model is based on the two-tower archi-tecture, which encodes the query and document separately andcomputes the semantic relevance score with the representations ofquery and document. Different from other text relevance modelswith dense embeddings, our model encodes the query and docu-ment into the Bag-of-Words vectors and calculates the relevancescore from sparse BoW representations.In this section, we introduce the components of our model indetail. We first describe the multi-granularity encoder to aggregatethe character-grained feature and word-grained feature. Next, weintroduce two different sparse BoW representations including theterm-weighting BoW and the synonym-weighting BoW. Then, weshow how to use N-gram hashing to reduce the semantic loss fromword segmentation and enhance the quality of the sparse BoWrepresentation. Finally, we describe the training process of ourmodel in detail and show the deployment of our DeepBoW model tothe online e-commerce search system. shows an overviewof the DeepBoW model.",
  "Multi-Granularity Encoder": "The text encoder aims to obtain the input texts contextual repre-sentations. We choose Transformer encoder as our sentenceencoder because of its excellent performance in many tasks. TheTransformer encoder is a stack of identical layers, and each layerincludes a multi-head self-attention and a fully connected feed-forward network. For the input senquence , we obtain the out-put encoding matrix of -th layer as = {1,2, , }, where R is the word embedding vector and is the number of wordsin . Same to ReprBERT , we aggregate the output of each layeras the text encoding representation according to:",
  "where Wm R, Wagg R, bm, bagg R, is the con-catenate operation": "Usually the character-based model performs better than theword-based model in Chinese NLP tasks , and most Chinese-BERT models are character-based Transformer architecture. How-ever, since our model intends to catch the relationship betweenthe semantic and the word of the sentence, we propose to encodenot only the character-segmentation sequence but also the word-segmentation sequence separately. For convenience, we denotethe character-segmentation sequence and the word-segmentationsequence of the input text as = {1,2, ,} and ={1,2, ,}, respectively, where and are the indices ofthe token in the vocabulary. The output encoding matrices of thecharacter-segmentation sequence and the word-segmentation se-quence are = {1,2, , } and = {1,2, ,},where and are the lengths of the character-segmentation andthe word-segmentation sequence. The text encoding representationof these two sequences are and .",
  "Sparse BoW Representation": "Unlike traditional two-tower architecture models that encode textinto the \"embedding\" which is a dense distributed representation,our model encodes the text into the sparse BoW representation.The sparse BoW representation is a set of word-weight pairs, whereeach word corresponds to a weight that indicates the importance orthe relevance of this word to the input text. In this section, we in-troduce two different sparse BoW representations: term-weightingBoW representation and synonym-weighting BoW representation,and describe the module to generate these two sparse BoW repre-sentations in detail. 3.3.1Term-Weighting BoW Representation.In the e-commerce search system, the query inputted by user maycontain some redundant or unrelated words. These words can beexcised with negligible impact on the text semantic. For example,for the input query from Taobao like \"2024\"1, \"\" and \"\" both mean a pregnant woman,but \"\" is more accurate than \"\" at semantic level asthe latter word is polysemous and more colloquial. \"\" whichmeans suitable, can be regarded as a stop word in the e-commercescenario. So \"\" and \"\" can be discarded and the otherwords should be retained.Term-Weighting BoW includes all words of the input text, andeach word is assigned a weight that indicates its significance withinthe texts semantics. Key words like brand and category shouldhave greater importance weights than the other words. (a) shows the architecture to generate the term-weighting BoWrepresentation. Then, the term-weighting BoW representation canbe produced as follows:",
  "!#!$": ": An overview of the DeepBoW model. Figure (a) shows the architecture that encodes the input text into the Term-Weighting BoW representation, which gathers the attention weight of each word as its weight in the term-weighting BoWrepresentation. Figure (b) shows the architecture that encodes the input text into the Synonym-Expansion BoW representation,which generates sparse BoW representation from character embedding and word embedding respectively, and aggregates thesetwo representations as the synonym-expansion BoW representation. 3.3.2synonym-expansion BoW Representation.Since queries in e-commerce search systems are entered by layusers, they may differ from the product descriptions and includecolloquialisms and polysemes. Some adjectives or category wordsmay also have synonyms. Term-weighting BoW representation canonly compute the importance weight of each word in the text, but isunable to add relevant words and synonyms. Synonym expansioncan greatly improve the performance of the e-commerce searchsystem. Therefore, we propose the synonym-expansion BoW repre-sentation to enhance the retrieval performance of the sparse BoWrepresentation. (b) shows the architecture to generate thesynonym-expansion BoW representation.We sample words from the training corpora as the vocabularyV according to the frequency of the word. Our model leverages therelevance between these words and the input text to represent thesemantics of the query and the products. First, our model aggregatesthe word-based text encoding representation and the character-based text encoding representation as follows:",
  "N-gram Hashing Vocabulary": "In the preceding section, we describe the sparse BoW representationin detail. Unfortunately, due to the limitation of models parametersize, we can only leverage the vocabulary within a limited numberof words. Using the [UNK] to replace all Out-Of-Vocabulary (OOV)words may lead to significant semantic loss. To mitigate this issue,we introduce an ensemble of hashing tokens into the vocabulary,where the OOV word can be replaced with its hashing tokens2.Semantic loss may occur between the raw text and its BoW rep-resentation, particularly when syntactically cohesive phrases arefragmented during the word segmentation process. This issue canlead to misalignment for the essential semantics such as producttypes, entity names, or brand identifiers in query/product. For ex-ample, the brand name L'ORAL Paris could be inaccuratelydivided into separate tokens during word segmentation. To addressthis problem, we introduce an N-gram hashing vocabulary strategy.Concretely, N-gram phrases are incorporated into the texts BoWand are subsequently replaced with their respective hashing tokens,analogous to the treatment of OOV words. The significance of aparticular N-gram phrase is directly proportional to the frequencyof its occurrence within relevant query-product pairs in the cor-pora. Our model is equipped to ascertain the importance of theseN-gram hashing tokens through the analysis of large-scale corpora.Consequently, the semantics of these N-gram phrases are retainedwithin the sparse BoW representation.",
  "DeepBoW Relevance Model": "In this section, we describe the method to compute the relevancescore between the query and the product from the sparse BoWrepresentations. Note that in the search engine scenario the prod-uct should match all the semantics of query, while conversely thequery does not need to match all the semantics of the product.Accordingly, we encode the query as the term-weighting BoW rep-resentation while encode the product as the synonym-expansionBoW representation. The relevance score of the query/product canbe calculated as follows:",
  "= (5)": "where is the query, is the product, and (, ) is the relevancescore between and . We call our DeepBoW model with thisrelevance score as DeepBoW(Q-Weight).We leverage the cross-entropy loss between the output score andthe ground truth to train our model. In addition, we also optimizethe L2 norm of BoWSE() to enhance the sparsity of the synonym-expansion BoW representation, so that only the most relevant wordscan get high scores. The loss function is as follows:",
  "Since most online search systems have strict latency limitations,we pre-compute the sparse BoW representation of the queries and": "the products offline. We discard the word-weight pairs in the sparseBoW representation whose weights are lower than the given thresh-old to optimize memory usage. We sort the word-weight pairs of-fline by the word index. The relevance score of two sparse BoWrepresentations can be calculated by using the two-pointer algo-rithm. Then the time complexity of Eq.5 and Eq.7 can be optimizedto O(), which is much faster than the state-of-the-art deep rel-evance model . Although some deep relevance models withcosine similarity can achieve comparable efficiency, the perfor-mance of these models is much lower than our model as shown inthe section 4.5.",
  "EXPERIMENTS4.1Dataset": "There is no public dataset and benchmark for the Chinese e-commercerelevance task, so we conduct experiments on three different typesof industrial datasets to learn the DeepBoW model. The first is alarge-scale Human-Annotation dataset which contains query-product pairs sampled from the Taobao search logs. Each query-product pair is labeled as Good (relevant) or Bad (irrelevant) by ex-perienced human annotators. This is a daily task running in Taobao,which has accumulated more than 8 million labeled samples. Wesplit the human-annotated datasets into training, validation andtest sets, as detailed in .",
  ": Statistic for human-annotation dataset": "The second dataset for training is built by knowledge distilla-tion, similar to Yao et al. . We leverage the training set of thehuman-annotation dataset to finetune the StructBERT model,which results in an interaction-based teacher model with strongperformance. Then the teacher model predicts the relevance scoresof the large unlabeled query-product pairs sampled from the searchlogs of Taobao within a year. This training dataset is denoted asSearch-Logs in . Third, we also sample click-through datafrom search logs and investigate the performance of our model onthis training set. We denote this dataset as Click-Through in. Although these are training datasets of different sources,we all use the human-annotation validation and test dataset toevaluate the model performance.",
  "Training Details": "We employ Transformer as both the character-based encoder andthe word-based encoder. We reduce the total 12-layer encoder toimprove efficiency. After balancing the effectiveness and efficiency,our model adopts 2 layers that can still achieve competitive perfor-mance. We select the top 50000 words as the vocabulary V accordingto the words frequency in corpora, and we also add another 10000hashing tokens into the vocabulary for the OOV words and theN-gram phrases.",
  "Baseline": "We explore the performance of DeepBoW(Q-Weight) and DeepBoW(Q-Synonym) respectively. The main difference between the twomethods is for DeepBoW(Q-Synonym) we leverage the synonym-expansion BoW representation to replace the term-weighting BoWrepresentation for the query. To reduce memory usage and compu-tation, we truncate the BoW representation to make it sparse. Thereare two methods to truncate the sparse BoW representation, oneis to keep the largest words according to their respective values,and the other is to discard the terms whose values are smaller thanthe giving threshold.In addition, we adopt several state-of-the-art methods for com-parison. BERT , RoBERTa and StructBERT belong tothe interaction-based architecture which is also known as the cross-encoder architecture. Siamese BERT , MASM , Poly-encoders and ReprBERT belong to the two-tower architecture whichis also known as the bi-encoder architecture. Besides, we investigatethe performance of ReprBERT with cosine similarity score insteadof MLP for online computation of relevance from query/productembeddings. For fair comparison, we also leverage the pre-trainedmodel like RoBERTa and StructBERT as the encoder of the two-tower model. These models are also baselines in our experiment,which denote as DSSMRoBERTa and DSSMStructBERT.",
  "Evaluation Metrics": "We evaluate our model on both offline and online metrics. In offlineevaluation, since the human annotation is binary, the task is evalu-ated as a classification task. The Receiver Operator CharacteristicArea Under Curve (ROC-AUC) is widely adopted in text relevancetasks . Note that in the e-commerce relevance scenario,most instances are positive and we are more concerned about neg-ative instances. Therefore the PR-AUC used in this paper is thenegative PR-AUC that treats Bad as 1 and Good as 0 following Yaoet al. . This metric is denoted as Neg PR-AUC.Besides, we also evaluate the different model complexity of pa-rameters and online computation efficiency. The FLOPs / tokenis computed according to Molchanov et al. which shows thefloating-point operations per second (FLOPs) when there is only1 token being considered. The plus sign separates the online andoffline calculation FLOPs, which means the former part of compu-tation can be pre-computed offline. Memory indicates the onlinememory overhead for storing pre-computed query and productvectors where we use vector size for comparison. In online evalua-tion, we use the rate of Good annotated by human annotators and the number of transactions as the evaluation metrics. The query-product pairs for human relevance judgment are randomly sampledfrom the online search logs according to the amount of Page View(PV) as the sample weight.",
  "Results": "presents an evaluative comparison across various mod-els. Our DeepBoW model demonstrates robust performance acrossthree different training sets. For human-annotation dataset, Struct-BERT has the best performance, and interaction-based models out-perform two-tower models. Unfortunately, it is infeasible to deploythe interaction-based model in industrial system because of prohib-itive computation and resource requirements. The ROC-AUC andNeg PR-AUC of two-tower models are much lower than the pre-trained model, because the human-annotation data is insufficientand the pre-trained model can introduce extra knowledge. Nonethe-less, our DeepBoW model still outperforms other two-tower models.The data enhancement sampled from the search logs and labeledby the teacher model can greatly improve the performance of two-tower models. Our model achieves the best performance aroundthe two-tower models in search-logs training sets. Click-throughdata is used to train the relevance model in some cases wherelack of human-annotation training data. The models trained onthe click-through data get weak performance. The main reasonis that the click-through data in e-commerce is much more noisyand misleading, which is not only affected by the query-productrelevance but also by many factors including price, attractive titles,or images. Even so, our model also performs better than the othermodels since it explicitly encodes the semantics to the sparse bag-of-words while the other models may capture the personalizedinformation beyond the textual feature.Our proposed architecture can truncate the sparse BoW represen-tation to reduce memory usage in the online search system. We caneither truncate the sparse BoW representation to the fixed lengthor discard the word-weight pairs whose values are smaller than agiven threshold. Benefiting from the interpretability of our sparseBoW representation, both truncation methods achieve competitiveperformance and only produce a slight loss of performance. Wefurther explore the distribution of the sparse BoW representationas case study in Appx. A. shows the parameter, computation and memory con-sumption of each model. While our model has a considerable num-ber of parameters since we project the dense vector into the vo-cabulary (78.8% parameters about 126M come from Wc and Ww inEq.3), DeepBoW is the most efficient at online inference. Our modelonly uses CPU to compute the relevance score while the other mod-els need GPU to speed the inference. shows the inferencetime of each model. The experiments are performed on a local CPUplatform. We report the average inference time of the model toscore 1000 products per query. The results show that our model ismuch faster than the ReprBERT which has been deployed inthe Taobao search system.",
  "Interaction-Based Models:BERT0.8500.662----RoBERTa0.9060.692----StructBERT0.9230.721----": "Two-Tower Models:Siamese BERT0.7650.5650.8210.648--MASM0.7950.4840.7930.5820.6150.283Poly-Encoder0.8080.6230.8460.605--DSSMRoBERTa0.8730.673----DSSMStructBERT0.8580.658----ReprBERT 0.8320.5430.8940.7020.7980.521ReprBERT +Cosine Similarity0.7980.4520.8470.6010.7270.399 Ours:DeepBoW(Q-Weight)0.8740.6650.9080.7050.8030.579DeepBoW(Q-Weight) +128-Trunc0.8650.6450.8990.6980.7870.566DeepBoW(Q-Weight) +0.4-Trunc0.8690.6580.9060.7010.7960.572DeepBoW(Q-Synonym)0.8800.6740.9140.7120.8120.585DeepBoW(Q-Synonym) +128-Trunc0.8730.6700.9060.7050.7990.571DeepBoW(Q-Synonym) +0.4-Trunc0.8770.6720.9110.7100.8070.575 : Comparison results on test set. Best scores are in bold. +128-Trunc means keeping 128 largest terms according to the valueof the word-weight pair. +0.4-Trunc means discarding the terms that the value is smaller than 0.4. We only finetune the pre-trainedbased models on the Human-Annotation dataset and do not evaluate these models in the other two training sets, since weleverage StructBERT as teacher model to label the search-logs dataset. We do not evaluate the performance of MASM andPoly-Encoder on Click-Through training dataset, because the two models do not converge on the other two training datasets.",
  "Ours:DeepBoW +128-Trunc33.4M + 126M159.4M + 128128DeepBoW +0.4-Trunc33.4M + 126M159.4M + 2828/144": ": The models efficiency. The Params of our modelsinclude the encoder and the final vocabulary projection (Wcand Ww in Eq.3). The Mem of DeepBoW +0.4-Trunc is the meanvalue throughout the whole corpora (query/product). investigate the performance of the 2-layer encoder and the 6-layerencoder. Besides, we remove the word encoder and the characterencoder separately to show the importance of encoding at bothword-level and character-level. We also employ different capacitiesof vocabulary and hashing tokens in the sparse BoW representation.To further explore the sparsity of BoW representation, we removethe 2 norm from the loss function. This may lead to a sparsitydeterioration in the sparse BoW representation.",
  ": Inference time of different models that score 1000query-product pairs": "shows the results of the ablation study. We can see thateach module in our model does contribute to the overall perfor-mance. The performance of our model has a significant deteriora-tion if either the word encoder or the character encoder is removed.Increasing the capacity of vocabulary or hashing tokens cannotimprove the models performance, because it may lead to insuffi-cient training for each token. Optimizing our model without the 2norm loss can lead to slight performance decline. There is almost nodifference between the 2-layer and 6-layer encoders in our model.",
  "Online Evaluation": "Online A/B testing is also conducted to evaluate our DeepBoWmodel, by replacing the online ReprBERT model with the DeepBoWmodel for comparison. Both experiments take about 2.5% proportionof Taobao search traffic, and the A/B testing lasts for a month. Asa result, DeepBoW improves the number of transactions by about0.4% on average. The daily human annotation results show thatDeepBoW also improves the rate of relevance by 0.5%. Online A/B",
  ": Ablation study of components of DeepBoW": "testing verifies the proposed DeepBoW is superior to previousstate-of-the-art models, and can achieve significant online profitconsidering the extremely large traffic of Taobao every day.Our DeepBoW model has already served the entire Taobao searchtraffic. After pre-computing the representations of queries andproducts, the online serving latency can be optimized to as low as4ms on the distributed computing system with CPUs. This is muchfaster than the previous online relevance serving model ReprBERT of 10ms with GPUs and can satisfy the extremely large trafficof Taobao.",
  "CONCLUSION AND FUTURE WORK": "In this paper, we study an industrial task of measuring the seman-tic relevance of queries and products. We propose the DeepBoWrelevance model, which is an efficient and interpretable relevancearchitecture for Chinese e-commerce search system. Our modelencodes the query and product as a set of word-weight pairs, whichis called the sparse BoW representation. The model is evalutedon three different training datasets, and the results show that ourmodel achieves promising performance and efficiency. The modelhas been deployed in the Taobao search system.In future work, we will explore integrating external knowledgeinto the DeepBoW relevance model to improve the performance.The proposed model can also be evaluated on datasets of otherlanguage. Yang Bai, Xiaoguang Li, Gang Wang, Chaoliang Zhang, Lifeng Shang, Jun Xu,Zhaowei Wang, Fangshan Wang, and Qun Liu. 2020. SparTerm: Learning Term-based Sparse Representation for Fast Text Retrieval. ArXiv abs/2010.00768 (2020). David Carmel, Elad Haramaty, Arnon Lazerson, Liane Lewin-Eytan, and YoelleMaarek. 2020. Why Do People Buy Seemingly Irrelevant Items in Voice ProductSearch? On the Relation between Product Relevance and Customer Satisfactionin eCommerce. In Proceedings of the 13th International Conference on Web Searchand Data Mining (Houston, TX, USA) (WSDM 20). Association for ComputingMachinery, New York, NY, USA, 7987. Jesse Davis and Mark Goadrich. 2006. The relationship between Precision-Recall and ROC curves. In Proceedings of the 23rd International Conference onMachine Learning (Pittsburgh, Pennsylvania, USA) (ICML 06). Association forComputing Machinery, New York, NY, USA, 233240. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding. InProceedings of the 2019 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies, Volume 1 (Long andShort Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Associationfor Computational Linguistics, Minneapolis, Minnesota, 41714186.",
  "on Conference on Information and Knowledge Management (Indianapolis, Indiana,USA) (CIKM 16). Association for Computing Machinery, New York, NY, USA,5564": "Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014.Convolu-tional Neural Network Architectures for Matching Natural Language Sen-tences. In Advances in Neural Information Processing Systems, Z. Ghahramani,M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger (Eds.), Vol. 27. Cur-ran Associates, Inc. Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and LarryHeck. 2013. Learning deep structured semantic models for web search usingclickthrough data. In Proceedings of the 22nd ACM International Conference onInformation & Knowledge Management (San Francisco, California, USA) (CIKM13). Association for Computing Machinery, New York, NY, USA, 23332338. Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. 2019.Poly-encoders: Architectures and Pre-training Strategies for Fast and AccurateMulti-sentence Scoring. In International Conference on Learning Representations. Jyun-Yu Jiang, Mingyang Zhang, Cheng Li, Michael Bendersky, Nadav Gol-bandi, and Marc Najork. 2019. Semantic Text Matching for Long-Form Docu-ments. In The World Wide Web Conference (San Francisco, CA, USA) (WWW19). Association for Computing Machinery, New York, NY, USA, 795806. Yunjiang Jiang, Yue Shang, Rui Li, Wen-Yun Yang, Guoyu Tang, Chaoyi Ma, YunXiao, and Eric Zhao. 2019. A unified neural network approach to e-commercerelevance learning. In Proceedings of the 1st International Workshop on DeepLearning Practice for High-Dimensional Sparse Data (Anchorage, Alaska) (DLP-KDD 19). Association for Computing Machinery, New York, NY, USA, Article 10,7 pages. Xiaoya Li, Yuxian Meng, Xiaofei Sun, Qinghong Han, Arianna Yuan, and JiweiLi. 2019. Is Word Segmentation Necessary for Deep Learning of Chinese Rep-resentations?. In Proceedings of the 57th Annual Meeting of the Association forComputational Linguistics, Anna Korhonen, David Traum, and Llus Mrquez(Eds.). Association for Computational Linguistics, Florence, Italy, 32423252. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, OmerLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: ARobustly Optimized BERT Pretraining Approach. ArXiv abs/1907.11692 (2019). Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. 2017.Pruning Convolutional Neural Networks for Resource Efficient Inference. InInternational Conference on Learning Representations. Priyanka Nigam, Yiwei Song, Vijai Mohan, Vihan Lakshman, Weitian (Allen) Ding,Ankit Shingavi, Choon Hui Teo, Hao Gu, and Bing Yin. 2019. Semantic ProductSearch. In Proceedings of the 25th ACM SIGKDD International Conference onKnowledge Discovery & Data Mining (Anchorage, AK, USA) (KDD 19). Associationfor Computing Machinery, New York, NY, USA, 28762885.",
  "H. Palangi, L. Deng, Y. Shen, J. Gao, X. He, J. Chen, X. Song, and R. Ward. 2015.Semantic Modelling with Long-Short-Term Memory for Information Retrieval.arXiv:1412.6629 [cs.IR]": "Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen,Xinying Song, and Rabab Ward. 2016. Deep sentence embedding using longshort-term memory networks: analysis and application to information retrieval.IEEE/ACM Trans. Audio, Speech and Lang. Proc. 24, 4 (apr 2016), 694707. Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Shengxian Wan, and Xueqi Cheng.2016. Text matching as image recognition. In Proceedings of the Thirtieth AAAIConference on Artificial Intelligence (Phoenix, Arizona) (AAAI16). AAAI Press,27932799. Ankur Parikh, Oscar Tckstrm, Dipanjan Das, and Jakob Uszkoreit. 2016. ADecomposable Attention Model for Natural Language Inference. In Proceedings ofthe 2016 Conference on Empirical Methods in Natural Language Processing, Jian Su,Kevin Duh, and Xavier Carreras (Eds.). Association for Computational Linguistics,Austin, Texas, 22492255.",
  "Rong Xiao, Jianhui Ji, Baoliang Cui, Haihong Tang, Wenwu Ou, Yanghua Xiao,Jiwei Tan, and Xuan Ju. 2019. Weakly Supervised Co-Training of Query Rewriting": "and Semantic Matching for e-Commerce. In Proceedings of the Twelfth ACM Inter-national Conference on Web Search and Data Mining (Melbourne VIC, Australia)(WSDM 19). Association for Computing Machinery, New York, NY, USA, 402410. Shaowei Yao, Jiwei Tan, Xi Chen, Keping Yang, Rong Xiao, Hongbo Deng, andXiaojun Wan. 2021. Learning a Product Relevance Model from Click-ThroughData in E-Commerce. In Proceedings of the Web Conference 2021 (Ljubljana, Slove-nia) (WWW 21). Association for Computing Machinery, New York, NY, USA,28902899. Shaowei Yao, Jiwei Tan, Xi Chen, Juhao Zhang, Xiaoyi Zeng, and Keping Yang.2022. ReprBERT: Distilling BERT to an Efficient Representation-Based RelevanceModel for E-Commerce. In Proceedings of the 28th ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining (Washington DC, USA) (KDD 22).Association for Computing Machinery, New York, NY, USA, 43634371.",
  "ACASE STUDY": "shows two examples of our DeepBoW model. Both queryand product are encoded to the synonym-expansion BoW repre-sentation. The sparse BoW representation consists of a collectionof word-weight pairs, which can be regarded as the bag-of-wordswith soft weight. The synonym-expansion representation can notonly capture the importance of the words in the original text,but also incorporates pertinent synonymous terms. The relevancescore can be calculated by aggregating the matching terms of thequerys/products sparse BoW representation.These two examples show that our proposed sparse BoW repre-sentation has positive interpretability, signifying that the developercan analyze bad cases from the online search system and implementtargeted optimizations. Furthermore, the developer can modify theterms in the sparse BoW representation directly to achieve the ex-pected result. In a word, our DeepBoW model surpasses other deeprelevance modeling approaches in terms of interpretability and flex-ibility, thereby rendering it eminently suitable for the e-commercesearch system.",
  "Query: BoWSE(Query): [(, 0.30148), (, 0.25785), (, 0.2277), (, 0.21297)]": "Product: V18-24VA2023BoWSE(Product): [(, 1.0), (v, 1.0), (, 0.99999), (, 0.99999), (, 0.99999), (, 0.99999), (, 0.99998), (,0.99998), (, 0.99998), (, 0.99995), (, 0.99993), (, 0.99993), (18, 0.99992), (, 0.99991), (, 0.9999), (,0.99985), (, 0.99981), (, 0.9998), (, 0.9998), (, 0.99977), (, 0.99972), (, 0.9997), (, 0.99968), (a,0.99961), (, 0.99957), (24, 0.99954), (, 0.99944), (, 0.9994), (, 0.99934), (, 0.9992), (, 0.99897), (, 0.99862),(, 0.99824), (, 0.9981), (, 0.99805), (, 0.99726), (2023, 0.99721), (, 0.99687), (, 0.99657), (, 0.99627),(2023, 0.99622), (, 0.99621), (, 0.99618), (, 0.99616), (, 0.99583), (, 0.99577), (,0.99536), (, 0.99512), (, 0.99502), (, 0.9946), (, 0.9943), (, 0.99379), (, 0.99247), (, 0.99237),(18-24, 0.99174), (, 0.99117), (, 0.99094), (, 0.99014), (, 0.98937), (, 0.98916), (2019, 0.98725), (, 0.98703),(, 0.98666), (, 0.98649), (, 0.98606), (, 0.98587), (, 0.98563), (a, 0.98546), (, 0.98471), (, 0.98461),(, 0.98451), (, 0.98445), (, 0.98441), (, 0.98398), (, 0.98365), (, 0.98294), (, 0.9822),(, 0.9794), (, 0.97937), (, 0.97758), (, 0.97752), (, 0.97674), (, 0.97423), (, 0.9712), (, 0.96755), (, 0.96739), (, 0.96676), (, 0.96633), (, 0.9661), (, 0.96411), (, 0.96329),(, 0.96265), (, 0.96137), (, 0.95868), (, 0.95856), (, 0.95822), (, 0.95057), (,0.95046), (, 0.94854), (, 0.94758), (, 0.94602), (, 0.94598)]",
  "Query: BoWSE(Query): [(, 0.343), (, 0.13778), (, 0.06248), (, 0.09616), (, 0.10872), (, 0.202)]": "Product: //KissDear/2021BoWSE(Product): [(, 1.0), (, 1.0), (, 1.0), (2021, 1.0), (, 0.99999), (, 0.99999), (, 0.99999),(, 0.99998), (, 0.99997), (, 0.99997), (, 0.99993), (, 0.99988), (, 0.99979), (, 0.99979), (,0.99979), (, 0.99975), (, 0.99975), (, 0.99965), (, 0.99955), (, 0.99948), (, 0.99926), (, 0.99919), (,0.99906), (, 0.99891), (, 0.9984), (, 0.99832), (, 0.9981), (dear, 0.99762), (, 0.99754), (, 0.99744),(, 0.99703), (, 0.99627), (, 0.99615), (kiss, 0.99586), (, 0.9952), (, 0.99498), (, 0.9935), (, 0.98934), (, 0.98758), (, 0.98725), (2021, 0.98496), (, 0.978), (, 0.96437), (, 0.95704), (, 0.94864), (, 0.93963), (, 0.9209), (, 0.90725), (, 0.89087), (, 0.88608), (, 0.87247), (, 0.86876), (,0.85713), (, 0.85588), (, 0.82923), (, 0.82689), (, 0.82627), (, 0.81626), (, 0.78414), (, 0.77402), (, 0.76135), (, 0.6812), (, 0.60279), (, 0.59161), (, 0.56128), (, 0.55233), (, 0.54342), (, 0.54124), (,0.52361), (, 0.51875), (, 0.51707), (, 0.51121), (, 0.50859), (, 0.48766), (, 0.48612), (, 0.46126),(, 0.45578), (4, 0.44235), (, 0.43786), (, 0.42136), (, 0.40858), (, 0.39326), (, 0.38609), (,0.35582), (, 0.34883), (, 0.32625), (1.5, 0.30884), (, 0.29069), (, 0.28852), (, 0.28552), (, 0.28548), (, 0.28526), (, 0.28513), (, 0.28248), (, 0.27353), (, 0.26865), (, 0.26277), (, 0.25459)]"
}