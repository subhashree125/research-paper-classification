{
  "ABSTRACT": "Machine learning-based forecasting models are commonly usedin Intelligent Transportation Systems (ITS) to predict traffic pat-terns and provide city-wide services. However, most of the existingmodels are susceptible to adversarial attacks, which can lead to in-accurate predictions and negative consequences such as congestionand delays. Therefore, improving the adversarial robustness of thesemodels is crucial for ITS. In this paper, we propose a novel frame-work for incorporating adversarial training into spatiotemporaltraffic forecasting tasks. We demonstrate that traditional adver-sarial training methods designated for static domains cannot bedirectly applied to traffic forecasting tasks, as they fail to effectivelydefend against dynamic adversarial attacks. Then, we propose areinforcement learning-based method to learn the optimal nodeselection strategy for adversarial examples, which simultaneouslystrengthens the dynamic attack defense capability and reduces themodel overfitting. Additionally, we introduce a self-knowledge dis-tillation regularization module to overcome the \"forgetting issue\"caused by continuously changing adversarial nodes during train-ing. We evaluate our approach on two real-world traffic datasetsand demonstrate its superiority over other baselines. Our methodeffectively enhances the adversarial robustness of spatiotemporaltraffic forecasting models. The source code for our framework isavailable at",
  "Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 23, August 610, 2023, Long Beach, CA, USA. 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00 ACM Reference Format:Fan Liu, Weijia Zhang, and Hao Liu. 2023. Robust Spatiotemporal TrafficForecasting with Reinforced Dynamic Adversarial Training. In Proceedingsof the 29th ACM SIGKDD Conference on Knowledge Discovery and DataMining (KDD 23), August 610, 2023, Long Beach, CA, USA. ACM, New York,NY, USA, 13 pages.",
  "INTRODUCTION": "Machine learning-based forecasting models are widely used to accu-rately and promptly predict traffic patterns for providing city-wideservices in intelligent traffic systems (ITS) . However,these models can be fooled by carefully crafted perturbations, lead-ing to inaccurate traffic conditions predictions . For example, asmall perturbation to a traffic flow map can stimulate a machinelearning model to predict a traffic jam where there was none, lead-ing to unnecessary congestion and delays. demonstratesthe impact of an adversarial attack on the spatiotemporal forecast-ing model, resulting in significant bias in predictions. Fortunately,recent studies identify that incorporating a defense strategy caneffectively improve the adversarial robustness of machine learningmodels . Therefore, there is a pressing need to investigate suit-able defense strategies to stabilize the spatiotemporal forecastingmodels, particularly for ITS.Adversarial training is a technique that has been shown to en-hance the robustness of deep neural networks (DNNs) against adver-sarial attacks, particularly in static domains such as image and graph classification. This is achieved by incorpo-rating adversarial examples, generated through adversarial attacks,into the training process. The adversarial training is formulated asa min-max optimization problem, where the inner maximizationstep generates adversarial examples to explore worst-case scenarioswithin the adversarial perturbation space. These small, yet percepti-ble, perturbations are designed to cause the model to make incorrectpredictions. In the outer minimization step, the model is exposed toboth the original input data and the adversarial examples, which areused to update the model and improve its overall robustness againstsuch perturbations. Despite its efficacy in static domains , ad-versarial training for spatiotemporal traffic forecasting remainsunder-explored in dynamic fields.In this paper, we reveal the limitations of traditional adversarialtraining methods in defending against dynamic adversarial attacksin spatiotemporal traffic forecasting tasks. We demonstrate thatstatic strategies for selecting and generating adversarial examples,",
  "(c)": ": An example of adversarial attack and defense onspatiotemporal traffic forecasting model using the PeMS-Baydataset. (a) The adversarial attack injects malicious perturba-tions into the geo-distributed data sources. (b) As a result, theforecasting models performance degrades and produces bi-ased predictions under the adversarial attack. (c) The defenseagainst adversarial attacks is achieved through adversarialtraining, resulting in improved model performance and ac-curate traffic predictions. such as using degree and PageRank, fail to effectively defend againstthese attacks as shown in (a). First, we identify the staticapproach can not provide adequate defense against spatiotemporaladversarial attacks. Furthermore, we show that generating adver-sarial examples for all geo-distributed data sources also fails toeffectively defend against dynamic attacks, as a higher proportionof adversarial nodes may lead to overfitting and reduces modelperformance as shown in (b). Counterintuitively, a lowerproportion of adversarial nodes results in better model performancecompared to a higher proportion of adversarial nodes. Additionally,we highlight the issue of instability that arises when adversarialnodes change continuously during the training process, resultingin the \"forgetting issue\" where the model lacks robustness againststronger attack strengths. Such observations highlight highlightsthe need for an effective and robust approach to adversarial trainingin spatiotemporal traffic forecasting tasks.To overcome the aforementioned limitations, we propose a novelframework for incorporating adversarial training into traffic fore-casting tasks. Our approach involves dynamically selecting a subsetof nodes as adversarial examples, which not only reduces overfittingbut also improves defense capability against dynamic adversarialattacks. However, the task of selecting this subset from the totalset of nodes is a computationally challenging problem, known tobe NP-hard. To address this issue, we propose a reinforcementlearning-based method to learn the optimal node selection strategy.Specifically, we model the node selection problem as a combinato-rial optimization problem and use a policy-based network to learnthe node selection strategy that maximizes the inner loss. In de-tail, we design a spatiotemporal attention-based policy networkto model spatiotemporal geo-distributed data. To evaluate the so-lutions generated by the policy network, we propose a balancedstrategy for the reward function, providing stable and efficient feed-back to the policy network and mitigating the issue of inner lossdecreasing during training. The final pre-trained policy networkcan be used as a node selector. To overcome the forgetting issue,we also introduce a new self-knowledge distillation regularizationmodule for adversarial training, where the current model is trained",
  "(b)": ": (a) Comparison of model performance under static(PageRank and Degree) and dynamic (Random) defensestrategies in adversarial attacks. The graph shows the resultsof the traffic forecasting model under an adversarial attack,with defense using static PageRank and degree-based sam-pling strategies (left two bars) and dynamic random samplingstrategy (right bar). It is evident that the dynamic randomsampling strategy results in better performance than thestatic defense strategies. (b) Comparison of model perfor-mance under varying levels of adversarial node ratio whilemaintaining a consistent attack strength. The X-axis showsthe proportion of adversarial nodes, and the Y-axis showsthe models performance. A lower proportion of adversarialnodes results in better model performance compared to ahigher proportion of adversarial nodes. (Clean MAE refers tothe models performance when evaluated on clean examples,whereas Adversarial MAE indicates the models performancewhen evaluated on adversarial examples.) using knowledge distilled from the previous models experiencewith adversarial attacks.Our contributions can be summarized as follows. 1). To ourknowledge, we are the first to defend against adversarial attacks forspatiotemporal traffic forecasting by systematically analyzing howto apply adversarial training to traffic forecasting models. 2). Wepropose a novel framework for improving the adversarial robust-ness of spatiotemporal traffic forecasting. This includes modelingthe node selection problem as a combinatorial optimization prob-lem and using a reinforcement learning-based method to learn theoptimal node selection strategy. Furthermore, we incorporate self-knowledge distillation as a new training technique to tackle thechallenge of continuously evolving adversarial nodes, thus avoid-ing the \"forgetting issue. 3). We conduct extensive experiments ontwo real-world datasets and demonstrate the effectiveness of ourframework and its individual components.",
  "NOTATION AND PRELIMINARIES": "We first provide an overview of nations, and then delve into thetopics of spatiotemporal traffic forecasting, adversarial training,and the threat model.The traffic network can be represented by the graph G = (V, E),where V is a set of nodes (such as traffic sensors, road stretches,highway segments, etc.) and E is a set of edges. We denote the ad-jacency matrix A to represent the traffic network G = (V, E). Fur-thermore, we use the X = {x1,, x2,, x3,, , x, } geo-distributeddata features, where x, represents the traffic conditions (e.g., traf-fic speed, the volume of vehicles, and traffic flow etc.) and context",
  "Threat Model": "We follow the taxonomy of adversarial attacks on spatiotemporaltraffic forecasting models as presented in . Attackers goal.The goal of the attacker is to create adversarial traffic states that willcause spatiotemporal forecasting models to derive biased predic-tions. Evasion attack. The attack is launched during the inferencestage after the model has already been trained. Attackers capabil-ity. Our focus is on spatiotemporal feature-level attacks, in whichthe attacker can alter the spatiotemporal features by injecting ad-versarial perturbations into geo-distributed data sources. We donot focus on graph structure-level attacks because spatiotempo-ral feature-level attacks can lead to higher attack success rates.Note that the attacker does not have the ability to manipulatethe models architecture or parameters during the attack. Attackstrength. The attacker can inject adversarial perturbations into %of the geo-distributed data sources. As the proportion of hackedgeo-distributed data sources increases, the attack becomes strongerand more intense.Problem definition: The goal of this research is to develop anadversarial robust spatiotemporal traffic forecasting model, denotedas F (), that is capable of defending against adversarial attacks ongeo-distributed data sources.",
  "This section presents an experimental investigation of the use ofadversarial training for spatiotemporal traffic forecasting. Our pro-posed framework, outlined in detail in .2, centers on the": "dynamic selection of a subset of nodes as adversarial nodes. In.3, we mathematically model the selection of the optimalsubset of nodes as a combinatorial optimization problem and in-troduce a spatiotemporal attention-based representation moduleto improve the learning of node representations and aid in policylearning. To address the issue of instability, .4 introducesa self-distillation regularization term to prevent forgetting.",
  "Framework Overview": "illustrates the framework of Reinforced Dynamic Adver-sarial Training (RDAT), which aims to enhance the robustness ofspatiotemporal traffic forecasting models against adversarial at-tacks. Our method employs dynamic selection of a subset of nodesas adversarial examples, which improves defense against dynamicattacks while reducing overfitting. To determine the optimal subsetof adversarial nodes, we propose a reinforcement learning-basedapproach. Specifically, we formulate the node selection problemas a combinatorial optimization problem and use a policy-basednetwork to learn the strategy that maximizes the inner loss. Ourapproach includes a spatiotemporal attention-based policy networkthat models spatiotemporal geo-distributed data, and a balancedreward function strategy to provide stable and efficient feedback tothe policy network and alleviate the issue of decreasing inner lossduring training. The final pre-trained policy network can be used asa node selector. To address the \"forgetting issue,\" we also introducea self-knowledge distillation regularization for adversarial training,where the current model is trained using knowledge distilled fromthe previous models experience with adversarial attacks. 3.2Adversarial Training FormulationIn this section, we investigate the application of traditional adver-sarial training methods to spatiotemporal traffic forecasting andpresent our proposed adversarial training formulation.Initially, we hypothesized that protecting a larger proportion ofnodes during adversarial training would lead to improved robust-ness of the forecasting model. However, our exploratory experimentshowed that this was not the case. In fact, a higher proportion ofpoisoned nodes resulted in a greater degradation of the forecastingmodels performance. To test this, we conducted an experiment inwhich we randomly selected varying proportions of nodes as ad-versarial samples during adversarial training, as shown in .The results of the experiment were counter-intuitive and revealedthat a smaller proportion of dynamically selected nodes resulted ina more robust model. provides insight into the relationship between the pro-portion of adversarial nodes and the training loss. The figure showstwo different scenarios, where the x-axis represents the numberof training steps and the y-axis represents the training loss. In Fig-ure 4 (a), a high proportion of adversarial nodes (80% randomlyselected among all nodes) is used, resulting in overfitting and anunstable training curve. This is likely due to the model becomingtoo specialized to the specific set of protected nodes, leading topoor generalization to new samples. In contrast, (b) demon-strates that a lower proportion of adversarial nodes (10% randomlyselected among all nodes) tends to mitigate the overfitting issueand results in a more stable training curve.",
  ": The figure illustrates the training loss over time. Ateach training step, adversarial nodes are randomly sampledas part of the training process": "Spatiotemporal adversarial examples. Our approach is basedon the insight that the key to improving the robustness of themodel is to actively identify and focus on the most extreme casesof adversarial perturbation. Specifically, following , the worst-case scenario in traffic forecasting models involves both spatialand temporal aspects. From the temporal aspect, the attacker caninject adversarial perturbations into the feature space. To effectivelydefend against various types of attacks, it is crucial to thoroughlyexplore the worst-case scenarios in the adversarial perturbationspace , which is similar to the approach taken in the field ofimage recognition. From a spatial perspective, it devises a dynamicnode selection approach in each training epoch to maximize theinner loss and ensure that all nodes had a fair opportunity to bechosen. To achieve this, we dynamically select a subset of nodesthat exhibit spatiotemporal dependence from the full set of nodesat each training iteration. To operationalize this, we first define thepermitted adversarial perturbation space as follows:",
  "where is the spatiotemporal adversarial example. t is thespatiotemporal adversarial perturbations. The matrix I {0, 1}": "is the adversarial nodes indicator, which is a diagonal matrix whoseth diagonal element denotes whether or not node has beenchosen as an adversarial node at time. Specifically, the th diagonalelement of the matrix is equal to 1 if node has been selected asan adversarial node and 0 otherwise. The parameter is the budgetfor the number of nodes, and is the budget for the adversarialperturbation.The adversarial training approach for spatiotemporal traffic fore-casting is formulated as follows:",
  "TL (F ( :; ), +1:+ ),(4)": "where : = { , , } is the adversarial trafficstates from time slot to . T represents the set of time stepsof all training samples. L () represents the user-specified lossfunction for adversarial training, which can include commonly usedmetrics such as Mean Squared Error (MSE) or others. The innermaximization aims to find the optimal adversarial perturbation thatmaximizes the loss. In the outer minimization, the model parametersare updated to minimize the prediction loss.",
  "Reinforced Optimal Node Subset Learning": "In this section, we formulate the problem of selecting the opti-mal subset of nodes from a set of spatiotemporal geo-distributeddata sources as a combinatorial optimization problem. The probleminstance, denoted as , consists of nodes represented by spatiotem-poral features : from time slot to . The goal is to select nodes from the full set of nodes, represented by a subset of nodes = (1, ,), where {1, , } and .Given a problem instance, the objective is to learn the parameter of a stochastic policy ( | ) using the chain rule to factor-ize the probability of the solution. The policy network uses thisinformation to determine the optimal subset of nodes to select inorder to explore the most extreme case of adversarial perturbationat each training iteration.",
  "The policy network includes the encoder and decoder parts. Theencoder is to produce the geo-distributed data into the embeddings.The decoder generates the sequence of": "3.3.1Policy Network Design. The policy network takes the spa-tiotemporal features : as input and produces the solution. It is composed of a spatiotemporal encoder and a multi-head-attention decoder. The encoder converts the spatiotemporal fea-tures into embeddings, and the decoder constructs the solution inan auto-regressive manner, selecting a node at a time and usingthe previous selection to choose the next node until the completesolution is generated.Spatiotemporal encoder. We utilized a spatiotemporal encoder,which is similar to the GraphWave Net , to transform spatiotem-poral traffic data into embeddings. The spatiotemporal encoderreceives spatiotemporal traffic data as input and produces embed-dings of nodes as output. The spatiotemporal encoder is typicallycomposed of multiple spatial and temporal layers.",
  "Robust Spatiotemporal Traffic Forecasting with Reinforced Dynamic Adversarial TrainingKDD 23, August 610, 2023, Long Beach, CA, USA": "3.4.1Adversarial Training on Spatiotemporal Traffic ForecastingModel . The training process is divided into two stages. In thefirst stage, we train the policy network using Algorithm 1. In thesecond stage, we use the pre-trained policy network to select theadversarial nodes for computational efficiency, and then computethe adversarial examples by using the PGD method. The adversarialexamples are computed using Equation 19 with the adversarialtraining loss L in Equation 23. Finally, we update the forecastingmodel parameters using the Adam optimizer. The entire trainingprocess is outlined in Algorithm 2 in Appendix A.",
  "where F is the set of node embeddings, and F is the embedding fornode . The average of all the node embeddings is denoted as thegraph embedding and can be represented as 1": "F.Multi-Head-Attention decoder. The decoder generates a se-quence of nodes, , by iteratively selecting individual nodes, ,at each step , using both the encoders embedding and the outputof previous steps, , for < , as input.Specifically, the decoders input comprises the graph embeddingand the embedding of the last node, with the first selected nodesembedding being a learned embedding. The decoder calculates theprobability of each node being selected as an adversarial node, whilealso taking computational efficiency into consideration. Duringdecoding process, context is represented by a special context node(c). To this end, we incorporate the attention-based decoder tocompute an attention layer on top of the decoder, with messagesonly to the context node (c). The context node embedding is definedas follows:",
  "where is the probability of node and is the current node.The node with the highest probability among all nodes is selectedas the next sampling node": "3.3.2Balanced-reward Function Design . The main challenge inpolicy network learning is evaluating the solutions generated bythe policy network. One approach is to use the inner loss, computedusing the solutions , as the reward, with larger values indicatingbetter solutions. However, as the training progresses, the inner lossis expected to decrease as the model becomes more robust, whichcan lead to incorrect feedback and suboptimal solutions. To addressthis, we propose a balanced strategy for the reward function. Insteadof solely using the inner loss, we compare the results generated bythe policy network to those generated by a baseline node selectorand use the difference as the reward. This approach provides stableand efficient feedback to the policy network and helps to mitigatethe issue of decreasing inner loss during training.Specifically, we first obtain the set of adversarial nodes indi-cator I based on the solution = (1, ,), where {1, , }, using the following function:",
  "() = L (F (X:; A), +1:+ )(17)": "where L () is the MSE loss, and X = {X, , X }.To ensure that the policy network receives stable and efficientfeedback, we implement a balanced strategy for the reward. Specif-ically, we use a baseline node selector (e.g., Random selector,randomly select the nodes, etc.) to select nodes as the solution .The results generated by the policy network are then compared tothe baseline results, and the difference is used as the reward. Thisis represented by the following equation:",
  "Where () is the solution generated by policy network, and ()": "is the solution generated by the baseline selector, we use the su-perscript (p) and (b) to align with the policy network selector andbaseline selector, respectively. In this way, the balanced-rewardfunction (()) is used as the reward signal to guide the policynetwork to update the solution . In practice, we adopt a heuristicapproach as a baseline selector to select nodes named TNDS . 3.3.3Policy Network Training. The training of the policy networkis executed by alternately training the policy network and thespatiotemporal traffic forecasting model in an adversarial manner.Specifically, the policy network generates a solution sequence, de-noted as , based on the input :. The balanced-reward is thencalculated and used to update the policy network. Subsequently,the final node selection indicators are employed to calculate theadversarial examples, denoted as :, via Projected Gradient",
  "(): =clip :, ( (1):+ sign(L (F (X:; A), +1:+ ) I)),(19)": "where clip :, () operator is used to limit the maximum per-turbation of the variable : to a budget of . The adversarialexamples at the th iteration is represented by ():. is thestep size, and I is the final node selection indicators obtained fromthe policy network, and L () is the mean squared error lossfunction.Subsequently, the spatiotemporal traffic forecasting model istrained on the adversarial examples to optimize the forecastingmodel loss as follows,",
  "Adversarial Training with Regularization": "Another challenge in adversarial training for spatiotemporal trafficforecasting is instability, which can occur when the adversarialnodes are constantly changing during the training process. Thiscan lead to a situation where the model is unable to effectivelyremember all the historical adversarial nodes, resulting in a lack ofrobustness against stronger attack strengths, commonly referredto as the \"forgetting issue\" . To address this, we proposeusing knowledge distillation (KD) to transfer knowledge from ateacher model to a student model. Previous studies have shownthat KD can improve the adversarial robustness of models .However, traditional teacher models are static and cannot providedynamic knowledge. To overcome this limitation, we introduce anew self-knowledge distillation regularization for adversarial train-ing. Specifically, we use the model from the previous epoch as theteacher model, meaning that the current spatiotemporal traffic fore-casting model is trained using knowledge distilled from the previousmodel. In this way, the current model can learn from the previousmodels experience with adversarial attacks. The knowledge distilloss is defined as follows, L = L (F ( :; ), F (:; ))(22)where L () is the knowledge distillation loss (e.g.,. MSE etc.).and F () is the teacher model, which is adopted from last trainedmodel. In summary, the final adversarial training loss is defined asfollows,",
  "Evaluation": "Datasets. We use two real-world spatiotemporal traffic forecastingdatasets, PEMS-BAY and PEMS-D4 , which were collectedby the California Performance of Transportation (PeMS) and con-tain traffic speed and flow data, respectively. These datasets aresorted by time in ascending order and have a time interval of 5minutes between consecutive points. We allocate 70% of the datafor training, 10% for validation, and 20% for testing.Baselines. There are few studies in the current literature thatcan be directly applied to the real-value traffic forecasting defensesetting. Therefore, to ensure fair comparisons, we use state-of-the-art adversarial training methods, including adversarial training(AT) , TRADE , Mixup , and GrpahAT with therandom node selection strategy. Additionally, we use the recentstate-of-the-art traffic forecasting attack method, TDNS , as adynamic node selection method in combination with AT, which werefer to as AT-TDNS. These methods serve as our baselines.Target model. We adopt the state-of-the-art spatiotemporaltraffic forecasting model, GraphWave Net , as the target modelto evaluate the generalization of our adversarial training framework.Results on additional target models are included in the appendix B.3.Evaluation metrics. For evaluating the adversarial robustnessof spatiotemporal traffic forecasting models, we adopt the meanabsolute error (MAE), and root mean squared error (RMSE) asevaluation metrics.Implement details. We conduct our experiments using Pytorchon a Linux Centos Server with 12 RTX 3090 GPUs and 2 RTX A40GPUs. The traffic data is normalized to the range , and theinput and output lengths are set to = 12 and = 12, respectively.",
  ": Adversarial robustness performance under differentattack strengths": "We follow the attack setting in , using PGD-Random, PGD-PR,PGD-Centrality, PGD-Degree, and PGD-TNDS as the attacker. Theregularization parameter is set to 0.4. The perturbation magnitude is 0.5 for both training and testing. During training, we select10% of the total nodes as adversarial examples at each epoch, whilein testing, we use a stronger attack strength and select 20% of thetotal nodes as adversarial examples. We conducted the experimentsfive times and present the average results along with the standarddeviations (STD) of the metrics.",
  "EQ1: Main Results": "To answer the first evaluation question (EQ1), we compare theproposed method with state-of-the-art adversarial training meth-ods on two real-world traffic forecasting datasets, PEMS-BAY andPEMS-D4. and 2 present the overall adversarial robustnessperformance of our proposed defense method against adversarialtraffic forecasting attacks and five baselines, evaluated using twometrics. Our adversarial training method significantly improvesthe adversarial robustness of traffic forecasting models, as shownby the (68.55%, 66.0350%) and (69.70%, 69.0343%) improvementsin the PeMS-BAY and PEMS-D4 datasets, respectively, under thePGD-Random attack where no defense strategy was applied. Ad-ditionally, our method achieved a (1.12%, 2.10%) improvement inclean performance and (7.65%, 12.19%), (7.75%, 3.31%), and (7.35%,2.81%) improvements in adversarial robustness under the PGD-Random, PGD-PR, PGD-Degree attacks, respectively, comparedto almost all baselines in terms of PEMS-BAY. While our methodwas slightly weaker than GraphAT (4.4545 compared to 4.3762)in terms of RMSE under the PGD-TNDS attack, our method hadlower standard deviation, indicating increased stability. Overall,our method significantly enhances the adversarial robustness ofthe traffic forecasting model against adversarial traffic attacks.We further examines the robustness of our traffic forecastingmodel against adversarial attacks, in comparison to five baselines,under four different attack strengths ( = 40, 60, 80, 100). The re-sults displayed in demonstrate that our method demon-strates superior performance across all attack strengths, exempli-fied by a 13.9842% and 2.8602% improvement on PeMS-BAY andPeMS-D4 under 100% attack strength. Notably, TRADE methodperformed inferiorly to other adversarial training methods (AT,Mixup, GraphAT, AT-TNDS) under stronger attack strengths, likelydue to the trade-off between clean performance and adversarialperformance. Furthermore, the results indicate that AT-TNDS out-performs almost all other baselines, which verifies the effectivenessof dynamically selecting a subset of nodes as the adversarial nodes.",
  "MethodNon-attackPGD-RandomPGD-PRPGD-CentralityPGD-DegreePGD-TNDS": "Non-defense27.8656 (0.5019)/41.6828 (0.4553)88.6441 (4.1227)/128.5004 (6.7317)94.3075 (4.8237)/144.2537 (9.1144)86.0901 (7.3620)/154.8890 (18.2470)94.8585 (6.0286)/155.8989 (11.8078)86.0901 (7.3620)/154.8890 (18.2470) AT33.3386 (3.9365)/47.1510 (3.5193)39.8153 (5.5781)/56.2785 (7.2508)44.2567 (7.8589)/63.2609 (12.7096)48.7779 (10.4840)/75.9330 (20.7510)48.3946 (10.5063)/71.7793 (18.3587)50.6256 (11.2085)/77.1303 (20.7762) Mixup38.2206 (2.7247)/52.9308 (3.5875)46.7455 (4.3982)/64.0452 (5.2427)48.2348 (5.2173)/65.9415 (6.4053)50.7168 (5.3034)/73.2369 (10.1490)49.1789 (5.1138)/68.4766 (7.6452)50.8233 (7.3831)/70.6203 (11.0908) TRADE34.5135 (4.7824)/49.4893 (5.7589)58.1406 (14.6082)/82.7603 (19.7603)65.8004 (18.4610)/96.5914 (28.5557)69.3065 (15.2959)/115.3199 (28.3532)70.8713 (18.3982)/110.2185 (32.3967)76.3904 (20.9501)/123.2154 (38.2284) GraphAT47.1434 (12.6269)/60.8673 (12.0997)55.8650 (12.1506)/72.7158 (12.2657)59.2201 (12.6541)/78.5934 (15.0033)61.9119 (12.6672)/86.6466 (17.7716)62.5326 (14.0952)/85.1818 (18.9048)62.5210 (12.8110)/85.6943 (16.0209) AT-TNDS31.8291 (1.3078)/66.6957 (1.2478)46.8909 (1.1684)/66.6957 (1.2478)49.4102 (1.8226)/70.5907 (1.2680)55.1733 (2.1787)/91.3877 (6.6509)53.2867 (1.4795)/80.0251 (1.1357)54.3390 (2.4273)/83.5821 (6.1762) Ours34.2860 (5.9462)/48.8814 (6.9747)41.5189 (10.2116)/58.0855 (12.5508)43.7104 (11.3853)/61.4063 (15.0858)46.0493 (11.5675)/68.5447 (18.2039)45.4336 (11.7635)/65.1032 (16.5949)44.9669 (9.8571)/64.15829 (13.1911)",
  "EQ2: Ablation Study": "In order to answer EQ 2, we examined the impact of differentcomponents of our adversarial training framework on the per-formance of traffic forecasting models by conducting an ablationstudy using the mean absolute error (MAE) metric on the PeMS-BAY dataset. We evaluated four variations of our method: (1) AT-Degree, which selects nodes based on their normalized degree ina static manner, (2) AT-Random, which selects nodes randomly ina dynamic manner, (3) AT-TNDS, which selects nodes based on aspatiotemporal-dependent method, (4) AT-Policy, which uses a pre-trained policy network to choose nodes without self-distillation,and (5) our method, which uses a pre-trained policy network withself-distillation regularization to choose nodes. As reported in Fig-ure 6, removing any component causes significant performancedegradation. In particular, we observed a significant degradationwhen using the static node selection strategy, which demonstratesthe effectiveness of our adversarial training framework. Secondly,by comparing AT-Random and AT-Policy, we found that the policynetwork plays an important role in selecting the subset of nodes asadversarial examples. Lastly, we also observed that self-distillation",
  "EQ3: Parameter Analysis": "To answer EQ 3, we conducted a sensitivity analysis to evaluate theimpact of hyperparameters on the performance of our adversarialtraining framework using the PeMS-D4 dataset as an example. Theparameters studied were the number of inner iteration (b) and theregularization parameter (), while all other parameters remainedconstant. The results showed an overall increasing trend in perfor-mance with increasing number of inner iterations, with a peak at 30",
  "Case Study": "In this section, we conduct the case study to show the effectivenessof our adversarial training framework. The case study presented inthe provides a visual representation of the effectiveness ofour proposed adversarial training framework for spatiotemporaltraffic forecasting tasks. (a) illustrates the results of a trafficforecasting model that has not been defended against adversarrialattacks, while (b) illustrates the results of the same modelwith our proposed adversarial training defense. It is clear from thefigures that the model without defense provides biased predictionsunder adversarial attack, while the model with defense (adversarialtraining) maintains its prediction accuracy and is able to providesimilar results as the original predictions.",
  "In recent years, deep learning has found widespread applicationsin diverse domains, including job skill valuation , time seriesprediction and spatiotemporal traffic forecasting . Among": "these applications, spatiotemporal traffic forecasting plays a crucialrole in the success of intelligent transportation systems . The ability to predict traffic patterns in both spaceand time is critical for effective traffic management and improvedtravel experiences for commuters. To address these forecastingproblems, deep learning models have been extensively exploreddue to their superior ability to model the complex spatial and tem-poral dependencies present in traffic data. Various methods havebeen proposed to enhance the accuracy of traffic forecasting, suchas STGCN , DCRNN and GraphWave Net , each ofwhich utilizes different techniques to capture spatial and temporalinformation. Despite the advancements made, the vulnerability andadversarial robustness of spatiotemporal traffic forecasting modelsremain an unexplored area of research.",
  "Adversarial Training": "The existing literature has explored various methods to improvethe robustness of deep learning models against adversarial at-tacks . One popular approach is adversarialtraining, which aims to enhance the intrinsic robustness of themodel by augmenting the training data with adversarial examples.This approach is based on a min-max optimization problem, wherethe solution is found by identifying the worst-case optimum. Ad-versarial training methods have been proposed for different typesof data, such as images and graphs. For example, TRADE is amethod that balances the trade-off between clean performance androbustness, while GraphAT is a variant of adversarial trainingspecifically designed for graph-structured data. However, currentadversarial training methods mostly focus on the static domain,such as images and graphs, and the dynamic domain, such as spa-tiotemporal forecasting, is less explored.",
  "CONCLUSION": "In conclusion, this paper presents a novel framework for incorpo-rating adversarial training into spatiotemporal traffic forecastingtasks. We reveal that traditional adversarial training methods instatic domains are not suitable for defending against dynamic ad-versarial attacks in traffic forecasting tasks. To this end, we proposea reinforcement learning-based method to learn the optimal strat-egy for selecting adversarial examples, which improves defenseagainst dynamic attacks and reduces overfitting. Additionally, weintroduce a self-knowledge distillation regularization to overcomethe \"forgetting issue\" caused by constantly changing adversarialnodes during training. We evaluate our approach on two real-worldtraffic datasets and demonstrate its superiority over other baselines.Our method effectively enhances the adversarial robustness of spa-tiotemporal traffic forecasting models, which is essential in ITS toprovide accurate predictions and avoid negative consequences suchas congestion and delays. This research was supported in part by the National Natural ScienceFoundation of China under Grant No.62102110, Guangzhou Scienceand Technology Plan Guangzhou-HKUST(GZ) Joint Project No.2023A03J0144, and Foshan HKUST Projects (FSUST21-FYTRI01A,FSUST21-FYTRI02A).",
  "KDD 23, August 610, 2023, Long Beach, CA, USA.Fan Liu, Weijia Zhang, & Hao Liu": "Dmitrii Avdiukhin, Slobodan Mitrovic, Grigory Yaroslavtsev, and Samson Zhou.2019. Adversarially Robust Submodular Maximization under Knapsack Con-straints. In Proceedings of the 25th ACM SIGKDD International Conference onKnowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8,2019, Ankur Teredesai, Vipin Kumar, Ying Li, Rmer Rosales, Evimaria Terzi, andGeorge Karypis (Eds.). ACM, 148156. Jinghui Chen and Quanquan Gu. 2020. RayS: A Ray Searching Method forHard-label Adversarial Attack. In KDD 20: The 26th ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27,2020, Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash (Eds.). ACM,17391747. Yinpeng Dong, Zhijie Deng, Tianyu Pang, Jun Zhu, and Hang Su. 2020. Ad-versarial distributional training for robust deep learning. Advances in NeuralInformation Processing Systems 33 (2020), 82708283. Yinpeng Dong, Ke Xu, Xiao Yang, Tianyu Pang, Zhijie Deng, Hang Su, andJun Zhu. 2022. Exploring Memorization in Adversarial Training. In The TenthInternational Conference on Learning Representations, ICLR 2022, Virtual Event,April 25-29, 2022. OpenReview.net. Ziquan Fang, Yuntao Du, Xinjun Zhu, Danlei Hu, Lu Chen, Yunjun Gao, andChristian S. Jensen. 2022. Spatio-Temporal Trajectory Similarity Learning inRoad Networks. In KDD 22: The 28th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining, Washington, DC, USA, August 14 - 18, 2022, AidongZhang and Huzefa Rangwala (Eds.). ACM, 347356.",
  "Fuli Feng, Xiangnan He, Jie Tang, and Tat-Seng Chua. 2021. Graph AdversarialTraining: Dynamically Regularizing Based on Graph Structure. IEEE Trans. Knowl.Data Eng. 33, 6 (2021), 24932504": "Micah Goldblum, Liam Fowl, Soheil Feizi, and Tom Goldstein. 2020. AdversariallyRobust Distillation. In The Thirty-Fourth AAAI Conference on Artificial Intelligence,AAAI 2020, The Thirty-Second Innovative Applications of Artificial IntelligenceConference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances inArtificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020. AAAIPress, 39964003. Shengnan Guo, Youfang Lin, Ning Feng, Chao Song, and Huaiyu Wan. 2019.Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic FlowForecasting. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference,IAAI 2019, The Ninth AAAI Symposium on Educational Advances in ArtificialIntelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019.AAAI Press, 922929. Liangzhe Han, Bowen Du, Leilei Sun, Yanjie Fu, Yisheng Lv, and Hui Xiong.2021. Dynamic and Multi-faceted Spatio-temporal Deep Learning for TrafficSpeed Forecasting. In KDD 21: The 27th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining, Virtual Event, Singapore, August 14-18, 2021, FeidaZhu, Beng Chin Ooi, and Chunyan Miao (Eds.). ACM, 547555. Xiaowei Jia, Sheng Li, Handong Zhao, Sungchul Kim, and Vipin Kumar. 2019.Towards Robust and Discriminative Sequential Data Learning: When and How toPerform Adversarial Training?. In Proceedings of the 25th ACM SIGKDD Interna-tional Conference on Knowledge Discovery & Data Mining, KDD 2019, Anchorage,AK, USA, August 4-8, 2019, Ankur Teredesai, Vipin Kumar, Ying Li, Rmer Rosales,Evimaria Terzi, and George Karypis (Eds.). ACM, 16651673.",
  "Wouter Kool, Herke van Hoof, and Max Welling. 2019. Attention, Learn to SolveRouting Problems!. In International Conference on Learning Representations": "Xiaoliang Lei, Hao Mei, Bin Shi, and Hua Wei. 2022. Modeling Network-levelTraffic Flow Transitions on Sparse Data. In KDD 22: The 28th ACM SIGKDDConference on Knowledge Discovery and Data Mining, Washington, DC, USA,August 14 - 18, 2022, Aidong Zhang and Huzefa Rangwala (Eds.). ACM, 835845. Ting Li, Junbo Zhang, Kainan Bao, Yuxuan Liang, Yexin Li, and Yu Zheng. 2020.AutoST: Efficient Neural Architecture Search for Spatio-Temporal Prediction. InKDD 20: The 26th ACM SIGKDD Conference on Knowledge Discovery and DataMining, Virtual Event, CA, USA, August 23-27, 2020, Rajesh Gupta, Yan Liu, JiliangTang, and B. Aditya Prakash (Eds.). ACM, 794802. Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. 2018. Diffusion ConvolutionalRecurrent Neural Network: Data-Driven Traffic Forecasting. In 6th InternationalConference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April30 - May 3, 2018, Conference Track Proceedings. OpenReview.net. Fan Liu, Hao Liu, and Wenzhao Jiang. 2022. Practical Adversarial Attacks onSpatiotemporal Traffic Forecasting Models. In Advances in Neural Information Pro-cessing Systems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and KyunghyunCho (Eds.).",
  "Javier Maroto, Guillermo Ortiz-Jimnez, and Pascal Frossard. 2022. On the benefitsof knowledge distillation for adversarial robustness. CoRR abs/2203.07159 (2022).arXiv:2203.07159": "Zheyi Pan, Yuxuan Liang, Weifeng Wang, Yong Yu, Yu Zheng, and Junbo Zhang.2019. Urban Traffic Prediction from Spatio-Temporal Data Using Deep MetaLearning. In Proceedings of the 25th ACM SIGKDD International Conference onKnowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8,2019, Ankur Teredesai, Vipin Kumar, Ying Li, Rmer Rosales, Evimaria Terzi, andGeorge Karypis (Eds.). ACM, 17201730. Ren Pang, Xinyang Zhang, Shouling Ji, Xiapu Luo, and Ting Wang. 2020. Ad-vMind: Inferring Adversary Intent of Black-Box Attacks. In KDD 20: The 26thACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event,CA, USA, August 23-27, 2020, Rajesh Gupta, Yan Liu, Jiliang Tang, and B. AdityaPrakash (Eds.). ACM, 18991907. Xiong Peng, Feng Liu, Jingfeng Zhang, Long Lan, Junjie Ye, Tongliang Liu, andBo Han. 2022. Bilateral Dependency Optimization: Defending Against Model-inversion Attacks. In KDD 22: The 28th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining, Washington, DC, USA, August 14 - 18, 2022, AidongZhang and Huzefa Rangwala (Eds.). ACM, 13581367. Weili Shi, Ronghang Zhu, and Sheng Li. 2022. Pairwise Adversarial Trainingfor Unsupervised Class-imbalanced Domain Adaptation. In KDD 22: The 28thACM SIGKDD Conference on Knowledge Discovery and Data Mining, Washington,DC, USA, August 14 - 18, 2022, Aidong Zhang and Huzefa Rangwala (Eds.). ACM,15981606. Satya Narayan Shukla, Anit Kumar Sahu, Devin Willmott, and J. Zico Kolter. 2021.Simple and Efficient Hard Label Black-box Adversarial Attacks in Low QueryBudget Regimes. In KDD 21: The 27th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining, Virtual Event, Singapore, August 14-18, 2021, FeidaZhu, Beng Chin Ooi, and Chunyan Miao (Eds.). ACM, 14611469.",
  "Florian Tramer and Dan Boneh. 2019. Adversarial training and robustness formultiple perturbations. Advances in Neural Information Processing Systems 32(2019)": "Binghui Wang, Jinyuan Jia, Xiaoyu Cao, and Neil Zhenqiang Gong. 2021. CertifiedRobustness of Graph Neural Networks against Adversarial Structural Perturba-tion. In KDD 21: The 27th ACM SIGKDD Conference on Knowledge Discovery andData Mining, Virtual Event, Singapore, August 14-18, 2021, Feida Zhu, Beng ChinOoi, and Chunyan Miao (Eds.). ACM, 16451653. Yutong Wang, Yufei Han, Hongyan Bao, Yun Shen, Fenglong Ma, Jin Li, andXiangliang Zhang. 2020. Attackability Characterization of Adversarial EvasionAttack on Discrete Data. In KDD 20: The 26th ACM SIGKDD Conference onKnowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27,2020, Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash (Eds.). ACM,14151425.",
  "Ronald J. Williams. 1992. Simple Statistical Gradient-Following Algorithms forConnectionist Reinforcement Learning. Mach. Learn. 8 (1992), 229256": "Dongxia Wu, Liyao Gao, Matteo Chinazzi, Xinyue Xiong, Alessandro Vespignani,Yi-An Ma, and Rose Yu. 2021. Quantifying Uncertainty in Deep SpatiotemporalForecasting. In KDD 21: The 27th ACM SIGKDD Conference on Knowledge Dis-covery and Data Mining, Virtual Event, Singapore, August 14-18, 2021, Feida Zhu,Beng Chin Ooi, and Chunyan Miao (Eds.). ACM, 18411851. Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, and Chengqi Zhang. 2019.Graph WaveNet for Deep Spatial-Temporal Graph Modeling. In Proceedings ofthe Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI2019, Macao, China, August 10-16, 2019, Sarit Kraus (Ed.). ijcai.org, 19071913. Junchen Ye, Leilei Sun, Bowen Du, Yanjie Fu, Xinran Tong, and Hui Xiong.2019. Co-Prediction of Multiple Transportation Demands Based on Deep Spatio-Temporal Neural Network. In Proceedings of the 25th ACM SIGKDD InternationalConference on Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK,USA, August 4-8, 2019, Ankur Teredesai, Vipin Kumar, Ying Li, Rmer Rosales,Evimaria Terzi, and George Karypis (Eds.). ACM, 305313.",
  "Proceedings of the Twenty-Seventh International Joint Conference on Artificial In-telligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, Jrme Lang (Ed.).ijcai.org, 36343640": "Chaojian Yu, Bo Han, Li Shen, Jun Yu, Chen Gong, Mingming Gong, and TongliangLiu. 2022. Understanding Robust Overfitting of Adversarial Training and Beyond.In International Conference on Machine Learning, ICML 2022, 17-23 July 2022,Baltimore, Maryland, USA (Proceedings of Machine Learning Research, Vol. 162),Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvri, Gang Niu, andSivan Sabato (Eds.). PMLR, 2559525610. Hongyi Zhang, Moustapha Ciss, Yann N. Dauphin, and David Lopez-Paz. 2018.mixup: Beyond Empirical Risk Minimization. In 6th International Conference onLearning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3,2018, Conference Track Proceedings. OpenReview.net. Weijia Zhang, Hao Liu, Jindong Han, Yong Ge, and Hui Xiong. 2022. Multi-agentgraph convolutional reinforcement learning for dynamic electric vehicle chargingpricing. In Proceedings of the 28th ACM SIGKDD conference on knowledge discoveryand data mining. 24712481. Weijia Zhang, Hao Liu, Yanchi Liu, Jingbo Zhou, and Hui Xiong. 2020. Semi-supervised hierarchical recurrent graph neural network for city-wide parkingavailability prediction. In Proceedings of the AAAI Conference on Artificial Intelli-gence, Vol. 34. 11861193.",
  "Jingbo Zhou and Anthony KH Tung. 2015. Smiler: A semi-lazy time series pre-diction system for sensors. In Proceedings of the 2015 ACM SIGMOD InternationalConference on Management of Data. 18711886": "Dingyuan Zhu, Ziwei Zhang, Peng Cui, and Wenwu Zhu. 2019. Robust GraphConvolutional Networks Against Adversarial Attacks. In Proceedings of the 25thACM SIGKDD International Conference on Knowledge Discovery & Data Mining,KDD 2019, Anchorage, AK, USA, August 4-8, 2019, Ankur Teredesai, Vipin Kumar,Ying Li, Rmer Rosales, Evimaria Terzi, and George Karypis (Eds.). ACM, 13991407. Daniel Zgner and Stephan Gnnemann. 2020. Certifiable Robustness of GraphConvolutional Networks under Structure Perturbations. In KDD 20: The 26thACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event,CA, USA, August 23-27, 2020, Rajesh Gupta, Yan Liu, Jiliang Tang, and B. AdityaPrakash (Eds.). ACM, 16561665. ASPATIOTEMPORAL ADVERSARIALTRAININGThe training process is divided into two stages. In the first stage, wetrain the policy network using Algorithm 1. In the second stage, weuse the pre-trained policy network to select the adversarial nodesfor computational efficiency, and then compute the adversarial examples by using the PGD method. The adversarial examples arecomputed using Equation 19 with the adversarial training loss Lin Equation 23. Finally, we update the forecasting model parameters using the Adam optimizer in Algorithm 2.",
  "BSUPPLEMENTARY EXPERIMENTSB.1Implements Details": "We conduct our experiments using Pytorch on a Linux CentosServer with 12 RTX 3090 GPUs and 2 RTX A40 GPUs. The trafficdata is normalized to the range , and the input and outputlengths are set to = 12 and = 12, respectively. We adopt theattack setting as outlined in , utilizing PGD-Random, PGD-PR, PGD-Centrality, PGD-Degree, and PGD-TNDS as the attackers.The attack steps are set to 5, with a step size of = 0.1. To thor-oughly assess the adversarial robustness of our forecasting models,the attacks are conducted in a white-box setting, following themethodology in . The regularization parameter is set to 0.4.The perturbation magnitude is 0.5 for both training and testing.During training, we select 10% of the total nodes as adversarialexamples at each epoch, while in testing, we use a stronger attackstrength and select 20% of the total nodes as adversarial examples.We conducted the experiments five times and present the averageresults along with the standard deviations (STD) of the metrics.",
  "B.2Baselines": "Adversarial training is an effective approach to improving the ro-bustness of machine learning models against adversarial attacks. Inthis work, we evaluate the performance of several state-of-the-artadversarial training methods including: 1) AT : This methodapplies traditional adversarial training by selecting adversarialnodes randomly. 2) Mixup : A data augmentation technique thatblends pairs of training examples and their corresponding labels ina weighted combination to enhance the robustness of the model.3) TRADE : This method utilizes a trade-off based approach toperform adversarial training. It balances the accuracy of the modeland its robustness against adversarial attacks. 4) GraphAT : Thismethod is specifically designed for graph-structured data and ap-plies adversarial training to graph-based models. 5) AT-TNDS :This method implements a dynamic strategy for selecting the victimnodes and performs traditional adversarial training. The dynamicselection strategy allows for adaptability to changing adversarialpatterns during training.",
  "B.3Adversarial Robustness Comparison": "In order to further evaluate the effectiveness of our proposed method(RDAT), we conduct adversarial training experiments on othermethods including ASTGCN and STGCN on PeMS-D4. Theresults of these experiments are presented in and 4. Weobserved that adversarial training shows a considerable amountof variability in the ASTGCN and STGCN models. This might beassociated with the model architecture, as Graphwave Net has thecapability to directly learn the spatial relationships through thedata, potentially leading to assigning smaller weights to nodes thatare more susceptible to attacks."
}