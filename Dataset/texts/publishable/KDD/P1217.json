{
  "Abstract": "We study lower bounds for the problem of approximating a one dimensional distribution given(noisy) measurements of its moments. We show that there are distributions on that cannotbe approximated to accuracy in Wasserstein-1 distance even if we know all of their moments tomultiplicative accuracy (1 2(1/)); this result matches an upper bound of Kong and Valiant[Annals of Statistics, 2017]. To obtain our result, we provide a hard instance involving distributionsinduced by the eigenvalue spectra of carefully constructed graph adjacency matrices. Efficientlyapproximating such spectra in Wasserstein-1 distance is a well-studied algorithmic problem, and arecent result of Cohen-Steiner et al. [KDD 2018] gives a method based on accurately approximatingspectral moments using 2O(1/) random walks initiated at uniformly random nodes in the graph. As a strengthening of our main result, we show that improving the dependence on 1/ in this resultwould require a new algorithmic approach. Specifically, no algorithm can compute an -accurateapproximation to the spectrum of a normalized graph adjacency matrix with constant probability,even when given the transcript of 2(1/) random walks of length 2(1/) started at random nodes.",
  "Introduction": "A fundamental problem in linear algebra is to approximate the full list of eigenvalues, 1 . . . n R, of a symmetric matrix A Rnn, ideally in less time than it takes to compute a fulleigendecomposition.1 We focus on the particular problem of spectral density estimation where given (0, 1) and the assumption that A2 1, the goal is find approximate eigenvalues 1 . . . nsuch that their average absolute error is bounded by , i.e.,",
  "arXiv:2307.00474v1 [cs.DS] 2 Jul 2023": "Spectral density estimation is distinct from and in many ways more challenging than related problemslike low-rank approximation, where we only seek to approximate the largest magnitude eigenvaluesof A. Nevertheless, efficient randomized algorithms for spectral density estimation were developedin the early 1990s and have been applied widely in computational physics and chemistry [Ski89;SR94; Wan94; WWAF06]. These algorithms, which include the kernel polynomial and stochasticLanczos quadrature methods, achieve accuracy with high probability in roughly O(n2/) time,improving on the (n) cost of a full eigendecomposition for moderate values of [CTU21]. More recently, there has been a resurgence of interest in spectral density estimation within themachine learning and data science communities. Research activity in this area has been fueled byemerging applications in analyzing and understanding deep neural networks [PSG18; MM19; Pap18],in optimization [GKX19; Sag+17], and in network science [DBB19; CKSV18].",
  "Spectral Density Estimation for Graphs": "Interestingly, when A is the normalized adjacency matrix2 of an undirected graph G, there arefaster spectral density estimation algorithms than for general matrices. Specifically, assume thatwe can randomly sample a node from G and, given a node, randomly sample a neighbor, both inO(1) time. This is possible, for example, in the word RAM model when given arrays containing theneighbors for each node in G, and is also a commonly assumed access for computing on extremelylarge implicit networks [KLS11]. It was recently shown that the O(n2/) runtime of general purposealgorithms like stochastic Lanczos quadrature can be improved to O(n/poly()) [BKM22].3 Thisruntime is sublinear in the size of A, e.g., when the matrix has (n2) non-zero entries. Perhaps even more surprisingly, it is possible to solve spectral density estimation for normalizedadjacency matrices without any dependence on n. Suppose that we are given a weighted graph G,and again that we can randomly sample a node from G in O(1) time. Also assume that, for anygiven node, we can randomly sample a neighbor with probability proportional to its edge weightin O(1) time. In other words, we can initialize and take steps of an edge-weighted random walkin G in O(1) time.4 Then Cohen-Steiner et al. [CKSV18] gives an algorithm for any weightedundirected graph that solves the spectral density estimation problem with high probabilty in 2O(1/) time.5 While completely independent of the graph size, the poor dependence on in the result ofCohen-Steiner et al. [CKSV18] unfortunately makes the algorithm impractical for any reasonablelevel of accuracy. As such, an interesting question is whether the exponential dependence on canbe improved (maybe even to polynomial), while still avoiding any dependence on the graph size n.",
  "Central to this question is the connection between spectral density estimation and the problem oflearning a one dimensional distribution p given noisy measurements of ps (raw) moments. In this": "2If A is the unnormalized adjacency matrix of G and D is its diagonal degree matrix, we can equivalently considerthe asymmetric matrix, D1 A or the symmetric one, D1/2 AD1/2, as they have the same eigenvalues.3We use O(m) to denote O(m log m). The runtime in [BKM22] can be improved by a logarithmic factor toO(n/poly()) if we have access to a precomputed list of the degrees of nodes in G.4To be more concrete, if a node x is connected to neighbors y1, . . . , yd with edge weights w1, . . . , wd, then the walksteps from x to yi with probability wi/",
  "1x3p(x)dx, . . .": "Recent work of Kong and Valiant [KV17] shows that, for a fixed constant c, if the first = c/moments of any two distributions p and q supported on match exactly, then the Wasserstein-1distance between those distributions is at most . Given that the left hand size of (1) exactly equalsthe Wasserstein-1 distance W1(p, q) between the discrete distributions p(x) = 1",
  "nni=1 (x i) andq(x) = 1": "nni=1 (x i), the approach in Cohen-Steiner et al. [CKSV18] is to approximate the first moments of p, and then to find a set of approximate eigenvalues and eigenvalue multiplicities thatcorrespond to a discrete distribution q with the same moments. Given the approximate moments,finding q can be done in poly() time using linear programming algorithms.",
  "ntr(Aj). Thistrace can in turn be estimated by random walks of length j in A: if we start a random walk ata random node v, the probability that we return to v at the jth step is exactly equal to 1": "ntr(Aj).So, we can obtain an unbiased estimate for the jth moment by simply running random walks fromrandom starting nodes and calculating the empirical frequency that we return to our starting point. This approach leads to the remarkably simple algorithm of Cohen-Steiner et al. [CKSV18]. So wheredoes the 2O(1/) runtime dependence come from? The issue is that the result of Kong and Valiant[KV17] is brittle to noise. In particular, if the sum of squared distances between ps moments andqs moments differ by , the bound from Kong and Valiant [KV17] weakens, only showing that theWasserstein-1 distance is bounded by O(1 + 3). To obtain accuracy , it is necessary to set = O(1/) and thus equal to 2O(1/). By standard concentration inequalities, to obtain such anaccurate estimate to ps moments, we need to run an exponential number of random walks of length1, . . . , . Accordingly, an important step towards answering Question 1.1 is to understand if suchextremely accurate estimates of the moments is necessary for spectral density estimation. Note that many other spectral density estimation algorithms for general matrices are also based onmoment-matching. A common approach is to use randomized trace estimation methods [Hut90;MMMW21] to estimate moments of the form 11 Tj(x)p(x)dx = 1 ntr(Tj(A)), where Tj(x) is a degreej polynomial, not equal to xj. If Tj is the jth Chebyshev or Legendre polynomial, then it can beshown that only poly(1/) accurate estimates of the first = c/ moments are needed to approximatethe spectral density to error in Wasserstein-1 distance [BKM22]. A natural question then is, canthese general polynomial moments be estimated using random walks in time independent of n forgraph adjacency matrices? Unfortunately, it is not known how to do so: the challenge is that the th",
  "For all positive integers j, moments mj(p1) = 11 xjp1(x)dx and mj(p2) = 11 xjp2(x)dx areexponentially close: (1 )mj(p1) mj(p2) (1 + )mj(p1) for some 16 21/4": "Theorem 1.2 shows that Kong and Valiant [KV17]s requirement that each moment be estimated toaccuracy 2O(1/) cannot be avoided if we want an accurate approximation in Wasserstein distance.It thus rules out a direct improvement to the analysis of the spectral density estimation algorithmfrom of Cohen-Steiner et al. [CKSV18]. In particular, even if we had a procedure that returnedexponentially accurate multiplicative estimates to the moments of a graphs spectral density,6 andeven if it returns such estimates for all of the moments (not just the first O(1/)), then we wouldnot be able to distinguish between G1 and G2. Our proof of Theorem 1.2 is based on a hard instance built using cycle graphs. It is not hard toshow that the spectral densities of two disjoint cycles of length 1/ and of one cycle of length 2/differ by in Wasserstein-1 distance. Additionally, it can be shown that the first c/ moments ofthese graphs are exponentially close. This example would thus prove Theorem 1.2 if we restrictedour attention to moments of degree j c/. However, for the cycle graph, higher moments can bemore informative: for example, the jth moment for j = O(1/2) can be shown to distinguish thecycles of different length, even when only estimated to polynomial additive accuracy. To see whythis is the case, note that, since a random walk of length O(1/2) mixes on the cycle, the probabilityof it returning in the shorter cycle is roughly twice that as in the longer cycle. To avoid this issue, we modify the cycle graph to diminish the value of higher degree moments. Inparticular, we force all high moments close to zero by creating a graph that consists of many disjointcycles, either of length 1/ or 2/, joined by a lightweight complete graph on all nodes. If weightedcorrectly, then any walk of length (1/) will exit the cycle it starts in (via the complete graph) withhigh probability, and the chance of returning to its starting point can be made extremely low bymaking the graph large enough. At the same time, the lower moments are not effected significantly,so we can show that the graphs remain far in Wasserstein-1 distance. Theorem 1.2 has potentially interesting implications beyond showing a limitation for graph spectrumestimation. For example, related to the discussion about generalized moment methods above, itimmediately implies that for any , the th Chebyshev polynomial cannot be approximated toaccuracy 1/poly() with a polynomial (of any degree!) whose maximum coefficient is 2. If itcould, we could use less than exponentially accurate measures of the raw moments to approximatethe Chebyshev moments, and then use these moments to approximate the spectral density, followingBraverman et al. [BKM22]. However, by Theorem 1.2, this is impossible. While Theorem 1.2 rules out direct improvements to the moment-based method of Cohen-Steineret al. [CKSV18], it does not rule out the possibility of some other algorithm that can estimatethe spectral density to accuracy using fewer random walk steps. For example, we could considermethods that use more information about each random walk than checking whether or not thelast step returns to the starting node. However, our next theorem shows that, in fact, no suchalgorithm can beat the exponential dependence on 1/; we show that, information theoretically,2(1/) samples from random walks started from random nodes are necessary to estimate the spectraldensity accurately in Wasserstein-1 distance. 6When run for O(1/2) steps, the random walk method of Cohen-Steiner et al. [CKSV18] actually achieves aweaker moment approximation with additive rror . This is always greater than m(p1) because all of p1s momentsare upper bounded by 1 since it is supported on . Theorem 1.3. For any < 1/2, no algorithm that is given access to the transcript of m, length Trandom walks initiated at m uniformly random nodes in a given graph G can approximate Gs spectraldensity to accuracy in the Wasserstein-1 distance with probability > 3/4, unless m T > 1",
  "21/4": "Proof. The theorem follows from Lemma 4.2 and Lemma 4.4. In particular, choose to equal thelargest odd integer smaller than 1/4 and choose n = 2 22. Then consider graphs G1 and G2generated as in Definition 4.1 with random node labels. By Lemma 4.2, W1(G1, G2) > 2. So, thereis no distribution p that is -close in Wasserstein distance to both the spectral density of G1 and G2.Accordingly, any algorithm that estimates the SDE of a graph to error with probability 3/4 canbe used to correctly distinguish samples from DG1 and DG2 with probability 3/4. However, with nset as above, we have that dTV(DG1, DG2) 2m2T 2",
  "Open Problems and Outlook": "Our main results open a number of interesting directions for future inquiry. Most directly, thebound from Theorem 1.3 is based on an instance involving weighted graphs. It would be great toextend the lower bound to unweighted graphs, which are common in practice. While we believethe same lower bound should hold, such an extension is surprisingly tricky: for example, replacingthe lightweight complete graph in our hard instances with, e.g., an unweighted expander graphsignificantly impacts the spectra of both graphs, making them more challenging to analyze. A bigger open question is to extend our lower bounds to what we call the adaptive random walkmodel, which means that the algorithm is allowed to start a random walk either at a random node,or at any other node it wishes. Since this model allows for e.g. sampling random neighbors ofany node, it is closely related to other access models. For example, up to logarithmic factors, thenumber of random walk steps required in the adaptive model is equal to the number of memoryaccesses needed when given access to data structure storing an array of neighbors for each node inthe graph [BKM22]. Currently, the best lower bound we can prove in the adaptive random walkmodel is that just (1/2) steps are necessary; we show this result in Appendix A. Proving a lowerbound exponential in 1/ or finding a faster algorithm that runs in this model would be a nicecontribution. Even a conjectured hard instance would be nice currently we dont have any. Finally, we note that our graph-based lower bounds show that, with non-adaptive random walks,it is impossible to distinguish if the spectral densities of two graphs are identical or -far away inWasserstein-1 distance with 2o(1/) steps. Consequently this result constitutes a particular type ofhardness for comparing graphs. However, one might consider other notions of graph comparison.For example, in Appendix C, we consider estimating the spectrum of the difference A1 A2 betweentwo normalized adjaceny matrices A1 graphs A2 corresponding to graphs G1 and G2 with the samenode degress. We show that an 2O(1/) upper bound is obtainable. Seeking matching upper andlower bounds for this and related problems is another interesting direction for future work.",
  "Paper Organization": "In we introduce notation and preliminaries. In we prove a lower bound forspectrum estimation based on moments, establishing Theorem 1.2. In we prove lowerbound for spectrum estimation based on random walks, establishing Theorem 1.3. In Appendix A,we give an (1/2) lower bound for approximating graph spectra in the (stronger) adaptive randomwalk model. In Appendix B, we use cycle graphs to construct distributions that are 2/ far inWasserstein-1 distance and have the same first 1 moments, slightly strengthening a resultfrom Kong and Valiant [KV17]. In Appendix C, we show a new algorithm that uses alternatingrandom walks to estimate the spectrum of the difference of two normalized adjacency matrices.",
  "Preliminaries": "General notation. We use : R R to denote the indicator function with (0) def= 1 and (x) def= 0for all x = 0. We use 1 Rn to denote the all ones vector when n is clear from context. We useP[E] to denote the probability of an event E. We let Ec denote the complement of a random eventE, so P[Ec] = 1 P[E]. Graphs and graph spectra.We consider undirected graphs G = (V, E) where each edge e Ehas a non-negative weight we R0. We call G unweighted when we = 1 for all e E. We useA RV V0to denote the weighted adjacency matrix of G where A(v, v) = we if e = (v, v) E andA(v, v) = 0 otherwise. We use D RV V0to denote the diagonal degree matrix of G where D is",
  "adjacency matrix of G, i.e. A(G) def= D1/2 AD1/2. We refer to D1 A as the random walk matrixand note that, for degree-regular graphs, A(G) = D1 A": "For an n-vertex graph G, we let 1 1 2 n 1 be the eigenvalues of the normalizedadjacency matrix A(G), and use = (G) to denote this sorted (in ascending order) eigenvaluelist. We let p(x) : denote the spectral density of G, i.e., p(x) = 1 ni[n] (x i),which is the density of the distribution on induced by i (for brevity, we do not distinguishbetween spectral density and the distribution it induces). We use mj(p) to denote the jth momentof p, i.e., mj(p) = 1",
  "Wasserstein distance.In this work, we consider the standard Wasserstein-1 distance betweendistributions, which we may simply refer to as the Wasserstein distance for brevity": "Definition 2.1. The Wasserstein-1 distance W1(p1, p2) between two distributions, p1 and p2,supported on the real line is defined as the minimum cost of moving probability mass in p1 to p2,where the cost of moving probability mass from value a to b is |a b|. Concretely, let be the set ofall couplings (x, y) between p1 and p2, i.e., contains all joint distributions (x, y) over x Rand y R with marginals equal to p1 and p2. Then:",
  "Rf(x) (p1(x) p2(x))dx.(2)": "Above, the supremum is taken over all 1-Lipschitz functions f, i.e., that satify |f(a) f(b)| |a b|for all a, b R. Overloading notation, for graphs G1 and G2 with spectral densities p1 and p2respectively, we let W1(G1, G2) def= W1(p1, p2) to denote the Wasserstein-1 distance between p1 andp2. We note that, for any two n-vertex graphs G1 and G2, it can be checked (see, e.g. [KV17]) that:",
  "n (G1) (G2)1 .(3)": "Access models.As discussed in the introduction, we consider several possible data accessmodels for estimating the spectral density of a normalized graph adjacency matrix, A(G) forG = (V, E, w). First, we consider algorithms that, for some integer j 0 and accuracy parameter ,have access to -accurate approximations, m1, . . . , mj, to the first j moments of Gs spectral densityp, m1(p), . . . , mj(p). Specifically, we have that | mj mj(p)| mj(p). A natural generalization of the setting where approximate moments are available is to consideralgorithms that access G via random walks, since repeated random walks can be used to approximatemoments [CKSV18]. In this work, we primarily consider a non-adaptive random walk model, wherethe algorithm can run m random walks each of length T 1, starting at m vertices v(1)0 , . . . , v(m)0chosen uniformly at random from G. For each walk, the algorithm can observe the entire sequenceof vertex labels visited in order We call this information the the walk transcript and denote theset of transcripts by S = {S1, , Sm}. Note that, at vertex v, the probability that the next vertexin the random walk is equal to v is the (v, v) entry of D1 A. In Appendix A, we also consider the richer random walk model that we refer to as the adaptiverandom walk model where the algorithm can choose the starting node v(1)0 , . . . , v(m)0. This is incontrast to the non-adaptive random walk model where starting nodes are uniformly random. Cycle spectra.Our lower bound instances in this paper involve collections of cycle graphs. Welet Rc denote an undirected cycle graph of length c, and we let Rkc denote a collection of k suchcycles. Recall that we use A(Rkc) to denote the normalized adjacency matrix and (Rkc) for a sortedlist of eigenvalues for the normalized adjacency matrix. We leverage the following basic lemma onthe spectrum of cycle graphs.",
  "= 1 cos(/) + cos(/) cos(2/) + + cos(( 1)/) (1) = 2": "Remark 2.4. The first j < moments of the spectral density of R2 and R2 are the same. This istrue because the number of ways a walk of length j < can return to its starting node is the same inboth R2 and R2: 2 jj/2 for even j and 0 for odd j.",
  "Limits on Moment Estimation Methods": "In this section, we construct two weighted graphs G1, G2 with a same number of vertices, i.e.,|V1| = |V2|, that we prove are -far in Wasserstein distance but have exponentially close moments.We detail the construction in the definition below. Definition 3.1. G1 is constructed by starting with a collection of 2n isolated vertices and 2ndisjoint cycles, each of size . G2 is constructed by starting with a collection 2n isolated verticesand n disjoint cycles, each of size 2. In both graphs, the edges in the cycle have weight 1/4 and",
  "2n + 2": ": Diagram depicting graphs G1 and G2 from Definition 3.1. G1 contains 2n cycles of length and 2n isolated vertices, and G2 contains n cycles of length 2 and 2n isolated vertices. Inboth graphs, the purple edges have weight 1/4 + 1/(4n), the grey edges connect all the verticesnot connected by purple edges (including self-loops), with weight 1/(4n), and the green edges areself-loops of weight 1. every vertex in a cycle is then connected to all other cycle vertices with weight 1/(4n) (including aself-loop); the isolated vertices only have self-loop with weight 1. We choose to be an odd numberand let n = 2/4. Note that each graph has 4n vertices.",
  ": Spectral Density of G1 and G2 as defined in Definition 3.1, with cycles of length 11 and22, respectively": "are disjoint cycles, we only need to focus on the Wasserstein distance between a cycle of size 2 and 2disjoint cycles of size . Applying Lemma 2.3, we get (R1) (R2)1 = n 2 W1(R2, R2) = 2n.Plugging this back we get the claimed Wasserstein distance W1(G1, G2) = 1/(4).",
  ".(5)": "Since R2nand Rn2 are disjoint cycles, the moments of the spectral density of R2nand R2nare thesame as the moments of the spectral density of R2 and R2. This is true because the eigenvalues ofthe disjoint copies of A(R2n ) and A(R2n ) are the same as the eigenvalues of the disjoint copies ofA(R2) and A(R2) with increased multiplicity, which is scaled by the size of the respective graphs.Since the first j < moments of R2 and R2 are the same (see Remark 2.4), we get from (5) that",
  "= 0": "We briefly remark that the proof of Lemma 3.3 required picking a value of n that is exponentiallylarge in to ensure that when a random walk leaves the cycle it started from, it only comes back tothe same cycle with a very low probability. Otherwise, we would not have been able to show thatthe higher moments of G1 and G2 (j ) are close.",
  "Limits on Random Walk Methods": "In this section, we prove Theorem 1.3, which can be viewed as a strengthening of Theorem 1.2.While Theorem 1.2 rules out directly improving SDE algorithms like that of [CKSV18] based onestimating moments, Theorem 1.3 shows that no method that performs less than 2O(1/) stepsof non-adaptive random walks in a graph can reliably estimate the spectral density to error inWasserstein distance, whether or not the algorithm is based on moment estimation or not. To prove Theorem 1.3, we construct a hard pair of graphs that are far in Wasserstein distance,but difficult to distinguish based on random walks. This pair is identical to the hard instanceconstructed in the previous section, although without isolated nodes. These nodes were necessaryto show that even accurate relative error moment estimates do not suffice for spectral densityestimation. However, they are not needed for the random-walk lower bound, and eliminating themsimplifies the analysis. Formally, the construction is as follows: Definition 4.1. G1 is constructed by starting with a collection of 2n disjoint cycles, each havingsize for odd integer . The edges in the cycle have weight 1/4. After constructing the cycles, weconnect every vertex in G1 to all other vertices with weight 1/(4n) (including a self-loop). G2 isconstructed by starting with a collection of n disjoint cycles, each having size 2. The edges in thecycle have weight 1/4 and every vertex is then connected to all other vertices with weight 1/(4n)(including a self-loop). We choose n = 2 22.",
  "We next show that the transcripts of randomly started, non-adaptive random walks generated onG1 and G2 have similar distributions": "Definition 4.3. For m non-adaptive random walks, each with length T, a random walk transcriptS is a collection of m individual walk, S = {S1, . . . , Sm} where each Si consists of a list of T nodelabels vi,1, . . . vi,T (the nodes visited in the walk). Let DG1 and DG2 denote probability distributionover random walk transcripts generated when walking in G1 and G2, respectively, with nodes labeledusing a uniform random permutation of the integers 1, . . . , 2n.",
  "Proof of Lemma 4.4. We define a coupling D by describing a process that explicitly generates tworandom walk transcripts S1 and S2 which are distributed according to DG1 and DG1. To do so, we": "use a lazy labelling procedure that randomly labels nodes as they are visited in the random walks.To support that labeling, we define two dictionaries, L1 : V1 1, . . . , 2n and L2 : V2 1, . . . , 2nthat maps the vertex sets of G1 and G2 (denoted as V1 and V2) to labels. Initially, Li(v) returnsNULL for any vertex v Vi. However, if we set Li(v) j for a label j, then for all future calls tothe dictionary, Li(v) returns j. Additionally, in our description of the coupling we will refer to thecycle that a node v lies in (in G1 or G2) and to vs left neighbor and right neighbor. Referringto Definition 4.1, these terms refer to the cycle that v would be in if the lightweight copy of thecomplete graph had not been added to to the graph, and respectively to vs neighbors in that cycle.",
  "sampled from DG2. So, we are left to argue that, with high probability, S1 = S2": "To do so, we use the fact that the transcripts are identical if two events hold. To define these events,note that each walk in each transcript begins in a cycle in G1 or G2, and then takes a randomnumber of steps left and right in that cycle until resetting with probability 1/2 to a uniformlyrandom node in the graph (which could bring the walk to a new cycle, the same cycle it is currentlyin, or a cycle visited previously). For transcript S1, let R11, . . . , R1q denote the list of cycles visitedbetween each RESET step across all m walks in that transcript. Likewise, let R21, . . . , R2q denotethe set of cycles visited in S2. S1 and S2 are always identical if the following events occur:",
  "Event 2: For all j 1, . . . , s, we take fewer than left/right steps in R1j and R2j before a RESET": "To see why this is the case, note that, if Event 1 occurs, the only way that S1 and S2 would differis if, while random walking in R1j and R2j, we move to nodes v1 and v2 where L1(v1) is defined butL2(v2) is NULL, or vice-versa. However, the only way this can happen is if we complete an entireloop around R1j, and thus return to a node that was previously labeled. Since each R1j has nodes,such a loop cannot be completed if we always take < steps before resetting to a new cycle. We proceed to show that Event 1 and Event 2 both occur with high probability. First, consider Event1. We take at most mT RESET steps across all m random walks. At each step, the probabilitywe return to a cycle we had previously visited is at most mT",
  "n.(6)": "Next, consider Event 2. Note that, since we take a left/right step with probability 1/2 (and RESETwith probability 1/2) the chance that we take steps or more in a given cycle is equal to (1/2).We visit at most q = mT cycles, so by a union bound, we take less then left/right steps in eachcycle with probability:",
  "With Lemma 4.4 in place, we can prove our main lower bound result for non-adaptive random walks:": "Theorem 1.3. For any < 1/2, no algorithm that is given access to the transcript of m, length Trandom walks initiated at m uniformly random nodes in a given graph G can approximate Gs spectraldensity to accuracy in the Wasserstein-1 distance with probability > 3/4, unless m T > 1",
  "[AI19]Myron B. Allen and Eli L. Isaacson. Chebyshev Polynomials. In: Numerical Analysisfor Applied Science. John Wiley & Sons, Ltd, 2019, pp. 555557. doi: 10.1002/9781119245476.app3": "[BGKS20]Jess Banks, Jorge Garza-Vargas, Archit Kulkarni, and Nikhil Srivastava. Pseu-dospectral Shattering, the Sign Function, and Diagonalization in Nearly MatrixMultiplication Time. In: 2020 IEEE 61st Annual Symposium on Foundations ofComputer Science (FOCS). 2020, pp. 529540. arXiv: 1912.08805. [BKM22]Vladimir Braverman, Aditya Krishnan, and Christopher Musco. Sublinear TimeSpectral Density Estimation. In: Proceedings of the 54th Annual ACM Symposiumon Theory of Computing (STOC). 2022, pp. 11441157. arXiv: 2104.03461. [CTU21]Tyler Chen, Thomas Trogdon, and Shashanka Ubaru. Analysis of stochastic Lanczosquadrature for spectrum approximation. In: Proceedings of the 38th InternationalConference on Machine Learning (ICML). 2021. arXiv: 2105.06595. [CKSV18]David Cohen-Steiner, Weihao Kong, Christian Sohler, and Gregory Valiant. Ap-proximating the Spectrum of a Graph. In: Proceedings of the 25th ACM SIGKDDInternational Conference on Knowledge Discovery and Data Mining (KDD). 2018,pp. 12631271. arXiv: 1712.01725. [DBB19]Kun Dong, Austin R Benson, and David Bindel. Network density of states. In:Proceedings of the 25th ACM SIGKDD International Conference on KnowledgeDiscovery and Data Mining (KDD). 2019, pp. 11521161. arXiv: 1905.09758. [GKX19]Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An Investigation into NeuralNet Optimization via Hessian Eigenvalue Density. In: Proceedings of the 36th In-ternational Conference on Machine Learning (ICML). Vol. 97. 2019, pp. 22322241.arXiv: 1901.10159. [Hut90]Michael F. Hutchinson. A stochastic estimator of the trace of the influence matrixfor Laplacian smoothing splines. In: Communications in Statistics-Simulation andComputation 19.2 (1990), pp. 433450. doi: 10.1080/03610919008812866.",
  "[Kan40]Leonid V Kantorovich. On an effective method of solving certain classes of extremalproblems. In: Dokl. Akad. Nauk., USSR 28 (1940), pp. 212215": "[Kan42]Leonid V Kantorovich. On the translocation of masses. In: Dokl. Akad. Nauk.,USSR 37 (1942). English translation in J. Math. Sci. 133, 4 (2006), 13811382.,pp. 199201. doi: 10.1007/s10958-006-0049-2. [KLS11]Liran Katzir, Edo Liberty, and Oren Somekh. Estimating Sizes of Social Networksvia Biased Sampling. In: Proceedings of the 20th International World Wide WebConference (WWW). 2011, pp. 597606. doi: 10.1145/1963405.1963489.",
  "[KV17]Weihao Kong and Gregory Valiant. Spectrum estimation from samples. In: TheAnnals of Statistics 45.5 (2017), pp. 22182247. arXiv: 1602.00061": "[MM19]Michael Mahoney and Charles Martin. Traditional and Heavy Tailed Self Regulariza-tion in Neural Network Models. In: Proceedings of the 36th International Conferenceon Machine Learning (ICML). Vol. 97. 2019, pp. 42844293. arXiv: 1901.08276. [MMMW21]Raphael A. Meyer, Cameron Musco, Christopher Musco, and David Woodruff.Hutch++: Optimal Stochastic Trace Estimation. In: Proceedings of the 4th Sympo-sium on Simplicity in Algorithms (SOSA) (2021). arXiv: 2010.09649.",
  "ALower Bound for the Adaptive Random Walk Model": "In this section, we consider lower bounds against a possibly richer class of spectral density estimationalgorithms that can access graphs via adaptive random walks. Specifically, the algorithm is allowedto start random walks (of any length) at any node of its choosing and can store the entire transcriptof these walks. In the adaptive model, the algorithm also has the ability to uniformly sample nodesfrom the graph, as in the non-adaptive random walk model considered for Theorem 1.3. Interestingly, an adaptive algorithm can solve the hard instance from Theorem 1.3 using roughlyO(log(1/)/) random walk steps. Specifically, for any node, the algorithm can identify its adjacentcycle nodes with high probability by taking a logarithmic number of 1-step random walks andidentifying the two nodes that are visited most frequently. This allows it to walk one way aroundthe cycle, check its length, and thus distinguish between G1 and G2. Proving a lower bound in the adaptive random walk setting appears to be much harder than thenon-adaptive setting, and we do not have any proposed constructions that we conjecture couldestablish that 2O(1/) random walk steps are necessary. However, in this section we give a simpleargument for a lower bound of (1/2) steps. The lower bound is via a reduction to a naturalsampling problem, introduced below. Problem A.1. For a parameter (1/2, 1) and integer n, suppose we have a jar that containseither n red marbles and (1 ) n blue marbles (Case 1) or contains (1 ) n red marbles and n blue marbles (Case 2). Our goal is to determine if we are in Case 1 or 2 given a sample of s",
  "Lemma A.2. Let (0, 1/2), let = (1 + )/2, and let n = 2/4. There is no algorithm thatsolves Problem A.1 with probability > 3/4 unless s > 1/(42)": "Proof. Suppose we draw s marbles from the jar and encode the result in a length s vector (e.g., witha 0 at position i if the ith marble drawn is red, and a 1 if it is blue). Let X(s)1denote the distributionover vectors observed in Case 1, and let X(s)2denote the distribution for Case 2. We will show thatdTV(X(s)1 , X(s)2 ) is small. To do so, we introduce two auxiliary distributions: let X(s)1denote thedistribution over vectors observed if we are in Case 1 and draw marbles randomly with replacementand let X(s)2denote the distribution if we are in Case 2 and draw marbles with replacement. We first show that dTV(X(s)i, X(s)i) is small for i {1, 2} when n is large. To do so, let E be theevent that in s independent draws with replacement, we never pick a previously picked marble. Let[ X(s)i]E denote the distribution X(s)iconditioned on E, and note that [ X(s)i]E = X(s)i. The probabilitythat E happens is equal to 1 (1 1",
  "dTV( X(m)i, X(m)i) s2/n.(8)": "Next, we show that dTV( X(s)1 , X(s)2 ) is small. Doing so is equivalent to bounding the total variationdistance between s independent draws from a Bernoulli distribution with mean 1 and s inde-pendent draws from Bernoulli distribution with mean . Let DKL(p, q) denote the KullbackLeiblerdivergence between distributions p and q. Applying Pinskers inequality, we have:",
  "For any s 1/(42) and n = 2/4 we conclude that dTV(X(m)1, X(m)2) < 1/2. Accordingly, noalgorithm can distinguish between X(s)1and X(s)2with probability 3/4 unless s > 1/(42)": "With Lemma A.2 in place, we are now ready to prove our lower bound for spectral density estimation.To do so, we will show that any adaptive random walk algorithm that can estimate the spectraldensity of a graph to accuracy using s total random walk steps can solve the Problem A.1 using s samples. This reduction requires introducing a second pair of hard graphs that are close inWasserstein distance. In comparison to the hard instance in Theorem 1.3, these graphs are alsobased on collection of cycles. The main difference is that we consider two graphs that each containa mixture of cycles of length 2 and , but in different proportions. Definition A.3. For odd integer and parameter (0.5, 1), let G1 be a collection of n disjointcycles of length 2 and 2(1 )n cycles of size . Similarly, let G2 be a collection of (1 )n cyclesof length 2 and 2n cycles of size . Both graphs have 2n vertices in total.",
  "We now have all the ingredients in place to prove the main result of this section:": "Theorem A.5. For any < 1/6, no algorithm that takes s adaptive random walks steps in agiven graph G can approximate Gs spectral density to accuracy in the Wasserstein-1 distance withprobability > 3/4, unless s 1/(362). Proof. Suppose we had such an algorithm (call it A) that uses s < 1/(42) random walk steps tooutput an /3-accurate spectral density with probability greater than 3/4. We will show that thealgorithm could be used to solve Problem A.1 using < 1/(42) samples from the jar with probabilitygreater than 3/4, which is impossible by Lemma A.2. To prove this reduction we associated an instance of Problem A.1 with a hidden graph G that iseither isomorphic to G1 or G2 as defined in Definition A.3. To make the association, every marblewill correspond to 2 vertices in the graph with some fixed set of known labels. However, theconnections between those nodes is hidden. In particular, if the marble is red, the 2 vertices arearranged in a single cycle of length 2. Otherwise, they are arranged in two cycles of length . Theordering of nodes in both cases is known in advance, but we do not know which of the two cases weare in. Also note that there are no other connections between vertices. Observe that if we are inCase 1 for Problem A.1, G is isomorphic to G1 and if we are in Case 2, G is isomorphic to G2. Soin particular, Gs spectral density is either equal to the spectral density of G1 or G2. Our main claim is that we can run algorithm A on the hidden graph G while only accessing smarbles from the jar. To so do, every time the algorithm requests to visit a specific node, we drawthe marble from the jar associated with that nodes label. In doing so, we learned all edges in thering containing that node (as well as other edges), so we can perform any future random walk stepsinitiated from that node. Since A takes s steps, we at most need to draw s marbles over the courseof running the algorithm. At the same time, note that when we choose = 1 (considering self loops)and = (1 + )/2 as in Lemma A.2, Lemma A.4 implies that the Wasserstein distance between G1and G2 is equal to . So, if A returns an /3-accurate spectral density with probability 3/4, we can",
  "BWasserstein Distance Bounds via Chebyshev Polynomials": "In this section, we give an alternative proof of a lower-bound by Kong and Valiant [KV17], whichshows that there exist distributions whose first 1 moments match exactly, but the Wassersteindistance between the distributions is greater than 1/(2( + 1)). Our analysis tightens their result bya factor of 4, showing two such distributions with Wasserstein distance 2/. Moreover, we provethat the Wasserstein distance is (1) for any distributions p, q whose first 1 moments are thesame and whose -th moments differ by (2). Lemma B.1 (Improvement of [KV17, Proposition 2]). For any odd , there exists a pair ofdistributions p, q, each consisting of (+1)/2 point masses, supported within the unit interval such that p and q have identical first 1 moments, and the Wasserstein distance W1(p, q) 2/. Proof. Recall that we use R2 to denote 2 disjoint cycles of length , and use R2 to denote a cycleof length 2, where is an odd number. We know the spectrum of R2 and R2 from Lemma 2.3.Let p and q denote the spectral density of A(R2) and A(R2). We first note that the first 1moments of the spectral density of p and q are the same because a random walk of length 1cannot distinguish R2 from R2 (see Remark 2.4). Also, recall we use (R2) to denote the sorted eigenvalue list of A(R2) and (R2) to denote thesorted eigenvalue list of A(R2). We make the following observations about the spectrum of A(R2)and A(R2) based on Lemma 2.3.",
  ". All the eigenvalues of A(R2) lies in": "Let (2) denote the sorted list of eigenvalues where we remove all the eigenvalues from (R2) thatoccurs in (R2). Let (1) be the set of removed eigenvalues. The following observations followfrom (10). The size of (2), and (1) is . Moreover (1) has the same eigenvalues as (R2) wherethe frequency of each unique eigenvalue is (R2) is reduced by a factor of 2. Consequently, wedefine p(x) = 1",
  ", where the penultimate equalityfollows from the dual characterization of Wasserstein distance in (2) and the last equality followsfrom Lemma 2.3": "We complement Proposition B.1 with the following Lemma B.3, which shows that for two distributionsp and q such that all their first 1 moments are the same and the -th moment differ only by(2), even then the Wasserstein distance between p, q is large. The proof follows just by usingthe fact that there are 1-Lipschitz polynomials with large leading coefficient. We note the followingstandard facts about the Chebyshev polynomials which can, for example, be found in [AI19].",
  "CAnother Spectral Metric for Graph Comparison": "Throughout this section we consider two graphs G1, G2 with the same vertex size n and same vertexlabeling V = [n], and their un-normalized adjacency matrix A1 and A2 with a common degreematrix D. Here we consider learning the spectrum of their difference matrix, i.e., A(G1) A(G2) =D1/2 A1D1/2 D1/2 A2D1/2, or equivalently D1( A1 A2). We provide a simple proof thatexp(O(1/)) number of samples also suffice to estimate this distribution up to -Wasserstein distance,using similar techniques as in Cohen-Steiner et al. [CKSV18].",
  "k+ 3k(b a) 2) for some absolute constant C": "We define a variant of the non-adaptive random walk access model, represented via an oracleONARW(G1, G2, j, {xi}i[j]), specifically for this problem, which outputs the random trajectoryafter taking a length j random walk starting from a uniformly randomly chosen vertex, where atstep i [j] it the follows probabilistic transition of D1 A1 when xi = 1 and D1 A2 when xi = 0.We consider Algorithm 1 for estimating the spectral density of matrix D1( A1 A2). Algorithm 1 computes estimates of the moments of difference matrix D1( A1 A2). Together withthe procedure of computing a distribution based on first k moments using linear programming asstated in Cohen-Steiner et al. [CKSV18], we have the following guarantee. Theorem C.2. Given any two graphs G1, G2 on same set of vertices with a common degreematrix D, Algorithm 1 with k = 4C/ and = /(32k+2) outputs a distribution p that is -closein Wasserstein-1 distance with the spectral density function of A(G1) A(G2) with probability 0.9,using a total of 2O(1/) calls to ONARW(G1, G2, j, ), j [O(1/)]."
}