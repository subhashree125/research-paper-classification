{
  "ABSTRACT": "Graph Neural Networks (GNNs) have shown promising results inmodeling graphs in various tasks. The training of GNNs, especiallyon specialized tasks such as bioinformatics, demands extensive ex-pert annotations, which are expensive and usually contain sensitiveinformation of data providers. The trained GNN models are oftenshared for deployment in the real world. As neural networks canmemorize the training samples, the model parameters of GNNshave a high risk of leaking private training data. Our theoreticalanalysis shows the strong connections between trained GNN pa-rameters and the training graphs used, confirming the traininggraph leakage issue. However, explorations into training data leak-age from trained GNNs are rather limited. Therefore, we investigatea novel problem of stealing graphs from trained GNNs. To obtainhigh-quality graphs that resemble the target training set, a graphdiffusion model with diffusion noise optimization is deployed asa graph generator. Furthermore, we propose a selection methodthat effectively leverages GNN model parameters to identify train-ing graphs from samples generated by the graph diffusion model.Extensive experiments on real-world datasets demonstrate the ef-fectiveness of the proposed framework in stealing training graphsfrom the trained GNN.",
  "INTRODUCTION": "Graph-structured data pervade numerous real-world applicationssuch as social networks , finance systems , and moleculargraphs . Graph Neural Networks (GNNs) have shown promis-ing results in modeling graphs by adopting a message passing Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 25, August 37, 2025, Toronto, Canada 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-XXXX-X/18/06",
  ": Illustration of GNNs training and releasing": "scheme , which updates a nodes representation by ag-gregating information from its neighbors. The learned representa-tion can preserve both node attributes and local graph structuralinformation, facilitating various tasks, such as node classification []and graph classifications .As shown in Fig 1, generally, abundant training data is requiredto train a high-performance GNN model. This becomes particularlyexpensive for critical applications such as molecule property pre-diction, which demands expert annotations. Moreover, the trainingdata may hold sensitive information belonging to its providers.Consequently, protecting the privacy of the training data is imper-ative. After training on the private training data, the trained GNNmodel is often released for downstream applications. For example, awell-trained molecule property predictor may be made open source,supporting the direct deployment for the designed task or modelinitialization for other tasks. However, neural networks can mem-orize the training data, even when they have great generalizationability . It is also demonstrated that the trained parameters ofMLP are linear combinations of the derivatives of the network ata set of training data points , causing potential private dataleakage. Our theoretical analysis in Theorem 4.1 further shows thatthe above observations can be extended to GNNs. Hence, releasingGNN models potentially threatens the privacy of the private train-ing data. Therefore, in this paper, we investigate a novel problem ofstealing training graphs from trained GNN models when the modelarchitecture and parameters are known/released.Several initial efforts are made in model inversion attacks, whichaim to reconstruct graph topologies or infer the sensitive nodeattributes attributes . However, they are proposed to reconstructthe train graph/node from node/graph embeddings, which cannotbe applied for training graph stealing as embeddings of privatetraining data are unavailable for attackers. Recently, GraphMI proposes to reconstruct the adjacency matrix of the training graphin a white-box setting where the trained model parameters areavailable. However, GraphMI requires all the node attributes of thetraining graph to be available for topology reconstruction. Thisassumption is impractical in real-world applications. Therefore, inthis paper, we propose to steal the training graphs from the trained",
  "KDD 25, August 37, 2025, Toronto, CanadaMinhua Lin, Enyan Dai, Junjie Xu, Jinyuan Jia, Xiang Zhang, and Suhang Wang": "GNN model without any information of the training data, which isnot explored by existing works.However, it is non-trivial to perform the graph stealing attack.There are two major challenges to be addressed. First, it is challeng-ing to ensure both the realism and quality of graphs reconstructedfrom the trained GNN model. Existing model inversion attacks onGNNs mainly focus at the node level, concentrating on reconstruct-ing either links or node attributes of the targetgraphs. However, this focus is inadequate for graph stealing at-tacks because graph-structured data comprises both graph topologyand node attributes, rendering these methods insufficient for com-prehensive graph-level reconstruction. Our empirical analysis inSec. 5.2.1 further demonstrates that current methods encounter dif-ficulties in providing realistic and high-quality graphs during graph-level reconstruction. Second, how to effectively utilize the trainedGNN model parameters to extract the training graph information?Previous works on model inversion attacks on GNNs typically rely on comprehensive side information of target samplesfor graph reconstruction. The investigations on extracting trainingdata information from trained GNN parameters without any partialtraining data information poses a new challenge.In an effort to address the aforementioned challenges, we proposea novel Graph Stealing attack framework (GraphSteal). Specifically,to overcome the challenge of reconstructing high-quality graphs,we employ a graph diffusion model as the graph generator. Inaddition, this graph diffusion model used in GraphSteal adopts adiffusion noise optimization algorithm, which can produce a setof candidate graphs that more closely resemble the training set ofthe target GNN. Moreover, according to our theoretical analysis inTheorem 4.1, there is a strong connection between the GNN modelparameters and training data. Based on the theorem, we propose amodel parameter-guided graph selection method, which leveragesthe parameters of GNNs to identify the training graphs from thecandidate graph set generated by the graph diffusion model. Insummary, our main contributions are:",
  "RELATED WORK": "Graph Neural Networks. Graph Neural Networks (GNNs) have shown great power in modeling graph-structured data, which have been deployed to various applica-tions such as social network analysis , drug discov-ery and energy network analysis . The suc-cess of GNNs lies in the message-passing mechanism, which it-eratively aggregates a nodes neighborhood information to refinethe nodes representations. For example, GCN combines anodes neighborhood information by averaging their representa-tions with the target center nodes. To improve the expressivity ofGNNs, GIN further incorporates a hidden layer in combining the neighbors information. Inspired by the success of transform-ers in modeling image and text , graph transformer isalso proposed . Generally, graph transformer has aglobal attention mechanism for graph embedding learning. It showspromising results, especially in molecule property prediction. De-spite the great achievements, GNNs could be vulnerable to privacyattacks, which largely constrain their adoption in safety-criticaldomains such as bioinformatics and financial analysis. Privacy Attacks on GNNs. The training of GNNs requires a largeamount of data. In critical domains such as bioinformatics andhealthcare , sensitive data of users will be collected to train apowerful GNN to facilitate the services. However, recent studiesshow that privacy attacks can extract various private informationfrom GNN models , threatening the privacy ofusers. For example, membership inference attacks can identify whether a target sample is in the training set. This isachieved by learning a binary classifier on patterns such as poste-riors that training and test samples exhibit different distributions.Model extract attacks aim to steal the target model by buildinga model that behaves similarly to the target model. Model inver-sion attacks, also known as reconstruction attacks, try to infer theprivate information of the test/training data. For example, Dudduet al. propose to infer sensitive attributes of users from theirnode embeddings. The reconstruction of the adjacency matrix fromnode embeddings is studied in . GraphMI further considersa white-box setting for adjacency matrix reconstruction, wherethe target GNN model parameters and node features are availablefor attackers. Our GraphSteal lies in the model inversion attack.However, GraphSteal is inherently different from the aforemen-tioned methods because (i) we focus on a new problem of stealingtraining graphs from the trained GNN without even partial informa-tion of training set; (ii) we propose a novel framework GraphStealwhich designs a diffusion noise optimization algorithm and a modelparameter-guided graph selection mechanism to reconstruct high-quality training graphs of the target GNN model. Graph Diffusion Models. Diffusion models have showcased theirexceptional performance on various tasks, including image gen-eration , text-to-image generation and video gen-eration . Generally, a diffusion model has two phases: (i) thediffusion phase, where noise is incrementally added to the cleandata; and (ii) the denoising phase, where a model learns to predictand remove the noise. After trained, the denoise module can effec-tively generate realistic samples with noises as input. Recent workstry to extend diffusion models to graphs. For example, Vignac et al. proposes DiGress to bridge the gap between the discretenessof graph structure and the continuity of the diffusion and denois-ing process. Jo et al. uses a system of stochastic differentialequations to model the joint distribution of nodes and edges. Luoet al. conduct the graph diffusion process from the spectraldomain. EDM and GeoDiff aim to preserve the equivariantinformation of 3D graphs.",
  "Stealing Training Graphs from Graph Neural NetworksKDD 25, August 37, 2025, Toronto, Canada": "a generative model. To calculate this distance, firstly, we use theactivations of the penultimate layer of ChemNet to obtain therepresentation of each molecule. We then calculate the mean andcovariance of these activations for the two distributions. The twodistributions ((), ()) are compared using the Wasserstein-2distance . Formally, FCD (, ) is given by: 2((m, C), (m, C)) = ||m m||22 + Tr(C + C 2 (CC)1/2),(43)where (m, C) are the mean and covariance of Gaussian ()obtained from real-world, and (m, C) are the mean and covarianceof Gaussian () obtained from the generative model. Note thatFCD here is reported as 2(, ) analogously to . B.2.2How To Evaluate the Exactly Graph Matching? We utilizeSMILES strings of molecule graphs for graph matching. SMILES(Simplified Molecular Input Line Entry System) is a widely usednotation in chem-informatics for encoding the structure of chemicalmolecules using short ASCII strings, thereby facilitating efficientrepresentation and manipulation of chemical structures. The recon-structed graph is matched with the target graph if their SMILESstrings are identical . Specifically, let T = {1, . . . ,}and T = {1, . . . ,} denote the SMILES strings sets of recon-structed graphs D and training graphs D, respectively. Then,Reconstruction rate = |T T |/|T |, where |T T | denotes thenumber of the SMILES strings of reconstructed graphs exactlymatched with those of graphs in D. This method is efficient andreasonable for matching molecule graphs as SMILES strings covervarious aspects of molecular structure, encoding information aboutatoms, bonds, and connectivity within a molecule.",
  "Graph Diffusion Model": "Graph diffusion models have demonstrated a robust capability ingenerating realistic graphs. In this paper, we mainly employ Di-Gress , a popular discrete denoising graph diffusion model, asthe graph generator. The diffusion process of DiGress is a Markovprocess consisting of successive graphs edits (edge addition or dele-tion, node or edge category edit) that can occur independently oneach node or edge. As the nodes and edges are considered to belongto one of the given categories, the noises are modeled as transitionmatrices (1, . . . , ), where is the probability of jumpingfrom category to category . To invert this diffusion process, ittrains a graph transformer network to predict the clean graph froma noisy input. Specifically, DiGress diffuses each node and edgefeature separately by applying transition matrices. The diffusionprocess at -th step can treated as sampling node type and edgetype of G from the categorical distributions X1X and E1E,respectively, which can be written as",
  "Threat Model": "3.2.1Attackers Goal. Given a target GNN classifier trained ona private target graph dataset D = {G1, . . . , G| D |}, the goal ofthe adversary in graph stealing attack is to reconstruct graphs inD from the target model . 3.2.2Attackers Knowledge and Capability. We focus on the graphstealing attacks in the white-box setting. The information of thetarget model including model architecture and model parame-ters is available to the attacker. This is a reasonable setting inreal-world as model providers will often release the trained modelto customers for downstream applications. Moreover, we consider apractical setting that the attacker does not have access to any sensi-tive information of the target dataset D, including graph topologyA, node attributes X and graph label of G D as the privatedata could contain sensitive information or is intellectual propertyof the data owner. However, we assume that an auxiliary datasetD = {G,}| D |=1 , which shares a similar low dimensional mani-fold as that of D, is available to the attackers. This assumptionis reasonable because in realistic scenarios, e.g., drug design ,the attacker has access to many publicly available molecules andhas high motivation to steal private molecules that are easy tosynthesize and have high activity on specific targets.",
  "Problem Definition": "With the above notations and the description of graph stealingattack in Sec. 3.2, the objective of graph stealing attack is to re-construct graphs in the target dataset D that are used to trainthe target GNN model without any partial information of D. Thisproblem can be formally defined as: Problem 1 (Graph Stealing Attack). Given a target GNN classifier: : G y trained on a privately owned dataset D = {(G,)}|D |=1 ,where y {1, 2, . . . ,} is Gs label, and an auxiliary dataset Dthat share the same manifold characteristics as D with D D = ,we aim to extract a subset of private training data D D. Thearchitecture and model parameters of are accessible.",
  "METHODOLOGY": "In this section, we present the details of our GraphSteal, which aimsto reconstruct the training graphs from the target GNN model with-out even partial information of the target dataset. There are mainlytwo challenges to be addressed for achieving better reconstructionperformance: (i) how to ensure both the realism and quality ofgraphs reconstructed from the trained GNN model; and (ii) how toutilize the trained GNN model parameters to extract the traininggraph information. To address these challenges, we propose a novelframework GraphSteal, which is illustrated in . GraphSteal iscomposed of a graph generator G, a noise generator , a recon-structed graph selector and the target GNN model . Specifically,a graph diffusion model is adopted as the graph generator G togenerate realistic and high-quality graphs. The noise generator takes graphs from the auxiliary dataset D as inputs to learn inputnoises for the graph generator G, aiming to ensure G can gen-erate graphs D mimic to the graphs within the target dataset D.D is the training set of . Finally, we perform a reconstructionselection by leveraging parameters of the target classifier toselect the top- most representative graphs from D that closelyresemble the target dataset D. Next, we give the detailed designof the proposed framework.",
  "Reconstruction Generation": "In this subsection, we give the details of reconstruction generation.We first train a graph diffusion model as the graph generator toguarantee the quality of the reconstructions in Sec. 4.1.1. We thenpropose to devise input noises for the graph diffusion model bysolving an optimization problem in Sec. 4.1.2. Finally, we utilize theoptimized input noises to produce the reconstructions in Sec. 4.1.3. 4.1.1Building Graph Generator. To ensure that we can reconstructthe high-quality and realistic training graphs from trained GNNmodel, one promising direction is to utilize graph diffusion modelsthat have powerful generation capability to generate graphs. Graphdiffusion models are usually trained on a specific dataset to learn itsdistribution. The trained graph diffusion model can then generatediverse realistic samples that are representatives of the learned datadistribution. As the auxiliary dataset D share a similar distributionas the target dataset D, we first train a graph diffusion model Gon D to learn the underlying data distribution. After G is trained,it can generate high-quality graphs that are likely to follow thedistribution of D. We adopt DiGress introduced in Sec. 3.1 as thegraph diffusion model due to its effectiveness. 4.1.2Diffusion Noise Optimization. Though we can use G togenerate realistic graphs, there are two issues: (i) If we use randomnoise as input to G to generate graphs, the chance of hitting thetraining graph in D is very low as the noise input space is verylarge; and (ii) Though the auxiliary dataset D shares a similarlow dimensional manifold with D, they might have a slightlydistribution shift from each other. Hence, directly applying Gtrained on D to generate the reconstructed graphs may still notaccurately represent the graphs in D due to the distribution shift.To address this issue, inspired by , we propose to first findinput noises that are likely to results in graphs closely resemblingthose in D, which reformulates the problem of generating traininggraphs in D as noise optimization problem, i.e., how to design theinput noise G for the graph diffusion model G such that G cangenerate graphs closely resembling those in D?To solve the above problem, we propose a novel diffusion noisegenerator to learn the input noise to guide the generation of thegraph diffusion model to generate graphs better represent those inthe target dataset. Our intuition is that given a graph G with theground-truth label , if G has higher probability to be predicted tothe target class by the GNN model , G is more likely to be thetraining graph of . Moreover, as the auxiliary dataset is availableto the attackers, to improve the efficiency of learning input noise,we can then select graphs with higher prediction scores as the input noises to guide the graph diffusion model in generating graphs withsimilar characteristics to those in target dataset .Therefore, we first use a metric to measure the prediction scoreof the graphs with the target model for selecting graphs fromD as the input noises for the graph diffusion model G. Formally,given a graph G D, the metric score is defined as:",
  "(G,; ) = (G)(5)": "where denotes the ground-truth label of G. After getting themetric score of each graph in D, we select graphs with the top- highest score in each class. The set of the selected graphs aredenoted as D = {G1, . . . , G}, where = .To further ensure the similarity of selected graphs to the targetgraphs, thereby improving reconstruction performance, we treat theadjacency matrix A and the node attributes X of the graph G D as variables to be optimized. Specifically, they are optimized byminimizing the loss between the prediction (G) and as:",
  "where L is the training loss (e.g., cross-entropy loss) of . Theoptimized graphs are then denoted as D = {G1, . . . , G}": "4.1.3Generating Graphs. After generating optimized graphs basedon Sec. 4.1.2, one straightforward way of reconstruction is to usethese optimized graphs directly as the reconstructed ones. How-ever, such optimized graphs often fall short in terms of realism andvalidity when applied to real-world scenarios, rendering the recon-struction process ineffective. This shortfall is further evidenced bythe experimental results in Sec. 5.4, which show the unrealistic na-ture and invalidity of these optimized graphs. To address this issue,inspired by , we propose to apply SDEdit to generategraphs to enhance the generation quality and the resemblance ofthe generated graphs to those within the target dataset. The keyidea of SDEdit is to hijack the generative process of the graphdiffusion G by adding a suitable amount of noise to smooth out theundesirable details. This noise addition aims to blur out unwanteddetails while preserving the key structure of the input graphs. Thenoised graphs are then fed back into G to progressively elimi-nate the noise, yielding denoised outputs that are both realistic andclosely resemble the graphs in the target dataset D. Specifically,given an input noise G D optimized in Sec. 4.1.2, we first diffuseG for steps to obtain G using diffusing module of G as",
  "G(G) = (. . . ( (G ), 1) . . . , 0),(8)": "It bridges the gap between the input data distribution (which isusually close to the target data distribution) and the auxiliary datadistribution, ensuring that the generated graphs are realistic andclosely similar to the target graphs in D. The set of the generatedgraphs are denoted as D = {G1, . . . , G}.",
  "Reconstruction Selection": "With the above process in Sec. 4.1, we can get a collection of high-quality labeled graphs that follow a similar distribution with thetarget dataset D. However, it is possible that some generatedgraphs, while resembling the distribution of D, do not preciselymatch the graphs in D. Thus, to further refine the performanceof our graph stealing attack and enhance the accuracy of our re-constructions, we propose a novel model parameter-guided graphselection method to select the most representative samples amongthe generated graphs, prioritizing those that most closely approxi-mate the actual graphs in D. Our major intuition comes from thestrong correlation between the model parameters and the trainingdata throughout the training process of neural networks .Building upon this insight, we leverage the parameters of the tar-get GNN classifier to select the graphs that most closely resemblethe target training graphs of . Specifically, we first introduce theconnection between model parameters and the training data inSec. 4.2.1. Then, we present a novel approach based on the aboveconnection to select samples from D that most closely resembleto graphs in the target dataset D in Sec. 4.2.2.",
  "(G;) = (G;), > 0(9)": "Note that essentially any message-passing based GNNs (e.g.GCN and SGC ) with ReLU activations is homogeneousw.r.t the parameters if it does not have any skip-connections (e.g.,GraphSage ) or bias terms, except possibly for the first layer.More details of the proof and discussion is in Appendix A.2.We then present Lemma 4.1 below to show the connection be-tween model parameters and the training data of homogeneousneural networks. Essentially, the lemma shows that the processof training GNNs with gradient descent or gradient flow can beformulated as a constrained optimization problem related to marginmaximization, which is formally given as: Lemma 4.1 (). Let be a homogeneous ReLU neural networkwith model parameters . Let D = {(G,)}| D |=1be the classifica-tion training dataset of , where {1, . . . ,}. Assume is trainedby minimizing the cross entropy loss L = | D |=1 log(1 + ( ))over D using gradient flow, where () = (G) (G) and (G) denotes the-th entry of the logit score vector (G) beforeapplying the softmax normalization. Then, gradient flow convergesin direction towards a first-order stationary point of the followingmax-margin optimization problem:",
  "= 0if ( ) 1, |D |.(14)": "The proof is in Appendix A.3. Eq. (11) implies that the parameters are a linear combination of the derivatives of the target GNN at the training graphs G D. Especially, according to Eq. (14),it is possible that = 0 for some training graph G D, whichimplies that such G has no impact on the final model parametersof . Thus, to recover training dataset D from , it is naturalto identify graphs {G1, . . . , G} from D with > 0 to satisfyEq. (11), which means they contribute to . 4.2.2Optimizing Selection Mask. Based on Theorem 4.1, the se-lection of the reconstructed graphs can be reformulated as findingoptimal graph selection masks = {1, . . . , } for the set of gen-erated graphs D = {G1, . . . , G} in Sec. 4.1.3. Specifically, ourobjective is to optimize to select graphs from D to closely ap-proximate the model parameters for . The objective functionfor selection can be written as:",
  ",": "s.t. 0, {1, . . . , }(15)where and G D are known and {1, . . . , } is the set ofgraph selection mask to be optimized. can be view as the metricscore of selecting G as the final reconstructed graphs. Since thisis a constrained optimization problem, to fulfill the constraint inEq. (15), we introduce a constraint loss L(), which is defined as:",
  "Reconstruction Algorithm": "The reconstruction algorithm of GraphSteal is in Alg. 10. Specifi-cally, we train a graph diffusion model G as the graph generator(line 1), and select graphs from the auxiliary dataset D as the inputgraphs D (line 2). D is then optimized based on Eq. (6) (line 3) touse to generate D based on Eq. (8) (line 4). From line 5 to line 8,we learn the graph selection masks based on Eq. (15). Finally, weselect graphs with the top- highest value in as D (line 9).",
  "EXPERIMENTS": "In this section, we conduct experiments on various real-worlddatasets to answer the following research questions: (i) Q1: CanGraphSteal effectively reconstruct private training graphs of GNNsby leveraging the parameters of GNNs? (ii) Q2: How do the numberof final selected graphs affect the performance of reconstruction?(iii) Q3: How does each component of GraphSteal contribute to theeffectiveness in stealing training graphs?",
  "Experimental Setup": "5.1.1Datasets. We conduct experiments on 3 public real-worlddatasets, i.e., FreeSolv, ESOL and QM9 . FreeSolv and ESOLare small-scale molecular datasets. QM9 is a large-scale-moleculardataset. For each experiment, 20% of randomly selected graphsfrom the original dataset serves as the target dataset for trainingthe target GNN classifier. Additionally, another 10% of the graphsare set as the validation set, while the remaining 70% are assignedas the test set. Note that we set the test set as the auxiliary datase forattackers to train the graph generator and conduct reconstruction.The statistics of the datasets are summarized in Tab. 1. More detailsof the dataset settings are shown in Appendix B.1. 5.1.2Baselines. To the best of our knowledge, GraphSteal is thefirst graph stealing attack to extract training graphs without accessto the private graph-level dataset. To demonstrate the effectivenessof GraphSteal, we first propose three variants:",
  "BL-Conf: This method directly selects the top- most confidencegraphs from the auxiliary dataset as the reconstructed graphs": "BL-Diff: It applies the graph diffusion model trained on theauxiliary datasets to generate graphs without any modificationand selection.Moreover, we compare GraphSteal with a state-of-the-art modelinversion attack method for graph neural networks: GraphMI-G: This method is extended from GraphMI , whichis designed for node classification. Specifically, GraphMI aimsto reconstruct the adjacency matrix of the target graphs basedon the node features and labels of the target graphs. To adaptit to our setting, where there is no partial information aboutthe target dataset, we randomly select graphs from the auxiliarydataset as input graphs. We then apply GraphMI to modify theadjacency matrices of these graphs. The modified graphs arefinally regarded as the reconstructed graphs.",
  "Additionally, we also consider a state-of-the-art model-level expla-nation method as the baseline:": "XGNN : It generates representative graphs for each classthat the target classifier is most confident with as model-levelexplanations. To adapt it to our setting, we treat these generatedrepresentative graphs as reconstructed training graphs. 5.1.3Implementation Details. In this paper, we conduct experi-ments on the inductive supervised graph classification task, wherethe adversary aims to extract training graphs from the privatelyowned dataset. To demonstrate the transferability of GraphSteal,we target GNNs with various architectures, i.e., 2-layer GCN ,2-layer GIN and 9-layer Graph Transformer (GTN) . Di-Gress is set as the basic graph diffusion model to generategraphs. All hyperparameters of the compared methods are tunedfor fair comparisons. Each experiment is conducted 5 times on anA6000 GPU with 48G memory and the average results are reported.",
  "Evaluation Metrics. First, we adopt the following metrics toevaluate the realism and quality of the reconstructed graphs:": "Validity calculates the fraction of the reconstructed moleculargraphs adhering to fundamental chemical rules and principles asValidity = (D)/|D |, where (D) is the number of graphsin D adhering to fundamental chemical rules and principlesmeasured by RDKit 1. A larger validity score implies the betterquality and realism of the reconstructed graphs. Uniqueness measures the proportion of graphs that exhibituniqueness within the entire set of reconstructed graphs, whichis calculated by Uniqueness = (D)/|D |, where (D) is thenumber of unique graphs within D. A larger uniqueness scoreimplies a better diversity in reconstruction.",
  "Second, we apply the following metrics to evaluate the fidelity of thereconstructed graphs in mirroring the graphs in the target dataset:": "Reconstruction Rate represents the fraction of reconstructedgraphs matching those in the target dataset as Reconstructionrate = |D D |/|D |, where |D D | denotes the number ofreconstructed graphs exactly matched with those in D. A largerreconstruction rate indicates a better reconstruction method.More details of the exact graph matching are in Appendix B.2.2. Frchet ChemNet Distance (FCD) quantifies the distancebetween the distributions of the representations of the recon-structed dataset and the target dataset. The representations arelearned from the pre-trained ChemNet neural networks.More details of the computation process of FCD is in Appen-dix B.2.1. A lower FCD indicates a better reconstruction method.",
  "Reconstruction Results": "To answer Q1, we compare GraphSteal with baselines in recon-structing private graphs from the target training dataset on threemolecular graph datasets. We also evaluate the performance ofGraphSteal against various GNN models to validate its flexibility.Moreover, the impacts of the training/auxiliary set distribution shiftand split ratio are further investigated in Appendix D. 5.2.1Comparisons with Baselines. We focus on attacking the GNNsfor the graph classification task. The target GNN is set as GCN witha sum pooling layer. The final selection number is set as 100. Theaverage reconstruction results on FreeSolv, ESOL, and QM9 arereported in Tab. 2. Note that a lower FCD indicates a better recon-struction performance. From Tab. 2, we can observe that: Existing baselines give poor performance in validity, reconstruc-tion rate and FCD among all datasets. It implies the necessity ofdeveloping graph stealing attacks for reconstructing realistic andhigh-quality graphs from the privately owned training dataset. GraphSteal gives superior validity and uniqueness than baselines.It indicates the effectiveness of GraphSteal in reconstructingrealistic graphs that are valid in real-world scenarios even on thelarge-scale dataset QM9.",
  ": Reconstruction results on QM9 for various GNNs": "GraphSteal shows a much better reconstruction rate and FCDthan baselines. This demonstrates GraphSteal can obtain high-quality reconstructions by generating and selecting graphs thatclosely match those in the target dataset. 5.2.2Flexibility to Model Architecture. To show the flexibility ofGraphSteal to different GNN models, we consider two more GNNs,i.e., GIN and GTN , as the target models to conduct graphstealing attack. The number of selected reconstructed graphs isset as 100. All other settings are the same as that in Sec. 5.1.3. Wecompare GraphSteal with baselines on QM9. The average results ofvalidity, and reconstruction rate are reported in . Similar trendsare also observed on other datasets. From the figure, we observethat GraphSteal consistently achieves better reconstruction rate,while maintaining high validity, compared to baselines for both GINand GTN models. It demonstrates the effectiveness of GraphSteal inreconstructing realistic and high-quality graphs for various GNNmodels, which shows the flexibility of GraphSteal to steal graphsfrom various GNN classifiers.",
  "Impact of the Number of Selected Graphs": "To answer Q2, we conduct experiments to explore the attack per-formance of GraphSteal given different budgets in the numbersof selected graphs after reconstruction. Specifically, we vary thenumber of selected graphs as {10, 20, 50, 100, 200, 500}. GCN is set asthe target GNN model. The other settings are the same as Sec. 5.1.3. reports the validity and reconstruction rate on QM9 datasets.More results on other datasets can be found in Appendix C. Fromthe figure, we observe that:",
  ": Impact of the numbers of selected graphs on QM9": "As the number of selected graphs increases, the reconstructionrate of our method would slightly decrease. However, it is worthnoting that the absolute number of the matched reconstructedgraphs (# Selected Graphs Recon. Rate) increases, which satis-fies our expectation. This is because according to Theorem 4.1,the trained GNN classifier is regarded as a weighted combina-tion of a subset of the derivative of training graphs. Thus, thereis a theoretical upper bound to the number of graphs that can bereconstructed based on . Moreover, our proposed reconstruc-tion selection method can effectively identify the reconstructedgraphs that are more likely to be training graphs, leading toa high reconstruction rate when the selected graph number issmall. As the number of selected graphs increases, the numberof matched reconstructed graphs will gradually reach its upperbound, resulting in a diminished reconstruction rate. More detailsof the absolute number of matched reconstructed graphs are inAppendix C. Our method consistently outperforms BL-diff in terms of recon-struction rate as the number of selected graphs increases, whilestill maintaining high validity, which demonstrates the effective-ness of our method in reconstructing realistic and high-qualitygraphs in real-world scenarios.",
  "Ablation Study": "To answer Q3, we conduct ablation studies to understand the ef-fects of the reconstruction generation and reconstruction selectionmethod. (i) To demonstrate the effectiveness of the diffusion noiseoptimization in Sec.4.1.2, we implement a variant GraphSteal/Othat replaces the optimized graphs with the Gaussian noise as theinput to the graph diffusion model. (ii) GraphSteal deploys a graphdiffusion model to improve the realism and quality of reconstruc-tion. To prove its effectiveness, we directly regard the optimizedgraphs in Sec.4.1.2 as the reconstructed graphs and train a variantnamed GraphSteal/D. (iii) A variant named GraphSteal/S is trainedby replacing the reconstruction selection component with the ran-dom selection. (iv) We also propose a variant named GraphSteal/Gby directly conducting selection from the auxiliary dataset basedon Eq. (15) without generating graphs to show the effectiveness ofthe overall reconstruction generation. BL-Diff is also adopted asa reference. GCN is set as the target GNN model. The number ofselected graphs is set as 100.The average results on FreeSolv and QM9 are reported in .From the figure, we observe that: (i) GraphSteal/O and Graph-Steal/S perform significantly worse than GraphSteal in terms ofreconstruction rate, while they still outperform BL-Diff. It showsthe effectiveness of our proposed diffusion noise optimization and QM9FreeSolv Validity (%) GS/OGS/DGS/S GS/GBL-DiffOurs",
  ": Reconstructed training graphs of QM9": "graph selection mask optimization, respectively; (ii) GraphStealoutperforms GraphSteal/D and GraphSteal/G by a large margin inreconstruction rate. This demonstrates the proposed graph gener-ator can effectively enhance the realism and quality of the recon-structed graphs; and (iii) GraphSteal/D exhibits extremely lowervalidity than all other methods. This aligns with our description inSec. 4.1.3 that directly using graphs optimized based on Sec. 4.1.2 asreconstructions would encounter issues of unrealism and invalidity.",
  "Reconstruction Visualization": "In this subsection, we conduct a case study to further demonstratethe effectiveness of GraphSteal. We conduct graph stealing attackusing both GraphMI-G and GraphSteal on QM9 and then visualizethe top-8 representative reconstructed graphs. The visualizationsare plotted in . From the figure, we observe that the graphsreconstructed by GraphMI-G appear unrealistic and invalid, whichimplies the poor performance of GraphMI-G in graph stealing at-tacks. In contrast, the graphs reconstructed by our method exhibitboth high quality and realism, thereby verifying the effectivenessof our method in reconstructing high-quality graphs by leveragingthe model parameters of GNNs.",
  "CONCLUSION AND FUTURE WORK": "In this paper, we study a novel privacy attack problem of stealingtraining graphs from trained GNNs. We propose a novel frame-work, GraphSteal, to reconstruct training graphs by leveraging theparameters of GNNs. Specifically, we apply a graph diffusion modelas the graph generator to reconstruct graphs realistically. We thenimplement a diffusion noise optimization to enhance the resem-blance of the graphs generated by the graph generator to the targettraining data. Moreover, a model parameter-guided graph selectionmethod is proposed to identify the training graphs from the gen-erated graphs by leveraging the parameters of GNNs. Extensiveexperiments on different real-world datasets demonstrate the effec-tiveness of GraphSteal in reconstructing realistic and high-qualitytraining graphs from trained GNNs. There are two directions thatneed further investigation. First, in this paper, we only focus onperforming graph stealing attack in the white-box setting. Thus, itis also interesting to in investigate how to conduct the black-boxgraph stealing attack effectively. Second, it is also worthwhile toinvestigate how to defend against the graph stealing attack. The dis-cussions of the potential countermeasures and ethical implicationsare in Appendix E and Appendix F, respectively. Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov,Kunal Talwar, and Li Zhang. 2016. Deep learning with differential privacy. InProceedings of the 2016 ACM SIGSAC conference on computer and communicationssecurity. 308318. Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, KarstenKreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. 2022. ediffi:Text-to-image diffusion models with an ensemble of expert denoisers. arXivpreprint arXiv:2211.01324 (2022).",
  "Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification withGraph Convolutional Networks. In ICLR": "Greg Landrum. 2013. Rdkit documentation. Release 1, 1-79 (2013), 4. O-Joun Lee et al. 2024. Transitivity-Preserving Graph Representation Learningfor Bridging Local Connectivity and Role-Based Similarity. In Proceedings of theAAAI Conference on Artificial Intelligence, Vol. 38. 1245612465. Xiaoxiao Li, Yuan Zhou, Nicha Dvornek, Muhan Zhang, Siyuan Gao, JuntangZhuang, Dustin Scheinost, Lawrence H Staib, Pamela Ventola, and James S Dun-can. 2021. Braingnn: Interpretable brain graph neural network for fmri analysis.Medical Image Analysis 74 (2021), 102233.",
  "Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher,and Pascal Frossard. 2023. DiGress: Discrete Denoising diffusion for graphgeneration. In ICLR": "Binghui Wang, Minhua Lin, Tianxiang Zhou, Pan Zhou, Ang Li, Meng Pang,Hai Li, and Yiran Chen. 2024. Efficient, direct, and restricted black-box graphevasion attacks to any-layer graph neural networks via influence function. InProceedings of the 17th ACM International Conference on Web Search and DataMining. 693701. Daixin Wang, Jianbin Lin, Peng Cui, Quanhui Jia, Zhen Wang, Yanming Fang,Quan Yu, Jun Zhou, Shuang Yang, and Yuan Qi. 2019. A Semi-supervised GraphAttentive Network for Financial Fraud Detection. In ICDM. 598607.",
  "Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and KilianWeinberger. 2019. Simplifying graph convolutional networks. In Internationalconference on machine learning. 68616871": "Yixin Wu, Xinlei He, Pascal Berrang, Mathias Humbert, Michael Backes,Neil Zhenqiang Gong, and Yang Zhang. 2024. Link Stealing Attacks AgainstInductive Graph Neural Networks. arXiv preprint arXiv:2405.05784 (2024). Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Ge-niesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. 2018. MoleculeNet: abenchmark for molecular machine learning. Chemical science 9, 2 (2018), 513530.",
  "Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. 2022.Geodiff: A geometric diffusion model for molecular conformation generation.arXiv preprint arXiv:2203.02923 (2022)": "Haotian Xue, Alexandre Araujo, Bin Hu, and Yongxin Chen. 2023. Diffusion-BasedAdversarial Sample Generation for Improved Stealthiness and Controllability. InThirty-seventh Conference on Neural Information Processing Systems. Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He,Yanming Shen, and Tie-Yan Liu. 2021. Do transformers really perform badly forgraph representation? NeurIPS 34 (2021), 2887728888.",
  "A.2Homogeneity of Graph Neural Networks": "Before we start to discuss the homogeneity of GNNs, we recall thedefinition of homogeneous neural networks in Sec. 4.2.1 as follows:Definition 4.1 (Homogeneous Neural Networks ). Let be aneural network with model parameters as . Then is a homogeneousneural network if there is a number > 0 such that the model output (G;) satisfies the following equation:",
  "(G;) = (G;), > 0.(24)": "Following , any fully-connected or convolutional neuralnetwork with ReLU activations is homogeneous w.r.t the modeparameters if it does not have any bias terms or skip-connections.To extend to GNNs, the message passing in GNNs can be regarded asa generalized form of convolution from the spectral perspective .Therefore, we can claim that any message-passing based GNNs (e.g.GCN and SGC ) with ReLU activations is homogeneousw.r.t the parameters if it does not have any skip-connections(e.g., GraphSage ) or bias terms, except possibly for the firstlayer. Our experimental results in Sec. 5.2.2 also demonstrate theeffectiveness of GraphSteal across various GNN architectures.We further take SGC and GCN as examples.",
  "A.3Proof of Theorem 4.1": "Before beginning our proof, we first recall Lemma 4.1 in Sec. 4.2.1.Lemma 4.1 (). Let be a homogeneous ReLU neural networkwith model parameters . Let D = {(G,)}| D |=1be the classifica-tion training dataset of , where {1, . . . ,}. Assume is trainedby minimizing the cross entropy loss L = | D |=1 log(1 + ( ))over D using gradient flow, where () = (G) (G) and (G) denotes the-th entry of the logit score vector (G) beforeapplying the softmax normalization. Then, gradient flow convergesin direction towards a first-order stationary point of the followingmax-margin optimization problem:",
  "min12 || ||22,s.t. () 1, [|D |], []\\{}.(37)": "Given a homogeneous ReLU GNN model , according to Lemma 4.1,the training of using gradient descent can be viewed as solv-ing a max-margin optimization problem to maximize the margin () = (G) (G) over all possible directions.Moreover, the optimization problem in Eq. (37) can be reformu-lated to the following constrained optimization problem:",
  "(b) Recon. Rate": ": Impact of the numbers of selected graphs on Free-Solvbe proved that Eq. (38) satisfies the Mangasarian-Fromovitz Con-straint Qualification (MFCQ). KKT conditions are then first-ordernecessary conditions for the global optimality of Eq. (38). Thus,after convergence of training in direction to a KKT point usinggradient descent, there exist 1, . . . , | D | R such that:",
  "BADDITONAL DETAILS OF EXPERIMENTSETTINGSB.1Dataset Settings": "We conduct experiments on 3 well-known datasets, i.e., FreeSolv,ESOL and QM9 , that are widely used for various graph-leveltasks, including graph generation and graph regression. Given ourfocus on graph classification, we adapt these datasets to our settingby categorizing each graph according to the magnitude of its regres-sion values and ensuring an equal division of graphs within eachclass. More specifically, for FreeSolv and ESOL, we partition thedatasets into two classes, maintaining an equal number of graphsin each. To further validate the performance of GraphSteal in themulti-class graph classification task, we divide the QM9 dataset intothree classes, also ensuring each class contains an equal number ofgraphs.",
  "DIMPACT OF THE TRAINING/AUXILIARYSETD.1Training/Auxiliary Set Distribution Shift": "To investigate performance where there is a distribution shift be-tween the auxiliary and target training datasets, we propose todivide QM9 in the following way to make the auxiliary and targettraining datasets have a distribution shift. Specifically, we first traina GCN with graph labels and use the trained GCN to obtain graphrepresentations. We then apply K-Means to cluster these graphsinto 8 groups, setting 1 group as the training set and the remaininggroups as the auxiliary set. Hence, there is a significant distributionshift between the two sets. The results on the QM9 dataset arereported in Tab. 5. From the table, we can observe that GraphStealstill outperforms the baselines in this setting, demonstrating itseffectiveness in the case of there is a significant distribution shiftbetween the training and auxiliary datasets.",
  "D.2Training/Auxiliary Set Split Ratio": "In this section, we investigate the impact of the training/auxiliarydataset split (T/A split). We set the T/A split at {10%/70%, 50%/30%,60%/20%}. Specifically, 50%/30% means that 50% and 30% of theoriginal dataset serve as the training set for the GNN classifier andthe auxiliary set, respectively. The target GNN model is set as GCN.All other settings are the same as that in Sec. 5.3. The results onQM9 are reported in Tab. 6. From the table, we observe that: GraphSteal achieves good performance across all T/A split set-tings. This demonstrates the effectiveness of GraphSteal in vari-ous T/A split ratios, especially when the size of the auxiliary setis small. As the T/A split ratio increases, the reconstruction rate rises. Weanalyze that the reason is that although the size of the auxiliarydataset decreases as the T/A split ratio increases, it remains suffi-cient for the graph diffusion model to acquire enough knowledgeto generate novel and valid graphs for the reconstruction selec-tion to identify the generated graphs that belong to the targettraining graphs. Moreover, as the size of the training dataset alsoincreases, our framework can better match the reconstructedgraphs with more training graphs, leading to an increase in thereconstruction rate.We further fix the training dataset at 10% of the original datasetand vary the size of the auxiliary dataset to {20%, 30%, 70%} of theoriginal dataset. The results on QM9 are reported in Tab. 7. Fromthe table, we observe that GraphSteal consistently achieves goodperformance across all auxiliary dataset sizes, further validatingthe effectiveness of GraphSteal in various auxiliary dataset sizes.",
  "EPOTENTIAL COUNTERMEASURES": "In this section, we explore the potential countermeasures of graphstealing attacks. Since graph stealing attacks represent a new typeof privacy attack that steals private training graphs from trainedGNN classifiers, there is no existing work studying the defenseagainst this attack. One potential defense could be differential pri-vacy (DP). Here, we investigate the effectiveness of DP againstgraph stealing attacks. Following , we add Gaussian noise to thegradients in each training iteration of target GNN classifier trainingto ensure (,) DP. We fix = 105 and vary the noise scale to{1.0, 5.0, 10.0}. The reconstruction results on QM9 are reported inTab. 4. From the table, we can observe that as the privacy budget decreases, GraphSteal consistently shows good validity, unique-ness, and FCD but only a slight drop in the reconstruction rate. Thisindicates that applying differential privacy to GNNs cannot preventgraph stealing attacks effectively. Therefore, there is an emergingneed to design more effective countermeasures against GraphSteal.",
  "FETHICAL IMPLICATIONS": "In this paper, we study a novel privacy attack problem of extractingprivate training graphs from the trained GNN. Our work uncoversthe vulnerability of GNNs to the graph stealing attack and discussespotential countermeasures against this attack. Our work aims toraise awareness about the privacy issues inherent in GNNs andinspire the following works to develop more advanced privacy-preserving methods to protect against graph stealing attacks. Alldatasets we used in this paper are publicly available, no sensitive"
}