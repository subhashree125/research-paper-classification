{
  "Abstract": "Graph-level anomaly detection is significant in diverse domains.To improve detection performance, counterfactual graphs havebeen exploited to benefit the generalization capacity by learningcausal relations. Most existing studies directly introduce pertur-bations (e.g., flipping edges) to generate counterfactual graphs,which are prone to alter the semantics of generated examples andmake them off the data manifold, resulting in sub-optimal per-formance. To address these issues, we propose a novel approach,Motif-consistent Counterfactuals with Adversarial Refinement (Mo-tifCAR), for graph-level anomaly detection. The model combinesthe motif of one graph, the core subgraph containing the identifi-cation (category) information, and the contextual subgraph (non-motif) of another graph to produce a raw counterfactual graph.However, the produced raw graph might be distorted and cannotsatisfy the important counterfactual properties: Realism, Validity,Proximity and Sparsity. Towards that, we present a GenerativeAdversarial Network (GAN)-based graph optimizer to refine theraw counterfactual graphs. It adopts the discriminator to guide thegenerator to generate graphs close to realistic data, i.e., meet theproperty Realism. Further, we design the motif consistency to forcethe motif of the generated graphs to be consistent with the realisticgraphs, meeting the property Validity. Also, we devise the contex-tual loss and connection loss to control the contextual subgraph andthe newly added links to meet the properties Proximity and Spar-sity. As a result, the model can generate high-quality counterfactualgraphs. Experiments demonstrate the superiority of MotifCAR.",
  "Corresponding Author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain. 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "Introduction": "Graph-level anomaly detection aims to identify graph instancesthat are significantly different from the majority of graphs. As a fewanomalies may cause tremendous loss, detecting anomalous datahas significant implications for various domains ranging from iden-tifying abnormal proteins in biochemistry and distinguishing braindisorders in brain networks, to uncovering fraudulent activities inonline social networks . Numerous corresponding detectionmethods have been introduced by taking advantage of differentdeep learning techniques for anomaly detection, such asself-supervised learning , knowledge distillation and tailored Graph Neural Networks (GNNs) .However, these deep learning models are prone to learndataset-dependent spurious correlations based on statistical associ-ations . This might hinder well-trained models from generaliz-ing well to newly observed anomalies, resulting in detection errors.Counterfactual data augmentation can help the model alleviate theproblem of spurious correlations by learning causal relations andenhance the generalization capacity in tabular data, image data, andtext data . While, for graph data, research on counterfactualaugmentation is insufficient due to the presence of complex struc-ture and node information. The limited existing research principallyfocuses on introducing perturbations into graphs or matching coun-terfactual data with different treatments .Whereas, when applying to anomaly detection, these methodsconfronts the crucial problems: (1) Perturbations might alter thegraph semantics, adversely impacting model robustness. In anom-aly detection, the category of a graph (e.g., normal or anomalouslabel) can be determined by the presence of specific edges .Perturbing these edges may change its category, as illustrated in. However, the augmented samples will be labeled with",
  "G1: normalG2: normalG3: abnormal4: AugmentedG": ": An example of perturbations altering semantics.1 and 2 are normal graphs. While, 3 is abnormal one asits fully connected structure deviates significantly from thenormal ones. However, by pruning a few edges, abnormalgraph 3 is transformed into normal graph 4, leading to analtered semantic. the same category as the original ones , resulting inincorrect labels and degraded performance. (2) These methods maygenerate suboptimal augmented data, leading to limited effective-ness in enhancing generalization capacity. Perturbation methodsoften lead to distorted examples , which are usually off the datamanifold . In such cases, deep models can be deceived becausethey do not generalize well to unseen test data . Also, matchingapproaches only seek desired samples from existing data ,which cannot expand the training data.In this paper, we propose a Motif-consistent Counterfactual datageneration model with Adversarial Refinement (MotifCAR) to gen-erate high-quality counterfactual samples for graph-level anomalydetection. In this model, to address the problem (1), instead of con-ducting perturbations on graphs, we introduce the discriminativemotif for counterfactual data generation. According to , the dis-criminative motif is a subgraph of a graph that decides the categoryof this graph. Hence, it can be regarded as the core subgraph con-taining the identification (category) information of a given graph.Correspondingly, the remaining nodes can form a subgraph as thecontextual graph. For brevity, we refer to the discriminative motifsimply as motif. Based on this concept, we combine the motif ofone graph and the contextual subgraph of another graph to forma raw counterfactual graph, i.e., unseen combinations of the motifand the contextual subgraph. These two source graphs can be fromthe same/different categories/clusters.To address the problem (2), we design a Generative Adversar-ial Network (GAN)-based graph optimizer along with our tailoredlosses to refine the raw counterfactual graphs into the high-qualityones. According to , good counterfactual data requires satis-fying a set of properties (a.k.a. counterfactual properties), including(1) Realism: the counterfactual examples should lie close to the datamanifold so that they appear realistic. (2) Validity: the model shouldassign the counterfactual examples to the corresponding categorylabel in order to be valid; (3) Proximity: the distance of a counterfac-tual and original data should be close; (4) Sparsity: the number ofperturbations on nodes/edges should be sparse. To meet the desiredcounterfactual properties, we design three specific losses for thegraph optimizer. The refined counterfactual graphs can enlarge thedistribution of training data and help the model handle the situationwith varying environments.More succinctly, our MotifCAR model is composed of two funda-mental components: the raw counterfactual graph producer and theGAN-based graph optimizer. The graph producer takes as inputstwo graphs, and , and produces a raw counterfactual graph bycombining the motif of and the contextual subgraph (non-motif)of . This producer first merges these two graphs into one graph, and then discards the contextual subgraph of and the motif of. The remaining parts (the motif of and the contextual sub-graph of ) are connected randomly to form a raw counterfactualgraph. The generated graph preserves the identification (category)information of but involves the environment characteristics of .These unseen combinations can enlarge the distribution of trainingdata and help the model learn transferable relations across differentenvironments. However, the random links between both subgraphsin the raw counterfactual graphs may distort the graph structure,which does not adhere well to the desired counterfactual properties.Hence, we further design the GAN-based graph optimizer torefine the raw graphs by adjusting the edges. The optimizer is com-posed of the graph generator and the discriminator. The generatoraims to adjust the edges of the graphs and the discriminator tries todistinguish generated graphs from realistic ones. This adversarialtraining will encourage the generated graphs to conform to the pat-terns of realistic graphs, i.e., meet the requirement of the property(1) Realism. On the basis of the basic GAN framework, we furtherdesign three losses to improve the quality of the generated graphs.Firstly, we propose a motif consistency loss to force the motif of thegenerated graphs to be consistent with the realistic graphs. Sincethe motif contains the identification information of the graphs, thisloss ensures the identification information invariant with the realis-tic graphs, i.e., meets the property (2) Validity. Secondly, we devisethe contextual loss to keep the degree distribution of the generatedcontextual subgraphs to be similar to the realistic ones. Coupledwith the motif consistency loss, these losses facilitate the proximityof the counterfactual graphs to the original ones, i.e., the property(3) Proximity. Thirdly, we present the connection loss to control thenew links between the motif and the contextual subgraph which, inturn, can control the number of perturbations, i.e., meet the prop-erty (4) Sparsity. Through the adversarial training with these losses,the model can generate counterfactual graphs that conform to thecounterfactual properties. Finally, these generated counterfactualgraphs and realistic graphs are adopted to train a robust anomalydetection model. The contributions of this study are threefold:",
  "where is number of classes of graphs. Accordingly, the problemof graph-level anomaly detection is defined as follows:": "Problem 1. Graph-level anomaly detection aims to identify indi-vidual graphs within a given set that exhibit anomalous behavior.In this study, the problem of graph-level anomaly detection can beregarded as the task of identifying anomalous graphs based ona limited set of graph labels = {0, 1}, where 0 signifies that thegraph is considered an anomaly.",
  "Graphon": "We will leverage grahpons to produce counterfactual graphs. Herewe present an introduction to the graphon and estimation method.Graphon. A graphon is a continuous, bounded and symmetricfunction 2 which may be thought of as theweight matrix of a graph with infinite number of nodes. Then,given two points , , (, ) represents the probabilitythat nodes and are related with an edge. For a set of graphs with a given category label, a graphon can be estimated based onthese graphs. Conversely, arbitrarily sized graphs can be sampledfrom this graphon, and sampled graphs will preserve the samecategory label with this graphon .Graphon Estimation. Give a graph set = {1,2, . . . ,},graphon estimation aims to deduce a graphon based on . It isintractable because a graphon is an unknown function without aclosed-form expression for real-world graphs . Hence, the stepfunction estimation methods are generally adopted to approximategraphons . The typical step function estimation method iscomposed of two stages: aligning the nodes in a set of graphs andestimating the step function from all the aligned adjacency matri-ces . For the node alignment, this method first aligns multiplegraphs based on node measurements (e.g., degree), and then selectsthe top nodes from the aligned graphs and calculates the averagematrix of their adjacency matrices. For function estimation, the goalis to obtain a matrix W = [] , where denotesthe probability of an edge existing between node and node and is the number of the selected nodes, with the default being theaverage number of nodes in all graphs. In particular, thestep function W 2 is defined as follows:",
  "Theorem 1. For a graph which is sampled from the graphon, its discriminative motif exists in": "Proof Sketch. We verify this by stating that the homomorphismdensity of the discriminative motif in generated (sampled) graphswill be approximately equal to that in the graphon with high prob-ability. In other words, the sampled graphs will preserve the dis-criminative motif of the graphon with a very high probability. Here,homomorphism density is defined to measure the relative frequencythat the graph appears in graph .",
  "Producing Raw Counterfactual Graph": "Here we present the raw graph producer, which takes two graphs asinputs and extracts the motif subgraph from one graph and contex-tual subgraph from another to generate a raw counterfactual graph.The generated graph possesses the label of the first graph whileinvolving the environment information of the second graph. To thisend, this producer first merges the two graphs into one graph, andthen builds a masked matrix based on graphons of the categoriesof these two graphs to filter the motif and the contextual subgraph.Finally, the raw counterfactual graphs are produced by combiningthe adjacency matrix of the merged graph and the masked matrix.We adopt this generation way because: (1) this fixed merging pat-tern ensures a stable outcome for each merging attempt, and (2) itenables uniform expansion of the node counts when the two graphshave different numbers of nodes.Formally, suppose that a graph with nodes is from the graphset and graph with nodes from the graph set . Here and can have the same/different cluster or category label andtheir graphons can be computed using Equation 1, denoted as and , individually. To produce raw counterfactual graphs, wefirst merge two graphs and into one. Since and mighthave different numbers of nodes, we extend their node sets to a set , and their adjacency matrices can be computed as:",
  "= ext + ext = ,(3)": "where is a matrix indicating the cross-graph connectivity be-tween the nodes in and . and are the aligned adjacencymatrices, i.e., their first and elements are aligned withtheir corresponding graphon nodes. We randomly sample edgesto connect and to ensure the produced graph is connected.To remove the contextual (non-motif) nodes in and remove themotif nodes in , we build a mask matrix which is used to performan XNOR operation with to eliminate these nodes. Since themotifs of and exist in the graphons and , individually,we build this mask matrix based on and . To this end, wefirst extend row and column numbers of and to be the samewith and , respectively:",
  "= .(7)": "According to the adjacency matrix , we can obtain the rawcounterfactual graph raw(, ), where is derived from and is composed of the motif nodes of and contextual nodes of .Since raw(, ) is produced based on the motif of one graphand the contextual subgraph of another graph, it should possessthe identification of the first graph and also involve the environ-ment characteristics of the second graph. However, the motif nodesand the contextual nodes are connected randomly, which may re-sult in distorted links. Hence, we further feed the produced rawcounterfactual graphs into the graph optimizer to refine the graphs.",
  "Optimizing Counterfactual Graphs": "In the raw counterfactual graph raw(, ), the edges between themotif nodes and the contextual nodes (i.e., in Equation 3) arerandomly assigned, which might not conform to the distributionof realistic graphs. To obtain high-quality graphs, we here presenta GAN-based graph optimizer to refine the raw graphs to meetthe four counterfactual properties (Realism, Validity, Proximity andSparsity). This optimizer is composed of two essential modules: thegraph generator and the graph discriminator. The generator aims to generate refined graphs through adjusting edges. The discriminatoris designed to distinguish between the graphs generated by thegenerator and realistic graphs. The graph generator and the graphdiscriminator are trained with our designed losses in an adversarialstyle to generate high-quality counterfactual graphs. These gener-ated graphs will be used to train a robust classification model forbetter anomaly detection. 3.2.1Graph Generator. For the raw graph raw(, ), the graphgenerator is designed to refine the edges in order to make thegenerated graph more closely resemble the real graph. We assumethat each edge (, ) in raw is associated with a random variable , (), where R is a learnable matrix, isa binary matrix with size , (, ) is in raw if , = 1and is dropped otherwise.Inspired by the work , we relax the discrete , to a continu-ous variable with values in the range (0, 1) to facilitate end-to-endtraining of the generator:",
  "where R represents a random matrix with eachelement sampled from a uniform distribution: , (0, 1).() =1": "1+ is the Sigmoid function, and (0, 1] is a hyper-parameter used to make , approach either 0 or 1. Here, can beregarded as an approximation of the generated adjacency matrix.Motif Consistency Loss. The purpose of this loss is to en-sure that the identification information of the generated graphis consistent with that of the given realistic graph. Since thegraphon contains the motif of the graph and the motif decidesthe identification information, we adopt the graphon to buildthis consistency loss, i.e., we force the graphon of the generatedgraphs to be close to that of the realistic graphs. For the generatedgraph set gen = {1gen, 2gen, , gen} and the realistic graph set",
  "motif = ,gen(, ) rel(, )2 .(9)": "This loss forces the generated graph to have the same motif as therealistic graphs. Since the motif decides the category label, this lossencourages the generated graphs to meet the property Validity.Contextual loss. This loss is proposed to force the contextualsubgraph to be similar to the realistic contextual one. Since thegenerator mainly adjusts the edges, we consider the edge-relatedfeature the degree distribution for this loss, i.e., we force thedegree distributions of the generated and realistic contextual sub-graphs to be similar. We introduce the degree entropy to measurethe degree distribution of the subgraph. Supposing that () rep-resents the degree number of node in a generated contextualsubgraph, the degree entropy is computed as:",
  "gen rel,(11)": "where rel is the entropy of the realistic contextual subgraph and denotes the number of the subgraphs. Both the contextual lossand the motif consistency loss compel the generated contextualsubgraph and the motif to approach the realistic ones, which canmeet the property Proximity.Connection loss. This objective is designed to control the num-ber of edges connecting the motif nodes and the contextual nodesin the generated graph. Inspired by the edge dropout , we set aratio and train to generate graphs with con connectionedges. Here con is the edge set connecting the motif nodes and thecontextual nodes in the realistic graph. For a batch of generatedgraphs gen = {1gen, 2gen, ..., gen}, we calculate their average as theconnection loss. Formally, this loss is computed as:",
  "reg = 1 motif + 2 context + 3 con,(13)": "where 1, 2 and 3 are hyper-parameters to balance the influencesof these losses.Moreover, for the sake of efficiency, we initialize rather thantraining from scratch. To be specific, we introduce an initializationrate to constrain the number of the connection edges atthe beginning. Also the similarity of nodes is considered for theinitialized values. Consequently, is initialized as follows:",
  "(14)": "where is the edge set in the motif and the contextual subgraph,con and are the same to Equation 12, , is the node similar-ity which can be computed based on node embedding or nodeattributes, and is a candidate set of new connection edges. 3.2.2Graph Discriminator. The discriminator is a graph-level clas-sifier designed to distinguish between real and generated graphs.This can help the generator produce samples that are close to real-istic data, i.e., meet the property Realism. Specifically, this discrimi-nator takes a graph as input and determines whether it is real orfake. Supposing that rel and gen denote the realistic graph setand the generated graph set, individually, for each rel gen,we utilize a GNN encoder to encode the representations of eachnode:{z } = ().(15)",
  "dis = log() (1 ) log(1 ).(18)": "3.2.3Model Training. We here present the model training proce-dure of the GAN-based graph optimizer. The graph generator andthe discriminator are optimized sequentially and iteratively.For the generator, in each iteration, an augmented graph genis generated and then the regularization loss (cf. Equation 13) iscomputed. In consideration of generating high-quality graphs, anadversarial classification loss is incorporated to cheat the graphdiscriminator by labeling gen with 1. According to the first term inEquation 18 and the regularization loss (cf. Equation 13), we havethe final loss of the generator:",
  "Graph-level Anomaly Detection": "After training, the model is used to generate a number of coun-terfactual graphs. Both the generated counterfactual graphs genand realistic graphs rel are adopted to train a robust classifier toidentify anomalies from normal graphs. We adopt the discrimina-tor as the classifier after changing its loss. Specifically, for each rel gen, we first adopt Equation 17 to obtain its graphrepresentation. Then we label the normal graph with 1 and theanomalous graph with 0. Supposing that denotes the label, theloss becomes:",
  "Experiments Setup": "Datasets. We adopt four public datasets for our experiments, whosestatistics are presented in . IMDB-BINARY (IMDB-B) andIMDB-MULTI (IMDB-M) are movie collaboration datasets. Eachgraph corresponds to an ego-network for each actor/actress, wherenodes correspond to actors/actresses and an edge is drawn betweentwo actors/actresses if they appear in the same movie. Each graphis derived from a pre-specified genre of movies, which is regardedas the category label. REDDIT-BINARY (REDDIT-B) and REDDIT-MULTI-5K (REDDIT-M) are balanced datasets where each graphcorresponds to an online discussion thread and nodes correspond tousers. An edge is drawn between two nodes if at least one of themresponds to anothers comment. The community or subreddit isconsidered as the label of the graph. Following previous works , we downsample one of the categories as the anomalous one,and others as the normal data.Baselines. We conduct a comparison between our developed frame-work MotifCAR and three categories of baselines: (1) The state-of-the-art GNN models (g-U-Nets , SAGPool , DIFFPOOL, and GMT ) leverage different pooling strategies and spe-cially designed pooling layers for learning the graph-level repre-sentations. (2) The counterfactual graph augmentation methods(CFGL-LCR , CFAD , CGC , and CF-HGExp ) generatecounterfactual graphs to enhance the node or graph representationsand further improve the classification performance. (3) The graph-level anomaly detection approaches (OCGTL , GLocalKD ,iGAD and GmapAD ) explore the tailored classificationmodels or graph-level anomaly patterns to the anomalies.Experiments Settings. We use the Adam algorithm to optimizethe model with learning rate 0.001. For hyper-parameters, we set = 0.0001 in Equation 8, = 0.75 in Equation 14, 1 = 1, 2 = 0.9, 3 =0.6 in Equation 5, = 0.5 for IMDB-M and REDDIT-M and = 0.8for IMDB-B and REDDIT-B in Equation 12. We split the dataset intotrain/validation/test data by 2 : 4 : 4. The best test epoch is selected",
  "Anomaly Detection Results": "We report the anomaly detection performance of MotifCAR andthe baselines in and have the following observations. First,in terms of overall detection results, our model MotifCAR consis-tently outperforms all baseline models across the four datasets. Inparticular, our method achieves more gains on IMDB-Binary andREDDIT-Binary. This is because graphon estimation can performbetter on dense graphs and the graphs in these two datasetsare denser. Correspondingly, our model exhibits more significantadvantages on these two datasets. This result verifies that our modelcan generate effective counterfactual graphs and enhance detec-tion performance. Besides, the -test indicates that MotifCARsperformance is statistically significant compared to the baselines.Second, some of the counterfactual graph augmentation meth-ods, such as CGC and CF-HGExp, acquire relatively good perfor-mance, compared with the GNN models. This indicates that theappropriate counterfactual data can benefit graph-level anomalydetection. However, our model outperforms these state-of-the-artmodels, which validates the efficacy of the proposed GAN-basedgraph optimizer with the tailored losses for graph-level anomalydetection.Third, the method GmapAD, which is specially designed forgraph-level anomaly detection, exhibits remarkable performance.It can explore both the intra- and inter-graph node informationto enhance the graph representations and detection performance.However, when faced with a smaller amount of training data, thetrained model cannot generalize well to the test data, leading to aperformance decline. However, our model can generate counterfac-tual data to handle varying environments and hence, outperformthis method.",
  ": Ablation study: Variants of MotifCAR": "(2) In MotifCAR-Moti, we remove the motif consistency loss in theGAN-based optimizer. (3) In MotifCAR-Cext, we remove the con-textual loss in the optimizer. (4) In MotifCAR-Conn, we remove theconnection loss in the optimizer.As shown in , no matter which part we remove in Mo-tifCAR, the models performance degrades. This suggests the ef-ficacy of our designed GAN-based optimizer as well as the lossesin this optimizer. Moreover, when removing the motif consistencyloss, the performance of MotifCAR-Moti delines more dramaticallythan MotifCAR-Cext and MotifCAR-Conn. This indicates that themotif consistency loss is more important when refining the counter-factual graphs. The reason is that this loss can ensure the generatedgraphs to have the corresponding identification information, whichis vital for training a robust classification model.",
  "Qualitative Analysis of the Counterfactuals": "We here investigate the quality of the generated counterfactualgraphs from the aspects of the counterfactual properties: Realism,Proximity, Validity, and Sparsity. According to the works ,the Realism score reflects the change of the shift detection resultbetween the realistic graphs and the counterfactual graphs. TheProximity score estimates the mean of feature distances betweenthe counterfactual graph and the realistic graph. The Validity scoregauges the fraction of the generated counterfactual examples thatare correctly predicted by the classifier to the corresponding class.The Sparsity score measures the difference between the edges ofthe counterfactual graph and the realistic graph. We compare Mo-tifCAR with counterfactual data generation baselines: CFGL-LCR(CFGL) , CFAD , CGC, and CF-HGExp (CF-HG) ). demonstrates the Realism and Proximity scores. For theRealism score (the lower the better), our method achieves the bestresult on the four datasets. The reason is that adversarial trainingis beneficial to generate counterfactual graphs that are close torealistic ones. Also, the motif consistency and contextual loss alsofacilitate aligning the motif and contextual subgraph well. For theProximity score (the lower the better), our model MotifCAR doesnot obtain the best result. This is because MotifCAR replaces thecontextual subgraph in the counterfactual graphs, which results in afew differences in the feature space. While our result is competitiveto the best baseline, CF-HGExp. In fact, both of their scores are veryclose. However, our model can achieve better detection performancethan CF-HGExp.",
  "REDDIT-M0.6560.6230.1850.1290.149": "The Validity and Sparsity scores are presented in , wherethe misalignment of points on the x-axis is caused by discontinu-ous Sparsity values. As shown, our proposed MotifCAR achievesthe best Validity performance at all levels of sparsity on the fourdatasets. This is because the raw counterfactual graphs containthe motif, the core subgraph of the graph, which can decide theircategory. Further, during the graph refinement procedure, the motifconsistency provides strong consistent power for the counterfac-tual graphs keeping the identification information. As a result, thegenerated counterfactual graphs can possess the identification in-formation and obtain higher Validity scores.",
  "Hyper-Parameter Sensitivity": "In this section, we analyze the sensitivity of the hyper-parameters,, 1, 2, and 3 in Equation 12 and 13. The results are reported in. For , the performance increases with the rise of the value. A smaller value leads to a lower F1-score. This is because determines the number of edges between the motif and the contex-tual subgraph, and a smaller value might cause the two subgraphs todisconnect. Correspondingly, the generated counterfactual graphsmainly contain the motif, which has little effect on improving themodels generalization ability.For 1, 2, and 3, these three parameters determine the weightsof the motif consistency loss, the contextual loss, and the connectionloss, individually. These figures show that F1-score increases withthe rise of 1, which suggests that more attention should be paid tothe motif consistency loss. The reason is that this loss determinesthe identification information of the generated counterfactual sam-ples, which is quite important for training a robust model. Besides,the detection performance increases with the rise of 2 and 3, andthen starts to decrease when 2 > 0.9 and 3 > 0.6. When they aretoo small, the contextual subgraph cannot effectively be incorpo-rated into the generated counterfactual graphs, resulting in a lowerperformance. On the contrary, the higher 2 and 3 might attenuatethe effectiveness of other losses, leading to inferior performance.",
  "Related Work": "Graph-level Anomaly Detection. Graph-level Anomaly Detec-tion focuses on detecting entire abnormal graphs, rather thanlocalizing anomalies in graphs. Most graph anomaly detectionwork is devoted to detecting irregular nodes and edges withingraphs . Recently, graph-level anomaly detection hasstarted to receive in-depth exploration. Initially, researchers adoptshallow learning techniques to detect graph-level anomalies. Theymainly exploit graph kernels and graph signals to detect graph-levelanomalies. For example, graph kernels (e.g., Weisfeiler-Leman Ker-nel and propagation kernels) are explored to measure the pairwisesimilarity of nodes in the graph, and the similarity score is usedto identify anomalies based on the structural characteristics of thegraph . Changedar outlines anomaly detection using graphsignals generated by anomalous node sets. By examining patterns of signal changes, this approach can effectively pinpoint anomalieswithin the graph. Also, a number of works leveragefrequent graph motifs to model the network topology among nodesand motifs. These frequently occurring subgraphs capture crucialhigh-order structural information, which enables models to detectanomalies more comprehensively. However, the shallow learning-based methods may achieve sub-optimal performance due to thelow coupling of the detector and graph representation learning .The deep learning-based methods, in contrast, are end-to-endstrategies that have been successfully applied to both static anddynamic graphs for anomaly detection . The remark-able development of GNNs has led to great progress in thefield of graph-level anomaly detection. One approach involves uti-lizing GNNs in conjunction with classification loss functions totrain a graph-level anomaly detection framework, exemplified bythe works such as OCGIN and OCGTL . However, theseclassification methods do not cope well with imbalanced datasets,resulting in an underfit for anomalous graphs. iGAD tacklesthis problem by modeling Point Mutual Information. And the out-of-distribution problem of graph data is addressed for better anomalydetection . Another method centers around the identifica-tion of anomalies by scrutinizing the irregular attributes withineach graph concerning the overall graph structure, as evidencedin GmapAD and GLADST . Moreover, GLocalKD andGLADC capture anomalies from global and local graph per-spectives. Additionally, some approaches concurrently considernode-level anomalies and substructure anomalies in anomaly de-tection, such as iGAD , GLAM and HO-GAT .Counterfactual Graph Learning. The works about counterfac-tual graph learning primarily fall into two folds: counterfactualexplanations and counterfactual data augmentation. The formeraims to identify the necessary changes to the input graph that canalter the prediction outcome, which can help to filter out spuri-ous explanations . The related methods try to find a counter-factual graph by conducting minimal perturbations (i.e., addingor removing the minimum number of edges) that could lead tocounterfactual predictions . Counterfactual data augmen-tation is a promising technique used to augment training data toreduce model reliance on spurious correlations and aid in learn-ing causal representations . To alleviate the problem ofspurious correlations and enhance models generalization capacity,some researchers try to inject interventions (perturbations) on thenode attributes and the graph structure to generate counterfactualdata . While, others attend to match counterfactual exampleswhich are the most similar items with different treatments to boost models robustness.",
  "Conclusion": "In this paper, we proposed a novel framework, MotifCAR, for graph-level anomaly detection. We designed a counterfactual graph pro-ducer to produce raw counterfactual graphs by combining the dis-criminative motif and the contextual subgraph. It can generatehigh-quality counterfactual graphs and effectively alleviate theperformance decline issue under varying environments. We alsoproposed a GAN-based graph optimizer to refine the raw graphs",
  "Kaize Ding, Zhe Xu, Hanghang Tong, and Huan Liu. 2022. Data augmentationfor deep graph learning: A survey. ACM SIGKDD Explorations Newsletter 24, 2(2022), 6177": "Jingcan Duan, Siwei Wang, Pei Zhang, En Zhu, Jingtao Hu, Hu Jin, Yue Liu, andZhibin Dong. 2023. Graph anomaly detection via multi-scale contrastive learningnetworks with augmented view. In AAAI. 74597467. Hongyang Gao and Shuiwang Ji. 2019. Graph u-nets. In ICML. 20832092. Zhimeng Guo, Teng Xiao, Charu Aggarwal, Hui Liu, and Suhang Wang. 2023.Counterfactual Learning on Graphs: A Survey. arXiv:2304.01391 (2023).",
  "Xiaotian Han, Zhimeng Jiang, Ninghao Liu, and Xia Hu. 2022. G-mixup: Graphdata augmentation for graph classification. In ICML. 82308248": "Bryan Hooi, Leman Akoglu, Dhivya Eswaran, Amritanshu Pandey, Marko Jeremi-nov, Larry Pileggi, and Christos Faloutsos. 2018. Changedar: Online localizedchange detection for sensor data on a graph. In CIKM. 507516. Ling Huang, Ye Zhu, Yuefang Gao, Tuo Liu, Chao Chang, Caixing Liu, Yong Tang,and Chang-Dong Wang. 2021. Hybrid-order anomaly detection on attributednetworks. IEEE Transactions on Knowledge and Data Engineering (2021)."
}