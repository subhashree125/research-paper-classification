{
  "Abstract": "This paper is an extended abstract of our origi-nal work [Sun et al., 2023b] published in KDD23,where we won the best research paper award. Thepaper introduces a novel approach to bridging thegap between pre-trained graph models and the di-verse tasks theyre applied to, inspired by the suc-cess of prompt learning in NLP. Recognizing thechallenge of aligning pre-trained models with var-ied graph tasks (node level, edge level, and graphlevel), which can lead to negative transfer and poorperformance, we propose a multi-task promptingmethod for graphs. This method involves unify-ing graph and language prompt formats, enablingNLPs prompting strategies to be adapted for graphtasks. By analyzing the task space of graph appli-cations, we reformulate problems to fit graph-leveltasks and apply meta-learning to improve promptinitialization for multiple tasks. Experiments showour methods effectiveness in enhancing model per-formance across different graph tasks.Beyond the original work, in this extended abstract,we further discuss the graph prompt from a biggerpicture and provide some of the latest work towardthis area.",
  "Introduction": "Graph neural networks (GNNs) [Sun et al., 2021a] are in-creasingly applied across various fields [Sun et al., 2023c;Sun et al., 2022b; Li et al., 2024a; Sun et al., 2022c;Chen et al., 2020; Sun et al., 2023a]. The focus has shifted to-wards optimizing graph model training for specific problems.Traditional graph learning methods depend heavily on labels,often scarce or unfit for real-world complexities, leading tooverfitting, especially with out-of-distribution data [Shen etal., 2021]. A popular mitigation strategy involves pre-trainingon accessible data, then fine-tuning for specific tasks [Jin et Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan.All in one: Multi-task prompting for graph neural networks. In Pro-ceedings of the 29th ACM SIGKDD Conference on Knowledge Dis-covery and Data Mining, pages 21202131, 2023. al., 2020], despite challenges in aligning pre-trained modelswith diverse downstream tasks.A novel approach, inspired by NLP, combines pre-trainingwith prompt learning and fine-tuning, where prompts facil-itate task-specific model adjustments without extensive re-training.This method shows promise for efficient modeladaptation, especially in scenarios with limited data. How-ever, applying the concept of language prompts to GNNs in-troduces challenges, such as defining prompt content and in-tegration with graph structures, and ensuring prompts effec-tively bridge pre-training tasks with varied downstream ap-plications. Current efforts in graph prompt learning are lim-ited and typically focus on single-task scenarios [Sun et al.,2022a]. We extend NLP prompt methods to GNNs for multi-task applications, addressing challenges in prompt design,task reformulation, and prompt optimization. Our contribu-tions include a unified prompt format for language and graphdomains, a strategy to reformulate tasks for better alignmentwith pre-training, and the application of meta-learning to en-hance prompt efficacy across multiple tasks. Our extensiveevaluations demonstrate the superiority of our approach.",
  "Motivations": "Graph pre-training [Sun et al., 2021b] employs strategiesto imbue GNNs with broad knowledge, reducing the needfor task-specific annotations.Techniques vary from nodeand edge comparisons to graph-level contrastive learning,which proves superior in learning graph knowledge by en-hancing graph representation or adjusting model parame-ters for consistency across perturbations [You et al., 2020;Xia et al., 2022; Sun et al., 2023c]. Intuitively, the abovegraph-level pre-training strategies have some intrinsic simi-larities with the language-masked prediction task: aligningtwo graph views generated by node/edge/feature mask orother perturbations is very similar to predicting some vacantblanks on graphs. To this end, we aim to merge graph pre-trainings depth with prompt learnings adaptability, address-ing the multifaceted challenges in deploying GNNs acrossvarious tasks more effectively.",
  "arXiv:2403.07040v1 [cs.LG] 11 Mar 2024": "the transferability of pre-trained graph models across varioustasks without altering the original model architecture.Objective: Our primary goal is to develop a graph promptthat seamlessly integrates with original graphs, thereby align-ing pre-trained graph models more closely with diversedownstream tasks and improving knowledge transfer acrossdomains.Framework Overview: We introduce a multi-task prompt-ing framework that first standardizes different graph tasksinto a uniform format, focusing on graph-level tasks. We thendesign a novel graph prompt that incorporates learnable to-kens, structures, and adaptive insertion patterns. To optimizethe prompt for various tasks, we employ a meta-learning strat-egy, enabling the framework to adjust prompts dynamicallyfor improved performance across multiple tasks.Reformulating Tasks for Generalization: Recognizing thechallenge of diverse task requirements in graphs, we refor-mulate node-level and edge-level tasks into graph-level tasks.This approach, inspired by the hierarchical nature of graphoperations, allows for a broader application of pre-trainingknowledge by treating operations like node or edge modifica-tions as graph-level changes.Designing the Prompt Graph: We draw parallels betweenNLP and graph prompting, aiming for a unified representationthat includes prompt tokens, token structures, and insertionpatterns. This ensures that our graph prompts are both mean-ingful and adaptable to the structure of the original graph.Let a graph instance be G=(V, E) where V={v1, v2, , vN} is the node set containing N nodes; eachnode has a feature vector denoted by xi R1d for nodevi; E = {(vi, vj)|vi, vj V} is the edge set where eachedge connects a pair of nodes in V. With the previous dis-cussion, we here present our prompt graph as Gp = (P, S)where P = {p1, p2, , p|P|} denotes the set of prompt to-kens and |P| is the number of tokens. Each token pi P canbe represented by a token vector pi R1d with the samesize of node features in the input graph; Note that in practice,we usually have |P| N and |P| dh where dh is the sizeof the hidden layer in the pre-trained graph model. With thesetoken vectors, the input graph can be reformulated by addingthe j-th token to graph node vi (e.g., xi = xi + pj). Then,we replace the input features with the prompted features andsend them to the pre-trained model for further processing.S = {(pi, pj)|pi, pj P} is the token structure denotedby pair-wise relations among tokens. Unlike the NLP prompt,the token structure in the prompt graph is usually implicit. Tosolve this problem, we propose three methods to design theprompt token structures: (1) the first way is to learn tunableparameters:",
  "A =|P|1i=1j=i+1{aij}": "where aij is a tunable parameter indicating how possible thetoken pi and the token pj should be connected; (2) the secondway is to use the dot product of each prompt token pair andprune them according to the dot value. In this case, (pi, pj) S iff (pi pj) < where () is a sigmoid function and isa pre-defined threshold; (3) the third way is to treat the tokensas independent and then we have S = .",
  "(b) graph tasks": ": Task space in NLP and graph. Realizing the intrinsic na-ture of task space in the graph area, we reformulate node-level andedge-level tasks to graph-level tasks to achieve more general capa-bilities for graph models. Let be the inserting function that indicates how to add theprompt graph Gp to the input graph G, then the manipulatedgraph can be denoted as Gm = (G, Gp). We can define theinserting pattern as the dot product between prompt tokensand input graph nodes, and then use a tailored connection likexi = xi + |P|k=1 wikpk where wik is a weighted value toprune unnecessary connections:",
  "wik =(pk xTi ),if (pk xTi ) > 0,otherwise(1)": "As an alternative and special case, we can also use a moresimplified way to get xi = xi + |P|k=1 pk.Meta-Learning for Prompt Optimization: We leveragemeta-learning to refine our prompting approach, structuringthe learning process to accommodate multiple tasks simulta-neously. This method updates prompt parameters based ontask-specific performances, ensuring that the final promptsare well-suited to a wide array of graph tasks.",
  "pre-train+fine-tune": "GraphCL+GAT76.0576.78 81.96 87.64 88.40 89.93 57.37 66.42 67.43 78.67 72.26 95.65 76.03 77.05 80.02GraphCL+GCN78.7579.13 84.90 87.49 89.36 90.25 55.00 65.52 74.65 96.00 95.92 98.33 69.37 70.00 74.74GraphCL+GT73.8074.12 82.77 88.50 88.92 91.25 63.50 66.06 68.04 94.39 93.62 96.97 75.00 78.45 75.05SimGRACE+GAT76.8577.48 83.37 90.50 91.00 91.56 56.59 65.47 67.77 84.50 84.73 89.69 72.50 68.21 81.97SimGRACE+GCN77.2076.39 83.13 83.50 84.21 93.22 58.00 55.81 56.93 95.00 94.50 98.03 77.50 75.71 87.53SimGRACE+GT77.4078.11 82.95 87.50 87.05 91.85 66.00 69.95 70.03 79.00 73.42 97.58 70.50 73.30 74.22",
  "prompt": "GraphCL+GAT76.5077.26 82.99 88.00 90.52 91.82 57.84 67.02 75.33 80.01 75.62 97.96 77.50 78.26 83.02GraphCL+GCN79.2079.62 85.29 88.50 91.59 91.43 56.00 68.57 78.82 96.50 96.37 98.70 72.50 72.64 79.57GraphCL+GT75.0076.00 83.36 91.00 91.00 93.29 65.50 66.08 68.86 95.50 95.43 97.56 76.50 79.11 76.00SimGRACE+GAT76.9578.51 83.55 93.00 93.14 92.44 57.63 66.64 69.43 95.50 95.43 97.56 73.00 74.04 81.89SimGRACE+GCN77.8576.57 83.79 90.00 89.47 94.87 59.50 55.97 59.46 95.00 95.24 98.42 78.00 78.22 87.66SimGRACE+GT78.7579.53 85.03 91.00 91.26 95.62 69.50 71.43 70.75 86.00 83.72 98.24 73.00 73.79 76.64",
  "appr. Label Ratio of our 100-shot setting 25% 18% 1.7% 7.3% 1.5%": "4Why It Works?Comparison to Prior Work:While GPPT [Sun et al.,2022a] represents an early attempt at graph prompting, fo-cusing on edge prediction for node classification, our methodextends this concept significantly. Unlike GPPT, our frame-work is more versatile, accommodating a broader range ofgraph tasks and pre-training strategies beyond edge predic-tion, including advanced graph-level strategies like GraphCL[You et al., 2020] and SimGRACE [Xia et al., 2022].Flexibility: Our approach introduces the concept of a promptgraph comprising multiple tokens with learnable structures,offering a more nuanced and flexible method for graph ma-nipulation to better align with various pre-training strategies.We demonstrate that this flexibility allows for more effectiveadaptations of the graph structure to suit different tasks, re-ducing the error margin in representing manipulated graphs.The nature of prompting is to manipulate the input data tomatch the pretext. Therefore, the flexibility of data opera-tions is the bottleneck of prompting performance. Let g beany graph-level transformation such as changing node fea-tures, adding or removing edges/subgraphs etc., and be the frozen pre-trained graph model. For any graph G withadjacency matrix A and node feature matrix X, Fang et al.[Fang et al., 2022] have proved that we can always learn anappropriate prompt token p making the following equationstand:",
  "(A, X + p) = (g(A, X)) + Op(2)": "This means we can learn an appropriate token applied to theoriginal graph to imitate any graph manipulation. Here Opdenotes the error bound between the manipulated graph andthe prompting graph w.r.t. their representations from the pre-trained graph model. This error bound is related to some non-linear layers of the model (unchangeable) and the quality ofthe learned prompt (changeable), which is promising to befurther narrowed down by a more advanced prompt scheme.In this paper, we extend the standalone token to a promptgraph that has multiple prompt tokens with learnable innerstructures. Unlike the indiscriminate inserting in Equation (2) (X + p means the prompt token should be added to everynode of the original graph), the inserting pattern of our pro-posed prompt graph is highly customized. Let (G, Gp) de-note the inserting pattern defined in section 3; G is the origi-nal graph, and Gp is the prompt graph, then we can learn anoptimal prompt graph Gp to extend Equation (2) as follows:",
  "Evaluation": "We compare our methods with other approaches on five pub-lic datasets including Cora [Welling and Kipf, 2016], Cite-Seer [Welling and Kipf, 2016], Reddit [Hamilton et al.,2017], Amazon [Shchur et al., 2018], and Pubmed [Wellingand Kipf, 2016].We compare our method with super-vised, pre-training plus fine-tuning, and other prompt meth-ods across node, edge, and graph-level tasks. Key findingsinclude our methods superior performance in multi-task set-tings, showcasing notable improvements over existing meth-ods.Multi-Task Performance Our study evaluates the perfor-mance of our prompt-based method across node-level, edge-level, and graph-level tasks in few-shot learning settings,comparing it against supervised methods and pre-training ap-proaches.Results in show that supervised meth-ods struggle due to limited annotations available in few-shot scenarios, while pre-training methods offer better per-formance by leveraging prior knowledge. However, select-ing and fine-tuning a pre-trained model for a specific task iseffort-intensive and not always transferable to other tasks.Our method, by incorporating prompts, shows compati-bility improvements across all task levels, achieving perfor-mance boosts ranging from 1.10% to 8.81% for node-leveltasks, 1.28% to 12.26% for edge-level tasks, and 0.14% to 10.77% for graph-level tasks. Notably, our approach under amore challenging setting (with only 100 labeled samples perclass) still outperforms the GPPT model, which uses a 30% to50% label ratio, indicating superior efficiency and adaptabil-ity of our method in few-shot learning contexts across variousgraph tasks. Please see the original paper for more task per-formance like edge-level and graph-level tasks.",
  "node levelhard40.5011.8529.48fine-tune46.0054.2437.26prompt59.5068.7355.90": "Transferability: Our method demonstrates enhanced adapt-ability, outperforming both hard transfer and fine-tuning ap-proaches in transferring models to new tasks (as shown in) and domains (as shown in ). This is particu-larly evident in tasks requiring significant adaptation, whereour prompting framework facilitates more effective knowl-edge transfer.Graph Transformation Flexibility:Our approach ef-fectively minimizes the error in representing manipulatedgraphs, demonstrating its capacity to support a wide rangeof graph transformations. This is further illustrated by visu-alizations that highlight the improved graph representationsachieved through our prompting method.For more experiments, please see in our original paper.",
  "Acc46.0087.5088.0050.0091.0095.50F162.7689.1188.1210.0093.9095.60AUC54.2386.3394.9990.8591.4798.47": "6A Bigger Picture of Graph PromptsIn the rapidly evolving field of Artificial General Intelligence(AGI) [Li et al., 2024b], significant advancements have beenmade, especially with applications like ChatGPT in NLP andMidjourney in Computer Vision (CV), greatly enhancing ourefficiency and creativity. Yet, the application of AGI in graphdata analysis remains nascent, despite its potential to revo-lutionize areas such as drug design and battery developmentdue to challenges in harmonizing information across modali-ties, domains, and tasks.",
  "[Shen et al., 2021] Zheyan Shen, Jiashuo Liu, Yue He,Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. To-wards out-of-distribution generalization: A survey. arXivpreprint arXiv:2108.13624, 2021": "[Sun et al., 2021a] Xiangguo Sun, Hongzhi Yin, Bo Liu,HongxuChen,JiuxinCao,YingxiaShao,andNguyen Quoc Viet Hung.Heterogeneous hypergraphembedding for graph classification.In Proceedings ofthe 14th acm international conference on web search anddata mining, pages 725733, 2021. [Sun et al., 2021b] Xiangguo Sun, Hongzhi Yin, Bo Liu,Hongxu Chen, Qing Meng, Wang Han, and Jiuxin Cao.Multi-level hyperedge distillation for social linking pre-diction on sparsely observed networks. In Proceedings ofthe Web Conference 2021, pages 29342945, 2021. [Sun et al., 2022a] Mingchen Sun, Kaixiong Zhou, Xin He,Ying Wang, and Xin Wang. GPPT: Graph pre-training andprompt tuning to generalize graph neural networks. In Pro-ceedings of the 28th ACM SIGKDD Conference on Knowl-edge Discovery and Data Mining, pages 17171727, 2022. [Sun et al., 2022b] Xiangguo Sun, Bo Liu, Liya Ai, DanniLiu, Qing Meng, and Jiuxin Cao. In your eyes: Modalitydisentangling for personality analysis in short video. IEEETransactions on Computational Social Systems, 2022. [Sun et al., 2022c] Xiangguo Sun, Hongzhi Yin, Bo Liu,Qing Meng, Jiuxin Cao, Alexander Zhou, and HongxuChen. Structure learning via meta-hyperedge for dynamicrumor detection. IEEE Transactions on Knowledge andData Engineering, 2022. [Sun et al., 2023a] Xiangguo Sun, Hong Cheng, Hang Dong,Bo Qiao, Si Qin, and Qingwei Lin. Counter-empirical at-tacking based on adversarial reinforcement learning fortime-relevant scoring system.IEEE Transactions onKnowledge and Data Engineering, 2023. [Sun et al., 2023b] Xiangguo Sun, Hong Cheng, Jia Li,Bo Liu, and Jihong Guan. All in one: Multi-task prompt-ing for graph neural networks. In Proceedings of the 29thACM SIGKDD Conference on Knowledge Discovery andData Mining, pages 21202131, 2023. [Sun et al., 2023c] Xiangguo Sun, Hong Cheng, Bo Liu, JiaLi, Hongyang Chen, Guandong Xu, and Hongzhi Yin.Self-supervised hypergraph representation learning for so-ciological analysis. IEEE Transactions on Knowledge andData Engineering, 2023.",
  "[Sun et al., 2023d] Xiangguo Sun,Jiawen Zhang,XixiWu, Hong Cheng, Yun Xiong, and Jia Li.Graphprompt learning: A comprehensive survey and beyond.arXiv:2311.16534, 2023": "[Welling and Kipf, 2016] Max Welling and Thomas N Kipf.Semi-supervised classification with graph convolutionalnetworks. In J. International Conference on Learning Rep-resentations (ICLR 2017), 2016.[Xia et al., 2022] Jun Xia, Lirong Wu, Jintao Chen, BozhenHu, and Stan Z Li. Simgrace: A simple framework forgraph contrastive learning without data augmentation. InProceedings of the ACM Web Conference 2022, pages10701079, 2022.[Yang et al., 2023] Haoran Yang,Hongxu Chen,SixiaoZhang, Xiangguo Sun, Qian Li, Xiangyu Zhao, and Guan-dong Xu. Generating counterfactual hard negative samplesfor graph contrastive learning. In Proceedings of the ACMWeb Conference 2023, pages 621629, 2023.[You et al., 2020] Yuning You, Tianlong Chen, Yongduo Sui,Ting Chen, Zhangyang Wang, and Yang Shen. Graph con-trastive learning with augmentations. Advances in NeuralInformation Processing Systems, 33:58125823, 2020.[Zhang et al., 2022a] Sixiao Zhang, Hongxu Chen, Xiang-guo Sun, Yicong Li, and Guandong Xu.Unsuper-vised graph poisoning attack via contrastive loss back-propagation. In Proceedings of the ACM Web Conference2022, pages 13221330, 2022.[Zhang et al., 2022b] Sixiao Zhang, Hongxu Chen, HaoranYang, Xiangguo Sun, Philip S Yu, and Guandong Xu.Graph masked autoencoders with transformers.arXivpreprint arXiv:2202.08391, 2022.[Zhao et al., 2024] Haihong Zhao, Aochuan Chen, Xiang-guo Sun, Hong Cheng, and Jia Li. All in one and one forall: A simple yet effective method towards cross-domaingraph pretraining. 2024."
}