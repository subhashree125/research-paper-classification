{
  "ABSTRACT": "Algorithmic trading systems are often completely automated, anddeep learning is increasingly receiving attention in this domain.Nonetheless, little is known about the robustness properties of thesemodels. We study valuation models for algorithmic trading fromthe perspective of adversarial machine learning. We introduce newattacks specific to this domain with size constraints that minimizeattack costs. We further discuss how these attacks can be used asan analysis tool to study and evaluate the robustness properties offinancial models. Finally, we investigate the feasibility of realisticadversarial attacks in which an adversarial trader fools automatedtrading systems into making inaccurate predictions.",
  "INTRODUCTION": "Machine learning serves an increasingly large role in financial ap-plications. Recent trends have seen finance professionals rely onautomated machine learning systems for algorithmic trading ,as robo-advisers that allocate investments and rebalance portfolios, in fraud detection systems that identify illicit transactions, for risk models that approve/deny loans , and in high-frequency trading (HFT) systems that make decisions on timescalesthat cannot be checked by humans . With the wide-spread use of such models, it is increasingly important to have toolsfor analyzing their security and reliability.In the mainstream machine learning literature, it is widely knownthat many machine learning models are susceptible to adversarial",
  "Both authors contributed equally to this research": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from 21, August, 2021, virtual 2021 Association for Computing Machinery.ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00 attacks in which small but deliberately chosen perturbations tomodel inputs result in dramatic changes to model outputs . Ithas already been demonstrated that adversarial attacks can be usedto change someones identity in a facial recognition system ,bypass copyright detection systems , and interfere with objectdetectors . In this work, we investigate adversarial attacks onmodels for algorithmic and high-frequency trading.Trading bots historically rely on simple ML models and increas-ingly leverage the cutting-edge performance of neural networks. Security and reliability issues are particularly relevantto HFT systems, as actions are determined and executed in ex-tremely short periods. These short time horizons make it impos-sible for human intervention to prevent deleterious behavior, andit is known that unstable behaviors and security vulnerabilitieshave contributed to major market events (e.g., the 2010 flash crash).We focus on adversarial attacks on the stock price predictionsystems employed by trading bots. In particular, we examine adver-sarial attacks from the following two perspectives.Adversarial analysis of reliability and stability. After a priceprediction model has been trained, it is important to understandthe reliability of the model and any unseen instabilities or volatilebehaviors it may have. A popular analysis method is backtesting,wherein one feeds recent historical stock market data into a modeland examines its outputs. However, models may have extreme be-haviors and instabilities that arise during deployment that cannotbe observed on historical data. This is especially true when com-plex and uninterpretable neural models are used . Furthermore,market conditions can change rapidly, resulting in a domain shiftthat degrades model performance .We propose the use of adversarial attacks to reveal the most ex-treme behaviors a model may have. Our adversarial attacks generatesynthetic market conditions under which models behave poorlyand can be used to determine whether instabilities exist in whichsmall changes to input data result in extreme or undesirable behav-ior. They are in part a tool to interrogate the reliability of a modelbefore deployment or during postmortem analysis after a majorevent.Adversarial attacks as a vulnerability. We assess the poten-tial for adversarial attacks to be used maliciously to manipulatetrading bots. One unique factor that contributes to the potentialvulnerability of stock price models is how directly the data canbe perturbed. In mainstream adversarial literature, attackers oftencannot directly control model inputs; adversaries modify a physi-cal object (e.g., a stop sign) with the hope that their perturbation",
  "KDD 21, August, 2021, virtualGoldblum, et al": "attacks with low relative size, on Nvidia. As a result of this quanti-zation effect, we are almost unable to impair models on Nvidia. Thisobservation leads us to believe that price prediction on assets withthis property is more robust in general (at least using the class ofSWA-based models studies in our setting). In the ensuing sections,we focus on Ford and General Electric stocks, which have relativelyhigher volume and lower cost per share.",
  "BACKGROUND2.1Machine Learning for High-FrequencyTrading": "High-frequency trading bots can profit from extremely short-termarbitrage opportunities in which a stock price is predicted fromimmediately available information, or by reducing the market im-pact of a large transaction block by breaking it into many smalltransactions over a trading day . These high-frequencytrading systems contain a complex pipeline that includes price pre-diction and execution logic. Price prediction models, or valuations,are a fundamental part of any trading system since they forecast thefuture value of equities and quantify uncertainty. Execution logicconsists of proprietary buy/sell rules that account for firm-specificfactors including transaction costs, price-time priority of orders,latency, and regulatory constraints .Since the ability to anticipate trends is the core of algorithmictrading, we focus on price prediction and its vulnerability to attack.Price predictors use various streams of time-series data to predictthe future value of assets. Traditional systems use linear models,such as autoregressive moving-average (ARMA) and support-vectormachine (SVM), because models like these are fast for both trainingand inference .As hardware capabilities increase, training and inference accel-erate with advances in GPU, FPGA, and ASIC technologies. Largemodels, like neural networks, have gathered interest as computa-tions that were once intractable on a short timescale are becomingfeasible . Recent works use LSTM and temporal convolutionalnetworks to predict future prices with short time horizons . Common trading strategies, typical of technical analysis, rely onmean-reversion or momentum-following algorithms to identify anoversold or overbought asset. Mean-reversion methods assume thatfluctuations in stock prices will eventually relax to their correspond-ing long-term average, whereas momentum-following methods aimat identifying and following persistent trends in asset prices. Sim-ple trading algorithms identify an indicator of either strategy andact when prices cross a predefined threshold, thus signaling for apotentially profitable trade over the oversold or overbought asset. We use this type of thresholding for our trading systems.More details on our setup can be found in .2.Historically, traders have engaged in spoofing in which an adver-sary places orders on either side of the best price to fake supply ordemand, often with the intent of canceling the orders before exe-cution . Spoofing is now illegal in the United States under theDodd-Frank Act of 2010. These orders placed by a trader engagingin spoofing can be thought of as naive hand-crafted adversarialperturbations. In this work, we use techniques from mathematicaloptimization to automate the construction of effective and efficientperturbations. Our perturbations, crafted with optimization tech-niques, may be less detectable than handcrafted versions and thusmay present a new problem for market regulators.",
  "Adversarial Attacks": "While it is known that neural networks are vulnerable to small per-turbations to inputs that dramatically change a networks output,the most popular setting for studying these adversarial attacks todate is image classification. Adversarial attacks in this domain per-turb an image to fool a classifier while constraining the perturbationto be small in some norm.While classical adversarial perturbations are designed for a singleimage, universal adversarial perturbations are crafted to fool theclassifier when applied to nearly any image . Other domains,such as natural language and graph structured data, have attractedthe attention of adversarial attacks, but these attacks are impractical. For example, in NLP, adversarial text may be nonsensical,and in graph structured data attacks are weak.",
  "The Order Book": "The order book keeps track of buy orders and sell orders for afinancial asset on an exchange. We use limit order book data fromthe Nasdaq Stock Exchange. Limit orders are orders in which anagent either offers to buy shares of a stock at or below a specifiedprice or offers to sell shares of a stock at or above a specified price.Since agents can place, for example, buy orders at a low price, limitorders may not execute immediately and can remain in the orderbook for a long time. When the best buy order price (the highestprice at which someone will buy) matches or exceeds the best sellorder price (the lowest price at which someone is willing to sell)orders are filled and leave the book.At any given time, the order book contains prices and theircorresponding sizes. Size refers to the total number of shares beingoffered or demanded at a particular price. The size entries for a givenprice level may contain orders of many agents, and these orders aretypically filled in a first-in-first-out manner. More complex rulescan come into play, for example if orders require tie breaking, or if",
  "BUILDING VALUATION MODELS": "In this section, we describe the simple valuation models that weattack. Our models digest a time series of size-weighted averageprice levels over a one minute interval, and make a prediction abouta stock price ten seconds in the future. The models used here werechosen for their simplicity they are meant to form a testbed foradversarial attacks and not to compete with the proprietary stateof the art. Still, we verify that our models learn from patterns inthe data and outperform simple baselines.",
  "Data": "We use centisecond-resolution data from September and October2019, and we only use the first hour of trading each day since thisperiod contains the largest volume of activity. We split one monthof data per asset, using the first 16 days for training and the lastfour for testing.1 We choose well-known stocks with high volatilityrelative to order book thickness: Ford (F), General Electric (GE),and Nvidia (NVDA). Order book data was furnished by LOBSTER,a financial data aggregator that makes historical order book datafor Nasdaq assets available for academic use .Consider that each row of the order book contains the ten bestbuy prices and their corresponding sizes, {(1 ,1 ), ..., (10,10)},as well as the ten best sell prices and their corresponding sizes,{(1,1 ), ..., (10,10)}. These rows are each a snapshot of the bookat a particular time. We process this data by creating the size-weighted average (SWA),",
  "=1 + 10=1 10=1 + 10=1 ,": "for each row. Movement in the size-weighted average may representa shift in either or both price and size. The SWA is a univariatesurrogate for the price of an asset.After computing the SWA of the order book data, inputs tothe models are 60 second time-series of this one-dimensional data.Since we use order book snapshots at intervals of 0.01 seconds, asingle input contains 6, 000 SWA entries. See for a visualdepiction.We focus on three-class classification models for concreteness,but the vulnerabilities we highlight may be artifacts of training onorder book data in general. The model classifies an equity as likelyto increase in price above a threshold, decrease below a threshold,or remain between the thresholds. Thresholds are chosen to besymmetric around the origin, and so that the default class (nosignificant change) contains one third of all events, making theclassification problem approximately balanced.",
  ": This sample of the SWA curve for GE data showsan example of an input snippet. The label for this input isdetermined by the change in price from the right end of thegreen snippet to the green dot": "each of width 8,000. The LSTM models have 3 layers and a hidden-layer size of 100.We train each model with cross-entropy loss over the threeclasses. Since our data can be sampled by taking any 60 secondsnippet from the first hour of any of the trading days in our trainingset, we randomly choose a batch of 60-second snippets from all ofthe training data and perform one iteration of SGD on this batchbefore resampling. These batches range in size from 2,000 to 3,000data points depending on the model. Further details can be foundin Appendix 7.",
  "Comparison to Baselines": "We verify that our models are indeed learning from the data bychecking that they exceed simple baselines. There are two naturalbaselines for this problem: a random guess and a strategy thatalways predicts the label most commonly associated with thatstock in the training data.2 The latter baseline always performedworse than a random guess on the test data, so the most naturalperformance baseline accuracy is 33.33%. shows accuracymeasurements which should be read with this in mind. Note thatprice prediction is a difficult task, and profitable valuation modelstypically achieve performance only a few percentage points betterthan baselines .Since we have a large quantity of data, we randomly sample10,000 snippets from the test set in order to measure accuracy.We report confidence intervals in . All test accuracy mea-surements in this work have standard error bounded above by100",
  "Rules for Perturbing the Order Book": "A number of unique complexities arise when crafting adversarialattacks on order book data. We consider an adversary that mayplace orders at the ten best buy and sell prices (orders at extremeprice levels are typically discarded by analysts as they are unlikelyto be executed). Equivalently, we consider perturbations to the sizeentries, but only to whole numbers, in the raw order book data.Note that using the SWA as input and only perturbing size entriesprevents the adversary from making too great an impact on the dataseen by the model. Since the SWA lacks the informational contentof raw order book data, an attacker, without changing the priceentries, can at most perturb the SWA to the maximum or minimumprice in the ten best buy/sell orders at a given time-stamp. Wewill see that this small range gives the attacker enough freedom toinflict damage.We now describe a simple differentiable trading simulation whichenables the adversary to anticipate the impact of their orders on theorder book snapshots digested by their victims. Both the algorithmand its implementation need to be carefully considered to ensurethat they are reasonably fast and differentialble.Propagating orders through the book. Consider the sequenceof order book snapshots used for size-weighted average, {x}, andthe sequence of adversarial orders (on a per-row basis), {a}. Wecannot simply add these two quantities to compute the perturbedorder book since the adversarial orders may remain on the bookafter being placed. On the other hand, we cannot blindly propagatethe orders to the end of the snippet because transactions may occurin which adversarial orders are executed, removed from the book,and therefore excluded from subsequent snapshots. Thus, denotethe propagation function that accumulates adversarial orders, ac-counts for transactions, and returns the sequence of perturbationsto the snapshots {}, by {x } ({a}). The function {x } takes asinput a sequence of orders, which indicates an integer number ofshares to add to the order book at a particular time (or row in theraw data). This quantity is added to all subsequent rows in the datathat include orders at that price, until any transaction occurs atthat price, at which point our model considers the order filled. Wemake this assumption about limit order book execution as is doneby commercial backtesting software . The output is a pertur-bation {}. Finally, the input to a model is SWA({x + }). Thepropagation function can be thought of as a basic differentiable backtesting engine that we will use to compute gradients throughthe adversarys interactions with the market.The following complexity arises when implementing propaga-tion. The order book is organized by price level, or distance fromthe best price. Since the best price can change, this format doesnot lend itself to propagating orders, since each order is placed ata fixed dollar value. To add size at a particular price then requiresthat different entries in each row must be modified. If price levelsdo not shift in a snippet, then one column of data organized byprice level corresponds to one column of data organized by abso-lute price, so an adversarys orders can be added column-wise tothe latter representation. However, price levels shift frequently, sotranslating between these two views of the order book data in a fast,parallelized fashion is non-trivial. At the same time, the adversarywill compute numerous gradient steps during an attack and musttranslate between the two data representations during each step.This translation task must be handled efficiently by the propagationfunction.",
  "Quantifying Perturbation Size": "In order to determine just how sensitive our models are, we mustquantify the size of the perturbations being placed. We considerthree possible metrics: cost, capital required, and relative size.We compute cost to the attacker as the change in the total valueof his or her assets from the start of the snippet to the end. Sinceassets include both cash and stocks, cost accounts for changes inthe price of stocks purchased when a buy order is filled (and weexclude transaction fees). We operate under the assumption thatall attack orders are transacted when other orders at that priceare transacted. Capital required is the hypothetical total cost ofexecuting all orders placed by the attacker. This quantity is thetotal dollar value of orders in the perturbation. An agent wouldneed to have this much capital to place the orders. Relative size of aperturbation refers to the size (number of shares) of the adversaryspropagated perturbation as a percentage of total size on the bookduring the snippet.Since transactions occur infrequently at this time-scale, andprices go up roughly as much as they go down, the average costacross attacks is less than $1.00 for each asset-model combination.Therefore, our algorithms limit perturbation size in terms of cap-ital required. We denote the capital required for an attack {a}as ({a}). Additionally, we measure cost and relative size in ourexperiments.",
  "Robustness to Random Noise": "Before describing our attack, we establish a baseline by randomlyinserting orders instead of using optimization. We propagate theperturbations through the order book with a propagation function. We compare our attacks to this baseline to demonstrate that theoptimization process is actually responsible for impairing classifi-cation. In fact, random attacks with a high budget barely impairclassification at all. See where we denote test accuracy onthe test data, changes to test accuracy resulting from random per-turbations, and changes to test accuracy resulting from adversarialperturbations by Atest, Arand, and Aadv, respectively. We also re-port the average capital the adversary must have before making aneffective perturbation, and the average relative size of successfulattacks, both of which are computed as described in .2.The average cost to the attacker per asset is less than $1.00. Allrandom attacks have a budget over $2,000,000, a sum far higherthan any optimization-based attack we consider.",
  "The attacker runs through this iterative method with a small stepsize until either the model no longer predicts the correct label, , or( ({a})) exceeds the capital constraint, , where = x + is": "a floor operator that rounds size entry to the greatest integer lessthan or equal to + (i.e. 0.3(0.8) = 1). When the attack terminates,we return the adversarial perturbation, ({a}). We choose and to be $100,000 and 0.95, respectively, in our experiments. See.2 for a discussion on choice of attack hyperparameters. Note 1. In a standard PGD attack, attacks are randomly initial-ized in an ball, and perturbations are projected onto the ball eachiteration. In contrast to bounded attacks, there is no natural dis-tribution for random initialization within the set, {{a} : (a) }.Moreover, there is no unique projection onto this set. Thus, we insteadopt for a randomized learning rate in order to inject randomness intothe optimization procedure . depicts the effect of adversarial orders crafted in thismanner on price prediction. Empirically, these trading models arenot robust to low-budget perturbations. Moreover, gradient-basedattacks are far more effective at fooling our models than randomlyplaced orders with far higher budget, indicating that our valuationmodels are robust to noise but not to cleverly placed orders. MLPs,while generally of higher natural accuracy than linear models, arealso significantly less robust. In all experiments, MLPs achievelower robust accuracy, even with much smaller perturbations. Onthe other hand, LSTM models seemingly employ gradient maskingas they are far more difficult to attack using gradient information.To further confirm the gradient masking hypothesis, we see belowin that LSTM models are the most easily fooled modelsby transferred universal perturbations.",
  "The Effect of High Price-to-Size Ratio": "We find that valuation models for equities with high price per shareand low trading volume were difficult to attack. For such stocks, theincrements by which an adversary can perturb the SWA are coarse,so gradient-based methods with rounding are ineffective. For ex-ample, in the data we use, Nvidia stock costs around $180/sharecompared to about $9/share for Ford stock during the date rangewe considered. This fact, combined with roughly one fifth of thetrading volume, makes it difficult to perform low cost attacks, or",
  "UNIVERSAL TRANSFER ATTACK: AFEASIBLE THREAT MODEL": "The adversarial attacks discussed above provide a useful basis foranalyzing model robustness but do not pose any real security threatbecause of three unrealistic assumptions. First, the attacker is per-forming an untargeted attack, meaning that the attacker knows theclass that a snippet belongs to (i.e., whether it will go up, down,or remain), and is only trying to cause the prediction to change.Second, attacks are crafted while looking at every row of the orderbook simultaneously. For this reason, the attacker may use knowl-edge about the end of a snippet when deciding what orders to placein the book at the beginning of the snippet. Third, the attackermust know their victims architecture and parameters. In short,the strategies displayed above require the attacker to know thefuture and their opponents proprietary technology an entirelyunrealistic scenario.In this section, we consider making universal perturbations. Wecraft a single attack that works on a large number of historicaltraining snippets with the hope that it transfers to unseen testingsnippets. These attacks can be deployed in real time without know-ing the future. Furthermore, we use targeted attacks, so that theattacker may anticipate the behavior of the victim. In order to addeven more realism, we use transfer attacks in which the attackerdoes not know the parameters of the model they are attacking. Inthis situation, we train surrogate models and use universal pertur-bations generated on the surrogates to attack the victim.We craft universal perturbations by solving",
  "L (SWA({x + }), + ({}),": "where L is cross-entropy loss, {} = {x }({a}), {} is a setof order book snapshots in the training data, is a target label,and ({}) is a penalty term which may be used to encourage theperturbation to require, for example, less size relative to the sizeon the book. The resulting perturbation {a} represents the size oforders placed by the adversary at particular price levels and timestamps. That is, the adversary must only consider how many levelsa price is from the best offer rather than specific prices or dollaramounts. We solve this problem with SGD by sampling training dataand minimizing loss. See .2 below for hyperparameters. Weapply this pre-computed sequence of adversarial orders to test databy propagating orders through the order book for each individualinput sequence and adding to the original snippet. We measure andreport the success rate of labeling correctly classified inputs fromoutside the target class with the target label, and we refer to this asfooling the model.We find that universal adversarial perturbations in this settingwork well. In particular, we were able to find universal perturba-tions that are small relative to the order book and yet fool the model a significant percentage of the time. Moreover, we find that theseattacks transfer between models and that our relative size penaltyis effective at reducing the budget of an attacker while maintaininga high fool rate. See for targeted universal perturbationtransfer attack results. We denote the universal perturbations com-puted on the linear model and on the MLP by Ulinear and UMLP,respectively. We craft universal adversarial perturbations on a lin-ear model and an MLP and assess the performance of three modelsunder these attacks. Both of these attacks were computed with apenalty on relative size.These targeted universal perturbations are prototypical patternsfor convincing a model to predict a particular SWA movement. Forexample, to force the victim to predict a downwards trend, theadversary places sell orders followed by buy orders and finallymore sell orders to feign downwards momentum in the SWA. Seethe visualization of a universal perturbation in , which ishelpful for interpreting why our models make the decisions theydo.",
  "PATTERNS IN ADVERSARIAL ORDERS": "We observe patterns in adversarial attacks that can help explainthe behavior of classifiers. The non-universal attacks on particu-lar inputs highlight the vulnerability of the valuation models. Forexample, in , the perturbation is so small compared to thesize on the book that a human would not distinguish the attackedsignal from the clean one. The attack, however, is concentrated onthe fringes of the order book, much like a spoofing attack. shows a case in which a universal adversary has learned an in-terpretable perturbation in which it creates a local minimum inthe size-weighted average by alternately placing perturbations onopposite sides of the book. The perturbation on the top of was computed without constraint, and when transferred to testdata and a different model, it fooled the victim on 157 inputs (out of341 correctly classified) and accounts for a relative size on the bookof 3.8%. The perturbation on the bottom is the result of the sameprocess with an added penalty on relative size. The penalized attackshows a sparser perturbation with relative size 0.9% while causingmisclassification of almost the same number of inputs (123).Universal perturbations bring to light two important featuresof these price predictors. The first is that targeted universal per-turbations, which are generally expected to be less effective, havea major impact on the performance of these models. The modelsare vulnerable even to these weak attacks. Second, the targeteduniversal attacks expose interpretable model behavior. In , we see that the attacker creates local extrema in the order bookwhich cause the model to anticipate mean reversion.",
  "Size": ": The unperturbed and perturbed order books (top left and top right, respectively). The bottom two images showthe unpropagated and propagated adversarial perturbations (left and right, respectively). In this instance, the addition ofthe bottom right image to the clean signal in the top left yields the perturbed input in the top right. Note that the differencebetween the perturbed and unperturbed signals is not noticeable here, where the model was fooled into incorrectly classifyingthe SWA as going up.",
  "HYPERPARAMETERS ANDIMPLEMENTATION DETAILS7.1Training": "For each asset, we train linear predictors, MLPs, and LSTMs. Thelinear models are trained for 5,000 iterations of SGD with a batchsize of 2,500 and an initial learning rate of 0.5. The learning rate isthen decreased by a factor of 0.5 after iterations 50, 500, 1,000, 2,000,3,000, 4,000 and 4,500. We conducted an architecture search, whichled to MLP models of 4 hidden layers, each of 8,000 neurons. TheMLPs are trained for 3,000 iterations of SGD with a batch size of3,000 and an initial learning rate of 0.01. The learning rate decreasesby a factor of 0.5 after iterations 10, 20, 100, 200, 400, 500, 1,000,2,000, and 2,500. Our architecture search for LSTM models led toLSTMs with 3 layers and a hidden size of 100 neurons. The LSTMsare trained for 50 iterations of ADAM with a batch size of 2,000and an initial learning rate of 0.01. The learning rate decreases by afactor of 0.5 after iterations 25 and 40.",
  "Attacks": "When conducting non-universal attacks, we use 200 steps and 0 =40, so that U(0, 40). The attack terminates on a given sampleas soon as the sample is misclassified, so that we rarely actually runthrough all 200 steps. The run-time of these attacks is dominated by the gradient computation, and the worst-case time complexityof 200 gradient computations per sample is rarely achieved.For universal attacks, we adopt a similar procedure to . Westart by initializing an attack, {a} to be zeros. Then, we samplebatches of 50 training samples and perform 5 gradient descentsteps on each one with a random learning rate, U(0, 10). Next,we average the resulting steps and add 1.5 times this average to{a}, clipping all negative values. We use 200 batches to createone universal adversarial perturbation. In order to create sparseperturbations, in some experiments, we use a penalty term whichis 5 times the relative size of the perturbation.In order to propagate perturbations through the order book in adifferentiable way, we must store data in two forms: raw order booksnapshots and a form which stores size entries corresponding to thesame price together. This doubles the space complexity but allowsus to parallelize propagation. During propagation, each entry istouched exactly twice, so this process is O().",
  ": Two targeted universal adversarial attacks com-puted on the same model (MLP trained on Ford data)": "in this setting than traditional linear valuation models, we findthem to be less robust. We further notice that the same adversarialpatterns that fool one model also fool others and that these patternsare highly interpretable to humans. The transferability of theseattacks, combined with their ability to be effective with a smallattack budget, suggests that they could possibly be leveraged by amalicious agent possessing only limited knowledge."
}