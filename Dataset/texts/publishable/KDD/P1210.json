{
  "ABSTRACT": "One of the most fundamental tasks in data science is to assist a userwith unknown preferences in finding high-utility tuples within alarge database. To accurately elicit the unknown user preferences, awidely-adopted way is by asking the user to compare pairs of tuples.In this paper, we study the problem of identifying one or more high-utility tuples by adaptively receiving user input on a minimumnumber of pairwise comparisons. We devise a single-pass streamingalgorithm, which processes each tuple in the stream at most once,while ensuring that the memory size and the number of requestedcomparisons are in the worst case logarithmic in , where is thenumber of all tuples. An important variant of the problem, whichcan help to reduce human error in comparisons, is to allow usersto declare ties when confronted with pairs of tuples of nearly equalutility. We show that the theoretical guarantees of our method canbe maintained for this important problem variant. In addition, weshow how to enhance existing pruning techniques in the literatureby leveraging powerful tools from mathematical programming.Finally, we systematically evaluate all proposed algorithms overboth synthetic and real-life datasets, examine their scalability, anddemonstrate their superior performance over existing methods.",
  "This work was done while the author was with KTH Royal Institute of Technology": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 23, August 610, 2023, Long Beach, CA, USA 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00",
  "INTRODUCTION": "One of the most fundamental tasks in data science is to assist a userwith unknown preferences in finding high-utility tuples within alarge database. Such a task can be used, for example, for findingrelevant papers in scientific literature, or recommending favoritemovies to a user. However, utility of tuples is highly personalized.One persons trash is another persons treasure, as the saying goes.Thus, a prerequisite to accomplishing this task is to efficiently andaccurately elicit user preferences.It has long been known, both from studies in psychology as well as from personal experience, that humans are better at per-forming relative comparisons than absolute assessments. For instance,it is typically easy for a user to select a favorite movie betweentwo given movies, while it is difficult to score the exact utility of agiven movie. This fact has been used in many applications, such asclassification , ranking , and clustering .In this paper we leverage the observation that humans are betterat comparing rather than scoring information items, and use rela-tive comparisons to facilitate preference learning and help usersfind relevant tuples in an interactive fashion, i.e., by adaptivelyasking users to compare pairs of tuples. To cope with the issueof information overload, it is usually not necessary to identify allrelevant tuples for a user. Instead, if there exists a small set ofhigh-utility tuples in the database, a sensible goal is to identify atleast one high-utility tuple by making a minimum number of com-parisons. In particular, assuming that a user acts as an oracle, thenumber of requested comparisons, which measures the efficiencyof preference learning, is known as query complexity.More specifically, in this paper we focus on the following setting.We consider a database consisting of tuples, each representedas a point in R. User preference is modeled by an unknown linearfunction on the numerical attributes of the data tuples. Namely, weassume that a user is associated with an unknown utility vector u R, and the utility of a tuple x R for that user is defined to be",
  "KDD 23, August 610, 2023, Long Beach, CA, USAGuangyi Zhang, Nikolaj Tatti, and Aristides Gionis": "interest. Qian et al. iteratively select a hyperplane (i.e., pair)whose normal vector is the most orthogonal to the current estimateof u. Wang et al. maintain disjoint regions of u over R, onefor each tuple, where a tuple is the best if u is located within itsregion. Then, they iteratively select a hyperplane that separatesthe remaining regions as evenly as possible. However, these greedystrategies are highly computationally expensive, and do not haveany theoretical guarantee.Compared to aforementioned existing work, our proposed al-gorithm makes minimal assumptions, is scalable, and enjoys thestrongest worst-case guarantee. It is worth mentioning that existingresearch often assumes that increasing any tuple attribute alwaysimproves utility, by requiring R+ and u R+ .We do not make such an assumption in this paper. Active learning. The IRM problem can be viewed as a specialhighly-imbalanced linear classification problem. Consider a binaryclassification instance, where the top tuple is the only one with apositive label and the rest are all negative. Such labeling is alwaysrealizable by a (non-homogeneous) linear hyperplane, e.g., ={x R : u x = u x } for any sufficiently small 0. Notethat non-homogeneous can be replaced by a homogeneous one(i.e., without the offset term ) by lifting the tuples into R+1.Active learning aims to improve sample complexity that is re-quired for learning a classifier by adaptive labeling. Active learningwith a traditional labeling oracle has been extensively studied. Theabove imbalanced problem instance happens to be a difficult casefor active learning with a labeling oracle . We refer the readerto Hanneke et al. for a detailed treatment.Active learning with additional access to pairwise comparisonshas been studied by Kane et al. . That is, one can use bothlabeling and comparison oracles. Importantly, Kane et al. in-troduce a notion of inference dimension, with which they designan algorithm to effectively infer unknown labels. However, dueto technical conditions, the inference technique is only useful forclassification in low dimension ( 2) or special instances. Asone of our main contributions, we are the first to show that theinference technique can be adapted for the IRM problem. Ranking with existing pairwise comparisons. A different prob-lem setting, is to rank collection of tuples by aggregating a set of(possibly incomplete and conflicting) pairwise comparisons, insteadof adaptively selecting which pair of tuples to compare. This prob-lem has been extensively studied in the literature within differentabstractions. From a combinatorial perspective, it is known as thefeedback arc-set problem on tournaments, where the objective isto find a ranking by removing a minimum number of inconsistentcomparisons . There also exist statistical approaches to find aconsistent ranking, or the top- tuples, by estimating underlyingpreference scores . In machine learning, the problem isknown as learning to rank with pairwise preferences , wherethe aim is to find practical ways to fit and evaluate a ranking.",
  "Nanongkai et al. O( log(/))-Jamieson and Nowak O()O( log)Xie et al. O()O(1/)Algorithms 1 and 2 in this paperO( log2(/) log)-": "Example 1. Every tuple being a point in R3 represents a computerwith three attributes: price, CPU speed, and hard disk capacity. It isreasonable to assume that the utility of a computer grows linearly in,for example, the hard disk capacity. Thus, a user may put a differentweight on each attribute, as one entry in the utility vector u R3,which measures its relative importance. For the setting described above with a linear utility function, itis obvious that at most 1 comparisons suffice to find the besttuple, by sequentially comparing the best tuple so far with a nexttuple. Surprisingly, despite the importance of this problem in manyapplications, improvement over the nave sequential strategy, inthe worst case, has remained elusive. A positive result has onlybeen obtained in a very restricted case of two attributes, i.e., atuple is a point in R2 . Other existing improvements rely onstrong assumptions , for example, when every tuple is almostequally probable to be the best. To the best of our knowledge, weare the first to offer an improvement on the query complexity thatis logarithmic in , in the worst case. We refer the reader to for a detailed comparison with existing work.There exist heuristics in the literature that are shown to performempirically better than the nave sequential strategy, in terms ofthe number of requested comparisons. For example, a popular ideais to compare a carefully-chosen pair in each round of interactionwith the user . However, these methods are computationallyexpensive, and require multiple passes over the whole set of tuples.To illustrate this point, finding a good pair with respect to a givenmeasure of interest can easily take O(2) time, as one has to goover 2 candidate pairs. Furthermore, while such heuristics maywork well in practice, they may require () pairwise comparisons,in the worst case.We also address the problem of finding a high-utility tuple re-liably, where we do not force a user to make a clear-cut decisionwhen confronted with two tuples that have nearly equal utilityfor the user. In this way we can avoid error-prone decisions by auser. Instead, we allow the user to simply declare a tie between thetwo tuples. To our knowledge, this is the first paper that considersa scenario of finding a high-utility tuple with ties and providestheoretical guarantees to such a scenario.Our contributions in this paper are summarized as follows: () Wedevise a single-pass streaming algorithm that processes each tupleonly once, and finds a high-utility tuple by making adaptive pair-wise comparisons; () The proposed algorithm requires a memorysize and has query complexity that are both logarithmic in , in theworst case, where is the number of all tuples; () We show howto maintain the theoretical guarantee of our method, even if ties are allowed when comparing tuples with nearly equal utility; ()We offer significant improvement to existing pruning techniques inthe literature, by leveraging powerful tools from mathematical pro-gramming; () We systematically evaluate all proposed algorithmsover synthetic and real-life datasets, and demonstrate their superiorperformance compared to existing methods.The rest of the paper is organized as follows. We formally definethe problem in . We discuss related work in . Then,we describe the proposed algorithm in , and its extensionin when ties are allowed in a comparison. Enhancementto existing techniques follows in . Empirical evaluation isconducted in , and we conclude in .",
  "PROBLEM DEFINITION": "In this section, we formally define the interactive regret minimization(IRM) problem.The goal of the IRM problem is to find a good tuple among allgiven tuples R in a database. The goodness, or utility, of atuple x is determined by an unknown utility vector u R viathe dot-product operation util(x) = u x. However, we assume thatwe do not have the means to directly compute util(x), for a giventuple x. Instead, we assume that we have access to an oracle thatcan make comparisons between pairs of tuples: given two tuplesx and y the oracle will return the tuple with the higher utility.These assumptions are meant to model users who cannot quantifythe utility of a given tuple on an absolute scale, but can performpairwise comparisons of tuples.In practice, it is usually acceptable to find a sufficiently good tuplex in , instead of the top one x. The notion of sufficiently good ismeasured by the ratio in utility loss util(x)util(x)",
  "Finding Favourite Tuples on Data Streams with Provably Few ComparisonsKDD 23, August 610, 2023, Long Beach, CA, USA": "too slow to be used, and we have to settle for its LP counterpartPair-LP in subsequent experiments.Let us limit the comparison to those LP-based filters. Pair-LPand HS-LP are more computationally expensive than List-LP. ForPair-LP, the reason is obvious: as discussed at the end of Appen-dix C.2, Pair-LP makes relatively more comparisons and every com-pared pair of tuples adds two more parameters to the LP. For HS-LP,the number of parameters in its LP depends linearly on both thedimension and number of compared pairs, while List-LP onlydepends on the latter. Thus, HS-LP is less scalable by design. Effect of regret parameter . The effect of the regret parameter can be found in for all real-life datasets. Generally, a largervalue of decreases the running time, as each filter can be benefitedby more aggressive pruning.The running time of List-QP deteriorates dramatically for a smallvalue of , and the number of comparisons needed also rises con-siderably. The reason is that, most numerical methods for solvinga mathematical program have a user-defined precision parameter.Small precision gives a more accurate solution, and at the sametime causes a longer running time. When gets close to the defaultprecision, or to the actual precision after the maximum number ofiterations is exceeded, List-QP fails to prune tuples. Thus, List-QPis advised to be used for a relatively large regret value .In regard to the memory size, as we can see in , List-QP andList-LP consistently use a much smaller memory size than Pair-LPand HS-LP. This also demonstrates the advantage of using a sortedlist over a set of compared pairs.",
  "Assumption 1. No two tuples in have the same utility. Moreover,the best tuple x has non-negative utility, i.e., util(x) 0": "Without loss of generality, we further assume that u2 = 1 andx2 1, for all x , which can be easily achieved by scaling. Asa consequence of our assumptions, we have = util(x) 1. Theproposed method in this paper essentially finds an /-regret tuple,which is feasible for the IRM problem when = 1. Our solutionstill makes sense, i.e., a relatively small regret /, if is not toosmall or a non-trivial lower bound of can be estimated in advance.On the other hand, if is very small, there exists no tuple in that can deliver satisfactory utility in the first place, which meansthat searching for the top tuple itself is also less rewarding. Forsimplicity of discussion, we assume that = 1 throughout the paper.For all problems we study in this paper, we focus on efficientalgorithms under the following computational model. Definition 1 (One-pass data stream model). An algorithm is aone-pass streaming algorithm if its input is presented as a (random-order) sequence of tuples and is examined by the algorithm in a singlepass. Moreover, the algorithm has access to limited memory, generallylogarithmic in the input size. This model is particularly useful in the face of large datasets. It isstrictly more challenging than the traditional offline model, whereone is allowed to store all tuples and examine them with randomaccess or over multiple passes. A random-order data stream is anatural assumption in many applications, and it is required for ourtheoretical analysis. In particular, this assumption will always bemet in an offline model, where one can easily simulate a randomstream of tuples. Extending our results to streams with an arbitraryorder of tuples is a major open problem.One last remark about the IRM problem is the intrinsic dimensionof the database . Tuples in are explicitly represented by vari-ables, one for each dimension, and is called the ambient dimension.The intrinsic dimension of is the number of variables that areneeded in a minimal representation of . More formally, we saythat has an intrinsic dimension of if there exist orthonormalvectors b1, . . . , b R such that is minimal and every tuplex can be written as a linear combination of them. It is com-mon that the intrinsic dimension of realistic data is much smallerthan its ambient dimension. For example, images with thousandsof pixels can be compressed into a low-dimensional representationwith little loss. The proposed method in this paper is able to adaptto the intrinsic dimension of without constructing its minimalrepresentation explicitly.",
  "In the rest of this section, we review existing hardness resultsfor the ITT and IRM problems": "Lower bounds. By an information-theoretical argument, one canshow that (log) comparisons are necessary for the ITT prob-lem . By letting = and = {e} for [], where e is avector in the standard basis, () comparisons are necessary tosolve the ITT problem, as a comparison between any two dimen-sions reveals no information about the rest dimensions.Therefore, one can expect a general lower bound for the IRMproblem to somewhat depend on both and log. Thanks to thetolerance of regret in utility, a refined lower bound ( log(1/))for the IRM problem is given by Nanongkai et al. [24, Theorem 9].",
  "RELATED WORK": "Interactive regret minimization. A database system providesvarious operators that return a representative subset of tuples (i.e.,points in R) to a user. Traditional top- operators return thetop- tuples according to an explicitly specified scoring function. Inthe absence of a user utility vector u for a linear scoring function, theskyline operators return a tuple if it has the potential to be thetop tuple for at least one possible utility vector. In the worst case, askyline operator can return the entire dataset. Nanongkai et al. introduce a novel -regret operator that achieves a balance betweenthe previous two problem settings, by returning tuples such thatthe maximum regret over all possible utility vectors is minimized.Nanongkai et al. further minimize regret in an interactivefashion by making pairwise comparisons. They prove an upperbound on the number of requested comparisons by using synthe-sized tuples for some comparisons. In fact, their method learnsapproximately the underlying utility vector. However, synthesizedtuples are often not suitable for practical use.Jamieson and Nowak deal with a more general task of find-ing a full ranking of tuples. By assuming that every possibleranking is equally probable, they show that O( log) comparisonssuffice to identify the full ranking in expectation. Nevertheless, inthe worst case, one cannot make such an assumption, and their algo-rithm may require (2) comparisons for identifying a full rankingor () comparisons for identifying the top tuple. Another similarproblem assumes a distribution over the utility vector u withoutaccess to the embedding of the underlying metric space . Theproblem of combinatorial nearest neighbor search is also related,where one is to find the top tuple as the nearest neighbor of a giventuple u without access to the embedding .Xie et al. observe that the ITT problem is equivalent to aspecial linear program, whose pivot step for the simplex method canbe simulated by making a number of comparisons. Thus, an imme-diate guarantee can be obtained by leveraging the fact that O(1/)pivot steps are needed in expectation for the simplex method .Here the expectation is taken over some distribution over . Alsoin the special case when = 2, they develop an optimal binarysearch algorithm . Zheng and Chen suggest letting a usersort a set of displayed tuples in each round of interaction, but theirapproaches are similar to Xie et al. , and do not use a sorted listthe way we do.There are other attempts to the ITT problem that adaptivelyselect a greedy pair of tuples with respect to some measure of",
  "Function best(): return the best tuple x1 in": "for managing filters, while Algorithm 2 specifies a specific filterwe propose. As we will see in the framework can also beused for other filters.The filter we propose relies on a remarkable inference techniqueintroduced by Kane et al. . Note that the technique wasoriginally developed for active learning in a classification task,and its usage is restricted to low dimension ( 2) or specialinstances under technical conditions. We adapt this technique todevise a provably effective filter for the IRM problem. In addition,we strengthen their technique with a high-probability guaranteeand a generalized symmetrization argument.The core idea is to construct a filter from a small random sampleof tuples. It can be shown that the filter is able to identify a largefraction of sub-optimal tuples in without further comparisons.Fixing a specific type of filter with the above property, Algorithm 1iteratively constructs a new filter in a boosting fashion to handle theremaining tuples. Finally, one can show that, with high probability,at most O(log) such filters will be needed.We proceed to elaborate on the mechanism of a filter. The idea isto maintain a random sample of tuples, and sort them in order oftheir utility. The total order of the tuples in can be constructed bypairwise comparisons, e.g., by insertion sort combined with binarysearch. Suppose that = {x1, . . . , x}, where x1 has the best utility.Notice that u (x+1 x) 0 for any . Thus, a sufficient condition",
  "=1 (x+1 x)such that 0for all .(1)": "This condition asks to verify whether x lies within a cone withapex x1, along direction u. The parameters can be efficientlycomputed by a standard Linear Program (LP) solver. If Condition (1)can be satisfied for x, then x can be pruned for further consideration.Actually, it is possible to act more aggressively and prune tuplesslightly better than x1, as long as it is assured that not all feasible tu-ples will be pruned. Specifically, we can remove any x that deviatesfrom the aforementioned cone within a distance of , that is,",
  "=1 (x+1 x)2 s.t. 0 for all . (2)": "To test whether a given tuple x satisfies the above condition, oneneeds to search for parameters over [0, ) for all . The searchcan be implemented as an instance of constrained least squares,which can be efficiently solved via a quadratic program (QP).Given a sorted sample where x1 is the top tuple, we write",
  "if a tuple x can be approximately represented by vectors in in aform of Eq. (2).An example that illustrates the mechanism of a filter is displayedin , on which we elaborate below": "Example 2. In , a random sample = {x1, x2, x3} of threeblue points is collected and sorted, where x1 has the highest utility.This means that util(x+1) util(x) = u (x+1 x) < 0, for any {1, 2}. Compared to the point x1, a new point x in the form ofx = x1 + {1,2} (x+1 x) with 0 can only have a lowerutility than util(x1), since",
  "{1,2} (x+1 x) util(x1)": "Thus, such a point x can be safely pruned. Geometrically, all suchprunable points x form a cone with apex x1, as highlighted in the blueregion in . According to . (2), any point that is sufficientlyclose to (within a distance of ) the blue cone can also be pruned. Upon a random-order stream of tuples, Algorithms 1 and 2 collecta pool of initial tuples as a testbed for filter performance. Then,subsequent tuples are gradually added into the first sample set 1,until a filter based on 1 can prune at least a = 5/8 fraction of. Then, 1 is ready, and is used to prune tuples in the pool andfuture tuples over the stream. Future tuples that survive the filterformed by 1 will be gradually added into the pool and a secondsample set 2, and the process is repeated iteratively. Finally, thealgorithm returns the best tuple among all samples. The followingtheorem states our main result about Algorithms 1 and 2. Theorem 1. Assume > 0 and let = || be the size of data. Let =util(x) be the utility of the best tuple x. Under Assumption 1,with a pool size = 64 ln 2 and = 5/8, Algorithms 1 and 2 returnan /-regret tuple for the IRM problem.Let = 16 ln(2/), where is the intrinsic dimension of . Then,with probability at least 1 1/, at most",
  "comparisons are made": "The memory size, i.e., the number of tuples that will be kept bythe algorithm during the execution, is O(log() 4), which is alsologarithmic in .In fact, Algorithms 1 and 2 are an anytime algorithm, in the sensethat the data stream can be stopped anytime, while the algorithmis still able to return a feasible solution among all tuples that havearrived so far. Theorem 2. Under Assumption 1, the data stream may terminateat any moment during the execution of Algorithms 1 and 2, and an/-regret tuple will be returned for the IRM problem among all tuplesthat have arrived so far.",
  "FINDING A TUPLE: ORACLE WITH TIES": "In this section, we first introduce a natural notion of uncomparablepairs to avoid error-prone comparisons, and then we show howthis new setting affects our algorithms.It is clearly more difficult for a user to distinguish a pair of tupleswith nearly equal utility. Thus, it is reasonable to not force the userto make a choice in the face of a close pair, and allow the user tosimply declare the comparison a tie instead. We make this intuitionformal below.",
  "Function best(): return the best tuple in": "Typically, the value is fixed by nature, unknown to us, andcannot be controlled. Note that when is sufficiently small, we re-cover the previous case in where every pair is comparableunder Assumption 1. By allowing the user to not make a clear-cutcomparison for a u-similar pair, one can no longer be guaranteedtotal sorting. Indeed, it could be that every pair in is u-similar.In Algorithm 3, we provide a filter to handle ties under Assump-tion 2. We maintain a totally sorted subset of representative tu-ples in a sample set . For each representative y , we create agroup y. Upon the arrival of a new tuple x, we sort x into ifno tie is encountered. Otherwise, we encounter a tie with a tupley such that x y, and we add x into a group y. In the end,the best tuple in will be returned.To see whether a filter in Algorithm 3 can prune a given tuple x,we test the following condition. Let = x1, . . . be the sorted list ofrepresentive tuples, where x1 is the top tuple. Let G = 1, . . . bethe corresponding groups. A tuple x can be pruned if there existsx such that x x2 , where",
  "y12y = 1 and all , 0": "The idea is similar to Eq. (2), except that the top tuple x1 in Eq. (2) isreplaced by an aggregated tuple by convex combination, and everypair difference x+1 x is replaced by pair differences between twogroups. We avoid using pair differences between two consecutivegroups, as tuples in group may not have higher utility thantuples in +1. If the above condition is met, then we write",
  "Gsim xand, if G is constructed using ,sim x.(5)": "The number of comparisons that is needed by Algorithm 3 de-pends on the actual input, specifically, , the largest size of anypairwise u-similar subset of . Note that the guarantee below re-covers that of Theorem 1 up to a constant factor, if assuming As-sumption 1 where = 1. However, in the worst case, = O() andthe guarantee becomes vacuous. Theorem 3. Assume > 0 and let = || be the size of data. Let =util(x) be the utility of the best tuple x. Under Assumption 2,with a pool size = 256 ln 2 and = 3/16, Algorithms 1 and 3return an (/ + 2)-regret tuple for the IRM problem.Let = 16 ln(2/), where is the intrinsic dimension of , and be the largest size of a pairwise u-similar subset of . Then, withprobability at least 1 1/, at most",
  "IMPROVING BASELINE FILTERS": "In this section, we improve existing filters by Xie et al. , by usinglinear and quadratic programs. We will use these baselines in theexperiments. Previously, their filters rely on explicit computationof convex hulls, which is feasible only in very low dimension .Technical details are deferred to Appendix C.Existing filters iteratively compare a pair of random tuples, allof which are kept in = {}, where = (y, z) such that util(y) <util(z), and use them to prune potential tuples.",
  "EXPERIMENTAL EVALUATION": "In this section, we evaluate key aspects of our method and the pro-posed filters. Less important experiments and additional details aredeferred to Appendix D. In particular, we investigate the followingquestions. () How accurate is the theoretical bound in Lemma 8?More specifically, we want to quantify the sample size required byAlgorithm 2 to prune at least half of the tuples, and understand itsdependance on the data size , dimension , and regret parameter. () Effect of parameters of Algorithm 1. (Appendix D.1) () How",
  "player 17 38620youtube 29 40650game 60 496100house 303 03278car 1 002 35021": "scalable are the proposed filters? () How do the proposed filtersperform over real-life datasets? () How do ties in comparisonsaffect the performance of the proposed filters? Our implementationis available at a Github repository.1 Next, let us introduce the adopted datasets and baselines.Datasets. A summary of the real-life datasets we use for our evalu-ation can be found in . To have more flexible control overthe data parameters, we additionally generate the following twotypes of synthesized data. sphere: Points sampled from the unit-sphere S1 uniformly at random. clusters: Normally distributedclustered data, where each cluster is centered at a random point onunit -sphere S1. To simulate an oracle, we generate a randomutility vector u on the unit -sphere for every run. More detailsabout datasets can be found in Appendix D.Baselines. A summary of all algorithms is given in . Wemainly compare with (enhanced) pruning techniques (Pair-QP,Pair-LP and HS-LP) by Xie et al. , halfspace-based pruning(HS), and a random baseline (Rand). Discussion of other baselinesis deferred to Appendix D. We instantiate every filter (except forthe HS and Rand) in the framework provided in Algorithm 1, thatis, we iteratively create a new filter that can prune about half of theremaining tuples. This is a reasonable strategy, and will be justifiedin detail in .2. For pair-based filters, a new pair is madeafter two consecutive calls of the add function. The pool size andthreshold in Algorithm 1 are set to be 100 and 0.5, respectively.Since the proposed algorithm List-QP only guarantees a regret of/util(), where is the best tuple in the dataset, we pre-computethe value of util() , and adjust the regret parameter ofList-QP to be util().",
  "Sample size in practice": "Lemma 8 proves a theoretical bound on the size of a random samplerequired by Algorithm 2 to prune at least half of a given set of tuples in expectation. This bound is 2 where = 16 ln(2/).Importantly, the bound does not depend on the data size ||, whichwe verify later in .2.In Figs. 5a and 5b (in Appendix), we compute and present theexact required size for synthesized data, and illustrate how the sizechanges with respect to the dimension and regret parameter . Ascan be seen, the bound provided in Lemma 8 captures a reasonablyaccurate dependence on and , up to a constant factor.",
  "NameBrief description": "List-[QP|LP]Our method: prune a tuple if it is close to a coni-cal hull formed by a sorted list of random tuples(Algorithm 2), equipped with a QP or LP solver.Pair-[QP|LP]Prune a tuple if it is close to a conical hull formedby a set of compared random pairs, equipped withQP (Eq. (7)) or LP solver.HS-LPPrune a tuple if LP (Eq. (6)) is infeasible, i.e., thetuple is dominated by a set of compared randompairs over the entire constrained utility space.HSPrune a tuple x if x (zy) < 0 for any comparedpair z, y such that util(y) < util(z), that is, tuplex falls outside the constrained utility space for u.RandReturn the best tuple among a subset of 50 ran-dom tuples.",
  ": Scalability of filters for synthetic data": "keeps. In a, we compute and show the required memory sizefor a filter to prune half of a given set of tuples, and how thesize changes with respect to the data size = ||. Impressively,most competing filters that adopt a randomized approach onlyrequire constant memory size, regardless of the data size. This alsoconfirms the effectiveness of randomized algorithms in pruning.Based on the above observation, it is usually not feasible to main-tain a single filter to process a large dataset . If a filter requires tuples in memory to prune half of , then at least log(||) tuplesare expected to process the whole dataset . However, the runningtime for both LP and QP solvers is superlinear in the memory sizeof a filter , which means that running a filter with log(||)tuples is considerably slower than running log(||) filters, eachwith tuples. The latter approach enables also parallel computingfor faster processing.",
  ": The effect of ties and parameter": "Therefore, we instantiate each competing filter (except for HSand Rand) in the framework provided in Algorithm 1, and measurethe running time it takes to solve the IRM problem. In the rest ofthis section, we investigate the effect of the data dimension andregret parameter on the running time. Effect of data dimension . In b, we fix a regret parameter = 0.01, and examine how the running time of a filter varies withrespect to the data dimension on synthesized data.The first observation from b is that LP-based filters aremore efficient than their QP counterparts. Particularly, Pair-QP is",
  "The case of oracles with no ties": "The performance of competing filters can be found in for allreal-life datasets. The average and standard error of three randomruns are reported. We instantiate each competing filter (except forHS and Rand) in the framework provided in Algorithm 1 to solvethe IRM problem. Meanwhile, we vary the regret parameter toanalyze its effect. We also experimented with a smaller value suchas 0.005, the observations are similar except that the List-QP filteris significantly slower for reasons we mentioned in .2.Except HS and Rand, every reasonable filter succeeds in return-ing a low-regret tuple. We limit our discussion to only these reason-able filters. In terms of the number of comparisons needed, List-QPoutperforms the rest on most datasets provided that the regret value is not too small. We rate List-LP as the runner-up, and it becomesthe top one when the regret value is small. Besides, List-LP is thefastest to run. The number of comparisons needed by HS-LP andPair-LP is similar, and they sometimes perform better than others,for example, over the youtube dataset.Let us make a remark about the regret value . Being able toexploit a large value of in pruning is the key to improving perfor-mance. Notice that both Pair-LP and List-LP cannot benefit from alarge regret value by design. Though HS-LP is designed with in mind, it is more conservative as its pruning power depends onu x instead of u x, where x is the tuple to prune.In summary, we can conclude that the List-QP filter is recom-mended for a not too small regret parameter (i.e., 0.1), andthe List-LP filter is recommended otherwise. In practice, since bothList-QP and List-LP follow an almost identical procedure, one could",
  "Effect of ties": "According to Assumption 2, the oracle returns a tie if the differencein utility between two given tuples is within a parameter . Forfilters like Pair-LP and HS-LP, the most natural strategy to handle atie for a pair of tuples is to simply discard one of them. It is expectedthat ties worsen the performance of a filter, as they fail to provideadditional information required by the method for pruning.In , we vary the value of parameter to see how it affectsthe performance of the proposed filters. It is not surprising that asthe value of increases, the number of ties encountered and thenumber of comparisons made by all algorithms both increase.Notably, the running time of List-QP and List-LP grows signif-icantly as increases. This is because one parameter is neededin their solvers for every pair of tuples between two consecutivegroups ,, and the total number of parameters can increasesignificantly if the size of both groups increases. This behavioralso reflects the fact that a partially sorted list is less effective forpruning. However, how to handle a large remains a major openproblem. Hence, we conclude that the proposed algorithms workwell provided that the parameter is not too large. Summary. After the systematical evaluation, we conclude withthe following results. () LP-based filters are more efficient thantheir QP counterparts, but less effective in pruning. () List-LP isthe most scalable filter. The runner-up is List-QP, provided that thedata dimension is not too large ( < 128) and the regret parameter is not too small ( 0.1). () To minimize the number of requestedcomparisons, List-QP is recommended for a not too small ( 0.1).When is small, we recommend List-LP. () Good performancecan be retained if the oracle is sufficiently discerning ( 0.01).Otherwise, a better way to handle ties will be needed.",
  "CONCLUSION": "We devise a single-pass streaming algorithm for finding a high-utility tuple by making adaptive pairwise comparisons. We alsoshow how to maintain the guarantee when ties are allowed in acomparison between two tuples with nearly equal utility. Our worksuggests several future directions to be explored. Those includefinding a high-utility tuple in the presence of noise, incorporatingmore general functions for modeling tuple utility, devising methodswith provable quarantees for arbitrary-order data streams, anddevising more efficient algorithms to handle ties. This research is supported by the Academy of Finland projects MAL-SOME (343045) and MLDB (325117), the ERC Advanced Grant RE-BOUND (834862), the EC H2020 RIA project SoBigData++ (871042),and the Wallenberg AI, Autonomous Systems and Software Pro-gram (WASP) funded by the Knut and Alice Wallenberg Foundation.",
  "Kevin G Jamieson and Robert Nowak. 2011. Active ranking using pairwisecomparisons. Advances in neural information processing systems 24 (2011)": "Daniel M Kane, Shachar Lovett, and Shay Moran. 2018. Generalized ComparisonTrees for Point-Location Problems. In 45th International Colloquium on Automata,Languages, and Programming (ICALP 2018). Schloss Dagstuhl-Leibniz-Zentrumfuer Informatik. Daniel M Kane, Shachar Lovett, Shay Moran, and Jiapeng Zhang. 2017. Activeclassification with comparison queries. In 2017 IEEE 58th Annual Symposium onFoundations of Computer Science (FOCS). IEEE, 355366. Amin Karbasi, Stratis Ioannidis, and Laurent Massouli. 2012. Comparison-basedlearning with rank nets. In Proceedings of the 29th International Coference onInternational Conference on Machine Learning. 12351242.",
  "where e2 and {0, 1, 2}": "Let = {x1, . . . , x }. Lemma 6 can be easily extended to holdfor the intrinsic dimension of , by first applying Lemma 6 to theminimal representation y1, . . . , y B of .In Lemma 6, we have x x, where x is a shorthandfor \\ {x}. Note that this is exactly the condition we use in Step 3in Algorithm 2 for pruning. Denote by filter() the set of all suchpruned tuples, i.e.,",
  "where the expectation is taken over": "Proof. The proof is by a symmetrization argument introducedby Kane et al. . Let y be the last tuple added into . Write = y. Given , the distribution of y is a uniform distributionfrom \\. Let x be a random sample from . Since filter(),we havePr [ x | ] Pr [ y | ] .Then,",
  "random-order stream, which implies that every tuple in is equallyprobable to be the last tuple y. Hence, we have Pr [ y y | ] 3/4 by Lemma 7, proving immediately the claim": "Another important issue to handle is to ensure that our prun-ing strategy will not discard all feasible tuples. This is preventedby keeping track of the best tuple in any sample set so far, andguaranteed by Theorem 2. Proof of Theorem 2. Denote by all tuples that have arrivedso far. Suppose x is the best tuple among . Tuple x is eithercollected into our sample sets, or pruned by some sample set .In the former case, our statement is trivially true. In the lattercase, suppose = {x1, . . .}, where x1 is the best tuple in . If x1 isfeasible, then x is feasible as well, as it is at least as good as x1. If x1is infeasible, i.e., util(x) util(x1) > , then x cannot be prunedby by design, a contradiction. This completes the proof.",
  "andPr[ E[] ] 22": "Proof of Theorem 1. The feasibility of the returned tuple x isdue to Theorem 2. In the rest of the proof, we upper bound thesize of every sample and the number of samples we keep in thesequence S.For any sample with at least 4 samples and any subset ,let = filter() and by Lemma 8 we have E[| |] 3",
  "] 2/82,": "where the last step invokes Lemma 9. Since there can be at most samples, the probability that any sample fails to pass the pool testis upper bounded by 2/82.We continue to upper bound the number of sample sets. At mostlog() sample sets suffice if every sample can prune at least halfof the remaining tuples. Fix an arbitrary sample , and let to bethe set of remaining tuples. The pool is a random sample from of size . Thus, E[||]/ = | |/| |. Consequently, if | | < | |/2,then E[||] < /2 and",
  "BPROOFS FOR SECTION 5": "The proof is similar to that of Theorem 1, except that we need anew proof for the key Lemma 8, since in the presence of ties, wemay not be able to totally sort a sample . Instead, we show thata partially sorted set of a sufficient size can also be effective inpruning.From now on, we treat the sample as a sequence instead of aset, as a different arrival order of may result in a different filterby Algorithm 3.",
  "where = 16 ln(2/), is the largest size of a pairwise u-similarsubset of , and G x are the groups with x removed from its group": "Proof. Note that by the definition of , for any particular tuplex , there are at most 2( 1) tuples that are u-similar with tuplex. Thus, G must contain at least 8 groups, and we split all groupsin G into two parts, those with an odd index and those with aneven index.In each part, we can extract a totally sorted list of size at least ,by picking exactly one tuple from each group. We remove one tuplew from such that w w, whose existence is guaranteed by Lemma 6. Eq. (4) guarantees that G xsim x.We repeatedly do so until less than groups remain in each part,which means that the number of remaining tuples is at most 2in each part. As a result, we are able to remove at least 16 4tuples, concluding the claim. Although the above lemma appears similar to Lemma 7, a crucialdifference is that the set of prunable tuples in now depends onthe arrival order of , which causes non-trivial technical challengesin the analysis. A critical observation that enables our analysis isthe following result.",
  "By Lemma 10, we know that || 3": "4 ||. For an arbitrary tuplez , suppose z is assigned to a group G. We call a tuple zgood if || = 1 or z is not a representative in in Algorithm 3. LetG be the groups constructed by Algorithm 3 using z. If z isgood, then G = G z. Therefore, for a good tuple z we alwayshave zsim z.By definition, it is easy to see that there are at most ||/2 tuples in that are not good, proving the lemma.",
  "where the last step is by Lemma 11, and the first step is due to doublecounting, as every sequence appears || times in the right-handside, completing the proof": "Proof. The proof is similar to Theorem 1 on a high level. Weonly elaborate on their differences.We first prove the guarantee on the regret. If the optimal tuplex is in the pool once the algorithm is done, then the regret is atmost . If x is not in the pool, then the proof of Theorem 2 showsthat there is x in one of the sample, say , that yields a regret of /.The top representative of that sample yields / + regret. Finally,the final top tuple yields / + 2 regret.Next, we upper bound the size of every sample and the numberof samples similarly to the proof of Theorem 1. We require everysample to prune at least 1/8 fraction of the remaining tuples instead",
  "CIMPROVING BASELINE FILTERS": "In this section, we improve existing filters by Xie et al. , byusing linear and quadratic programs. Previously, their filters rely onexplicit computation of convex hulls, which is feasible only in verylow dimension. For example, the convex hull size, and consequentlythe running time of these existing techniques, have an exponentialdependence on .",
  "C.1Improving constrained utility space filter": "One of the most natural strategies is to iteratively compare a pairof random tuples. The feasible space for the utility vector u isconstrained by the list of pairs = {} that have been compared,where = (y, z) such that util(y) < util(z). Note that every pairof tuples y, z forms a halfspace in R, i.e., = {u R :u (y z) < 0}. Specifically, the unknown u S1 is containedin the intersection of a set of halfspaces, one by each pair.Xie et al. [33, Lemma 5.3] propose to prune a tuple x if for everypossible u there exists a tuple w in some pair of such thatutil(w) util(x). They first compute all extreme points of , andthen check if the condition holds for every extreme point. How-ever, this approach is highly inefficient, as potentially there is anexponential number of extreme points.Instead, we propose to test the pruning condition by asking tofind a vector u that satisfies",
  "u (z y) 1,u (x z) 1,u ((1 )x z) 1.(6)": "If there is no such vector u we prune x. This test can be donewith a linear program (LP). Note that the test is stronger than thatby Xie et al. as it has been extended to handle -regret.We claim that a given tuple x can be safely pruned if there is novector u satisfying LP (Eq. (6)).",
  "The inequalities in Eqs. (11)(12) are all proper. Consequently,we can scale u so that the left-hand sides in Eqs. (11)(12) are atleast 1, that is, there exists a solution to LP (Eq. (6))": "Notice that the second set of constraints in LP (Eq. (6)) (i.e.,u (x z) 1) is redundant provided util(x) 0. Actually, even ifutil(x) < 0, the test only lets in x that is slightly worse than the besttuple in , which is unlikely since util(x) < 0. Thus, in practice werecommend to omit the second set of constraints to speed up thetest. A filter for maintaining the constrained utility space is concep-tually different from the filter proposed in . A small utilityspace of u is the key for such a filter to be effective, while a filterin maintains no explicit knowledge about u and mainlyrelies on the geometry of the tuples.",
  "C.2Improving conical hull of pairs filter": "Another pruning strategy proposed by Xie et al. [33, Lemma 5.6]is the following. Consider again a list of compared pairs = {},where = (y, z) such that util(y) < util(z), and consider a coneformed by all pairs in . A tuple x can now be pruned if there isanother tuple w kept by the algorithm, such that",
  "Proposition 5. Let u x = . A tuple x can be pruned if theobjective value of the quadratic program (Eq. (7)) is at most /": "Proof. We only discuss the case = 0. When > 0, for anypruned tuple, there exists a tuple in some pair of that is at mosta distance of away from it, and thus maintains at least one/-regret tuple.The first sum in QP (Eq. (7)) can be seen as an aggregated tupleby convex combination, whose utility is no better than the top tuplein . The second term only further decreases the utility of the firstterm. Thus, if a tuple x can be written as a sum of the first andsecond terms, its utility is no better than the top tuple in , andcan be pruned.",
  "=(y,z) = 1,and1,2, 0for all": "As a final remark about the above QP, we compare its pruningpower with that of the proposed filter (Eq. (2)) in . Obvi-ously, its pruning power increases as the number of compared pairsin increases. For a fixed integer , a number of comparisonsresult in pairs for the above QP, while in , comparisonscan produce a sorted list of /log() tuples and /log()2 pairs.Hence, the above QP is less comparison-efficient than the one in. Also, for a fixed number of compared pairs, the numberof parameters is larger in QP (Eq. (7)) than in the proposed filter,",
  "DADDITIONAL EXPERIMENTS": "Datasets. A summary of the real-life datasets we use for our eval-uation can be found in . The datasets contain a number oftuples up to 1M and a dimension up to 100. Previous studies aremostly restricted to a smaller data size and a dimension size lessthan 10, and a skyline operator is used to further reduce the datasize in advance . Note that running a skyline operatoritself is already a time-consuming operation, especially for high-dimension data , and becomes even more difficult to apply with limited memory size in the streaming setting. Besides, a fundamen-tal assumption made by a skyline operator, namely, pre-definedpreference of all attributes, does not hold in our setting. Accordingto this assumption, it is required to know beforehand whether anattribute is better with a larger or smaller value. This correspondsto knowing beforehand whether utility entry u is positive or nega-tive for the i-th attribute. As we mentioned in , we do notmake such an assumption about u, and allow an arbitrary direction.This is reasonable, as preference towards some attributes may bediverse among different people. One example is the floor level inthe housing market, where some may prefer a lower level, whileothers prefer higher. Hence, we do not pre-process the data with askyline operator.Details on the data generation process and the actual synthesizeddata can be found in our public Github repository.Baselines. We do not consider methods that synthesize fake tu-ples in pairwise comparisons, such as Nanongkai et al. . Overa random-order stream, the algorithm by Jamieson and Nowak is the same as the baseline HS-LP when adapted to find thetop tuple instead of a full ranking. The UH-Simplex method that simulates the simplex method by pairwise comparisons is notincluded, as it is mainly of theoretical interest, designed for offlinecomputation, and has been shown to have inferior empirical perfor-mance compared to other baselines. We do not consider baselinesthat iteratively compare a greedy pair (among all 2 pairs) withrespect to some measure of interest, such as Qian et al. , Wanget al. , because they are designed for offline computation and itis computationally prohibited to decide even the first greedy pairfor the adopted datasets.Misc. We adopt the OSQP solver and the HIGHS LP solver .The maximum number of iterations for the solvers is set to 4000,which is the default value in the OSQP solver. All experimentswere carried out on a server equipped with 24 processors of AMDOpteron(tm) Processor 6172 (2.1 GHz), 62GB RAM, running Linux 2.6.-32-754.35.1.el6.x86_64. The methods are implemented in Python 3.8.5.",
  "D.1Effect of parameters": "Recall that in Algorithm 1, a pool of tuples is used to test theperformance of a new filter. A new filter will be ready when itcan prune at least a fraction of tuples in . In , we runAlgorithm 1 with a List-QP filter on a dataset of 10k tuples. We fixone parameter ( = 100 or = 0.5) and vary the other.Parameter roughly specifies the expected fraction of tuples afilter should be able to prune. A larger implies a need for fewerfilters but a larger sample size for each filter. It is beneficial to usea large which leads a smaller number of comparisons overall.Nevertheless, as we will see shortly, such a large filter can be time-consuming to run, especially when the dimension is large.A larger value of improves the reliability of the testbed , whichhelps reducing the number of comparisons. However, a larger also results in longer time to run filters over the testbed ."
}