{
  "Abstract": "Live streaming services are becoming increasingly popular due toreal-time interactions and entertainment. Viewers can chat andsend comments or virtual gifts to express their preferences for thestreamers. Accurately modeling the gifting interaction not onlyenhances users experience but also increases streamers revenue.Previous studies on live streaming gifting prediction treat this taskas a conventional recommendation problem, and model users pref-erences using categorical data and observed historical behaviors.However, it is challenging to precisely describe the real-time con-tent changes in live streaming using limited categorical informa-tion. Moreover, due to the sparsity of gifting behaviors, capturingthe preferences and intentions of users is quite difficult. In this work,we propose MMBee based on real-time Multi-Modal Fusion andBehaviour Expansion to address these issues. Specifically, we firstpresent a Multi-modal Fusion Module with Learnable Query (MFQ)to perceive the dynamic content of streaming segments and processcomplex multi-modal interactions, including images, text commentsand speech. To alleviate the sparsity issue of gifting behaviors, wepresent a novel Graph-guided Interest Expansion (GIE) approachthat learns both user and streamer representations on large-scalegifting graphs with multi-modal attributes. It consists of two mainparts: graph node representations pre-training and metapath-based",
  "KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0490-1/24/08": "behavior expansion, all of which help model jump out of the specifichistorical gifting behaviors for exploration and largely enrich thebehavior representations. Comprehensive experiment results showthat MMBee achieves significant performance improvements onboth public datasets and Kuaishou real-world streaming datasetsand the effectiveness has been further validated through online A/Bexperiments. MMBee has been deployed and is serving hundredsof millions of users at Kuaishou.",
  "Graph, Multi-modal Learning, Live Streaming Recommendation": "ACM Reference Format:Jiaxin Deng, Shiyao Wang, Yuchen Wang, Jiansong Qi, Liqin Zhao, GuoruiZhou, and Gaofeng Meng. 2024. MMBee: Live Streaming Gift-Sending Rec-ommendations via Multi-Modal Fusion and Behaviour Expansion. In Pro-ceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery andData Mining (KDD 24), August 2529, 2024, Barcelona, Spain. ACM, NewYork, NY, USA, 10 pages.",
  "KDD 24, August 2529, 2024, Barcelona, SpainJiaxin Deng et al": "Due to the rapid development of mobile device hardware andthe Internet, live streaming has become a prevalent social servicefor peoples daily lives. As one of the most popular live streamingplatforms in China, Kuaishou has reached 386.6 million daily activeusers and the revenue generated by the live streaming businessreached RMB 9.7 billion as of the third quarter of 2023, whichheavily relies on Kuaishous continuous optimization of the livestreaming ecosystem and improvement of the recommendationsystem. As shown in , on live streaming platforms, contentcreators can share their produced video content with users in real-time, and users can interact with streamers and peers throughlive comments or discussions. They can even send virtual giftsto their favorite streamers, which is one of the main sources ofrevenue for the live-streaming business. Therefore, the task oflive streaming gifting prediction is vital not only for enhancinguser experience and streamer revenue but also for increasing thebusiness effectiveness of the platform.Recent years have witnessed several relevant methods for rec-ommendation and gifting prediction inlive streaming. For example, MARS introduces a two-stage rec-ommendation approach applied in the Multi-Stream Party scenario,aiming to maximize reward earnings while optimizing user personalexperience at the same time. However, this approach ignores theclose connection between users gifting behavior and the rapidlychanging live content in the living room. To address this issue,MTA designs a novel orthogonal module that fully utilizes themulti-modal features in live streaming. However, MTA treats thegift prediction as a time series prediction problem which does notconsider users personalization. Although typical behavior-basedmethods like SIM can achieve personalized recommendationsfor gifting prediction, they may face the challenge of behavior spar-sity in the context of live streaming. According to , DNN-basedmethods typically require a minimum of 5-10 historical behaviorsequences to learn meaningful representations for modeling userinterests. However, the average length of users gifting behavior isas low as 0.3 anchors in our scenario. Therefore, gifting predictionrequires a comprehensive consideration that combines user person-alization under sparse behaviors and real-time content modeling toachieve optimal recommendation effectiveness.To address these challenges, we propose MMBee: an efficientlive streaming gifting prediction method based on real-time Multi-Modal Fusion and Behaviour Expansion. Specifically, we first designa Multi-modal Fusion Module with Learnable Query (MFQ). It helpsthe model to perceive the real-time content changes in live stream-ing through processing the complex visual frames, comments andaudio in each streaming segment. In addition, aiming to addressthe sparsity problem in gifting prediction, we propose a novelGraph-guided Interest Expansion (GIE) approach. We first constructlarge-scale gifting graphs based on the history of gifting interac-tions. Then a graph pre-training scheme via contrastive learning(GraphCL) is adopted to learn general and robust streamer and userrepresentations. Apart from these learned self-supervised embed-dings, we further extend behavior sequences through metapathswith the graph structural information and optimize the representa-tions in an end-to-end manner with online recommendation model.Both of the self-supervised and end-to-end learning schemes help model jump out of the specific historical gifting behaviors for po-tential preferences exploration and largely enrich the behaviorrepresentation. Finally, to meet the low latency requirements ofthe online serving system, we propose a decoupled graph offlinetraining and online inference strategy. MMBee has now been de-ployed on the live-streaming recommendation system of Kuaishou,serving millions of active users every day.Overall, our contributions are shown as follows:",
  "The proposed Multi-modal Fusion with Learnable Query (MFQ)module leverages the dynamic multimodal content of live stream-ing and captures the distinct characteristics among streamers": "Graph-guided Interest Expansion (GIE) module largely enrichesthe observed history behaviors of users and streamers with bothself-supervised graph representation learning and metapath-based behavior expansion to alleviate the sparsity problem. We validate the effectiveness of MMBee through extensive of-fline experiments on Kuaishous 3 billion scale industrial datasetand public dataset. Online A/B tests further show that MMBeebrings significant online benefits and we build efficient industrialinfrastructure to deploy MMBee on the real-world online livestreaming recommendation.",
  "Live Streaming Gifting Recommendation": "Existing works on live streaming gifting recommendation systemsprimarily view the whole live room as recommendation target andmodel the interaction between streamers and viewers only withcategorical data. For instance, MARS proposes a novel recom-mendation scenario called Multi-Stream Party (MSP) and designstwo-phase methods to jointly maximize the reciprocal responseof donations and optimize MSP personal satisfaction. LSEC-GNN models the live stream e-commerce scenario using GNN andfully leverages the interaction information among streamers, users,and products. However, previous research ignores that dramaticcontent changes can occur even within the same live room thus itis vital to make full use of the multi-modal feature in live streaming.Aiming to solve this issue, MTA introduces a novel orthogonalprojection model to capture the cross-modal information interac-tion of real-time content. However, MTA formulates the giftingprediction task as a time series prediction problem and neglects thepersonalization modeling of users interests. In conclusion, therestill exists great room for improvement in existing methods forlive-streaming gifting prediction.",
  "Personalized Recommendation": "The most widely adopted personalized recommendation methods inthe industry are based on deep neural networks. For instance, DIN models users diverse interests in different target items by in-troducing attention mechanisms. SIM proposes an online two-stage retrieval method that models relevant behaviors from a userslong-term history based on the features of the current candidateitem. However, in live-streaming gifting scenarios, it is challeng-ing to achieve satisfactory results with these methods due to thesparsity issue in streamers and user interactions. Recently, severalworks combined with GNNs have introduced multimodal featuresto enrich the embedding of graph nodes. For instance, MMGCN",
  "MMBee: Live Streaming Gift-Sending Recommendations via Multi-Modal Fusion and Behaviour ExpansionKDD 24, August 2529, 2024, Barcelona, Spain": "captures user preferences across different modalities by con-structing a modal-specific user-item bipartite graph. EgoGCN introduces a novel EGO fusion operation that enables inter-modalmessage spreading. However, the aforementioned methods all relyon recursive graph convolution to study the node embedding, whichcan result in exponential computation cost and significant inferencelatency, especially in live streaming gifting recommendation scenar-ios where the model needs to handle millions of nodes and ensurelow latency during inference. Therefore, it is crucial to design anefficient graph architecture for training and inference. 3PreliminariesIn live streaming platforms, we use users to represent the viewerswho watch live streaming and use authors to represent streamers. = {1,2, ,} is the set of users and = {1,2, ,}is the set of authors who are broadcasting at the current time, where is the numbers of users and is the numbers of authors. Previousstudies treat the whole live streaming room as the recommendationtarget while ignoring the real-time change of streaming content.Thus, different from traditional recommendation tasks, we divideeach live room into multiple consecutive 30s live segments andthe live segment of author at the current time is denoted by. We formulate that all live streaming segments of the currentmoment are the recommendation target and = {,,} isthe multi-modal raw data tuple, where , and represent thevisual frames, speech and comment text gathered from frame .Given a set of triples,,, = 1 means that send gift to, otherwise = 0. Thus, the gift-through-rate (GTR) predictionproblem is to predict whether user will send gift to giventhe multi-modal raw data = {,, } in the current livestreaming segment :",
  "= ,, (1)": "where is termed as the gift through rate (GTR) and () is the GTRprediction model. In this work, we choose SIM as our founda-tional model, considering its widespread usage in the industry andits online efficiency and effectiveness. The objective function uti-lized in our method is the negative log-likelihood function, whichis defined as follows:",
  "Multi-modal Fusion with Learnable Query": "For each live streaming segment, three frames are evenly sampledfrom each segment and necessary filtering process is conducted toclean the gathered ASR (Automatic Speech Recognition) and com-ment text. Then, we extract the multi-modal feature of raw data withKuaishous internal pre-trained 8 billion parameters multi-modalmodel K7-8B1 and the extracted multi-modal feature sequences tu-ple of visual, speech and comment at the current moment of author are represented with , and , respectively.Since processing and integrating information from differentmodalities is quite important , we propose a multi-modal fusion with learnable queries to ensure efficient modality inter-actions. Inspired by , we adopt the orthogonal projection(OP) operation to maximize the complementation effects betweendifferent modalities. For example, take as target modality, wecalculate the relevant scores between the visual modality withanother two modalities by using correlation operations:",
  "+ (1 )(4)": "Note that 1 represents the dissimilarity vector that measuresthe difference between two modes representation. It helps to pre-serve the parts of other modalities that are orthogonal to the targetmodality and remove duplicate information to prevent redundancy.Then, as shown in the online stage of , we utilize theorthogonal latent features in a hybrid fusion manner appliedwith cross-attention and self-attention alternately. The fusedfeature is gotten with:",
  "(5)": "However, the fused feature can only reflect the content-levelrepresentation, thus lacking the connection to distinctive charac-teristics across various types of authors. To address this issue, weproduce several learnable query tokens R to ex-tract streamer-aware content patterns. Note that each author keepsa set number of learnable query embeddings which are randomlyinitialized. represents the number of query tokens for each au-thor. The learnable query first interacts with fused multi-modalfeatures through cross-attention layers as:",
  "= SelfAttention( , , )(7)": "The multi-modal fusion module benefits from the learnablequeries in two major aspects: 1) Each author has learnable tokensthat store their specific highlight content patterns. The tokens canbe activated at certain moments of awesome content, which isquite useful for gifting prediction. 2) These queries help align themultimodal representations with the ID embedding based recom-mendation space, thereby maximizing their mutual information.Consequently, the integration of learnable queries further enhancesmodels ability to capture real-time content.",
  "User-to-Author and Author-to-Author Graph": "Based on the users donation history, we first construct a User-to-Author(U2A) graph 1( , 1) that represents the correlationbetween users and authors, where and are the sets of users andauthors respectively and 1 represents the donation relationshipbetween users and authors. As illustrated in (a), the circlerepresents the user, and the square represents the author. If a userhas previously made a donation, an edge exists between the userand the donated author in this graph. The weight of the edge is theamount of donated money and an author node has the attributeof aggregated multi-modal feature. In this way, the large User-to-Author graph is constructed.",
  "+ | |(8)": "where is the set of users who have made donations on author and is the set of authors that donated by user .A2U graph is established through donation relationship betweenusers and authors. The design of edge weights and sampling strate-gies helps enrich the representations of authors who have a richhistory of being donated. However, there are some new or cold-startauthors. Their limited donation history makes it difficult to benefitfrom the A2U graph. Fortunately, A2A graph is built from the swingsimilarity defined in Equation 8, which finds substitutable authorsbased on the substructures of user-author donation bi-partitivegraph. It is useful for linking cold-start author to warm-start authorand encouraging the engagement of cold-start authors, so A2Agraph is quite necessary.After constructing U2A and A2A graphs, we first leverage thegraph node representation learning approach to train graph embed-ding layer in .2. Next, we propose metapath based behaviorexpansion process to enrich sparse behavior sequences in .3. To provide a precise demonstration of the abovementionedmethods, we first establish the following definition: Definition 1 (Metapath). Metapath is defined as a relationsequence to capture the specific structural relation between objects.In A2U and A2A graph, we define five metapaths: three metapaths22, 222, 22 begin from target user, for example 22 =",
  "metapath indicates that user makedonation to authors in U2A graph, and these authors further retrievesimilar authors in A2A graph, and we define two metapath 2 and22 which begin form target author": "Definition 2 (Metapath-guided Neighbors). Given a node and a metapath (start from ) in the graph, the metapath-guidedneighbors are defined as the set of all visited nodes when the node walks along the given metapath. We denote the -th step neighborsof object as N ()(). For example, give the metapath 22 = , we can get metapath-guided neighbors asN (1)22 () = {1,2}, N (2)22 () = {1,2,3}.",
  "Node Representation Pre-training with GraphCL": "Previous studies have shown that graph node em-bedding algorithms are beneficial for recommendation systems fortackling data sparsity problem because these methods are able toeffectively capture the user-author relatedness from graph struc-tures. To leverage the connectivity information of the whole graph,we apply the graph contrastive learning (GraphCL) framework totrain the graph embedding layer. Aiming to cluster similar nodestogether while pushing away dissimilar ones, we loop through allnodes in the whole graph 1 and obtain positive sample set through the metapath-guided neighbor process and the negativenodes set are sampled randomly. The positive and negative nodesare utilized with the Cross-Entropy loss L and InfoNCE L loss for optimizing the parameters of the node embeddinglayers. Algorithm 1 shows the core of our approach and the trainedgraph node embedding implies the connectivity information fromthe whole graph. The InfoNCE loss is defined with Equation 9.",
  "Metapath-guided Behavior Expansion throughEnd-to-End Training": "When analyzing the node number distribution of the constructedA2U graph, we observe that the average outdegree of user nodesis 0.32. It becomes difficult for widely used behavior-based modelslike SIM to study meaningful representations and explore potentialgifting preferences. Furthermore, the graph embedding in .2 is trained in a self-supervised manner which is not directly opti-mized for the recommendation model. To address these challenges,we expand the behavior sequence of the target user and authorusing various pre-defined metapaths . Due to the computationcost, we perform up to 3-hop neighbors on both U2A and A2AGraph. We enumerate all possible metapaths and five metapathswith the highest scores are selected using commonly used featureimportance filtering methods as follows: N (2)22 () begins with the target user and follow this meta-path. The retrieved behavior sequence is a set of users who sharethe same authors as the target user. Therefore, this metapath getssimilar users who share the similar interests of the target user.",
  "N (1)2 () begins with the target author, it retrieves the similarauthors in the A2A graph. Therefore, this metapath helps obtainsimilar authors to the target author": "N (2)22 () indicates that a group of users donates to the targetauthor in the U2A graph, and these users subsequently donate toanother group of authors. Therefore, this metapath helps identifypotential interest authors for the target author.Based on these metapath-guided neighbors, we significantlyenrich the behavior sequence of the target user and author. Duringthe offline GIE stage, we store the pre-aggregated embeddings ofthe metapath-guided expanded neighbors of each user and authoron the graph into memories or key-value databases to be furtherutilized in the online training stage.In order to eliminate the gap between pre-trained node represen-tation and online recommendation model, we gather the expandedsequences and optimize them with GTR prediction objective inrecommendation model for end-to-end training. The generation ofuser side expanded graph representation E() can be formalizedas:",
  ": The deployment of MMBee in online live streamingGTR prediction system": "As shown in , our recommendation model and graphembedding layer are trained on Kuaishous large-scale distributedtraining system. Each day, hundreds of millions of users visit Kuaishou,actively watching and interacting with live-streaming content, re-sulting in the generation of hundreds of millions of logs for watch-ing and interaction. These logs are collected, preprocessed in real-time, and utilized for training the model. Our training system in-crementally updates the model parameters by incorporating thelatest user-author interactions, multi-modal content features, andtrained graph embedding. The trained parameters are synchronizedto the online inference model for online serving. To train graphembedding, we first gather the users historical donation behaviorand utilize it to build the User-Author and Author-Author donationgraphs. The topology of these two graphs is stored in a key-valuebased storage system called KGNN2. Then the graph embeddingtrainer requests the KGNN server with Algorithm 1 for training thenode embedding layer and the KGNN storage updates once a day.During the training and inference processes of the recommen-dation model, it needs to request the metapath-guided neighborsof the target user and author. As shown by the red dashed line in, one approach is to dynamically request the KGNN stor-age. However, this method can impose significant computationaloverhead on the KGNN server and result in great time delays whenwalking on the entire graph. To address this issue, as shown by thegreen dashed line in , we apply the pre-requested expansionmanner and store the metapath-guided neighbors of all nodes inthe graph in the Graph Behavior Offline Storage in advance. As aresult, the online recommendation model can directly access theGraph Behavior Offline Storage to retrieve the sequence withouthaving to walk on the graph.",
  "Dataset": "6.1.1Kuaishou Dataset We first test our method on companyinternal dataset called Kuaishou Dataset. It includes about 3 billionuser interaction logs with live-streaming content in Kuaishou App.This dataset is collected as follows: We first apply a 30s slidingwindow to generate the streaming segment samples. If the userrequests the recommendation service and makes a donation attime , then only the segment containing will be taken as thepositive training sample while other samples will be ignored. Onthe contrary, if the recommended live broadcast has impressed butusers donation behavior does not occur until exiting, the segmentwhen user exit will be adopted as negative sample . With thisprocess, the sparsity of of Kuiashou dataset is 99.969% which isreasonable. Kuaishou dataset is composed of two parts: and, where is users real interaction logs from 7 days ofall live streaming content during that period for the training phase.The is sampled from the following one-day logs after is collected, which is used to test models performance. 6.1.2Public Dataset To prove the effectiveness of our proposedMFQ and GIE module, we also compare our method on two publicshort video recommendation datasets: TikTok and MovieLens. Thestatistics of datasets are shown in .: The statistics of public datasets. V, A, and T representthe dimensions of visual, acoustic and textual features.",
  "Tiktok726,06576,085 36,65699.97%128128128Movielens1,239,5085,98655,48599.63%2048128100": "Movielens3 is a widely used dataset for the rec-ommendation task. The raw data is initially acquired by collectingmovie descriptions from Movielens-10M and crawling the corre-sponding trailers from YouTube. Textual features are subsequentlyextracted from the descriptions using the Sentence2Vector . Forvisual modality, key frames are initially extracted from the retrievedvideos and then processed by a pre-trained ResNet50 model to obtain visual features. The acoustic features are obtained usingVGGish , following a soundtrack separation procedure imple-mented with the FFmpeg software.TikTok4 is published by TikTok, a micro-video sharing platformthat enables users to create and share micro-videos with durationsranging from 3 to 15 seconds. TikTok comprises users, micro-videos,and their interactions, such as clicks. The features of the micro-videos in each modality are extracted and made available withoutproviding the raw data. Specifically, the textual characteristics areextracted from the micro-video captions provided by users.",
  "MTA leverages multimodal time-series analysis to effectivelyintegrate information from different modalities. This approachdoes not consider modeling personalized preferences": "EgoFusion allows the spread of inter-modal messages inEgoGCN. In our work, we apply the Ego fusion operation to themulti-modal feature of node attribution to generate the multi-modal embedding and we exclude the MFQ module for a faircomparison in this baseline.On the public dataset, we compare the performance of our methodwith the following GCN-based models: NGCF exploits high-order connectivity and collaborativesignal by propagating embeddings on user-item graph structure.",
  "Overall Performance": "shows the performance of all models on the Kuaishoudataset. Note that given the large number of users and samplesin Kuaishou dataset, an improvement of 0.5% in AUC, UAUC, andGAUC during offline evaluation holds significant value to bringobvious online gains for business. presents the performanceof several competitors on public Tiktok and Movielens datasets.First, our method surpasses all baselines by a significantmargin on Kuaishou dataset. Our method MFQ significantly out-performs traditional live streaming recommendation models BDRand MTA in UAUC and GAUC for two main reasons. Firstly, BDRignores the modeling of multi-modal content, while MTA lacks theconnection to distinctive characteristics across various types of au-thors. In contrast, our MFQ successfully leverages the multi-modalcontent of the target live-streaming room and adopts learnablequeries to extract streamer-aware content patterns. Additionally,our method GIE also outperforms the graph-based method EgoFu-sion which provides evidence that the metapath-guided behaviorexpansion process greatly enhances behavior representation andexplores potential donation preferences.Secondly, our method exhibits generalizability to a com-mon behavior-based model. Our method has seamlessly inte-grated into two widely used behavior-based methods, MMoE andSIM, both of which demonstrate significant performance improve-ments. Moreover, MMBee is not limited to these two behavior-basedmodels and can be easily adapted to other methods such as DIN and DIEN as well.Thirdly, our method is not restricted to gifting predictiontasks and it also proves effectiveness in multi-modal recom-mendation tasks. As shown in , our method exhibits greatimprovement when compared to several strong multi-modal rec-ommendation baselines. This gain mainly comes from two folds: (1)The metapath-guided neighbors in our method enable better cap-ture of user preferences, but other graph-based methods only relyon implicit learning from graph embeddings. (2) The MFQ moduleenhances the fusion of multi-modal features from short videos andclusters different videos with learnable queries initialized with itemembedding, thereby benefiting further performance improvementof the recommendation model.",
  "Graph-level Ablation: In order to investigate the importance ofdifferent metapath neighbors and the effect of graph embeddingtraining, we remove five expanded sequences in turn and evaluate": "the performance of ablated graph embedding features. The resultsare presented in , where we use () to represent the re-moved part or feature. For example, 22 () means removingthe metapath neighbors N (2)22 () in recommendation model,() denotes removing the learned graph node embedding layersbut remaining the expanded sequence and () represents remov-ing all features of graph modeling. From table 7, we can observe that() drops -0.1100% of AUC and () also leads to a significantdrop in performance which means that the GIE modeling is a veryimportant supplement to the observed history behaviors. This sug-gests that the explicit metapath-based behavior expansion processand implicit graph node embedding learning are all beneficial tomodels performance. Furthermore, among five expanded behaviorsequences, we observed the metapath of 22 and 222 arethe most important sequences among them.Multi-modal Ablation: We also investigate the influence of themulti-modal feature in MFQ module. Specifically, () denotesremoving all multi-modal content and () represents removingthe learnable query and cross attention. shows that whenremoving the multi-modal feature MMBee suffers significant perfor-mance drops. We further study the influence of different modalitiesand report the ablation results in . We find visual modal-ity has the most important impact, causing the most performancedegradation when removed. The speech and comment modalityhave a lesser impact factor but still show an innegligible effect onthe models overall performance.",
  "K154.61M0.0000%64141.76K190.27M0.1744%128132.73K229.30M0.2105%": "Segment Length. We additionally choose 10/20 consecutivelive segments and compared them with 5 segments on Kuaishoudataset. shows that 10 live segments get obvious gainbut when it comes to 20 the further gain is modest. However, 10segments significantly increase resource costs (including storage,training and serving) making it infeasible to deploy in production.So we use 5 segments in MMBee.",
  ": Visualization of the learnable query distribution inMFQ, where each point indicates an author": "We conduct experiment to visualize the learnable query represen-tations in MFQ. We randomly sample 10,000 authors and visualizethese representations using t-SNE in 2 dimensions, as illus-trated in . The points in this graph represent the sampledauthors, and it is obvious that there are several distinct clusteringcenters and we mark two of them by the yellow and red boxes.To demonstrate the characteristics of each clustering center, weprovide some visual frames for further explanation. We observethat authors in the yellow box tend to be chatting authors, whilegaming authors tend to appear in the red box. These phenomenasupport our assumption that learnable query can represent distinc-tive characteristics of various types of authors.",
  ": Left shows the response time of different metapathsand right shows the systems overall response time changeduring one day": "We investigate the online response time when recommendationrequests the KGNN server and (left) shows the differentresponse time when requesting different metapath behaviors. It isobvious that the max lag can reach 8.79 ms but this is not allowed inreal-world applications. So we applied the pre-request of expansionbehaviors and stored it in advance (described in .4) so theonline recommendation model could access the embedding serverinstead of walking through the graph on the fly. We evaluate theefficiency of offline storage by comparing the time cost betweenthe baseline system and the system equipped with MMBee. Theresponse time (in milliseconds) with millions of queries per secondduring Jan. 24, 2024 is presented in (right), where theyellow and green lines represent the response time of the baselinesystem and MMBee. Empirical evidence shows that the responsetime of MMBee is only about 1 ms more than that of the baselinesystem on average, which is brought by the extra expanded graphbehavior retrieving and computational overhead of inference.",
  "Online Result": "To evaluate the online performance of MMBee, we conduct strictonline A/B tests on Kuaishous business scenarios of live stream-ing main page spanning from 2023/10/05 to 2023/10/09 and wecompare the performance of MMBee and SIM with 1% main trafficfor experiments. Note that MMBee integrates our proposed MFQand GIE into SIM backbone. We use NGU (Number of users whosent gifts) and NGC (the total number of gifts sent) as main on-line metrics. Online evaluation shows that MMBee has achieved2.862% on NGU and 4.775% lift on NGC metric, which indicatesthat MMBee achieves much better recommendation results andbrings considerable revenue increments for the platform.",
  "Conclusion": "In this paper, we propose a novel real-time multi-modal fusion andbehavior expansion model called MMBee for live streaming giftingprediction. The model efficiently leverages real-time multi-modalfeatures and effectively exploits metapath-guided expanded behav-iors to enhance the performance of GTR prediction. We address twoimportant challenges in live streaming gifting prediction, namelythe multi-modal modeling and behavior sparsity, by introducingthe Multi-modal Query Fusion (MFQ) and Graph-guided InterestExpansion (GIE) modules. Extensive experiments on real-worlddatasets demonstrate the excellent performance of MMBee.",
  "Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017. A simple but tough-to-beat baseline for sentence embeddings. In International conference on learningrepresentations": "Hedi Ben-Younes, Remi Cadene, Nicolas Thome, and Matthieu Cord. 2019. Block:Bilinear superdiagonal fusion for visual question answering and visual relation-ship detection. In Proceedings of the AAAI conference on artificial intelligence,Vol. 33. 81028109. Rose Catherine and William Cohen. 2016. Personalized recommendations usingknowledge graphs: A probabilistic logic programming approach. In Proceedingsof the 10th ACM conference on recommender systems. 325332. Feiyu Chen, Junjie Wang, Yinwei Wei, Hai-Tao Zheng, and Jie Shao. 2022. Break-ing Isolation: Multimodal Graph Fusion for Multimedia Recommendation byEdge-wise Modulation. In Proceedings of the 30th ACM International Conferenceon Multimedia. 385394. Shaohua Fan, Junxiong Zhu, Xiaotian Han, Chuan Shi, Linmei Hu, Biyu Ma, andYongliang Li. 2019. Metapath-guided heterogeneous graph neural network forintent recommendation. In Proceedings of the 25th ACM SIGKDD internationalconference on knowledge discovery & data mining. 24782486. Mihajlo Grbovic and Haibin Cheng. 2018. Real-time personalization using em-beddings for search ranking at airbnb. In Proceedings of the 24th ACM SIGKDDinternational conference on knowledge discovery & data mining. 311320.",
  "Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recom-mendation. In 2018 IEEE international conference on data mining (ICDM). IEEE,197206": "Jin-Hwa Kim, Kyoung Woon On, Woosang Lim, Jeonghee Kim, Jung-Woo Ha,and Byoung-Tak Zhang. 2017. Hadamard Product for Low-rank Bilinear Pooling.In The 5th International Conference on Learning Representations. Hsu-Chao Lai, Jui-Yi Tsai, Hong-Han Shuai, Jiun-Long Huang, Wang-Chien Lee,and De-Nian Yang. 2020. Live multi-streaming and donation recommendationsvia coupled donation-response tensor factorization. In Proceedings of the 29thACM International Conference on Information & Knowledge Management. 665674. Hsu-Chao Lai, Philip S Yu, and Jiun-Long Huang. 2023. Learning the Co-evolutionProcess on Live Stream Platforms with Dual Self-attention for Next-topic Recom-mendations. In Proceedings of the 32nd ACM International Conference on Informa-tion and Knowledge Management. 11581167. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrappinglanguage-image pre-training with frozen image encoders and large languagemodels. arXiv preprint arXiv:2301.12597 (2023). Fengqi Liang, Baigong Zheng, Liqin Zhao, Guorui Zhou, Qian Wang, and YananNiu. 2024. Ensure Timeliness and Accuracy: A Novel Sliding Window Data StreamParadigm for Live Streaming Recommendation. arXiv preprint arXiv:2402.14399(2024). Jiahao Liang, Xiangyu Zhao, Muyang Li, Zijian Zhang, Wanyu Wang, HaochenLiu, and Zitao Liu. 2023. MMMLP: multi-modal multilayer perceptron for sequen-tial recommendations. In Proceedings of the ACM Web Conference 2023. 11091117. Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018.Modeling task relationships in multi-task learning with multi-gate mixture-of-experts. In Proceedings of the 24th ACM SIGKDD international conference onknowledge discovery & data mining. 19301939.",
  "Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learningwith contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018)": "Enrico Palumbo, Giuseppe Rizzo, and Raphal Troncy. 2017. Entity2rec: Learninguser-item relatedness from knowledge graphs for top-n item recommendation.In Proceedings of the eleventh ACM conference on recommender systems. 3236. Enrico Palumbo, Giuseppe Rizzo, Raphal Troncy, Elena Baralis, Michele Osella,and Enrico Ferro. 2018. Knowledge graph embeddings with node2vec for itemrecommendation. In The Semantic Web: ESWC 2018 Satellite Events: ESWC 2018Satellite Events, Heraklion, Crete, Greece, June 3-7, 2018, Revised Selected Papers 15.Springer, 117120. Qi Pi, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, XiaoqiangZhu, and Kun Gai. 2020. Search-based user interest modeling with lifelongsequential behavior data for click-through rate prediction. In Proceedings of the29th ACM International Conference on Information & Knowledge Management.26852692. Jrmie Rappaz, Julian McAuley, and Karl Aberer. 2021. Recommendation onlive-streaming platforms: Dynamic availability and repeat consumption. In Pro-ceedings of the 15th ACM Conference on Recommender Systems. 390399.",
  "networks. In Proceedings of the 13th international conference on web search anddata mining. 519527": "Chenguang Song, Nianwen Ning, Yunlei Zhang, and Bin Wu. 2021. A multimodalfake news detection model based on crossmodal attention residual and multi-channel convolutional neural networks. Information Processing & Management58, 1 (2021), 102437. Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.2019. BERT4Rec: Sequential recommendation with bidirectional encoder rep-resentations from transformer. In Proceedings of the 28th ACM internationalconference on information and knowledge management. 14411450. Wei Tu, Chen Yan, Yiping Yan, Xu Ding, and Lifeng Sun. 2018. Who is earning?Understanding and modeling the virtual gifts behavior of users in live streamingeconomy. In 2018 IEEE conference on multimedia information processing andretrieval (MIPR). IEEE, 118123.",
  "Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.Journal of machine learning research 9, 11 (2008)": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processing systems 30 (2017). Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.Neural graph collaborative filtering. In Proceedings of the 42nd international ACMSIGIR conference on Research and development in Information Retrieval. 165174. Yinwei Wei, Xiang Wang, Liqiang Nie, Xiangnan He, and Tat-Seng Chua. 2020.Graph-refined convolutional network for multimedia recommendation withimplicit feedback. In Proceedings of the 28th ACM international conference onmultimedia. 35413549. Yinwei Wei, Xiang Wang, Liqiang Nie, Xiangnan He, Richang Hong, and Tat-SengChua. 2019. MMGCN: Multi-modal graph convolution network for personalizedrecommendation of micro-video. In Proceedings of the 27th ACM internationalconference on multimedia. 14371445.",
  "Dung-Ru Yu, Chiao-Chuan Chu, Hsu-Chao Lai, and Jiun-Long Huang. 2020. SocialAttentive Network for Live Stream Recommendation. In Companion Proceedingsof the Web Conference 2020. 2425": "Sanshi Yu, Zhuoxuan Jiang, Dong-Dong Chen, Shanshan Feng, Dongsheng Li,Qi Liu, and Jinfeng Yi. 2021. Leveraging tripartite interaction information fromlive stream e-commerce for improving product recommendation. In Proceedingsof the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining.38863894. Xiao Yu, Xiang Ren, Yizhou Sun, Quanquan Gu, Bradley Sturt, Urvashi Khandel-wal, Brandon Norick, and Jiawei Han. 2014. Personalized entity recommendation:A heterogeneous information network approach. In Proceedings of the 7th ACMinternational conference on Web search and data mining. 283292. Zhou Yu, Jun Yu, Jianping Fan, and Dacheng Tao. 2017. Multi-modal factorizedbilinear pooling with co-attention learning for visual question answering. InProceedings of the IEEE international conference on computer vision. 18211830.",
  "Shuai Zhang, Hongyan Liu, Jun He, Sanpu Han, and Xiaoyong Du. 2021. A deepbi-directional prediction model for live streaming recommendation. InformationProcessing & Management 58, 2 (2021), 102453": "Yujing Zhang, Zhangming Chan, Shuhao Xu, Weijie Bian, Shuguang Han, HongboDeng, and Bo Zheng. 2022. KEEP: An industrial pre-training framework for onlinerecommendation via knowledge extraction and plugging. In Proceedings of the31st ACM International Conference on Information & Knowledge Management.36843693. Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, XiaoqiangZhu, and Kun Gai. 2019. Deep interest evolution network for click-through rateprediction. In Proceedings of the AAAI conference on artificial intelligence, Vol. 33.59415948. Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, YanghuiYan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-throughrate prediction. In Proceedings of the 24th ACM SIGKDD international conferenceon knowledge discovery & data mining. 10591068."
}