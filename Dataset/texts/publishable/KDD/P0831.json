{
  "ABSTRACT": "The widespread application of pre-trained language models (PLMs)in natural language processing (NLP) has led to increasing con-cerns about their explainability. Selective rationalization is a self-explanatory framework that selects human-intelligible input sub-sets as rationales for predictions. Recent studies have shown thatapplying existing rationalization frameworks to PLMs will result insevere degeneration and failure problems, producing sub-optimal ormeaningless rationales. Such failures severely damage trust in ratio-nalization methods and constrain the application of rationalizationtechniques on PLMs. In this paper, we find that the homogeneity oftokens in the sentences produced by PLMs is the primary contrib-utor to these problems. To address these challenges, we proposea method named Pre-trained Language Models Rationalization(PLMR), which splits PLMs into a generator and a predictor to dealwith NLP tasks while providing interpretable rationales. The gen-erator in PLMR also alleviates homogeneity by pruning irrelevanttokens, while the predictor uses full-text information to standardizepredictions. Experiments conducted on two widely used datasetsacross multiple PLMs demonstrate the effectiveness of the proposedmethod PLMR in addressing the challenge of applying selectiverationalization to PLMs. Codes:",
  "Both authors contributed equally to this research.Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 25, August 37, 2025, Toronto, ON, Canada 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-1245-6/25/08",
  "INTRODUCTION": "The widespread application of deep learning models, particularlypre-trained language models (PLMs), across critical fields in naturallanguage processing (NLP) has led to increasing concerns abouttheir explainability . Selective rationalization, as an inter-pretable method, has received continuous research in the field inrecent years . Lei et al. were the first to propose thisframework for rationalizing neural predictions (RNP), which con-sists of a generator and a predictor. The generator selects a human-intelligible subset of the entire input sentence as a rationale, whilethe predictor then makes a judgment only based on this rationale,ensuring that the explanation is faithful, as shown in . GeneratorPredictor",
  ": The selective rationalization framework RNP": "The vanilla RNP framework in suffers from rationaliza-tion degeneration and failure problems . The questionwith rationalization degeneration lies in its capacity to yield lowerprediction loss even with a poor quality of rationale. For example,the final prediction is correct even if the rationale, as shown in Fig-ure 2, is not as relevant to the gold annotations. The rationalizationfailure goes further, as the chosen rationale was completelyabsurd, but the label can still be accurately predicted. Many meth-ods have been proposed to alleviate these issues . In",
  "KDD 25, August 37, 2025, Toronto, ON, CanadaLibing Yuan, Shuaibo Hu, Kui Yu, and Le Wu": "Wei Liu, Jun Wang, Haozhao Wang, Ruixuan Li, Zhiying Deng, Yuankai Zhang,and Yang Qiu. 2023. D-separation for causal self-explanation. In Proceedingsof the 37th International Conference on Neural Information Processing Systems.4362043633. Wei Liu, Jun Wang, Haozhao Wang, Ruixuan Li, Yang Qiu, YuanKai Zhang,Jie Han, and Yixiong Zou. 2023. Decoupled Rationalization with AsymmetricLearning Rates: A Flexible Lipschitz Restraint. In Proceedings of the 29th ACMSIGKDD Conference on Knowledge Discovery and Data Mining. 15351547. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, OmerLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2020. RoBERTa: ARobustly Optimized BERT Pretraining Approach. In In Proceedings of the 8thInternational Conference on Learning Representations.",
  "BERT32.033.239.549.138.4": "Thus, the questions naturally arise: what factors lead to thepoor performance of the selective rationalization framework withinPLMs, and what strategies can improve it? Motivated by these twocore concerns, we conducted a series of analyses and proposeda method called Pre-trained Language Models Rationalization(PLMR) that can work on PLMs. Our contributions are as follows:First, we conduct experiments to validate the severe rationaliza-tion degeneration and failure problems that occur when we applythe BERT model (a commonly used PLM) to the current rationaliza-tion frameworks. We then deeply analyze why these two problemsoccur when using the BERT model. We find that the over-learningof contextual information of PLMs causes the input tokens to havehomogeneous representations, leading to severe rationalizationdegeneration and failure problems. Second, based on the analysis above, we propose a novel ap-proach named PLMR. PLMR Truncates the PLM into two indepen-dent parts: the rationale generation part and the prediction part.But from a perspective outside of PLM, the two parts still exist asa whole. In order to further reduce the learning of context-freeinformation, PLMR cut out irrelevant context before rationale gen-eration. In the prediction part, PLMR utilizes full-text informationto regularize the predictions of the predictor, further improving theperformance of the model.Thirdly, the F1 score for rationale selection using PLMR is upto 9% higher than methods using GRU and up to 17% higher thanprevious methods also using PLMs. These results indicate that ourPLMR can use PLM in rationalization and effectively address issuesof rationalization degeneration and failure, making an essential steptowards rationalizing the prediction of PLMs for their explainability.",
  "RELATED WORK": "The base selective rationalization framework named RNP usesonly the rationale extracted by the generator as justifications tomake predictions, thereby ensuring the interpretation of the cooper-ative model is faithful . Such cooperative frameworks betweengenerator and predictor are hard to optimize as well as train. Toaddress this problem, Bao et al. used Gumbel-softmax to re-parameterize gradient estimates. Bastings et al. employed arectified Kumaraswamy distribution to replace Bernoulli sampling.Then it was found that rationalization degeneration and failureproblems occur due to the predictor using only the rationale forprediction. To address the degeneration problem, a series of studiesused additional information to regularize the predictor. 3PLAYER adds a complementary predictor that uses text not selected asthe rationale. A2R uses soft attention from the generator toinput full-text information into the predictor. DMR aligns thefeature and output distributions of the rationales with the full-textinput. DARE improves rationale representations by reducingthe mutual information between rationale and non-rationale partsof the input. FR uses the same encoder between the genera-tor and the predictor to convey information. DAR utilizes anauxiliary module pretrained on the full input to align the selectedrationale and the original input discriminatively. Meanwhile, somework addresses the degeneration problem from a causal perspective.INVRAT uses a game-theoretic approach to constrain the outputof rationale across multiple environments. Inter-RAT uses abackdoor adjustment method theory to remove spurious correla-tions. MCD proposes the Minimum Conditional Dependence(MCD) criterion to uncover causal rationales. MRD Rationaliza-tion failure undermines the users trust more severely by selectingmeaningless rationale. G-RAT addresses rationalization failureproblems by using a guidance module to regularize the selection ofthe generator.The above approach typically uses simple recurrent models suchas GRU and LSTM . Recent experiments have shown that these rationalization frameworks withPLMs lead to the poor quality of rationales. Although some studieshave been conducted using PLMs , the quality of rationalesremains inferior compared to methods utilizing GRU. Our workanalyzes the reasons for the problems of degeneration and failure",
  "PRELIMINARIES": "Selective Rationalization. We consider a text classification task,where the input text is = , with representingthe -th token and representing the number of tokens in the text. is the label corresponding to . The selective rationalizationframework consists of a generator () and a predictor (), with and representing the parameters of the generator and thepredictor, respectively. In the training set (,) , the rationaleis unknown. The goal of the generator is to learn a sequence ofbinary mask = {0, 1} from the input , andthen use to generate a subset of the input text as the rationale :",
  "ANALYZING DEGENERATION AND FAILUREIN RATIONALIZATION WITH PLMS": "In , both previous studies and our experiments indicatethat using PLMs in a rationalization framework can result in severerationalization degeneration and failure problems, providing unsat-isfactory explanations. This section will first validate the presenceof more severe rationalization degeneration and failure phenomena.Subsequently, we will identify the root causes of these issues byanalyzing the operational mechanism of PLMs. Our primary ex-periments are performed using BERT-base-uncased, which has 12layers (transformer blocks), 12 attention heads, and 110 millionparameters.",
  "Rationalization Degeneration and Failure": "4.1.1Rationalization Degeneration. For two subsets, 1 and 2,within the input text , where 1 is the golden rationale corre-sponding to label , the high correlation between 1 and 2 resultsin a spurious correlation between 2 and . This spurious corre-lation is the possible cause of degeneration. The strength of the correlation between 1 and 2 reflects the extent of the spuri-ous correlation. Therefore, we demonstrate through the followingexperiments that using pre-trained language models (PLMs) in arationalization framework leads to more severe degeneration.",
  ": Comparison of spurious correlation strength inBERT and GRU representations": "In the BeerAdvocate dataset, each text contains multiple ratio-nales (appearance, aroma, palate) and corresponding labels. First,we train a classifier using a GRU on the appearance aspect andits corresponding labels. Then we use this classifier to predict thearoma and palate aspects. As shown in (a), the GRU classi-fier can effectively predict the untrained aroma and palate aspects,which demonstrates a significant correlation between different ra-tionales. These correlations result in spurious associations betweenaroma, palate, and the label of appearance, ultimately causing therationalization model to select spurious rationales. Next, we con-duct the same experiment using a BERT classifier, ensuring that theBERT and GRU classifiers had the same prediction capability (F1score) on appearance. Comparing the results of the experimentswith GRU and BERT in (a), we find that the BERT classifierachieved higher F1 scores on the aroma and palate aspects. Theresults consistently showed that, compared to GRU, BERT signifi-cantly increases the influence of spurious correlations. We obtainthe same experimental results for the other two aspects in (b) and 3(c). The increasing impact of spurious correlations leadsto more severe degeneration in rationalization for PLMs like BERT,and we also provide an additional example in Appendix B.1 for afurther illustration. 4.1.2Rationalization Failure. When the generator selects mean-ingless tokens as its rationale, the predictor should theoreticallyfail to make accurate predictions. However, there are occasionswhen the predictor still succeeds in making correct predictions.This occurrence causes the generator to mistakenly believe thesetokens support its predictions, leading to what is known as rational-ization failure. Based on the reasons for the rationalization failurementioned above, we make the assumption to illustrate whetherrationale failure occurs: Assumption 1. Given a rationalization framework, which consistsof generator () and predictor (). For (, ,) test , () = ,( ) = . Rationalization failure occurs when the learned rationale meets the following conditions:(1) ( ) = = ; The predictor () predicts correctly using .(2) = | |/|| 1; The proportion of correct tokens in the is less than 1.",
  "(3) punct, prep, pron, art, conj /| | 2; The proportion of mean-ingless tokens such as punctuation, prepositions, pronouns, articles,and conjunctions in is greater than 2": "Therefore, we explored the likelihood of rationalization failuresin PLMs and RNNs through the selection rationalization frame-work. Our experiments utilized BERT and GRU within the RNPframework. By adjusting parameters 1 and 2, we observed theoccurrence of rationalization failures in the test set test. As shownin , BERT-RNP experienced a significantly higher rate ofrationalization failures compared to GRU-RNP. The result showsthat using PLMs in a rationalization framework often leads to theselection of numerous meaningless tokens as rationales, resultingin a significant rationalization failure. The experimental details arein Appendix C.3.",
  "Homogeneity Among Tokens and Clauses": "Unlike recurrent neural networks, the self-attention mechanism ofthe Transformer model allows each token to establish direct connec-tions with other tokens in the sentence. This mechanism enableseach token to attend to any position in the sentence, thereby cap-turing long-distance dependencies. However, in PLMs with multi-layered Transformer structures, each token learns information fromother tokens at every layer of the Transformer. This over-learningof contextual information leads to highly similar final token rep-resentations, resulting in a lack of heterogeneity. We refer to thephenomenon where different tokens within a sentence exhibit sim-ilar semantic information as token homogeneity. In this section, weempirically demonstrate the occurrence of homogeneity in PLMsand explain how it leads to more severe rationalization degenerationand rationalization failure problems.To illustrate the homogeneity of token representations gener-ated by BERT, we utilize the traditional likelihood-based variance-covariance matrix homogeneity test method to observe thedegree of discrepancy between token representations.Given the hidden states generated by BERT, denoted as , where is an matrix, is the number of tokens in the sentence, and is the dimensionality of the token representations. The calculationof the variance-covariance matrix is as follows:",
  ": The average trace tr() of all sentences in differentlayers. Testing on the HotelReview dataset. The results areverified on three models: Bert, Electra, and Roberta": "We conduct experiments on three types of PLMs, each includingboth base and large versions. shows the trace tr() ofhidden states across different transformer layers in PLMs. Theresults show that the trace tr() of the final output hidden states ofthe PLMs are significantly smaller than those of other layers. Thisindicates severe homogeneity of token representations in sentencesgenerated by PLMs. This homogeneity implies that each token hasa similar attention-weight vector in the attention heads, whichmeans all tokens have similar attention dependencies. AppendixA.1 shows the distribution of attention weight vectors at differentlayers in Bert to support the above experiments.Within the framework of rationale selection, the mask selection() needs to select an interpretable subset of tokens as rationalesfrom the different tokens in a sentence. Similar to clustering tasks,we need to select the golden rationale within a sentence. How-ever, the homogeneity of tokens makes this process exceptionallychallenging. On the one hand, this will lead to further correlationbetween different subclauses of the input text, making it easierfor the generator to select sub-optimal subsets. On the other hand,token homogeneity means that the heterogeneity between differenttokens is reduced, and it is challenging to distinguish rationalesfrom other meaningless tokens, causing the generator to selectmeaningless tokens in the early stages of training easily. Whenboth situations occur, powerful predictors will overfit the wrongresults and make accurate predictions, leading to more severe ra-tionalization degeneration and rationalization failure problems.Therefore, we understand that the more severe rationalizationdegeneration and failure occur due to the generator and the pre-dictor issues. The key reason lies in the homogeneity of tokensproduced by the generator. Another reason is that the predictormakes correct predictions based on incorrect rationales.",
  "Overall Architecture": "The overall architecture of the proposed method PLMR is illus-trated in . We divide PLMs into earlier layers of PLMs asthe generator and the later layers of PLMs as the predictor. Thegenerator is comprised of transformer layers and dimension re-duction layers (Dim-Reduction layers). The detailed design of theDim-Reduction layers is shown in .As shown in , the generator in PLMR first inputs thetext into a multi-layer transformer to learn the hidden states .Then the hidden states are passed through the Dim-Reductionlayers to generate the rationale mask , from which we obtain therationale = . The predictor then uses only the rationale for task prediction to ensure the explanation is faithful in theinference phase.",
  "Rationale Selection Module": "In this section, we explain the idea of the generator and describeits rationale selection process. Traditional rationalization methodsfail on PLMs because of the homogeneity among tokens. Therefore,we address this issue with the following two methods. 5.2.1Selecting for Token Heterogeneity. In contrast to homogene-ity, heterogeneity is crucial. The generators task is to select thegolden rationale that best supports the prediction from the entireinput text. The homogeneity of tokens within the text makes itchallenging for the rationale mask selector to identify the correcttokens as the rationale accurately. shows that only thefinal representations of PLMs approach homogeneity, while the token representations generated by the intermediate transformerlayers maintain better heterogeneity.Therefore, the earlier layers of PLMs are used as the generator () to select rationales, which ensures that there is good hetero-geneity between token representations when the mask selector() determines whether tokens are rationales. The mask of therationale can be calculated by the following equation:",
  "= () = (Transformer0 ()) ,(8)": "where 0 indicates that the generator uses the first layers oftransformers in the PLMs. The token representations generated bythe earlier layers in exhibit good heterogeneity, so howmany layers should we choose as generators? (1) The generatorrequires sufficient transformer layers to learn better representations,enabling it to extract the correct rationale from the sentence. (2) Thepredictor needs sufficient transformer layers to use the rationale forthe prediction task. Poor performance in either aspect will lead to adecline in overall performance. In order to balance the performanceof the generator and the predictor, the number of layers of the twoshould be similar, so ideally, the generator should choose the firsthalf of the transformer layer of PLMs. The specific choice will beverified in the experiment. 5.2.2Pruning Sequence in Dim-Reduction layers. The methodsmentioned in section 5.2.1 have mitigated the impact of homo-geneity on rationale selection. However, generators inevitablemulti-layer transformer leads to different tokens acquiring exces-sive contextual information. Especially for larger PLMs such asBert-large-uncased (24-layer transformers), the generator will stillcontain numerous layers of transformers, which will lead to exces-sive fusion of information between irrelevant tokens and rationaletokens, reducing the heterogeneity between tokens. To tackle thisproblem, we propose the Dim-Reduction layers to prune irrelevanttokens. Since the Transformer model processes all input sequencetokens in parallel, context pruning can be seen as reducing thesequence dimension.As shown in , the Transformer layers in the generator aredivided into two parts. One part consists of standard Transformerlayers that generate the hidden states .",
  "where LN refers to the layer normalization. Finally, we multiply themask with to get the hidden state of the Dim-Reductionlayer. = .(12)": "If this layer is the last layer of the Dim-Reduction layers, isthe final rationale mask . Existing methods used as shownin Eq.(3) in to control the brevity and continuity of therationale. PLMR consists of multiple Dim-Reduction layers, each ofwhich needs to control the proportion of tokens pruned. Similar toprevious research, we control the sparsity of tokens retained inthe -th Dim-Reduction layer. The value of should decrease pro-gressively across multiple layers, ensuring that each layer prunesa certain number of less relevant tokens. The final layers corre-sponds to the sparsity of rationale tokens that need to be selected.In this paper, we set the variation of to be linear change. Addi-tionally, we apply a continuity control at each Dim-Reduction layer.Therefore, we compute the sparsity constraint and the continuityconstraint for each of the Dim-Reduction layers and compute themean value as the constraint term :",
  "where the function () is monotonically increasing and > 0": "5.3.2The Objective of task and match . Many studies use additional information to guide the predictor. Inspired by this,we use task to ensure the predictor utilizes full-text representa-tions for prediction. Early in training, when the quality of learnedrationales is poor, we want task and task to be similar. As trainingprogresses and rationale quality improves, we want the rationalesto make more accurate predictions, task task . We achieve thisby setting the function () in the regularization term match tobe monotonically increasing. The detailed theoretical analysis is inAppendix A.2.",
  "EXPERIMENTS6.1Datasets": "We use two widely used datasets in rationalization .BeerAdvocate is a dataset for multi-aspect sentiment pre-diction on beer reviews, where users rate each aspect on a scaleof . Following previous work , we use the originaldataset with highly correlated aspects (referred to as the corre-lated dataset) to validate the effectiveness of PLMR in addressingsevere rationalization degeneration and failure. HotelReview is another dataset for multi-aspect sentiment prediction on hotelreviews, where each aspect is also rated on a scale of .Consistent with prior work, we binarize the labels in datasets,with ratings 0.6 as positive and ratings 0.4 as negative. Bothdatasets contain human-annotated rationales as the test set.",
  "Baselines, Implementations, and Metrics": "6.2.1Baselines. To validate the application of our method in arationalization framework for PLMs, we compared it with four base-lines: rationalizing neural prediction (RNP ), invariant rational-ization (INVART ), folded rationalization (FR ), and causal ra-tionalization (CR ), which also utilize Bert-base-uncased. In theRNP, INVART, and FR frameworks, GRU was originally used. Here,we re-experiment using BERT. In addition, to demonstrate the greatimprovement in the interpretation performance of our method,we compare it with the latest methods that can provide the bestrationale using GRU, such as folded rationalization (FR ), Min-imum Conditional Dependence (MCD ), and Guidance-basedRationalization method (G-RAT ). 6.2.2Implementations. For a fair comparison, we adopted the samesettings as previous work. The specific implementation details areprovided in the appendix C.1. In both the Dim-reduction layersand the predictor, the MLP employs a single linear layer, which isconsistent with previous methods. In PLMR, layers 0 of PLM arethe generator, which contains layers of Dim-reduction layers. Inthe main experiment, we validate the effectiveness of the methodusing Bert-base-uncased (12-layer transformers), where = 7 and = 2. Additionally, we analyze the effect of different and valueson the quality of the rationale through several experiments. Wealso use other PLMs such as ELECTRA-base , Roberta-base ,and Bert-large, ELECTRA-large, Roberta-large with more extensiveparameters (24-layer transformers) for supplementary experiments.To demonstrate the effectiveness of the rationale selection moduleand rationale prediction module, we remove match in the predic-tion module to serve as an ablation experiment. 6.2.3Metrics. As with previous methods, we focus on the qual-ity of selected rationales. The test set includes human-annotatedtokens, and we measure the alignment between the selected ratio-nales and the human-annotated tokens using token-level precision(P), recall (R), and F1 score. Acc represents the precision of theprediction task using the selected rationales. S denotes the averageproportion of selected tokens to the original text.",
  "Results": "6.3.1Comparison with state-of-the-art. We compare PLMR withrationalization frameworks using Bert and using GRU, respectively. shows the results with previous methods using BERT.Under the same experimental settings, our method achieved sig-nificant improvements in F1 scores across all three aspects of beer reviews, with an increase of up to 31% in the appearance aspect.This huge improvement shows the superiority of PLMR in solvingsevere rationalization degeneration and failure, which enables ra-tionalization to be applied in PLMs and achieve ideal performance.Appendix B.3 visualizes some rationales from RNP and PLMR. shows the results with previous methods using GRU. Weset the rationale sparsity to approximately 10%, 20%, and 30% forcomparison. As shown in , we achieved significant improve-ments in F1 scores across various aspects. Therefore, compared tothese highly effective GRU-based models, our Bert-based methodPLMR still provides better explanations, aligning with the expectedoutcomes of using PLMs. presents the experimental resultson the HotelReview dataset using GRU. In this dataset, we set therationale sparsity close to the human-annotated rationales.These results demonstrate that our method not only addressesthe degeneration and failure in rationalization within PLMs butalso offers more accurate explanations than the existing methods. 6.3.2Ablation study. To validate the effectiveness of the rationaleselection module and rationale prediction module, we remove the and in the rationale prediction module during training.As the line PLMR in , excluding the regularizationterm , decreases the F1 score, indicating that the effec-tiveness of and in the prediction module. However, thedecreased F1 score is still significantly higher than that of othermethods, which demonstrates that the rationale selection modulecan select relatively correct rationales, avoiding rationalization de-generation and failure. Simultaneously, we note that the F1 scorein appearance does not decrease significantly. That is because thegolden rationale in appearance tends to appear early in the textand is less impacted by spurious correlation. As a result, there hasbeen a slight improvement in the performance of match . 6.3.3Analysis of the Generators Layers . How should the numberof layers in the generator and predictor be allocated to optimize theselected rationale? To analyze the effect of the generators layers ,we keep the number of Dim-reduction layers = 1 unchanged, andthe generators layer from 1 to 11, with the number of layers in thepredictor corresponding to 12 . Experiments were conducted onthe aroma aspect of the beer review dataset using BERT-base andELECTRA-base, respectively. As shown in , both smallerand larger values will result in a decrease in the F1 score. Thereason is that the generator and predictor work together to extractrationales and make task predictions in selective rationalization.A small value can result in an excessive number of layers in the",
  "RNP*11.097.534.232.933.5FR*11.594.544.844.744.8G-RAT**12.197.944.447.146.3MCD*11.897.047.048.647.8PLMR12.898.346.552.349.2": "predictor, leading to overfitting on incorrect rationales. Conversely,a large value increases the homogeneity among tokens, whichsimilarly reduces the quality of the selected rationale. Therefore,in (a), the F1 score in PLMR reaches the maximum when = 7. At this point, the layers of the generator and the predictorachieve a balance, allowing the selection of the optimal rationale.The results in (b) validate the above analysis as well.",
  ": Analysis of the generators layers . The F1 scoreis averaged over five different random seeds. The rationaleselection sparsity is approximately 0.2": "6.3.4Analysis of the Dim-reduction layers. Based on the aboveanalysis, selecting rationales in the middle layers yields better re-sults. Therefore, we analyze the impact of Dim-reduction layers inthe middle four layers of BERT-base and BERT-large, respectively.As shown in , the value at coordinates (x, y) represents theF1 score for the Dim-reduction layers from layer y to layer x. Fig-ure 8(a) shows that using two Dim-reduction layers in BERT-baseresults in a much higher F1 score compared to using just one layer.This demonstrates the effectiveness of multi-layer context pruning.Additionally, it is observed that a greater number of Dim-reductionlayers (3 layers) are required with BERT-large to reach optimal ra-tionale quality (F1-score) in (b). This is because Bert-largehas more layers, and a larger generator will make rationale selec-tion more difficult, thus requiring more Dim-reduction layers. Thisobservation indirectly confirms the utility of Dim-reduction layersin pruning irrelevant tokens. This experiment can also be used asan ablation experiment to analyze the Dim-reduction layers.",
  "Boosting Explainability through Selective Rationalization in Pre-trained Language ModelsKDD 25, August 37, 2025, Toronto, ON, Canada": "using the same = 7 and = 2. We also conduct experiments withlarger models, such as BERT-large, ELECTRA-large, and Roberta-large. Based on the previous analysis, more Dim-reduction layersare required to generate the optimal rationale in these models.Therefore, we set = 3 and = 13. In , the larger-parameterPLMs still provide highly effective rationales when using the PLMRframework. The results demonstrate that our method effectivelyapplies rationale selection to PLMs, enabling them to provide faith-ful and human-understandable rationales while completing NLPtasks.",
  "CONCLUSION AND FUTURE WORK": "We propose the method PLMR to address the challenge of applyingselective rationalization in pre-trained language models. Specifi-cally, PLMR reduces the homogeneity among tokens in the genera-tor, making it easier for the generator to select meaningful tokens.Simultaneously, it uses full-text information to regularize the pre-dictor. Experiments on multiple pre-trained language models havedemonstrated the superior performance of PLMR. In addition, sev-eral ablation experiments have validated the effectiveness of thegenerator, the predictor, and the Dim-reduction layers. This methodenables the initial application of selective rationalization in PLMsto achieve optimal performance. Although our method, like previ-ous studies, focuses on classification problems, as an interpretableframework, our work can also be applied to other types of tasks,such as regression and unsupervised clustering. We can also ex-plore other methods to address degeneration and failure further toadvance rationalization research in PLMs. Additionally, investigat-ing whether PLMR can provide robustness to adversarial attacksby selecting precise rationales is also significant research in future.Our current work is an essential step toward building interpretablemethods scalable to more complex models, and we plan to exploreLLM explainability in future work.",
  "This work is supported by the National Science and TechnologyMajor Project of China (2021ZD0111801) and the National NaturalScience Foundation of China (under grant 62376087)": "Yujia Bao, Shiyu Chang, Mo Yu, and Regina Barzilay. 2018. Deriving MachineAttention from Human Rationales. In Proceedings of the 2018 Conference onEmpirical Methods in Natural Language Processing. 19031913. Jasmijn Bastings, Wilker Aziz, and Ivan Titov. 2019. Interpretable Neural Pre-dictions with Differentiable Binary Variables. In Proceedings of the 57th AnnualMeeting of the Association for Computational Linguistics. 29632977.",
  "Shiyu Chang, Yang Zhang, Mo Yu, and Tommi S Jaakkola. 2020. Invariantrationalization. In Proceedings of the 37th International Conference on MachineLearning. 14481458": "Howard Chen, Jacqueline He, Karthik Narasimhan, and Danqi Chen. 2022. CanRationalization Improve Robustness?. In Proceedings of the 2022 Conference of theNorth American Chapter of the Association for Computational Linguistics: HumanLanguage Technologies. 37923805. Kyunghyun Cho, Bart van Merrinboer, alar Gulehre, Dzmitry Bahdanau,Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning PhraseRepresentations using RNN EncoderDecoder for Statistical Machine Translation.In Proceedings of the 2014 Conference on Empirical Methods in Natural LanguageProcessing (EMNLP). 17241734. Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020.ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.In In Proceedings of the 8th International Conference on Learning Representations. Marina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, andPrithviraj Sen. 2020. A Survey of the State of Explainable AI for Natural LanguageProcessing. In Proceedings of the 1st Conference of the Asia-Pacific Chapter ofthe Association for Computational Linguistics and the 10th International JointConference on Natural Language Processing. 447459. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding. InProceedings of the 2019 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies, Volume 1 (Long andShort Papers). 41714186.",
  "Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016. Rationalizing Neural Pre-dictions. In Proceedings of the 2016 Conference on Empirical Methods in NaturalLanguage Processing. 107117": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, AbdelrahmanMohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART:Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,Translation, and Comprehension. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics. 78717880. Wei Liu, Zhiying Deng, Zhongyu Niu, Jun Wang, Haozhao Wang, YuanKai Zhang,and Ruixuan Li. [n. d.]. Is the MMI Criterion Necessary for Interpretability?Degenerating Non-causal Features to Plain Noise for Self-Rationalization. In TheThirty-eighth Annual Conference on Neural Information Processing Systems. Wei Liu, Haozhao Wang, Jun Wang, Zhiying Deng, Yuankai Zhang, Cheng Wang,and Ruixuan Li. 2024. Enhancing the rationale-input alignment for self-explainingrationalization. In 2024 IEEE 40th International Conference on Data Engineering(ICDE). 22182230. Wei Liu, Haozhao Wang, Jun Wang, Ruixuan Li, Xinyang Li, YuanKai Zhang, andYang Qiu. 2023. MGR: Multi-generator Based Rationalization. In Proceedings ofthe 61st Annual Meeting of the Association for Computational Linguistics (Volume1: Long Papers). 1277112787. Wei Liu, Haozhao Wang, Jun Wang, Ruixuan Li, Chao Yue, and Yuankai Zhang.2022. FR: folded rationalization with a unified encoder. In Proceedings of the 36thInternational Conference on Neural Information Processing Systems. 69546966.",
  "Julian McAuley, Jure Leskovec, and Dan Jurafsky. 2012. Learning attitudes andattributes from multi-aspect reviews. In In 2012 IEEE 12th International Conferenceon Data Mining. 10201025": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limitsof transfer learning with a unified text-to-text transformer. Journal of machinelearning research 21, 140 (2020), 167. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N. Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all youneed. In Proceedings of the 31st International Conference on Neural InformationProcessing Systems. 60006010. Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010. Latent aspect ratinganalysis on review text data: a rating regression approach. In Proceedings of the16th ACM SIGKDD international conference on Knowledge discovery and datamining. 783792. Mo Yu, Shiyu Chang, Yang Zhang, and Tommi Jaakkola. 2019. Rethinking Coop-erative Rationalization: Introspective Extraction and Complement Control. InProceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-cessing and the 9th International Joint Conference on Natural Language Processing(EMNLP-IJCNLP). 40944103. Mo Yu, Yang Zhang, Shiyu Chang, and Tommi Jaakkola. 2021. UnderstandingInterlocking Dynamics of Cooperative Rationalization. In Proceedings of the 35thInternational Conference on Neural Information Processing Systems. 1282212835. Linan Yue, Qi Liu, Yichao Du, Yanqing An, Li Wang, and Enhong Chen. 2022.DARE: disentanglement-augmented rationale extraction. In Proceedings of the36th International Conference on Neural Information Processing Systems. 2660326617. Linan Yue, Qi Liu, Yichao Du, Li Wang, Weibo Gao, and Yanqing An. 2024. To-wards Faithful Explanations: Boosting Rationalization with Shortcuts Discovery.In In Proceedings of the 12th International Conference on Learning Representations. Linan Yue, Qi Liu, Li Wang, Yanqing An, Yichao Du, and Zhenya Huang. 2023.Interventional Rationalization. In Proceedings of the 2023 Conference on EmpiricalMethods in Natural Language Processing. 1140411418. Wenbo Zhang, Tong Wu, Yunlong Wang, Yong Cai, and Hengrui Cai. 2023.Towards trustworthy explanation: on causal rationalization. In Proceedings of the40th International Conference on Machine Learning. 4171541736. Yiming Zheng, Serena Booth, Julie Shah, and Yilun Zhou. 2022. The Irrationalityof Neural Rationale Models. In Proceedings of the 2nd Workshop on TrustworthyNatural Language Processing (TrustNLP 2022). 6473.",
  "= AttentionHead() = .(20)": "Therefore, we use the score , to represent how much attentionthat token pays to token in the -th attention head of thetransformer. The attention weight of token on all tokens in the -th attention header can be expressed as the attention-weight vector =,1, ,,. Figures 9(a) and 9(b) illustrate the distribu-tion of attention-weight vectors for different tokens in the first sixlayers and the last six layers of BERT, respectively. We observedthat in the initial layers, there are significant discrepancies in thedistribution of the weight vectors. In the final layers, the attention-weight vectors converge, reaching full clustering in the last layerwith minimal distribution discrepancies. The clustering of the fi-nal layer attention-weight vectors indicates that all tokens exhibitsimilar attention dependencies. This will result in a highly similartoken representation, which we refer to as token homogeneity. A.2Analysis of matchFirst, we assume that > 0 in Eq. 16. In the early training phase, thegenerator selects poor rationales, yet the complete text encompassesall necessary information for better prediction support. Thus we canobtain: ( | ) ( | ). The entropy of the actual distributioninfluences the value of the cross-entropy loss; higher entropy inthe actual distribution typically results in a higher cross-entropyloss for a given prediction distribution. So there is:",
  "( | ) ( | ) task task .(21)": "At this time, = task task 0, and we expect to optimize therationale quality by minimizing match = (). Thus, with theguarantee that > 0, () monotonically increases when 0.In the later training phase, the generator selects superior ratio-nales, while the complete text includes tokens that are irrelevantto the labels. So ( | ) ( | ). Similar to Equation 21:",
  "BERT-RNPPLMR(ours)": "Aspect: Beer-AromaAspect: Beer-AromaLabel: Positive, Pred: PositiveLabel: Positive, Pred: PositiveText: this beer poured out as a nice golden color with a whitehead on top . the head retention was pretty good on this brew . ifound the smell of the beer to have a nice aroma of caramel andsome light hops on the nose . the taste of the beer was reallynicely done i thought . the flavors of sweet malts and the bitterfinish worked well together . the mouthfeel was very drinkableand could easlier have serval back to back . overall this brew ispretty good and i would nt nmind drinking it again one day . Text: this beer poured out as a nice golden color with a whitehead on top . the head retention was pretty good on this brew . ifound the smell of the beer to have a nice aroma of caramel andsome light hops on the nose . the taste of the beer was reallynicely done i thought . the flavors of sweet malts and the bitterfinish worked well together . the mouthfeel was very drinkableand could easlier have serval back to back . overall this brew ispretty good and i would nt nmind drinking it again one day .",
  "BeerAdvocate": "Train on the appearance aspectLabel:Appearance(0.6,positive),Aroma(0.4,negative),Palate(0.3, negative)Text: a : poured a hazy amber color with a nice white cap s :faded piney hops , caramel t : caramel and faded hops battlefor dominance while the metallic and old earthy notes fight forsecond place . bad . m : medium to full body and not one i wouldrecommend hiolding on your palate o : poor ipa , drainpouredafter two sipsPrediction(Bert): Appearance(positive), Aroma(negative),Palate(negative)Prediction(GRU): Appearance(positive), Aroma(positive),Palate(positive)",
  "BMORE RESULTSB.1The example in": "To more clearly illustrate how PLMs like BERT can strengthen theimpact of spurious correlations, we add the following example textin .We trained classifiers using BERT and GRU, respectively, basedon appearance. Next, predictions were made on three aspects ofthe text. In , the BERT-based classifier still made correctpredictions on the untrained aspects of aroma and palate, while theGRU-based classifier made incorrect predictions.",
  "B.2Parameters Sensitivity Analysis": "We conducted experiments on three aspects of the BeerAdvocateand the HotelReview Datasets. As shown in , both smalland large values of lead to lower F1 scores. According to theanalysis in Appendix A.2, we expect = task task 0 whenthe rationale is poor and 0 when the rationale is good; however,a large leads to a rapid decrease in , which results in 0 whenthe rationale is poor. Similarly, a smaller leads to a slow decreasein , leading to 0 when the rationale is better. thus neither largenor small meets our expectations of match , or even seriouslyaffects the rationalization.",
  "CEXPERIMENTAL SETUPC.1Implementation Details": "During training, the learning rate was set to 5e-6 for transformerlayers and 2e-5 for the MLP in the predictor and the dimension-reduction layer. The batch size was 64, and the maximum sequencelength was 256. We trained for 30 epochs on the BeerAdvocatedataset and 100 epochs on the HotelReview dataset. For hyperpa-rameter tuning, parameter was set to 1, and sparsity control wasadjusted within 0.1-0.3 to ensure rationale percentages of around0.1, 0.2, or 0.3. 1 and 2 were also adjusted to find the optimalsettings. For a fair comparison, the experiments in used ashort-text version of the dataset by filtering the texts over a lengthof 120, as was done with CR. All experiments were conducted usingPyTorch on A100 GPUs."
}