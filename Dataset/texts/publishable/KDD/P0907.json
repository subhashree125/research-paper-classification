{
  "Abstract": "Graphs are ubiquitous for modeling complex relationships betweenobjects across various fields. Graph neural networks (GNNs) havebecome a mainstream technique for graph-based applications, buttheir performance heavily relies on abundant labeled data. To re-duce labeling requirement, pre-training and prompt learning hasbecome a popular alternative. However, most existing prompt meth-ods do not differentiate homophilic and heterophilic characteristicsof real-world graphs. In particular, many real-world graphs arenon-homophilic, not strictly or uniformly homophilic with mix-ing homophilic and heterophilic patterns, exhibiting varying non-homophilic characteristics across graphs and nodes. In this paper,we propose ProNoG, a novel pre-training and prompt learningframework for such non-homophilic graphs. First, we analyze exist-ing graph pre-training methods, providing theoretical insights intothe choice of pre-training tasks. Second, recognizing that each nodeexhibits unique non-homophilic characteristics, we propose a con-ditional network to characterize the node-specific patterns in down-stream tasks. Finally, we thoroughly evaluate and analyze ProNoGthrough extensive experiments on ten public datasets. Codes areavailable at",
  "Co-first authors. Work was done while at the University of Tokyo.Corresponding authors": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 25, August 37, 2025, Toronto, ON, Canada. 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-1245-6/25/08",
  "Introduction": "Graph data are pervasive in real-world applications, such as citationnetworks , social networks , and molecular graphs . Traditionalmethods typically train graph neural networks (GNNs) orgraph transformers in a supervised manner. However, theyrequire re-training and substantial labeled data for each specifictask.To mitigate the limitations of supervised methods, pre-trainingmethods have gained significant traction . Theyfirst learn universal, task-independent properties from unlabeledgraphs, and then fine-tune the pre-trained models to various down-stream tasks using task-specific labels . However, a signif-icant gap occurs between the pre-training objectives and down-stream tasks, resulting in suboptimal performance . More-over, fine-tuning large pre-trained models is costly and still requiressufficient task-specific labels to prevent overfitting. As an alter-native to fine-tuning, prompt learning has emerged as a popularparameter-efficient technique for adaptation to downstream tasks. They first utilize a universal template to unifypre-training and downstream tasks. Then, a learnable prompt isemployed to modify the input features or hidden embeddings ofthe pre-trained model to align with the downstream task withoutupdating the pre-trained weights. Since a prompt has far fewerparameters than the pre-trained model, prompt learning can beespecially effective in low-resource settings .However, current graph pre-train, prompt approaches relyon the homophily assumption or overlook the presence of het-erophilic edges. Specifically, the homophily assumption states that neighboring nodes should share the same labels, whereasheterophily refers to the opposite scenario where two neighboringnodes have different labels. We observe that real-world graphs aretypically non-homophilic, meaning they are neither strictly or uni-formly homophilic and mix both homophilic and heterophilic patterns. In this work, we investigate the pre-training and promptlearning methodology for non-homophilic graphs. We first revisitexisting graph pre-training methods for such graphs, followedby proposing a Prompt learning framework for Non-homophilicGraphs (or ProNoG in short). The solution is non-trivial, as thenotion of homophily encompasses two key aspects, each with itsown unique challenge.",
  ": Non-homophilic characteristics of graphs": "First, different graphs exhibit varying degrees of non-homophily.As shown in (a), the Cora citation network that is generally con-sidered largely homophilic with 81% homophilic edges1, whereasthe Wisconsin webpage graph links different kinds of webpages,which is highly heterophilic with only 21% homophilic edges. More-over, the non-homophilic characteristics of a graph also depends onthe target label. For example, in a dating network shown in (a),taking gender as the node label, the graph is more heterophilicwith 2/7 homophilic edges. However, taking hobbies as the nodelabel, the graph becomes more homophilic with 4/7 homophilicedges. Hence, how do we pre-train a graph model irrespective of thegraphs homophily characteristics? In this work, we propose defi-nitions for homophily tasks and homophily samples. We show thatpre-training with non-homophily samples increases the loss of anyhomophily task. Meanwhile, a less homophilic graph results in ahigher number of non-homophily samples, subsequently increas-ing the pre-training loss for homophily tasks. This motivates us tomove away from homophily tasks for graph pre-training and instead choose a non-homophily task .Second, different nodes within the same graph are distributeddifferently in terms of their non-homophilic characteristics. Asshown in (c), on both Cora and Cornell, their nodes havea diverse homophily ratios2. Hence, how do we capture the fine-grained, node-specific non-homophilic characteristics? Due to thediverse characteristics across nodes, a one-size-fits-all solution forall nodes would be inadequate. However, existing approaches gen-erally apply a single prompt to all nodes , treating allnodes uniformly. Thus, these methods overlook the fine-grainednode-wise non-homophilic characteristics, leading to suboptimal",
  "Defined as edges connecting two nodes of the same label; see Eq. (1) in Sect. 3.2Defined as the fraction of a nodes neighbors with the same label; see Eq. (2) in Sect. 3": "performance. For example, a standard graph prompt learning ap-proach generally performs worse when the homophily ratiosof nodes decrease, as shown in (d),even with a non-homophilypretext task . Though some recent works have proposednode-specific prompts, they are not designed to account for thevariation in nodes non-homophilic charactersitics. Inspired by con-ditional prompt learning , we propose generating a uniqueprompt from each node with a conditional network (condition-net)to capture the fine-grained, distinct characteristics of each node.We first capture the non-homophilic patterns of each node by read-ing out its multi-hop neighborhood. Then, conditioned on thesenon-homophilic patterns, the condition-net produces a series ofprompts, one for each node that reflects its varying non-homophiliccharacteristics. These prompts can adjust the node embeddings tobetter align them with the downstream task.In summary, the contributions of this work are threefold: (1)We observe varying degrees of homophily across graphs, whichmotivates us to revisit graph pre-training tasks. We provided theo-retical insights which guide us to choose non-homophily tasks forgraph pre-training. (2) We further observe that, within the samegraph, different nodes have diverse distributions of non-homophiliccharacteristics. To adapt to the unique non-homophilic patterns ofeach node, we propose the ProNoG framework for non-homophilicprompt learning, which is equipped with a condition-net to gener-ate a series of prompts conditioned on each node. The node-specificprompts enables fine-grained, node-wise adaptation for the down-stream tasks. (3) We perform extensive experiments on ten bench-mark datasets, demonstrating the superior performance of ProNoGcompared to a suite of state-of-the-art methods.",
  "Related Work": "Graph representation learning. GNNs are mainstream technique for graph representation learning. Theytypically operate on a message-passing framework, where nodesiteratively update their representations by aggregating messagesreceived from their neighboring nodes . How-ever, the effectiveness of GNNs heavily relies on abundant task-specific labeled data and requires re-training for various tasks.Inspired by the success of pre-training methods in the language and vision domains, pre-training methods have been widely exploredfor graphs. These methods first pre-train a graph encoder based onself-supervised tasks, then transfer prior knowledge to downstreamtasks. However, all these GNNs and pre-training methods are basedon the homophilic assumption, overlooking that real-world graphsare generally non-homophilic. Non-homophilic graph learning. Many GNNs have been proposed for non-homophilic graphs, employingmethods such as capturing high-frequency signals , discover-ing potential neighbors , and high-order message passing. Moreover, recent works have explored pre-training on non-homophilic graphs by capturing neighborhood infor-mation to construct unsupervised tasks for pre-training the graphencoder and then transferring prior non-homophilic knowledgeto downstream tasks through fine-tuning with task-specific super-vision. However, a significant gap exists between the objectives",
  "Non-Homophilic Graph Pre-Training and Prompt LearningKDD 25, August 37, 2025, Toronto, ON, Canada": "of pre-training and fine-tuning . While pre-training fo-cuses on learning inherent graph attributes without supervision,fine-tuning adapts these insights to downstream tasks based ontask-specific supervision. This discrepancy hinders effective knowl-edge transfer and negatively impacts downstream performance. Graph prompt learning. Originally developed for the languagedomain, prompt learning effectively unifies pre-training and down-stream objectives . Recently, graph prompt learning hasemerged as a popular alternation to fine-tuning methods . These methods first propose a unified template,then design prompts specifically tailored to each downstream task,allowing them to better align with the pre-trained model whilekeeping the pre-trained parameters frozen. However, current graphprompt learning methods typically assume graphs are homophilic, neglecting the fact that real-world graphs are generally non-homophilic, exhibiting a mixture of homophilic and heterophilicpatterns. Furthermore, these methods usually apply a single promptfor all nodes, overlooking the unique characteristics of each nodesnon-homophilic pattern.",
  "Preliminaries": "Graph. A graph is defined as = (, ), where representsthe set of nodes and represents the set of edges. The nodes arealso associated with a feature matrix X R|V|, such that x R is a row of X representing the feature vector for node . For a collection of multiple graphs, we use the notation G ={1,2, . . . , }. Homophily ratio. Given a mapping between the nodes of a graphand a predefined set of labels, let denote the label mapped to node. The homophily ratio H () evaluates the relationships betweenthe labels and the graph structure , measuring the fractionof homophilic edges whose two end nodes share the same label.More concretely,",
  "where |N ()| is the set of neighboring nodes of . Note that bothH () and H () are in . Graphs or nodes with a larger propor-tion of homophilic edges have a higher homophily ratio": "Graph encoder. Graph encoders learn latent representations ofgraphs, embedding their nodes into some feature space. A widelyused family of graph encoders is GNNs, which typically utilize amessage-passing mechanism . Specifically, each node aggre-gates messages from its neighbors to generate its own representa-tion. By stacking multiple layers, GNNs enables recursive messagepassing throughout the graph. Formally, the embedding of a node in the -th GNN layer, denoted as h, is computed as follows.",
  "h = Aggr(h1, {h1: N ()};),(3)": "where are the learnable parameters in the -th layer, and Aggr()is the aggregation function, which can take various forms . In the first layer, the input node embedding h0 is typicallyinitialized from the node feature vector x. The full set of learnableparameters is denoted as = {1,2, . . .}. For simplicity, we definethe output node representations of the final layer as h, which canthen be fed into the loss function for a specific task. Problem statement. In this work, we aim to pre-train a graph en-coder and develop a prompt learning framework for non-homophilicgraphs. More specifically, both the pre-training and prompt learn-ing are not sensitive to the homophilic characteristics of the graphand its nodes.To evaluate our non-homophilic pre-training and prompt learn-ing, we focus on two common tasks on graph data: node classi-fication and graph classification, in few-shot settings. For nodeclassification within a graph = (, ), let be the set of nodeclasses. Each node has a class label . Similarly, forgraph classification across a set of graphs G, let Y be the set ofpossible graph labels. Each graph G has a class label Y.In the few-shot setting, there are only labeled samples per class,where is a small number (e.g., 10). This scenario is known as-shot classification . Note that the homophily ratio isdefined with respect to some predefined set of labels, which maynot be related to the class labels in downstream tasks.",
  "A sim(h, h)A sim(h, h) + B sim(h, h) ,(5)": "where sim(, ) represents a similarity function such as cosine sim-ilarity in our experiment, A is the set of positive instancesfor node , and B is the set of negative instances for . The opti-mization objective of task in Eq. (4) is to maximize the similaritybetween and its positive instances while minimizing the similaritybetween and its negative instances. Based on this loss, we furtherpropose the definitions of homophily tasks and homophily samples. Definition 1 (Homophily Task). On a graph = (, ), a pre-training task = ({A : }, {B : }) is a homophily taskif and only if, , A, B, (,) (,) . Atask that is not a homophily task is called a non-homophily task.",
  "Theorem 1. For a homophily task , adding a homophily samplealways results in a smaller loss than adding a non-homophily sample": "Proof. Consider a homophily sample (,,) for some (,) and (,) , as well as a non-homophily sample (,,) forsome (,) and (,) . Let the overall loss with (,,)be , and that with (,,) be . Since (,,) is homophily,we have sim(,) > sim(,), and thus (,,) > 0.5. Moreover,since (,,) is non-homophily, we have sim(,) sim(,),and thus (,,) 0.5. Hence, (,,) > (,,), implyingthat < .",
  "Non-homophilic Graph Pre-training": "Consider a homophily task . According to Theorem 2, for non-homophilic graphs with lower homophily ratios, on average thereare fewer homophily samples and more non-homophily samplesfor . Consequently, based on Theorem 1, adding a non-homophilysample always results in a larger loss than adding a homophilysample. Therefore, for non-homophilic graphs, especially thosewith low homophily ratio, non-homophily tasks are a better choicecompared to homophily tasks when optimizing the training loss.We revisit mainstream graph pre-training methods and catego-rize them into two categories: homophily methods that employhomophily tasks, and non-homophily methods that do not. Specifi-cally, GMI , GraphPrompt , MultiGPrompt , HGPrompt and GraphPrompt+ are all homophily methods, since theirpre-training tasks utilizes a form of link prediction, where A is aset of nodes linked to , and B is a set of nodes not linked from. In contrast, DGI , GraphCL , and GraphACL are non-homophily methods, since A and B in their pre-trainingtasks are not related to the connectivity with . Further detailsof these methods are shown in Appendix. B. In our experiment,we use the non-homophily method GraphCL as the pretext taskto obtain our main results for non-homophily graphs, as it is aclassic pre-training method with competitive performance. We alsoexperiment with link prediction and GraphACL for furtherevaluation, as shown in .",
  "Overall framework": "We illustrate the overall framework of ProNoG in . It involvestwo stages: (a) graph pre-training and (b) downstream adaptation.In graph pre-training, we pre-train a graph encoder using a non-homophilic pre-training task, as shown in (a). Subsequently,to adapt the pre-trained model to downstream tasks, we proposea conditional network (condition-net) that generates a series ofprompts, as depicted in (b). As a result, each node is equippedwith its own prompt, which can be used to modify its featuresto align with the downstream task. More specifically, the promptgeneration is conditioned on the unique patterns of each node, inorder to achieve fine-grained adaptation catering to the diversenon-homophilic characteristics of each node, as detailed in (c).",
  "Prompt Generation and Tuning": "Prompt generation. In non-homophilic graphs, different nodesare characterized by unique non-homophilic patterns. Specifically,different nodes typically have diverse homophily ratios H (), indi-cating distinct topological structures linking to their neighboringnode. Moreover, even nodes with similar homophily ratios mayhave different neighborhood distributions in terms of the varyinghomophily ratios of the neighboring nodes. Therefore, instead oflearning a single prompt for all nodes as in standard graph promptlearning , we design a condition-net to generatea series of non-homophilic pattern-conditioned prompts. Conse-quently, each node is equipped with its own unique prompt, aimingto adapt to its distinct non-homophilic characteristics.First, the non-homophilic patterns of a node can be characterizedby considering a multi-hop neighborhood around the node. Specifi-cally, given a node , we readout their -hop ego-network , whichis an induced subgraph containing the node and nodes reachablefrom in at most steps. Inspired by GGCN ,the readout isweighted by the similarity between and their neighbors, as shownin (c) , obtaining a representation of the subgraph given by",
  ": Overall framework of ProNoG": "Next, for each downstream task, our goal is to assign a uniqueprompt vector to each node. However, directly parameterizing theseprompt vectors would significantly increase the number of learn-able parameters, which may overfit to the lightweight supervisionin few-shot settings. To cater to the unique non-homophilic char-acteristics of each node with minimal parameters, we propose toemploy a condition-net to generate node-specific prompt vec-tors. Specifically, conditioned on the subgraph readout s of a node, the condition-net generates a unique prompt vector for w.r.t. atask , denoted by p,, as follows.",
  "p, = CondNet(s;),(8)": "where CondNet is the condition-net parameterized by . It outputsa unique prompt vector p,, which varies based on the input that characterizes the non-homophily patterns of node . Note thatthis is a form of hypernetworks , which employs a secondarynetwork to generate the parameters for the main network condi-tioned on the input feature. In our context, the condition-net isthe secondary network, generating prompt parameters without ex-panding the number of learnable parameters in the main network.The secondary network CondNet can be any learnable function,such as a fully-connected layer or a multi-layer perceptron (MLP).We employ an MLP with a compact bottleneck architecture .Subsequently, we perform fine-grained, node-wise adaptation totask . Concretely, the prompt p, for node is employed to adjusts features or its embeddings in the hidden or output layers . Inour experiments, we choose a simple yet effective implementationthat modifies the nodes output embeddings through an element-wise product, as follows.",
  "where the prompt p, is generated with an equal dimension as h": "Prompt tuning. In this work, we focus on two common typesof downstream task: node classification and graph classification.The prompt tuning process does not directly optimize the promptvectors; instead it optimizes the condition-net, which subsequentlygenerates the prompt vectors, for a given downstream task.We utilize a loss function based on node/graph similarity fol-lowing previous work . Formally, for a task with a labeledtraining set D = {(1,1), (2,2), . . .}, where can be either anode or a graph, and is s class label from a set of classes",
  "exp1 sim( h,, h,) , (10)": "where h, denotes the output embedding of node /graph fortask . Specifically, for node classification h, is the output embed-ding in Eq. 9; for graph classification, h, = h, involvingan additional graph readout. The prototype embedding for class ,h,, is the average of the output embedding of all nodes/graphsbelonging to class .During prompt tuning, we update only the lightweight parame-ters of the condition-net (), while freezing the pre-trained GNNweights. Thus, our prompt tuning is parameter-efficient and amenableto few-shot settings, where D contains only a small number oftraining examples for task .",
  "Algorithm. We detail the main steps for conditional prompt gen-eration and tuning in Algorithm 1, Appendix A": "Complexity analysis. For a downstream graph , the computa-tional process of ProNoG involves two main parts: encoding nodesvia a pre-trained GNN, and conditional prompt learning. The firstparts complexity is determined by the GNNs architecture, akin toother methods employing a pre-trained GNN. In a standard GNN,each node aggregates features from up to neighbors per layer.Assuming the aggregation involves at most neighbors, the com-plexity of calculating node embeddings over layers is ( | |),where | | denotes the number of nodes. The second part, condi-tional prompt learning, has two stages: prompt generation andprompt tuning. In the prompt generation stage, each subgraph em-bedding is fed into the condtion-net. In our experment, we use a 2layer MLP as condition-net, resulting in a complexity of (2 | |).During prompt tuning, each node in is adjusted using a promptvector, with a complexity of (| |). Therefore, the total complexityfor conditional prompt learning is (3 | |).In conclusion, the overall complexity of ProNoG is (( +3) | |). The first part dominates the overall complexity, as (",
  "Experimental Setup": "Datasets. We conduct experiments on ten benchmark datasets.Wisconsin , Cornell , Chameleon , and Squirrel areall webpage graphs. Each dataset features a single graph wherenodes correspond to web pages and edges represent hyperlinksconnecting these pages. Cora and Citeseer are citationnetworks. These datasets consist of a single graph each, with nodessignifying scientific papers and edges indicating citation relation-ships. PROTEINS consists of a series of protein graphs. Nodesin these graphs denote secondary structures, while edges depictneighboring relationships either within the amino acid sequenceor in three-dimensional space. ENZYMES , BZR , and COX2 are collections of molecular graphs. These datasets describeenzyme structures from the BRENDA enzyme database, ligands re-lated to benzodiazepine receptors, and cyclooxygenase-2 inhibitors,respectively. We summarize these datasets in , and presentfurther details in Appendix C. Baselines. We evaluate ProNoG against a series of state-of-the-artmethods, categorized into three primary groups:(1) End-to-end graph neural networks: GCN , GAT , H2GCN, and FAGCN are trained in a supervised manner directlyusing downstream labels. Specifically, GCN and GAT are originallydesigned for homophilic graphs, H2GCN for heterophilic graphs,and FAGCN for non-homophilic graphs.(2) Graph pre-training models: DGI , GraphCL , DSSL, GraphACL follow the pre-train, fine-tune paradigm.(3) Graph prompt learning models: GPPT , GraphPrompt ,and GraphPrompt+ use self-supervised pre-training tasks witha single type of prompt for downstream adaptation. Note that GPPTis specifically designed for node classification and cannot be directlyused for graph classification. Therefore, in our experiments, we useGPPT exclusively for node classification tasks.We provide further details on these baselines in Appendix D. Itsworth noting that some graph few-shot learning methods, such asMeta-GNN , AMM-GNN , RALE , VNT , and ProG, are based on the meta-learning paradigm , which requiresan additional set of labeled base classes in addition to the few-shotclasses. Consequently, these methods are not directly comparableto our framework. Parameter settings. For all baselines, we use the original authorscode and follow their recommended settings, while further tuningtheir hyperparameters to ensure optimal performance. Detaileddescriptions of the implementations and settings for both the base-lines and our ProNoG are provided in Appendix E.",
  "Setup of downstream tasks. We conduct two types of down-stream task: node classification, and graph classification. These tasksare set up as -shot classification problems, meaning that for each": "class, instances (nodes or graphs) are randomly selected for su-pervision. Given that all low-homophily datasets, i.e., Wisconsin,Squirrel, Chameleon and Cornell only comprise a single graph andcannot be directly used for graph classification. Thus, following pre-vious research , we generate multiple graphs by construct-ing ego-networks centered on the labeled nodes in each dataset.We then perform graph classification on these ego-networks, eachlabeled according to its central node. For datasets with high ho-mophily ratios, PROTEINS, ENZYMES, BZR and COX2 have originalgraph labels, so we directly conduct graph classification on thesegraphs. Since the -shot tasks are balanced classification problems,we use accuracy to evaluate performance, in line with prior stud-ies . We pre-train the graph encoder once for eachdataset and then use the same pre-trained model for all downstreamtasks. We generate 100 -shot tasks for both node classification andgraph classification by repeating the sampling process 100 times.Each task is executed with five different random seeds, leading to atotal of 500 results per task type. We report the mean and standarddeviation of these 500 outcomes.",
  "We first evaluate one-shot classification tasks. Then, we vary thenumber of shots to investigate their impact on performance": "One-shot performance. We present the results of one-shot nodeand graph classification tasks on non-homophilic graphs in Ta-bles 1 and 2, respectively. We make the following observations: (1)ProNoG surpasses all baseline methods across all settings, outper-forming the best competitor by up to 21.49% on node classificationand 6.50% on graph classification. These results demonstrate itseffectiveness in learning prior knowledge from non-homophilicgraphs and capturing nodes specific patterns. (2) Other graphprompt learning methods, i.e., GPPT, GraphPrompt, and Graph-Prompt+, significantly lag behind ProNoG. Their suboptimal per-formance can be attributed to their inability to account for a varietyof node-specific patterns. These results underscore the importanceof our conditional prompting in characterizing node embeddingsto capture nodes specific patterns. (3) GPPT is at best comparableto, and often performs worse than other baselines because it is notspecifically designed for few-shot learning. Few-shot performance. To assess the performance of ProNoGwith different amounts of labeled data, we vary the number ofshots in the downstream tasks and present the results in and Appendix F. Note that given the limited number of nodes inWisconsin and Cornell, we only conduct tasks up to 3-shot. Weobserve that: (1) ProNoG significantly outperforms all baselinesin low-shot scenarios with very limited labeled data (e.g., 5),showcasing the effectiveness of our approach in these situations.(2) As the number of shots increases, all methods generally showimproved performance as expected. However, ProNoG remainscompetitive and often surpasses the other methods, demonstratingthe robustness of ProNoG.",
  "MethodsWisconsinSquirrelChameleonCornellPROTEINSENZYMESCiteseerCora": "GCN21.39 6.5620.00 0.2925.11 4.1921.81 4.7143.32 9.3548.08 4.7131.27 4.5328.57 5.07GAT28.01 5.4021.55 2.3024.82 4.3523.03 13.1931.79 20.1135.32 18.7230.76 5.4028.40 6.25H2GCN23.60 4.6421.90 2.1525.89 4.9632.77 14.8829.60 6.9937.27 8.7326.98 6.2534.58 9.43FAGCN35.03 17.9220.91 1.7922.71 3.7428.67 17.6432.63 9.9435.87 13.4726.46 6.3428.28 9.57 DGI28.04 6.4720.00 1.8619.33 4.5732.54 15.6645.22 11.0948.05 14.8345.00 9.1954.11 9.60GraphCL29.85 8.4621.42 2.2227.16 4.3124.69 14.0646.15 10.9448.88 15.9843.12 9.6151.96 9.43DSSL28.46 10.3120.94 1.8827.92 3.9320.36 5.3840.42 10.0866.59 19.2839.86 8.6040.79 7.31GraphACL34.57 10.4624.44 3.9426.72 4.6733.17 16.0642.16 13.5047.57 14.3635.91 7.8746.65 9.54 GPPT27.39 6.6720.09 0.9124.53 2.5525.09 2.9235.15 11.4035.37 9.3721.45 3.4515.37 4.51GraphPrompt31.48 5.1821.22 1.8025.36 3.9931.00 13.8847.22 11.0553.54 15.4645.34 10.5354.25 9.38GraphPrompt+31.54 4.5421.24 1.8225.73 4.5031.65 14.4846.08 9.9657.68 13.1245.23 10.0152.51 9.73",
  "MethodsWisconsinSquirrelChameleonCornellPROTEINSENZYMESBZRCOX2": "GCN21.39 6.5611.77 3.1017.21 4.8026.36 4.3551.66 10.8719.30 6.3645.06 16.3043.84 13.94GAT24.93 7.5920.70 1.5125.71 3.3222.66 12.4651.33 11.0220.24 6.3946.28 15.2651.72 13.70H2GCN22.23 6.3820.69 1.4226.76 3.9823.11 11.7853.81 8.8519.40 5.5750.28 12.1353.70 11.73FAGCN23.81 9.5020.83 1.4325.93 4.0325.71 13.1255.45 11.5719.95 5.9450.93 12.4150.22 11.50 DGI29.77 6.2220.50 1.5224.29 4.3318.60 12.7950.32 13.4721.57 5.3749.97 12.6354.84 14.76GraphCL27.93 5.2721.01 1.8626.45 4.3020.03 10.0554.81 11.4419.93 5.6550.50 18.6247.64 22.42DSSL22.05 3.9020.74 1.6126.19 3.7218.38 10.6352.73 10.9823.14 6.7149.04 8.7554.23 14.17GraphACL22.98 5.8920.80 1.2826.28 3.9326.50 17.1856.11 13.9520.28 5.6049.24 17.8749.59 23.93",
  "WisconsinSquirrelChameleon PROTEINS ENZYMESCiteseerWisconsinSquirrelChameleon PROTEINS ENZYMESCOX2": "NoPrompt25.413.1320.601.3022.713.5447.2211.05 66.5919.2843.129.6120.856.7420.181.3022.344.1553.618.9021.856.1754.2917.31SinglePrompt32.765.2120.851.3222.783.3530.3319.5965.3221.67 48.6410.0925.776.2420.680.9127.033.9856.3510.5919.387.1247.2415.53NodeCond35.564.6521.263.9521.132.2336.0119.7068.5419.31 48.3010.2225.304.6220.981.5627.245.2456.6110.0320.706.6755.9214.66ProNoG\\sim30.654.0520.050.5920.964.2133.7317.8236.0220.6448.742.6622.055.8619.930.4220.201.1152.3010.9416.701.2850.0517.67ProNoG44.7211.93 24.593.4130.673.7348.9510.85 72.9420.23 49.0210.66 31.545.30 20.921.3728.505.3056.1110.1922.556.70 56.4614.57 with a classifier for downstream adaptation. SinglePrompt uses asingle prompt instead of conditional prompting to modify all nodes.NodeCond directly uses the output embedding of the pre-trainedgraph encoder as input to the condition-net to generate the promptwithout reading out the subgraph in Eq. 7. ProNoG\\sim readoutthe subgraph via mean-pooling without similarity between cen-tral nodes and their neighbors as in Eq. 7. As shown in ,ProNoG consistently outperforms or is at least competitive withthese variants. This highlights the necessity of readout subgraphsweighted by similarity to capture nodes non-homophilic patterns,",
  "Analysis on Pre-Training Methods": "To further evaluate homophily and non-homophily tasks, usingProNoG for downstream adaptation, we employ homophily taskslink prediction used in GraphPrompt , and non-homophily tasksGraphCL and DSSL , respectively. Note that link predicitionin GPPT is in a generative format, thus falling beyond thescope of homophily task, but its also affected by non-homophilyin graphs. We compare these pretext tasks and show the results in",
  "Conclusions": "In this paper, we explored pre-training and prompt learning onnon-homophilic graphs. The objectives are twofold: learning com-prehensive knowledge irrespective of the varying non-homophilycharacteristics of graphs, and adapting the nodes with diverse distri-butions of non-homophily patterns to downstream applications in afine-grained, node-wise manner. We first revisit graph pre-trainingon non-homophilic graphs, providing theoretical insights into thechoice of pre-training tasks. Then, for downstream adaptation, weproposed condition-net to generate a series of prompts conditionedon various non-homophilic patterns across nodes. Finally, we con-ducted extensive experiments on ten public datasets, demonstrat-ing that ProNoG significantly outperforms diverse state-of-the-artbaselines.",
  "Deyu Bo, Xiao Wang, Chuan Shi, and Huawei Shen. 2021. Beyond low-frequencyinformation in graph convolutional networks. In AAAI. 39503957": "Karsten M Borgwardt, Cheng Soon Ong, Stefan Schnauer, SVN Vishwanathan,Alex J Smola, and Hans-Peter Kriegel. 2005. Protein function prediction via graphkernels. Bioinformatics 21, suppl_1 (2005), i47i56. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shot learners. NeurIPS 33 (2020),18771901.",
  "Taoran Fang, Yunchao Zhang, Yang Yang, Chunping Wang, and Lei Chen. 2024.Universal prompt tuning for graph neural networks. NeurIPS (2024)": "Yuchen Fang, Yanjun Qin, Haiyong Luo, Fang Zhao, Bingbing Xu, Liang Zeng,and Chenxing Wang. 2023. When spatio-temporal meet wavelets: Disentangledtraffic forecasting via efficient spectral graph attention networks. In ICDE. Yuchen Fang, Yanjun Qin, Haiyong Luo, Fang Zhao, and Kai Zheng. 2023.STWave+: A Multi-Scale Efficient Spectral Graph Attention Network With Long-Term Trends for Disentangled Traffic Flow Forecasting. IEEE Transactions onKnowledge and Data Engineering (2023).",
  "Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making Pre-trained LanguageModels Better Few-shot Learners. In ACL. 38163830": "Qingqing Ge, Zeyuan Zhao, Yiding Liu, Anfeng Cheng, Xiang Li, ShuaiqiangWang, and Dawei Yin. 2024. PSP: Pre-training and Structure Prompt Tuning forGraph Neural Networks. In Joint European Conference on Machine Learning andKnowledge Discovery in Databases. 423439. Chenghua Gong, Xiang Li, Jianxiang Yu, Yao Cheng, Jiaqi Tan, and ChengchengYu. 2024. Self-pro: A Self-prompt and Tuning Framework for Graph NeuralNetworks. In Joint European Conference on Machine Learning and KnowledgeDiscovery in Databases. 197215.",
  "Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, andYang Shen. 2020. Graph contrastive learning with augmentations. NeurIPS 33(2020), 58125823": "Xingtong Yu, Yuan Fang, Zemin Liu, Yuxia Wu, Zhihao Wen, Jianyuan Bo, Xin-ming Zhang, and Steven CH Hoi. 2024. Few-Shot Learning on Graphs: fromMeta-learning to Pre-training and Prompting. arXiv preprint arXiv:2402.01440(2024). Xingtong Yu, Zhenghao Liu, Yuan Fang, Zemin Liu, Sihong Chen, and XinmingZhang. 2024. Generalized graph prompt: Toward a unification of pre-trainingand downstream tasks on graphs. IEEE Transactions on Knowledge and DataEngineering (2024).",
  "AppendicesAAlgorithm": "We detail the main steps for conditional prompt generation and tun-ing in Algorithm 1. In brief, we iterate through each downstreamtask to learn the corresponding prompt vectors individually. Inlines 35, we compute the embedding for each node using the pre-trained graph encoder, with the pre-trained weights 0 remainingfixed throughout the adaptation process. In lines 822, we optimizethe condition-net. Specifically, we performt similarity-weightedreadout (lines 911), generate prompts (lines 1213), modify nodesembeddings using these prompts (lines 1215), and update the em-beddings for the prototypical nodes/graphs based on the few-shotlabeled data provided in the task (lines 1819). Note that updatingprototypical nodes/graphs is necessary only for classification tasks.",
  "CFurther Descriptions of Datasets": "We conduct experiments on ten benchmark datasets. We summarythese datasets in . Wisconsin is a network of 251 nodes, where each node standsfor a webpage, and 199 edges signify the hyperlinks connectingthese pages. The features of the nodes are derived from a bag-of-words representation of the webpages. These pages are manuallyclassified into five categories: student, project, course, staff, andfaculty. The edge homophily ratio is 0.21. Cornell is also a webpage network. It comprises 183 nodes,each symbolizing a webpage, and 295 edges, which represent thehyperlinks between these pages. The node features are obtainedfrom a bag-of-words representation of the webpages. These pagesare manually sorted into five categories: student, project, course,staff, and faculty. The edge homophily ratio is 0.22. Chameleon is a Wikipedia network, consisting of 2,277 Wikipediapages. The pages are divided into five categories according totheir average monthly traffic. This dataset creates a network ofpages with 36,101 connections, and the node features consist ofvarious key nouns extracted from the Wikipedia pages. The edgehomophily ratio is 0.23. Squirrel comprises 5,201 Wikipedia web pages of discussingthe defined topics. The dataset is also divided into five categoriesaccording to their average monthly traffic. This dataset is a page-page network with 217,073 edges, and the node features are basedon several informative nouns in the Wikipedia pages. The edgehomophily ratio is 0.30.",
  "Homophily ratio is calculated by Eq. 1. Note that BZR and COX2 do not have anynode label, and thus it is not able to calculate their homophily ratios": "dataset, each node represents a secondary structure, and eachedge signifies a neighboring relationship either within the aminoacid sequence or in three-dimensional space. Nodes are classifiedinto three categories, while the graphs themselves are dividedinto two classes. The edge homophily ratio is 0.66. ENZYMES is a collection of 600 enzymes, sourced from theBRENDA enzyme database. The enzymes are divided into 6 dif-ferent classes, following their top-level EC enzyme classification.The edge homophily ratio is 0.67. Citeseer contains 3,312 scientific papers, divided into six dif-ferent categories. The dataset includes a citation network with4,732 edges. Each paper is represented by a binary word vector,indicating the presence or absence of each word from a dictio-nary comprising 3,703 unique terms. The edge homophily ratiois 0.74. Cora includes 2,708 scientific papers, divided into seven dis-tinct categories. The dataset features a citation network with5,429 edges. Each paper is represented by a binary word vec-tor, indicating whether each of the 1,433 unique words from thedictionary is present or absent. The edge homophily ratio is 0.81.",
  "DFurther Descriptions of Baselines": "In this section, we present more details for the baselines used inour experiments.(1) End-to-end Graph Neural Networks GCN : GCN utilizes a mean-pooling strategy for neighbor-hood aggregation to integrate information from neighboringnodes. GAT : GAT also leverages neighborhood aggregation for end-to-end node representation learning, uniquely assigns varyingattention weights to different neighbors, thereby adjusting theirimpact on the aggregation process. H2GCN : H2GCN improves node classification by separat-ing ego- and neighbor-embeddings, using higher-order neigh-borhoods, and combining intermediate representations. These de-signs help it perform well on both homophilous and heterophilousgraphs. FAGCN : FAGCN improves node representation by adaptivelycombining low- and high-frequency signals using a self-gatingmechanism, making it effective for different network types andreducing over-smoothing.",
  "(2) Graph Pre-training Models": "DGI : DGI operates as a self-supervised pre-training method-ology tailored for homogeneous graphs. It is predicated on themaximization of mutual information (MI), aiming to enhancethe estimated MI between locally augmented instances and theirglobal counterparts. GraphCL : GraphCL leverages a variety of graph augmen-tations for self-supervised learning, tapping into the intrinsicstructural patterns of graphs. The overarching goal is to amplifythe concordance between different augmentations throughoutgraph pre-training. DSSL : DSLL uses latent variable modeling to decouple se-mantics in neighborhoods, avoiding augmentations and opti-mizing with variational inference to capture local and globalinformation, enhancing node representation learning. GraphACL : GraphACL considers each node from two per-spectives: identity representation and context representation.The model trains the former by predicting the context represen-tation of one-hop neighbors using an asymmetric predictor, andthen reconstructs the same latter of the central node by enforcingidentity representations from two-hop neighbors.",
  "(3) Graph Prompt Models": "GPPT : GPPT utilizes a GNN model pre-trained via a linkprediction task which is a strong homophily method. The down-stream prompt module is designed specifically for node classifi-cation, aligning it with the pre-training link prediction task. GraphPrompt : GraphPrompt employs subgraph similar-ity calculations as a unified template to bridge the gap betweenpre-training and downstream tasks, including node and graphclassification. A learnable prompt is fine-tuned during down-stream adaptation to incorporate task-specific knowledge. GraphPrompt+ : GraphPrompt+ builds on GraphPrompt byintroducing a series of prompt vectors within each layer of thepre-trained graph encoder. This technique utilizes hierarchicalinformation from multiple layers, beyond just the readout layer.",
  "EImplementation Details of Approaches": "Details of baselines. We use the official code provided for all open-source baselines. Each model is tuned according to the settingsrecommended in their respective publications to ensure optimalperformance. We use early stopping strategy for training and setpatience to 50 steps. The number of training epochs is set to 2,000. For the baseline GCN , we employ a 3-layer architecture onWisconsin, Squirrel, Chameleon, Cornell datasets and 2-layerarchitecture on Cora, Citeseer, ENZYMES, PROTEINS, COX2,BZR datasets. Hidden dimensions is 256.",
  "For DGI , we utilize a 1-layer GCN as the base model and setthe hidden dimensions to 256. Additionally, we employ prelu asthe activation function": "For GraphCL , a 1-layer GCN is also employed as its basemodel, with the hidden dimensions set to 256. Specifically, weselect edge dropping as the augmentations, with a default aug-mentation ratio of 0.2. For DSSL , the hidden dimension search space is in {64, 256,2048}. We report the best performance on PROTEINS and EN-ZYMES with hidden size of 64 , Cora and Citeseer with 2048, andthe rest datasets with 256. We keep the other hyper-parametersthe same as in the original demonstrations in their Github repos-itory.",
  "For GraphPrompt+ , we employ a 2-layer GCN on Cora,Citeseer, ENZYMES, PROTEINS, COX2, BZR datasets and 3-layerGCN on the rest datasets. Hidden dimensions are set to 256": "Details of ProNoG. For our proposed ProNoG, we utilize a 2-layerFAGCN architecture as backbone for pre-training task with graphcontrastive methods for Wisconsin, Squirrel, Chameleon, Cornell.Especially, we implement edge-dropping on sub-graph level forWisconsin, Squirrel, Chameleon, Cornell. Hidden dimensions areset to 256. For Cora, Citeseer, BZR, COX2, we employ 1-layer GCNas base model for pre-training task. Hidden dimensions are set to256. For PROTEINS, we employ 1-layer GCN on link prediction taskfor pre-training. Hidden dimensions is set to 64. For ENZYMES,we implement DSSL for pretraining. Hidden dimensions is set to64. All experiments are undertaken with the seed of 39. Especially,we found that on Chameleon, Squirrel, keeping the original nodefeatures as input without normalization performs the best, while forothers, normalization of node features remains routine. Except forDSSL, we use cosine-similarity loss on node level as loss function.",
  "GParameters efficiency": "We evaluate the parameter efficiency of ProNoG compared toother notable methods. Specifically, we evaluate the number ofparameters that need to be updated or tuned during the down-stream adaptation phase, and present the results in . ForGCN and FAGCN, since these models are trained end-to-end, allmodel weights must be updated, leading to the lowest parameter",
  ": Sensitivity study of": "efficiency. In contrast, for GraphCL and GraphACL, only the down-stream classifier is updated, while the pre-trained model weightsremain unchanged, significantly reducing the number of param-eters that require tuning. Prompt-based methods GraphPrompt,GraphPrompt+, and ProNoG are the most parameter-efficient, asprompts or condition-net are lightweight and contain fewer param-eters than typical classifiers like fully connected layers. Note thatthe reported number of parameters for ProNoG are based on = 2,given that ProNoG still performs competitively with such hyperpa-rameter setting. Although our conditional prompt design requiresto update more parameters than GraphPrompt and GraphPrompt+during downstream adaptation, the increase is minor compared toupdating the entire classifier or model weights, and thus does notpose a major issue.",
  "HHyperparameter Analysis": "In our experiment, we use a 2-layer MLP with a bottleneck struc-ture as the condition-net. We evaluate the impact of the hiddendimension of the condition-net and report the correspondingperformance in . We observe that for both node and graphclassification, as increases from 2, the performance generally firstdecreases because a larger introduces more learnable parameters,which may lead to worse performance in few-shot settings. How-ever, after reaching a trough, accuracy starts to gradually increaseas grows further, since higher dimensions increase model capac-ity, until reaching a peak. Then the performance further declinesas improves, given more learnable parameters. Note that theoverall variation in performance is generally small, and the peak isgenerally at = 2 or = 64. In our experiment, we set = 64 inour experiments."
}