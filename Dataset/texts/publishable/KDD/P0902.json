{
  "Politecnico di Torino, Italy{firstname.lastname}@polito.it2 University of California, Santa Cruz,": "Abstract. Concept drift is a common phenomenon in data streamswhere the statistical properties of the target variable change over time.Traditionally, drift is assumed to occur globally, affecting the entiredataset uniformly. However, this assumption does not always hold truein real-world scenarios where only specific subpopulations within thedata may experience drift. This paper explores the concept of localizeddrift and evaluates the performance of several drift detection techniquesin identifying such localized changes. We introduce a synthetic datasetbased on the Agrawal generator, where drift is induced in a randomlychosen subgroup. Our experiments demonstrate that commonly adopteddrift detection methods may fail to detect drift when it is confined toa small subpopulation. We propose and test various drift detection ap-proaches to quantify their effectiveness in this localized drift scenario.We make the source code for the generation of the synthetic benchmarkavailable at",
  "Introduction": "In the realm of data stream mining, the detection of concept drift is of funda-mental importance to maintain the accuracy and reliability of predictive mod-els. Concept drift refers to the change in the statistical properties of the targetvariable that the model is trying to predict. Traditionally, drift detection tech-niques make the (often implicit) assumption that the drift occurs globally, i.e.,the change is uniformly distributed across the entire dataset. This assumption,however, may not always hold in real-world situations where drift can occur ina localized manner, affecting only certain subpopulations within the data (e.g.,only young women employed in the IT sector).Localized drift poses a significant challenge for traditional drift detectionmethods. These methods are designed to identify global changes and may over-look drifts that are confined to a small subset of the data. As a result, modelsmay fail to adapt to these local changes, leading to degraded performance andinaccurate predictions. For instance, a subgroup covering 2% of the population",
  "F. Giobergia et al": "the Subgroup Agrawal Drift Dataset, a synthetic data generator that injects aspecific subgroup of a desired size with noise. The experimental results show in-deed that commonly adopted techniques only detect subgroup drifts when thesecover a large fraction of the dataset, producing a large number of false negativesin the case of smaller diverging subgroups. As a natural next step, we plan onaddressing this shortcoming of current drift detection techniques.",
  ". Accuracy computed on the overall dataset and on the drifting subgroup (2% ofthe dataset), throughout a drifting event": "To investigate the limitations of existing drift detection methods in the con-text of localized drift, we introduce a synthetic dataset inspired by the Agrawalgenerator . In this dataset, drift is intentionally induced in a randomly chosensubgroup of a specific size, while the rest of the data remains stable. This setupallows us to simulate a scenario where only a specific subpopulation is subjectto drift, thereby providing a controlled environment to evaluate the effectivenessof various drift detection techniques.The primary contributions of this paper are as follows:",
  "Proposed dataset": "We introduce a novel dataset based on the synthetic one proposed in . Inparticular, we propose (i) identifying a randomly selected subgroup of the pop-ulation, defined as a slice of the datasets attributes and of a user-specifiedsize, and (ii) only injecting this target subgroup with noise to simulate a situa-tion where the drift occurs locally, instead of globally. The code is available at",
  "Subgroup Agrawal Drift Dataset": "To explore the concept of localized drift, we define a synthetic dataset based onthe Agrawal generator . The Agrawal generator is commonly used for simu-lating data streams and generates samples x in a domain D with six numericalattributes and three categorical attributes, producing binary classification tasks.The attributes are as follows:",
  "salary, uniformly distributed from $20,000 to $150,000 commission, 0 if salary has a value below $75,000, otherwise it is uniformlydistributed from $10,000 to $75,000": "age, uniformly distributed from 20 to 80 elevel (education level), uniformly chosen from 0 to 4 car (car maker), uniformly chosen from 1 to 20 zipcode (zip code of the town), uniformly chosen from 0 to 8 hvalue (house value), uniformly distributed from $50, 000 zipcode to$100, 000 zipcode. Different zip codes, as such, are associated with differentaverage house prices",
  "hyears (years the house has been owned), 1 to 30 uniformly distributed loan (total loan amount requested), uniformly distributed from $0 to $500,000": "Each synthetic record is associated with a binary outcome (i.e., whetherthe loan is approved or not). Ten different functions, f0(x), f1(x), , f9(x) havebeen proposed in the original work to map a given record x to the binary groundtruth value, fi : D {0, 1}. This completely defines any record {x, fk(x)} usedeither during training or in deployment. A perturbation can also be included soas not to make the classification task trivial. This perturbation (ranging from0% to 100%) affects the attributes of x after the class has been assigned, thusadding an element of fuzzyness in the sample/class relationship.A common technique to introduce concept drift consists of adopting aclassification function fi for the original concept and a different one fj (i = j)for the drift concept. At step t, the function is defined as a random variable F:",
  "Subgroup definition": "To simulate a localized drift, we need to define a target subgroup within thedataset. We produce meaningful subgroups by identifying slices of the domain,e.g., { age , salary [$75,000, $100,000] }. We fully automate thesynthetic dataset generation phase by introducing a random subgroup definitionpolicy. This policy produces, for a desired subgroup size (i.e., subgroup support),a slice of the population that approximately encompasses it.We adopt a greedy policy to identify a subset of slices that, combined, wellapproximate the target subgroup size. We do this by identifying random rangesof values (e.g., [c, d]) for randomly chosen attributes (e.g., attr U(a, b), a c < d b). The uniform distribution makes it trivial to compute the probabilityof belonging to the random range of values, as P(attr [c, d]) = P(attr) =dcba. Additionally, the attributes are independent from one another3. As such,their combined probability can be computed as the product of the separateprobabilities, P(attr.1)P(attr.2). . .P(attr.n). We either include or discarda candidate slice based on whether it gets the current probability closer to thetarget one. provides an example where a subgroup of approximately thetarget size (10%) is iteratively defined by identifying a first slice on age, followedby a second one on salary. Because of the greedy nature of the algorithm, slicesthat do not provide an immediate improvement in terms of support are discarded.The algorithm terminates when either the subgroup size is within a tolerancethreshold of the target one, or a maximum number of iterations is reached.",
  "|0.125 0.1| < |0.167 0.1|?": ". Example of the greedy process adopted to randomly generate subgroups on2 attributes. From top to bottom, the target subgroup is built by iteratively addingrandomly generated slices of the attributes, if their inclusion produces a better approx-imation of the desired support (in the example, 10%). The checkered area at each steprepresents the size (support) of the current subgroup.",
  "Frequency": ". Distribution of the absolute difference between target and corresponding ob-tained subgroup sizes, for 1,000 generated subgroups and various target sizes, toleranceof 0.01 (marked in red), maximum number of iterations set to 1,000. We report four examples of generated subgroups in . For each, wereport the target (desired) size (5%, 10%, 25% and 50% of the population, re-spectively), the one computed according to the greedy policy adopted for sub-group generation, and the actual (empirical) size, as observed over a generatedsample of 10,000 points. Both computed and actual sizes are close to the targetone. If needed, the gap between computed and target subgroup sizes can be low-ered by changing the maximum number of allowed iterations and/or the desiredtolerance threshold.",
  "For each method, we identify the best-performing configuration of hyperpa-rameters through a grid search on the dataset": "Dataset. We adopt the proposed synthetic dataset for benchmarking drift de-tection techniques as the drifting subgroup sizes vary. In particular, we are in-terested in the performance when the drifting subgroups are small, as these arethe drifts that are intuitively more likely to go undetected. We sample subgroupsizes from 1% to 100% (i.e., the full population) logarithmically.For each subgroup size, we conduct 100 experiments. For half of them, weinject drift to a random subgroup of the desired size (positive experiments). Theother half is instead not injected with any drift (negative experiments).For positive experiments, we randomly choose one out of the 10 classificationfunctions for the original concept, and a different one for the drift concept. Fornegative experiments, we instead use a single concept throughout the entireexperiment. For all experiments, we build a training set comprised of 10,000points sampled from the underlying distribution and associated with the originalconcept. We train a simple decision tree classification model with a depth of up to5 nodes on this training set. Subsequently, we sample 200 batches of data (1,000points each). For positive experiments, the concept drift is injected gradually,as detailed in Subsection 2.1. The injection is centered around the 100th batch,with a width of 100 batches. The subgroup accuracy in provides avisual intuition of the setting. We introduce a perturbation of 25% of the inputattributes to make the classification problem non-trivial. For each experiment,the various drift detection techniques are used to monitor and potentially detectdrifts. Since each algorithm can potentially produce multiple drift detections, wecount the number of detections. We determine the threshold on the minimumnumber of detections to trigger a drift alert using a ROC curve computed on30% of the experiments. We use the rest of the experiments to compute theperformance in terms of accuracy, F1 score, False Positive Rate (FPR) and FalseNegative Rate (FNR), of various drift detection techniques. Results. summarizes the main results. Both accuracy and F1 high-light how all considered techniques achieve near-perfect performance in detect-ing drifts when the drifting subgroup is large enough (approximately 10% ofthe dataset or more). Instead, none of the approaches achieved satisfactory re-sults for lower support sizes. To better understand the cause of this drop inperformance, we additionally computed the FPR and FNR for each techniquefor various sizes of drifting subgroups.Interestingly, the FPR is largely unaffected by the size of the drifting sub-group. In other words, none of the considered approaches produces an excess offalse positive predictions when smaller subgroups are drifting. This is in accor-dance with what was expected: drifts of smaller subgroups go unnoticed, meaningthat fewer positive predictions are produced overall.Instead, the FNR plot presents a different perspective. In this case, it is clearthat there exists an abundance of false negatives when the drifting subgroupsare smaller in size. These false negatives are drifts that are not being detected:",
  "A Synthetic Benchmark to Explore Limitations of Localized Drift Detections9": "as expected, the various drift detection techniques cannot handle properly driftsof smaller subpopulations.As the drifting subpopulations grow, the number of false positives produceddecreases. Some approaches, such as DDM, have an earlier and sharper reductionin FNR, whereas other approaches (more significantly, EDDM) have a delayedresponse, meaning that they struggle to detect drifts even when the target sub-groups are larger. Drifting subgroup size 0.0 0.2 0.4 0.6 0.8 1.0 F1 score DDMEDDMFHDDMHDDM_W Drifting subgroup size 0.0 0.2 0.4 0.6 0.8 1.0",
  "Conclusions": "In this work, we highlighted a problem that affects commonly adopted drift de-tection techniques: drifts are only detected if they affect a large fraction of theoriginal data. This implies that drifts affecting smaller subpopulations (e.g., mi-norities) may go undetected. This is problematic, since it implies that modelsmay be silently drifting and underperforming for certain populations. To bench-mark the performance of various detectors under subgroup drifts, we introduce",
  "Acknowledgements": "This work is partially supported by the FAIR - Future Artificial Intelligence Re-search (PIANO NAZIONALE DI RIPRESA E RESILIENZA (PNRR) MIS-SIONE 4 COMPONENTE 2, INVESTIMENTO 1.3 D.D. 1555 11/10/2022,PE00000013) and the spoke FutureHPC & BigData of the ICSC - CentroNazionale di Ricerca in High-Performance Computing, Big Data and QuantumComputing, both funded by the European Union - NextGenerationEU. Thismanuscript reflects only the authors views and opinions, neither the EuropeanUnion nor the European Commission can be considered responsible for them.",
  ". Agrawal, R., Imielinski, T., Swami, A.: Database mining: A performance perspec-tive. IEEE Transactions on Knowledge and Data Engineering 5(6), 914925 (1993)": "2. Baena-Garca, M., del Campo-vila, J., Fidalgo, R., Bifet, A., Gavalda, R., Morales-Bueno, R.: Early drift detection method. In: Fourth International Workshop onKnowledge Discovery from Data Streams. vol. 6, pp. 7786 (2006) 3. Frias-Blanco, I., del Campo-vila, J., Ramos-Jimenez, G., Morales-Bueno, R., Ortiz-Diaz, A., Caballero-Mota, Y.: Online and non-parametric drift detection methodsbased on hoeffdings bounds. IEEE Transactions on Knowledge and Data Engineer-ing 27(3), 810823 (2014)"
}