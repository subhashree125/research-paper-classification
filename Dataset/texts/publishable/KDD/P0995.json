{
  "ABSTRACT": "Temporal Graph Neural Networks (TGNN) have the ability to cap-ture both the graph topology and dynamic dependencies of inter-actions within a graph over time. There has been a growing needto explain the predictions of TGNN models due to the difficultyin identifying how past events influence their predictions. Sincethe explanation model for a static graph cannot be readily appliedto temporal graphs due to its inability to capture temporal depen-dencies, recent studies proposed explanation models for temporalgraphs. However, existing explanation models for temporal graphsrely on post-hoc explanations, requiring separate models for predic-tion and explanation, which is limited in two aspects: efficiency andaccuracy of explanation. In this work, we propose a novel built-inexplanation framework for temporal graphs, called Self-ExplainableTemporal Graph Networks based on Graph Information Bottleneck(TGIB). TGIB provides explanations for event occurrences by in-troducing stochasticity in each temporal event based on the Infor-mation Bottleneck theory. Experimental results demonstrate thesuperiority of TGIB in terms of both the link prediction performanceand explainability compared to state-of-the-art methods. This is thefirst work that simultaneously performs prediction and explanationfor temporal graphs in an end-to-end manner. The source code ofTGIB is available at",
  "INTRODUCTION": "Temporal Graph Neural Networks (TGNN) possess the capabil-ity to capture interactions over time in graph-structured data anddemonstrate high utility in areas such as user-item interaction ine-commerce and friend relationships in social networks .TGNNs incorporate both temporal dynamics and graph topology intheir approach and focus on learning time-dependent node repre-sentation to predict future evolutions . However, TGNNmodels are considered as black boxes with limited transparency dueto the inability to discern how past events influence outcomes. Offer-ing insights based on the logic of predictions in TGNN contributesto an improved comprehension of the models decision-makingand provides rationality for predictions. Explainability for TGNNcan be applied in high-risk situations such as healthcare forecast-ing and fraud detection to enhance the modelsreliability, and assists in examining and mitigating issues related toprivacy, fairness, and safety in real-world applications .Explainability aims to provide users with evidence within thedata that influenced a model prediction. With the emerging neces-sity for explainability, explanation models for static graphs havebeen actively studied in recent times . These modelsinduce perturbations in the input of the model to detect the nodesand edges, which significantly impact the final prediction. However,these models cannot be easily generalized to temporal graphs dueto the high dynamicity of temporal graphs. Specifically, explanationmodels for static graphs cannot capture the graph topology that isdynamic in nature . For example, in a temporal graph, multi-ple events may occur over time between the same pair of nodes, andthese events may have different importance depending on whenthe event occurred. In other words, events that occurred a longtime ago may have less influence on the current event compared toevents that occurred more recently.Recently, T-GNNExplainer attempted to explain the modelpredictions on temporal graphs. Specifically, T-GNNExplainer con-sists of a navigator that learns inductive relationships betweentarget events (those that are to be predicted) and candidate events(those that may serve as reasons for the prediction), and an explorerthat explores the optimal combinations of candidate events for each",
  ": Comparison between T-GNNexplainer and TGIB": "target event based on Monte Carlo Tree Search (MCTS) (upperpart of ). T-GNNExplainer is a post-hoc explanation methodbecause explanations are generated based on a pretrained basemodel (i.e., TGNN). Despite its effectiveness, T-GNNExplainer hastwo major drawbacks that originate from its post-hoc manner ofgenerating explanations. First, since a temporal graph consistentlyencounter changes in the topology due to its dynamic nature, thebase model needs to be consistently retrained. This results in therepeated retraining of the explanation model (i.e., navigator and ex-plorer) based on the retrained base model (i.e., TGNN), which makesT-GNNExplainer inefficient especially when the base model is large.The complexity issue aggravates as the explanations are generatedbased on MCTS, which is a highly inefficient search algorithm.Second, since post-hoc explanation methods provide explanationsby examining the behavior of an already trained base model, itbecomes challenging to fully comprehend the learning process ofthe base model, and to provide accurate explanations .As a solution to address the drawbacks of post-hoc explanationmethods when applied to temporal graphs, we propose to allowthe model to simultaneously perform both predictions and expla-nations in temporal graphs by generating intrinsic explanationswithin the model itself (i.e., built-in explanation method) (lower partof ). Existing post-hoc explanation methods for temporalgraphs such as T-GNNExplainer focus on deriving subgraphs thatgenerate predictions as similar as possible to the predictions of thebase model for the target event. On the other hand, since our pro-posed built-in explanation method does not have a base model, weinduce interactions between the target event representation at thecurrent timestamp and the candidate event representations at pasttimestamps to extract the importance probability of each candidateevent. To consider the interaction of representations at differenttimestamps, we generate time-aware representations by taking intoaccount the time spans between the target event and candidateevents. The time-aware representations facilitates our model toidentify important candidate events that are used as explanationsfor the model predictions. Our goal is to detect significant pastevents in temporal graphs based on the constructed time-awarerepresentations.To this end, we propose a novel built-in explanation frameworkfor temporal graphs, called Self-Explainable Temporal Graph Net-works based on Graph Information Bottleneck (TGIB). The main idea is to build an end-to-end model that can simultaneously gener-ate predictions for temporal graphs along with explanations basedon the Information Bottleneck (IB) approach, which enables themodel to detect important subgraphs by leveraging the time-awarerepresentations. Specifically, TGIB considers the interaction be-tween the target event and candidate events to extract importantcandidate events, eventually predicting the occurrence of the targetevent. We formalize the prediction process for temporal graphswith the IB to restrict the flow of information from candidateevents to predictions by injecting stochasticity into edges .The stochasticity for label-relevant components decreases duringtraining, whereas the stochasticity for label-irrelevant componentsis maintained. This difference in stochasticity eventually providesexplanations for the occurrence of the target events. We expectimproved generalization performance of TGIB by penalizing theamount of information from input data.We conducted extensive experiments to evaluate the predictionperformance and the explainability of TGIB in the event occurrenceprediction task. Our results show that TGIB outperforms existinglink prediction models in both transductive and inductive environ-ments. We also evaluated the explainability of TGIB in capturingthe label information by evaluating its prediction performance withonly the detected explanation graphs. We also demonstrate thatexplanation graphs with varying sparsities exhibit a higher expla-nation performance than existing explanation models. Overall, ourresults show that TGIB not only significantly improves the perfor-mance of the event occurrence prediction, but also provides superiorperformance in terms of explanations. To the best of our knowledge,this is the first study to simultaneously perform predictions andexplanations in temporal graphs.In summary, our main contributions are summarized as follows:",
  "RELATED WORK2.1Temporal Graph Neural Networks": "Unlike ordinary Graph Neural Networks for a static graph, TGNNsproduce dynamic node embeddings from a temporal graph evolvingwith a series of events. Early methods used RNN-basedarchitectures to produce temporal node embeddings by only consid-ering nodes involved in each of specific events. As these methodsmerely use direct connectivity represented as a single event, theSelf-Attention Mechanism (SAM) was adopted in recent meth-ods for modeling more complex spatial and temporal relationships.TGAT applied SAM for simultaneous modeling of both spatialand temporal relationships with functional time encodings basedon Bochners theorem. TGN first updates the node memory fortemporal dependency by using an RNN-based model and computes",
  "Self-Explainable Temporal Graph Networks based onGraph Information BottleneckKDD 24, August 2529, 2024, Barcelona, Spain": "As shown in the , G and may have a spurious corre-lation due to the factors excluding R from G. In other words,the correlation between G \\ R and the label is a spuriouscorrelation and not a real factor in determining the label. Predicting based on G has a risk of capturing spurious correlations, whichcan lead to a decrease in the models generalization performance.According to Proposition 1, this problem can be solved by optimiz-ing the objective function of TGIB. It shows that event occurrencecan be predicted based on without spurious correlations. As aresult, TGIB can improve the generalization performance of predic-tions by removing spurious correlations for event occurrence.",
  "Graph Information Bottleneck": "As the Information Bottleneck (IB) principle enables a model to ex-tract information within the input data that is relevant to the target(i.e., label) information, it has been widely adopted in the variousfields of machine learning . Inspired by their successes,recent studies regarding the IB principle on graph-structured datawere proposed. GIB extended the IB principle on GNNs byextracting both minimal and sufficient information from both thegraph structure and node features. Extending this, GIB uti-lized the IB principle for recognizing an important subgraph (i.e.,IB-Graph) within the input graph, and then applied the IB-Graphfor improving the graph classification performance. Additionally,VGIB utilized learnable random noise injection in the subgraphrecognition process to enable flexible subgraph compression. Mean-while, CGIB applied the graph information bottleneck in molec-ular relational learning by finding a core subgraph of one moleculebased on the paired molecule and the target label. Furthermore,PGIB approached the information bottleneck principle froma prototype perspective to provide prototypes with the subgraphfrom the input graph which is important for model prediction.",
  "GNN Explainability": "Although Graph Neural Networks (GNNs) have been shown to beeffective on graph data , analyzing the reason of the predic-tion and the decision-making process of these models has been along-standing challenge due to their complex architectures. For thisreason, approaches for explainable AI (XAI) have been recently pro-posed to understand black-box GNNs . Despite theireffectiveness, as these approaches are post-hoc methods, they areboth ineffective and inefficient when changes occur not only in thetraining data but also the trained model to be explained. Therefore,self-explainable (i.e., built-in) approaches have been re-cently gained attention. They contain an explanatory module insidethe model to make predictions and explanations simultaneously,addressing the limitations of post-hoc approaches. However, all theaforementioned methods are designed for static graphs, and can-not be readily applied to temporal graphs due to their dynamicity.Recently, T-GNNExplainer is proposed to give an explanationon TGNNs trained on temporal graphs, which, however, has majordrawbacks due to its post-hoc manner as mentioned in . Ad-ditionally, STExplainer generates separate explanation graphsfor spatial and temporal graphs in traffic and crime prediction. How-ever, to explain event occurrence predictions in a way that is easyfor humans to understand, it is necessary to generate a comprehen-sive explanation graph as a set of temporal events. To this end, wepropose TGIB that can simultaneously generate predictions andexplanations in temporal graphs by detecting past events that areimportant for the predictions based on the IB principle.",
  "PRELIMINARIES3.1Temporal Graph Model": "A temporal graph contains a series of continuous events = {1,2, }with timestamps, where = {, ,, att} indicates an interactionevent between node and at timestamp with edge attributeatt. The event set composes a temporal graph G = (V, E), whereE is regarded as edges with timestamps and V represents the nodesincluded in E. Since the definitions of V and E are interdependent,we consider a temporal graph G as a set of events. We define G asthe graph constructed immediately before the timestamp whichincludes all events {1,2, ,1} excluding . Let denote aself-explainable temporal graph model that we aim to learn. Themodel simultaneously predicts the occurrence of interactionsbetween two nodes at a certain timestamp, and explains the rea-son for its prediction regarding the occurrence or absence of theevent . The output of the model , i.e., (G)[], consists of twocomponents: the label prediction indicating whether the event is present or absent, i.e., , and an explanation for the presence orabsence of , i.e., R G, where G is the -hop computationgraph of , and R is a subgraph of G considered as importantevents. Note that G of = {, ,, att} is a combination ofthe -hop computation graphs of node and node .",
  "min (;) + (;),(2)": "where enables the regulation of the balance between two termsas the Lagrange multiplier. The IB principle has found recent appli-cation in learning a bottleneck graph, referred to as the IB-Graph,for a given graph G, which aims to preserve the minimal necessaryinformation concerning the properties of G while capturing themaximal information about the label . This approach compres-sively identifies label-related information from the original graphG, inspired by the Graph Information Bottleneck (GIB) principleby optimizing the objective function as:",
  "minG (; G) + (G; G),(3)": "where G is the IB-Graph and is the label of G. The first termaims to maximize the mutual information between the graph labeland the subgraph to include as much graph label information aspossible in the subgraph G. The second term aims to minimizethe mutual information between the original graph and the sub-graph to include the original graph G in the subgraph G to aminimum extent.",
  "GIB-based Objective for Temporal Graph": "We provide the objective of the Graph Information Bottleneck fortemporal graphs. TGIB extracts a bottleneck code R for the targetedge from its -hop neighborhood G. Specifically, the bottle-neck code R is a subgraph of s -hop computation graph. Thebottleneck mechanism on neighborhood information provides ex-planations for the prediction of . The objective function of thegraph information bottleneck is provided as follows:",
  "E,GKLR |, GR LMI,(6)where Ris the variational approximation of Rand KL": "represents the Kullback-Leibler(KL) divergence. Rcan be flex-ibly applied to various distributions, including normal distributions.We can minimize the upper bound of (R;, G) by minimizingLMI.Finally, we obtain the final loss function for the graph informa-tion bottleneck as follows: L = Lcls + LMI. 4.2Time-aware event representationWe generate time-aware event representations to capture the tem-poral information of events. Inspired by , we obtain node rep-resentations by performing self-attention based on temporal en-coding. We consider the neighboring nodes for node at time asN (;) = {1,2, ,}, where is the number of neighbors. Theinteraction between and one of its neighbor has edge attribute, Redge and occurs at time ,, which is earlier than . We usethe representations of neighbors, attributes of their interactions,and their temporal information as the input to the self-attentionlayer. We use () () R to denote the representation of node at time in the -th layer, where (0)() is the raw node feature ofnode , denoted as Rnode, that is invariant over time . For theself-attention mechanism, we define the query, key and value as:",
  ",(7)": "where : R R is time encoding that provides a continuousfunctional mapping from the time domain to the vector space with dimensions. According to the translation-invariant assumptionof time encoding, we use ,1, ,2, , , as the in-teraction times. This means that we only consider the time spansince time encoding is designed to measure the temporal distancebetween nodes, which is more important than the absolute valueof time.Each node collects information from its neighboring nodes, and",
  "where ()() is the -th row of () (). The attention weight": "()() indicates the contribution of node to the features of node within the topological structure N (;), considering the interac-tion time with computed at layer . We utilize self-attention toincorporate temporal interactions into node features and structuralinformation. The representation for a node within the neighbor-hood N (;) is calculated as ()() ()() based on the attention",
  "() = () () ( ) att,(12)": "where all satisfy the following condition : G, where1 1. In the time encoding of the above equations, sim-ilar to node representation, we consider the time span to measurethe temporal distance between each interaction. It allows time-aware explanations to sufficiently incorporate temporal distanceinformation and ensures that the importance scores of multipleevents occurring between the same pair of nodes over time aredependent on the time span. Therefore, we regard the interactiontime as , where is the occurrence time of the target event, and is the occurrence time of the candidate event .",
  "G\\R(1 ),(13)": "where is the probability of given and G, i.e., ( |, G),following Bernoulli distribution. We apply the Gumbel-Softmaxtechnique for sampling in order to allow gradients to propa-gate from the classifier to the time-aware bottleneck module inoptimization. Each is computed as the output of an MLP thattakes the target event representation and the candidate eventrepresentation as input as follows:",
  "= ( |, G) = (), ()(14)": "where () is the sigmoid function and is an MLP. A large indicates that is important for predicting in G.Next, we define the variational approximation (R) of the mar-ginal distribution (R). For every edge in the graph G, wesample (), where is a predefined hyperparam-eter. We eliminate all edges in G and reinstate all edges with = 1.We assume that the graph obtained in this process is R. Conse-quently, we use a multivariate Bernoulli distribution for (R) asfollows :(R) = |R | (1 )| G ||R |.(15)Finally, the mutual information loss LMI for the time-awarebottleneck in Equation 6 is calculated as follows:",
  "+ = Readout ()| G.(18)": "Negative Sample. We utilize negative samples to effectively trainthe predictor |R. To generate a negative sample of ={, ,, att}, we fix the node and replace by randomlysampling a node from the entire graph G. We denote the randomlysampled node as , and the event representation neg () corre-sponding to the negative sample is defined as follows:",
  "neg () = () () (0) (19)": "In the same manner as (), we extract the bottleneck codebased on neg () along with a candidate event (), and samplestochastic weights from the Bernoulli distribution. Consequently,using the Bernoulli variables derived based on neg (), we selectimportant events from candidate events and obtain the selectedcandidate graph embedding through a readout function.Finally, we use the time-aware link prediction loss function asfollows:",
  "(20)where is an MLP, is the number of negative samples, and is a distribution from which negative samples are sampled": "Explanability. The explainability of TGIB is established by inject-ing stochasticity into past candidate events. LMI in Equation 16aims to assign high stochasticity to all candidate events, while Lclsin Equation 20 simultaneously learns to reduce the stochasticity forexplanation graphs that are important for the occurrence of . Wegenerate an importance score derived from the interactions ofevents at different timestamps, i.e., and , which allows the bot-tleneck code to help generate a time-aware explanation. TGIB canrank all candidate events according to and detect the top-rankedcandidate events as explanation graphs.",
  ": Spurious Correlation Removal of TGIB": "TGIB eliminates spurious correlations within the input data andensures interpretability. If there is a correspondence between theexplanation of event occurrence R and the label Y, we can provethat R is the optimal solution for the objective function of TGIB(i.e., Equation 4). Proposition 1. Assume that each G contains R, which deter-mines . In other words, for some deterministic invertable func-tion with randomness that is independent of G, it satisfies = (R) + . Then, for any , the optimal R thatminimizes (; R) + (R;, G) is R.",
  "= minR(1 ) (; G |R ) + (R;, G | ),": "where, since R is a subgraph of G, implying (G, R) holdsno additional information over G, it follows (; |G, R) = (; |G) in the third equation and (; G, R) = (; G)in the fourth equation. The derivation of the equations is basedon the chain rule of mutual information, and since the objectiveis to find the R that optimizes the equation, the terms that areirrelevant to R have been removed (detailed derivation is providedin Appendix A).The above derivation shows that R that minimizes (1 ) (; G |R) + (R;, G |) can also minimize (; R) + (R;, G). Since mutual information is always non-negative,(1) (; G |R)+ (R;, G |) reaches its minimum whenboth (; G |R) and (R;, G |) are equal to 0. = (R) + , where is independent from G, implies that (; G |R) = 0. Similarly, R = 1( ) where is inde-pendent from G implies that (R;, G |) = 0. Therefore,the following holds: (1 ) (; G |R) + (R;, G |) =0. In other words, the optimal R that minimizes (; R) + (R;, G) is R.",
  "WikipediaSocial9,227157,4741721 monthUCISocial1,89958,835-196 daysUSLegisPolitics22560,396112 termsCanParlPolitics73474,478114 yearsEnronSocial184125,235-3 yearsRedditSocial10,984672,4471721 month": "Wikipedia consists of 9,227 nodes representing editors andWiki pages, and 157,474 edges representing timestamped postrequests. Each edge has a Linguistic Inquiry and Word Count(LIWC) feature vector of the requested text with each vectorhaving a length of 172. UCI is a social network within the online community of Uni-versity of California, Irvine students spanning 196 days. Nodesrepresent students, and edges denote messages exchanged be-tween two students, each with a timestamp in seconds. USLegis is a co-sponsorship network among U.S. senators.Each node represents legislators, and two legislators are connectedif they have jointly sponsored a bill. The weight assigned to eachedge represents the number of times two legislators have jointlysponsored bills over a period of 12 terms. CanParl is a network capturing interactions among CanadianMembers of Parliament from 2006 to 2019. Nodes represent Mem-bers of Parliament, and two members are connected if they bothvote in favor of a specific bill. The weight assigned to each edgerepresents the number of times one member has voted in favor ofanother member within one year.",
  "company. Nodes represent employees of the Enron company, andedges represent the exchanged emails between two employees": "Reddit represents a network of posts created by users insubreddits for one month. The nodes represent users and posts,and the edges represent timestamped post requests. Similar toWikipedia, the features of the edges have LIWC feature vectors ofthe requested text , with each vector having a length of 172. Evalutation Protocol. We split the total time into [0,train],[train,val], and [val,test] and use the events occurring withineach interval as the training set, validation set, and test set, respec-tively. We set train = 0.7 and val = 0.85. We set the number oflayers to 2 and the dropout rate as 0.1 for the aggregation processwithin the attention mechanism to obtain time-aware event repre-sentations. Our model is trained for 10 epochs using the Adam SGDoptimizer with a learning rate of 0.00001. We set the dimensionsof the node embeddings and time encodings to be identical to theraw features of the events. For the -hop computational graph, weset as 2. We evaluate the performance, which is averaged over 5independent runs with different random seeds.",
  "Setup. To measure performance in various settings, we assess linkprediction performance in both transductive and inductive settings": "Transductive Setting. We use all events occurring in the threeintervals (i.e., [0,train], [train,val], and [val,test]) as the train-ing set, validation set, and test set, respectively. This means thatduring the training time, all events occurring before train canbe observed. Inductive Setting. We predict the occurrence of events involv-ing nodes not observed during the training time. Specifically, 1)we split the training set, validation set, and test set as shownin .1. 2) We randomly select 10% of nodes from thetraining set, and remove all events containing these nodes fromthe training set. 3) We include the events involving these nodesonly in the validation and test sets. Experiment Results. The experimental results for link predictionin the transductive setting and inductive setting are presented in Ta-ble 2 and , respectively. We measured the mean and standarddeviation of Average Precision (AP) on the test set. We obtainedthe following observations: 1) TGIB demonstrated the highest per-formance on 5 out of 6 datasets compared to the baselines for thetemporal graphs in both transductive and inductive settings. Theremaining dataset (i.e., Enron) showed the second-highest perfor-mance compared to the baselines. We attribute the superior per-formance of TGIB to its capturing of important past events, whicheliminates spurious correlations for an event occurrence by pre-dicting based on R as stated in Proposition 1. 2) TGIB achievedhigh prediction performance in politics networks such as USLegisand CanParl compared to the baselines. Specifically, in the inductivesetting, USLegis achieved a 47.4% performance improvement over",
  "TGIB99.28 0.1191.26 0.1686.42 0.1679.56 0.7980.64 0.5999.54 0.02": "the runner-up baseline, and CanParl achieved a 42.3% improvementover the runner-up baseline. These findings suggest that in a po-litical network, specific individuals or groups can have significantinfluence, and the GIB method effectively identifies these importantpeople or groups and analyzes their roles and influence. 3) Addi-tionally, TGIB achieved very high AP scores of 99.28% and 99.68%in the Wikipedia and Reddit datasets, respectively. Even thoughWikipedia and Reddit already have high baseline performances,with runner-up performances of 98.28% and 98.30% respectively,TGIB achieves nearly perfect performance on both datasets. TGIBis the only model that demonstrates performance exceeding 99%on both datasets.",
  "Explanation Performance": "Baselines. We use six explanation methods, i.e. Random, ATTN, Grad-CAM , GNNExplainer , PGExplainer ,T-GNNExplainer as baselines. Random is the explanationresults obtained by randomly sampling a number of nodes thatsatisfy sparsity. Details on baselines are presented in Appendix E. Setup. We used TGAT as the base model for all baselines thatgenerate explanations in a post-hoc manner. Since the overall rangeof Fidelity used as an evaluation metric in TGNNExplainer has vari-ability depending on the dataset or base model, calculating the areaof the graph related to Fidelity may not provide consistent results.Therefore, to evaluate the performance of explanations, we mea-sure the proportion of predictions that match the models originalpredictions when the model performs predictions based on expla-nations. In other words, predictions based on superior explanationshave the same prediction label as predictions from the originalgraph. We also evaluate the explanation performance over vari-ous sparsity levels , where the sparsity is defined as |R |/|G |.Since higher levels of sparsity would always yield better explana-tion performance, we evaluate explanation performance in various sparsity environments. We divide the sparsity level from 0 to 0.3with intervals of 0.002, measure the performance of the explanationgraph R, and then calculate the area under the sparsity-accuracycurve. Experiment Results. We have presented the experimental resultsfor explanation performance in . We have the following ob-servations: 1) TGIB outperforms all the baselines, indicating that itprovides a high quality of explanation for the predictions. 2) TGIBachieved up to 27.5% improvement in performance on the UCIdataset compared to the baselines. Moreover, 2 out of 5 baselines(i.e., ATTN and Grad-CAM) show worse performance than randomexplanations. 3) Some baselines, such as ATTN and Grad-CAM,fail to provide explanation, performing even worse than randomexplanations depending on the datasets, which significantly affectthe reliability of the explainable model. However, TGIB demon-strates stable performance in explainablity, and its performance isfurther supported by a theoretical background. Furthermore, wepresent the inference time in Appendix C, demonstrating the effi-ciency of our methods evaluation while maintaining its superiorexplanations.",
  "Explanation Visualization": "We compare the explanation visualizations for static graph expla-nation models, GNNExplainer and PGExplainer, with the temporalgraph explanation model, TGIB, as shown in the . In thefigure, the target events are depicted by red solid lines, and theexplanation events are depicted by blue solid lines. Additionally, wemarked the difference in occurrence timestamps between the targetevent and each of the five explanation events. Then, the mean timeintervals are calculated as follows: (a) 1228523.4, (b) 1475006.2, and(c) 612333.2. This shows that the time interval for (c) is smallerthan that for (a) and (b), and that the timestamps of the explanationevents in TGIB are closer to the timestamps of the target events",
  ": Comparison of explanation visualization for explanation models for static graphs and TGIB": "than those in GNNExplainer and PGExplainer. In temporal graphs,events that occurred recently should have a greater influence onthe target event compared to events that happened a long time ago;however, GNNExplainer and PGExplainer fail to capture this. Inother words, it shows that GNNExplainer and PGExplainer cannotbe easily generalized to temporal graphs because they dont capturetemporal dynamics. In contrast, TGIB considers the temporal infor-mation of the target event to generate explanations for temporalgraphs. Therefore, we can observe that TGIB is capable of capturingtemporal dependencies along with graph topology.",
  "with all (TGIB)91.61 0.3491.41 0.4187.07 0.44": "We conduct ablation studies to investigate the efficiency of the pro-posed model (i.e., TGIB). provides the ablation study of ourproposed method based on link prediction. The with all settingindicates our final model including all components of TGIB. Weperformed ablation studies on time-aware event representations(i.e., () and ()) and losses relevant to mutual informa-tion (i.e., (; R) and (R;, G)). We obtain the followingobservations: 1) Capturing temporal information from event repre-sentations in the prediction based on the IB principle contributesto performance improvement. 2) The model experiences declines in performance when the terms relevant to mutual information, (; R) and (R;, G), are not considered. Specifically, theremoval of (; R) results in R containing a large amount oflabel-irrelevant information, leading to difficulties in the final labelprediction. Moreover, when (G;, R) is not considered, it leadsto a decrease in the generalization performance of the predictiondue to the portions of G that have spurious correlations with ,as mentioned in .5.",
  "CONCLUSION": "In this work, we propose TGIB, a more reliable and practical ex-planation model for temporal graphs that can simultaneously per-form prediction and explanation tasks. The main idea is to providetime-aware explanations for the occurrence of the target events byrestricting the flow of information from candidate events to predic-tions based on the IB theory. We demonstrate that TGIB exhibitssignificant performance in both prediction and explanation acrossvarious datasets, and its explanation visualizations show that it cancapture temporal relationships to extract important past events.Consequently, TGIB represents a more adaptable explainable modelcompared to existing models, capable of being efficiently trainedon dynamically evolving graph settings without the exhaustiverequirement for retraining from scratch, unlike other methods. Acknowledgement. This work was supported by the NationalResearch Foundation of Korea(NRF) grant funded by the Koreagovernment(MSIT) (RS-2024-00335098), Institute of Information &communications Technology Planning & Evaluation (IITP) grantfunded by the Korea government(MSIT) (No.2022-0-00157), andNational Research Foundation of Korea(NRF) funded by Ministryof Science and ICT (NRF-2022M3J6A1063021).",
  "Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. 2016. Deepvariational information bottleneck. arXiv preprint arXiv:1612.00410 (2016)": "Julia Amann, Alessandro Blasimme, Effy Vayena, Dietmar Frey, Vince I Madai,and Precise4Q Consortium. 2020. Explainability for artificial intelligence inhealthcare: a multidisciplinary perspective. BMC medical informatics and decisionmaking 20 (2020), 19. Weilin Cong, Si Zhang, Jian Kang, Baichuan Yuan, Hao Wu, Xin Zhou, HanghangTong, and Mehrdad Mahdavi. 2023. Do We Really Need Complicated ModelArchitectures For Temporal Networks? arXiv preprint arXiv:2302.11636 (2023).",
  "Finale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of inter-pretable machine learning. arXiv preprint arXiv:1702.08608 (2017)": "Valeria Gelardi, Didier Le Bail, Alain Barrat, and Nicolas Claidiere. 2021. Fromtemporal network data to the dynamics of social relationships. Proceedings of theRoyal Society B 288, 1959 (2021), 20211164. Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot,Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. 2016. beta-vae:Learning basic visual concepts with a constrained variational framework. InInternational conference on learning representations. Shenyang Huang, Yasmeen Hitti, Guillaume Rabusseau, and Reihaneh Rabbany.2020. Laplacian change point detection for dynamic graphs. In Proceedings ofthe 26th ACM SIGKDD International Conference on Knowledge Discovery & DataMining. 349358.",
  "Thomas N Kipf and Max Welling. 2017. Semi-supervised classification with graphconvolutional networks. In International Conference on Learning Representations": "Srijan Kumar, Xikun Zhang, and Jure Leskovec. 2019. Predicting dynamic em-bedding trajectory in temporal interaction networks. In Proceedings of the 25thACM SIGKDD international conference on knowledge discovery & data mining.12691278. Namkyeong Lee, Dongmin Hyun, Gyoung S Na, Sungwon Kim, Junseok Lee, andChanyoung Park. 2023. Conditional Graph Information Bottleneck for MolecularRelational Learning. arXiv preprint arXiv:2305.01520 (2023). Namkyeong Lee, Kanghoon Yoon, Gyoung S Na, Sein Kim, and Chanyoung Park.2023. Shift-robust molecular relational learning with causal substructure. InProceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery andData Mining. 12001212. Shuang Li, Mingquan Feng, Lu Wang, Abdelmajid Essofi, Yufeng Cao, JunchiYan, and Le Song. 2021. Explaining point processes by learning interpretabletemporal logic rules. In International Conference on Learning Representations. Zhao Li, Pengrui Hui, Peng Zhang, Jiaming Huang, Biao Wang, Ling Tian, JiZhang, Jianliang Gao, and Xing Tang. 2021. What happens behind the scene?Towards fraud community detection in e-commerce from online to offline. InCompanion Proceedings of the Web Conference 2021. 105113. Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, HaifengChen, and Xiang Zhang. 2020. Parameterized explainer for graph neural network.Advances in neural information processing systems 33 (2020), 1962019631.",
  "Siqi Miao, Mia Liu, and Pan Li. 2022. Interpretable and generalizable graph learn-ing via stochastic attention mechanism. In International Conference on MachineLearning. PMLR, 1552415543": "Pietro Panzarasa, Tore Opsahl, and Kathleen M Carley. 2009.Patterns anddynamics of users behavior and interaction: Network analysis of an onlinecommunity. Journal of the American Society for Information Science and Technology60, 5 (2009), 911932. Xue Bin Peng, Angjoo Kanazawa, Sam Toyer, Pieter Abbeel, and Sergey Levine.2018. Variational discriminator bottleneck: Improving imitation learning, inverserl, and gans by constraining information flow. arXiv preprint arXiv:1810.00821(2018).",
  "Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan.2020. Inductive representation learning on temporal graphs. arXiv preprintarXiv:2002.07962 (2020)": "Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec.2019. Gnnexplainer: Generating explanations for graph neural networks. Ad-vances in neural information processing systems 32 (2019). Jiaxuan You, Tianyu Du, and Jure Leskovec. 2022. ROLAND: graph learningframework for dynamic graphs. In Proceedings of the 28th ACM SIGKDD Confer-ence on Knowledge Discovery and Data Mining. 23582366. Junchi Yu, Jie Cao, and Ran He. 2022. Improving subgraph recognition with vari-ational graph information bottleneck. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition. 1939619405.",
  "NotationDescription": "= {1,2, }Series of continuous eventsTimestampG = (V, E)Temporal graph with nodes V and edges ESelf-explainable temporal graph model, User and item node, respectively = (, ,,)Event between node and at time with attribute GGraph constructed immediately before the timestamp GL-hop computation graph of RSubgraph of G considered as important events (i.e., explanation).Ground truth labelLabel prediction (, )Mutual information functionN (;) = {1, ,}Neighboring nodes for node at time , where is # of neighbors,Attribute of an interaction between and () ()Representation of node at time in the -th layerDimension of the node representationRaw feature of node nodeDimension of the raw node featureedgeDimension of the raw edge featureTime encoding functionOutput dimension of a function Target eventCandidate eventNegative sample event for Negative sample node for ()Time-aware representation for the target event at time ()Time-aware representation for the candidate event at time ()Valid event representation for the candidate event at time Probability of given , respectivelyMLP function calculating the probability ()Mask for sampled from ()Representation of the explanation graph R",
  "TGIB0.110.53": "In this section, we measure the inference time to produce an expla-nation to investigate the effectiveness of TGIB. shows theinference time for several explanation models on Wikipedia andReddit datasets. All baselines are models that generate explanationsin a post-hoc manner, and TGAT is used as their base model. Theinference time is calculated for all test events. We obtain the fol-lowing observations: 1) TGNNexplainer is time-consuming due toits reliance on the MCTS algorithm. On the other hand, TGIB effi-ciently detects important candidate events based on the IB principleby injecting stochasticity into past candidate events to generateexplanations. 2) Some explanation models for static graphs havefast inference times, but according to , they exhibit low-quality explanations for temporal graphs. However, although TGIBis slower than them, it does not demonstrate a significant time",
  "Grad-CAM provides post-hoc explanation for the GNN byusing importance score computed with gradients of the logitvalue with respect to the node embeddings": "GNNExplainer is a post-hoc explanation model for provid-ing explanations for GNN predictions. Specifically, this modellearns to mask the input graph while including label informationin the detected subgraphs. PGExplainer uses parameterized edge distributions ob-tained by explanation network to make explanation subgraph.The explanation network is optimized by maximizing the mutualinformation between the explanatory subgraph and predictionsof the GNN. T-GNNExplainer aims to explain TGNNs post-hoc, em-ploying both navigator and explorer components. The pretrainednavigator captures inductive relationships among events, andthe explorer seeks the optimal combination of candidates forexplanation."
}