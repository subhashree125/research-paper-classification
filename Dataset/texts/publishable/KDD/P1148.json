{
  "ABSTRACT": "Link prediction is a common task on graph-structured data thathas seen applications in a variety of domains. Classically, hand-crafted heuristics were used for this task. Heuristic measures arechosen such that they correlate well with the underlying factors re-lated to link formation. In recent years, a new class of methods hasemerged that combines the advantages of message-passing neuralnetworks (MPNN) and heuristics methods. These methods performpredictions by using the output of an MPNN in conjunction with apairwise encoding that captures the relationship between nodes inthe candidate link. They have been shown to achieve strong perfor-mance on numerous datasets. However, current pairwise encodingsoften contain a strong inductive bias, using the same underlyingfactors to classify all links. This limits the ability of existing meth-ods to learn how to properly classify a variety of different linksthat may form from different factors. To address this limitation,we propose a new method, LPFormer, which attempts to adap-tively learn the pairwise encodings for each link. LPFormer modelsthe link factors via an attention module that learns the pairwiseencoding that exists between nodes by modeling multiple factorsintegral to link prediction. Extensive experiments demonstrate thatLPFormer can achieve SOTA performance on numerous datasetswhile maintaining efficiency. The code is available at The code isavailable at",
  "link prediction, graph transformer": "ACM Reference Format:Harry Shomer, Yao Ma, Haitao Mao, Juanhui Li, Bo Wu, and Jiliang Tang.2024. LPFormer: An Adaptive Graph Transformer for Link Prediction. InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discoveryand Data Mining (KDD 24), August 2529, 2024, Barcelona, Spain. ACM, NewYork, NY, USA, 16 pages. Permission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0490-1/24/08. : Example of multiple heuristic scores for the candi-date links (source, 5), (source, 6), and (source, 7). Each heuris-tic corresponds to a different LP factor local (CNs), global(Katz), and feature proximity (Feat-Sim).",
  "INTRODUCTION": "Link prediction (LP) attempts to predict unseen edges in a graph.It has been adopted in many applications including recommendersystems , social networks , and drug discovery . Tradi-tionally, hand-crafted heuristics were used to identify new links inthe graph . Heuristics are often chosen based on factorsthat typically correlate well with the formation of new links. Forexample, a popular heuristic is common neighbors (CNs), whichassume that the links are more likely to exist between node pairswith more shared neighbors. It has been found that these factors,which we refer to as LP Factors, often stem from the local andglobal structural information and feature proximity . We give anexample in that demonstrates different heuristic scores formultiple candidate links. Each heuristic score corresponds to one ofthe LP factors: CNs for local information, Katz for global, and Feat-Sim for feature proximity. We can observe that the pair (source, 5)has the highest CN and Katz score of the candidate links, indicatingan abundance of local and global structural information betweenthe pair. On the other hand, the feature similarity for (source, 5) isthe lowest among the candidate links. This indicates that differentLP factors and heuristics have distinct assumptions about why linksare formed.More recently, message passing neural networks (MPNNs) ,which are able to learn effective node representations via messagepassing, have been widely adopted for LP tasks. They predict theexistence of a link by combining the node representations of bothnodes in the link. However, such a node-centric view is unableto incorporate the pairwise information between the nodes in thelink. Because of this, conventional MPNNs have been demonstrated",
  "KDD 24, August 2529, 2024, Barcelona, SpainHarry Shomer, Yao Ma, Haitao Mao, Juanhui Li, Bo Wu, and Jiliang Tang": "struggles on ogbl-citation2 in comparison to NCNC. We observe that this is due to the need of the PPR matrix, which while sparse, requires alarge amount of memory and processing time. In the future, we plan to fix this problem by performing a simple and efficient pre-processingstep. Specifically, before training, we can iterate over all target links and extract the relevant PPR scores. This would obviate the need tostore the PPR matrix and determine the nodes for each link. Furthermore, this only needs to be done once before tuning the model. Thiswould greatly reduce the storage and time needed to train LPFormer on all datasets and is an avenue we plan to explore in the future.",
  "BACKGROUND2.1Related Work": "Link prediction (LP) aims to model how links are formed in a graph.The process by which links are formed, i.e., link formation, is oftengoverned by a set of underlying factors . We refer to these asLP factors. Two categories of methods are used for modeling thesefactors heuristics and MPNNs. We describe each class of methods.We further include a discussion on existing graph transformers.Heuristics for Link Prediction. Heuristics methods attempt to explicitly model the LP factors via hand-crafted mea-sures. Recently, Mao et al. have shown that there are threemain factors that correlate with the existence of a link: (1) localstructural information, (2) global structural information, and (3) fea-ture proximity. Local structural information only considers theimmediate neighborhood of the target link. Representative methodsinclude Common Neighbors (CN) , Adamic Adar (AA) , andResource Allocation (RA) . They are predicated on the assump-tion that nodes that share a greater number neighbors exhibit ahigher probability of forming connections. Global structural in-formation further considers the global structure of the graph. Suchmethods include Katz and Personalized PageRank (PPR) .These methods posit that nodes interconnected by a higher num-ber of paths are deemed to have larger similarity and, therefore,are more likely to form connections. Lastly, feature proximityassumes nodes with more similar features connect . Previouswork have shown that leveraging the node features arehelpful in predicting links. Lastly, we note that Mao et al. hasrecently shown that to properly predict a wide variety of links, itsintegral to incorporate all three of these factors.MPNNs for Link Prediction. Message Passing Neural Net-works (MPNNs) aim to learn node representations via themessage passing mechanism. Traditional MPNNs have been usedfor LP including GCN , SAGE , and GAE . However, theyhave been shown to be suboptimal for LP as they arent expressiveenough to capture important pairwise patterns . SEAL and NBFNet try to address this by customizing the messagepassing process to each target link. This allows for the messagepassing to learn pairwise information specific to the target link.However, these methods have been shown to be unduly expensiveas they require a separate round of message passing for each targetlink. As such, recent methods have been proposed to instead de-couple the message passing and pairwise information ,reducing the time needed to do message passing. Such methodsinclude NCN/NCNC which exploit the common neighbor in-formation and BUDDY and Neo-GNN which consider theglobal structural information.Graph Transformers. Recent work has attempted to extendthe original Transformer architecture to graph-structured data.Graphormer learns node representations by attending all nodesto each other. To properly model the structural information, they",
  "LPFormer: An Adaptive Graph Transformer for LPKDD 24, August 2529, 2024, Barcelona, Spain": "propose to use multiple types of structural encodings (i.e., struc-tural, centrality, and edge). SAN further considers the use ofthe Laplacian positional encodings (LPEs) to enhance the learntstructural information. Alternatively, TokenGT considers allnodes and edges as tokens in the sequence when performing at-tention. Due to the large complexity of these models, they areunable to scale to larger graphs. To address this, several graphtransformers have been proposed for node classificationthat attempt to efficiently attend to the graph. However, while somework have formulated transformers for knowledge graphcompletion, to our knowledge, there are no graph transformersdesigned specifically for LP on uni-relational graphs.",
  "Preliminaries": "We denote a graph as G = {V, E}, where V and E are the setsof nodes and edges in G, respectively. The adjacency matrix isrepresented as R| || |. The -dimensional node features arerepresented by the matrix R| |. The set of neighbors for anode is given by N (). The set of overlapping neighbors betweentwo nodes and, i.e., the common neighbors (CNs), is expressed byNCN(,). We further denote the set of nodes that are 1-hop neighbors",
  "THE PROPOSED FRAMEWORK": "In , we highlighted the importance of adaptively model-ing multiple types of LP factors. However, current methods thatuse pairwise encodings, i.e., DP-MPNNs, struggle to appropriatelyachieve this goal. This is due to two issues: (1) They only attemptto model a subset of the potential LP factors (e.g., only local struc-tural information), limiting their ability to model multiple factors.(2) They use a one-size-fits-all approach in regard to pairwise en-coding, using the same combination of LP factors for each targetlink. These issues strongly limit the potential of such methods toproperly model a variety of different target links. To overcomethese problems, we propose LPFormer, a new transformer-basedmethod that can adaptively customize the pairwise information foreach target link by considering a variety of different LP factors inan efficient manner.",
  "(,) = MLPh h (,),(1)": "where is the representation of node encoded by the MPNN.Various DP-MPNNs adopt different ways to model the pairwiseencoding. For example, NCN models the pairwise encoding(,) as the summation of the node representations of the CNs. Thedefinitions of (,) for other prominent DP-MPNNs can be foundin Appendix A. The pairwise encodings in these existing methods are typically manually selected or extracted from the graph, whichlimits the LP factors they can cover. For example, (,) in NCN andNCNC only capture the local structural information. BUDDY ignores the node features when computing the pairwise encoding.To flexibly model multiple types of LP factors, we propose a generalformulation for pairwise encodings as follows,",
  "V(,,) (,,),(2)": "where (,,) measures the importance of node to (,), and(,,) is the encoding of node relative to (,). By consideringwhich nodes should be considered for (,) and how they are relatedto the node pair, Eq. (2) can model different LP factors by manuallydefining (,,) and (,,). In particular, we demonstrate howthe heuristic methods corresponding to different LP factors can fitinto this framework.Common Neighbors (CNs) : CNs considers the local struc-tural information and is defined for a pair of nodes (,) as NCN(,) =N () N (). Eq. (2) is equal to the CNs when (,,) = 1 and:",
  ",": "where B|V| is a one-hot vector for a node .Feature Similarity: The feature similarity of the pair of nodes(,) is expressed by dis(x, x) where x are the node featuresof node and dis() is a distance function (e.g., euclidean dis-tance). This can be rewritten as Eq. (2) by substituting (,,) =dis(x, x) and (,,) = .These examples demonstrate that the general formulation canindeed model many different LP factors including local and globalstructural information and feature proximity. We further show inAppendix B that Eq. (2) can model a variety of additional LP factorsincluding RA , the pairwise encodings used in NCN/NCNC and Neo-GNN . However, fitting these methods into the for-mulation in Eq. (2) requires manually defining both (,,) and(,,). This constrains the information represented by (,)based on the choice of design. Motivated by this, in the next sectionwe introduce our method that does not rely on a handcrafting both(,,) and (,,).",
  "Modeling Pairwise Encodings via Attention": "In .1, we introduced a general formulation for pairwiseencodings in Eq. (2), which is able to capture a variety of differ-ent LP factors. However, it requires manually defining both termsin the equation. This limits our ability to customize the pairwiseinformation to each target link. As such, we further aim to move",
  "(,,) =exp( (,,)) V(,) exp( (,,)) ,(4)": "where V(,) = V \\ {,}. The attention weight (,,) can beconsidered as the impact of a node on (,) relative to all nodesin G. This allows the model to emphasize different LP factors foreach target link. The node encoding (,,) includes the featuresof node in conjunction with the RPE and is defined as:",
  "(,,) = Wh rpe(,,).(5)": "By substituting Eq. (4) and Eq. (5) into Eq. (2) we can compute thepairwise information (,). We further define () in Eq. (4) as theGATv2 attention mechanism. The detailed formulation is givenin Appendix D. The feature representations h are computed via aMPNN. We use GCN in this work. However, it is unclear howto properly encode the RPE of a node relative to (,), rpe(,,).We aim to design the RPE to capture both the local and globalstructural relationship between the node and target link while also",
  "PPR-Based Relative Positional Encodings": "In this section, we introduce our strategy for computing the RPEof a node relative to a target link (,). Intuitively, we want theRPE to reflect the positional relationship between and (,) suchthat different types of information (i.e., local vs. global) are encodeddifferently. Using as an example, since node 3 is a CN of(source, 5) we expect it to have a much different relationship to thetarget link than node 6, which is a 2-hop neighbor of both nodes.An enticing option is to use the double radius node labeling (DRNL)trick introduced by Zhang and Chen . However, Chamberlainet al. have shown it to be prohibitively expensive to calculate forlarger graphs. Furthermore, existing RPEs are typically infeasible tocalculate on larger graphs as they often rely on pairwise distancesor the eigenvectors of the Laplacian .As such, we seek an RPE that can both distinguish the relation-ship of different nodes to the target link while also being efficientto calculate. To motivate our RPE design, we draw inspiration fromthe following Proposition. Proposition 1. Consider a target link (,) and a node V \\{,}. The PPR score of a root node and target node withteleportation probability is denoted by ppr(, ). Let () be theprobability of a walk of length beginning at node and terminatingat . We define , () := () + (). We also define a weight",
  "=0, ().(6)": "The detailed proof is given in Appendix C. From Proposition 1,we can make the following observations: (1) The PPR scores encodethe weighted sum of the probabilities of different length randomwalks connecting two nodes. (2) Walks of shorter length are givenhigher importance, as evidenced by the dampening factor =(1 ) which decays with the increase in . These observationsimply that a larger value of (,,) correlates with theexistence of many shorter walks connecting node to theboth nodes in the target link (,).Therefore, the PPR scores can be used as an intuitive and usefulmethod to understand the structural relationship between node and both nodes in the target link (,). If both scores, ppr(,)and ppr(,), are high, there exists a high probability that manyshorter walks connect to both nodes in the target link. Thisimplies that node has a stronger impact on the nodes in the targetlink. On the other hand, if both PPR scores are low, there is likelyvery little relationship between and the target link. This allowsfor a convenient way of differentiating how a node structurallyrelates to the target link. Furthermore, we note that the PPR matrixcan be efficiently pre-computed using the algorithm introducedby Andersen et al. , allowing for easy computation and use.Following this idea, to calculate the RPE of a node , we use thePPR scores of a node relative to both nodes in the target link (,).",
  "rpe(,,) = MLP (ppr(,), ppr(,)) .(7)": "By introducing learnable parameters to (), it allows for the modellearn the importance of individual PPR scores and how they interactwith each other. To ensure that Eq. (7) is invariant to the order ofthe nodes in the target link, i.e., (,) and (,), we further set theRPE to be equal to the summation of the representations given byboth (,) and (,):",
  "rpe(,,) = rpe(,,) + rpe(,,).(8)": "However, a concern with Eq. (8) is that it is not guaranteed to be ableto distinguish certain types of nodes from each other. For example,it is necessary to clearly distinguish CNs from other nodes dueto their important role in link formation . To overcome thisissue, we fit three separate MLPs for when is a: CN of (,), a1-hop neighbor of either and , and a >1-hop neighbor of both and . This ensures that we can properly distinguish betweenthese three types of nodes. We verify the effectiveness of this designin .4. Lastly, we note that while other work hasconsidered the use of random-walk based positional encodings,they are only designed for use on the node-level and are unable tobe used for link-level tasks like LP.",
  "Efficiently Attending to the Graph Context": "The proposed attention mechanism in .2 attends to allnodes in the graph, sans those in the link itself. This makes itdifficult to scale to large graphs. Motivated by selective andsparse attention, we opt to attend to only a small portion ofthe nodes.At a high level, we are interested in determining a subset ofnodes N (,) V to attend to for the target link (,). Ourgoal is to choose the set of nodes N (,) such that they are (a)few in number to improve scalability and (b) provide importantcontextual information to the pair (,) to best learn the pairwiseinformation. This can be achieved by only considering all nodeswhere the importance of the node to the target link (,) isconsidered high. Formally, we can write this as the following whereI(,,) is a function that denotes the importance of a node tothe target link (,):",
  "N (,) = { V \\ {,} | I(,,) > }.(9)": "The threshold allows us to distinguish those nodes that are suffi-ciently important to the target link. This allows for a simple andefficient way of determining the set N (,). However, what do weuse to model the importance I(,,)? For ease of optimization andbetter efficiency, we avoid parameterizing the function I(,,).Instead, we want to choose a metric such that can properly serveas a proxy for the importance of a node to (,) while also beingconcentrated in a small subset of nodes. Such a metric will allowEq. (9) to choose a small but influential set of nodes to attend to.A measure that satisfies both criteria is Personalized Pagerank(PPR) . In .3 we discussed that the PPR score can serveas a good tool to model the influence of a one node on another.Furthermore, existing work shows that the PPR scorestend to be highly localized in a small subset of nodes. Therefore by",
  "N(,) = { N(,) | ppr(,) > , ppr(,) > },(10)": "where N(,) is the filtered node set for all nodes of the type {CN, 1Hop, >1Hop} and is the corresponding PPR threshold.We note that while other work has used PPR to filter thenodes on the node-level, no existing work has done so on the link-level.We corroborate this design by demonstrating that LPFormer canachieve SOTA performance in LP (.2) while achieving afaster runtime than the second-best method, NCNC , on densergraphs (.7). This is despite the fact that LPFormer canattend to a wider variety of nodes. We further show in .5that the performance is stable with regards to the values of chosen,allowing us to easily choose a proper threshold on any dataset. 3.5LPFormerWe now define the overall framework LPFormer. The overall pro-cedure is given in : (1) We first learn node representationsfrom the input adjacency and node features via an MPNN. We notethat this step is agnostic to the target link. (2) For a target link (,)we extract the nodes to attend to, i.e. N (,). This is done via thePPR thresholding technique defined in .4. (3) We apply layers of attention, using the mechanism defined in .2.The output is the pairwise encoding (,). (4) We generate theprediction of the target link using three types of information: theelement-wise product of the node representation, the pairwise en-coding, and the number of CN, 1-Hop, and >1-Hop nodes identifiedby Eq. (10). The score function is given by:",
  "EXPERIMENTS": "In this section, we conduct extensive experiments to validate theeffectiveness of LPFormer. Specifically, we attempt to answer thefollowing questions: (RQ1) Can LPFormer consistently outperformbaseline methods on a variety of different benchmark datasets?(RQ2) Is LPFormer able to model a variety of different LP factors?(RQ3) Can LPFormer be run efficiently on large dense graphs? Wefurther conduct studies ablating each component of our model andanalyzing the effect of the PPR-based threshold on performance.",
  "Experimental Settings": "Datasets. We include Cora, Citeseer, and Pubmed and ogbl-collab, ogbl-ppa, ogbl-ddi, and ogbl-citation2 . Furthermore, forCora, Citeseer, and Pubmed we experiment under a single fixed split(see Appendix E.1 for further discussion). The detailed statistics foreach dataset are shown in .Baseline Models. We compare LPFormer against a wide va-riety of baselines including: CN , AA , RA , GCN ,SAGE , GAE , SEAL , NBFNet , Neo-GNN ,BUDDY , and NCNC . Results on Cora, Citeseer, and Pubmedare taken from Li et al. . Results for the heuristic methods arefrom Hu et al. . All other results are either from their respectivestudy or Chamberlain et al. .Hyperparameters: The learning rate is tuned from {13, 53},the decay from {0.95, 0.975, 1}, and the dropout from [0, 0.7], andthe weight decay from {0, 14, 17}. The size of the hidden di-mension is set to 64 for ogbl-ppa and ogbl-citation2, 128 for Cora,Pubmed, and ogbl-collab, and 256 for Citeseer. Lastly, the PPRthreshold is tuned from {12, 13, 14}.Evaluation Metrics. Each positive target link is evaluated againsta set of given negative links. The rank of the positive link amongthe negatives is used to evaluate performance. The two types ofmetrics that are used to evaluate this ranking are Hits@K and MRR.For the OGB datasets we use the metric used in the original study.This includes Hits@50 for ogbl-collab, Hits@100 for ogbl-ppa andMRR for ogbl-citation2. For Cora, Citeseer, Pubmed we follow Liet al. and use MRR. Lastly, the same set of negative links is usedfor all positive links except on ogbl-citation2, where providesa customized set of 1000 negatives for each individual positive link.",
  "Main Results": "We present the results of LPFormer compared with baselines onmultiple benchmark datasets. Note that we omit ogbl-ddi from themain results due to recent issues discovered by Li et al. (seeAppendix E.2 for more details). The results are shown in .We observe that LPFormer can achieve SOTA performance on 5/6datasets, significantly outperforming other baselines. Moreover,LPFormer is also the most consistent of all the methods, achievingstrong performance on all datasets. This is as opposed to previousSOTA methods, NCNC and BUDDY, which tend to struggle onCora and Pubmed. We attribute the consistency of LPFormer tothe flexibility of our model, allowing it to customize the LP factorsneeded to each link and dataset.",
  "Performance by LP Factor": "In this section, we measure the ability of LPFormer to capturea variety of different LP factors. To measure this, we identify allpositive target links when there is only one dominant LP factor.For example, one group would contain all target links where theonly dominant factor is the local structural information. We focus onlinks that correspond to one of the three groups identified in :local structural information, global structural information, andfeature proximity.We identify these groups by using popular heuristics as proxiesfor each factor. For local structural information, we use CNs ,for global structural information we use PPR as its the most",
  "LPFormer39.425.7865.424.6540.171.9268.140.5163.320.6389.810.131.2": "computationally efficient of all global methods, and for featureproximity, we use the cosine similarity of the features. Using theseheuristics, we determine if only one factor is dominant by com-paring the relative score of each heuristic. This is done by firstcomputing the score for each factor for the target link (,) (,). For each factor, we then compute the score correspondingto the -th percentile among all links, . We choose a larger valueof (i.e. 90%) such that a score indicates that a significantamount of pairwise information exists for that factor. For a singletarget link, we then compare the score of each factor (,) to . If (,) is true for only one factor, this implies that the scorefor only one factor is high. Therefore there is a notable amountof pairwise information existing for only one factor for the link(,). This ensures that only one factor is strongly expressed. Ifthis is true, we then assign the target link (,) to factor . Pleasesee Appendix E.4 for a more detailed explanation.We demonstrate the results on Cora, Citeseer, and ogbl-collabin . We observe that LPFormer typically performs best foreach individual LP factor on all datasets. Furthermore, it is also themost consistently well-performing on each factor as compared toother methods. For example, on Cora the other methods strugglefor links that correspond to the feature proximity factor. LPFormer,on the other hand, is able to significantly outperform them onthose target links, performing around 33% better than the secondbest method. Lastly, we note that most methods tend to performwell on the links corresponding to the global factor, even if theydont explicitly model such information. This is caused by a strong correlation that tends to exist between local and global structuralinformation, often resulting in considerable overlap between bothfactors . These results show that LPFormer can indeed adapt tomultiple types of LP factors, as it can consistently perform well onsamples belonging to a variety of different LP factors. Additionalresults are given in Appendix E.5.",
  "Ablation Study": "We further include an ablation study to verify the effectiveness ofthe proposed components in LPFormer. In particular, we introduce6 variants of LPFormer. (a) w/o Learnable Att: No attention islearned. As such, we set all attention weights to 1 and removethe RPE. (b) w/o Features in Att: We remove the node featureinformation from the attention mechanism. (c) w/o RPE in Att:We remove the RPE from the attention mechanism. (d) w/o PPRRPE: We replace the PPR-based RPE with a learnable embeddingfor each of CN, 1-Hop, and >1-Hop nodes. (e) w/o PPR RPE byNode Type: We dont fit a separate function for each node typewhen determining the PPR RPE (see .3). Instead we use onefor all nodes. (f) w/o Counts: We remove the counts of differentnodes from the scoring function.The results are shown in . We include ogbl-collab, ogbl-ppa, and Citeseer. We observe that ablating a component always de-creases the performance. However, the magnitude of the decrease isdataset-dependent. For example, on ogbl-collab, ablating the featureinformation in the attention marginally affects the performance.",
  ": Performance on links that contain one dominant LP factor. Results are on (a) Cora, (b) Citeseer, and (c) ogbl-collab": "However, on ogbl-ppa and Citeseer, removing the feature informa-tion results in a large decrease in performance. On the other hand,while removing learnable attention results in a modest decreaseon ogbl-ppa, for the other two datasets we see a large drop. Thishighlights the importance of each component of our framework, asthey are each necessary for consistently strong performance acrossmultiple datasets.",
  "Effect of the PPR Thresholds": "We examine the effect of varying the PPR threshold for both 1-Hopand >1Hop nodes as described in Eq. (10). The results for ogbl-collab and ogbl-citation2 are shown in . When varying the1-Hop threshold, we fix the value of the >1Hop threshold to 1e-2for both datasets. When varying the >1Hop threshold, we fix thevalue of the 1-Hop threshold to 1e-4 for both datasets. We can observe that modifying the threshold has little effecton the underlying performance of the model. For both datasets, avalue of 1e-2 works well for the >1Hop threshold and 1e-4 workswell for the 1-Hop threshold. We typically find that setting bothvalues to 1e-2 provides a good trade-off between performance andefficiency.",
  "Performance on HeaRT Setting": "We further test the performance of our method on the HeaRT evaluation setting, which considers a more realistic and difficultevaluation setting for link prediction. This is done by introducing amuch harder and more realistic set of negative samples during eval-uation. Li et al. observe that this results in a large decrease inperformance on all datasets. Furthermore, compared to the originalevaluation setting, MPNNs designed specifically for link predictionare often outperformed by heuristics or other MPNNs.The full results can be found in . We observe that LP-Former performs considerably better than all other models. Forinstance, the mean rank of LPFormer is 3.1x better than the 2ndbest-performing model, NCN. This indeed shows the advantageof LPFormer, as it can consistently achieve extraordinary perfor-mance across all datasets under the much more challenging HeaRTevaluation setting. This is as opposed to other LP-specific methodsthat often perform similarly to standard MPNN methods.",
  "Runtime Analysis": "In this section, we compare the runtime of LPFormer against NCNC,which is the strongest performing baseline. The results are shownin on all four OGB datasets We further include the meandegree of each dataset in parentheses. We observe that LPFormershines on denser datasets, taking significantly less time to train oneepoch. This is despite that LPFormer can attend to nodes beyondthe 1-hop radius of the target link. This underscores the importanceof the PPR thresholding technique introduced in .4, as itallows for efficient attention to a wider variety of nodes. Lastly,we note that LPFormer struggles on the ogbl-citation2 dataset dueto the large number of nodes in the dataset (i.e., 2,927,963), whichrequires the sparse PPR matrix to be quite large. For future work",
  "CONCLUSION": "In this paper we introduce a new framework, LPFormer, that aimsto integrate a wider variety of pairwise information for link predic-tion. LPFormer does this via a specially designed graph transformer,which adaptively considers how a node pair relate to each otherin the context of the graph. Extensive experiments demonstratethat LPFormer can achieve SOTA performance on a wide vari-ety of benchmark datasets while retaining efficiency. We furtherdemonstrate LPFormers supremacy at modeling multiple types ofLP factors. For future work, we plan on exploring other methodsof incorporating multiple LP factors with an emphasis on globalstructural information. We also plan to investigate the potential ofalternative relative positional encodings. This research is supported by the National Science Foundation(NSF) under grant numbers CNS 2246050, IIS1845081, IIS2212032,IIS2212144, IOS2107215, DUE 2234015, DRL 2025244 and IOS2035472,the Army Research Office (ARO) under grant number W911NF-21-1-0198, the Home Depot, Cisco Systems Inc, Amazon Faculty Award,Johnson&Johnson, JP Morgan Faculty Award and SNAP. Khushnood Abbas, Alireza Abbasi, Shi Dong, Ling Niu, Laihang Yu, Bolun Chen,Shi-Min Cai, and Qambar Hasan. 2021. Application of network link prediction indrug discovery. BMC bioinformatics 22 (2021), 121.",
  "Reid Andersen, Fan Chung, and Kevin Lang. 2006. Local graph partitioningusing pagerank vectors. In 2006 47th Annual IEEE Symposium on Foundations ofComputer Science (FOCS06). IEEE, 475486": "Dzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Bengio. 2015. Neural ma-chine translation by jointly learning to align and translate. In 3rd InternationalConference on Learning Representations, ICLR 2015. Albert-Laszlo Barabsi, Hawoong Jeong, Zoltan Nda, Erzsebet Ravasz, AndrasSchubert, and Tamas Vicsek. 2002. Evolution of the social network of scientificcollaborations. Physica A: Statistical mechanics and its applications 311, 3-4 (2002),590614. Aleksandar Bojchevski, Johannes Gasteiger, Bryan Perozzi, Amol Kapoor, MartinBlais, Benedek Rzemberczki, Michal Lukasik, and Stephan Gnnemann. 2020.Scaling graph neural networks with approximate pagerank. In Proceedings ofthe 26th ACM SIGKDD International Conference on Knowledge Discovery & DataMining. 24642473.",
  "Shaked Brody, Uri Alon, and Eran Yahav. 2022.How Attentive are GraphAttention Networks?. In International Conference on Learning Representations": "Benjamin Paul Chamberlain, Sergey Shirobokov, Emanuele Rossi, Fabrizio Frasca,Thomas Markovich, Nils Hammerla, Michael M Bronstein, and Max Hansmire.2022. Graph Neural Networks for Link Prediction with Subgraph Sketching.arXiv preprint arXiv:2209.15486 (2022). Jinsong Chen, Kaiyuan Gao, Gaichao Li, and Kun He. 2022. NAGphormer: A tok-enized graph transformer for node classification in large graphs. In The EleventhInternational Conference on Learning Representations.",
  "Fan Chung. 2007. The heat kernel as the pagerank of a graph. Proceedings of theNational Academy of Sciences 104, 50 (2007), 1973519740": "Gonalo M Correia, Vlad Niculae, and Andr FT Martins. 2019. Adaptively SparseTransformers. In Proceedings of the 2019 Conference on Empirical Methods inNatural Language Processing and the 9th International Joint Conference on NaturalLanguage Processing (EMNLP-IJCNLP). 21742184. Nur Nasuha Daud, Siti Hafizah Ab Hamid, Muntadher Saadoon, Firdaus Sahran,and Nor Badrul Anuar. 2020. Applications of link prediction in social networks:A review. Journal of Network and Computer Applications 166 (2020), 102716. Yuxiao Dong, Reid A Johnson, Jian Xu, and Nitesh V Chawla. 2017. Structuraldiversity and homophily: A study across more than one hundred big networks.In Proceedings of the 23rd ACM SIGKDD International Conference on KnowledgeDiscovery and Data Mining. 807816. Philippe Flajolet, ric Fusy, Olivier Gandouet, and Frdric Meunier. 2007. Hyper-loglog: the analysis of a near-optimal cardinality estimation algorithm. Discretemathematics & theoretical computer science Proceedings (2007). Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George EDahl. 2017. Neural message passing for quantum chemistry. In Internationalconference on machine learning. PMLR, 12631272.",
  "Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representationlearning on large graphs. Advances in neural information processing systems 30(2017)": "Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: Datasets formachine learning on graphs. Advances in neural information processing systems33 (2020), 2211822133. Hong Huang, Jie Tang, Lu Liu, JarDer Luo, and Xiaoming Fu. 2015. Triadicclosure pattern analysis and prediction in social networks. IEEE Transactions onKnowledge and Data Engineering 27, 12 (2015), 33743389.",
  "Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification withGraph Convolutional Networks. In International Conference on Learning Repre-sentations (ICLR)": "Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent Ltourneau, and Pru-dencio Tossou. 2021. Rethinking graph transformers with spectral attention.Advances in Neural Information Processing Systems 34 (2021), 2161821629. Juanhui Li, Harry Shomer, Haitao Mao, Shenglai Zeng, Yao Ma, Neil Shah, JiliangTang, and Dawei Yin. 2023. Evaluating Graph Neural Networks for Link Predic-tion: Current Pitfalls and New Benchmarking. arXiv preprint arXiv:2306.10453(2023). Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec. 2020. Distance encod-ing: Design provably more powerful neural networks for graph representationlearning. Advances in Neural Information Processing Systems 33 (2020), 44654478.",
  "Mark EJ Newman. 2001. Clustering and preferential attachment in growingnetworks. Physical review E 64, 2 (2001), 025102": "Maximilian Nickel, Xueyan Jiang, and Volker Tresp. 2014. Reducing the rank inrelational factorization models by including observable patterns. Advances inNeural Information Processing Systems 27 (2014). Vardaan Pahuja, Boshi Wang, Hugo Latapie, Jayanth Srinivasa, and Yu Su. 2023.A retrieve-and-read framework for knowledge graph link prediction. In Proceed-ings of the 32nd ACM International Conference on Information and KnowledgeManagement. 19922002. Ladislav Rampek, Michael Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu,Guy Wolf, and Dominique Beaini. 2022. Recipe for a general, powerful, scalablegraph transformer. Advances in Neural Information Processing Systems 35 (2022),1450114515.",
  "Zhilin Yang, William Cohen, and Ruslan Salakhudinov. 2016. Revisiting semi-supervised learning with graph embeddings. In International conference on ma-chine learning. PMLR, 4048": "Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He,Yanming Shen, and Tie-Yan Liu. 2021. Do transformers really perform badlyfor graph representation? Advances in Neural Information Processing Systems 34(2021), 2887728888. Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton,and Jure Leskovec. 2018. Graph convolutional neural networks for web-scalerecommender systems. In Proceedings of the 24th ACM SIGKDD internationalconference on knowledge discovery & data mining. 974983. Seongjun Yun, Seoyoon Kim, Junhyun Lee, Jaewoo Kang, and Hyunwoo J Kim.2021. Neo-gnns: Neighborhood overlap-aware graph neural networks for linkprediction. Advances in Neural Information Processing Systems 34 (2021), 1368313694.",
  "Muhan Zhang and Yixin Chen. 2018. Link prediction based on graph neuralnetworks. Advances in neural information processing systems 31 (2018)": "Muhan Zhang, Pan Li, Yinglong Xia, Kai Wang, and Long Jin. 2021. Labeling trick:A theory of using graph neural networks for multi-node representation learning.Advances in Neural Information Processing Systems 34 (2021), 90619073. Muhan Zhang, Pan Li, Yinglong Xia, Kai Wang, and Long Jin. 2021. Labeling trick:A theory of using graph neural networks for multi-node representation learning.Advances in Neural Information Processing Systems 34 (2021), 90619073.",
  "This weighting scheme ensures that CNs play a larger role in the pairwise information than non-CNs": "BUDDY : BUDDY considers counting the number of nodes that correspond to different labels given by the double radius node labelingtrick . We first define the number of nodes that are a distance and from nodes and as A . We further define the numberof nodes where max(,) > as []. The pairwise encoding concatenates the counts belonging to all combination of = 1 . Thecounts are estimated using subgraph sketching algorithms and are denoted A and B. The pairwise encoding for a target link (,) isgiven by the following where [] = {1 }:",
  ".(28)": "Personalized Pagerank (PPR) Score : The personalized pagerank score is the pagerank score localized to a root node . The localizationis via a teleportation probability that transports the random walk back to the root node. We show that Eq. (2) can be rewritten as the PPRscore when setting (,,) equal to (28) and, following Chung , setting (,,) to:",
  "and (,,) = where B|V| is a one-hot vector for a node": "NCN : The pairwise encoding used in NCN is defined as the summation of the representations for the CNs of a link. Eq. (2) can berewritten as NCN when (,,) is equal to Eq. (23). (,,) is equal to the node representation encoded by a MPNN, i.e., (,,) = hwhere = MPNN(,). NCNC : NCNC extends NCNC by further weighting the 1-hop (non-CN) by their probability of linking to the other nodes. Given Eq. (2),the weight (,,) is equal to following where 1-hop neighbors are weighted by their probability of linking with the other node:",
  "CPROOF OF PROPOSITION 1": "Proposition 1. Consider a target link (,) and a node V \\ {,}. The PPR score of a root node and target node with teleportationprobability is denoted by ppr(, ). Let () be the probability of a walk of length beginning at node and terminating at . We define, () := () + (). We also define a weight := (1 ) for all walks of length . The PPR scores, (,) and (,), along withthe random walk probabilities of disparate lengths, are interconnected through the following relationship.",
  "=0(1 ) ,(34)": "where is a the random walk matrix and is a preference vector that is a one-hot vector for element . We note that pr () represents thelanding probability of node given the root node . As such, by definition, pr () = ppr(,). Furthermore, it is clear that = RV represents the probability of a walk of length beginning at node and stop all other nodes, individually. Also, the probabilities of all walksof length are weighted by = (1 ). (,,) can be obtained by first taking the sum of the PPR vectors for nodes and ,",
  "EADDITIONAL EXPERIMENTAL DETAILSE.1Planetoid splits": "We note that for each of Cora, Citeseer, Pubmed we use a fixed split. This follows the recent work of . Li et al. observe that for Cora,Citeseer, Pubmed there exists no unified data split between studies. They find that while recent work use 10 random splits, priorwork use a fixed split and train over 10 random seeds. Furthermore, there exists discrepancies in the preprocessing between thoseworks that use the random splits. Chamberlain et al. only use the largest connected component of each dataset while Wang et al. use the whole dataset. This makes any comparison of the published results difficult. Due to these discrepancies, we use the performance onthe fixed split given by Li et al. , as its the only split where all methods are evaluated and compared under the same setting.",
  "E.2Omission of ogbl-ddi under the Existing Evaluation": "We further omit the results of ogbl-ddi in . This is due to the observation made by Li et al. that there exists a poor relationshipbetween the validation and test performance. This extends to recent pairwise MPNNs, including NCN , Neo-GNN , and BUDDY .This makes tuning on the validation set difficult, as it doesnt guarantee good test performance. Due to this, they observe that when tuningon a fixed set of hyperparameter ranges, they are unable to achieve comparable results to the reported performance. Often they observe thatthe performance is actually much lower. Due to these concerns we believe ogbl-ddi is not suitable for the task of transductive link predictionand dont report the performance. For more details and discussion, please see Appendix D in Li et al. . However, they show that thisproblem does not afflict ogbl-ddi under the newly proposed HeaRT evaluation setting. As such, we further include the results for ourmethod under HeaRT in .",
  "E.3Computation of the PPR Matrix": "We compute the PPR matrix via the efficient approximation algorithm introduced by Andersen et al. . The estimation is controlled by atolerance parameter . The parameter controls both the speed of computation and the sparsity of the solution (i.e., a higher value of will produce a sparser PPR matrix). We use: = 17 for Cora and Citeseer, = 55 for ogbl-collab and ogbl-ppa, = 1 5 for Pubmed,and = 53 for ogbl-Citation2. The value of is chosen as a trade-off between accuracy and sparsity to allow for ease of storage in GPUmemory.",
  "E.4Splitting Target Links by LP Factor": "In .3 we demonstrate the performance on samples that correspond to a single LP factor. In this section we further detail thealgorithm used to determine the set of samples corresponding to each factor. We consider the three main factors: local structural information,global structural information, and feature proximity. We measure each using a single representative heuristic: CNs for local information,PPR for global information, and cosine feature similarity for feature proximity. For each sample, we check if the score is only high in oneheuristic. In this way, it tells us that there is a dominant factor present in the pairwise information.This determination is done by comparing the the heuristic scores of each target link against a threshold value. For a LP factor andtarget link (,), we denote the heuristic score as (,). The threshold value for factor is represented by and is chosen such that itcorresponds to a higher score. We desire to be a higher score such that any score than it indicates that a plethora of pairwise informationexists corresponding to factor . This is done by setting the threshold equal to the -th percentile value for that heuristic among all targetlinks. For example, for CNs, the 80th percentile score on one dataset may be 9. The value of is chosen to be high (e.g., 80%) due to theaforementioned reasoning. Given these inputs, for each target link we compare the score for factor against the threshold value of that factor.Continuing our example, if (,) only has 2 CNs, it is below the previously defined threshold. We only consider a sample as belongingto a single factor when it is (,) is true for one only one factor . So if the heuristic score for (,) is below the -th percentilethreshold for CNs and PPR but above for feature similarity, then feature proximity will be considered the dominant LP factor. However, if itsabove the threshold for both local and structural information, it will not be assigned to any group. This is done as we want to isolate linksthat only highly express one LP factor. This allows us to better understand how certain methods can model that specific factor. The detailedalgorithm is given in Algorithm 1.We note that each target link may not belong to a category. This can be due to there being no or many dominant LP factor. We furtherset the percentile equal to 90% on all datasets except for ogbl-collab for which we use 80%. These values were chosen as we wanted thepercentile to be suitably high such that we are confident that the corresponding factor is relevant to the target link. Furthermore, we use alower value for ogbl-collab as we found it produced a more even distribution of links by factor.",
  "E.5Additional Results for the LP Factor Experiments": "In .3 we observed the performance of various methods on target links where only a single LP factor is expressed. This is donethrough the use of heuristic scores. We further demonstrate the results on the Pubmed and ogbl-ppa datasets. Of note is that for ogbl-ppa theinitial node features are one-hot vectors that signify the species that the protein belongs to. We observe that due to the sparseness of thesefeatures, feature proximity measures are unable to properly predict any target links on their own. As such, the factor corresponding tofeature proximity is not expressed. We therefore exclude that factor for this analysis on ogbl-ppa.",
  "(b) ogbl-ppa": ": Performance for target links when there is only one LP factor strongly expressed. Results are on (a) Pubmed, (b)ogbl-ppa. We note that due the quality of features used, we omit the feature proximity factor for ogbl-ppa from our analysis The results for both Pubmed and ogbl-ppa datasets are given in . As shown earlier in , LPFormer can most consistentlyperform well across each factor. This suggests that LPFormer is best able to both model a variety of factors and adapt accordingly for eachtarget link.",
  "E.6Performance on Heterophilic Datasets": "In this section we evaluate LPFormer on multiple heterophilic datasets. Heterophily refers to the tendency of dissimilar nodes to be connected.This is as opposed to homophily, in which nodes with similar attributed are more likely to be connected. Since most graphs used forbenchmark datasets tend to contain homophilic patterns, heterophilic graphs present an interesting challenge regarding the effectiveness ofgraph-based methods. For a more detailed discussion on heterophilic graphs, please see .We test on two prominent heterophilic datasets, Squirrel and Chameleon . The statistics for each are in . We limit our comparisonto those LP methods that tend achieve the best results, including GCN, BUDDY, and NCNC. In , we report the MRR over five randomseeds. Note that we test under the original evaluation setting and not HeaRT. We observe that LPFormer can achieve a large increase overother methods, with a 14% and 9% increase in performance on Squirrel and Chameleon, respectively. These results indicate the superiorability of LPFormer to accurately model LP on heterophilic graphs, as compared to other methods.",
  "Require:": "CN() = Maps (, ) to # of CNs of the pairPPR() = Maps (, ) to PPR score of the pairFS() = Maps (, ) to feature cosine similarity of the pair = Percentile used to determine whether a factor is presentEtest = Positive test links 1: // Compute the score corresponding to the -th percentile for each heuristic2: CN = Percentile(, { (, ) | (, ) Etest})3: FS = Percentile(, { (, ) | (, ) Etest})4: PPR = Percentile(, {(, ) | (, ) Etest})"
}