{
  "ABSTRACT": "Compressed Neural Networks have the potential to enable deeplearning across new applications and smaller computational envi-ronments. However, understanding the range of learning tasks inwhich such models can succeed is not well studied. In this work, weapply sparse and binary-weighted Transformers to multivariate timeseries problems, showing that the lightweight models achieve accu-racy comparable to that of dense floating-point Transformers of thesame structure. Our model achieves favorable results across threetime series learning tasks: classification, anomaly detection, andsingle-step forecasting. Additionally, to reduce the computationalcomplexity of the attention mechanism, we apply two modifications,which show little to no decline in model performance: 1) in the clas-sification task, we apply a fixed mask to the query, key, and valueactivations, and 2) for forecasting and anomaly detection, whichrely on predicting outputs at a single point in time, we propose anattention mask to allow computation only at the current time step.Together, each compression technique and attention modificationsubstantially reduces the number of non-zero operations necessaryin the Transformer. We measure the computational savings of ourapproach over a range of metrics including parameter count, bitsize, and floating point operation (FLOPs) count, showing up to a53 reduction in storage size and up to 10.5 reduction in FLOPs.",
  "transformer; sparse; pruned; binary; deep learning; multivariatetime series; anomaly detection; classification; forecasting; lotteryticket hypothesis": "ACM Reference Format:Matt Gorbett, Hossein Shirazi, and Indrakshi Ray. 2023. Sparse BinaryTransformers for Multivariate Time Series Modeling. In Proceedings of the29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining(KDD 23), August 610, 2023, Long Beach, CA, USA. ACM, New York, NY,USA, 13 pages. Permission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 23, August 610, 2023, Long Beach, CA, USA 2023 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0103-0/23/08.",
  "INTRODUCTION": "The success of deep learning can largely be attributed to the avail-ability of massive computational resources . Models suchas the Transformer have changed machine learning in funda-mental ways, producing state-of-the-art results across fields suchas natural language processing (NLP), computer vision , andtime series learning . Much effort has been aimed at scalingthese models towards NLP efforts on large datasets , however,such models cannot practically be deployed in resource-constrainedmachines due to their high memory requirements and power con-sumption.Parallel to the developments of the Transformer, the LotteryTicket Hypothesis demonstrated that neural networks containsparse subnetworks that achieve comparable accuracy to that ofdense models. Pruned deep learning models can substantially de-crease computational cost, and enable a lower carbon footprint andthe democratization of AI. Subsequent work showed that we canfind highly accurate subnetworks within randomly-initialized mod-els without training them , including binary-weighted neuralnetworks . Such lottery-ticket style algorithms have mostlyexperimented with image classification using convolutional archi-tectures, however, some work has shown success in pruning NLPTransformer models such as BERT .In this work, we extend the Lottery Ticket Hypothesis to timeseries Transformers, showing that we can prune and binarize theweights of the model and still maintain an accuracy similar to thatof a Dense Transformer of the same structure. To achieve this,we employ the Biprop algorithm , a state-of-the-art techniquewith proven success on complex datasets such as ImageNet .The combination of weight binarization and pruning is uniquefrom previous efforts in Transformer compression. Moreover, eachcompression technique offers separate computational advantages:neural network pruning decreases the number of non-zero floatingpoint operations (FLOPs), while binarization reduces the storagesize of the model. The Biprop algorithms two compression meth-ods rely on each other during the training process to identify ahigh-performing subnetwork within a randomly weighted neuralnetwork. The combination of pruning and weight binarization isdepicted in a.We apply our approach to multivariate time series modeling. Re-search has shown that Transformers achieve strong results on timeseries tasks such as classification , anomaly detection ,and forecasting . Time series data is evident in systems suchas IoT devices , engines , and spacecraft , where newinsights can be gleaned from the large amounts of unmonitoredinformation. Moreover, such systems often suffer from resourceconstraints, making regular deep learning models unrealistic for",
  "KDD 23, August 610, 2023, Long Beach, CA, USAMatt Gorbett, Hossein Shirazi, and Indrakshi Ray": "To convert each layer to binary weights Biprop introduces gainterm R, which is common to Binary Neural Networks (BNNs). The gain term utilizes floating-point weights prior to bina-rization during training. During test-time, the alpha parameterscales the binarized weight vector. The parameter rescales binaryweights B {1, 1} to {, }, and the network function becomes (;(B M)). is calculated as",
  "Transformers in Time Series": "Various works have applied Transformers to time series learningtasks . The main advantage of the Transformer architectureis the attention mechanism, which learns the pairwise similarityof input patterns. Moreover, it can efficiently model long-rangedependencies compared to other deep learning frameworks suchas LSTMs . Zerveas et al. showed that we can use un-supervised pretrained Transformers for downstream time serieslearning tasks such as regression and classification. Additional workin time series classification has proposed using a two tower\" atten-tion approach with channel-wise and time-step-wise attention ,while other work has highlighted the benefits of Transformers forsatellite time series classification compared to both recurrent andconvolutional neural networks .For anomaly detection tasks, Transformers have shown favorableresults compared to traditional ML and deep learning techniques. : A sparse binary linear layer (left) and various atten-tion modules (right). a) An example of a sparse and binarylinear module, with binary weights B scaled to {, }. b) Afully-connected attention module, where each point repre-sents a time step ( = 6). c) The Step-T attention module,where each past time point attends to itself and the latesttime point attends to all past time points. d) An attentionmodule with sparse Query (Q), Key (K), and Value (V) activa-tions. Notably, Meng et al. applied the model to NASA telemetrydatasets and achieved strong accuracy (0.78 F1) in detecting anom-alies. TranAD proposed an adversarial training procedure toexaggerate reconstruction errors in anomalies. Xu et al. achievestate-of-the-art results in detecting anomalies in multivariate timeseries via association discrepancy. Their key finding is that anom-alies have high association with adjacent time points and low asso-ciations with the whole series, accentuating anomalies.Finally, Transformer variations have been proposed for time se-ries forecasting to lower the attention complexity of long sequencetime series , add stochasticity , and incorporatetraditional time series learning methods . Li et al. in-troduce LogSparse attention, which allows each cell to attend onlyto itself and its previous cells with an exponential step size. TheInformer method selects dominant queries to use in the at-tention module based on a sparsity measurement. Pyraformer introduces a pyramidal attention mechanism for long-range timeseries, allowing for linear time and memory complexity. Wu et al. use a Sparse Transformer as a generator in an encoder-decoderarchitecture for time series forecasting, using a discriminator toimprove the prediction.",
  "Compressed Neural Networks": "Pruning unimportant weights from neural networks was first shownto be effective by Lecun et al. . In recent years, deep learninghas scaled the size and computational cost of neural networks. Nat-urally, research has been directed at decreasing size and energyconsumption of deep learning models.The Lottery Ticket Hypothesis showed that randomly ini-tialized neural networks contain sparse subnetworks that, whentrained in isolation, achieve comparable accuracy to a trained densenetwork of the same structure. The implications of this finding",
  "Sparse Binary Transformers for Multivariate Time Series ModelingKDD 23, August 610, 2023, Long Beach, CA, USA": "are that over-parameterized neural networks are no longer neces-sary, and we can prune large models and still maintain the originalaccuracy.Subsequent work found that we do not need to train neuralnetworks at all to find accurate sparse subnetworks; instead, wecan find a high performance subnetwork using the randomly ini-tialized weights . Edge-Popup applied a scoringparameter to learn the importance of each weight, using the straight-through estimator to find a high accuracy mask over randomlyinitialized models. Diffenderfer and Kailkhuram introducedthe Multi-Prize Lottery Ticket Hypothesis, showing that 1) multipleaccurate subnetworks exist within randomly initialized neural net-works, and 2) these subnetworks are robust to quantization, such asbinarization of weights. In this work, we use the Biprop algorithmproposed in to binarize the weights of Transformer models.",
  "Compressed and Efficient Transformers": "Large-scale Transformers such as the BERT (110 million parameters)are a natural candidate for pruning and model compression .Chen et al. first showed that the Lottery Ticket Hypothesis holdsfor BERT Networks, finding accurate subnetworks between 40%and 90% sparsity. Jaszczur et al. proposed scaling Transformersby using sparse variants for all layers in the Transformer. Otherworks have reported similar findings , showing that sparsitycan help scale Transformer models to even larger levels.Other works have proposed modifications for more efficientTransformers aside from pruning . Most research has focusedon improving the O(2) complexity of attention, via methods suchas fixed patterns , learnable patterns , low rank/kernel meth-ods , and downsampling . Various other methods havebeen proposed for compressing BERT networks such as pruningvia post-training mask searches , block pruning , and 8-bitquantization . We refer readers to Tay et al. for details.Despite the various works compressing Transformers, we werenot able to find any research using both pruning and binarization.Utilizing both methods allows for more efficient computation (mea-sured using FLOPs) as well as a significant decrease in storage (dueto binary weights). Additionally, we find that our proposed model isstill a fraction of the size of compressed NLP Transformers modelswhen trained on time series tasks. For instance, TinyBERT con-tains 14.5 million parameters and 1.2 billion FLOPs, compared toour models which contain less than 1.5 million binary parametersand 38 million FLOPs.",
  "METHOD": "Our model consists of a Transformer encoder with several mod-ifications. We base our model off of Zerveas et al. , who proposeusing a common Transformer framework for several time seriesmodeling tasks. To begin, we describe the base architecture of theTransformer as applied to multivariate time series. Subsequently, wedescribe the techniques used for pruning and binarization. Finally,we describe the two changes applied to the attention mechanism.",
  "We denote fully trained Transformers with no pruning and floatingpoint 32 (FP32) weights as Dense Transformers. Let Xt R": "be a model input for time with window size and features.Each input contains feature vectors x R : Xt R =[xtw, xtw+1, ..., xt], ordered in time sequence of size . In classi-fication datasets is predefined at the sample or dataset level. Foranomaly detection and forecasting tasks, we fix to 50 or 200 anduse an overlapping sliding window as inputs.The standard architecture (pre-binarization) projects featuresonto a -dimensional vector space using a linear module with learn-able weights Wp R and bias bp R. We use the stan-dard positional encoder proposed by Vaswani et al. , and werefer readers to the original work for details. For the Dense Trans-former classification models, we use learnable positional encoder. Zerveas et al. propose using batch normalization insteadof layer normalization used in traditional Transformer NLP models.They argue that batch normalization mitigates the effects of outliersin time series data. We found that for classification tasks, batchnormalization performed the best, while in forecasting tasks layernormalization worked better. For anomaly detection tasks we foundthat neither normalization technique was needed.Each Transformer encoder layer consists of a multi-head atten-tion module followed by ReLU layers. The self-attention moduletakes input Zt R and projects it onto a Query (Q), Key (K), andValue (V), each with learnable weights W R and bias b R.Attention is defined as (Q, K, V) = QK V.Queries, keys, and values are projected by the number of heads() to create multi-head attention. The resultant output Zt under-goes a nonlinearity before being passed to the next encoder layer.The Transformer consists of encoder layers followed by a finaldecoder layer. For classification tasks, the decoder outputs classifi-cation labels: Xt R, which are averaged over . For anomalydetection and forecasting, the decoder reconstructs the full input:Xt R.",
  "Sparse Binary Transformer": "Central to our binarization architecture is the Biprop algorithm, which uses randomly initialized floating point weights tofind a binary mask over each layer. Given a neural network withweight matrix W R initialized with a standard method suchas Kaiming Normal , we can express a subnetwork over neuralnetwork (; W) as (; W M), where M {0, 1} is a binarymask and is an elementwise multiplication.To find M, parameter S R is initialized for each correspond-ing W R. S acts as a score assigned to each weight dictating theimportance of the weights contribution to a successful subnetwork.Using backpropagation as well as the straight-through estimator, the algorithm takes pruning rate hyperparameter ,and on the forward pass computes M at layer as",
  "||M||1(2)": "with M being multiplied by for gradient descent (the straight-through estimator is still used for backpropagation). This calcula-tion was originally derived by Rastegari et al. .In our approach we create sparse and binary modules for eachlinear and layer normalization layer. Our model consists of twolinear layers at the top most level: one for projecting the initialinput (embedding in NLP models) and one used for the decoderoutput. Additionally, each encoder layer consists of six linear layers:Q, K, and V projections, the multi-head attention output projection,and two additional layers to complement multi-head attention.",
  "Attention Modifications": "In this section we describe two modifications made to the attentionmodule to reduce its quadratic complexity. Several previous workshave proposed changes to attention in order to lessen this bottle-neck, such as Sparse Transformers , ProbSparse Attention ,and Pyramidal Attention . While each of these works presentquality enhancements to the memory bottleneck of attention, we in-stead seek to evaluate whether simple sparsification approaches canretain the accuracy of the model compared to canonical attention.Our primary motivation for the following attention modificationsare to test whether a compressed Transformer can retain the sameaccuracy as a Dense Transformer. 3.3.1Fixed Q,K, and V Projection Mask. To reduce the computa-tional complexity of the matrix multiplications within the attentionmodule, we apply random fixed masks to the Q, K, and V projections.We hypothesize that we can retain the accuracy of full attentionby using this naive activation pruning approach, which requiresno domain knowledge. We argue that the success of this approachprovides insight into the necessity of full attention computations. Inother words, Transformers are expressive and powerful enough forcertain tasks that we can prune the models in an unsophisticatedway and maintain accuracy. Moreover, many time series datasetsand datasets generated at the edge are often times simplistic enoughthat we can apply this unsophisticated pruning .To apply this pruning, on model initialization we create randommasks with prune rate {0, 1} for each attention module andeach projection Q,K, and V. Attention heads within the same moduleinherit identical Q, K, or V masks. The mask is applied to eachprojection during train and test. In each of our models we set theprune rate of the attention module equal to the prune rate ofthe linear modules ( = ). 3.3.2Step-t Attention Mask. For anomaly detection and single-stepforecasting tasks, the Sparse Binary Transformer (SBT) algorithmrelies on reconstructing or predicting outputs at the current timestep for each feature, despite time steps of data being provided : Step-t Attention Mask Left: For the forecastingtask we mask inputs during training in order to simulateunknown future time points. Right: The Step-T attentionmask used to calculate attention only at the current time-step versus past values. Using this mask rather than settingour Query dimension to one enables us to pass time windowvectors along multiple encoder layers. to the model. Specifically, the SBT model is only interested in inputvector xt R. For anomaly detection, the model reconstructs xtfrom the input, while in forecasting tasks the model masks xt = 0prior to model input, reconstructs the actual values during trainingand inference.In both tasks, vector xt contains the only values necessary forthe model to learn, and our loss function reflects this by only com-puting error for these values. As a result, computing attention foreach other time step adds unnecessary computation. As depictedin , we pass a static mask to the attention module to com-pute attention only at step-T. We additionally exclude attentioncomputation at step-T with itself, forcing the variable to attendto historical time points for prediction. Finally, we add diagonalones to the attention mask at all past time points to add stabilityto training. This masking method allows us to propagate the fullinput sample to multiple attention layers, helping us retain relevanthistorical information for downstream layers that would not bepossible by changing the sizes of Q, K, and V to only model the time step.",
  "EXPERIMENTS": "In this section we detail our experiments for time series classifica-tion, anomaly detection, and forecasting. Common to each learningtask, we normalize each dataset prior to training such that eachfeature dimension has zero mean and unit variance. We use theTransformer Encoder as described in , training each learn-ing task and dataset using the Dense Transformer and the SBT tocompare accuracy. Finally, we run each experiment three timeswith a different weight seed, and present the average result. Forthe SBT model, varying the weight seed shows evidence of therobustness to hyperparameters. Specific modifications to the modelare made for each learning task, which we describe in the followingsections. Additional training and architecture details can be foundin the Appendix.",
  "=0.7598.678.561.385.365.877.9": ": Accuracy of time series classification models on fivedatasets. Results are obtained from . SBT modelsachieve higher accuracy than prior works (excluding theDense Transformer) in each case, except for the JapaneseVowels dataset. Additionally, SBT models achieve accuracywithin 2.7% of the Dense Transformer for each dataset. (204-30,000), number of features (13-200), and window size (30-405). We choose three datasets with the largest test set size (InsectWingbeats, Spoken Arabic Digits, and Face Detection) as well astwo smaller datasets (JapaneseVowels, Heartbeat). Each datasetcontains a set window size except for Insect Wingbeats and JapaneseVowels, which contain a window size up to 30 and 29, respectively.In these datasets, we pad samples with smaller windows to givethem consistent window sizes. The decoder in our classificationarchitecture is a classification head, rather than a full reconstructionof the input as is used in anomaly detection and forecasting tasks.The SBT classification model is trained and tested using the fixedQ,K,V projection mask as described in .3.Results In , we show that SBTs perform as well as, or sim-ilar to, the Dense Transformer for each dataset at = 0.5 and = 0.75. Our models are averaged over three runs with differentweight seeds. When comparing our model to state-of-the-art ap-proaches, we find that the SBT achieves strong results across eachdataset, with the highest reported performance on three out of thefive datasets. Further, the SBT models perform consistently acrossdatasets while models such as Rocket and Fran et al. havelower performance on one or more datasets.Surprisingly, the SBT model achieves stronger average accuracythan the Dense Transformer (80.2% versus 78.8%), indicating that thepruned and binarized Transformer achieves a robust performanceacross datasets. Despite this, Insect Wingbeats and Japanese Vowelsdatasets achieved a slightly lower performance at = 0.5 with amore substantial dropoff at = 0.75, indicating the model may losesome of its power on certain tasks.",
  "SMAPP93.993.785.984.9R100100100100F196.996.892.491.8": ": Anomaly detection results with benign sample win-dows. We evaluate Precision (P), Recall (R), and the F1 scoreusing both manual threshold and POT threshold technique.We find that the single time step prediction window achieveshigh accuracy when each past time-step in is benign. = 200 for SMD and = 50 for SMAP and MSL. These resultsindicate that when given time to stabilize after an anoma-lous event, our SBT framework can detect new anomalieswith high accuracy. We evaluate our results using a manualthreshold (=0.5% for SMD, 1% for others) and the POT auto-matic threshold selector. such as radiation and temperature, while SMD logs computer serverdata such as CPU load and memory usage. The datasets containbenign samples in the training set, while the test set contains la-beled anomalies (either sequences of anomalies or single pointanomalies).Our model takes sliding window data as input and reconstructsdata at xt given previous time points. We use MSE to reconstructeach feature in xt. We use the step-T attention mask as described in. To evaluate our results, we adopt an adjustment strategysimilar to previous works : if any anomaly is detectedwithin a successive abnormal segment of time, we consider allanomalies in this segment to have been detected. The justificationis that detecting any anomaly in a time segment will cause an alertin real-world applications.To flag anomalies, we retrieve reconstruction loss xt and thresh-old , and consider anomalies where xt > . Since our model istrained with benign samples, anomalous samples in the test setshould yield a higher xt. We compute using two methods fromprevious works: A manual threshold and the Peak Over Thresh-old (POT) method . For the manual threshold, we consider pro-portion of the validation set as anomalous. For SMD = 0.5%, andfor MSL and SMAP = 1%. For the POT method, similar to Om-niAnomaly and TranAd , we use the automatic thresholdselector to find . Specifically, given our training and validation setreconstruction losses, we use POT to fit the tail portion of a proba-bility distribution using the generalized Pareto Distribution. POTis advantageous when little information is known about a scenario,such as in datasets with an unknown number of anomalies.",
  "ModelSMDMSLSMAPAvg": "LOF46.761.257.655.2IsolationForest53.666.555.558.5OCSVM56.270.856.361.1DAGMM57.374.668.566.8VAR74.177.964.872.3MMPCACD75.070.081.775.6ITAD79.576.173.976.5Deep-SVDD79.183.669.077.2SBT=0.982.578.570.677.2CL-MPPCA79.180.472.977.5BeatGAN78.187.569.678.4SBT=0.58778.469.878.4SBT=0.7588.079.370.679.3LSTM-VAE82.382.678.181.0OmniAnomaly85.287.786.986.6Anomaly Transformer92.393.696.794.2 : F1 scores of various time series anomaly detectionmodels. We compare our SBT framework with several state-of-the-art algorithms on the anomaly detection task. The ta-ble is ordered by average F1 accuracy across each dataset. Weevaluate our algorithm using the traditional method (differ-ent from ), where each sample can contain anomalousevents in its input window. We use a manual threshold toreport results for the SBT model. Results In we report the unique findings of our single-stepanomaly detection method using Precision, Recall, and F1-scores.Specifically, we find that when only considering inputs with fullybenign examples in window , both the SBT and the Dense Trans-former achieve high accuracy on all three datasets (F1 between 90.6and 100). In other words, we find that our model performance isbest when we filter examples that have an anomalous sequence ordata point in [xtw, xtw+1, ..., xt1]. For SMD, = 200 and forSMAP and MSL = 50. This observation implies that the modelneeds time to stabilize after an anomalous period. Intuitively, if ananomaly occurred recently, new benign observations will have ahigher reconstruction loss as a result of their difference with theanomalous examples in their input window. We argue that this val-idation metric is logical in real-world scenarios, where monitoringof a system after an anomalous period of time is necessary.We additionally report F1-scores compared to state-of-the-arttime series anomaly detection models in . To accuratelycompare our model against existing methods, we use the full testset without filtering out benign inputs with anomalies in the nearpast. SBT results are much more modest, with F1-scores between70 and 88. Despite this, our method still performs stronger thannon-temporal algorithms such as the Isolation Forest, as well asother deep-learning based approaches such as Deep-SVDD andBeatGan.",
  "Forecasting": "We test our method on single-step forecasting using the Step-T at-tention mask. Specifically, using the framework outlined by Zerveaset al. , we train our model by masking the input at the fore-casting time-step . For example, input Xt containing featuresand time-steps [xtw, xtw+1, ..., xt] is passed through the net-work with xt = 0. We then reconstruct this masked input with theTransformer model, using mean squared error between the maskedinputs reconstruction and the actual value. The masking methodsimulates unseen future data points during train time, making itcompatible with the forecasting task during deployment.We test our model on three datasets used in previous works: ECLcontains electricity consumption of 321 clients in Kwh. The datasetis converted to hourly consumption values due to missing data.Weather contains data for twelve hourly climate features for 1,600location in the U.S. ETTm1 (Electricity Transformer Temperature)contains 15-minute interval data including oil temperature andsix additional power load features. Additional training details areavailable in the Appendix.We compare our method against the Informer and thePyra-former trained with single-step forecasting. Both arecurrent state-of-the-art models that have shown robust results com-pared against a variety of forecasting techniques. Importantly, eachmethod is compatible with multivariate time series forecasting asopposed to some research. We note that these models are builtprimarily for long-term time series forecasting (LSTF), which wedo not cover in this work.Results We evaluate results in using MSE and MAE onthe test set of each dataset. Results indicate that the SBT modelachieves accuracy comparable to the Dense architecture in eachdataset at = 0.5. Interestingly, the Weather at ETTm1 SBT modelsachieved better accuracy than the dense model at = 0.5. Bothmodels additionally showed robustness to higher prune rates, withaccuracy dropping off slowly. ECL on the other hand showed somesensitivity to prune rate, with a slight drop off when increasingthe prune rate. We find that datasets with a higher dimensionalityperformed the worst: ECL contains 321 features, while Insect Wing-beats contains 200. Increasing the dimensionality of the model ()mitigated some of these effects, however it was at the cost of model",
  ": Time series predictions on the ETTm1 dataset forthe Pyraformer (top) and Sparse Binary Transformer (bot-tom) . We show 600 predictions across each model for twofeatures (HULL, LUFL)": "size and complexity. Despite this, we find that the SBT model isable to predict the general trend of complex patterns in data, asdepicted in .Compared to state-of-the-art approaches such as the Pyraformerand Informer architectures, our general purpose forecasting ap-proach performs comparably, or slightly worse, on the single-stepforecasting task. Metrics were not substantially different for anyof the models except for the ECL dataset, where Pyraformer waseasily the best model. Comparing the architectures, we find that theSBT model achieves substantially lower computational cost thanboth the Informer and Pyraformer models. For example, on theECL dataset, Pyraformer contains 4.7 million parameters and theInformer 12.7 million parameters (both FP32, while the SBT modelcontains 1.5 million binary parameters.",
  "Architecture": "Each model in our framework consists of 2 encoder layers eachwith a multi-head attention module containing two heads. Thefeedforward dimensionality for each model is 256 with ReLU usedfor nonlinearity. Classification models had the best results usingBatch Normalization layers, similar to , while forecasting mod-els used Layer Normalization typical of other Transformer models.For anomaly detection we did not use Batch or Layer Normalization.For the output of our models, anomaly detection and forecastingrely on a single decoder linear layer which reconstructs the out-put to size (, ), while classification outputs size (, .)and takes the mean of to formulate a final classification predic-tion. Further details are included in the Appendix and the coderepository.",
  "COMPUTATIONAL SAVINGS": "In this section we estimate the computational savings achieved byusing the SBT model. We will begin by introducing the metricsused to estimate computational savings, and will then summarizethe results of these metrics for each model and task. We note that several works (highlighted in ) have pro-posed modifications to the Transformer in order to make attentionmore efficient. In this section, we concentrate on the enhancementsachieved by 1) creating a sparsely connected Transformer withbinary weights, and 2) simplifying the attention module for timeseries specific tasks such as single-step prediction and classifica-tion. We argue that these enhancements are independent of theachievements made by previous works.",
  "Metrics": "FLOPs (Non-zero). In the field of network pruning, FLOPs, or thenumber of multiply-adds, is a commonly used metric to quantify theefficiency of a neural network . The metric computes the numberof floating point operations required for an input to pass through aneural network. We use the ShrinkBench tool to calculate FLOPs, aframework proposed by Blalock et al. to perform standardizedevaluation on pruned neural networks.Our Transformer architecture contains FP32 activations at eachlayer along with binary weights scaled to {, }. As a result, nobinary operations are performed, and our total FLOPs count isa function of prune rate . For example, a linear module with astandard FLOPs count of has a new FLOPs count of ,where . Linear layers outside of attention do not needwindow size added to the matrix multiply because the inputs arepermuted such that batch size is the second dimension of the layerinput. Each equation counts the number of nonzero multiply-addsnecessary for the neural network.",
  "Q,K,V Mask(2)()2(2)": ": Non-zero FLOPs equations for various attention mod-ules. These calculations assume Q, K and V are equal sizedprojections in R, and = /. QV and AV are addition-ally multiplied by . Q-scaling and softmax FLOPs excludedfrom this table. Furthermore, we modify the FLOPs for the attention module toaccount for step-t attention mask and the fixed Q, K, V mask, assummarized in . In the standard attention module whereQ, K and V are equal sized projections, matrix multiply operations(QV, AV) for each head equate to 2, where = /. For step-tattention, we only require computation at the current time step (thelast row in ), while each each of the identities for past timesteps equates to one. AV requires double the computations becauseV contains FP32 activations multiplied by the diagonal in A. For thefixed mask, since Q and K are sparse projections, we only require()2 nonzero computations in the matrix multiply. Since A is adense matrix, we require 2 FLOPs to multiply sparse matrix V.A simplified equation for network FLOPs becomes 2 + (2 +), where is a linear layer, is the number of attention layers,and is the multihead attention FLOPs (details described in). Several FLOP counts are omitted from this equation, which",
  "ETTm112 200 64102.03.315.50.5100.00.12.632.65.9": ": Computational savings for Dense Transformers compared to SBTs. SBT models achieve a substantial reduction in sizeand FLOPs count across all models. We denote parameters in thousands and size and FLOPs in millions, with savings calculatedby dividing the Dense values by the SBT values. we include in our code, including positional encoding, -scaling,and layer and batch norm.Storage Size. We measure the size of each model in total bits. Stan-dard networks rely on weights optimized with the FP32 data type(32 bits). We consider each binarized module in our architectureto contain single bit weights with a single FP32 parameter foreach layer. Anomaly detection and classification datasets contain 14binarized modules, and forecasting contains 18 with the additionalbinarization of the layer normalization. We note that the binarizedquantities are only theoretical as a result of the PyTorch frameworknot supporting the binary data type. Hardware limitations are alsoreported in other works .",
  "Model Size Selection": "Important to our work is tuning the size of each model. We ana-lyze whether we can create a Dense Transformer with a smallernumber of parameters and still retain a performance on par with alarger model. Our motivation for model size selection is two-fold:1) Previous research has found that neural networks need to besufficiently overparameterized to be pruned and retain the sameaccuracy of the dense model and 2) The time series datasets studiedin this paper have a smaller number of dimensions than the visiondatasets studied in most pruning and model compression papers.The effect of model overparameterization is that we need a densemodel with enough initial parameters in order to prune it and stillretain high performance. Theoretical estimates on the number ofrequired parameters are proposed by the Strong Lottery TicketHypothesis and are further explored in other pruning pa-pers . On the other hand, the limited features of some time series datasets (such as Weather with 7 features) leads us to wonderwhether we could simply create a smaller model.To alter the model size, we vary the embedding dimension of the model. To find the ideal size of the model, we start froma small embedding dimension (such as 8 or 16), and increase thevalue in the Dense Transformer until the model performance onthe validation set stops increasing. With this value of , we test theSBT model.Our results show that in each dataset, Dense Transformers witha smaller embedding dimension either a) perform worse than theSBT at the optimized size, b) contain more parameters (as measuredin total bits), c) have more FLOPs, or d) some combination of theabove. In almost every dataset, the smaller Dense Transformerperforms worse than the SBT while also requiring more size andFLOPs. The exception to this was Spoken Arabic Digits, wherethe smaller Dense Transformers ( = 16 and = 32) performedslightly better than the SBT with = 64. Additionally, these modelshad a lower FLOPs count. The advantage of the SBT model in thisscenario was a substantially lower storage cost than both smallerDense models. Even if both Dense Transformer models were ableto be quantized to 8-bit weights, the storage of the SBT would stillbe many times lower. The ETTm1 dataset additionally had highperformance Dense Transformers with a smaller size ( = 16, =32). However, both models were substantially more costly in termsof storage and additionally had a higher FLOPs count. Detailedresults are provided in the Appendix.",
  "Analysis": "Results in highlight the large computational savings achievedby SBT. We find that layer pruning reduces FLOPs count (due tothe added nonzero computations), while binarization helps withthe storage size.Notably, all models have a FLOPs count at least two times lessthan the original Dense model. FLOPs are dramatically reducedin the anomaly detection and forecasting datasets, largely due tothe step-t masking. Classification datasets have a dense attentionmatrix, leading to a smaller FLOPs reduction due to the softmaxoperation and the calculation (where is sparse). We note thatusing a higher prune rate can reduce the FLOPs more, howeverwe include results at 50% prune rate for classification since thesemodels achieved slightly better accuracy.We highlight the storage savings of SBT models by measuring bitsize and parameter count. summarizes the substantial reduc-tion in bit size for every model, with only two SBT models havinga bit size greater than 1 million (Insect Wingbeats and ECL). Thetwo models with a larger size also had the highest dimensionality, and consequently .We note that SBT models contain a small number of FP32 valuesdue to the single parameter in each module. Additionally, weforego a learnable encoding layer in SBT classification models,leading to a smaller overall count. Finally, no bias term is added tothe SBT modules, leading to a smaller number of overall parameters.Compared to other efficient models, our model generally has alower FLOPs count. For example, MobileV2 has 16.4 millionFLOPs when modeling CIFAR10, while EfficientNetV2 has 18.1million parameters.",
  "Applications": "SBTs retain high performance compared to dense models, coupledwith a large reduction in computational cost. As a result, SBTs havethe potential to impact a variety of new domains. For example, sen-sors and small embedded systems such as IoT devices could employSBTs for intelligent and data-driven decisions, such as detecting amalicious actor or forecasting a weather event. Such devices couldbe extended into new areas of research such as environmentalmonitoring. Other small capacity applications include implantabledevices, healthcare monitoring, and various industrial applications.Finally, lightweight deep learning models can also benefit largerendeavors. For example, space and satellite applications, such as inthe MSL and SMAP telemetry datasets, collect massive amounts ofdata that is difficult to monitor. Employing effective and intelligentalgorithms such as the Transformer could help in the processingand auditing of such systems.",
  "Limitations and Future Work": "Although SBTs theoretically reduce computational costs, the methodis not optimized for modern libraries and hardware. Python librariesdo not binarize weights to single bits, but 8-bit counts. Special hard-ware in IoT devices and satellites could additionally make implemen-tation a burden. Additionally, while our implementation shows thatsparse binarized Transformers exist, the Biprop algorithm requiresbackpropagation over a dense network with randomly initializedFP32 weights. Hence, finding accurate binary subnetworks requiresmore computational power during training than it does duringdeployment. This may be a key limitation in devices seeking au-tonomy. In addition to addressing these limitations, a logical stepfor future work would be to implement SBTs in state-of-the-artTransformer models such as the Pyramformer for forecasting andthe Anomaly Transformer for time series anomaly detection.SBTs have the potential to enable widespread use of AI acrossnew applications. The Transformer stands as one of most powerfuldeep learning models in use today, and expanding this architectureinto new domains provides promising directions for the future.",
  "This work was supported in part by funding from NSF under AwardNumbers ATD 2123761, CNS 1822118, NIST, ARL, Statnett, AMI,NewPush, and Cyber Risk Research": "A. Bagnall, J. Lines, A. Bostrom, J. Large, and E. Keogh. 2017. The Great TimeSeries Classification Bake Off: a Review and Experimental Evaluation of RecentAlgorithmic Advances. Data Mining and Knowledge Discovery 31 (2017), 606660.Issue 3. Sriram Baireddy, Sundip R Desai, James L Mathieson, Richard H Foster, Moses WChan, Mary L Comer, and Edward J Delp. 2021. Spacecraft time-series anomalydetection using transfer learning. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition. 19511960.",
  "Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag.2020. What is the state of neural network pruning? Proceedings of machinelearning and systems 2 (2020), 129146": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Ka-plan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, TomHenighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad-ford, Ilya Sutskever, and Dario Amodei. 2020.Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, Vol. 33.Curran Associates, Inc., 18771901. Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, ZhenhuaLiu, Siwei Ma, Chunjing Xu, Chao Xu, and Wen Gao. 2021. Pre-Trained ImageProcessing Transformer. In 2021 IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR). IEEE, Nashville, TN, USA, 1229412305. Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, ZhangyangWang, and Michael Carbin. 2020. The lottery ticket hypothesis for pre-trainedbert networks. Advances in neural information processing systems 33 (2020),1583415846. Daiki Chijiwa, Shin ya Yamaguchi, Yasutoshi Ida, Kenji Umakoshi, and TomohiroINOUE. 2021. Pruning Randomly Initialized Neural Networks with IterativeRandomization. In Advances in Neural Information Processing Systems, Vol. 34.",
  "Andrew A Cook, Gksel Msrl, and Zhong Fan. 2019. Anomaly detection for IoTtime-series data: A survey. IEEE Internet of Things Journal 7, 7 (2019), 64816494": "Angus Dempster, Franois Petitjean, and Geoffrey I Webb. 2020. ROCKET: excep-tionally fast and accurate time series classification using random convolutionalkernels. Data Mining and Knowledge Discovery 34, 5 (2020), 14541495. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Im-ageNet: A large-scale hierarchical image database. In 2009 IEEE Conference onComputer Vision and Pattern Recognition. 248255. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding. InProceedings of the 2019 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies, Volume 1 (Long andShort Papers). Association for Computational Linguistics, Minneapolis, Minnesota,41714186. James Diffenderfer and Bhavya Kailkhura. 2021. Multi-Prize Lottery TicketHypothesis: Finding Accurate Binary Neural Networks by Pruning A RandomlyWeighted Network. In International Conference on Learning Representations.",
  "Jonathan Frankle and Michael Carbin. 2019. The Lottery Ticket Hypothesis:Finding Sparse, Trainable Neural Networks. (March 2019). arXiv: 1803.03635": "Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan, Yin Yang, HassanSajjad, Preslav Nakov, Deming Chen, and Marianne Winslett. 2021. Compressinglarge-scale transformer-based models: A case study on bert. Transactions of theAssociation for Computational Linguistics 9 (2021), 10611080. Matt Gorbett, Hossein Shirazi, and Indrakshi Ray. 2022. Local Intrinsic Dimen-sionality of IoT Networks for Unsupervised Intrusion Detection. In Data andApplications Security and Privacy XXXVI: 36th Annual IFIP WG 11.3 Conference,DBSec 2022, Newark, NJ, USA, July 1820, 2022, Proceedings. Springer, 143161.",
  "Song Han, Jeff Pool, John Tran, and William J. Dally. 2015. Learning both Weightsand Connections for Efficient Neural Networks. (Oct. 2015). arXiv: 1506.02626": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Delving deepinto rectifiers: Surpassing human-level performance on imagenet classification.In Proceedings of the IEEE international conference on computer vision. 10261034. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep ResidualLearning for Image Recognition. In 2016 IEEE Conference on Computer Visionand Pattern Recognition (CVPR). IEEE, Las Vegas, NV, USA, 770778. Kyle Hundman, Valentino Constantinou, Christopher Laporte, Ian Colwell, andTom Soderstrom. 2018. Detecting spacecraft anomalies using lstms and nonpara-metric dynamic thresholding. In Proceedings of the 24th ACM SIGKDD interna-tional conference on knowledge discovery & data mining. 387395. Sebastian Jaszczur, Aakanksha Chowdhery, Afroz Mohiuddin, Lukasz Kaiser,Wojciech Gajewski, Henryk Michalewski, and Jonni Kanerva. 2021. Sparseis enough in scaling transformers. Advances in Neural Information ProcessingSystems 34 (2021), 98959907. Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, FangWang, and Qun Liu. 2020. TinyBERT: Distilling BERT for Natural LanguageUnderstanding. In Findings of the Association for Computational Linguistics:EMNLP 2020. Association for Computational Linguistics, Online, 41634174.",
  "Nikita Kitaev, ukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficienttransformer. arXiv preprint arXiv:2001.04451 (2020)": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. ImageNet Clas-sification with Deep Convolutional Neural Networks. In Advances in NeuralInformation Processing Systems, Vol. 25. Curran Associates, Inc. Woosuk Kwon, Sehoon Kim, Michael W. Mahoney, Joseph Hassoun, KurtKeutzer, and Amir Gholami. 2022. A Fast Post-Training Pruning Frameworkfor Transformers. In Advances in Neural Information Processing Systems, Al-ice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (Eds.).",
  "Yann LeCun, John Denker, and Sara Solla. 1989. Optimal Brain Damage. InAdvances in Neural Information Processing Systems, Vol. 2. Morgan-Kaufmann": "Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat,Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020. Gshard:Scaling giant models with conditional computation and automatic sharding. arXivpreprint arXiv:2006.16668 (2020). Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang,and Xifeng Yan. 2019. Enhancing the locality and breaking the memory bottle-neck of transformer on time series forecasting. Advances in neural informationprocessing systems 32 (2019). Minghao Liu, Shengqi Ren, Siyuan Ma, Jiahui Jiao, Yizhou Chen, Zhiguang Wang,and Wei Song. 2021. Gated transformer networks for multivariate time seriesclassification. arXiv preprint arXiv:2103.14438 (2021). Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, andSchahram Dustdar. 2021. Pyraformer: Low-complexity pyramidal attention forlong-range time series modeling and forecasting. In International Conference onLearning Representations. Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, and Ohad Shamir. 2020. Prov-ing the Lottery Ticket Hypothesis: Pruning is All You Need. In Proceedings ofthe 37th International Conference on Machine Learning. PMLR, 66826691. ISSN:2640-3498. Pankaj Malhotra, Anusha Ramakrishnan, Gaurangi Anand, Lovekesh Vig, PuneetAgarwal, and Gautam Shroff. 2016. LSTM-based encoder-decoder for multi-sensoranomaly detection. arXiv preprint arXiv:1607.00148 (2016). Hengyu Meng, Yuxuan Zhang, Yuanxiang Li, and Honghua Zhao. 2019. Space-craft anomaly detection via transformer reconstruction error. In InternationalConference on Aerospace System Science and Engineering. Springer, 351362.",
  "Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih, Sinong Wang, and JieTang. 2019. Blockwise self-attention for long document understanding. arXivpreprint arXiv:1911.02972 (2019)": "Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, andMohammad Rastegari. 2020. Whats Hidden in a Randomly Weighted NeuralNetwork?. In Computer Vision and Pattern Recognition (CVPR). Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. 2016.Xnor-net: Imagenet classification using binary convolutional neural networks. InComputer VisionECCV 2016: 14th European Conference, Amsterdam, The Nether-lands, October 1114, 2016, Proceedings, Part IV. Springer, 525542.",
  "Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2022. EfficientTransformers: A Survey. ACM Comput. Surv. (apr 2022). Just Accepted": "Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, AlexandreSablayrolles, and Herve Jegou. 2021. Training data-efficient image transformers& distillation through attention. In Proceedings of the 38th International Conferenceon Machine Learning. PMLR, 1034710357. ISSN: 2640-3498. Shreshth Tuli, Giuliano Casale, and Nicholas R. Jennings. 2022. TranAD: deeptransformer networks for anomaly detection in multivariate time series data.Proceedings of the VLDB Endowment 15, 6 (Feb. 2022), 12011214. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017.Attention isAll you Need. In Advances in Neural Information Processing Systems, Vol. 30.Curran Associates, Inc.",
  "Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Lin-former: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768(2020)": "Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan,and Liang Sun. 2022. Transformers in Time Series: A Survey. (March 2022). Number: arXiv:2202.07125 arXiv:2202.07125 [cs,eess, stat]. Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer: De-composition transformers with auto-correlation for long-term series forecasting.Advances in Neural Information Processing Systems 34 (2021), 2241922430. Sifan Wu, Xi Xiao, Qianggang Ding, Peilin Zhao, Ying Wei, and Junzhou Huang.2020. Adversarial sparse transformer for time series forecasting. Advances inneural information processing systems 33 (2020), 1710517115.",
  "Tien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze. 2017. Designing Energy-EfficientConvolutional Neural Networks using Energy-Aware Pruning. (April 2017). arXiv: 1611.05128": "Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019. Q8bert:Quantized 8bit bert. In 2019 Fifth Workshop on Energy Efficient Machine Learningand Cognitive Computing-NeurIPS Edition (EMC2-NIPS). IEEE, 3639. George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty,and Carsten Eickhoff. 2021. A Transformer-Based Framework for MultivariateTime Series Representation Learning. In Proceedings of the 27th ACM SIGKDDConference on Knowledge Discovery & Data Mining (Virtual Event, Singapore)(KDD 21). Association for Computing Machinery, New York, NY, USA, 21142124. Hang Zhang, Yeyun Gong, Yelong Shen, Weisheng Li, Jiancheng Lv, Nan Duan,and Weizhu Chen. 2021. Poolingformer: Long document modeling with poolingattention. In International Conference on Machine Learning. PMLR, 1243712446. Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,and Wancai Zhang. 2021. Informer: Beyond efficient transformer for long se-quence time-series forecasting. In Proceedings of the AAAI Conference on ArtificialIntelligence, Vol. 35. 1110611115. Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin.2022. FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting. In Proceedings of the 39th International Conference onMachine Learning (Proceedings of Machine Learning Research, Vol. 162), KamalikaChaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and SivanSabato (Eds.). PMLR, 2726827286.",
  "SUPPLEMENTAL MATERIALSAABLATION STUDIES": "We conduct two ablation studies testing the effects of removing theindividual pruning mechanisms from the attention computation.We note that the attention pruning methods complement Biprop Biprop mainly reduces the model size, whereas attention pruningdoes a better job at reducing the FLOPs. Each ablation experimentis averaged over three experimental runs with different seeds. highlights the effects of removing random pruning fromthe time series classification models. Notably, Biprop plus random pruning performs comparably to, or better than, Biprop on its own.Adding random pruning even outperforms using only Biprop withthe Japanese Vowels dataset. highlights the results of attention variations for bothanomaly detection and forecasting tasks. Specifically, we look at ourproposed approach (Biprop+Step-T Mask), Biprop plus an identitymatrix mask in the attention layers, and finally Biprop only. Wereport results using mean squared error (MSE) loss averaged overthree runs.Results show that Biprop plus the Step-T mask performs com-parably to using Biprop only. For anomaly detection tasks, theMSE is even lower compared to just using Biprop. Comparing bothmethods to the Biprop plus the identity matrix attention mask, wecan see a significant difference in the results: the identity matrixattention mask attains a higher loss in each case.",
  "ETTm10.0590.0680.070ECL0.1980.2040.182Weather0.1660.1800.166": ": We compare Biprop plus the Step-T attention maskwith two other methods. We find that Biprop with the Step-Tmask performs similarly to using Biprop with full attention(Biprop Only). Biprop with an Identity Mask on the attentioncomputation performs worse than the other two methods.We report results using MSE loss averaged across three runs.",
  "BTRAINING DETAILS": "Each model is trained with Adam optimization with a learningrate of 1e-3 except for InsectWingbeats, where we use a learningrate of 1e-4. For Dense Transformer classification models we use alearnable positional encoding, while in all other models we use astandard positional encoding.We found that SBT models sometimes take slightly longer to con-verge, hence we train the models for more epochs in the forecastingand classification tasks. These numbers are specified in the config-uration files in the code repository. Batch Normalization is used",
  "CANALYSISC.1Attention Magnitude Pruning versusRandom Pruning": "As apart of our attention pruning analysis, we also applied magni-tude pruning to the attention layers. However, this method requiresextra computation as a result of the sorting required to take thetop activations for each input. Below we compare the results ofmagnitude pruning versus random pruning, finding that randompruning achieves similar accuracy to magnitude pruning at a lowercomputational cost.",
  "EMODEL SIZE SELECTION": "We measure model performance as well as computational cost atvarying sizes for each model. To vary the size, we increase theembedding dimension for each model and dataset combination.Tables 8 and 9 show the results for each model size and datasetcombination. Overall, we find that the SBT generally performsbetter than the smaller Dense Transformer in terms of performance,except in a few cases. In all scenarios, the SBT model has at leastone computational advantage in terms of storage size or FLOPscount.Additionally we find that, common with our intuition, datasetswith a higher dimensionality need a higher embedding dimension,while simpler datasets are successful with a smaller embedding di-mension. For example, Insect Wingbeats ( = 200), Face Detection( = 144), and ECL ( = 321) require 128 to achieve optimalperformance."
}