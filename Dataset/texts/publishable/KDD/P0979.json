{
  "ABSTRACT": "Graph Neural Architecture Search (GNAS) has achieved superiorperformance on various graph-structured tasks. However, exist-ing GNAS studies overlook the applications of GNAS in resource-constrained scenarios. This paper proposes to design a joint graphdata and architecture mechanism, which identifies important sub-architectures via the valuable graph data. To search for optimallightweight Graph Neural Networks (GNNs), we propose a Light-weight Graph Neural Architecture Search with Curriculum GraphSparsIfication and Network Pruning (GASSIP) approach. In partic-ular, GASSIP comprises an operation-pruned architecture searchmodule to enable efficient lightweight GNN search. Meanwhile, wedesign a novel curriculum graph data sparsification module with anarchitecture-aware edge-removing difficulty measurement to helpselect optimal sub-architectures. With the aid of two differentiablemasks, we iteratively optimize these two modules to search forthe optimal lightweight architecture efficiently. Extensive experi-ments on five benchmarks demonstrate the effectiveness of GASSIP.Particularly, our method achieves on-par or even higher node clas-sification performance with half or fewer model parameters ofsearched GNNs and a sparser graph.",
  "Computing methodologies Semi-supervised learningsettings; Neural networks": "The two first authors made equal contributions.Corresponding authors. DCST is the abbreviation of Department of Computer Scienceand Technology. BNRist is the abbreviation of Beijing National Research Center forInformation Science and Technology. Permission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0490-1/24/08.",
  "Graph Neural Network, Graph Sparsification, Neural ArchitectureSearch": "ACM Reference Format:Beini Xie, Heng Chang, Ziwei Zhang, Zeyang Zhang, Simin Wu, Xin Wang,Yuan Meng, and Wenwu Zhu. 2024. Towards Lightweight Graph NeuralNetwork Search with Curriculum Graph Sparsification. In Proceedings ofthe 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining(KDD 24), August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA,11 pages.",
  "INTRODUCTION": "Graph data is ubiquitous in our daily life ranging from social net-works and protein interactions to transportation and transaction networks . Graph Neural Networks (GNNs)are effective for their ability to model graphs in various down-stream tasks such as node classification, link prediction, graphclustering, and graph classification . In order toutilize the graph structure, many GNNs like GCN , GAT ,and GraphSAGE build the neural architecture following themessage-passing paradigm : nodes receive and aggregate mes-sages from neighbors and then update their own representations.However, facing diverse graph data and downstream tasks, themanual design of GNNs is laborious. Graph Neural ArchitectureSearch (GNAS) tackles this problem and bearsfruit for automating the design of high-performance GNNs.Compared to traditional GNAS, lightweight GNAS offers a widerrange of application scenarios by reducing computing resourcerequirements. Recently, how to modify graph data to unlock the po-tential of various graph models is a crucial question in data-centricgraph learning . This is especially essential for lightweightGNNs due to the trade-off between model capacity and data vol-ume while existing studies have overlooked the significance of thisneed. Despite concerns about the accuracy of lightweight models,the lottery ticket hypothesis suggests that sub-networks,which remove unimportant parts of the neural network, can achieve",
  ": Overlaps of removed edges for GCN, GAT, GIN, and Random under diffident graph data sparsification": "comparable performance to the full network. Therefore, the coreobjective of lightweight GNAS is to efficiently search for the ef-fective sub-architecture corresponding to the high-performancesub-networks. As a result, to achieve the objective of lightweightGNAS, we need to address the two challenges: (1) How to effectively search for GNN sub-architectures?(2) How to efficiently conduct lightweight GNAS?Regarding the issue of effectiveness, graph neural sub-architecturesremain black-box due to neural network complexity .This black-box issue becomes even more significant when graphsparsification is taken into account, which is demonstrated by thefollowing observation:Observation: Different architectures have their views of redundantinformation.For illustration, as shown in , we first train structure masksfor three manual-designed GNNs: GCN, GAT, and GIN. Then, weremove edges with the lowest mask scores, and the remove ratio iscontrolled by a sparsity parameter %. We calculate the overlaps inthe removed edges(M1, M2) = (AM1)(AM2) %| E|, where M1, M2 aretrained binarized structure masks under different manual-designedGNNs. We also add a baseline Random which randomly removes %edges to demonstrate the low similarities between various masks.We could see that the removal edges are diverse especially when thenumber of removal edges is small, which indicates the difference inarchitectures judgment for the structure redundancy. This obser-vation indicates the optimization of GNN architecture is essentialfor a graph with a given sparsity. By leveraging the informationprovided by the sparse graph, we can identify the correspondingsub-architectures.Regarding the issue of efficiency, directly realizing the light-weight GNAS goal with a first-search-then-prune pipeline wouldsuffer from large computational costs since it needs two GNN train-ing sessions and is therefore undesirable. Instead, joint optimizationby viewing the search and pruning stages as a whole would simplifythe optimization process and ease the burden of computation.In this paper, we propose Lightweight Graph Neural ArchitectureSearch with Curriculum Graph SparsIfication and Network Pruning(GASSIP) based on the following intuition: the underlying assump-tion of graph sparsification is the existence of a sparse graph thatcan preserve the accuracy of the full graph for given tasks [10, 32, 37, 61]. Therefore, it is reasonable to infer that the effectivesub-architecture plays a crucial role in processing the informativesparse graph. As shown in , GASSIP performs iterative dataand architecture optimization through two components: operation-pruned architecture search and curriculum graph data sparsifica-tion. The former component helps to construct lightweight GNNswith fewer parameters and the latter one helps to search for moreeffective lightweight GNNs. In particular, we conduct operationpruning with a differentiable operation weight mask to enablethe identification of important parts of the architecture in theoperation-pruned architecture search. Meanwhile, in the curricu-lum graph data sparsification, we use a differentiable graph struc-ture mask to identify useful edges in graphs and further help searchfor optimal sub-architectures. To conduct a proper judgment ofuseful/redundant graph data information, we exploit curriculumlearning with an edge-removing difficulty estimatorand sample(nodes) reweighting to learn graph structure better.Meanwhile, our designed joint search and pruning mechanismhas comparable accuracy and is far more efficient compared withthe first-search-then-pruning pipeline, as shown in experiments.The graph data and operation-pruned architectures are iterativelyoptimized. Finally, GASSIP generates the optimal sub-architectureand a sparsified graph.Our contributions are summarized as follows:",
  "We propose an operation-pruned efficient architecture searchmethod for lightweight GNNs": "To recognize the redundant parts of graph data and further helpidentify effective sub-architectures, we design a novel curriculumgraph data sparsification algorithm by an architecture-awareedge-removing difficulty measurement. We propose an iterative optimization strategy for operation-pruned architecture search and curriculum graph data sparsi-fication, while the graph data sparsification process assists thesub-architecture searching. Extensive experiments on five datasets show that our methodoutperforms vanilla GNNs and GNAS baselines with half or evenfewer parameters. For example, on the Cora dataset, we improvevanilla GNNs by 2.42% and improve GNAS baselines by 2.11%;the search cost is reduced from 16 minutes for the first-search-then-prune pipeline to within one minute.",
  "= +": ": The iterative training framework of GASSIP. The graph data and architecture parameters are iteratively optimized.The operation-pruned architecture search first receives the current learned graph structure and then interactively performssupernet training and operation pruning. For the curriculum graph data sparsification, it estimates edge-removing difficultyfrom node- and architecture-view and updates the graph structure via architecture sampling and sample reweighting.",
  "RELATED WORK2.1Graph Neural Architecture Search": "The research of Graph Neural Architecture Search (GNAS) hasflourished in recent years for automating the GNN architecturedesign . We refer the readers to the GNAS sur-vey for details. GraphNAS is the first attempt to build theGNN search space and utilizes reinforcement learning to find theoptimal architecture. For a more efficient search, many works adopt the differentiable architecture search algorithm. Ona continuous relaxation of the search space, all candidate opera-tions are mixed via architecture parameters, which are updatedwith operation parameters. Considering the certain noises in graphdata, GASSO conducts a joint optimization for architectureand graph structure. All previous works only focus on searchingfor high-performance architectures but overlook searching for alightweight GNN. As far as we know, the most related work toours is ALGNN . ALGNN searches for lightweight GNNs withmulti-objective optimization, but it neglects the vital role of thegraph structure, which is important not only for graph represen-tation learning but also for guiding the graph neural architecturesearch. Aside from the GNAS literature, Yan et al. also pro-posed HM-NAS to improve the architecture search performanceby loosening the hand-designed heuristics constraint with threehierarchical masks on operations, edges, and network weights. Incontrast, our focus is different from HM-NAS as we aim to searchfor a lightweight GNN considering co-optimizing the graph struc-ture. To achieve this goal, we design a novel lightweight graphneural architecture search algorithm that exploits graph data toselect optimal lightweight GNNs with a mask on network weights.",
  "Graph Data Sparsification": "Graph data sparsification is to sparsify the graph structure whichremoves several edges, maintains the information needed for down-stream tasks, and allows efficient computations . Some meth-ods rebuild the graph structure through similarity-related kernelsbased on node embeddings. For example, GNN-Guard exploitscosine similarity to measure edge weights. Additionally, some algo-rithms leverage neural networks to produce intermediategraph structures and then use discrete sampling to refine the graphstructure. Furthermore, the direct learning algorithm takes the edge weights as parameters by learning a structure maskand removing lower-weight edges. In this paper, we perform graphdata sparsification through graph structure learning using the toolsfrom curriculum learning and jointly conduct the architecturesearch.",
  "Lightweight Graph Neural Networks": "The key to building lightweight neural networks is reducing themodel parameters and complexity, which further enables neural net-works to be deployed to mobile terminals. GNN computation accel-eration poses a faster computation for GNNs. Knowledgedistillation follows a teacher-student learning paradigmand transfers knowledge from resource-intensive teacher models toresource-efficient students but keeps performance. Network prun-ing enables more zero elements in weight matrices. As a result,pruned networks have quicker forward passes for not requiringmany floating-point multiplications. For example, leveragethe iterative magnitude-based pruning and uses the gradualmagnitude pruning to prune model weights.",
  "PRELIMINARIES": "Let G = (A, X) denotes one graph with nodes V = {V, V },where V is the labeled node set and V is the unlabeled node set,A R represents the adjacency matrix (the graph structure)and X R 0 represents the input node features. E is the edgeset in G.For a node classification task with classes, given a GNN , itupgrades the node representations through feature transformation,message propagation, and message aggregation, and outputs nodepredictions Z R :",
  "s. t. W() = arg minWL(W, ),(2)": "where is the architecture parameter indicating the GNN architec-ture, and W is the learnable weight parameters for all candidateoperations. W() is the best weight for current architecture based on the training set and is the best architecture accordingto validation set.Here, we resort to the Differentiable Neural Architecture Search(DARTS) algorithm to conduct an efficient search. Consideringthe discrete nature of architectures, DARTS adopts continuous re-laxation of the architecture representation and enables an efficientsearch process. In particular, DARTS builds the search space withthe directed acyclic graph (DAG) (shown as supernet in )and each directed edge (, ) is related to a mixed operation based on",
  "the continuous relaxation (,) (x) = Oexp ( (,)) O exp ( (,)) (,) (x),": "where x is the input of node in DAG, O stands for the candidateoperation set (e.g., message-passing layers), and is the learnablearchitecture parameter. In the searching phase, weight and archi-tecture parameters are iteratively optimized based on the gradientdescent algorithm. In the evaluation phase, the best GNN architec-ture is induced from mixed operations for each edge in DAG, andthe optimal GNN is trained for final evaluation.Nonetheless, the problem Eq.2 does not produce lightweightGNNs. Next, we introduce the lightweight graph neural architecturesearch problem and our proposed method.",
  "Problem Formulation": "Here, we introduce two learnable differentiable masks M, M forthe graph structure A and operation weights W in the supernet. Thevalue of the operation weight mask indicates the importance level ofoperation weights in the architecture and therefore helps to selectimportant parts in GNN architectures. The trained graph structuremask could identify useful edges and remove redundant ones andthus helps to select important architectures while searching.The goal of GASSIP could be formulated as the following opti-mization problem:",
  "(3)": "where denotes the element-wise product operation, M indicatesthe best structure mask based on the current supernet and thestructure loss function L, W and M are optimal for andcurrent input graph structure A M. The target of GASSIP isto find the best discrete architecture according to the architectureparameters , obtain the sparsified graph based on the structuremask M and get the pruned network from the weight mask M .In practice, we use sparse matrix-based implementation, whichmeans that M is a |E|-dimensional vector.",
  "Operation-pruned Architecture Search": "We leverage network pruning, which reduces the number of trainedparameters, to build lightweight GNNs. In contrast with directlybuilding smaller GNNs with fewer hidden channels, building GNNswith reasonable hidden channels and then performing pruningcould realize the lightweight goal without compromising accuracy.In GASSIP, we prune the operation in the supernet while searchingand name it the operation-pruned architecture search. Specifically,we co-optimize candidate operation weights W and their learnableweight mask M = (S ) in the searching phase, where S is atrainable parameter and is a sigmoid function which restricts themask score between 0 and 1. The differentiable operation weightmask helps to identify important weights in operations.",
  "Curriculum Graph Data Sparsification": "Effective sub-architectures could better utilize useful graph infor-mation to compete with full architectures. Useful graph data couldhelp to select the most important parts of the GNN architecturewhile unsuitable removal of the graph data may mislead the sub-architecture searching process. Here, we exploit graph structurelearning to help search for optimal sub-architectures. Besides, weconduct a further graph sparsification step which removes redun-dant edges after the whole training procedure. The calculation ofmessage-passing layers includes the edge-level message propaga-tion, in which all nodes receive information from their neighborswith |E| complexity. A sparser graph, compared to a dense graph,has less inference cost because of the decrease in edge-wise messagepropagation. Hence, eliminating several edges in graph data helpsto reduce the model complexity and boosts the model inferenceefficiency.",
  "Towards Lightweight Graph Neural Network Search with Curriculum Graph SparsificationKDD 24, August 2529, 2024, Barcelona, Spain": "In this section, we answer the first question in the Sec. 1 and pro-pose our curriculum graph data sparsification algorithm to guidethe lightweight graph neural architecture search in a positive way.A successful graph sparsification could recognize and remove redun-dant edges in the graph structure. For GNNs, it is natural to identifystructure redundancy as edges with low mask scores. However,for GNAS, plenty of architectures are contained in one supernetwhile different architectures have their own views of redundantinformation, which is illustrated by observation in Sec. 1.Structure Redundancy Estimation. In order to estimate thegraph structure redundancy, we exploit structure learning and for-mulate the graph structure mask M with the sigmoid function:M = (S ),(4) where S R is a learnable mask score parameter and isa learnable mask threshold parameter which helps to control thegraph data sparsity. The number of non-zero elements in S equals|E|. The sigmoid function restricts the graph structure mask scoreinto (0, 1). Smaller structure mask scores indicate that correspond-ing edges are more likely to be redundant. The structure mask isdifferentiable and updated through the calculated gradient of theloss function L.Intuitively, if an edge is redundant, it would be regarded as aredundant one no matter what the architecture is. If the updatedgradients are consistent under several architectures, we have moreconfidence to update the structure mask score. Considering the Ob-servation, we propose to leverage backward gradients on differentarchitectures to formulate the structure mask update confidence. Inparticular, we first sample top- architectures {1,2, ..., } fromthe supernet according to the product of the candidate operationprobability in each layer:",
  ", .(7)": "Curriculum Design. Some redundant edges are easier to recognizethan others. For example, if several architectures have different judg-ments of one edges redundancy, it is hard to decide whether thisedge should be removed or not. For GNAS, false structure removalin the early stage of searching may misguide the search process.As a result, we introduce curriculum learning into the graph spar-sification process based on the architecture-aware edge-removingdifficulty measurement and the sample re-weighting strategy. Ourmethod belongs to a more general definition of curriculum learningin which we schedule the training process by softly reweightingand selecting sample nodes rather than directly controlling thenode difficulty . Specifically, we evaluate the architecture-aware edge-removingdifficulty from two views: the architecture view and the node view.From the architecture view, if several architectures have disparatejudgments of mask update, the corresponding edge moving shouldbe more difficult. For edge between node and node , the edge-removing difficulty under the architecture-view is defined as",
  "D() = std(S, ),(8)": "where std indicates the standard deviation. It is worth mentioningthat D() has already been calculated in the structure redun-dancy estimation step, which could be saved in memory withoutrepeating the calculation.From the node view, edges that link similar nodes are harder toremove and nodes with a lower information-to-noise ratio havemore difficult edges. Here, we measure the information-to-noise ra-tio with label divergence. Therefore, the node-view edge-removingdifficulty is evaluated as:",
  "|N |,(9)": "where 1 is a hyper-parameter balancing the node-view difficulty,N denotes neighbors of node . () is the 0-1 indicator functionand represents the cosine similarity function. z stands forthe final representation of node calculated in the architectureparameter training phase. represents the predicted label and is the pseudo-label assigned based on predictions for the output z:",
  "|N |,(12)": "where 2 is a hyper-parameter. In this way, the node difficulty isdefined as the average edge-removing difficulty for all its neighbors.Following the idea of Hard Example Mining , we regard dif-ficult edges are more informative and need to be weighted morein training. We assign nodes with higher node/edge-removing dif-ficulty and higher sample weights. The node weight is calculatedas = softmax(D()), V(13)",
  "VL ((A M, X), ) + L (M), (14)": "where L is the classification loss based on the assigned pseudo-labels. L is the mean entropy of each non-zero element in M,which forces the mask score to be close to 0 or 1. is a hyper-parameter balancing the classification and entropy loss.The overall curriculum graph data sparsification algorithm issummarized in Algorithm 1. In line 1, pseudo-labels are assigned",
  "An Iterative Optimization Approach": "In this section, we introduce the solution to the second question inthe introduction and solve the optimization problem in Eq. 3 in aniterative manner.Since the informative continuous graph structure helps to selectproper operations from the search space while redundant graphdata (e.g., noise edges) will deteriorate the architecture search re-sult, we iteratively perform graph sparsification and architecturesearch optimization. Using the valuable graph data, we pinpointkey components of the GNN for both operations and weights. Fur-thermore, the introduction of two trainable masks in Eq. 3 enablesus to efficiently select useful graph structures and essential partsof the architecture. Fully differentiable parameters, according toDARTS algorithms, can cut the search time of lightweight GNNsfrom several hours to minutes (shown in Sec. 5.3).Training Procedure. We summarize the whole training proce-dure in Algorithm 2. Lines 1-6 provide the detailed training processof GASSIP. For the first warm-up epochs, only candidate oper-ation weights and their masks are updated. Then, the operationweights/masks, structure masks, and architecture parameters areiteratively optimized by calculating the gradient descending of ob-jectives in Eq. 3. In practice, the pruning mask becomes quite sparseafter several iterations. Therefore, the pruning is mostly sparse ma-trix multiplication, which is more efficient compared with densematrix multiplication. After finishing training, the continuous graph structure maskand operation weight mask are binarized to perform graph spar-sification and operation pruning in Line 7. In detail, we initializebinarized structure mask M = M and remove edges that havemask values lower than the threshold : M, = 0, if M, < .Meanwhile, to formulate the binarized weight mask M , we forcethe operation weight mask values that have non-positive values tozero and weights that have positive mask scores to one. The zeroelements will not be trained during the evaluation phase.",
  "Experimental Results": "Analysis of Model Accuracy. We compared GASSIP with vanillaGNNs and automated baselines on the node classification task onfive datasets in . The test accuracy (meanstd) is reportedover 100 runs under different random seeds. We find that our pro-posed algorithm outperforms other baselines in all five datasets.Meanwhile, we can observe that the stds are relatively small, there-fore the searched result is not sensitive to the choice of randomseed. Among all baselines, only DropEdge and GASSO are able toconduct graph sparsification/graph structure learning. DropEdgesurpasses the automated baselines in some scenarios, which provesthe possible performance improvements of removing edge. In com-parison, GASSIP selects removing edges with curriculum sparsifica-tion jointly with architecture search rather than random sampling.Compared with GASSO, on the one hand, GASSO directly uses thesupernet performance as the classification results without inducingan optimal architecture, which hinders its application in memory-limited scenarios. On the other hand, our method further conductsan edge deleting step after the graph structure learning and is ableto perform operation pruning, which makes the searched GNNsmore lightweight. Meanwhile, GASSIP achieves better performancethan GUASS on smaller graphs, but GUASS could handle graphswith more nodes and edges as it is specially developed for large-scale datasets. However, our trained model is more lightweight",
  "OursGASSIP83.200.4271.410.5779.500.3098.460.0671.300.23": "and therefore can be applied in scenarios where computationalresources are limited, which does not apply to GUASS.Analysis of Model Parameters. We further visualized the rela-tionship between model parameter counts and classification testaccuracy in scatter plots shown in . Except for manually-designed GNNs (GCN, GAT, DropEdge) and GNAS methods (DARTS,GraphNAS), we also compare with an iteratively magnitude-basedpruning (IMP) method on GCN and the unified GNN sparsifi-cation (UGS) framework . IMP iteratively removes 1% (we set1 = 20%) weights and retrains GCN from rewinding weights. UGSsimultaneously prunes the graph structure and the model weightsalso in an iteratively magnitude-based pruning way. We set theiterative edge removing probability 2 = 5%. We report the besttest performance of IMP and UGS based on the validation perfor-mance. The hidden size of various baselines is kept the same foreach dataset to make a fair comparison.As shown in , GASSIP achieves higher performance withfewer parameter counts. For the Cora dataset, GASSIP reserves only50% parameters compared with GCN and 13% compared with GAT.For CiteSeer, our method has 8% parameter counts compared withGAT and 15% compared with ARMA. For Physics, the proposedmethod keeps only 6% parameters compared to GAT. Among allbaselines, only DropEdge, UGS, and GASSIP (with in )could generate sparsified graph. DropEdge needs to load the wholegraph in memory to perform edge sampling in each GNN layer. Asa result, only UGS and GASSIP have the potential to reduce theinference cost from the edge-level message propagation calculation.When Co-considering the model parameters and the graph spar-sity, our proposed method keeps only 13%50% parameter countscompared to vanilla GNN baselines and removes 7%17% edgeson Cora, keeps 8%50% model parameters and eliminates 8%19%edges on CiteSeer.",
  "To get a better understanding of the functional components inGASSIP, we further conduct ablation studies on operation pruningand the curriculum graph sparsification parts. shows bar": "plots of the test accuracy on Cora and Physics. We evaluate theperformance under the same search/training hyper-parameters andreport the average accuracy over 100 runs. We compare our methodwith three variants: w/o op prn means to search without pruningoperations and only perform curriculum graph data sparsification,w/o sp stands for searching architectures without the curriculumgraph data sparsification and only conduct operation pruning, w/ocur indicates search architectures with the graph data sparsificationpart but without the curriculum scheduler.By comparing GASSIP with its w/o sp variant in light green,we could find that GASSIP gains performance improvement fromthe curriculum graph sparsification part largely. This phenomenonshows that the graph sparsification component leads the operation-pruned architecture search in a positive way and further substan-tiates the effectiveness of leveraging data to search optimal sub-architectures. Within the curriculum graph sparsification part, per-forming graph sparsification (graph structure learning) with thecurriculum scheduler (w/o op prn) behaves better than without it(w/o cur). Therefore, the curriculum scheduler helps to learn thegraph structure mask better. Besides, the iterative optimizationof graph data and operation-pruned architecture works well ingaining performance improvement.To further illustrate the effectiveness of graph sparsification inour method, we add a new ablation study to substitute our graphsparsification algorithm with DropEdge , which conducts ran-dom edge dropping in the differentiable architecture search pro-cess. The classification accuracy on Cora is 79.420.63 (DARTS81.650.48, ours 83.200.42). This result shows that poorly designededge removal may be harmful to architecture search.",
  "MethodsDARTSDART+UGSGASSOGraphNASGASSIPSearch Time (min)0.5515.790.62223.800.98": "Retraining efficiency. We show the (re-)training time cost (s)of searched GNNs in . For GNAS methods, we report theretraining time of searched GNNs. For manual-designed GNNs,we directly report the training time. For a fair comparison, we fixtraining epochs as 300 and report the averaged training time over100 runs. Results on three datasets illustrate that the training time",
  "Sensitivity Analysis": "hyper-parameters 1 and 2. We further conduct a sensitivityanalysis for curriculum learning hyper-parameters 1 and 2 in. Larger 1 indicates that the label divergence is more im-portant in difficulty calculation in our curriculum algorithm whilelarger 2 suggests that node similarity matters more. In the edge-removing difficulty measurement, with these two node difficultyterms (1 > 0, 2 > 0) have greater performance than without them( = 0), which further illustrates the difficulty measurement in ourcurriculum algorithm is reasonable.hyper-parameters . We conduct a sensitivity analysis experi-ment for hyper-parameter and the results are shown in .As the sampled architecture number in graph structure redun-dancy estimation gets larger, the classification performance firstincreases and then drops. A smaller indicates that only a few",
  ": Lineplots for hyper-parameters (a) 1 (fix 2 = 1)and (b) 2 (fix 1 = 1)": "architectures that are most likely to be selected by NAS are sampledto evaluate the redundancy of the graph structure. When = 1,only the top-1 architecture induced by current is sampled. How-ever, it may not be enough to estimate the redundancy, leading topoor graph sparsification. A larger includes more architectures inthe redundancy estimation. Still, it may contain a lot more architec-tures that are unlikely to be selected in the search phase and causepoor classification results. As a result, we choose to be 2 in ourexperiment. We will add this sensitivity analysis in our revision.",
  "Defend against Adversarial Attacks": "By incorporating the graph data into the optimization process, ourmethod can effectively handle noisy or attackers manipulated data.By incorporating the graph data into the optimization process, ourmethod can effectively handle noisy data or data that has beenmanipulated by attackers . Specifically, the curriculumgraph sparsification allows GASSIP to filter out edges that areeither noisy or have been added maliciously by attackers. As aresult, our approach exhibits a degree of robustness in the face ofsuch adversarial scenarios.Noisy Data. We compare our methods with various baselines suchas graph sparsification methods like DropEgde , PTDNet ,and NeuralSparse on noisy Cora data by randomly adding 1k/5kedges in . This result demonstrates that when there existnoisy edges, GASSIP could achieve the best performance comparedwith baselines.Poisoning Data. To prove the defensive potential of our jointdata and architecture optimization algorithm in countering ad-versarial attacks, we conduct experiments on perturbed data in-cluding comparison with state-of-the-art defensive methods likeGCN-Jaccard and RGCN . demonstrates that GAS-SIP exhibits defensive abilities against perturbed data. However, inorder to further enhance its robustness against adversaries, it is nec-essary to develop special designs tailored to attack settings. Despitethis, the current experiment clearly illustrates that incorporating",
  "CONCLUSION AND FUTURE WORKS": "In this paper, we propose an efficient lightweight graph neural ar-chitecture search algorithm, GASSIP. It iteratively optimizes graphdata and architecture through curriculum graph sparsification andoperation-pruned architecture search. Our method can reduce theinference cost of searched GNNs at the architecture level by re-ducing the model parameters, and at the data level by eliminatingredundant edges. To the best of our knowledge, this is the firstwork to search for lightweight GNN considering both data andarchitecture. Our future works include evaluating GASSIP on otherlarge-scale graphs, providing a theoretical analysis of the conver-gence of our iterative optimization algorithm, and developing aunified benchmark for lightweight GNAS. This work is supported by the National Key Research and Devel-opment Program of China No.2023YFF1205001, National NaturalScience Foundation of China (No. 62222209, 62250008, 62102222,62206149), Beijing National Research Center for Information Sci-ence and Technology under Grant No. BNR2023RC01003, BNR2023TD03006,and Beijing Key Lab of Networked Multimedia.",
  "KDD 24, August 2529, 2024, Barcelona, SpainBeini Xie et al": "Sergi Abadal, Akshay Jain, Robert Guirado, Jorge Lpez-Alonso, and EduardAlarcn. 2021. Computing graph neural networks: A survey from algorithms toaccelerators. ACM Computing Surveys (CSUR) 54, 9 (2021), 138. Filippo Maria Bianchi, Daniele Grattarola, Lorenzo Livi, and Cesare Alippi. 2021.Graph neural networks with convolutional arma filters. IEEE Transactions onPattern Analysis and Machine Intelligence (2021). Jie Cai, Xin Wang, Haoyang Li, Ziwei Zhang, and Wenwu Zhu. 2024. MultimodalGraph Neural Architecture Search under Distribution Shifts. In Proceedings ofthe AAAI Conference on Artificial Intelligence, Vol. 38. 82278235.",
  "Heng Chang, Jie Cai, and Jia Li. 2023. Knowledge Graph Completion withCounterfactual Augmentation. In Proceedings of the ACM Web Conference 2023.26112620": "Heng Chang, Yu Rong, Tingyang Xu, Wenbing Huang, Somayeh Sojoudi, JunzhouHuang, and Wenwu Zhu. 2021. Spectral graph attention network with fast eigen-approximation. In Proceedings of the 30th ACM International Conference onInformation & Knowledge Management (CIKM). 29052909. Heng Chang, Yu Rong, Tingyang Xu, Wenbing Huang, Honglei Zhang, Peng Cui,Xin Wang, Wenwu Zhu, and Junzhou Huang. 2022. Adversarial Attack Frameworkon Graph Embedding Models with Limited Knowledge. IEEE Transactions onKnowledge and Data Engineering (TKDE) (2022). Heng Chang, Yu Rong, Tingyang Xu, Wenbing Huang, Honglei Zhang, PengCui, Wenwu Zhu, and Junzhou Huang. 2020. A restricted black-box adversarialframework towards attacking graph embedding models. In Proceedings of theAAAI conference on Artificial Intelligence (AAAI), Vol. 34. 33893396.",
  "William L. Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive RepresentationLearning on Large Graphs. In NIPS": "Ming Jin, Heng Chang, Wenwu Zhu, and Somayeh Sojoudi. 2021. Power up!Robust Graph Convolutional Network via Graph Powering. In Proceedings ofthe AAAI Conference on Artificial Intelligence (AAAI), Vol. 35. 80048012. Yongcheng Jing, Yiding Yang, Xinchao Wang, Mingli Song, and Dacheng Tao.2021. Amalgamating knowledge from heterogeneous graph neural networks.In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. 1570915718. Chaitanya K Joshi, Fayao Liu, Xu Xun, Jie Lin, and Chuan Sheng Foo. 2022. On rep-resentation knowledge distillation for graph neural networks. IEEE Transactionson Neural Networks and Learning Systems (2022).",
  "Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018. Darts: Differentiablearchitecture search. arXiv preprint arXiv:1806.09055 (2018)": "Zirui Liu, Kaixiong Zhou, Zhimeng Jiang, Li Li, Rui Chen, Soo-Hyun Choi, andXia Hu. 2023. DSpar: An Embarrassingly Simple Strategy for Efficient GNN Train-ing and Inference via Degree-Based Sparsification. Transactions on MachineLearning Research (2023). Dongsheng Luo, Wei Cheng, Wenchao Yu, Bo Zong, Jingchao Ni, Haifeng Chen,and Xiang Zhang. 2021. Learning to drop: Robust graph neural network viatopological denoising. In Proceedings of the 14th ACM international conferenceon web search and data mining. 779787. Naoto Minakawa, Kiyoshi Izumi, Hiroki Sakaji, and Hitomi Sano. 2022. GraphRepresentation Learning of Banking Transaction Network with Edge Weight-Enhanced Attention and Textual Information. In Companion Proceedings of theWeb Conference 2022. 630637. Yijian Qin, Xin Wang, Ziwei Zhang, Hong Chen, and Wenwu Zhu. 2024. Multi-task graph neural architecture search with task-aware collaboration and curricu-lum. Advances in neural information processing systems 36 (2024).",
  "Yu Rong et al. 2020. Dropedge: Towards deep graph convolutional networks onnode classification. ICLR (2020)": "Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick. 2016. Training region-based object detectors with online hard example mining. In Proceedings of theIEEE conference on computer vision and pattern recognition. 761769. Damian Szklarczyk, Annika L Gable, David Lyon, Alexander Junge, Stefan Wyder,Jaime Huerta-Cepas, Milan Simonovic, Nadezhda T Doncheva, John H Mor-ris, Peer Bork, Lars J Jensen, and Christian von Mering. 2018. STRING v11:proteinprotein association networks with increased coverage, supporting func-tional discovery in genome-wide experimental datasets. Nucleic Acids Research(2018).",
  "Xin Wang, Yudong Chen, and Wenwu Zhu. 2021. A survey on curriculumlearning. IEEE transactions on pattern analysis and machine intelligence 44, 9(2021), 45554576": "Xiaoyang Wang, Yao Ma, Yiqi Wang, Wei Jin, Xin Wang, Jiliang Tang, CaiyanJia, and Jian Yu. 2020. Traffic flow prediction via spatial temporal graph neuralnetwork. In Proceedings of The Web Conference 2020. 10821092. Huijun Wu, Chen Wang, Yuriy Tyshetskiy, Andrew Docherty, Kai Lu, and LimingZhu. 2019. Adversarial Examples for Graph Data: Deep Insights into Attack andDefense. In Proceedings of the 28th International Joint Conference on ArtificialIntelligence. AAAI Press, 48164823. Beini Xie, Heng Chang, Ziwei Zhang, Xin Wang, Daixin Wang, Zhiqiang Zhang,Rex Ying, and Wenwu Zhu. 2023.Adversarially robust neural architecturesearch for graph neural networks. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition. 81438152. Peng Xu, Lin Zhang, Xuanzhou Liu, Jiaqi Sun, Yue Zhao, Haiqin Yang, and Bei Yu.2023. Do not train it: a linear neural architecture search of graph neural networks.In International Conference on Machine Learning. PMLR, 3882638847. Shen Yan, Biyi Fang, Faen Zhang, Yu Zheng, Xiao Zeng, Mi Zhang, and HuiXu. 2019. Hm-nas: Efficient neural architecture search via hierarchical masking.In Proceedings of the IEEE/CVF International Conference on Computer VisionWorkshops. 00.",
  "Xiang Zhang and Marinka Zitnik. 2020. Gnnguard: Defending graph neuralnetworks against adversarial attacks. Advances in neural information processingsystems 33 (2020), 92639275": "Yanfu Zhang, Shangqian Gao, Jian Pei, and Heng Huang. 2022. Improving socialnetwork embedding via new second-order continuous graph neural networks.In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discoveryand Data Mining. 25152523. Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Zhou Qin, and WenwuZhu. 2022. Dynamic graph neural networks under spatio-temporal distributionshift. Advances in neural information processing systems 35 (2022), 60746089. Zeyang Zhang, Xin Wang, Ziwei Zhang, Guangyao Shen, Shiqi Shen, and WenwuZhu. 2024. Unsupervised graph neural architecture search with disentangledself-supervision. Advances in Neural Information Processing Systems 36 (2024). Ziwei Zhang, Xin Wang, and Wenwu Zhu. 2021. Automated Machine Learning onGraphs: A Survey. In Proceedings of the Thirtieth International Joint Conferenceon Artificial Intelligence, IJCAI-21. 47044712.",
  "Huan ZHAO, Quanming YAO, and Weiwei TU. 2021. Search to aggregate neigh-borhood for graph neural network. In 2021 IEEE 37th International Conferenceon Data Engineering (ICDE). 552563": "Tong Zhao, Yozen Liu, Leonardo Neves, Oliver Woodford, Meng Jiang, and NeilShah. 2021. Data augmentation for graph neural networks. In Proceedings of theaaai conference on artificial intelligence, Vol. 35. 1101511023. Cheng Zheng, Bo Zong, Wei Cheng, Dongjin Song, Jingchao Ni, Wenchao Yu,Haifeng Chen, and Wei Wang. 2020. Robust graph representation learning vianeural sparsification. In International Conference on Machine Learning. PMLR,1145811468. Dingyuan Zhu, Ziwei Zhang, Peng Cui, and Wenwu Zhu. 2019. Robust GraphConvolutional Networks Against Adversarial Attacks. In Proceedings of the25th ACM SIGKDD International Conference on Knowledge Discovery & DataMining (KDD 19). 13991407.",
  "ALIMITATIONS": "The main purpose of this paper is to search for a lightweight GNN(i.e., lightweight GNN design) that offers a wider range of appli-cation scenarios (e.g., edge computing) by limited computationalresource requirements. Therefore, the current implementation ofGASSIP has difficulty to be integrated with graphs with billions ofnodes, This difficulty of scalability commonly hinders both graphsparsification and current GNAS research in applications with con-tained resources without a specifically designed sampling strategy.",
  "BEXPERIMENTAL SETTINGS": "Dataset. We evaluate the node classification performance on 5datasets: Cora, CiteSeer, PubMed, Physics, and Ogbn-Arxiv. Thestatistics of all datasets are shown in . The first three datasetsfollow a traditional semi-node classification train-valid-test split.Physics and Ogbn-Arxiv represent large datasets where we ran-domly split train:valid:test=50%:25%:25% for Physics and follow thedefault setting for Ogbn-Arxiv.",
  "Cora2,70810,5561,4337Citeseer3,3279,1043,7036Pubmed19,71788,6485003Physics34,493495,9248,4155Ogbn-Arxiv169,3431,166,24312840": "Baselines. We compare our method with representative hand-crafted GNNs: GCN , GAT , ARMA , and three graphsparsification methods: DropEdge , PTDNet , and Neu-ralSparse . We also compare with representative GNAS base-lines, including DARTS , GraphNAS , GASSO , andGUASS . We use GASSO as the representative of GNAS withstructure learning.Implementation Details. For GASSIP, we set the number of layersas 2 for CiteSeer, Cora, PubMed, Physics, and 3 for Ogbn-Arxiv.In building search space, we adopt GCNConv , GATConv ,SAGEConv , ArmaConv , and Linear as candidate operations.Due to the memory limit, the search space is narrowed down toGCNConv,GATConv, ArmaConv, and Linear for Physics and Ogbn-Arxiv. The supernet is constructed as a sequence of layers, We setbatch normalization (only for Physics and Ogbn-Arxiv) and dropoutbefore each layer, and use ReLU as the activation function.Detailed Hyper-parameters. For vanilla GNNs, we follow thehyper-parameters in the original paper except tuning hyper-parameterslike hidden channels in {16, 64, 128, 256} and dropout in {0.5, 0.6, 0.8}.For GNAS methods, we use the Adam optimizer to learn param-eters. We retrain for 100 runs on their searched optimal architec-tures to make a fair comparison of the architecture performance.For GASSIP, we fix the number of sampled architectures as = 2,entropy loss the coefficient in curriculum graph data sparsificationas = 0.001, and the edge-removing difficulty hyper-parameters1 = 1, 2 = 1. The supernet training epoch is fixed to 250 and thewarm-up epoch number is set as = 10."
}