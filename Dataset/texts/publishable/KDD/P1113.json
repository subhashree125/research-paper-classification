{
  "ABSTRACT": "High-quality and consistent annotations are fundamental to the suc-cessful development of robust machine learning models. Traditionaldata annotation methods are resource-intensive and inefficient, of-ten leading to a reliance on third-party annotators who are notthe domain experts. Hard samples, which are usually the mostinformative for model training, tend to be difficult to label accu-rately and consistently without business context. These can ariseunpredictably during the annotation process, requiring a variablenumber of iterations and rounds of feedback, leading to unforeseenexpenses and time commitments to guarantee quality.We posit that more direct involvement of domain experts, usinga human-in-the-loop system, can resolve many of these practicalchallenges. We propose a novel framework we call Video Annotator(VA) for annotating, managing, and iterating on video classificationdatasets. Our approach offers a new paradigm for an end-user-centered model development process, enhancing the efficiency,usability, and effectiveness of video classifiers. Uniquely, VA allowsfor a continuous annotation process, seamlessly integrating datacollection and model training.We leverage the zero-shot capabilities of vision-language founda-tion models combined with active learning techniques, and demon-strate that VA enables the efficient creation of high-quality models.VA achieves a median 8.3 point improvement in Average Precisionrelative to the most competitive baseline across a wide-rangingassortment of tasks. We release a dataset with 153k labels across 56video understanding tasks annotated by three professional videoeditors using VA, and also release code to replicate our experimentsat github.com/netflix/videoannotator.",
  "vision language models, active learning, data annotation tools, videounderstanding": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 25-29, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-x-xxxx-xxxx-x/YY/MM ACM Reference Format:Amir Ziai and Aneesh Vartakavi. 2024. Video Annotator: A frameworkfor efficiently building video classifiers using vision-language models andactive learning. In Proceedings of KDD (KDD 24). ACM, New York, NY, USA,9 pages.",
  "INTRODUCTION": "Conventional techniques for training machine learning classifiersare resource intensive. They involve a cycle where domain expertsannotate a dataset, which is then transferred to data scientists totrain models, review outcomes, and make changes. This labelingprocess tends to be time-consuming and inefficient, often haltingafter a few annotation cycles. Consequently, less effort is investedin annotating larger datasets compared to iterating on more com-plex models and algorithmic methods to improve performance andfix edge cases, as a result of which ML systems grow rapidly incomplexity. Furthermore, constraints on time and resources oftenresult in leveraging third-party annotators rather than domain ex-perts. These annotators perform the labeling task without a deepunderstanding of the models intended deployment or usage, oftenmaking consistent labeling of borderline or hard examples, espe-cially in more subjective tasks, a challenge. This often necessitatesmultiple review rounds with domain experts, leading to unexpectedcosts and delays. This lengthy cycle can also result in model drift, asit takes longer to fix edge cases and deploy new models, potentiallyhurting usefulness and stakeholder trust in these technologies.We suggest that more direct involvement of domain experts,using a human-in-the-loop system, can resolve many of these prac-tical challenges described above. We introduce a novel framework,Video Annotator (VA), for annotating, managing, and iterating onvideo classification datasets. Our interactive framework employsactive learning techniques, to guide users to focus their efforts onprogressively harder examples, enhancing the models sample effi-ciency and keeping costs low. Equipped with sufficient knowledgeand context, they can rapidly make informed decisions on hardsamples during the annotation process. We leverage the zero-shotcapabilities of large vision-language models to aid the annotationprocess and to serve as backbones for lightweight binary classi-fiers that we train for each label. This design simplifies modeldeployment, enabling us to scale to a large number of labels with-out significantly increasing complexity. VA seamlessly integratesmodel building into the data annotation process, facilitating uservalidation of the model before deployment, therefore helping withbuilding trust and fostering a sense of ownership. VA supports acontinuous annotation process, allowing users to rapidly deploymodels, monitor their quality in production, and swiftly fix anyedge cases by annotating a few more examples and deploying a newmodel version. This self-service architecture empowers users to",
  "KDD 24, August 25-29, 2024, Barcelona, SpainAmir Ziai and Aneesh Vartakavi": ": We first divide the data for each source into batchesof size , illustrated here as VA, ZS (zero shot), and (ran-dom). At = 0, we independently evaluate the first batch foreach source and compute . At = 1, the algorithm choosesVA, prompting us to concatenate the first three batches witha new VA batch of size (i.e., VA2). At = 2, the algorithmselects , which leads to appending 2. 40 steps and achieved the highest performance. Upon replicatingthese experiments with a larger batch size of = 50, we observedvery similar outcomes. However, both larger ( 100) and smaller( 10) batch sizes led to a decline in performance. 5.2.3Results. For each label, we calculate the gain (similar to.1.2) compared to VA without an algorithm for candidatesource selection as the baseline. summarizes the distributionfor each algorithm. We find that the median gain is positivefor all algorithms. UCB achieves the highest performance of 3.4points (excluding the greedy oracle), resulting in a cumulative me-dian improvement of 8.3 points over the most competitive baselinemethod CC (see .1). Interestingly, even a basic round robinapproach can improve performance over the baseline.",
  "RELATED WORK2.1Vision Language Models (VLMs)": "Self-supervised methods using convolutional or transformer-basedarchitectures have dominated much of the progress in video un-derstanding in recent years . Specifically,VLMs such as CLIP , demonstrate very good zero-shot naturallanguage understanding performance across a wide variety of vi-sual tasks . However, zero-shot methods often lackthe requisite nuance and reliability in many real-world applications, especially for the long tail of visual concepts .SeeSaw tackles this problem by aligning CLIP queryvectors using user feedback via an interactive system for search over image databases. Similar to SeeSaw, we use VLMs to enable search.In contrast to SeeSaw, our approach focuses on video classificationfor the purpose of generalization to unseen videos vs. interactivesearch sessions, and uses active learning instead of query alignment.",
  "Video Annotation Tooling": "Numerous commercial and open-source tools for video analyticsand annotation exist . Many of these approaches focuson identifying and tracking specific objects within a video ,while others focus on interactive search sessions . Incontrast, our approach focuses on video classification for highlyspecific tasks. Our goal is to construct models that can generalizeto unseen data and can be used by either end-users or other algo-rithms, rather than focusing on interactive search sessions over afixed dataset. Nevertheless, we outline some approaches that sharesimilarities with our work.Zelda is a video analytics tool that employs VLMs to assistusers in retrieving relevant and diverse results through promptengineering techniques. Similar to Zelda, we use VLMs to facilitatesemantic search using natural language. In contrast to Zelda, weleverage active learning to annotate a labeled dataset that capturesnuances that are hard to express solely with prompt engineering.VOCALExplore offers a system for early data explorationon video datasets using active learning. Both approaches harnessVLMs, but VOCALExplore uses them only as feature extractors,while we leverage VLMs as both feature extractors and to enabletext-video search. While both VA and VOCALExplore are inter-active systems aiming to minimize latency, they adopt differentstrategies to achieve this goal. VA employs a pre-processing stepover a large corpus of videos, enabling both text-to-video search anduses lightweight models over a VLM backbone to reduce latency. Incontrast, VOCALExplore uses a Task Scheduler and active learningstrategies to avoid the pre-processing step. VA is less restrictiveand offers more user choice and freedom, allowing users to freelysearch for specific examples that they are interested in labeling,while VOCALExplore requires users to specify a labeling budgetand suggests a batch of samples to review.To the best of our knowledge, VA represents the first-of-its-kindsystem for granular video classification, leveraging VLMs to facili-tate semantic search through natural language and active learningto enhance sample efficiency. It offers a unique approach to an-notating, managing, and iterating on video classification datasets,emphasizing the importance of direct involvement of domain ex-perts in a human-in-the-loop system.",
  "METHODOLOGY": "We describe our proposed framework, called Video Annotator (VA),which is designed for building binary video classifiers for an exten-sible set of labels (see Fig 2), enabling granular video understanding.This formulation allows for improving one model independent ofthe others. Fig 3 depicts an overview of VA. Our system uses video() and text encoders () from a Vision-Language Model (VLM) toextract embeddings. We gather a diverse set of video clips, the em-beddings for which are extracted using , and stored for efficientretrieval (e.g. FAISS ) alongside other metadata about each clip.",
  "Video Annotator: A framework for efficiently building video classifiers using vision-language models and active learningKDD 24, August 25-29, 2024, Barcelona, Spain": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, YanaHasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al.2022. Flamingo: a visual language model for few-shot learning. Advances inNeural Information Processing Systems 35 (2022), 2371623736. Areen Alsaid and John D Lee. 2022. The DataScope: A mixed-initiative archi-tecture for data labeling. In Proceedings of the Human Factors and ErgonomicsSociety Annual Meeting, Vol. 66. SAGE Publications Sage CA: Los Angeles, CA,15591563. Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lui, andCordelia Schmid. 2021. Vivit: A video vision transformer. In Proceedings of theIEEE/CVF international conference on computer vision. 68366846. Kithmi Ashangani, KU Wickramasinghe, DWN De Silva, VM Gamwara, AnupiyaNugaliyadde, and Yashas Mallawarachchi. 2016. Semantic video search by auto-matic video annotation using TensorFlow. In 2016 Manufacturing & IndustrialEngineering Symposium (MIES). IEEE, 14.",
  "VA guides the annotator through the labeling process by displayingmodel quality and data diversity scores": "3.1.1Model quality score. We want the classifier to reliably sep-arate positives from negatives. To measure this, we use -foldcross-validation and report the 25th percentile across the Bal-anced Accuracy (BA) scores. We picked BA because its notsensitive to class imbalance and is an intuitive metric to understand.We report the 25th percentile instead of mean/median in order tobe more conservative in our reporting. We use = 5 in this work. 3.1.2Data diversity score. A model solely optimized for qualitycould potentially exhibit bias and fail to generalize. A diverse train-ing set encompassing a broad spectrum of visual concepts is neces-sary for the construction of robust models. We therefore formulateda heuristic we call the data diversity score to guide our users.The goal is to accumulate annotations from a variety of visualelements. We identify these elements using -means clusteringover the embeddings of all the clips within the corpus. We mapthe positive and negative annotations independently onto theseclusters, and compute the resulting counts, capping it to a maximumvalue . We then sum all the counts, and normalize (i.e. dividing by2 ). An example is depicted in . We use = = 10 forthis study. The scores range from 0 to 1. Labeling examples fromthe randomly selected feed helps improve this score, but note thatit may be difficult to achieve a perfect score. In the worst case, thebest achievable score is 0.5, which happens when a label perfectlyaligns with one of the clusters.",
  ": Step 1: Text-to-video search to bootstrap the anno-tation process": "as shown in . We used the Condensed Movies Dataset ,which consists of close to 30k movie scenes belonging to 3.5k moviesacross a wide range of genres, decades, and countries. First, we usedPySceneDetect to segment each scene into individual shots.Then, we extracted shot-level Clip4CLIP (C4C) embeddingsand used those to remove near-duplicate shots. We followed the",
  "EXPERIMENTS": "This section details our experiments to study the sample efficiencyof VA compared to baselines. For every label, 20% of the labeled data,obtained through uniform sampling, is allocated for testing, whilethe remainder serves as the training set. Each experiment is repeatedfive times, each time with independently sampled bootstraps of thetraining set and evaluated against the same test set. The evaluationmetric employed is the mean Average Precision () . We used",
  "The results from this experiment suggest that VA, in additionto enhanced sample efficiency, offers continuous improvements inmodel quality as the number of samples increases": "5.1.3Data diversity score. As referenced in .1.2, VA moti-vates annotators to label visually diverse clips. Fig 10 represents theaggregated data diversity score as a function of . We find that VAproduces datasets with higher visual diversity relative to the othermethods, which possibly contributes to the improved generalizationperformance we noted earlier.",
  "Overall1.20.41.52.9": ": Model quality gain for using VA relative to CombinedClassifier (CC). For each group, we take the median gainacross all labels belonging to the group and a specific valueof . The last row represents the median across all labels. : Mean and standard deviation of the data scorefor different methods as a function of . Upper bound isthe maximum possible score at each value . Data score forall methods except for VA are computed at fixed values {25, 50, 100, 500, 1000}.",
  "Experiment 2: annotation candidate sourceselection": "VA offers annotators the flexibility to choose the source of candi-dates for annotation, providing options between search and anyof the four feeds (as discussed in ). As demonstrated inExperiment 1, this strategy typically yields superior model qualityand data diversity scores.Nevertheless, anecdotal feedback suggested that our users, whileappreciating the VA workflow and user experience, were some-times uncertain about the optimal action to take following a modelretraining step. VA doesnt impose any specific direction on the user, giving them the freedom to choose what samples to label next.However, we hypothesized that this freedom could be preservedwhile also assisting users by intermittently recommending a batchof examples for annotation. To test this hypothesis, we designed anexperiment using a multi-armed bandit formulation, which recom-mended one of three actions to the user: the VA process, annotatingrandom samples, or annotating zero-shot samples.Each source is divided into batches of fixed size . At each step, an algorithm determines one source for annotation. We emulatethe presentation of a batch of clips to an annotator and recordthe absolute gain in model quality compared to the previous step.At each time step , the following occur:",
  "(1) Selection: The algorithm picks a source or \"action\", denotedas . The choice of source depends on the bandit algorithmand the score associated with each source, which we denoteas": "(2) Action: We add the next batch from the chosen source tothe data from the previous step, which we denote as 1.This results in a new dataset, , on which we train a model(using the same setup as in the previous experiment). Wethen record the median test Average Precision (AP) as . (3) Update: We update the per-source score for the chosen source,setting equal to the difference between the current andthe most recent value 1. We also ran experiments usingsimple and exponentially-weighted averages over the se-quence of scores for each source, but found that they wereinferior to using the most recent value. We determine the initial score for each source, , by evaluatingthe first batch for each source independently. For the first time step = 1, we set 0 to the concatenation of the first batch from allthree sources. An illustration of this process can be found in Fig 11.",
  "ln /, where is the total number of steps executed so far, and is thenumber of times arm has been selected before the currentstep. We tested various and found = 102 toperform the best": "(4) Greedy oracle: Intended as a greedy upper bound, this algo-rithm evaluates all three actions at each step and selects theone yielding the highest . Note that this is an oracle methodand a priori knowledge of is not possible in practice. 5.2.2Experimental setup. The design of VA enables users to haltthe labeling process once certain minimum conditions are satisfied(see ). We used the final set of annotations and the corre-sponding model for each label (i.e. the largest value of ), whichvaries across labels.We experimented with various values of batch size and setit to = 25 for this experiment, which resulted in a maximum of",
  "LIMITATIONS AND FUTURE WORK": "We introduced Video Annotator (VA) as a framework for efficientlyconstructing binary video classifiers. There is a vast space of designdecisions that can be explored in future work.At a high level, the frameworks applicability extends beyondvideo to other media assets such as audio, images, text, and 3Dobjects. It can leverage advancements in joint embeddings acrossmultiple modalities, such as ImageBind . While we primarilyexplored context-free labels in this work, we could also extend thisto long-form video by incorporating contextual encoders.We can try to improve model performance in many ways, forexample a superior video encoder could improve both sample effi-ciency and the final models quality. Leveraging multimodal repre-sentations (e.g., audio and subtitles) might enhance performance forcertain tasks, and advanced active learning strategies could furtheraugment sample efficiency.In terms of user experience, VA could be improved by refiningmetrics (model and data scores), proactively identifying potentialissues (e.g. where the models prediction and the users label do notagree), and suggesting additional search terms . More advancedsearches, such as textual combined with metadata (e.g., genre, year),or video-to-video searches (i.e., identifying videos similar to a ref-erence video) could also provide benefits.",
  "CONCLUSION": "We presented Video Annotator (VA), a novel, interactive frameworkthat addresses many challenges associated with conventional tech-niques for training machine learning classifiers. VA leverages thezero-shot capabilities of large vision-language models and activelearning techniques to enhance sample efficiency and reduce costs.It offers a unique approach to annotating, managing, and iteratingon video classification datasets, emphasizing the direct involvementof domain experts in a human-in-the-loop system. By enabling theseusers to rapidly make informed decisions on hard samples duringthe annotation process, VA increases the systems overall efficiency.Moreover, it allows for a continuous annotation process, allowingusers to swiftly deploy models, monitor their quality in production,and rapidly fix any edge cases. This self-service architecture em-powers domain experts to make improvements without the activeinvolvement of data scientists or third-party annotators, and fostersa sense of ownership, thereby building trust in the system.We conducted experiments to study the performance of VA, andfound that it yields a median 8.3 point improvement in AveragePrecision relative to the most competitive baseline across a wide-ranging assortment of video understanding tasks. We release adataset with 153k labels across 56 video understanding tasks anno-tated by three professional video editors using VA, and also releasecode to replicate our experiments.",
  "Brandon Castellano. [n. d.]. Breakthrough/pyscenedetect: Python and opencv-based scene cut/transition detection program and library": "Boris Chen, Amir Ziai, Rebecca S Tucker, and Yuchen Xie. 2023. Match Cutting:Finding Cuts with Smooth Visual Transitions. In Proceedings of the IEEE/CVFWinter Conference on Applications of Computer Vision. 21152125. Sihan Chen, Xingjian He, Longteng Guo, Xinxin Zhu, Weining Wang, Jinhui Tang,and Jing Liu. 2023. Valor: Vision-audio-language omni-perception pretrainingmodel and dataset. arXiv preprint arXiv:2304.08345 (2023).",
  "Ira Cohen and Moises Goldszmidt. 2004. Properties and benefits of calibratedclassifiers. In European conference on principles of data mining and knowledgediscovery. Springer, 125136": "Maureen Daum, Enhao Zhang, Dong He, Stephen Mussmann, Brandon Haynes,Ranjay Krishna, and Magdalena Balazinska. 2023. VOCALExplore: Pay-as-You-Go Video Data Exploration and Model Building. arXiv preprint arXiv:2303.04068(2023). Ali Diba, Mohsen Fayyaz, Vivek Sharma, Manohar Paluri, Jrgen Gall, RainerStiefelhagen, and Luc Van Gool. 2020. Large scale holistic video understanding.In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August2328, 2020, Proceedings, Part V 16. Springer, 593610. Sam Esmail. 2023. Leave The World Behind. Netflix (2023). Boris Chen et al. 2023. Building In-Video Search. Netflix TechBlog (2023). Vi Iyengar et.al. 2022. New Series: Creating Media with Machine Learning. NetflixTechBlog (2022).",
  "Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fa-had Shahbaz Khan, and Mubarak Shah. 2022. Transformers in vision: A survey.ACM computing surveys (CSUR) 54, 10s (2022), 141": "Joosung Kim, Ryu-Hyeok Gwon, Jin-Tak Park, Hakil Kim, and Yoo-Sung Kim.2013. A semi-automatic video annotation tool to generate ground truth forintelligent video surveillance systems. In Proceedings of International Conferenceon Advances in Mobile Computing & Multimedia. 509513. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrappinglanguage-image pre-training for unified vision-language understanding andgeneration. In International Conference on Machine Learning. PMLR, 1288812900.",
  "Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, and YuQiao. 2023. Unmasked Teacher: Towards Training-Efficient Video FoundationModels. arXiv:2303.16058 [cs.CV]": "Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and HanHu. 2022. Video swin transformer. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition. 32023211. Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and TianruiLi. 2022. Clip4clip: An empirical study of clip for end to end video clip retrievaland captioning. Neurocomputing 508 (2022), 293304. Mayug Maniparambil, Chris Vorster, Derek Molloy, Noel Murphy, Kevin McGuin-ness, and Noel E OConnor. 2023. Enhancing clip with gpt-4: Harnessing visualdescriptions as prompts. In Proceedings of the IEEE/CVF International Conferenceon Computer Vision. 262271.",
  "Chris Smith. 2021. Operation Varsity Blues: The College Admissions Scandal.Netflix (2021)": "Lubomir Stanchev, Hanson Egbert, and Benjamin Ruttenberg. 2020. Automatingdeep-sea video annotation using machine learning. In 2020 IEEE 14th InternationalConference on Semantic Computing (ICSC). IEEE, 1724. Jiaqi Wang, Zhengliang Liu, Lin Zhao, Zihao Wu, Chong Ma, Sigang Yu, HaixingDai, Qiushi Yang, Yiheng Liu, Songyao Zhang, et al. 2023. Review of large visionmodels and visual prompt engineering. Meta-Radiology (2023), 100047. Hongwei Xue, Yuchong Sun, Bei Liu, Jianlong Fu, Ruihua Song, Houqiang Li,and Jiebo Luo. 2022. Clip-vip: Adapting pre-trained image-text model to video-language representation alignment. arXiv preprint arXiv:2209.06430 (2022). Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang,Yejin Choi, and Jianfeng Gao. 2021. Vinvl: Revisiting visual representations invision-language models. In Proceedings of the IEEE/CVF conference on computervision and pattern recognition. 55795588."
}