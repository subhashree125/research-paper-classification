{
  "ABSTRACT": "With the advancement of GPS, remote sensing, and computationalsimulations, large amounts of geospatial and spatiotemporal dataare being collected at an increasing speed. Such emerging spa-tiotemporal big data assets, together with the recent progress ofdeep learning technologies, provide unique opportunities to trans-form society. However, it is widely recognized that deep learningsometimes makes unexpected and incorrect predictions with un-warranted confidence, causing severe consequences in high-stakedecision-making applications (e.g., disaster management, medicaldiagnosis, autonomous driving). Uncertainty quantification (UQ)aims to estimate a deep learning models confidence. This paperprovides a brief overview of UQ of deep learning for spatiotempo-ral data, including its unique challenges and existing methods. Weparticularly focus on the importance of uncertainty sources. Weidentify several future research directions for spatiotemporal data. ACM Reference Format:Wenchong He and Zhe Jiang. 2024. Uncertainty Quantification of DeepLearning for Spatiotemporal Data: Challenges and Opportunities. In Pro-ceedings of 2nd KDD Workshop on Uncertainty Reasoning and Quantificationin Decision Making (UDM-KDD23 ). ACM, New York, NY, USA, 5 pages.",
  "INTRODUCTION": "With the advancement of GPS, remote sensing, and computationalsimulations, large amounts of geospatial and spatiotemporal dataare being collected at an increasing speed . Such emerging spa-tiotemporal big data assets, together with the recent progress ofdeep learning technologies, provide unique opportunities to trans-form society in broad applications. For example, deep learningis widely used to process spatiotemporal data from radar or lidarsensors and video cameras to monitor road conditions, detect pedes-trians, and navigate through traffic . In disaster management,deep learning systems have been developed to analyze satelliteor drone imagery to enhance situational awareness during deadlyhurricane flood disasters . Although deep learning is knownfor higher prediction accuracy compared with many traditionalmachine learning techniques, it is widely recognized that they Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from -KDD23 , August 07, 2023, Long Beach, CA 2024 Association for Computing Machinery.ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00 can sometimes make unexpected and incorrect predictions withunwarranted confidence, particularly in complex real-world envi-ronments . This can have serious consequences in high-stakesapplications like autonomous driving , medical diagnosis ,and disaster response . Therefore, uncertainty quantification isessential for a deep learning model to be aware of its limitationsand avoid overconfident predictions.Applications: One important application is disaster response.Deep learning has been used to predict the track of hurricanes orestimate flooded areas, the results of which directly impact decision-makers in planning evacuation and rescue efforts. Thus, in hurri-cane tracking, scientists often provide not only the most likelypoint of landfall but also provide a cone of uncertainty acrossother likely points of impact and future trajectories of the storm.Similarly, in autonomous driving, complex environmental factorslike extreme weather or the ambiguous appearance of nearby ve-hicles can fool a deep learning model to ignore an obstacle andcause traffic crashes. In the medical domain, deep learning hasbeen widely used for medical image analysis, clinical diagnosis, andtreatment planning. Overconfident predictions can not only causeunnecessary medical expenses but also endanger patient life .Challenges: Uncertainty quantification of deep learning forspatiotemporal data poses unique challenges due to their specialdata characteristics. First, spatiotemporal data violate the commonassumption that samples follow an identical and independent distri-bution. Instead, implicit dependency structures exist in continuousspace and time (e.g., spatial and temporal autocorrelation, and tem-poral dynamics) . Thus, the uncertainty quantificationprocess should be aware of such a dependency structure. Second,spatiotemporal data have various spatial, temporal, and spectral res-olutions and diverse sources of noise and errors (e.g., sensor noise,obstacles, and atmospheric effects in remote sensing signals ,GPS errors). Analyzing such data often requires the co-registrationof different layers (e.g., points, lines, polygons, geo-rasters) into thesame spatial reference system. The process is subject to registra-tion uncertainty due to GPS errors or annotation mistakes in mapgeneration . Third, spatiotemporal data are heterogeneous(non-stationary), i.e., the data distribution often varies across dif-ferent regions or time periods . Thus, a deep learning modeltrained in one region (or time) may not generalize well to anotherregion (or time). Spatiotemporal non-stationary requires charac-terizing uncertainty due to out-of-distribution samples . Thisissue is particularly important when spatial observation samplesare sparsely distributed, causing uncertainty when inferring theobservations at other locations in continuous space . More-over, modeling such uncertainty needs to consider sample densityboth in the non-spatial feature space and in the geographic space.",
  "UDM-KDD23 , August 07, 2023, Long Beach, CAHe et al": "to learn the predictive distribution (|) using the conditionaldeep generative model (cDGM) . Uncertainty quantificationmodels based on cDGMs aim to learn a conditional density overthe prediction , given the input feature . This involves learning amodel (, ) : X Y, where the generative model (, ) with () approximates the true unknown distribution true(|).The variability of the prediction distribution (|) is encoded inthe latent variable and the generative model. During inference,for any X, we can generate samples with = (, ),where (). From these samples {}=1, we can quantify theprediction uncertainty by measuring their variability.",
  "Data uncertainty": "Generally speaking, data uncertainty is represented by the entropyof the distribution (|, ), where is the neural network pa-rameters. Existing approaches for learning this distribution can beclassified into deep discriminative models and generative models. 3.2.1Deep discriminative models. Deep discriminative models canbe further categorized as parametric or non-parametric based onthe format of the distribution. To quantify data uncertainty, a dis-criminative model directly outputs a predictive distribution using aneural network. The distribution can be represented by a parametricmodel, which assumes a parameterized family of probability dis-tributions (e.g., Gaussian or mixture Gaussian) whose parameters(e.g., mean and variance) are predicted by the neural network .Alternatively, a non-parametric model does not make any assump-tions about the underlying distributions and outputs a predictioninterval . Prediction intervals provide a lower and upper bound, within which the ground truth is expected to fall with aprescribed confidence level of 1 (i.e., ( ) > 1 ).However, a drawback is that standard optimization strategies maynot be applicable. 3.2.2Deep generative models. Deep generative models (DGMs)are capable of learning the intractable data distribution data() inthe high-dimensional feature space X R from a large numberof independent and identically distributed observed samples. Toquantify DNN data uncertainty, the basic idea is to employ DGMs",
  "Model uncertainty": "2.2.1Source of model uncertainty. Model uncertainty, also knownas aleatoric uncertainty, encompasses the uncertainty in a modelspredictions resulting from imperfections in the model training pro-cess. In the context of spatiotemporal data, model uncertainty canbe attributed to three primary sources: uncertainty in model archi-tecture, uncertainty in model parameters, and uncertainty due todataset distribution mismatch. Uncertainty in model architecturearises from the lack of understanding regarding the most suitablemodel architecture for a given geospatial dataset. For instance, indeep learning models, uncertainty may exist regarding the optimalnumber of neural network layers and neurons in each layer, as anoverly complex model can lead to overfitting. Uncertainty in modelparameters stems from unknown optimal parameter values. Thisuncertainty can arise due to factors such as an improper trainingstrategy, limited geospatial training instances, or convergence to alocal optimum rather than the global minimum of the loss function.Therefore, the models weight values may not accurately representthe true optimal solution. The last type of model uncertainty iscaused by dataset distribution drift, where the distribution of testsamples differs from that of the training dataset. This issue, knownas out-of-distribution (OOD) data, is not uncommon in real-worldspatiotemporal deployments, as the test cases often involve complexand diverse scenarios. 2.2.2Model uncertainty representation. Representing model uncer-tainty in spatiotemporal data poses challenges due to its multiplesources. Different methods can be adopted to estimate and representuncertainty associated with each type. For uncertainty stemmingfrom model parameters, Bayesian neural networks (BNNs) are com-monly used . BNNs assume a prior distribution over the modelparameters and aim to infer the posterior distribution to reflect pa-rameter uncertainty. Uncertainty arising from model architecturescan be estimated using deep ensembles. This approach involvesconstructing an ensemble of neural network architectures, train-ing each model separately, and generating predictions that forma distribution on the target variable. The variance of these predic-tions serves as an estimation of prediction uncertainty. Uncertaintyresulting from dataset distribution mismatch can be assessed byconsidering the proximity of new test samples to the training sam-ples. As the test sample deviates further from the distribution oftraining data, the model uncertainty increases.In summary, model uncertainty in spatiotemporal data arisesfrom misspecifications in model architectures, parameters, anddataset distributions. Depending on the specific application, one",
  "(Y|X)(1)": "One approach is to select an approximation () from a pa-rameterized class of distributions Q to approximate the posterior.Popular optimization methods for selecting () include varia-tional inference and Laplace approximation , both of whichimpose assumptions and restrictions on the form of the approxi-mated posterior. However, these restrictions can lead to inaccuraciesin predictions and uncertainty quantification. Another widely usedapproach is the MC dropout method , which is simple and easy toimplement. It demonstrates that optimizing a neural network witha dropout layer is equivalent to approximating a BNN using varia-tional inference on a parametric Bernoulli distribution. Uncertaintyestimation is obtained by computing the variance across multiplestochastic forward predictions with different dropout masks. How-ever, MC dropout tends to be less calibrated than other baselineuncertainty quantification methods in many benchmark datasets. 3.1.2Ensemble models. Ensemble models are a powerful approachthat involves combining multiple neural network models duringthe prediction process. By aggregating the predictions of individ-ual models, an output distribution is formed. The variability inpredictions among the ensemble models can serve as an indicatorof model uncertainty, where a higher variance implies a greaterdegree of uncertainty. To capture this uncertainty stemming fromvarious factors, several strategies for constructing ensembles can",
  "be employed, such as bootstrapping or combining different neuralnetwork architectures": "3.1.3Sample-density aware neural networks. The mentioned ap-proaches do not effectively handle model uncertainty arising fromlow sample density, where samples lying far from the training setsupport may lead to overly confident predictions. To address this,various approaches have been developed to create sample density-aware neural networks capable of capturing model uncertainty insuch scenarios. These approaches include Gaussian process mod-eling (e.g., kriging ) for spatial data, deep Gaussian processes,and distance-aware neural networks. Distance-aware neural net-works, inspired by Gaussian process models, aim to characterizeuncertainty based on the density of sample features. They utilizethe feature extraction capabilities of deep neural networks (DNNs)to learn a hidden representation () that reflects meaningful dis-tances within the data manifold .",
  "The combination of model and datauncertainty": "Many frameworks have been proposed to jointly consider both dataand model uncertainty, aiming for more accurate uncertainty quan-tification. One straightforward approach is to select one methodfrom each category and combine them within a single framework.For instance, Bayesian neural network (BNN) models like Monte-Carlo dropout or ensemble models can be merged with predictiondistributions, where uncertainty can be obtained from the totalvariance of the prediction. However, such simple combinations of-ten introduce significant computational complexity. Alternatively,other methods have developed evidential deep learning , whichintegrates both data and model uncertainty within a single deter-ministic model using evidence theory. This approach offers compu-tational efficiency, but it requires the design of new optimizationstrategies and may not be suitable for certain network architectures.",
  "FUTURE DIRECTIONS4.1Spatial Sample Density and Nonstationarity": "As discussed in the introduction, one major challenge of spatiotem-poral data is spatiotemporal non-stationarity. The data distributioncan vary from one region (time) to another region (time). Suchphenomena can be characterized as out-of-distribution (OOD) dataor spatiotemporal outliers. Given a training data distribution (),the OOD data are those samples that are either unlikely underthe training data distribution or outside the support of (). Ac-curate detection of OOD samples is of paramount importance inspatiotemporal model generalizability. Because the space is con-tinuous and the boundary of individual homogeneous sub-regionsis implicit, the model needs to learn such spatial patterns in orderto quantify uncertainty due to nonstationarity. Another relevantsource of spaital uncertainty is due to sparse training samples in thegeographical space. This is often due to limited sensor observations.Traditionally, Gaussian process has been widely used to quantifysuch spatial uncertainty in continuous space. However, for deepneural network models, new techniques are needed that considersample density both in the non-spatial feature space and in thegeographic space.",
  "The goal of the imaging process is to reconstruct an unknown im-age from measurements, which is an inverse problem commonlyused in medical imaging (e.g., magnetic resonance imaging and": "X-ray computed tomography) and geophysical inversion (e.g., seis-mic inversion) . However, this process is challenging due to thelimited and noisy information used to determine the original im-age, leading to structured uncertainty and correlations betweennearby pixels in the reconstructed image . To overcome thisissue, current research in uncertainty quantification of inverse prob-lems employs conditional deep generative models, such as cVAE,cGAN, and conditional normalizing flow models . These meth-ods utilize a low-dimensional latent space for image generation butmay overlook unique data characteristics, such as structural con-straints from domain physics in certain types of image data, suchas remote sensing images, MRI images, or geological subsurfaceimages . The use of physics-informed models may improve un-certainty quantification in these cases. Its promising to incorporatethe physics constraints for quantifying the uncertainty associatedwith the imaging process.",
  "UQ for physics-aware DNN models": "Many applications in the field of spatiotemporal data minging in-volve physical systems that can be described using principles suchas partial differential equations (PDEs) governing diffusion pro-cesses. While deep learning has proven effective in extracting com-plex patterns from data, standard deep neural networks lack explicitincorporation of the underlying physics . To address thischallenge, physics-informed neural networks (PINNs) have beendeveloped to integrate physical principles and domain knowledgeinto deep learning for consistent predictions . PINNs encode thegoverning equations as residual losses to guide the optimizationof neural networks , enabling the incorporation of physicalconstraints during model training.However, many physical systems exhibit non-deterministic orunknown underlying principles. Chaotic and stochastic behavioris inherent in these systems, as exemplified by weather forecast-ing and climate prediction, where small perturbations can lead tosignificant differences in the predicted state (known as the \"but-terfly effect\") . Even with deterministic initial conditions, thegoverning equations (e.g., PDEs) driving the system may be sto-chastic, described by stochastic differential equations. Uncertaintyquantification is crucial for improving the reliability of predictions,especially under distribution shifts. The uncertainty in modelingphysical systems can arise from multiple sources. First, the initialand boundary conditions may be non-deterministic, and the systemitself may exhibit chaotic behavior . Second, the underlyingphysical principles may not be fully known, or the parameters ofthe governing equations may be stochastic, leading to violations ofconservation laws in imperfect systems .These cases highlight the presence of inherent data uncertaintyin stochastic physical systems. Probabilistic models offer a naturalway to model distributions and incorporate stochasticity and uncer-tainty into neural networks. However, quantifying uncertainty inPINNs poses specific challenges. It requires simultaneously consid-ering the physical principles and their uncertainties. Incorporatingphysical constraints can help mitigate data and model uncertainty. Various sources of uncertainty may arise, including random-ness in the physical system itself, measurement errors, and limitedknowledge of the governing equations. One potential approach to",
  "CONCLUSION": "In this paper, we provides a brief overview of UQ of deep learningfor spatiotemporal data, including its unique challenges and existingmethods. We particularly focus on the importance of uncertaintysources. We also identify several future research directions relatedto spatiotemporal data. Firoj Alam, Muhammad Imran, and Ferda Ofli. 2017. Image4act: Online socialmedia image processing for disaster response. In Proceedings of the 2017 IEEE/ACMinternational conference on advances in social networks analysis and mining 2017.601604.",
  "David M Blei, Alp Kucukelbir, and Jon D McAuliffe. 2017. Variational inference:A review for statisticians. Journal of the American statistical Association 112, 518(2017), 859877": "Reynold Cheng, Tobias Emrich, Hans-Peter Kriegel, Nikos Mamoulis, MatthiasRenz, Goce Trajcevski, and Andreas Zfle. 2014. Managing uncertainty in spatialand spatio-temporal data. In 2014 IEEE 30th International Conference on DataEngineering. IEEE, 13021305. Jiwoong Choi, Dayoung Chun, Hyun Kim, and Hyuk-Jae Lee. 2019. Gaussianyolov3: An accurate and fast object detector using localization uncertainty forautonomous driving. In Proceedings of the IEEE/CVF International Conference onComputer Vision. 502511. Garoe Dorta, Sara Vicente, Lourdes Agapito, Neill DF Campbell, and Ivor Simpson.2018. Structured uncertainty prediction networks. In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition. 54775485.",
  "Wenchong He and Zhe Jiang. 2023. A Survey on Uncertainty QuantificationMethods for Deep Neural Networks: An Uncertainty Source Perspective. arXivpreprint arXiv:2302.13425 (2023)": "Wenchong He, Zhe Jiang, Marcus Kriby, Yiqun Xie, Xiaowei Jia, Da Yan, and YangZhou. 2022. Quantifying and Reducing Registration Uncertainty of Spatial VectorLabels on Earth Imagery. In Proceedings of the 28th ACM SIGKDD Conference onKnowledge Discovery and Data Mining. 554564. Wenchong He, Arpan Man Sainju, Zhe Jiang, Da Yan, and Yang Zhou. 2022.Earth Imagery Segmentation on Terrain Surface with Limited Training Labels: ASemi-supervised Approach based on Physics-Guided Graph Co-Training. ACMTransactions on Intelligent Systems and Technology (TIST) 13, 2 (2022), 122.",
  "Zhe Jiang. 2018. A survey on spatial prediction methods. IEEE Transactions onKnowledge and Data Engineering 31, 9 (2018), 16451664": "Zhe Jiang, Wenchong He, Marcus Stephen Kirby, Arpan Man Sainju, ShaowenWang, Lawrence V Stanislawski, Ethan J Shavers, and E Lynn Usery. 2022. WeaklySupervised Spatial Deep Learning for Earth Image Segmentation Based on Im-perfect Polyline Labels. ACM Transactions on Intelligent Systems and Technology(TIST) 13, 2 (2022), 120. Zhe Jiang, Arpan Man Sainju, Yan Li, Shashi Shekhar, and Joseph Knight. 2019.Spatial ensemble learning for heterogeneous geographic data with class ambigu-ity. ACM Transactions on Intelligent Systems and Technology (TIST) 10, 4 (2019),125. Laurent Valentin Jospin, Hamid Laga, Farid Boussaid, Wray Buntine, and Mo-hammed Bennamoun. 2022. Hands-on Bayesian neural networksA tutorialfor deep learning users. IEEE Computational Intelligence Magazine 17, 2 (2022),2948.",
  "Alex Kendall and Yarin Gal. 2017. What uncertainties do we need in bayesiandeep learning for computer vision? Advances in neural information processingsystems 30 (2017)": "Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael WMahoney. 2021. Characterizing possible failure modes in physics-informed neuralnetworks. Advances in Neural Information Processing Systems 34 (2021), 2654826560. Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. 2017. Simpleand scalable predictive uncertainty estimation using deep ensembles. Advancesin neural information processing systems 30 (2017).",
  "Jos-Mara Montero, Gema Fernndez-Avils, and Jorge Mateu. 2015. Spatial andspatio-temporal geostatistical modeling and kriging. John Wiley & Sons": "Philipp Oberdiek, Gernot A Fink, and Matthias Rottmann. 2022. UQGAN: AUnified Model for Uncertainty Quantification of Deep Classifiers trained viaConditional GANs. arXiv preprint arXiv:2201.13279 (2022). Tim Pearce, Alexandra Brintrup, Mohamed Zaki, and Andy Neely. 2018. High-quality prediction intervals for deep learning: A distribution-free, ensembledapproach. In International conference on machine learning. PMLR, 40754084.",
  "Dieter Pfoser and Christian S Jensen. 1999. Capturing the uncertainty of moving-object representations. In International Symposium on Spatial Databases. Springer,111131": "Markus Reichstein, Gustau Camps-Valls, Bjorn Stevens, Martin Jung, JoachimDenzler, and Nuno Carvalhais. 2019. Deep learning and process understandingfor data-driven Earth system science. Nature 566, 7743 (2019), 195204. Shashi Shekhar, Zhe Jiang, Reem Y Ali, Emre Eftelioglu, Xun Tang, Venkata MVGunturi, and Xun Zhou. 2015. Spatiotemporal data mining: A computationalperspective. ISPRS International Journal of Geo-Information 4, 4 (2015), 23062338.",
  "Jian Sun, Kristopher A Innanen, and Chao Huang. 2021. Physics-guided deeplearning for seismic inversion with hybrid training and uncertainty analysis.Geophysics 86, 3 (2021), R303R317": "Joost Van Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin Gal. 2020. Uncer-tainty estimation using a single deep deterministic neural network. In Interna-tional conference on machine learning. PMLR, 96909700. Bin Wang, Jie Lu, Zheng Yan, Huaishao Luo, Tianrui Li, Yu Zheng, and GuangquanZhang. 2019. Deep uncertainty quantification: A machine learning approachfor weather forecasting. In Proceedings of the 25th ACM SIGKDD InternationalConference on Knowledge Discovery & Data Mining. 20872095."
}