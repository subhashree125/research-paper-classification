{
  "Abstract": "Large Language Models (LLMs) have greatly contributed to thedevelopment of adaptive intelligent agents and are positioned asan important way to achieve Artificial General Intelligence (AGI).However, LLMs are prone to produce factually incorrect informa-tion and often produce \"phantom\" content that undermines theirreliability, which poses a serious challenge for their deploymentin real-world scenarios. Enhancing LLMs by combining externaldatabases and information retrieval mechanisms is an effective path.To address the above challenges, we propose a new approach calledWeKnow-RAG, which integrates Web search and KnowledgeGraphs into a \"Retrieval-Augmented Generation (RAG)\" system.First, the accuracy and reliability of LLM responses are improvedby combining the structured representation of Knowledge Graphswith the flexibility of dense vector retrieval. WeKnow-RAG thenutilizes domain-specific knowledge graphs to satisfy a variety ofqueries and domains, thereby improving performance on factualinformation and complex reasoning tasks by employing multi-stageweb page retrieval techniques using both sparse and dense retrievalmethods. Our approach effectively balances the efficiency and accu-racy of information retrieval, thus improving the overall retrievalprocess. Finally, we also integrate a self-assessment mechanism forthe LLM to evaluate the trustworthiness of the answers it generates.Our approach proves its outstanding effectiveness in a wide rangeof offline experiments and online submissions.",
  "Computing methodologies Artificial intelligence; Machinelearning": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from Cup CRAG Workshop 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-XXXX-X/18/06",
  "Introduction": "Large language models (LLMs) have significantly propelled thedevelopment of adaptable intelligent agents, positioning themselvesas a promising pathway towards achieving artificial general intelli-gence (AGI). Despite these advancements, the inherent nature ofLLMs introduces critical challenges that obstruct their deploymentin real-world production scenarios . One of main key issuesis the tendency of LLMs to produce factually incorrect informa-tion, often resulting in \"hallucinated\" content that undermines theirreliability . demonstrated that GPT-4s exhibits excellentperformances in responding to questions about both slow-changingor fast-changing facts, with an accuracy rate below 15%. However,even for stable (never-changing) facts, GPT-4s accuracy in address-ing questions about less popular (torso-to-tail) entities is under 35%.Researchers have implemented a variety of methods to enhanceknowledge in LLMs, one of which is Retrieval-Augmented Genera-tion (RAG) method. RAG method enhances LLMs by incorporatingexternal databases and information retrieval mechanisms . Thistechnique dynamically incorporates relevant information into theLLMs prompt during inference without altering the weights ofthe used LLMs. By integrating external knowledge, RAG methodscan reduce hallucinations of LLMs achieving better performancethat can surpass traditional fine-tuning approaches, especially inapplications that require high precision and up-to-date information.Implementations of RAG methods typically depend on the densevector similarity search for retrieval. Yet, this technique, whichdivides the corpus into text chunks and uses dense retrieval systemsexclusively, falls short for complex queries . Some methodsaddress this issue by employing metadata filtering or hybrid search",
  "KDD Cup CRAG Workshop 24, August 2529, 2024, Barcelona, SpainXie et al": "techniques , but these methods are constrained by the pre-defined scope of metadata by developers. Furthermore, achievingthe necessary granularity to answer complex queries within similarvector space chunks remains challenging . The inefficiencycomes from the methods inability to selectively retrieve relevantinformation, leading to large amounts of chunk data retrieval thatmay not directly answer the queries .An ideal RAG system should retrieve only the essential content,reducing the inclusion of irrelevant information. This is whereKnowledge Graphs (KGs) can assist by providing a structured andexplicit representation of entities and relationships that are moreprecise than information retrieval through vector similarity .KGs enable searching for \"things, not strings\" by maintaining exten-sive collections of explicit facts structured as accurate, mutable, andinterpretable knowledge triples . A knowledge triple typicallyrepresents a fact in the format (entity) - relationship (entity). In addition, KGs can expand by continuously incorporatingnew information, and experts can build domain-specific KGs todeliver precise and trustworthy data within particular fields .Well-known KGs such as Freebase, Wikidata, and YAGO could serveas practical examples of this concept. Considerable researches haveemerged at the crossroads of graph-based methodologies and LLMs,showcasing applications like reasoning over graphs and enhancingthe integration of graph data with LLMs .To tackle the above challenges, we propose a novel approachnamed WeKnow-RAG that integrates Web Search and KnowledgeGraphs in a Retrieval-Augmented Generation (RAG) system. Ourapproach combines the structured representation of KGs with theflexibility of dense vector retrieval to enhance the accuracy andreliability of LLM responses.The main contributions of our work are as follows:",
  "We present an adaptive framework that intelligently com-bines KG-based and web-based RAG methods based on thecharacteristics of different domains": "The proposed WeKnow-RAG won 3rd place in the final evalua-tion of Task 3 at the Meta KDD CUP 2024, which was assessed usingthe Comprehensive RAG Benchmark (CRAG), a factual question-answering benchmark consisting of numerous question-answerpairs and mock APIs to simulate web and Knowledge Graph search.Our experimental results on the CRAG dataset demonstrate theeffectiveness of our approach, achieving significant improvementsin accuracy and reducing hallucinations across various domainsand question types.",
  "Related Work": "In order to improve the accuracy and reliability of LLMs in questionanswer tasks, many previous works have proven effective. Accord-ing to whether the parameters of LLMs to be modified or not, wecategorize them into two main approaches : Fine-tuning and calibration: Fine-tuning the LLM on aspecific domain or task can improve its accuracy and reducehallucinatory responses. In addition, calibrating the model toprovide uncertainty estimates with responses can help usersassess the reliability of the generated information. Thisapproach requires modifying the parameters of the LLMs. External Knowledge Integration: Integrating externalknowledge sources into LLM can help enhance its compre-hension and reduce hallucinatory responses. These sourcescan be massively updated web pages, databases, knowledgegraphs, additional LLMs, etc. RAG is a method of utiliz-ing many knowledge sources to provide informed answers,thereby improving the accuracy of LLMs generated responses.This approach does not require modification of the parame-ters of the LLMs.",
  "External Knowledge Integration": "Recent endeavors have leveraged RAG to improve LLMs acrossdiverse tasks , particularly those requiring up-to-date andaccurate knowledge such as question answering (QA), AI4Science,and software engineering. For instance, Lozano et al. devel-oped a scientific QA system that dynamically retrieves scientificliterature. MolReGPT uses RAG to boost ChatGPTs in-contextlearning for molecular discovery. Moreover, RAG has been shown toeffectively mitigate hallucinations in conversational tasks .",
  "WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation ...KDD Cup CRAG Workshop 24, August 2529, 2024, Barcelona, Spain": "knowledge base plays a key role in various question-answeringtasks, drawing inspiration from the construction of large knowledgebases such as DBpedia. By parsing a question into step-by-stepsub-questions or sub-functions, establishing the relationship be-tween the sub-questions and the model application programminginterface (API), and solving a series of sub-questions using condi-tional lookups, set searches, comparisons, aggregations, multi-hopqueries, and post-processing operations, the entire question can beanswered. 4.2.1Domain classification. Our method categorizes the questioninto domains by making an initial call to an LLM. At this point,the LLM acts as an intelligent linguistic categorization tool. Forexample, for the question, \"How much did Funko open at today?\",it is automatically categorized as the financial domain. For thequestion, \"What label is Taylor Swift signed to?\", it is automaticallycategorized as the music domain. Domain categorization is criticalfor subsequent processing. For this reason, in the four domains ofmovie, sport, finance, and music, we suggest that the model shouldonly make a determination if it has more than 90% certainty. Whencertainty is below this threshold, the domains are categorized asopen domains. 4.2.2Query generation. Depending on the domain categorizationof the problem, the corresponding domain cueing phase is entered,where the second call to the LLM is made. The hints provided aredifferent for each domain, and the model needs to return structuredanalysis results. For example, in the music domain, it needs to returnthree kinds of functions, categorized into artist, song, and year,with each category containing a variety of attribute information.For the instruction to generate the function calling query, refer toAppendix A.2. Simultaneously, questions that do not fall into thesethree categories are answered directly using the model or the RAGworkflow. The query generator will transform the analysis resultsinto structured queries compatible with the KG API. For example,a multi-hop problem will be decomposed into a series of API calls,each of which resolves a link in the information chain. 4.2.3Answer Retrieval and Post-processing. A set of candidate an-swers is retrieved by performing a structured query on the KGthrough an application programming interface (API). For post-processing problems, additional reasoning is applied. We employa rule-based system that utilizes machine learning techniques tohandle temporal reasoning, numerical computation, and logicalreasoning . With this hybrid approach, we can identify incon-sistencies between the retrieved data and the problem hypotheses,effectively solving challenging false premise problems.",
  "Preliminaries": "A RAG QA system takes a question as input and outputs an an-swer ; the answer is generated by LLMs according to informationretrieved from external sources or directly from the knowledgeinternalized in the model . The answer should provide usefulinformation to answer the question without adding any hallucina-tions under ideal conditions.Task description The process to obtain an answer from a ques-tion can be called a query. For each question , both web searchresults and Mock APIs for information retrieval are available forthe generation of answer . The web search results comprise 50web page candidates. The Mock Knowledge Graphs (MKGs) containstructured data pertinent to the queries; the answers may or maynot be present in the MKGs. The Mock APIs accept input parame-ters, which are often derived from a query, and deliver structureddata from the MKGs to assist in answer generation.RAG systems are evaluated using a scoring method that ratesresponse quality as correct (1 point), missing (0 points), and incor-rect (-1 point). A response is rated as missing if it is I dont know.Otherwise, an LLM will be used to determine whether the responseis correct or incorrect .Dataset description The dataset used is the ComprehensiveRAG Benchmark, covering five domains: finance, sports, music,movie, and open domain, and eight types of questions as detailed in. Specifically, CRAG includes questions with answers rangingfrom seconds to years; it considers the popularity of entities andcovers not only head but also torso and tail facts. It contains sim-ple factual questions as well as seven types of complex questions,including comparison, aggregation, set questions and so on, to testthe reasoning and synthesis capabilities of RAG solutions.For the web search results, we used the question text as thesearch query and stored up to 50 web pages retrieved from theBrave search API. Each web page contains some key-value pairs,detailed in the following data schema:",
  "Web-based RAG": "4.1.1Web Content Parsing. To utilize the data source in RAG ap-proaches, content parsing is a critical process. It helps to compre-hend unstructured data, convert it into structured data, and acquirethe necessary information to answer the question.To obtain more complete content for the source page, we parsethe original HTML source code with the BeautifulSoup library1. 4.1.2Chunking. Chunking is the process of dividing a documentinto multiple paragraphs. The effectiveness of chunking directlyinfluences the performance of question-answering systems.Various chunking strategies exist, including sentence-level, token-level, and semantic-level approaches . In our methods, we optfor token-level chunking for ease of use.To determine the optimal configurations, a series of experimentsare carried out to compare various chunk sizes in .3. 4.1.3Multi-stage Retrieval. In the context of RAG, efficiently re-trieving relevant documents from the data source is crucial forobtaining accurate answers and minimizing hallucinations.We employ a multi-stage retrieval approach, as illustrated in, to strike a balance between the effectiveness and efficiencyof retrieving relevant paragraphs based on a query.The explanation provided in effectively outlines the ini-tial stage of utilizing sparse retrieval to gather candidates from pageresult chunks and snippet chunks. It clarifies that sparse retrieval isemployed to efficiently obtain pertinent paragraphs. Additionally,it highlights the selection process of the top candidates based ontheir BM25 scores, as denoted in Eq. 1.",
  ": Multi-stage Retrieval methods": "the second stage, We employ hybrid search using the most commonpattern , which combines a sparse retriever (BM25) with a denseretriever (embedding similarity), as their strengths are complemen-tary. The sparse retriever excels at finding relevant documentsbased on keywords, while the dense retriever identifies relevantdocuments based on semantic similarity.The sparse retrieval approach in the second stage uses the BM25score, just like in the first stage, to select the top candidates basedon the highest score.The dense retrieval approach in second stage comprises denseembedding retrieval and reranking methods. A bge-large-en-v1.5 embedding model is employed for dense embedding retrieval,selecting the top candidates based on the highest embeddingsimilarity with the query. Subsequently, bge-reranker-large reranker model is applied to obtain a more accurate relevance scoreand rerank the top candidates in the reranking approach of . Notice that >> + , which allowed us to obtain K relevantcandidates in the first stage with low latency. This enabled us to con-duct dense retrieval to obtain N relevant candidates and rerankingusing models with more parameters (such as the bge-reranker-largemodel, which has 0.5B parameters) to obtain M relevant candidateswith fewer data in the second stage.It is worth mentioning that to include as much relevant informa-tion as possible, we added not only the parsed HTML content inthe database but also the page name and page snippet for retrieval. 4.1.4Answer Generation with Self-Assessment. Hallucination isa common issue in the generated content of LLMs. In order toaddress this challenge, we propose a self-assessment mechanismthat evaluates the confidence level of the generated answer anddetermines its suitability for selection.In particular, we instruct the LLM to indicate the confidencelevel (high, medium, low) corresponding to the generated answer.We decide to accept the answer only when the confidence levelmeets the specified requirement. If the confidence level is below thethreshold, we conclude that the LLM lacks sufficient confidence toanswer the question and will output \"I dont know\". Experiments onperformance with various confidence level thresholds are detailed in.3, and the complete instructions for the LLM are providedin Appendix A.1.",
  "Integrated method": "The diverse nature of the CRAG dataset requires a dynamic ap-proach that adapts to the distinct characteristics of each domain,particularly in terms of the velocity at which query informationevolves. To address this, we characterize the time distribution ofeach domain, by analyzing the key \"static-or-dynamic\" defined inthe dataset, which are categorized as \"static\", \"slow-changing\", \"fast-changing\" and \"real-time\", and then propose an adaptive frameworkthat intelligently balances the use of Knowledge Graphs (KGs) andWeb-based RAG methods. For stable domains such as the Encyclopedia Open domain,where the velocity of query information change is minimal, oursystem follows the rule of prioritizing the output of KG Workflowand not activating the whole Web-based RAG Workflow, as demon-strated in . The robustness and reliability of KGs in thesedomains ensure high accuracy for questions that do not necessitateup-to-the-minute data. This approach aligns with the findings ofNeumaier et al., who underscore the effectiveness of KGs instable informational contexts.For domains with gradual information change, such as Musicand Movies, we maintain the primacy of KGs while incorporatingperiodic updates to capture the latest information. This strategyensures that our KGs remain relevant for answering queries thatmay involve recent but not instantaneous changes. The updatefrequency is determined by a domain-specific change detectionalgorithm, which is controlled the LLM.",
  "Experiments5.1Experimental settings": "We thoroughly introduce our experimental settings, encompassingthe datasets, evaluation metrics, and parameter settings.Datasets. The entire CRAG dataset contains over 2000 questions,and testing on the whole dataset would be very time-consuming.During offline testing, to quickly iterate and optimize, we tested ona subset of the CRAG dataset containing 200 questions.Evaluation metrics. For each question, we use a three-wayscoring system, assigning 1 for correct answers, -1 for incorrectanswers, and 0 for missing answers. An answer is considered ac-curate if it exactly matches the ground truth and missing if it is Idont know. Otherwise, we use model-based automatic evaluationwith GPT-4 to determine whether the response is correct or incor-rect. There are four evaluation metrics: Accuracy, Hallucination,Missing, and Score. Accuracy, Hallucination, and Missing repre-sent the percentage of accurate, incorrect, and missing answersin the test set, respectively, while Score is the difference betweenAccuracy and Hallucination.Parameter settings. We set the number of retrieving relevantcandidates in the first stage of Web-based RAG as 200 for parsedpage result chunks and page snippet chunks, respectively. The num-ber of sparse retrieval candidates is set as 5, and the numberof dense retrieval candidates is set as 20 in the second stage. ,, are selected empirically and include correct answer informa-tion whenever possible. We employed the bge-large-en-v1.5 asthe dense retrieval model, bge-reranker-large as the rerankermodel, and llama-3-70b-instruct-awq (an awq quantization versionof Meta-Llama-3.1-70B-Instruct) as our Chat LLM model. All ofthem can be downloaded from the Hugging Face Hub2.",
  "Single domain0.1000.0350.8650.0640+ Four domains0.1250.0250.8500.1000+ Classification0.3400.1900.4700.1499+ Open Optimize0.3400.1850.4750.1550": "including examples of such queries in the prompts. Third, the hintsrelated to opening, closing, and price were adjusted. Overall, thestrategy of the entire method involves first using the KG processto directly find answer-related information, strictly requiring theKG part to answer only questions with certainty to significantlyreduce the error rate. The RAG process is then used to synthesizethe information provided by the KG or from web pages to deliverthe final answer to the question.The competition accepted a limited number of submissions. Tothoroughly validate the methods enhancement, a local test setcomprising 200 data points was designed offline, primarily focusingon validating the efficacy of the KGs component. The methodsefficacy is underscored in . Initially, a classification promptwas devised for accurate categorization by the LLM, followed bya domain-specific prompt for movies, resulting in a base score of0.064. Progressing to four domains and refining the strategy ledto consistent improvement. While the KGs for individual domainswere well-crafted, a lack of robust classification capability posed asignificant constraint on further enhancement. To address this, anillegal problem optimization was integrated into the classificationprompt, ensuring the models classification into the sports, movies,music, and finance domains with a reasonable degree of certainty,boosting the score to 0.1499. Subsequent refinements focusing ondetailed optimization for the finance domain and open domainsfurther enhanced the performance.",
  "Ablation study": "To enhance performance in each phase, a series of experiments arecarried out using a subset of the CRAG dataset.Chunk size. In the chunking process of RAG, the size of thechunk plays a critical role in obtaining semantically complete para-graphs. We conducted experiments with various chunk sizes asillustrated in . The results indicate that a chunk size of 750yields the best performance in a small subset of CRAG dataset. InRound 1 submissions, we experimented with chunk sizes of 500and 750. After comparing the results, we determined that a chunksize of 500 yielded better outcomes. Therefore, we have decided tostick with a chunk size of 500 for the final submission.",
  "Model analysis": "The experimental results above demonstrate that the WeKnow-RAGapproach we proposed achieves effective performance by leveragingthe contribution of each module in the Web-based RAG workflowand KG workflow.KG workflow provides accurate answers with minimal errors byusing function calling to extract specific information from knowl-edge graphs.While the web-based RAG workflow provides more relevantinformation from a large number of web pages through multi-stageretrieval, it also reduces hallucination through a self-assessmentapproach.",
  "Conclusions": "While LLM has shown promising applications across various do-mains, highlighting substantial development opportunities and re-search significance, its intrinsic traits can lead to outputs that devi-ate from factual accuracy, generating \"hallucinatory\" content thatposes a significant obstacle to research advancement. To addressthe above challenges, we propose a new approach called WeKnow-RAG that integrates Web search and Knowledge Graphs into aRAG system. Our approach combines the structured representationof knowledge graphs with the flexibility of dense vector retrievalto improve the accuracy and reliability of LLM responses. By de-signing a specialized RAG system, WeKnow-RAG utilizes domain-specific knowledge graphs to satisfy a wide range of queries anddomains, improving the performance of factual information andcomplex reasoning tasks. By employing multi-stage web page re-trieval techniques, both sparse and dense retrieval methods areused. Our approach effectively balances the efficiency and accu-racy of information retrieval, thus enhancing the entire retrievalprocess. Meanwhile, we integrate a self-assessment mechanismfor LLMs to evaluate the trustworthiness of the answers they gen-erate. This mechanism reduces illusions and thus improves thequality of model-generated answers.The WeKnow-RAG frameworkintroduces an adaptive methodology that intelligently combinesKG-based RAG methods with web-based RAG methods. This inte-gration is tailored to the unique characteristics of different domains",
  "Nurendra Choudhary and C. Reddy. 2023. Complex Logical Reasoning overKnowledge Graphs using Large Language Models. ArXiv abs/2305.01157 (2023)": "Jens Drpinghaus and Marc Jacobs. 2020. Knowledge detection and discoveryusing semantic graph embeddings on large knowledge graphs generated ontext mining results. In 2020 15th conference on computer science and informationsystems (fedcsis). IEEE, 169178. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi,Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2024.Retrieval-Augmented Generation for Large Language Models: A Survey.arXiv:2312.10997 [cs.CL] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, HaotianWang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and TingLiu. 2023. A Survey on Hallucination in Large Language Models: Principles,Taxonomy, Challenges, and Open Questions. arXiv:2311.05232 [cs.CL]",
  "Xu Jing and Szlam Arthur. 2021. Beyond goldfish memory: Long-term open-domain conversation. arXiv preprint arXiv:2107.07567 (2021)": "Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: ALarge Scale Distantly Supervised Challenge Dataset for Reading Comprehension.In Proceedings of the 55th Annual Meeting of the Association for ComputationalLinguistics (ACL). 16011611. Weize Kong, Swaraj Khadanga, Cheng Li, Shaleen Kumar Gupta, MingyangZhang, Wensong Xu, and Michael Bendersky. 2022. Multi-aspect dense retrieval.In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery andData Mining. 31783186. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, et al. 2019. NaturalQuestions: a Benchmark for Question Answering Research. Transactions of theAssociation for Computational Linguistics 7 (2019), 453466. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,Naman Goyal, Heinrich Kttler, Mike Lewis, Wen-tau Yih, Tim Rocktschel,et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks.Advances in Neural Information Processing Systems 33 (2020), 94599474. Jiatong Li, Yunqing Liu, Wenqi Fan, Xiao-Yong Wei, Hui Liu, Jiliang Tang, andQing Li. 2024. Empowering molecule discovery for molecule-caption transla-tion with large language models: A chatgpt perspective. IEEE Transactions onKnowledge and Data Engineering (2024). Alejandro Lozano, Scott L Fleming, Chia-Chun Chiang, and Nigam Shah. 2023.Clinfo. ai: An open-source retrieval-augmented large language model system foranswering medical questions using scientific literature. In PACIFIC SYMPOSIUMON BIOCOMPUTING 2024. World Scientific, 823. Y. Luan, Jacob Eisenstein, Kristina Toutanova, and M. Collins. 2020. Sparse,Dense, and Attentional Representations for Text Retrieval. Transactions of theAssociation for Computational Linguistics 9 (2020), 329345.",
  "Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021.Retrieval augmentation reduces hallucination in conversation. arXiv preprintarXiv:2104.07567 (2021)": "Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, YeyunGong, Heung-Yeung Shum, and Jian Guo. 2023. Think-on-graph: Deep andresponsible reasoning of large language model with knowledge graph. arXivpreprint arXiv:2307.07697 (2023). Kai Sun, Yifan Ethan Xu, Hanwen Zha, Yue Liu, and Xin Luna Dong. 2023. Head-to-tail: How knowledgeable are large language models (llm)? AKA will llmsreplace knowledge graphs? arXiv preprint arXiv:2308.10168 (2023). Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, ChrisTar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, et al. 2023. Freshllms: Refresh-ing large language models with search engine augmentation. arXiv preprintarXiv:2310.03214 (2023). Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu, Zhibo Xu,Tianyuan Shi, Zhengyuan Wang, Shizheng Li, Qi Qian, et al. 2024. Search-ing for Best Practices in Retrieval-Augmented Generation.arXiv preprint",
  "Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023.C-Pack: Packaged Resources To Advance General Chinese Embedding.arXiv:2309.07597 [cs.CL]": "Linyao Yang, Hongyang Chen, Zhao Li, Xiao Ding, and Xindong Wu. 2023.Chatgpt is not enough: Enhancing large language models with knowledge graphsfor fact-aware language modeling. arXiv preprint arXiv:2306.11489 (2023). Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, SajalChoudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, Lingkun Kong, BrianMoran, Jiaqi Wang, Yifan Ethan Xu, An Yan, Chenyu Yang, Eting Yuan, HanwenZha, Nan Tang, Lei Chen, Nicolas Scheffer, Yue Liu, Nirav Shah, Rakesh Wanga,Anuj Kumar, Wen tau Yih, and Xin Luna Dong. 2024. CRAG ComprehensiveRAG Benchmark. arXiv:2406.04744 [cs.CL]",
  "Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji,and Meng Jiang. 2022. A survey of knowledge-enhanced text generation. Comput.Surveys 54, 11s (2022), 138": "Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang,Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. 2023. Instruction Tuning forLarge Language Models: A Survey. arXiv preprint arXiv:2308.10792 (2023). Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, XintingHuang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. 2024. Sirens song in the AIocean: A survey on hallucination in large language models, 2023. URL (2024).",
  "The prompt for Web-based RAG to generate an answer and thecorresponding confidence level is displayed below": "System:You are provided with a question, current time and variousreferences. Your task is to answer the question succinctly, usingthe FEWEST words possible. If you are absolutely sure, morethan 97\\% confident, please answer directly. If you are not sure,please respond with 'I don't know'.Please answer the question and provide the confidence tier (high,medium, low) for your answer. Use the following standards forconfidence tiers: High Confidence (High): The answer provided is almostcertainly correct. There is strong evidence or overwhelmingconsensus supporting this answer. The model has a high level ofcertainty and little to no doubt about this answer. Medium Confidence (Medium): The answer provided is likelyto be correct. There is some evidence or reasonable support forthis answer, but it is not conclusive. The model has some level ofcertainty but acknowledges that there is a possibility of error oralternative answers. Low Confidence (Low): The answer provided is uncertain orspeculative. There is little to no solid evidence or support for thisanswer. The model has significant doubts about the accuracy ofthis answer and recognizes that it could easily be incorrect."
}