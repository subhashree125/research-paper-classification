{
  "Abstract": "Hallucination is a key roadblock for applications of Large LanguageModels (LLMs), particularly for enterprise applications that are sen-sitive to information accuracy. To address this issue, two generalapproaches have been explored: Retrieval-Augmented Generation(RAG) to supply LLMs with updated information as context, andfine-tuning the LLMs with new information and desired outputstyles. In this paper, we propose Honest AI: a novel strategy tofine-tune \"small\" language models to say \"I dont know\" to reducehallucination, along with several alternative RAG approaches. Thesolution ranked 1st in Task 2 for the false premise question1. Thealternative approaches include using RAG with search engine andknowledge graph results, fine-tuning base LLMs with new informa-tion and combinations of both approaches. Although all approachesimprove the performance of the LLMs, RAG alone does not signifi-cantly improve the performance and fine-tuning is needed for betterresults. Finally, the hybrid approach achieved the highest score inthe CRAG benchmark . In addition, our approach emphasizesthe use of relatively small models with fewer than 10 billion param-eters, promoting resource efficiency.",
  "Large Language Models (LLM), Retrieval Augmented Generation(RAG), Knowledge Graph, Search": "ACM Reference Format:Xinxi Chen, Li Wang, Wei Wu, Qi Tang, and Yiyao Liu. 2024. Honest AI:Fine-Tuning \"Small\" Language Models to Say \"I Dont Know\", and ReducingHallucination in RAG. In Proceedings of 2024 KDD Cup Workshop for RetrievalAugmented Generation at the 30th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining (KDDCup 24). ACM, New York, NY, USA, 8 pages. These authors contributed equally to this work. Emails are provided for contactpurposes only and do not represent an endorsement by any institution.1The Meta CRAG Challenge has over 2000 participants with over 5500 submissions. Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, Aug 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",
  "Introduction": "Large Language Models (LLMs), as a type of foundation models withgeneral language capabilities, have eclipsed traditional Natural Lan-guage Processing (NLP) models that focus on specific tasks in major-ity of NLP applications since the inception of GPT . Supervisedfine tuning (SFT) using labeled data and reinforcement learningfrom human feedback (RLHF) using preference data have proveneffective in further enhancing LLMs performance and alignment(e.g., ChatGPT for question answering applications). However,LLMs suffer from hallucination, which hinders their application inaccuracy-sensitive scenarios, such as the enterprise applications.To alleviate the hallucinations of LLMs, several approaches havebeen proposed, including retrieval-augmented generations (RAGs) and fine-tuning with domain-specific knowledge. In this paper,we summarize the approaches we tried in the 2024 Meta KDD Cupcompetition with Comprehensive RAG Benchmark (CRAG) data. Since the CRAG benchmark focuses on difficult problems forLLMs, vanilla LLMs fail to perform well right out of the box. It turnsout that RAG alone is not enough to alleviate hallucination in thebenchmark and fine-tuning is needed to achieve higher accuracy.Our results show that the hybrid approach using both RAG and finetuning performs best in CRAG. With those approaches combined,our team, Team Future, ranked high in each task of the competitionand won first place in the false premise question in Task 2 ().The review of related work is summarized in section 2, followedby a description of methodologies in section 3. The results areshown in section 4, with conclusions and future works discussedin section 5 and 6.",
  "Related Work": "LLMs are very good at memorizing the content used for their pre-training. However, there are many drawbacks to using the mem-orization capability directly for Question Answering (QA) tasks.For example, depending on the size of the model, the quality of thepretraining data, and the type of questions, LLMs memorizationcapability can be very limited and difficult to control. LLMs arealso challenging to update except through retraining or fine-tuning,so they cannot handle questions on recent events if deployed. Ar-guably, the most problematic drawback of using LLMs for QA isthat they can hallucinate, especially when the models are unsure ifthe context contains the information needed to answer a question.Fine-tuning is a straightforward way to update the knowledgeof LLMs when new information becomes available. However, dueto the scarcity of GPUs and limited access to high-quality data, itis not feasible for many use cases. Furthermore, the behavior offine-tuning language models is not well studied, and fine-tuningwith poor data or practices might decrease the models capabilities,causing modality collapse unless more complex methods such asRLHF are used .Retrieval-Augmented Generation (RAG) is a popularmethod to address the shortcomings of LLMs, by augmenting themwith non-parametric data sources, and leveraging LLMs powerfulin-context learning capability. grouped the approaches ofusing RAG for LLMs into three categories: Naive RAG, AdvancedRAG , and Modular RAG . Naive and Advanced RAGapproaches are widely used in practice due their simplicity andlow development cost. These approaches generally consist of threeparts: curating non-parametric databases, retrieving relevant snip-pets from the databases given the query, and generating responsesusing LLMs through in-context learning and prompt engineeringwith the related snippets.While the research on combining LLMs and RAG for QA mainlyfocuses on text, there is also research exploring the use of resourcesbeyond text, such as images , audio , video , and code to enhance the capabilities of language models.There are various efforts to create RAG benchmarks and propos-ing appropriate evaluation metrics in recent years. is one of therecent ones, which forms the foundation of this paper. createda factual question answering benchmark of 4,409 question-answerpairs and mock APIs to simulate web and Knowledge Graph (KG)searches. It also proposes an evaluation mechanism that distin-guishes between hallucinations and missing answers, and assignsa higher penalty to hallucinations. created a RAG evaluationbenchmark in both English and Chinese, and analyzed differentLLMs from 4 aspects: noise robustness, negative rejection, informa-tion integration, and counterfactual. They found that LLMs demon-strate a certain degree of noise robustness, but struggle significantlyin other aspects.Apart from specific RAG datasets, there are many existing QAdatasets that include context passages for each question. Thesedatasets can also be used for RAG experiments, and cover a widerange of questions such as multiple-choice QA , single-hop QA , multi-hop QA , and domain-specificQA .",
  "Dataset": "The CRAG dataset includes five question domains (e.g. movie,sports, and etc.) with varying levels of complexity, ranging fromstraightforward facts to those requiring reasoning (e.g. false premiseand multi-hop reasoning) . It also considers facts with differentlevels of timeliness (e.g. real-time, fast-changing, slow-changing,and stable). illustrates the distribution of question typesacross different domains and timeliness categories. Notably, thecharacteristics of question distributions in the movie and financedomains differ significantly. Real-time and fast-changing questionsnecessitate access to relevant, up-to-date data sources and effec-tive RAG implementations. In contrast, the movie domain containsmore static questions than the finance domain, which may be easierto address.",
  ": The Distribution of Questions in Different Domainsand Types of Timeliness": "Our solution ranked first in Task 2 for false premise questions. Afalse premise question is defined as a question with a false assump-tion. For example, \"Whats the name of Taylor Swifts rap albumbefore she transitioned to pop?\" This is a false premise question be-cause Taylor Swift didnt release any rap albums, and the expectedanswer is \"invalid question\" .In this competition, we chose a divide-and-conquer approachto design a RAG system to tackle different types of questions. Weassumed that categorizing these questions could be done easily inpractice. The objective is to identify the key parametric differences,such as varying prompts, parsing techniques, chunking sizes, top-kthresholds, and rule designs, to optimize performance and achievea higher score during implementation.",
  "RAG": "RAG is a popular approach to alleviate the hallucinations ofLLMs. There are various architectures for RAG in real applications,including advanced RAG techniques. In this paper, we exploredseveral naive RAG approaches. Generally, naive RAG selects thehighest cosine similarity results from a vector database and suppliesthe context as inputs for LLMs. There are multiple ways to findthe relevant information to implement RAG. We tried using naivecosine similarity and using LLMs as retrieval and ranking modelsto find relevant information for each web page.Furthermore, as an extreme case, we also tried using the state-of-the-art Gemini 1.5 pro model with a 1 million token contextwindow to supply all the retrieved web pages to LLMs. Thesemodels with long context windows are promising because theyeliminate the need to truncate information from retrieved results.However, with vanilla Gemini 1.5 pro and raw retrieved web pages,we achieved similar results like vanilla RAG with severe hallucina-tion, which is disappointing. Therefore, we didnt further investi-gate more advanced RAG techniques. An example of hallucinationis shown in Appendix A.2.These results for RAG are summarized in .",
  "Fine-tuning": "Our initial investigation revealed that answers in most categoriesare very challenging, and the evaluation metric heavily penalizeshallucinations. Therefore, it is better for the model to be \"honest\"about its limits by replying \"i dont know\" in cases of uncertainty,rather than providing wrong answers. However, it is non-trivialto assess LLMs confidence level reliably . Instead, we hypoth-esize that by explicitly teaching LLMs to reply \"i dont know\" tochallenging questions while providing real answers for easy ques-tions, LLMs may be able to learn the ability to distinguish betweenchallenging and easy questions.To test this hypothesis, we decided to use the QLoRA tech-nique to fine-tune the Llama-2-7b-chat in 4-bit precision andoptimize VRAM usage, due to limited GPU resources. More specifi-cally, as it is shown in , we used the training data providedby the organizer, reserved 250 instances for testing, and made afew modifications to the rest of the data. If the question_type iscomparison or false_premise, or the answer is \"yes\", \"no\", \"true\"or \"false\", we do not modify the answers to the questions; otherwise,we replace the original answer with \"i dont know\". Then we usedthe modified data to fine-tune the model.We use Alpha=16, r=64, and Dropout=0.1 for QLoRA. Addition-ally, we use a batch size of 8, a learning rate of 0.0002, a weightdecay of 0.001, and fine-tune the model for 5 epochs. We evalu-ate the fine-tuned model on the 250 withheld questions, using theoffline evaluation script provided by the organizer.We also experimented with Meta-Llama-3-8B-Instruct, whichperformed consistently worse.",
  "Hybrid Approach": "With the fine-tuned model mentioned in the above section, weachieved high rankings in the competition, and first place for thefalse premise question type in Task 2: Knowledge Graph and WebRetrieval. However, the method did not fully utilize the additionalknowledge from the web search results and the knowledge graph.To improve the results, we developed the following hybrid approachto better utilize the knowledge in certain domains.The hybrid approach leveraged the benefits of both the RAGmodel and the Fine-tuned Question Type model (). The hybridapproach first utilized the vanilla Meta-Llama-3-8B-Instruct toserve as the RAG model to generate results that include domain andanswer information. The next step is to determine if the domainbelongs to the movie domain, which has a lower level of halluci-nation based on the CRAG benchmark results . If it is in themovie domain and the answer is valid, the answer is used as thefinal answer. If the answer is invalid, the question is sent to theFine-tuned Question Type Model, which is good at answering falsepremise questions, and more modest by answering \"i dont know\"for other types of questions. Specifically, an answer is considered\"invalid\" in two scenarios: when the response is \"invalid question,\"or when there is a JSON processing error.For the input of the RAG model, we also applied a Pruner toextract the top k sentences from the web search results. This prunercomputes the cosine similarity of each sentence Wij in the docu-ment with the question Qi, after converting them to Sentence-BERTembeddings . For each top sentence, the following m sentences in the paragraph are appended to the context to enrich the infor-mation. To ensure content quality, an additional cosine similaritythreshold n is applied, and the answer is used only if this thresholdis met for top k sentences.",
  "QiWij(1)": "Based on offline evaluation, this approach improved the totalscore from 0.073 to 0.86 using results from 300 samples, comparedwith the Fine-tuned Question Type model. Since our Fine-tunedQuestion Type model achieved a score of 0.0960 in the online eval-uation for Task 3 (without the holdout test), this hybrid approach isexpected to achieve a higher score. It has not been evaluated onlineyet, because the online evaluation system was closed after phase 2.In addition, the prompt in Appendix A.1 is used to get the domainname of the question in a JSON output, which can be used for easyprocessing.",
  "Results": "We firstly tested the Llama3 8b pretrained model with or withoutRAG and prompt tuning in the domain of movie vs. finance andtypes of questions of simple vs. post-processing and multi-hop. Theresults of which are shown in . Each test case ran 100 samplesout of the 2.7k samples in CRAG. Surprisingly, we observed thatadding more detailed instructions in the prompt actually droppedthe overall performance significantly on the Llama3 8b pretrainedmodel. One hypothesis is that the models performance is highlysensitive to the format of prompting and needs to be properlyconfigured and fine-tuned.Given that the retrieval might not be correct, we also tried Gemini1.5 pro with a 1 million token context window as a long contextwindow models to see if feeding all the retrieved information to theLLMs would perform better than any RAG approaches. The resultsshow no improvement, and we didnt further investigate on this. shows the overall results from our fine-tuned model,which achieved 0.096 with 323 samples from the online judgingsystem. With this model, we achieved the highest score in Task 2for the false premise problems (). Finally, our hybrid approachresults with cosine similarity threshold of 0.75 show that the scoreimproved by 0.013 from 0.073 to 0.086 () with fine-tuningand RAG combined. The accuracy increased by 0.026 and there wasa slight increase in hallucination by 0.013.Furthermore, 4 shows some key examples of differences in predic-tions comparing the fine-tuned only model and the hybrid approach.For the first false premise question \"when did hamburg become thebiggest city of germany\", both models provide the answer \"invalidquestion\", since the largest city in Germany is Berlin. For the secondquestion which is a comparison question, both models also providethe same correct answer. For the third question related to movie do-main, the fine-tuned model responds with \"i dont know\", while thehybrid approach provides \"inception\" which is a correct answer. Forthe forth question related to movie domain, the fine-tuned modelprovides \"i dont know again\", while the hybrid approach providea relevant answer including who are professionals relevant to the 1The prompt tuning strategies detailed in predominantly follow instructionssuch as: answer the question given the context, and regulate the answer format. Promptexamples: answer \"i dont know\" directly, if ..., or \"invalid question\", if ...",
  "Discussion": "We tried various approaches to alleviate the hallucinations of LLMs,including Retrieval-Augmented Generations (RAGs) and fine-tuningwith domain-specific knowledge, while participating in the 2024Meta KDD Cup with Comprehensive RAG Benchmark (CRAG) data.It turns out that RAG alone is not enough to perform well in thebenchmark, and fine-tuning is needed to achieve a higher score.Our results show that the hybrid approach using both RAG andfine-tuning performs best in CRAG.For the basic RAG approach, we are uncertain whether themodels accurate answers stem from the pretrained models priorknowledge or the retrieved data from the reference. At first glance,it appears that basic RAG does not significantly improve the finalscore. One possible reason is that the retrieved content, while rel-evant (cosine similarity score > 0.7), might not be useful becauseit is too basic, general, or vague. Additionally, improper prompt-ing significantly reduces accuracy compared to using no prompts,likely because the pretrained model is misled by the instruction and loses its own prior knowledge to answer the question. More ad-vanced RAG and prompt design should be tested to draw definitiveconclusions.With further improvement in the hybrid RAG approach, focusingon the movie domain, it is clear that better quality of retrievedcontent helps improve the RAG results. In the , the secondquestion shows that if the birth date is correct in the retrievedcontent, the prediction of Hybrid RAG Approach is correct. Thereis also another offline example that there are two different birth datesearch results for Woody Allen: November 30, 1935 and December1, 1935, and RAG can generate a wrong answer depending on whichsearch results it retrieved.The CRAG question dataset also contains many questions like\"In year x, which type y movie was recognized with the best type ymovie in Oscar\". a common hallucination type occurs in this way: itprovides a movie which was released in year x, but it won Oscar inyear x+1. In the table 5, it shows an example that \"the incredibles\"was provided by RAG as the best animated feature film in Oscar2004. However, the ground truth is \"finding nemo\", since \"FindingNemo\" won Oscar in 2004, and \"The Incredibles\" which was releasedin 2004 won Oscar in 2005.",
  "Future Work": "With hallucination being one of the biggest challenges in apply-ing LLMs to real-world applications, addressing different types ofhallucination continues to require deeper and more extensive re-search and efforts. Based on our experimental results that the hybridapproach provides the best outcome, we believe in the directionof an adaptive methodology using divide-and-conquer: splittingthe solution into two high-level parts: hallucination detection andhallucination correction, and adopting tailored hallucination cor-rection methods based on the detected hallucination types andprobabilities.In terms of detection, questions with low probabilities of hallu-cination can be questions in common domains, requiring simplelogic, and/or about a past fact that doesnt change; questions withhigh probabilities of hallucination can often be in specialized do-mains, requiring complex reasoning and multi-hop steps, and/or",
  "<DOC> What is the Michael Baydate of birth?. . . . . .The DOB for Michael Baywas 17 Feb 1965": "fast-changing facts. Notice that the hallucination types and proba-bilities are a full spectrum, and they dont simply depend on thequestion itself, but also on the external context. For example, thequestion \"which country wins the most gold medals in OlympicsGames Paris\" is fast-changing while the game is ongoing and willbecome a past fact after the game is over.In terms of correction, different strategies of answer correctioncan be adopted based on the detection results. For example, a real-time question will require real-time querying of the world knowl-edge (e.g. search engines that index the web in real-time); a special-ized domain question could leverage external knowledge.While both detection and correction remain extremely challeng-ing, the proposed adaptive approaches using divide-and-conquerremain a promising direction for solving the hallucination problems",
  "Honest AI: Fine-Tuning \"Small\" Language Models to Say \"I Dont Know\", and Reducing Hallucination in RAGKDDCup 24, Aug 2529, 2024, Barcelona, Spain": "Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, ChrisHesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, andDario Amodei. 2020. Language Models are Few-Shot Learners. In Advances inNeural Information Processing Systems, Vol. 33. Curran Associates, Inc., 18771901. Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking LargeLanguage Models in Retrieval-Augmented Generation. Proceedings of the AAAIConference on Artificial Intelligence 38, 16 (Mar. 2024), 1775417762. Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William Cohen. 2022. MuRAG:Multimodal Retrieval-Augmented Generator for Open Question Answering overImages and Text. In Proceedings of the 2022 Conference on Empirical Methods inNatural Language Processing. Association for Computational Linguistics, AbuDhabi, United Arab Emirates, 55585570.",
  "Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, CarissaSchoenick, and Oyvind Tafjord. 2018. Think you have Solved Question Answer-ing? Try ARC, the AI2 Reasoning Challenge": "Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gard-ner. 2021. A Dataset of Information-Seeking Questions and Answers Anchoredin Research Papers. In Proceedings of the 2021 Conference of the North Ameri-can Chapter of the Association for Computational Linguistics: Human LanguageTechnologies. Association for Computational Linguistics. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.QLoRA: Efficient Finetuning of Quantized LLMs. In Advances in Neural Informa-tion Processing Systems, Vol. 36. Curran Associates, Inc., 1008810115.",
  "Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Ji-awei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-Augmented Generationfor Large Language Models: A Survey": "Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020.Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reason-ing Steps. In Proceedings of the 28th International Conference on ComputationalLinguistics. International Committee on Computational Linguistics, Barcelona,Spain (Online). Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: ALarge Scale Distantly Supervised Challenge Dataset for Reading Comprehension.In Proceedings of the 55th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers). Association for Computational Linguistics,Vancouver, Canada, 16011611. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, AnkurParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee,Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M.Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: ABenchmark for Question Answering Research. Transactions of the Association forComputational Linguistics 7 (08 2019), 453466. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,Naman Goyal, Heinrich Kttler, Mike Lewis, Wen-tau Yih, Tim Rocktschel,Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented Generation forKnowledge-Intensive NLP Tasks. In Advances in Neural Information ProcessingSystems, Vol. 33. Curran Associates, Inc., 94599474.",
  "Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. 2024. Generating with Confidence:Uncertainty Quantification for Black-box Large Language Models. Transactionson Machine Learning Research (2024)": "Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. QueryRewriting in Retrieval-Augmented Large Language Models. In Proceedings of the2023 Conference on Empirical Methods in Natural Language Processing. Associationfor Computational Linguistics, Singapore, 53035315. Timo Mller, Anthony Reina, Raghavan Jayakumar, and Malte Pietsch. 2020.COVID-QA: A Question Answering Dataset for COVID-19. In Proceedings of the1st Workshop on NLP for COVID-19 at ACL 2020. Association for ComputationalLinguistics. Noor Nashid, Mifta Sintaha, and Ali Mesbah. 2023. Retrieval-Based Prompt Selec-tion for Code-Related Few-Shot Learning. In 2023 IEEE/ACM 45th InternationalConference on Software Engineering (ICSE). 24502462. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, PamelaMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022.Training language models to follow instructions with human feedback. Advancesin neural information processing systems 35 (2022), 2773027744. Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang,Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, andSamuel Bowman. 2022. QuALITY: Question Answering with Long Input Texts,Yes!. In Proceedings of the 2022 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies.Association for Computational Linguistics, Seattle, United States, 53365358. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.SQuAD: 100,000+ Questions for Machine Comprehension of Text. In Proceed-ings of the 2016 Conference on Empirical Methods in Natural Language Processing.Association for Computational Linguistics, Austin, Texas, 23832392. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, TimothyLillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat,Julian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking multimodal understandingacross millions of tokens of context. Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddingsusing Siamese BERT-Networks. In Proceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the 9th International Joint Conferenceon Natural Language Processing (EMNLP-IJCNLP). Association for ComputationalLinguistics, Hong Kong, China, 39823992. Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and WeizhuChen. 2023. Enhancing Retrieval-Augmented Large Language Models withIterative Retrieval-Generation Synergy. In Findings of the Association for Com-putational Linguistics: EMNLP 2023. Association for Computational Linguistics,Singapore, 92489274. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. Com-monsenseQA: A Question Answering Challenge Targeting Commonsense Knowl-edge. In Proceedings of the 2019 Conference of the North American Chapter of theAssociation for Computational Linguistics: Human Language Technologies, Volume1 (Long and Short Papers). Association for Computational Linguistics, Minneapolis,Minnesota, 41494158. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.2022. MuSiQue: Multihop Questions via Single-hop Question Composition.Transactions of the Association for Computational Linguistics 10 (2022), 539554. Xidong Wang, Guiming Chen, Song Dingjie, Zhang Zhiyi, Zhihong Chen, Qingy-ing Xiao, Junying Chen, Feng Jiang, Jianquan Li, Xiang Wan, Benyou Wang, andHaizhou Li. 2024. CMB: A Comprehensive Medical Benchmark in Chinese. InProceedings of the 2024 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies (Volume 1: LongPapers). Mexico City, Mexico, 61846205. Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, and Cordelia Schmid. 2023. Vid2Seq: Large-ScalePretraining of a Visual Language Model for Dense Video Captioning. 2023IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023),1071410726. Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, SajalChoudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, Lingkun Kong, BrianMoran, Jiaqi Wang, Yifan Ethan Xu, An Yan, Chenyu Yang, Eting Yuan, HanwenZha, Nan Tang, Lei Chen, Nicolas Scheffer, Yue Liu, Nirav Shah, Rakesh Wanga,Anuj Kumar, Wen tau Yih, and Xin Luna Dong. 2024. CRAG ComprehensiveRAG Benchmark. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, RuslanSalakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A Dataset forDiverse, Explainable Multi-hop Question Answering. In Proceedings of the 2018Conference on Empirical Methods in Natural Language Processing. Association forComputational Linguistics, Brussels, Belgium. Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal,Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023. Generate rather thanRetrieve: Large Language Models are Strong Context Generators. In The EleventhInternational Conference on Learning Representations. Jinming Zhao, Gholamreza Haffari, and Ehsan Shareghi. 2023. Generating Syn-thetic Speech from SpokenVocab for Speech Translation. In Findings of the Asso-ciation for Computational Linguistics: EACL 2023. Association for ComputationalLinguistics, Dubrovnik, Croatia, 19751981. Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H.Chi, Quoc V Le, and Denny Zhou. 2024. Take a Step Back: Evoking Reasoning viaAbstraction in Large Language Models. In The Twelfth International Conferenceon Learning Representations.",
  "AResearch MethodsA.1Prompt of Generating Answers withDomain in JSON format": "You are an agent that only outputs JSON. You are given aQuery and References. Do the following:1. Determine the domain the query is about. The domainshould be one of the following: \"finance\", \"sports\", \"music\",\"movie\", \"encyclopedia\". If none of the domains apply, use\"other\". Use \"domain\" as the key in the result json.2. Answer the question in as few words as possible. Pleasefollow these guidelines when formulating your answer. Ifthe question contains a false premise or assumption, answer",
  "\"invalid question\". If you are uncertain or dont know theanswer, respond with \"I dont know\". Use \"answer\" as the keyin the result json.### Question {query}### References {references}### Answer": "Example Output{\"domain\": \"movie\", \"answer\": \"Amy\" }\"\"\"# Step 1: Determine the domaindomain = \"movie\"# Step 2: Answer the questionanswer = \"Amy\"# Create the result JSONresult = {\"domain\": domain, \"answer\": answer}return resultSince sometimes the model generate additional text besides JSONoutput, additional code is written to extract the first JSON in theoutput to solve the problem.",
  "For example, a simple movie question \"can you tell me the date thatRandall Wallace was born?\"(Ground truth: 1949-07-28), the modelanswers the same question repetitively:": "According to the provided information, Randall Wallace wasborn on July 28, 1949;According to my sources, Randall Wallace, the Americanscreenwriter and director, was born on July 28, 1949;I can confirm it now: Randall Wallace was born on November7, 1948.Randall Wallaces birthday is September 1, 1948.Ive got it nailed down now! Randall Wallace was indeedborn on November 7, 1949I finally have it exact: Randall Wallace was born on July 1,1948 (no, wait!), I mean... September 1, 1949 (no), actually itis November 7, 1946. Yes, thats correct!"
}