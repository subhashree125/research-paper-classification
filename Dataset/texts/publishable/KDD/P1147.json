{
  "ABSTRACT": "Graph partitioning aims to divide a graph into disjoint subsetswhile optimizing a specific partitioning objective. The majority offormulations related to graph partitioning exhibit NP-hardness dueto their combinatorial nature. Conventional methods, like approx-imation algorithms or heuristics, are designed for distinct parti-tioning objectives and fail to achieve generalization across otherimportant partitioning objectives. Recently machine learning-basedmethods have been developed that learn directly from data. Further,these methods have a distinct advantage of utilizing node featuresthat carry additional information. However, these methods assumedifferentiability of target partitioning objective functions and can-not generalize for an unknown number of partitions, i.e., they as-sume the number of partitions is provided in advance. In this study,we develop NeuroCUT with two key innovations over previousmethodologies. First, by leveraging a reinforcement learning-basedframework over node representations derived from a graph neuralnetwork and positional features, NeuroCUT can accommodate anyoptimization objective, even those with non-differentiable func-tions. Second, we decouple the parameter space and the partitioncount making NeuroCUT inductive to any unseen number of parti-tion, which is provided at query time. Through empirical evaluation,we demonstrate that NeuroCUT excels in identifying high-qualitypartitions, showcases strong generalization across a wide spectrumof partitioning objectives, and exhibits strong generalization tounseen partition count.",
  "These authors contributed equally to this research": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08...$15.00",
  "INTRODUCTION": "Graph partitioning is a fundamental problem in network analysiswith numerous real-world applications in various domains such assystem design in online social networks , dynamic ride-sharingin transportation systems , VLSI design , and preventingcascading failure in power grids . The goal of graph partitioningis to divide a given graph into disjoint subsets where nodes withineach subset exhibit strong internal connections while having limitedconnections with nodes in other subsets. Generally speaking, theaim is to find somewhat balanced partitions while minimizing thenumber of edges across partitions.",
  "Related Work": "Several graph partitioning formulations have been studied in theliterature, mostly in the form of discrete optimization .The majority of the formulations are NP-hard and thus the proposedsolutions are either heuristics or algorithms with approximate solu-tions . Among these, two widely used methods are Spectral Clus-tering and hMETIS . Spectral Clustering partitions a graphinto clusters based on the eigenvectors of a similarity matrix de-rived from the graph. hMETIS is a hypergraph partitioning methodthat divides a graph into clusters by maximizing intra-cluster sim-ilarity while minimizing inter-cluster similarity. However, suchtechniques are confined to specific objective functions and are un-able to leverage the available node features in the graph. Recently,there have been attempts to solve graph partitioning problem vianeural approaches. The neural approaches have a distinct advantagethat they can utilize node features. Node features supply additionalinformation and provide contextual insights that may improve theaccuracy of graph partitioning. For instance, has recognizedthe importance of incorporating node attributes in graph clusteringwhere nodes are partitioned into disjoint groups. Note that there areexisting neural approaches to solve",
  "KDD 24, August 2529, 2024, Barcelona, SpainRishi Shah, Krishnanshu Jain, Sahil Manchanda, Sourav Medya, & Sayan Ranu": "other NP-hard graph combinatorial problems (e.g., minimum vertexcover). However these methods are not generic enough to solvegraph partitioning. For example, although S2VDQN learns tosolve the Maxcut problem, it is designed for the specific case ofbi-partitioning. This hinders its applicability to the target problemsetup of -way partitioning1. In this paper, we build a single framework to solve several graphpartitioning problems. One of the most relevant to our work is themethod DMoN by Tsitsulin et al. . This method designs a neuralarchitecture for cluster assignments and use a modularity-based ob-jective function for optimizing these assignments. Another method,that is relevant to our work is GAP, which is an unsupervisedlearning method to solve the balanced graph partitioning prob-lem . It proposes a differentiable loss function for partitioningbased on a continuous relaxation of the normalized cut formulation.Deep-MinCut being an unsupervised approach learns both nodeembeddings and the community structures simultaneously wherethe objective is to minimize the mincut loss . Another methodsolves the multicut problem where the number of partitions is notan input to the problem . The idea is to construct a reformu-lation of the multicut ILP constraints to a polynomial programas a loss function. However, the problem formulation is differentthan the generic normalized cut or mincut problem. Another relatedwork is DGCluster , which proposes to solve the attributed graphclustering problem while maximizing modularity when is notknown beforehand. Finally, solves the normalized cut problemonly for the case where the number of partitions is exactly two.Nevertheless, these neural approaches for the graph partitioningproblem often do not use node features and only limited to a distinctpartitioning objective. Here, we point out notable drawbacks thatwe address in our framework. Non-inductivity to partition count: The number of partitionsrequired to segment a graph is an input parameter. Hence, itis important for a learned model to generalize to any partitioncount without retraining. In -way graph partitioning, a modeldemonstrates inductivity to the number of partitions when it caninfer on varying partition numbers without specific training foreach. Existing neural approaches are non-inductive to the num-ber of partitions, i.e they can only infer on number of partitionson which they are trained. Additionally, it is worth noting thatthe optimal number of partitions is often unknown beforehand.This capability is crucial in practical applications like chip design,where graph partitioning optimizes logic cell placement by divid-ing netlists (circuits) into smaller subgraphs, aiding independentplacement. As the optimal partition count is frequently unknownin advance, experimenting with different partition numbers is acommon practice. Hence, it is a common practice to experimentwith different partition counts and evaluate their impact on thepartitioning objective. While the existing methods assume that the number of partitions () is known beforehand,our proposed method can generalize to any . Non-generalizability to different cut functions: Multipleobjective functions for graph cut have been studied in the parti-tioning literature. The optimal objective function hinges uponthe subsequent application in question. For instance, the two",
  "Our Contributions": "In this paper, we circumvent the above-mentioned limitations throughthe following key contributions. Versatile objectives: We develop NeuroCUT; an auto-regressive,graph reinforcement learning framework that integrates posi-tional information, to solve the graph partitioning problem forattributed graphs. Diverging from conventional algorithms, Neu-roCUT can solve multiple partitioning objectives. Moreover, un-like other neural methods, NeuroCUT can accommodate diversepartitioning objectives, without the necessity for differentiability. Inductivity to number of partitions: The parameter space ofNeuroCUT is independent of the partition count. This innova-tive decoupled architecture endows NeuroCUT with the abilityto generalize effectively to arbitrary partition count specifiedduring inference. Empirical Assessment: We perform comprehensive experi-ments employing real-world datasets, evaluating NeuroCUTacross four distinct graph partitioning objectives. Our empiricalinvestigation substantiates the efficacy of NeuroCUT in parti-tioning tasks, showcasing its robustness across a spectrum ofobjective functions. We also demonstrate the capability of Neu-roCUT to generalize effectively to partition sizes that have notbeen seen during training.",
  "Here, vol (, V) := , V (, )": "(4) Sparsest Cut : Two-way sparsest cuts minimize the cut edgesrelative to the number of nodes in the smaller partition. Wegeneralize it to -way sparsest cuts by summing up the valuefor all the partitions. The intuition behind sparsest cuts is thatany partition should neither be very large nor very small.",
  "min(||, | |)(5)": "Problem 1 (Learning to Partition Graph ). Given a graphG and the number of partitions , the goal is to find a partitioning Pof the graph G that optimizes a target objective function (G, P).Towards this end, we aim to learn a policy that assigns each node V to a partition in P. In addition to our primary goal of finding a partitioning thatoptimizes a certain objective function, we also desire policy tohave the following properties:(1) Inductive: Policy is inductive if the parameters of the policyare independent of both the size of the graph and the number ofpartitions . If the policy is not inductive then it will be unableto infer on unseen size graphs/number of partitions. (2) Learning Versatile Objectives: To optimize the parametersof the policy, a target objective function is required. The opti-mization objective may not be differentiable and it might not bealways possible to obtain a differentiable formulation. Hence,",
  "NEUROCUT: PROPOSED METHODOLOGY": "describes the framework of NeuroCUT. For a given inputgraph G, we first construct the initial partitions of nodes using aclustering based approach. Subsequently, a message-passing Gnnembeds the nodes of the graph ensuring inductivity to differentgraph sizes. Next, the assignment of nodes to partitions proceeds ina two-phased strategy. We first select a node to change its partitionand then we choose a suitable partition for the selected node. Fur-ther, to ensure inductivity on the number of partitions, we decouplethe number of partitions from the direct output representationsof the model. This decoupling allows us to query the model to anunseen number of partitions. After a nodes partition is updated,the reward with respect to change in partitioning objective valueis computed and the parameters of the policy are optimized. Thereward is learned through reinforcement learning (RL) .The choice of using RL is motivated through two observations.Firstly, cut problems on graphs are generally recognized as NP-hard, making it impractical to rely on ground-truth data, whichwould be computationally infeasible to obtain. Secondly, the cutobjective may lack differentiability. Therefore, it becomes essen-tial to adopt a learning paradigm that can be trained even underthese non-differentiable constraints. In this context, RL effectively",
  "PartitionScores": ": Architecture of NeuroCUT. First, the initial partitioning of the graph is performed based on node features andpositional embeddings. These embeddings are refined using GNN to infuse toplogical information from neighborhood. At eachstep a node is selected and its partitioned is updated. During training the GNN parameters are updated and hence embeddingsare re-computed. During inference, the GNN is called only once to compute the embeddings of the nodes of the graph. addresses both of these critical requirements. Additionally, in theprocess of sequentially constructing a solution, RL allows us tomodel the gain obtained by perturbing the partition of a node.Markov Decision Process. Given a graph G, our objective isto find the partitioning P that maximizes/minimizes the targetobjective function (G, P). We model the task of iterativelyupdating the partition for a node as a Markov Decision Process(MDP) defined by the tuple (, A, , ,). Here, is the state space,A is the set of all possible actions, : A denotesthe state transition probability function, : A R denotesthe reward function and (0, 1) the discounting factor. We nextformalize each of these notions in our MDP formulation.",
  "State: Initialization & Positional Encoding": "Initialization. Instead of directly starting from empty partitions,we perform a warm start operation that clusters the nodes of thegraph to obtain the initial graph partitions. The graph is clusteredinto clusters based on their raw features and positional embed-dings (discussed below). We use K-means algorithm for thistask. The details of the clustering are present in Appendix A.1. Positional Encoding (Embeddings). Given that the partitioningobjectives are NP-hard mainly due to the combinatorial natureof the graph structure, we look for representations that capturethe location of a node in the graph. Positional encodings providean idea of the position in space of a given node within the graph.Two nodes that are closer in the graph, should be closer in theembedding space. Towards this, we use Lipschitz Embedding .Let A = {1, , } V be a randomly selected subset of nodes. We call them anchor nodes. From each anchor node , thewalker starts a random walk and jumps to a neighboring node with a transition probability (W) governed by the transitionprobability matrix W R| || |. Furthermore, at each step, withprobability the walker jumps to a neighboring node and returnsto the node with 1 . Let corresponds to the probability ofthe random walker starting from node and reaching node . = W + (1 )(6)",
  "Eq. 6 describes the random walk starting at node . In vector R| |1, only the element (the initial anchor node) is 1, and therest are set to zero. We set W =1": "() if edge E, 0otherwise. The random walk with restart process is repeated for iterations, where is a hyper-parameter. Here R| |1 and isa scalar.Based upon the obtained random walk vectors for the set ofanchor nodes A, we embed all nodes V in a -dimensionalfeature space:pos() = (7) To accommodate both raw feature and positional informationwe concatenate the raw node features i.e X[] with the positionalembedding pos() for each node to obtain the initial embeddingwhich will act as input to our neural model. Specifically,embinit() = X[]pos()(8)",
  "Action: Selection of Nodes and Partitions": "Towards finding the best partitioning scheme for the target parti-tion objective we propose a 2-step action strategy to update thepartitions of nodes. The first phase consists of identifying a nodeto update its partition. Instead of arbitrarily picking a node, wepropose to prioritize selecting nodes for which the new assign-ment is more likely to improve the overall partitioning objective.In comparison to a strategy that arbitrarily selects nodes, the abovemechanism promises greater improvement in the objective withless number of iterations. In the second phase, we calculate thescore of each partition P with respect to the selected node from thefirst phase and then assign it to one of the partitions based upon thepartitioning scores. We discuss both these phases in details below. 3.2.1Phase 1: Node Selection to Identify Node to Perturb.Let PART(P, ) denote the partition of the node at step . Ourproposed formulation involves selecting a node at step belongingto partition PART(P, ) and then assigning it to a new partition.The newly assigned partition and the current partition of the nodecould also be same.Towards this, we design a heuristic to prioritize selecting nodeswhich when placed in a new partition are more likely to improvethe overall objective value. A node is highly likely to be movedfrom its current partition if most of its neighbours are in a differentpartition than that of node . Towards this, we calculate the scoreof nodes V in the graph as the ratio between the maximumnumber of neighbors in another partition and the number of neigh-bors in the same partition as . Intuitively, if a partition exists inwhich the majority of neighboring nodes of a given node belong,and yet node is not included in that partition, then there is ahigh probability that node should be subjected to perturbation.Specifically the score of node at step is defined as:",
  "()(10)": "For a node of interest , the numerator computes the maximumnumber of neighbors in a different partition than that of . Asdescribed in , PART(, ) refers to the partition of at step. The expression \\ PART(, ) computes all other partitionsexcept the partition of . The term || N () PART(,) = |computes the number of neighbors of in the partition . Thedenominator computes the number of neighbors of node in thesame partition as , referred to as PART(, ). Further, a nodehaving a higher degree implies that it has several edges associatedto it. Hence, an incorrect placement of it could contribute to a",
  "higher partitioning value. Therefore, we normalize the scores bythe of the node": "3.2.2Phase 2: Inductive Method for Partition Selection.Once a node is selected, the next phase involves choosing the newpartition for the node. Towards this, we design an approach em-powered by Graph Neural Networks (GNNs) which enablesthe model to be inductive with respect to size of graph. Further, in-stead of predicting a fixed-size score vector for the numberof partitions, our proposed method of computing partition scoresallows the model to be inductive to the number of partitions too.We discuss both above points in section below. Message Passing through Graph Neural NetworkTo capture the interaction between different nodes and their fea-tures along with the graph topology, we parameterize our policy bya Graph Neural Network (GNN) . GNNs combine node featureinformation and the graph structure to learn better representationsvia feature propagation and aggregation.We first initialize the input layer of each node V in graph ash0 = embinit() using eq. 8. We perform layers of message pass-ing to compute representations of nodes. To generate the embeddingfor node at layer+1 we perform the following transformation:",
  "Nh(11)": "where h()is the node embedding in layer . W1 and W2 aretrainable weight matrices at layer .Following layers of message passing, the final node represen-tation of node in the layer is denoted by h R. Intuitivelyh characterizes using a combination of its own features andfeatures aggregated from its neighborhood. Scoring Partitions. Recall from eq. 9, each partition at time isrepresented using the nodes belonging to that partition. Buildingupon this, we compute the score of each partition P withrespect to the node selected in Phase 1 using all the nodes in .In contrast to predicting a fixed-size score vector correspondingto number of partitions , the proposed design choice makesthe model inductive to the number of partitions. Specifically, thenumber of partitions are not directly tied to the output dimensionsof the neural model.Having obtained the transformed node embeddings through aGnn in Eq. 11, we now compute the (unnormalized) score for node selected in phase 1 for each partition P as follows:PartScore(, )= AGG({MLP((|))",
  "N () PART(P,) = })(12)": "The above equation concatenates the selected node s embed-ding with its neighbors N () that belong to the partition under consideration. In general, the strength of a partition assign-ment to a node is higher if its neighbors also belong to the samepartition. The above formulation surfaces this strength in the em-bedding space. The concatenated representation (|) N ()is passed through an MLP that converts the vector into a score(scalar). We then apply an aggregation operator (e.g., mean) overall neighbors of belonging to P to get an unnormalized scorefor partition . Here is an activation function.",
  "((G, P) + (G, P+1)) (15)": "Here is a hyperparameter that is used to scale the reward. Theabove reward expression incentivizes significant improvements inthe objective function. This is achieved mathematically by consider-ing both the change and the current value of the objective function.This design steers the model towards prioritizing substantial im-provements, especially in low objective function regions, ultimatelyguiding it towards the overall minimum.However, above definition of reward focuses on short-term im-provements instead of long-term. Hence, to prevent this local greedybehavior and to capture the combinatorial aspect of the selections,we use discounted rewards to increase the probability of actionsthat lead to higher rewards in the long term . The discountedrewards are computed as the sum of the rewards over a horizonof actions with varying degrees of importance (short-term andlong-term). Mathematically,",
  "=0 +(16)": "where is the length of the horizon and (0, 1] is a discountingfactor (hyper-parameter) describing how much we favor immediaterewards over the long-term future rewards.The above reward mechanism provides flexibility to our frame-work to be versatile to objectives of different nature, that mayor may not be differentiable. This is an advantage over existingneural methods where having a differentiable form of thepartitioning objective is a pre-requisite.Policy Loss Computation and Parameter Update. Our objec-tive is to learn parameters of our policy network in such a waythat actions that lead to an overall improvement of the partition-ing objective are favored more over others. Towards this, we useREINFORCE gradient estimator to optimize the parameters of",
  "(18)": "Training and Inference. For a given graph, we optimize the pa-rameters of the policy network for steps. Note that the tra-jectory length is not kept very large to avoid the long-horizonproblem. During inference, we compute the initial node embed-dings, obtain initial partitioning and then run the forward pass ofour policy to improve the partitioning objective over time.3.4Time Complexity The time complexity of NeuroCUT during inference isO( |E|) + (|E| + ) . Here is the number of anchornodes, is number of random walk iterations, is the numberof partitions and is the number of iterations during inference.Typically , and are << |V| and = (|V|). Further, forsparse graphs |E| = O(|V|). Hence time complexity of NeuroCUTis (|V|2). For detailed derivation please see Appendix A.2.",
  "EXPERIMENTS": "In this section, we demonstrate the efficacy of NeuroCUT againststate-of-the-art methods and establish that: Efficacy and Robustness: NeuroCUT produces the best resultsover diverse partitioning objective functions. This establishes therobustness of NeuroCUT. Inductivity: As one of the major strengths, unlike existing neuralmodels such as GAP , DMon and MinCutPool , ourmethod NeuroCUT is inductive on the number of partitions andconsequently can generalize to unseen number of partitions.Code: Our code base is accessible at",
  "Experimental Setup": "4.1.1Datasets: We use four real datasets for our experiments. Theyare described below and their statistics are present in . Forall datasets we use their largest connected component. Cora and Citeseer : These are citation networks wherenodes correspond to individual papers and edges represent cita-tions between papers. The node features are extracted using abag-of-words approach applied to paper abstracts.",
  "Harbin : This is a road network extracted from Harbin city,China. The nodes correspond to road intersections and nodefeatures represent latitude and longitude of a road intersection": "Actor : This dataset is based upon Wikipedia data whereeach node in the graph corresponds to an actor, and the edge be-tween two nodes denotes co-occurrence on the same Wikipediapage. Node features correspond to keywords in Wikipedia pages.This is a heterophilous dataset where nodes tend to connectto other nodes that are dissimilar.",
  "NeuroCUT: A Neural Approach for Robust Graph PartitioningKDD 24, August 2529, 2024, Barcelona, Spain": "Facebook : In social network analysis, a -way cut can de-lineate communities of users with minimal inter-communityconnections. This partitioning unveils hidden social circles andprovides insights into how different groups interact within thelarger network. The Facebook dataset, used for community de-tection, does not include node features. In this dataset, nodesrepresent users, and edges represent friendships. Stochastic Block Models (SBM) : SBMs are synthetic net-works and produce graphs with communities, where subsets ofnodes are characterized by specific edge densities within thecommunity. This allows us to validate our method by comparingthe identified partitions with the ground truth communities. Thisdataset does not include node features.",
  "SBM5005150-20.60.18-0.0059": "4.1.2Partitioning objectives: We evaluate our method on a diverseset of four partitioning objectives described in , namelyNormalized Cut, Balanced Cut, k-MinCut, and Sparsest Cut. In addi-tion to evaluating on diverse objectives, we also choose a diversenumber of partitions for evaluation, specifically, = 2, 5 and 10. 4.1.3Baselines: We compare our proposed method with both neu-ral as well as non-neural methods.Neural baselines: We compare with DMon , GAP , Min-CutPool and Ortho . DMon is the state-of-the-art neuralattributed-graph clustering method. GAP is optimized for balancednormalized cuts with an end-to-end framework with a differen-tiable loss function which is a continuous relaxation version ofnormalized cut. MinCutPool optimizes for normalized cut anduses an additional orthogonality regularizer. Ortho is the orthogo-nality regularizer described in DMon and MinCutPool. We alsocompare with DRL which solves for Normalized cut at = 2in Appendix A.5Non-neural baselines: Following the settings of DMon , wecompare with K-means clustering applied on raw node features. Wealso compare with standard graph clustering methods hMetis and Spectral clustering in App Sec. A.4. 4.1.4Other settings: We run all our experiments on an Ubuntu20.04 system running on Intel Xeon 6248 processor with 96 coresand 1 NVIDIA A100 GPU with 40GB memory for our experiments.For NeuroCUT we used GraphSage as our GNN with numberof layers = 2, learning rate as 0.0001, hidden size = 32. We usedAdam optimizer for training the parameters of our policy network. For computing discounted reward in RL, we use discount factor = 0.99. We set the length of trajectory during training as 2.At time step , the rewards are computed from time to + andparameters of the policy are updated. The default number ofanchor nodes for computing positional embeddings is set to 35. Weset = 100 and = 0.85 for Eq. 6. We set scaling factor = 100 ineq. 15.",
  "K-means0.480.540.80MinCutPool0.250.350.42DMon0.170.390.44Ortho0.350.650.83GAP0.090.120.26NeuroCUT 0.0 0.0 0.0": "are trained and tested with the same number of partitions. Ta-bles 3-6 present the results for different partitioning objectivesacross all datasets and methods. For the objectives under consid-eration, a smaller value depicts better performance. NeuroCUTdemonstrates superior performance compared to both neural andnon-neural baselines across four distinct partitioning objectives,highlighting its robustness. Further, we would also like to pointout that in many cases, existing baselines produce nan valueswhich are represented by - in the result tables. This is due to thereason that every partition is not assigned atleast one node whichleads to the situation where denominator becomes 0 in normalized,balanced and sparsest cut objectives as defined in Sec. 2. Our method NeuroCUT incorporates dependency during in-ference and this leads to robust performance. Specifically, as thearchitecture is auto-regressive, it takes into account the currentstate before moving ahead as opposed to a single shot pass in meth-ods such as GAP . Further, unlike other methods, NeuroCUTalso incorporates positional information in the form of Lipschitzembedding to better contextualize global node positional informa-tion. We also observe that the non-neural method K-means fails toperform on all objective functions since its objective is not alignedwith the main objectives under consideration.",
  "Results on Inductivity to Partition Count": "As detailed in .2.2, the decoupling of parameter size andthe number of partitions allows NeuroCUT to generalize effectivelyto an unseen number of partitions. In this section, we empiricallyanalyze the generalization performance of NeuroCUT to an un-known number of partitions. We compare it with the non-neuralbaseline K-means, as the neural methods like GAP, DMon, Or-tho, and MinCutPool cannot be employed to infer on an unseen",
  "HarbinK-means5.4017.030.545.41NeuroCUT-T0.280.820.030.27NeuroCUT-I0.270.870.030.29": "number of partitions. In , we present the results on induc-tivity. Specifically, we trained NeuroCUT on different partitionsizes ( = 5 and = 8) and tested it on an unseen partition size = 10. The inductive version, referred to as NeuroCUT-I3, ob-tains high-quality results on different datasets. Note that while thenon-neural method such as K-means needs to be re-run for theunseen partitions, NeuroCUT only needs to perform forward passto produce the results on an unseen partition size.",
  "(d) k-MinCut": ": Results on the initial warm start and the final cut values obtained by NeuroCUT at =10. It shows that our neuralmodel NeuroCUT (Final) performs more accurate node and partition selection to optimize the objective function. Subsequently,there is a significant difference between the initial and final cut values. CoraCiteSeerHarbin % Gain NormalizedBalancedk-MinCutSparsest-Cut : Node Selection in Phase 1 with Heuristic vs Random:Relative % improvement (gain) in cut values obtained byNeuroCUT when using different node selection strategies for = 5. In most cases, our heuristic finds significantly bettercuts than a random node selection procedure. fine-tuned auto-regressively through the phase 1 and phase 2 ofNeuroCUT. In this section, we measure, how much the partitioningobjective has improved since the initial clustering? shedslight on this question. Specifically, it shows the difference betweenthe initial cut value after clustering and the final cut value fromthe partitions produced by our method. We note that there is asignificant difference between the initial and final cut values, whichessentially shows the effectiveness of NeuroCUT.Impact of Node Selection Procedures. In this section, weexplore the effectiveness of our proposed node selection heuristicin enhancing overall quality. Specifically, we measure the relativeperformance gain(in %) obtained by NeuroCUT when using thenode selection heuristic as proposed in .2.1 over a randomnode selection strategy. In our experiments for the random nodeselection strategy, we selected the best performing run across multi-ple seeds. shows the percentage gain for three datasets. Weobserve that a simpler node selection where we select all the nodesone by one in a random order, yields substantially inferior resultsin comparison to the heuristic proposed by us. This suggests thatthe proposed sophisticated node selection strategy plays a crucialrole in optimizing the overall performance of NeuroCUT. In Ap-pendix A.7, we conduct a more detailed analysis of the observationspresented in , with particular attention to the variation ingains related to the Balanced Cut objective.",
  "CONCLUSION": "In this work, we study the problem of graph partitioning with nodefeatures. Existing neural methods for addressing this problem re-quire the target objective to be differentiable and necessitate priorknowledge of the number of partitions. In this paper, we intro-duced NeuroCUT, a framework to effectively address the graphpartitioning problem with node features. NeuroCUT tackles thesechallenges using a reinforcement learning-based approach that canadapt to any target objective function. Further, attributed to itsdecoupled parameter space and partition count, NeuroCUT cangeneralize to an unseen number of partitions. The efficacy of ourapproach is empirically validated through an extensive evaluationon four datasets, four graph partitioning objectives and diverse par-tition counts. Notably, our method shows significant performancegains when compared to the state-of-the-art techniques, provingits competence in both inductive and transductive settings.Limitations and Future Directions: Achieving a sub-quadraticcomputational complexity with an inductive neural method forattributed graph partitioning is an open challenge. In NeuroCUT,node selection and perturbation are performed in a sequential fash-ion. One direction to improve the efficiency of NeuroCUT couldbe batch processing the selection and perturbation of multiple in-dependent nodes simultaneously. Another interesting directioncould be designing explanations of the neural methodssuch asNeuroCUTfor graph combinatorial problems .",
  "Martin Ester, Hans-Peter Kriegel, Jrg Sander, Xiaowei Xu, et al. 1996. A density-based algorithm for discovering clusters in large spatial databases with noise. Inkdd, Vol. 96. 226231": "Maxime Gasse, Didier Chtelat, Nicola Ferroni, Laurent Charlin, and AndreaLodi. 2019. Exact combinatorial optimization with graph convolutional neuralnetworks. Advances in neural information processing systems 32 (2019). Alice Gatti, Zhixiong Hu, Tess Smidt, Esmond G Ng, and Pieter Ghysels. 2022.Graph partitioning and sparse matrix ordering using reinforcement learning andgraph neural networks. The Journal of Machine Learning Research 23, 1 (2022),1367513702.",
  "Sahil Manchanda, Shubham Gupta, Sayan Ranu, and Srikanta J Bedathur. 2024.Generative modeling of labeled graphs under data scarcity. In Learning on GraphsConference. PMLR, 321": "Sahil Manchanda, Sofia Michel, Darko Drakulic, and Jean-Marc Andreoli. 2022.On the generalization of neural combinatorial optimization heuristics. In JointEuropean Conference on Machine Learning and Knowledge Discovery in Databases.Springer, 426442. Sahil Manchanda, Akash Mittal, Anuj Dhawan, Sourav Medya, Sayan Ranu,and Ambuj Singh. 2020. Gcomb: Learning budget-constrained combinatorialalgorithms over billion-sized graphs. Advances in Neural Information ProcessingSystems 33 (2020), 2000020011.",
  "A.2Time Complexity Analysis of NeuroCUT": "(1) First the positional embeddings for all nodes in the graph arecomputed. This involves running RWR for anchor nodes for iterations (Eq. 6). This takes O( |E|) time. (2) Next, GNN iscalled to compute embeddings of node. In each layer of GNN a node V aggregates message from neighbors where is the averagedegree of a node. This takes O(V) time. This operation is repeat for layers. Since is typically 1 or 2 hence we ignore this factor. (3)The node selection algorithm is used that computes score for eachnode based upon its neighborhood using eq. 10. This consumesO(|V| ) time. (4) Finally for the selected node, its partitionhas to be determined using eq. 12 and 13. This takes O( + )time, as we consider only the neighbors of the selected node tocompute partition score. Steps 2-4 are repeated for iterations.Hence overall running time is O( |E|) + (|V| +) .Typically , and are << |V| and = (|V|). Since |V| |E|, hence complexity is (|E| |V| Further, for sparse graphs|E| = O(|V|). Hence time complexity of NeuroCUT is (|V|2).",
  "A.3Time Complexity Comparison": "presents the complexities of NeuroCUT and other promi-nent neural and non-neural baselines algorithms. In the table, |V|,|E|, , are the number of nodes, edges, partitions, and averagenode degree respectively. While some neural algorithms exhibitfaster complexity, they require separate training for each partitionsize (). In contrast, NeuroCUT, once trained, can generalize to anyvalue. Consequently, for practical workloads, NeuroCUT presents amore scalable option in terms of computation overhead and storage(one model versus separate models for each). The quadratic time complexity of NeuroCUT may pose chal-lenges for very large graphs. However, this complexity remainsfaster than spectral clustering, a widely used graph partitioning al-gorithm. Also, for the baseline neural methods(DMon, MinCutPool,Ortho and GAP), the time complexity provided is per iteration. Thenumber of iterations often ranges between 1000 to 2000 and thereis no clear understanding of how it varies as a function of the graph(like density, diameter, etc.)",
  "NeuroCUT0.020.020.010.170.1910.257Gatti et al.0.3550.290.131.000.720.92": "not compatible with datasets having raw node features, we com-pare with two datasets namely Facebook and Stochastic BlockModel(SBM) which dont have node features for this compar-ision. compares the performance of NeuroCUT againsthMETIS and Spectral Clustering. The details of these datasets arepresented in . In addition to non-neural methods, we alsoshow the performance of neural methods MinCutPool, DMon, Or-tho and GAP on these datasets. The performance of NeuroCUT is better than non-neural meth-ods hMETIS, Spectral and Gomory-Hu Tree in most of the caseson Facebook dataset. Further, on SBM dataset, it matches the per-formance of hMETIS and Spectral clustering. The SBM datasethas a clear community structure, making it an easy instance forpartitioning algorithms, resulting in similar performance acrossdifferent methods. We would also like to highlight that in the case of, Gomory-Hu Tree algorithm generated trivial partitionsfor these datasets where almost all nodes were assigned the samepartition. The other neural methods such as MinCutPool, DMon,Ortho, and GAP fail to produce a valid solution in most of thecases. This is possibly because these baselines are not robust to per-form on datasets without raw node features. Overall, NeuroCUToutperforms the baselines on diverse objectives and datasets.",
  "Cora0.642 0.00650.753 0.05740.6722Citeseer0.248 0.00490.307 0.0390.2520.5Harbin0.1087 0.00100.1313 0.0110.129319": "Cora and CiteSeer using 3 different initialization namely K-means,density-based clustering DBSCAN , and Random initialization.In we observe the performance on normalized cut at = 5.We observe that K-means performs the best in this experiment.The improvement observed when using K-means or DBSCAN overRandom shows that a good initialization i.e. warm-up does help inimproving quality of partitions. In DBSCAN we set = 0.9 and_ = 2. It is worth noting that DBSCANs performancecan vary based on the parameter selection. Nonetheless, the primarygoal of this experiment is to demonstrate the advantageous role of agood initialization, specifically in contrast to random initialization.",
  "A.7Impact of Node Selection Procedures": "In in main paper, random selection performs well only inBalanced Cuts. The objective in Balanced Cuts (Eq. 3.) is not only afunction of the combined weight of cut edges, but also balancing thenumber of nodes across partitions. This additional node-balancingterm is not present in the objective functions of the other cut defi-nitions. Due to this reduced importance of optimizing the cut value,random does well, since even when an incorrect node is selected, itcan still be utilized to keep the partition sizes balanced. Moreover,we find that when the warm-up assignment is less accurate andthe percentage of nodes re-assigned by NeuroCUT to a differentpartition is high, a random selection of nodes is closer in efficacyto our heuristic selection. In contrast, in Harbin, the percentageof nodes perturbed to a new partition is significantly smaller. Inthis scenario, the probability that random selection will select this",
  "A.8Impact of Cluster Strength on Performance": "To understand the impact of community structure on performanceof NeuroCUT, we generate Stochastic Model Block(SBM) graphswith different intra cluster strength. In , we observe theperformance of different methods on the normalized cut objectiveat = 5. The baseline methods hMETIS and Spectral Clustering per-form worse when the strength within communities is low indicatedby Intra Cluster Edge Probability and Clustering Coefficient values.Although NeuroCUT outperforms or matches existing methodsin all cases, however, the gap between baselines and NeuroCUTincreases significantly when the community structure is not strong.Further, the neural baselines DMon, MinCutPool and Ortho gener-ated nan values for this experiment."
}