{
  "ABSTRACT": "This paper introduces a reinforcement learning (RL) platform thatenhances end-to-end user journeys in healthcare digital tools throughpersonalization. We explore a case study with SwipeRx, the mostpopular all-in-one app for pharmacists in Southeast Asia, demon-strating how the platform can be used to personalize and adapt userexperiences. Our RL framework is tested through a series of experi-ments with product recommendations tailored to each pharmacybased on real-time information on their purchasing history andin-app engagement, showing a significant increase in basket size.By integrating adaptive interventions into existing mobile healthsolutions and enriching user journeys, our platform offers a scal-able solution to improve pharmaceutical supply chain management,health worker capacity building, and clinical decision and patientcare, ultimately contributing to better healthcare outcomes.",
  "reinforcement learning, behavioral AI, e-commerce recommenda-tions, adaptive interventions, adaptive customer journey": "ACM Reference Format:Ana Fernndez del Ro, Michael Brennan Leong, Paulo Saraiva, Ivan Nazarov,Aditya Rastogi, Moiz Hassan, Dexian Tang, and frica Periez. 2024. Adap-tive User Journeys in Pharma E-Commerce with Reinforcement Learning: Permission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD CJ Workshop 24, August 25, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s).",
  "INTRODUCTION": "Personalization and adaptability are crucial for enhancing user ex-perience and customer journeys in digital tools. Leveraging detaileduser in-app behavior data, reinforcement learning (RL) providesadaptive journeys that improve user experience. This is particu-larly relevant for mobile health solutions, especially for healthcareworkers in low- and middle-income countries (LMICs), where suchtools can help mitigate the lack of resources. Pharmacies play acritical role in healthcare systems everywhere. They are often theonly primary healthcare contact in LMICs for many patients whilefacing barriers like a shortage of trained professionals, medication,and suboptimal supply chains . Supporting pharmacistswith inventory management, easy-to-access clinical guides, andconnection to their peers and the rest of the health system cansignificantly impact their communities.",
  "The Reinforcement Learning Platform": "We propose an artificial intelligence (AI) data-centric platform thatcan integrate into already existing healthcare-related digital toolsto enhance them with an adaptive user journey through reinforce-ment learning (RL). By enabling the system to learn and adapt itsbehavior based on the feedback it receives, RL is used to optimizethe user experience, personalizing it and making it more efficient.The end-to-end machine learning (ML) platform can integrate intodifferent healthcare related software and leverages the behavioraland clinical logs from these solutions, together with other contex-tual information sources, to deliver adaptive interventions directlyto their recipients through these tools in the form of personalizedrecommendations, reminders, incentives, or in-app content andworkflows, providing an adaptive user experience and journey. Wewill illustrate the framework and technologies proposed by focusing",
  ": Schematic representation of the RL platforms architecture": "on a pharmacist-facing tool, particularly its bussiness-to-bussines(B2B) e-commerce component. depicts the platforms architecture, which consists of aSoftware Developer Kit (SDK) which is embedded into the digitaltools to track labeled data and deliver interventions, a frontendinterface with analytics, model management, intervention and ex-perimentation functionalities, and the backend, which takes care oflog ingestion, data transformation, scheduled dispatch of nudges(message or content interventions) and hosts the algorithmic ma-chine learning (ML) engine. The platform has also been describedin and has been designed to integrate with mobile health solu-tions beyond the pharma supply chain, including patient manage-ment and clinical decision support, facility logistics, patientsupport, and patient management. A shorter version of this paperis also available . 1.1.1SDK. One of the platforms core functionalities is data track-ing, organizing, and labeling. Ensuring data collection and its qual-ity is arguably the most critical step in moving towards a solid AIapproach to personalization in digital health. The standardized de-sign of event logging enables the platform to be a uniform interfacefor transformation and aggregation across different application do-mains, e.g., supply chain and e-commerce marketplace, medicationtracking, and patient communication. The SDK is a well-definedlibrary and data structure at the heart of the platforms data side. Itprovides the tools for collecting relevant events on an edge device(e.g., mobile, wearable monitor, or smart device) and the messagingservice that delivers the interventions. 1.1.2Backend. The incoming logs through the SDK are processedby the tailor-made data pipeline and categorized into dynamicand static traits, which can aggregate through arbitrary time res-olution. These traits represent insights about the users and theirinteractions with different in-app content and other subjects (phar-macies, patients, drugs), and once derived, they become available throughout the platform. They can be used to track behavior andoutcomes, to group subjects or content (e.g., drugs or CPD mod-ules) into meaningful cohorts (e.g., for pharmacy segmentation),as features/covariates for statistical and predictive modeling, orto make up the contexts and rewards for bandit-based decisionalgorithms. The backend orchestrates the storage and processingof domain-knowledge analytical traits, the computation of featuresand predictions derived from traits using advanced machine learn-ing, and, finally, issues personalized nudges back to SDK instances. 1.1.3Algorithmic engine. The machine learning component hostedin the backend is composed of analytic and predictive modeling, anML recommendation engine, and an algorithmic decision-makingservice. These modules configure, train, host, and manage modelsfor statistical analysis, time series forecasting, deep and ensemblesurvival analysis, item recommendations, and sequential decisionsvia reinforcement learning (see for more technical details).These services expose their functionality via JSON request-responseAPI to the backend. 1.1.4Frontend. The frontend provides an intuitive user interfacefor configuring models and interventions and an analytics dash-board for monitoring ongoing ones. The configuration UI guidesthe user from the subject cohort and sample definition throughalgorithm selection to feature selection and target specification,including nudge alternatives. The dashboard represents varioustraits, predictions, and metrics in a convenient format for easierinterpretation of the results.",
  ": Screenshots of the SwipeRx application": "Cambodia, and Vietnam. The platform provides comprehensive ser-vices, including online education, centralized purchasing, logisticsand financing, news, drug directory, adverse event reporting, andmore, addressing the unique needs of this vital public health sector. shows some screenshots of the app.The network is especially crucial in the region, where theresa heavy reliance on pharmacies due to a scarcity of professionalphysicians (with patients visiting pharmacies ten times more thandoctors every year). Through its app, SwipeRx fosters knowledgesharing and collaboration among pharmacists, enhancing the qual-ity of patient care in local communities. Over 80,000 pharmacyprofessionals have already been educated through its digital pro-fessional education.In Southeast Asia, more than 80% of the pharmacies are inde-pendent, facing challenges of quality, availability, and affordabilityof essential medicines. SwipeRx is helping to solve this by em-powering pharmacy professionals with the technologies and toolsthey need and addressing the operational challenges these predom-inantly small, family-owned pharmacies face, such as financing andsupply chain management. By facilitating bulk purchases and ana-lyzing drug consumption data, SwipeRx enables these pharmaciesto procure medicines at competitive prices, ensuring they can meetpatient needs effectively.",
  "Adaptive User Journey": "We propose a user-centric approach where each users journey istailored to their evolving needs, preferences, and the exact stageof their relationship with the service or tool to ensure that everyusers unique requirements are met. This is made possible by the RLplatforms adaptive algorithms (see .3) ability to systemat-ically use the information from all users and continually adapt toimprove user experience.It begins with a personalized onboarding process by adjusting thelevel of support and introduction to app functionalities according to each users requirements. An adaptive algorithm can determinethe complexity of tips offered or whether to provide tips at all,based on user data such as location, experience, gender, job title,app usage patterns, and previous onboarding interactions. Thisensures that tech-savvy users can navigate the app unhurdled byunnecessary popups, while less experienced users receive step-by-step guidance. An adaptive onboarding experience can also adjustworkflow complexity based on the users expertise. For example,community health workers (CHWs) struggling with technology aregiven a simplified patient screening workflow to keep them engagedwithout overwhelming them while collecting essential information.As they gain experience, the workflow introduces more complexoptions, allowing experienced users to provide detailed patient data,referrals, clinical history, or preventive education provided.Beyond onboarding, the personalized journey continues with tai-lored in-app content and workflows and with messages (via popups,notifications, WhatsApp or SMS) with personalized motivationalprompts, tips, discounts, and incentives, to foster long-term userengagement. In tools such as SwipeRx, messages with personalizedproduct recommendations (like those discussed in ) andin-app ML-ordered product lists help increase or maintain purchas-ing engagement. The adaptive delivery mechanisms, in this case,can also use the outputs of predictive models to be more efficient,such restocking recommendations based on demand prediction thatcan also help reduce stockouts of essential drugs. Churn predictionmodels also play a significant role, as they can be used to precisiontarget users at high risk with specific interventions, such as specialdiscounts. Specific adaptive algorithms (see .3.3) able toallocate limited resources such as discounts or even a call from asales representative efficiently.",
  "Predictive Modeling": "Predictive modeling can help to define who should be targetedwith interventions and the right way and moment to do so. Notethat while predictions are not used in the first set of interventionsdescribed in , we include this subsection for completenessas different predictions are readily available in the platform.The platform includes different families of predictive models suit-able for different use cases. Through its frontend, platform users caneasily create models from these families by designating the traits tobe used as targets and features and the specific algorithm with itsparameter values. The model lifecycle can also be managed throughthe platform, including training schedules and performance moni-toring. All resulting outputs of the models, including estimationsof uncertainty and feature importance, are available to create addi-tional traits that can be used to characterize subjects and contentthroughout the rest of the platform.Survival analysis approaches offer a way of char-acterizing user (pharmacy, patient, distribution center) behavior bymodeling the evolution with time of the probability of occurrence ofdifferent events of interest. Formally, the methods learn a function: X [0, +) from the observed data (,,)=1 with",
  "KDD CJ Workshop 24, August 25, 2024, Barcelona, SpainFernndez del Ro and Leong, et al": ": Breakdown by interaction (opened, closed or ignored), and recommendation type (personalized or random) ofmessages that lead to purchase in XP3s adaptive intervention group (11.2% of all messages) to the left and for personalizedrecommendations only (11.1% of the messages) to the right. : Breakdown by interaction (opened, closed or ignored), and experimental group (MAB for adaptive intervention,AB for non-adaptive one) of random recommendations that lead to purchase in XP3 (11.8% of all messages) to the left and byinteraction only to the right, with both the adaptive and non adaptive groups considered.",
  "Recommendation Algorithms": "As will be described in section 2.3, the main mechanism in placeto learn and exploit user preferences for recommendation and per-sonalization will be that of adaptive delivery, i.e., bandit-based to alarge extent. Other recommendation algorithms, however, mightprovide valuable insights that can be leveraged by the reinforce-ment learning algorithms and used for content recommendations.The RL platform includes a neural embedding based item-to-itemrecommender , in which items are recommended based ontheir similarity to other items. This similarity is based both on thefixed known information about each item and on the interactionhistory of users with them. Transformers are used to convert avail-able features written in natural language (e.g., name, description,manufacturer, ingredients...) into numerical vectors. Principal com-ponent analysis (PCA) is then applied to reduce their dimensionality.The PCA-transformed natural language vectors together with theinteraction history (e.g., purchase orders item content) are fed intoa fully connected neural network with three layers. The output ofthis system are the L2-normalized embedding characterizing eachof the items, whose similarity to other items can be then computedas the Euclidean distance between their embeddings. However, theset of interventions discussed in this paper relies on the simplerrule-based algorithm described below. 2.2.1Item-pair recommendation. The underlying idea is to findproducts that the pharmacy is very likely already purchasing, al-beit elsewhere, and make them aware that these are also part ofSwipeRxs catalog. This is achieved by recommending pairs of prod-ucts which are typically purchased together by the user populationto pharmacies that are only ordering one of the items frequently.Formally, consider the pair = (, ), where and representproducts. Define as at time , the number of days since user purchased product . Let be at time the average numberof days between purchases in the last months. For the initialintervention with SwipeRx, was chosen to be three months. If has never been purchased by , we adopt the convention that = 1. The recommendation follows a two-step process. Inthe first step, the list of candidate pairs is generated. Namely, thetop 100 pairs purchased together are selected by ranking themaccording to either the number of times they have been purchasedtogether or the generated revenue in the past months. For theinitial intervention with SwipeRx, revenue was chosen. Keep inthe list only pairs in which both items are in stock. In the secondstep, user-specific filtering is made. First, for user , retain the listof = (, )s such that, without loss of generality, has beenpurchased recently, defined as / (0, 1). For user , pick such that = 1 or / (0, 1). Retain the pairs suchthat = 1. If no such exists for user , retain pairs with thehighest values for |/ / |, i.e., a large differencein expected recency between and . Finally, from the ultimatelyfinalized list of pairs, one is selected at random.",
  "Bandits and Adaptive Intervention Delivery": "The proposed framework aims to leverage data to intervene andnudge behavior. As such, the algorithmic piece deciding what in-terventions to use (or not), and when for each user is central. Theintervention decision problem can be modeled as a Markov Deci-sion Process (MDP), and reinforcement learning is the appropriateparadigm to drive personalization. It allows balancing betweenoptimization and knowledge extraction depending on the use caseand continually adapts to the evolving needs and preferences ofthe intervention subjects.Stochastic contextual and restless multiarmed bandits (MABs)are good frameworks for instances of personalization and resourceallocation where the long-term state evolution plays a minor role,such as those applications where we are concerned mainly with itsimmediate impact, or where simple, useful approximations to statedynamics can be found. Problems that need to robustly reconcileshort and long-term goals through sequential decisions that opti-mize for the multi-step problem, such as decisions associated withmedical treatment of certain conditions, require frameworks thatmodel the full MDP , in the spirit of collaborative, interac-tive recommendation systems , or at least addressthe problems of long-term credit assignment, when learning theintervention policy.Bandit algorithms can be thought of as online optimization meth-ods with built-in model identification from partial feedback: thealgorithm must make sequential decisions based on the observedinteraction history with the ultimate goal of eliminating subopti-mal choices . During the course of repeated interactions, thealgorithm observes a context , then, given the history of its pastcontexts, interactions, and their outcomes H< = {,,}<, itpicks an action from a finite set. It concludes the interaction byreceiving a response-reward . In this context, the action repre-sents the intervention decision at time , and both the context and reward are chosen from within the static and dynamic traitsdescribed in section 1.1.2. The (contextless) MABs are a specialcase, wherein the context is empty, i.e., an interaction-independentconstant .During its rollout, a bandit algorithm gradually hones in on aset of optimal actions with a guaranteed confidence level for the",
  "Adaptive User Journeys in Pharma E-Commerce with Reinforcement Learning: Insights from SwipeRxKDD CJ Workshop 24, August 25, 2024, Barcelona, Spain": "The authors want to thank Susan Murphy for insightful discussions.This work was supported, in whole or in part, by the Bill & MelindaGates Foundation INV-060956. Under the grant conditions of theFoundation, a Creative Commons Attribution 4.0 Generic Licensehas been assigned to the Author Accepted Manuscript version thatmight arise from this submission. Shipra Agrawal and Navin Goyal. 2013. Thompson Sampling for ContextualBandits with Linear Payoffs. In Proceedings of the 30th International Conferenceon Machine Learning. PMLR, Atlanta GA USA, 127135. ISSN: 1938-7228.",
  "Linear Bandits. A -armed linear bandit assumes a linearmodel of the reward conditional on a context-action pair: = + , where R is the feature vector, ()=1 R": "are the coefficients of the -the arm reward regression, and is aconditionally independent subgaussian noise .Originally used Upper Confidence Bound (UCB) for pickingactions, which selects arg max + ucb; H<, where is the current -th arms reward models coefficient estimatebased on H<, and the second term represents the arms optimismin face of uncertainty, computed from the best admissible linearmodel in a highly probable region determined from the interac-tion history [31, ch. 19]. The intervention described in is delivered as proposed in . Namely, a Bayesian approximationapproach with the Normal likelihood with Gaussian-Gamma conju-gate prior is used, which allows for the assignment of better-definedprobabilities for the selected actions using Thompson sampling: P arg max , where is the current poste-rior belief ( | H). Thompson Sampling is implemented as atwo-step sampling procedure. Still, it is worth noting that since theaffine transformation of the conditional Gaussian partof the Gaussian-Gamma distribution yields a Normal-Gamma mix-ture, it is possible to sample in a single step from a location-scalestudent-.The linearity assumption equips linear bandits with regret guar-antees that are sublinear in, which stem from subgaussian concen-tration inequalities. They present an efficient learning alternativefor use cases where the optimal action is to be selected based ona small collection of variables, for example, whether to send re-minders to take medication and/or the importance of treatment ad-herence depending on self-reported adherence (or lack thereof) and interaction with received reminders in the previous few timesteps.It is also the approach used to send the item-pair recommendationsintroduced in .2.1 with the setup and results discussed in. They facilitate causal inference (i.e., statistical reasoningon under which circumstances the reminders work best) and arethus very useful for adaptive experimentation (see sSction 2.4). 2.3.2Beyond Linear Bandits. Linear bandits present substantiallimitations in their representational power . Deeper featureextractors can be used, i.e., replacing the reward model with(,) (or other nonlinear models) and learning the relevantfeature representation along with the interaction. Increasing thecomplexity of the representations obscures statistical reasoningbut can be the best alternative for intervention decisions for whichoptimality is expected to depend in complex ways on a variety ofinterrelated variables. This is the case, for example, of suggestingproducts to order with the goal of minimizing stockouts. The bestrecommendations will depend in complicated ways on the evolu-tion of the demand and availability for multiple related productson different sites, the users in-app activity, ordering behavior, andresponses to previous suggestions, as well as on seasonal and envi-ronmental factors (disease outbreaks, weather, pollen levels,...).One of the most used approaches to developing deep neuralbandits is stacked neural-linear bandit: a linear bandit isgrafted atop a deep feature extractor, .For example, adopts the stacked neural-linear bandit ap-proach and utilizes a small-capacity experience replay queue ,and Gaussian-Gamma conjugate prior in the linear head to mitigatecatastrophic forgetting. In particular, the method used updates alower-level deep feature extractor on the experience buffer (mostrecent H<), then in an empirical Bayes fashion, re-fits a prior forthe bandits head using reward variance matching based on theupdated representations, and, finally, applies the Bayesian updateto recompute the posterior.The bandit problem is approached as a linearized Gaussian state-space model for the context-action-reward data in . Specifically,the observation equation for the exogenous context and arm ismodeled by a neural network (,;), with its parameters beingthe unobserved state, which follows stochastic constant dynamics.The authors approximated the posterior of using multivariateGaussian distribution and suggested updating it with partial feed-back observations using the extended Kalman Filter . Toreduce the space complexity of the posterior, they re-parameterized as a point in a low-dimensional linear subspace, either randomly,or inferred from the parameters trace during a pre-training step. 2.3.3Restless bandits. In contrast to multi-armed bandits, the rest-less bandit setup (RMAB) is closer to the full MDP RL formulationin that the present actions (or inactions) affect the future, makingplanning a more pressing concern. Subjects have a simple inter-nal state (often a binary switch, e.g., is the patient adhering totreatment or is the drug in stock for the pharmacy) with evolutionfollowing simple intervention dynamics that can be subject-specific.The number of available interventions (e.g. follow-up call to assessadherence and remind the patient of its importance or drug stocksavailable for distribution) is limited, and the goal is to maximize thenumber of subjects with the desired internal state (in the examples,",
  "Experimentation": "The RL-platform allows to perform experiments in order to mea-sure the impact of interventions. Different experimental designsare available. Assignment to control and the intervention strategieson trial can be fully random or adaptive, the latter using stochasticMABs (see section 2.4) . For repeated interventions(e.g., weekly order suggestions), intra-subject assignment (i.e., as-signment multiple times of the same subject throughout the exper-iment) is also an option to increase effective sample sizes whenthe impact of the intervention is expected to concentrate immedi-ately after it is delivered, in the so-called micro-randomized trialdesign when assignment is fully randomized.",
  "Intervention Impact Analysis": "The following subsections explore methods to assess the effective-ness of interventions and measure their impact. Some methodsare suitable only for randomized or adaptive designs, but all arerelevant in setups using a linear bandit for adaptive interventions,which function as adaptive experiments. In these experiments aportion of the user cohort is left out as a pure control1 to evaluatethe interventions overall effect. This approach is used in item-pairrecommendations, discussed in .",
  "XP1XP2XP3 adaptiveXP3 non-adaptive": "T-test: days with significant effect0%57%0%61%T-test: largest effect-0.12-0.28T-test: largest statistical power-0.65-0.81T-test: average effect-0.10-0.25T-test: average statistical power-0.55-0.72LMM: nudged that week16.8214.0011.31 per / 11.72 ranLMM: baseline expenditure90516063101Bandit: assigned to nudge26%70%28.6% per / 50.1% ranBandit: majority assigned to nudge1/88/91/7 per / 5/7 ranSuccessful recommendations18.2%22.9%11.2% (11.1% per / 11.2% ran)15.5% control groups is different is the Students (when both have equalvariance) or Welchs t-test (when they dont) . For the lon-gitudinal data collected under intervention, we apply the Welchunequal variance t-test both to the daily values of relevant metrics,as well as to the accumulated values for those metrics since thebeginning of the experiment. The test statistic for Welchs t-test",
  "+ (2/)2": "1). By comparing the calcu-lated value to the critical value from the t-distribution with thecalculated degrees of freedom at a chosen significance level we candecide whether to reject the null hypothesis. In practice, we can com-pute the p-value corresponding to the test statistic. If the p-value isless than , we reject the null hypothesis, indicating that there is astatistically significant difference between the two sample means.Whenever the null hypothesis is rejected, the effect size is computedas Cohend d ( =",
  "calculated as =": "2 / + 2 /), and the associated statisticalpower, i.e., the likelihood that the test is correctly rejecting the nullhypothesis, as Power = ( > /2, ) + ( < /2, ),where follows a non-central t-distribution with non-centralityparameter and degrees of freedom .We also perform stratified analyses to explore heterogenouseffects. That is, by dividing the study population into homogeneoussubgroups, or strata, based on one or more confounding variablesand performing the hypothesis testing within each stratum, we tryto understand which user traits condition the size of the effect. 2.5.2Linear mixed-effects models. A linear mixed effects model(LMM) extends the classical linear model by incorporating bothfixed effects, which are the same across individuals, and randomeffects, which vary between individuals. In RCT with repeatedmeasurements for the same subjects over time, LMMs model thewithin-subject correlation over time. Fixed effects are the systematicinfluences shared across all subjects, and random effects capturesubject-specific variability . The general form of a linearmixed effects model is = 0 + 1 + + , where isthe response variable for subject at time , 0 and 1 are fixedeffect coefficients, is the predictor variable for subject at time, is the random intercept for subject , assumed to be normallydistributed with mean 0 and variance 2, and is the residualerror term, assumed to be normally distributed with mean 0 andvariance 2.In the context of experiments with digital interventions for anadaptive user journey, LMMs can be used for baseline value andother covariate adjustements and to model the effect with time.For example, in a setup where the interventions are messages thatare sent adaptively to improve user engagement, we might specify = 0 +10 +2X +3M +4T +5(XT) + + where is the outcome for subject at time and its baseline value, is 1 if the user is part of the intervention and 0 if in pure control, indicates whether the subject was nudged at at time , 0 is theintercept, 1 the baseline adjustment, 2 is the fixed effect of thetreatment, 3 is the fixed effect o a nudge, 4 is the fixed effect oftime, 5 is the fixed effect of the interaction between treatment andtime, is the random effect for subject, and is the residual errorterm. The random effects are assumed to be normally distributedwith mean zero and variance 2, and the residual errors areassumed to be normally distributed with mean zero and variance2 . The time varying effect at time is 2+5T for users not nudgedat time and 2 +3 +5T for user who received a message at time. For the analysis presented in the results described in .2,",
  "Intervention Setup and Experiments": "The intervention involves sending weekly in-app messages withitem-pair recommendations (see .2.1), which appear aspopups (or on the homepage when they open the app if users wereinactive when they were sent). The messages, in Bahasa, state:\"Pharmacies in your area typically purchase A and B. Click hereto order now!\" Top 20% spenders were excluded from the initialexperiments, targeting users in Indonesia with Bahasa languagesettings, linked to pharmacies with one or two users, and whologged in regularly (at least once in the previous 40 days and atleast once a week on average on the previous two months). AGaussian-Gamma linear bandit model was used in the adaptive arm,with the (log transformed) total expenditure over the next week asthe reward. Three experiments were conducted, each lasting 8-10weeks. The context always included the normalized days since the last nudge, some measure of purchasing frequency (over the last90 days for the first experiment and over the last 30 days for thesecond and the third) and of baseline expenditure (over the last 90days for the first and third experiment and on the ongoing monthfor the second), with different experiments incorporating differentadditional traits.In the first experiment (XP1), messages were initially sent onTuesdays and later switched (on week 5) to Mondays (in both casesat 6 am local time) to align the nudging with the day of the weekwhen the largest fraction of purchases take place. The second ex-periment (XP2) used a smaller sample size, since all users in theintervention in XP1 were excluded. XP1 also included region asan additional context while XP2 incorporated login frequency (inthe last 60 days) and in-app time (in the last 30 days). For XP1 andXP2, around 30% and 40%of participants assigned to pure controlrespectively.The third experiment (XP3) added a non-adaptive (i.e., wheresubjects were nudged every Monday) treatment group with randomitem-pair recommendations (the same message as the personalizedones but with both items selected at random) and included an ad-ditional arm in the adaptive intervention with the same randomrecommendations. Participants from XP1 were included again, butnot from XP2. XP3 had a participant split of 5% non-adaptive, 60%adaptive, and 35% pure control, with context similar to XP2 butdiffering in the baseline expenditure periods and excluding 40%(instead of 20%) of top spenders. When discussing the results below,we will refer to the adaptive or MAB experimental group inter-changeably, and similarly for non-adaptive and AB (as in AB test).",
  "Results": "All impact metrics considered across the experiments are consis-tent with a significant uplift in expenditure due to the intervention.The impact is most prominent when the recommendations are anovelty, with the effect decaying as the experiments unfold. Userspreviously exposed to the same nudging in an earlier experimentare less likely to respond positively. There is, however, no indica-tion of this fatigue turning the effect into negative over the periodsconsidered. There is also some evidence that higher spenders reactbetter to the recommendations. While the most significant part ofthe effect is concentrated in the week a nudge is received, thereare indications of delayed effects. compares a few impactmetrics across the experiments for the different monitoring anglesdescribed in .5. Additional details are collected for theinterested reader in the appendices A to C, which include, for eachexperiment, whenever not included in the main paper, plots of thedaily and accumulated expenditures with associated t-test confi-dence intervals, the bandits rewards, arm assignment and t-snevisualization of best arm and confidence, and the breakdown byinteraction of successful recommendations; and tables with the ban-dits sensitivity to its context and the LMM estimated parameters,both for the full expression presented in .5.2 and for thefinal one with all terms significant (and used in ).The beginning of the first experiment (XP1) suffered from thelack of alignment between the nudging and the day of the weekwith higher purchasing engagement and from its start in the middleof the anomalous Christmas period. However, it shows an increase",
  "SUMMARY AND CONCLUSIONS": "We have introduced a framework enabling message- and content-based interventions in healthcare-related digital tools through inte-gration with an RL platform. We have also described how it couldenrich the user experience and provide an adaptive user journey.To illustrate this approach, we have discussed the results of a seriesof initial experiments with personalized and adaptive item-pairrecommendations delivered through in-app messages to customersof SwipeRx, the largest all-in-one app for pharmacies in SoutheastAsia. The significant increase in basket size measured highlights thepotential of this framework, which is flexible by design to enableintegration with already existing tools to scale its impact.",
  "Oren Barkan and Noam Koenigstein. 2016. Item2Vec: Neural Item Embedding forCollaborative Filtering. cite arxiv:1603.04259": "Konstantinos Benidis, Syama Sundar Rangapuram, Valentin Flunkert, YuyangWang, Danielle Maddix, Caner Turkmen, Jan Gasthaus, Michael Bohlke-Schneider,David Salinas, Lorenzo Stella, Franois-Xavier Aubet, Laurent Callot, and TimJanuschowski. 2022. Deep Learning for Time Series Forecasting: Tutorial andLiterature Survey. Comput. Surveys 55, 6 (Dec. 2022), 121:1121:36. Biswarup Bhattacharya. 2018. Restless Bandits visiting Villages: A PreliminaryStudy on distributing Public Health Services. In Proceedings of the 1st ACM SIGCASConference on Computing and Sustainable Societies (COMPASS 18). Associationfor Computing Machinery, New York, NY, USA, 18. Arpita Biswas, Gaurav Aggarwal, Pradeep Varakantham, and Milind Tambe.2021. Learn to Intervene: An Adaptive Learning Policy for Restless Bandits inApplication to Preventive Healthcare. In Proceedings of the Thirtieth InternationalJoint Conference on Artificial Intelligence, IJCAI-21, Vol. 4. International JointConferences on Artificial Intelligence Organization, Montreal, Canada, 40394046. ISSN: 1045-0823.",
  "Raaz Dwivedi, Susan Murphy, and Devavrat Shah. 2022. Counterfactual inferencefor sequential experimental design. arXiv:2202.06891 [stat.ML]": "Simen Eide and Ning Zhou. 2018.Deep neural network marketplace rec-ommenders in online experiments. In Proceedings of the 12th ACM Confer-ence on Recommender Systems (Vancouver, British Columbia, Canada) (Rec-Sys 18). Association for Computing Machinery, New York, NY, USA, 387391. William Fedus, Prajit Ramachandran, Rishabh Agarwal, Yoshua Bengio, HugoLarochelle, Mark Rowland, and Will Dabney. 2020. Revisiting Fundamentals ofExperience Replay. In Proceedings of the 37th International Conference on MachineLearning. PMLR, Virtual Only, 30613071.",
  "fedus20a.html ISSN: 2640-3498": "Ana Fernndez del Ro, Michael Brennan Leong, Paulo Saraiva, Ivan Nazarov,Aditya Rastogi, Moiz Hassan, Dexian Tang, and frica Periez. 2024. AdaptiveBehavioral AI: Reinforcement Learning to Enhance Pharmacy Services. In Pro-ceedings of the 1st AIBS ACM SIGKDD International Workshop on AI BehavioralScience. Association for Computing Machinery, Barcelona, Spain. Ronald A. Fisher. 1935. The Principles of Experimental Design and StatisticalAnalysis in Comparative Experiments. Philosophical Transactions of the RoyalSociety of London. Series B, Biological Sciences 221, 448-450 (1935), 309368.",
  "Wei Fu and Jeffrey S. Simonoff. 2016. Survival trees for left-truncated and right-censored data, with application to time-varying covariate data. Biostatistics 18, 2(2016), 352369": "Mihajlo Grbovic, Vladan Radosavljevic, Nemanja Djuric, Narayan Bhamidipati,Jaikit Savla, Varun Bhagwan, and Doug Sharp. 2015. E-commerce in Your Inbox:Product Recommendations at Scale. In Proceedings of the 21th ACM SIGKDDInternational Conference on Knowledge Discovery and Data Mining (Sydney, NSW,Australia) (KDD 15). Association for Computing Machinery, New York, NY, USA,18091818. Husnain Hamid, Rizwan Ali Masood, Hira Tariq, Wahab Khalid, Muham-mad Ateeb Rashid, and Muhammad Usman Munir. 2020. Current pharmacypractices in low- and middle-income countries: recommendations in response tothe COVID-19 pandemic. Drugs & Therapy Perspectives 36, 8 (may 2020), 355357. Geoffrey E Hinton and Sam Roweis. 2002.Stochastic Neighbor Em-bedding. In Advances in Neural Information Processing Systems, S. Becker,S. Thrun, and K. Obermayer (Eds.), Vol. 15. MIT Press, Vancouver,Canada, 857864. Eugene Ie, Vihan Jain, Jing Wang, Sanmit Narvekar, Ritesh Agarwal, Rui Wu,Heng-Tze Cheng, Morgane Lustman, Vince Gatto, Paul Covington, Jim McFadden,Tushar Chandra, and Craig Boutilier. 2019. Reinforcement learning for slate-basedrecommender systems: A tractable decomposition and practical methodology.arXiv:1905.12767 [cs.LG] Ifunanya Ikhile, Claire Anderson, Simon McGrath, and Stephanie Bridges. 2018.Is the Global Pharmacy Workforce Issue All About Numbers? American Journal ofPharmaceutical Education 82, 6 (aug 2018), 6818. Jackson A. Killian, Manish Jain, Yugang Jia, Jonathan Amar, Erich Huang, andMilind Tambe. 2023. Equitable Restless Multi-Armed Bandits: A General Frame-work Inspired By Digital Health. arXiv:2308.09726 [cs.LG] Jackson A. Killian, Andrew Perrault, and Milind Tambe. 2021. Beyond \"To Actor Not to Act\": Fast Lagrangian Approaches to General Multi-Action RestlessBandits. In AAMAS 21: 20th International Conference on Autonomous Agentsand Multiagent Systems, Virtual Event, United Kingdom, May 3-7, 2021, FrankDignum, Alessio Lomuscio, Ulle Endriss, and Ann Now (Eds.). ACM, VirtualOnly, 710718. Predrag Klasnja, Eric B. Hekler, Saul Shiffman, Audrey Boruvka, Daniel Almi-rall, Ambuj Tewari, and Susan A. Murphy. 2015. Microrandomized trials: Anexperimental design for developing just-in-time adaptive interventions. HealthPsychology 34 (2015), 12201228. Place: USPublisher: American Psychological Association.",
  "Tor Lattimore and Csaba Szepesvri. 2020. Bandit Algorithms. Cambridge Uni-versity Press, Cambridge": "Changhee Lee, Jinsung Yoon, and Mihaela van der Schaar. 2020. Dynamic-DeepHit: A deep learning approach for dynamic survival analysis with competingrisks based on longitudinal data. IEEE Transactions on Biomedical Engineering 67,1 (2020), 122133. Elliot Lee, Mariel S. Lavieri, and Michael Volk. 2019. Optimal Screening forHepatocellular Carcinoma: A Restless Bandit Model. Manufacturing & ServiceOperations Management 21, 1 (Jan. 2019), 198212. Publisher: INFORMS. Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. 2010. A contextual-bandit approach to personalized news article recommendation. In Proceedings ofthe 19th international conference on World wide web (WWW 10). Association forComputing Machinery, New York, NY, USA, 661670. Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, TomErez, Yuval Tassa, David Silver, and Daan Wierstra. 2019. Continuous controlwith deep reinforcement learning. arXiv:1509.02971 [cs.LG] Bryan Lim, Sercan O. Ark, Nicolas Loeff, and Tomas Pfister. 2021. TemporalFusion Transformers for interpretable multi-horizon time series forecasting.International Journal of Forecasting 37, 4 (Oct. 2021), 17481764.",
  "Rosalind Miller and Catherine Goodman. 2016. Performance of retail pharmaciesin low- and middle-income Asian settings: a systematic review. Health Policyand Planning 31, 7 (mar 2016), 940953": "Ofir Nabati, Tom Zahavy, and Shie Mannor. 2021. Online Limited MemoryNeural-Linear Bandits with Likelihood Matching. In Proceedings of the 38thInternational Conference on Machine Learning. PMLR, Virtual Only, 79057915. ISSN: 2640-3498. Siddharth Nishtala, Lovish Madaan, Aditya Mate, Harshavardhan Kamarthi,Anirudh Grama, Divy Thakkar, Dhyanesh Narayanan, Suresh Chaudhary, NehaMadhiwalla, Ramesh Padmanabhan, Aparna Hegde, Pradeep Varakantham,Balaraman Ravindran, and Milind Tambe. 2021. Selective Intervention Plan-ning using Restless Multi-Armed Bandits to Improve Maternal and Child HealthOutcomes. arXiv:2103.09052 [cs]. Babaniyi Yusuf Olaniyi, Ana Fernndez del Ro, frica Periez, and LaurenBellhouse. 2022. User Engagement in Mobile Health Applications. In Proceedingsof the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining(Washington DC, USA) (KDD 22). Association for Computing Machinery, NewYork, NY, USA, 47044712. Han-Ching Ou, Christoph Siebenbrunner, Jackson Killian, Meredith B. Brooks,David Kempe, Yevgeniy Vorobeychik, and Milind Tambe. 2022. NetworkedRestless Multi-Armed Bandits for Mobile Interventions. arXiv:2201.12408 [cs]. frica Periez, Ana Fernndez del Ro, Ivan Nazarov, Enric Jan, Moiz Hassan,Aditya Rastogi, and Dexian Tang. 2024. The Digital Transformation in Health:How AI Can Improve the Performance of Health Systems. (2024). (In press). frica Periez, Kathrin Smchmitz, Lazola Makhupula, Moiz Hassan, MoetiMoleko, Ana Fernndez del Ro, Ivan Nazarov, Aditya Rastogi, and Dexian Tang.2024. Optimizing HIV Patient Engagement with Reinforcement Learning inResource-Limited Settings. In Proceedings of the 7th epiDAMIK ACM SIGKDDInternational Workshop on Epidemiology meets Data Mining and Knowledge Dis-covery. Association for Computing Machinery, Barcelona, Spain. Tianchen Qian, Ashley E. Walton, Linda M. Collins, Predrag Klasnja, Stephanie T.Lanza, Inbal Nahum-Shani, Mashfiqui Rabbi, Michael A. Russell, Maureen A.Walton, Hyesun Yoo, and Susan A. Murphy. 2022.The microrandomizedtrial for developing digital interventions: Experimental design and data anal-ysis considerations. Psychological Methods 27, 5 (Oct. 2022), 874894. Yundi Qian, Chao Zhang, Bhaskar Krishnamachari, and Milind Tambe. 2016.Restless Poachers: Handling Exploration-Exploitation Tradeoffs in SecurityDomains. In Proceedings of the 2016 International Conference on AutonomousAgents & Multiagent Systems (AAMAS 16). International Foundation for Au-tonomous Agents and Multiagent Systems, Richland, SC, 123131. Carlos Riquelme, George Tucker, and Jasper Snoek. 2018. Deep Bayesian BanditsShowdown: An Empirical Comparison of Bayesian Deep Networks for ThompsonSampling. In International Conference on Learning Rrepresentations. PMLR, PlayaBlanca, Lanzarote, Canary Islands, Spain. David Salinas, Michael Bohlke-Schneider, Laurent Callot, Roberto Medico,and Jan Gasthaus. 2019.High-dimensional multivariate forecasting withlow-rank Gaussian Copula Processes. In Advances in Neural InformationProcessing Systems, Vol. 32. Curran Associates, Inc., Vancouver, Canada,68276837. David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. 2020.DeepAR: Probabilistic forecasting with autoregressive recurrent networks. Inter-national Journal of Forecasting 36, 3 (July 2020), 11811191. Matthias Seeger, Syama Rangapuram, Yuyang Wang, David Salinas, Jan Gasthaus,Tim Januschowski, and Valentin Flunkert. 2017. Approximate Bayesian Inferencein Linear State Space Models for Intermittent Demand Forecasting at Scale. arXiv:1709.07638 [cs, stat]. Gerald L. Smith, Stanley F. Schmidt, and Leonar A. McGee. 1962. Application ofStatistical Filter Theory to the Optimal Estimation of Position and Velocity on Boarda Circumlunar Vehicle. Technical Report. NASA.",
  "P. Whittle. 1988. Restless Bandits: Activity Allocation in a Changing World.Journal of Applied Probability 25 (1988), 287298. Applied Probability Trust": "Marvin N. Wright, Theresa Dankowski, and Andreas Ziegler. 2017. Unbiasedsplit variable selection for random survival forests using maximally selected rankstatistics. Statistics in Medicine 36, 8 (2017), 12721284. Ding Xiang, Rebecca West, Jiaqi Wang, Xiquan Cui, and Jinzhou Huang. 2022.Multi Armed Bandit vs. A/B Tests in E-Commerce - Confidence Interval andHypothesis Test Power Perspectives. In Proceedings of the 28th ACM SIGKDDConference on Knowledge Discovery and Data Mining (Washington DC, USA)(KDD 22). Association for Computing Machinery, New York, NY, USA, 42044214. Pan Xu, Zheng Wen, Handong Zhao, and Quanquan Gu. 2022. Neural Contex-tual Bandits with Deep Representation and Shallow Exploration. In The TenthInternational Conference on Learning Representations, ICLR 2022, Virtual Event,April 25-29, 2022. OpenReview.net, Virtual Only. Jiayu Yao, Emma Brunskill, Weiwei Pan, Susan Murphy, and Finale Doshi-Velez.2021. Power constrained bandits. In Proceedings of the 6th Machine Learning forHealthcare Conference (Proceedings of Machine Learning Research, Vol. 149), KenJung, Serena Yeung, Mark Sendak, Michael Sjoding, and Rajesh Ranganath (Eds.).PMLR, Virtual, 209259. Hsiang-Fu Yu, Nikhil Rao, and Inderjit S Dhillon. 2016. Temporal Regularized Ma-trix Factorization for High-dimensional Time Series Prediction. In Advancesin Neural Information Processing Systems, D. Lee, M. Sugiyama, U. Luxburg,I. Guyon, and R. Garnett (Eds.), Vol. 29. Curran Associates, Inc., Barcelona,Spain, 847855. Yantao Yu, Zhen Wang, and Bo Yuan. 2019. An Input-aware Factorization Machinefor Sparse Prediction. In Proceedings of the Twenty-Eighth International JointConference on Artificial Intelligence, IJCAI-19. International Joint Conferences onArtificial Intelligence Organization, Macao, China, 14661472.",
  "A.2Bandit Assignment and Sensitivity": "shows the evolution of the average reward per arm anddecision point for XP1s adaptive interventions arms, while the resulting proportions of assigned participants to each of thearms. contains the sensitivity classification of the item pairrecommendation arm to each of the contextual traits for XP1. Asthere are only two arms, sensitivity of the control arm is the samein magnitude and opposite sign. shows the t-sne visualization in the contextual traitspace of the best arm and and of its confidence computedas the difference in the probabilities of picking the best arm andsecond best arm (right) for XP1.",
  "A.5Qualitative Interviews": "All pharmacists interviewed had purchased at least one recommen-dation, three after closing the message, two after ignoring it andanother two after opening it. Several respondents recall closing themessage and then searching for the product in the app and othersappeared to view it as merely informative, so it seems it was unclear to many that the messages could be interacted with. A pharmacistsreferred their desire to be able to review the information later onas they could be busy when they received the message. Half ofthem recounted they found the recommendations useful and sev-eral showed interest in receiving tips when discounts are availableand when products they have looked for are back again in stock.",
  "Item-pair": "Expenditure previous 3 monthsDays since last nudge+Days with order previous 3 months-Bali region+Banten regionDI Yogyakarta regionDKI Jakarta region+Jawa Barat regionJawa Tengah regionJawa Timur region-NAD regionSurabaya region : T-sne visualization in the contextual trait spacefor XP1. The figure depicts its confidence (difference betweenthe probabilities of the best arm and the second best arm) onthe last decision point. The equivalent best arm plot can befound in 3.",
  "B.2Bandit Assignment and Sensitivity": "shows the evolution of the average reward per arm anddecision point for XP2s adaptive interventions arms, while the resulting proportions of assigned participants to each of thearms. contains the sensitivity classification of the item pairrecommendation arm to each of the contextual traits for XP2. Asthere are only two arms, sensitivity of the control arm is the samein magnitude and opposite sign. shows the t-sne visualization in the contextual traitspace of the best arm (left) and of its confidence computed as the",
  ": Fraction of participants assigned to each of XP2s adaptive intervention arms at each decision point": ": T-sne visualization in the contextual trait space for XP2. The figure to the left represents the best arm for eachparticipant at the last decision point, with the size of the point proportional to its confidence (difference between the probabilitiesof the best arm and the second best arm), which is also plotted in the figure to the right.",
  "C.2Bandit Assignment and Sensitivity": "shows the evolution of the average reward per arm anddecision point for XP3s adaptive interventions arms, while the resulting proportions of assigned participants to each of thearms. contains the sensitivity classification of all arms to each ofthe contextual traits for XP3 (there is a non trivial relation betweenthem as there are three arms and the sensitivities are obtained bynormalizing between -1 and +1 the soft-thresholded values). shows the t-sne visualization in the contextual traitspace of the best arm (left) and of its confidence computed as thedifference in the probabilities of picking the best arm and secondbest arm (right) for XP3.",
  "C.3LMM estimation": "contains the estimated parameters, standard errors, and p-value of the terms Wald test for the full LMM described in .5.2 for XP3, with the addition that the two types of intervention(adaptive and non-adaptive) and of nudges (personalized or randomitem-pair) are considered explicitly. Additionally, a parameter toaccount for the lack of novelty the recommendations representedfor those users that had already taken part in XP1 was also intro-duced. is the equivalent table for the fitted LMM where allcoefficients are significant.",
  "C.4Recommendation Success Analysis": "For XP3, for 11.6% of the messages, considering both the adap-tive and non-adaptive arms and both personalized and randomrecommendations, the item that had previously been infrequentlypurchased was ordered at a later time during the experiment. Fig-ure 21 reflects the breakdowns by type of interaction, experimentalgroup and recommendation type to the left and by interaction andrecommendation type only to the right.When focusing on the adaptive intervention experimental grouponly, 11.6% of all messages ended in purchase, 11.1% of the per-sonalized ones and 11.2% of the random ones. shows thebreakdown by interaction and recommendation type to the left andfor personalized recommendations only by interaction to the left,while s left plot for random recommendations.Figures 23 and explore in some detail the breakdownsfor random recommendations exclusively. The former shows dif-ferent breakdowns combining both the adaptive and non-adaptiveexperimental arms, while the latter shows the adaptive group tothe left and the non-adaptive to the right.",
  "PersonalizedRandomControl": "Expenditure previous 90 days---Days between logins last 60 days--In-app time last 30 days---Days since last nudge---Days since first login--Days with order previous 30 days-+-Opened nudges last 14 days--- : T-sne visualization in the contextual trait space for XP3. The figure to the left represents the best arm for eachparticipant at the last decision point, with the size of the point proportional to its confidence (difference between the probabilitiesof the best arm and the second best arm), which is also plotted in the figure to the right.",
  "Coef.Std.Err.p-value": "Intercept38.6433.8140.000Nudged that week (personalized)11.3073.2120.000Nudged that week (random)11.7232.6130.000XP1 participant-10.5064.0290.009Baseline expenditure905.24115.0930.000Week number-5.0330.3640.000 : Breakdown by interaction (opened, closed or ignored), experimental group (MAB for adaptive intervention, AB fornon-adaptive one), and recommendation type (personalized or random) of messages that lead to purchase in XP3 (11.6% of allmessages) to the left and by interaction and type of recommendation only (both experimental groups combined) to the right."
}