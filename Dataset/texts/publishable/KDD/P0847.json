{
  "ABSTRACT": "In this paper, we tackle the challenge of predicting stock movementsin financial markets by introducing Higher Order Transformers, anovel architecture designed for processing multivariate time-seriesdata. We extend the self-attention mechanism and the transformerarchitecture to a higher order, effectively capturing complex mar-ket dynamics across time and variables. To manage computationalcomplexity, we propose a low-rank approximation of the potentiallylarge attention tensor using tensor decomposition and employ kernelattention, reducing complexity to linear with respect to the data size.Additionally, we present an encoder-decoder model that integratestechnical and fundamental analysis, utilizing multimodal signalsfrom historical prices and related tweets. Our experiments on theStocknet dataset demonstrate the effectiveness of our method, high-lighting its potential for enhancing stock movement prediction infinancial markets.",
  "INTRODUCTION": "Predicting stock movements in financial markets is of paramountimportance for investors and traders alike, as it enables informeddecision-making and enhances profitability. However, this task is in-herently challenging due to the stochastic nature of market dynamics,the non-stationarity of stock prices, and the influence of numerousfactors beyond historical prices, such as social media sentiment andinter-stock correlations.Traditional approaches in stock prediction have primarily focusedon technical analysis (TA) and fundamental analysis (FA), lever-aging historical price data and key financial metrics, respectively. While these methods have provided valuable insights, theyoften fail to capture the complex interdependencies and the high-dimensional structure of financial data . Recent advancements inmachine learning, particularly in natural language processing andgraph neural networks, have begun to address these limitations byintegrating multimodal data sources, such as news articles and socialmedia sentiment, thereby offering a more nuanced understanding ofmarket dynamics .Despite these advancements, existing models still struggle withthe sheer volume and variability of financial data, often result-ing in suboptimal predictive performance when dealing with high-dimensional, multivariate time-series data. To address these chal-lenges, we introduce a novel architecture called Higher Order Trans-formers. This architecture extends the traditional transformer model by incorporating higher-order data structures in the self-attentionmechanism, enabling it to capture more complex interrelationshipsacross time and variables.The contributions of this paper can be summarized as follows:",
  "RELATED WORK": "Predicting stock movements has traditionally utilized technical anal-ysis (TA), focusing on historical price data and macroeconomicindicators, with common methods like GARCH and neural net-works being prevalent for their ability to identify temporal patterns. However, such methods often fail to account for exter-nal factors that can significantly influence market dynamics, thuslimiting their overall predictive scope.Fundamental analysis (FA) attempts to fill this gap by integratingbroader market elements such as investor sentiment and news, withthe help of advancements in natural language processing (NLP).NLP has enabled more effective sentiment analysis from diverse datasources like news articles and social media, thereby enhancing the FAapproaches . Innovative models such as the HybridAttention Networks (HAN) and StockNet have emerged, whichblend attention mechanisms and variational auto-encoders to analyzetext alongside price data, though the assumption of independentstock movements often hampers their effectiveness .Graph Neural Networks (GNNs) have been introduced as a so-lution to address the interconnected nature of financial markets, bystructuring market data into graph formats where nodes representcompanies, allowing for enhanced data representation and learn-ing through contextual inter-node relationships .These networks are particularly effective when combined with at-tention mechanisms and have been expanded upon with financialknowledge graphs to incorporate domain-specific knowledge .Despite progress in graph-based and NLP-enhanced stock predic-tion methods, fully integrated multimodal approaches that leverageboth textual data and inter-stock relationships are still in their in-fancy. Recent proposals for such multimodal models aim to harnessthese diverse data sets to improve the predictive accuracy of stock",
  "0if < 11otherwise(1)": "where represents the adjusted closing price1 on day . Here, = 0indicates that the stock price has decreased, and = 1 indicates thatthe stock price has increased.The objective of this task is to predict the price movement +1 ofa stock based on its historical price data and related tweets withina time window of days.",
  "METHOD5.1Tokenization": "In this section, we explain the process of tokenizing the inputmultivariate time-series data. Following prior work , we con-struct a price vector for each stock at each day in the form of, = , comprising the stocks adjusted closing price,highest price, and lowest price.In line with the multivariate time-series forecasting literature, we also found it beneficial to add date features, includingthe day of the month, month of the year, and year. The combinationof price and date features forms a six-dimensional vector for eachstock on each day.Inspired by , we add stock-specific learnable tokens to thebeginning of each time-series and treat them as the common CLStoken in transformer encoders. Similar to BERT and ViT , weused the hidden state of this special token as the stock representationover the whole time window for the classification task.",
  "Higher Order Transformer": "In this section, we first review the self-attention mechanism in Trans-former layers . Then, we extend it to higher orders by tensorizingqueries, keys, and values, thereby formulating higher order trans-former layers. Given that computing attention on tensors is prohibi-tively costly, we propose a low-rank approximation using Kroneckerdecomposition. Additionally, we incorporate the attention kerneltrick to significantly reduce the computational complexity. 5.2.1Standard Transformer Layer. A Transformer encoder layer comprises two primary components:a self-attention layer Attn : R R and an elementwisefeedforward layer MLP : R R. For a set of inputvectors R, a Transformer layer computes the following2:",
  "=1( (1) (2)),(9)": "where (1)and (2)represent lower-order attention matrices overthe variable and time dimensions, respectively. Having attentionmatrices acts similarly to multi-head attention with heads. Inpractice, we adopt the multi-head attention mechanism instead ofthe summation shown in Equation (9), as it uses more parametersand can be more expressive. THEOREM 5.1. Given any fourth-order attention tensor A R , which can be reshaped into a matrix R ,there exists a rank such that matrix can be expressed as thesum of Kronecker products of matrices R and R .Formally:",
  "=1 ,(15)": "where R and R . As approaches the rank of ,this approximation converges to an exact representation. Thus, weverify the theorems claim that any attention matrix derived fromtensor A can be expressed in terms of Kronecker products, achievinguniversal representability at full rank. Now we delve into the computation of the lower-order attentionmatrices (1) and (2). For simplicity, we omit the index . Weproject the input tensor X onto the query, key, and value tensorsusing mode-3 tensor products: Q = X 3 , K = X 3 , and",
  "(V 1 (1)):: = 11 (1(Q))(1(K)) V::,(21)": "where V:: denotes the tensor slice corresponding to the -th timestep.The computational sequence initiates by calculating ( (K)) V::followed by the subsequent operations, yielding a computationalcomplexity of O(2). The choice of kernel function is flexible,and we utilize the same kernel function as in , which has beenvalidated both theoretically and empirically. Similarly, the samecomputational approach is applied to compute (2) using the samekernel function .",
  "Model Architecture": "Our models architecture consists of a multilayer transformer net-work. Input tensors are transformed through a linear projection layerto align the features with the hidden dimensions required by themodel and its attention modules. We adopt pre-normalization tech-niques, specifically RMSNorm , in every layer following theapproach suggested by Touvron et al. . Rotary Positional Em-bedding is applied to the query and key matrices exclusivelyfor computing the temporal attention (2); stock-wise attention doesnot involve positional embeddings as the ordering in this dimensionis not meaningful.",
  ": Multimodal transformer architecture. As depicted inthe figure, tweet encodings are fed to the transformer encoder,and the historical price data are given to the transformer de-coder": "Inspired by , our proposed multimodal model follows anencoder-decoder architecture where the modality of the data differsacross encoder and decoder. More specifically, text encodings areprocessed by the transformer encoder, and price timeseries data bythe transformer decoder as presented in figure 2. Cross-attentionlayers in the network facilitate information blending between thesetwo modalities.",
  "EXPERIMENTS6.1Dataset": "We demonstrate the capability of Higher Order Transformers onstock market movement classification using a multimodal datasetcalled Stocknet which comprises historical data from 88 stocksextracted from Yahoo Finance3 and related news crawled from Twit-ter over two years. We adopt the data processing methodology asoutlined in , shifting a 5-day lag window along trading daysto generate samples. Samples are labeled based on the movementpercentage of the closing price, with movements 0.55% labeledas positive and 0.5% as negative. Samples lacking either prices ortweets on trading days are discarded to ensure data consistency.Text encodings are generated using the FinBert model by pro-cessing all tweets for each stock on each day through the transformer",
  "Model Configuration": "We configured the models using the Adam optimizer with an initiallearning rate of 0.0001. Tweet embeddings from FinBert are dimen-sioned at 768. Training extends for up to 1000 epochs with an earlystopping criterion based on validation F1 performance, stoppingfurther training when no improvement is observed. Hyperparam-eter tuning is conducted through a grid search with the followingparameters:",
  "Metrics": "In line with prior research in stock prediction , we evaluateclassification performance using three metrics: accuracy, F1 score,and Matthews correlation coefficient (MCC) 4. We particularlyinclude MCC as it provides a more balanced performance measurethat accounts for both positive and negative classes and adjusts forany class imbalance by incorporating True Negatives. This makes itespecially valuable in contexts where the data may be skewed. MCCis defined as:",
  "Results": "In this section, we analyze the benchmark performance of our modelagainst various baseline models on the StockNet dataset. The results,as summarized in , demonstrate the superior performanceof our model over all of the existing baselines except NL-LSTM,which significantly outperforms the rest of the methods in terms ofaccuracy, F1, and MCC. NL-LSTM reported the highest accuracyperformance on binary stock movement prediction by incorporatingfuzzy logic in sentiment analysis .",
  "Ablation Study": "We further investigate the impact of different aspects of our modelthrough an ablation study, focusing on the types of attention mech-anisms used, the data modalities, and the attention method. Theresults are provided in Tables 2, and 3 respectively.The influence of data modality on performance is depicted in. The multimodal approach that integrates both price data andTwitter news significantly outperforms single-modality approaches,underscoring the benefit of leveraging diverse data sources. More-over, the text-based models performs better than the timeseries-basedmodels with a significant gap, showing the rich context present in thenews data crawled from Twitter for the stock movement predictiontask. also explores the effect of using linear versus standardattention mechanisms over different modalities. The results high-light the advantages of linear attention in terms of efficiency andeffectiveness, particularly in multimodal settings.",
  "CONCLUSION": "In this paper, we presented the Higher Order Transformers, a novelarchitecture tailored to predict stock movements by processing mul-timodal stock data. By expanding the self-attention mechanism andtransformer architecture to incorporate higher-order interactions, ourmodel adeptly captures the intricate dynamics of financial marketsover both stock and time. To address computational constraints, weimplemented low-rank approximations through tensor decomposi-tion and integrated kernel attention to achieve linear computationalcomplexity. Extensive testing on the Stocknet dataset demonstratedthat our approach significantly surpasses most of the existing modelsin predicting stock movements. An ablation study further validatedthe effectiveness of specific architectural components, highlightingtheir contributory value to the models performance. Looking ahead,we plan to train our model on other multimodal stock datasets suchas ASTOCK and Dhaka Stock Exchange , and perform prof-itability analysis on real-world stock data to further test the practicalapplication and financial viability of our proposed method.",
  "Robert Goodell Brown. 2004. Smoothing, forecasting and prediction of discretetime series. Courier Corporation": "Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, An-dreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, LukaszKaiser, David Belanger, Lucy Colwell, and Adrian Weller. 2022. RethinkingAttention with Performers. arXiv:2009.14794 [cs.LG] Divyanshu Daiya and Che Lin. 2021. Stock movement prediction and portfoliomanagement via multimodal learning with transformer. In ICASSP 2021-2021IEEE International Conference on Acoustics, Speech and Signal Processing(ICASSP). IEEE, 33053309.",
  "Higher Order Transformers: Enhancing Stock Movement Prediction On Multimodal Time-Series Data": "Natthawut Kertkeidkachorn, Rungsiman Nararatwong, Ziwei Xu, and RyutaroIchise. 2023. FinKG: A Core Financial Knowledge Graph for Financial Analysis.In 2023 IEEE 17th International Conference on Semantic Computing (ICSC).IEEE, 9093. Raehyun Kim, Chan Ho So, Minbyul Jeong, Sanghoon Lee, Jinkyu Kim, andJaewoo Kang. 2019. Hats: A hierarchical graph attention network for stockmovement prediction. arXiv preprint arXiv:1908.07999 (2019). Qing Li, Jinghua Tan, Jun Wang, and Hsinchun Chen. 2020. A multimodal event-driven LSTM model for stock prediction using online news. IEEE Transactionson Knowledge and Data Engineering 33, 10 (2020), 33233337. Jintao Liu, Hongfei Lin, Xikai Liu, Bo Xu, Yuqi Ren, Yufeng Diao, and LiangYang. 2019. Transformer-Based Capsule Network For Stock Movement Prediction.In Proceedings of the First Workshop on Financial Technology and NaturalLanguage Processing, Chung-Chi Chen, Hen-Hsen Huang, Hiroya Takamura, andHsin-Hsi Chen (Eds.). Macao, China, 6673.",
  "Daiki Matsunaga, Toyotaro Suzumura, and Toshihiro Takahashi. 2019. Exploringgraph neural networks for stock market predictions with rolling window analysis.arXiv preprint arXiv:1909.10660 (2019)": "Tashreef Muhammad, Anika Bintee Aftab, Muhammad Ibrahim, Md. Mainul Ah-san, Maishameem Meherin Muhu, Shahidul Islam Khan, and Mohammad ShafiulAlam. 2023. Transformer-Based Deep Learning Model for Stock Price Pre-diction: A Case Study on Bangladesh Stock Market.International Journalof Computational Intelligence and Applications 22, 03 (April 2023). Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Hena Ghonia, RishikaBhagwatkar, Arian Khorasani, Mohammad Javad Darvishi Bayazi, GeorgeAdamopoulos, Roland Riachi, Nadhir Hassen, Marin Bilo, Sahil Garg, AndersonSchneider, Nicolas Chapados, Alexandre Drouin, Valentina Zantedeschi, YuriyNevmyvaka, and Irina Rish. 2024. Lag-Llama: Towards Foundation Models forProbabilistic Time Series Forecasting. arXiv:2310.08278 [cs.LG] Ramit Sawhney, Shivam Agarwal, Arnav Wadhwa, and Rajiv Shah. 2020. Deepattentive learning for stock movement prediction from social media text andcompany correlations. In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP). 84158426. Ramit Sawhney, Shivam Agarwal, Arnav Wadhwa, and Rajiv Ratn Shah. 2020.Spatiotemporal hypergraph convolution network for stock movement forecasting.In 2020 IEEE International Conference on Data Mining (ICDM). IEEE, 482491. Yejun Soun, Jaemin Yoo, Minyong Cho, Jihyeong Jeon, and U Kang. 2022.Accurate Stock Movement Prediction with Self-supervised Learning from SparseNoisy Tweets. In 2022 IEEE International Conference on Big Data (Big Data).16911700.",
  "Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and YunfengLiu. 2023. RoFormer: Enhanced Transformer with Rotary Position Embedding.arXiv:2104.09864 [cs.CL]": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-AnneLachaux, Timothe Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro,Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guil-laume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models.arXiv:2302.13971 [cs.CL] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J. Zico Kolter, Louis-PhilippeMorency, and Ruslan Salakhutdinov. 2019. Multimodal Transformer for Un-aligned Multimodal Language Sequences. arXiv:1906.00295 [cs.CL]",
  "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. Attention Is All YouNeed. arXiv:1706.03762 [cs.CL]": "Qianqian Xie, Weiguang Han, Yanzhao Lai, Min Peng, and Jimin Huang. 2023.The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModalStock Movement Prediction Challenges. arXiv:2304.05351 [cs.CL] Yumo Xu and Shay B Cohen. 2018. Stock movement prediction from tweets andhistorical prices. In Proceedings of the 56th Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Papers). 19701979. Zheng Yangjia, Li Xia, Ma Junteng, and Chen Yuan. 2022. Fundamental Analysisbased Neural Network for Stock Movement Prediction. In Proceedings of the 21stChinese National Conference on Computational Linguistics, Maosong Sun, YangLiu, Wanxiang Che, Yang Feng, Xipeng Qiu, Gaoqi Rao, and Yubo Chen (Eds.).Chinese Information Processing Society of China, Nanchang, China, 973984.",
  "Biao Zhang and Rico Sennrich. 2019. Root Mean Square Layer Normalization.arXiv:1910.07467 [cs.LG]": "Qiuyue Zhang, Chao Qin, Yunfeng Zhang, Fangxun Bao, Caiming Zhang, andPeide Liu. 2022.Transformer-based attention network for stock movementprediction. Expert Systems with Applications 202 (2022), 117239. Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,and Wancai Zhang. 2021. Informer: Beyond Efficient Transformer for LongSequence Time-Series Forecasting. arXiv:2012.07436 [cs.LG] Jinan Zou, Haiyao Cao, Lingqiao Liu, Yuhao Lin, Ehsan Abbasnejad, andJaven Qinfeng Shi. 2022. Astock: A New Dataset and Automated Stock Tradingbased on Stock-specific News Analyzing Model. arXiv:2206.06606 [cs.CL]"
}