{
  "ABSTRACT": "Accurate traffic forecasting is crucial for the development of In-telligent Transportation Systems (ITS), playing a pivotal role inmodern urban traffic management. Traditional forecasting meth-ods, however, struggle with the irregular traffic time series result-ing from adaptive traffic signal controls, presenting challengesin asynchronous spatial dependency, irregular temporal depen-dency, and predicting variable-length sequences. To this end, wepropose an Asynchronous Spatio-tEmporal graph convolutionalnEtwoRk (ASeer) tailored for irregular traffic time series forecast-ing. Specifically, we first propose an Asynchronous Graph Diffu-sion Network to capture the spatial dependency between asyn-chronously measured traffic states regulated by adaptive trafficsignals. After that, to capture the temporal dependency withinirregular traffic state sequences, a personalized time encoding isdevised to embed the continuous time signals. Then, we propose aTransformable Time-aware Convolution Network, which adaptsmeta-filters for time-aware convolution on the sequences withinconsistent temporal flow. Additionally, a Semi-AutoregressivePrediction Network, comprising a state evolution unit and a semi-autoregressive predictor, is designed to predict variable-length traf-fic sequences effectively and efficiently. Extensive experiments on anewly established benchmark demonstrate the superiority of ASeercompared with twelve competitive baselines across six metrics. 1",
  "Corresponding author.1This project is available at": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain. 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "Traffic forecasting; irregular time series analysis; convolutionalnetworks; spatio-temporal modeling": "ACM Reference Format:Weijia Zhang, Le Zhang, Jindong Han, Hao Liu, Yanjie Fu, Jingbo Zhou,Yu Mei, and Hui Xiong. 2024. Irregular Traffic Time Series ForecastingBased on Asynchronous Spatio-Temporal Graph Convolutional Networks.In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discoveryand Data Mining (KDD 24), August 2529, 2024, Barcelona, Spain. ACM, NewYork, NY, USA, 12 pages.",
  "INTRODUCTION": "Recent years have witnessed significant advancements in trafficforecasting, which plays a pivotal role in underpinning IntelligentTransportation Systems (ITS) , facilitating emergency responseand management , and is integral to the development of au-tonomous driving . In particular, timely and accurate trafficforecasting is of great importance to help the Intelligent TrafficSignal Control Systems (ITSCS) to anticipate future traffic state vari-ations, thereby providing crucial insights to support the systematicanalysis, informed decisions, and optimal control optimization ofITSCS to enhance the overall transportation system efficiency .In practice, the traffic dynamics of the road network is jointlydecided by the vehicles on the road and the intervention of trafficsignals, e.g., intersection traffic lights, ramp metering lights, andlane allocation signals . On the one hand, the trafficsignal adaptively adjusts its control cycles in response to real-timetraffic flow variations . On the other hand, traffic flows aredynamically regulated by these adaptive signal control strategieswith varying cycle lengths. As a result, the urban traffic states,entangling both length-varying traffic signal cycles and the corre-sponding traffic flows, exhibit significant irregularity and rendermore complex traffic dynamics, as depicted in .",
  "KDD 24, August 2529, 2024, Barcelona, Spain.Weijia Zhang et al": ": The framework overview of ASeer, which consists of three major components: Asynchronous Graph DiffusionNetwork (AGDN), Transformable Time-aware Convolution Network (TTCN), and Semi-Autoregressive Prediction Network(SAPN). The traffic states are first inputted to AGDN to obtain spatial representations, which are incorporated by TTCN toacquire the spatiotemporal representations. After that, SAPN predicts the variable-length traffic state sequence based on thespatiotemporal representations. Throughout the entire process, personalized time encoding is used to embed continuous time. embed cycle-related patterns for each node that will be detailed inthe next section.Once the proximity weights are obtained, we asynchronouslyintegrate nodes stored traffic messages received from neighbors viaan attentive graph convolution to obtain the spatial representation:",
  "Irregular Traffic Time Series Forecasting Based on Asynchronous Spatio-Temporal Graph Convolutional NetworksKDD 24, August 2529, 2024, Barcelona, Spain": "time encoding function with separate parameters so that can learnits unique cycle-related patterns.Due to the data missing problem, some nodes may have toosparse measurement data to learn a satisfactory unique time encod-ing function. Hence, we also jointly learn a generic time encoding(), which has a similar function expression to Eq. (7) but isshared by all nodes. Then, we introduce a learnable weight foreach node to adaptively integrate the above two time encoding:",
  "METHODOLOGY": "Framework Overview. shows the framework overviewof ASeer, which consists of three major components. Specifically,Asynchronous Graph Diffusion Network (AGDN) models asynchro-nous spatial dependency based on a traffic diffusion graph. Whena node (i.e., traffic sensor) has a traffic state measurement, AGDNasynchronously diffuses the nodes traffic measurement to its neigh-bors, which receive and store the diffused traffic state into theirmessage buffers. Next, the node performs an asynchronous graphconvolution to obtain spatial representation through attentivelyintegrating the stored traffic messages, and then the buffer willbe cleared. After that, a Transformable Time-aware ConvolutionNetwork (TTCN) is adopted to model the temporal dependencywithin irregular traffic state sequences. TTCN learns meta-filtersto derive time-aware convolution filters with transformable filtersizes based on spatial representations obtained from AGDN andtraffic measurements along with personalized time encoding. Thenthe derived time-aware convolution filters are applied for efficienttemporal convolution on irregular traffic state sequences to acquirethe spatiotemporal representation for each node. Finally, a Semi-Autoregressive Prediction Network (SAPN) is devised to iterativelypredict variable-length traffic state sequences. In each predictionstep, a State Evolution Unit (SEU), whose hidden state is initializedby spatiotemporal representations, is first introduced to evolve eachnodes future traffic hidden state with the elapsed time, then a Semi-Autoregressive Predictor (SAP) is adopted to predict a sequenceof consecutive traffic states based on both evolutionary and initialtraffic hidden states, as well as predicted elapsed time.",
  "Asynchronous Spatial DependencyModeling": "Previous traffic forecasting studies model spatial dependency byintroducing graph neural networks to synchronously diffuse and ag-gregate time-aligned traffic states between different sensor nodes . However, in our problem, the observed traffic state measure-ments of different sensors cannot be aligned due to the distincttimestamps of their traffic signal cycles and the data missing issue,which causes severe asynchrony in spatial dependency modeling.To this end, by linking sensors via a traffic diffusion graph, wepropose an Asynchronous Graph Diffusion Network (AGDN), asillustrated in , to model asynchronous spatial dependencybetween time-misaligned traffic measurements. We detail it below.Diffusion Graph Construction. To model spatial dependencybetween traffic sensors, we construct a traffic diffusion graph G =(V, XV, E, XE), where the graph nodes V = V represents a setof sensors, XV = X[T+1: ] denotes features of nodes V, E area set of edges indicating proximity between nodes, and XE arefeatures in edges E. Specifically, we define proximity E as: = 1 if the geographical distance between node and issmaller than a threshold , otherwise = 0, and there is no self-loop for each node. We also define some edge features XEbetween nodes and , including geographical distance and thedirect reachability in the lane-level road network. Note that it isnot limited to geographical proximity and reachability, other graphconstruction approaches can also be embraced.Asynchronous Diffusion and Storage. Assume a traffic statemeasurement of node is observed at timestamp , then",
  "Store {B : N }.(3)": "Since the timestamps of traffic state measurements are misalignedfor different nodes, the traffic messages diffusion and storage pro-cesses perform in an asynchronous way.Asynchronous Graph Convolution. An immediate problem ishow to exploit traffic messages stored in the message buffer toenhance each nodes spatial perception. We achieve this by enforc-ing each node asynchronously integrates the traffic messagesin its message buffer B via an asynchronous graph convolutionoperation, which is triggered when a measurement is observed.Specifically, we first employ to query the message buffer B",
  ",(5)": "where MLP represents a multi-layer perceptron. It is noteworthythat after each asynchronous graph convolution operation on B,all the traffic messages in it will be cleared. It indicates that eachnode only integrates adjacent traffic messages from its last trafficmeasurements timestamp to the current measurements timestamp, which guarantees each message is utilized exactly once to avoidredundant information and computation.There could be some messages received and stored in the mes-sage buffer B after timestamp of the last observed traffic statemeasurement of node during historical time window T. Thus,we perform a similar asynchronous graph convolution operationfor these remaining messages by adding a virtual measurement at timestamp without traffic state values. The obtained spatial",
  "Irregular Temporal Dependency Modeling": "Convolutional Neural Network (CNN) is widely applied to clas-sical traffic forecasting tasks for its both efficiency and effectivenessin temporal dependency modeling . However, apply-ing CNN to our task faces two problems. First, CNN fails to directlyprocess irregular traffic sequences with variable sequence lengths.Second, CNN is incompetent to model temporal dependency inthe sequence with varying time intervals, as its filter parameters",
  "are fixed and cannot adaptively adjust according to the inconsis-tent temporal flow of sequence, which leads to distinct patterns ofsequence dependencies": "To tackle the above problems, we propose a Transformable Time-aware Convolution Network (TTCN) that enables to model irregularsequences with transformable time-aware convolution filters, andfurther devise a personalized time encoding function to embedthe unique cycle-related patterns for each node. Specifically, giventhe historical time window T before , for each node , we firstconcatenate the traffic state measurement during T with thecorresponding spatial representation and the encoding of timeintervals ( ) from timestamp to timestamp of thelast observed traffic measurement:",
  "= ( ).(6)": "3.2.1Personalized Time Encoding. The desired time encodingshould not only indicate the absolute time interval but also implythe unique cycle-related patterns of traffic dynamics in differentnodes. For example, a time interval may signify a distinct numberof traffic signal cycles for different sensors, which is important fortemporal dependency modeling, especially when the time intervalspans multiple missing traffic states. Inspired by the positionalencoding in Transformer , we introduce a personalized timeencoding by adopting a learnable trigonometric function to embedthe time interval for each node:",
  "is initialized to be close to zero so that the nodes with limited oreven no available data can weigh more on generic time encoding": "3.2.2Transformable Time-aware Convolution Network. Thissection assumes all the operations performed on node , thus weomit the superscript to ease the presentation. We first definez[T+1: ] = and as the sequence length. As illus-trated in , we leverage meta-filters to derive the time-awareconvolution filters with dynamic parameters and transformablefilter size based on sequence inputs, formulated as:",
  "Norm (F ()) =exp(F ()) z[T+1: ] exp(F ()) ,(9)": "where f R is the derived filter for -th feature map, andF denotes a meta-filter that can be instantiated by learnable neuralnetworks. We normalize the derived filter parameters along thetemporal dimension to ensure consistent scaling of the convolutionresults for variable-length sequences.With filters derived according to Eq. (9), we obtain the traf-fic sequence representation R via the following temporalconvolution operation:",
  "where denotes the convolution operation. Then we attain theoverall spatiotemporal representation for each node via the repre-sentations integration: h = +": "Compared to CNN, TTCN has several advantages in adaptivelymodeling sequences with inconsistent temporal flow. First, the de-rived filter is transformable according to sequence length, whichenables it to adaptively process variable-length sequences. More-over, it can derive tailored parameterized filters for sequences withchangeable temporal flow or other characteristics. Furthermore, itis worth noting that as the learnable parameters of meta-filter areindependent of sequence length, TTCN is allowed to directly modelthe long-term temporal dependency via an arbitrarily large-sizeconvolution filter without increasing any filter parameters.",
  "Variable-Length Traffic Sequence Prediction": "Our goal is to predict the complete traffic state sequences, includinga sequence of traffic signal cycle lengths and the correspondingtraffic flows, for all nodes in a future time window. However, thesequences to be predicted have variable lengths in terms of thedifferences in sensors, time windows, or prediction algorithms, and the sequence lengths cannot be known in advance. While anautoregressive prediction model that iteratively predicts the nextsteps value based on previously predicted values seems feasiblefor the variable-length sequence prediction, the prediction for longsequence can lead to severe error accumulation and poor predictionefficiency issues in this approach . To tackle the above problems, as displayed in , we designa Semi-Autoregressive Prediction Network (SAPN) to iterativelypredict sub-sequences until the complete sequence meets the re-quirements of the task. It not only enables variable-length sequenceprediction in an efficient way but also mitigates the error accumula-tion issue for long sequence prediction. Since prediction processesare the same for all nodes, we omit the superscript to ease thepresentation as well.",
  "+(+1)=++1 = SAPh++1 h ( ++1), (11)": "where is the prediction step size, 0 denotes -th predictionstep, +(+1)=++1 respectively represent a sequence of con-secutive cycle lengths and unit time (per second) traffic flows, and++1 indicates the elapsed time to the timestamp of nodeslast observed measurement. ++1 is initialized to 1 when = 0,and iteratively updates based on the accumulation of predictedcycle lengths:",
  "=++1 = +(+1)=++1 +(+1)=++1 ,(14)": "where denotes Hadamard product. By iteratively performingthe above prediction step until the predicted sequence covers therequired time window, we can derive the variable-length trafficstate sequence we expect.Compared to autoregressive models, our SAPN predicts a variable-length sequence with fewer prediction steps, which improves pre-diction efficiency and may reduce the risks of causing predictionerror accumulation. It is evident that both the autoregressive andnon-autoregressive prediction models can be regarded as a specialcase of semi-autoregressive model when the prediction step size isset to one or the length of sequence. Thus, our SAPN can also beconsidered as incorporating both strengths of autoregressive and",
  "+,(15)": "+ is a mask term, which equals zero if the ground truth value + is missing, otherwise it equals one, and 1 denotes the numberof nonzero mask items for each node.To further mitigate error accumulation in cycle length prediction,we additionally introduce a timing loss to improve the accuracy ofpredicted elapsed time accumulated by cycle lengths:",
  "EXPERIMENTS4.1Experimental Setup": "Datasets. We conduct experiments on two real-world datasets,Zhuzhou and Baoding, which represent two major pilot citiesfor ITSCS and autonomous driving in China. Both datasets consistof a set of entrance lanes connecting to smart intersections andtraffic state measurements of lanes collected by the installed camerasensors. The statistics of the datasets are summarized in .We take the data from the first 60% of the entire time range as thetraining set, the following 20% for validation, and the remaining20% as the test set. For both datasets, we set both historical andpredicted time window lengths T and to one hour. Please referto Section A.1 for more description and analysis of the datasets.Implementation Details. All experiments are performed on aLinux server with 20-core Intel(R) Xeon(R) Gold 6148 CPU @2.40GHz and NVIDIA Tesla V100 GPU. We calculate spherical dis-tance as the geographical distance and choose distance threshold = 1km and prediction step size = 12. We adopt three layersMLPs for asynchronous graph convolution, semi-autoregressivepredictor, and meta-filters. The dimension for time encoding is setto = 16, and dimensions for convolution filters, state evolution",
  "/ 21364 / 155": "unit, and hidden layers of the above MLPs are all set to 64. To reduceparameter magnitude, in the implementation, we individualize thelast layers parameters but share the other parameters of MLP formeta-filters. We employ Adam optimizer to train our model, setlearning rate to 0.001. ASeer and all learnable baselines are trainedwith an early stop criterion if the loss doesnt decrease lower onthe validation set over 10 epochs.Evaluation Metrics. We define six metrics to comprehensivelyevaluate the forecasting performance, including C-MAE, C-RMSE,and C-MAPE to evaluate the accuracy of predicted traffic signalcycle lengths, and F-MAE, F-RMSE, and F-AAE for the traffic flowprediction evaluation. Lower is better for all these metrics.For the evaluation of signal cycle length prediction, we quan-tify the predicted errors of both the beginning timestamps andcycle lengths via masked Mean Absolute Error (MAE), Root MeanSquared Error (RMSE), and Mean Absolute Percentage Error (MAPE):",
  ",(21)": "where = / , and = /, are thepredicted and ground truth traffic flow densities of sensor attimestamp , respectively. is the mask term at , which equalsone if can be obtained from observed measurement, and zerootherwise. In our experiments, the timestamp is in seconds, and weuse a normalization term Z to obtain the average result in minutes.Baselines. We compare our approach with the following twelvebaselines, including two heuristic approaches (LAST, HA), twoclassical sequence modeling approaches (TCN , GRU ), fourcompetitive irregular time series modeling approaches (T-LSTM ,GRU-D , mTAND , Warpformer ), and four competitiveclassical traffic forecasting approaches (DCRNN , GWNet ,STAEformer , PDFormer ). For fair comparison, all learnablebaseline models are set to predict the cycle lengths and unit timetraffic flows by optimizing the hybrid loss function in Eq. (18) likeASeer. In addition, except for autoregressive models (GRU, T-LSTM,GRU-D, DCRNN), other baselines predict in a semi-autoregressivemanner with the same prediction step size as ASeer. We carefullytune major hyper-parameters of each baseline based on their rec-ommended settings for better performance on our datasets. Pleaserefer to Section A.2 for more details of baselines.",
  "Overall Performance": "reports the overall performance of ASeer and all com-pared baselines on two datasets w.r.t. six metrics. As can be seen,ASeer achieves the best overall performance among all the com-pared approaches on two datasets, which demonstrates our modelssuperiority in irregular traffic forecasting task. Besides, we haveseveral observations. Firstly, all learnable approaches outperformthe statistical approaches (i.e., LAST, HA), which validates thatthe data-driven approaches to learn complex non-linear interac-tions within traffic data is helpful for this task. Secondly, we findCNN-based baselines TCN and GWNet do not achieve a desiredperformance for the reason that classical CNN with the fixed param-eterized filters is incompetent to model the temporal dependency in irregular sequences. Thirdly, we observe ASeer obtains a supe-rior overall performance than approaches (i.e., GRU-D, T-LSTM,mTAND, and Warpformer) for irregular time series, as these ap-proaches fail to model the complex spatial dependencies betweenlarge-scale sensors. From these approaches, we notice mTAND has aslight advantage in C-MAPE than ASeer on Zhuzhou. This is prob-ably because mTAND as a powerful approach for interpolation taskperforms well in the short-term future cycles beginning times pre-diction. However, ASeer significantly outperforms mTAND in theother metrics. Lastly, we observe a notable performance improve-ment by comparing ASeer with the state-of-the-art approaches (i.e.,DCRNN, GWNet, STAEformer, and PDFormer) for classical traf-fic forecasting. The improvement can primarily be attributed tothe capability of ASeer to effectively model asynchronous spatialdependency and irregular temporal dependency in the irregulartraffic forecasting problem.",
  "Ablation Study": "We evaluate the performance of ASeer and its four variants onboth Zhuzhou and Baoding across six metrics. (1) w/o AGDNremoves the AGDN module; (2) w/o TTCN replaces TTCN witha 1D CNN, whose filter size is set to the maximal sequence lengthin the dataset; (3) w/o PTE removes personalized time encoding;(4) w/o SAPN replaces SAPN with an autoregressive MLP predic-tor. The results of ablation study are shown in , As can beseen, removing any component causes notable overall performancedegradation compared to ASeer, which demonstrates the effective-ness of each component. From these results, we observe w/o TTCNalmost results in significant performance descent for all metrics onboth datasets, which verifies the effectiveness of TTCN to improveclassical CNN to model the temporal dependency within irregulartraffic sequences. In addition, w/o AGDN causes a remarkable ac-curacy decline for all the metrics w.r.t. traffic flow, which validatesthe effect of AGDN on modeling asynchronous spatial dependencyof traffic dynamics. We also observe w/o AGDN causes a moreobvious accuracy decline on Baoding than Zhuzhou for threemetrics w.r.t. cycle lengths. This is probably because the distribu-tion of cycle lengths in Baoding is denser, AGDNs smoothnessinduces a more precise prediction. Moreover, we notice that w/oPTE leads to a consistent performance reduction for all metrics on",
  ": Results of ablation study. \"Z\" and \"B\" denoteZhuzhou and Baoding, respectively": "both datasets, which demonstrates that a well-learned personalizedtime encoding function to embed continuous time for each sensorcan facilitate the prediction of both cycle lengths and traffic flows.Finally, by comparing ASeer with w/o SAPN, we observe a moreobvious performance degradation on Baoding for metrics w.r.t.cycle lengths, which is probably because the sequence is longer onBaoding, an autoregressive model causes a severe error accumu-lation problem on cycle length prediction. w/o SAPN also showsa consistent performance descent for three metrics w.r.t. trafficflow, which confirms that SAPN improves the long traffic sequenceprediction performance.",
  "Parameter Sensitivity": "We conduct experiments for two important hyper-parameters, i.e.,the prediction step size and dimension of all hidden layers, onboth Zhuzhou and Baoding to study the sensitivity of these hyper-parameters. We report experimental results on metrics C-MAE,F-MAE, and F-AAE to evaluate the models prediction performanceon both cycle lengths and traffic flows. shows the results of varying the prediction step size from 1 to 48. As can be seen, there is a notable overall predictionperformance improvement by increasing from 1 (autoregressivemodel) to 12 (semi-autoregressive model), which demonstrates theeffectiveness of SAPN to mitigate error accumulation problem inthe autoregressive prediction model. However, we also observe aperformance degradation when the prediction step size is too large.This is probably because a too-large prediction step size may resultin under-training for SAPN to make predictions based on differentelapsed times.",
  ": Effect of different hidden dimensions": "We vary the dimension of models all hidden layers from 16 to 256.The results are shown in . We can observe a remarkableprediction performance improvement by increasing the hiddendimension from 16 to 32, and the performance is continuouslyimproving and achieves the best when the dimension is set to128. However, a larger hidden dimension also takes more expensivecomputational overhead. Thus, we have to balance the performanceand computation cost for the selection of models hidden dimension.",
  "Prediction Efficiency Analysis": "We conduct experiments to test the prediction efficiency of differentmodels. To ensure a fair comparison, we eliminate the influenceof different models on the prediction lengths by standardizing theprediction process. This involves allowing all models to predict themaximum lengths of the corresponding ground truth sequences.Efficiency of SAPN. To evaluate the effect of SAPN on predictionefficiency, we conduct experiments on both Zhuzhou and Baodingto specifically test SAPNs average prediction latency based on dif-ferent prediction step sizes from 1 to 48. We report the respectiveresults of predicting future 1, 4, and 24 hours traffic states in Fig-ure 6. As can be seen, the prediction latency is notably reduced bycomparing semi-autoregressive models ( > 1) with autoregressivemodel ( = 1) due to the reduction of total prediction steps. Themagnitude of latency reduction even approaches the prediction stepsize when we predict longer sequences or the step size is not toolarge, which demonstrates the significant effectiveness of SAPN toimprove prediction efficiency. We also observe with the predictionstep size increasing, the prediction latency is consistently reduced,and with the predicted hours rising, the model can have a signif-icantly higher prediction efficiency by setting a larger predictionstep size. This observation indicates that we can choose a largerprediction step size with the predicted sequence length increasingfor higher prediction efficiency.Efficiency of TTCN. To study TTCNs efficiency, we replace TTCNwith several commonly used modules in temporal modeling, i.e.,CNN, GRU, and Transformer, and test their running time costs. Asillustrated in , TTCN achieves more than 40% and 33% fasterresults than GRU and transformer, respectively, on both datasets.Furthermore, to our surprise, TTCN exhibits even faster than CNN.",
  "RELATED WORK": "Traffic Forecasting. Recently years, deep learning models havedominated the traffic forecasting tasks for their extraordinary ca-pability in modeling the complex spatio-temporal characteristicswithin traffic data .For spatial modeling, a part of studies first partition a cityinto a grid-based region map, then utilize Convolutional NeuralNetworks (CNNs) to capture spatial dependencies between adjacentregions. After that, Graph Neural Networks (GNNs) are widely used to model the non-euclidean spatial dependen-cies in traffic data . For example, studies employ GNNs to model the traffic flow diffusion process in the roadnetwork. Studies incorporate attention mecha-nism into GNNs to learn the dynamic spatial dependencies betweenthe road network sensors. In addition to the pre-defined relationalgraph derived from road networks, some works attempt to directly learn the latent graph structure from traffic data.For temporal modeling, CNNs and RecurrentNeural Networks (RNNs) are frequently adopted to cap-ture temporal dependencies within traffic data. Compared to RNNs,CNNs enable parallel computing for all time steps, which exhibitsextreme advantages in computational efficiency. However, all themethods of above studies are designed for the time-aligned trafficdata with fixed time interval, which fails to handle the challengesof asynchronous spatial dependency and irregular temporal depen-dency in the irregular traffic forecasting problem.Irregularly Sampled Time Series. This work is also related tothe literature about learning from irregularly sampled time series,which is a kind of time series data characterized by varying time intervals between temporally adjacent observations . A straight-forward approach is to divide the irregularly sampled time seriesinto a regular one with fixed time intervals . However, such atemporal discretization method may lead to information loss anddata missing problems . Recent studies tend to directly learnfrom irregularly sampled time series. Specifically, some studies im-prove RNNs by using a time gate , a time decay term , ormemory decomposition mechanism to adjust RNNs memoryupdate for adapting irregular time series. Another line of studiesintroduces neural Ordinary Differential Equations (ODEs) tomodel the continuous dynamics in time series, and assume thelatent states of time series are continuously evolving through con-tinuous time . Besides, attention mechanism is also appliedto model irregularly sampled time series . Forexample, Shukla and Marlin proposes a multi-time attentionnetwork to learn embedding of continuous time. Zhang et al. employs a doubly self-attention to learn representation from theinput data unified by a warping module. Zhang et al. intro-duce GNNs to capture time-varying dependencies between sensorsby performing the graph convolution operation at all timestampswhen there is an observation at an arbitrary sensor. However, itwill be extremely time-consuming once the data is significantlyasynchronous across large-scale sensors like us. Furthermore, theabove studies primarily focus on solving irregular time series clas-sification instead of forecasting tasks. Finally, to our knowledge,there are no prior studies attempting to modify CNNs to adapt tothe irregular time series modeling.",
  "CONCLUSION": "In this paper, we investigated a new irregular traffic forecastingproblem that aims to predict irregular traffic time series result-ing from adaptive traffic signal controls, and presented ASeer, anAsynchronous Spatio-Temporal Graph Convolutional Network, toaddress this problem. Specifically, by representing the traffic sen-sors as nodes and linking them via a traffic diffusion graph, we firstproposed an Asynchronous Graph Diffusion Network to model thespatial dependency between the time-misaligned traffic state mea-surements of nodes. After that, to capture the temporal dependencywithin irregular traffic state sequences, we devised a personalizedtime encoding to embed the continuous time for each node andproposed a Transformable Time-aware Convolution Network toperform efficient temporal convolution on sequences with inconsis-tent temporal flow. Furthermore, a Semi-Autoregressive PredictionNetwork was designed to iteratively predict variable-length trafficstate sequences effectively and efficiently. Finally, extensive experi-ments on two newly constructed real-world datasets demonstratedthe superiority of ASeer compared with twelve competitive base-line approaches across six metrics.",
  "Z Che, S Purushotham, K Cho, D Sontag, and Y Liu. 2018. Recurrent NeuralNetworks for Multivariate Time Series with Missing Values. Scientific reports(2018), 60856085": "Jiming Chen, Weixin Lin, Zidong Yang, Jianyuan Li, and Peng Cheng. 2019.Adaptive ramp metering control for urban freeway using large-scale data. IEEETransactions on Vehicular Technology 68, 10 (2019), 95079518. Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. 2018.Neural ordinary differential equations. In Proceedings of the 32nd InternationalConference on Neural Information Processing Systems. 65726583.",
  "Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. 2014.Empirical evaluation of gated recurrent neural networks on sequence modeling.In NIPS Workshop on Deep Learning": "Michal Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolu-tional neural networks on graphs with fast localized spectral filtering. Advancesin neural information processing systems (2016), 38443852. Zheng Fang, Qingqing Long, Guojie Song, and Kunqing Xie. 2021. Spatial-temporal graph ode networks for traffic flow forecasting. In Proceedings of the27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 364373. Shengnan Guo, Youfang Lin, Ning Feng, Chao Song, and Huaiyu Wan. 2019.Attention based spatial-temporal graph convolutional networks for traffic flowforecasting. In Proceedings of AAAI conference on artificial intelligence. 922929. Shengnan Guo, Youfang Lin, Huaiyu Wan, Xiucheng Li, and Gao Cong. 2021.Learning dynamics and heterogeneity of spatial-temporal graph data for trafficforecasting. IEEE Transactions on Knowledge and Data Engineering 34, 11 (2021),54155428. Jindong Han, Weijia Zhang, Hao Liu, Tao Tao, Naiqiang Tan, and Hui Xiong. 2024.BigST: Linear Complexity Spatio-Temporal Graph Neural Network for TrafficForecasting on Large-Scale Road Networks. Proceedings of the VLDB Endowment(2024), 10811090.",
  "Max Horn, Michael Moor, Christian Bock, Bastian Rieck, and Karsten Borg-wardt. 2020. Set functions for time series. In International Conference on MachineLearning. 43534363": "Jiahao Ji, Jingyuan Wang, Chao Huang, Junjie Wu, Boren Xu, Zhenhe Wu, JunboZhang, and Yu Zheng. 2023. Spatio-temporal self-supervised learning for trafficflow prediction. In Proceedings of the AAAI conference on artificial intelligence,Vol. 37. 43564364. Jiawei Jiang, Chengkai Han, Wayne Xin Zhao, and Jingyuan Wang. 2023.PDFormer: Propagation Delay-aware Dynamic Long-range Transformer for Traf-fic Flow Prediction. In Proceedings of the AAAI conference on artificial intelligence.43654373.",
  "Serbjeet Kohli, Steer Davies Gleave, and Luis Willumsen. 2016. Traffic Forecastingand Autonomous Vehicles. In 2016 European Transprot Conference, Barcelona. 57": "Shiyong Lan, Yitong Ma, Weikang Huang, Wenwu Wang, Hongyu Yang, andPyang Li. 2022. Dstagnn: Dynamic spatial-temporal aware graph neural networkfor traffic flow forecasting. In International Conference on Machine Learning.1190611917. Ibai Lana, Javier Del Ser, Manuel Velez, and Eleni I Vlahogianni. 2018. Road trafficforecasting: Recent advances and new challenges. IEEE Intelligent TransportationSystems Magazine 10, 2 (2018), 93109.",
  "Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. 2018. Diffusion ConvolutionalRecurrent Neural Network: Data-Driven Traffic Forecasting. In InternationalConference on Learning Representations": "Zachary C Lipton, David Kale, and Randall Wetzel. 2016. Directly modelingmissing data in sequences with rnns: Improved classification of clinical timeseries. In Machine learning for healthcare conference. 253270. Fan Liu, Weijia Zhang, and Hao Liu. 2023. Robust Spatiotemporal Traffic Fore-casting with Reinforced Dynamic Adversarial Training. In Proceedings of the 29thACM SIGKDD Conference on Knowledge Discovery and Data Mining. 14171428. Hangchen Liu, Zheng Dong, Renhe Jiang, Jiewen Deng, Jinliang Deng, Quan-jun Chen, and Xuan Song. 2023. Spatio-temporal adaptive embedding makesvanilla transformer sota for traffic forecasting. In Proceedings of the 32nd ACM",
  "Fernando Moreno-Pino, Pablo M Olmos, and Antonio Arts-Rodrguez. 2023.Deep autoregressive models with spectral attention. Pattern Recognition (2023),109014": "Daniel Neil, Michael Pfeiffer, and Shih-Chii Liu. 2016. Phased LSTM: acceleratingrecurrent network training for long or event-based sequences. In Proceedingsof the 30th International Conference on Neural Information Processing Systems.38893897. Rezaur Rahman and Samiul Hasan. 2023. A deep learning approach for network-wide dynamic traffic prediction during hurricane evacuation. TransportationResearch Part C: Emerging Technologies 152 (2023), 104126. Yulia Rubanova, Ricky TQ Chen, and David Duvenaud. 2019. Latent ODEs forirregularly-sampled time series. In Proceedings of the 33rd International Conferenceon Neural Information Processing Systems. 53205330.",
  "Mona Schirmer, Mazin Eltayeb, Stefan Lessmann, and Maja Rudolph. 2022. Mod-eling irregular time series with continuous recurrent units. In International Con-ference on Machine Learning. 1938819405": "Zezhi Shao, Zhao Zhang, Fei Wang, and Yongjun Xu. 2022. Pre-training enhancedspatial-temporal graph neural network for multivariate time series forecasting.In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery andData Mining. 15671577. Zezhi Shao, Zhao Zhang, Wei Wei, Fei Wang, Yongjun Xu, Xin Cao, and Chris-tian S Jensen. 2022. Decoupled dynamic spatial-temporal graph neural networkfor traffic forecasting. Proceedings of the VLDB Endowment (2022), 27332746.",
  "Satya Narayan Shukla and Benjamin M Marlin. 2020. A survey on principles,models and methods for learning from irregularly sampled time series. arXivpreprint arXiv:2012.00168 (2020)": "Qian Sun, Le Zhang, Huan Yu, Weijia Zhang, Yu Mei, and Hui Xiong. 2023.Hierarchical reinforcement learning for dynamic autonomous vehicle navigationat intelligent intersections. In Proceedings of the 29th ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining. 48524861. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processing systems 30 (2017).",
  "Petar Velikovi, Guillem Cucurull, Arantxa Casanova, Adriana Romero, PietroLi, and Yoshua Bengio. 2018. Graph Attention Networks. In International Con-ference on Learning Representations": "Chen Wang, Bertrand David, Ren Chalon, and Chuantao Yin. 2016. Dynamicroad lane management study: A Smart City application. Transportation researchpart E: logistics and transportation review 89 (2016), 272287. Yizhe Wang, Xiaoguang Yang, Hailun Liang, Yangdong Liu, et al. 2018. A review ofthe self-adaptive traffic signal control system based on future traffic environment.Journal of Advanced Transportation 2018 (2018).",
  "Hua Wei, Guanjie Zheng, Vikash Gayah, and Zhenhui Li. 2019. A survey ontraffic signal control methods. arXiv preprint arXiv:1904.08117 (2019)": "Yuxi Wei, Juntong Peng, Tong He, Chenxin Xu, Jian Zhang, Shirui Pan, andSiheng Chen. 2023. Compatible transformer for irregularly sampled multivariatetime series. In 2023 IEEE International Conference on Data Mining (ICDM). IEEE,14091414. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, andS Yu Philip. 2020. A comprehensive survey on graph neural networks. IEEEtransactions on neural networks and learning systems (2020), 424. Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, and Chengqi Zhang. 2019.Graph wavenet for deep spatial-temporal graph modeling. In Proceedings of theInternational Joint Conference on Artificial Intelligence. 19071913. Huaxiu Yao, Fei Wu, Jintao Ke, Xianfeng Tang, Yitian Jia, Siyu Lu, Pinghua Gong,Zhenhui Li, Jieping Ye, and Didi Chuxing. 2018. Deep multi-view spatial-temporalnetwork for taxi demand prediction. In Proceedings of the AAAI conference onartificial intelligence. 25882595. Kun Yi, Qi Zhang, Wei Fan, Hui He, Liang Hu, Pengyang Wang, Ning An, Long-bing Cao, and Zhendong Niu. 2023. FourierGNN: Rethinking multivariate timeseries forecasting from a pure graph perspective. Advances in Neural InformationProcessing Systems 36 (2023). Bing Yu, Haoteng Yin, and Zhanxing Zhu. 2018. Spatio-temporal graph convolu-tional networks: a deep learning framework for traffic forecasting. In Proceedingsof the 27th International Joint Conference on Artificial Intelligence. 36343640. Zixuan Yuan, Hao Liu, Yanchi Liu, Denghui Zhang, Fei Yi, Nengjun Zhu, andHui Xiong. 2020. Spatio-temporal dual graph attention network for query-poimatching. In Proceedings of the 43rd international ACM SIGIR conference on researchand development in information retrieval. 629638.",
  "Junbo Zhang, Yu Zheng, and Dekang Qi. 2017. Deep spatio-temporal residual net-works for citywide crowd flows prediction. In Proceedings of the AAAI conferenceon artificial intelligence. 16551661": "Weijia Zhang, Hao Liu, Jindong Han, Yong Ge, and Hui Xiong. 2022. Multi-agent graph convolutional reinforcement learning for dynamic electric vehiclecharging pricing. In Proceedings of the 28th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining. 24712481. Weijia Zhang, Hao Liu, Yanchi Liu, Jingbo Zhou, and Hui Xiong. 2020. Semi-supervised hierarchical recurrent graph neural network for city-wide parkingavailability prediction. In Proceedings of the AAAI Conference on Artificial Intelli-gence. 11861193. Xiang Zhang, Marko Zeman, Theodoros Tsiligkaridis, and Marinka Zitnik. 2022.Graph-Guided Network for Irregularly Sampled Multivariate Time Series. InInternational Conference on Learning Representations. Chuanpan Zheng, Xiaoliang Fan, Cheng Wang, and Jianzhong Qi. 2020. Gman: Agraph multi-attention network for traffic prediction. In Proceedings of the AAAIconference on artificial intelligence. 12341241.",
  "ASUPPLEMENTARY EXPERIMENTSA.1Data Description and Analysis": "A.1.1Datasets Description. The statistics of the datasets aresummarized in . Specifically, there are total 19,824,504 and13,093,975 traffic state measurements on Zhuzhou and Baoding,and the missing period ratios of the two datasets are 44.2% and27.2%, respectively. Each measurement includes information aboutthe beginning and end timestamps and cycle length of the trafficsignal cycle, as well as the lanes traffic flow, i.e., the number ofvehicles passing through the camera of lane, during the signal cycle.Besides, Zhuzhou has 620 lanes with sensors and ranges from July20, 2022 to October 2, 2022. Baoding has 264 lanes with sensors andranges from December 1, 2021 to February 25, 2022. The average,maximal ground truth sequence length to be predicted for the futureone hour is 57, 213 on Zhuzhou, and 64, 155 on Baoding. A.1.2Datasets Analysis. The overall distributions of traffic sig-nal cycle lengths on two datasets are depicted in , where wecan observe the cycle lengths can significantly vary from around 40to 200 seconds on both datasets, indicating the pronounced irregu-larity within time series, and Baoding has a denser cycle lengthdistribution than Zhuzhou.Besides, illustrates temporal distributions of traffic sig-nal cycle lengths and traffic flows across different hours on bothdatasets. We can observe cycle length and traffic flow consistentlyexhibit higher values during the daytime periods compared toovernight periods. Moreover, they display similar peak patterns",
  ": Overall distributions of traffic signal cycle lengths": "during the morning and evening rush hours and tend to vary in apositively correlated manner.To further investigate the correlations between these two trafficstates, we illustrate the variations in traffic flow distributions acrossdifferent cycle lengths and vice versa in . As can be seen in(a) and (b), traffic flow maintains an upward trendat first along with the increase of cycle length. A similar positivecorrelation can also be observed in (c) and (d),which display the variations in cycle length distributions acrossdistinct traffic flows. However, we notice that with a further increasein cycle length, traffic flow tends to decrease. A similar situationis also shown in (c). This can be attributed to the factthat although a positive correlation is expected between trafficflow and cycle length for the same lane, the lanes with the longestcycle lengths may not necessarily correspond to the highest trafficflows due to different traffic conditions and signal control strategiesamong these lanes, and vice versa.Additionally, displays the spatial distributions of cam-era sensors and corresponding average traffic flows and cycle lengthson Zhuzhou as a representative. It can be noticed that both trafficflows and cycle lengths exhibit remarked geographical proximity,indicating that neighboring sensors tend to have similar trafficstates. This finding provides partial justification for the effective-ness of the spatial dependency modeling component, AGDN, in theirregular traffic forecasting task.",
  "A.2Baselines": "We compare our approach with the following twelve baselines.These baseline models take the same inputs as ASeer by directlyutilizing observed traffic state measurements. All these models aimto predict both traffic flow and cycle lengths by optimizing the hy-brid loss function in equation Eq. (18). The autoregressive models,i.e., GRU, T-LSTM, GRU-D, and DCRNN, iteratively predict the nextstep traffic states based on their previous predictions. Since theother non-autoregressive models require the predicted sequencelength to be fixed, to enable variable-length sequence prediction, weallow them to predict in a semi-autoregressive way that they itera-tively predict a fixed-length sub-sequence based on observed andpreviously predicted sequences. The prediction step size is set thesame as ours to ensure a fair comparison. We carefully tuned majorhyper-parameters of each baseline based on their recommendedsettings for better performance on our datasets.LAST predicts future traffic states using the last historical traf-fic state measurement of each sensor. HA predicts future trafficstates using the average of each sensors historical traffic statemeasurements. TCN is the temporal convolutional networkconsisting of causal and dilated convolutions. We apply it to ourdatasets by padding or intercepting all the sequences to a fixedlength. We stack 6 temporal convolution layers with filter size of 3.GRU is a powerful variant of recurrent neural networks with agated recurrent unit. T-LSTM is a time-aware Long-Short TermMemory (LSTM) model with memory decomposition for irregu-lar time series classification. We modify it to predict traffic statesusing a LSTM-based decoder. GRU-D improves GRU with atime-aware decay mechanism for irregular time series classification.We modify it to predict traffic states using a GRU-based decoder.mTAND is a state-of-the-art transformer-based approach for",
  ": Spatial distributions of camera sensors and corresponding average traffic flows and cycle lengths on Zhuzhou": "irregularly sampled multivariate time series classification and inter-polation tasks. It adopts multi-time attention with time embeddingto produce a fixed-length representation of a variable-length timeseries. The reference point number is set to 64. Warpformer isanother state-of-the-art transformer-based approach for irregularlysampled multivariate time series classification tasks. It employs awarping module to unify inputs and a doubly self-attention modulefor representation learning. We set the lengths of warp layers to0, 24. DCRNN is a representative approach based on GNNsand RNNs for classical traffic forecasting tasks, which replaces thematrix multiplications in GRU with a graph convolution operation.The used graph structure is the same as ASeer, and the diffusionstep is set to 1. To apply DCRNN to our problem, we pad the inputtraffic sequences of all nodes to the same length. GWNet isa representative approach based on GNNs and CNNs for classicaltraffic forecasting. It stacks multiple spatial-temporal blocks thatare constructed by the graph convolution layer and gated TCN layer, where the graph convolution is performed on the combi-nation of pre-defined and self-learned adjacency matrix. The pre-defined graph structure is the same as ASeer. We stack 3 blockswith 4 convolution layers and set the convolution filter size to 3. Itadopts the same padding strategy as DCRNN. STAEformer isa state-of-the-art approach for classical traffic forecasting based ontransformer and spatio-temporal adaptive embeddings. We followthe recommended settings the authors give for embedding dimen-sions and the number of layers and heads and adopts the samepadding strategy as DCRNN. PDFormer is another state-of-the-art transformer-based approach for classical traffic forecasting.It adopts self-attentions for both spatial and temporal dependenciesmodeling. A graph-masked self-attention mechanism is employedto capture both geographic and semantic spatial dependencies anda delay-aware feature transformation module is used to model thetime delay in spatial information propagation. The depth of encoderlayers is set to 2. It adopts the same padding strategy as DCRNN."
}