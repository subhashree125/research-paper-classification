{
  "ABSTRACT": "Existing neural constructive solvers for routing problems have pre-dominantly employed transformer architectures, conceptualizingthe route construction as a set-to-sequence learning task. However,their efficacy has primarily been demonstrated on entirely randomproblem instances that inadequately capture real-world scenarios.In this paper, we introduce realistic Traveling Salesman Problem(TSP) scenarios relevant to industrial settings and derive the fol-lowing insights: (1) The optimal next node (or city) to visit oftenlies within proximity to the current node, suggesting the potentialbenefits of biasing choices based on current locations. (2) Effectivelysolving the TSP requires robust tracking of unvisited nodes andwarrants succinct grouping strategies. Building upon these insights,we propose integrating a learnable choice layer inspired by Hy-pernetworks to prioritize choices based on the current location,and a learnable approximate clustering algorithm inspired by theExpectation-Maximization algorithm to facilitate grouping the un-visited cities. Together, these two contributions form a hierarchicalapproach towards solving the realistic TSP by considering bothimmediate local neighbourhoods and learning an intermediate setof node representations. Our hierarchical approach yields superiorperformance compared to both classical and recent transformermodels, showcasing the efficacy of the key designs.",
  "neural constructive solver, traveling salesman problem, deep rein-forcement learning": "Permission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0490-1/24/08 ACM Reference Format:Yong Liang Goh, Zhiguang Cao, Yining Ma, Yanfei Dong, MohammedHaroon Dupty, and Wee Sun Lee. 2024. Hierarchical Neural Construc-tive Solver for Real-world TSP Scenarios. In Proceedings of the 30th ACMSIGKDD Conference on Knowledge Discovery and Data Mining (KDD 24),August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.",
  "INTRODUCTION": "The Traveling Salesman Problem (TSP) is a classical combinatorialoptimization problem. Simply put, the TSP asks the following: givena set of cities, what is the shortest route where a salesman can visitevery city only once and return back to his starting city? Whileit is simple to state, the TSP is a very difficult problem known tobe NP-hard. Nevertheless, the TSP is a crucial problem to study, asmany parallel problems can be reduced to solving the TSP, such aschip placement , the study of spin glass problems in physics, DNA sequencing , and many others.Given its prevalence across a multitude of domains, the TSPhas been extensively researched in the community. Particularly,the main approaches can be broken down into exact methods andapproximate methods. Exact methods often materialize in the formof mathematical programming. Some popular exact solvers, suchas Concorde , are developed based on linear programming andcutting planes. Approximate methods tend to be in the form of ex-pert heuristics. An example would be the Lin-Kernighan-Helsgaun(LKH-3) algorithm, which utilizes heuristics and local search meth-ods to update and improve initial solutions. As their names describe,exact methods return the true optimal routes while approximateones return solutions often within some error bound of the op-timal one. As the size of the problems grows, exact methods areintractable due to the NP-hard nature of the problem.More recently, the deep learning community has put much effortinto establishing practical neural solvers. These typically appear inthe form of deep reinforcement learning , whichpresents a label-free approach to improve the models. This is pre-ferred over supervised learning approaches (e.g, ) since theyrequire large amounts of labelled data, which is often challengingto obtain given the limited scalability of exact solvers.It is important to note that learning-based solvers may performwell on specific target distributions they are trained on, but oftensuffer from poor generalization to other arbitrary instances. This",
  "KDD 24, August 2529, 2024, Barcelona, SpainYong Liang Goh et al": "Once we have the set of embeddings, we update the represen-tation at every step of decoding by subtracting a weighted sum ofthe current nodes embedding; this computes the weighted sum ofembeddings of unvisited cities, instead of all cities. Thus, at timestep , if the agent is current at node , we update via",
  "RELATED WORK2.1Constructive Neural Solvers": "Deep reinforcement learning forms the hallmark of training con-structive neural solvers. Early works from proposed to use thePointer Network based on the sequence-to-sequence architec-ture in to solve TSP and Knapsack problems. They employan actor-critic approach and achieved strong results on the TSP.Follow-up works from further improve the performance of thePointer Network.Following on from this, the transformer network based on atten-tion was proposed by to solve the TSP, Capacited VehicleRouting Problem (CVRP) and the Orienteering Problem (OP). Pri-marily, the work showed that one can train a neural solver using theREINFORCE algorithm and a simple greedy rollout of the net-work with a lagging baseline. Since then, multiple works based onthe same architecture have been proposed to improve the predictivepower of such solvers further . POMO was introduced andobserved that constructive solvers were limited by their startingnodes. Hence, to effectively explore the search space of solutions,one should use all nodes as starting nodes, effectively constructinga simple beam search. Additionally, they showed that a stable base-line can be found in the average of all solutions. Sym-NCO wasproposed to exploit the symmetry of TSP by introducing symmetrylosses to regularize the transformer network. Recently, ELG wasintroduced by that defined a local learnable policy based on ak-Nearest Neighbor graph. They exemplified the generalizabilityof the network on large CVRP instances.",
  ": POMO model making a minor mistake with a poorselection of a local node": "by algorithms such as 2-opt, whereby the solver first starts with acomplete route, and a heuristic is then used to select edges to deleteor add so as to edit the solution. Such methods are measured basedon how quickly they can reach a strong solution. Work such as uses neural networks to learn the 2-opt heuristic for improvement,while uses the transformer to select node pairs for swaps forthe TSP. Ma et al. then extended the transformer network tolearn node and edge embeddings separately, which is then upgradedfor pickup and delivery problems in and flexible k-opt in ,pushing the iterative solvers performance further.",
  "Search-based techniques": "The previous two approaches are based on some form of learning-based search: constructive solvers try to perform a global searchby learning heuristics entirely from data, whereas iterative solverslearn to guide local search techniques instead. Besides these, thereis a class of search-based techniques that involve applying searchduring inference. Efficient Active Search (EAS) was proposed by to introduce lightweight learnable layers at inference thatcould be tuned to improve the predictive power of a model on testsamples. Other works such as showcase that one can leverage asmall pre-trained network and combine it with search techniquessuch as Monte-Carlo Tree Search (MCTS) so as to solve large-scale TSPs. The work in then combined both MCTS and EAS toimprove the search capabilities further. Lastly, another work showcased how one could combine dynamic programming with apre-trained neural network to scale the TSP to 10,000 nodes.Previous works attempt to regularize the networks via symmetryor scale the solver to larger problems. However, they essentially arestill based on transformer models trained on arbitrary distributions.Apart from ELG, these works do not consider the impact of localchoices nor explore further how to represent unvisited cities better,which are critical aspects of solving the TSP in our view.",
  "OUR APPROACH": "Generally, we find that neural solvers tend to make two classes oferrors in route construction compared to the optimal solution forsuch practical scenarios. The first class often appears as a minorerror, where a poor decision is made in a local neighbourhood, asshown in . This results in a sub-optimal route because alocal choice is not picked first. The second class tends to appear",
  ": POMO model making a major mistake by not visit-ing nodes that are near it, causing cross-cluster routes thatare inefficient": "in problems with more structure, where the agent fails to visit allreasonable nodes within a local cluster and has to backtrack to thearea, exemplified in . This tends to give solutions that aresignificantly poorer than optimal.Our approach seeks to tackle these two errors more effectively.We propose two main architectural improvements to the base trans-former model to address these issues. Firstly, we propose a learnablelocal choice decoder that accentuates certain choices based on theagents current location (and hence locality). Secondly, we proposea differentiable clustering algorithm to learn a set of representa-tions to capture and summarize the set of remaining cities. Our fullapproach is illustrated in .",
  "Recap: Constructive Neural Solvers": "In this subsection, we review previous works in well-known con-structive neural solvers such as the attention model and thePOMO model . The problem can be defined as an instance ona fully connected graph of nodes, where each node representsa city. Each node where {1, ...,} has features , typicallythe 2D coordinate. A solution is defined as a TSP tour, and is statedas a permutation of the nodes given by = (1, ...,), such that {1, ...,}. Note that since the tour does not allow backtracking, , . The formulation yields the following policy:",
  "h = LN ( hi + FF( hi))(3)": "where is the embedding of the -th node at the -th layer, the dimension size of the embeddings, MHA is the standard multi-headed attention layer , LN is a layer normalization function,and FF is a simple feed-forward multi-layer perceptron (MLP). Eachnodes embeddings go through a total of -layers before beingpassed into a decoder.",
  "h() = hlast + hstart(4)": "where h() is known as a contextual embedding. In this instance, thecontext given is the sum of the -th encoder layers representationof the current node and the starting node. This is then first passedthrough a multi-headed attention layer where visited nodes aremasked, followed by a single-headed attention layer for decision-making. In this decision layer, we obtain the following",
  "= (), = (), = ()(5)": "where is the set of all node embedding after the decoders multi-headed attention layer. With this, we compute the compatibilityof the query with all nodes together with a mask, where the maskindicates previously visited nodes. This ensures that the decodercannot pick an already visited node. Mathematically, we use thefollowing attention mechanism",
  "Improving local decision making with theChoice Decoder": "More often than not, good selections for the TSP tend to lie withinthe locality of the current position. In the standard transformerarchitecture, the decoder attempts to capture this by using thecurrent nodes representation in a single-headed attention layer, asshown in Equations 6 and 7. In fact, Equation 6 is essentially a kernelfunction between the current querying node and the candidatenodes, an observation made in . Effectively, we can view thecompatibility score as",
  ")(10)": "Given that we aim to focus more on the current nodes vicin-ity and features, we propose to generate a set of attention weightsbased on the current embeddings. This aims to amplify or nullifythe compatibility scores further by conditioning it on the currentnode. Hypernetworks are small neural networks designed togenerate a set of weights for a main network. Its goal is to serve asa form of relaxed weight sharing. This approach allows the hyper-network to take as input some information about the problem, suchas its structure, and adapt the main networks weights towardsthe problem. Inspired by this approach, we construct the set ofattention weights using an MLP as a hypernetwork, with the inputbeing the current nodes embedding. Concretely, we modify thecompatibility function as follows:",
  ")(13)": "where is the element-wise product. Inherently, choice now re-duces to a set of learnable scalars which serve to modify the compat-ibility scores between and . Additionally, these learnable scalarsare now conditioned upon , the current node, since it is generatedvia the MLP. The complexity of the MLP also now reduces fromproducing an output of R to R.",
  "Exploiting structure with HierarchicalDecoder": "In its current state, the decoder models the TSP as a set-to-sequencefunction. A key aspect of the input is the contextual embedding,h(). This embedding serves to represent the current state themodel is in and is often a combination of the starting node, currentnode, and some global representation of the problem. For the globalrepresentation, works such as and use an average of allnode embeddings, while others such as , maintain an average ofall visited nodes so far. Essentially, all of these approaches attemptto capture various nuances of the TSP.However, for realistic problem settings, it is important to exploitthe structure of the distribution of the cities. A single global rep-resentation would not be effective enough to capture the intricatecorrelations present between the cities. One notable and ubiqui-tous case is the presence of cluster patterns wherein certain citiesare located near to each other while being distant to others. Thisclustering pattern, if captured within the global and contextualrepresentation, can potentially provide the model with importantclues to determine the next city to visit. Thus, for such problems,we propose to maintain a set of representations that are able tosummarize the set of unvisited cities left, instead of a simple singlerepresentation. We postulate that this is meaningful as structuredproblems have frequent cities in fixed areas of the map, and beingable to identify if a node belongs to a certain area could be beneficialto the decision-making process.Prior works in other domains have shown the efficacy of clusterconstruction in applications such as node classification . In thiswork, we wish to group all cities into representations. To thisend, we design the following layer inspired by the Expectation-Maximization (EM) algorithm . We first briefly review the EMalgorithm for the Gaussian Mixture Model (GMM). Let = {, }denote the set of parameters, the coefficients of Gaussians andits associated means (covariance is assumed to be known),X = {x()} denote the set of data points, and Z = {()} denotethe set of latent variables associated with the data. The maximum-likelihood objective is given by",
  "()=1(x() |(); )(() |)(14)": "In general, Equation 14 yields no closed-form solution. Additionally,it is non-convex, and its derivatives are expensive to compute. Sincelatent variable () exists for every observation and we have a suminside a log, we look at the EM algorithm to solve this. Typically, the EM algorithm involves two steps: the E-step computes the posteriorprobability over given the current parameters, and the M-step,which assumes that given that the data was generated with () = ,finds the set of parameters that maximizes this. Effectively, for astandard GMM, this yields the following E-step, given an initial setof parameters :",
  "= ()(17)": "Essentially, Equation 15 estimates the contribution of each Gauss-ian model given the current set of parameters. While Equations 16and 17, highlight closed-form update equations for the Gaussianparameters.Now, suppose a TSP instance drawn from a fixed map can berepresented efficiently with latent representations. Since we aredealing with subsets of problems from the same space, these representations can be fixed and learnable. Let C R denotethis set of representations, where we have = {1,2, ..., }. Wepropose to learn and update these representations by considering amixture model, where the latent variables are modelled by theselatent embeddings. Similar to the EM algorithm, we produce a setof mixing coefficients using an attention layer and its attentionweights. Concretely, our soft clustering algorithm estimates itsmixing coefficients via the attention mechanism, and using thesescores, the clusters are then updated with a weighted sum of theembeddings. This can be shown as",
  ",(21)": "where is the matrix containing all coefficients ,, and areparameters for the attentional scores, and is the set of learnableembeddings for the distribution. Given a single set of parametersin the attention layer, the embeddings and are passed throughthis same layer a total of times iteratively, mimicking a \"rollout\"of a soft clustering algorithm of E-steps and M-steps within eachiteration. Loosely, we can see that Equation 20 resembles a similarcalculation of the E-step, wherein we use a set of parameters toestimate the coefficients instead of minimizing for the Euclideandistance in the GMM case. Equation 21 is similar to the M-step ofGMMs, where we update the centers (in our case the embeddingsare the latent variables) with a weighted sum of the data.",
  "h() = combine[last,1,2, ..., ] + hfirst(23)": "where [] is the concatenation operation, and combine is a simplelinear layer to combine the embeddings. We keep hfirst separate soas to preserve the importance of the starting node. As the decoderconstructs the solutions, the embeddings get updated along theway, maintaining a small set of unvisited cities so as to keep trackof the solution.",
  "EXPERIMENTAL SETUP4.1Data Generation": "We present three different benchmarks for comparison. For all sce-narios, we look at TSP-100 problems. Firstly, we generate randomuniform data on a square and a fixed test set of 10, 000 in-stances. This is done to show if the addition of other layers interfereswith the base performance of the transformer model.To generate realistic data, we sample instances from 3 differentcountries, available at and shown in . Namely, they are",
  "BM33708 - 33,708 cities across the country of Burma JA9847 - 9,847 cities across the country of Japan": "Each country is first normalized to a square. Then, at everytraining epoch, we randomly sample problems of size 100 from themap. Naturally, clusters that are denser across the country will besampled more frequently. A test set size of 10, 000 samples is alsodrawn and held aside for evaluation. Each test set is fully solvedvia Concorde to get the optimal length of each tour. We define1 epoch to be 100, 000 samples, and the models are trained for 200epochs to prevent overfitting. In totality, the model sees 20, 000, 000different samples. Thirdly, we define a limited data setting. Thismimics a typical practical problem where the company does nothave unlimited access to data. We first define a small dataset of50, 000 samples for such a setting. Based on the experiment size,we sample the necessary amount of data. Likewise, the models aretested on the same test set of 10, 000 samples.Additionally, to show the generality of our approach beyondthe logistics domain, we include the PCB3038 dataset from TSPLib,where the goal is to find the shortest path across a circuit boardlayout. Here, we can view the problem as such: from all possible",
  "Benchmark Models": "We compare our approach to the following constructive neuralsolvers: POMO the classical transformer that forms the basisfor many follow-up works, Sym-NCO a follow-up work fromPOMO that improves neural solvers by exploiting problem sym-metry, and ELG a recent work that also focuses on localityby defining a separate local policy based on its k-Nearest Neigh-bors. All models are trained based on the POMO shared baseline.It should be noted that in ELG, the authors introduced a differenttraining algorithm. Since we wish to compare the efficacy of thearchitectural contributions, we train the ELG model using POMO.We reimplement all models and ran on the proposed dataset for ourexperimental results.",
  "Hyperparameters": "Since the neural models all share the same underlying backbonePOMO transformer model, we retain the same set of hyperparame-ters across them to ensure the contributions are purely architectural.We utilize 6 layers of the transformer encoder and 2 layers of thetransformer decoder. All models are trained for 200 epochs, with100,000 episodes per epoch and a 14 learning rate using the Adamoptimizer . Gradient clipping is set to for all models.For ELG, we used their recommended 50-nearest neighbours. Asfor our approach, we set = 5 embeddings and = 5 iterationsfor the hierarchical approach by using a validation set of 1,000samples for verification. Additional details for hyperparameterscan be found in Appendix A.1.",
  "RESULTS5.1Performance on Uniform RandomDistribution": "highlights the overall performance of the models on theclassic uniform random sampling dataset. Most models have solidperformance, with augmentation playing a prominent role in boost-ing the predictive power. Our model and ELG also show the impor-tance of having some form of local feature to improve the decision-making process. The addition of our unvisited city tracking via",
  "Performance on Structured Distributions": "showcases the different models performance on TSP100instances drawn from various countries. Our model has a clearadvantage in the USA and Japan, with a narrow margin in Burma.Interestingly, we also see that augmentation has minimal effect onincreasing the solvers performance, except for the case of Sym-NCO. It is likely because the maps are no longer symmetric innature, and simple transformations do not improve the chancesof finding a very different route. For Sym-NCO, we see that sinceits specifically trained to exploit augmentations by considering allsymmetries during training, its strong performance only appearswhen it can perform those transformations.",
  "Performance on Varied Sizes": "compares the models performance between the completeset of training, 50,000 fixed samples, and 10,000 fixed samples. Wecan see that the model performance degrades as expected once datais limited. However, the ranking of the models is still similar atthe 50,000 sample mark. Interestingly, reducing the dataset furtherto 10,000 samples sees ELG becoming the top neural constructivesolver. We attribute ELGs strong performance on limited data toits local policy scheme. ELG uses well-crafted local features in theform of polar coordinates to create their local policy. The direct useof these features allows their local policy to learn valuable featuresto alter the action probabilities easily with less data. It should benoted that our choice hypernetwork is possibly a larger functionclass that also encompasses this approach. Hence, our model canlearn a better overall function when more data is present.",
  "Performance on PCB3038": "showcases the performance of the models on the PCB3038TSPLib dataset. Evidently, the dataset is no longer as symmetric asbefore, since Sym-NCO struggles to greatly improve upon POMO.Instead, some local features are important to have, as displayed byELG and our models improvement over POMO. Since our approachis a superset function of ELG, it can learn a better-performing solver,significantly improving upon POMO.",
  "Evolution of Embeddings": "Soft clustering the nodes in the embedding space is a crucial part ofour overall approach. Therefore, we evaluate if our algorithm indeedis able to cluster the nodes into a set of meaningful clusters as thetraining progresses. Specifically, we conduct a 2D t-SNE plot of thenode and cluster embeddings during the training process. evidently shows that as the training progresses, the cluster centroidembeddings are better able to separate the node embeddings in thelatent space. This plausibly leads to a better representation of theunvisited embeddings and hence the problem space, allowing themodel to identify the groups of nodes so as to select similar onesfirst.",
  "Hierarchical Neural Constructive Solver for Real-world TSP ScenariosKDD 24, August 2529, 2024, Barcelona, Spain": ": Optimality gaps across various dataset sizes (lower is better). The x-axis dictates the model types, and the y-axis denotesthe optimality gap. The first row showcases standard data input, and the second shows data after augmentation. Countries areUSA, Japan, and Burma, from left to right. Numerical details can be found in Appendix A.2.",
  ": 2D t-SNE plot of cluster centroids (in blue) and a setof node embeddings (in orange) as the training progresses.Over time, the centroids surround and segregate the embed-dings better": "learn freely. Thirdly, POMO + Choice + Average Tracking removesthe soft clustering layer, instead averaging all unvisited nodes intoa single embedding. From the table, we first see that the local choicelayer is essential. Additionally, if we were to remove the ability tocondition on the current nodes position instead, the model wouldperform exceptionally poorly - even worse than the original trans-former network. Finally, if we adopt the simplistic approach ofaveraging all unvisited cities, the model performance also suffers,showcasing the importance of our soft clustering layer.",
  "In this work, we propose a more realistic approach to representingand generating Traveling Salesman Problems (TSPs) in real-world": "contexts. Our investigation reveals that previous state-of-the-artneural constructive solvers do not fully exploit the problems in-tricacies to enhance predictive capability. To address this gap, wepresent a dual strategy to deal with the problem from two fronts.Firstly, we emphasize the importance of considering current agentpositions, leading us to introduce a hypernetwork, which enablesdynamic fine-tuning to the decision-making process based on theagents current node position. Secondly, we recognize that realisticTSP scenarios are often structured, and therefore, improving so-lutions in such scenarios necessitates a deeper understanding ofthe structure of the set of unvisited nodes. Instead of treating allunvisited nodes uniformly, we propose a soft clustering algorithminspired by the EM algorithm. This approach enhances the neu-ral solvers performance by grouping nodes based on similarities,thereby increasing the likelihood of selecting nodes from the samecluster for early resolution. We illustrate the effectiveness of thesemethods across diverse geographical structures. Importantly, ourmethods are complementary and can be integrated with existingmodels like ELG or Sym-NCO to create more robust solutions. This work was funded by the Grab-NUS AI Lab, a joint collabo-ration between GrabTaxi Holdings Pte. Ltd. and National Univer-sity of Singapore, and the Industrial Postgraduate Program (Grant:S18-1198-IPP-II) funded by the Economic Development Board ofSingapore. This research is also supported by the National ResearchFoundation, Singapore under its AI Singapore Programme (AISGAward No: AISG3-RP-2022-031).",
  "Marco Caserta and Stefan Vo. 2014. A hybrid algorithm for the DNA sequencingproblem. Discrete Applied Mathematics 163 (2014), 8799": "Guillaume Chaslot, Sander Bakkes, Istvan Szita, and Pieter Spronck. 2008. Monte-carlo tree search: A new framework for game ai. In Proceedings of the AAAIConference on Artificial Intelligence and Interactive Digital Entertainment, Vol. 4.216217. Jinho Choo, Yeong-Dae Kwon, Jihoon Kim, Jeongwoo Jae, Andr Hottung, KevinTierney, and Youngjune Gwon. 2022. Simulation-guided beam search for neuralcombinatorial optimization. Advances in Neural Information Processing Systems35 (2022), 87608772. Paulo R d O Costa, Jason Rhuggenaath, Yingqian Zhang, and Alp Akcay. 2020.Learning 2-opt heuristics for the traveling salesman problem via deep reinforce-ment learning. In Asian conference on machine learning. PMLR, 465480.",
  "Yanfei Dong, Mohammed Haroon Dupty, Lambert Deng, Zhuanghua Liu,Yong Liang Goh, and Wee Sun Lee. 2024. Differentiable Cluster Graph Neu-ral Network. arXiv:2405.16185 [cs.LG]": "Zhang-Hua Fu, Kai-Bin Qiu, and Hongyuan Zha. 2021. Generalize a small pre-trained model to arbitrarily large TSP instances. In Proceedings of the AAAIconference on artificial intelligence, Vol. 35. 74747482. Chengrui Gao, Haopu Shang, Ke Xue, Dong Li, and Chao Qian. 2023. Towardsgeneralizable neural solvers for vehicle routing problems via ensemble withtransferrable local policy. arXiv preprint arXiv:2308.14104 (2023).",
  "Ratnesh Kumar and Zhonghui Luo. 2003. Optimizing the operation sequenceof a chip placement machine using TSP model. IEEE Transactions on ElectronicsPackaging Manufacturing 26, 1 (2003), 1421": "Yeong-Dae Kwon, Jinho Choo, Byoungjip Kim, Iljoo Yoon, Youngjune Gwon,and Seungjai Min. 2020. Pomo: Policy optimization with multiple optima forreinforcement learning. Advances in Neural Information Processing Systems 33(2020), 2118821198. Yining Ma, Zhiguang Cao, and Yeow Meng Chee. 2023. Learning to SearchFeasible and Infeasible Regions of Routing Problems with Flexible Neural k-Opt.In Thirty-seventh Conference on Neural Information Processing Systems. Yining Ma, Jingwen Li, Zhiguang Cao, Wen Song, Hongliang Guo, Yuejiao Gong,and Yeow Meng Chee. 2022. Efficient Neural Neighborhood Search for Pickup andDelivery Problems. In Proceedings of the Thirty-First International Joint Conferenceon Artificial Intelligence. 47764784. Yining Ma, Jingwen Li, Zhiguang Cao, Wen Song, Le Zhang, Zhenghua Chen, andJing Tang. 2021. Learning to iteratively solve routing problems with dual-aspectcollaborative transformer. Advances in Neural Information Processing Systems 34(2021), 1109611107.",
  "Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learningwith neural networks. Advances in neural information processing systems 27(2014)": "Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency,and Ruslan Salakhutdinov. 2019. Transformer dissection: a unified understandingof transformers attention via the lens of kernel. arXiv preprint arXiv:1908.11775(2019). Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processing systems 30 (2017).",
  "Ronald J Williams. 1992. Simple statistical gradient-following algorithms forconnectionist reinforcement learning. Machine learning 8 (1992), 229256": "Yaoxin Wu, Wen Song, Zhiguang Cao, Jie Zhang, and Andrew Lim. 2021. Learningimprovement heuristics for solving routing problems. IEEE transactions on neuralnetworks and learning systems 33, 9 (2021), 50575069. Haoran Ye, Jiarui Wang, Helan Liang, Zhiguang Cao, Yong Li, and Fanzhang Li.2024. Glop: Learning global partition and local construction for solving large-scale routing problems in real-time. In Proceedings of the AAAI Conference onArtificial Intelligence, Vol. 38. 2028420292. Cong Zhang, Yaoxin Wu, Yining Ma, Wen Song, Zhang Le, Zhiguang Cao, and JieZhang. 2023. A review on learning to solve combinatorial optimisation problemsin manufacturing. IET Collaborative Intelligent Manufacturing 5, 1 (2023), e12072.",
  "A.2Model performance across varying datasetsize": "showcases in detail the bar plot results from . Here,we compare the models performance across three different sizes: afull training dataset of 20,000,000 samples, a small dataset of 50,000samples repeated across 200 epochs, and an extremely small dataset of 10,000 samples repeated across 200 epochs. In totality, we cansee that even though we reduced the dataset size significantly, theperformance at 50,000 samples is still remarkable. The models retainthe ranking in performance in this scenario. However, reducingthis dataset further sees ELG become the top performing model.This is likely because ELG leverages distinct features in the form ofpolar coordinates in a small k-Nearest Neighborhood. By havingthese explicit features, the model requires less data to translate it toa meaningful representation. Whereas for our model, we learn theimportance of the pairings entirely in the latent space, requiringmore data. It should also be noted that our representation of localityis likely a superset of ELGs approach.",
  "A.3Comparison with General Solvers": "While our approach biases the solver towards the distribution, re-cent works such as GLOP proposed generic solvers that utilizethe attention model as a Hamiltonian path solver. comparesour neural solver against GLOP. As GLOPs main premise is tobreak down a large problem and solve multiple sub-paths usinglocal solvers trained on smaller sizes. Since our test results are onTSP100, we utilize the TSP25 and TSP50 solvers. Overall, we can seethat our approach is stronger than a generically trained solver inboth speed and performance. Nevertheless, since GLOP is based onthe transformer model, our contributions can easily be integrated."
}