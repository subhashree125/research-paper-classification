{
  "ABSTRACT": "The ease of spreading false information online enables individualswith malicious intent to manipulate public opinion and destabilizesocial stability. Recently, fake news detection based on evidenceretrieval has gained popularity in an effort to identify fake news re-liably and reduce its impact. Evidence retrieval-based methods canimprove the reliability of fake news detection by computing the tex-tual consistency between the evidence and the claim in the news.In this paper, we propose a framework for fake news detectionbased on MUlti-Step Evidence Retrieval enhancement (MUSER),which simulates the steps of human beings in the process of readingnews, summarizing, consulting materials, and inferring whetherthe news is true or fake. Our model can explicitly model depen-dencies among multiple pieces of evidence, and perform multi-stepassociations for the evidence required for news verification throughmulti-step retrieval. In addition, our model is able to automaticallycollect existing evidence through paragraph retrieval and key ev-idence selection, which can save the tedious process of manualevidence collection. We conducted extensive experiments on real-world datasets in different languages, and the results demonstratethat our proposed model outperforms state-of-the-art baseline meth-ods for detecting fake news by at least 3% in F1-Macro and 4% inF1-Micro. Furthermore, it provides interpretable evidence for endusers.",
  "Summarizer": "1st - step retrieval 2nd - step retrieval CDCCOVID vaccinemandatoryattend school CDC voted in favor of adding COVID-19 vaccines to the CDCs recommended, routine immunization schedule for adults and children. States establish vaccination requirements for attending school or daycare, not the CDC. State officials consider the CDC advisory committees recommendations when setting vaccine requirements, but not necessarily follow it. MUSERs styleHumans style : A motivating example of MUSER model. Our modelsimulates a human evaluating news through three steps: (1)Summarization of the key information, (2) Retrieval andevaluation of relevant evidence: the model assesses the suffi-ciency and quality of the evidence, determining if additionalinquiries are necessary, (3) Conclusion regarding the truth-fulness of the news based on the gathered evidence. economy, and public health . This phenomenon is characterizedby the dissemination of sensationalized and alarmist content, whichcaters to the mindset of netizens and is easily exploited by the \"head-line party\" . To garner more attention, individuals are prone toshare news articles or retweet tweets featuring captivating head-lines without conducting a diligent evaluation. Consequently, thishas facilitated the rapid dissemination of fake news through socialmedia platforms, outpacing the circulation of authentic news. .An overwhelming amount of fake news on social media has made itdifficult for individuals to distinguish truth from falsehood, thereby",
  "KDD 23, August 610, 2023, Long Beach, CA, USAHao Liao and Jiahao Peng et al": "posing a substantial threat to societal stability . In light ofthese challenges, the emerging automated fake news detection hasdrawn widespread attention.Generally, the detrimental effects of fake news tend to exacerbateover time. To mitigate the ramifications of fake news dissemina-tion, it is important to promptly identify them on social platforms.Meanwhile, fake news detection can help netizens improve theirability to distinguish between true and fake news, thereby fosteringthe well-being and sustainability of social networks. Various effortshave been made by websites and social media platforms to com-bat fake news, such as Metas encouragement for users to reportuntrustworthy posts and Sina Weibos provision of a channel fordebunking rumors . Besides, fact-checking sites like FactCheck1,PolitiFact2 and Full Fact3 have also begun to hire professionals toconduct fact-checking. However, the diversity and complexity ofthe increasing volume of news data make manual verification atime-consuming and unscalable process.To tackle this problem, data mining and machine learning tech-niques were introduced to detect fake news . Intuitively, thetask of fake news detection can be framed as a binary classificationproblem. These methods commonly employ supervised learningtechniques, utilizing textual features such as sentence semanticsand news entities, to distinguish between genuine and fabricatednews articles . Though effective, these content-based meth-ods exhibit some limitations, as fake news often resembles realnews in textual features and lacks important information, such associal context . To overcome these limitations, multi-modal fakenews detection frameworks have been proposed, which considersocial context by analyzing news propagation patterns on social me-dia, such as retweet relationship networks , and user-friendrelationships . Fake news can spread rapidly and become dif-ficult to control once it has reached a wide audience . Methodsbased on social context information require a substantial amountof social context information, which may not curb the dissemina-tion of fake news in a timely manner. In addition to the temporaldelay issue of detection, methods based on social context face thechallenge of user privacy preservation. Therefore, recent researchendeavors have increasingly focused on evidence-based verificationtechniques as a means to detect fake news. These methods perceivefake news detection as an inferential process, wherein external evi-dence is employed to scrutinize the veracity of the claims presentedin news articles. By extracting and incorporating relevant informa-tion from the given evidence for claim verification, these methodsaim to improve the interpretability of fake news detection. Notably,recent studies have showcased promising outcomes regarding theeffectiveness of these approaches. .Despite substantial advancements over the years, fake newsdetection still confronts numerous challenges. Evidence-based de-tection methods suffer from the assumption that evidence is easilyaccessible, ignoring the large amount of manual effort requiredfor evidence collection. Furthermore, prior work has inadequatelyexplored complex, long-range semantic dependencies in evidence,neglecting the intricate relationships between information. Inspired by brain science , we propose a fake news inferenceframework MUlti-Step Evidence Retrieval (MUSER). The cognitiveprocesses involved in human news consumption typically involvethree steps as shown in : First, a summary of the keyfindings or claims in the text is made. Second, supporting evidencefor the claims is located and evaluated for quality, which may in-clude sources such as website data, official experiments, or research.Finally, conclusions are drawn based on the evaluated evidence. Byfollowing these steps, it is possible to ascertain the sources of infor-mation, the evidence used, evidence quality, and limitations, thushelping readers to make informed judgments about the validity ofthe information. MUSER4 automatically retrieves existing evidencefrom Wikipedia through paragraph retrieval and key evidence se-lection, eliminating the need for manual evidence collection. Piecesof evidence needed for news verification are correlated throughmulti-step retrieval. Furthermore, our model can perform early de-tection without relying on social context information and providesreasons for the authenticity of the news through retrieved evidence.Although social media can provide external information for earlyfake news detection, there are two drawbacks - privacy concernsrelated to user comments and the presence of noisy informationamong user posts. Our main contributions can be summarized asfollows: We propose an automatic fact-checking framework for fake newsdetection that is based on multi-step evidence retrieval. Ourframework can explicitly model dependencies among multiplepieces of evidence and retrieves the evidence necessary for newsverification through multi-step retrieval. The framework sim-ulates the searching behavior of people when verifying newscontent on the Internet, making it possible to narrow the gapbetween computers and human experts in fake news detection. The implementation of our proposed model includes three coremodules: text summarization, multi-step retrieval, and text rea-soning. In the multi-step retrieval module, we employ the methodof key evidence selection to control the number of hops, realizingadaptive retrieval step control. We conduct extensive experiments on three real-world datasets, and the results demonstrate the effectiveness of our model interms of improved interpretability and good performance whencompared with state-of-the-art models.",
  "RELATED WORK2.1Fake News Detection": "In recent years, researchers have collaborated with the news ecosys-tem to better define and characterize fake news through newscontent and social feedback from web users. We briefly introducerelated work from the following aspects: 1) content-based; 2) socialcontext-based; 3) evidence-based.Content-based: Content-based methods detect fake news byexploiting news text, writing style, or external knowledge aboutnews entities. Some works detect fake news by extracting newstext features, e.g., n-gram distribution and/or utilize LinguisticInquiry and Word Count (LIWC) features and sentence re-lationships based on Rhetorical Structure Theory (RST) . The",
  "MUSER: A MUlti-Step Evidence Retrieval Enhancement Framework for Fake News DetectionKDD 23, August 610, 2023, Long Beach, CA, USA": ": Performance comparison of Our model w.r.t. base-lines. We repeat the experiment 10 times, and average theresults. \"F1-Ma\" and \"F1-Mi\" denote the metrics F1-Macroand F1-Micro, respectively. \"-T\" represents \"True News asPositive\" and \"-F\" denotes \"Fake news as Positive\" in the con-text of computing the precision and recall values. A t-test isperformed on five dataset splits, with < .05. The superioroutcomes are indicated in bold and statistically significantimprovements are denoted by *.",
  "Retrieval Enhancement": "Recent work has shown that retrieving additional informationcan improve the performance of various downstream tasks .Such tasks include open-domain question answering, fact-checking,fact completion, long-form question answering, Wikipedia articlegeneration, and dialogue. In the classic and simplest form of fact-checking, with claims as query conditions, the relevant passages = {1, 2, . . . , | |} needed to verify the claims are obtained.Evidence may be contained within a paragraph, or even within asentence. Retrieve multiple relevant passages by a givenquery Q, and let the reading comprehension model extract the an-swer from . These studies all used a single-step search.Contrary to the case of single-step retrieval, evidence for sometypes of queries cannot be obtained through a single retrieval and requires multiple iterative queries. The ability to retrieve informa-tion with multiple iterations is known in the literature as multi-stepretrieval . In multi-step retrieval, evidence may need to be ob-tained with additional information from a previous search, whichmight otherwise be interpreted as not being fully relevant to thequestion and no evidence could be found. We extend the capabilityof multi-step retrieval to fake news claim verification, queryingrelevant evidence passages in an iterative retrieval manner.",
  "Natural Language Inference": "Given a statement and selected evidence sentences, the task of NLIis to predict their relation labels . The advent of large annotateddatasets, such as SNLI , CreditAssess , FEVER , hasfacilitated the development of many different neural NLI models,facilitating model development for this task . The fact verifi-cation task related to natural language inference aims to classify apair of claims and evidence extracted from Wikipedia into three cat-egories: entailment, contradiction, or neutrality. NSMN uses aconnected system of three homogeneous neural semantic matchingmodels that jointly perform document retrieval, sentence selection,and claim verification for fact extraction and verification. Soleimaniet al. retrieve and validate claims using a BERT model.With the popularity of graph neural networks, graph-based modelsare also used for semantic reasoning. EVIN proposes an evi-dence reasoning network, which extracts core semantic conflicts ofclaims as evidence to explain verification results. Our work differsfrom prior research in that we focus on classifying news claims astrue or false on a comprehensive examination of relevant evidence.",
  "PROBLEM STATEMENT": "In this section, we first define the problem of fake news detectionbased on evidence retrieval enhancement. We draw a parallel be-tween the detection of fake news and the process by which humanbeings verify the authenticity of a news article. First, we read thenews content and summarize the key information expressed inthe news (content summary), then query the evidence in multiplesteps based on the summary (multi-step retrieval), and finally inferthe authenticity of the news (i.e., Natural Language Inference). Soour problem is defined as follows: the input is only news text ,and then the news key statement is obtained through the textsummarization module. Retrieve relevant passages in Wikipediathrough to get = {1, 2, 3, . . .}, and then perform evidenceextraction to obtain = {1,2,3, . . .}. The output is the predictedprobability of news authenticity = (, ), where is the naturallanguage inference verification model. And {0, 1} representsthe binary classification labels. In this context, = 0 correspondsto fake news, while = 1 corresponds to true news.",
  "THE PROPOSED MODEL": "In this section, we propose a framework for fake news detectionbased on MUlti-Step Evidence Retrieval augmentation(MUSER). illustrates the overall architecture of MUSER. Our modelmainly consists of three modules:Part 1: Text summarization module: Simulating the humanbehavior of reading news and summarizing key news information,the proposed module extracts the key information in the news and",
  "= ({i } )": "S : Our framework unfolds in three steps: (a) Summarization of the initial news text to obtain the key statement ,corresponding to the human process of summarizing key information, (b) Evidence finding through multi-step retrieval,corresponding to the human process of querying external relevant information based on the news claim. The retriever sendsthe first paragraphs to the evidence selector, which evaluates whether the evidence meets the requirements. The correlationcoefficient between and evidence snippets is represented by (,), and a settable correlation score threshold, , is used tojudge the quality of the evidence, and (c) The textual reasoner infers the consistency of evidence and claims, corresponding tothe human process of judging news based on evidence. filters out the interference of redundant or unimportant informationin the news.Part 2: Multi-step retrieval module: Simulating the behaviorof humans querying external relevant information in response tonews statements, we incorporate a retrieval module into our model.To handle situations where the initially retrieved paragraph may notcontain the answer, we adopt a multi-step iterative retrieval method.This process starts by updating the query vector based on the keyinformation and the current query vector. The retriever modulethen uses this updated query vector for re-retrieval, enabling adeeper exploration of relevant evidence.Part 3: Text reasoning module: Simulating the behavior ofhumans to judge true or fake news based on the supplementary in-formation queried, this module can extract semantic links betweennews claims and evidence, and then classify news into two cate-gories: true news and fake news. Through the method of evidenceretrieval enhancement, the interpretability of fake news detectionis improved, thus mitigating the labor-intensive process of manualevidence extraction.",
  "Text Summarization Module": "Naturally, when reading a news article, individuals have a tendencyto summarize the key content conveyed within. In order to simulatethe ability of humans to summarize news information, we first pre-train a text summarization module. The purpose of this module is toextract the key information in the news and extract the statementsworth checking. Although pre-trained language models, such asBERT and UniLM , have achieved remarkable results in NLP scenarios, the word and subword mask language models usedin the models may not be suitable for generative text summarizationtasks. The reason is that the summarization task requires a coarser-grained semantic understanding, such as sentence and paragraphsemantic level understanding, for an effective summary generation.Inspired by the recent success in masking words and continuousspans, we pre-train a transformer-based encoder-decoder modelon a large text corpus for news summarization generation . Toleverage a large text corpus for pre-training, we design a sequence-to-sequence self-supervised objective without abstract summariza-tion. We mask sentences from news text and generate an outputsequence from the remaining sentences for extracting news sum-maries. To enhance the relevance of the generated summaries, weselect sentences that are deemed important or central to the news.A piece of news contains multiple sentences, that is, = {} ,where is the number of sentences. We select the set of sentences with the highest scores based on importance. As a proxyfor importance, we compute ROUGE1-F1 between the sentenceand the rest of the news.",
  "Multi-step Retrieval Module": "The purpose of this module is to perform retrieval enhancementbased on the key information in the news extracted in the previousstep, which is similar to humans looking up data, and finding sup-plementary information to assist in the identification of true andfake news. Single-step retrieval may lead to insufficient auxiliaryinformation retrieved. Therefore, we adopt a multi-step iterativeretrieval method to improve information sufficiency . Throughiterative retrieval and supplementation, relevant information canbe extracted more comprehensively, so as to better assist in judgingthe authenticity of news. When implementing this module, it isimportant to consider how to effectively extract the retrieved keyinformation and how to maintain the sufficiency of informationduring the multi-step iterative retrieval process.The multi-step retrieval problem we attempt to address is dividedinto three steps. In the first step, the news statement is usedto retrieve the relevant paragraph from the Wikipedia corpus.The second step is to extract evidence from the retrieved longparagraphs and extract the key evidence of the paragraphs. Finally,in the case where no evidence is found in the retrieved paragraphs,the information retrieved in this step is fused with statement togenerate a new statement for the retrieval iteration. The searchterminates when evidence is found in the retrieved passages.Paragraphs retrieval: Paragraphs retrieval is the selection ofParagraphs on Wikipedia that are relevant to a given statement.The paragraph retrieval module is based on BERT and createsdense vectors for paragraphs by computing their average tokenembedding. The relevance of paragraph to statement is givenby their dot product:",
  "=1 (,)(5)": "where (,) is the embedding of the -th token in paragraph, and || is the number of tokens in .Key evidence selection: Key evidence selection is to extractevidence-related key sentences from the retrieved relevant passages.Similar to paragraph retrieval, sentence selection can also be per-ceived as a semantic matching task, wherein each sentence withina paragraph is compared to a given statement query to identifythe most plausible evidence interval. Since the search space hasbeen reduced to a controllable size via the paragraph retrieval inthe previous step, we can directly traverse all relevant paragraphsto find key evidence. In this paper, we employ two approaches forkey evidence selection: a relevance score-based approach and acontext-aware approach.Relevance score-based selection methods rely on vector repre-sentations of statements and sentences in paragraphs. For a givenstatement , we select sentences from the retrieved relevant pas-sages = {1,2, . . . ,} whose relevance score (,) is greaterthan a certain threshold set experimentally. Details on settinglambda values can be found in Appendix A.2.3.The context-aware sentence selection method uses a BERT-basedsequence tagging model. We take as input the concatenation ofstatement claim = {1,2, ...,} and passages = {1, 2, ..., }and separate them using special tokens: [][] []. Forthe output of the model, we adopt the BIO token format, whichclassifies all irrelevant tokens as O, the first token of an evidencesentence as B evidence, and the remaining tokens of an evidencesentence as I evidence. We train a RoBERTa-large based model ,minimizing the cross-entropy loss:",
  "=1( ( ))(6)": "where is the number of examples in the training batch, is thenumber of non-padding tokens of the -th example, and ( ) isthe estimated softmax probability of the correct label for the -thtoken of the -th example. We train this model on Factual-NLI with batch size 64, Adam optimizer, and initial learning rate 5105 until convergence.Multi-step retrieval: In the process of selecting key evidence,we assess the sufficiency of the evidences relevance using a thresh-old . When the evidence is insufficient, we use iterative retrievalto supplement information. To prioritize the most significant frag-ments in the paragraph, we rank the selected fragments based ontheir scores. Similar to human behavior of recursively queryingexternal sources like Wikipedia step by step until the desired infor-mation is found, only the fragments with the highest scores will bekept. The fragment with the highest score, referred to as the \"win-ner,\" is then incorporated into the current query [[]].A reformulated query will be generated by combining the currentquery with current relevant paragraph information and updating itthrough a transformer.",
  "#Real News3994,219436#Fake News3453,393311#Total7447,612747": "The reformulated query is fed back to the retriever, which uses it toreformulate and rank the passages in the corpus. fully interactswith the snippet through the transformer, avoiding information lossduring the embedding process. The new query +1 is again sub-jected to paragraph retrieval and key evidence selection, achievingthe effect of multi-step iterative retrieval. This multi-step iterativeapproach allows our model to combine the multi-step informationneeded to validate claims from multiple Wikipedia pages.",
  "Text Reasoning Module": "The last step of our model is to infer whether the news is true orfalse through multi-step retrieved evidence and news statements.This step aligns with human behavior, where individuals gatherinformation from external sources and then evaluate the credibilityof the news based on that information. Given a news claim andrelevant evidence retrieved through a multi-step retrieval pro-cess, our text reasoning module performs a logical inference fromthe evidence to the claim. The textual reasoning model acts as anevaluator to judge whether a statement is logically consistent withthe retrieved evidence, thus identifying a pair of claims and relatedevidence as true or false. Thus, the training task of a text reasoningmodel can be perceived as a binary classification task, where thegoal is to minimize the binary cross-entropy loss function for eachnews item and its associated evidence. The cross-entropy loss isdefined as follows:",
  "=1( (, )) + (1 )(1 (, )) (8)": "is the number of samples in the current batch, = 1 meansthat claim and evidence are logically consistent, and = 0means that and are contradictory. is a pre-trained languagemodel that can perform discriminative classification tasks, suchas BERT , ALBERT and RoBERTa . In this work wechoose BERT as the discriminator, we concatenate the claim andthe evidence as the input of the discriminator, the input is [CLS]C [SEP] E [SEP], the batch size is 64, Adam optimizer and aninitial learning rate of 5 105 until convergence.",
  "Experimental Setup": "5.1.1Datasets. We conduct experiments on three real-world datasetsfor fake news detection, including two English datasets (PolitiFactand GossipCop) and one Chinese dataset (Weibo). The Englishdatasets PolitiFact and GossipCop are collected through FakeNews-Net . The Weibo dataset is obtained through crawler tools .Their key statistics are shown in .PolitiFact: Within this dataset, the news articles are divided intotwo distinct categories: real news and fake news. This classificationis determined based on the assessments provided by journalists andexperts who review political news on various websites.GossipCop: In this dataset, entertainment news articles withratings are collected from various media.Weibo: The data in this dataset are hot news topics from the SinaWeibo platform, and news is marked as rumors and non-rumors.The datasets mentioned above contain both labeled news contentand associated social information. However, since our work centerson curbing the initial propagation of fake news, we only utilize thenews text without social information. This scenario resembles thesituations where fake news detection must be performed beforesocial information becomes available.",
  "MAC (ACL21) : MAC combines multi-head word-levelattention and multi-head document-level attention, whichfacilitates interpretation for fake news detection at bothword-level and evidence-level": "GET (WWW22) : GET models claims and pieces of evi-dence as graph-structured data to explore complex semanticstructures and reduces information redundancy through thesemantic structure refinement layer. 5.1.3Implementation Details. Fake news detection is commonlyperceived as a binary classification problem, and the indicatorsused for model performance evaluation are F1, Precision, Recall,F1-Macro, and F1-Micro . The dataset is partitioned into twosets, with 75% of the data as the training set and the remaining 25%of the data as the test set. The learning rate of the Adam optimizeris uniformly set to 5 105 across all datasets. And the number oftraining epochs is set to 20 for both our model and the baselines.The hyperparameters for the baselines are configured based oncorresponding papers, with key hyperparameters carefully tunedfor optimal performance (e.g., learning rate and embedding size).All experiments are conducted on Linux servers equipped withGeForce RTX 3080 GPUs (32GB memory each) using PyTorch 1.8.0.The implementation details are in the appendix and repository.",
  "Performance Results (RQ1)": "We compare our model, MUSER, to 9 baselines, including 4 content-based methods and 5 evidence-based methods. The results are re-ported in Tables 2, 3, and 4, and we have the following observations:Firstly, it is worth noting that evidence-based methods tend topredict more correctly than content-based methods (i.e., the firstfour methods in the tables), indicating the extra value of incorpo-rating additional evidential information, which can well make upfor the insufficiency of news content features alone. The evidence-based methods rely on external evidence to verify the validity ofthe claims, reducing excessive reliance on textual schemas.Secondly, in comparison to three recent evidence-based meth-ods (GET, EHIAN, MAC), our proposed MUSER achieves superiorresults (MUSER > GET > EHIAN > MAC). In particular, MUSERimproves the performance by 3% on F1-Macro and F1-Micro com-pared to the current SOTA baseline GET on the three datasets,which can better reflect the overall detection ability of the model.Furthermore, for more fine-grained evaluation, we computed \"Truenews as Positive\" and \"Fake news as Positive\" separately. MUSERalso achieved superior results in F1, Precision, and Recall scoreson the three datasets. Accuracy is equivalent to F1-Macro and thusomitted in the evaluation.Finally, our results demonstrate that MUSER outperforms allbaseline methods in fake news detection, as indicated by the posi-tive detection metric. For instance, as far as GossipCop is concerned,the F1-False, Precision-False, and Recall-False values have beenincreased by 5%, 0.4%, and 11%, respectively. Similar obvious im-provements can be observed on other datasets. These results showthat our method exhibits a higher degree of accuracy in discerningfake news. Enhanced by multi-step iterative evidence retrieval, ourmodel can extract relevant information, so as to better assist inassessing the veracity of news. Furthermore, extensive experimentsare conducted on large public datasets for the detection of fakenews. Detailed information can be found in Appendix A.2.1.",
  "Retrieve Steps Comparison (RQ2)": "Next, we investigate the performance improvement of the numberof retrieval steps in the multi-step retrieval module. The evaluationis conducted using the commonly used F1-Macro and F1-Microscores on each dataset and results are presented in . Inorder to examine the effectiveness of key evidence selection in themulti-step retrieval process, we remove it and use a fixed number",
  "Weibo": ": Results of ablation study. MUSER represents thecomplete model performance, MUSER-RM represents theremoval of the multi-step retrieval module and MUSER-RSrepresents the removal of the text summary module. of retrieval steps to conduct experiments, and then compare it tothe model with the key evidence selection function.Firstly, we can find that in experiments where key evidence se-lection is not enabled, as the number of retrievals increases, theperformance decreases instead. This is because there is no evidencescreening for the retrieved paragraphs, which may contain redun-dant information, leading to a decrease in performance.Secondly, we observe that enabling key evidence selection resultsin improved performance compared to the scenario where keyevidence selection is not enabled. In the key evidence selectionstage, our model determines whether the current retrieval resultsinclude key evidence. When key evidence is successfully retrieved,the iterative retrieval process is halted to minimize the interferencecaused by redundant information. In other words, the selectionstrategy follows an exploratory approach, where the emphasis ison exploring relevant information first. Importantly, increasing thenumber of retrieval steps does not result in an increase in redundantinformation. The key takeaway from this experiment is that multiple retrievalsteps consistently improve performance compared to single-step re-trieval. That is, even if relevant evidence passages are not retrievedin the initial step, the retriever will continue in the subsequent iter-ative retrieval process. The performance of the model reaches itspeak around 2 to 3 retrieval steps. Beyond this point, increasing thenumber of steps does not yield significant benefits and, in fact, leadsto a degradation in performance. Interestingly, despite variations inthe difficulty level of the datasets, the optimal number of retrievalsteps remains consistent.",
  "Ablation Study (RQ3)": "In this part, comparative performance experiments are conductedto assess the necessity of each module. As depicted in ,MUSER outperforms MUSER-RM, proving the critical role of multi-step iterative evidence retrieval. Additionally, the text summariza-tion module is also important. By extracting key statements inthe news, the interference of unrelated information is mitigated,thereby achieving more accurate predictions. Furthermore, MUSERperforms better than MUSER-RS and MUSER-RM, showing thatremoving any of them leads to performance degradation, whichdemonstrates the effectiveness of our main components.",
  "Explainability Study (RQ4)": "5.5.1Case Study. In this part, we demonstrate the effectivenessof our model in facilitating a deeper understanding of the multi-step retrieval process. In particular, we present a specific exampleinvolving the evaluation of a news story concerning US PresidentDonald Trumps efforts to combat drug-related issues. The newssays \"Donald Trump marshaled the full power of government tostop deadly drugs, opioids, and fentanyl from coming into ourcountry. As a result, drug overdose deaths declined nationwidefor the first time in nearly 30 years.\" By employing key evidenceextraction and conducting a multi-step search for supplementaryevidence, MUSER successfully identifies this news as fake. This",
  "pj : , = 0.91031": "snippet : A verification example generated by MUSER in theCase study. The evidence correlation score (,) obtained bythe first step of retrieval is smaller than the threshold weset. Then proceed to the second step of retrieval to obtainmore sufficient evidence. particular case serves as a compelling demonstration of MUSERscapability to accurately assess the authenticity of the news.Specifically, shows the steps of the verification processes.After key information is extracted in the text summarization model,the first step of retrieval is performed, and relevant paragraph datais obtained from the corpus. Evidence extraction identifies informa-tion related to Donald Trump and data on drug overdose deaths inthe United States. The calculated (,) from the key evidence selec-tion is less than the preset limit value , indicating the necessity foranother retrieval step. In the second step, the snippet informationretrieved is carried forward and the statement \"The overdose deathrate did drop from 2017 to 2018. But the overdose death rate rosefrom 2018 to 2021.\" is obtained. Finally, the reasoning module judgesthe news to be fake. Evidence from multi-step retrieval makes iteasier for users to understand the judgments made by the modelon the authenticity of the news. 5.5.2User Study. In this part, we aim to determine if real-worldusers are able to accurately assess the veracity of news articles basedon the evidence retrieved by MUSER. Specifically, we conduct auser study in which there are 60 news articles randomly selectedfrom PolitiFact, GossipCop, and Weibo, with 10 fake and 10 realnews articles from each dataset. We compare the evidence retrievedby MUSER with the evidence obtained by the GET model afterrefinement by semantic structure and ask 8 participants to scorethe evidence. For each piece of news, we will give the relevant evi-dence of MUSER or GET, and then ask the participant to determinewhether the news is true or fake based on the given evidence withinthree minutes. Moreover, participants are asked to give an adjustedconfidence score about her/his conclusion according to a 5-pointLikert scale. To ensure fairness in our user study experiment, eachparticipant is given the news articles to be judged in a randomizedmanner and participate in the experiment independently. shows the results of the experiments. By comparing thelabels given by different participants, we find that the conclusionsdrawn by the participants have a high level of consistency withthe predicted labels produced by the MUSER model. This indicates",
  "CONCLUSION": "In this paper, we propose a framework for fake news detectionbased on multi-step evidence retrieval enhancementMUSER. Ourmodel leverages a three-phase methodology inspired by humanverification processes, including summarization, retrieval, and rea-soning. Through text summarization, key information is extractedfrom the news, reducing irrelevant information. The multi-stepretrieval phase enables evidence association for news verification,increasing the dependency between multiple pieces of evidence.Finally, the semantic connection between the news statement andthe evidence is analyzed for news classification into two categories:true news and fake news. The results of our experiments on threereal-world demonstrated its effectiveness. Moreover, our resultsalso show that evidence association via multi-step retrieval en-hances the interpretability of the fake news detection task, makingit easier for users to assess the credibility of information and formtheir own valid judgments.",
  "Mohammad Hadi Goldani, Saeedeh Momtazi, and Reza Safabakhsh. Detect-ing fake news with capsule neural networks.Applied Soft Computing, 101:106991, 2021. doi: URL": "Huaiwen Zhang, Quan Fang, Shengsheng Qian, and Changsheng Xu. Multi-modalknowledge-aware event memory network for social media rumor detection. InProceedings of the 27th ACM International Conference on Multimedia, pages 19421951, 2019. doi: 10.1145/3343031.3350850. URL Martin Potthast, Johannes Kiesel, Kevin Reinartz, Janek Bevendorff, and BennoStein. A stylometric inquiry into hyperpartisan and fake news. In Proceedings ofthe 56th Annual Meeting of the Association for Computational Linguistics (Volume1: Long Papers), pages 231240, 2018. doi: 10.18653/v1/P18-1022. Aditi Gupta, Ponnurangam Kumaraguru, Carlos Castillo, and Patrick Meier.Tweetcred: Real-time credibility assessment of content on twitter. In Internationalconference on social informatics, pages 228243, 2014. doi: 10.1007/978-3-319-13734-6_16. Yang Liu and Yi-Fang Wu. Early detection of fake news on social media throughpropagation path classification with recurrent and convolutional networks. Pro-ceedings of the AAAI Conference on Artificial Intelligence, 32(1):254261, 2018. doi:10.1609/aaai.v32i1.11268. Yi-Ju Lu and Cheng-Te Li.GCAN: Graph-aware co-attention networks forexplainable fake news detection on social media. In Proceedings of the 58thAnnual Meeting of the Association for Computational Linguistics, pages 505514,2020. doi: 10.18653/v1/2020.acl-main.48. Mansour Davoudi, Mohammad R. Moosavi, and Mohammad Hadi Sadreddini.Dss: A hybrid deep model for fake news detection using propagation tree andstance network. Expert Systems with Applications, 198:116635, 2022. doi: Van-Hoang Nguyen, Kazunari Sugiyama, Preslav Nakov, and Min-Yen Kan. Fang:Leveraging social context for fake news detection using graph representation. InProceedings of the 29th ACM international conference on information & knowledgemanagement, pages 11651174, 2020. doi: 10.1145/3517214.",
  "Soroush Vosoughi, Deb Roy, and Sinan Aral. The spread of true and false newsonline. Science, 359(6380):11461151, 2018. doi: 10.1126/science.aap9559": "Jing Ma, Wei Gao, Shafiq Joty, and Kam-Fai Wong. Sentence-level evidence embed-ding for claim verification with hierarchical attention networks. In Proceedingsof the 57th Annual Meeting of the Association for Computational Linguistics, pages25612571, 2019. doi: 10.18653/v1/P19-1244. Kashyap Popat, Subhabrata Mukherjee, Andrew Yates, and Gerhard Weikum.DeClarE: Debunking fake news and false claims using evidence-aware deeplearning. In Proceedings of the 2018 Conference on Empirical Methods in NaturalLanguage Processing, pages 2232, 2018. doi: 10.18653/v1/D18-1003. Nguyen Vo and Kyumin Lee. Hierarchical multi-head attentive network forevidence-aware fake news detection. In Proceedings of the 16th Conference of theEuropean Chapter of the Association for Computational Linguistics: Main Volume.,pages 965975, 2021. Weizhi Xu, Junfei Wu, Qiang Liu, Shu Wu, and Liang Wang. Evidence-awarefake news detection with graph neural networks. In Proceedings of the ACM WebConference 2022, pages 25012510, 2022. doi: 10.1145/3485447.3512122. Robert C. Coghill, John G. McHaffie, and Yi-Fen Yen. Neural correlates of interindi-vidual differences in the subjective experience of pain. Proceedings of the NationalAcademy of Sciences, 100(14):85388542, 2003. doi: 10.1073/pnas.1430684100. Andrew Gordon, Jonathan C.W. Brooks, Susanne Quadflieg, Ullrich K.H. Ecker,and Stephan Lewandowsky. Exploring the neural substrates of misinformationprocessing. Neuropsychologia, 106:216224, 2017. doi:",
  "AAAI Conference on Artificial Intelligence, 35(1):8189, 2021. doi: 10.1609/aaai.v35i1.16080": "Arkaitz Zubiaga, Ahmet Aker, Kalina Bontcheva, Maria Liakata, and Rob Procter.Detection and resolution of rumours in social media: A survey. ACM ComputingSurveys (CSUR), 51(2), 2018. doi: 10.1145/3161603. Kai Shu, Limeng Cui, Suhang Wang, Dongwon Lee, and Huan Liu. Defend:Explainable fake news detection. In Proceedings of the 25th ACM SIGKDD Inter-national Conference on Knowledge Discovery and Data Mining, pages 395405,2019. doi: 10.1145/3292500.3330935. Yiqiao Jin, Xiting Wang, Ruichao Yang, Yizhou Sun, Wei Wang, Hao Liao, andXing Xie. Towards fine-grained reasoning for fake news detection. In Proceedingsof the AAAI Conference on Artificial Intelligence, volume 36, pages 57465754,2022. doi: 10.1609/aaai.v36i5.20517. Erxue Min, Yu Rong, Yatao Bian, Tingyang Xu, Peilin Zhao, Junzhou Huang, andSophia Ananiadou. Divide-and-conquer: Post-user interaction network for fakenews detection on social media. In Proceedings of the ACM Web Conference 2022,pages 11481158, 2022. doi: 10.1145/3485447.3512163. Ruichao Yang, Xiting Wang, Yiqiao Jin, Chaozhuo Li, Jianxun Lian, and Xing Xie.Reinforcement subgraph reasoning for fake news detection. In Proceedings of the28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages22532262, 2022. doi: 10.1145/3534678.3539277. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,Naman Goyal, Heinrich Kttler, Mike Lewis, Wen-tau Yih, Tim Rocktschel, Se-bastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Proceedings of the 34th International Conference on NeuralInformation Processing Systems, 2020. doi: 10.5555/3495724.3496517. Zhijiang Guo, Michael Schlichtkrull, and Andreas Vlachos. A survey on auto-mated fact-checking. Transactions of the Association for Computational Linguistics,10:178206, 2022. doi: 10.1162/tacl_a_00454. Canasai Kruengkrai, Junichi Yamagishi, and Xin Wang. A multi-level attentionmodel for evidence-based fact checking. In Findings of the Association for Com-putational Linguistics: ACL-IJCNLP 2021, pages 24472460, August 2021. doi:10.18653/v1/2021.findings-acl.217. Yair Feldman and Ran El-Yaniv. Multi-hop paragraph retrieval for open-domainquestion answering. In Proceedings of the 57th Annual Meeting of the Associationfor Computational Linguistics, pages 22962309, July 2019. doi: 10.18653/v1/P19-1222. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Man-ning. A large annotated corpus for learning natural language inference. InProceedings of the 2015 Conference on Empirical Methods in Natural LanguageProcessing, pages 632642, 2015. doi: 10.18653/v1/D15-1075. Kashyap Popat, Subhabrata Mukherjee, Jannik Strtgen, and Gerhard Weikum.Credibility assessment of textual claims on the web. In Proceedings of the 25thACM International on Conference on Information and Knowledge Management,pages 21732178, 2016. doi: 10.1145/2983323.2983661. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal.FEVER: a large-scale dataset for fact extraction and VERification. In Proceedingsof the 2018 Conference of the North American Chapter of the Association for Com-putational Linguistics: Human Language Technologies, pages 809819, 2018. doi:10.18653/v1/N18-1074. Ankur Parikh, Oscar Tckstrm, Dipanjan Das, and Jakob Uszkoreit. A decom-posable attention model for natural language inference. In Proceedings of the2016 Conference on Empirical Methods in Natural Language Processing, pages22492255, 2016. doi: 10.18653/v1/D16-1244. Yixin Nie, Haonan Chen, and Mohit Bansal. Combining fact extraction andverification with neural semantic matching networks. in Proceedings of the AAAIConference on Artificial Intelligence, 33:68596866, 07 2019. doi: 10.1609/aaai.v33i01.33016859.",
  "Amir Soleimani, Christof Monz, and Marcel Worring. BERT for Evidence Retrievaland Claim Verification, pages 359366. 04 2020. doi: 10.1007/978-3-030-45442-5_45": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT:pre-training of deep bidirectional transformers for language understanding. InProceedings of the 2019 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies, NAACL-HLT 2019,pages 41714186, 2019. doi: 10.18653/v1/n19-1423. Jing Ma, Wei Gao, Shafiq Joty, and Kam-Fai Wong. Sentence-level evidence embed-ding for claim verification with hierarchical attention networks. In Proceedingsof the 57th Annual Meeting of the Association for Computational Linguistics (ACL2019), pages 25612571, 2019. doi: 10.18653/v1/P19-1244. Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, JianfengGao, Ming Zhou, and Hsiao-Wuen Hon. Unified language model pre-trainingfor natural language understanding and generation. In Proceedings of the 33rdInternational Conference on Neural Information Processing Systems, pages 1306313075, 2019. doi: 10.5555/3454287.3455457. Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In Proceed-ings of the 37th International Conference on Machine Learning, 2020.",
  "Jeff Johnson, Matthijs Douze, and Herv Jgou. Billion-scale similarity searchwith gpus. IEEE Transactions on Big Data, 7(3):535547, 2019": "Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings usingSiamese BERT-networks. In Proceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the 9th International Joint Conferenceon Natural Language Processing (EMNLP-IJCNLP), pages 39823992, 2019. doi:10.18653/v1/D19-1410. URL Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, OmerLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustlyoptimized bert pretraining approach. ArXiv, abs/1907.11692, 2019. Chris Samarinas, Wynne Hsu, and Mong Li Lee. Latent retrieval for large-scalefact-checking and question answering with nli training. In 2020 IEEE 32ndInternational Conference on Tools with Artificial Intelligence (ICTAI), pages 941948, 2020. doi: 10.1109/ICTAI50040.2020.00147. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, PiyushSharma, and Radu Soricut. ALBERT: A lite BERT for self-supervised learn-ing of language representations. In 8th International Conference on LearningRepresentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020. URL Kai Shu, Deepak Mahudeswaran, Suhang Wang, Dongwon Lee, and Huan Liu.Fakenewsnet: A data repository with news content, social context, and spatiotem-poral information for studying fake news on social media. Big Data, 8(3):171188,2020. Qiang Liu, Feng Yu, Shu Wu, and Liang Wang. Mining significant microblogs formisinformation identification: An attention-based approach. ACM Trans. Intell.Syst. Technol., 9(5), 2018. doi: 10.1145/3173458. Yoon Kim. Convolutional neural networks for sentence classification. In Proceed-ings of the 2014 Conference on Empirical Methods in Natural Language Processing(EMNLP), pages 17461751, 2014. doi: 10.3115/v1/D14-1181. Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy.Hierarchical attention networks for document classification. In Proceedings ofthe 2016 Conference of the North American Chapter of the Association for Compu-tational Linguistics: Human Language Technologies, pages 14801489, 2016. doi:10.18653/v1/N16-1174. Feng Qian, Chengyue Gong, Karishma Sharma, and Yan Liu. Neural user responsegenerator: Fake news detection with collective user intelligence. In Proceedings ofthe 27th International Joint Conference on Artificial Intelligence, page 38343840,2018. doi: 10.5555/3304222.3304302. Lianwei Wu, Yuan Rao, Xiong Yang, Wanzhen Wang, and Ambreen Nazir.Evidence-aware hierarchical interactive attention networks for explainable claimverification. In Proceedings of the Twenty-Ninth International Joint Conference onArtificial Intelligence, IJCAI-20, pages 13881394, 2020. doi: 10.24963/ijcai.2020/193.",
  "A.2Supplementary Experiment": "A.2.1Large Dataset Experiments. We further validate the perfor-mance of MUSER on two large public datasets. The first is the LIARdataset ( du/ william/data/liar_dataset.zip).We transform the original LIAR multi-class dataset into a binaryclassification format, where each sample is labeled as either \"true\"or \"false\". We merge the original multiple categories into these twobinary labels. We merge \"mostly true\" and \"true\" into \"true\", and\"barely-true\", \"false\", and \"pants-fire\" into \"false\" to better suit theneeds of binary classification problems. Moreover, we compare tworepresentative baseline methods, and the experimental results aregiven in Table A1.We also conduct experiments on the Fakeddit dataset ( which contains multi-modal fake newsdata (image, text). For this experiment, we select textual data only.",
  "The label used is the 2-way introduced in the data set, which isdivided into two categories: true and false. The experimental resultsare given in Table A2": "A.2.2Retrieval Budget Experiments. In order to assess the effec-tiveness of multi-step retrieval and investigate whether increasingthe retrieval budget can serve as a substitute, we performed the fol-lowing experiments. To compare with our MUSERs configuration,which is a 3-step retrieval with 1 = 30, 2 = 30, and 3 = 30, weuse three configurations of 1-step retrieval with 1 = 30, 1 = 60or 1 = 90. The results are reported in Table A3. Surprisingly,increasing the pool of 1-step retrieval has a negative effect onthe performance, and the patterns are consistent across the threedatasets. The reasons for this can be two folds. First, there exists adegree of interdependence among certain pieces of evidence, whichrequires additional information from previous retrieval steps foraccurate identification. Second, the inclusion of an excessive num-ber of paragraphs introduces a higher level of noise into the text,ultimately leading to suboptimal results. Consequently, our investi-gation has demonstrated that the proposed 1-step retrieval with anaugmented budget is not a viable alternative to MUSERs multi-stepretrieval approach. A.2.3Threshold selection. essentially serves as an evaluationmetric for the correlation between evidence and passage text. Inour research, we employed a unified threshold value of = 0.9across all datasets. To investigate the effect of value, we conductexperiments to analyze its impact on the retrieved evidence. Andthe results reveal that a low value tends to introduce more noisein the retrieved evidence, whereas a high value may inadvertentlyexclude critical evidence. For your reference, the comparison resultswith different values are given in Table A4.",
  "A.3Code Resources": "We compare the proposed framework, MUSER, with 9 baselinemethods discussed in .2, the content-based methods includ-ing TextCNN, TextRNN, TCNNURG, BERT, and the evidence-basedmethods including DeClarE, HAN, EHIAN, MAC, and GET. Theimplementation details of our proposed framework, including codeand settings, are available through the following link: Other codes were obtained as follows:",
  "A.4Corpus processing": "In this article, we use Wikipedia data as the retrieval corpus. Thedownload address of Wikipedia Chinese corpus is: and the download address of WikipediaEnglish corpus is: extract the Wikipedia corpus through WikiExtractor, whichcan extract the main article content of the corpus ending with .bzdownloaded from Wikipedia. The download address of the tool is:"
}