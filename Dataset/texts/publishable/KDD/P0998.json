{
  "ABSTRACT": "Pre-trained vision-language models like CLIP have shown powerfulzero-shot inference ability via image-text matching and prove tobe strong few-shot learners in various downstream tasks. However,in real-world scenarios, adapting CLIP to downstream tasks mayencounter the following challenges: 1) data may exhibit long-taileddata distributions and might not have abundant samples for allthe classes; 2) There might be emerging tasks with new classesthat contain no samples at all. To overcome them, we propose anovel framework to achieve efficient and long-tailed generalization,which can be termed as Candle. During the training process, wepropose compensating logit-adjusted loss to encourage large mar-gins of prototypes and alleviate imbalance both within the baseclasses and between the base and new classes. For efficient adap-tation, we treat the CLIP model as a black box and leverage theextracted features to obtain visual and textual prototypes for predic-tion. To make full use of multi-modal information, we also proposecross-modal attention to enrich the features from both modalities.For effective generalization, we introduce virtual prototypes fornew classes to make up for their lack of training images. Candleachieves state-of-the-art performance over extensive experimentson 11 diverse datasets while substantially reducing the trainingtime, demonstrating the superiority of our approach. The sourcecode is available at",
  "Equal contribution.Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain. 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "INTRODUCTION": "Over the past few years, the rapid development of deep learning and the emergence of web-scale datasets havemade large-scale pre-trained models possible. Particularly, Vision-Language (V-L) models have become a recentresearch hype due to their strong generalization capabilities aswell as promising transferability to downstream tasks. One of themost successful pre-trained V-L models is CLIP . Trained ona massive dataset of 400 million image-text pairs, CLIP utilizes acontrastive objective to align the visual and textual representationsand manages to establish a connection between images and naturallanguage. During inference, CLIP can perform zero-shot imagerecognition by simply using the class names. For example, one canadopt a prompt template like a photo of a {class} as input to thetext encoder and generate the classification weights for each class.The weights can then be used to calculate cosine similarity withimage features to get classification scores.With the rise of such powerful V-L models, extensive efforts havebeen invested into finding potential solutions to better adapt thesemodels to downstream tasks. For instance, several previous worksincluding CoOp, CoCoOp and MaPLe have exploredthe idea of prompt learning, where a sequence of learnable contextvectors is used to replace carefully-chosen hard prompts. Thesemethods have achieved impressive improvements.Despite delivering promising results, a number of existing workssuffer from two practical limitations. a) significant performancedecline under real-world long-tailed data distributions. The naturallong-tail distribution phenomenon brings class imbalance andmakes it hard to collect data for all classes, leaving some rare classes",
  "Candle (ours)2011 min71.6": "entirely void of samples. Most works fail to consider the real-worlddata distributions and suffer from severe performance degradationunder imbalanced scenarios. They also tend to overlook the valuablelabel information for unseen categories, which may be a leadingcause for a notable performance drop on these label-only classes. b)extensive computational overhead. Despite using fewer trainableparameters, most methods still need to calculate gradients throughthe models backbone and require access to the models weights.As the size of foundation V-L models continues to grow (e.g., up to80 billion ) and industry standards gradually switch to providingonly API, they may become impractical for actual application.In this paper, we aim to address the above issues and proposea novel framework to achieve efficient and long-tailed generaliza-tion which can be named as Candle. During the training process,we propose compensating logit-adjusted loss to encourage largemargins of virtual prototypes and alleviate imbalance both withinthe base classes and between the base and new classes. For efficientadaptation, we treat the CLIP model as a black box and leverage theextracted features to obtain visual and textual prototypes for predic-tion. To make full use of multi-modal information, we also proposecross-modal attention to enrich the features from both modalities.For effective generalization, we introduce virtual prototypes fornew classes to make up for their lack of training images. As shown",
  "in and , our method achieves impressive improve-ments over previous methods while cutting down the training time.In summary, the main contributions of this work include:": "We propose a novel framework named Candle for efficient andlong-tailed generalization of CLIP. To the best of our knowledge,this is the first work to explore the adaptation of V-L modelsunder an imbalanced setting. To make full use of both visual and textual information, wepropose to perform cross-modal attention on the feature space. Forbetter new class generalization, we introduce virtual prototypesand propose a novel compensating logit adjusted loss to simulta-neously alleviate the imbalance within the base classes as well asbetween the base and new classes. Our extensive experimental results demonstrate the strengthof Candle, which achieves state-of-the-art results over varioussettings while substantially reducing the training time.",
  "RELATED WORK": "Vision-Lanuage (V-L) Models. V-L foundation models have ex-perienced a substantial surge in recent years with the emergenceof different architectures such as Flamingo , CLIP , ALIGN, BLIP , CoCa , etc. These models are usually trained ona web-scale dataset comprised of massive image-text pairs to learna joint embedding space. Due to their strength in understandingopen-vocabulary concepts, V-L models have been widely exploredin various downstream tasks, such as few-shot learning ,continual learning and adversarial learning . In thiswork, we focus on adapting CLIP for new class generalization.Fine-tuning V-L Models. Despite the effectiveness of V-L mod-els (e.g., CLIP) towards generalizing to new concepts, its massivescale makes it infeasible to fine-tune the full model for downstreamtasks. Linear probing serves as a naive solution, while its per-formance deteriorates significantly under few-shot settings. CoOp proposes the idea of prompt learning, which optimizes a set ofcontext vectors instead of using the standard prompt template aphoto of a {class}. CoCoOp aims to learn more robust promptsthrough image conditioning, which optimizes an instance-specificprompt by training a meta-network. CoCoOp also proposes thenovel base-to-new setting for better examination of a models gen-eralizability. MaPLe simultaneously learns the prompts forboth the vision and language branches of CLIP. While these meth-ods have achieved impressive results under few-shot settings, theirtraining cost can be prohibitive in terms of both time and memory.Aside from prompt learning, another line of work utilizes adaptermodules for lightweight and fast adaptation. For instance, CLIP-Adapter proposes to add an MLP layer after the final visual layerand mix the transformed output with the original zero-shot outputvia a residual connection. TIP-Adapter further replaces theMLP layer with a carefully designed linear layer, whose weights arecomprised of labeled visual embeddings. Although these works havesignificantly reduced the training cost for fine-tuning CLIP, theyperform poorly under the base-to-new setting, with TIP-Adapter even unable to test on new classes.Furthermore, subsequent works have attempted to improve adap-tation by leveraging multi-modal information or adopting agenerative approach to synthesize features for categories without",
  "Zero-shot CLIP89.388.965.670.489.227.165.246.054.169.868.666.7Visual Prototypes93.480.271.795.981.441.369.864.275.278.161.773.9+4.1-8.7+6.1+25.5-7.8+14.2+4.6+18.2+21.1+8.3-6.9+7.2": "data . However, they still suffer from expensive training costs orunsatisfactory new-class generalizations. In contrast to the afore-mentioned works, this paper presents a lightweight frameworkbuilt directly upon the feature space for efficient adaptation, as wellas virtual prototypes with a novel loss function for effective newclass generalization.Imbalance Learning via Pre-trained Models. Recent researchhas found that models pre-trained on large-scale datasets can learnmore generalized representations, and can serve as an effective toolfor alleviating class imbalance issues. For instance, BALLAD and VL-LTR fine-tunes both of the entire image and text en-coder of CLIP on the downstream tasks. Wang et al. proposesto ignore the text branch of CLIP and append a decoder consistingof transformer blocks after the image encoder. LPT adopts atwo-stage method to learn both shared prompts and group-specificprompts to capture both general and specialized knowledge. PEL systematically investigates different parameter-efficient fine-tuning modules for long-tailed recognition tasks. In contrast tothese previous works, this paper deals with the new class gener-alization setting and proposes an efficient and effective approach.",
  "CLIP3.1Premilinaries": "Contrastive Language-Image Pretraining, known as CLIP ,is mainly comprised of an image encoder () and a text encoder (), which map input from the respective modality into a jointembedding space. The image encoder can be in the form of eitherResNet or ViT , whereas the text encoder is built on top ofthe Transformer architecture.During training, CLIP goes through 400 million image and cap-tion pairs, adopting a contrastive loss to pull together the corre-sponding image-text pairs while pushing apart unmatched ones.After training, CLIP can be readily used for downstream imageclassification in a zero-shot manner. Let be the input imageand {1, , } be the class descriptions. These descriptionscan be generated through prompt templates like a photo of a{class}, where the {class} token denotes the corresponding classname. Then, it extracts image features = (), textual prototypes = {1, , } = { (1), , ()}, and the predicted resultfor is:",
  "Practical Limitations of CLIP": "Despite delivering promising results, a number of existing workssuffer from two practical limitations. a) significant performancedecline under real-world long-tailed data distributions. The naturallong-tail distribution phenomenon brings class imbalance andmakes it hard to collect data for all classes, leaving some rare classesentirely void of samples. Most works fail to consider the real-worlddata distributions and suffer from severe performance degradationunder imbalanced scenarios. They also tend to overlook the valuablelabel information for unseen categories, which may be a leadingcause for a notable performance drop on these label-only classes. b)extensive computational overhead. Despite using fewer trainableparameters, most methods still need to calculate gradients throughthe models backbone and require access to the models weights.As the size of foundation V-L models continues to grow (e.g., up to80 billion ) and industry standards gradually switch to providingonly API, they may become impractical for actual application.Moreover, despite the effectiveness of CLIP in general cases, itsinsufficient usage of visual information remains a weak point. CLIPrelies highly on image-text matching for downstream zero-shotprediction, which may cause potential risks. For instance, on theFGVCAircraft dataset, the class names are different numeralversions such as 737-200 and 737-300, which hardly containany useful information; or on the UCF101 dataset, the imagesamples consist of frames from a video and do not precisely matchthe prompt templates such as a photo of a {class}.Based on this motivation, we conduct our empirical study by com-paring image-image matching with image-text matching. Specif-ically, we calculate visual prototypes as the mean value of the16-shot image features for each class. Then, we replace the textualprototypes in zero-shot CLIP with visual prototypes for prediction.Formally, let = {1, , } be the visual prototypes for eachclass, then the predicted result is:",
  ": An overview of the proposed framework": "which can be named as Candle. During the training process, wepropose compensating logit-adjusted loss to mitigate the long-tailproblems, as well as to avoid the risk of neglecting new classes dur-ing the optimization. For efficient adaptation, we directly leveragethe features extracted from the model, calculate the correspondingvisual and textual prototypes, and propose cross-modal attentionto enrich the information for both modalities. For effective general-ization, we propose to generate virtual prototypes for new classes,by which we compensate for their lack of training images.",
  "Compensating Logit-Adjusted Loss": "Loss functions designed to deal with class imbalance usually donot apply to new class generalization since there is no sample fornew classes. Therefore, we propose to consider new class gener-alization as an extreme case of class imbalance and treat visualprototypes as samples to alleviate such imbalance.On top of this, we introduce our Compensating Logit-AdjustedLoss (CLA Loss) inspired by to handle such imbalanced sce-narios. Let denote the predicted logit for class , then CLA losstakes the form of :",
  "=1 exp( + log( = ))(3)": "where ( = ) is the class prior probability. Let denote the num-ber of samples for class , ( = ) can be estimated as /=1 .Particularly, we suppose = 1 for new classes and treat the cor-responding visual prototypes as training samples. In this way, wehave found a solution to not only deal with the imbalance withinthe base classes but also compensate for the imbalance betweenthe base and new classes.",
  "Feature-Level Cross-Modal Attention": "To overcome the efficiency issue, we introduce cross-modal at-tention to leverage both visual and textual information. To furtherenhance its efficiency and practicality, we treat the model as ablack box following and apply optimizations directly withinthe feature space.Our method can divided into the following steps. First, we pre-compute and save the visual and textual prototypes for each class.Then, the features together with the prototypes are fed into the",
  "=1 exp(cos( (), ( ))/)(4)": "where is the temperature for image-text matching. However,the scarce data in the downstream tasks still makes it difficult toachieve satisfactory adaptations.To remedy this issue, we propose to enrich the features fromboth modalities by leading them to interact with each other throughcross-modal attention. Specifically, we concatenate the image fea-tures, visual prototypes, and textual prototypes together and thenfeed them into a self-attention module, considering its abil-ity to establish connections for long-dependency embeddings. LetAttn() denote the multi-head self-attention function, the output is",
  "Efficient and Long-Tailed Generalization for Pre-trained Vision-Language ModelKDD 24, August 2529, 2024, Barcelona, Spain": "To solve this issue, we introduce learnable virtual prototypes fornew classes to hold the place of missing visual prototypes. Specif-ically, we freeze the precomputed textual prototypes as well asthe visual prototypes for base classes, while treating the virtualprototypes as the corresponding visual prototypes for new classes,and optimizing them during the training stage. Other than this,the entire procedure is the same as in .2. Formally, let",
  "EXPERIMENTS5.1Experimental Settings": "We evaluate our approach Candle in the following problem set-tings: 1) generalization from base to new classes under imbalanceand few-shot settings; 2) cross-dataset transfer and 3) domain gen-eralization. For the imbalanced settings, all the training data isgenerated by down-sampling the base classes to obey an exponen-tial decay of different ratios. Let denote the number of samples inthe -th class, imbalance ratio is defined as max{}/min{}. Themaximum number of samples per class of the generated dataset isset to either 100 (if has) or the maximum number of samples perclass of the original dataset.Datasets and Evaluation. For new class generalization andcross-dataset transfer, the experiments are conducted over a totalof 11 diverse image classification datasets, including ImageNet and Caltech101 for generic object recognition, OxfordPets ,StanfordCars , Flowers102 , Food101 and FGVCAircraft for fine-grained image recognition, SUN397 for scenerecognition, DTD for texture classification, EuroSAT forsatellite image classification, and UCF101 for action recognition.For domain generalization, we use ImageNet as the source datasetand four other variants that exhibit different types of domain shiftas the target datasets, including ImageNet-A , ImageNetV2 ,ImageNet-Sketch , and ImageNet-R .Details of the 11 datasets used in base-to-new generalization andcross-dataset transfer, and the 4 datasets used during testing for do-main generalization, are shown respectively in and .We report mean-class accuracy for the imbalanced settings, whichis different from overall accuracy for datasets with imbalanced test",
  "ImageNet-A 2007500ImageNetV2 100010000ImageNet-Sketch 100050889ImageNet-R 20030000": "sets. The test set for some datasets have varying numbers of sampleper class, which is indicated in the rightmost column.Following the setting in CoCoOp , we examine our modelon a similar but more practical scenario, where the base trainingset follows an imbalanced distribution. We also report test resultsof new class generalization in the balanced few-shot form to showthe robustness of our model. Note that for imbalanced scenarios,we report mean-class accuracy instead of overall accuracy.Baselines. We compare our method to zero-shot CLIP ,CoOp , CoCoOp and LFA , which also focuses onfeature-level adaptation for CLIP. For the imbalanced settings, ourmethod is compared to CoOp and CoCoOp by switching their lossfunction to Logit-Adjusted (LA) Loss to ensure fairness. LFA isonly compared under the balanced setting because its frameworkis not compatible to different loss functions.Implementation Details. We use ViT-B/16 as the vision back-bone for all methods for fair comparison. Our models are trainedfor 10-100 epochs on each dataset and use the SGD optimizer with abatch size of 128, learning rate of 3104, weight decay of 5104,and momentum of 0.9. The temperature parameter for image-textmatching is set to 0.01 following CLIP, whereas the for image-image matching is decided by searching from {0.005, 0.01, 0.02, 0.05,0.1} on each dataset. For the baseline methods, the results are gen-erated by following the exact setting as introduced in the originalarticles. All the experiments are carried out on a single NVIDIAGeForce RTX 3090.",
  "Candle (Ours)94.9595.8371.7884.6290.7036.6878.0565.6980.1781.7278.20": "Absolute improvement (%) OxfordPets Food101 Caltech101 EuroSATSUN397 Flowers102 DTD StanfordCars FGVCAircraft UCF101 -0.6-0.2 +1.0 +1.2 +2.4 +3.4 +3.6 +4.0 +5.1 +5.6 Ours vs. CoCoOp + LA Loss, Imbalance ratio=10 Absolute improvement (%) OxfordPets Food101EuroSAT Caltech101 SUN397 StanfordCars Flowers102 UCF101 FGVCAircraft DTD -1.1-0.3 +0.5 +1.5+1.6 +3.3 +3.7 +5.0 +6.2 +7.5 Ours vs. CoCoOp + LA Loss, Imbalance ratio=20 Absolute improvement (%) Food101 OxfordPetsCaltech101 EuroSATSUN397 StanfordCars UCF101 DTD FGVCAircraft Flowers102 -0.2+0.0 +0.7 +1.0 +1.3 +1.9 +3.4 +3.7 +5.1 +6.8 Ours vs. CoCoOp + LA Loss, Imbalance ratio=50",
  ": Absolute improvement on the base classes with imbalance ratio 10, 20, 50": "Absolute improvement (%) OxfordPets Food101SUN397 Caltech101 StanfordCars DTD FGVCAircraft UCF101 Flowers102 EuroSAT -0.8-0.6+0.2+0.6+0.7+1.5 +3.2 +7.0 +9.3 +27.6 Ours vs. CoCoOp + LA Loss, Imbalance ratio=10 Absolute improvement (%) Food101 OxfordPetsCaltech101 SUN397 StanfordCars FGVCAircraft DTD Flowers102 UCF101EuroSAT -0.5-0.4-0.3+0.9+1.8 +4.4+5.0+5.0 +15.2 +30.3 Ours vs. CoCoOp + LA Loss, Imbalance ratio=20 Absolute improvement (%) Caltech101 Food101 OxfordPets StanfordCars SUN397 FGVCAircraft UCF101 DTD Flowers102 EuroSAT -0.6-0.6 +0.8 +1.7 +2.4 +4.4+4.9 +7.9 +9.2 +21.8 Ours vs. CoCoOp + LA Loss, Imbalance ratio=50",
  "Main Results": "Generalization from base to new classes. For generalizationfrom base to new classes, we partition each dataset into two disjointsubsets, namely base classes and new classes. Then, the model istrained on the imbalanced base set and subsequently tested on baseand new classes to demonstrate its generalization ability. presents the harmonic mean values for the base-to-new settingover imbalance ratios {10, 20, 50}. ImageNet is skipped here dueto the extremely high training cost for CoCoOp under this setting.The results show that Candle consistently achieves state-of-the-artresults across different imbalance ratios. Specifically, the harmonicmean values of Candle outperform the best previous method by an",
  "Candle (Ours)91.388.964.668.385.524.266.144.648.467.264.9": "average of 3.67%, 4.60%, and 3.88% under the three imbalance ratios.We also illustrate the absolute improvements of Candle compared tothe previous best method in and . The results showaverage improvements of 2.58%, 2.79%, and 2.27% on the base classesand higher average improvements of 4.87%, 6.11%, and 5.19% onthe new classes, affirming that it does indeed compensate for newclasses. Together, the above results demonstrate the effectiveness ofour approach in addressing imbalance both within the base classesand between the base and new classes. We present detailed resultsfor each setting in the appendix due to the page limit. To examine the robustness of Candle, we also report the resultsfor 16-shot base-to-new generalization in . In this setting,Candle still achieves an improvement of 1.21% in average harmonicmean over the best previous method. Specifically, it performs com-parably with LFA on the base classes (+0.14% by average) but out-performs LFA on the new classes by a large margin (+1.99% byaverage), thus validating its ability to help with new classes.In addition, by taking a closer look at the results for each dataset,Candle achieves significant gains on datasets such as Flowers102,FGVCAircraft, EuroSAT, and UCF101. This is in accordance with",
  "Candle (Ours)71.649.162.848.375.0": "the analysis in .2 that CLIP performs poorly on datasetswhere textual information is relatively unreliable, and our proposedapproaches alleviate this issue by leveraging both visual and textualinformation.Cross dataset transfer. For cross-dataset transfer, we train themodel on an imbalanced ImageNet subset with an imbalance ratio of100 and subsequently test the model on the other 10 datasets. presents the results for cross-dataset transfer. Candle shows similarresults compared to CoCoOp with LogitAdjusted Loss across the10 target datasets, achieving an average improvement of 0.3%. Itsworth noting that the baseline methods require much more trainingtime compared to ours. For CoCoOp, 10 epochs of training lasts for1 day and 6 hours and inference alone takes up 3 hours, while ourmethod only needs about 20 minutes for the whole training pro-cess. Nonetheless, our method Candle is able to deliver comparableresults while significantly reducing computational cost. Similar tothe base-to-new generalization task, performance gains on specificdatasets can be observed in this task as well.Domain generalization. For domain generalization, we trainthe model on an imbalanced ImageNet subset with an imbalanceratio of 100 and evaluate the model on four domain-shift targetdatasets. The results are presented in . Candle achieves im-provements over the previous best method on 3 out of 4 targetdatasets, with an average increase of 0.15% on the target datasets,and an increase of 0.3% on the source dataset. The results demon-strate the robustness of Candle against domain shifts.",
  "Ablation Study": "Impacts of different loss functions. In the main results, we equipthe baseline methods with the balanced LA loss for fair compari-son. Here, we further examine the robustness of different methodsagainst class imbalance without the assistance of such a tailoredloss function. Specifically, we use cross entropy (CE) loss for allthe methods and run on the imbalanced base-to-new generaliza-tion task. The results are shown in . It can be observed thatCoCoOp suffers from a more severe performance drop without LAloss, i.e., a decrease of 5.78% in average harmonic value. In con-trast, our method Candle manages to hold on with a drop of only1.51%. This indicates that even without the balanced logit-adjustedloss, our model still shows potential strength in dealing with classimbalance.Effect of cross-modal attention. We conduct ablation studyon the imbalanced base-to-new generalization task to examine theeffectiveness of the cross-modal attention module. For the sake ofsimplicity, we examine on the imbalance ratio = 50 setting. The",
  ": Ablation studies on cross-modal attention (left)and virtual prototypes (right). The experiment is conductedon the imbalanced base-to-new generalization task with animbalance ratio of 50": "current model is compared to one with only linear projection afterthe extracted features, with the rest of the settings the same. Theresults are shown in the left part of . Without the cross-modal attention module, the average results on the base and newclasses experience a drop of 1.19% and 0.85% respectively, leading toa 1.02% decline of harmonic mean value. These figures clearly showthat our cross-modal attention module acts contributes positivelyand significantly to the models overall performance.Effect of virtual prototypes. We further examine the effec-tiveness of the virtual prototypes. Since the removal of virtualprototypes renders the image-image matching for new classes un-achievable, the model in comparison can only leverage image-textmatching on the new classes. We conduct comparison experimentsand report the results in the right part of . The results showthat, the performance gap on the base classes is relatively small,with our proposed model holding an average advantage of 0.46%.However, the performance on the new classes drops remarkably inresponse to the removal of virtual prototypes, showing an averagedecline of 2.27% across different datasets. The results prove thatthe introduction of virtual prototypes significantly helps with newclass generalization.",
  "CONCLUSION": "In this paper, we aim to address the new class generalization prob-lem for vision-language models under more practical scenarios,where the data may exhibit a long-tailed distribution. We proposea novel and simple framework named Candle to solve this issue inan efficient manner. Candle achieves state-of-the-art performanceover extensive experiments on diverse image classification datasets,with an especially strong generalization on the new classes. Justas significantly, the proposed framework directly optimizes in thefeature space and does not need access to model weights, whichalso contributes to its economical training cost compared to pastmethods. We hope our work serves as an inspiration for furtheradvances in exploring efficient and long-tailed generalization forvision-language models.",
  "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residuallearning for image recognition. In CVPR": "Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. 2019.EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and LandCover Classification. IEEE Journal of Selected Topics in Applied Earth Observationsand Remote Sensing (2019). Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, EvanDorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. 2021. Themany faces of robustness: A critical analysis of out-of-distribution generalization.In ICCV.",
  "Jiawei Ren, Cunjun Yu, Xiao Ma, Haiyu Zhao, Shuai Yi, et al. 2020. Balancedmeta-softmax for long-tailed visual recognition. In NeurIPS": "Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, RossWightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, MitchellWortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Lud-wig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. 2022. LAION-5B: An openlarge-scale dataset for training next generation image-text models. In NeurIPS. Christoph Schuhmann, Robert Kaczmarczyk, Aran Komatsuzaki, Aarush Katta,Richard Vencu, Romain Beaumont, Jenia Jitsev, Theo Coombes, and ClaytonMullis. 2021. LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs. In NeurIPS Workshop Datacentric AI.",
  "AADDITIONAL RESULTS": "Generating imbalanced datasets. Following the method pro-posed by Cao et al. , we generate imbalanced versions from theoriginal datasets to obey an exponential decay of a given ratio. Let denote the number of samples in the -th class, the imbalanceratio is defined as max{}/min{}. From , we can seethat the training set for some datasets only has around 30 samplesper class (FGVCAircraft) whereas some has over 1000 (ImageNet).Therefore, we set the maximum number of samples per class of thegenerated dataset to be either 100 (if has) or the maximum numberof samples per class of the original dataset, to guarantee enoughdata to form a valid imbalanced distribution. Additionally, in caseswhere the maximum number of samples per class is lower than the imbalance ratio, we ensure there is at least 1 sample instead of 0for the tail classes.Imbalanced base-to-new generalization details. In ,we show the full results of imbalanced base-to-new generaliza-tion, including the accuracy of different methods on base and newclasses, under imbalance ratios {10, 20, 50}. On the base classes, ourmethod exhibits a slight edge over CoOp+LA Loss with an increaseof 0.3% averaging across different ratios and shows a clearer im-provement (2.7%) over CoCoOp+LA Loss, which is the previous bestmethod in harmonic mean value. On the new classes, our methodfar outperforms CoOp+LA Loss with an advantage of 14.4% andstill leads CoCoOp+LA Loss by 5.3%. This again demonstrates thatour method compensates significantly for the new classes whilepreserving a strong performance on the base classes.",
  "Different attention strategies. As mentioned in the article, weperform cross-modal attention by concatenating image features,visual prototypes and textual prototypes together, and then feed": "them into a self-attention module. Here, we provide analysis ofdifferent attention strategies by adding different input masks. Forthe sake of simplicity, we consider the concatenated inputs to becomprised of two parts, the visual part (image features + visualprototypes) and the texual part (texual prototypes). Hence, thereare three different kinds of masks to choose from, including mask-ing attention within each part and between each part. shows the results of comparing different attention strategies withthe original method (no mask at all). The consistent decline provesthe superiority of the original design. Particularly, the removal ofattention between visual and textual part leads to the largest dropon both base and new classes, which goes to show that the interac-tion between different modalities does contribute to improving themodels performance."
}