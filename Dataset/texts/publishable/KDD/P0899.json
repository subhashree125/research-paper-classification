{
  "AI Institute, University of South Carolina, Columbia, SC": "AbstractDistracted driving is a leading cause of road accidents globally. Identification of distracted driving involves reliably detectingand classifying various forms of driver distraction (e.g., texting, eating, or using in-car devices) from in-vehicle camera feedsto enhance road safety. This task is challenging due to the need for robust models that can generalize to a diverse set of driverbehaviors without requiring extensive annotated datasets. In this paper, we propose KiD3, a novel method for distracteddriver detection (DDD) by infusing auxiliary knowledge about semantic relations between entities in a scene and the structuralconfiguration of the drivers pose. Specifically, we construct a unified framework that integrates the scene graphs, and driverspose information with the visual cues in video frames to create a holistic representation of the drivers actions. Our resultsindicate that KiD3 achieves a 13.64% accuracy improvement over the vision-only baseline by incorporating such auxiliaryknowledge with visual information. The source code for KiD3 is available at:",
  ". Introduction": "Distracted driving is a leading cause of road accidentsglobally, posing significant challenges to road safety. Ac-cording to the National Highway Traffic Safety Adminis-tration (NHTSA)1 approximately 3,308 people lost theirlives in the United States in 2022 due to distracted driving,and nearly 290,000 people were injured. Almost 20% ofthose killed in distracted driving-related crashes werepedestrians, cyclists, and others outside the vehicle. Inaddition to the loss of lives and injuries, the financial bur-den from distracted driving crashes collectively amountsto $98 billion in 2019 alone, highlighting the urgency ofdeveloping effective detection methods. The task of identifying distracted driving involves re-liably detecting and classifying various forms of driverdistraction, such as texting, eating, or using other ob-jects/devices from in-vehicle camera feeds. This task ischallenging due to the need for robust models that cangeneralize to a diverse set of driver behaviors withoutrequiring extensive annotated datasets. Traditionally, theDDD task has been solved using various end-to-end learn- KiL24: Workshop on Knowledge-infused Learning co-located with30th ACM KDD Conference, August 26, 2024, Barcelona, Spain (I. B. Balappanawar); (A. Chamoli); (R. Wickramarachchi); (A. Mishra); (P. Kumaraguru); (A. Sheth)",
  "Copyright for this paper by its authors. Use permitted under Creative Commons LicenseAttribution 4.0 International (CC BY 4.0).1": "ing and computer vision techniques, including, but notlimited to, object detection, pose estimation, and actionrecognition. On the other hand, recent advancementsin knowledge infusion and Neurosymbolic AI provide new opportunities for challenging tasks in sceneunderstanding and context understanding .Hence, we posit that there is valuable auxiliary knowl-edge that can be either computed/ derived from the visualinputs. Specifically, we hypothesize that by infusing suchknowledge with current computer vision models wouldimprove the overall detection capabilities and robustnesswhile not requiring the heavy computation demands ofultra-high parameter models. To this end, we propose KiD3, a novel, simplisticmethod for distracted driver detection that infuses aux-iliary knowledge about inherent semantic relations be-tween entities in a scene and the structural configurationof the drivers pose. Specifically, we construct a unifiedframework that integrates scene graphs and the driverspose information with visual information to enhancethe models understanding of distraction behaviors (see). Conducting experiments on a real-world, open dataset,our results indicate that incorporating such auxiliaryknowledge with visual information significantly im-proves detection accuracy. KiD3 achieves a 13.64% accu-racy improvement over the vision-only baseline, demon-strating the effectiveness of integrating semantic andpose information in DDD tasks. This improvement high-lights the potential of our method to contribute to saferdriving environments by providing a more reliable, effi-",
  "Sampled FramePose EstimationObject InformationScene Graph Information": ": This figure illustrates the process of extracting detailed information from a scene to analyze driver behavior. Theextreme left panel shows an image of a driver which is sampled from the video. The middle left panel presents the correspondingestimated pose, highlighting how structured representations can be derived from raw image data. The middle right panelpresents the object information obtained via object detection.The extreme right panel provides an sample relation from thescene graph, capturing the relationships between different objects and actions.",
  ". Related Work": "Distracted Driver Detection is generally formulated asone of 2 tasks: Action Recognition/Classification andTemporal Action Localization (TAL). Action recogni-tion is a computer vision task that involves classifyinga given image or a video into a set of pre-defined setof actions or classes. TAL, on the other hand detectsactivities being performed in a video streams and outputsstart and end timestamps. In this paper, we focus onsolving the action recognition task by classifying framesinto various distracted driver activities. Here, we explorerelated work considering two directions: (1) methodsfor distracted driver identification and (2) methods forgenerating/encoding semantic graphs from visual scenes. Existing Methods for DDD: Vats et al. proposesKey Point-Based Driver Activity Recognition that ex-tracts static and movement-based features from driverpose and facial features and trains a frame classificationmodel for action recognition. Then, a merge procedure isused to identify robust activity segments while ignoringoutlier frame activity predictions.",
  "In their work, Tran et al. utilize multi-view syn-chronization across videos by training an ensemble 3D": "action recognition model on each view and taking theaverage probability over all the views as the final output.The outputs are then post-processed for predicting theaction label and temporal localization of the predictedaction. This work utilizes the X3D family of networks for video classification instead of relying on manualfeature engineering. Wei Zhou et al. improve uponthis work by fine-tuning large pre-trained models insteadof training from scratch and by empirically selecting spe-cific camera views for specific distracted action classes. Previous works mainly focus on the use of so-phisticated post-processing algorithms, use of largerencoder-decoder architectures and multi-view syn-chronization to improve action recognition and TALperformance. In contrast, our work aims to improveclassification performance by incorporating auxiliaryknowledge (e.g., semantic entities/relationships of a frame,pose information) that can be derived and infused asgraphs into the encoder side of our architecture. Next,we will explore the state-of-the-art methods for scenegraph generation. Scene Graph Generation (SGG) refers to the task ofautomatically mapping an image or a video into a seman-tic structural scene graph, which requires the correctlabeling of detected objects and their relationships .Yuren Cong et al. pose SGG as a set prediction prob-lem. They propose an end-to-end SGG model, RelTR,with an encoder-decoder architecture. In contrast tomost existing scene graph generation methods, such asNeural Motif, VCTree, and Graph R-CNN, which RelTR used as benchmarks, RelTR is a one-stagemethod that predicts sparse scene graphs directly onlyusing visual appearance without combining entities andlabeling all possible predicates. Due to its simplicity, ef-ficiency and SOTA performance, we selected RelTR togenerate SGGs for our experiments.",
  "We define a classifier model : R3 18": "that maps a video frame to a probability distributionover the 18 activities. Specifically, (x) = p, wherep = [1, 2, . . . , 18] and represents the probabilitythat the frame x belongs to class , such that 18=1 =1and0 1 {1, . . . , 18}. The predictedclass for the frame x can therefore be determined by: = arg max .",
  ". Datasets for DDD": "The real-world datasets for distracted driver identifica-tion typically include annotated video sequences fromcameras mounted inside the vehicle. While several opendatasets are available, such as StateFarmDataset2, wehave selected SynDDv1 to be used for experimentsdue to the higher number of distracted behavior classesand the diversity, including variations in lighting con-ditions, driver appearances, and the use of objects and",
  "Shaking or dancing with music": "people in the background. SynDDv1 consists of 30 videoclips in the training set and 30 videos in the test set. Thedataset consists of images collected using three in-vehiclecameras positioned at locations: on the dashboard, nearthe rear-view mirror, and on the top right-side windowcorner, as shown in and . The videosequences are sampled at 30 frames per second at a reso-lution of 19201080 and are manually synchronized forthe three camera views. Each video is approximately10 minutes long and contains all 18 distracted activitiesshown in . The driver executed these activitieswith or without an appearance block, such as a hat or sun-glasses, in random order for a random duration. Thereare six videos for each driver: three videos with an ap-pearance block and three videos without any appearanceblock.",
  ". Data Preprocessing": "From the dataset, we selected the Dashboard variant, re-sulting in 10 videos for training and 10 videos for testing.Sets of (frame, label) were created by sampling framesfrom the videos at regular intervals and obtaining thecorresponding labels from the annotations. The publiclyavailable dataset contains various inconsistencies in theannotation format provided as CSV files. These inconsis-tencies, such as different naming conventions, variationsin capitalization, and extra spaces in names, have beenresolved to ensure consistency across all data splits.",
  ". Background": "Pose estimation is a critical component in understand-ing the spatial configuration of a subjects body, whichin this case is the driver. By capturing the positions ofkey body parts, pose estimation provides valuable in-formation about the drivers posture and movements.This information is essential for accurately classifyingthe drivers activities. Various methods can be employedfor pose estimation, including 2D and 3D approaches.We opted to use a state-of-the-art 2D pose estimationtechnique to effectively capture the required spatial data.",
  ". Technical Details": "We utilized OpenPose , a state-of-the-art 2D pose es-timation model, to extract pose information. OpenPosecan detect and output a set of key points correspondingto various body parts, such as the head, shoulders, el-bows, and hands. These key points are represented ascoordinates in a 2D space. The process involves detectingthe spatial locations of these joints and constructing apose structure that reflects the drivers body configura-tion. Mathematically, each key point can be representedas: k = (, ) where k denotes the -th key pointwith and being its coordinates in the image frame.",
  ". Pre-processing and Adaptation": "To adapt the pose estimation data for our task, we pre-processed the key point coordinates obtained from Open-Pose. The key points were normalized and structured toconsistently represent the drivers pose.Additionally, we derived features such as the distancebetween the hands and eyes/face, the angle formed bythe eyes with the neck, and the distance between thehands and objects like a phone or bottle (if detected usingYOLO ). These features were crucial for enhancing themodels ability to accurately interpret and classify thedrivers activities.",
  ". Unified Pipeline": "We construct a simple machine-learning pipeline to com-bine the latent encodings of the above modules. Eachmodule takes an image as input and processes it into ameaningful vector representation. We then concatenatethese representations using a feed-forward MLP to clas-sify the input image. Algorithm 1 succinctly outlines themain steps of this pipeline.",
  "Pre-processing and Adaptation. To adapt VGG16 for our task,": "we ne-tuned the model to obtain image embeddings. Specically,we discarded the last 2 classier layers of the pre-trained VGG16model and retained the base model along with the rst 4 classierlayers. This conguration results in a 4096-dimensional image em-bedding vector. The rationale for discarding the last 2 layers is thatthe nal layer reduces the dimensionality to only 18, which is insuf-cient for our needs. Additionally, the earlier layers capture moregeneral features, which are benecial for transfer learning. Theseembeddings are then used for further processing and classicationtasks.",
  "Background. Scene graphs structurally represent the rela-": "tionships between various objects in a given image. Each node inthe graph represents an object, while edges denote the relation-ships between these objects; for example consider the triple: \" manholding phone \". Scene graphs capture the high-level contextualand semantic information of the scene, going beyond pixel-leveldata. They are also essential for scene understanding and reasoningand allow us to explicitly inject knowledge into the pipeline. Forexample, considering DDD task, a scene graph containing the triple\" person drinking_from bottle \" might indicate distracted driv-ing activity. Modeling such important relations can otherwise beachieved implicitly using methods such as convolutional-network-based image encoders, with some uncertainty.",
  "Technical Details. We utilized OpenPose , a state of the": "art 2D pose estimation model, to extract pose information. Open-Pose can detect and output a set of key points corresponding tovarious body parts, such as the head, shoulders, elbows, and hands.These key points are represented as coordinates in a 2D space. Theprocess involves detecting the spatial locations of these joints andconstructing a pose structure that reects the drivers body con-guration. Mathematically, each key point can be represented as:k8 = (G8,~8) where k8 denotes the 8-th key point with G8 and ~8being its coordinates in the image frame.",
  ". Training": "We first fine-tune the pre-trained image encoder on thedistracted driver classification task to obtain task-suitableembeddings. During training, we freeze the Image En-coding and Pose Information modules and only train thelinear classifier and the GCN graph encoder in the SceneGraph Encoding module. We use activationin the final layer of the feed-forward MLP and use theCross-Entropy loss function.",
  ". Method 1 - Vision Only": "In the first experiment, we utilized existing computer vi-sion (CV) models to establish a baseline performancefor the frame classification task.We fine-tuned theVGG-16 model to assess the performance of traditionalCV models. To achieve this, we froze the weights ofthe entire model and unfroze only the classificationlayers (model.classifier[1...6]). The sixth classificationlayer nn.Linear(4096,1000) was replaced withnn.Linear(4096, 18) to match the number of activ-ity classes. The modified model was then fine-tuned on",
  ". Method 2 - Vision + Scene Graphs": "In the second experiment, we use the VGG-16 similar tohow it was used in Method 1; however, out of the lastsix classifier layers, we discarded the last two layers andused the base model with the first four classifier layersto obtain a 4096-dimensional image embedding vector.The rationale is that the final layer could not be utilizedbecause it reduces the image embedding to only 18 di-mensions, which is insufficient for capturing the richfeatures needed for our task. Moreover, earlier layers inthe network capture more general features beneficial fortransfer learning. Then, we integrate image embeddingswith scene graphs encoded using a Graph ConvolutionalNetwork (GCN) . The embeddings derived from theGCN are concatenated with the image embeddings ob-tained from the VGG-16 model. Linear layers are used asa head to combine these information streams, forming aunified representation. This combined model was trainedon the same classification objective, leveraging both thevisual and relational features present in the data.",
  ". Method 3 - Vision + Scene Graphs +Pose Information": "In the final experiment, we further enrich the scene rep-resentation by incorporating pose information, enhanc-ing its ability to understand the drivers activities. Thepose details included the location of objects via bound-ing boxes and the outline of the human skeleton withcoordinates of key points such as the eyes, nose, andfists. We engineered additional features based on exter-nal knowledge, including the distance between the handand face and the distance between the hand and a phoneor bottle (if detected using YOLO ). These engineeredfeatures were added to the concatenation of image em-beddings and scene graph embeddings. The model isthen re-trained on the classification task with these addi-tional features, providing a holistic understanding of thedrivers activities.",
  ". Results": "summarizes the results of our experiments on thetest set and the ablation studies across different methodvariations. We evaluate the performance using two met-rics: accuracy and the F1 score. The vision-only modelachieves 79.64 overall accuracy and 0.81 F1 score, respec-tively. With the inclusion of scene graphs, the accuracyand the F1 score increased by 11.88% and 9.88%, respec-tively. Finally, the complete model incorporating bothscene graphs and pose information achieves the peakperformance of 90.5% accuracy and 0.91 F1 score, respec-tively. : F1 scores and support for individual activity (i.e.,Class 1 - 18) prediction across three methods, with Method 2(i.e., Vision + SGG) and Method 3 (i.e., Vision + SGG + PoseInfo) showing improvements over Method 1 (i.e., Vision only). We have observed (see ) that our methodsare particularly effective in identifying classes such asEating (class 5), Adjusting Control Panel (class 10), andSinging with Music (class 17). We interpret this as evi-",
  ". Discussion": "Our results clearly support the initial hypothesis thatthe inclusion of valuable auxiliary knowledge with vi-sual features would enhance the performance of the DDDtask. The ablation study further establishes each auxiliaryknowledge types role in the overall performance. Scenegraphs provided the most significant auxiliary knowl-edge, highlighting the importance of explicitly encodingsemantic information and infusing it with visual features.By incorporating pose information of driver actions, wewere able to further enrich overall accuracy and robust-ness. However, several limitations to our approach war-rant further investigation.",
  ". Limitations": "One limitation is the reliance on annotated data for train-ing. While we used a combination of supervised and un-supervised learning techniques to mitigate this issue, theavailability of annotated data remains a key constraint.Additionally, our method may struggle with complex andhighly variable driving scenarios where the relationshipsbetween objects and actions are less clear. Finally, wehave not considered using foundation models like Vi-sion Language Models (VLMs) for our experiments. Ourmain focus in this work is to evaluate the impact of aux-iliary knowledge on the DDD task without the need forcomplex, high-parameter models.",
  ". Conclusions and Future Work": "In this paper, we proposed a novel, simple approach todistracted driver detection by infusing two types of aux-iliary knowledge with visual information. Our methodleverages scene graphs and estimated pose informationwith visual embeddings to comprehensively representdriver actions. Our experimental results showcase the ef-fectiveness of infusing each type of auxiliary knowledgewith visual features to achieve 90.5% peak performanceon the DDD task.Future work will address the limitations mentionedabove, such as the reliance on annotated data and thehandling of complex driving scenarios. Additionally, weplan to explore the integration of other types of knowl-edge representations, such as temporal graphs, to furtherenhance the performance of distracted driver detectionsystems Further, we plan to investigate the role of VLMsin this task. Zhe Cao, Tomas Simon, Shih-En Wei, and YaserSheikh. 2017. Realtime Multi-Person 2D Pose Esti-mation Using Part Affinity Fields. In Proceedings ofthe IEEE Conference on Computer Vision and PatternRecognition (CVPR).",
  "Thomas N. Kipf and Max Welling. 2017.Semi-Supervised Classification with Graph Convolu-tional Networks. arXiv:1609.02907 [cs.LG]": "Alessandro Oltramari, Jonathan Francis, Cory Hen-son, Kaixin Ma, and Ruwan Wickramarachchi. 2020.Neuro-Symbolic Architectures for Context Under-standing. In Knowledge Graphs for eXplainable Ar-tificial Intelligence: Foundations, Applications andChallenges. IOS Press, 143160. Peng Ping, Cong Huang, Weiping Ding, YongkangLiu, Miyajima Chiyomi, and Takeda Kazuya. 2023.Distracted driving detection based on the fusion ofdeep learning and causal reasoning. InformationFusion 89 (2023), 121142. Mohammed Shaiqur Rahman, Archana Venkatacha-lapathy, Anuj Sharma, Jiyang Wang, Senem Veli-pasalar Gursoy, David Anastasiu, and Shuo Wang.2023. Synthetic distracted driving (SynDD1) datasetfor analyzing distracted behaviors and various gazezones of a driver. Data in Brief 46 (2023), 108793. Joseph Redmon, Santosh Divvala, Ross Girshick,and Ali Farhadi. 2016. You Only Look Once: Unified,Real-Time Object Detection. In Proceedings of theIEEE Conference on Computer Vision and PatternRecognition (CVPR)."
}